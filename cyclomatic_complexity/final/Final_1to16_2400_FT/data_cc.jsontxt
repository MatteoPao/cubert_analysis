{"function": "\n\ndef __init__(self, connection):\n    self.connection = connection\n", "label": 0}
{"function": "\n\ndef get_cdn_log_retention(self, container):\n    '\\n        Returns the status of the setting for CDN log retention for the\\n        specified container.\\n        '\n    return self._manager.get_cdn_log_retention(container)\n", "label": 0}
{"function": "\n\ndef test_multipath_joins():\n    (app, db, admin) = setup()\n\n    class Model1(db.Model):\n        id = db.Column(db.Integer, primary_key=True)\n        val1 = db.Column(db.String(20))\n        test = db.Column(db.String(20))\n\n    class Model2(db.Model):\n        id = db.Column(db.Integer, primary_key=True)\n        val2 = db.Column(db.String(20))\n        first_id = db.Column(db.Integer, db.ForeignKey(Model1.id))\n        first = db.relationship(Model1, backref='first', foreign_keys=[first_id])\n        second_id = db.Column(db.Integer, db.ForeignKey(Model1.id))\n        second = db.relationship(Model1, backref='second', foreign_keys=[second_id])\n    db.create_all()\n    view = CustomModelView(Model2, db.session, filters=['first.test'])\n    admin.add_view(view)\n    client = app.test_client()\n    rv = client.get('/admin/model2/')\n    eq_(rv.status_code, 200)\n", "label": 0}
{"function": "\n\ndef teetsv(table, source=None, encoding=None, errors='strict', write_header=True, **csvargs):\n    '\\n    Convenience function, as :func:`petl.io.csv.teecsv` but with different\\n    default dialect (tab delimited).\\n\\n    '\n    csvargs.setdefault('dialect', 'excel-tab')\n    return teecsv(table, source=source, encoding=encoding, errors=errors, write_header=write_header, **csvargs)\n", "label": 0}
{"function": "\n\ndef test_no_transaction_create(self):\n    manifest = shippo.Manifest.create(**self.create_mock_manifest())\n    self.assertEqual(manifest.object_status, 'NOTRANSACTIONS')\n", "label": 0}
{"function": "\n\ndef fps_return_url(self, request):\n    uri = request.build_absolute_uri()\n    parsed_url = urlparse(uri)\n    resp = self.fps_connection.verify_signature(UrlEndPoint=('%s://%s%s' % (parsed_url.scheme, parsed_url.netloc, parsed_url.path)), HttpParameters=parsed_url.query)\n    if (not (resp.VerifySignatureResult.VerificationStatus == 'Success')):\n        return HttpResponseForbidden()\n    return self.transaction(request)\n", "label": 0}
{"function": "\n\ndef __init__(self, channel, msg):\n    self._channel = channel\n    self._msg = msg\n    self._terminal_state = False\n", "label": 0}
{"function": "\n\n@utils.positional((1 + Property._positional))\ndef __init__(self, name=None, compressed=False, **kwds):\n    if compressed:\n        kwds.setdefault('indexed', False)\n    super(GenericProperty, self).__init__(name=name, **kwds)\n    self._compressed = compressed\n    if (compressed and self._indexed):\n        raise NotImplementedError(('GenericProperty %s cannot be compressed and indexed at the same time.' % self._name))\n", "label": 0}
{"function": "\n\ndef __init__(self, value, context):\n    self.value = value\n    super().__init__(context)\n", "label": 0}
{"function": "\n\ndef set_DBName(self, DBName):\n    self.add_query_param('DBName', DBName)\n", "label": 0}
{"function": "\n\ndef selflaunch(ip, line):\n    \" Launch python script with 'this' interpreter\\n    \\n    e.g. d:\\x0coo\\\\ipykit.exe a.py\\n    \\n    \"\n    tup = line.split(None, 1)\n    if (len(tup) == 1):\n        print('Launching nested ipython session')\n        os.system(sys.executable)\n        return\n    cmd = ((sys.executable + ' ') + tup[1])\n    print('>', cmd)\n    os.system(cmd)\n", "label": 0}
{"function": "\n\ndef DeferredSerializer(original, context):\n    '\\n    Serialize the result of the given Deferred without affecting its result.\\n\\n    @type original: L{defer.Deferred}\\n    @param original: The Deferred being serialized.\\n\\n    @rtype: L{defer.Deferred}\\n    @return: A Deferred which will be called back with the result of\\n        serializing the result of C{original} or which will errback if\\n        either C{original} errbacks or there is an error serializing the\\n        result of C{original}.\\n    '\n    d = defer.Deferred()\n\n    def cb(result):\n        d2 = defer.maybeDeferred(flat.serialize, result, context)\n        d2.chainDeferred(d)\n        return result\n\n    def eb(error):\n        d.errback(error)\n        return error\n    original.addCallbacks(cb, eb)\n    return d\n", "label": 0}
{"function": "\n\ndef handle_echo(remote_addr):\n    while True:\n        message = until('\\r\\n')\n        send(('you said: %s' % message))\n", "label": 0}
{"function": "\n\ndef GetCodeTypeFromDictionary(self, def_dict):\n    \"Convert a json schema type to a suitable Java type name.\\n\\n    Overrides the default.\\n\\n    Args:\\n      def_dict: (dict) A dictionary describing Json schema for this Property.\\n    Returns:\\n      A name suitable for use as a class in the generator's target language.\\n    \"\n    json_type = def_dict.get('type', 'string')\n    json_format = def_dict.get('format')\n    datatype = self.TYPE_FORMAT_TO_DATATYPE.get((json_type, json_format))\n    if datatype:\n        native_format = datatype\n    else:\n        native_format = utilities.CamelCase(json_type)\n    return native_format\n", "label": 0}
{"function": "\n\ndef test_valid_inputs(self):\n    '\\n        Ensure that valid values return the expected validated data.\\n        '\n    for (input_value, expected_output) in get_items(self.valid_inputs):\n        assert (self.field.run_validation(input_value) == expected_output)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/bio_engineer/dna_template/shared_dna_template_dalyrake.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef set_node_maintenance(self, node_id, maintenance_mode, params=''):\n    self.ironic('node-set-maintenance', params='{0} {1} {2}'.format(node_id, maintenance_mode, params))\n", "label": 0}
{"function": "\n\ndef upload_artifacts_to_staging(self, local_artifacts, repo_id, print_created_artifacts=True, upload_filelist=False):\n    '\\n        :param local_artifacts: list[LocalArtifact]\\n        :param repo_id: name of staging repository\\n        :param print_created_artifacts: if True prints to stdout what was uploaded and where\\n        :param staging: bool\\n        :param upload_filelist: if True, creates and uploads a list of uploaded files\\n\\n        :return: list[RemoteArtifact]\\n        '\n    hostname_for_download = self._staging_repository_url\n    path_prefix = 'service/local/staging/deployByRepositoryId'\n    remote_artifacts = self.upload_artifacts(local_artifacts, repo_id, print_created_artifacts, hostname_for_download, path_prefix)\n    if upload_filelist:\n        coord_list = [a.get_coordinates_string() for a in remote_artifacts]\n        data = '\\n'.join(coord_list)\n        remote_path = '{path_prefix}/{repo_id}/{filelist_path}'.format(path_prefix=path_prefix, repo_id=repo_id, filelist_path=self._get_filelist_path(repo_id))\n        self._send(remote_path, method='POST', data=data, headers={\n            'Content-Type': 'text/csv',\n        })\n    return remote_artifacts\n", "label": 0}
{"function": "\n\ndef CalculateModelPredictions(self, inCoeffs, inDataCacheDictionary):\n    x_in = inDataCacheDictionary['X']\n    a = inCoeffs[0]\n    b = inCoeffs[1]\n    c = inCoeffs[2]\n    try:\n        temp = (a * numpy.exp((b / (x_in + c))))\n        return self.extendedVersionHandler.GetAdditionalModelPredictions(temp, inCoeffs, inDataCacheDictionary, self)\n    except:\n        return (numpy.ones(len(inDataCacheDictionary['DependentData'])) * 1e+300)\n", "label": 0}
{"function": "\n\ndef delete(self, count=100000):\n    start = _time.time()\n    for i in range(count):\n        self.r.delete(('key-%d' % i))\n    finish = _time.time()\n    return (start, finish)\n", "label": 0}
{"function": "\n\ndef test_sessionmanager_save(self):\n    '\\n        Test SessionManager.save method\\n        '\n    self.session['y'] = 1\n    self.session.save()\n    s = Session.objects.get(session_key=self.session.session_key)\n    Session.objects.save(s.session_key, {\n        'y': 2,\n    }, s.expire_date)\n    del self.session._session_cache\n    self.assertEqual(self.session['y'], 2)\n", "label": 0}
{"function": "\n\ndef test_Predict32(self):\n    theano.config.floatX = 'float32'\n    for t in SPARSE_TYPES:\n        sparse_matrix = getattr(scipy.sparse, t)\n        X = sparse_matrix((8, 4), dtype=numpy.float32)\n        yp = self.nn._predict(X)\n        assert_equal(yp.dtype, numpy.float32)\n", "label": 0}
{"function": "\n\ndef __init__(self, html=None, type=None, **kwargs):\n    if ((type is None) and html):\n        type = 'xhtml'\n    super(Content, self).__init__(type=type, **kwargs)\n    if (html is not None):\n        self.html = html\n", "label": 0}
{"function": "\n\n@contextlib.contextmanager\ndef bayesdb(metamodel=None, **kwargs):\n    if (metamodel is None):\n        crosscat = local_crosscat()\n        metamodel = CrosscatMetamodel(crosscat)\n    bdb = bayeslite.bayesdb_open(builtin_metamodels=False, **kwargs)\n    bayeslite.bayesdb_register_metamodel(bdb, metamodel)\n    try:\n        (yield bdb)\n    finally:\n        bdb.close()\n", "label": 0}
{"function": "\n\ndef unpackdict(table, field, keys=None, includeoriginal=False, samplesize=1000, missing=None):\n    \"\\n    Unpack dictionary values into separate fields. E.g.::\\n\\n        >>> import petl as etl\\n        >>> table1 = [['foo', 'bar'],\\n        ...           [1, {'baz': 'a', 'quux': 'b'}],\\n        ...           [2, {'baz': 'c', 'quux': 'd'}],\\n        ...           [3, {'baz': 'e', 'quux': 'f'}]]\\n        >>> table2 = etl.unpackdict(table1, 'bar')\\n        >>> table2\\n        +-----+-----+------+\\n        | foo | baz | quux |\\n        +=====+=====+======+\\n        |   1 | 'a' | 'b'  |\\n        +-----+-----+------+\\n        |   2 | 'c' | 'd'  |\\n        +-----+-----+------+\\n        |   3 | 'e' | 'f'  |\\n        +-----+-----+------+\\n\\n    See also :func:`petl.transform.unpacks.unpack`.\\n\\n    \"\n    return UnpackDictView(table, field, keys=keys, includeoriginal=includeoriginal, samplesize=samplesize, missing=missing)\n", "label": 0}
{"function": "\n\ndef get_pdu_json(self, time_now=None):\n    pdu_json = self.get_dict()\n    if ((time_now is not None) and ('age_ts' in pdu_json['unsigned'])):\n        age = (time_now - pdu_json['unsigned']['age_ts'])\n        pdu_json.setdefault('unsigned', {\n            \n        })['age'] = int(age)\n        del pdu_json['unsigned']['age_ts']\n    pdu_json['unsigned'].pop('redacted_because', None)\n    return pdu_json\n", "label": 0}
{"function": "\n\ndef __init__(self, consumer_key, consumer_secret, token, token_secret):\n    self.consumer = oauth2.Consumer(consumer_key, consumer_secret)\n    self.token = oauth2.Token(token, token_secret)\n", "label": 0}
{"function": "\n\ndef add_data(self, data):\n    'Add a new data set to the widget\\n\\n        :returns: True if the addition was expected, False otherwise\\n        '\n    if (data in self.client):\n        return\n    self.client.add_layer(data)\n    self._coordinator._add_data(data)\n    return True\n", "label": 0}
{"function": "\n\n@toolz.memoize\ndef table_of_metadata(metadata, name):\n    if (metadata.schema is not None):\n        name = '.'.join((metadata.schema, name))\n    if (name not in metadata.tables):\n        metadata.reflect(views=metadata.bind.dialect.supports_views)\n    return metadata.tables[name]\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Creature()\n    result.template = 'object/mobile/shared_dressed_imperial_admiral_m_2.iff'\n    result.attribute_template_id = 9\n    result.stfName('npc_name', 'human_base_male')\n    return result\n", "label": 0}
{"function": "\n\ndef async_barrier(self):\n    fr = self.future_responses[:]\n    exceptions = []\n    for r in fr:\n        try:\n            self.async_response(r)\n        except Exception as e:\n            exceptions.append(e)\n    if exceptions:\n        raise Exception(max(exceptions))\n", "label": 0}
{"function": "\n\ndef _sub(self, match):\n    (before, prefix, name, postfix) = match.groups()\n    self.names.append(name)\n    if (name in self.builtins):\n        return (before + self.builtins[name])\n    elif ((prefix == '>') or (postfix == '<')):\n        return (before + self._textarea(name))\n    elif ('\"' in (prefix, postfix)):\n        self.attrs.append(name)\n        return (before + (self.require(name) or (('{{' + name) + '}}')))\n    else:\n        return (before + self._textarea(name))\n", "label": 0}
{"function": "\n\ndef find_all_tests(suite):\n    'Yields all the tests and their names from a given suite.'\n    suites = [suite]\n    while suites:\n        s = suites.pop()\n        try:\n            suites.extend(s)\n        except TypeError:\n            (yield (s, ('%s.%s.%s' % (s.__class__.__module__, s.__class__.__name__, s._testMethodName))))\n", "label": 0}
{"function": "\n\ndef test_verify_networks_resp_error(self):\n    cluster_db = self.env.create(cluster_kwargs={\n        \n    }, nodes_kwargs=[{\n        'api': False,\n    }, {\n        'api': False,\n    }])\n    (node1, node2) = self.env.nodes\n    nets_sent = [{\n        'iface': 'eth0',\n        'vlans': range(100, 105),\n    }]\n    nets_resp = [{\n        'iface': 'eth0',\n        'vlans': range(100, 104),\n    }]\n    task = Task(name='super', cluster_id=cluster_db.id)\n    task.cache = {\n        'args': {\n            'nodes': self.nodes_message((node1, node2), nets_sent),\n            'offline': 0,\n        },\n    }\n    self.db.add(task)\n    self.db.commit()\n    kwargs = {\n        'task_uuid': task.uuid,\n        'status': 'ready',\n        'nodes': self.nodes_message((node1, node2), nets_resp),\n    }\n    self.receiver.verify_networks_resp(**kwargs)\n    self.db.flush()\n    self.db.refresh(task)\n    self.assertEqual(task.status, 'error')\n    error_nodes = []\n    for node in self.env.nodes:\n        error_nodes.append({\n            'uid': node.id,\n            'interface': 'eth0',\n            'name': node.name,\n            'absent_vlans': [104],\n            'mac': node.interfaces[0].mac,\n        })\n    self.assertEqual(task.message, '')\n    self.assertEqual(task.result, error_nodes)\n", "label": 0}
{"function": "\n\ndef get_channel_list(self):\n    '\\n        Returns a sorted list of cached channels.\\n\\n        '\n    channels = listitems(self)\n    channels.sort()\n    return [value for (key, value) in channels]\n", "label": 0}
{"function": "\n\ndef get_object(self, bits):\n    slug = bits[0]\n    return Author.objects.get(slug=slug)\n", "label": 0}
{"function": "\n\ndef inner_view(request):\n    content = Template('{% url outer as outer_url %}outer:{{ outer_url }},{% url inner as inner_url %}inner:{{ inner_url }}').render(Context())\n    return HttpResponse(content)\n", "label": 0}
{"function": "\n\ndef read(self, inp):\n    '\\n        :param inp:\\n        :return: a tuple of (Enum-name, Enum-value-name)\\n        '\n    return tuple(inp.read_utf(), inp.read_utf())\n", "label": 0}
{"function": "\n\ndef gotCData(self, cdata):\n    self._gotStandalone(CDATASection, cdata)\n", "label": 0}
{"function": "\n\ndef test_super_in_class_methods_working(self):\n    d = D()\n    self.assertEqual(d.cm(), (d, (D, (D, (D, 'A'), 'B'), 'C'), 'D'))\n    e = E()\n    self.assertEqual(e.cm(), (e, (E, (E, (E, 'A'), 'B'), 'C'), 'D'))\n", "label": 0}
{"function": "\n\n@test.create_stubs({\n    neutronclient: ('create_ipsecpolicy',),\n})\ndef test_ipsecpolicy_create(self):\n    ipsecpolicy1 = self.api_ipsecpolicies.first()\n    form_data = {\n        'name': ipsecpolicy1['name'],\n        'description': ipsecpolicy1['description'],\n        'auth_algorithm': ipsecpolicy1['auth_algorithm'],\n        'encryption_algorithm': ipsecpolicy1['encryption_algorithm'],\n        'encapsulation_mode': ipsecpolicy1['encapsulation_mode'],\n        'lifetime': ipsecpolicy1['lifetime'],\n        'pfs': ipsecpolicy1['pfs'],\n        'transform_protocol': ipsecpolicy1['transform_protocol'],\n    }\n    ipsecpolicy = {\n        'ipsecpolicy': self.api_ipsecpolicies.first(),\n    }\n    neutronclient.create_ipsecpolicy({\n        'ipsecpolicy': form_data,\n    }).AndReturn(ipsecpolicy)\n    self.mox.ReplayAll()\n    ret_val = api.vpn.ipsecpolicy_create(self.request, **form_data)\n    self.assertIsInstance(ret_val, api.vpn.IPSecPolicy)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    return self.lst[key]\n", "label": 0}
{"function": "\n\ndef _make_base_dirs(self):\n    '\\n        Ensure that base dirs exists.\\n        '\n    for dir_ in [self.release_dir, self.pre_release_dir]:\n        if (not isdir(dir_)):\n            makedirs(dir_)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return (self.flickrid == other.flickrid)\n", "label": 0}
{"function": "\n\ndef _body_callback(self, buf):\n    if (self._logfile is not None):\n        self._logfile.write(buf)\n    if (self._downloadfile is not None):\n        self._downloadfile.write(buf)\n    else:\n        self._contents.append(buf)\n", "label": 0}
{"function": "\n\ndef __init__(self, server='localhost', port='8080', ssl=False):\n    '\\n        :type server: str\\n        :param server: the host to connect to that is running KairosDB\\n        :type port: str\\n        :param port: the port, as a string, that the KairosDB instance is running on\\n        :type ssl: bool\\n        :param ssl: Whether or not to use ssl for this connection.\\n        '\n    self.ssl = ssl\n    self.server = server\n    self.port = port\n    self._generate_urls()\n    metadata.get_server_version(self)\n", "label": 0}
{"function": "\n\ndef __init__(self, game):\n    self.leaderboards = {\n        \n    }\n    self.ordered_leaderboards = []\n    self.leaderboard_path = None\n    self.issues = []\n    yaml_path = unicode(get_absolute_path(join_path(game.path, 'leaderboards.yaml')))\n    total_yaml_errors = 0\n    if path_exists(yaml_path):\n        try:\n            f = open(yaml_path, 'r')\n            try:\n                file_meta = yaml.load(f)\n                for (i, m) in enumerate(file_meta):\n                    key = m['key']\n                    leaderboard = Leaderboard(game, key, m, i)\n                    num_errors = len(leaderboard.errors)\n                    if (num_errors > 0):\n                        total_yaml_errors += num_errors\n                        self.issues.append((key, {\n                            'errors': leaderboard.errors,\n                            'warnings': leaderboard.warnings,\n                        }))\n                    elif (len(leaderboard.warnings) > 0):\n                        self.issues.append((key, {\n                            'errors': leaderboard.errors,\n                            'warnings': leaderboard.warnings,\n                        }))\n                    self.leaderboards[key] = leaderboard\n                    self.ordered_leaderboards.append(leaderboard)\n            finally:\n                f.close()\n        except (IOError, yaml.YAMLError) as e:\n            LOG.error('Failed loading leaderboards: %s', str(e))\n            raise LeaderboardError(('Failed loading leaderboards.yaml file: %s' % str(e)))\n    else:\n        raise LeaderboardsUnsupported()\n    if (total_yaml_errors > 0):\n        raise ValidationException(self.issues)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef selected_reporting_group_ids(mobile_user_and_group_slugs):\n    return [g[3:] for g in mobile_user_and_group_slugs if g.startswith('g__')]\n", "label": 0}
{"function": "\n\ndef handle_deploy_cb(self, handle, connection, status, deployer, **kwargs):\n    _log.analyze(self.node.id, '+ DEPLOYED', {\n        'status': status.status,\n    })\n    if status:\n        self.send_response(handle, connection, (json.dumps({\n            'application_id': deployer.app_id,\n            'actor_map': deployer.actor_map,\n            'placement': kwargs.get('placement', None),\n            'requirements_fulfilled': (status.status == calvinresponse.OK),\n        }) if deployer.app_id else None), status=status.status)\n    else:\n        self.send_response(handle, connection, None, status=status.status)\n", "label": 0}
{"function": "\n\ndef test_bypass_in_process_cache_for_account_entities(testbed):\n    context = ndb.get_context()\n    assert (context.get_cache_policy() == context.default_cache_policy)\n    snippets.bypass_in_process_cache_for_account_entities()\n    assert (context.get_cache_policy() != context.default_cache_policy)\n", "label": 0}
{"function": "\n\ndef _combine(self, other, conn):\n    if (not isinstance(other, Q)):\n        raise TypeError(other)\n    obj = type(self)()\n    obj.add(self, conn)\n    obj.add(other, conn)\n    return obj\n", "label": 0}
{"function": "\n\ndef test_http_auth_issue_creation(self):\n    settings.REDMINE_USERNAME = 'sentry'\n    settings.REDMINE_PASSWORD = 'sentry'\n    group = GroupedMessage.objects.all()[0]\n    response = self.client.post(CreateRedmineIssue.get_url(group.pk), {\n        'subject': 'test',\n        'description': 'foo',\n    }, follow=True)\n    self.assertEquals(response.status_code, 200)\n    self.assertTemplateUsed(response, 'sentry/group/details.html')\n    self.assertTrue(RedmineIssue.objects.filter(group=group).exists())\n    group = GroupedMessage.objects.get(pk=group.pk)\n    self.assertTrue((group.data['redmine']['issue_id'] > 0))\n", "label": 0}
{"function": "\n\n@resource.register('hdfstore://.+', priority=11)\ndef resource_hdfstore(uri, datapath=None, dshape=None, **kwargs):\n    fn = uri.split('://')[1]\n    try:\n        f = pd.HDFStore(fn, **filter_kwargs(pd.HDFStore, kwargs))\n    except RuntimeError as e:\n        raise type(e)(pytables_h5py_explanation)\n    if (dshape is None):\n        return (f.get_storer(datapath) if datapath else f)\n    dshape = datashape.dshape(dshape)\n    if (datapath in f):\n        return f.get_storer(datapath)\n    return EmptyHDFStoreDataset(f, datapath, dshape)\n", "label": 0}
{"function": "\n\ndef __init__(self, stream, default_style=None, default_flow_style=None, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None, encoding=None, explicit_start=None, explicit_end=None, version=None, tags=None):\n    Emitter.__init__(self, stream, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break)\n    Serializer.__init__(self, encoding=encoding, explicit_start=explicit_start, explicit_end=explicit_end, version=version, tags=tags)\n    SafeRepresenter.__init__(self, default_style=default_style, default_flow_style=default_flow_style)\n    Resolver.__init__(self)\n", "label": 0}
{"function": "\n\ndef __init__(self, project='project', connection=None):\n    self.project = project\n    self.connection = connection\n", "label": 0}
{"function": "\n\ndef get_leaking_objects(objects=None):\n    'Return objects that do not have any referents.\\n\\n    These could indicate reference-counting bugs in C code.  Or they could\\n    be legitimate.\\n\\n    Note that the GC does not track simple objects like int or str.\\n\\n    .. versionadded:: 1.7\\n    '\n    if (objects is None):\n        gc.collect()\n        objects = gc.get_objects()\n    try:\n        ids = set((id(i) for i in objects))\n        for i in objects:\n            ids.difference_update((id(j) for j in gc.get_referents(i)))\n        return [i for i in objects if (id(i) in ids)]\n    finally:\n        del objects, i\n", "label": 0}
{"function": "\n\ndef close(self, callback=None):\n    'cleanly closes the connection to the server.\\n\\n        all pending tasks are flushed before connection shutdown'\n    if (self.status not in (status.OPENING, status.OPENED)):\n        raise AmqpStatusError('connection is not open')\n    self._close_callback = callback\n    self.status = status.CLOSING\n    for ch in self.channels.values():\n        if ((ch is not self) and (ch.status in (status.OPENING, status.OPENED))):\n            ch.close(self.close)\n    m = Close(reply_code=0, reply_text='', class_id=0, method_id=0)\n    self.send_method(m, self._close_callback)\n", "label": 0}
{"function": "\n\n@Appender(_index_shared_docs['_shallow_copy'])\ndef _shallow_copy(self, values=None, **kwargs):\n    if (values is None):\n        values = self.values\n    attributes = self._get_attributes_dict()\n    attributes.update(kwargs)\n    return self._simple_new(values, **attributes)\n", "label": 0}
{"function": "\n\ndef test_rst_render_empty(self):\n    self.create_file('empty.rst')\n    empty_rst = self.make_target(':empty_rst', target_type=Page, source='empty.rst')\n    task = self.create_task(self.context(target_roots=[empty_rst]))\n    task.execute()\n", "label": 0}
{"function": "\n\ndef get_messages_for_user(self, user):\n    '\\n        Get messages for a particular user.\\n        \\n        @param user: user object whose messages to get\\n        '\n    return self.filter(users=user)\n", "label": 0}
{"function": "\n\ndef test_simple(self):\n    xs = rules.cached_segment_enriched_tokens(self.s1)\n    self.assertTrue(isinstance(xs, list))\n    self.assertTrue(all((isinstance(x, RichToken) for x in xs)))\n", "label": 0}
{"function": "\n\ndef test_uri_normalized(self):\n    docuri = DocumentURI({\n        'uri': 'http://example.com/',\n    })\n    assert (docuri.uri_normalized == 'http://example.com')\n", "label": 0}
{"function": "\n\ndef test_sadmin_removes_ownership(self):\n    self._create_domain('domain.tld', withtpl=True)\n    dom = Domain.objects.get(name='domain.tld')\n    self.client.logout()\n    self.client.login(username='admin', password='password')\n    self.ajax_get('{0}?domid={1}&daid={2}'.format(reverse('admin:permission_remove'), dom.id, self.user.id), {\n        \n    })\n    self._check_limit('domains', 0, 2)\n    self._check_limit('domain_admins', 0, 2)\n    self._check_limit('mailboxes', 0, 2)\n    self._check_limit('mailbox_aliases', 0, 2)\n", "label": 0}
{"function": "\n\ndef onPlayerMessage(self, payload):\n    player = self.filterName(payload['player'])\n    message = payload['message']\n    self.msgQueue.append(('<%s> %s' % (player, message)))\n", "label": 0}
{"function": "\n\n@_require_mech\ndef unwrap(self, incoming):\n    '\\n        Unwrap a message from the SASL server. Depending on the negotiated\\n        quality of protection, this may check a signature, decrypt the message,\\n        or leave the message unaltered.\\n        '\n    return self._chosen_mech.unwrap(incoming)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, label=None):\n    \"\\n        Returns the PointCloud that contains this label represents on the group.\\n        This will be a subset of the total landmark group PointCloud.\\n\\n        Parameters\\n        ----------\\n        label : `string`\\n            Label to filter on.\\n\\n        Returns\\n        -------\\n        pcloud : :map:`PointCloud`\\n            The PointCloud that this label represents. Will be a subset of the\\n            entire group's landmarks.\\n        \"\n    if (label is None):\n        return self.lms.copy()\n    return self._pointcloud.from_mask(self._labels_to_masks[label])\n", "label": 0}
{"function": "\n\ndef resolve_environment(name):\n    if (name is None):\n        return None\n    try:\n        eid = int(name)\n    except ValueError:\n        try:\n            env = dbsession.query(models.Environment).filter((models.Environment.name == name)).one()\n        except NoResultFound:\n            return None\n    else:\n        try:\n            env = dbsession.query(models.Environment).get(eid)\n        except NoResultFound:\n            return None\n    return env\n", "label": 0}
{"function": "\n\ndef test_run_call_order__error_in_test(self):\n    events = []\n    result = LoggingResult(events)\n\n    def setUp():\n        events.append('setUp')\n\n    def test():\n        events.append('test')\n        raise RuntimeError('raised by test')\n\n    def tearDown():\n        events.append('tearDown')\n    expected = ['startTest', 'setUp', 'test', 'addError', 'tearDown', 'stopTest']\n    unittest2.FunctionTestCase(test, setUp, tearDown).run(result)\n    self.assertEqual(events, expected)\n", "label": 0}
{"function": "\n\ndef get_keys(self, list_names):\n    return [self.get_key(list_name) for list_name in list_names]\n", "label": 0}
{"function": "\n\ndef test_installer_unmatch(self):\n    test_unmatch = {\n        'os_installer_name': 'dummy',\n        'os_name': 'CentOS',\n    }\n    matcher = self.os_matcher.match(**test_unmatch)\n    self.assertFalse(matcher)\n", "label": 0}
{"function": "\n\ndef WithValidator(self, validator):\n    'Add a validator callback to this Measurement, chainable.'\n    if (not callable(validator)):\n        raise ValueError('Validator must be callable', validator)\n    self.validators.append(validator)\n    return self\n", "label": 0}
{"function": "\n\ndef test_client_getattr(self):\n    c = delicious.DeliciousClient('username', 'password')\n    self.assertEqual(c.username, 'username')\n    self.assertEqual(c.password, 'password')\n    self.assertEqual(c.method, 'v1')\n    c2 = c.foo.bar.baz\n    self.assertEqual(c2.username, 'username')\n    self.assertEqual(c2.password, 'password')\n    self.assertEqual(c2.method, 'v1/foo/bar/baz')\n", "label": 0}
{"function": "\n\ndef loopbackUNIX(server, client, noisy=True):\n    'Run session between server and client protocol instances over UNIX socket.'\n    path = tempfile.mktemp()\n    from twisted.internet import reactor\n    f = policies.WrappingFactory(protocol.Factory())\n    serverWrapper = _FireOnClose(f, server)\n    f.noisy = noisy\n    f.buildProtocol = (lambda addr: serverWrapper)\n    serverPort = reactor.listenUNIX(path, f)\n    clientF = LoopbackClientFactory(client)\n    clientF.noisy = noisy\n    reactor.connectUNIX(path, clientF)\n    d = clientF.deferred\n    d.addCallback((lambda x: serverWrapper.deferred))\n    d.addCallback((lambda x: serverPort.stopListening()))\n    return d\n", "label": 0}
{"function": "\n\ndef _get_container_info(self, env):\n    '\\n        Retrieves x-container-meta-web-index, x-container-meta-web-error,\\n        x-container-meta-web-listings, x-container-meta-web-listings-css,\\n        and x-container-meta-web-directory-type from memcache or from the\\n        cluster and stores the result in memcache and in self._index,\\n        self._error, self._listings, self._listings_css and self._dir_type.\\n\\n        :param env: The WSGI environment dict.\\n        :return container_info: The container_info dict.\\n        '\n    self._index = self._error = self._listings = self._listings_css = self._dir_type = None\n    container_info = get_container_info(env, self.app, swift_source='SW')\n    if is_success(container_info['status']):\n        meta = container_info.get('meta', {\n            \n        })\n        self._index = meta.get('web-index', '').strip()\n        self._error = meta.get('web-error', '').strip()\n        self._listings = meta.get('web-listings', '').strip()\n        self._listings_label = meta.get('web-listings-label', '').strip()\n        self._listings_css = meta.get('web-listings-css', '').strip()\n        self._dir_type = meta.get('web-directory-type', '').strip()\n    return container_info\n", "label": 0}
{"function": "\n\ndef look_at(self, pos):\n    delta = (Vector3(pos) - self.clientinfo.eye_pos)\n    if (delta.x or delta.z):\n        self.look_at_rel(delta)\n    else:\n        self.look(self.clientinfo.position.yaw, delta.yaw_pitch.pitch)\n", "label": 0}
{"function": "\n\ndef _to_sizes(self, object):\n    '\\n        Request a list of instance types and convert that list to a list of\\n        OpenNebulaNodeSize objects.\\n\\n        Request a list of instance types from the OpenNebula web interface,\\n        and issue a request to convert each XML object representation of an\\n        instance type to an OpenNebulaNodeSize object.\\n\\n        :return: List of instance types.\\n        :rtype:  ``list`` of :class:`OpenNebulaNodeSize`\\n        '\n    sizes = []\n    size_id = 1\n    attributes = [('name', str, None), ('ram', int, 'MEMORY'), ('cpu', float, None), ('vcpu', float, None), ('disk', str, None), ('bandwidth', float, None), ('price', float, None)]\n    for element in object.findall('INSTANCE_TYPE'):\n        element = self.connection.request(('/instance_type/%s' % element.attrib['name'])).object\n        size_kwargs = {\n            'id': size_id,\n            'driver': self,\n        }\n        values = self._get_attributes_values(attributes=attributes, element=element)\n        size_kwargs.update(values)\n        size = OpenNebulaNodeSize(**size_kwargs)\n        sizes.append(size)\n        size_id += 1\n    return sizes\n", "label": 0}
{"function": "\n\ndef test_init(self):\n    '\\n        Use git.init to init a new repo\\n        '\n    new_repo = tempfile.mkdtemp(dir=integration.TMP)\n    self.assertEqual(self.run_function('git.init', [new_repo]), 'Initialized empty Git repository in {0}/.git/'.format(new_repo))\n    shutil.rmtree(new_repo)\n", "label": 0}
{"function": "\n\ndef extract_from_service(self, service):\n    'Extract all known entities from a service call.\\n\\n        Will return all entities if no entities specified in call.\\n        Will return an empty list if entities specified but unknown.\\n        '\n    with self.lock:\n        if (ATTR_ENTITY_ID not in service.data):\n            return list(self.entities.values())\n        return [self.entities[entity_id] for entity_id in extract_entity_ids(self.hass, service) if (entity_id in self.entities)]\n", "label": 0}
{"function": "\n\ndef set_admin_password(self, ctxt, instance, new_pass):\n    cctxt = self.client.prepare(version='1.29')\n    cctxt.cast(ctxt, 'set_admin_password', instance=instance, new_pass=new_pass)\n", "label": 0}
{"function": "\n\ndef _translate_setnb(self, tb, instruction):\n    self._translate_set(tb, instruction, 'nb')\n", "label": 0}
{"function": "\n\ndef delete(self):\n    '\\n        Deletes the feed and its markers.\\n        '\n    super(BaseNotificationFeed, self).delete()\n    args = []\n    if self.track_unseen:\n        args.append('unseen')\n    if self.track_unread:\n        args.append('unread')\n    self.feed_markers.flush(*args)\n", "label": 0}
{"function": "\n\ndef check_notaction(self, iamgroup_item):\n    '\\n        alert when an IAM Group has a policy containing \\'NotAction\\'.\\n        NotAction combined with an \"Effect\": \"Allow\" often provides more privilege\\n        than is desired.\\n        '\n    self.library_check_iamobj_has_notaction(iamgroup_item, policies_key='grouppolicies')\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (self.msg != None):\n        self.reporter(((self.msg + ': ') + repr(other)))\n    else:\n        self.reporter(repr(other))\n    if (self.value is not Dumper.UNSET_VALUE):\n        return (self.value == other)\n    return True\n", "label": 0}
{"function": "\n\ndef test_static_columns(self):\n    if (PROTOCOL_VERSION < 2):\n        raise unittest.SkipTest('Native protocol 2+ required, currently using: {0}'.format(PROTOCOL_VERSION))\n\n    class StaticModel(Model):\n        id = columns.Integer(primary_key=True)\n        c = columns.Integer(primary_key=True)\n        name = columns.Text(static=True)\n    drop_table(StaticModel)\n    session = get_session()\n    with mock.patch.object(session, 'execute', wraps=session.execute) as m:\n        sync_table(StaticModel)\n    self.assertGreater(m.call_count, 0)\n    statement = m.call_args[0][0].query_string\n    self.assertIn('\"name\" text static', statement)\n    sync_table(StaticModel)\n    with mock.patch.object(session, 'execute', wraps=session.execute) as m2:\n        sync_table(StaticModel)\n    self.assertEqual(len(m2.call_args_list), 0)\n", "label": 0}
{"function": "\n\ndef process_precedence(option, option_str, option_value, parser, builder):\n    if (option_str == '--build'):\n        builder.allow_builds()\n    elif (option_str == '--no-build'):\n        builder.no_allow_builds()\n    elif (option_str == '--wheel'):\n        setattr(parser.values, option.dest, True)\n        builder.use_wheel()\n    elif (option_str in ('--no-wheel', '--no-use-wheel')):\n        setattr(parser.values, option.dest, False)\n        builder.no_use_wheel()\n    else:\n        raise OptionValueError\n", "label": 0}
{"function": "\n\ndef test_success(self):\n    app_id = 'app_id'\n    token = 'token'\n    response = facebook.save(app_id, token)\n    self.assertIsInstance(response, RedirectResponse)\n", "label": 0}
{"function": "\n\ndef get_remote():\n    repo_check(require_remote=True)\n    reader = repo.config_reader()\n    if (not reader.has_option('legit', 'remote')):\n        return repo.remotes[0]\n    remote_name = reader.get('legit', 'remote')\n    if (not (remote_name in [r.name for r in repo.remotes])):\n        raise ValueError('Remote \"{0}\" does not exist! Please update your git configuration.'.format(remote_name))\n    return repo.remote(remote_name)\n", "label": 0}
{"function": "\n\n@orca.column('tours')\ndef dest_density_index(tours, land_use):\n    return reindex(land_use.density_index, tours.destination)\n", "label": 0}
{"function": "\n\ndef save_load(jid, load):\n    '\\n    Save the load to the specified jid id\\n    '\n    with _get_serv(commit=True) as cur:\n        sql = 'INSERT INTO `jids`\\n               (`jid`, `load`)\\n                VALUES (%s, %s)'\n        try:\n            cur.execute(sql, (jid, json.dumps(load)))\n        except MySQLdb.IntegrityError:\n            pass\n", "label": 0}
{"function": "\n\ndef insert_before(self, before, item):\n    ' Inserts an item into the group before the specified item.\\n\\n        Parameters\\n        ----------\\n        before : ActionManagerItem\\n            The item to insert before.\\n        item : ActionManagerItem, Action or callable\\n            The item to insert.\\n\\n        Returns\\n        -------\\n        index, item : int, ActionManagerItem\\n            The position inserted, and the item actually inserted.\\n\\n        Notes\\n        -----\\n\\n        If the item is an ActionManagerItem instance it is simply inserted.\\n        If the item is an Action instance, an ActionItem is created for the\\n        action, and that is inserted.  If the item is a callable, then an\\n        Action is created for the callable, and then that is handled as above.\\n        '\n    index = self._items.index(before)\n    self.insert(index, item)\n    return (index, item)\n", "label": 0}
{"function": "\n\ndef debug(self, text):\n    'Helper method for \"logging\" to the console.'\n    if self.PRINT_DEBUG:\n        print(('[FileHistory] ' + text))\n", "label": 0}
{"function": "\n\ndef test_parse_args_1(self):\n    test_data = {\n        'address': '192.168.1.2',\n        'user': 'admin',\n        'pswd': 'superpswd',\n        'port': '8010',\n        'type': 'foscam',\n    }\n    expeted_data = {\n        'address': '192.168.1.2:8010',\n        'user': 'admin',\n        'pswd': 'superpswd',\n        'name': None,\n    }\n    self.assertEqual(parse_args(test_data), expeted_data)\n", "label": 0}
{"function": "\n\ndef enqueue(self, item):\n    if (self.front is None):\n        new_node = QueueNode(item)\n        self.front = new_node\n        self.back = new_node\n    else:\n        new_back = QueueNode(item)\n        self.back.next = new_back\n        self.back = new_back\n    self.size += 1\n", "label": 0}
{"function": "\n\ndef _link_fields(self, field_refs, acount, progress_monitor):\n    count = 0\n    progress_monitor.start('Parsing fields', acount)\n    log = gl.LinkerLog(self, self.field_kind.kind)\n    for scode_reference in field_refs:\n        if self._reject_reference(scode_reference):\n            progress_monitor.work('Skipped reference', 1)\n            continue\n        (field_name, fqn_container) = self._get_field_name(scode_reference)\n        code_elements = []\n        code_elements.extend(self._get_field_elements(field_name, PREFIX_FIELD_LINKER, self.field_kind))\n        code_elements.extend(self._get_field_elements(field_name, PREFIX_ENUM_VAL_LINKER, self.enum_value_kind))\n        code_elements.extend(self._get_field_elements(field_name, PREFIX_ANN_FIELD_LINKER, self.ann_field_kind))\n        (code_element, potentials) = self.get_code_element(scode_reference, code_elements, field_name, fqn_container, log)\n        count += gl.save_link(scode_reference, code_element, potentials, self)\n        if (not log.custom_filtered):\n            reclassify_java(code_element, scode_reference)\n        progress_monitor.work('Processed field', 1)\n    log.close()\n    progress_monitor.done()\n    print('Associated {0} fields'.format(count))\n", "label": 0}
{"function": "\n\ndef keypair_delete(self, name, ignore_exceptions=False):\n    'Try to delete keypair by name.'\n    try:\n        self.openstack(('keypair delete ' + name))\n    except exceptions.CommandFailed:\n        if (not ignore_exceptions):\n            raise\n", "label": 0}
{"function": "\n\ndef _check_276(self, engine, data):\n    self.assertColumnExists(engine, 'instance_extra', 'vcpu_model')\n    self.assertColumnExists(engine, 'shadow_instance_extra', 'vcpu_model')\n    instance_extra = oslodbutils.get_table(engine, 'instance_extra')\n    shadow_instance_extra = oslodbutils.get_table(engine, 'shadow_instance_extra')\n    self.assertIsInstance(instance_extra.c.vcpu_model.type, sqlalchemy.types.Text)\n    self.assertIsInstance(shadow_instance_extra.c.vcpu_model.type, sqlalchemy.types.Text)\n", "label": 0}
{"function": "\n\ndef test_valid_lifecycle(self):\n    bucket_uri = self.CreateBucket()\n    fpath = self.CreateTempFile(contents=self.lifecycle_doc)\n    self.RunGsUtil(['lifecycle', 'set', fpath, suri(bucket_uri)])\n    stdout = self.RunGsUtil(['lifecycle', 'get', suri(bucket_uri)], return_stdout=True)\n    self.assertEqual(json.loads(stdout), self.lifecycle_json_obj)\n", "label": 0}
{"function": "\n\ndef _unbind_router(self, context, router_id, agent_id):\n    with context.session.begin(subtransactions=True):\n        query = context.session.query(RouterL3AgentBinding)\n        query = query.filter((RouterL3AgentBinding.router_id == router_id), (RouterL3AgentBinding.l3_agent_id == agent_id))\n        query.delete()\n", "label": 0}
{"function": "\n\ndef addFailure(self, offset, line):\n    \"A 'failure:' directive has been read.\"\n    self._outcome(offset, line, self._failure, self.parser._reading_failure_details)\n", "label": 0}
{"function": "\n\ndef __init__(self, id, schedule, host, database, username, password, select_query, insert_query, table, depends_on=None):\n    'Constructor for the SqlNode class\\n\\n        Args:\\n            id(str): id of the object\\n            schedule(Schedule): pipeline schedule\\n            database(str): database name on the RDS host\\n            sql(str): sql to be executed\\n            table(str): table to be read\\n        '\n    if (not isinstance(schedule, Schedule)):\n        raise ETLInputError('Input schedule must be of the type Schedule')\n    if (not depends_on):\n        depends_on = list()\n    kwargs = {\n        'id': id,\n        'type': 'SqlDataNode',\n        'schedule': schedule,\n        'database': database,\n        'selectQuery': select_query,\n        'insertQuery': insert_query,\n        'table': table,\n        'dependsOn': depends_on,\n    }\n    super(PostgresNode, self).__init__(**kwargs)\n", "label": 0}
{"function": "\n\ndef destroy_node(self, node):\n    domain = self._get_domain_for_node(node=node)\n    return (domain.destroy() == 0)\n", "label": 0}
{"function": "\n\ndef get_parser(self, prog_name):\n    parser = super(UpdateNode, self).get_parser(prog_name)\n    parser.add_argument('--name', metavar='<name>', help=_('New name for the node'))\n    parser.add_argument('--profile', metavar='<profile-id>', help=_('ID of new profile to use'))\n    parser.add_argument('--role', metavar='<role>', help=_('Role for this node in the specific cluster'))\n    parser.add_argument('--metadata', metavar='<key1=value1;key2=value2...>', help=_('Metadata values to be attached to the node. Metadata can be specified multiple times, or once with key-value pairs separated by a semicolon'), action='append')\n    parser.add_argument('node', metavar='<node>', help=_('Name or ID of node to update'))\n    return parser\n", "label": 0}
{"function": "\n\ndef get_config(self):\n    config = {\n        'target_shape': self.target_shape,\n    }\n    base_config = super(Reshape, self).get_config()\n    return dict((list(base_config.items()) + list(config.items())))\n", "label": 0}
{"function": "\n\n@web.asynchronous\n@gen.engine\ndef delete(self, *args, **kwargs):\n    'Handle delete of an item\\n\\n        :param args:\\n        :param kwargs:\\n\\n        '\n    self.model = self.get_model(kwargs.get('id'))\n    result = (yield self.model.fetch())\n    if (not result):\n        self.not_found()\n        return\n    if (not self.has_delete_permission()):\n        self.permission_denied()\n        return\n    self.model.delete()\n    self.set_status(204)\n    self.finish()\n", "label": 0}
{"function": "\n\ndef test_container_create_fail(self):\n    CREATE_CONTAINER_FAIL = copy.deepcopy(CREATE_CONTAINER)\n    CREATE_CONTAINER_FAIL['wrong_key'] = 'wrong'\n    self.assertRaisesRegexp(exceptions.InvalidAttribute, ('Key must be in %s' % ','.join(containers.CREATION_ATTRIBUTES)), self.mgr.create, **CREATE_CONTAINER_FAIL)\n    self.assertEqual([], self.api.calls)\n", "label": 0}
{"function": "\n\ndef test_switch_to_new_window(self):\n    page = AlertPage()\n    handler = WindowsHandler()\n    page.open()\n    parent = handler.active_window\n    handler.save_window_set()\n    assert_false(handler.is_new_window_present())\n    page.open_new_window_link.click()\n    ok_(handler.is_new_window_present())\n    new = handler.new_window\n    handler.switch_to_new_window()\n    eq_(new, handler.active_window)\n    handler.drop_active_window()\n    eq_(parent, handler.active_window)\n", "label": 0}
{"function": "\n\ndef __init__(self, max_pool_size=MAX_POOL_SIZE):\n    'Constructor.\\n\\n    Args:\\n      max_pool_size: maximum pools size in bytes before flushing it to db.\\n    '\n    self.max_pool_size = max_pool_size\n    self.puts = ItemList()\n    self.deletes = ItemList()\n", "label": 0}
{"function": "\n\n@classmethod\ndef load_from_config(cls, config):\n    if ('env' in config):\n        config['env'] = parse_env_dict(config['env'])\n    cfg = config.copy()\n    w = cls(name=config.pop('name'), cmd=config.pop('cmd'), **config)\n    w._cfg = cfg\n    return w\n", "label": 0}
{"function": "\n\ndef test_constructor_headerlist_with_add_header(self):\n    response = Response(headers={\n        'key1': 'value1',\n    })\n    expected_content_type = ('key1', 'value1')\n    self.assertIn(expected_content_type, response.headerlist)\n", "label": 0}
{"function": "\n\ndef test_latest(self):\n    from datetime import datetime, timedelta\n    now = datetime.now()\n    for (index, pk) in self.standard_id.items():\n        obj = Standard.objects.get(pk=pk)\n        obj.date = Date.objects.language('en').create(shared_date=(now + timedelta(days=index)), translated_date=(now - timedelta(days=index)))\n        obj.save()\n    manager = get_translation_aware_manager(Standard)\n    with translation.override('en'):\n        self.assertEqual(manager.latest('date__shared_date').pk, self.standard_id[self.standard_count])\n        self.assertEqual(manager.latest('date__translated_date').pk, self.standard_id[1])\n", "label": 0}
{"function": "\n\ndef get_member(self, client):\n    matches = tuple(filter((lambda x: (x.client == client)), self.members))\n    return ((len(matches) > 0) and matches[0])\n", "label": 0}
{"function": "\n\ndef translate(self, x, y):\n    self.matrix[2] += ((self.matrix[0] * x) + (self.matrix[1] * y))\n    self.matrix[5] += ((self.matrix[3] * x) + (self.matrix[4] * y))\n", "label": 0}
{"function": "\n\ndef encode_request_params(self, request):\n    params = request.params\n    try:\n        params = tuple(params.items())\n    except AttributeError:\n        pass\n    return tuple(((self.encode(key), self.encode(val)) for (key, val) in params))\n", "label": 0}
{"function": "\n\n@decorators.json_view\n@account_decorators.login_required\ndef user_geoip(request):\n    'AJAX method\\n    Retrieves the estimated location of a given user using the IP address of\\n    the request.\\n    :param request: The GET HTTP request.\\n    :return: JSON object, { latitude: $lat, longitude: $lng }\\n    '\n    (lat, lng) = gis.get_remote_user_location(ip=ipware_get_ip(request))\n    return {\n        'latitude': lat,\n        'longitude': lng,\n    }\n", "label": 0}
{"function": "\n\ndef setUp(self, core_plugin=None, dm_plugin=None, ext_mgr=None):\n    super(HostingDeviceConfigAgentNotifierTestCase, self).setUp(core_plugin, dm_plugin, ext_mgr)\n    fake_notifier.reset()\n", "label": 0}
{"function": "\n\ndef __init__(self, type=None, keys=None, count=None, dynamic_count=None):\n    self.type = type\n    self.keys = keys\n    self.count = count\n    self.dynamic_count = dynamic_count\n", "label": 0}
{"function": "\n\ndef dialogSetItems(self, items):\n    '\\n        dialogSetItems(\\n          JSONArray items)\\n\\n        Set alert dialog list items.\\n        '\n    return self._rpc('dialogSetItems', items)\n", "label": 0}
{"function": "\n\ndef test_search(self):\n    response = self.api.tips.search(params={\n        'll': self.default_geo,\n    })\n    assert ('tips' in response)\n", "label": 0}
{"function": "\n\n@delete.setter\ndef delete(self, value):\n    self['_delete'] = value\n", "label": 0}
{"function": "\n\ndef ex_remove_tag_key(self, tag_key):\n    '\\n        Modify a specific tag key\\n\\n        :param tag_key: The tag key you want to remove (required)\\n        :type  tag_key: :class:`DimensionDataTagKey` or ``str``\\n\\n        :rtype: ``bool``\\n        '\n    tag_key_id = self._tag_key_to_tag_key_id(tag_key)\n    remove_tag_key = ET.Element('deleteTagKey', {\n        'xmlns': TYPES_URN,\n        'id': tag_key_id,\n    })\n    response = self.connection.request_with_orgId_api_2('tag/deleteTagKey', method='POST', data=ET.tostring(remove_tag_key)).object\n    response_code = findtext(response, 'responseCode', TYPES_URN)\n    return (response_code in ['IN_PROGRESS', 'OK'])\n", "label": 0}
{"function": "\n\ndef participants(self, conference_sid):\n    '\\n        Return a :class:`~twilio.rest.resources.Participants` instance for the\\n        :class:`~twilio.rest.resources.Conference` with given conference_sid\\n        '\n    base_uri = ('%s/Conferences/%s' % (self.account_uri, conference_sid))\n    return Participants(base_uri, self.auth, self.timeout)\n", "label": 0}
{"function": "\n\ndef test_get_ongoing_events(self):\n    'Test the events manager can find all events overlapping today.\\n\\n        Include events that (according to the timestamp) are not ongoing,\\n        but which started or finished today.\\n        '\n    ongoing_events = Event.objects.ongoing_events()\n    event_slugs = [e.slug for e in ongoing_events]\n    correct_slugs = ['ends-tomorrow-ongoing', 'ends-today-ongoing']\n    self.assertCountEqual(event_slugs, correct_slugs)\n", "label": 0}
{"function": "\n\ndef test_conversation_report(self):\n    argv = ['conversation', 'report', 'ue90']\n    _cli = self.cli(argv)\n    parser_args = _cli.parser.parse_args(argv)\n    self.assertParser(_cli, parser_args, argv)\n    self.assertTrue(_cli.client.report_sender.called)\n", "label": 0}
{"function": "\n\ndef __init__(self, config):\n    self.keytree = Key(None)\n    self.commandmap = {\n        \n    }\n    self.multikeys = ''\n    self.update(DEFAULT_KEYMAP)\n    self.update(config)\n", "label": 0}
{"function": "\n\ndef closeEvent(self, event):\n    self.mainFrame.browser.CloseBrowser()\n", "label": 0}
{"function": "\n\ndef _is_not_a_sequence(self, value):\n    return (not isinstance(value, collections.Sequence))\n", "label": 0}
{"function": "\n\ndef post(self, request, *args, **kwargs):\n    user = self.get_object()\n    user.send_reset_password_email()\n    return Response({\n        \n    }, status=status.HTTP_200_OK)\n", "label": 0}
{"function": "\n\ndef test_clone(self):\n    '\\n        Test cloning an existing repo\\n        '\n    clone_parent_dir = tempfile.mkdtemp(dir=integration.TMP)\n    self.assertTrue(self.run_function('git.clone', [clone_parent_dir, self.repo]))\n    shutil.rmtree(clone_parent_dir)\n", "label": 0}
{"function": "\n\ndef tunnel_data_handler(self, payload):\n    ' Map received data over tunnel to the correct link and tunnel '\n    try:\n        tunnel = self.tunnels[payload['from_rt_uuid']][payload['tunnel_id']]\n    except:\n        _log.analyze(self.rt_id, '+ ERROR_UNKNOWN_TUNNEL', payload, peer_node_id=payload['from_rt_uuid'])\n        raise Exception('ERROR_UNKNOWN_TUNNEL')\n    try:\n        tunnel.recv_handler(payload['value'])\n    except Exception as e:\n        _log.exception('Check error in tunnel recv handler')\n        _log.analyze(self.rt_id, '+ EXCEPTION TUNNEL RECV HANDLER', {\n            'payload': payload,\n            'exception': str(e),\n        }, peer_node_id=payload['from_rt_uuid'], tb=True)\n", "label": 0}
{"function": "\n\ndef render(self, name, value, attrs=None):\n    return mark_safe(('%s' % value))\n", "label": 0}
{"function": "\n\ndef _create_interfaces_from_meta(self, node):\n    for interface in node.meta['interfaces']:\n        interface = NodeNICInterface(mac=interface.get('mac'), name=interface.get('name'), ip_addr=interface.get('ip'), netmask=interface.get('netmask'))\n        self.db.add(interface)\n        node.nic_interfaces.append(interface)\n    self.db.flush()\n    if node.cluster_id:\n        self.network_manager.assign_networks_by_default(node)\n    if (node.nic_interfaces and (not filter((lambda i: (node.mac == i.mac)), node.nic_interfaces))):\n        node.nic_interfaces[0].mac = node.mac\n    self.db.commit()\n", "label": 0}
{"function": "\n\ndef __new__(cls, name, bases, args):\n\n    def gen_lex():\n\n        def test(self):\n            tokens = list(lexical(self.asm))\n            self.assertEquals(len(tokens), len(self.lex))\n            for (i, l) in enumerate(self.lex):\n                self.assertEquals(l[0], tokens[i]['type'])\n                self.assertEquals(l[1], tokens[i]['value'])\n        return test\n\n    def gen_syn():\n\n        def test(self):\n            tokens = [{\n                'type': l[0],\n                'value': l[1],\n            } for l in self.lex]\n            ast = syntax(tokens)\n            self.assertEquals(1, len(self.syn))\n        return test\n\n    def gen_sem():\n\n        def test(self):\n            tokens = [{\n                'type': l[0],\n                'value': l[1],\n            } for l in self.lex]\n            ast = [{\n                'type': self.syn[0],\n                'children': tokens,\n            }]\n            compiled = semantic(ast)\n            self.assertEquals(compiled, self.code)\n        return test\n    args['test_lexical'] = gen_lex()\n    args['test_syntax'] = gen_syn()\n    args['test__semantic'] = gen_sem()\n    return type.__new__(cls, name, bases, args)\n", "label": 0}
{"function": "\n\ndef close(self, streamgroup):\n    if isinstance(streamgroup, IMLiveStreamGroup):\n        server_sg = streamgroup._g\n        ns = self._store[server_sg.namespace]\n        del ns[server_sg.name]\n    return defer.succeed(None)\n", "label": 0}
{"function": "\n\ndef strptime(string, format='%a %b %d %H:%M:%S %Y', to_datetime=False):\n    date = TimeParser(format).match(string)\n    result = datetime.date(date[0], date[1], date[2])\n    if (to_datetime and (len(date) > 3)):\n        time = datetime.time(date[3], date[4], date[5])\n        result = datetime.datetime.combine(result, time)\n        result = result.replace(tzinfo=None)\n    return result\n", "label": 0}
{"function": "\n\ndef test_nodummy_dydxmean(self):\n    me = self.res1.get_margeff(at='mean')\n    assert_almost_equal(me.margeff, self.res2.margeff_nodummy_dydxmean, DECIMAL_4)\n    assert_almost_equal(me.margeff_se, self.res2.margeff_nodummy_dydxmean_se, DECIMAL_4)\n", "label": 0}
{"function": "\n\ndef test_frames_equal_mismatched_items():\n    expected = pd.DataFrame({\n        'a': [1],\n    })\n    actual = pd.DataFrame({\n        'a': [2],\n    })\n    with pytest.raises(AssertionError) as info:\n        testing.assert_frames_equal(actual, expected)\n    assert (info.value.message == \"\\nItems are not equal:\\n ACTUAL: 2\\n DESIRED: 1\\n\\nColumn: 'a'\\nRow: 0\")\n", "label": 0}
{"function": "\n\ndef delete_table(self, dataset, table):\n    'Delete a table from the dataset.\\n\\n        Parameters\\n        ----------\\n        dataset : str\\n            The dataset to delete the table from.\\n        table : str\\n            The name of the table to delete\\n\\n        Returns\\n        -------\\n        Union[bool, dict]\\n            bool indicating if the table was successfully deleted or not,\\n            or response from BigQuery if swallow_results is set for False.\\n        '\n    try:\n        response = self.bigquery.tables().delete(projectId=self.project_id, datasetId=dataset, tableId=table).execute()\n        if self.swallow_results:\n            return True\n        else:\n            return response\n    except HttpError as e:\n        logger.error('Cannot delete table {0}.{1}\\nHttp Error: {2}'.format(dataset, table, e.content))\n        if self.swallow_results:\n            return False\n        else:\n            return {\n                \n            }\n", "label": 0}
{"function": "\n\ndef test_earlyorder():\n    (x, y) = (var(), var())\n    assert (earlyorder((eq, 2, x)) == ((eq, 2, x),))\n    assert (earlyorder((eq, 2, x), (eq, 3, x)) == ((eq, 2, x), (eq, 3, x)))\n    assert (earlyorder((membero, x, y), (eq, y, (1, 2, 3)))[0] == (eq, y, (1, 2, 3)))\n", "label": 0}
{"function": "\n\ndef trigger(self, event, *args, **kw):\n    'trigger all functions from an event'\n    if ((not (event in self.events)) or (not self.events[event])):\n        return False\n    list(map((lambda x: x(*args, **kw)), self.events[event]))\n    return True\n", "label": 0}
{"function": "\n\ndef update(self, testable_resource_id, patch):\n    return self._update(resource_id=testable_resource_id, patch=patch)\n", "label": 0}
{"function": "\n\ndef test_default_field_with_queryset(self):\n    qs = mock.NonCallableMock(spec=[])\n    f = ModelChoiceFilter(queryset=qs)\n    field = f.field\n    self.assertIsInstance(field, forms.ModelChoiceField)\n    self.assertEqual(field.queryset, qs)\n", "label": 0}
{"function": "\n\ndef create_pcap():\n    '\\n    Create a capture file from the test fixtures.\\n    '\n    tfile = tempfile.NamedTemporaryFile()\n    if (sys.version_info[0] >= 3):\n        capture = pickle.loads(base64.b64decode(fixture.TESTPCAP3))\n    else:\n        capture = pickle.loads(fixture.TESTPCAP2.decode('base64'))\n    open(tfile.name, 'wb').write(capture)\n    return tfile\n", "label": 0}
{"function": "\n\ndef delete_attachment(self, attachment_id=''):\n    try:\n        self.rr_store.delete_attachment(attachment_id, attachment_name=self.DEFAULT_ATTACHMENT_NAME)\n    finally:\n        return self.delete(attachment_id, del_associations=True)\n", "label": 0}
{"function": "\n\ndef __init__(self, command):\n    super(NixCompletion, self).__init__(command)\n", "label": 0}
{"function": "\n\ndef test_region_show(self):\n    region_id = self._create_dummy_region()\n    raw_output = self.openstack(('region show %s' % region_id))\n    region = self.parse_show_as_object(raw_output)\n    self.assertEqual(region_id, region['region'])\n    self.assertEqual('None', region['parent_region'])\n", "label": 0}
{"function": "\n\ndef get_indexer_for(self, target, **kwargs):\n    ' guaranteed return of an indexer even when non-unique '\n    if self.is_unique:\n        return self.get_indexer(target, **kwargs)\n    (indexer, _) = self.get_indexer_non_unique(target, **kwargs)\n    return indexer\n", "label": 0}
{"function": "\n\ndef compare(self, vf, version):\n    if (LooseVersion(version) < '0.18.0'):\n        data = read_msgpack(vf, encoding='latin-1')\n    else:\n        data = read_msgpack(vf)\n    self.check_min_structure(data)\n    for (typ, dv) in data.items():\n        assert (typ in self.all_data), 'unpacked data contains extra key \"{0}\"'.format(typ)\n        for (dt, result) in dv.items():\n            assert (dt in self.all_data[typ]), 'data[\"{0}\"] contains extra key \"{1}\"'.format(typ, dt)\n            try:\n                expected = self.data[typ][dt]\n            except KeyError:\n                continue\n            comparator = getattr(self, 'compare_{typ}_{dt}'.format(typ=typ, dt=dt), None)\n            if (comparator is not None):\n                comparator(result, expected, typ, version)\n            else:\n                check_arbitrary(result, expected)\n    return data\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Tangible()\n    result.template = 'object/tangible/furniture/elegant/shared_end_table_s01.iff'\n    result.attribute_template_id = 6\n    result.stfName('frn_n', 'frn_end_table')\n    return result\n", "label": 0}
{"function": "\n\ndef _splitSpec(spec):\n    ' \\n        Takes an input specification, and returns a (path, pattern) tuple.\\n\\n        The splitting works as follows:\\n            - First, the spec is split on \".\".\\n            - We then try to maximally match as much as possible of the\\n              start of the spec to an existing file or directory.\\n            - The remainder is considered the mark pattern.\\n\\n        If no path is found, the first element of the return tuple is the empty\\n        string.\\n    '\n    parts = spec.split('.')\n    dirOffset = 0\n    fileOffset = 0\n    for i in range(1, (len(parts) + 1)):\n        if os.path.isdir('.'.join(parts[:i])):\n            dirOffset = i\n        elif os.path.isfile(('.'.join(parts[:i]) + '.py')):\n            fileOffset = i\n    if (dirOffset > fileOffset):\n        target = '.'.join(parts[:dirOffset])\n        pattern = '.'.join(parts[dirOffset:])\n    elif fileOffset:\n        target = ('.'.join(parts[:fileOffset]) + '.py')\n        pattern = '.'.join(parts[fileOffset:])\n    else:\n        target = ''\n        pattern = '.'.join(parts)\n    if (target and (pattern == 'py')):\n        pattern = ''\n    return (target, pattern)\n", "label": 0}
{"function": "\n\ndef process(conn, coro=None):\n    global n\n    if (sys.version_info.major >= 3):\n        eol = ord('/')\n    else:\n        eol = '/'\n    data = ''.encode()\n    while True:\n        data += (yield conn.recv(128))\n        if (data[(- 1)] == eol):\n            break\n    conn.close()\n    n += 1\n    print(('recieved: %s' % data))\n", "label": 0}
{"function": "\n\ndef test_signed_item_is_verifiable(self):\n    '\\n        Check that the resulting item is able to be verified.\\n        '\n    item = {\n        'foo': 'bar',\n        'baz': [1, 2, 3],\n    }\n    signed_item = get_signed_item(item, PUBLIC_KEY, PRIVATE_KEY)\n    self.assertTrue(verify_item(signed_item))\n", "label": 0}
{"function": "\n\n@classmethod\ndef initialize(cls, output_path, authority, username, begin=None):\n    '\\n        Generate a certificate signed by the supplied root certificate.\\n\\n        :param FilePath output_path: Directory where the certificate will be\\n            written.\\n        :param CertificateAuthority authority: The certificate authority with\\n            which this certificate will be signed.\\n        :param unicode username: A UTF-8 encoded username to be included in\\n            the certificate.\\n        :param datetime begin: The datetime from which the generated\\n            certificate should be valid.\\n        '\n    key_filename = (username + '.key')\n    cert_filename = (username + '.crt')\n    name = ('user-' + username)\n    organizational_unit = authority.organizational_unit\n    dn = DistinguishedName(commonName=name, organizationalUnitName=organizational_unit)\n    keypair = flocker_keypair()\n    request = keypair.keypair.requestObject(dn)\n    serial = os.urandom(16).encode(b'hex')\n    serial = int(serial, 16)\n    cert = sign_certificate_request(authority.credential.keypair.keypair, authority.credential.certificate.original.get_subject(), request, serial, EXPIRY_20_YEARS, b'sha256', start=begin, additional_extensions=[crypto.X509Extension(b'extendedKeyUsage', False, b'clientAuth')])\n    credential = FlockerCredential(path=output_path, keypair=keypair, certificate=cert)\n    credential.write_credential_files(key_filename, cert_filename)\n    instance = cls(credential=credential, username=username)\n    return instance\n", "label": 0}
{"function": "\n\ndef read_simple_binding(jboss_config, binding_name, profile=None):\n    '\\n    Read jndi binding in the running jboss instance\\n\\n    jboss_config\\n        Configuration dictionary with properties specified above.\\n    binding_name\\n        Binding name to be created\\n    profile\\n        The profile name (JBoss domain mode only)\\n\\n    CLI Example:\\n\\n        .. code-block:: bash\\n\\n        salt \\'*\\' jboss7.read_simple_binding \\'{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}\\' my_binding_name\\n       '\n    log.debug('======================== MODULE FUNCTION: jboss7.read_simple_binding, %s', binding_name)\n    return __read_simple_binding(jboss_config, binding_name, profile=profile)\n", "label": 0}
{"function": "\n\ndef set(name, value):\n    'To set a vim variable to a given value.'\n    if isinstance(value, basestring):\n        val = \"'{0}'\".format(value)\n    elif isinstance(value, bool):\n        val = '{0}'.format((1 if value else 0))\n    else:\n        val = value\n    vim.command('let {0} = {1}'.format((prefix + name), val))\n", "label": 0}
{"function": "\n\ndef filter(self, *data):\n    lens = [len(seq) for seq in data[0]]\n    if (self.percentile > 0):\n        max_len = np.percentile(lens, self.percentile)\n        max_len = np.clip(max_len, self.min_max_len, self.max_len)\n    else:\n        max_len = self.max_len\n    valid_idxs = [i for (i, l) in enumerate(lens) if (l <= max_len)]\n    if (len(data) == 1):\n        return list_index(data[0], valid_idxs)\n    else:\n        return tuple([list_index(d, valid_idxs) for d in data])\n", "label": 0}
{"function": "\n\ndef test_materialize_throw(self):\n    ex = 'ex'\n    scheduler = TestScheduler()\n    xs = scheduler.create_hot_observable(on_next(150, 1), on_error(250, ex))\n\n    def create():\n        return xs.materialize()\n    results = scheduler.start(create).messages\n    assert (len(results) == 2)\n    assert ((results[0].value.kind == 'N') and (results[0].value.value.kind == 'E') and (results[0].value.value.exception == ex))\n    assert (results[1].value.kind == 'C')\n", "label": 0}
{"function": "\n\ndef __init__(self, auth=None, client_id=None, email_address=None):\n    self.client_id = client_id\n    self.email_address = email_address\n    super(Person, self).__init__(auth)\n", "label": 0}
{"function": "\n\ndef setup_sahara_engine():\n    periodic.setup()\n    engine = _get_infrastructure_engine()\n    service_ops.setup_ops(engine)\n    remote_driver = _get_remote_driver()\n    remote.setup_remote(remote_driver, engine)\n", "label": 0}
{"function": "\n\n@utils.positional(1)\ndef __new__(cls, kind, properties, ancestor):\n    'Constructor.'\n    obj = object.__new__(cls)\n    obj.__kind = kind\n    obj.__properties = properties\n    obj.__ancestor = ancestor\n    return obj\n", "label": 0}
{"function": "\n\ndef _saveVideo(self, videoFields):\n    params = (videoFields['source_id'],)\n    self._testCase.assertIn(params, self._saveVideoReturn)\n    self.saveVideoLog.append(videoFields)\n    return self._saveVideoReturn[params]\n", "label": 0}
{"function": "\n\ndef get_db_prep_lookup(self, value, connection):\n    if isinstance(value, (tuple, list)):\n        params = ([connection.ops.Adapter(value[0])] + list(value)[1:])\n    else:\n        params = [connection.ops.Adapter(value)]\n    return ('%s', params)\n", "label": 0}
{"function": "\n\n@requires_tz_support\ndef test_now_template_tag_uses_current_time_zone(self):\n    tpl = Template('{% now \"O\" %}')\n    self.assertEqual(tpl.render(Context({\n        \n    })), '+0300')\n    with timezone.override(ICT):\n        self.assertEqual(tpl.render(Context({\n            \n        })), '+0700')\n", "label": 0}
{"function": "\n\n@base.remotable_classmethod\ndef get_by_host(cls, context, host, expected_attrs=None, use_slave=False):\n    db_inst_list = cls._db_instance_get_all_by_host(context, host, columns_to_join=_expected_cols(expected_attrs), use_slave=use_slave)\n    return _make_instance_list(context, cls(), db_inst_list, expected_attrs)\n", "label": 0}
{"function": "\n\ndef test_materialize_empty(self):\n    scheduler = TestScheduler()\n    xs = scheduler.create_hot_observable(on_next(150, 1), on_completed(250))\n\n    def create():\n        return xs.materialize()\n    results = scheduler.start(create).messages\n    assert (len(results) == 2)\n    assert ((results[0].value.kind == 'N') and (results[0].value.value.kind == 'C') and (results[0].time == 250))\n    assert ((results[1].value.kind == 'C') and (results[1].time == 250))\n", "label": 0}
{"function": "\n\ndef registerProducer(self, producer, streaming):\n    assert (self.producer is None)\n    self.producer = producer\n    self.streamingProducer = streaming\n    self._pollProducer()\n", "label": 0}
{"function": "\n\ndef addPackage(self, name, version='*'):\n    self.packageList.addPackage(name, version)\n", "label": 0}
{"function": "\n\n@dispatch(Expr)\ndef _print_python(expr, leaves=None):\n    raise NotImplementedError(('Do not know how to write expressions of type %s to Python code' % type(expr).__name__))\n", "label": 0}
{"function": "\n\n@transform(countLcaTaxa, suffix('.count'), '.count.load')\ndef loadCountLcaTaxa(infile, outfile):\n    '\\n    load taxa level counts\\n    '\n    P.load(infile, outfile)\n", "label": 0}
{"function": "\n\ndef quote_name_unless_alias(self, name):\n    \"\\n        A wrapper around connection.ops.quote_name that doesn't quote aliases\\n        for table names. This avoids problems with some SQL dialects that treat\\n        quoted strings specially (e.g. PostgreSQL).\\n        \"\n    if (name in self.quote_cache):\n        return self.quote_cache[name]\n    if (((name in self.query.alias_map) and (name not in self.query.table_map)) or (name in self.query.extra_select)):\n        self.quote_cache[name] = name\n        return name\n    r = self.connection.ops.quote_name(name)\n    self.quote_cache[name] = r\n    return r\n", "label": 0}
{"function": "\n\n@mock.patch('nova.compute.utils.refresh_info_cache_for_instance')\ndef test_delete_security_group_in_use(self, refresh_info_cache_mock):\n    sg = self._create_sg_template().get('security_group')\n    self._create_network()\n    db_inst = fakes.stub_instance(id=1, nw_cache=[], security_groups=[])\n    _context = context.get_admin_context()\n    instance = instance_obj.Instance._from_db_object(_context, instance_obj.Instance(), db_inst, expected_attrs=instance_obj.INSTANCE_DEFAULT_FIELDS)\n    neutron = neutron_api.API()\n    with mock.patch.object(nova.db, 'instance_get_by_uuid', return_value=db_inst):\n        neutron.allocate_for_instance(_context, instance, security_groups=[sg['id']])\n    req = fakes.HTTPRequest.blank(('/v2/fake/os-security-groups/%s' % sg['id']))\n    self.assertRaises(webob.exc.HTTPBadRequest, self.controller.delete, req, sg['id'])\n", "label": 0}
{"function": "\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\n    shutil.rmtree(self.dirpath)\n    return None\n", "label": 0}
{"function": "\n\ndef write(self, data):\n    with AnalysisServer._write_lock:\n        data = (json.dumps(data) + '\\n').encode('utf-8')\n        _logger.debug('writing to stdin: %s', data)\n        self.stdin.write(data)\n        self.stdin.flush()\n", "label": 0}
{"function": "\n\ndef get_url_decorator(self, pattern):\n    'Fixes bug in Application.get_url_decorator wich would decorate None values.'\n    permissions = self.get_permissions(pattern.name)\n    if (permissions is not None):\n        return permissions_required(permissions)\n    return None\n", "label": 0}
{"function": "\n\ndef headers_received(self, start_line, headers):\n    self.set_request(httputil.HTTPServerRequest(connection=self.connection, start_line=start_line, headers=headers))\n    if self.stream_request_body:\n        self.request.body = Future()\n        return self.execute()\n", "label": 0}
{"function": "\n\ndef _update_revision_txn(self, txn, user_id, room_id, next_id):\n    'Update the latest revision of the tags for the given user and room.\\n\\n        Args:\\n            txn: The database cursor\\n            user_id(str): The ID of the user.\\n            room_id(str): The ID of the room.\\n            next_id(int): The the revision to advance to.\\n        '\n    txn.call_after(self._account_data_stream_cache.entity_has_changed, user_id, next_id)\n    update_max_id_sql = 'UPDATE account_data_max_stream_id SET stream_id = ? WHERE stream_id < ?'\n    txn.execute(update_max_id_sql, (next_id, next_id))\n    update_sql = 'UPDATE room_tags_revisions SET stream_id = ? WHERE user_id = ? AND room_id = ?'\n    txn.execute(update_sql, (next_id, user_id, room_id))\n    if (txn.rowcount == 0):\n        insert_sql = 'INSERT INTO room_tags_revisions (user_id, room_id, stream_id) VALUES (?, ?, ?)'\n        try:\n            txn.execute(insert_sql, (user_id, room_id, next_id))\n        except self.database_engine.module.IntegrityError:\n            pass\n", "label": 0}
{"function": "\n\ndef resolve_source(self, model, source):\n    try:\n        return resolve_orm_path(model, source)\n    except FieldDoesNotExist:\n        return None\n", "label": 0}
{"function": "\n\n@base.resource(acc.Access)\ndef access(self, access_id):\n    '\\n        Return the resource corresponding to a single access.\\n        '\n    return acc.Access(self, access_id)\n", "label": 0}
{"function": "\n\ndef test_index_ints():\n    A = npr.randn(5, 6, 4)\n\n    def fun(x):\n        return to_scalar(x[(3, 0, 1)])\n    d_fun = (lambda x: to_scalar(grad(fun)(x)))\n    check_grads(fun, A)\n    check_grads(d_fun, A)\n", "label": 0}
{"function": "\n\ndef test_wait_timed_out_with_capacity(self):\n    inst = instance.Instance(self.request_data, 'name', self.proxy, max_concurrent_requests=1)\n    inst._started = True\n    self.mox.StubOutWithMock(inst._condition, 'wait')\n    self.mox.ReplayAll()\n    self.assertTrue(inst.wait(0))\n    self.mox.VerifyAll()\n", "label": 0}
{"function": "\n\ndef __setattr__(self, name, value):\n    setattr(self.value, name, value)\n", "label": 0}
{"function": "\n\ndef add_link(self, rel, href, **kwargs):\n    self.builder.add_link(rel, self.make_target(href), **kwargs)\n", "label": 0}
{"function": "\n\ndef test_without_support_without_session(self):\n    message = ConnectMessage(self.version)\n    self.assertEqual(message.version, self.version)\n    self.assertEqual(message.support, [self.version])\n    self.assertFalse(message.has_session())\n    self.assertIsNone(message.session)\n", "label": 0}
{"function": "\n\ndef test_post_hanlder(self):\n\n    def post_hanlder(emulator, instruction, parameter):\n        paramter.append(True)\n    asm = ['mov eax, ebx']\n    x86_instrs = map(self._asm_parser.parse, asm)\n    self.__set_address(3735928559, x86_instrs)\n    reil_instrs = map(self._translator.translate, x86_instrs)\n    paramter = []\n    self._emulator.set_instruction_post_handler(post_hanlder, paramter)\n    (reil_ctx_out, reil_mem_out) = self._emulator.execute_lite(reil_instrs[0])\n    self.assertTrue((len(paramter) > 0))\n", "label": 0}
{"function": "\n\ndef addGetSpeech(self, **kwargs):\n    return self.append(GetSpeech(**kwargs))\n", "label": 0}
{"function": "\n\ndef _post_update(self, existing_obj):\n    existing_obj.code = (self.code if existing_obj.approved else None)\n", "label": 0}
{"function": "\n\ndef _check_301(self, engine, data):\n    self.assertColumnExists(engine, 'compute_nodes', 'cpu_allocation_ratio')\n    self.assertColumnExists(engine, 'compute_nodes', 'ram_allocation_ratio')\n", "label": 0}
{"function": "\n\ndef test_basic_usage(self):\n    cmd = commands.Command()\n    self.assertEqual('usage: foo', cmd.format_usage('foo').strip())\n    cmd.add_argument('-h', '--help', action='store_true')\n    self.assertEqual('usage: foo [-h]', cmd.format_usage('foo').strip())\n    cmd.add_argument('bar')\n    self.assertEqual('usage: foo [-h] bar', cmd.format_usage('foo').strip())\n", "label": 0}
{"function": "\n\ndef test_inheritance(self):\n    self.assertTrue(issubclass(self.app.json_encoder, DummyEncoder))\n    json_encoder_name = self.app.json_encoder.__name__\n    self.assertEqual(json_encoder_name, 'MongoEngineJSONEncoder')\n", "label": 0}
{"function": "\n\ndef test_load():\n    s_metrics = structured_metrics.StructuredMetrics()\n    errors = s_metrics.load_plugins()\n    assert (len(errors) == 0)\n", "label": 0}
{"function": "\n\ndef __init__(self, seeds=1, max_feval=50000.0, max_iter=100000.0):\n    self.seeds = seeds\n    self.max_feval = max_feval\n    self.max_iter = max_iter\n    self.model = None\n    self.n_dims = None\n    self.kernel = None\n    self._kerns = None\n    self._kernf = None\n    self._kernb = None\n", "label": 0}
{"function": "\n\ndef star_output_mode(cluster, logdir, cmdline, *args):\n    'Select output mode: [stream|ordered|off]'\n    modes = ('stream', 'ordered', 'off')\n    if (args[0] not in modes):\n        raise ValueError(('Output mode must be one of: %s' % repr(modes)))\n    cluster.output_mode = args[0]\n    print(('Output mode set to %s' % cluster.output_mode))\n", "label": 0}
{"function": "\n\n@mock.patch('cloudferry.lib.os.compute.nova_compute.NovaCompute.create_flavor')\ndef test_access_not_updated_for_public_flavors(self, _):\n    flavors = {\n        'flavor1': {\n            'flavor': {\n                'is_public': True,\n                'name': 'flavor1',\n                'tenants': [],\n                'ram': 48,\n                'vcpus': 1,\n                'disk': 0,\n                'ephemeral': 1,\n                'swap': 0,\n                'rxtx_factor': 1.0,\n            },\n            'meta': {\n                \n            },\n        },\n    }\n    tenant_map = {\n        \n    }\n    config = mock.Mock()\n    config.migrate.override_rules = None\n    cloud = mock.MagicMock()\n    cloud.position = 'dst'\n    nc = nova_compute.NovaCompute(config, cloud)\n    nc._create_flavor_if_not_exists = mock.Mock()\n    nc._add_flavor_access_for_tenants = mock.Mock()\n    nc.get_flavor_list = mock.Mock()\n    nc.get_flavor_list.return_value = []\n    nc._deploy_flavors(flavors, tenant_map)\n    assert (not nc._add_flavor_access_for_tenants.called)\n", "label": 0}
{"function": "\n\ndef create_comment(self, revision, message, silent=False):\n    \"Make a comment on the specified 'revision'.\\n\\n        :revision: id of the revision to comment on\\n        :message: the string message to leave as a comment, may be empty\\n        :silent: mail notifications won't be sent if False\\n        :returns: None\\n\\n        \"\n    _ = silent\n    self._data.assert_is_revision(revision)\n    str(message)\n    self._data.set_changed()\n", "label": 0}
{"function": "\n\ndef match_type(self, event_type):\n    return (self._included_type(event_type) and (not self._excluded_type(event_type)))\n", "label": 0}
{"function": "\n\ndef test_leq():\n    calculated = leq(levels)\n    real = 79.806989166\n    assert_almost_equal(calculated, real)\n", "label": 0}
{"function": "\n\ndef test_set_get_name(self):\n    '\\n        Application name accessors behave properly\\n        '\n    application = Application()\n    application.set_name('foo')\n    self.assertEqual('foo', application.get_name(), msg='.set_name() sets the name of the application')\n", "label": 0}
{"function": "\n\ndef test_drop_while(self, space):\n    w_res = space.execute('return [1, 2, 3, 4, 5, 0].drop_while { |i| i < 3 }')\n    assert (self.unwrap(space, w_res) == [3, 4, 5, 0])\n    w_res = space.execute('return [1, 2, 3].drop_while { |i| i == 0 } ')\n    assert (self.unwrap(space, w_res) == [1, 2, 3])\n    w_res = space.execute('return [].drop_while { |i| i > 3 }')\n    assert (self.unwrap(space, w_res) == [])\n", "label": 0}
{"function": "\n\ndef _prepare_toplevel_for_item(self, layout):\n    ' Returns a sized toplevel QDockWidget for a LayoutItem.\\n        '\n    if isinstance(layout, PaneItem):\n        dock_widget = self._get_dock_widget(layout)\n        if (dock_widget is None):\n            logger.warning(('Cannot retrieve dock widget for pane %r' % layout.id))\n        else:\n            if (layout.width > 0):\n                dock_widget.widget().setFixedWidth(layout.width)\n            if (layout.height > 0):\n                dock_widget.widget().setFixedHeight(layout.height)\n        return dock_widget\n    elif isinstance(layout, LayoutContainer):\n        return self._prepare_toplevel_for_item(layout.items[0])\n    else:\n        raise MainWindowLayoutError('Leaves of layout must be PaneItems')\n", "label": 0}
{"function": "\n\ndef clone(self, deep=True):\n    \"\\n        Return a clone of this tag. If deep is True, clone all of this tag's\\n        children. Otherwise, just shallow copy the children list without copying\\n        the children themselves.\\n        \"\n    if deep:\n        newchildren = [self._clone(x, True) for x in self.children]\n    else:\n        newchildren = self.children[:]\n    newattrs = self.attributes.copy()\n    for key in newattrs:\n        newattrs[key] = self._clone(newattrs[key], True)\n    newslotdata = None\n    if self.slotData:\n        newslotdata = self.slotData.copy()\n        for key in newslotdata:\n            newslotdata[key] = self._clone(newslotdata[key], True)\n    newtag = Tag(self.tagName, attributes=newattrs, children=newchildren, render=self.render, filename=self.filename, lineNumber=self.lineNumber, columnNumber=self.columnNumber)\n    newtag.slotData = newslotdata\n    return newtag\n", "label": 0}
{"function": "\n\ndef initialize_options(self, *args, **kwargs):\n    install.initialize_options(self, *args, **kwargs)\n    self.corpora_zip_url = None\n", "label": 0}
{"function": "\n\ndef test_servers_rebuild(self):\n    subs = {\n        'image_id': fake.get_valid_image_id(),\n        'host': self._get_host(),\n        'glance_host': self._get_glance_host(),\n        'access_ip_v4': '1.2.3.4',\n        'access_ip_v6': '80fe::',\n    }\n    uuid = self._post_server(use_common_server_api_samples=False)\n    response = self._do_post(('servers/%s/action' % uuid), 'server-action-rebuild-req', subs)\n    subs['hostid'] = '[a-f0-9]+'\n    subs['id'] = uuid\n    self._verify_response('server-action-rebuild-resp', subs, response, 202)\n", "label": 0}
{"function": "\n\ndef test_contributor_cohort_analysis(self):\n    c1 = Cohort.objects.get(kind__code=CONTRIBUTOR_COHORT_CODE, start=self.start_of_first_week)\n    eq_(c1.size, 10)\n    c1r1 = c1.retention_metrics.get(start=(self.start_of_first_week + timedelta(weeks=1)))\n    eq_(c1r1.size, 2)\n    c1r2 = c1.retention_metrics.get(start=(self.start_of_first_week + timedelta(weeks=2)))\n    eq_(c1r2.size, 5)\n    c2 = Cohort.objects.get(kind__code=CONTRIBUTOR_COHORT_CODE, start=(self.start_of_first_week + timedelta(weeks=1)))\n    eq_(c2.size, 8)\n    c2r1 = c2.retention_metrics.get(start=(self.start_of_first_week + timedelta(weeks=2)))\n    eq_(c2r1.size, 2)\n", "label": 0}
{"function": "\n\ndef line(self, source, x, y, agg=None):\n    'Compute a reduction by pixel, mapping data to pixels as a line.\\n\\n        For aggregates that take in extra fields, the interpolated bins will\\n        receive the fields from the previous point. In pseudocode:\\n\\n        >>> for i in range(len(rows) - 1):    # doctest: +SKIP\\n        ...     row0 = rows[i]\\n        ...     row1 = rows[i + 1]\\n        ...     for xi, yi in interpolate(row0.x, row0.y, row1.x, row1.y):\\n        ...         add_to_aggregate(xi, yi, row0)\\n\\n        Parameters\\n        ----------\\n        source : pandas.DataFrame, dask.DataFrame\\n            The input datasource.\\n        x, y : str\\n            Column names for the x and y coordinates of each vertex.\\n        agg : Reduction, optional\\n            Reduction to compute. Default is ``any()``.\\n        '\n    from .glyphs import Line\n    from .reductions import any\n    if (agg is None):\n        agg = any()\n    return bypixel(source, self, Line(x, y), agg)\n", "label": 0}
{"function": "\n\n@wrap_exceptions\ndef get_process_uids(self):\n    (real, effective, saved) = _psutil_osx.get_process_uids(self.pid)\n    return nt_uids(real, effective, saved)\n", "label": 0}
{"function": "\n\ndef putmask(self, mask, value):\n    '\\n        return a new Index of the values set with the mask\\n\\n        See also\\n        --------\\n        numpy.ndarray.putmask\\n        '\n    values = self.values.copy()\n    try:\n        np.putmask(values, mask, self._convert_for_op(value))\n        return self._shallow_copy(values)\n    except (ValueError, TypeError):\n        return self.astype(object).putmask(mask, value)\n", "label": 0}
{"function": "\n\n@receiver(post_delete, sender=MediaFile)\ndef _mediafile_post_delete(sender, instance, **kwargs):\n    instance.delete_mediafile()\n    logger.info(('Deleted mediafile %d (%s)' % (instance.id, instance.file.name)))\n", "label": 0}
{"function": "\n\ndef test_monmap(self):\n    result = paths.mon.monmap('mycluster', 'myhostname')\n    assert result.startswith('/')\n    assert result.endswith('tmp/mycluster.myhostname.monmap')\n", "label": 0}
{"function": "\n\ndef items(self):\n    return [option['name'] for option in self.options]\n", "label": 0}
{"function": "\n\ndef __init__(self, att, docstring=''):\n    self.__doc__ = docstring\n    self._att = att.split('.')\n", "label": 0}
{"function": "\n\ndef _prepare_verified_images(self, verify_image_url):\n    self._verified_images = self._verify_images(self._find_images(), verify_image_url)\n    print(self._verified_images)\n", "label": 0}
{"function": "\n\ndef _flow_control_change_from_settings(self, old_value, new_value):\n    '\\n        Update flow control windows in response to a change in the value of\\n        SETTINGS_INITIAL_WINDOW_SIZE.\\n\\n        When this setting is changed, it automatically updates all flow control\\n        windows by the delta in the settings values. Note that it does not\\n        increment the *connection* flow control window, per section 6.9.2 of\\n        RFC 7540.\\n        '\n    delta = (new_value - old_value)\n    for stream in self.streams.values():\n        stream.outbound_flow_control_window = guard_increment_window(stream.outbound_flow_control_window, delta)\n", "label": 0}
{"function": "\n\n@dispatch(object)\ndef _print_python(expr, leaves=None):\n    return (repr(expr), {\n        \n    })\n", "label": 0}
{"function": "\n\ndef get_touched_payload_files(payload):\n    ' Return a set of files modified in payload commits.\\n    '\n    touched = set()\n    for commit in payload['commits']:\n        for filelist in (commit['added'], commit['modified']):\n            touched.update(filelist)\n        for filename in commit['removed']:\n            if (filename in touched):\n                touched.remove(filename)\n    current_app.logger.debug('Touched files {}'.format(', '.join(touched)))\n    return touched\n", "label": 0}
{"function": "\n\ndef _set_interfaces_if_not_set_in_meta(self, node_id, meta):\n    if ((not meta) or ('interfaces' not in meta)):\n        self._add_interfaces_to_node(node_id)\n", "label": 0}
{"function": "\n\ndef _validate(self, value):\n    if (not isinstance(value, (int, long))):\n        raise datastore_errors.BadValueError(('Expected integer, got %r' % (value,)))\n    return int(value)\n", "label": 0}
{"function": "\n\ndef add_input(self, string_input):\n    \"Adds the specified string to the pending input that will be returned by calls to 'recv' on this\\n        socket.\\n\\n        Testing code can use this to simulate bytes being received.\\n\\n        @param string_input: The input.\\n        \"\n    original_position = self.__stream.tell()\n    self.__stream.seek(0, 2)\n    self.__stream.write(string_input)\n    self.__stream.seek(original_position)\n", "label": 0}
{"function": "\n\ndef is_type_compatible(self, kind):\n    return (kind == self.inferred_type)\n", "label": 0}
{"function": "\n\n@returns_clone\ndef join(self, dest, join_type=None, on=None):\n    src = self._query_ctx\n    if (not on):\n        require_join_condition = (isinstance(dest, SelectQuery) or (isclass(dest) and (not src._meta.rel_exists(dest))))\n        if require_join_condition:\n            raise ValueError('A join condition must be specified.')\n    elif isinstance(on, basestring):\n        on = src._meta.fields[on]\n    self._joins.setdefault(src, [])\n    self._joins[src].append(Join(src, dest, join_type, on))\n    if (not isinstance(dest, SelectQuery)):\n        self._query_ctx = dest\n", "label": 0}
{"function": "\n\ndef _evaluate_nz(self, tb):\n    return tb._negate_reg(self._flags['zf'])\n", "label": 0}
{"function": "\n\ndef test_01_complex(self):\n    ab = array([[0.0, 0.0, 2.0, 2.0], [(- 99), (- 1j), (- 1j), (- 1j)], [4.0, 4.0, 4.0, 4.0]])\n    b = array([(2 - 1j), (4.0 - 1j), (4 + 1j), (2 + 1j)])\n    x = solveh_banded(ab, b)\n    assert_array_almost_equal(x, [0.0, 1.0, 1.0, 0.0])\n", "label": 0}
{"function": "\n\ndef __init__(self, next_hop_type, id=None, address_prefix=None, next_hop_ip_address=None, provisioning_state=None, name=None, etag=None):\n    super(Route, self).__init__(id=id)\n    self.address_prefix = address_prefix\n    self.next_hop_type = next_hop_type\n    self.next_hop_ip_address = next_hop_ip_address\n    self.provisioning_state = provisioning_state\n    self.name = name\n    self.etag = etag\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    '\\n        Starts websocket server\\n        '\n    autodiscover()\n    mease.run_websocket_server(host=options['host'], port=options['port'], debug=options['debug'])\n", "label": 0}
{"function": "\n\ndef cloneNode(self, deep=0, parent=None):\n    return Comment(self.nodeValue, parent)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef get_model_parameter_bounds():\n    '\\n        Returns a dict where each key-value pair is a model parameter and a\\n        tuple with the lower and upper bounds\\n        '\n    inf = float('inf')\n    params = dict(mu=(0.0, (2 * pi)), rho=(0.0, inf))\n    return params\n", "label": 0}
{"function": "\n\ndef test_util2d_external_free_nomodelws():\n    model_ws = os.path.join(out_dir)\n    if os.path.exists(model_ws):\n        shutil.rmtree(model_ws)\n    os.mkdir(model_ws)\n    base_dir = os.getcwd()\n    os.chdir(out_dir)\n    ml = flopy.modflow.Modflow()\n    stress_util2d_for_joe_the_file_king(ml, 1, 1, 1)\n    stress_util2d_for_joe_the_file_king(ml, 10, 1, 1)\n    stress_util2d_for_joe_the_file_king(ml, 1, 10, 1)\n    stress_util2d_for_joe_the_file_king(ml, 1, 1, 10)\n    stress_util2d_for_joe_the_file_king(ml, 10, 10, 1)\n    stress_util2d_for_joe_the_file_king(ml, 1, 10, 10)\n    stress_util2d_for_joe_the_file_king(ml, 10, 1, 10)\n    stress_util2d_for_joe_the_file_king(ml, 10, 10, 10)\n    os.chdir(base_dir)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Tangible()\n    result.template = 'object/tangible/deed/faction_perk/hq/shared_hq_s05.iff'\n    result.attribute_template_id = 2\n    result.stfName('deed', 'hq_s05')\n    return result\n", "label": 0}
{"function": "\n\ndef test_localtime_without_arg(self):\n    lt0 = time.localtime()\n    lt1 = time.localtime(None)\n    t0 = time.mktime(lt0)\n    t1 = time.mktime(lt1)\n    self.assertAlmostEqual(t1, t0, delta=0.2)\n", "label": 0}
{"function": "\n\ndef consume(self):\n    try:\n        while True:\n            result = self.reader.read()\n            if (not result):\n                break\n    except IOError:\n        pass\n", "label": 0}
{"function": "\n\ndef suggest(self, history, searchspace):\n    'Randomly suggest params from searchspace.\\n        '\n    return searchspace.rvs(self.seed)\n", "label": 0}
{"function": "\n\ndef _actor_migrate(self, to_rt_uuid, callback, actor_id, requirements, extend, move, status, peer_node_id=None, uri=None):\n    ' Got link? continue actor migrate '\n    if status:\n        msg = {\n            'cmd': 'ACTOR_MIGRATE',\n            'requirements': requirements,\n            'actor_id': actor_id,\n            'extend': extend,\n            'move': move,\n        }\n        self.network.links[to_rt_uuid].send_with_reply(callback, msg)\n    elif callback:\n        callback(status=status)\n", "label": 0}
{"function": "\n\ndef test_subscription_create_page(self):\n    ' Subscription create page renders successfully. '\n    self.login()\n    response = self.client.get(url_for('billing.create'), follow_redirects=True)\n    assert (response.status_code == 200)\n", "label": 0}
{"function": "\n\n@mock.patch('pkg_resources.iter_entry_points')\ndef test_entry_points(self, iep):\n\n    def function():\n        pass\n\n    def entry_points(group, *args):\n        (yield function)\n    iep.side_effect = entry_points\n    collector = begin.subcommands.Collector()\n    collector.load_plugins('entry.point')\n    self.assertEqual(list(collector.commands()), [function])\n", "label": 0}
{"function": "\n\ndef queue(self, *args, **kwargs):\n    return gevent.queue.Queue(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _parse_minimum_links(self, config):\n    value = 0\n    match = re.search('port-channel min-links (\\\\d+)', config)\n    if match:\n        value = int(match.group(1))\n    return dict(minimum_links=value)\n", "label": 0}
{"function": "\n\ndef test_peer_review_modal_on_response_question(self):\n    client = self.get_logged_in_client()\n    kwargs = {\n        'HTTP_X_REQUESTED_WITH': 'XMLHttpRequest',\n    }\n    response = client.post('/teacher/course/1/peer_review/1/peer_review_modal', {\n        'question_id': 1,\n        'question_type': settings.RESPONSE_QUESTION_TYPE,\n        'submission_id': 4,\n    }, **kwargs)\n    self.assertEqual(response.status_code, 200)\n    self.assertIn(b'peer_review_modal', response.content)\n", "label": 0}
{"function": "\n\ndef test_named_property_setter(self):\n    d = self.d\n    d.CompareMode = 42\n    d.Item['foo'] = 1\n    d.Item['bar'] = 'spam foo'\n    d.Item['baz'] = 3.14\n    self.assertAccessInterface(d)\n", "label": 0}
{"function": "\n\ndef prefetch(sq, *subqueries):\n    if (not subqueries):\n        return sq\n    fixed_queries = prefetch_add_subquery(sq, subqueries)\n    deps = {\n        \n    }\n    rel_map = {\n        \n    }\n    for prefetch_result in reversed(fixed_queries):\n        query_model = prefetch_result.model\n        if prefetch_result.fields:\n            for rel_model in prefetch_result.rel_models:\n                rel_map.setdefault(rel_model, [])\n                rel_map[rel_model].append(prefetch_result)\n        deps[query_model] = {\n            \n        }\n        id_map = deps[query_model]\n        has_relations = bool(rel_map.get(query_model))\n        for instance in prefetch_result.query:\n            if prefetch_result.fields:\n                prefetch_result.store_instance(instance, id_map)\n            if has_relations:\n                for rel in rel_map[query_model]:\n                    rel.populate_instance(instance, deps[rel.model])\n    return prefetch_result.query\n", "label": 1}
{"function": "\n\ndef pushlines(self, lines):\n    self._lines[:0] = lines[::(- 1)]\n", "label": 0}
{"function": "\n\ndef apply(self, query, value):\n    flt = {\n        ('%s__nin' % self.column.name): value,\n    }\n    return query.filter(**flt)\n", "label": 0}
{"function": "\n\ndef _on_invalid_digest(self, message, **kwargs):\n    self._invalid_digests += 1\n    self._write_status(message, True)\n", "label": 0}
{"function": "\n\n@property\ndef headers(self):\n    h = [DataTablesColumn(mark_safe('\\n                    Select  <a class=\"select-visible btn btn-xs btn-info\">all</a>\\n                    <a class=\"select-none btn btn-xs btn-default\">none</a>\\n                    '), sortable=False, span=3), DataTablesColumn(_('View Form'), span=2), DataTablesColumn(_('Username'), prop_name='form.meta.username', span=3), DataTablesColumn((_('Submission Time') if self.by_submission_time else _('Completion Time')), prop_name=self.time_field, span=3), DataTablesColumn(_('Form'), prop_name='form.@name')]\n    h.extend([DataTablesColumn(field) for field in self.other_fields])\n    return DataTablesHeader(*h)\n", "label": 0}
{"function": "\n\ndef setup(self, node):\n    ' Setup the the layout saver.\\n\\n        '\n    self.stack = []\n", "label": 0}
{"function": "\n\ndef test_setup(self):\n    for (p, c) in self.edges:\n        child = self.target_map[c]\n        parent = self.target_map[p]\n        self.assertTrue((parent in child.deps))\n", "label": 0}
{"function": "\n\ndef bool_value(value, default=False):\n    '\\n    Convert strings like \\'yes\\', \\'y\\', \\'true\\', \\'1\\',\\n    \\'no\\', \\'n\\', \\'false\\', \\'0\\' or \\'\\' to boolean values.\\n    :param str value: \"Value to parse to bool\"\\n    :param bool default: \"Default value if unable to convert\"\\n    :rtype: bool\\n    '\n    if (not value):\n        return default\n    true_values = ('yes', 'y', 'true', '1')\n    false_values = ('no', 'n', 'false', '0', '')\n    normalized_value = value.strip().lower()\n    if (normalized_value in true_values):\n        return True\n    elif (normalized_value in false_values):\n        return False\n    else:\n        return default\n", "label": 0}
{"function": "\n\ndef _Cmd(self, command, mode=None):\n    'CiscoXR wrapper for ParamikoDevice._Cmd().'\n    result = super(CiscoXrDevice, self)._Cmd(command, mode)\n    if result.endswith(\"% Invalid input detected at '^' marker.\\r\\n\"):\n        raise exceptions.CmdError(('Invalid input: %s' % command))\n    if result.endswith('% Bad hostname or protocol not running\\r\\n'):\n        raise exceptions.CmdError(('Bad hostname or protocol not running: %s' % command))\n    if result.endswith('% Incomplete command.\\r\\n'):\n        raise exceptions.CmdError(('Incomplete command: %s' % command))\n    return result\n", "label": 0}
{"function": "\n\ndef prepare_kernel_parameters(self):\n    params = []\n    for (inp, isdev) in zip(self.norm_inputs, self._is_device_array):\n        if isdev:\n            params.append(inp)\n        else:\n            params.append(self.to_device(inp))\n    assert all((self.is_device_array(a) for a in params))\n    self.kernel_parameters = params\n", "label": 0}
{"function": "\n\ndef test_named_command_class_should_set_command_name_if_not_defined(self):\n    description = 'some description'\n\n    class SomeCommand(NamedCommand):\n\n        def __init__(self, lizy):\n            super(SomeCommand, self).__init__(lizy, description)\n\n        def _process_command(self, lizy, args):\n            return (None, None)\n    subparser_mock = (lambda : 0)\n    subparser_mock.add_parser = MagicMock()\n    no_name = SomeCommand(Pylease(None, subparser_mock, None))\n    eq_(no_name.name, 'some')\n    subparser_mock.add_parser.assert_called_once_with('some', help=description)\n", "label": 0}
{"function": "\n\ndef initialize(self, in_obj):\n    assert hasattr(in_obj, 'layers'), 'MultiCost must be passed a layer container'\n    terminals = in_obj.get_terminal()\n    for (c, ll) in zip(self.costs, terminals):\n        c.initialize(ll)\n", "label": 0}
{"function": "\n\ndef test_remove_all_jobs(jobstore, create_add_job):\n    create_add_job(jobstore, dummy_job, datetime(2016, 5, 3))\n    create_add_job(jobstore, dummy_job2, datetime(2014, 2, 26))\n    jobstore.remove_all_jobs()\n    jobs = jobstore.get_all_jobs()\n    assert (jobs == [])\n", "label": 0}
{"function": "\n\n@register.filter\ndef model_attr_verbose_name(obj, attr):\n    '\\n    Returns the verbose name of a model field or method.\\n    '\n    try:\n        return utils.model_field_verbose_name(obj, attr)\n    except FieldDoesNotExist:\n        return utils.model_method_verbose_name(obj, attr)\n", "label": 0}
{"function": "\n\ndef __init__(self, custUnit=None, builtInUnit=None, dispUnitsLbl=None, extLst=None):\n    self.custUnit = custUnit\n    self.builtInUnit = builtInUnit\n    self.dispUnitsLbl = dispUnitsLbl\n", "label": 0}
{"function": "\n\ndef set_transition_sels(view, sels):\n    'Set the updated transition selections and marks.\\n    '\n    view.add_regions('transition_sels', sels, scope=_TRANSITION_CURSOR_SCOPE_TYPE, icon=_TRANSITION_CURSOR_ICON, flags=_TRANSITION_CURSOR_FLAGS)\n", "label": 0}
{"function": "\n\ndef remove(self, obj):\n    'Removes a child given its name or object\\n\\n        If the node was added with name, it is better to remove by name, else\\n        the name will be unavailable for further adds (and will raise an \\n        Exception if add with this same name is attempted)\\n\\n        If the node was part of the active scene, its :meth:`on_exit` method \\n        will be called.\\n\\n        Arguments:\\n            obj (str or object):\\n                Name of the reference to be removed or object to be removed.\\n        '\n    if isinstance(obj, string_types):\n        if (obj in self.children_names):\n            child = self.children_names.pop(obj)\n            self._remove(child)\n        else:\n            raise Exception(('Child not found: %s' % obj))\n    else:\n        self._remove(obj)\n", "label": 0}
{"function": "\n\ndef test_get_users(self):\n    users = list(get_users())\n    assert_equal(len(users), 1)\n    assert_not_in(self.unconfirmed, users)\n    assert_equal(users, [self.user])\n", "label": 0}
{"function": "\n\ndef test_margeff_dummy_overall(self):\n    me = self.res1.get_margeff(dummy=True)\n    assert_almost_equal(me.margeff, self.res2.margeff_dummy_overall, DECIMAL_4)\n    assert_almost_equal(me.margeff_se, self.res2.margeff_dummy_overall_se, DECIMAL_4)\n", "label": 0}
{"function": "\n\ndef getEdgeOppositeHorizontally(self, pp, symbol, x, onY2):\n    xMin = pp.getXMin()\n    xMax = pp.getXMax()\n    xMid = symbol.getBaseline()\n    if (Double.NaN == xMid):\n        xMid = ((xMin + xMax) / 2.0)\n    xMinPx = pp.xToPixel(xMin)\n    xMaxPx = pp.xToPixel(xMax)\n    xMidPx = pp.xToPixel(xMid)\n    xPx = pp.xToPixel(x)\n    prevXPx = Double.NaN\n    nextXPx = Double.NaN\n    width = symbol.getWidth(pp)\n    symWidth = self.getAdjustedWidth(width, xPx, prevXPx, nextXPx, xMinPx, xMaxPx, xMidPx)\n    if (Double.NaN == symWidth):\n        return Double.NaN\n    xLeft = self.getUpperLeftX(width, xPx, prevXPx, nextXPx, xMinPx, xMaxPx, xMidPx, pp.getXMousePlotArea())\n    if (Double.NaN == xLeft):\n        return Double.NaN\n    result = (xLeft + symWidth)\n    if (abs((xLeft - xPx)) > abs((result - xPx))):\n        result = xLeft\n    return result\n", "label": 0}
{"function": "\n\ndef test_plugin_context(self):\n    glossary = {\n        'link_content': 'Knopf',\n        'button-type': 'btn-default',\n    }\n    model_instance = add_plugin(self.placeholder, BootstrapButtonPlugin, 'en', glossary=glossary)\n    button_plugin = model_instance.get_plugin_class_instance()\n    context = button_plugin.render({\n        \n    }, model_instance, None)\n    self.assertIn('instance', context)\n    self.assertIsInstance(context['instance'], LinkElementMixin)\n    self.assertListEqual(button_plugin.get_css_classes(model_instance), ['btn', 'btn-default'])\n    self.assertEqual(button_plugin.get_identifier(model_instance), 'Knopf')\n", "label": 0}
{"function": "\n\ndef resolve(d):\n    '\\n    .. todo::\\n\\n        WRITEME\\n    '\n    tag = pylearn2.config.get_tag(d)\n    if (tag != 'dataset'):\n        raise TypeError(('pylearn2.datasets.config asked to resolve a config dictionary with tag \"%s\"' % tag))\n    typename = pylearn2.config.get_str(d, 'typename')\n    try:\n        resolver = resolvers[typename]\n    except KeyError:\n        reraise_as(TypeError(('pylearn2.datasets does not know of a dataset type \"%s\"' % typename)))\n    return resolver(d)\n", "label": 0}
{"function": "\n\ndef test_query_table_with_sqlite3_connection(self):\n    database_file = 'sometable.db'\n    db = Database(sqlite3.connect(database_file))\n    db.open()\n    db.execute('CREATE TABLE IF NOT EXISTS SomeTable (id VARCHAR(255))')\n    db.execute('INSERT INTO SomeTable VALUES (1)')\n    db.execute('INSERT INTO SomeTable VALUES (?)', (2,))\n    db.execute('SELECT * FROM SomeTable')\n    rows = db.fetchall()\n    self.assertEqual(True, isinstance(rows, types.GeneratorType))\n    self.assertEqual(['1', '2'], [row[0] for row in rows])\n    db.rollback()\n    db.close()\n    os.remove(database_file)\n", "label": 0}
{"function": "\n\ndef remote_flow_control_window(self, stream_id):\n    '\\n        Returns the maximum amount of data the remote peer can send on stream\\n        ``stream_id``.\\n\\n        This value will never be larger than the total data that can be sent on\\n        the connection: even if the given stream allows more data, the\\n        connection window provides a logical maximum to the amount of data that\\n        can be sent.\\n\\n        The maximum data that can be sent in a single data frame on a stream\\n        is either this value, or the maximum frame size, whichever is\\n        *smaller*.\\n\\n        :param stream_id: The ID of the stream whose flow control window is\\n            being queried.\\n        :type stream_id: ``int``\\n        :returns: The amount of data in bytes that can be received on the\\n            stream before the flow control window is exhausted.\\n        :rtype: ``int``\\n        '\n    stream = self._get_stream_by_id(stream_id)\n    return min(self.inbound_flow_control_window, stream.inbound_flow_control_window)\n", "label": 0}
{"function": "\n\ndef _remove_punct(self, inStr):\n    '\\n        Function to remove punctuation from Unicode string.\\n        :param input: the input string\\n        :return: Unicode string after remove all punctuation\\n        '\n    punc_cat = set(['Pc', 'Pd', 'Ps', 'Pe', 'Pi', 'Pf', 'Po'])\n    return ''.join((x for x in inStr if (unicodedata.category(x) not in punc_cat)))\n", "label": 0}
{"function": "\n\ndef test_download_object_as_stream_escaped(self):\n    container = Container(name='foo & bar_container', extra={\n        \n    }, driver=self.driver)\n    obj = Object(name='foo & bar_object', size=1000, hash=None, extra={\n        \n    }, container=container, meta_data=None, driver=self.driver)\n    stream = self.driver.download_object_as_stream(obj=obj, chunk_size=None)\n    self.assertTrue(hasattr(stream, '__iter__'))\n", "label": 0}
{"function": "\n\ndef min(x, axis=None, keepdims=False):\n    'Minimum value in a tensor.\\n    '\n    axis = _normalize_axis(axis, ndim(x))\n    return tf.reduce_min(x, reduction_indices=axis, keep_dims=keepdims)\n", "label": 0}
{"function": "\n\ndef parseJSON(self, json_data):\n    self.json = json.loads(json_data)\n", "label": 0}
{"function": "\n\ndef testInstanceMetadataSignals(self):\n    n = GafferTest.AddNode()\n    ncs = GafferTest.CapturingSlot(Gaffer.Metadata.nodeValueChangedSignal())\n    pcs = GafferTest.CapturingSlot(Gaffer.Metadata.plugValueChangedSignal())\n    Gaffer.Metadata.registerNodeValue(n, 'signalTest', 1)\n    Gaffer.Metadata.registerPlugValue(n['op1'], 'signalTest', 1)\n    self.assertEqual(len(ncs), 1)\n    self.assertEqual(len(pcs), 1)\n    self.assertEqual(ncs[0], (GafferTest.AddNode.staticTypeId(), 'signalTest', n))\n    self.assertEqual(pcs[0], (GafferTest.AddNode.staticTypeId(), 'op1', 'signalTest', n['op1']))\n    Gaffer.Metadata.registerNodeValue(n, 'signalTest', 1)\n    Gaffer.Metadata.registerPlugValue(n['op1'], 'signalTest', 1)\n    self.assertEqual(len(ncs), 1)\n    self.assertEqual(len(pcs), 1)\n    Gaffer.Metadata.registerNodeValue(n, 'signalTest', 2)\n    Gaffer.Metadata.registerPlugValue(n['op1'], 'signalTest', 2)\n    self.assertEqual(len(ncs), 2)\n    self.assertEqual(len(pcs), 2)\n    self.assertEqual(ncs[1], (GafferTest.AddNode.staticTypeId(), 'signalTest', n))\n    self.assertEqual(pcs[1], (GafferTest.AddNode.staticTypeId(), 'op1', 'signalTest', n['op1']))\n", "label": 0}
{"function": "\n\ndef setup_hist_calc_counter(self):\n    self.setup_subset()\n    m = MagicMock()\n    self.artist._calculate_histogram = m\n    return m\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    BaseTestCase.setUp(self)\n    self.rawData = []\n    self.dataByKey = {\n        \n    }\n    for i in range(1, 11):\n        timeTuple = (2002, 12, 9, 0, 0, 0, 0, 0, (- 1))\n        timeInTicks = ((time.mktime(timeTuple) + (i * 86400)) + (i * 8640))\n        dateCol = cx_Oracle.TimestampFromTicks(int(timeInTicks))\n        if (i % 2):\n            timeInTicks = ((time.mktime(timeTuple) + ((i * 86400) * 2)) + (i * 12960))\n            nullableCol = cx_Oracle.TimestampFromTicks(int(timeInTicks))\n        else:\n            nullableCol = None\n        tuple = (i, dateCol, nullableCol)\n        self.rawData.append(tuple)\n        self.dataByKey[i] = tuple\n", "label": 0}
{"function": "\n\ndef test_group_by_composed(self):\n    table = self.tables.some_table\n    expr = (table.c.x + table.c.y).label('lx')\n    stmt = select([func.count(table.c.id), expr]).group_by(expr).order_by(expr)\n    self._assert_result(stmt, [(1, 3), (1, 5), (1, 7)])\n", "label": 0}
{"function": "\n\ndef testLimitView(self):\n    cb = self.make_connection()\n    d = cb.queryAll('beer', 'brewery_beers', limit=10)\n\n    def _verify(o):\n        self.assertIsInstance(o, BatchedView)\n        rows = list(o)\n        self.assertEqual(len(rows), 10)\n    return d.addCallback(_verify)\n", "label": 0}
{"function": "\n\ndef get_unapplied_migrations(migrations, applied_migrations):\n    applied_migration_names = [('%s.%s' % (mi.app_name, mi.migration)) for mi in applied_migrations]\n    for migration in migrations:\n        is_applied = (('%s.%s' % (migration.app_label(), migration.name())) in applied_migration_names)\n        if (not is_applied):\n            (yield migration)\n", "label": 0}
{"function": "\n\ndef _get_content(item, base_url=None):\n    '\\n    Return a dictionary of content, for documents, objects and errors.\\n    '\n    return {_unescape_key(key): _primative_to_document(value, base_url) for (key, value) in item.items() if (key not in ('_type', '_meta'))}\n", "label": 0}
{"function": "\n\ndef make_node(self, images):\n    '\\n        .. todo::\\n\\n            WRITEME\\n        '\n    images = as_cuda_ndarray_variable(images)\n    assert (images.ndim == 4)\n    channels_broadcastable = images.type.broadcastable[0]\n    batch_broadcastable = images.type.broadcastable[3]\n    rows_broadcastable = False\n    cols_broadcastable = False\n    targets_broadcastable = (channels_broadcastable, rows_broadcastable, cols_broadcastable, batch_broadcastable)\n    targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)\n    targets = targets_type()\n    return Apply(self, [images], [targets])\n", "label": 0}
{"function": "\n\ndef get_quality_ratio(self, mimetype):\n    if (mimetype == 'text/x-textile'):\n        return 8\n    return 0\n", "label": 0}
{"function": "\n\ndef getCenterX_2(self, pp, symbol, prevX, x, nextX):\n    xMin = pp.getXMin()\n    xMax = pp.getXMax()\n    xMid = symbol.getBaseline()\n    if (Double.NaN == xMid):\n        xMid = ((xMin + xMax) / 2.0)\n    xMinPx = pp.xToPixel(xMin)\n    xMaxPx = pp.xToPixel(xMax)\n    xMidPx = pp.xToPixel(xMid)\n    xPx = pp.xToPixel(x)\n    prevXPx = pp.xToPixel(prevX)\n    nextXPx = pp.xToPixel(nextX)\n    width = symbol.getWidth(pp)\n    symWidth = self.getAdjustedWidth(width, xPx, prevXPx, nextXPx, xMinPx, xMaxPx, xMidPx)\n    if (Double.NaN == symWidth):\n        return Double.NaN\n    xLeft = self.getUpperLeftX(width, xPx, prevXPx, nextXPx, xMinPx, xMaxPx, xMidPx, pp.getXMousePlotArea())\n    if (Double.NaN == xLeft):\n        return Double.NaN\n    xCenter = (xLeft + (symWidth / 2.0))\n    return xCenter\n", "label": 0}
{"function": "\n\ndef generate_plot(filename, rplot='A50.rplot', rpdf='A50.pdf'):\n    from jcvi.apps.r import RTemplate\n    rplot_template = '\\n    library(ggplot2)\\n\\n    data <- read.table(\"$rplot\", header=T, sep=\"\\t\")\\n    g <- ggplot(data, aes(x=index, y=cumsize, group=fasta))\\n    g + geom_line(aes(colour=fasta)) +\\n    xlab(\"Contigs\") + ylab(\"Cumulative size (Mb)\") +\\n    opts(title=\"A50 plot\", legend.position=\"top\")\\n\\n    ggsave(file=\"$rpdf\")\\n    '\n    rtemplate = RTemplate(rplot_template, locals())\n    rtemplate.run()\n", "label": 0}
{"function": "\n\ndef start_historyserver(cluster):\n    hs = vu.get_historyserver(cluster)\n    if hs:\n        run.start_historyserver(hs)\n", "label": 0}
{"function": "\n\ndef receive(self, payload):\n    if (not isinstance(payload, bytes)):\n        payload = payload.encode('utf-8')\n    if (len(payload) > 0):\n        self.message_buffer += payload\n        self.bytes_received += len(payload)\n", "label": 0}
{"function": "\n\ndef list_sizes(self):\n    return [NodeSize(driver=self.connection.driver, **i) for i in self._instance_types.values()]\n", "label": 0}
{"function": "\n\n@classmethod\ndef _get_cache_key(cls, key):\n    key_hash = (hashlib.md5(key).hexdigest() if key else '')\n    return 'django-exp-backoff.{}'.format(key_hash)\n", "label": 0}
{"function": "\n\ndef get_berry_flavors(self, obj):\n    flavor_map_objects = BerryFlavorMap.objects.filter(berry=obj)\n    flavor_maps = BerryFlavorMapSerializer(flavor_map_objects, many=True, context=self.context).data\n    flavors = []\n    for map in flavor_maps:\n        del map['berry']\n        flavors.append(map)\n    return flavors\n", "label": 0}
{"function": "\n\ndef _adjust_fn_spec(self, fn, named):\n    if named:\n        fn = self._wrap_fn_for_kw(fn)\n    if self.legacy_signatures:\n        try:\n            argspec = util.get_callable_argspec(fn, no_self=True)\n        except TypeError:\n            pass\n        else:\n            fn = legacy._wrap_fn_for_legacy(self, fn, argspec)\n    return fn\n", "label": 0}
{"function": "\n\ndef computeExpression(self, constraint_collection):\n    (new_node, change_tags, change_desc) = ExpressionBuiltinTypeBase.computeExpression(self, constraint_collection)\n    if (new_node is self):\n        str_value = self.getValue().getStrValue()\n        if (str_value is not None):\n            new_node = wrapExpressionWithNodeSideEffects(new_node=str_value, old_node=self.getValue())\n            change_tags = 'new_expression'\n            change_desc = \"Predicted 'str' built-in result\"\n    return (new_node, change_tags, change_desc)\n", "label": 0}
{"function": "\n\ndef test_filling_image_file_field(self):\n    self.dummy = mommy.make(DummyImageFieldModel)\n    field = DummyImageFieldModel._meta.get_field('image_field')\n    self.assertIsInstance(field, ImageField)\n    import time\n    path = ('%s/%s/mock-img.jpeg' % (gettempdir(), time.strftime('%Y/%m/%d')))\n    from django import VERSION\n    if (VERSION[1] >= 4):\n        self.assertEqual(abspath(self.dummy.image_field.path), abspath(path))\n        self.assertTrue(self.dummy.image_field.width)\n        self.assertTrue(self.dummy.image_field.height)\n", "label": 0}
{"function": "\n\ndef mention(sourceURL, targetURL, vouchDomain=None):\n    'Process the Webmention of the targetURL from the sourceURL.\\n\\n    To verify that the sourceURL has indeed referenced our targetURL\\n    we run findMentions() at it and scan the resulting href list.\\n    '\n    app.logger.info(('discovering Webmention endpoint for %s' % sourceURL))\n    mentions = ronkyuu.findMentions(sourceURL)\n    result = False\n    app.logger.info(('mentions %s' % mentions))\n    for href in mentions['refs']:\n        if ((href != sourceURL) and (href == targetURL)):\n            app.logger.info(('post at %s was referenced by %s' % (targetURL, sourceURL)))\n            result = processWebmention(sourceURL, targetURL, vouchDomain)\n    app.logger.info(('mention() returning %s' % result))\n    return result\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    levelname = record.levelname\n    if (self.use_color and (levelname in COLORS)):\n        levelname_color = (((COLOR_SEQ % (30 + COLORS[levelname])) + levelname) + RESET_SEQ)\n        record.levelname = levelname_color\n    return logging.Formatter.format(self, record)\n", "label": 0}
{"function": "\n\ndef _volume_is_attached_to_vm(self, vol_obj):\n    match_vm_id = 0\n    for attached_dev in vol_obj.attachments:\n        for dst_vm in self.dst_vm_list:\n            if (dst_vm.id == attached_dev['server_id']):\n                match_vm_id += 1\n    return (len(vol_obj.attachments) == match_vm_id)\n", "label": 0}
{"function": "\n\ndef get_func(self, *args, **kwargs):\n    from django.contrib.gis.geos.prototypes.threadsafe import GEOSFunc\n    func = GEOSFunc(self.func_name)\n    func.argtypes = (self.argtypes or [])\n    func.restype = self.restype\n    if self.errcheck:\n        func.errcheck = self.errcheck\n    return func\n", "label": 0}
{"function": "\n\ndef _round_over(self):\n    message = ('left score: %d\\nright score: %d' % (self._left_score, self._right_score))\n    self._operations.show_text_on_feed(message)\n    if self._continue_protocol:\n        self._operations.pause_shot_detection(True)\n        self._new_round_thread = Thread(target=self._new_round, name='new_round_thread')\n        self._new_round_thread.start()\n", "label": 0}
{"function": "\n\ndef eventFilter(self, source, event):\n    typ = event.type()\n    if (typ == QtCore.QEvent.Drop):\n        self.dropEvent(event)\n    elif (typ == QtCore.QEvent.DragEnter):\n        self.dragEnterEvent(event)\n    return super(_DropEventFilter, self).eventFilter(source, event)\n", "label": 0}
{"function": "\n\n@property\ndef crop_size(self):\n    from cropduster.resizing import Size\n    size_json = (self.get('size', {\n        \n    }).get('json') or None)\n    if size_json:\n        return size_json\n    size_w = (self.get('size', {\n        \n    }).get('w') or None)\n    size_h = (self.get('size', {\n        \n    }).get('h') or None)\n    if ((not size_w) and (not size_h)):\n        return None\n    return Size('crop', w=size_w, h=size_h)\n", "label": 0}
{"function": "\n\ndef post(self):\n    username = self.get_argument('username', None)\n    password = self.get_argument('password', None)\n    if ((not username) or (not password)):\n        self.flash.error = 'You must enter a username and password to proceed. Please try again.'\n        self.redirect('/signup')\n        return\n    if (password != self.get_argument('password2', None)):\n        self.flash.error = 'Passwords do not match. Please try again.'\n        self.redirect('/signup')\n        return\n    user = User.instance(username, password)\n    Mongo.db.ui.users.insert(user)\n    self.flash.info = 'Successfully created your account, please log in.'\n    self.redirect('/login')\n", "label": 0}
{"function": "\n\ndef _translate_xor(self, tb, instruction):\n    oprnd0 = tb.read(instruction.operands[0])\n    oprnd1 = tb.read(instruction.operands[1])\n    tmp0 = tb.temporal((oprnd0.size * 2))\n    tb.add(self._builder.gen_xor(oprnd0, oprnd1, tmp0))\n    if (self._translation_mode == FULL_TRANSLATION):\n        self._clear_flag(tb, self._flags['of'])\n        self._clear_flag(tb, self._flags['cf'])\n        self._update_sf(tb, oprnd0, oprnd1, tmp0)\n        self._update_zf(tb, oprnd0, oprnd1, tmp0)\n        self._update_pf(tb, oprnd0, oprnd1, tmp0)\n        self._undefine_flag(tb, self._flags['af'])\n    tb.write(instruction.operands[0], tmp0)\n", "label": 0}
{"function": "\n\ndef list_processes(self):\n    return [mach.list_processes() for mach in self]\n", "label": 0}
{"function": "\n\ndef install_dependencies(self, bundle_store, parent_dict, dest_path, copy):\n    \"\\n        Symlink or copy this bundle's dependencies into the directory at dest_path.\\n        The caller is responsible for cleaning up this directory.\\n        \"\n    precondition(os.path.isabs(dest_path), ('%s is a relative path!' % (dest_path,)))\n    pairs = self.get_dependency_paths(bundle_store, parent_dict, dest_path, relative_symlinks=(not copy))\n    for (target, link_path) in pairs:\n        if os.path.exists(link_path):\n            path_util.remove(link_path)\n        if copy:\n            path_util.copy(target, link_path, follow_symlinks=False)\n        else:\n            os.symlink(target, link_path)\n", "label": 0}
{"function": "\n\ndef store(self, filename):\n    'Write the variables into a file'\n    file = open(filename, 'w')\n    merged_table = self.get_merged_dict()\n    keys = list(merged_table.keys())\n    keys.sort()\n    for k in keys:\n        file.write(('%s = %r\\n' % (k, merged_table[k])))\n    file.close()\n", "label": 0}
{"function": "\n\ndef _create_link(self):\n    ' Create a ComponentLink form the state of the GUI\\n\\n        Returns\\n        -------\\n        A new component link\\n        '\n    expression = str(self.ui.expression.toPlainText())\n    pattern = '[^\\\\s]*:[^\\\\s]*'\n\n    def add_curly(m):\n        return (('{' + m.group(0)) + '}')\n    expression = re.sub(pattern, add_curly, expression)\n    pc = parse.ParsedCommand(expression, self._labels)\n    label = (str(self.ui.new_label.text()) or 'new component')\n    new_id = core.data.ComponentID(label)\n    link = parse.ParsedComponentLink(new_id, pc)\n    return link\n", "label": 0}
{"function": "\n\ndef test_dependency_order(self):\n    ' Test whether a proper dependency tree generated the correct output. '\n    sections = self.old_manifest.formula_sections()\n    assert (sections.index('git') < sections.index('sub')), 'Dependency is out of order! git comes after sub'\n", "label": 0}
{"function": "\n\n@staticmethod\ndef split_program_source_bc(lst):\n    (py_files, pyc_files) = ([], [])\n    for f in lst:\n        dest = (py_files if f.lower().endswith('.py') else pyc_files)\n        dest.append(f)\n    return (py_files, pyc_files)\n", "label": 0}
{"function": "\n\ndef console(self, channel, payload):\n    if self.config['IRC']['show-channel-server']:\n        self.rawConsole({\n            'text': ('[%s] ' % channel),\n            'color': 'gold',\n            'extra': payload,\n        })\n    else:\n        self.rawConsole({\n            'extra': payload,\n        })\n", "label": 0}
{"function": "\n\ndef GenerateCommand(self):\n    super_cmds = super(MsgBox, self).GenerateCommand()\n    cmds = []\n    cmds.extend(['--button1', self._button1])\n    if self._button2:\n        cmds.extend(['--button2', self._button2])\n    if self._button3:\n        cmds.extend(['--button3', self._button3])\n    if self._informative_text:\n        cmds.extend(['--informative-text', self._informative_text])\n    if self._float:\n        cmds.append('--float')\n    super_cmds.extend(cmds)\n    return super_cmds\n", "label": 0}
{"function": "\n\ndef _make_subnet_args(self, detail, subnet, subnetpool_id):\n    gateway_ip = (str(detail.gateway_ip) if detail.gateway_ip else None)\n    args = {\n        'tenant_id': detail.tenant_id,\n        'id': detail.subnet_id,\n        'name': subnet['name'],\n        'network_id': subnet['network_id'],\n        'ip_version': subnet['ip_version'],\n        'cidr': str(detail.subnet_cidr),\n        'subnetpool_id': subnetpool_id,\n        'enable_dhcp': subnet['enable_dhcp'],\n        'gateway_ip': gateway_ip,\n        'description': subnet.get('description'),\n    }\n    if ((subnet['ip_version'] == 6) and subnet['enable_dhcp']):\n        if validators.is_attr_set(subnet['ipv6_ra_mode']):\n            args['ipv6_ra_mode'] = subnet['ipv6_ra_mode']\n        if validators.is_attr_set(subnet['ipv6_address_mode']):\n            args['ipv6_address_mode'] = subnet['ipv6_address_mode']\n    return args\n", "label": 0}
{"function": "\n\ndef git_config_get_value(section, option, default=None, as_bool=False):\n    'Get config value for section/option.'\n    cmd = ['git', 'config', '--get', ('%s.%s' % (section, option))]\n    if as_bool:\n        cmd.insert(2, '--bool')\n    if LOCAL_MODE:\n        (__, git_dir) = git_directories()\n        cmd[2:2] = ['-f', os.path.join(git_dir, 'config')]\n    try:\n        return run_command_exc(GitConfigException, *cmd).strip()\n    except GitConfigException as exc:\n        if (exc.rc == 1):\n            return default\n        raise\n", "label": 0}
{"function": "\n\ndef autonomous(self):\n    'Called when autonomous mode is enabled'\n    timer = wpilib.Timer()\n    timer.start()\n    while (self.isAutonomous() and self.isEnabled()):\n        if (timer.get() < 2.0):\n            self.robot_drive.arcadeDrive((- 1.0), (- 0.3))\n        else:\n            self.robot_drive.arcadeDrive(0, 0)\n        wpilib.Timer.delay(0.01)\n", "label": 0}
{"function": "\n\ndef get_dialect(self, dialect_source):\n    'Get a CSV dialect instance for this data.'\n    if dialect_source:\n        (_valid, rv) = csv_dialect.validate(dialect_source)\n        if (not _valid):\n            raise exceptions.InvalidSpec\n        return rv\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef password_reset_from_key(request, uidb36, key, **kwargs):\n    form_class = kwargs.get('form_class', ResetPasswordKeyForm)\n    template_name = kwargs.get('template_name', 'account/password_reset_from_key.html')\n    token_generator = kwargs.get('token_generator', default_token_generator)\n    (group, bridge) = group_and_bridge(kwargs)\n    ctx = group_context(group, bridge)\n    try:\n        uid_int = base36_to_int(uidb36)\n    except ValueError:\n        raise Http404\n    user = get_object_or_404(User, id=uid_int)\n    if token_generator.check_token(user, key):\n        if (request.method == 'POST'):\n            password_reset_key_form = form_class(request.POST, user=user, temp_key=key)\n            if password_reset_key_form.is_valid():\n                password_reset_key_form.save()\n                messages.add_message(request, messages.SUCCESS, ugettext('Password successfully changed.'))\n                password_reset_key_form = None\n        else:\n            password_reset_key_form = form_class()\n        ctx.update({\n            'form': password_reset_key_form,\n        })\n    else:\n        ctx.update({\n            'token_fail': True,\n        })\n    return render_to_response(template_name, RequestContext(request, ctx))\n", "label": 0}
{"function": "\n\ndef _generate_init(class_, class_manager):\n    'Build an __init__ decorator that triggers ClassManager events.'\n    original__init__ = class_.__init__\n    assert original__init__\n    func_body = 'def __init__(%(apply_pos)s):\\n    new_state = class_manager._new_state_if_none(%(self_arg)s)\\n    if new_state:\\n        return new_state._initialize_instance(%(apply_kw)s)\\n    else:\\n        return original__init__(%(apply_kw)s)\\n'\n    func_vars = util.format_argspec_init(original__init__, grouped=False)\n    func_text = (func_body % func_vars)\n    if util.py2k:\n        func = getattr(original__init__, 'im_func', original__init__)\n        func_defaults = getattr(func, 'func_defaults', None)\n    else:\n        func_defaults = getattr(original__init__, '__defaults__', None)\n        func_kw_defaults = getattr(original__init__, '__kwdefaults__', None)\n    env = locals().copy()\n    exec(func_text, env)\n    __init__ = env['__init__']\n    __init__.__doc__ = original__init__.__doc__\n    if func_defaults:\n        __init__.__defaults__ = func_defaults\n    if ((not util.py2k) and func_kw_defaults):\n        __init__.__kwdefaults__ = func_kw_defaults\n    return __init__\n", "label": 0}
{"function": "\n\ndef process(args):\n    conduit = phlsys_makeconduit.make_conduit(args.uri, args.user, args.cert, args.act_as_user)\n    if args.text:\n        text = args.text\n    elif args.text_file:\n        text = args.text_file.read()\n    else:\n        print('error: you have not specified any content for the paste')\n        sys.exit(1)\n    result = phlcon_paste.create_paste(conduit, text, args.title, args.language)\n    print((result.uri if (not args.format_id) else result.id))\n", "label": 0}
{"function": "\n\ndef test_Transform():\n    add1 = Transform((lambda x: (x + 1)), (lambda x: ((x % 2) == 1)))\n    assert (add1[1] == 2)\n    assert ((1 in add1) is True)\n    assert (add1.get(1) == 2)\n    raises(KeyError, (lambda : add1[2]))\n    assert ((2 in add1) is False)\n    assert (add1.get(2) is None)\n", "label": 0}
{"function": "\n\ndef main():\n    parser = make_parser()\n    args = parser.parse_args()\n    logger = Logger(args.verbose)\n    storage = Storage(args.storage, logger)\n    if (args.command == 'list'):\n        for file in storage.query():\n            print(file)\n    elif (args.command == 'compare'):\n        results_table = TableResults(args.columns, args.sort, first_or_value(args.histogram, False), NAME_FORMATTERS[args.name], logger)\n        groups = load(storage, args.glob_or_file, args.group_by)\n        results_table.display(TerminalReporter(), groups, progress_reporter=report_noprogress)\n        if args.csv:\n            results_csv = CSVResults(args.columns, args.sort, logger)\n            (output_file,) = args.csv\n            results_csv.render(output_file, groups)\n    else:\n        parser.error('Unknown command {0!r}'.format(args.command))\n", "label": 0}
{"function": "\n\ndef test_uidvalidity_changed(self):\n    db = mock_db()\n    account = _account_for_test(db=db)\n    msg13_bis_data = (13, set(['\\\\Seen']), 'Subject: another message')\n    with mock_worker(fol1={\n        13: msg13_data,\n        'UIDVALIDITY': 1234,\n    }):\n        account.perform_update()\n    with mock_worker(fol1={\n        13: msg13_bis_data,\n        'UIDVALIDITY': 1239,\n    }):\n        account.perform_update()\n    account2 = _account_for_test(db=db)\n    fol1 = account2.get_folder('fol1')\n    self.assertEqual(fol1._uidvalidity, 1239)\n    self.assertEqual([m.raw_headers for m in fol1.list_messages()], [msg13_bis_data[2]])\n", "label": 0}
{"function": "\n\ndef _set_byteorder(self, order):\n    if (order not in (0, 1)):\n        raise ValueError('Byte order parameter must be 0 (Big Endian) or 1 (Little Endian).')\n    wkb_writer_set_byteorder(self.ptr, order)\n", "label": 0}
{"function": "\n\ndef _local_settings_acked(self):\n    '\\n        Handle the local settings being ACKed, update internal state.\\n        '\n    changes = self.local_settings.acknowledge()\n    if (INITIAL_WINDOW_SIZE in changes):\n        setting = changes[INITIAL_WINDOW_SIZE]\n        self._inbound_flow_control_change_from_settings(setting.original_value, setting.new_value)\n    return changes\n", "label": 0}
{"function": "\n\ndef test_warp_no_reproject_res(runner, tmpdir):\n    srcname = 'tests/data/shade.tif'\n    outputname = str(tmpdir.join('test.tif'))\n    result = runner.invoke(warp.warp, [srcname, outputname, '--res', 30])\n    assert (result.exit_code == 0)\n    assert os.path.exists(outputname)\n    with rasterio.open(srcname) as src:\n        with rasterio.open(outputname) as output:\n            assert (output.crs == src.crs)\n            assert numpy.allclose([30, 30], [output.affine.a, (- output.affine.e)])\n            assert (output.width == 327)\n            assert (output.height == 327)\n", "label": 0}
{"function": "\n\ndef __init__(self, rules, header=None, separator=''):\n    '\\n            Constructor.\\n\\n            :param rules:\\n                Child rules\\n            :param header:\\n                Header text\\n            :param separator:\\n                Child rule separator\\n        '\n    if header:\n        rule_set = ([Header(header)] + list(rules))\n    else:\n        rule_set = list(rules)\n    super(FieldSet, self).__init__(rule_set, separator=separator)\n", "label": 0}
{"function": "\n\ndef insert(self, event_key, propagate):\n    target = event_key.dispatch_target\n    assert isinstance(target, type), 'Class-level Event targets must be classes.'\n    stack = [target]\n    while stack:\n        cls = stack.pop(0)\n        stack.extend(cls.__subclasses__())\n        if ((cls is not target) and (cls not in self._clslevel)):\n            self.update_subclass(cls)\n        else:\n            if (cls not in self._clslevel):\n                self._clslevel[cls] = collections.deque()\n            self._clslevel[cls].appendleft(event_key._listen_fn)\n    registry._stored_in_collection(event_key, self)\n", "label": 0}
{"function": "\n\ndef test_select_x():\n\n    def _check(data, expected):\n        v = api.Viz(data)\n        v.select_x()\n        assert (v.encoding.x.name == expected)\n    data = dict(col1=[1.0, 2.0, 3.0], col2=['A', 'B', 'C'], col3=pd.date_range('2012', periods=3, freq='A'), col4=pd.date_range('2012', periods=3, freq='A'))\n    _check(data, 'col3')\n    data = dict(col1=[1.0, 2.0, 3.0], col2=['A', 'B', 'C'], col3=['A', 'B', 'C'])\n    _check(data, 'col2')\n    data = dict(col1=[1.0, 2.0, 3.0], col2=np.arange(3))\n    _check(data, 'col1')\n    with pytest.raises(AssertionError):\n        v = api.Viz(None)\n        v.select_x()\n    with pytest.raises(AssertionError):\n        v = api.Viz(dict())\n        v.select_x()\n    data = dict(col1=[1.0, 2.0, 3.0], col2=['A', 'B', 'C'], col3=pd.date_range('2012', periods=3, freq='A'), col4=pd.date_range('2012', periods=3, freq='A'))\n    v = api.Viz(data)\n    v.select_x(['N', 'T', 'Q', 'O'])\n    assert (v.encoding.x.name == 'col2')\n", "label": 0}
{"function": "\n\ndef get_test_problem(task='regression'):\n    X = sp.csc_matrix(np.array([[6, 1], [2, 3], [3, 0], [6, 1], [4, 5]]), dtype=np.float64)\n    y = np.array([298, 266, 29, 298, 848], dtype=np.float64)\n    V = np.array([[6, 0], [5, 8]], dtype=np.float64)\n    w = np.array([9, 2], dtype=np.float64)\n    w0 = 2\n    if (task == 'classification'):\n        y_labels = np.ones_like(y)\n        y_labels[(y < np.median(y))] = (- 1)\n        y = y_labels\n    return (w0, w, V, y, X)\n", "label": 0}
{"function": "\n\ndef build_waiter_state_description(self):\n    description = self._waiter_config.description\n    if (not description):\n        description = 'Wait until '\n        for acceptor in self._waiter_config.acceptors:\n            if (acceptor.state == 'success'):\n                description += self._build_success_description(acceptor)\n                break\n        description += self._build_operation_description(self._waiter_config.operation)\n    description += self._build_polling_description(self._waiter_config.delay, self._waiter_config.max_attempts)\n    return description\n", "label": 0}
{"function": "\n\ndef _selected_location_entries(self, mobile_user_and_group_slugs):\n    location_ids = self.selected_location_ids(mobile_user_and_group_slugs)\n    if (not location_ids):\n        return []\n    return map(self.utils.location_tuple, SQLLocation.objects.filter(location_id__in=location_ids))\n", "label": 0}
{"function": "\n\ndef setup(self):\n    self.mi = MultiIndex.from_tuples([(x, y) for x in range(1000) for y in range(1000)])\n    self.s = Series(np.random.randn(1000000), index=self.mi)\n    self.df = DataFrame(self.s)\n", "label": 0}
{"function": "\n\ndef run_backfill(config_path, config_id, config, start_at, end_at):\n    collector_path = join(dirname(sys.executable), 'pp-collector')\n    query_path = get_query_path(config_path, config_id)\n    credentials_path = join(config_path, 'credentials', 'ga.json')\n    token_path = get_token_path(config_path, config['token'])\n    platform_path = join(config_path, 'performanceplatform.json')\n    command = [collector_path, '-q', query_path, '-c', credentials_path, '-t', token_path, '-b', platform_path, '-s', start_at.isoformat()]\n    if (end_at is not None):\n        command += ['-e', end_at.isoformat()]\n    status = subprocess.call(command, stderr=sys.stdout.fileno(), stdout=sys.stdout.fileno())\n    if (status != 0):\n        print('Failed!')\n        raise SystemExit(1)\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('name,term', [('jip.cluster.Slurm', '%j'), ('jip.cluster.PBS', '$PBS_JOBID'), ('jip.cluster.LSF', '%J'), ('jip.cluster.SGE', '$JOB_ID')])\ndef test_resolving_log_file_names(name, term):\n    fakeBinDir = createFakeBinaries()\n    Job = namedtuple('Job', 'job_id')\n    j = Job(1)\n    cluster = cl.get(name)\n    assert (cluster.resolve_log(j, ('log-%s' % term)) == 'log-1')\n    removeFakeBinaries(fakeBinDir)\n", "label": 0}
{"function": "\n\n@ensure_tag(['r'])\ndef is_bold(r):\n    '\\n    The function will return True if the r tag passed in is considered bold.\\n    '\n    w_namespace = get_namespace(r, 'w')\n    rpr = r.find(('%srPr' % w_namespace))\n    if (rpr is None):\n        return False\n    bold = rpr.find(('%sb' % w_namespace))\n    return style_is_false(bold)\n", "label": 0}
{"function": "\n\ndef _get_base_value_unwrapped_as_list(self, entity):\n    'Like _get_base_value(), but always returns a list.\\n\\n    Returns:\\n      A new list of unwrapped base values.  For an unrepeated\\n      property, if the value is missing or None, returns [None]; for a\\n      repeated property, if the original value is missing or None or\\n      empty, returns [].\\n    '\n    wrapped = self._get_base_value(entity)\n    if self._repeated:\n        if (wrapped is None):\n            return []\n        assert isinstance(wrapped, list)\n        return [w.b_val for w in wrapped]\n    else:\n        if (wrapped is None):\n            return [None]\n        assert isinstance(wrapped, _BaseValue)\n        return [wrapped.b_val]\n", "label": 0}
{"function": "\n\ndef get_db_prep_save(self, value):\n    if (value is not None):\n        value = smart_unicode(value)\n    return super(TimeZoneField, self).get_db_prep_save(value)\n", "label": 0}
{"function": "\n\ndef __init__(self, manager, user_state=None, project_state=None):\n    if (not user_state):\n        user_state = {\n            \n        }\n    if (not project_state):\n        project_state = {\n            \n        }\n    self.manager = manager\n    if ('name' not in user_state):\n        user_state['name'] = 'test1'\n    if ('name' not in project_state):\n        project_state['name'] = 'testproj'\n    if ('manager_user' not in project_state):\n        project_state['manager_user'] = 'test1'\n    self.user = manager.create_user(**user_state)\n    self.project = manager.create_project(**project_state)\n", "label": 0}
{"function": "\n\ndef _validate(self):\n    for (group, dbs) in self.partition_config['groups'].items():\n        for db in dbs:\n            if (db not in self.database_config):\n                raise PartitionValidationError('{} not in found in DATABASES'.format(db))\n    shards_seen = set()\n    previous_range = None\n    for (group, shard_range) in sorted(self.partition_config['shards'].items(), key=(lambda x: x[1])):\n        if (not previous_range):\n            if (shard_range[0] != 0):\n                raise NotZeroStartError('Shard numbering must start at 0')\n        elif ((previous_range[1] + 1) != shard_range[0]):\n            raise NonContinuousShardsError('Shards must be numbered consecutively: {} -> {}'.format(previous_range[1], shard_range[0]))\n        shards_seen |= set(range(shard_range[0], (shard_range[1] + 1)))\n        previous_range = shard_range\n    num_shards = len(shards_seen)\n    if (not _is_power_of_2(num_shards)):\n        raise NotPowerOf2Error('Total number of shards must be a power of 2')\n", "label": 1}
{"function": "\n\ndef project_documentation_file(request, snapshot):\n    project = Domain.get(snapshot)\n    if project.documentation_file_path:\n        documentation_file = project.fetch_attachment(project.documentation_file_path)\n        return HttpResponse(documentation_file, content_type=project.documentation_file_type)\n    else:\n        raise Http404()\n", "label": 0}
{"function": "\n\ndef bytes2human(use_si_units=False):\n    if use_si_units:\n        prefixes = ('TB', 'GB', 'MB', 'kB', 'B')\n        values = (1000000000000.0, 1000000000.0, 1000000.0, 1000.0, 1)\n    else:\n        prefixes = ('TiB', 'GiB', 'MiB', 'KiB', 'B')\n        values = ((2 ** 40), (2 ** 30), (2 ** 20), (2 ** 10), 1)\n\n    def b2h(nbytes):\n        for (prefix, value) in zip(prefixes, values):\n            scaled = (float(nbytes) / value)\n            if (scaled >= 1):\n                break\n        return ('%.1f%s' % (scaled, prefix))\n    return b2h\n", "label": 0}
{"function": "\n\ndef fetch_gerrit_hook_ssh(path, username, server, port=None):\n    ' Fetch the ``commit-msg`` hook from gerrit\\n\\n    '\n    if (port is None):\n        port = 22\n    git_hooks_dir = os.path.join(path, '.git', 'hooks')\n    if (not os.path.isdir(git_hooks_dir)):\n        qisys.sh.mkdir(git_hooks_dir)\n    if sys.platform.startswith('win'):\n        git_hooks_dir = qisys.sh.to_posix_path(git_hooks_dir, fix_drive=True)\n    scp = qisys.command.find_program('scp', raises=False)\n    if (not scp):\n        return (False, 'Could not find scp executable')\n    cmd = [scp, '-P', str(port), ('%s@%s:hooks/commit-msg' % (username, server)), git_hooks_dir]\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    (out, _) = process.communicate()\n    if (process.returncode == 0):\n        return (True, '')\n    else:\n        return (False, out)\n", "label": 0}
{"function": "\n\ndef __init__(self, verbose_name=None, name=None, timezone=None, **kwargs):\n    if isinstance(timezone, basestring):\n        timezone = smart_str(timezone)\n    if (timezone in pytz.all_timezones_set):\n        self.timezone = pytz.timezone(timezone)\n    else:\n        self.timezone = timezone\n    super(LocalizedDateTimeField, self).__init__(verbose_name, name, **kwargs)\n", "label": 0}
{"function": "\n\ndef visit_Call(self, node):\n    if isinstance(node.func, ast.Attribute):\n        if isinstance(node.func.value, ast.Name):\n            if (node.func.value.id == 'context'):\n                if (node.func.attr == 'getFrame'):\n                    self.contextReads.add('frame')\n                elif (node.func.attr == 'getTime'):\n                    self.contextReads.add('frame')\n                    self.contextReads.add('framesPerSecond')\n                elif (node.func.attr == 'getFramesPerSecond'):\n                    self.contextReads.add('framesPerSecond')\n                elif (node.func.attr == 'get'):\n                    if (not isinstance(node.args[0], ast.Str)):\n                        raise SyntaxError('Context name must be a string')\n                    self.contextReads.add(node.args[0].s)\n    ast.NodeVisitor.generic_visit(self, node)\n", "label": 1}
{"function": "\n\ndef __init__(self, names, propagate_map_exceptions=False):\n    'Initialise the resource handler by loading the plugins.\\n\\n        The ResourceHandler uses stevedore to load the resource plugins.\\n        The handler can handle and report exceptions raised in the plugins\\n        depending on the value of the propagate_map_exceptions parameter.\\n        It is useful in testing to propagate exceptions so they are exposed\\n        as part of the test. If exceptions are not propagated they are\\n        logged at error level.\\n\\n        Any named plugins that are not located are logged.\\n\\n        :param names: the list of plugins to load by name\\n        :param propagate_map_exceptions: True indicates exceptions in the\\n        plugins should be raised, False indicates they should be handled and\\n        logged.\\n        '\n    self._mgr = stevedore.NamedExtensionManager(namespace=RESOURCE_NAMESPACE, names=names, propagate_map_exceptions=propagate_map_exceptions, invoke_on_load=True)\n    if self._mgr.names():\n        LOG.warning(_LW('The Extensible Resource Tracker is deprecated and will be removed in the 14.0.0 release. If you use this functionality and have custom resources that are managed by the Extensible Resource Tracker, please contact the Nova development team by posting to the openstack-dev mailing list. There is no future planned support for the tracking of custom resources.'))\n    self._log_missing_plugins(names)\n", "label": 0}
{"function": "\n\ndef _split_args(args):\n    try:\n        args = shlex.split(args, posix=False)\n    except:\n        args = args.split()\n    new_args = []\n    temp = ''\n    for arg in args:\n        if arg.startswith('-'):\n            new_args.append(arg)\n        elif temp:\n            arg = ((temp + ' ') + arg)\n            try:\n                eval(arg)\n            except:\n                temp = arg\n            else:\n                new_args.append(arg)\n                temp = ''\n        elif (arg.startswith(('(', '[', '{')) or ('(' in arg)):\n            try:\n                eval(arg)\n            except:\n                temp = arg\n            else:\n                new_args.append(arg)\n        else:\n            new_args.append(arg)\n    if temp:\n        new_args.append(temp)\n    return new_args\n", "label": 1}
{"function": "\n\ndef look_ahead(self, s):\n    if ((self.index + len(s)) >= len(self.input)):\n        return False\n    return (self.input[self.index:(self.index + len(s))] == s)\n", "label": 0}
{"function": "\n\ndef __init__(self, session_id=None):\n    self.session_id = (session_id or 1)\n    self.expiry = None\n    self.promoted = None\n    self._delete_called = False\n", "label": 0}
{"function": "\n\ndef valueChange(self, event):\n    f = event.getProperty().getValue()\n    v = self._tree.getValue()\n    if (((f is not None) and (f != v)) or ((f is None) and (v is not None))):\n        self._tree.setValue(f)\n    self._window.removeSubwindows()\n", "label": 0}
{"function": "\n\ndef flush(self, timestamp, interval):\n    if (self.count is None):\n        return []\n    try:\n        return [self.formatter(hostname=self.hostname, device_name=self.device_name, tags=self.tags, metric=self.name, value=self.count, timestamp=timestamp, metric_type=MetricTypes.COUNT, interval=interval)]\n    finally:\n        self.prev_counter = self.curr_counter\n        self.curr_counter = None\n        self.count = None\n", "label": 0}
{"function": "\n\ndef interpretkeyevent(keyEvent):\n    'Returns the character represented by the pygame.event.Event object in keyEvent. This makes adjustments for the shift key and capslock.'\n    key = keyEvent.key\n    if (((key >= 32) and (key < 127)) or (key in (ord('\\n'), ord('\\r'), ord('\\t')))):\n        caps = bool((keyEvent.mod & KMOD_CAPS))\n        shift = bool(((keyEvent.mod & KMOD_LSHIFT) or (keyEvent.mod & KMOD_RSHIFT)))\n        char = chr(key)\n        if (char.isalpha() and (caps ^ shift)):\n            char = char.upper()\n        elif (shift and (char in _shiftchars)):\n            char = _shiftchars[char]\n        return char\n    return None\n", "label": 1}
{"function": "\n\ndef apply_settings(self, view):\n    ' Applies the settings from the settings file '\n    if (self.pluginSettings is None):\n        self.pluginSettings = sublime.load_settings((__name__ + '.sublime-settings'))\n        self.pluginSettings.clear_on_change('glsv_validator')\n        self.pluginSettings.add_on_change('glsv_validator', self.clear_settings)\n    if (view.settings().get('glsv_configured') is None):\n        view.settings().set('glsv_configured', True)\n        for setting in self.DEFAULT_SETTINGS:\n            settingValue = self.DEFAULT_SETTINGS[setting]\n            if (self.pluginSettings.get(setting) is not None):\n                settingValue = self.pluginSettings.get(setting)\n            view.settings().set(setting, settingValue)\n", "label": 0}
{"function": "\n\ndef __init__(self, digest_provider, starting_bucket, starting_prefix, public_key_provider, digest_validator=None, on_invalid=None, on_gap=None, on_missing=None):\n    '\\n        :type digest_provider: DigestProvider\\n        :param digest_provider: DigestProvider object\\n        :param starting_bucket: S3 bucket where the digests are stored.\\n        :param starting_prefix: An optional prefix applied to each S3 key.\\n        :param public_key_provider: Provides public keys for a range.\\n        :param digest_validator: Validates digest using a validate method.\\n        :param on_invalid: Callback invoked when a digest is invalid.\\n        :param on_gap: Callback invoked when a digest has no parent, but\\n            there are still more digests to validate.\\n        :param on_missing: Callback invoked when a digest file is missing.\\n        '\n    self.starting_bucket = starting_bucket\n    self.starting_prefix = starting_prefix\n    self.digest_provider = digest_provider\n    self._public_key_provider = public_key_provider\n    self._on_gap = on_gap\n    self._on_invalid = on_invalid\n    self._on_missing = on_missing\n    if (digest_validator is None):\n        digest_validator = Sha256RSADigestValidator()\n    self._digest_validator = digest_validator\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return (isinstance(other, self.__class__) and (self.__dict__ == other.__dict__))\n", "label": 0}
{"function": "\n\ndef _security_group_rule_exists(self, security_group, values):\n    'Indicates whether the specified rule values are already\\n           defined in the given security group.\\n        '\n    for rule in security_group.rules:\n        if ('group_id' in values):\n            if (rule['group_id'] == values['group_id']):\n                return True\n        else:\n            is_duplicate = True\n            for key in ('cidr', 'from_port', 'to_port', 'protocol'):\n                if (rule[key] != values[key]):\n                    is_duplicate = False\n                    break\n            if is_duplicate:\n                return True\n    return False\n", "label": 0}
{"function": "\n\n@classmethod\ndef pull_users_and_groups(cls, domain, mobile_user_and_group_slugs, include_inactive=False):\n    user_ids = cls.selected_user_ids(mobile_user_and_group_slugs)\n    user_types = cls.selected_user_types(mobile_user_and_group_slugs)\n    group_ids = cls.selected_group_ids(mobile_user_and_group_slugs)\n    users = []\n    if (user_ids or (HQUserType.REGISTERED in user_types)):\n        users = util.get_all_users_by_domain(domain=domain, user_ids=user_ids, simplified=True, CommCareUser=CommCareUser)\n    user_filter = tuple([HQUserToggle(id, (id in user_types)) for id in range(4)])\n    other_users = util.get_all_users_by_domain(domain=domain, user_filter=user_filter, simplified=True, CommCareUser=CommCareUser, include_inactive=include_inactive)\n    groups = [Group.get(g) for g in group_ids]\n    all_users = (users + other_users)\n    user_dict = {\n        \n    }\n    for group in groups:\n        user_dict[('%s|%s' % (group.name, group._id))] = util.get_all_users_by_domain(group=group, simplified=True)\n    users_in_groups = [user for sublist in user_dict.values() for user in sublist]\n    users_by_group = user_dict\n    combined_users = remove_dups((all_users + users_in_groups), 'user_id')\n    return _UserData(users=all_users, admin_and_demo_users=other_users, groups=groups, users_by_group=users_by_group, combined_users=combined_users)\n", "label": 0}
{"function": "\n\ndef AddColumn(self, column, default='', col_index=(- 1)):\n    \"Appends a new column to the table.\\n\\n    Args:\\n      column: A string, name of the column to add.\\n      default: Default value for entries. Defaults to ''.\\n      col_index: Integer index for where to insert new column.\\n\\n    Raises:\\n      TableError: Column name already exists.\\n\\n    \"\n    if (column in self.table):\n        raise TableError(('Column %r already in table.' % column))\n    if (col_index == (- 1)):\n        self._table[0][column] = column\n        for i in xrange(1, len(self._table)):\n            self._table[i][column] = default\n    else:\n        self._table[0].Insert(column, column, col_index)\n        for i in xrange(1, len(self._table)):\n            self._table[i].Insert(column, default, col_index)\n", "label": 0}
{"function": "\n\ndef test_req_file_parse_egginfo_end_of_line_with_url(self, tmpdir, finder):\n    '\\n        Test parsing comments in a requirements file\\n        '\n    with open(tmpdir.join('req1.txt'), 'w') as fp:\n        fp.write('https://example.com/foo.tar.gz#egg=wat')\n    reqs = list(parse_requirements(tmpdir.join('req1.txt'), finder, session=PipSession()))\n    assert (len(reqs) == 1)\n    assert (reqs[0].name == 'wat')\n", "label": 0}
{"function": "\n\ndef run(self):\n    repo = self.get_repo()\n    if (not repo):\n        return\n    status = self.get_porcelain_status(repo)\n    if (not status):\n        status = [GIT_WORKING_DIR_CLEAN]\n\n    def on_done(idx):\n        if ((idx == (- 1)) or (status[idx] == GIT_WORKING_DIR_CLEAN)):\n            return\n        (state, filename) = (status[idx][0:2], status[idx][3:])\n        (index, worktree) = state\n        if (state == '??'):\n            return sublime.error_message('Cannot show diff for untracked files.')\n        window = self.window\n        if (worktree != ' '):\n            window.run_command('git_diff', {\n                'repo': repo,\n                'path': filename,\n            })\n        if (index != ' '):\n            window.run_command('git_diff', {\n                'repo': repo,\n                'path': filename,\n                'cached': True,\n            })\n    self.window.show_quick_panel(status, on_done, sublime.MONOSPACE_FONT)\n", "label": 0}
{"function": "\n\ndef test_create_sequence_in_schema(self):\n    'Create a sequence within a non-public schema'\n    inmap = self.std_map()\n    inmap.update({\n        'schema s1': {\n            'sequence seq1': {\n                'start_value': 1,\n                'increment_by': 1,\n                'max_value': None,\n                'min_value': None,\n                'cache_value': 1,\n            },\n        },\n    })\n    sql = self.to_sql(inmap, ['CREATE SCHEMA s1'])\n    assert (fix_indent(sql[0]) == 'CREATE SEQUENCE s1.seq1 START WITH 1 INCREMENT BY 1 NO MAXVALUE NO MINVALUE CACHE 1')\n", "label": 0}
{"function": "\n\ndef put_continue_creation(self, token, content, content_range, last=False):\n    'Continue object upload with PUTs.\\n\\n    This implements the resumable upload XML API.\\n\\n    Args:\\n      token: upload token returned by post_start_creation.\\n      content: object content.\\n      content_range: a (start, end) tuple specifying the content range of this\\n        chunk. Both are inclusive according to XML API.\\n      last: True if this is the last chunk of file content.\\n\\n    Raises:\\n      ValueError: if token is invalid.\\n    '\n    gcs_file = _AE_GCSFileInfo_.get_by_key_name(token)\n    if (not gcs_file):\n        raise ValueError('Invalid token')\n    if content:\n        (start, end) = content_range\n        if (len(content) != ((end - start) + 1)):\n            raise ValueError(('Invalid content range %d-%d' % content_range))\n        blobkey = ('%s-%d-%d' % (token, content_range[0], content_range[1]))\n        self.blob_storage.StoreBlob(blobkey, StringIO.StringIO(content))\n        new_content = _AE_GCSPartialFile_(parent=gcs_file, partial_content=blobkey, start=start, end=(end + 1))\n        new_content.put()\n    if last:\n        self._end_creation(token)\n", "label": 0}
{"function": "\n\ndef cleanup_object_infos(self):\n    for q in sorted(self.objs_by_size.values()):\n        first_initial = None\n        try:\n            while ((not first_initial) or (q[0] != first_initial)):\n                obj_info = q[0]\n                if obj_info[2]:\n                    if (not first_initial):\n                        first_initial = obj_info\n                    q.rotate((- 1))\n                else:\n                    (yield q.popleft())\n        except IndexError:\n            pass\n", "label": 0}
{"function": "\n\ndef body(self, result):\n    body = 'OK'\n    if (self.response_type == 'json'):\n        body = json_dumps(result)\n    return body\n", "label": 0}
{"function": "\n\ndef process_input(self, *args, **kwargs):\n    '\\n        Convert input json data into a statement object.\\n        '\n    if (not args):\n        raise TypeError('process_input expects at least one positional argument')\n    input_json = args[0]\n    text = input_json['text']\n    del input_json['text']\n    return Statement(text, **input_json)\n", "label": 0}
{"function": "\n\ndef test_notfound(webapp):\n    try:\n        urlopen(('%s/test_notfound' % webapp.server.http.base))\n    except HTTPError as e:\n        assert (e.code == 404)\n        assert (e.msg == 'Not Found')\n    else:\n        assert False\n", "label": 0}
{"function": "\n\ndef decompress(self, value):\n    '\\n        Receives an instance of `MultiLingualFile` and returns a list of\\n        broken-out-files corresponding in position to the current ordering\\n        of settings.LANGUAGES.\\n        '\n    text_dict = (dict(((code, getattr(value, code)) for (code, verbose) in LANGUAGES)) if value else {\n        \n    })\n    return [text_dict.get(code) for (code, verbose) in LANGUAGES]\n", "label": 0}
{"function": "\n\ndef test_displaying_more_than_one_blank_form(self):\n    ChoiceFormSet = formset_factory(Choice, extra=3)\n    formset = ChoiceFormSet(auto_id=False, prefix='choices')\n    form_output = []\n    for form in formset.forms:\n        form_output.append(form.as_ul())\n    self.assertEqual('\\n'.join(form_output), '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" /></li>\\n<li>Votes: <input type=\"text\" name=\"choices-0-votes\" /></li>\\n<li>Choice: <input type=\"text\" name=\"choices-1-choice\" /></li>\\n<li>Votes: <input type=\"text\" name=\"choices-1-votes\" /></li>\\n<li>Choice: <input type=\"text\" name=\"choices-2-choice\" /></li>\\n<li>Votes: <input type=\"text\" name=\"choices-2-votes\" /></li>')\n    data = {\n        'choices-TOTAL_FORMS': '3',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': '',\n        'choices-0-votes': '',\n        'choices-1-choice': '',\n        'choices-1-votes': '',\n        'choices-2-choice': '',\n        'choices-2-votes': '',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([form.cleaned_data for form in formset.forms], [{\n        \n    }, {\n        \n    }, {\n        \n    }])\n", "label": 0}
{"function": "\n\ndef _contains(self, other):\n    if (other.is_positive and other.is_integer):\n        return S.true\n    elif ((other.is_integer is False) or (other.is_positive is False)):\n        return S.false\n", "label": 0}
{"function": "\n\n@base.remotable_classmethod\ndef get_by_uuid(cls, context, uuid, expected_attrs=None, use_slave=False):\n    if (expected_attrs is None):\n        expected_attrs = ['info_cache', 'security_groups']\n    columns_to_join = _expected_cols(expected_attrs)\n    db_inst = cls._db_instance_get_by_uuid(context, uuid, columns_to_join, use_slave=use_slave)\n    return cls._from_db_object(context, cls(), db_inst, expected_attrs)\n", "label": 0}
{"function": "\n\ndef roleTypeName(modelXbrl, roleURI, *args, **kwargs):\n    modelManager = modelXbrl.modelManager\n    if hasattr(modelManager, 'efmFiling'):\n        modelRoles = modelXbrl.roleTypes.get(roleURI, ())\n        if (modelRoles and modelRoles[0].definition):\n            return re.sub('\\\\{\\\\s*(transposed|unlabeled|elements)\\\\s*\\\\}', '', modelRoles[0].definition.rpartition('-')[2], flags=re.I).strip()\n        return roleURI\n    return None\n", "label": 0}
{"function": "\n\ndef add_device(self, bind_data, cert, transports=0):\n    certificate = object_session(self).query(Certificate).filter((Certificate.fingerprint == b2a_hex(cert.fingerprint(hashes.SHA1())))).first()\n    if (certificate is None):\n        certificate = Certificate(cert)\n    return Device(self, bind_data, certificate, transports)\n", "label": 0}
{"function": "\n\ndef poll(self, timeout):\n    if (timeout < 0):\n        timeout = None\n    events = self._kqueue.control(None, KqueueLoop.MAX_EVENTS, timeout)\n    results = defaultdict((lambda : POLL_NULL))\n    for e in events:\n        fd = e.ident\n        if (e.filter == select.KQ_FILTER_READ):\n            results[fd] |= POLL_IN\n        elif (e.filter == select.KQ_FILTER_WRITE):\n            results[fd] |= POLL_OUT\n    return results.items()\n", "label": 0}
{"function": "\n\ndef get_tree(self, user, repo, ref='master', recursive=False, callback=None, **kwargs):\n    'Get a git tree'\n    path = 'repos/{user}/{repo}/git/trees/{ref}'.format(**locals())\n    if recursive:\n        params = kwargs.setdefault('params', {\n            \n        })\n        params['recursive'] = True\n    return self.github_api_request(path, callback, **kwargs)\n", "label": 0}
{"function": "\n\n@property\ndef elementAttributesTuple(self):\n    return tuple(((name, value) for (name, value) in self.items()))\n", "label": 0}
{"function": "\n\ndef _register_variable(self, objective_scope, variable_name, node, is_implicit=False, is_builtin=False, is_function=False):\n    variable = self._create_variable(is_implicit=is_implicit, is_builtin=is_builtin)\n    objective_variable_list = objective_scope[('functions' if is_function else 'variables')]\n    same_name_variables = objective_variable_list.setdefault(variable_name, [])\n    same_name_variables.append(variable)\n    self.link_registry.link_variable_to_declarative_identifier(variable, node)\n    current_scope = self.get_current_scope()\n    self.link_registry.link_identifier_to_context_scope(node, current_scope)\n", "label": 0}
{"function": "\n\ndef mode_init(self, request):\n    '\\n        This is called by render_POST when the client requests an init\\n        mode operation (at startup)\\n\\n        Args:\\n            request (Request): Incoming request.\\n\\n        '\n    suid = request.args.get('suid', ['0'])[0]\n    remote_addr = request.getClientIP()\n    host_string = ('%s (%s:%s)' % (SERVERNAME, request.getRequestHostname(), request.getHost().port))\n    if (suid == '0'):\n        suid = md5(str(time.time())).hexdigest()\n        self.databuffer[suid] = []\n        sess = WebClientSession()\n        sess.client = self\n        sess.init_session('webclient', remote_addr, self.sessionhandler)\n        sess.suid = suid\n        sess.sessionhandler.connect(sess)\n    return jsonify({\n        'msg': host_string,\n        'suid': suid,\n    })\n", "label": 0}
{"function": "\n\ndef __get__(self, obj, cls):\n    if (obj is None):\n        return self\n    value = self._deferred(obj)\n    setattr(obj, self._deferred.func_name, value)\n    return value\n", "label": 0}
{"function": "\n\ndef __init__(self, layers, loss):\n    self.layers = layers\n    self.loss = loss\n    self.bprop_until = next((idx for (idx, l) in enumerate(self.layers) if isinstance(l, ParamMixin)), 0)\n    self.layers[self.bprop_until].bprop_to_x = False\n    self.collection = self.layers\n    self._initialized = False\n", "label": 0}
{"function": "\n\ndef generate_random_string(string_length=88, chars=((string.ascii_lowercase + string.ascii_uppercase) + string.digits)):\n    '\\n    Generate a random string.\\n    :param string_length:\\n    :param chars:\\n    :return:\\n    '\n    return ''.join((random.SystemRandom().choice(chars) for _ in range(string_length)))\n", "label": 0}
{"function": "\n\ndef to_user(self, bin):\n    assert (len(bin) == self.size)\n    lb = _split_str(bin, self.length)\n    li = []\n    for b in lb:\n        i = 0\n        for x in range(self.length):\n            c = b[:1]\n            i = ((i * 256) + ord(c))\n            b = b[1:]\n        li.append(i)\n    return tuple(li)\n", "label": 0}
{"function": "\n\ndef as_entity(self, with_table=False):\n    if with_table:\n        return Entity(self.model_class._meta.db_table, self.db_column)\n    return Entity(self.db_column)\n", "label": 0}
{"function": "\n\ndef validURL(targetURL):\n    'Validate the target URL exists.\\n\\n    In a real app you would need to do a database lookup or a HEAD request, here we just check the URL\\n    '\n    if ('/article' in targetURL):\n        result = 200\n    else:\n        result = 404\n    return result\n", "label": 0}
{"function": "\n\ndef perform(self, node, inputs, outputs):\n    (x, y, p) = inputs\n    (out,) = outputs\n    if _is_sparse(x):\n        raise TypeError(x)\n    if _is_sparse(y):\n        raise TypeError(y)\n    if (not _is_sparse(p)):\n        raise TypeError(p)\n    out[0] = p.__class__(p.multiply(numpy.dot(x, y.T)))\n", "label": 0}
{"function": "\n\ndef wait(self, timeout=900, check_interval_secs=5):\n    'Waits for a deployment to finish\\n\\n        If a deployment completes successfully, True is returned, if it fails\\n        for any reason a DeploymentFailed exception is raised. If the\\n        deployment does not complete within ``timeout`` seconds, a\\n        DeploymentFailed exception is raised.\\n\\n        ``check_interval_secs`` is used to determine the delay between\\n        subsequent checks. The default of 5 seconds is adequate for normal\\n        use.\\n        '\n    _started = datetime.datetime.utcnow()\n    while True:\n        time.sleep(check_interval_secs)\n        if self.check():\n            return True\n        delta = (datetime.datetime.utcnow() - _started)\n        if (delta.total_seconds() > timeout):\n            raise DeploymentFailed(('Timed out: %d seconds' % timeout))\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    return ('Binary Expression: %s %s' % (self.operator, [str(x) for x in self.args]))\n", "label": 0}
{"function": "\n\ndef render_errors(self, errors, request, response):\n    data = {\n        'errors': errors,\n    }\n    if getattr(self, 'on_invalid', False):\n        data = self.on_invalid(data, **self._arguments(self._params_for_on_invalid, request, response))\n    response.status = HTTP_BAD_REQUEST\n    if getattr(self, 'invalid_outputs', False):\n        response.content_type = self.invalid_content_type(request, response)\n        response.data = self.invalid_outputs(data, **self._arguments(self._params_for_invalid_outputs, request, response))\n    else:\n        response.data = self.outputs(data, **self._arguments(self._params_for_outputs, request, response))\n", "label": 0}
{"function": "\n\ndef get_wildfire_apikey(session_key):\n    'Given a splunk session_key returns a clear text API Key from a splunk password container'\n    try:\n        entities = entity.getEntities(['admin', 'passwords'], namespace=APPNAME, owner='nobody', sessionKey=session_key)\n    except Exception as e:\n        exit_with_error(('Could not get %s credentials from splunk. Error: %s' % (APPNAME, str(e))))\n    for (i, c) in entities.items():\n        if (c['username'] == 'wildfire_api_key'):\n            return c['clear_password']\n    logger.warn('There are Palo Alto Networks WildFire malware events, but no WildFire API Key found, please set the API key in the SplunkforPaloAltoNetworks App set up page')\n    exit_with_error('No Wildfire API key is set, set apikey in App configuration.')\n", "label": 0}
{"function": "\n\ndef AppendAdditionalCoefficientBounds(self, inModel):\n    if inModel.baseEquationHasGlobalMultiplierOrDivisor_UsedInExtendedVersions:\n        if (inModel.upperCoefficientBounds != []):\n            inModel.upperCoefficientBounds.append(None)\n        if (inModel.lowerCoefficientBounds != []):\n            inModel.lowerCoefficientBounds.append(None)\n    else:\n        if (inModel.upperCoefficientBounds != []):\n            inModel.upperCoefficientBounds.append(None)\n            inModel.upperCoefficientBounds.append(None)\n        if (inModel.lowerCoefficientBounds != []):\n            inModel.lowerCoefficientBounds.append(None)\n            inModel.lowerCoefficientBounds.append(None)\n", "label": 0}
{"function": "\n\ndef test_sync_skips_unconfigured_projects(qisrc_action, git_server, test_git):\n    git_server.create_repo('foo.git')\n    qisrc_action('init', git_server.manifest_url)\n    git_worktree = TestGitWorkTree()\n    cwd = py.path.local(os.getcwd())\n    new_proj = cwd.mkdir('new_proj')\n    git = test_git(new_proj.strpath)\n    git.initialize()\n    git_worktree.add_git_project(new_proj.strpath)\n    rc = qisrc_action('sync', retcode=True)\n    assert (rc != 0)\n", "label": 0}
{"function": "\n\ndef reclassify_java(code_element, scode_reference):\n    reclassified = False\n    if ((scode_reference.snippet is not None) or (code_element is not None)):\n        return reclassified\n    automatic_reclass = set(['method', 'field', 'annotation field', 'enumeration value', 'annotation', 'enumeration'])\n    unknown_kind = cu.get_value(PREFIX_UNKNOWN, UNKNOWN_KEY, gl.get_unknown_kind, None)\n    if (scode_reference.kind_hint.kind in automatic_reclass):\n        scode_reference.kind_hint = unknown_kind\n        reclassified = True\n    elif (scode_reference.child_references.count() == 0):\n        scode_reference.kind_hint = unknown_kind\n        reclassified = True\n    if reclassified:\n        scode_reference.save()\n    return reclassified\n", "label": 0}
{"function": "\n\ndef __call__(self, instance, dictionary, bulkload_state):\n    'Implementation of StatPropertyTypePropertyNameKindPostExport.\\n\\n    See class docstring for more info.\\n\\n    Args:\\n      instance: Input, current entity being exported.\\n      dictionary: Output, dictionary created by property_map transforms.\\n      bulkload_state: Passed bulkload_state.\\n\\n    Returns:\\n      Dictionary--same object as passed in dictionary.\\n    '\n    kind_name = dictionary['kind_name']\n    property_name = dictionary['property_name']\n    property_type = dictionary['property_type']\n    if kind_name.startswith('__'):\n        return None\n    if (property_type == 'NULL'):\n        return None\n    property_key = (kind_name, property_name)\n    if (kind_name != self.last_seen):\n        self.last_seen = kind_name\n        separator = (KIND_PREAMBLE % dictionary)\n    elif (property_key in self.seen_properties):\n        separator = (PROPERTY_DUPE_WARNING % dictionary)\n    else:\n        separator = ''\n    self.seen_properties[property_key] = (self.seen_properties.get(property_key, 0) + 1)\n    dictionary['separator'] = separator\n    return dictionary\n", "label": 0}
{"function": "\n\ndef unstage(self, repo, files):\n    if self.no_commits(repo):\n        return self.git((['rm', '--cached', '--'] + files), cwd=repo)\n    return self.git((['reset', '-q', 'HEAD', '--'] + files), cwd=repo)\n", "label": 0}
{"function": "\n\ndef __get_raw_record(self, model_id, record_id):\n    try:\n        return deepcopy(self._db['records'][model_id][record_id])\n    except KeyError:\n        raise backend_exceptions.RecordNotFound(('(%s, %s)' % (model_id, record_id)))\n", "label": 0}
{"function": "\n\ndef getwithinrange(value, min=0, max=255):\n    '\\n    Returns value if it is between the min and max number arguments. If value is greater than max, then max is returned. If value is less than min, then min is returned. If min and/or max is not specified, then the value is not limited in that direction.\\n    '\n    if ((min is not None) and (value < min)):\n        return min\n    elif ((max is not None) and (value > max)):\n        return max\n    else:\n        return value\n", "label": 0}
{"function": "\n\ndef _compute_transitive_deps_by_target(self):\n    'Map from target to all the targets it depends on, transitively.'\n    sorted_targets = reversed(sort_targets(self.context.targets()))\n    transitive_deps_by_target = defaultdict(set)\n    for target in sorted_targets:\n        transitive_deps = set()\n        for dep in target.dependencies:\n            transitive_deps.update(transitive_deps_by_target.get(dep, []))\n            transitive_deps.add(dep)\n        if hasattr(target, 'java_sources'):\n            for java_source_target in target.java_sources:\n                for transitive_dep in java_source_target.dependencies:\n                    transitive_deps_by_target[java_source_target].add(transitive_dep)\n        transitive_deps_by_target[target] = transitive_deps\n    return transitive_deps_by_target\n", "label": 0}
{"function": "\n\n@classmethod\ndef to_path(cls, f):\n    if isinstance(f, SerializableFunction):\n        f.dumps_simple()\n    else:\n        return ('%s.%s' % (f.__module__, f.__name__))\n", "label": 0}
{"function": "\n\ndef spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n    'Pads the 2nd and 3rd dimensions of a 4D tensor\\n    with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\\n    '\n    if (dim_ordering == 'th'):\n        pattern = [[0, 0], [0, 0], [padding[0], padding[0]], [padding[1], padding[1]]]\n    else:\n        pattern = [[0, 0], [padding[0], padding[0]], [padding[1], padding[1]], [0, 0]]\n    return tf.pad(x, pattern)\n", "label": 0}
{"function": "\n\ndef decodeValue(self, value):\n    'Return True, False or -1 decoded from ``value``.'\n    if (value == '?'):\n        return (- 1)\n    if (value in 'NnFf '):\n        return False\n    if (value in 'YyTt'):\n        return True\n    raise ValueError(('[%s] Invalid logical value %r' % (self.name, value)))\n", "label": 0}
{"function": "\n\ndef test_content_submission_from_url(reddit, oauth, refresh_token, terminal):\n    url = 'https://www.reddit.com/r/AskReddit/comments/2np694/'\n    SubmissionContent.from_url(reddit, url, terminal.loader)\n    SubmissionContent.from_url(reddit, url, terminal.loader, order='new')\n    with terminal.loader():\n        SubmissionContent.from_url(reddit, url, terminal.loader, order='fake')\n    assert (not terminal.loader.exception)\n    with terminal.loader():\n        SubmissionContent.from_url(reddit, url[:(- 2)], terminal.loader)\n    assert isinstance(terminal.loader.exception, praw.errors.NotFound)\n    oauth.config.refresh_token = refresh_token\n    oauth.authorize()\n    url = 'https://np.reddit.com//r/LifeProTips/comments/441hsf//czmp112.json'\n    with terminal.loader():\n        SubmissionContent.from_url(reddit, url, terminal.loader)\n    assert (not terminal.loader.exception)\n", "label": 0}
{"function": "\n\ndef test_nested_constraints_file(self, monkeypatch):\n    line = '-c another_file'\n    req = InstallRequirement.from_line('SomeProject')\n    import pip.req.req_file\n\n    def stub_parse_requirements(req_url, finder, comes_from, options, session, wheel_cache, constraint):\n        return [(req, constraint)]\n    parse_requirements_stub = stub(call=stub_parse_requirements)\n    monkeypatch.setattr(pip.req.req_file, 'parse_requirements', parse_requirements_stub.call)\n    assert (list(process_line(line, 'filename', 1)) == [(req, True)])\n", "label": 0}
{"function": "\n\ndef kill_process(process):\n    '\\n    Kill the provided process by sending it TERM signal using \"pkill\" shell\\n    command.\\n\\n    Note: This function only works on Linux / Unix based systems.\\n\\n    :param process: Process object as returned by subprocess.Popen.\\n    :type process: ``object``\\n    '\n    kill_command = shlex.split(('sudo pkill -TERM -s %s' % process.pid))\n    try:\n        status = subprocess.call(kill_command)\n    except Exception:\n        LOG.exception('Unable to pkill process.')\n    return status\n", "label": 0}
{"function": "\n\ndef test_zero_division_nan(self, space):\n    w_res = space.execute('return 0.0 / 0.0')\n    assert math.isnan(self.unwrap(space, w_res))\n", "label": 0}
{"function": "\n\ndef action_get_by_request_id(self, ctxt, instance, request_id):\n    if (not instance['cell_name']):\n        raise exception.InstanceUnknownCell(instance_uuid=instance['uuid'])\n    cctxt = self.client.prepare(version='1.5')\n    return cctxt.call(ctxt, 'action_get_by_request_id', cell_name=instance['cell_name'], instance_uuid=instance['uuid'], request_id=request_id)\n", "label": 0}
{"function": "\n\ndef test_basic_auth(httpbin_both):\n    r = http('--auth=user:password', 'GET', (httpbin_both + '/basic-auth/user/password'))\n    assert (HTTP_OK in r)\n    assert (r.json == {\n        'authenticated': True,\n        'user': 'user',\n    })\n", "label": 0}
{"function": "\n\ndef purge_content(self, account_id, urls):\n    'Purges one or more URLs from the CDN edge nodes.\\n\\n        :param int account_id: the CDN account ID from which content should\\n                               be purged.\\n        :param urls: a string or a list of strings representing the CDN URLs\\n                     that should be purged.\\n        :returns: true if all purge requests were successfully submitted;\\n                  otherwise, returns the first error encountered.\\n        '\n    if isinstance(urls, six.string_types):\n        urls = [urls]\n    for i in range(0, len(urls), MAX_URLS_PER_PURGE):\n        result = self.account.purgeCache(urls[i:(i + MAX_URLS_PER_PURGE)], id=account_id)\n        if (not result):\n            return result\n    return True\n", "label": 0}
{"function": "\n\ndef get_port_configuration(self, port):\n    conf = []\n    if port.shutdown:\n        conf.append('shutdown')\n    if port.description:\n        conf.append(\"description '{}'\".format(port.description))\n    if (port.mode and (port.mode != 'access')):\n        conf.append('switchport mode {}'.format(port.mode))\n    if port.access_vlan:\n        conf.append('switchport access vlan {}'.format(port.access_vlan))\n    if port.trunk_native_vlan:\n        conf.append('switchport general pvid {}'.format(port.trunk_native_vlan))\n    if port.trunk_vlans:\n        conf.append('switchport {} allowed vlan add {}'.format(port.mode, to_vlan_ranges(port.trunk_vlans)))\n    if (port.spanning_tree is False):\n        conf.append('spanning-tree disable')\n    if port.spanning_tree_portfast:\n        conf.append('spanning-tree portfast')\n    if (port.lldp_transmit is False):\n        conf.append('no lldp transmit')\n    if (port.lldp_receive is False):\n        conf.append('no lldp receive')\n    if (port.lldp_med_transmit_capabilities is False):\n        conf.append('no lldp med transmit-tlv capabilities')\n    if (port.lldp_med_transmit_network_policy is False):\n        conf.append('no lldp med transmit-tlv network-policy')\n    return conf\n", "label": 1}
{"function": "\n\ndef one_to_dict(cur=None, row=None, col_names=None):\n    'Fetch one row from a cursor and make it as a dict.\\n\\n    If `col_names` or `row` is provided, it will be used first.\\n\\n    :rtype: dict\\n    '\n    if (col_names is None):\n        assert (cur is not None), 'You must specify cur or col_names.'\n        col_names = extract_col_names(cur)\n    if (row is None):\n        assert (cur is not None), 'You must specify cur or row.'\n        row = cur.fetchone()\n    return dict(izip(col_names, row))\n", "label": 0}
{"function": "\n\ndef _format_lines(self, tokensource):\n    for (tag, line) in HtmlFormatter._format_lines(self, tokensource):\n        if (tag == 1):\n            line = ('<span class=line>%s</span>' % line)\n        (yield (tag, line))\n", "label": 0}
{"function": "\n\ndef tests_should_return_proper_list(self):\n    w = sdg.generate_separated_multinomial_weights(self.A_good, self.C_good)\n    assert isinstance(w, list)\n    assert (len(w) == len(self.A_good))\n", "label": 0}
{"function": "\n\ndef directiveOrStatement(self, argstr, h):\n    i = self.skipSpace(argstr, h)\n    if (i < 0):\n        return i\n    j = self.graph(argstr, i)\n    if (j >= 0):\n        return j\n    j = self.sparqlDirective(argstr, i)\n    if (j >= 0):\n        return j\n    j = self.directive(argstr, i)\n    if (j >= 0):\n        return self.checkDot(argstr, j)\n    j = self.statement(argstr, i)\n    if (j >= 0):\n        return self.checkDot(argstr, j)\n    return j\n", "label": 0}
{"function": "\n\ndef activate(self, X):\n    if self.is_elementwise():\n        raise Exception('No identity nodes allowed.')\n    return X\n", "label": 0}
{"function": "\n\ndef test_map(self, space):\n    w_res = space.execute('return [1, 2, 3, 4, 5].map { |i| i + 1 }')\n    assert (self.unwrap(space, w_res) == range(2, 7))\n    w_res = space.execute('return [1, 2, 3, 4, 5].collect { |i| i + 1 }')\n    assert (self.unwrap(space, w_res) == range(2, 7))\n", "label": 0}
{"function": "\n\n@pytest.mark.unit\ndef test_client_static_ip_custom_port():\n    from bulby.client import HueBridgeClient\n    with mock.patch('bulby.client.HueBridgeClient.connect') as connect:\n        client = HueBridgeClient(ip_address='192.168.1.1', port=1337)\n    assert (client.ip_address == '192.168.1.1')\n    assert (client.port == 1337)\n", "label": 0}
{"function": "\n\ndef test_waveform_amplitude(masks, waveforms):\n    waveforms *= 0.1\n    masks *= 0.1\n    waveforms[:, 10, :] *= 10\n    masks[:, 10] *= 10\n    mean_waveforms = mean(waveforms)\n    mean_masks = mean(masks)\n    amplitude = get_waveform_amplitude(mean_masks, mean_waveforms)\n    assert np.all((amplitude >= 0))\n    assert (amplitude.shape == (mean_waveforms.shape[1],))\n", "label": 0}
{"function": "\n\ndef encode_qs_params(self, request):\n    query = port.urlparse(request.uri).query\n    params = port.parse_qsl(query, True)\n    return tuple(((self.encode(key), self.encode(val)) for (key, val) in params))\n", "label": 0}
{"function": "\n\ndef test_find_alternative_namespace(self):\n    application = Application()\n    application.add(FooCommand())\n    application.add(Foo1Command())\n    application.add(Foo2Command())\n    application.add(Foo3Command())\n    try:\n        application.find('Unknown-namespace:Unknown-command')\n        self.fail('.find() raises an Exception if namespace does not exist')\n    except Exception as e:\n        self.assertRegex(str(e), 'There are no commands defined in the \"Unknown-namespace\" namespace.')\n    try:\n        application.find('foo2:command')\n        self.fail('.find() raises an tException if namespace does not exist')\n    except Exception as e:\n        self.assertRegex(str(e), 'There are no commands defined in the \"foo2\" namespace.')\n        self.assertRegex(str(e), 'foo', msg='.find() raises an tException if namespace does not exist, with alternative \"foo\"')\n        self.assertRegex(str(e), 'foo1', msg='.find() raises an tException if namespace does not exist, with alternative \"foo1\"')\n        self.assertRegex(str(e), 'foo3', msg='.find() raises an Exception if namespace does not exist, with alternative \"foo2\"')\n", "label": 0}
{"function": "\n\ndef _get_value(self, entity):\n    if (entity._projection and (self._name in entity._projection)):\n        return super(ComputedProperty, self)._get_value(entity)\n    value = self._func(entity)\n    self._store_value(entity, value)\n    return value\n", "label": 0}
{"function": "\n\ndef utcoffset(self, dt):\n    if self._isdst(dt):\n        return DSTOFFSET\n    else:\n        return STDOFFSET\n", "label": 0}
{"function": "\n\ndef InjectScript(content, content_type, script_to_inject):\n    \"Inject |script_to_inject| into |content| if |content_type| is 'text/html'.\\n\\n  Inject |script_to_inject| into |content| immediately after <head>, <html> or\\n  <!doctype html>, if one of them is found. Otherwise, inject at the beginning.\\n\\n  Returns:\\n    content, already_injected\\n    |content| is the new content if script is injected, otherwise the original.\\n    |already_injected| indicates if |script_to_inject| is already in |content|.\\n  \"\n    already_injected = False\n    if (content_type and (content_type == 'text/html')):\n        already_injected = ((not content) or (script_to_inject in content))\n        if (not already_injected):\n\n            def InsertScriptAfter(matchobj):\n                return ('%s<script>%s</script>' % (matchobj.group(0), script_to_inject))\n            (content, is_injected) = HEAD_RE.subn(InsertScriptAfter, content, 1)\n            if (not is_injected):\n                (content, is_injected) = HTML_RE.subn(InsertScriptAfter, content, 1)\n            if (not is_injected):\n                (content, is_injected) = DOCTYPE_RE.subn(InsertScriptAfter, content, 1)\n            if (not is_injected):\n                content = ('<script>%s</script>%s' % (script_to_inject, content))\n                logging.warning('Inject at the very beginning, because no tag of <head>, <html> or <!doctype html> is found.')\n    return (content, already_injected)\n", "label": 0}
{"function": "\n\ndef _run_main(self, args, parsed_globals):\n    self.handle_args(args)\n    self.setup_services(parsed_globals)\n    self._call()\n    if ((self._invalid_digests > 0) or (self._invalid_logs > 0)):\n        return 1\n    return 0\n", "label": 0}
{"function": "\n\ndef _propsetcursory(self, value):\n    \"\\n        Set the cursor's y coordinate.\\n\\n        value - The new y coordinate. A negative value can be used to specify\\n        the y coordinate in terms of its relative distance to the bottom border\\n        of the surface. No operation will be performed if value is greater than\\n        or equal to the height of the surface.\\n        \"\n    y = int(value)\n    if ((y >= self._height) or (y <= (- self._height))):\n        return\n    if (y < 0):\n        y = (self._height + y)\n    self._cursory = y\n", "label": 0}
{"function": "\n\n@patch('paver.git.sh')\ndef test_branch_list_correctly_parses_git_output(sh):\n    output = git.branch_list(path='repo_path', __override__='\\n* git_support\\n  master\\n  virtualenv_in_folder\\n    ')\n    assert (output == ('git_support', ['git_support', 'master', 'virtualenv_in_folder']))\n", "label": 0}
{"function": "\n\ndef _Tuple(child, ctx):\n    if (len(child) > 1):\n        return Tuple(child, ctx)\n    else:\n        return child[0]\n", "label": 0}
{"function": "\n\ndef get_context_data(self, request):\n    job_binary_id = self.tab_group.kwargs['job_binary_id']\n    try:\n        job_binary = saharaclient.job_binary_get(request, job_binary_id)\n    except Exception:\n        job_binary = {\n            \n        }\n        exceptions.handle(request, _('Unable to fetch job binary.'))\n    return {\n        'job_binary': job_binary,\n    }\n", "label": 0}
{"function": "\n\ndef _load_aggregates(self, conn):\n    for (name, (klass, num_params)) in self._aggregates.items():\n\n        def make_aggregate():\n            instance = klass()\n            return (instance, instance.step, instance.finalize)\n        conn.createaggregatefunction(name, make_aggregate)\n", "label": 0}
{"function": "\n\n@classmethod\ndef validate(cls, mapper_spec):\n    'Validates mapper specification.\\n\\n    Args:\\n      mapper_spec: an instance of model.MapperSpec to validate.\\n    '\n    if (mapper_spec.output_writer_class() != cls):\n        raise errors.BadWriterParamsError('Output writer class mismatch')\n    output_sharding = cls._get_output_sharding(mapper_spec=mapper_spec)\n    if ((output_sharding != cls.OUTPUT_SHARDING_NONE) and (output_sharding != cls.OUTPUT_SHARDING_INPUT_SHARDS)):\n        raise errors.BadWriterParamsError(('Invalid output_sharding value: %s' % output_sharding))\n    params = _get_params(mapper_spec)\n    filesystem = cls._get_filesystem(mapper_spec)\n    if (filesystem not in files.FILESYSTEMS):\n        raise errors.BadWriterParamsError((\"Filesystem '%s' is not supported. Should be one of %s\" % (filesystem, files.FILESYSTEMS)))\n    if (filesystem == files.GS_FILESYSTEM):\n        if (not (cls.GS_BUCKET_NAME_PARAM in params)):\n            raise errors.BadWriterParamsError(('%s is required for Google store filesystem' % cls.GS_BUCKET_NAME_PARAM))\n    elif (params.get(cls.GS_BUCKET_NAME_PARAM) is not None):\n        raise errors.BadWriterParamsError(('%s can only be provided for Google store filesystem' % cls.GS_BUCKET_NAME_PARAM))\n", "label": 0}
{"function": "\n\ndef validate(self):\n    initial_validation = super(RegisterForm, self).validate()\n    if (not initial_validation):\n        return False\n    user = User.query.filter_by(email=self.email.data).first()\n    if user:\n        self.email.errors.append('Email already registered')\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef timestamp_from_string(self, timestamp):\n    'try with the user-defined timestamp then try the default timestamp.'\n    formats = (self.TIMESTAMP_FORMAT, self.DEFAULT_TIMESTAMP_FORMAT, self.OLD_DEFAULT_TIMESTAMP_FORMAT)\n    for format_string in formats:\n        try:\n            history_time = time.strptime(timestamp, format_string)\n        except ValueError:\n            pass\n        else:\n            return int(time.mktime(history_time))\n    self.debug(('The timestamp \"%s\" does not match any of the formats %s' % (timestamp, formats)))\n", "label": 0}
{"function": "\n\ndef add_examples(list_of_examples, path, example_type=None, skip=None):\n    example_path = join(example_dir, path)\n    if (skip is not None):\n        skip = set(skip)\n    for f in os.listdir(example_path):\n        flags = 0\n        if f.startswith(('_', '.')):\n            continue\n        elif f.endswith('.py'):\n            if (example_type is not None):\n                flags |= example_type\n            elif (('server' in f) or ('animate' in f)):\n                flags |= Flags.server\n            else:\n                flags |= Flags.file\n        elif f.endswith('.ipynb'):\n            flags |= Flags.notebook\n        else:\n            continue\n        if ('animate' in f):\n            flags |= Flags.animated\n            if (flags & Flags.file):\n                raise ValueError(\"file examples can't be animated\")\n        if (skip and (f in skip)):\n            flags |= Flags.skip\n        list_of_examples.append((join(example_path, f), flags))\n    return list_of_examples\n", "label": 1}
{"function": "\n\ndef _receive_data_frame(self, frame):\n    '\\n        Receive a data frame on the connection.\\n        '\n    flow_controlled_length = frame.flow_controlled_length\n    try:\n        window_size = self.remote_flow_control_window(frame.stream_id)\n    except NoSuchStreamError:\n        self.inbound_flow_control_window -= flow_controlled_length\n        raise\n    if (flow_controlled_length > window_size):\n        raise FlowControlError(('Cannot receive %d bytes, flow control window is %d.' % (flow_controlled_length, window_size)))\n    events = self.state_machine.process_input(ConnectionInputs.RECV_DATA)\n    self.inbound_flow_control_window -= flow_controlled_length\n    stream = self._get_stream_by_id(frame.stream_id)\n    (frames, stream_events) = stream.receive_data(frame.data, ('END_STREAM' in frame.flags), flow_controlled_length)\n    return (frames, (events + stream_events))\n", "label": 0}
{"function": "\n\ndef put(self, *args):\n    if ((not self.api_key.can_create_user()) and (self.key_username != args[0])):\n        raise HTTPError(403)\n    try:\n        (password, user_type, key, extra) = self.fetch_user(update=True)\n    except ValueError:\n        raise HTTPError(400)\n    try:\n        self.auth_mgr.update_user(args[0], password, user_type=user_type, key=key, extra=extra)\n    except UserConflict:\n        raise HTTPError(409)\n    self.write({\n        'ok': True,\n    })\n", "label": 0}
{"function": "\n\ndef _send_request(payload=None, session=None):\n    '\\n    Using requests we send a SOAP envelope directly to the\\n    vCenter API to reset an alarm to the green state.\\n\\n    :param payload:\\n    :param session:\\n    :return:\\n    '\n    stub = session\n    host_port = stub.host\n    url = 'https://{0}/sdk'.format(host_port)\n    logging.debug('Sending {0} to {1}'.format(payload, url))\n    res = requests.post(url=url, data=payload, headers={\n        'Cookie': stub.cookie,\n        'SOAPAction': 'urn:vim25',\n        'Content-Type': 'application/xml',\n    }, verify=False)\n    if (res.status_code != 200):\n        logging.debug('Failed to reset alarm. HTTP Status: {0}'.format(res.status_code))\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef _parse_sync_args(self, args):\n    parent = super(ContainerReplicatorRpc, self)\n    remote_info = parent._parse_sync_args(args)\n    if (len(args) > 9):\n        remote_info['status_changed_at'] = args[7]\n        remote_info['count'] = args[8]\n        remote_info['storage_policy_index'] = args[9]\n    return remote_info\n", "label": 0}
{"function": "\n\ndef __init__(self, name, size, bold=False, italic=False, dpi=None):\n    super(FreeTypeFont, self).__init__()\n    if (dpi is None):\n        dpi = 96\n    lname = ((name and name.lower()) or '')\n    if ((lname, bold, italic) in self._memory_fonts):\n        font = self._memory_fonts[(lname, bold, italic)]\n        self._set_face(font.face, size, dpi)\n        return\n    ft_library = ft_get_library()\n    match = self.get_fontconfig_match(name, size, bold, italic)\n    if (not match):\n        raise base.FontException(('Could not match font \"%s\"' % name))\n    f = FT_Face()\n    if (fontconfig.FcPatternGetFTFace(match, FC_FT_FACE, 0, byref(f)) != 0):\n        value = FcValue()\n        result = fontconfig.FcPatternGet(match, FC_FILE, 0, byref(value))\n        if (result != 0):\n            raise base.FontException(('No filename or FT face for \"%s\"' % name))\n        result = FT_New_Face(ft_library, value.u.s, 0, byref(f))\n        if result:\n            raise base.FontException(('Could not load \"%s\": %d' % (name, result)))\n    fontconfig.FcPatternDestroy(match)\n    self._set_face(f, size, dpi)\n", "label": 1}
{"function": "\n\ndef connect(self, initialize=True):\n    if initialize:\n        self.initialize()\n        self.add_source_files_to_schema()\n    return sqlite3.connect(self.app.config['SHELF_SQLITE_FILEPATH'])\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    if (type(other) == ChronykDelta):\n        newtimest = (self.timestamp() + other.seconds)\n        return Chronyk(newtimest, timezone=self.timezone)\n    if (type(other) in [int, float]):\n        newtimest = (self.timestamp() + other)\n        return Chronyk(newtimest, timezone=self.timezone)\n    return NotImplemented\n", "label": 0}
{"function": "\n\ndef getList(self):\n    'Background loop.'\n    url = 'http://xkcd.com/archive/'\n    try:\n        xml_str = str(urllib.request.urlopen(url).read()).split('(Hover mouse over title to view publication date)<br /><br />', 1)[(- 1)].split('</div>\\\\n<div id=\"bottom\" class=\"box\">', 1)[0].strip()\n    except urllib.error.URLError as e:\n        print(('Xkcd: %s: URL error %s reading list' % (__name__, str(e.reason))))\n    clean_xml_str = xml_str.split('<br/>')\n    self.menu_list = []\n    for line in clean_xml_str:\n        if (line is not None):\n            line_id = line.split('\"/', 1)[(- 1)].split('/\"', 1)[0]\n            line_date = line.split('=\"', 2)[(- 1)].split('\">', 1)[0]\n            line_title = line.split('\">', 1)[(- 1)].split('</', 1)[0]\n            self.menu_list.append([line_title, line_id, line_date])\n    sublime.set_timeout_async(self.window.show_quick_panel(self.menu_list, self.on_chosen), 0)\n", "label": 0}
{"function": "\n\ndef download(url, filename):\n    if (not os.path.exists('data')):\n        os.makedirs('data')\n    out_file = os.path.join('data', filename)\n    if (not os.path.isfile(out_file)):\n        urllib.urlretrieve(url, out_file)\n", "label": 0}
{"function": "\n\ndef process_payload_files(payload, github_auth):\n    ' Return a dictionary of file paths to raw JSON contents and file IDs.\\n    '\n    if (('action' in payload) and ('pull_request' in payload)):\n        return process_pullrequest_payload_files(payload, github_auth)\n    if (('commits' in payload) and ('head_commit' in payload)):\n        return process_pushevent_payload_files(payload, github_auth)\n    raise ValueError('Unintelligible webhook payload')\n", "label": 0}
{"function": "\n\ndef __get__(self, instance, instance_type=None):\n    if (instance is not None):\n        return self.rel_model.select().where((self.field == getattr(instance, self.field.to_field.name)))\n    return self\n", "label": 0}
{"function": "\n\ndef _restart_monitoring(self):\n    self._update_mtime()\n    for e in self.editor.clones:\n        try:\n            w = e.modes.get(self.__class__)\n        except KeyError:\n            pass\n        else:\n            w._restart_monitoring()\n    self._timer.start()\n", "label": 0}
{"function": "\n\ndef bulk_download_url(self, **options):\n    url = self._bulk_download_path()\n    url = ((ApiConfig.api_base + '/') + url)\n    if ('params' not in options):\n        options['params'] = {\n            \n        }\n    if ApiConfig.api_key:\n        options['params']['api_key'] = ApiConfig.api_key\n    if ApiConfig.api_version:\n        options['params']['api_version'] = ApiConfig.api_version\n    if list(options.keys()):\n        url += ('?' + urlencode(options['params']))\n    return url\n", "label": 0}
{"function": "\n\n@classmethod\ndef by_foreign_id(cls, foreign_id):\n    if (foreign_id is None):\n        return\n    return cls.all().filter_by(foreign_id=foreign_id).first()\n", "label": 0}
{"function": "\n\ndef __read_datasource(jboss_config, name, profile=None):\n    operation = '/subsystem=datasources/data-source=\"{name}\":read-resource'.format(name=name)\n    if (profile is not None):\n        operation = ('/profile=\"{profile}\"'.format(profile=profile) + operation)\n    operation_result = __salt__['jboss7_cli.run_operation'](jboss_config, operation)\n    return operation_result\n", "label": 0}
{"function": "\n\n@mock.patch('traceback.format_exception', wraps=fake_format_exception)\ndef test_frame_stripping(self, mock_format_exception):\n    'On assertion error, testify strips head and tail frame which originate from testify.'\n    test_result = TestResult((lambda : 'wat'), runner_id='foo!')\n    test_result.start()\n    root_tb = tb = mock.Mock()\n    testify_frames = [True, True, False, True, False, True, True]\n    for testify_frame in testify_frames:\n        tb.tb_next = mock.Mock()\n        tb = tb.tb_next\n        f_globals = ({\n            '__testify': True,\n        } if testify_frame else {\n            \n        })\n        tb.configure_mock(**{\n            'tb_frame.f_globals': f_globals,\n        })\n    tb.tb_next = None\n    tb = root_tb.tb_next\n    test_result.end_in_failure((AssertionError, 'wat', tb))\n    formatted = test_result.format_exception_info()\n    assert_equal(formatted, 'Traceback: AssertionError\\n')\n    mock_format_exception.assert_called_with(AssertionError, 'wat', tb.tb_next.tb_next, 3)\n", "label": 0}
{"function": "\n\ndef _debugdirtyFn(self, x, y):\n    if self._screendirty[x][y]:\n        return 'D'\n    else:\n        return '.'\n", "label": 0}
{"function": "\n\ndef first(self):\n    res = self.execute()\n    res.fill_cache(1)\n    try:\n        return res._result_cache[0]\n    except IndexError:\n        pass\n", "label": 0}
{"function": "\n\ndef sanitize_ohlc(open_, high, low, close):\n    if (low > open_):\n        low = open_\n    if (low > close):\n        low = close\n    if (high < open_):\n        high = open_\n    if (high < close):\n        high = close\n    return (open_, high, low, close)\n", "label": 0}
{"function": "\n\ndef order_modified_iter(cursor, trim, sentinel):\n    \"\\n    Yields blocks of rows from a cursor. We use this iterator in the special\\n    case when extra output columns have been added to support ordering\\n    requirements. We must trim those extra columns before anything else can use\\n    the results, since they're only needed to make the SQL valid.\\n    \"\n    for rows in iter((lambda : cursor.fetchmany(GET_ITERATOR_CHUNK_SIZE)), sentinel):\n        (yield [r[:(- trim)] for r in rows])\n", "label": 0}
{"function": "\n\ndef test_push_status_message_expected_error(self):\n    status_message = 'This is a message'\n    try:\n        push_status_message(status_message, kind='error')\n        assert_true(False, 'push_status_message() should have generated a ValidationError exception.')\n    except ValidationError as e:\n        assert_equal(e.detail[0], status_message, 'push_status_message() should have passed along the message with the Exception.')\n    except RuntimeError:\n        assert_true(False, 'push_status_message() should have caught the runtime error and replaced it.')\n    except:\n        assert_true(False, 'Exception from push_status_message when called from the v2 API with type \"error\"')\n", "label": 0}
{"function": "\n\ndef test_args(self, *args, **kwargs):\n    args = [(arg if isinstance(arg, str) else arg.encode()) for arg in args]\n    return ('%s\\n%s' % (repr(tuple(args)), repr(kwargs)))\n", "label": 0}
{"function": "\n\ndef __init__(self, func, sig, identity=None, targetoptions={\n    \n}):\n    if (not targetoptions.pop('nopython', True)):\n        raise TypeError('nopython flag must be True')\n    if targetoptions:\n        opts = ', '.join([repr(k) for k in targetoptions.keys()])\n        fmt = 'The following target options are not supported: {0}'\n        raise TypeError(fmt.format(opts))\n    self.py_func = func\n    self.identity = parse_identity(identity)\n    self.signature = sig\n    (self.inputsig, self.outputsig) = parse_signature(self.signature)\n    assert (len(self.outputsig) == 1), 'only support 1 output'\n    self.kernelmap = OrderedDict()\n", "label": 0}
{"function": "\n\ndef groupsizes_to_partition(*gsizes):\n    '\\n    >>> from logpy.assoccomm import groupsizes_to_partition\\n    >>> groupsizes_to_partition(2, 3)\\n    [[0, 1], [2, 3, 4]]\\n    '\n    idx = 0\n    part = []\n    for gs in gsizes:\n        l = []\n        for i in range(gs):\n            l.append(idx)\n            idx += 1\n        part.append(l)\n    return part\n", "label": 0}
{"function": "\n\ndef WithArgs(self, **kwargs):\n    'Creates a new Measurement, see openhtf.PhaseInfo.WithArgs.'\n    new_meas = mutablerecords.CopyRecord(self)\n    if ('{' in new_meas.name):\n        formatter = (lambda x: (x.format(**kwargs) if x else x))\n    else:\n        formatter = (lambda x: ((x % kwargs) if x else x))\n    new_meas.name = formatter(self.name)\n    new_meas.docstring = formatter(self.docstring)\n    return new_meas\n", "label": 0}
{"function": "\n\ndef _replace_datalayout(llvmir):\n    '\\n    Find the line containing the datalayout and replace it\\n    '\n    lines = llvmir.splitlines()\n    for (i, ln) in enumerate(lines):\n        if ln.startswith('target datalayout'):\n            tmp = 'target datalayout = \"{0}\"'\n            lines[i] = tmp.format(default_data_layout)\n            break\n    return '\\n'.join(lines)\n", "label": 0}
{"function": "\n\ndef NamedTemporaryFile23(*args, **kwargs):\n    'Works exactly as a wrapper to tempfile.NamedTemporaryFile except that\\n       in python2.x, it excludes the \"encoding\" parameter when provided.'\n    if (sys.version_info[0] == 2):\n        kwargs.pop('encoding', None)\n    return NamedTemporaryFile(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _fill_argtypes(self):\n    '\\n        Get dtypes\\n        '\n    for (i, ary) in enumerate(self.arrays):\n        if (ary is not None):\n            self.argtypes[i] = ary.dtype\n", "label": 0}
{"function": "\n\ndef to_event(self, notification_body, condenser=None):\n    if (condenser is None):\n        condenser = DictionaryCondenser()\n    event_type = notification_body['event_type']\n    message_id = notification_body['message_id']\n    edef = None\n    for d in self.definitions:\n        if d.match_type(event_type):\n            edef = d\n            break\n    if (edef is None):\n        msg = ('Dropping Notification %(type)s (uuid:%(msgid)s)' % dict(type=event_type, msgid=message_id))\n        if self.catchall:\n            logger.error(msg)\n        else:\n            logger.debug(msg)\n        return None\n    return edef.to_event(notification_body, condenser)\n", "label": 0}
{"function": "\n\ndef get_output_shape_for(self, input_shape):\n    if (self._output_shape is None):\n        if (K._BACKEND == 'tensorflow'):\n            if (type(input_shape) is list):\n                xs = [K.placeholder(shape=shape) for shape in input_shape]\n                x = self.call(xs)\n            else:\n                x = K.placeholder(shape=input_shape)\n                x = self.call(x)\n            if (type(x) is list):\n                return [K.int_shape(x_elem) for x_elem in x]\n            else:\n                return K.int_shape(x)\n        return input_shape\n    elif (type(self._output_shape) in {tuple, list}):\n        nb_samples = (input_shape[0] if input_shape else None)\n        return ((nb_samples,) + tuple(self._output_shape))\n    else:\n        shape = self._output_shape(input_shape)\n        if (type(shape) not in {list, tuple}):\n            raise Exception('output_shape function must return a tuple')\n        return tuple(shape)\n", "label": 1}
{"function": "\n\ndef iter_dist_files(dist):\n    if dist.has_metadata('RECORD'):\n        for line in dist.get_metadata_lines('RECORD'):\n            line = line.split(',')[0]\n            if line.endswith('.pyc'):\n                continue\n            (yield os.path.normpath(os.path.join(dist.location, line)))\n    elif dist.has_metadata('installed-files.txt'):\n        for line in dist.get_metadata_lines('installed-files.txt'):\n            if line.endswith('.pyc'):\n                continue\n            (yield os.path.normpath(os.path.join(dist.location, dist.egg_info, line)))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef to_model(notify_api_object):\n    if notify_api_object.get('on-success', None):\n        on_success = NotificationsHelper._to_model_sub_schema(notify_api_object['on-success'])\n    else:\n        on_success = None\n    if notify_api_object.get('on-complete', None):\n        on_complete = NotificationsHelper._to_model_sub_schema(notify_api_object['on-complete'])\n    else:\n        on_complete = None\n    if notify_api_object.get('on-failure', None):\n        on_failure = NotificationsHelper._to_model_sub_schema(notify_api_object['on-failure'])\n    else:\n        on_failure = None\n    model = NotificationSchema(on_success=on_success, on_failure=on_failure, on_complete=on_complete)\n    return model\n", "label": 0}
{"function": "\n\ndef test_error():\n    proto = MessageProtocol()\n    proto.sendMessage = MagicMock()\n    expected_message = {\n        'type': 'error',\n        'description': 'BANG!',\n    }\n    proto.error('BANG!')\n    assert (1 == proto.sendMessage.call_count)\n    assert (expected_message == proto.sendMessage.call_args[0][0])\n", "label": 0}
{"function": "\n\ndef test_mflist():\n    ml = flopy.modflow.Modflow(model_ws='temp')\n    dis = flopy.modflow.ModflowDis(ml, 10, 10, 10, 10)\n    sp_data = {\n        0: [[1, 1, 1, 1.0], [1, 1, 2, 2.0], [1, 1, 3, 3.0]],\n        1: [1, 2, 4, 4.0],\n    }\n    wel = flopy.modflow.ModflowWel(ml, stress_period_data=sp_data)\n    m4ds = ml.wel.stress_period_data.masked_4D_arrays\n    sp_data = flopy.utils.MfList.masked4D_arrays_to_stress_period_data(flopy.modflow.ModflowWel.get_default_dtype(), m4ds)\n    assert np.array_equal(sp_data[0], ml.wel.stress_period_data[0])\n    assert np.array_equal(sp_data[1], ml.wel.stress_period_data[1])\n    assert np.array_equal(sp_data[9], ml.wel.stress_period_data[1])\n    pth = os.path.join('..', 'examples', 'data', 'mf2005_test')\n    ml = flopy.modflow.Modflow.load(os.path.join(pth, 'swi2ex4sww.nam'), verbose=True)\n    m4ds = ml.wel.stress_period_data.masked_4D_arrays\n    sp_data = flopy.utils.MfList.masked4D_arrays_to_stress_period_data(flopy.modflow.ModflowWel.get_default_dtype(), m4ds)\n    wel = flopy.modflow.ModflowWel(ml, stress_period_data=sp_data)\n    flx1 = m4ds['flux']\n    flx2 = wel.stress_period_data.masked_4D_arrays['flux']\n    flx1 = np.nan_to_num(flx1)\n    flx2 = np.nan_to_num(flx2)\n    assert (flx1.sum() == flx2.sum())\n", "label": 0}
{"function": "\n\ndef delete_association(self, association=''):\n    '\\n        Delete an association between two IonObjects\\n        @param association  Association object, association id or 3-list of [subject, predicate, object]\\n        '\n    if ((type(association) in (list, tuple)) and (len(association) == 3)):\n        (subject, predicate, obj) = association\n        assoc_id_list = self.find_associations(subject=subject, predicate=predicate, object=obj, id_only=True)\n        success = True\n        for aid in assoc_id_list:\n            success = (success and self.rr_store.delete(aid, object_type='Association'))\n        return success\n    else:\n        return self.rr_store.delete(association, object_type='Association')\n", "label": 0}
{"function": "\n\ndef CalculateModelPredictions(self, inCoeffs, inDataCacheDictionary):\n    x_in = inDataCacheDictionary['X']\n    x_LogX = inDataCacheDictionary['LogX']\n    a = inCoeffs[0]\n    b = inCoeffs[1]\n    c = inCoeffs[2]\n    try:\n        temp = numpy.exp(((a + (b / x_in)) + (c * x_LogX)))\n        return self.extendedVersionHandler.GetAdditionalModelPredictions(temp, inCoeffs, inDataCacheDictionary, self)\n    except:\n        return (numpy.ones(len(inDataCacheDictionary['DependentData'])) * 1e+300)\n", "label": 0}
{"function": "\n\ndef _clean_join_name(opposite_side_colnames, suffix, c):\n    if (c.name not in opposite_side_colnames):\n        return c\n    else:\n        return c.label((c.name + suffix))\n", "label": 0}
{"function": "\n\ndef results(cl):\n    for res in cl.result_list:\n        (yield list(items_for_result(cl, res)))\n", "label": 0}
{"function": "\n\ndef get_URL(self, obj, logfile=None, cookiefile=None, **kwargs):\n    'Simple interface for making requests with uniform headers.\\n\\n        If you supply a string or URL object, it is taken as the URL to\\n        fetch. If you supply a list, multiple requests are issued. Returns\\n        two lists, as the perform() method does.\\n\\n        '\n    obj_t = type(obj)\n    if issubclass(obj_t, (str, urlparse.UniversalResourceLocator)):\n        r = HTTPRequest(obj, **kwargs)\n        resp = r.perform(logfile, cookiefile)\n        if resp.error:\n            return ([], [resp])\n        else:\n            return ([resp], [])\n    elif issubclass(obj_t, HTTPRequest):\n        resp = obj.perform(logfile)\n        if resp.error:\n            return ([], [resp])\n        else:\n            return ([resp], [])\n    else:\n        for url in iter(obj):\n            r = HTTPRequest(url, **kwargs)\n            self._requests.append(r)\n            return self.perform(logfile)\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    super(S3ObjectStoreTestCase, self).setUp()\n    options.options.domain = 'goviewfinder.com'\n    secrets.InitSecretsForTest()\n    self.object_store = S3ObjectStore('test-goviewfinder-com')\n    self.key = ('test/hello%d' % random.randint(1, 1000000))\n    self.listkey = ('test/list%d' % random.randint(1, 1000000))\n    self.listkeyA = '/'.join((self.listkey, 'a'))\n    self.listkeyB = '/'.join((self.listkey, 'b'))\n    self.listitems = [('item%d' % i) for i in range(0, 5)]\n    self.meter = counters.Meter(counters.counters.viewfinder.s3)\n    self.meter_start = time.time()\n", "label": 0}
{"function": "\n\ndef is_verb(self, word):\n    for n in self.verbs:\n        if word.startswith(n):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef streamStarted(self, rootElement):\n    '\\n        Called by the stream when it has started.\\n\\n        This examines the default namespace of the incoming stream and whether\\n        there is a requested hostname for the component. Then it generates a\\n        stream identifier, sends a response header and adds an observer for\\n        the first incoming element, triggering L{onElement}.\\n        '\n    xmlstream.ListenAuthenticator.streamStarted(self, rootElement)\n    if (rootElement.defaultUri != self.namespace):\n        exc = error.StreamError('invalid-namespace')\n        self.xmlstream.sendStreamError(exc)\n        return\n    if (not self.xmlstream.thisEntity):\n        exc = error.StreamError('improper-addressing')\n        self.xmlstream.sendStreamError(exc)\n        return\n    self.xmlstream.sendHeader()\n    self.xmlstream.addOnetimeObserver('/*', self.onElement)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _build_initializer_body_with_properties(properties_to_set):\n    initializer_body = ''\n    initializer_body += '\\n'.join(['    project.set_property(\"{0}\", \"{1}\")'.format(k, v) for (k, v) in properties_to_set])\n    if (not initializer_body):\n        initializer_body += '    pass'\n    return initializer_body\n", "label": 0}
{"function": "\n\ndef setLineCap(self, lineCap):\n    if (lineCap.strip().lower == GWTCanvasConsts.BUTT):\n        self.context.lineCap = BUTT\n    else:\n        self.context.lineCap = lineCap\n", "label": 0}
{"function": "\n\ndef normalize(self):\n    if (not self.words):\n        return NullQuery\n    if (len(self.words) == 1):\n        return Term(self.fieldname, self.words[0])\n    return self.__class__(self.fieldname, [w for w in self.words if (w is not None)], slop=self.slop, boost=self.boost)\n", "label": 0}
{"function": "\n\ndef on_get(self, req, resp, subnet_id):\n    'Shows information for a specified subnet. (subnet-show)\\n\\n        @param req: Http Request body\\n        @param resp: Http Response body\\n        @param subnet_id: subnet id\\n        @return: Bad request if the id is not a valid integer\\n        '\n    client = req.env['sl_client']\n    tenant_id = req.env['auth']['tenant_id']\n    try:\n        subnet_id = int(subnet_id)\n    except Exception:\n        return error_handling.bad_request(resp, message='Malformed request body')\n    subnet = client['Network_Subnet'].getObject(id=subnet_id, mask=SUBNET_MASK)\n    resp.body = {\n        'subnet': format_subnetwork(subnet, tenant_id),\n    }\n    resp.status = 200\n", "label": 0}
{"function": "\n\ndef _do_layout_node(self, node, level, y):\n    if (self.hide_root and (node is self.root)):\n        level -= 1\n    else:\n        node.x = ((self.x + self.indent_start) + (level * self.indent_level))\n        node.top = y\n        if node.size_hint_x:\n            node.width = ((self.width - (node.x - self.x)) * node.size_hint_x)\n        y -= node.height\n        if (not node.is_open):\n            return y\n    for cnode in node.nodes:\n        y = self._do_layout_node(cnode, (level + 1), y)\n    return y\n", "label": 0}
{"function": "\n\ndef update(self, validate=False):\n    \"\\n        Update the DB instance's status information by making a call to fetch\\n        the current instance attributes from the service.\\n\\n        :type validate: bool\\n        :param validate: By default, if EC2 returns no data about the\\n                         instance the update method returns quietly.  If\\n                         the validate param is True, however, it will\\n                         raise a ValueError exception if no data is\\n                         returned from EC2.\\n        \"\n    rs = self.connection.get_all_dbinstances(self.id)\n    if (len(rs) > 0):\n        for i in rs:\n            if (i.id == self.id):\n                self.__dict__.update(i.__dict__)\n    elif validate:\n        raise ValueError(('%s is not a valid Instance ID' % self.id))\n    return self.status\n", "label": 0}
{"function": "\n\ndef __setitem__(self, column, value):\n    for i in xrange(len(self)):\n        if (self._keys[i] == column):\n            self._values[i] = value\n            return\n    self._keys.append(column)\n    self._values.append(value)\n    self._BuildIndex()\n", "label": 0}
{"function": "\n\ndef build_es_query(self, case_type=None, afilter=None, status=None):\n\n    def _domain_term():\n        return {\n            'term': {\n                'domain.exact': self.domain,\n            },\n        }\n    subterms = ([_domain_term(), afilter] if afilter else [_domain_term()])\n    if case_type:\n        subterms.append({\n            'term': {\n                'type.exact': case_type,\n            },\n        })\n    if status:\n        subterms.append({\n            'term': {\n                'closed': (status == 'closed'),\n            },\n        })\n    es_query = {\n        'query': {\n            'filtered': {\n                'query': {\n                    'match_all': {\n                        \n                    },\n                },\n                'filter': {\n                    'and': subterms,\n                },\n            },\n        },\n        'sort': self.get_sorting_block(),\n        'from': self.pagination.start,\n        'size': self.pagination.count,\n    }\n    return es_query\n", "label": 0}
{"function": "\n\ndef _build_dom(self, content, mode):\n    assert (mode in ('html', 'xml'))\n    if (mode == 'html'):\n        if (not hasattr(THREAD_STORAGE, 'html_parser')):\n            THREAD_STORAGE.html_parser = HTMLParser()\n        dom = parse(StringIO(content), parser=THREAD_STORAGE.html_parser)\n        return dom.getroot()\n    else:\n        if (not hasattr(THREAD_STORAGE, 'xml_parser')):\n            THREAD_STORAGE.xml_parser = XMLParser()\n        dom = parse(BytesIO(content), parser=THREAD_STORAGE.xml_parser)\n        return dom.getroot()\n", "label": 0}
{"function": "\n\ndef sneak(self, sneak=True):\n    self._entity_action((constants.ENTITY_ACTION_SNEAK if sneak else constants.ENTITY_ACTION_UNSNEAK))\n    self.sneaking = sneak\n", "label": 0}
{"function": "\n\ndef hashed_name(self, name, content=None):\n    parsed_name = urlsplit(unquote(name))\n    clean_name = parsed_name.path\n    if (content is None):\n        if (not self.exists(clean_name)):\n            raise ValueError((\"The file '%s' could not be found with %r.\" % (clean_name, self)))\n        try:\n            content = self.open(clean_name)\n        except IOError:\n            return name\n    (path, filename) = os.path.split(clean_name)\n    (root, ext) = os.path.splitext(filename)\n    md5 = md5_constructor()\n    for chunk in content.chunks():\n        md5.update(chunk)\n    md5sum = md5.hexdigest()[:12]\n    hashed_name = os.path.join(path, ('%s.%s%s' % (root, md5sum, ext)))\n    unparsed_name = list(parsed_name)\n    unparsed_name[2] = hashed_name\n    if (('?#' in name) and (not unparsed_name[3])):\n        unparsed_name[2] += '?'\n    return urlunsplit(unparsed_name)\n", "label": 0}
{"function": "\n\ndef test_singleStep(self, spinbox):\n    assert (spinbox.singleStep() == 1)\n    assert (spinbox.setSingleStep(10) == 10)\n    assert (spinbox.setSingleStep((- 10)) == 10)\n    with pytest.raises(TypeError) as excinfo:\n        spinbox.setSingleStep('')\n        spinbox.setSingleStep(0.1212)\n    assert ('int' in str(excinfo.value))\n    assert (spinbox.setSingleStep(0) == 0)\n", "label": 0}
{"function": "\n\ndef test_compact_shorter_path(self, monkeypatch):\n    monkeypatch.setattr(pip.req.req_uninstall, 'is_local', (lambda p: True))\n    monkeypatch.setattr('os.path.exists', (lambda p: True))\n    short_path = os.path.normcase(os.path.abspath(os.path.join(os.path.sep, 'path')))\n    ups = UninstallPathSet(dist=Mock())\n    ups.add(short_path)\n    ups.add(os.path.join(short_path, 'longer'))\n    assert (ups.compact(ups.paths) == set([short_path]))\n", "label": 0}
{"function": "\n\ndef test_nils_20Feb04(self):\n    n = 2\n    A = (random([n, n]) + (random([n, n]) * 1j))\n    X = zeros((n, n), 'D')\n    Ainv = inv(A)\n    R = (identity(n) + (identity(n) * 0j))\n    for i in arange(0, n):\n        r = R[:, i]\n        X[:, i] = solve(A, r)\n    assert_array_almost_equal(X, Ainv)\n", "label": 0}
{"function": "\n\n@task\ndef sync_project(project_pk):\n    'Syncronize all dependencies of a project.\\n\\n    This syncronizes all package versions and creates proper\\n    log entries on updates as well as starts the notification\\n    routing.\\n    '\n    project = Project.objects.get(pk=project_pk)\n    log_entries = []\n    for dependency in project.dependencies.all():\n        package = dependency.package\n        package.sync_versions()\n        versions = list(package.versions.values_list('version', flat=True))\n        if versions:\n            versions.sort(key=LooseVersion)\n            if (LooseVersion(dependency.version) >= LooseVersion(versions[(- 1)])):\n                ProjectDependency.objects.filter(pk=dependency.pk).update(update=None)\n                continue\n            pv = PackageVersion.objects.get(package=package, version=versions[(- 1)])\n            if (pv.pk == dependency.update_id):\n                continue\n            dependency.update = pv\n            dependency.save()\n            tz = timezone.utc\n            since = str(timezone.make_naive(pv.release_date, tz))\n            log_entries.append(Log(type='project_dependency', action='update_available', project=project, package=package, data={\n                'version': versions[(- 1)],\n                'since': since,\n            }))\n    if log_entries:\n        Log.objects.bulk_create(log_entries)\n        send_notifications(project, log_entries)\n", "label": 0}
{"function": "\n\ndef summariseList(lst):\n    '\\n        Takes a sorted list of numbers, and returns a summary.\\n        Eg. [1, 2, 3, 4, 9] -> [(1, 4), 9]\\n            [1, 2, 3, 7, 8, 9] -> [(1, 3), (7, 9)]\\n    '\n    if (len(lst) < 2):\n        return lst\n    ranges = []\n    start = 0\n    for i in range(1, len(lst)):\n        if ((lst[i] - lst[(i - 1)]) > 1):\n            if ((i - 1) == start):\n                start = i\n                ranges.append(lst[(i - 1)])\n            else:\n                ranges.append((lst[start], lst[(i - 1)]))\n                start = i\n    if (lst[start] == lst[i]):\n        ranges.append(lst[i])\n    else:\n        ranges.append((lst[start], lst[i]))\n    return ranges\n", "label": 0}
{"function": "\n\ndef set_etag_header(self):\n    \"Sets the response's Etag header using ``self.compute_etag()``.\\n\\n        Note: no header will be set if ``compute_etag()`` returns ``None``.\\n\\n        This method is called automatically when the request is finished.\\n        \"\n    etag = self.compute_etag()\n    if (etag is not None):\n        self.set_header('Etag', etag)\n", "label": 0}
{"function": "\n\ndef test_should_output_single_float_multinomial(self):\n    kl = qtu.KL_divergence(mcmext.p_MultinomialComponentModel, self.params_mult, self.weights_mult, self.M_c_mult, self.X_L_mult, self.X_D_mult)\n    assert isinstance(kl, float)\n    assert (kl >= 0.0)\n", "label": 0}
{"function": "\n\ndef getStND(self, x, mue=0.0, sig=1.0):\n    x = ((x - mue) / sig)\n    if (abs(x) >= 4.0):\n        return 1e-09\n    x = int((((x + 4.0) / 8.0) * 1000))\n    return (stND[x] / sig)\n", "label": 0}
{"function": "\n\ndef is_alpha_number(number):\n    'Checks if the number is a valid vanity (alpha) number such as 800\\n    MICROSOFT. A valid vanity number will start with at least 3 digits and\\n    will have three or more alpha characters. This does not do region-specific\\n    checks - to work out if this number is actually valid for a region, it\\n    should be parsed and methods such as is_possible_number_with_reason() and\\n    is_valid_number() should be used.\\n\\n    Arguments:\\n    number -- the number that needs to be checked\\n\\n    Returns True if the number is a valid vanity number\\n    '\n    if (not _is_viable_phone_number(number)):\n        return False\n    (extension, stripped_number) = _maybe_strip_extension(number)\n    return bool(fullmatch(_VALID_ALPHA_PHONE_PATTERN, stripped_number))\n", "label": 0}
{"function": "\n\n@pytest.fixture\ndef deep_stats(depth=sys.getrecursionlimit(), skip_if_no_recursion_error=True):\n\n    def x0(frames):\n        frames.append(sys._getframe())\n    locals_ = locals()\n    for x in range(1, depth):\n        code = dedent(('\\n        import sys\\n        def x%d(frames):\\n            frames.append(sys._getframe())\\n            x%d(frames)\\n        ' % (x, (x - 1))))\n        exec_(code, locals_)\n    f = locals_[('x%d' % (depth - 1))]\n    frames = []\n    try:\n        f(frames)\n    except RuntimeError:\n        pass\n    else:\n        if skip_if_no_recursion_error:\n            pytest.skip('Recursion limit not exceeded')\n    profiler = TracingProfiler()\n    profiler._profile(frames[(- 1)], 'call', None)\n    spin(0.5)\n    profiler._profile(frames[(- 1)], 'return', None)\n    (stats, __, __) = profiler.result()\n    return stats\n", "label": 0}
{"function": "\n\ndef test_page_paths_keys_exist_in_static(self):\n    static = self.site.static()[0]\n    for old_key in ('full', 'full-build'):\n        self._paths_key_exists(static, old_key)\n", "label": 0}
{"function": "\n\ndef __setattr__(self, key, value):\n    if key.startswith('_'):\n        object.__setattr__(self, key, value)\n    else:\n        self[key] = value\n", "label": 0}
{"function": "\n\ndef _key(self, obj):\n    memo_key = id(obj)\n    if (memo_key in self._repr_memo):\n        return self._repr_memo[memo_key]\n    result = self._format(obj)\n    self._repr_memo[memo_key] = result\n    return result\n", "label": 0}
{"function": "\n\ndef test_get_uninvoiced_events(self):\n    'Test that the events manager can find events that owe money'\n    uninvoiced_events = Event.objects.uninvoiced_events()\n    assert (len(uninvoiced_events) == self.num_uninvoiced_events)\n    assert any([x for x in uninvoiced_events if (not x.admin_fee)])\n", "label": 0}
{"function": "\n\ndef stop(self):\n    ' Stop recording S3Log messages (and return the messages) '\n    handler = self.handler\n    if (handler is not None):\n        logger = logging.getLogger(__name__)\n        logger.removeHandler(handler)\n        handler.close()\n        self.handler = None\n    strbuf = self.strbuf\n    if (strbuf is not None):\n        return strbuf.getvalue()\n    else:\n        return ''\n", "label": 0}
{"function": "\n\ndef __init__(self, host, username='guest', password='guest', vhost='/', port=5672, heartbeat=0, io_loop=None):\n    self.host = host\n    self.port = port\n    self.username = username\n    self.password = password\n    self.vhost = vhost\n    self.heartbeat = heartbeat\n    self.last_received_frame = None\n    self.frame_max = 0\n    self.io_loop = (io_loop or IOLoop.instance())\n    self.stream = None\n    self.channels = {\n        0: self,\n    }\n    self.last_channel_id = 0\n    self.channel_id = 0\n    self.on_connect = None\n    self.on_disconnect = None\n    self.on_error = None\n    self._close_callback = None\n    self._frame_count = 0\n    super(Connection, self).__init__(connection=self)\n", "label": 0}
{"function": "\n\ndef add_field_def(self, field_def):\n    self._check()\n    if (self._index != field_def.index):\n        raise ValueError('Invalid field index')\n    self._index += 1\n    self._field_defs.append(field_def)\n    return self\n", "label": 0}
{"function": "\n\ndef check_oob_score(name, X, y, n_estimators=20):\n    est = FOREST_ESTIMATORS[name](oob_score=True, random_state=0, n_estimators=n_estimators, bootstrap=True)\n    n_samples = X.shape[0]\n    est.fit(X[:(n_samples // 2), :], y[:(n_samples // 2)])\n    test_score = est.score(X[(n_samples // 2):, :], y[(n_samples // 2):])\n    if (name in FOREST_CLASSIFIERS):\n        assert_less(abs((test_score - est.oob_score_)), 0.1)\n    else:\n        assert_greater(test_score, est.oob_score_)\n        assert_greater(est.oob_score_, 0.8)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        est = FOREST_ESTIMATORS[name](oob_score=True, random_state=0, n_estimators=1, bootstrap=True)\n        assert_warns(UserWarning, est.fit, X, y)\n", "label": 0}
{"function": "\n\ndef init(self):\n    while (not self.wrapper.halt):\n        try:\n            self.log.info('Connecting to IRC...')\n            self.connect()\n            t = threading.Thread(target=self.queue, args=())\n            t.daemon = True\n            t.start()\n            self.handle()\n        except:\n            for line in traceback.format_exc().split('\\n'):\n                self.log.error(line)\n            self.disconnect('Error in Wrapper.py - restarting')\n        self.log.info('Disconnected from IRC')\n        time.sleep(5)\n", "label": 0}
{"function": "\n\ndef _format_call(translator, func, *args):\n    formatted_args = []\n    for arg in args:\n        fmt_arg = translator.translate(arg)\n        formatted_args.append(fmt_arg)\n    return '{0!s}({1!s})'.format(func, ', '.join(formatted_args))\n", "label": 0}
{"function": "\n\ndef test_distinct():\n    ((chunk, chunk_expr), (agg, agg_expr)) = split(t, count(t.amount.distinct()))\n    assert (chunk.schema == t.schema)\n    assert chunk_expr.isidentical(chunk.amount.distinct())\n    assert isscalar(agg.dshape.measure)\n    assert agg_expr.isidentical(count(agg.distinct()))\n", "label": 0}
{"function": "\n\ndef query(self, data=None):\n    if (len(self.job_queue) > 1000):\n        LOGGER.debug(('Skipping query. %s jobs already active.' % len(self.job_queue)))\n        return\n    if self.querying_for_jobs:\n        LOGGER.debug('Skipping query. Already querying for jobs.')\n        return\n    self.querying_for_jobs = True\n    if ((self.uuid_limits['start'] is None) and (self.uuid_limits['end'] is not None)):\n        uuid_limit_clause = (\"AND itemName() < '%s'\" % self.uuid_limits['end'])\n    elif ((self.uuid_limits['start'] is not None) and (self.uuid_limits['end'] is None)):\n        uuid_limit_clause = (\"AND itemName() > '%s'\" % self.uuid_limits['start'])\n    elif ((self.uuid_limits['start'] is None) and (self.uuid_limits['end'] is None)):\n        uuid_limit_clause = ''\n    else:\n        uuid_limit_clause = (\"AND itemName() BETWEEN '%s' AND '%s'\" % (self.uuid_limits['start'], self.uuid_limits['end']))\n    sql = (\"SELECT * \\n                FROM `%s` \\n                WHERE\\n                reservation_next_request < '%s' %s\\n                LIMIT 2500\" % (self.aws_sdb_reservation_domain, sdb_now(offset=self.time_offset), uuid_limit_clause))\n    sql = re.sub('\\\\s\\\\s*', ' ', sql)\n    self.current_sql = sql\n    LOGGER.debug(('Querying SimpleDB, \"%s\"' % sql))\n    d = self.sdb.select(sql, max_results=5000)\n    d.addCallback(self._queryCallback)\n    d.addErrback(self._queryErrback)\n", "label": 1}
{"function": "\n\ndef labelOrSubject(self, argstr, i, res):\n    j = self.skipSpace(argstr, i)\n    if (j < 0):\n        return j\n    i = j\n    j = self.uri_ref2(argstr, i, res)\n    if (j >= 0):\n        return j\n    if (argstr[i] == '['):\n        j = self.skipSpace(argstr, (i + 1))\n        if (j < 0):\n            self.BadSyntax(argstr, i, 'Expected ] got EOF')\n        if (argstr[j] == ']'):\n            res.append(self.blankNode())\n            return (j + 1)\n    return (- 1)\n", "label": 0}
{"function": "\n\ndef and_then(self, text):\n    index = self.testcase.asm[self.start:].find(text)\n    if (index > 0):\n        self.start += (index + len(text))\n        self.last = text\n    else:\n        print(self.testcase.asm)\n        raise AssertionError(('\"%s\" was not found after \"%s\" in code' % (text, self.last)))\n    return self\n", "label": 0}
{"function": "\n\ndef test_disabled_down_hosts_are_skipped(self):\n    active_host_names = ['active1', 'active2']\n    disabled_host_names = ['disabled1', 'disabled2', 'disabled3']\n    disabled_hosts = [self._host(name, enabled=False, up=False) for name in disabled_host_names]\n    active_hosts = [self._host(name) for name in active_host_names]\n    all_hosts = (active_hosts + disabled_hosts)\n    self.mock_client().services.list.return_value = all_hosts\n    self.mock_client().hosts.list.return_value = all_hosts\n    hosts = self.nova_client.get_compute_hosts()\n    self.assertIsNotNone(hosts)\n    self.assertTrue(isinstance(hosts, list))\n    for active in active_host_names:\n        self.assertIn(active, hosts)\n    for disabled in disabled_host_names:\n        self.assertNotIn(disabled, hosts)\n", "label": 0}
{"function": "\n\ndef find_kth(self, A, i, j, k):\n    p = self.pivot(A, i, j)\n    if (k == p):\n        return A[p]\n    elif (k > p):\n        return self.find_kth(A, (p + 1), j, k)\n    else:\n        return self.find_kth(A, i, p, k)\n", "label": 0}
{"function": "\n\ndef to_dot(self):\n    body = ''.join((c.to_dot() for c in self.columns))\n    return TABLE.format(self.name, self.header_dot, body)\n", "label": 0}
{"function": "\n\ndef crt(a_values, modulo_values):\n    'Chinese Remainder Theorem.\\n\\n    Calculates x such that x = a[i] (mod m[i]) for each i.\\n\\n    :param a_values: the a-values of the above equation\\n    :param modulo_values: the m-values of the above equation\\n    :returns: x such that x = a[i] (mod m[i]) for each i\\n    \\n\\n    >>> crt([2, 3], [3, 5])\\n    8\\n\\n    >>> crt([2, 3, 2], [3, 5, 7])\\n    23\\n\\n    >>> crt([2, 3, 0], [7, 11, 15])\\n    135\\n    '\n    m = 1\n    x = 0\n    for modulo in modulo_values:\n        m *= modulo\n    for (m_i, a_i) in zip(modulo_values, a_values):\n        M_i = (m // m_i)\n        inv = inverse(M_i, m_i)\n        x = ((x + ((a_i * M_i) * inv)) % m)\n    return x\n", "label": 0}
{"function": "\n\ndef beforeTest(self, test):\n    if (not hasattr(test.test, 'cls')):\n        return\n    plugin_base.before_test(test, test.test.cls.__module__, test.test.cls, test.test.method.__name__)\n", "label": 0}
{"function": "\n\n@presentation.render_for(Icon)\ndef render_Icon(self, h, comp, *args):\n    if (self.title is not None):\n        (h << h.i(class_=self.icon, title=self.title))\n        (h << self.title)\n    else:\n        (h << h.i(class_=self.icon, title=self.title))\n    return h.root\n", "label": 0}
{"function": "\n\ndef prepareShapeModel(datasetDirectory, shape):\n    import glob\n    datasetFiles_shape = glob.glob((((datasetDirectory + '/') + shape) + '.dat'))\n    if (len(datasetFiles_shape) < 1):\n        raise Exception(((('Dataset not available at ' + datasetDirectory) + ' for shape ') + shape))\n    shapeModeler = ShapeModeler(filename=datasetFiles_shape[0], num_principle_components=numParams)\n    return shapeModeler\n", "label": 0}
{"function": "\n\ndef export(self, lwrite, level, namespace_='DNSQueryObj:', name_='DNSResourceRecordsType', namespacedef_='', pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    showIndent(lwrite, level, pretty_print)\n    lwrite(('<%s%s%s' % (namespace_, name_, ((namespacedef_ and (' ' + namespacedef_)) or ''))))\n    already_processed = set()\n    self.exportAttributes(lwrite, level, already_processed, namespace_, name_='DNSResourceRecordsType')\n    if self.hasContent_():\n        lwrite(('>%s' % (eol_,)))\n        self.exportChildren(lwrite, (level + 1), namespace_, name_, pretty_print=pretty_print)\n        showIndent(lwrite, level, pretty_print)\n        lwrite(('</%s%s>%s' % (namespace_, name_, eol_)))\n    else:\n        lwrite(('/>%s' % (eol_,)))\n", "label": 0}
{"function": "\n\ndef test_IndexedBase_sugar():\n    (i, j) = symbols('i j', integer=True)\n    a = symbols('a')\n    A1 = Indexed(a, i, j)\n    A2 = IndexedBase(a)\n    assert (A1 == A2[(i, j)])\n    assert (A1 == A2[(i, j)])\n    assert (A1 == A2[[i, j]])\n    assert (A1 == A2[Tuple(i, j)])\n    assert all((a.is_Integer for a in A2[(1, 0)].args[1:]))\n", "label": 0}
{"function": "\n\ndef __call__(self, env, start_response):\n    if ('swift.trans_id' not in env):\n        raise Exception('Trans id should always be in env')\n    if self.error:\n        if (self.error == 'strange'):\n            raise StrangeException('whoa')\n        raise Exception('An error occurred')\n    if (self.body_iter is None):\n        return ['FAKE APP']\n    else:\n        return self.body_iter\n", "label": 0}
{"function": "\n\ndef _make_embedded_from(self, doc):\n    'Creates embedded navigators from a HAL response doc'\n    ld = utils.CurieDict(self._core.default_curie, {\n        \n    })\n    for (rel, doc) in doc.get('_embedded', {\n        \n    }).items():\n        if isinstance(doc, list):\n            ld[rel] = [self._recursively_embed(d) for d in doc]\n        else:\n            ld[rel] = self._recursively_embed(doc)\n    return ld\n", "label": 0}
{"function": "\n\ndef test_content_submission(reddit, terminal):\n    url = 'https://www.reddit.com/r/Python/comments/2xmo63/'\n    submission = reddit.get_submission(url)\n    content = SubmissionContent(submission, terminal.loader)\n    assert (len(content._comment_data) == 45)\n    assert (content.get((- 1))['type'] == 'Submission')\n    assert (content.get(40)['type'] == 'Comment')\n    for data in content.iterate((- 1), 1):\n        assert all(((k in data) for k in ('object', 'n_rows', 'offset', 'type', 'hidden')))\n        for val in data.values():\n            assert (not isinstance(val, six.binary_type))\n    with pytest.raises(IndexError):\n        content.get((- 2))\n    with pytest.raises(IndexError):\n        content.get(50)\n    content.toggle((- 1))\n    assert (len(content._comment_data) == 45)\n    content.toggle(2)\n    data = content.get(2)\n    assert (data['type'] == 'HiddenComment')\n    assert (data['count'] == 3)\n    assert (data['hidden'] is True)\n    assert (data['level'] >= content.get(3)['level'])\n    assert (len(content._comment_data) == 43)\n    content.toggle(2)\n    data = content.get(2)\n    assert (data['hidden'] is False)\n    assert (len(content._comment_data) == 45)\n", "label": 1}
{"function": "\n\ndef write_video_frame(self, img):\n    if ((img.shape[0] != self._output_size[1]) or (img.shape[1] != self._output_size[0])):\n        warn('Frame data is wrong size! Video will be corrupted.')\n    self._ffmpeg_pipe.write(img.tostring())\n", "label": 0}
{"function": "\n\n@classmethod\ndef make_static_url(cls, settings, path, include_version=True):\n    'Constructs a versioned url for the given path.\\n\\n        This method may be overridden in subclasses (but note that it\\n        is a class method rather than an instance method).  Subclasses\\n        are only required to implement the signature\\n        ``make_static_url(cls, settings, path)``; other keyword\\n        arguments may be passed through `~RequestHandler.static_url`\\n        but are not standard.\\n\\n        ``settings`` is the `Application.settings` dictionary.  ``path``\\n        is the static path being requested.  The url returned should be\\n        relative to the current host.\\n\\n        ``include_version`` determines whether the generated URL should\\n        include the query string containing the version hash of the\\n        file corresponding to the given ``path``.\\n\\n        '\n    url = (settings.get('static_url_prefix', '/static/') + path)\n    if (not include_version):\n        return url\n    version_hash = cls.get_version(settings, path)\n    if (not version_hash):\n        return url\n    return ('%s?v=%s' % (url, version_hash))\n", "label": 0}
{"function": "\n\ndef step(self, x):\n    '\\n        This routine applies the convergence acceleration to the individual terms.\\n\\n        A = sum(a_k, k = 0..infinity)\\n\\n        v, e = ...step(a_k)\\n\\n        output:\\n          v      current estimate of the series A\\n          e      an error estimate which is simply the difference between the current\\n                 estimate and the last estimate.\\n        '\n    if (self.variant != 'v'):\n        if (self.n == 0):\n            self.s = x\n            self.run(self.s, x)\n        else:\n            self.s += x\n            self.run(self.s, x)\n    else:\n        if isinstance(self.last_s, bool):\n            self.last_s = x\n            self.s = 0\n            self.last = 0\n            return (x, abs(x))\n        self.s += self.last_s\n        self.run(self.s, self.last_s, x)\n        self.last_s = x\n    value = (self.A[0] / self.B[0])\n    err = abs((value - self.last))\n    self.last = value\n    return (value, err)\n", "label": 0}
{"function": "\n\ndef handleTimeout(self, watcher):\n    if (self._new_modify and (not self._refreshed)):\n        sublime.set_timeout(functools.partial(self.handleTimeout, watcher), 1000)\n        self._new_modify = False\n    else:\n        if self._refreshed:\n            return\n        self._refreshed = True\n        watcher.refresh()\n", "label": 0}
{"function": "\n\ndef _convert_scalar_indexer(self, key, kind=None):\n    \"\\n        convert a scalar indexer\\n\\n        Parameters\\n        ----------\\n        key : label of the slice bound\\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\\n        \"\n    assert (kind in ['ix', 'loc', 'getitem', 'iloc', None])\n    if (kind == 'iloc'):\n        return self._validate_indexer('positional', key, kind)\n    if (len(self) and (not isinstance(self, ABCMultiIndex))):\n        if ((kind in ['getitem', 'ix']) and is_float(key)):\n            if (not self.is_floating()):\n                return self._invalid_indexer('label', key)\n        elif ((kind in ['loc']) and is_float(key)):\n            if (self.inferred_type not in ['floating', 'mixed-integer-float', 'string', 'unicode', 'mixed']):\n                return self._invalid_indexer('label', key)\n        elif ((kind in ['loc']) and is_integer(key)):\n            if (not self.holds_integer()):\n                return self._invalid_indexer('label', key)\n    return key\n", "label": 1}
{"function": "\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str(([c] + args))\n            p = subprocess.Popen(([c] + args), cwd=cwd, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if (e.errno == errno.ENOENT):\n                continue\n            if verbose:\n                print(('unable to run %s' % dispcmd))\n                print(e)\n            return None\n    else:\n        if verbose:\n            print(('unable to find command, tried %s' % (commands,)))\n        return None\n    stdout = p.communicate()[0].strip()\n    if (sys.version_info[0] >= 3):\n        stdout = stdout.decode()\n    if (p.returncode != 0):\n        if verbose:\n            print(('unable to run %s (error)' % dispcmd))\n        return None\n    return stdout\n", "label": 1}
{"function": "\n\ndef clean(self):\n    data = super(CreateImageForm, self).clean()\n    image_file = data.get('image_file', None)\n    if ((not data['copy_from']) and (not image_file)):\n        raise ValidationError(_('A image or external image location must be specified.'))\n    elif (data['copy_from'] and image_file):\n        raise ValidationError(_('Can not specify both image and external image location.'))\n    else:\n        return data\n", "label": 0}
{"function": "\n\ndef gen_url(scheme=None, subdomain=None, tlds=None):\n    \"Generates a random URL address\\n\\n    :param str scheme: Either http, https or ftp.\\n    :param str subdomain: A valid subdmain\\n    :param str tlds: A qualified top level domain name (e.g. 'com', 'net')\\n    :raises: ``ValueError`` if arguments are not valid.\\n    :returns: A random URL address.\\n    :rtype: str\\n\\n    \"\n    subdomainator = re.compile('^[a-zA-Z0-9][-\\\\w.~]*$')\n    schemenator = re.compile('^(https?|ftp)$')\n    tldsnator = re.compile('^[a-zA-Z]{1,3}$')\n    if scheme:\n        if (schemenator.match(scheme) is None):\n            raise ValueError('Protocol {0} is not valid.'.format(scheme))\n    else:\n        scheme = gen_choice(SCHEMES)\n    if subdomain:\n        if (subdomainator.match(subdomain) is None):\n            raise ValueError('Subdomain {0} is invalid.'.format(subdomain))\n    else:\n        subdomain = gen_choice(SUBDOMAINS)\n    if tlds:\n        if (tldsnator.match(tlds) is None):\n            raise ValueError('TLDS name {0} is invalid.'.format(tlds))\n    else:\n        tlds = gen_choice(TLDS)\n    url = '{0}://{1}.{2}'.format(scheme, subdomain, tlds)\n    return _make_unicode(url)\n", "label": 0}
{"function": "\n\n@nottest\ndef test_receipt(self, command):\n    'Test if the server send RECEIPT for a command'\n    m = Main()\n    m.tcp_connect()\n    m.send_connect({\n        \n    })\n    m.get_frame()\n    tmpId = str(uuid.uuid4())\n    m.send_frame(command, [('receipt', tmpId)], '')\n    (result, headers, body) = m.get_frame()\n    eq_(result, 'RECEIPT')\n    for (name, value) in headers:\n        if (name == 'receipt-id'):\n            eq_(value, tmpId)\n            return\n    raise Exception('No receipt-id')\n", "label": 0}
{"function": "\n\ndef _pop_styles(args):\n    styles = args.pop(STYLES_PARAM_NAME, [])\n    if isinstance(styles, str):\n        styles = styles.split(',')\n    styles = [S for S in styles if S.lower().endswith('.css')]\n    return styles\n", "label": 0}
{"function": "\n\ndef load(self):\n    self._common_kwargs = self._build_common_election_kwargs()\n    self._common_kwargs['reporting_level'] = 'precinct'\n    results = []\n    with self._file_handle as csvfile:\n        reader = unicodecsv.DictReader(csvfile, encoding='latin-1')\n        next(reader, None)\n        for row in reader:\n            if self._skip_row(row):\n                continue\n            if (row['votes'] == 'X'):\n                continue\n            rr_kwargs = self._common_kwargs.copy()\n            rr_kwargs['primary_party'] = row['party'].strip()\n            rr_kwargs.update(self._build_contest_kwargs(row))\n            rr_kwargs.update(self._build_candidate_kwargs(row))\n            jurisdiction = row['precinct'].strip()\n            county_ocd_id = [c for c in self.datasource._jurisdictions() if (c['county'].strip().upper() == row['county'].strip().upper())][0]['ocd_id']\n            rr_kwargs.update({\n                'party': row['party'].strip(),\n                'jurisdiction': jurisdiction,\n                'parent_jurisdiction': row['county'],\n                'ocd_id': '{}/precinct:{}'.format(county_ocd_id, ocd_type_id(jurisdiction)),\n                'office': row['office'].strip(),\n                'district': row['district'].strip(),\n                'votes': int(float(row['votes'])),\n            })\n            results.append(RawResult(**rr_kwargs))\n    RawResult.objects.insert(results)\n", "label": 0}
{"function": "\n\ndef _connect(self, database, **kwargs):\n    conn = apsw.Connection(database, **kwargs)\n    if (self.timeout is not None):\n        conn.setbusytimeout(self.timeout)\n    try:\n        self._add_conn_hooks(conn)\n    except:\n        conn.close()\n        raise\n    return conn\n", "label": 0}
{"function": "\n\ndef getNetworkKey(self, name):\n    for netkey in self.networks:\n        if (netkey.name == name):\n            return netkey\n    raise NodeError('Could not find network key with the supplied name.')\n", "label": 0}
{"function": "\n\ndef gplvm_simulation(optimize=True, verbose=1, plot=True, plot_sim=False, max_iters=20000.0):\n    from GPy import kern\n    from GPy.models import GPLVM\n    (D1, D2, D3, N, num_inducing, Q) = (13, 5, 8, 45, 3, 9)\n    (_, _, Ylist) = _simulate_matern(D1, D2, D3, N, num_inducing, plot_sim)\n    Y = Ylist[0]\n    k = kern.Linear(Q, ARD=True)\n    m = GPLVM(Y, Q, init='PCA', kernel=k)\n    m.likelihood.variance = 0.1\n    if optimize:\n        print('Optimizing model:')\n        m.optimize('bfgs', messages=verbose, max_iters=max_iters, gtol=0.05)\n    if plot:\n        m.X.plot('BGPLVM Latent Space 1D')\n        m.kern.plot_ARD('BGPLVM Simulation ARD Parameters')\n    return m\n", "label": 0}
{"function": "\n\ndef tearDownModule():\n    if (RUN_SEARCH and (not RUN_YZ)):\n        c = IntegrationTestBase.create_client()\n        b = c.bucket(testrun_search_bucket)\n        b.clear_properties()\n        c.close()\n", "label": 0}
{"function": "\n\ndef from_wire(cls, rdclass, rdtype, wire, current, rdlen, origin=None):\n    l = ord(wire[current])\n    current += 1\n    rdlen -= 1\n    if (l > rdlen):\n        raise dns.exception.FormError\n    latitude = wire[current:(current + l)]\n    current += l\n    rdlen -= l\n    l = ord(wire[current])\n    current += 1\n    rdlen -= 1\n    if (l > rdlen):\n        raise dns.exception.FormError\n    longitude = wire[current:(current + l)]\n    current += l\n    rdlen -= l\n    l = ord(wire[current])\n    current += 1\n    rdlen -= 1\n    if (l != rdlen):\n        raise dns.exception.FormError\n    altitude = wire[current:(current + l)]\n    return cls(rdclass, rdtype, latitude, longitude, altitude)\n", "label": 0}
{"function": "\n\ndef generate_generic_calls(base, ns):\n    methods = (m for m in METHODS if (m.split('.', 1)[0] == base))\n    for method in methods:\n        call_name = method.split('.', 1)[1]\n        if (call_name not in ns):\n            call_class = _get_call_class(method)\n            generic_call = GenericCall(call_class)\n            generic_call.__name__ = str(call_name)\n            generic_call.__doc__ = METHODS[method]\n            ns[call_name] = generic_call\n            if (('__all__' in ns) and (call_name not in ns['__all__'])):\n                ns['__all__'].append(call_name)\n        elif (not ns[call_name].__doc__):\n            ns[call_name].__doc__ = METHODS[method]\n", "label": 0}
{"function": "\n\ndef __init__(self, path, index_page='index.html', hide_index_with_redirect=False, **kw):\n    self.path = os.path.abspath(path)\n    if (not self.path.endswith(os.path.sep)):\n        self.path += os.path.sep\n    if (not os.path.isdir(self.path)):\n        raise IOError(('Path does not exist or is not directory: %r' % self.path))\n    self.index_page = index_page\n    self.hide_index_with_redirect = hide_index_with_redirect\n    self.fileapp_kw = kw\n", "label": 0}
{"function": "\n\ndef _end_creation(self, token):\n    'End object upload.\\n\\n    Args:\\n      token: upload token returned by post_start_creation.\\n\\n    Raises:\\n      ValueError: if token is invalid. Or file is corrupted during upload.\\n\\n    Save file content to blobstore. Save blobinfo and _AE_GCSFileInfo.\\n    '\n    gcs_file = _AE_GCSFileInfo_.get_by_key_name(token)\n    if (not gcs_file):\n        raise ValueError('Invalid token')\n    (error_msg, content) = self._get_content(gcs_file)\n    if error_msg:\n        raise ValueError(error_msg)\n    gcs_file.etag = hashlib.md5(content).hexdigest()\n    gcs_file.creation = datetime.datetime.utcnow()\n    gcs_file.size = len(content)\n    self.blob_storage.StoreBlob(token, StringIO.StringIO(content))\n    gcs_file.finalized = True\n    gcs_file.put()\n", "label": 0}
{"function": "\n\n@base.remotable\ndef destroy(self):\n    if (not self.obj_attr_is_set('id')):\n        raise exception.ObjectActionError(action='destroy', reason='already destroyed')\n    if (not self.obj_attr_is_set('uuid')):\n        raise exception.ObjectActionError(action='destroy', reason='no uuid')\n    if ((not self.obj_attr_is_set('host')) or (not self.host)):\n        constraint = db.constraint(host=db.equal_any(None))\n    else:\n        constraint = None\n    cell_type = cells_opts.get_cell_type()\n    if (cell_type is not None):\n        stale_instance = self.obj_clone()\n    try:\n        db_inst = db.instance_destroy(self._context, self.uuid, constraint=constraint)\n        self._from_db_object(self._context, self, db_inst)\n    except exception.ConstraintNotMet:\n        raise exception.ObjectActionError(action='destroy', reason='host changed')\n    if (cell_type == 'compute'):\n        cells_api = cells_rpcapi.CellsAPI()\n        cells_api.instance_destroy_at_top(self._context, stale_instance)\n    delattr(self, base.get_attrname('id'))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef get_pl_tags(xml, ignore=[]):\n    pl_tags = []\n    for elem in xml.iterchildren():\n        if (elem.tag not in ignore):\n            pl_tag = PLTag({\n                'tagname': elem.tag,\n                'value': elem.text,\n            })\n            pl_tags.append(pl_tag)\n    return pl_tags\n", "label": 0}
{"function": "\n\ndef get_site_path(self):\n    target = self\n    npath = [self.name]\n    while True:\n        if (target.parent is None):\n            break\n        else:\n            npath.append(target.parent.name)\n            target = target.parent\n    return '.'.join(npath)\n", "label": 0}
{"function": "\n\ndef message_pump_greenthread(connection):\n    logging.debug('Starting message pump')\n    exit_code = 0\n    try:\n        while (connection is not None):\n            connection.read_frames()\n            gevent.sleep()\n    except Exception as exc:\n        logger.exception(exc)\n        exit_code = 1\n    finally:\n        logging.debug('Leaving message pump')\n    return exit_code\n", "label": 0}
{"function": "\n\ndef test_queueSendIntentIsSentOnFailureCallbackOfPending(self):\n    self.resetCounters()\n    self.test_sendIntentToTransportUpToLimitAndThenQueueInternally()\n    self.testTrans.forTestingCompleteAPendingIntent(SendStatus.Failed)\n    self.assertEqual(self.successCBcalls, 0)\n    self.assertEqual(self.failureCBcalls, 1)\n    self.assertEqual((MAX_PENDING_TRANSMITS + 1), len(self.testTrans.intents))\n    self.assertEqual(1, len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n", "label": 0}
{"function": "\n\ndef process_pushevent_payload_files(payload, github_auth):\n    ' Return a dictionary of files paths from a push event payload.\\n    \\n        https://developer.github.com/v3/activity/events/types/#pushevent\\n    '\n    files = dict()\n    touched = get_touched_payload_files(payload)\n    touched |= get_touched_branch_files(payload, github_auth)\n    commit_sha = payload['head_commit']['id']\n    for filename in touched:\n        if relpath(filename, 'sources').startswith('..'):\n            continue\n        if (splitext(filename)[1] != '.json'):\n            continue\n        contents_url = (payload['repository']['contents_url'] + '{?ref}')\n        contents_url = expand_uri(contents_url, dict(path=filename, ref=commit_sha))\n        current_app.logger.debug('Contents URL {}'.format(contents_url))\n        got = get(contents_url, auth=github_auth)\n        contents = got.json()\n        if (got.status_code not in range(200, 299)):\n            current_app.logger.warning('Skipping {} - {}'.format(filename, got.status_code))\n            continue\n        if (contents['encoding'] != 'base64'):\n            raise ValueError('Unrecognized encoding \"{encoding}\"'.format(**contents))\n        current_app.logger.debug('Contents SHA {sha}'.format(**contents))\n        files[filename] = (contents['content'], contents['sha'])\n    return files\n", "label": 0}
{"function": "\n\n@webapi_check_local_site\n@webapi_login_required\n@webapi_response_errors(INVALID_FORM_DATA, NOT_LOGGED_IN, PERMISSION_DENIED)\n@webapi_request_fields(required={\n    'name': {\n        'type': six.text_type,\n        'description': 'The name of the default reviewer entry.',\n    },\n    'file_regex': {\n        'type': six.text_type,\n        'description': 'The regular expression used to match file paths in newly uploaded diffs.',\n    },\n}, optional={\n    'repositories': {\n        'type': six.text_type,\n        'description': 'A comma-separated list of repository IDs.',\n    },\n    'groups': {\n        'type': six.text_type,\n        'description': 'A comma-separated list of group names.',\n    },\n    'users': {\n        'type': six.text_type,\n        'description': 'A comma-separated list of usernames.',\n    },\n})\ndef create(self, request, local_site=None, *args, **kwargs):\n    \"Creates a new default reviewer entry.\\n\\n        Note that by default, a default reviewer will apply to review\\n        requests on all repositories, unless one or more repositories are\\n        provided in the default reviewer's list.\\n        \"\n    if (not self.model.objects.can_create(request.user, local_site)):\n        return self.get_no_access_error(request)\n    (code, data) = self._create_or_update(local_site, **kwargs)\n    if (code == 200):\n        return (201, data)\n    else:\n        return (code, data)\n", "label": 0}
{"function": "\n\ndef to_event(self, notification_body, condenser):\n    event_type = notification_body['event_type']\n    message_id = notification_body['message_id']\n    when = self._extract_when(notification_body)\n    condenser.add_envelope_info(event_type, message_id, when)\n    for t in self.traits:\n        trait_info = self.traits[t].to_trait(notification_body)\n        if (trait_info is not None):\n            condenser.add_trait(*trait_info)\n    return condenser\n", "label": 0}
{"function": "\n\ndef __call__(self, exc):\n    if (self.timeout is None):\n        self.client.cond.wait()\n    else:\n        assert (self.timeout > 0)\n        start_time = get_time()\n        self.client.cond.wait(self.timeout)\n        self.timeout -= (get_time() - start_time)\n        if (self.timeout <= 0):\n            self.client.waiting_for_result.discard(self.req_id)\n            raise exc\n", "label": 0}
{"function": "\n\ndef install_os(path=None, **kwargs):\n    \"\\n    Installs the given image on the device. After the installation is complete the device is rebooted,\\n    if reboot=True is given as a keyworded argument.\\n\\n    Usage:\\n\\n    .. code-block:: bash\\n\\n        salt 'device_name' junos.install_os '/home/user/junos_image.tgz' reboot=True\\n\\n\\n    Options\\n      * path: Path where the image file is present.\\n      * kwargs: keyworded arguments to be given such as timeout, reboot etc\\n\\n    \"\n    conn = __proxy__['junos.conn']()\n    ret = dict()\n    ret['out'] = True\n    if ('timeout' in kwargs):\n        conn.timeout = kwargs['timeout']\n    try:\n        install = conn.sw.install(path, progress=True)\n        ret['message'] = 'Installed the os.'\n    except Exception as exception:\n        ret['message'] = 'Installation failed due to : \"{0}\"'.format(exception)\n        ret['out'] = False\n    if (('reboot' in kwargs) and (kwargs['reboot'] is True)):\n        rbt = conn.sw.reboot()\n        ret['message'] = 'Successfully installed and rebooted!'\n    return ret\n", "label": 0}
{"function": "\n\ndef apply_kwargs(func, **kwargs):\n    'Call *func* with kwargs, but only those kwargs that it accepts.\\n    '\n    new_kwargs = {\n        \n    }\n    params = signature(func).parameters\n    for param_name in params.keys():\n        if (param_name in kwargs):\n            new_kwargs[param_name] = kwargs[param_name]\n    return func(**new_kwargs)\n", "label": 0}
{"function": "\n\ndef testEntryTypeConversion(self):\n    for entry in self.feed.entry:\n        if (entry.id.text == 'http://picasaweb.google.com/data/feed/api/user/sample.user/albumid/'):\n            self.assert_(isinstance(entry, gdata.photos.PhotoEntry))\n", "label": 0}
{"function": "\n\ndef _estimate_cpu_utilization(self):\n    'Estimates the cpu utilization within the last 500ms'\n    now = time.time()\n    if ((now - self._last_cpu_check) >= 0.5):\n        try:\n            self._last_cpu_usage = psutil.cpu_percent()\n            self._last_cpu_check = now\n        except (psutil.NoSuchProcess, ZeroDivisionError):\n            pass\n    return self._last_cpu_usage\n", "label": 0}
{"function": "\n\ndef __init__(self, config):\n    if (not config['host']):\n        raise SystemExit('Error: IRC controller needs to be configured with a hostname')\n    if (not config['nick']):\n        raise SystemExit('Error: IRC controller needs to be configured with a nick')\n    print(('Connecting to %s:%s as %s' % (config['host'], config['port'], config['nick'])))\n    irc.bot.SingleServerIRCBot.__init__(self, [(config['host'], config['port'])], config['nick'], config['realname'])\n    channels = config['channels']\n    if channels:\n        self.channel = channels\n        print(('Joining Channels: %s' % channels))\n    self.config = config\n", "label": 0}
{"function": "\n\ndef sort(self, cmp=None, key=None, reverse=False):\n    'Sorts rows in the texttable.\\n\\n    Args:\\n      cmp: func, non default sort algorithm to use.\\n      key: func, applied to each element before sorting.\\n      reverse: bool, reverse order of sort.\\n    '\n\n    def _DefaultKey(value):\n        'Default key func is to create a list of all fields.'\n        result = []\n        for key in self.header:\n            try:\n                result.append(float(value[key]))\n            except ValueError:\n                result.append(value[key])\n        return result\n    key = (key or _DefaultKey)\n    new_table = self._table[1:]\n    new_table.sort(cmp, key, reverse)\n    self._table = [self.header]\n    self._table.extend(new_table)\n    for (index, row) in enumerate(self._table):\n        row.row = index\n", "label": 0}
{"function": "\n\ndef get_details(self, language=None):\n    '\\n        Retrieves full information on the place matching the place_id.\\n\\n        Stores the response in the `place` property.\\n        '\n    if (self._place is None):\n        if (language is None):\n            try:\n                language = self._query_instance._request_params['language']\n            except KeyError:\n                language = lang.ENGLISH\n        place = _get_place_details(self.place_id, self._query_instance.api_key, self._query_instance.sensor, language=language)\n        self._place = Place(self._query_instance, place)\n", "label": 0}
{"function": "\n\ndef test_list_float_complex(self):\n    x = ([np.random.rand() for i in range(5)] + [(np.random.rand() + (1j * np.random.rand())) for i in range(5)])\n    x_rec = self.encode_decode(x)\n    self.assertTrue(np.allclose(x, x_rec))\n", "label": 0}
{"function": "\n\ndef _try_retry(self, invocation):\n    if invocation.connection:\n        return False\n    if (invocation.timeout < time.time()):\n        return False\n    invoke_func = functools.partial(self.invoke, invocation)\n    self.logger.debug('Rescheduling request %s to be retried in %s seconds', invocation.request, RETRY_WAIT_TIME_IN_SECONDS)\n    self._client.reactor.add_timer(RETRY_WAIT_TIME_IN_SECONDS, invoke_func)\n    return True\n", "label": 0}
{"function": "\n\ndef __init__(self, session):\n    m = re.search('/(\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+):(\\\\d+)\\\\[(\\\\d+)\\\\]\\\\((.*)\\\\)', session)\n    if m:\n        self.host = m.group(1)\n        self.port = m.group(2)\n        self.interest_ops = m.group(3)\n        for d in m.group(4).split(','):\n            (k, v) = d.split('=')\n            self.__dict__[k] = v\n    else:\n        raise Session.BrokenLine()\n", "label": 0}
{"function": "\n\ndef rx_tx_dump(interface):\n    for line in open('/proc/net/dev'):\n        if (interface in line):\n            data = line.split(('%s:' % interface))[1].split()\n            (rx, tx) = ([int(x) for x in data[0:8]], [int(x) for x in data[8:]])\n    return (rx, tx)\n", "label": 0}
{"function": "\n\ndef complete(self, text, state):\n    if (state == 0):\n        self.matches = filter((lambda choice: choice.startswith(text)), self.choices)\n    if ((self.matches is not None) and (state < len(self.matches))):\n        return self.matches[state]\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef test_put(doc):\n    new = client.action(doc, ['nested', 'update'], params={\n        'foo': 789,\n    })\n    assert (new == {\n        'nested': {\n            'new': 123,\n            'foo': 789,\n        },\n    })\n    assert (new.title == 'original')\n", "label": 0}
{"function": "\n\ndef __match_string(self, subject):\n    reasons = []\n    for item in self._expected:\n        if (not (item in subject)):\n            return (False, ['item {0!r} not found'.format(item)])\n        else:\n            reasons.append('item {0!r} found'.format(item))\n    if (len(subject) != len(''.join(self._expected))):\n        return (False, ['have a different length'])\n    return (True, reasons)\n", "label": 0}
{"function": "\n\ndef next(self):\n    __traceback_supplement__ = (Supplement, self.error_middleware, self.environ)\n    if self.closed:\n        raise StopIteration\n    try:\n        return self.app_iterator.next()\n    except StopIteration:\n        self.closed = True\n        close_response = self._close()\n        if (close_response is not None):\n            return close_response\n        else:\n            raise StopIteration\n    except:\n        self.closed = True\n        close_response = self._close()\n        exc_info = sys.exc_info()\n        response = self.error_middleware.exception_handler(exc_info, self.environ)\n        if (close_response is not None):\n            response += ('<hr noshade>Error in .close():<br>%s' % close_response)\n        if (not self.start_checker.response_started):\n            self.start_checker('500 Internal Server Error', [('content-type', 'text/html')], exc_info)\n        if six.PY3:\n            response = response.encode('utf8')\n        return response\n", "label": 0}
{"function": "\n\ndef test_push_status_message_no_response(self):\n    status_message = 'This is a message'\n    statuses = ['info', 'warning', 'warn', 'success', 'danger', 'default']\n    for status in statuses:\n        try:\n            push_status_message(status_message, kind=status)\n        except:\n            assert_true(False, 'Exception from push_status_message via API v2 with type \"{}\".'.format(status))\n", "label": 0}
{"function": "\n\ndef collect_request_parameters(self, request):\n    'Collect parameters in an object for convenient access'\n\n    class OAuthParameters(object):\n        \"Used as a parameter container since plain object()s can't\"\n        pass\n    query = urlparse(request.url.decode('utf-8')).query\n    content_type = request.headers.get('Content-Type', '')\n    if request.form:\n        body = request.form.to_dict()\n    elif (content_type == 'application/x-www-form-urlencoded'):\n        body = request.data.decode('utf-8')\n    else:\n        body = ''\n    headers = dict(encode_params_utf8(request.headers.items()))\n    params = dict(collect_parameters(uri_query=query, body=body, headers=headers))\n    oauth_params = OAuthParameters()\n    oauth_params.client_key = params.get('oauth_consumer_key')\n    oauth_params.resource_owner_key = params.get('oauth_token', None)\n    oauth_params.nonce = params.get('oauth_nonce')\n    oauth_params.timestamp = params.get('oauth_timestamp')\n    oauth_params.verifier = params.get('oauth_verifier', None)\n    oauth_params.callback_uri = params.get('oauth_callback', None)\n    oauth_params.realm = params.get('realm', None)\n    return oauth_params\n", "label": 0}
{"function": "\n\ndef to_op(self):\n    '\\n        Extracts the modification operation from the set.\\n\\n        :rtype: dict, None\\n        '\n    if ((not self._adds) and (not self._removes)):\n        return None\n    changes = {\n        \n    }\n    if self._adds:\n        changes['adds'] = list(self._adds)\n    if self._removes:\n        changes['removes'] = list(self._removes)\n    return changes\n", "label": 0}
{"function": "\n\ndef _compute_memory_address(self, mem_operand):\n    'Return operand memory access translation.\\n        '\n    size = self._arch_info.architecture_size\n    addr = None\n    if mem_operand.base:\n        addr = ReilRegisterOperand(mem_operand.base, size)\n    if (mem_operand.index and (mem_operand.scale != 0)):\n        index = ReilRegisterOperand(mem_operand.index, size)\n        scale = ReilImmediateOperand(mem_operand.scale, size)\n        scaled_index = self.temporal(size)\n        self.add(self._builder.gen_mul(index, scale, scaled_index))\n        if addr:\n            tmp = self.temporal(size)\n            self.add(self._builder.gen_add(addr, scaled_index, tmp))\n            addr = tmp\n        else:\n            addr = scaled_index\n    if (mem_operand.displacement != 0):\n        disp = ReilImmediateOperand(mem_operand.displacement, size)\n        if addr:\n            tmp = self.temporal(size)\n            self.add(self._builder.gen_add(addr, disp, tmp))\n            addr = tmp\n        else:\n            addr = disp\n    elif (not addr):\n        disp = ReilImmediateOperand(mem_operand.displacement, size)\n        addr = disp\n    return addr\n", "label": 0}
{"function": "\n\ndef pom_check_if_child_exists(parent, prefix, values, comparison_tags):\n    for child in parent:\n        match = 0\n        for tag in comparison_tags:\n            key = child.find((prefix + tag))\n            if (key.text == values[tag]):\n                match += 1\n        if (match == len(comparison_tags)):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef install_code_colorscheme(self, name, style_dict):\n    '\\n        Install a new code color scheme.\\n        '\n    assert isinstance(name, six.text_type)\n    assert isinstance(style_dict, dict)\n    self.code_styles[name] = style_dict\n", "label": 0}
{"function": "\n\ndef __init__(self, children, context):\n    (_for, self.var, _in, self.expr, self.body) = children[:5]\n    self.else_ = None\n    assert (_for.value == 'for'), _for\n    assert (_in.value == 'in'), _in\n    if (len(children) > 5):\n        (_else, self.else_) = children[5:]\n        assert (_else.value == 'else'), _else\n    super().__init__(context)\n", "label": 0}
{"function": "\n\ndef number_of_comparisons_test(self):\n    ' Test BaseErrorMeasure.initialize for behaviour if not enough dates match.'\n    dataOrg = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9]]\n    dataCalc = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5.1, 5], [6.1, 6], [7.1, 7], [8.1, 8], [9.1, 9]]\n    tsOrg = TimeSeries.from_twodim_list(dataOrg)\n    tsCalc = TimeSeries.from_twodim_list(dataCalc)\n    bem = BaseErrorMeasure(60.0)\n    bem.local_error = (lambda a, b: 1)\n    mse = MeanSquaredError(80.0)\n    if mse.initialize(tsOrg, tsCalc):\n        assert False\n    if (not mse.initialize(tsOrg, tsOrg)):\n        assert False\n", "label": 0}
{"function": "\n\ndef new_relation_input(node, modelclass, metadata, order_by=None):\n    choices = models.get_choices(dbsession, modelclass, metadata.colname, order_by)\n    if (not choices):\n        return node.new_para(('%s has no choices.' % metadata.colname))\n    if metadata.nullable:\n        choices.insert(0, (0, '----'))\n    elid = ('id_' + metadata.colname)\n    node.add_label(metadata.colname, elid)\n    return node.add_select(choices, name=metadata.colname, multiple=metadata.uselist, id=elid)\n", "label": 0}
{"function": "\n\ndef finalize(self):\n    string = '</table>'\n    self.handle.write(string)\n    if self.html_footer:\n        self.handle.write(self.html_footer)\n    if self.close_file:\n        self.handle.close()\n", "label": 0}
{"function": "\n\ndef register_aliases(self, aliases):\n    'Registers the given aliases to be exposed in parsed BUILD files.\\n\\n    :param aliases: The BuildFileAliases to register.\\n    :type aliases: :class:`pants.build_graph.build_file_aliases.BuildFileAliases`\\n    '\n    if (not isinstance(aliases, BuildFileAliases)):\n        raise TypeError('The aliases must be a BuildFileAliases, given {}'.format(aliases))\n    for (alias, target_type) in aliases.target_types.items():\n        self._register_target_alias(alias, target_type)\n    for (alias, target_macro_factory) in aliases.target_macro_factories.items():\n        self._register_target_macro_factory_alias(alias, target_macro_factory)\n    for (alias, obj) in aliases.objects.items():\n        self._register_exposed_object(alias, obj)\n    for (alias, context_aware_object_factory) in aliases.context_aware_object_factories.items():\n        self._register_exposed_context_aware_object_factory(alias, context_aware_object_factory)\n", "label": 0}
{"function": "\n\ndef batch_write_item(self):\n    table_batches = self.body['RequestItems']\n    for (table_name, table_requests) in table_batches.items():\n        for table_request in table_requests:\n            request_type = list(table_request)[0]\n            request = list(table_request.values())[0]\n            if (request_type == 'PutRequest'):\n                item = request['Item']\n                dynamodb_backend.put_item(table_name, item)\n            elif (request_type == 'DeleteRequest'):\n                key = request['Key']\n                hash_key = key['HashKeyElement']\n                range_key = key.get('RangeKeyElement')\n                item = dynamodb_backend.delete_item(table_name, hash_key, range_key)\n    response = {\n        'Responses': {\n            'Thread': {\n                'ConsumedCapacityUnits': 1.0,\n            },\n            'Reply': {\n                'ConsumedCapacityUnits': 1.0,\n            },\n        },\n        'UnprocessedItems': {\n            \n        },\n    }\n    return dynamo_json_dump(response)\n", "label": 0}
{"function": "\n\ndef _create_context(self, request, *args, **kwargs):\n    request_id = kwargs.get('request_id')\n    if request_id:\n        silk_request = Request.objects.get(pk=request_id)\n    else:\n        silk_request = None\n    show = request.GET.get('show', self.default_show)\n    order_by = request.GET.get('order_by', self.defualt_order_by)\n    if show:\n        show = int(show)\n    func_name = request.GET.get('func_name', None)\n    name = request.GET.get('name', None)\n    filters = request.session.get(self.session_key_profile_filters, {\n        \n    })\n    context = {\n        'show': show,\n        'order_by': order_by,\n        'request': request,\n        'func_name': func_name,\n        'options_show': self.show,\n        'options_order_by': self.order_by,\n        'options_func_names': self._get_function_names(silk_request),\n        'options_names': self._get_names(silk_request),\n        'filters': filters,\n    }\n    context.update(csrf(request))\n    if silk_request:\n        context['silk_request'] = silk_request\n    if func_name:\n        context['func_name'] = func_name\n    if name:\n        context['name'] = name\n    objs = self._get_objects(show=show, order_by=order_by, func_name=func_name, silk_request=silk_request, name=name, filters=[BaseFilter.from_dict(x) for (_, x) in filters.items()])\n    context['results'] = objs\n    return context\n", "label": 0}
{"function": "\n\ndef set_flowcontrol(self, name, direction, value=None, default=False, disable=False):\n    \"Configures the interface flowcontrol value\\n\\n        Args:\\n            name (string): The interface identifier.  It must be a full\\n                interface name (ie Ethernet, not Et)\\n\\n            direction (string): one of either 'send' or 'receive'\\n\\n            value (boolean): True if the interface should enable flow control\\n                packet handling, otherwise False\\n\\n            default (boolean): Specifies to default the interface flow control\\n                send or receive value\\n\\n            disable (boolean): Specifies to disable the interface flow control\\n                send or receive value\\n\\n        Returns:\\n            True if the operation succeeds otherwise False is returned\\n        \"\n    if (value is not None):\n        if (value not in ['on', 'off']):\n            raise ValueError('invalid flowcontrol value')\n    if (direction not in ['send', 'receive']):\n        raise ValueError('invalid direction specified')\n    commands = [('interface %s' % name)]\n    commands.append(self.command_builder(('flowcontrol %s' % direction), value=value, default=default, disable=disable))\n    return self.configure(commands)\n", "label": 0}
{"function": "\n\ndef test_components_get_all(self):\n    (uuid, assembly_uuid, plan_uuid) = self._create_component()\n    (resp, body) = self.client.get('v1/components')\n    data = json.loads(body)\n    self.assertEqual(resp.status, 200)\n    filtered = [com for com in data if (com['uuid'] == uuid)]\n    self.assertEqual(len(filtered), 1)\n    self.assertEqual(filtered[0]['uuid'], uuid)\n    self._delete_component(uuid)\n", "label": 0}
{"function": "\n\ndef dictfetchall(cursor):\n    'Returns all rows from a cursor as a dict'\n    desc = cursor.description\n    return [dict(zip([col[0] for col in desc], row)) for row in cursor.fetchall()]\n", "label": 0}
{"function": "\n\ndef _extract_size(self, compute):\n    '\\n        Extract size, or node type, from a compute node XML representation.\\n\\n        Extract node size, or node type, description from a compute node XML\\n        representation, converting the node size to a NodeSize object.\\n\\n        :type  compute: :class:`ElementTree`\\n        :param compute: XML representation of a compute node.\\n\\n        :rtype:  :class:`OpenNebulaNodeSize`\\n        :return: Node type of compute node.\\n        '\n    instance_type = compute.find('INSTANCE_TYPE')\n    try:\n        return next((node_size for node_size in self.list_sizes() if (node_size.name == instance_type.text)))\n    except StopIteration:\n        return None\n", "label": 0}
{"function": "\n\n@classmethod\ndef parse(cls, api, json):\n    result = cls(api)\n    if (json is not None):\n        for (k, v) in json.items():\n            setattr(result, k, v)\n    return result\n", "label": 0}
{"function": "\n\ndef _get_m2m_db_table(self, opts):\n    'Function that can be curried to provide the m2m table name for this relation'\n    if (self.rel.through is not None):\n        return self.rel.through._meta.db_table\n    elif self.db_table:\n        return self.db_table\n    else:\n        return util.truncate_name(('%s_%s' % (opts.db_table, self.name)), connection.ops.max_name_length())\n", "label": 0}
{"function": "\n\ndef get(name, type=None):\n    'To get the value of a vim variable.'\n    rawval = vim.eval((prefix + name))\n    if (type is bool):\n        return (False if (rawval == '0') else True)\n    elif (type is int):\n        return int(rawval)\n    elif (type is float):\n        return float(rawval)\n    else:\n        return rawval\n", "label": 0}
{"function": "\n\n@dispatch(count, Select)\ndef compute_up(t, s, **kwargs):\n    if (t.axis != (0,)):\n        raise ValueError('axis not equal to 0 not defined for SQL reductions')\n    al = next(aliases)\n    try:\n        s2 = s.alias(al)\n        col = list(s2.primary_key)[0]\n    except (KeyError, IndexError):\n        s2 = s.alias(al)\n        col = list(s2.columns)[0]\n    result = sa.func.count(col)\n    return select([list(inner_columns(result))[0].label(t._name)])\n", "label": 0}
{"function": "\n\ndef import_coordinator_root(coordinator, coordinator_definition_root, metadata=None):\n    xslt_definition_fh = open(('%(xslt_dir)s/coordinator.xslt' % {\n        'xslt_dir': os.path.join(conf.DEFINITION_XSLT_DIR.get(), 'coordinators'),\n    }))\n    tag = etree.QName(coordinator_definition_root.tag)\n    schema_version = tag.namespace\n    if (schema_version not in OOZIE_NAMESPACES):\n        raise RuntimeError((_('Tag with namespace %(namespace)s is not valid. Please use one of the following namespaces: %(namespaces)s') % {\n            'namespace': coordinator_definition_root.tag,\n            'namespaces': ', '.join(OOZIE_NAMESPACES),\n        }))\n    xslt = etree.parse(xslt_definition_fh)\n    xslt_definition_fh.close()\n    transform = etree.XSLT(xslt)\n    transformed_root = transform(coordinator_definition_root)\n    objects = serializers.deserialize('xml', etree.tostring(transformed_root))\n    _set_coordinator_properties(coordinator, coordinator_definition_root, schema_version)\n    _set_controls(coordinator, coordinator_definition_root, schema_version)\n    _reconcile_datasets(coordinator, objects, coordinator_definition_root, schema_version)\n    _set_properties(coordinator, coordinator_definition_root, schema_version)\n    if metadata:\n        _process_metadata(coordinator, metadata)\n    coordinator.schema_version = schema_version\n    coordinator.save()\n", "label": 0}
{"function": "\n\ndef get_level(self, parent, block):\n    ' Get level of indent based on list level. '\n    m = self.INDENT_RE.match(block)\n    if m:\n        indent_level = (len(m.group(1)) / self.tab_length)\n    else:\n        indent_level = 0\n    if self.parser.state.isstate('list'):\n        level = 1\n    else:\n        level = 0\n    while (indent_level > level):\n        child = self.lastChild(parent)\n        if (child and ((child.tag in self.LIST_TYPES) or (child.tag in self.ITEM_TYPES))):\n            if (child.tag in self.LIST_TYPES):\n                level += 1\n            parent = child\n        else:\n            break\n    return (level, parent)\n", "label": 0}
{"function": "\n\ndef __init__(self, station, start_year=None, end_year=None, cache_directory=None, cache_filename=None):\n    super(NOAAWeatherSourceBase, self).__init__(station, cache_directory, cache_filename)\n    self._year_fetches_attempted = set()\n    self.station_id = station\n    if ((start_year is not None) and (end_year is not None)):\n        self.add_year_range(start_year, end_year)\n    elif (start_year is not None):\n        self.add_year_range(start_year, date.today().year)\n    elif (end_year is not None):\n        self.add_year_range(date.today().year, end_year)\n    self._check_for_recent_data()\n", "label": 0}
{"function": "\n\ndef convert_value(self, value):\n    if (not isinstance(value, (list, tuple))):\n        return self._convert_value_to_string(value)\n    values = []\n    for item in value:\n        values.append(self._convert_value_to_string(item))\n    return values\n", "label": 0}
{"function": "\n\n@presentation.render_for(Board)\ndef render_Board(self, h, comp, *args):\n    'Main board renderer'\n    security.check_permissions('view', self)\n    h.head.css_url('css/themes/board.css')\n    h.head.css_url(('css/themes/%s/board.css' % self.theme))\n    h.head.javascript_url('js/jquery-searchinput/jquery.searchinput.js')\n    h.head.javascript_url('js/debounce.js')\n    h.head.css_url('js/jquery-searchinput/styles/jquery.searchinput.min.css')\n    h.head.javascript('searchinput', \"jQuery(document).ready(function ($) { $('#search').searchInput(); });\")\n    title = ('%s - %s' % (self.get_title(), self.app_title))\n    (h.head << h.head.title(title))\n    if security.has_permissions('edit', self):\n        (h << comp.render(h, 'menu'))\n    (h << self.modal)\n    with h.div(class_='board'):\n        if self.background_image_url:\n            (h << {\n                'class': ('board ' + self.background_image_position),\n                'style': ('background-image:url(%s)' % self.background_image_url),\n            })\n        with h.div(class_='header'):\n            with h.div(class_='board-title', style=('color: %s' % self.title_color)):\n                (h << self.title.render(h.AsyncRenderer(), (0 if security.has_permissions('edit', self) else 'readonly')))\n            (h << comp.render(h, 'switch'))\n        with h.div(class_='bbody'):\n            (h << comp.render(h.AsyncRenderer(), self.model))\n    return h.root\n", "label": 0}
{"function": "\n\ndef splitLPOintervals(lpoList, ival, targetIval=None):\n    'return list of intervals split to different LPOs'\n    if (ival.start < 0):\n        start = (- ival.stop)\n        stop = (- ival.start)\n    else:\n        start = ival.start\n        stop = ival.stop\n    l = []\n    i = (len(lpoList) - 1)\n    while (i >= 0):\n        offset = lpoList[i].offset\n        if (offset < stop):\n            if (offset <= start):\n                if (ival.start < 0):\n                    myslice = slice((offset - stop), (offset - start))\n                else:\n                    myslice = slice((start - offset), (stop - offset))\n                if (targetIval is not None):\n                    l.append((lpoList[i], myslice, targetIval))\n                else:\n                    l.append((lpoList[i], myslice))\n                return l\n            else:\n                if (ival.start < 0):\n                    myslice = slice((offset - stop), 0)\n                else:\n                    myslice = slice(0, (stop - offset))\n                if (targetIval is not None):\n                    l.append((lpoList[i], myslice, targetIval[(offset - start):]))\n                    targetIval = targetIval[:(offset - start)]\n                else:\n                    l.append((lpoList[i], myslice))\n                stop = offset\n        i -= 1\n    raise ValueError('empty lpoList or offset not starting at 0?  Debug!')\n", "label": 1}
{"function": "\n\ndef handle_get_metainfo_meter(self, handle, connection, match, data, hdr):\n    try:\n        data = self.metering.get_actors_info(match.group(1))\n        status = calvinresponse.OK\n    except:\n        _log.exception('handle_get_metainfo_meter')\n        status = calvinresponse.NOT_FOUND\n    self.send_response(handle, connection, (json.dumps(data) if (status == calvinresponse.OK) else None), status=status)\n", "label": 0}
{"function": "\n\ndef __init__(self, children, context):\n    self.ifs = []\n    self.else_ = None\n    chit = iter(children)\n    for k in chit:\n        if (k.value in ('if', 'elif')):\n            self.ifs.append((next(chit), next(chit)))\n        else:\n            assert (k.value == 'else'), k\n            self.else_ = next(chit)\n    super().__init__(context)\n", "label": 0}
{"function": "\n\ndef get_item(self):\n    name = self.body['TableName']\n    key = self.body['Key']\n    hash_key = key['HashKeyElement']\n    range_key = key.get('RangeKeyElement')\n    attrs_to_get = self.body.get('AttributesToGet')\n    try:\n        item = dynamodb_backend.get_item(name, hash_key, range_key)\n    except ValueError:\n        er = 'com.amazon.coral.validate#ValidationException'\n        return self.error(er, status=400)\n    if item:\n        item_dict = item.describe_attrs(attrs_to_get)\n        item_dict['ConsumedCapacityUnits'] = 0.5\n        return dynamo_json_dump(item_dict)\n    else:\n        er = 'com.amazonaws.dynamodb.v20111205#ResourceNotFoundException'\n        return self.error(er, status=404)\n", "label": 0}
{"function": "\n\ndef raise_app(self, app):\n    app_zi = self.window_zindex[app.identifier]\n    for t in self.window_zindex.keys():\n        w = self.window[t]\n        zi = self.window_zindex[t]\n        if (zi > app_zi):\n            self.set_app_zindex(t, (zi - 1))\n    app_zi = (len(self.window) - 1)\n    self.set_app_zindex(app.identifier, app_zi)\n", "label": 0}
{"function": "\n\ndef clean(self, value):\n    super(ZAIDField, self).clean(value)\n    if (value in EMPTY_VALUES):\n        return ''\n    value = value.strip().replace(' ', '').replace('-', '')\n    match = re.match(id_re, value)\n    if (not match):\n        raise ValidationError(self.error_messages['invalid'])\n    g = match.groupdict()\n    try:\n        d = date((int(g['yy']) + 2000), int(g['mm']), int(g['dd']))\n    except ValueError:\n        raise ValidationError(self.error_messages['invalid'])\n    if (not luhn(value)):\n        raise ValidationError(self.error_messages['invalid'])\n    return value\n", "label": 0}
{"function": "\n\ndef _program_in_path(program):\n    path = os.environ.get('PATH', os.defpath).split(os.pathsep)\n    path = [os.path.join(dir, program) for dir in path]\n    path = [True for file in path if (os.path.isfile(file) or os.path.isfile((file + '.exe')))]\n    return bool(path)\n", "label": 0}
{"function": "\n\ndef call_method(self, ident, params):\n    if __debug__:\n        self.log.debug(('%s method ident=%r obj=%r method=%r req_id=%r' % (params['command'], ident, params['object'], params['method'], b2a_hex(params['req_id']))))\n    has_signal_attr = True\n    try:\n        objname = params['object']\n        try:\n            instance = self.instances[objname]\n        except KeyError:\n            raise NoInstanceError(objname)\n        try:\n            method = getattr(instance, params['method'])\n        except KeyError:\n            raise NoMethodError(params['method'])\n        has_signal_attr = hasattr(method, METHOD_RPC_AS_SIGNAL_ATTR)\n        if (((params['command'] == 'signal') and (not has_signal_attr)) or ((params['command'] == 'call') and has_signal_attr)):\n            warnings.warn(('wrong command match for %r' % method), SignalCallWarning)\n        ret = method(*params['args'], **params['kwargs'])\n        if (params['command'] == 'call'):\n            self.send_return(ident, params['req_id'], ret)\n    except Exception as exc:\n        if (self.transfer_exceptions and (not has_signal_attr)):\n            self.send_exception(ident, params['req_id'], exc)\n        else:\n            raise\n", "label": 1}
{"function": "\n\ndef get_snapshottable_plans(project):\n    project_plans = list(project.plans)\n    if (not project_plans):\n        return []\n    options = dict(db.session.query(ItemOption.item_id, ItemOption.value).filter(ItemOption.item_id.in_([p.id for p in project_plans]), (ItemOption.name == 'snapshot.allow')))\n    plan_list = []\n    for plan in project.plans:\n        if (plan.status != PlanStatus.active):\n            logging.info('Disallowing snapshot on plan [%s] due to status', plan.id)\n            continue\n        if (plan.snapshot_plan_id is not None):\n            logging.info('Disallowing snapshot on plan [%s] because it depends on another plan for its snapshot', plan.id)\n            continue\n        if (options.get(plan.id, '1') == '0'):\n            logging.info('Disallowing snapshot on plan [%s] due to snapshot.allow setting', plan.id)\n            continue\n        try:\n            if (not plan.steps[0].get_implementation().can_snapshot()):\n                logging.info('Disallowing snapshot on plan [%s] due to buildstep implementation', plan.id)\n                continue\n        except IndexError:\n            logging.info('Disallowing snapshot on plan [%s] due to invalid buildstep', plan.id)\n            continue\n        plan_list.append(plan)\n    return plan_list\n", "label": 1}
{"function": "\n\ndef get_batch_run_times(db, owner, repository):\n    \" Return dictionary of source paths to run time strings like '00:01:23'.\\n    \"\n    last_set = objects.read_latest_set(db, owner, repository)\n    return {run.source_path: run.state['process time'] for run in objects.read_completed_runs_to_date(db, last_set.id) if run.state}\n", "label": 0}
{"function": "\n\ndef ask_description(self, package, class_name):\n    status = ('Class \"%s\" will use \"%s\" package to organize import' % (class_name, package))\n    class_path = '.'.join([x for x in [package, class_name] if x])\n    if (not JavaUtils().is_class_path(class_path)):\n        status = 'Invalid package naming'\n    StatusManager().show_status(status, delay=(- 1), ref='ask_package_description')\n", "label": 0}
{"function": "\n\ndef get_metadata(self, container, prefix=None):\n    '\\n        Returns a dictionary containing the metadata for the container.\\n        '\n    headers = self.get_headers(container)\n    if (prefix is None):\n        prefix = CONTAINER_META_PREFIX\n    low_prefix = prefix.lower()\n    ret = {\n        \n    }\n    for (hkey, hval) in list(headers.items()):\n        if hkey.lower().startswith(low_prefix):\n            cleaned = hkey.replace(low_prefix, '').replace('-', '_')\n            ret[cleaned] = hval\n    return ret\n", "label": 0}
{"function": "\n\ndef GetEntityViaMemcache(entity_key):\n    'Get entity from memcache if available, from datastore if not.'\n    entity = memcache.get(entity_key)\n    if (entity is not None):\n        return entity\n    key = ndb.Key(urlsafe=entity_key)\n    entity = key.get()\n    if (entity is not None):\n        memcache.set(entity_key, entity)\n    return entity\n", "label": 0}
{"function": "\n\ndef test_diff2():\n    n3 = Rational(3)\n    n2 = Rational(2)\n    n6 = Rational(6)\n    (x, c) = map(Symbol, 'xc')\n    e = (((n3 * ((- n2) + (x ** n2))) * cos(x)) + ((x * ((- n6) + (x ** n2))) * sin(x)))\n    assert (e == (((3 * ((- 2) + (x ** 2))) * cos(x)) + ((x * ((- 6) + (x ** 2))) * sin(x))))\n    assert (e.diff(x).expand() == ((x ** 3) * cos(x)))\n    e = ((x + 1) ** 3)\n    assert (e.diff(x) == (3 * ((x + 1) ** 2)))\n    e = (x * ((x + 1) ** 3))\n    assert (e.diff(x) == (((x + 1) ** 3) + ((3 * x) * ((x + 1) ** 2))))\n    e = ((2 * exp((x * x))) * x)\n    assert (e.diff(x) == ((2 * exp((x ** 2))) + ((4 * (x ** 2)) * exp((x ** 2)))))\n", "label": 0}
{"function": "\n\ndef _EntriesGenerator(self):\n    'Retrieves directory entries.\\n\\n    Since a directory can contain a vast number of entries using\\n    a generator is more memory efficient.\\n\\n    Yields:\\n      A path specification (instance of path.FakePathSpec).\\n    '\n    location = getattr(self.path_spec, 'location', None)\n    if (location is None):\n        return\n    paths = self._file_system.GetPaths()\n    for path in iter(paths.keys()):\n        if ((not path) or (not path.startswith(location))):\n            continue\n        (_, suffix) = self._file_system.GetPathSegmentAndSuffix(location, path)\n        if (suffix or (path == location)):\n            continue\n        path_spec_location = self._file_system.JoinPath([path])\n        (yield fake_path_spec.FakePathSpec(location=path_spec_location))\n", "label": 0}
{"function": "\n\ndef keypress(self, size, key):\n    if (not self.app.input_buffer):\n        key = super(SideDiffCommentEdit, self).keypress(size, key)\n    keys = (self.app.input_buffer + [key])\n    commands = self.app.config.keymap.getCommands(keys)\n    if ((keymap.NEXT_SELECTABLE in commands) or (keymap.PREV_SELECTABLE in commands)):\n        if (((self.context.old_ln is not None) and (self.context.new_ln is not None)) or self.context.header):\n            if (self.focus_position == 3):\n                self.focus_position = 1\n            else:\n                self.focus_position = 3\n        return None\n    return key\n", "label": 0}
{"function": "\n\ndef python_value(self, value):\n    if isinstance(value, uuid.UUID):\n        return value\n    return (None if (value is None) else uuid.UUID(value))\n", "label": 0}
{"function": "\n\ndef _fetch_period(self, period):\n    years = []\n    if ((period.start is not None) and (period.end is not None)):\n        self.add_year_range(period.start.year, period.end.year)\n    elif (period.start is not None):\n        self.add_year(period.start.year)\n    elif (period.end is not None):\n        self.add_year(period.end.year)\n", "label": 0}
{"function": "\n\ndef safe_makedirs(*files):\n    for filename in files:\n        dirname = os.path.dirname(filename)\n        if (not os.path.isdir(dirname)):\n            os.makedirs(dirname)\n", "label": 0}
{"function": "\n\ndef print_provider(doc, provider, formatters, excludes=None, output=None):\n    output = (output or sys.stdout)\n    if (excludes is None):\n        excludes = []\n    print(file=output)\n    print('### {0}'.format(doc.get_provider_name(provider)), file=output)\n    print(file=output)\n    for (signature, example) in formatters.items():\n        if (signature in excludes):\n            continue\n        try:\n            lines = text_type(example).expandtabs().splitlines()\n        except UnicodeDecodeError:\n            lines = ['<bytes>']\n        except UnicodeEncodeError:\n            raise Exception('error on \"{0}\" with value \"{1}\"'.format(signature, example))\n        margin = max(30, (doc.max_name_len + 1))\n        remains = (150 - margin)\n        separator = '#'\n        for line in lines:\n            for i in range(0, ((len(line) // remains) + 1)):\n                print('\\t{fake:<{margin}}{separator} {example}'.format(fake=signature, separator=separator, example=line[(i * remains):((i + 1) * remains)], margin=margin), file=output)\n                signature = separator = ' '\n", "label": 1}
{"function": "\n\ndef _readinto_chunked(self, b):\n    assert (self.chunked != _UNKNOWN)\n    chunk_left = self.chunk_left\n    total_bytes = 0\n    mvb = memoryview(b)\n    while True:\n        if (chunk_left is None):\n            try:\n                chunk_left = self._read_next_chunk_size()\n                if (chunk_left == 0):\n                    break\n            except ValueError:\n                raise IncompleteRead(bytes(b[0:total_bytes]))\n        if (len(mvb) < chunk_left):\n            n = self._safe_readinto(mvb)\n            self.chunk_left = (chunk_left - n)\n            return (total_bytes + n)\n        elif (len(mvb) == chunk_left):\n            n = self._safe_readinto(mvb)\n            self._safe_read(2)\n            self.chunk_left = None\n            return (total_bytes + n)\n        else:\n            temp_mvb = mvb[0:chunk_left]\n            n = self._safe_readinto(temp_mvb)\n            mvb = mvb[n:]\n            total_bytes += n\n        self._safe_read(2)\n        chunk_left = None\n    self._read_and_discard_trailer()\n    self._close_conn()\n    return total_bytes\n", "label": 0}
{"function": "\n\ndef run_test_case(self, input_string, length_to_send, truncate_size=None):\n    input_buffer = cStringIO.StringIO()\n    input_buffer.write(struct.pack('!I', length_to_send))\n    input_buffer.write(input_string)\n    if (truncate_size is not None):\n        input_buffer.truncate(truncate_size)\n    num_bytes = input_buffer.tell()\n    input_buffer.seek(0)\n    result = Int32RequestParser(10).parse_request(input_buffer, num_bytes)\n    if (result is None):\n        self.assertEquals(0, input_buffer.tell())\n    else:\n        self.assertEquals((len(result) + struct.calcsize('!I')), input_buffer.tell())\n    return result\n", "label": 0}
{"function": "\n\ndef full_clean(self):\n    'Clean and validate all form fields.'\n    cache_type = (self['cache_type'].data or self['cache_type'].initial)\n    for (iter_cache_type, field) in six.iteritems(self.CACHE_LOCATION_FIELD_MAP):\n        self.fields[field].required = (cache_type == iter_cache_type)\n    return super(GeneralSettingsForm, self).full_clean()\n", "label": 0}
{"function": "\n\ndef choose_form_by_element(self, xpath):\n    elem = self.select(xpath).node()\n    while (elem is not None):\n        if (elem.tag == 'form'):\n            self._lxml_form = elem\n            return\n        else:\n            elem = elem.getparent()\n    self._lxml_form = None\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    ((a, b), (x, y)) = (self, other)\n    return ((a == x) and (b.dtype == y.dtype) and (type(b) == type(y)) and (b.shape == y.shape) and (abs((b - y)).sum() < (1e-06 * b.nnz)))\n", "label": 0}
{"function": "\n\ndef _safe_readinto(self, b):\n    'Same as _safe_read, but for reading into a buffer.'\n    total_bytes = 0\n    mvb = memoryview(b)\n    while (total_bytes < len(b)):\n        if (MAXAMOUNT < len(mvb)):\n            temp_mvb = mvb[0:MAXAMOUNT]\n            n = self.fp.readinto(temp_mvb)\n        else:\n            n = self.fp.readinto(mvb)\n        if (not n):\n            raise IncompleteRead(bytes(mvb[0:total_bytes]), len(b))\n        mvb = mvb[n:]\n        total_bytes += n\n    return total_bytes\n", "label": 0}
{"function": "\n\ndef multipliers(x):\n    'Convert values with si multipliers to numbers'\n    try:\n        return float(x)\n    except:\n        pass\n    try:\n        a = x[(- 1)]\n        y = float(x[:(- 1)])\n        endings = {\n            'G': 9,\n            'Meg': 6,\n            'k': 3,\n            'm': (- 3),\n            'u': (- 6),\n            'n': (- 9),\n            'p': (- 12),\n            's': 0,\n        }\n        return (y * (10 ** endings[a]))\n    except:\n        raise ValueError(\"I don't know what {} means\".format(x))\n", "label": 0}
{"function": "\n\ndef test_pex_bool_variables():\n    (Variables(environ={\n        \n    })._get_bool('NOT_HERE', default=False) is False)\n    (Variables(environ={\n        \n    })._get_bool('NOT_HERE', default=True) is True)\n    for value in ('0', 'faLsE', 'false'):\n        for default in (True, False):\n            (Variables(environ={\n                'HERE': value,\n            })._get_bool('HERE', default=default) is False)\n    for value in ('1', 'TrUe', 'true'):\n        for default in (True, False):\n            (Variables(environ={\n                'HERE': value,\n            })._get_bool('HERE', default=default) is True)\n    with pytest.raises(SystemExit):\n        Variables(environ={\n            'HERE': 'garbage',\n        })._get_bool('HERE')\n    assert (Variables().PEX_ALWAYS_CACHE is False)\n    assert (Variables({\n        'PEX_ALWAYS_CACHE': '1',\n    }).PEX_ALWAYS_CACHE is True)\n", "label": 0}
{"function": "\n\ndef setter(self, widget, value):\n    '\\n        Update the currently selected item to the one which stores value in\\n        its itemData\\n        '\n    try:\n        idx = _find_combo_data(widget, value)\n    except ValueError:\n        if (value is None):\n            idx = (- 1)\n        else:\n            raise ValueError(\"Cannot find data '{0}' in combo box\".format(value))\n    widget.setCurrentIndex(idx)\n", "label": 0}
{"function": "\n\ndef _add_vrfs(self, routing_settings):\n    'Add VRFs from given settings to BGPS runtime.\\n\\n        If any of the VRFs are miss-configured errors are logged.\\n        All valid VRFs are loaded.\\n        '\n    vpns_conf = routing_settings.setdefault('vpns', {\n        \n    })\n    for (vrfname, vrf) in vpns_conf.items():\n        try:\n            vrf[vrfs.VRF_NAME] = vrfname\n            call('vrf.create', **vrf)\n            LOG.debug('Added vrf  %s', vrf)\n        except RuntimeConfigError as e:\n            LOG.error(e)\n            continue\n", "label": 0}
{"function": "\n\ndef call_function(self, **parameters):\n    if (not self.interface.takes_kwargs):\n        parameters = {key: value for (key, value) in parameters.items() if (key in self.parameters)}\n    return self.interface(**parameters)\n", "label": 0}
{"function": "\n\ndef test_recursive_simple_and_not(self):\n    for k in range(2, 10):\n        G = self.worst_case_graph(k)\n        cc = sorted(nx.simple_cycles(G))\n        rcc = sorted(nx.recursive_simple_cycles(G))\n        assert_equal(len(cc), len(rcc))\n        for c in cc:\n            assert_true(any((self.is_cyclic_permutation(c, rc) for rc in rcc)))\n", "label": 0}
{"function": "\n\ndef _expected_cols(expected_attrs):\n    'Return expected_attrs that are columns needing joining.\\n\\n    NB: This function may modify expected_attrs if one\\n    requested attribute requires another.\\n    '\n    if (not expected_attrs):\n        return expected_attrs\n    simple_cols = [attr for attr in expected_attrs if (attr in _INSTANCE_OPTIONAL_JOINED_FIELDS)]\n    complex_cols = [('extra.%s' % field) for field in _INSTANCE_EXTRA_FIELDS if (field in expected_attrs)]\n    if complex_cols:\n        simple_cols.append('extra')\n    simple_cols = [x for x in simple_cols if (x not in _INSTANCE_EXTRA_FIELDS)]\n    return (simple_cols + complex_cols)\n", "label": 1}
{"function": "\n\ndef get_footer(email_account, footer=None):\n    'append a footer (signature)'\n    footer = (footer or '')\n    if (email_account and email_account.footer):\n        footer += '<div style=\"margin: 15px auto;\">{0}</div>'.format(email_account.footer)\n    footer += '<!--unsubscribe link here-->'\n    company_address = frappe.db.get_default('email_footer_address')\n    if company_address:\n        footer += '<div style=\"margin: 15px auto; text-align: center; color: #8d99a6\">{0}</div>'.format(company_address.replace('\\n', '<br>'))\n    if (not cint(frappe.db.get_default('disable_standard_email_footer'))):\n        for default_mail_footer in frappe.get_hooks('default_mail_footer'):\n            footer += '<div style=\"margin: 15px auto;\">{0}</div>'.format(default_mail_footer)\n    return footer\n", "label": 0}
{"function": "\n\ndef __call__(self, env, start_response):\n    '\\n        Main hook into the WSGI paste.deploy filter/app pipeline.\\n\\n        :param env: The WSGI environment dict.\\n        :param start_response: The WSGI start_response hook.\\n        '\n    env['staticweb.start_time'] = time.time()\n    if ('swift.authorize' not in env):\n        self.logger.warning('No authentication middleware authorized request yet. Skipping staticweb')\n        return self.app(env, start_response)\n    try:\n        (version, account, container, obj) = split_path(env['PATH_INFO'], 2, 4, True)\n    except ValueError:\n        return self.app(env, start_response)\n    if (env['REQUEST_METHOD'] not in ('HEAD', 'GET')):\n        return self.app(env, start_response)\n    if (env.get('REMOTE_USER') and (not config_true_value(env.get('HTTP_X_WEB_MODE', 'f')))):\n        return self.app(env, start_response)\n    if (not container):\n        return self.app(env, start_response)\n    context = _StaticWebContext(self, version, account, container, obj)\n    if obj:\n        return context.handle_object(env, start_response)\n    return context.handle_container(env, start_response)\n", "label": 0}
{"function": "\n\ndef ancestorOrSelf(domainMemberRelationshipSet, sourceConcept, result=None):\n    if (not result):\n        result = set()\n    if (not (sourceConcept in result)):\n        result.add(sourceConcept)\n        for rels in domainMemberRelationshipSet.toModelObject(sourceConcept):\n            ancestorOrSelf(domainMemberRelationshipSet, rels.fromModelObject, result)\n    return result\n", "label": 0}
{"function": "\n\ndef repo(args):\n    cd_conf = getattr(args, 'cd_conf', None)\n    for hostname in args.host:\n        LOG.debug('Detecting platform for host %s ...', hostname)\n        distro = hosts.get(hostname, username=args.username)\n        rlogger = logging.getLogger(hostname)\n        LOG.info('Distro info: %s %s %s', distro.name, distro.release, distro.codename)\n        if args.remove:\n            distro.packager.remove_repo(args.repo_name)\n        else:\n            install_repo(distro, args, cd_conf, rlogger)\n", "label": 0}
{"function": "\n\ndef get_actions(self, request):\n    actions = super(CommentsAdmin, self).get_actions(request)\n    if ((not request.user.is_superuser) and ('delete_selected' in actions)):\n        actions.pop('delete_selected')\n    if (not request.user.has_perm('comments.can_moderate')):\n        if ('approve_comments' in actions):\n            actions.pop('approve_comments')\n        if ('remove_comments' in actions):\n            actions.pop('remove_comments')\n    return actions\n", "label": 0}
{"function": "\n\ndef worst_case_graph(self, k):\n    G = nx.DiGraph()\n    for n in range(2, (k + 2)):\n        G.add_edge(1, n)\n        G.add_edge(n, (k + 2))\n    G.add_edge(((2 * k) + 1), 1)\n    for n in range((k + 2), ((2 * k) + 2)):\n        G.add_edge(n, ((2 * k) + 2))\n        G.add_edge(n, (n + 1))\n    G.add_edge(((2 * k) + 3), (k + 2))\n    for n in range(((2 * k) + 3), ((3 * k) + 3)):\n        G.add_edge(((2 * k) + 2), n)\n        G.add_edge(n, ((3 * k) + 3))\n    G.add_edge(((3 * k) + 3), ((2 * k) + 2))\n    return G\n", "label": 0}
{"function": "\n\ndef _send_fiff_command(self, command, data=None):\n    'Send a command through the data connection as a fiff tag\\n\\n        Parameters\\n        ----------\\n        command : int\\n            The command code.\\n\\n        data : str\\n            Additional data to send.\\n        '\n    kind = FIFF.FIFF_MNE_RT_COMMAND\n    type = FIFF.FIFFT_VOID\n    size = 4\n    if (data is not None):\n        size += len(data)\n    next = 0\n    msg = np.array(kind, dtype='>i4').tostring()\n    msg += np.array(type, dtype='>i4').tostring()\n    msg += np.array(size, dtype='>i4').tostring()\n    msg += np.array(next, dtype='>i4').tostring()\n    msg += np.array(command, dtype='>i4').tostring()\n    if (data is not None):\n        msg += np.array(data, dtype='>c').tostring()\n    self._data_sock.sendall(msg)\n", "label": 0}
{"function": "\n\ndef _get_pages(self):\n    if (self._pages is None):\n        hits = ((self.hits - 1) - self.orphans)\n        if (hits < 1):\n            hits = 0\n        self._pages = ((hits // self.num_per_page) + 1)\n    return self._pages\n", "label": 0}
{"function": "\n\ndef star_enable(cluster, logdir, cmdline, *args):\n    'Enable specific nodes by name, wildcard, network, or IP glob'\n    if (args and (args[0] != '*')):\n        cluster.enable(args)\n    else:\n        cluster.enable()\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    args = []\n    for arg in self.args:\n        try:\n            args.append(arg.resolve(context))\n        except VariableDoesNotExist:\n            args.append(None)\n    kwargs = {\n        \n    }\n    for (k, arg) in self.kwargs.items():\n        try:\n            kwargs[k] = arg.resolve(context)\n        except VariableDoesNotExist:\n            kwargs[k] = None\n    return self.render_with_args(context, *args, **kwargs)\n", "label": 0}
{"function": "\n\ndef Bind(self, name, flags=0, lHashVal=0):\n    'Bind to a name'\n    bindptr = BINDPTR()\n    desckind = DESCKIND()\n    ti = POINTER(ITypeInfo)()\n    self.__com_Bind(name, lHashVal, flags, byref(ti), byref(desckind), byref(bindptr))\n    kind = desckind.value\n    if (kind == DESCKIND_FUNCDESC):\n        fd = bindptr.lpfuncdesc[0]\n        fd.__ref__ = weakref.ref(fd, (lambda dead: ti.ReleaseFuncDesc(bindptr.lpfuncdesc)))\n        return ('function', fd)\n    elif (kind == DESCKIND_VARDESC):\n        vd = bindptr.lpvardesc[0]\n        vd.__ref__ = weakref.ref(vd, (lambda dead: ti.ReleaseVarDesc(bindptr.lpvardesc)))\n        return ('variable', vd)\n    elif (kind == DESCKIND_TYPECOMP):\n        return ('type', bindptr.lptcomp)\n    elif (kind == DESCKIND_IMPLICITAPPOBJ):\n        raise NotImplementedError\n    elif (kind == DESCKIND_NONE):\n        raise NameError(('Name %s not found' % name))\n", "label": 0}
{"function": "\n\ndef __buildKey(self, combination):\n    \"Computes the permutations' key based on the given combination\"\n    result = []\n    for key in sorted(combination):\n        value = combination[key]\n        if (value == True):\n            value = 'true'\n        elif (value == False):\n            value = 'false'\n        elif (value == None):\n            value = 'null'\n        result.append(('%s:%s' % (key, value)))\n    return ';'.join(result)\n", "label": 0}
{"function": "\n\ndef linklist(url):\n    'Return a JSON list of all on-domain links on the page.'\n    apexdom = apex(url)\n    import lxml.html\n    ro = requests.get(url, timeout=5)\n    doc = lxml.html.fromstring(ro.text)\n    doc.make_links_absolute(url)\n    somelinks = doc.xpath('/html/body//a/@href')\n    plinks = set()\n    for alink in somelinks:\n        if (urlparse.urlparse(alink)[1][(- len(apexdom)):] == apexdom):\n            plinks.add(alink)\n    plinks = list(plinks)\n    return plinks\n", "label": 0}
{"function": "\n\ndef runcode(self, code):\n    (sys.stdout, sys.stderr) = (StringIO(), StringIO())\n    InteractiveConsole.runcode(self, code)\n    (output, errors) = (sys.stdout.getvalue(), sys.stderr.getvalue())\n    (sys.stdout, sys.stderr) = (sys.__stdout__, sys.__stderr__)\n    if (output or errors):\n        self.wait_for_user_input()\n    if output:\n        self.write(output)\n    if errors:\n        self.write(errors)\n", "label": 0}
{"function": "\n\ndef getmoduleinfo(path):\n    'Get the module name, suffix, mode, and module type for a given file.'\n    filename = os.path.basename(path)\n\n    def _f(suffix, mode, mtype):\n        return ((- len(suffix)), suffix, mode, mtype)\n    suffixes = map(_f, imp.get_suffixes())\n    suffixes.sort()\n    for (neglen, suffix, mode, mtype) in suffixes:\n        if (filename[neglen:] == suffix):\n            return (filename[:neglen], suffix, mode, mtype)\n", "label": 0}
{"function": "\n\ndef get_tvtk_name(vtk_name):\n    \"Converts a VTK class name to a TVTK class name.\\n\\n    This function additionally converts any leading digits into a\\n    suitable string.\\n\\n    For example:\\n\\n      >>> get_tvtk_name('vtk3DSImporter')\\n      'ThreeDSImporter'\\n      >>> get_tvtk_name('vtkXMLDataReader')\\n      'XMLDataReader'\\n\\n    \"\n    if (vtk_name[:3] == 'vtk'):\n        name = vtk_name[3:]\n        dig2name = {\n            '1': 'One',\n            '2': 'Two',\n            '3': 'Three',\n            '4': 'Four',\n            '5': 'Five',\n            '6': 'Six',\n            '7': 'Seven',\n            '8': 'Eight',\n            '9': 'Nine',\n            '0': 'Zero',\n        }\n        if (name[0] in string.digits):\n            return (dig2name[name[0]] + name[1:])\n        else:\n            return name\n    else:\n        return vtk_name\n", "label": 0}
{"function": "\n\ndef to_business(self):\n    configuration = {\n        \n    }\n    for param in self.client_parameters:\n        try:\n            if ((param.value is not None) and len(param.value)):\n                if (param.parameter_type == 'string'):\n                    configuration[param.parameter_name] = param.value\n                elif (param.parameter_type == 'integer'):\n                    configuration[param.parameter_name] = int(param.value)\n                elif (param.parameter_type == 'floating'):\n                    configuration[param.parameter_name] = float(param.value)\n                elif (param.parameter_type == 'bool'):\n                    configuration[param.parameter_name] = (param.value.lower() == 'true')\n                else:\n                    print(('Unknown Experiment Client Parameter type %s' % param.parameter_type))\n        except (ValueError, TypeError) as e:\n            assert (e is not None)\n            traceback.print_exc()\n            continue\n    client = ExperimentClient(self.client, configuration)\n    return Experiment(self.name, self.category.to_business(), self.start_date, self.end_date, client, self.id)\n", "label": 1}
{"function": "\n\n@base.remotable\ndef refresh(self, use_slave=False):\n    extra = [field for field in INSTANCE_OPTIONAL_ATTRS if self.obj_attr_is_set(field)]\n    current = self.__class__.get_by_uuid(self._context, uuid=self.uuid, expected_attrs=extra, use_slave=use_slave)\n    current._context = None\n    for field in self.fields:\n        if self.obj_attr_is_set(field):\n            if (field == 'info_cache'):\n                self.info_cache.refresh()\n            elif (self[field] != current[field]):\n                self[field] = current[field]\n    self.obj_reset_changes()\n", "label": 0}
{"function": "\n\ndef joinBank(self, owner):\n    bank = self.getKernel().serviceManager().equipmentService().getEquippedObject(owner, 'bank')\n    if bank:\n        if (bank.scene_id == owner.scene_id):\n            SystemMessage.sendSystemMessage(owner, 'system_msg', 'already_member_of_bank', False, False)\n        elif (bank.scene_id == 0):\n            bank.scene_id = owner.scene_id\n            SystemMessage.sendSystemMessage(owner, swgpy.OutOfBand('system_msg', 'succesfully_joined_bank'), False, False)\n        else:\n            SystemMessage.sendSystemMessage(owner, swgpy.OutOfBand('system_msg', 'member_of_different_bank'), False, False)\n", "label": 0}
{"function": "\n\ndef hostname_or_ip(self, address):\n    result = self._cache.get(address, None)\n    now = time.time()\n    if ((result is None) or (result[0] <= now)):\n        logging.debug('Cache miss for %s in hostname_or_ip', address)\n        result = ((now + self.timeout), self._hostname_or_ip(address))\n        self._cache[address] = result\n    return result[1]\n", "label": 0}
{"function": "\n\ndef read_model_data(self, row):\n    models = {\n        \n    }\n    for (model_class, column_data) in self.columns_to_compare.items():\n        models[model_class] = []\n        for (idx, col_name) in column_data:\n            models[model_class].append(row[idx])\n    return models\n", "label": 0}
{"function": "\n\ndef set(self, key, value):\n    try:\n        self.cache.pop(key)\n    except KeyError:\n        if (len(self.cache) >= self.capacity):\n            self.cache.popitem(last=False)\n    self.cache[key] = value\n", "label": 0}
{"function": "\n\ndef assertParser(self, _cli, parser_args, argv):\n    '\\n        Assertion method for subparsers, subcommands and positional arguments.\\n        Parameter argv is a list where the first item is the name of the subparser,\\n        followed by the subcommand and the remaining items are positional arguments\\n        '\n    number_of_parsing_levels = 2\n    cmd = 'cmd_{0}_{1}'.format(argv[0], argv[1]).replace('-', '_')\n    self.assertEqual(parser_args.func.__name__, cmd)\n    self.assertTrue((argv[1] in _cli.subcommands))\n    self.assertTrue((argv[0] in _cli.subparsers))\n    number_of_positional_arguments = (len(argv) - number_of_parsing_levels)\n    if number_of_positional_arguments:\n        positional_args = []\n        for item in parser_args.func.arguments:\n            arg = item[0][0]\n            if arg.startswith('--'):\n                break\n            positional_args.append(arg)\n        for (index, arg) in enumerate(positional_args, start=number_of_parsing_levels):\n            args_type = parser_args.func.arguments[(index - number_of_parsing_levels)][1].get('type')\n            if args_type:\n                self.assertEqual(getattr(parser_args, arg), args_type(argv[index]))\n            else:\n                self.assertEqual(getattr(parser_args, arg), argv[index])\n            self.assertRaises(SystemExit, self.cli, argv[:index])\n", "label": 0}
{"function": "\n\ndef start(self, job_id, job_tag):\n    \"\\n        Apply the 'start' action to the session object\\n        If the request can be accepted it modifies the relevant fields\\n        and sets the need_update member to notify that the stored\\n        document needs to be updated\\n        \"\n    now = int(time.time())\n    time_since_last_start = (now - self.doc.get('time_start', 0))\n    if (job_tag > self.session_tag):\n        raise freezer_api_exc.BadDataFormat('requested tag value too high')\n    if (time_since_last_start <= self.doc.get('hold_off', 60)):\n        if (job_tag < self.session_tag):\n            self.action_result = 'success'\n            self.set_job_start(job_id, now)\n            self.need_update = True\n        else:\n            self.action_result = 'hold-off'\n            self.need_update = False\n    elif (time_since_last_start > self.doc.get('hold_off', 60)):\n        if (self.session_tag == job_tag):\n            self.session_tag += 1\n            self.doc['time_start'] = now\n            self.doc['status'] = 'running'\n            self.doc['result'] = ''\n            self.action_result = 'success'\n            self.set_job_start(job_id, now)\n            self.need_update = True\n        else:\n            self.action_result = 'out-of-sync'\n            self.need_update = False\n", "label": 0}
{"function": "\n\n@property\ndef rows(self):\n    locations = SQLLocation.objects.filter(parent__location_id=self.config['location_id'])\n    rows = []\n    for loc in locations:\n        try:\n            org_summary = OrganizationSummary.objects.filter(location_id=loc.location_id, date__range=(self.config['startdate'], self.config['enddate'])).aggregate(average_lead_time_in_days=Avg('average_lead_time_in_days'))\n        except OrganizationSummary.DoesNotExist:\n            continue\n        avg_lead_time = org_summary['average_lead_time_in_days']\n        if avg_lead_time:\n            avg_lead_time = ('%.1f' % avg_lead_time)\n        else:\n            avg_lead_time = 'None'\n        url = make_url(DeliveryReport, self.config['domain'], '?location_id=%s&filter_by_program=%s&datespan_type=%s&datespan_first=%s&datespan_second=%s', (loc.location_id, self.config['program'], self.config['datespan_type'], self.config['datespan_first'], self.config['datespan_second']))\n        rows.append([link_format(loc.name, url), avg_lead_time])\n    return rows\n", "label": 0}
{"function": "\n\n@pytest.mark.functional\ndef test_list_lights(bridge_client):\n    lights = bridge_client.get_lights()\n    assert (len(lights) > 0)\n    assert (lights[0].state.reachable in [True, False])\n    assert (lights[0].state.on in [True, False])\n    assert (lights[0].name is not None)\n", "label": 0}
{"function": "\n\ndef get_key_value(self, text, key):\n    try:\n        return json.loads(text)[key]\n    except KeyError:\n        return ''\n    except ValueError as e:\n        raise ValueError(((e.message + '\\n') + text))\n", "label": 0}
{"function": "\n\ndef generate_extension_documentation(source_dir, outdir, ignore_paths):\n    loader = ExtensionLoader(keep_going=True)\n    loader.clear()\n    loader.update(paths=[source_dir], ignore_paths=ignore_paths)\n    for ext_type in loader.extension_kinds:\n        if (not (ext_type in GENERATE_FOR)):\n            continue\n        outfile = os.path.join(outdir, '{}s.rst'.format(ext_type))\n        with open(outfile, 'w') as wfh:\n            wfh.write('.. _{}s:\\n\\n'.format(ext_type))\n            wfh.write(underline(capitalize('{}s'.format(ext_type))))\n            exts = loader.list_extensions(ext_type)\n            for ext in sorted(exts, key=(lambda x: x.name)):\n                wfh.write(get_rst_from_extension(ext))\n", "label": 0}
{"function": "\n\ndef onexit():\n    global kafka, consumer, producer\n    if consumer:\n        consumer.commit()\n        consumer.stop()\n        consumer = None\n    if producer:\n        producer.stop()\n        producer = None\n    if kafka:\n        kafka.close()\n        kafka = None\n    print('remote_rainter {0} is shutting down'.format(os.getpid()))\n", "label": 0}
{"function": "\n\ndef getNgrams(query, corpus, startYear, endYear, smoothing, caseInsensitive):\n    params = dict(content=query, year_start=startYear, year_end=endYear, corpus=corpora[corpus], smoothing=smoothing, case_insensitive=caseInsensitive)\n    if (params['case_insensitive'] is False):\n        params.pop('case_insensitive')\n    if ('?' in params['content']):\n        params['content'] = params['content'].replace('?', '*')\n    if ('@' in params['content']):\n        params['content'] = params['content'].replace('@', '=>')\n    req = requests.get('http://books.google.com/ngrams/graph', params=params)\n    res = re.findall('var data = (.*?);\\\\n', req.text)\n    if res:\n        data = {qry['ngram']: qry['timeseries'] for qry in literal_eval(res[0])}\n        df = DataFrame(data)\n        df.insert(0, 'year', list(range(startYear, (endYear + 1))))\n    else:\n        df = DataFrame()\n    return (req.url, params['content'], df)\n", "label": 0}
{"function": "\n\ndef get_vexrc(options, environ):\n    'Get a representation of the contents of the config file.\\n\\n    :returns:\\n        a Vexrc instance.\\n    '\n    if (options.config and (not os.path.exists(options.config))):\n        raise exceptions.InvalidVexrc('nonexistent config: {0!r}'.format(options.config))\n    filename = (options.config or os.path.expanduser('~/.vexrc'))\n    vexrc = config.Vexrc.from_file(filename, environ)\n    return vexrc\n", "label": 0}
{"function": "\n\ndef random(pages=1):\n    '\\n  Get a list of random Wikipedia article titles.\\n\\n  .. note:: Random only gets articles from namespace 0, meaning no Category, User talk, or other meta-Wikipedia pages.\\n\\n  Keyword arguments:\\n\\n  * pages - the number of random pages returned (max of 10)\\n  '\n    query_params = {\n        'list': 'random',\n        'rnnamespace': 0,\n        'rnlimit': pages,\n    }\n    request = _wiki_request(query_params)\n    titles = [page['title'] for page in request['query']['random']]\n    if (len(titles) == 1):\n        return titles[0]\n    return titles\n", "label": 0}
{"function": "\n\ndef AddOperands(self, lhs, rhs):\n    if (isinstance(lhs, Expression) and isinstance(rhs, Expression)):\n        self.args.insert(0, lhs)\n        self.args.append(rhs)\n    else:\n        raise ParseError(('Expected expression, got %s %s %s' % (lhs, self.operator, rhs)))\n", "label": 0}
{"function": "\n\ndef outputSide(self):\n    if (not self.getIsSidebarMode()):\n        return\n    (maxy, maxx) = self.screenControl.getScreenDimensions()\n    borderX = (maxx - self.WIDTH)\n    if (self.mode == COMMAND_MODE):\n        borderX = (len(SHORT_COMMAND_PROMPT) + 20)\n    usageLines = usageStrings.USAGE_PAGE.split('\\n')\n    if (self.mode == COMMAND_MODE):\n        usageLines = usageStrings.USAGE_COMMAND.split('\\n')\n    for (index, usageLine) in enumerate(usageLines):\n        self.printer.addstr((self.getMinY() + index), (borderX + 2), usageLine)\n    for y in range(self.getMinY(), maxy):\n        self.printer.addstr(y, borderX, '|')\n", "label": 0}
{"function": "\n\ndef convert(self):\n    (document, extension) = os.path.splitext(self.filename)\n    extension_from = extension.strip('.')\n    if ((self.extension_to is None) or (self.extension_to == extension_from)):\n        self.file_obj.seek(0)\n        content = self.file_obj.read()\n        self.file_obj.close()\n        mime = magic.Magic(mime=True)\n        mimetype = mime.from_buffer(content)\n        return [mimetype, content]\n    try:\n        func = getattr(self, ('%s_to_%s' % (extension_from, self.extension_to)))\n        return func()\n    except AttributeError:\n        return None\n", "label": 0}
{"function": "\n\ndef withinregion(x, y, region):\n    return ((x > region[0]) and (x < (region[0] + region[2])) and (y > region[1]) and (y < (region[1] + region[3])))\n", "label": 0}
{"function": "\n\n@webapi_check_login_required\ndef get_queryset(self, request, is_list=False, local_site=None, *args, **kwargs):\n    'Returns a queryset for DefaultReviewer models.\\n\\n        By default, this returns all default reviewers.\\n\\n        If the queryset is being used for a list of default reviewer\\n        resources, then it can be further filtered by one or more of the\\n        arguments listed in get_list.\\n        '\n    queryset = self.model.objects.filter(local_site=local_site)\n    if is_list:\n        if ('repositories' in request.GET):\n            for repo_id in request.GET.get('repositories').split(','):\n                try:\n                    queryset = queryset.filter(repository=repo_id)\n                except ValueError:\n                    pass\n        if ('users' in request.GET):\n            for username in request.GET.get('users').split(','):\n                queryset = queryset.filter(people__username=username)\n        if ('groups' in request.GET):\n            for name in request.GET.get('groups').split(','):\n                queryset = queryset.filter(groups__name=name)\n    return queryset\n", "label": 1}
{"function": "\n\ndef _refresh(self, http_request):\n    'Refreshes the access_token.\\n\\n        Skip all the storage hoops and just refresh using the API.\\n\\n        Args:\\n            http_request: callable, a callable that matches the method\\n                          signature of httplib2.Http.request, used to make\\n                          the refresh request.\\n\\n        Raises:\\n            HttpAccessTokenRefreshError: When the refresh fails.\\n        '\n    (response, content) = http_request(META, headers={\n        'Metadata-Flavor': 'Google',\n    })\n    content = _from_bytes(content)\n    if (response.status == http_client.OK):\n        try:\n            token_content = json.loads(content)\n        except Exception as e:\n            raise HttpAccessTokenRefreshError(str(e), status=response.status)\n        self.access_token = token_content['access_token']\n    else:\n        if (response.status == http_client.NOT_FOUND):\n            content += ' This can occur if a VM was created with no service account or scopes.'\n        raise HttpAccessTokenRefreshError(content, status=response.status)\n", "label": 0}
{"function": "\n\ndef get_imm_property(ipmicmd, propname):\n    propname = propname.encode('utf-8')\n    proplen = (len(propname) | 128)\n    cmdlen = (len(propname) + 1)\n    cdata = (bytearray([0, 0, cmdlen, proplen]) + propname)\n    rsp = ipmicmd.xraw_command(netfn=58, command=196, data=cdata)\n    rsp['data'] = bytearray(rsp['data'])\n    if (rsp['data'][0] != 0):\n        return None\n    propdata = rsp['data'][3:]\n    if (propdata[0] & 128):\n        return str(propdata[1:]).rstrip(' \\x00')\n    else:\n        raise Exception(('Unknown format for property: ' + repr(propdata)))\n", "label": 0}
{"function": "\n\ndef process_options(self, args, opts):\n    ScrapyCommand.process_options(self, args, opts)\n    try:\n        opts.spargs = arglist_to_dict(opts.spargs)\n    except ValueError:\n        raise UsageError('Invalid -a value, use -a NAME=VALUE', print_help=False)\n    if opts.output:\n        if (opts.output == '-'):\n            self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n        else:\n            self.settings.set('FEED_URI', opts.output, priority='cmdline')\n        feed_exporters = without_none_values(self.settings.getwithbase('FEED_EXPORTERS'))\n        valid_output_formats = feed_exporters.keys()\n        if (not opts.output_format):\n            opts.output_format = os.path.splitext(opts.output)[1].replace('.', '')\n        if (opts.output_format not in valid_output_formats):\n            raise UsageError((\"Unrecognized output format '%s', set one using the '-t' switch or as a file extension from the supported list %s\" % (opts.output_format, tuple(valid_output_formats))))\n        self.settings.set('FEED_FORMAT', opts.output_format, priority='cmdline')\n", "label": 0}
{"function": "\n\ndef clean(self, value):\n    return [v.strip() for v in value.split(',') if v.strip()]\n", "label": 0}
{"function": "\n\ndef clean_url(self):\n    '\\n        Validate the input for field *url* checking if the specified\\n        URL already exists. If it is an actual update and the URL has\\n        not been changed, validation will be skipped.\\n\\n        Returns cleaned URL or raises an exception.\\n        '\n    url = self.cleaned_data['url']\n    if ('url' in self.changed_data):\n        if (not url.endswith('/')):\n            url += '/'\n        URLDoesNotExistValidator()(url)\n    return url\n", "label": 0}
{"function": "\n\n@classmethod\ndef CreateWithAllComps(cls, oDict, obsPrior, compDictList):\n    ' Create MultObsModel, all K component Distr objects,\\n         and the prior Distr object in one call\\n    '\n    if (oDict['inferType'] == 'EM'):\n        raise NotImplementedError('TODO')\n    self = cls(oDict['inferType'], obsPrior=obsPrior)\n    self.K = len(compDictList)\n    self.comp = [None for k in range(self.K)]\n    for k in xrange(self.K):\n        self.comp[k] = BetaDistr(**compDictList[k])\n    return self\n", "label": 0}
{"function": "\n\ndef _find_arch(mycc):\n    for (i, cc) in enumerate(SUPPORTED_CC):\n        if (cc == mycc):\n            return cc\n        elif (cc > mycc):\n            if (i == 0):\n                raise NvvmSupportError(('GPU compute capability %d.%d is not supported (requires >=2.0)' % mycc))\n            else:\n                return SUPPORTED_CC[(i - 1)]\n    return SUPPORTED_CC[(- 1)]\n", "label": 0}
{"function": "\n\n@internationalizeDocstring\ndef levenshtein(self, irc, msg, args, s1, s2):\n    '<string1> <string2>\\n\\n        Returns the levenshtein distance (also known as the \"edit distance\"\\n        between <string1> and <string2>)\\n        '\n    max = self.registryValue('levenshtein.max')\n    if ((len(s1) > max) or (len(s2) > max)):\n        irc.error(_('Levenshtein distance is a complicated algorithm, try it with some smaller inputs.'))\n    else:\n        irc.reply(str(utils.str.distance(s1, s2)))\n", "label": 0}
{"function": "\n\ndef get_stack(limit=10):\n    if (not DEBUG):\n        return ()\n    frame = sys._getframe(1)\n    lines = []\n    while ((len(lines) < limit) and (frame is not None)):\n        f_locals = frame.f_locals\n        ndb_debug = f_locals.get('__ndb_debug__')\n        if (ndb_debug != 'SKIP'):\n            line = frame_info(frame)\n            if (ndb_debug is not None):\n                line += (' # ' + str(ndb_debug))\n            lines.append(line)\n        frame = frame.f_back\n    return lines\n", "label": 0}
{"function": "\n\ndef _validate_logical_condition_counts(fn, conditions):\n    if (len(conditions) < 2):\n        raise IntrinsicFuncInputError(fn._errmsg_min)\n    elif (len(conditions) > 10):\n        raise IntrinsicFuncInputError(fn._errmsg_max)\n", "label": 0}
{"function": "\n\ndef test_get(doc):\n    new = client.action(doc, ['nested', 'follow'])\n    assert (new == {\n        'new': 123,\n    })\n    assert (new.title == 'new')\n", "label": 0}
{"function": "\n\ndef on_activated(self, view):\n    if ((view.settings().get('git_view') == 'status') and get_setting('git_update_status_on_focus', True)):\n        goto = None\n        if view.sel():\n            goto = ('point:%s' % view.sel()[0].begin())\n        view.run_command('git_status_refresh', {\n            'goto': goto,\n        })\n", "label": 0}
{"function": "\n\ndef build_chain(self, obj):\n    chain_id = obj.id\n    pokemon_objects = PokemonSpecies.objects.filter(evolution_chain_id=chain_id).order_by('order')\n    summary_data = PokemonSpeciesSummarySerializer(pokemon_objects, many=True, context=self.context).data\n    ref_data = PokemonSpeciesEvolutionSerializer(pokemon_objects, many=True, context=self.context).data\n    chain = entry = OrderedDict()\n    current_evolutions = None\n    evolution_data = None\n    previous_entry = None\n    previous_species = None\n    for (index, species) in enumerate(ref_data):\n        if species['evolves_from_species']:\n            if previous_species:\n                if (previous_species['id'] == species['evolves_from_species']):\n                    current_evolutions = previous_entry['evolves_to']\n            entry = OrderedDict()\n            many = False\n            try:\n                evolution_object = PokemonEvolution.objects.get(evolved_species=species['id'])\n            except PokemonEvolution.MultipleObjectsReturned:\n                evolution_object = PokemonEvolution.objects.filter(evolved_species=species['id'])\n                many = True\n            evolution_data = PokemonEvolutionSerializer(evolution_object, many=many, context=self.context).data\n            current_evolutions.append(entry)\n        entry['is_baby'] = species['is_baby']\n        entry['species'] = summary_data[index]\n        entry['evolution_details'] = (evolution_data or None)\n        entry['evolves_to'] = []\n        previous_entry = entry\n        previous_species = species\n    return chain\n", "label": 0}
{"function": "\n\ndef uninstall(packages, purge=False, options=None):\n    '\\n    Remove one or more packages.\\n\\n    If *purge* is ``True``, the package configuration files will be\\n    removed from the system.\\n\\n    Extra *options* may be passed to ``apt-get`` if necessary.\\n    '\n    manager = MANAGER\n    command = ('purge' if purge else 'remove')\n    if (options is None):\n        options = []\n    if (not isinstance(packages, basestring)):\n        packages = ' '.join(packages)\n    options.append('--assume-yes')\n    options = ' '.join(options)\n    cmd = ('%(manager)s %(command)s %(options)s %(packages)s' % locals())\n    run_as_root(cmd, pty=False)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (type(other) != type(self)):\n        return False\n    return ((self.display_name == other.display_name) and (self.username == other.username) and (self.domain == other.domain))\n", "label": 0}
{"function": "\n\ndef formfield_for_foreignkey(self, db_field, request, **kwargs):\n    if ((db_field.name == 'lifestream') and (not request.user.is_superuser)):\n        kwargs['queryset'] = Lifestream.objects.filter(user=request.user)\n        return db_field.formfield(**kwargs)\n    return super(FeedAdmin, self).formfield_for_foreignkey(db_field, request, **kwargs)\n", "label": 0}
{"function": "\n\ndef test_refcountdisposable_singlereference():\n    d = BooleanDisposable()\n    r = RefCountDisposable(d)\n    assert (not d.is_disposed)\n    r.dispose()\n    assert d.is_disposed\n    r.dispose()\n    assert d.is_disposed\n", "label": 0}
{"function": "\n\ndef _isgroup(obj, *args, **kws):\n    \"return whether argument is a group or the name of a group\\n\\n    With additional arguments (all must be strings), it also tests\\n    that the group has an an attribute named for each argument. This\\n    can be used to test not only if a object is a Group, but whether\\n    it a group with expected arguments.\\n\\n        > x = 10\\n        > g = group(x=x, y=2)\\n        > isgroup(g), isgroup(x)\\n        True, False\\n        > isgroup('g'), isgroup('x')\\n        True, False\\n        > isgroup(g, 'x', 'y')\\n        True\\n        > isgroup(g, 'x', 'y', 'z')\\n        False\\n\\n    \"\n    _larch = kws.get('_larch', None)\n    if (_larch is None):\n        raise Warning('cannot run isgroup() -- larch broken?')\n    stable = _larch.symtable\n    if (isinstance(obj, (str, unicode)) and stable.has_symbol(obj)):\n        obj = stable.get_symbol(obj)\n    return isgroup(obj, *args)\n", "label": 0}
{"function": "\n\ndef stripcomments(url):\n    'Return HTML with comments stripped out'\n    html = url\n    if checkurl(url):\n        html = gethtml(url)\n        if (not html):\n            return None\n    pattern = '<!--.*?-->'\n    pat = re.compile(pattern, (re.IGNORECASE | re.DOTALL))\n    byeblock = pat.sub('', html)\n    return byeblock\n", "label": 0}
{"function": "\n\ndef clean_enabled_services(self):\n    'Clean the enabled_services field.\\n\\n        Raises:\\n            django.core.exceptions.ValidationError:\\n                Raised if an unknown service is attempted to be enabled.\\n        '\n    for service_id in self.cleaned_data['enabled_services']:\n        if (not avatar_services.has_service(service_id)):\n            raise ValidationError(('Unknown service \"%s\"' % service_id))\n    return self.cleaned_data['enabled_services']\n", "label": 0}
{"function": "\n\n@attr(migrated_tenant=['admin', 'tenant1', 'tenant2'])\ndef test_tenants_volumes_on_dst(self):\n    \"Validate deleted tenant's volumes were migrated.\"\n    undeleted_volumes = []\n    for (tenant_name, tenant) in self.deleted_tenants:\n        tenant_undeleted_volumes = self._tenant_volumes_exist_on_dst(tenant['cinder_volumes'])\n        if tenant_undeleted_volumes:\n            undeleted_volumes.append({\n                tenant_name: tenant_undeleted_volumes,\n            })\n    if undeleted_volumes:\n        msg = \"Tenant's cinder volumes with ids {0} exist on destination, but should be deleted!\"\n        self.fail(msg.format(undeleted_volumes))\n", "label": 0}
{"function": "\n\ndef __call__(self, environ, start_response):\n    '\\n        A note about the zappa cookie: Only 1 cookie can be passed through API\\n        Gateway. Hence all cookies are packed into a special cookie, the\\n        zappa cookie. There are a number of problems with this:\\n\\n            * updates of single cookies, when there are multiple present results\\n              in deletion of the ones that are not being updated.\\n            * expiration of cookies. The client no longer knows when cookies\\n              expires.\\n\\n        The first is solved by unpacking the zappa cookie on each request and\\n        saving all incoming cookies. The response Set-Cookies are then used\\n        to update the saved cookies, which are packed and set as the zappa\\n        cookie.\\n\\n        The second is solved by filtering cookies on their expiration time,\\n        only passing cookies that are still valid to the WSGI app.\\n        '\n    self.start_response = start_response\n    parsed = parse_cookie(environ)\n    if ('zappa' in parsed):\n        self.decode_zappa_cookie(parsed['zappa'])\n        self.filter_expired_cookies()\n        environ['HTTP_COOKIE'] = self.cookie_environ_string()\n    else:\n        self.request_cookies = dict()\n    response = self.application(environ, self.encode_response)\n    if self.redirect_content:\n        response = [self.redirect_content for item in response]\n    return ClosingIterator(response)\n", "label": 0}
{"function": "\n\ndef getch(self, *args):\n    if (len(args) == 0):\n        val = lib.wgetch(self._win)\n    elif (len(args) == 2):\n        val = lib.mvwgetch(self._win, *args)\n    else:\n        raise error('getch requires 0 or 2 arguments')\n    return val\n", "label": 0}
{"function": "\n\ndef filter_queryset(self, request, queryset, view):\n    if ('child_of' in request.GET):\n        try:\n            parent_page_id = int(request.GET['child_of'])\n            assert (parent_page_id >= 0)\n        except (ValueError, AssertionError):\n            raise BadRequestError('child_of must be a positive integer')\n        site_pages = pages_for_site(request.site)\n        try:\n            parent_page = site_pages.get(id=parent_page_id)\n            queryset = queryset.child_of(parent_page)\n            queryset._filtered_by_child_of = True\n            return queryset\n        except Page.DoesNotExist:\n            raise BadRequestError(\"parent page doesn't exist\")\n    return queryset\n", "label": 0}
{"function": "\n\ndef bulk_fetch_results(self, paginated_results):\n    \"\\n        This method gets paginated search results and returns a list of Django\\n        objects in the same order.\\n\\n        It preserves the order without doing any ordering in Python, even\\n        when more than one Django model are returned in the search results. It\\n        also uses the same queryset that was used to populate the search\\n        queryset, so any select_related/prefetch_related optimisations are\\n        in effect.\\n\\n        It is heavily based on Haystack's SearchQuerySet.post_process_results,\\n        but works on the paginated results instead of all of them.\\n        \"\n    objects = []\n    models_pks = loaded_objects = {\n        \n    }\n    for result in paginated_results:\n        models_pks.setdefault(result.model, []).append(result.pk)\n    search_backend_alias = self.results.query.backend.connection_alias\n    for model in models_pks:\n        ui = connections[search_backend_alias].get_unified_index()\n        index = ui.get_index(model)\n        queryset = index.read_queryset(using=search_backend_alias)\n        loaded_objects[model] = queryset.in_bulk(models_pks[model])\n    for result in paginated_results:\n        model_objects = loaded_objects.get(result.model, {\n            \n        })\n        try:\n            result._object = model_objects[int(result.pk)]\n        except KeyError:\n            pass\n        else:\n            objects.append(result._object)\n    return objects\n", "label": 0}
{"function": "\n\ndef _render(self, request, formencode=False, realm=None):\n    'Render a signed request according to signature type\\n\\n        Returns a 3-tuple containing the request URI, headers, and body.\\n\\n        If the formencode argument is True and the body contains parameters, it\\n        is escaped and returned as a valid formencoded string.\\n        '\n    (uri, headers, body) = (request.uri, request.headers, request.body)\n    if (self.signature_type == SIGNATURE_TYPE_AUTH_HEADER):\n        headers = parameters.prepare_headers(request.oauth_params, request.headers, realm=realm)\n    elif ((self.signature_type == SIGNATURE_TYPE_BODY) and (request.decoded_body is not None)):\n        body = parameters.prepare_form_encoded_body(request.oauth_params, request.decoded_body)\n        if formencode:\n            body = urlencode(body)\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    elif (self.signature_type == SIGNATURE_TYPE_QUERY):\n        uri = parameters.prepare_request_uri_query(request.oauth_params, request.uri)\n    else:\n        raise ValueError('Unknown signature type specified.')\n    return (uri, headers, body)\n", "label": 0}
{"function": "\n\n@classmethod\ndef from_obj(cls, obj, return_obj=None):\n    if (not obj):\n        return None\n    if (not return_obj):\n        return_obj = cls()\n    super(KillChainPhaseReference, cls).from_obj(obj, return_obj=return_obj)\n    return_obj.kill_chain_id = obj.kill_chain_id\n    return_obj.kill_chain_name = obj.kill_chain_name\n    return return_obj\n", "label": 0}
{"function": "\n\ndef getElementById(self, id):\n    childNodes = self.childNodes[:]\n    while childNodes:\n        node = childNodes.pop(0)\n        if node.childNodes:\n            childNodes.extend(node.childNodes)\n        if (hasattr(node, 'getAttribute') and (node.getAttribute('id') == id)):\n            return node\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((type(other) is CSM) and (other.format == self.format) and _kmap_eq(self.kmap, other.kmap))\n", "label": 0}
{"function": "\n\ndef test_coupon_code_not_redeemable(self):\n    ' Coupon code should be redeemable. '\n    self.login()\n    params = {\n        'coupon_code': 'foo',\n    }\n    response = self.client.post(url_for('billing.coupon_code'), data=params, follow_redirects=True)\n    data = json.loads(response.data)\n    assert (response.status_code == 404)\n    assert (data['error'] == _('Discount code not found.'))\n", "label": 0}
{"function": "\n\ndef run(self, parent, blocks):\n    block = blocks.pop(0)\n    prelines = block[:self.match.start()].rstrip('\\n')\n    if prelines:\n        self.parser.parseBlocks(parent, [prelines])\n    hr = util.etree.SubElement(parent, 'hr')\n    postlines = block[self.match.end():].lstrip('\\n')\n    if postlines:\n        blocks.insert(0, postlines)\n", "label": 0}
{"function": "\n\ndef on_text_command(self, view, command_name, args):\n    isDragSelect = (command_name == 'drag_select')\n    if (args == None):\n        return\n    isSelectingWord = (('by' in args.keys()) and (args['by'] == 'words'))\n    if (isDragSelect and isSelectingWord and (view.name() == 'Fuse - Auto Reload Result')):\n        view.run_command('fuse_goto_location')\n", "label": 0}
{"function": "\n\ndef __init__(self, title, description, get_current_value, get_values):\n    assert isinstance(title, six.text_type)\n    assert isinstance(description, six.text_type)\n    assert callable(get_current_value)\n    assert callable(get_values)\n    self.title = title\n    self.description = description\n    self.get_current_value = get_current_value\n    self.get_values = get_values\n", "label": 0}
{"function": "\n\ndef get_object_or_id(self, instance):\n    rel_id = instance._data.get(self.att_name)\n    if ((rel_id is not None) or (self.att_name in instance._obj_cache)):\n        if (self.att_name not in instance._obj_cache):\n            obj = self.rel_model.get((self.field.to_field == rel_id))\n            instance._obj_cache[self.att_name] = obj\n        return instance._obj_cache[self.att_name]\n    elif (not self.field.null):\n        raise self.rel_model.DoesNotExist\n    return rel_id\n", "label": 0}
{"function": "\n\ndef match(self, node):\n    if ((node.type in (token.COLON, token.COMMA, token.SEMI)) and (node.get_suffix() != ' ')):\n        if ((node.get_suffix().find('\\n') == 0) or (node.next_sibling and node.next_sibling.children and (node.next_sibling.children[0] == Newline()))):\n            return False\n        if (node.parent.type in [symbols.subscript, symbols.sliceop]):\n            return False\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef test():\n    started = Queue()\n    n = 1000\n    threads = []\n    touched = []\n    pool = EmptyListPool()\n    rand = SystemRandom()\n\n    def _run():\n        psleep = rand.uniform(0.05, 0.1)\n        with pool.transaction() as a:\n            started.put(1)\n            started.join()\n            a.append(rand.uniform(0, 1))\n            if (psleep > 1):\n                print(psleep)\n            sleep(psleep)\n    for i in range(n):\n        th = Thread(target=_run)\n        threads.append(th)\n        th.start()\n    for i in range(n):\n        started.get()\n        started.task_done()\n    for element in pool:\n        touched.append(element)\n    for thr in threads:\n        thr.join()\n    if (set(pool.elements) != set(touched)):\n        print((set(pool.elements) - set(touched)))\n        return False\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef _get_status(self, event):\n    state = event.get('state')\n    state_description = event.get('state_description')\n    status = 'UNKNOWN'\n    status_map = {\n        'building': 'BUILD',\n        'stopped': 'SHUTOFF',\n        'paused': 'PAUSED',\n        'suspended': 'SUSPENDED',\n        'rescued': 'RESCUE',\n        'error': 'ERROR',\n        'deleted': 'DELETED',\n        'soft-delete': 'SOFT_DELETED',\n        'shelved': 'SHELVED',\n        'shelved_offloaded': 'SHELVED_OFFLOADED',\n    }\n    if (state in status_map):\n        status = status_map[state]\n    if (state == 'resized'):\n        if (state_description == 'resize_reverting'):\n            status = 'REVERT_RESIZE'\n        else:\n            status = 'VERIFY_RESIZE'\n    if (state == 'active'):\n        active_map = {\n            'rebooting': 'REBOOT',\n            'rebooting_hard': 'HARD_REBOOT',\n            'updating_password': 'PASSWORD',\n            'rebuilding': 'REBUILD',\n            'rebuild_block_device_mapping': 'REBUILD',\n            'rebuild_spawning': 'REBUILD',\n            'migrating': 'MIGRATING',\n            'resize_prep': 'RESIZE',\n            'resize_migrating': 'RESIZE',\n            'resize_migrated': 'RESIZE',\n            'resize_finish': 'RESIZE',\n        }\n        status = active_map.get(state_description, 'ACTIVE')\n    if (status == 'UNKNOWN'):\n        logger.error(('Unknown status for event %s: state %s (%s)' % (event.get('message_id'), state, state_description)))\n    return status\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    super(VerifyDstDeletedTenantResources, self).setUp()\n    self.dst_cloud.switch_user(user=self.dst_cloud.username, password=self.dst_cloud.password, tenant=self.dst_cloud.tenant)\n    self.deleted_tenants = [(tenant['name'], tenant) for tenant in config.tenants if (('deleted' in tenant) and (tenant['deleted'] is True))]\n    self.dst_tenants = {ten.name: ten.id for ten in self.dst_cloud.keystoneclient.tenants.list()}\n    dst_cinder_volumes = self.dst_cloud.cinderclient.volumes.list(search_opts={\n        'all_tenants': 0,\n    })\n    self.dst_volumes_admin = {vol.display_name: vol for vol in dst_cinder_volumes}\n    self.dst_vm_list = self.dst_cloud.novaclient.servers.list(search_opts={\n        'tenant_id': self.dst_tenants[self.dst_cloud.tenant],\n    })\n", "label": 0}
{"function": "\n\ndef func(self):\n    '\\n        Create a new message and send it to channel, using\\n        the already formatted input.\\n        '\n    caller = self.caller\n    (channelkey, msg) = self.args\n    if (not msg):\n        caller.msg('Say what?')\n        return\n    channel = ChannelDB.objects.get_channel(channelkey)\n    if (not channel):\n        caller.msg((\"Channel '%s' not found.\" % channelkey))\n        return\n    if (not channel.has_connection(caller)):\n        string = \"You are not connected to channel '%s'.\"\n        caller.msg((string % channelkey))\n        return\n    if (not channel.access(caller, 'send')):\n        string = \"You are not permitted to send to channel '%s'.\"\n        caller.msg((string % channelkey))\n        return\n    msg = ('[%s] %s: %s' % (channel.key, caller.name, msg))\n    msgobj = create.create_message(caller, msg, channels=[channel])\n    channel.msg(msgobj)\n", "label": 0}
{"function": "\n\ndef test_prettyRanges(self):\n    c = libpry.coverage.File(None)\n    out = c.prettyRanges([1, 2, (3, 4)], 1, 5)\n    assert (out == ' 1 2\\n [3...4]')\n    out = c.prettyRanges([1, 2, 3, 4], 1, 5)\n    assert (out == ' 1 2\\n 3 4')\n    out = c.prettyRanges([11111, 22222, 33333], 1, 5)\n    assert (out == ' 11111\\n 22222\\n 33333')\n", "label": 0}
{"function": "\n\ndef _urllib2_fetch(self, uri, params, method=None):\n    if (self.opener == None):\n        self.opener = urllib2.build_opener(HTTPErrorProcessor)\n        urllib2.install_opener(self.opener)\n    if (method and (method == 'GET')):\n        uri = self._build_get_uri(uri, params)\n        req = PlivoUrlRequest(uri)\n    else:\n        req = PlivoUrlRequest(uri, urllib.urlencode(params))\n        if (method and ((method == 'DELETE') or (method == 'PUT'))):\n            req.http_method = method\n    authstring = base64.encodestring(('%s:%s' % (self.auth_id, self.auth_token)))\n    authstring = authstring.replace('\\n', '')\n    req.add_header('Authorization', ('Basic %s' % authstring))\n    response = urllib2.urlopen(req)\n    return response.read()\n", "label": 0}
{"function": "\n\ndef _dump_item(self, item, indent=''):\n    if isBadItem(item):\n        return\n    v = item.value\n    if v.vsHasField('children'):\n        (yield '{indent:s}<{tag:s}>'.format(indent=indent, tag=getTagName(item.header)))\n        for (_, c) in v.children:\n            if isBadItem(c):\n                continue\n            for l in self._dump_item(c, (indent + '  ')):\n                (yield l)\n        (yield '{indent:s}</{tag:s}>'.format(indent=indent, tag=getTagName(item.header)))\n    else:\n        (yield \"{indent:s}<{tag:s} type='{type_:s}'>{data:s}</{tag:s}>\".format(indent=indent, type_=formatValueType(item), data=self._formatValue(item), tag=getTagName(item.header)))\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    try:\n        while 1:\n            x = self.table.get(key, None)\n            if (not (x is None)):\n                return x\n            self = self.parent\n    except AttributeError:\n        return []\n", "label": 0}
{"function": "\n\n@cached(user=True)\ndef get_permission_objects(self, request):\n    odm = request.app.odm()\n    user = request.cache.user\n    perms = {\n        \n    }\n    with odm.begin() as session:\n        if user.is_authenticated():\n            session.add(user)\n            groups = set(user.groups)\n        else:\n            cfg = request.config\n            query = session.query(odm.group)\n            groups = set(query.filter_by(name=cfg['ANONYMOUS_GROUP']))\n        for group in groups:\n            perms.update(((p.name, p.policy) for p in group.permissions))\n    return perms\n", "label": 0}
{"function": "\n\ndef _test_burp_server_address(self, addr, retry=False):\n    'The :func:`burpui.misc.backend.burp1.Burp._test_burp_server_address`\\n        function determines if the given address is reachable or not.\\n\\n        :param addr: Address to look at\\n        :type addr: str\\n\\n        :param retry: Flag to stop trying because this function is recursive\\n        :type retry: bool\\n\\n        :returns: True or False wether we could find a valid address or not\\n        '\n    family = Burp._get_inet_family(addr)\n    try:\n        sock = socket.socket(family, socket.SOCK_STREAM)\n        sock.connect((addr, self.port))\n        sock.close()\n        return True\n    except socket.error:\n        self._logger('warning', 'Cannot contact burp server at %s:%s', addr, self.port)\n        if (not retry):\n            new_addr = ''\n            if (self.host == '127.0.0.1'):\n                new_addr = '::1'\n            else:\n                new_addr = '127.0.0.1'\n            self._logger('info', 'Trying %s:%s instead', new_addr, self.port)\n            if self._test_burp_server_address(new_addr, True):\n                self._logger('info', '%s:%s is reachable, switching to it for this runtime', new_addr, self.port)\n                self.host = new_addr\n                self.family = Burp._get_inet_family(new_addr)\n                return True\n            self._logger('error', 'Cannot guess burp server address')\n    return False\n", "label": 0}
{"function": "\n\ndef read_subject(self, subject_type='', predicate='', object='', assoc='', id_only=False):\n    if assoc:\n        if (type(assoc) is str):\n            assoc = self.read_association(assoc)\n        return (assoc.s if id_only else self.read(assoc.s))\n    else:\n        (sub_list, assoc_list) = self.find_subjects(subject_type=subject_type, predicate=predicate, object=object, id_only=True)\n        if (not sub_list):\n            raise NotFound(('No subject found for subject_type=%s, predicate=%s, object=%s' % (subject_type, predicate, object)))\n        elif (len(sub_list) > 1):\n            raise Inconsistent(('More than one subject found for subject_type=%s, predicate=%s, object=%s: count=%s' % (subject_type, predicate, object, len(sub_list))))\n        return (sub_list[0] if id_only else self.read(sub_list[0]))\n", "label": 0}
{"function": "\n\ndef _frame_loop(self, frame):\n    if self.heartbeat:\n        self.last_received_frame = time.time()\n    self.channels[frame.channel].process_frame(frame)\n    self._frame_count += 1\n    if self.stream:\n        if (self._frame_count == 5):\n            self._frame_count = 0\n            cb = (lambda : FrameReader(self.stream, self._frame_loop))\n            self._add_ioloop_callback(cb)\n        else:\n            FrameReader(self.stream, self._frame_loop)\n", "label": 0}
{"function": "\n\ndef _remove_expired_items_from_cache(self):\n    'Remove expired items from temporary URL cache\\n\\n        This function removes entries that will expire before the expected\\n        usage time.\\n        '\n    max_valid_time = (int(time.time()) + CONF.glance.swift_temp_url_expected_download_start_delay)\n    keys_to_remove = [k for (k, v) in six.iteritems(self._cache) if (v.url_expires_at < max_valid_time)]\n    for k in keys_to_remove:\n        del self._cache[k]\n", "label": 0}
{"function": "\n\ndef __init__(self, user, skip=None, *args, **kwargs):\n    if (skip is None):\n        skip = []\n    super(FilterForm, self).__init__(*args, **kwargs)\n    if ('name' in skip):\n        del self.fields['name']\n    else:\n        self.fields['name'].required = False\n        self.fields['name'].label = _('Name')\n    if ('contact_type' in skip):\n        del self.fields['contact_type']\n    else:\n        self.fields['contact_type'].queryset = Object.filter_permitted(user, ContactType.objects)\n        self.fields['contact_type'].required = True\n        self.fields['contact_type'].label = _('Contact type')\n", "label": 0}
{"function": "\n\ndef get_default_exprs(self, col, col_type):\n    if (col in self.col_blacklist):\n        return {\n            \n        }\n    d = {\n        \n    }\n    d[(col, 'non_null')] = 'COUNT({col})'\n    if (col_type in ['double', 'int', 'bigint', 'float', 'double']):\n        d[(col, 'sum')] = 'SUM({col})'\n        d[(col, 'min')] = 'MIN({col})'\n        d[(col, 'max')] = 'MAX({col})'\n        d[(col, 'avg')] = 'AVG({col})'\n    elif (col_type == 'boolean'):\n        d[(col, 'true')] = 'SUM(CASE WHEN {col} THEN 1 ELSE 0 END)'\n        d[(col, 'false')] = 'SUM(CASE WHEN NOT {col} THEN 1 ELSE 0 END)'\n    elif (col_type in ['string']):\n        d[(col, 'len')] = 'SUM(CAST(LENGTH({col}) AS BIGINT))'\n        d[(col, 'approx_distinct')] = 'APPROX_DISTINCT({col})'\n    return {k: v.format(col=col) for (k, v) in d.items()}\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    return '\\n'.join([('%s %s\\n  %s' % (record.name, record.levelname, '\\n'.join([line for line in record.getMessage().split('\\n') if line.strip()]))) for record in self.records])\n", "label": 0}
{"function": "\n\ndef cleanup_tempfiles():\n    for tmpfile in tempfiles:\n        try:\n            os.unlink(tmpfile)\n        except:\n            pass\n    for tmpdir in tempdirs:\n        shutil.rmtree(tmpdir, ignore_errors=True)\n", "label": 0}
{"function": "\n\ndef check_arbitrary(a, b):\n    if (isinstance(a, (list, tuple)) and isinstance(b, (list, tuple))):\n        assert (len(a) == len(b))\n        for (a_, b_) in zip(a, b):\n            check_arbitrary(a_, b_)\n    elif isinstance(a, Panel):\n        assert_panel_equal(a, b)\n    elif isinstance(a, DataFrame):\n        assert_frame_equal(a, b)\n    elif isinstance(a, Series):\n        assert_series_equal(a, b)\n    elif isinstance(a, Index):\n        assert_index_equal(a, b)\n    else:\n        assert (a == b)\n", "label": 1}
{"function": "\n\ndef _has_uploaded_file_rtm(self, name):\n    with self._events_lock:\n        for event in self.events:\n            if ((event['type'] == 'file_shared') and (event['file']['name'] == name) and (event['file']['user'] == self.testbot_userid)):\n                return True\n        return False\n", "label": 0}
{"function": "\n\n@cli_app.command('create-app')\n@click.option('--name', prompt='Your new app name', help='Your application name, directory will have this name')\n@click.option('--engine', prompt='Your engine type, SQLAlchemy or MongoEngine', type=click.Choice(['SQLAlchemy', 'MongoEngine']), default='SQLAlchemy', help='Write your engine type')\ndef create_app(name, engine):\n    '\\n        Create a Skeleton application (needs internet connection to github)\\n    '\n    try:\n        if (engine.lower() == 'sqlalchemy'):\n            url = urlopen(SQLA_REPO_URL)\n            dirname = 'Flask-AppBuilder-Skeleton-master'\n        elif (engine.lower() == 'mongoengine'):\n            url = urlopen(MONGOENGIE_REPO_URL)\n            dirname = 'Flask-AppBuilder-Skeleton-me-master'\n        zipfile = ZipFile(BytesIO(url.read()))\n        zipfile.extractall()\n        os.rename(dirname, name)\n        click.echo(click.style('Downloaded the skeleton app, good coding!', fg='green'))\n        return True\n    except Exception as e:\n        click.echo(click.style('Something went wrong {0}'.format(e), fg='red'))\n        if (engine.lower() == 'sqlalchemy'):\n            click.echo(click.style('Try downloading from {0}'.format(SQLA_REPO_URL), fg='green'))\n        elif (engine.lower() == 'mongoengine'):\n            click.echo(click.style('Try downloading from {0}'.format(MONGOENGIE_REPO_URL), fg='green'))\n        return False\n", "label": 0}
{"function": "\n\ndef _get_streams(self):\n    match = _url_re.match(self.url)\n    channel_name = match.group('channel')\n    form = {\n        'cid': channel_name,\n        'watchTime': 0,\n        'firstConnect': 1,\n        'ip': 'NaN',\n    }\n    res = http.post(API_URL, data=form, headers=HEADERS)\n    params = parse_query(res.text, schema=_schema)\n    if (params['status'] <= 0):\n        return\n    if (params['block_type'] != 0):\n        if (params['block_type'] == BLOCK_TYPE_VIEWING_LIMIT):\n            msg = BLOCKED_MSG_FORMAT.format(params.get('block_time', 'UNKNOWN'), params.get('reconnect_time', 'UNKNOWN'))\n            raise PluginError(msg)\n        elif (params['block_type'] == BLOCK_TYPE_NO_SLOTS):\n            raise PluginError('No free slots available')\n        else:\n            raise PluginError('Blocked for unknown reasons')\n    if ('token' not in params):\n        raise PluginError('Server seems busy, retry again later')\n    streams = {\n        \n    }\n    stream_names = ['sd']\n    if params['multibitrate']:\n        stream_names += ['hd']\n    for stream_name in stream_names:\n        playpath = params['playpath']\n        if (stream_name == 'hd'):\n            playpath += 'HI'\n        stream = RTMPStream(self.session, {\n            'rtmp': '{0}/{1}'.format(params['rtmp'], playpath),\n            'pageUrl': self.url,\n            'swfVfy': SWF_URL,\n            'weeb': params['token'],\n            'live': True,\n        })\n        streams[stream_name] = stream\n    return streams\n", "label": 1}
{"function": "\n\ndef _is_number_match_OO(numobj1_in, numobj2_in):\n    'Takes two phone number objects and compares them for equality.'\n    numobj1 = PhoneNumber()\n    numobj1.merge_from(numobj1_in)\n    numobj2 = PhoneNumber()\n    numobj2.merge_from(numobj2_in)\n    numobj1.raw_input = None\n    numobj1.country_code_source = None\n    numobj1.preferred_domestic_carrier_code = None\n    numobj2.raw_input = None\n    numobj2.country_code_source = None\n    numobj2.preferred_domestic_carrier_code = None\n    if ((numobj1.extension is not None) and (len(numobj1.extension) == 0)):\n        numobj1.extension = None\n    if ((numobj2.extension is not None) and (len(numobj2.extension) == 0)):\n        numobj2.extension = None\n    if ((numobj1.extension is not None) and (numobj2.extension is not None) and (numobj1.extension != numobj2.extension)):\n        return MatchType.NO_MATCH\n    country_code1 = numobj1.country_code\n    country_code2 = numobj2.country_code\n    if ((country_code1 != 0) and (country_code2 != 0)):\n        if (numobj1 == numobj2):\n            return MatchType.EXACT_MATCH\n        elif ((country_code1 == country_code2) and _is_national_number_suffix_of_other(numobj1, numobj2)):\n            return MatchType.SHORT_NSN_MATCH\n        return MatchType.NO_MATCH\n    numobj1.country_code = country_code2\n    if (numobj1 == numobj2):\n        return MatchType.NSN_MATCH\n    if _is_national_number_suffix_of_other(numobj1, numobj2):\n        return MatchType.SHORT_NSN_MATCH\n    return MatchType.NO_MATCH\n", "label": 1}
{"function": "\n\ndef get_object(self, queryset=None):\n    if (queryset is None):\n        queryset = self.get_queryset()\n    pk = self.kwargs.get(self.pk_url_kwarg, None)\n    slug = self.kwargs.get(self.slug_url_kwarg, None)\n    if (pk is not None):\n        queryset = queryset.filter(pk=pk)\n    elif (slug is not None):\n        slug_field = self.get_slug_field()\n        queryset = queryset.filter(**{\n            slug_field: slug,\n        })\n    else:\n        raise AttributeError(('Generic detail view %s must be called with either an object pk or a slug.' % self.__class__.__name__))\n    try:\n        obj = queryset.get()\n    except ObjectDoesNotExist:\n        try:\n            queryset = self.get_queryset()\n            queryset = queryset.filter(normalized_name=re.sub('[^A-Za-z0-9.]+', '-', slug).lower())\n            obj = queryset.get()\n        except ObjectDoesNotExist:\n            raise Http404((_('No %(verbose_name)s found matching the query') % {\n                'verbose_name': queryset.model._meta.verbose_name,\n            }))\n    return obj\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    s = ''\n    if (self.start is not None):\n        s = (s + str(self.start))\n    s = (s + ':')\n    if (self.stop is not None):\n        s = (s + str(self.stop))\n    if (self.step is not None):\n        s = ((s + ':') + str(self.step))\n    return s\n", "label": 0}
{"function": "\n\ndef falseColor(self, tile, color):\n    'Recolor a tile from 8bit to CMYRGB'\n    data32 = np.uint32(tile)\n    if ((color == 'C') or (color == 'cyan')):\n        fcdata = ((4278190080 + np.left_shift(data32, 8)) + np.left_shift(data32, 16))\n    elif ((color == 'Y') or (color == 'yellow')):\n        fcdata = ((4278190080 + np.left_shift(data32, 8)) + data32)\n    elif ((color == 'M') or (color == 'magenta')):\n        fcdata = ((4278190080 + np.left_shift(data32, 16)) + data32)\n    if ((color == 'R') or (color == 'red')):\n        fcdata = (4278190080 + data32)\n    elif ((color == 'G') or (color == 'green')):\n        fcdata = (4278190080 + np.left_shift(data32, 8))\n    elif ((color == 'B') or (color == 'blue')):\n        fcdata = (4278190080 + np.left_shift(data32, 16))\n    return fcdata\n", "label": 1}
{"function": "\n\ndef __init__(self, grammar, root_label='S', loop=1, trace=0):\n    '\\n        Create a new chunk parser, from the given start state\\n        and set of chunk patterns.\\n\\n        :param grammar: The grammar, or a list of RegexpChunkParser objects\\n        :type grammar: str or list(RegexpChunkParser)\\n        :param root_label: The top node of the tree being created\\n        :type root_label: str or Nonterminal\\n        :param loop: The number of times to run through the patterns\\n        :type loop: int\\n        :type trace: int\\n        :param trace: The level of tracing that should be used when\\n            parsing a text.  ``0`` will generate no tracing output;\\n            ``1`` will generate normal tracing output; and ``2`` or\\n            higher will generate verbose tracing output.\\n        '\n    self._trace = trace\n    self._stages = []\n    self._grammar = grammar\n    self._loop = loop\n    if isinstance(grammar, string_types):\n        self._read_grammar(grammar, root_label, trace)\n    else:\n        type_err = 'Expected string or list of RegexpChunkParsers for the grammar.'\n        try:\n            grammar = list(grammar)\n        except:\n            raise TypeError(type_err)\n        for elt in grammar:\n            if (not isinstance(elt, RegexpChunkParser)):\n                raise TypeError(type_err)\n        self._stages = grammar\n", "label": 0}
{"function": "\n\ndef _process(self, overlay, key=None):\n    if (len(overlay) != 2):\n        raise Exception('Overlay must contain at least to items.')\n    [target, kernel] = (overlay.get(0), overlay.get(1))\n    if (len(target.vdims) != 1):\n        raise Exception('Convolution requires inputs with single value dimensions.')\n    xslice = slice(self.p.kernel_roi[0], self.p.kernel_roi[2])\n    yslice = slice(self.p.kernel_roi[1], self.p.kernel_roi[3])\n    k = (kernel.data if (self.p.kernel_roi == (0, 0, 0, 0)) else kernel[(xslice, yslice)].data)\n    fft1 = np.fft.fft2(target.data)\n    fft2 = np.fft.fft2(k, s=target.data.shape)\n    convolved_raw = np.fft.ifft2((fft1 * fft2)).real\n    (k_rows, k_cols) = k.shape\n    rolled = np.roll(np.roll(convolved_raw, (- (k_cols // 2)), axis=(- 1)), (- (k_rows // 2)), axis=(- 2))\n    convolved = (rolled / float(k.sum()))\n    return Image(convolved, bounds=target.bounds, group=self.p.group)\n", "label": 0}
{"function": "\n\ndef initialize(self, description):\n    super(AggregateQueryResultWrapper, self).initialize(description)\n    self.all_models = set()\n    for (key, _, _, _) in self.column_map:\n        self.all_models.add(key)\n    self.models_with_aggregate = set()\n    self.back_references = {\n        \n    }\n    self.source_to_dest = {\n        \n    }\n    self.dest_to_source = {\n        \n    }\n    for metadata in self.join_list:\n        if metadata.is_backref:\n            att_name = metadata.foreign_key.related_name\n        else:\n            att_name = metadata.attr\n        is_backref = (metadata.is_backref or metadata.is_self_join)\n        if is_backref:\n            self.models_with_aggregate.add(metadata.src)\n        else:\n            self.dest_to_source.setdefault(metadata.dest, set())\n            self.dest_to_source[metadata.dest].add(metadata.src)\n        self.source_to_dest.setdefault(metadata.src, {\n            \n        })\n        self.source_to_dest[metadata.src][metadata.dest] = JoinCache(metadata=metadata, attr=(metadata.alias or att_name))\n    self.columns_to_compare = {\n        \n    }\n    key_to_columns = {\n        \n    }\n    for (idx, (key, model_class, col_name, _)) in enumerate(self.column_map):\n        if (key in self.models_with_aggregate):\n            self.columns_to_compare.setdefault(key, [])\n            self.columns_to_compare[key].append((idx, col_name))\n        key_to_columns.setdefault(key, [])\n        key_to_columns[key].append((idx, col_name))\n    for model_or_alias in self.models_with_aggregate:\n        if (model_or_alias not in self.columns_to_compare):\n            continue\n        sources = self.dest_to_source.get(model_or_alias, ())\n        for joined_model in sources:\n            self.columns_to_compare[model_or_alias].extend(key_to_columns[joined_model])\n", "label": 1}
{"function": "\n\ndef update_result(self, context):\n    self.device.pull_file(self.cyclictest_result, context.output_directory)\n    with open(os.path.join(context.output_directory, TXT_RESULT_NAME)) as f:\n        for line in f:\n            if (line.find('C:') is not (- 1)):\n                (key, sperator, remaing) = line.partition('C:')\n                index = key.find('T')\n                key = key.replace(key[index], RESULT_INTERPRETATION['T'])\n                index = key.find('P')\n                key = key.replace(key[index], RESULT_INTERPRETATION['P'])\n                index = sperator.find('C')\n                sperator = sperator.replace(sperator[index], RESULT_INTERPRETATION['C'])\n                metrics = (sperator + remaing).split()\n                for i in range(0, len(metrics), 2):\n                    full_key = ((key + ' ') + metrics[i][:(- 1)])\n                    value = int(metrics[(i + 1)])\n                    context.result.add_metric(full_key, value, 'microseconds')\n", "label": 0}
{"function": "\n\ndef _get_tr_css(row, tr_style):\n    if tr_style:\n        if isinstance(tr_style, string_types):\n            return tr_style\n        elif callable(tr_style):\n            return tr_style(row)\n        else:\n            raise ArgumentError(('expected string or callable, got %r' % tr_style))\n    return ''\n", "label": 0}
{"function": "\n\ndef _compare_diffs(self, diffs, compare_to):\n    diffs = [(cmd, (fk._get_colspec() if isinstance(fk, sa.ForeignKey) else ('tone_id_fk' if (fk is None) else fk)), tname, fk_info) for (cmd, fk, tname, fk_info) in diffs]\n    self.assertEqual(diffs, compare_to)\n", "label": 0}
{"function": "\n\n@cronjobs.register\ndef update_l10n_contributor_metrics(day=None):\n    'Update the number of active contributors for each locale/product.\\n\\n    An active contributor is defined as a user that created or reviewed a\\n    revision in the previous calendar month.\\n    '\n    if (day is None):\n        day = date.today()\n    first_of_month = date(day.year, day.month, 1)\n    if (day.month == 1):\n        previous_first_of_month = date((day.year - 1), 12, 1)\n    else:\n        previous_first_of_month = date(day.year, (day.month - 1), 1)\n    for locale in settings.SUMO_LANGUAGES:\n        for product in ([None] + list(Product.objects.filter(visible=True))):\n            num = num_active_contributors(from_date=previous_first_of_month, to_date=first_of_month, locale=locale, product=product)\n            WikiMetric.objects.create(code=L10N_ACTIVE_CONTRIBUTORS_CODE, locale=locale, product=product, date=previous_first_of_month, value=num)\n", "label": 0}
{"function": "\n\ndef expand_tag(self, str):\n    _str = str.group(0)\n    s = re.findall('([\\\\w\\\\-]+(?:=(?:\"[^\"]*\"|\\\\\\'[^\\\\\\']*\\\\\\'))?)', _str)\n    if (len(s) <= self.minimum_attribute_count):\n        return _str\n    tagEnd = re.search('/?>$', _str)\n    if (not (tagEnd == None)):\n        s += [tagEnd.group(0)]\n    tag = ('<' + s[0])\n    indent = (len(tag) + 1)\n    s = s[1:]\n    if self.first_attribute_on_new_line:\n        if self.indent_with_tabs:\n            indent = 0\n            extra_tabs = 1\n        else:\n            indent = self.indent_size\n            extra_tabs = 0\n    else:\n        if self.indent_with_tabs:\n            extra_tabs = int((indent / self.tab_size))\n            indent = (indent % self.tab_size)\n        else:\n            extra_tabs = 0\n        tag += (' ' + s[0])\n        s = s[1:]\n    for l in s:\n        tag += ((('\\n' + (((self.indent_level * self.indent_size) + extra_tabs) * self.indent_char)) + (indent * ' ')) + l)\n    return tag\n", "label": 0}
{"function": "\n\ndef free_variables(*terms, **kwargs):\n    '\\n    Returns a frozenset of variables free in given terms.\\n\\n    kwargs: by_name , False by default.\\n    If by_name=True, the result is a set of variable names, and\\n    binding occurs by name. Otherwise, the result is a set of Var\\n    objects, and ForAll X:s1 does not bind X:s2.\\n    '\n    by_name = kwargs.get('by_name', False)\n    _free_variables = partial(free_variables, by_name=by_name)\n    t = (terms[0] if (len(terms) == 1) else terms)\n    if (type(t) is Var):\n        return frozenset(((t.name if by_name else t),))\n    elif (type(t) in (tuple, Const, Apply, Eq, Ite, Not, And, Or, Implies, Iff)):\n        return union(*(_free_variables(x) for x in t))\n    elif (type(t) in (ForAll, Exists)):\n        return (_free_variables(t.body) - _free_variables(*t.variables))\n    elif hasattr(t, 'args'):\n        return union(*(_free_variables(x) for x in t.args))\n    else:\n        assert False, type(t)\n", "label": 1}
{"function": "\n\ndef placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n    'Instantiates a placeholder.\\n\\n    # Arguments\\n        shape: shape of the placeholder\\n            (integer tuple, may include None entries).\\n        ndim: number of axes of the tensor.\\n            At least one of {`shape`, `ndim`} must be specified.\\n            If both are specified, `shape` is used.\\n        dtype: placeholder type.\\n        name: optional name string for the placeholder.\\n\\n    # Returns\\n        Placeholder tensor instance.\\n    '\n    if (not shape):\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n    x = tf.placeholder(dtype, shape=shape, name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    return x\n", "label": 0}
{"function": "\n\n@permission_checker.require('add')\ndef add(request):\n    ImageModel = get_image_model()\n    ImageForm = get_image_form(ImageModel)\n    if (request.method == 'POST'):\n        image = ImageModel(uploaded_by_user=request.user)\n        form = ImageForm(request.POST, request.FILES, instance=image, user=request.user)\n        if form.is_valid():\n            image.file_size = image.file.size\n            form.save()\n            for backend in get_search_backends():\n                backend.add(image)\n            messages.success(request, _(\"Image '{0}' added.\").format(image.title), buttons=[messages.button(reverse('wagtailimages:edit', args=(image.id,)), _('Edit'))])\n            return redirect('wagtailimages:index')\n        else:\n            messages.error(request, _('The image could not be created due to errors.'))\n    else:\n        form = ImageForm(user=request.user)\n    return render(request, 'wagtailimages/images/add.html', {\n        'form': form,\n    })\n", "label": 0}
{"function": "\n\ndef plot_predict(self, start=None, end=None, exog=None, dynamic=False, alpha=0.05, plot_insample=True, ax=None):\n    from statsmodels.graphics.utils import _import_mpl, create_mpl_ax\n    _ = _import_mpl()\n    (fig, ax) = create_mpl_ax(ax)\n    forecast = self.predict(start, end, exog, 'levels', dynamic)\n    start = self.model._get_predict_start(start, dynamic=dynamic)\n    (end, out_of_sample) = self.model._get_predict_end(end, dynamic=dynamic)\n    if out_of_sample:\n        steps = out_of_sample\n        fc_error = self._forecast_error(steps)\n        conf_int = self._forecast_conf_int(forecast[(- steps):], fc_error, alpha)\n    if hasattr(self.data, 'predict_dates'):\n        from pandas import Series\n        forecast = Series(forecast, index=self.data.predict_dates)\n        ax = forecast.plot(ax=ax, label='forecast')\n    else:\n        ax.plot(forecast)\n    x = ax.get_lines()[(- 1)].get_xdata()\n    if out_of_sample:\n        label = '{0:.0%} confidence interval'.format((1 - alpha))\n        ax.fill_between(x[(- out_of_sample):], conf_int[:, 0], conf_int[:, 1], color='gray', alpha=0.5, label=label)\n    if plot_insample:\n        import re\n        k_diff = self.k_diff\n        label = re.sub('D\\\\d*\\\\.', '', self.model.endog_names)\n        levels = unintegrate(self.model.endog, self.model._first_unintegrate)\n        ax.plot(x[:((end + 1) - start)], levels[(start + k_diff):((end + k_diff) + 1)], label=label)\n    ax.legend(loc='best')\n    return fig\n", "label": 0}
{"function": "\n\ndef _get_battle_target(self):\n    if (self in self.battle.teamA):\n        other_team = self.battle.teamB\n    else:\n        other_team = self.battle.teamA\n    if self._battle_target:\n        if (not (self._battle_target in other_team)):\n            self._battle_target = other_team[0]\n        self.world.log.debug(('%s attack target: %s' % (self.fancy_name(), self._battle_target.fancy_name())))\n        return self._battle_target\n", "label": 0}
{"function": "\n\ndef test_board_options(platformio_setup, clirunner, validate_cliresult):\n    required_opts = set(['build', 'platform', 'upload', 'name'])\n    result = clirunner.invoke(cmd_platforms_search, ['--json-output'])\n    validate_cliresult(result)\n    search_result = json.loads(result.output)\n    assert isinstance(search_result, list)\n    assert len(search_result)\n    platforms = [item['type'] for item in search_result]\n    for (_, opts) in util.get_boards().iteritems():\n        assert required_opts.issubset(set(opts))\n        assert (opts['platform'] in platforms)\n", "label": 0}
{"function": "\n\ndef format_select_set(self):\n    context = self.context\n    formatted = []\n    for expr in self.select_set:\n        if isinstance(expr, ir.ValueExpr):\n            expr_str = self._translate(expr, named=True)\n        elif isinstance(expr, ir.TableExpr):\n            if context.need_aliases():\n                alias = context.get_ref(expr)\n                expr_str = ('{0}.*'.format(alias) if alias else '*')\n            else:\n                expr_str = '*'\n        formatted.append(expr_str)\n    buf = StringIO()\n    line_length = 0\n    max_length = 70\n    tokens = 0\n    for (i, val) in enumerate(formatted):\n        if val.count('\\n'):\n            if i:\n                buf.write(',')\n            buf.write('\\n')\n            indented = util.indent(val, self.indent)\n            buf.write(indented)\n            line_length = len(indented.split('\\n')[(- 1)])\n            tokens = 1\n        elif ((tokens > 0) and line_length and ((len(val) + line_length) > max_length)):\n            (buf.write(',\\n       ') if i else buf.write('\\n'))\n            buf.write(val)\n            line_length = (len(val) + 7)\n            tokens = 1\n        else:\n            if i:\n                buf.write(',')\n            buf.write(' ')\n            buf.write(val)\n            tokens += 1\n            line_length += (len(val) + 2)\n    if self.distinct:\n        select_key = 'SELECT DISTINCT'\n    else:\n        select_key = 'SELECT'\n    return '{0}{1}'.format(select_key, buf.getvalue())\n", "label": 1}
{"function": "\n\n@flow.StateHandler(next_state='ProcessListDirectory')\ndef ListDeviceDirectories(self, responses):\n    if (not responses.success):\n        raise flow.FlowError('Unable to query Volume Shadow Copy information.')\n    for response in responses:\n        device_object = response.GetItem('DeviceObject', '')\n        global_root = '\\\\\\\\?\\\\GLOBALROOT\\\\Device'\n        if device_object.startswith(global_root):\n            device_object = ('\\\\\\\\.' + device_object[len(global_root):])\n            path_spec = rdf_paths.PathSpec(path=device_object, pathtype=rdf_paths.PathSpec.PathType.OS)\n            path_spec.Append(path='/', pathtype=rdf_paths.PathSpec.PathType.TSK)\n            self.Log('Listing Volume Shadow Copy device: %s.', device_object)\n            self.CallClient('ListDirectory', pathspec=path_spec, next_state='ProcessListDirectory')\n            self.state.raw_device = aff4.AFF4Object.VFSGRRClient.PathspecToURN(path_spec, self.client_id).Dirname()\n            self.state.shadows.append(aff4.AFF4Object.VFSGRRClient.PathspecToURN(path_spec, self.client_id))\n", "label": 0}
{"function": "\n\ndef remove_nodes_from(self, nbunch):\n    'Remove multiple nodes.\\n\\n        Parameters\\n        ----------\\n        nodes : iterable container\\n            A container of nodes (list, dict, set, etc.).  If a node\\n            in the container is not in the graph it is silently\\n            ignored.\\n\\n        See Also\\n        --------\\n        remove_node\\n\\n        Examples\\n        --------\\n        >>> G = nx.Graph()   # or DiGraph, MultiGraph, MultiDiGraph, etc\\n        >>> G.add_path([0,1,2])\\n        >>> e = list(G.nodes())\\n        >>> e\\n        [0, 1, 2]\\n        >>> G.remove_nodes_from(e)\\n        >>> list(G.nodes())\\n        []\\n\\n        '\n    for n in nbunch:\n        try:\n            succs = self.succ[n]\n            del self.node[n]\n            for u in succs:\n                del self.pred[u][n]\n            del self.succ[n]\n            for u in self.pred[n]:\n                del self.succ[u][n]\n            del self.pred[n]\n        except KeyError:\n            pass\n", "label": 0}
{"function": "\n\ndef tweet(sender, instance, created, **kwargs):\n    recipients = set()\n    user = instance.sender\n    user_content_type = ContentType.objects.get_for_model(user)\n    followings = Following.objects.filter(followed_content_type=user_content_type, followed_object_id=user.id)\n    for follower in (following.follower_content_object for following in followings):\n        recipients.add(follower)\n    recipients.add(user)\n    match = reply_re.match(instance.text)\n    if match:\n        try:\n            reply_recipient = User.objects.get(username=match.group(1))\n            recipients.add(reply_recipient)\n        except User.DoesNotExist:\n            pass\n        else:\n            if notification:\n                notification.send([reply_recipient], 'tweet_reply_received', {\n                    'tweet': instance,\n                })\n    for recipient in recipients:\n        tweet_instance = TweetInstance.objects.create(text=instance.text, sender=user, recipient=recipient, sent=instance.sent)\n", "label": 0}
{"function": "\n\ndef import_set(dset, in_stream, headers=True, **kwargs):\n    'Returns dataset from CSV stream.'\n    dset.wipe()\n    kwargs.setdefault('delimiter', DEFAULT_DELIMITER)\n    if (not is_py3):\n        kwargs.setdefault('encoding', DEFAULT_ENCODING)\n    rows = csv.reader(StringIO(in_stream), **kwargs)\n    for (i, row) in enumerate(rows):\n        if ((i == 0) and headers):\n            dset.headers = row\n        else:\n            dset.append(row)\n", "label": 0}
{"function": "\n\ndef clean(self, value):\n    if ((not value) and (not self.required)):\n        return None\n    try:\n        gid = from_global_id(value)\n    except (TypeError, ValueError, UnicodeDecodeError, binascii.Error):\n        raise ValidationError(self.error_messages['invalid'])\n    try:\n        IntegerField().clean(gid.id)\n        CharField().clean(gid.type)\n    except ValidationError:\n        raise ValidationError(self.error_messages['invalid'])\n    return value\n", "label": 0}
{"function": "\n\ndef show(config_file=False):\n    \"\\n    Return a list of sysctl parameters for this minion\\n\\n    config: Pull the data from the system configuration file\\n        instead of the live data.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' sysctl.show\\n    \"\n    ret = {\n        \n    }\n    if config_file:\n        try:\n            for line in salt.utils.fopen(config_file):\n                if ((not line.startswith('#')) and ('=' in line)):\n                    SPLIT = ' = '\n                    if (SPLIT not in line):\n                        SPLIT = SPLIT.strip()\n                    (key, value) = line.split(SPLIT, 1)\n                    key = key.strip()\n                    value = value.lstrip()\n                    ret[key] = value\n        except (OSError, IOError):\n            log.error('Could not open sysctl file')\n            return None\n    else:\n        cmd = 'sysctl -a'\n        out = __salt__['cmd.run_stdout'](cmd, output_loglevel='trace')\n        for line in out.splitlines():\n            if ((not line) or (' = ' not in line)):\n                continue\n            comps = line.split(' = ', 1)\n            ret[comps[0]] = comps[1]\n    return ret\n", "label": 1}
{"function": "\n\ndef settings_get(name, default=None):\n    plugin_settings = sublime.load_settings('TypeScript.sublime-settings')\n    if (sublime.active_window() and sublime.active_window().active_view()):\n        project_settings = sublime.active_window().active_view().settings().get('TypeScript')\n    else:\n        project_settings = {\n            \n        }\n    if (project_settings is None):\n        project_settings = {\n            \n        }\n    setting = project_settings.get(name, plugin_settings.get(name, default))\n    return setting\n", "label": 0}
{"function": "\n\ndef enable(**kwargs):\n    \"\\n    Enable all scheduled jobs on the minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' schedule.enable\\n    \"\n    ret = {\n        'comment': [],\n        'result': True,\n    }\n    if (('test' in kwargs) and kwargs['test']):\n        ret['comment'] = 'Schedule would be enabled.'\n    else:\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({\n                'func': 'enable',\n            }, 'manage_schedule')\n            if res:\n                event_ret = eventer.get_event(tag='/salt/minion/minion_schedule_enabled_complete', wait=30)\n                if (event_ret and event_ret['complete']):\n                    schedule = event_ret['schedule']\n                    if (('enabled' in schedule) and schedule['enabled']):\n                        ret['result'] = True\n                        ret['comment'] = 'Enabled schedule on minion.'\n                    else:\n                        ret['result'] = False\n                        ret['comment'] = 'Failed to enable schedule on minion.'\n                    return ret\n        except KeyError:\n            ret['comment'] = 'Event module not available. Schedule enable job failed.'\n    return ret\n", "label": 1}
{"function": "\n\ndef _kmap_eq(a, b):\n    if ((a is None) and (b is None)):\n        return True\n    if ((a is None) or (b is None)):\n        return False\n    return numpy.all((a == b))\n", "label": 0}
{"function": "\n\ndef test_path_split():\n    expr = (t.amount.sum() + 1)\n    assert path_split(t, expr).isidentical(t.amount.sum())\n    expr = t.amount.distinct().sort()\n    assert path_split(t, expr).isidentical(t.amount.distinct())\n    t2 = transform(t, id=(t.id * 2))\n    expr = (by(t2.id, amount=t2.amount.sum()).amount + 1)\n    assert path_split(t, expr).isidentical(by(t2.id, amount=t2.amount.sum()))\n    expr = count(t.amount.distinct())\n    assert path_split(t, expr).isidentical(t.amount.distinct())\n    expr = summary(total=t.amount.sum())\n    assert path_split(t, expr).isidentical(expr)\n", "label": 0}
{"function": "\n\ndef __init__(self, master=None, **kw):\n    master = _setup_master(master)\n    global _TKTABLE_LOADED\n    if (not _TKTABLE_LOADED):\n        tktable_lib = os.environ.get('TKTABLE_LIBRARY')\n        if tktable_lib:\n            master.tk.eval(('global auto_path; lappend auto_path {%s}' % tktable_lib))\n        master.tk.call('package', 'require', 'Tktable')\n        _TKTABLE_LOADED = True\n    if (not ('padx' in kw)):\n        kw['padx'] = 1\n    if (not ('pady' in kw)):\n        kw['pady'] = 1\n    tkinter.Widget.__init__(self, master, 'table', kw)\n    self.contextMenuClick = ('<Button-2>' if (sys.platform == 'darwin') else '<Button-3>')\n", "label": 0}
{"function": "\n\n@login_required\ndef users(request):\n    context = {\n        \n    }\n    counts = {\n        \n    }\n    for questions in Question.objects.all():\n        email = questions.added_by.email\n        if (email not in counts):\n            counts[email] = 0\n        counts[email] += 1\n    for user in User.objects.all():\n        if (user.email not in counts):\n            counts[user.email] = 0\n    context['counts'] = sorted(counts.items(), key=operator.itemgetter(0))\n    context['counts'] = sorted(context['counts'], key=operator.itemgetter(1), reverse=True)\n    return r2r(request, 'users', context)\n", "label": 0}
{"function": "\n\ndef f_alarm(burglary, earthquake, alarm):\n    table = dict()\n    table['ttt'] = 0.95\n    table['ttf'] = 0.05\n    table['tft'] = 0.94\n    table['tff'] = 0.06\n    table['ftt'] = 0.29\n    table['ftf'] = 0.71\n    table['fft'] = 0.001\n    table['fff'] = 0.999\n    key = ''\n    key = ((key + 't') if burglary else (key + 'f'))\n    key = ((key + 't') if earthquake else (key + 'f'))\n    key = ((key + 't') if alarm else (key + 'f'))\n    return table[key]\n", "label": 0}
{"function": "\n\ndef ensure_timedelta(delta):\n    '\\n    Attempts to convert an object to a time delta.\\n\\n    @rtype\\n      `datetime.timedelta`\\n    '\n    if isinstance(delta, datetime.timedelta):\n        return delta\n    if isinstance(delta, str):\n        match = re.match('(\\\\d+) *(d|h|m|s)', delta)\n        if (match is not None):\n            (num, unit) = match.groups()\n            if (unit == 'd'):\n                return datetime.timedelta(int(num), 0)\n            else:\n                secs = (int(num) * {\n                    'h': 3600,\n                    'm': 60,\n                    's': 1,\n                }.get(unit))\n                return datetime.timedelta(0, secs)\n    raise TypeError('not a timedelta: {!r}'.format(delta))\n", "label": 0}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef spawn_processes(self):\n    'Spawn processes.\\n        '\n    if self.pending_socket_event:\n        self._status = 'stopped'\n        return\n    for i in self._found_wids:\n        self.spawn_process(i)\n        (yield tornado_sleep(0))\n    self._found_wids = {\n        \n    }\n    for i in range((self.numprocesses - len(self.processes))):\n        res = self.spawn_process()\n        if (res is False):\n            (yield self._stop())\n            break\n        delay = self.warmup_delay\n        if isinstance(res, float):\n            delay -= (time.time() - res)\n            if (delay < 0):\n                delay = 0\n        (yield tornado_sleep(delay))\n", "label": 0}
{"function": "\n\n@classmethod\ndef _check_properties(cls, property_names, require_indexed=True):\n    'Internal helper to check the given properties exist and meet specified\\n    requirements.\\n\\n    Called from query.py.\\n\\n    Args:\\n      property_names: List or tuple of property names -- each being a string,\\n        possibly containing dots (to address subproperties of structured\\n        properties).\\n\\n    Raises:\\n      InvalidPropertyError if one of the properties is invalid.\\n      AssertionError if the argument is not a list or tuple of strings.\\n    '\n    assert isinstance(property_names, (list, tuple)), repr(property_names)\n    for name in property_names:\n        assert isinstance(name, basestring), repr(name)\n        if ('.' in name):\n            (name, rest) = name.split('.', 1)\n        else:\n            rest = None\n        prop = cls._properties.get(name)\n        if (prop is None):\n            cls._unknown_property(name)\n        else:\n            prop._check_property(rest, require_indexed=require_indexed)\n", "label": 0}
{"function": "\n\ndef create(self, req, body):\n    'Creates a new security group.'\n    context = req.environ['nova.context']\n    if (not body):\n        return exc.HTTPUnprocessableEntity()\n    security_group = body.get('security_group', None)\n    if (security_group is None):\n        return exc.HTTPUnprocessableEntity()\n    group_name = security_group.get('name', None)\n    group_description = security_group.get('description', None)\n    self._validate_security_group_property(group_name, 'name')\n    self._validate_security_group_property(group_description, 'description')\n    group_name = group_name.strip()\n    group_description = group_description.strip()\n    LOG.audit(_('Create Security Group %s'), group_name, context=context)\n    self.compute_api.ensure_default_security_group(context)\n    if db.security_group_exists(context, context.project_id, group_name):\n        msg = (_('Security group %s already exists') % group_name)\n        raise exc.HTTPBadRequest(explanation=msg)\n    group = {\n        'user_id': context.user_id,\n        'project_id': context.project_id,\n        'name': group_name,\n        'description': group_description,\n    }\n    group_ref = db.security_group_create(context, group)\n    return {\n        'security_group': self._format_security_group(context, group_ref),\n    }\n", "label": 0}
{"function": "\n\ndef main():\n    parser = make_parser()\n    args = parser.parse_args()\n    if args.dump_cfg:\n        print('CFG dump is removed.')\n        sys.exit(1)\n    if args.dump_ast:\n        print('AST dump is removed.  Numba no longer depends on AST.')\n        sys.exit(1)\n    os.environ['NUMBA_DUMP_ANNOTATION'] = str(int(args.annotate))\n    if (args.annotate_html is not None):\n        try:\n            from jinja2 import Template\n        except ImportError:\n            raise ImportError(\"Please install the 'jinja2' package\")\n        os.environ['NUMBA_DUMP_HTML'] = str(args.annotate_html[0])\n    os.environ['NUMBA_DUMP_LLVM'] = str(int(args.dump_llvm))\n    os.environ['NUMBA_DUMP_OPTIMIZED'] = str(int(args.dump_optimized))\n    os.environ['NUMBA_DUMP_ASSEMBLY'] = str(int(args.dump_assembly))\n    cmd = [sys.executable, args.filename]\n    subprocess.call(cmd)\n", "label": 0}
{"function": "\n\ndef test_disabled_hosts_are_skipped(self):\n    active_host_names = ['active1', 'active2']\n    disabled_host_names = ['disabled1', 'disabled2']\n    disabled_hosts = [self._host(name, enabled=False) for name in disabled_host_names]\n    active_hosts = [self._host(name) for name in active_host_names]\n    all_hosts = (active_hosts + disabled_hosts)\n    self.mock_client().services.list.return_value = all_hosts\n    self.mock_client().hosts.list.return_value = all_hosts\n    hosts = self.nova_client.get_compute_hosts()\n    self.assertIsNotNone(hosts)\n    self.assertTrue(isinstance(hosts, list))\n    for active in active_host_names:\n        self.assertIn(active, hosts)\n    for disabled in disabled_host_names:\n        self.assertNotIn(disabled, hosts)\n", "label": 0}
{"function": "\n\ndef grad(self, inputs, gout):\n    (x, y) = inputs\n    (gz,) = gout\n    assert (_is_sparse_variable(x) and _is_dense_variable(y))\n    assert _is_sparse_variable(gz)\n    if ((gz.dtype == 'float64') and (y.dtype == 'float32')):\n        y = y.astype('float64')\n    if ((gz.dtype == 'float32') and (y.dtype == 'float64')):\n        gz = gz.astype('float64')\n    return (mul_s_v(gz, y), sp_sum((x * gz), axis=0, sparse_grad=True))\n", "label": 0}
{"function": "\n\ndef log_request(self, handler):\n    'Writes a completed HTTP request to the logs.\\n\\n        By default writes to the python root logger.  To change\\n        this behavior either subclass Application and override this method,\\n        or pass a function in the application settings dictionary as\\n        ``log_function``.\\n        '\n    if ('log_function' in self.settings):\n        self.settings['log_function'](handler)\n        return\n    if (handler.get_status() < 400):\n        log_method = access_log.info\n    elif (handler.get_status() < 500):\n        log_method = access_log.warning\n    else:\n        log_method = access_log.error\n    request_time = (1000.0 * handler.request.request_time())\n    log_method('%d %s %.2fms', handler.get_status(), handler._request_summary(), request_time)\n", "label": 0}
{"function": "\n\ndef filter_travis(self, entry):\n    'Only show the latest entry iif this entry is in a new state'\n    fstate = (entry.filename + '.state')\n    if os.path.isfile(fstate):\n        with open(fstate) as fd:\n            state = fd.read().strip()\n    else:\n        state = None\n    if ('failed' in entry.summary):\n        nstate = 'failed'\n    else:\n        nstate = 'success'\n    with open(fstate, 'w') as fd:\n        fd.write(nstate)\n    if (state != nstate):\n        build = entry.title.split('#')[1]\n        entry['title'] = 'Build #{0} {1}'.format(build, nstate)\n        return True\n", "label": 0}
{"function": "\n\ndef test_sync_branch_devel(qisrc_action, git_server, test_git):\n    git_server.create_repo('foo.git')\n    qisrc_action('init', git_server.manifest_url)\n    git_server.push_file('foo.git', 'foo.txt', 'a super change')\n    git_server.push_file('foo.git', 'bar.txt', 'a super bugfix')\n    git_worktree = TestGitWorkTree()\n    foo = git_worktree.get_git_project('foo')\n    test_git = TestGit(foo.path)\n    test_git.call('checkout', '-b', 'devel')\n    test_git.commit_file('developing.txt', 'like a boss')\n    git_server.push_file('foo.git', 'foobar.txt', 'some other change')\n    git_server.push_file('foo.git', 'bigchange.txt', 'some huge change')\n    qisrc_action('sync', '--rebase-devel')\n    test_git.call('checkout', 'master')\n    bigchange_txt = os.path.join(foo.path, 'bigchange.txt')\n    assert os.path.exists(bigchange_txt)\n    test_git.call('checkout', 'devel')\n    test_git.call('rebase', 'master')\n    assert os.path.exists(bigchange_txt)\n    developing_txt = os.path.join(foo.path, 'developing.txt')\n    assert os.path.exists(developing_txt)\n", "label": 0}
{"function": "\n\ndef create_queued_job(queue, files, job_url_template, commit_sha, owner, repo, status_url):\n    ' Create a new job, and add its files to the queue.\\n    '\n    filenames = list(files.keys())\n    file_states = {name: None for name in filenames}\n    file_results = {name: None for name in filenames}\n    job_id = calculate_job_id(files)\n    job_url = (job_url_template and expand_uri(job_url_template, dict(id=job_id)))\n    job_status = None\n    with queue as db:\n        task_files = add_files_to_queue(queue, job_id, job_url, files, commit_sha)\n        add_job(db, job_id, None, task_files, file_states, file_results, owner, repo, status_url)\n    return job_id\n", "label": 0}
{"function": "\n\ndef documentation(self, add_to=None, version=None, base_url='', url=''):\n    'Returns the documentation specific to an HTTP interface'\n    doc = (OrderedDict() if (add_to is None) else add_to)\n    usage = self.interface.spec.__doc__\n    if usage:\n        doc['usage'] = usage\n    for example in self.examples:\n        example_text = '{0}{1}{2}'.format(base_url, ('/v{0}'.format(version) if version else ''), url)\n        if isinstance(example, str):\n            example_text += '?{0}'.format(example)\n        doc_examples = doc.setdefault('examples', [])\n        if (not (example_text in doc_examples)):\n            doc_examples.append(example_text)\n    doc = super().documentation(doc)\n    if getattr(self, 'output_doc', ''):\n        doc['outputs']['type'] = self.output_doc\n    return doc\n", "label": 0}
{"function": "\n\n@cache_readonly\ndef fittedvalues(self):\n    model = self.model\n    endog = model.endog.copy()\n    k_ar = self.k_ar\n    exog = model.exog\n    if (exog is not None):\n        if ((model.method == 'css') and (k_ar > 0)):\n            exog = exog[k_ar:]\n    if ((model.method == 'css') and (k_ar > 0)):\n        endog = endog[k_ar:]\n    fv = (endog - self.resid)\n    return fv\n", "label": 0}
{"function": "\n\ndef convert(gr, raw_node):\n    '\\n    Convert raw node information to a Node or Leaf instance.\\n\\n    This is passed to the parser driver which calls it whenever a reduction of a\\n    grammar rule produces a new complete node, so that the tree is build\\n    strictly bottom-up.\\n    '\n    (type, value, context, children) = raw_node\n    if (not children):\n        t = tokens.get(type, None)\n        if (t is not None):\n            return t(value, context)\n    t = symbols.get(type, None)\n    if (t is not None):\n        return t(children, context)\n    try:\n        raise NotImplementedError(grammar.number2symbol[type])\n    except KeyError:\n        raise NotImplementedError(token.tok_name[type])\n", "label": 0}
{"function": "\n\ndef get_package_freq(local_ctx_id, source, prelease):\n    linksets = ReleaseLinkSet.objects.filter(code_reference__local_object_id=local_ctx_id).filter(code_reference__source=source).filter(project_release=prelease).all()\n    packages = defaultdict(int)\n    for linkset in linksets:\n        if (linkset.links.count() == 1):\n            package_name = find_package(linkset.first_link.code_element)\n            if (package_name is not None):\n                packages[package_name] += 1\n    package_freq = [(k, packages[k]) for k in packages]\n    package_freq.sort(key=(lambda v: v[1]), reverse=True)\n    return package_freq\n", "label": 0}
{"function": "\n\ndef _dup_rr_trivial_gcd(f, g, K):\n    'Handle trivial cases in GCD algorithm over a ring. '\n    if (not (f or g)):\n        return ([], [], [])\n    elif (not f):\n        if K.is_nonnegative(dup_LC(g, K)):\n            return (g, [], [K.one])\n        else:\n            return (dup_neg(g, K), [], [(- K.one)])\n    elif (not g):\n        if K.is_nonnegative(dup_LC(f, K)):\n            return (f, [K.one], [])\n        else:\n            return (dup_neg(f, K), [(- K.one)], [])\n    return None\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.stdout.write('sendalerts is now running')\n    ticks = 0\n    while True:\n        if self.handle_many():\n            ticks = 1\n        else:\n            ticks += 1\n        time.sleep(1)\n        if ((ticks % 60) == 0):\n            formatted = timezone.now().isoformat()\n            self.stdout.write(('-- MARK %s --' % formatted))\n", "label": 0}
{"function": "\n\n@classmethod\ndef sqlall(cls):\n    queries = []\n    compiler = cls._meta.database.compiler()\n    pk = cls._meta.primary_key\n    if (cls._meta.database.sequences and pk.sequence):\n        queries.append(compiler.create_sequence(pk.sequence))\n    queries.append(compiler.create_table(cls))\n    for field in cls._fields_to_index():\n        queries.append(compiler.create_index(cls, [field], field.unique))\n    if cls._meta.indexes:\n        for (field_names, unique) in cls._meta.indexes:\n            fields = [cls._meta.fields[f] for f in field_names]\n            queries.append(compiler.create_index(cls, fields, unique))\n    return [sql for (sql, _) in queries]\n", "label": 0}
{"function": "\n\ndef wait_for_erlang_prompt(self):\n    prompted = False\n    buffer = ''\n    while (not prompted):\n        line = self._server.stdout.readline()\n        if (len(line) > 0):\n            buffer += line\n        if re.search(('\\\\(%s\\\\)\\\\d+>' % self.vm_args['-name']), buffer):\n            prompted = True\n        if re.search('\"Kernel pid terminated\".*\\\\n', buffer):\n            raise Exception('Riak test server failed to start.')\n", "label": 0}
{"function": "\n\ndef generate_atom(self, event, event_type, content, content_type, categories=None, title=None):\n    template = '<atom:entry xmlns:atom=\"http://www.w3.org/2005/Atom\"><atom:id>urn:uuid:%(message_id)s</atom:id>%(categories)s<atom:title type=\"text\">%(title)s</atom:title><atom:content type=\"%(content_type)s\">%(content)s</atom:content></atom:entry>'\n    if (title is None):\n        title = event_type\n    if (categories is None):\n        cats = []\n    else:\n        cats = categories[:]\n    cats.append(event_type)\n    original_message_id = event.get('original_message_id')\n    if (original_message_id is not None):\n        cats.append(('original_message_id:%s' % original_message_id))\n    cattags = ''.join((('<atom:category term=\"%s\" />' % cat) for cat in cats))\n    info = dict(message_id=event.get('message_id'), original_message_id=original_message_id, event=event, event_type=event_type, content=content, categories=cattags, title=title, content_type=content_type)\n    return (template % info)\n", "label": 0}
{"function": "\n\ndef skip_reset(self):\n    outf = util.StringIO()\n    cm = pycurl.CurlMulti()\n    cm.setopt(pycurl.M_PIPELINING, 1)\n    eh = pycurl.Curl()\n    for x in range(1, 20):\n        eh.setopt(pycurl.WRITEFUNCTION, outf.write)\n        eh.setopt(pycurl.URL, 'http://localhost:8380/success')\n        cm.add_handle(eh)\n        while 1:\n            (ret, active_handles) = cm.perform()\n            if (ret != pycurl.E_CALL_MULTI_PERFORM):\n                break\n        while active_handles:\n            ret = cm.select(1.0)\n            if (ret == (- 1)):\n                continue\n            while 1:\n                (ret, active_handles) = cm.perform()\n                if (ret != pycurl.E_CALL_MULTI_PERFORM):\n                    break\n        (count, good, bad) = cm.info_read()\n        for (h, en, em) in bad:\n            print(('Transfer to %s failed with %d, %s\\n' % (h.getinfo(pycurl.EFFECTIVE_URL), en, em)))\n            raise RuntimeError\n        for h in good:\n            httpcode = h.getinfo(pycurl.RESPONSE_CODE)\n            if (httpcode != 200):\n                print(('Transfer to %s failed with code %d\\n' % (h.getinfo(pycurl.EFFECTIVE_URL), httpcode)))\n                raise RuntimeError\n            else:\n                print(('Recd %d bytes from %s' % (h.getinfo(pycurl.SIZE_DOWNLOAD), h.getinfo(pycurl.EFFECTIVE_URL))))\n        cm.remove_handle(eh)\n        eh.reset()\n    eh.close()\n    cm.close()\n    outf.close()\n", "label": 1}
{"function": "\n\ndef _ensure_index(index_like, copy=False):\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, 'name'):\n        return Index(index_like, name=index_like.name, copy=copy)\n    if isinstance(index_like, list):\n        if (type(index_like) != list):\n            index_like = list(index_like)\n        (converted, all_arrays) = lib.clean_index_list(index_like)\n        if ((len(converted) > 0) and all_arrays):\n            from .multi import MultiIndex\n            return MultiIndex.from_arrays(converted)\n        else:\n            index_like = converted\n    elif copy:\n        from copy import copy\n        index_like = copy(index_like)\n    return Index(index_like)\n", "label": 1}
{"function": "\n\ndef create(self, name, metadata=None, prefix=None, *args, **kwargs):\n    '\\n        Creates a new container, and returns a Container object that represents\\n        that contianer. If a container by the same name already exists, no\\n        exception is raised; instead, a reference to that existing container is\\n        returned.\\n        '\n    uri = ('/%s' % name)\n    headers = {\n        \n    }\n    if (prefix is None):\n        prefix = CONTAINER_META_PREFIX\n    if metadata:\n        metadata = _massage_metakeys(metadata, prefix)\n        headers = metadata\n    (resp, resp_body) = self.api.method_put(uri, headers=headers)\n    if (resp.status_code in (201, 202)):\n        (hresp, hresp_body) = self.api.method_head(uri)\n        num_obj = int(hresp.headers.get('x-container-object-count', '0'))\n        num_bytes = int(hresp.headers.get('x-container-bytes-used', '0'))\n        cont_info = {\n            'name': name,\n            'object_count': num_obj,\n            'total_bytes': num_bytes,\n        }\n        return Container(self, cont_info)\n    elif (resp.status_code == 400):\n        raise exc.ClientException(('Container creation failed: %s' % resp_body))\n", "label": 0}
{"function": "\n\n@property\ndef addresses(self):\n    if (self._addresses is None):\n        self._addresses = tuple([address for group in self._groups for address in group.addresses])\n    return self._addresses\n", "label": 0}
{"function": "\n\ndef normalize(self, obj):\n    '\\n        Runs an object through the normalizer mechanism, with the goal of producing a value consisting only of \"native types\" (:obj:`unicode`, :obj:`int`, :obj:`long`, :obj:`float`, :obj:`dict`, :obj:`list`, etc).\\n\\n        The resolution order looks like this:\\n\\n        - Loop through :attr:`self.normalizer_overrides[type(obj)] <normalizer_overrides>` (taking parent classes into account), should be a callable taking (obj, pushrod), falls through on :obj:`NotImplemented`\\n        - :attr:`self.normalizers[type(obj)] <normalizers>` (taking parent classes into account), should be a callable taking (obj, pushrod), falls through on :obj:`NotImplemented`\\n\\n        See :ref:`bundled-normalizers` for all default normalizers.\\n\\n        :param obj: The object to normalize.\\n        '\n    for cls in type(obj).__mro__:\n        for override in self.normalizer_overrides[cls]:\n            attempt = override(obj, self)\n            if (attempt is not NotImplemented):\n                return attempt\n    attempt = normalizers.normalize_object(obj, self)\n    if (attempt is not NotImplemented):\n        return attempt\n    for cls in type(obj).__mro__:\n        if (cls in self.normalizers):\n            attempt = self.normalizers[cls](obj, self)\n            if (attempt is not NotImplemented):\n                return attempt\n    return NotImplemented\n", "label": 0}
{"function": "\n\ndef width(self):\n    'Return the total width of the operation.'\n    if (not self._tasks):\n        return 0\n    task = self._tasks[(- 1)]\n    if (len(self._tasks) > 1):\n        return (task[3] * (task[1] or 1))\n    else:\n        return task[1]\n", "label": 0}
{"function": "\n\ndef remove(self, permission, user_or_group, obj=None):\n    '\\n        Removes the given permission from the given user or group, and returns\\n        the deleted AssignedPermission instance or None if not found.\\n        \\n        If an object is given, removes the permission for that object.\\n        '\n    (perm, ct) = self.get_permission_and_content_type(permission, obj)\n    assigned_dict = {\n        'permission': perm,\n        'content_type': ct,\n        'object_id': None,\n    }\n    if obj:\n        assigned_dict['object_id'] = obj.id\n    if isinstance(user_or_group, User):\n        assigned_dict['user'] = user_or_group\n    elif isinstance(user_or_group, Group):\n        assigned_dict['group'] = user_or_group\n    else:\n        raise TypeError('Permissions can only be removed from a User or Group instance.')\n    try:\n        assigned = AssignedPermission.objects.get(**assigned_dict)\n    except AssignedPermission.DoesNotExist:\n        return None\n    else:\n        assigned.delete()\n        return assigned\n", "label": 0}
{"function": "\n\ndef setup(self):\n    self.ER_G = zn.Graph()\n    for i in range(self.NUM_NODES):\n        self.ER_G.add_node(i)\n    for i in range(self.NUM_NODES):\n        for j in range((i + 1), self.NUM_NODES):\n            r = random.random()\n            w = random.random()\n            if (r < self.P):\n                w = math.ceil((w * 10.0))\n                self.ER_G.add_edge_(i, j, weight=w)\n                self.weights.append(w)\n", "label": 0}
{"function": "\n\ndef start_unite(state):\n    (state, old_state) = state_logic(state)\n    if (not state.buff):\n        state.buff = make_pyunite_buffer(state)\n    if (state.options['close_on_empty'] and (not len(state.source.candidates))):\n        return\n    saved = vim.current.window\n    window_logic(state, old_state)\n    set_buffer_syntax(state)\n    if (not state.options['focus_on_open']):\n        ui.change_window(saved, autocmd=True)\n    states.add(state)\n", "label": 0}
{"function": "\n\ndef getSubtag(self, name=None):\n    'determine whether current tag contains other tags, and returns\\n        the tag with a matching name (if name is given) or None (if not)'\n    if ('Icontain' in self.tag):\n        for subtag in self.tag['Icontain']:\n            if (subtag.name == name):\n                return subtag\n    return None\n", "label": 0}
{"function": "\n\ndef _base_notification(self, exists):\n    basen = exists.copy()\n    if ('bandwidth_in' not in basen):\n        basen['bandwidth_in'] = 0\n    if ('bandwidth_out' not in basen):\n        basen['bandwidth_out'] = 0\n    if ('rax_options' not in basen):\n        basen['rax_options'] = '0'\n    basen['original_message_id'] = exists.get('message_id', '')\n    return basen\n", "label": 0}
{"function": "\n\ndef merge(iterables):\n    'Merge multiple sorted inputs into a single sorted output.\\n\\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\\n    does not pull the data into memory all at once, and assumes that each of\\n    the input streams is already sorted (smallest to largest).\\n    '\n    (_heappop, _heapreplace, _StopIteration) = (heappop, heapreplace, StopIteration)\n    h = []\n    h_append = h.append\n    for (i, it) in enumerate(map(iter, iterables)):\n        try:\n            next = (it.next if hasattr(it, 'next') else it.__next__)\n            h_append([next(), i, next])\n        except _StopIteration:\n            pass\n    heapify(h)\n    while 1:\n        try:\n            while 1:\n                (v, i, next) = s = h[0]\n                s[0] = next()\n                (yield v)\n                _heapreplace(h, s)\n        except _StopIteration:\n            _heappop(h)\n            (yield v)\n        except IndexError:\n            return\n", "label": 0}
{"function": "\n\ndef test_data_removal_removes_subsets(self):\n    self.client.add_layer(self.data)\n    self.client.remove_layer(self.data)\n    self.data.new_subset()\n    assert (len(self.data.subsets) > 0)\n    for subset in self.data.subsets:\n        assert (not self.layer_present(subset))\n", "label": 0}
{"function": "\n\ndef get_output_fmt(output_param):\n    output_class = output_param['class']\n    output_fmt = ''\n    if output_class.startswith('array'):\n        item_class = output_class[6:]\n        if (item_class in class_to_dxclass):\n            output_fmt = (('[dxpy.dxlink(item) for item in ' + output_param['name']) + ']')\n        else:\n            output_fmt = output_param['name']\n    elif (output_class in class_to_dxclass):\n        output_fmt = (('dxpy.dxlink(' + output_param['name']) + ')')\n    else:\n        output_fmt = output_param['name']\n    return output_fmt\n", "label": 0}
{"function": "\n\ndef kicktire(linklist):\n    try:\n        linklist = eval(linklist)\n        for alink in linklist:\n            if ('tech' in alink):\n                return alink\n        for alink in linklist:\n            if ('business' in alink):\n                return alink\n        return '-'\n    except:\n        pass\n", "label": 0}
{"function": "\n\ndef get_match_children(self):\n    if (self.match_children is None):\n        self.match_children = []\n        for route in self.get_children():\n            for r in route.get_match_routes():\n                self.match_children.append(r)\n    for rv in self.match_children:\n        (yield rv)\n", "label": 0}
{"function": "\n\n@defun_wrapped\ndef legendre(ctx, n, x, **kwargs):\n    if ctx.isint(n):\n        n = int(n)\n        if ((n + (n < 0)) & 1):\n            if (not x):\n                return x\n            mag = ctx.mag(x)\n            if (mag < (((- 2) * ctx.prec) - 10)):\n                return x\n            if (mag < (- 5)):\n                ctx.prec += (- mag)\n    return ctx.hyp2f1((- n), (n + 1), 1, ((1 - x) / 2), **kwargs)\n", "label": 0}
{"function": "\n\n@webapi.route('/state.txt', methods=['GET'])\n@log_application_errors\ndef app_get_state_txt():\n    '\\n    '\n    with db_connect(current_app.config['DATABASE_URL']) as conn:\n        with db_cursor(conn) as db:\n            set = read_latest_set(db, 'openaddresses', 'openaddresses')\n            runs = read_completed_runs_to_date(db, set.id)\n    buffer = csvIO()\n    output = csvDictWriter(buffer, CSV_HEADER, dialect='excel-tab', encoding='utf8')\n    output.writerow({col: col for col in CSV_HEADER})\n    for run in sorted(runs, key=attrgetter('source_path')):\n        run_state = (run.state or {\n            \n        })\n        row = {col: run_state.get(col, None) for col in CSV_HEADER}\n        row['source'] = os.path.relpath(run.source_path, 'sources')\n        row['code version'] = run.code_version\n        row['cache'] = nice_domain(row['cache'])\n        row['sample'] = nice_domain(row['sample'])\n        row['processed'] = nice_domain(row['processed'])\n        row['output'] = nice_domain(row['output'])\n        output.writerow(row)\n    return Response(buffer.getvalue(), headers={\n        'Content-Type': 'text/plain; charset=utf8',\n    })\n", "label": 0}
{"function": "\n\ndef _replacement(self, node):\n    if (isinstance(node.left, ast.Constant) and (node.left.value == False)):\n        return node.right\n    if (isinstance(node.right, ast.Constant) and (node.right.value == False)):\n        return node.left\n    return None\n", "label": 0}
{"function": "\n\ndef testserver(self, server):\n    if server.ssl:\n        prefix = 'https://'\n    else:\n        prefix = 'http://'\n    try:\n        url = (prefix + ('%s' % server.serverdomain))\n        result = urlfetch.fetch(url, headers={\n            'Cache-Control': 'max-age=30',\n        }, deadline=10)\n    except DownloadError:\n        if server.falsepositivecheck:\n            self.serverisdown(server, 0)\n        else:\n            server.falsepositivecheck = True\n            server.put()\n    else:\n        if (result.status_code == 500):\n            self.serverisdown(server, result.status_code)\n        else:\n            self.serverisup(server, result.status_code)\n", "label": 0}
{"function": "\n\ndef use(self, player, game):\n    super().use(player, game)\n    if ((self.target.health <= player.effective_spell_damage(2)) and (isinstance(self.target, Minion) and (not self.target.divine_shield))):\n        self.target.damage(player.effective_spell_damage(2), self)\n        demons = CollectionSource([IsType(MINION_TYPE.DEMON)])\n        demons.get_card(player, player, self).summon(player, game, len(player.minions))\n    else:\n        self.target.damage(player.effective_spell_damage(2), self)\n", "label": 0}
{"function": "\n\ndef pagination_range(first, last, current):\n    r = []\n    r.append(first)\n    if ((first + 1) < last):\n        r.append((first + 1))\n    if (((current - 2) > first) and ((current - 2) < last)):\n        r.append((current - 2))\n    if (((current - 1) > first) and ((current - 1) < last)):\n        r.append((current - 1))\n    if ((current > first) and (current < last)):\n        r.append(current)\n    if (((current + 1) < last) and ((current + 1) > first)):\n        r.append((current + 1))\n    if (((current + 2) < last) and ((current + 2) > first)):\n        r.append((current + 2))\n    if ((last - 1) > first):\n        r.append((last - 1))\n    r.append(last)\n    r = list(set(r))\n    r.sort()\n    prev = 10000\n    for e in r[:]:\n        if ((prev + 1) < e):\n            try:\n                r.insert(r.index(e), '...')\n            except ValueError:\n                pass\n        prev = e\n    return r\n", "label": 1}
{"function": "\n\ndef __init__(self, repo, review_branch, review_hash, tracking_branch, tracking_hash, lander, repo_name, browse_url=None):\n    'Create a new relationship tracker for the supplied branch names.\\n\\n        :repo: a callable supporting git commands, e.g. repo(\"status\")\\n        :review_branch: the abdt_gittypes.GitReviewBranch\\n        :review_hash: the commit hash of the branch or None\\n        :tracking_branch: the abdt_gittypes.GitWorkingBranch\\n        :tracking_hash: the commit hash of the branch or None\\n        :lander: a lander conformant to abdt_lander\\n        :repo_name: a short string to identify the repo to humans\\n        :browse_url: a URL to browse the branch or repo (may be None)\\n\\n        '\n    self._repo = repo\n    self._review_branch = review_branch\n    self._review_hash = review_hash\n    self._tracking_branch = tracking_branch\n    self._tracking_hash = tracking_hash\n    self._lander = lander\n    assert self._review_branch_valid_or_none()\n    assert self._tracking_branch_valid_or_none()\n    self._repo_name = repo_name\n    self._browse_url = browse_url\n    assert (self._repo_name is not None)\n", "label": 0}
{"function": "\n\ndef __init__(self, text='Enter object label', parent=None, listItem=None):\n    super(LabelDialog, self).__init__(parent)\n    self.edit = QLineEdit()\n    self.edit.setText(text)\n    self.edit.setValidator(labelValidator())\n    self.edit.editingFinished.connect(self.postProcess)\n    layout = QVBoxLayout()\n    layout.addWidget(self.edit)\n    self.buttonBox = bb = BB((BB.Ok | BB.Cancel), Qt.Horizontal, self)\n    bb.button(BB.Ok).setIcon(newIcon('done'))\n    bb.button(BB.Cancel).setIcon(newIcon('undo'))\n    bb.accepted.connect(self.validate)\n    bb.rejected.connect(self.reject)\n    layout.addWidget(bb)\n    if ((listItem is not None) and (len(listItem) > 0)):\n        self.listWidget = QListWidget(self)\n        for item in listItem:\n            self.listWidget.addItem(item)\n        self.listWidget.itemDoubleClicked.connect(self.listItemClick)\n        layout.addWidget(self.listWidget)\n    self.setLayout(layout)\n", "label": 0}
{"function": "\n\ndef getEdgeTop(self, pp, symbol, y, onY2):\n    if onY2:\n        yMin = pp.getY2Max()\n        yMax = pp.getY2Min()\n    else:\n        yMin = pp.getYMax()\n        yMax = pp.getYMin()\n    yMid = symbol.getBaseline()\n    if (Double.NaN == yMid):\n        yMid = ((yMin + yMax) / 2.0)\n    yMinPx = pp.yToPixel(yMin, onY2)\n    yMaxPx = pp.yToPixel(yMax, onY2)\n    yMidPx = pp.yToPixel(yMid, onY2)\n    yPx = pp.yToPixel(y, onY2)\n    prevYPx = Double.NaN\n    nextYPx = Double.NaN\n    height = symbol.getHeight(pp, onY2)\n    symHeight = self.getAdjustedHeight(height, yPx, prevYPx, nextYPx, yMinPx, yMaxPx, yMidPx)\n    if (Double.NaN == symHeight):\n        return Double.NaN\n    yTop = self.getUpperLeftY(height, yPx, prevYPx, nextYPx, yMinPx, yMaxPx, yMidPx, pp.getYMousePlotArea())\n    if (Double.NaN == yTop):\n        return Double.NaN\n    result = yTop\n    return result\n", "label": 0}
{"function": "\n\ndef testReadArray(self):\n    stream = StringIO('[0,1,2,3,4,[0,1,2,3,4,[0,1,2,3,4]],[0,1,2,3,4]]')\n    reader = pulljson.JSONPullParser(stream)\n    arr = reader.readArray()\n    self.assertEqual(len(arr), 7)\n    for i in range(0, 5):\n        self.assertEqual(arr[i], i)\n    for i in range(0, 5):\n        self.assertEqual(arr[5][i], i)\n    for i in range(0, 5):\n        self.assertEqual(arr[5][5][i], i)\n    for i in range(0, 5):\n        self.assertEqual(arr[6][i], i)\n", "label": 0}
{"function": "\n\ndef notify_topic_subscribers(post):\n    topic = post.topic\n    if (post != topic.head):\n        old_lang = translation.get_language()\n        delete_url = reverse('pybb:delete_subscription', args=[post.topic.id])\n        current_site = Site.objects.get_current()\n        from_email = settings.DEFAULT_FROM_EMAIL\n        subject = render_to_string('pybb/mail_templates/subscription_email_subject.html', {\n            'site': current_site,\n            'post': post,\n        })\n        subject = ''.join(subject.splitlines())\n        mails = tuple()\n        for user in topic.subscribers.exclude(pk=post.user.pk):\n            try:\n                validate_email(user.email)\n            except:\n                continue\n            if (user.email == ('%s@example.com' % getattr(user, compat.get_username_field()))):\n                continue\n            lang = (util.get_pybb_profile(user).language or settings.LANGUAGE_CODE)\n            translation.activate(lang)\n            message = render_to_string('pybb/mail_templates/subscription_email_body.html', {\n                'site': current_site,\n                'post': post,\n                'delete_url': delete_url,\n                'user': user,\n            })\n            mails += ((subject, message, from_email, [user.email]),)\n        send_mass_mail(mails, fail_silently=True)\n        translation.activate(old_lang)\n", "label": 0}
{"function": "\n\ndef childConnectionLost(self, fd, reason):\n    if self.disconnected:\n        return\n    if (reason.value.__class__ == error.ConnectionDone):\n        if (fd == 'read'):\n            self._readConnectionLost(reason)\n        else:\n            self._writeConnectionLost(reason)\n    else:\n        self.connectionLost(reason)\n", "label": 0}
{"function": "\n\ndef score(self, X, Y):\n    'Compute score as 1 - loss over whole data set.\\n\\n        Returns the average accuracy (in terms of model.loss)\\n        over X and Y.\\n\\n        Parameters\\n        ----------\\n        X : iterable\\n            Evaluation data.\\n\\n        Y : iterable\\n            True labels.\\n\\n        Returns\\n        -------\\n        score : float\\n            Average of 1 - loss over training examples.\\n        '\n    if hasattr(self.model, 'batch_loss'):\n        losses = self.model.batch_loss(Y, self.model.batch_inference(X, self.w))\n    else:\n        losses = [self.model.loss(y, self.model.inference(y, self.w)) for (y, y_pred) in zip(Y, self.predict(X))]\n    max_losses = [self.model.max_loss(y) for y in Y]\n    return (1.0 - (np.sum(losses) / float(np.sum(max_losses))))\n", "label": 0}
{"function": "\n\ndef _flush_notifications(self):\n    'Flush notifications of engine registrations waiting\\n        in ZMQ queue.'\n    (idents, msg) = self.session.recv(self._notification_socket, mode=zmq.NOBLOCK)\n    while (msg is not None):\n        if self.debug:\n            pprint(msg)\n        msg_type = msg['msg_type']\n        handler = self._notification_handlers.get(msg_type, None)\n        if (handler is None):\n            raise Exception(('Unhandled message type: %s' % msg.msg_type))\n        else:\n            handler(msg)\n        (idents, msg) = self.session.recv(self._notification_socket, mode=zmq.NOBLOCK)\n", "label": 0}
{"function": "\n\ndef delete_instance(self, recursive=False, delete_nullable=False):\n    if recursive:\n        dependencies = self.dependencies(delete_nullable)\n        for (query, fk) in reversed(list(dependencies)):\n            model = fk.model_class\n            if (fk.null and (not delete_nullable)):\n                model.update(**{\n                    fk.name: None,\n                }).where(query).execute()\n            else:\n                model.delete().where(query).execute()\n    return self.delete().where(self._pk_expr()).execute()\n", "label": 0}
{"function": "\n\ndef interpreter_from_options(options):\n    interpreter = None\n    if options.python:\n        if os.path.exists(options.python):\n            interpreter = PythonInterpreter.from_binary(options.python)\n        else:\n            interpreter = PythonInterpreter.from_env(options.python)\n        if (interpreter is None):\n            die(('Failed to find interpreter: %s' % options.python))\n    else:\n        interpreter = PythonInterpreter.get()\n    with TRACER.timed(('Setting up interpreter %s' % interpreter.binary), V=2):\n        resolve = functools.partial(resolve_interpreter, options.interpreter_cache_dir, options.repos)\n        interpreter = resolve(interpreter, SETUPTOOLS_REQUIREMENT)\n        if (interpreter and options.use_wheel):\n            interpreter = resolve(interpreter, WHEEL_REQUIREMENT)\n        return interpreter\n", "label": 0}
{"function": "\n\ndef _increment_sparse_diagonal_precision(X, mean_vector, covariances, n, graph, n_features, n_features_per_vertex, dtype=np.float32, n_components=None, bias=0, verbose=False):\n    all_blocks = np.zeros((graph.n_vertices, n_features_per_vertex, n_features_per_vertex), dtype=dtype)\n    columns = np.zeros(graph.n_vertices)\n    rows = np.zeros(graph.n_vertices)\n    if verbose:\n        vertices = print_progress(range(graph.n_vertices), n_items=graph.n_vertices, prefix='Precision per vertex', end_with_newline=False)\n    else:\n        vertices = range(graph.n_vertices)\n    for v in vertices:\n        i_from = (v * n_features_per_vertex)\n        i_to = ((v + 1) * n_features_per_vertex)\n        edge_data = X[:, i_from:i_to]\n        m = mean_vector[i_from:i_to]\n        (_, covariances[v]) = _increment_multivariate_gaussian_cov(edge_data, m, covariances[v], n, bias=bias)\n        all_blocks[v] = _covariance_matrix_inverse(covariances[v], n_components)\n        rows[v] = v\n        columns[v] = v\n    rows_arg_sort = rows.argsort()\n    columns = columns[rows_arg_sort]\n    all_blocks = all_blocks[rows_arg_sort]\n    rows = rows[rows_arg_sort]\n    n_rows = graph.n_vertices\n    indptr = np.zeros((n_rows + 1))\n    for i in range(n_rows):\n        (inds,) = np.where((rows == i))\n        if (inds.size == 0):\n            indptr[(i + 1)] = indptr[i]\n        else:\n            indptr[i] = inds[0]\n            indptr[(i + 1)] = (inds[(- 1)] + 1)\n    return (bsr_matrix((all_blocks, columns, indptr), shape=(n_features, n_features), dtype=dtype), covariances)\n", "label": 0}
{"function": "\n\ndef clean_history(self, current_project_only):\n    if current_project_only:\n        self.__clean_history(self.get_current_project_key())\n    else:\n        orphan_list = []\n        open_projects = [self.get_project_key(window) for window in sublime.windows()]\n        for project_key in self.history:\n            if (is_ST2 or (project_key == 'global') or os.path.exists(project_key) or (project_key in open_projects)):\n                self.__clean_history(project_key)\n            else:\n                orphan_list.append(project_key)\n        for project_key in orphan_list:\n            self.debug(('Removing orphaned project \"%s\" from the history' % project_key))\n            del self.history[project_key]\n    self.__save_history()\n", "label": 1}
{"function": "\n\ndef update(self, data):\n    'Set the value of user fields. The field types will be the same.\\n\\n        data ... dict, with field name as key, field value as value\\n\\n        Returns None\\n\\n        '\n    self.loaddoc()\n    all_fields = self.document.getElementsByType(UserFieldDecl)\n    for f in all_fields:\n        field_name = f.getAttribute('name')\n        if (field_name in data):\n            value_type = f.getAttribute('valuetype')\n            value = data.get(field_name)\n            if (value_type == 'string'):\n                f.setAttribute('stringvalue', value)\n            else:\n                f.setAttribute('value', value)\n    self.savedoc()\n", "label": 0}
{"function": "\n\ndef clean_choices(self):\n    choices = self.cleaned_data['choices']\n    if (not self.poll.is_multiple_choice):\n        return choices\n    if (len(choices) > self.poll.choice_max):\n        raise forms.ValidationError((_('Too many selected choices. Limit is %s') % self.poll.choice_max))\n    if (len(choices) < self.poll.choice_min):\n        raise forms.ValidationError((_('Too few selected choices. Minimum is %s') % self.poll.choice_min))\n    return choices\n", "label": 0}
{"function": "\n\ndef __init__(self, layers, name=None, alphas=None):\n    super(Tree, self).__init__(name=name)\n    self.layers = []\n    for l in layers:\n        if isinstance(l, Sequential):\n            self.layers.append(l)\n        elif isinstance(l, list):\n            self.layers.append(Sequential(l))\n        elif isinstance(l, Layer):\n            self.layers.append(Sequential([l]))\n        else:\n            ValueError('Incompatible element for Tree container')\n    self.alphas = ([1.0 for _ in self.layers] if (alphas is None) else alphas)\n    self.betas = []\n    next_root = None\n    for l in reversed(self.layers):\n        root = l.layers[0]\n        beta = (1.0 if ((root is next_root) or (type(root) is not BranchNode)) else 0.0)\n        next_root = root\n        self.betas.append(beta)\n    self.betas.reverse()\n", "label": 1}
{"function": "\n\ndef __set__(self, instance, value):\n    if isinstance(value, self.rel_model):\n        instance._data[self.att_name] = getattr(value, self.field.to_field.name)\n        instance._obj_cache[self.att_name] = value\n    else:\n        orig_value = instance._data.get(self.att_name)\n        instance._data[self.att_name] = value\n        if ((orig_value != value) and (self.att_name in instance._obj_cache)):\n            del instance._obj_cache[self.att_name]\n    instance._dirty.add(self.att_name)\n", "label": 0}
{"function": "\n\n@util.debuglog\ndef send_signal_process(self, process, signum):\n    'Send the signum signal to the process\\n\\n        The signal is sent to the process itself then to all the children\\n        '\n    children = None\n    try:\n        children = process.children()\n        self.send_signal(process.pid, signum)\n        self.notify_event('kill', {\n            'process_pid': process.pid,\n            'time': time.time(),\n        })\n    except NoSuchProcess:\n        if (children is None):\n            return\n    for child_pid in children:\n        try:\n            process.send_signal_child(child_pid, signum)\n            self.notify_event('kill', {\n                'process_pid': child_pid,\n                'time': time.time(),\n            })\n        except NoSuchProcess:\n            pass\n", "label": 0}
{"function": "\n\ndef _iterate(self, element):\n    elements = [element]\n    while True:\n        try:\n            new_element = self.queue.get_nowait()\n            elements.append(new_element)\n        except Queue.Empty:\n            break\n    if (not self.stopped):\n        execute = True\n        with self.period_lock:\n            if ((time.time() - self._latest_update) <= self.next_period_between_updates):\n                execute = False\n            else:\n                self._latest_update = time.time()\n                self._update_period_between_updates()\n        if execute:\n            try:\n                self.scheduler.update()\n            except:\n                log.log(SchedulerTransactionsSynchronizer, log.level.Critical, 'Exception updating scheduler')\n                log.log_exc(SchedulerTransactionsSynchronizer, log.level.Critical)\n    self._notify_elements(elements)\n", "label": 0}
{"function": "\n\ndef type_cmp(a, b):\n    'Python 2 style comparison based on type'\n    (ta, tb) = (type(a).__name__, type(b).__name__)\n    if (ta == 'str'):\n        ta = 'unicode'\n    if (tb == 'str'):\n        tb = 'unicode'\n    if (ta > tb):\n        return 1\n    elif (ta < tb):\n        return (- 1)\n    else:\n        return 0\n", "label": 0}
{"function": "\n\ndef test_allQueueSendIntentsAreSentOnEnoughPendingCallbackCompletions(self):\n    numExtras = len(self.extraTransmitIds)\n    self.resetCounters()\n    self.test_sendIntentToTransportUpToLimitAndThenQueueInternally()\n    self.assertTrue((MAX_PENDING_TRANSMITS > numExtras))\n    for _extras in range(numExtras):\n        self.testTrans.forTestingCompleteAPendingIntent(SendStatus.Sent)\n    self.assertEqual(self.successCBcalls, numExtras)\n    self.assertEqual((MAX_PENDING_TRANSMITS + numExtras), len(self.testTrans.intents))\n    self.assertEqual(numExtras, len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n", "label": 0}
{"function": "\n\n@classmethod\ndef recognizes(cls, params):\n    if isinstance(params, six.string_types):\n        return False\n    try:\n        return all((isinstance(p, six.string_types) for p in params))\n    except TypeError:\n        return False\n", "label": 0}
{"function": "\n\ndef __get__(self, obj, type=None):\n    '\\n        This accessor retrieves the geometry, initializing it using the geometry\\n        class specified during initialization and the HEXEWKB value of the field.  \\n        Currently, only GEOS or OGR geometries are supported.\\n        '\n    geom_value = obj.__dict__[self._field.attname]\n    if isinstance(geom_value, self._klass):\n        geom = geom_value\n    elif ((geom_value is None) or (geom_value == '')):\n        geom = None\n    else:\n        geom = self._klass(geom_value)\n        setattr(obj, self._field.attname, geom)\n    return geom\n", "label": 0}
{"function": "\n\ndef _get_pdiff(example, diff):\n    (test_png, ref_png, diff_png) = get_example_pngs(example, diff)\n    info(('generated image: ' + test_png))\n    retrieved_reference_image = _get_reference_image_from_s3(example, diff)\n    if retrieved_reference_image:\n        ref_png_path = dirname(ref_png)\n        if (not exists(ref_png_path)):\n            os.makedirs(ref_png_path)\n        with open(ref_png, 'wb') as f:\n            f.write(retrieved_reference_image)\n        info(('saved reference: ' + ref_png))\n        code = process_image_diff(diff_png, test_png, ref_png)\n        if (code != 0):\n            warn('generated and reference images differ')\n            warn(('diff: ' + diff_png))\n        else:\n            ok('generated and reference images match')\n", "label": 0}
{"function": "\n\ndef _create_navigator(self, response, raise_exc=True):\n    'Create the appropriate navigator from an api response'\n    method = response.request.method\n    if ((method in (POST, PUT, PATCH, DELETE)) and (response.status_code in (http_client.CREATED, http_client.FOUND, http_client.SEE_OTHER, http_client.NO_CONTENT)) and ('Location' in response.headers)):\n        nav = HALNavigator(link=Link(uri=response.headers['Location']), core=self._core)\n    elif (method in (POST, PUT, PATCH, DELETE)):\n        nav = OrphanHALNavigator(link=None, core=self._core, response=response, parent=self)\n        nav._ingest_response(response)\n    elif (method == GET):\n        nav = self\n        nav._ingest_response(response)\n    else:\n        assert False, \"This shouldn't happen\"\n    return nav\n", "label": 0}
{"function": "\n\ndef __get_ll_type__(ll_type):\n    '\\n    Given an lltype value, retrieve its definition.\\n    '\n    res = [llt for llt in __LL_TYPES__ if (llt[1] == ll_type)]\n    assert (len(res) < 2), 'Duplicate linklayer types.'\n    if res:\n        return res[0]\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef get_shards(self):\n    'Returns a list of ShardMeta objects sorted by shard ID'\n    shard_config = self.partition_config['shards']\n    host_map = self.partition_config.get('host_map', {\n        \n    })\n    db_shards = []\n    for (db, shard_range) in shard_config.items():\n        db_shards.extend([DbShard(shard_num, db) for shard_num in range(shard_range[0], (shard_range[1] + 1))])\n    db_shards = sorted(db_shards, key=(lambda shard: shard.shard_id))\n    return [shard.to_shard_meta(host_map) for shard in db_shards]\n", "label": 0}
{"function": "\n\ndef iter_dicts(self):\n    self._set_headers()\n    for row in self:\n        result = {\n            \n        }\n        for (key, value) in zip(self._headers, row):\n            if value:\n                result[key] = value\n        if result:\n            (yield result)\n    self.close()\n", "label": 0}
{"function": "\n\ndef detectFileType(inFile):\n    firstLine = inFile.readline()\n    secondLine = inFile.readline()\n    thirdLine = inFile.readline()\n    inFile.seek(0)\n    if ((firstLine.find('nmap') != (- 1)) and (thirdLine.find('Host:') != (- 1))):\n        if ((firstLine.find('-sV') != (- 1)) or (firstLine.find('-A') != (- 1)) or (firstLine.find('-sSV') != (- 1))):\n            return 'gnmap'\n        else:\n            utility.Msg('Nmap version detection not used! Discovery module may miss some hosts!', LOG.INFO)\n            return 'gnmap'\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef test_account():\n    client = mock.MagicMock()\n    account = models.Account(client, 1, 2, storage_size=3456, guaranteed_cores=7, maximum_cores=8, created_at='created_at')\n    assert (account.account_id == 1)\n    assert (account.plan == 2)\n    assert (account.storage_size == 3456)\n    assert (account.guaranteed_cores == 7)\n    assert (account.maximum_cores == 8)\n    assert (account.created_at == 'created_at')\n", "label": 0}
{"function": "\n\ndef get_template_vars(self, ignorevars=[], sort=True, maxnestlevels=100):\n    'Return a list of all variables found in the template\\n\\n\\n        Arguments:\\n\\n            ignorevars  -- a list of variables that are removed from the output\\n            sort        -- True (default) or False if returned list should be sorted\\n            maxnestlevels -- a positve integer which defines how deep you can nest templates with includes\\n        '\n    tplvars = []\n    templates = []\n    templatesseen = []\n    nestlevels = 0\n    env = Environment(loader=FileSystemLoader(self.templatepath), undefined=StrictUndefined)\n    templates.append(self.templatename)\n    templatesseen.append(self.templatename)\n    while (len(templates) > 0):\n        tpl = templates.pop()\n        nested = False\n        tplsrc = env.loader.get_source(env, tpl)[0]\n        ast = env.parse(tplsrc)\n        for template in meta.find_referenced_templates(ast):\n            if (template in templatesseen):\n                raise Exception('Template loop detected: \"{}\" references \"{}\" which was seen earlier'.format(tpl, template))\n            else:\n                templates.append(template)\n                templatesseen.append(template)\n                nested = True\n        for e in meta.find_undeclared_variables(ast):\n            if (not (e in ignorevars)):\n                tplvars.append(e)\n        if (nested and (nestlevels >= maxnestlevels)):\n            raise Exception('Maximum template nesting depth of {} reached in template {}'.format(maxnestlevels, template))\n        else:\n            nestlevels += 1\n    if sort:\n        return sorted(tplvars)\n    else:\n        return tplvars\n", "label": 1}
{"function": "\n\ndef _literal(translator, expr):\n    if isinstance(expr, ir.BooleanValue):\n        typeclass = 'boolean'\n    elif isinstance(expr, ir.StringValue):\n        typeclass = 'string'\n    elif isinstance(expr, ir.NumericValue):\n        typeclass = 'number'\n    elif isinstance(expr, ir.TimestampValue):\n        typeclass = 'timestamp'\n    else:\n        raise NotImplementedError\n    return _literal_formatters[typeclass](expr)\n", "label": 0}
{"function": "\n\ndef run_vm_script(workflow_dict, context_dict, script, reverse=False, wait=0):\n    try:\n        instances_detail = workflow_dict['instances_detail']\n        final_context_dict = dict((context_dict.items() + workflow_dict['initial_context_dict'].items()))\n        if reverse:\n            instances_detail_final = instances_detail[::(- 1)]\n        else:\n            instances_detail_final = instances_detail\n        for instance_detail in instances_detail_final:\n            host = instance_detail['instance'].hostname\n            host_csattr = HostAttr.objects.get(host=host)\n            final_context_dict['IS_MASTER'] = instance_detail['is_master']\n            command = build_context_script(final_context_dict, script)\n            output = {\n                \n            }\n            return_code = exec_remote_command(server=host.address, username=host_csattr.vm_user, password=host_csattr.vm_password, command=command, output=output)\n            if return_code:\n                raise Exception('Could not run script. Output: {}'.format(output))\n            sleep(wait)\n        return True\n    except Exception:\n        traceback = full_stack()\n        workflow_dict['exceptions']['error_codes'].append(DBAAS_0015)\n        workflow_dict['exceptions']['traceback'].append(traceback)\n        return False\n", "label": 0}
{"function": "\n\n@login_required\ndef upload(request):\n    '\\n    Upload images for embedding in articles.\\n    '\n    js = ('js' in request.GET)\n    if (request.method == 'POST'):\n        form = ImageUploadForm(data=request.POST, files=request.FILES)\n        if form.is_valid():\n            image = form.save()\n            if js:\n                return HttpResponse(('<script>window.opener.editor.insertImage(\"[image:%d]\");window.close();</script>' % image.id))\n            else:\n                return redirect(image)\n    else:\n        form = ImageUploadForm()\n    return render(request, 'images/upload.html', {\n        'title': 'Upload',\n        'form': form,\n        'description': 'Use this form to upload an image.',\n        'action': 'Upload',\n        'js': js,\n    })\n", "label": 0}
{"function": "\n\ndef get_virtualenv_name(options):\n    if options.path:\n        return os.path.dirname(options.path)\n    else:\n        ve_name = (options.rest.pop(0) if options.rest else '')\n    if (not ve_name):\n        raise exceptions.NoVirtualenvName('could not find a virtualenv name in the command line.')\n    return ve_name\n", "label": 0}
{"function": "\n\ndef describe_new_commits(self, max_commits=100, max_size=16000):\n    'Return a string description of the new commits on the branch.'\n    hashes = None\n    previous = None\n    latest = self._review_branch.remote_branch\n    if self.is_new():\n        previous = self._review_branch.remote_base\n    else:\n        previous = self._tracking_branch.remote_branch\n    hashes = self._repo.get_range_hashes(previous, latest)\n    hashes.reverse()\n    revisions = self._repo.make_revisions_from_hashes(hashes)\n    message = ''\n    count = 0\n    message_size = 0\n    for r in revisions:\n        new_message = (((r.abbrev_hash + ' ') + r.subject) + '\\n')\n        count += 1\n        message_size += len(new_message)\n        if ((count > max_commits) or (message_size > max_size)):\n            message += '...{num_commits} commits not shown.\\n'.format(num_commits=((len(revisions) - count) + 1))\n            break\n        else:\n            message += new_message\n    return phlsys_textconvert.ensure_ascii(message)\n", "label": 0}
{"function": "\n\ndef fetch(self, name):\n    try:\n        target = {remote.name: remote for remote in self.repository.remotes}.get(name)\n        return (target.fetch() if target else None)\n    except OSError:\n        pass\n", "label": 0}
{"function": "\n\ndef allocate(self, shared_outputs=None):\n    alloc_layers = [l for l in self.layers if l.owns_output]\n    alloc_layers[(- 1)].allocate(shared_outputs)\n    for l in self.layers:\n        l.allocate()\n", "label": 0}
{"function": "\n\n@classmethod\ndef from_string(cls, s):\n    \"Parse the string given by 's' into a structure object.\"\n    if ((len(s) < 1) or (s[0] != cls.SIGIL)):\n        raise SynapseError(400, (\"Expected %s string to start with '%s'\" % (cls.__name__, cls.SIGIL)))\n    parts = s[1:].split(':', 1)\n    if (len(parts) != 2):\n        raise SynapseError(400, (\"Expected %s of the form '%slocalname:domain'\" % (cls.__name__, cls.SIGIL)))\n    domain = parts[1]\n    return cls(localpart=parts[0], domain=domain)\n", "label": 0}
{"function": "\n\ndef test_convert_is_robust_to_failures():\n    foo = NetworkDispatcher('foo')\n\n    def badfunc(*args, **kwargs):\n        raise NotImplementedError()\n\n    class A(object):\n        pass\n\n    class B(object):\n        pass\n\n    class C(object):\n        pass\n    discover.register((A, B, C))((lambda x: 'int'))\n    foo.register(B, A, cost=1.0)((lambda x, **kwargs: 1))\n    foo.register(C, B, cost=1.0)(badfunc)\n    foo.register(C, A, cost=10.0)((lambda x, **kwargs: 2))\n    with warnings.catch_warnings(record=True) as ws:\n        warnings.simplefilter('always')\n        assert (foo(C, A()) == 2)\n    assert (len(ws) == 1)\n    w = ws[0].message\n    assert isinstance(w, FailedConversionWarning)\n    assert ('B -> C' in str(w))\n", "label": 0}
{"function": "\n\ndef test_registered_functions(self):\n    'Test the correct functions are registered with gearman'\n    self.start_server()\n    t0 = time.time()\n    failed = True\n    while ((time.time() - t0) < 10):\n        if (len(self.gearman_server.functions) == 4):\n            failed = False\n            break\n        time.sleep(0.01)\n    if failed:\n        self.log.debug(self.gearman_server.functions)\n        self.fail(\"The correct number of functions haven't registered with gearman\")\n    hostname = os.uname()[1]\n    self.assertIn(('stop:turbo-hipster-manager-%s' % hostname), self.gearman_server.functions)\n", "label": 0}
{"function": "\n\ndef recover_view(self, request, version_id, extra_context=None):\n    'Displays a form that can recover a deleted model.'\n    if (not self.has_add_permission(request)):\n        raise PermissionDenied\n    version = get_object_or_404(Version, pk=version_id)\n    context = {\n        'title': (_('Recover %(name)s') % {\n            'name': version.object_repr,\n        }),\n    }\n    context.update((extra_context or {\n        \n    }))\n    return self.revisionform_view(request, version, (self.recover_form_template or self._get_template_list('recover_form.html')), context)\n", "label": 0}
{"function": "\n\ndef flatten_actual_args(args):\n    if isinstance(args, (list, tuple)):\n        return args\n    assert isinstance(args, ActualArgs), ('Unexpected args: %s' % (args,))\n    assert (len(args.keywords) == 0)\n    result = list(args.positional)\n    if args.starargs:\n        result.extend(tuple_elts(args.starargs))\n    return result\n", "label": 0}
{"function": "\n\n@cli.command('transpose')\n@click.option('-r', '--rotate', callback=convert_rotation, help='Rotates the image (in degrees)')\n@click.option('-f', '--flip', callback=convert_flip, help='Flips the image  [LR / TB]')\n@processor\ndef transpose_cmd(images, rotate, flip):\n    'Transposes an image by either rotating or flipping it.'\n    for image in images:\n        if (rotate is not None):\n            (mode, degrees) = rotate\n            click.echo(('Rotate \"%s\" by %ddeg' % (image.filename, degrees)))\n            image = copy_filename(image.transpose(mode), image)\n        if (flip is not None):\n            (mode, direction) = flip\n            click.echo(('Flip \"%s\" %s' % (image.filename, direction)))\n            image = copy_filename(image.transpose(mode), image)\n        (yield image)\n", "label": 0}
{"function": "\n\ndef decode_signed_value(secret, name, value, max_age_days=31, clock=None, min_version=None):\n    if (clock is None):\n        clock = time.time\n    if (min_version is None):\n        min_version = DEFAULT_SIGNED_VALUE_MIN_VERSION\n    if (min_version > 2):\n        raise ValueError(('Unsupported min_version %d' % min_version))\n    if (not value):\n        return None\n    value = utf8(value)\n    m = _signed_value_version_re.match(value)\n    if (m is None):\n        version = 1\n    else:\n        try:\n            version = int(m.group(1))\n            if (version > 999):\n                version = 1\n        except ValueError:\n            version = 1\n    if (version < min_version):\n        return None\n    if (version == 1):\n        return _decode_signed_value_v1(secret, name, value, max_age_days, clock)\n    elif (version == 2):\n        return _decode_signed_value_v2(secret, name, value, max_age_days, clock)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef __init__(self, domain=None, dependencies=None, setup=None, requirements=None, config_schema=None, platform_schema=None):\n    'Initialize the mock module.'\n    self.DOMAIN = domain\n    self.DEPENDENCIES = (dependencies or [])\n    self.REQUIREMENTS = (requirements or [])\n    if (config_schema is not None):\n        self.CONFIG_SCHEMA = config_schema\n    if (platform_schema is not None):\n        self.PLATFORM_SCHEMA = platform_schema\n    if (setup is None):\n        self.setup = (lambda hass, config: True)\n    else:\n        self.setup = setup\n", "label": 0}
{"function": "\n\ndef test_spread_stats():\n    stats = FrozenStatistics(children=[FrozenStatistics('foo', own_hits=10, children=[FrozenStatistics('foo', own_hits=20, children=[]), FrozenStatistics('bar', own_hits=30, children=[])]), FrozenStatistics('bar', own_hits=40, children=[]), FrozenStatistics('baz', own_hits=50, children=[])])\n    descendants = list(spread_stats(stats))\n    assert (len(descendants) == 5)\n    assert (descendants[0].name == 'foo')\n    assert (descendants[1].name == 'bar')\n    assert (descendants[2].name == 'baz')\n    assert (descendants[3].name == 'foo')\n    assert (descendants[4].name == 'bar')\n", "label": 0}
{"function": "\n\ndef is_backup_running(self, name=None, agent=None):\n    'See :func:`burpui.misc.backend.interface.BUIbackend.is_backup_running`'\n    if (not name):\n        return False\n    try:\n        filemap = self.status('c:{0}\\n'.format(name))\n    except BUIserverException:\n        return False\n    for line in filemap:\n        reg = re.search('^{0}\\\\s+\\\\d\\\\s+(\\\\w)'.format(name), line)\n        if (reg and (reg.group(1) not in ['i', 'c', 'C'])):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef exportChildren(self, lwrite, level, namespace_='DNSQueryObj:', name_='DNSQueryObjectType', fromsubclass_=False, pretty_print=True):\n    super(DNSQueryObjectType, self).exportChildren(lwrite, level, 'DNSQueryObj:', name_, True, pretty_print=pretty_print)\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    if (self.Transaction_ID is not None):\n        self.Transaction_ID.export(lwrite, level, 'DNSQueryObj:', name_='Transaction_ID', pretty_print=pretty_print)\n    if (self.Question is not None):\n        self.Question.export(lwrite, level, 'DNSQueryObj:', name_='Question', pretty_print=pretty_print)\n    if (self.Answer_Resource_Records is not None):\n        self.Answer_Resource_Records.export(lwrite, level, 'DNSQueryObj:', name_='Answer_Resource_Records', pretty_print=pretty_print)\n    if (self.Authority_Resource_Records is not None):\n        self.Authority_Resource_Records.export(lwrite, level, 'DNSQueryObj:', name_='Authority_Resource_Records', pretty_print=pretty_print)\n    if (self.Additional_Records is not None):\n        self.Additional_Records.export(lwrite, level, 'DNSQueryObj:', name_='Additional_Records', pretty_print=pretty_print)\n    if (self.Date_Ran is not None):\n        self.Date_Ran.export(lwrite, level, 'DNSQueryObj:', name_='Date_Ran', pretty_print=pretty_print)\n    if (self.Service_Used is not None):\n        self.Service_Used.export(lwrite, level, 'DNSQueryObj:', name_='Service_Used', pretty_print=pretty_print)\n", "label": 1}
{"function": "\n\ndef create(self, string):\n    'Deserialize an xml-formatted security group create request'\n    dom = minidom.parseString(string)\n    security_group = {\n        \n    }\n    sg_node = self.find_first_child_named(dom, 'security_group')\n    if (sg_node is not None):\n        if sg_node.hasAttribute('name'):\n            security_group['name'] = sg_node.getAttribute('name')\n        desc_node = self.find_first_child_named(sg_node, 'description')\n        if desc_node:\n            security_group['description'] = self.extract_text(desc_node)\n    return {\n        'body': {\n            'security_group': security_group,\n        },\n    }\n", "label": 0}
{"function": "\n\ndef create_security_group(self, body=None):\n    s = body.get('security_group')\n    if (not isinstance(s.get('name', ''), six.string_types)):\n        msg = 'BadRequest: Invalid input for name. Reason: None is not a valid string.'\n        raise n_exc.BadRequest(message=msg)\n    if (not isinstance(s.get('description.', ''), six.string_types)):\n        msg = 'BadRequest: Invalid input for description. Reason: None is not a valid string.'\n        raise n_exc.BadRequest(message=msg)\n    if ((len(s.get('name')) > 255) or (len(s.get('description')) > 255)):\n        msg = 'Security Group name great than 255'\n        raise n_exc.NeutronClientException(message=msg, status_code=401)\n    ret = {\n        'name': s.get('name'),\n        'description': s.get('description'),\n        'tenant_id': 'fake',\n        'security_group_rules': [],\n        'id': str(uuid.uuid4()),\n    }\n    self._fake_security_groups[ret['id']] = ret\n    return {\n        'security_group': ret,\n    }\n", "label": 0}
{"function": "\n\ndef _eval_imageset(self, f):\n    expr = f.expr\n    if (not isinstance(expr, Expr)):\n        return\n    if (len(f.variables) > 1):\n        return\n    n = f.variables[0]\n    c = f(0)\n    fx = (f(n) - c)\n    f_x = (f((- n)) - c)\n    neg_count = (lambda e: sum((_coeff_isneg(_) for _ in Add.make_args(e))))\n    if (neg_count(f_x) < neg_count(fx)):\n        expr = (f_x + c)\n    a = Wild('a', exclude=[n])\n    b = Wild('b', exclude=[n])\n    match = expr.match(((a * n) + b))\n    if (match and match[a]):\n        expr = ((match[a] * n) + (match[b] % match[a]))\n    if (expr != f.expr):\n        return ImageSet(Lambda(n, expr), S.Integers)\n", "label": 0}
{"function": "\n\ndef dump_header(iterable, allow_token=True):\n    'Dump an HTTP header again.  This is the reversal of\\n    :func:`parse_list_header`, :func:`parse_set_header` and\\n    :func:`parse_dict_header`.  This also quotes strings that include an\\n    equals sign unless you pass it as dict of key, value pairs.\\n\\n    >>> dump_header({\\'foo\\': \\'bar baz\\'})\\n    \\'foo=\"bar baz\"\\'\\n    >>> dump_header((\\'foo\\', \\'bar baz\\'))\\n    \\'foo, \"bar baz\"\\'\\n\\n    :param iterable: the iterable or dict of values to quote.\\n    :param allow_token: if set to `False` tokens as values are disallowed.\\n                        See :func:`quote_header_value` for more details.\\n    '\n    if isinstance(iterable, dict):\n        items = []\n        for (key, value) in iteritems(iterable):\n            if (value is None):\n                items.append(key)\n            else:\n                items.append(('%s=%s' % (key, quote_header_value(value, allow_token=allow_token))))\n    else:\n        items = [quote_header_value(x, allow_token=allow_token) for x in iterable]\n    return ', '.join(items)\n", "label": 0}
{"function": "\n\ndef wait_for_job(self, job, interval=5, timeout=60):\n    '\\n        Waits until the job indicated by job_resource is done or has failed\\n\\n        Parameters\\n        ----------\\n        job : Union[dict, str]\\n            ``dict`` representing a BigQuery job resource, or a ``str``\\n            representing the BigQuery job id\\n        interval : float, optional\\n            Polling interval in seconds, default = 5\\n        timeout : float, optional\\n            Timeout in seconds, default = 60\\n\\n        Returns\\n        -------\\n        dict\\n            Final state of the job resouce, as described here:\\n            https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.jobs.html#get\\n\\n        Raises\\n        ------\\n        Union[JobExecutingException, BigQueryTimeoutException]\\n            On http/auth failures or timeout\\n        '\n    complete = False\n    job_id = str((job if isinstance(job, (six.binary_type, six.text_type, int)) else job['jobReference']['jobId']))\n    job_resource = None\n    start_time = time()\n    elapsed_time = 0\n    while (not (complete or (elapsed_time > timeout))):\n        sleep(interval)\n        request = self.bigquery.jobs().get(projectId=self.project_id, jobId=job_id)\n        job_resource = request.execute()\n        self._raise_executing_exception_if_error(job_resource)\n        complete = (job_resource.get('status').get('state') == 'DONE')\n        elapsed_time = (time() - start_time)\n    if (not complete):\n        logger.error(('BigQuery job %s timeout' % job_id))\n        raise BigQueryTimeoutException()\n    return job_resource\n", "label": 0}
{"function": "\n\ndef list_downloads():\n    \"\\n    Return a list of all updates that have been downloaded locally.\\n\\n    :return: A list of updates that have been downloaded\\n    :rtype: list\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n       salt '*' softwareupdate.list_downloads\\n    \"\n    outfiles = []\n    for (root, subFolder, files) in os.walk('/Library/Updates'):\n        for f in files:\n            outfiles.append(os.path.join(root, f))\n    dist_files = []\n    for f in outfiles:\n        if f.endswith('.dist'):\n            dist_files.append(f)\n    ret = []\n    for update in _get_available():\n        for f in dist_files:\n            with salt.utils.fopen(f) as fhr:\n                if (update.rsplit('-', 1)[0] in fhr.read()):\n                    ret.append(update)\n    return ret\n", "label": 0}
{"function": "\n\ndef grad(self, inputs, gout):\n    (x,) = inputs\n    (gz,) = gout\n    if self.sparse_grad:\n        left = sp_ones_like(x)\n        right = gz\n        if ((right.dtype == 'float64') and (left.dtype == 'float32')):\n            left = left.astype('float64')\n        if ((right.dtype == 'float32') and (left.dtype == 'float64')):\n            right = right.astype('float64')\n        return [(left * right)]\n    else:\n        return [SparseFromDense(x.type.format)(gz)]\n", "label": 0}
{"function": "\n\n@property\ndef images(self):\n    '\\n    List of URLs of images on the page.\\n    '\n    if (not getattr(self, '_images', False)):\n        self._images = [page['imageinfo'][0]['url'] for page in self.__continued_query({\n            'generator': 'images',\n            'gimlimit': 'max',\n            'prop': 'imageinfo',\n            'iiprop': 'url',\n        }) if ('imageinfo' in page)]\n    return self._images\n", "label": 0}
{"function": "\n\ndef handle_argument(self, bit):\n    '\\n        Handle the current argument.\\n        '\n    if (self.current_argument is None):\n        try:\n            self.current_argument = self.arguments.pop(0)\n        except IndexError:\n            raise TooManyArguments(self.tagname, self.todo)\n    handled = self.current_argument.parse(self.parser, bit, self.tagname, self.kwargs)\n    while (not handled):\n        try:\n            self.current_argument = self.arguments.pop(0)\n        except IndexError:\n            if self.options.breakpoints:\n                raise BreakpointExpected(self.tagname, self.options.breakpoints, bit)\n            elif self.options.next_breakpoint:\n                raise BreakpointExpected(self.tagname, [self.options.next_breakpoint], bit)\n            else:\n                raise TooManyArguments(self.tagname, self.todo)\n        handled = self.current_argument.parse(self.parser, bit, self.tagname, self.kwargs)\n", "label": 0}
{"function": "\n\ndef avail(search=None, verbose=False):\n    \"\\n    Return a list of available images\\n\\n    search : string\\n        search keyword\\n    verbose : boolean (False)\\n        toggle verbose output\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' imgadm.avail [percona]\\n        salt '*' imgadm.avail verbose=True\\n    \"\n    ret = {\n        \n    }\n    imgadm = _check_imgadm()\n    cmd = '{0} avail -j'.format(imgadm)\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    result = {\n        \n    }\n    if (retcode != 0):\n        ret['Error'] = _exit_status(retcode)\n        return ret\n    for image in json.loads(res['stdout']):\n        if (image['manifest']['disabled'] or (not image['manifest']['public'])):\n            continue\n        if (search and (search not in image['manifest']['name'])):\n            continue\n        result[image['manifest']['uuid']] = _parse_image_meta(image, verbose)\n    return result\n", "label": 0}
{"function": "\n\ndef validate_args(fn, *args, **kwargs):\n    'Check that the supplied args are sufficient for calling a function.\\n\\n    >>> validate_args(lambda a: None)\\n    Traceback (most recent call last):\\n        ...\\n    MissingArgs: Missing argument(s): a\\n    >>> validate_args(lambda a, b, c, d: None, 0, c=1)\\n    Traceback (most recent call last):\\n        ...\\n    MissingArgs: Missing argument(s): b, d\\n\\n    :param fn: the function to check\\n    :param args: the positional arguments supplied\\n    :param kwargs: the keyword arguments supplied\\n    '\n    argspec = inspect.getargspec(fn)\n    num_defaults = len((argspec.defaults or []))\n    required_args = argspec.args[:(len(argspec.args) - num_defaults)]\n\n    def isbound(method):\n        return (getattr(method, '__self__', None) is not None)\n    if isbound(fn):\n        required_args.pop(0)\n    missing = [arg for arg in required_args if (arg not in kwargs)]\n    missing = missing[len(args):]\n    if missing:\n        raise MissingArgs(missing)\n", "label": 0}
{"function": "\n\ndef __init__(self, application):\n    from django.conf import settings\n    from django.core.exceptions import ImproperlyConfigured\n    from django import VERSION\n    for app in settings.INSTALLED_APPS:\n        if app.startswith('django.'):\n            raise ImproperlyConfigured(\"You must place 'djangae' before any 'django' apps in INSTALLED_APPS\")\n        elif (app == 'djangae'):\n            break\n    self.wrapped_app = application\n", "label": 0}
{"function": "\n\ndef trunc_decimal(val, places):\n    'Legacy compatibility, rounds the way the old satchmo 0.8.1 used to round.'\n    if ((val is None) or (val == '')):\n        return val\n    if (val < 0):\n        roundfactor = '-0.01'\n    else:\n        roundfactor = '0.01'\n    return round_decimal(val=val, places=places, roundfactor=roundfactor, normalize=True)\n", "label": 0}
{"function": "\n\ndef _existing_terms(self, ixreader, termset, reverse=False, phrases=True):\n    fieldname = self.fieldname\n    for word in self._words(ixreader):\n        t = (fieldname, word)\n        contains = (t in ixreader)\n        if reverse:\n            contains = (not contains)\n        if contains:\n            termset.add(t)\n", "label": 0}
{"function": "\n\ndef set_relay_option(self, code, value):\n    if ((code == LINK_SELECTION) or (code == SERVER_IDENTIFIER_OVERRIDE)):\n        value = socket.inet_aton(value)\n    elif (code == VSS):\n        value = binascii.unhexlify(('01' + value))\n    self.relay_options[code] = value\n", "label": 0}
{"function": "\n\ndef gen_info(gen):\n    if (not DEBUG):\n        return None\n    frame = gen.gi_frame\n    if gen.gi_running:\n        prefix = 'running generator '\n    elif frame:\n        if (frame.f_lasti < 0):\n            prefix = 'initial generator '\n        else:\n            prefix = 'suspended generator '\n    else:\n        prefix = 'terminated generator '\n    if frame:\n        return (prefix + frame_info(frame))\n    code = getattr(gen, 'gi_code', None)\n    if code:\n        return (prefix + code_info(code))\n    return (prefix + hex(id(gen)))\n", "label": 0}
{"function": "\n\ndef parse_parameters(doc, version):\n    p = docopt(doc, version=version)\n    p = {k.lstrip('-'): v for (k, v) in p.items()}\n    try:\n        return {\n            'input_is_hex': bool(p['hex']),\n            'max_key_length': int(p['max-keylen']),\n            'known_key_length': (int(p['key-length']) if p['key-length'] else None),\n            'most_frequent_char': (parse_char(p['char']) if p['char'] else None),\n            'brute_chars': bool(p['brute-chars']),\n            'brute_printable': bool(p['brute-printable']),\n            'frequency_spread': 0,\n            'filename': (p['FILE'] if p['FILE'] else '-'),\n        }\n    except ValueError as err:\n        raise ArgError(str(err))\n", "label": 0}
{"function": "\n\ndef dump_options_header(header, options):\n    'The reverse function to :func:`parse_options_header`.\\n\\n    :param header: the header to dump\\n    :param options: a dict of options to append.\\n    '\n    segments = []\n    if (header is not None):\n        segments.append(header)\n    for (key, value) in iteritems(options):\n        if (value is None):\n            segments.append(key)\n        else:\n            segments.append(('%s=%s' % (key, quote_header_value(value))))\n    return '; '.join(segments)\n", "label": 0}
{"function": "\n\ndef _format_message(self, msg, tobot=True, toname=False, colon=True, space=True):\n    colon = (':' if colon else '')\n    space = (' ' if space else '')\n    if tobot:\n        msg = '<@{}>{}{}{}'.format(self.testbot_userid, colon, space, msg)\n    elif toname:\n        msg = '{}{}{}{}'.format(self.testbot_username, colon, space, msg)\n    return msg\n", "label": 0}
{"function": "\n\ndef init_empty_package(base_dir, name, scala, java, python, r):\n    repo_name = name.split('/')[1]\n    package_dir = os.path.join(base_dir, repo_name)\n    if os.path.exists(package_dir):\n        raise RuntimeError(('Directory %s already exists' % package_dir))\n    license_id = get_license_id()\n    os.makedirs(package_dir)\n    os.chdir(package_dir)\n    create_license_file(license_id)\n    create_static_file('README.md')\n    create_static_file('.gitignore')\n    if ((not scala) and (not java)):\n        if python:\n            init_python_directories()\n        if r:\n            init_r_directories(name, license_id)\n    else:\n        init_src_directories('resources')\n        if (java or scala):\n            init_sbt_directories(name, license_id)\n        if java:\n            init_src_directories('java')\n        if scala:\n            init_src_directories('scala')\n        if python:\n            init_python_directories()\n        if r:\n            init_r_directories(name, license_id)\n", "label": 1}
{"function": "\n\ndef test_should_have_nan_entries_if_specified(self):\n    columns_list = [0]\n    optargs = [dict(missing_data=1.0)]\n    X = sdg.predictive_columns(self.M_c, self.X_L, self.X_D, columns_list, optional_settings=optargs)\n    assert numpy.all(numpy.isnan(X))\n    columns_list = [0, 1]\n    optargs = ([dict(missing_data=1.0)] * 2)\n    X = sdg.predictive_columns(self.M_c, self.X_L, self.X_D, columns_list, optional_settings=optargs)\n    assert numpy.all(numpy.isnan(X))\n    columns_list = [0, 1]\n    optargs = [dict(missing_data=1.0), None]\n    X = sdg.predictive_columns(self.M_c, self.X_L, self.X_D, columns_list, optional_settings=optargs)\n    assert numpy.all(numpy.isnan(X[:, 0]))\n    assert (not numpy.any(numpy.isnan(X[:, 1])))\n    columns_list = [0, 1]\n    optargs = [dict(missing_data=1.0), dict(missing_data=0.0)]\n    X = sdg.predictive_columns(self.M_c, self.X_L, self.X_D, columns_list, optional_settings=optargs)\n    assert numpy.all(numpy.isnan(X[:, 0]))\n    assert (not numpy.any(numpy.isnan(X[:, 1])))\n", "label": 0}
{"function": "\n\ndef run_(self, edit_token, original_message):\n    edit = self.view.begin_edit(edit_token, self.name(), original_message)\n    original_message['edit_token'] = edit\n    try:\n        all_rules = get_rules()\n        for rule in all_rules:\n            matched = True\n            pipeline_data = {\n                \n            }\n            message = deepcopy(original_message)\n            for definition in rule:\n                name = None\n                args = []\n                if isinstance(definition, str):\n                    name = definition\n                else:\n                    name = definition[0]\n                    args = definition[1:]\n                command = get_command(name)\n                if command:\n                    results = False\n                    try:\n                        results = command(message, args, pipeline_data)\n                    except Exception as e:\n                        print(e)\n                    finally:\n                        if results:\n                            pipeline_data[name] = results\n                        else:\n                            matched = False\n                            break\n            if matched:\n                break\n    finally:\n        self.view.end_edit(edit)\n", "label": 0}
{"function": "\n\ndef __init__(self, app, conf, logger=None):\n    self.app = app\n    self.logger = (logger or get_logger(conf, log_route='ratelimit'))\n    self.memcache_client = None\n    self.account_ratelimit = float(conf.get('account_ratelimit', 0))\n    self.max_sleep_time_seconds = float(conf.get('max_sleep_time_seconds', 60))\n    self.log_sleep_time_seconds = float(conf.get('log_sleep_time_seconds', 0))\n    self.clock_accuracy = int(conf.get('clock_accuracy', 1000))\n    self.rate_buffer_seconds = int(conf.get('rate_buffer_seconds', 5))\n    self.ratelimit_whitelist = [acc.strip() for acc in conf.get('account_whitelist', '').split(',') if acc.strip()]\n    self.ratelimit_blacklist = [acc.strip() for acc in conf.get('account_blacklist', '').split(',') if acc.strip()]\n    self.container_ratelimits = interpret_conf_limits(conf, 'container_ratelimit_')\n    self.container_listing_ratelimits = interpret_conf_limits(conf, 'container_listing_ratelimit_')\n", "label": 0}
{"function": "\n\n@blueprint.route('/hint', methods=['GET'])\n@api_wrapper\n@require_login\n@block_before_competition(WebError('The competition has not begun yet!'))\ndef request_problem_hint_hook():\n\n    @log_action\n    def hint(pid, source):\n        return None\n    source = request.args.get('source')\n    pid = request.args.get('pid')\n    if (pid is None):\n        return WebError('Please supply a pid.')\n    if (source is None):\n        return WebError('You have to supply the source of the hint.')\n    tid = api.user.get_team()['tid']\n    if (pid not in api.problem.get_unlocked_pids(tid)):\n        return WebError(\"Your team hasn't unlocked this problem yet!\")\n    hint(pid, source)\n    return WebSuccess('Hint noted.')\n", "label": 0}
{"function": "\n\ndef actions(self, action, comp):\n    if (action == 'delete'):\n        self.emit_event(comp, events.ColumnDeleted, comp)\n    elif (action == 'set_limit'):\n        self.card_counter.call(model='edit')\n    elif (action == 'purge'):\n        self.purge_cards()\n    self.emit_event(comp, events.SearchIndexUpdated)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(ServiceForm, self).__init__(*args, **kwargs)\n    autocompletes = ['category', 'business_owner', 'tech_owner', 'used_by']\n    for ac in autocompletes:\n        data = list(Service.objects.all().order_by(ac).distinct().values_list(ac, flat=True))\n        try:\n            data.remove('')\n        except ValueError:\n            pass\n        self.fields[ac].widget.attrs['class'] = 'service-auto-complete'\n        self.fields[ac].widget.attrs['data-autocomplete'] = json.dumps(data)\n    self.fields['systems'].widget.attrs['class'] = 'chosen-select'\n    self.fields['site'].widget.attrs['class'] = 'chosen-select'\n    self.fields['parent_service'].widget.attrs['class'] = 'chosen-select'\n    self.fields['depends_on'].widget.attrs['class'] = 'chosen-select'\n    self.fields['allocations'].widget.attrs['class'] = 'chosen-select service-allocations'\n    if self.instance.pk:\n        self.fields['parent_service'].queryset = Service.objects.exclude(pk=self.instance.pk)\n", "label": 0}
{"function": "\n\ndef add_item(self, filepath, filetype):\n    annotation_graph = poioapi.annotationgraph.AnnotationGraph(None)\n    if (filetype == poioapi.data.EAF):\n        annotation_graph.from_elan(filepath)\n    if (filetype == poioapi.data.EAFFROMTOOLBOX):\n        annotation_graph.from_elan(filepath)\n    elif (filetype == poioapi.data.TYPECRAFT):\n        annotation_graph.from_typecraft(filepath)\n    else:\n        raise poioapi.data.UnknownFileFormatError()\n    annotation_graph.structure_type_handler = poioapi.data.DataStructureType(annotation_graph.tier_hierarchies[0])\n    self.append((filepath, annotation_graph))\n", "label": 0}
{"function": "\n\ndef reset(ip=None, username=None):\n    'Reset records that match ip or username, and\\n    return the count of removed attempts.\\n    '\n    count = 0\n    attempts = AccessAttempt.objects.all()\n    if ip:\n        attempts = attempts.filter(ip_address=ip)\n    if username:\n        attempts = attempts.filter(username=username)\n    if attempts:\n        count = attempts.count()\n        attempts.delete()\n    return count\n", "label": 0}
{"function": "\n\ndef parseRow(self, csvRowDict):\n    dateTime = datetime.datetime.strptime(csvRowDict[self.__dateTimeColumn], self.__dateTimeFormat)\n    if (self.__timezone is not None):\n        if (self.__timeDelta is not None):\n            dateTime += self.__timeDelta\n        dateTime = dt.localize(dateTime, self.__timezone)\n    values = {\n        \n    }\n    for (key, value) in csvRowDict.items():\n        if (key != self.__dateTimeColumn):\n            values[key] = self.__converter(key, value)\n    return (dateTime, values)\n", "label": 0}
{"function": "\n\ndef __init__(self, lookup_func, dictionary, project, identifier):\n    super(PBXFileReference, self).__init__(lookup_func, dictionary, project, identifier)\n    if (kPBX_REFERENCE_lastKnownFileType in dictionary.keys()):\n        self.ftype = dictionary[kPBX_REFERENCE_lastKnownFileType]\n    if (kPBX_REFERENCE_fileEncoding in dictionary.keys()):\n        self.fileEncoding = dictionary[kPBX_REFERENCE_fileEncoding]\n    if (kPBX_REFERENCE_explicitFileType in dictionary.keys()):\n        self.explicitFileType = dictionary[kPBX_REFERENCE_explicitFileType]\n    if (kPBX_REFERENCE_includeInIndex in dictionary.keys()):\n        self.includeInIndex = dictionary[kPBX_REFERENCE_includeInIndex]\n", "label": 0}
{"function": "\n\ndef get_frame(self):\n    'Get one stomp frame'\n    while (not ('\\x00' in self.incomingData)):\n        data = self.socket.recv(1024)\n        if (not data):\n            raise Exception('Socket seem closed.')\n        else:\n            self.incomingData += data\n    split_data = self.incomingData.split('\\x00', 1)\n    self.incomingData = split_data[1]\n    frame_split = split_data[0].split('\\n')\n    command = frame_split[0]\n    headers = []\n    body = ''\n    headerMode = True\n    for x in frame_split[1:]:\n        if ((x == '') and headerMode):\n            headerMode = False\n        elif headerMode:\n            (key, value) = x.split(':', 1)\n            headers.append((key, value))\n        else:\n            body += (x + '\\n')\n    body = body[:(- 1)]\n    return (command, headers, body)\n", "label": 0}
{"function": "\n\ndef product_simplify(s):\n    'Main function for Product simplification'\n    from sympy.concrete.products import Product\n    terms = Mul.make_args(s)\n    p_t = []\n    o_t = []\n    for term in terms:\n        if isinstance(term, Product):\n            p_t.append(term)\n        else:\n            o_t.append(term)\n    used = ([False] * len(p_t))\n    for method in range(2):\n        for (i, p_term1) in enumerate(p_t):\n            if (not used[i]):\n                for (j, p_term2) in enumerate(p_t):\n                    if ((not used[j]) and (i != j)):\n                        if isinstance(product_mul(p_term1, p_term2, method), Product):\n                            p_t[i] = product_mul(p_term1, p_term2, method)\n                            used[j] = True\n    result = Mul(*o_t)\n    for (i, p_term) in enumerate(p_t):\n        if (not used[i]):\n            result = Mul(result, p_term)\n    return result\n", "label": 1}
{"function": "\n\ndef _build_rooms(self, data):\n    'Get or Create Rooms based on schedule type and set of Tracks'\n    created_rooms = []\n    rooms = sorted(set([x[self.ROOM_KEY] for x in data]))\n    for (i, room) in enumerate(rooms):\n        (room, created) = Room.objects.get_or_create(schedule=self.schedule, name=room, order=i)\n        if created:\n            created_rooms.append(room)\n    return created_rooms\n", "label": 0}
{"function": "\n\ndef run(self, edit, ask=True, edit_pattern=False):\n    window = self.view.window()\n    repo = self.get_repo()\n    if (not repo):\n        return\n    files = self.get_selected_files()\n    to_ignore = [f for (_, f) in files]\n    tracked = [f for (s, f) in files if (s != UNTRACKED_FILES)]\n    if (tracked and (not self.confirm_tracked(tracked))):\n        return\n    if (not to_ignore):\n        sublime.error_message(self.IGNORE_NO_FILES)\n        return\n    if edit_pattern:\n        patterns = []\n        to_ignore.reverse()\n\n        def on_done(pattern=None):\n            if pattern:\n                patterns.append(pattern)\n            if to_ignore:\n                filename = to_ignore.pop()\n                window.show_input_panel(self.IGNORE_LABEL, filename, on_done, noop, on_done)\n            elif patterns:\n                if ask:\n                    if (not self.confirm_ignore(patterns)):\n                        return\n                self.add_to_gitignore(repo, patterns)\n                goto = self.logical_goto_next_file()\n                self.update_status(goto)\n        filename = to_ignore.pop()\n        window.show_input_panel(self.IGNORE_LABEL, filename, on_done, noop, on_done)\n    else:\n        if ask:\n            if (not self.confirm_ignore(to_ignore)):\n                return\n        self.add_to_gitignore(repo, to_ignore)\n        goto = self.logical_goto_next_file()\n        self.update_status(goto)\n", "label": 1}
{"function": "\n\ndef _get_cluster_field(cluster, field):\n    if cluster.get(field):\n        return cluster[field]\n    if cluster.get('cluster_template_id'):\n        cluster_template = api.get_cluster_template(id=cluster['cluster_template_id'])\n        if cluster_template.get(field):\n            return cluster_template[field]\n    return None\n", "label": 0}
{"function": "\n\ndef find_best_match(worktree, token):\n    ' Find the best match for a project in a worktree\\n\\n    '\n    candidates = list()\n    for project in worktree.projects:\n        to_match = os.path.basename(project.src)\n        if (token in to_match):\n            candidates.append(project)\n    max_score = 0\n    best_project = None\n    for candidate in candidates:\n        to_match = os.path.basename(candidate.src)\n        sequence_matcher = difflib.SequenceMatcher(a=token, b=to_match)\n        score = sequence_matcher.ratio()\n        if (score > max_score):\n            max_score = score\n            best_project = candidate\n    if best_project:\n        return best_project.path\n", "label": 0}
{"function": "\n\ndef set(self, section, option, value):\n    \"Functions similarly to PythonConfigParser's set method, except that\\n        the value is implicitly converted to a string.\\n        \"\n    e_value = value\n    if (not isinstance(value, string_types)):\n        e_value = str(value)\n    if PY2:\n        if isinstance(value, unicode):\n            e_value = value.encode('utf-8')\n    ret = PythonConfigParser.set(self, section, option, e_value)\n    self._do_callbacks(section, option, value)\n    return ret\n", "label": 0}
{"function": "\n\ndef find_configdir(self):\n    if (self.args.get('--config') is not None):\n        return self.args.get('--config')\n    if ('GAFFERD_CONFIG' in os.environ):\n        return os.environ.get('GAFFERD_CONFIG')\n    if is_admin():\n        default_paths = system_path()\n    else:\n        default_paths = user_path()\n    for path in default_paths:\n        if os.path.isdir(path):\n            return path\n    return default_path()\n", "label": 0}
{"function": "\n\ndef connect(self):\n    'Connect to a host on a given (SSL) port.'\n    sock = socket_create_connection((self.host, self.port), self.timeout, self.source_address)\n    if self._tunnel_host:\n        self.sock = sock\n        self._tunnel()\n    server_hostname = (self.host if ssl.HAS_SNI else None)\n    self.sock = self._context.wrap_socket(sock, server_hostname=server_hostname)\n    try:\n        if self._check_hostname:\n            ssl.match_hostname(self.sock.getpeercert(), self.host)\n    except Exception:\n        self.sock.shutdown(socket.SHUT_RDWR)\n        self.sock.close()\n        raise\n", "label": 0}
{"function": "\n\n@staticmethod\ndef from_model(notify_model):\n    notify = {\n        \n    }\n    if getattr(notify_model, 'on_complete', None):\n        notify['on-complete'] = NotificationsHelper._from_model_sub_schema(notify_model.on_complete)\n    if getattr(notify_model, 'on_success', None):\n        notify['on-success'] = NotificationsHelper._from_model_sub_schema(notify_model.on_success)\n    if getattr(notify_model, 'on_failure', None):\n        notify['on-failure'] = NotificationsHelper._from_model_sub_schema(notify_model.on_failure)\n    return notify\n", "label": 0}
{"function": "\n\ndef parse_size(size):\n    if isinstance(size, (int, long)):\n        return max((0, size))\n    if isinstance(size, basestring):\n        size = size.upper()\n        if (size == 'OFF'):\n            return 0\n        else:\n            m = RE_SIZE.match(size)\n            if m:\n                try:\n                    p = UNITS.index(m.group('unit')[:1])\n                except ValueError:\n                    p = 0\n                return (int(m.group('number')) * (1024 ** p))\n    return 0\n", "label": 0}
{"function": "\n\n@property\ndef filter_options(self):\n    if hasattr(self, '_filter_options'):\n        return self._filter_options\n    slugs = self.view.kwargs[self.view_kwarg].split('/')\n    (id_map, unresolved) = Identifier.objects.resolve(slugs, exclude_apps=self.exclude_apps)\n    options = {\n        \n    }\n    if unresolved:\n        options['extra'] = []\n        for (key, items) in unresolved.items():\n            if (len(items) > 0):\n                raise Exception(('Could not resolve %s to a single item!' % key))\n            options['extra'].append(key)\n    if id_map:\n        for (slug, ident) in id_map.items():\n            ctype = ident.content_type.model\n            if (ctype not in options):\n                options[ctype] = []\n            options[ctype].append(ident)\n    self._filter_options = options\n    return options\n", "label": 0}
{"function": "\n\ndef _help(*args, **kws):\n    'show help on topic or object'\n    helper.buffer = []\n    _larch = kws.get('_larch', None)\n    if ((helper._larch is None) and (_larch is not None)):\n        helper._larch = _larch\n    if (args == ('',)):\n        args = ('help',)\n    if (helper._larch is None):\n        helper.addtext('cannot start help system!')\n    else:\n        for a in args:\n            helper.help(a)\n    if (helper._larch is not None):\n        helper._larch.writer.write(('%s\\n' % helper.getbuffer()))\n    else:\n        return helper.getbuffer()\n", "label": 0}
{"function": "\n\ndef processWebmention(sourceURL, targetURL, vouchDomain=None):\n    result = False\n    r = requests.get(sourceURL, verify=False)\n    if (r.status_code == requests.codes.ok):\n        mentionData = {\n            'sourceURL': sourceURL,\n            'targetURL': targetURL,\n            'vouchDomain': vouchDomain,\n            'vouched': False,\n            'received': datetime.date.today().strftime('%d %b %Y %H:%M'),\n            'postDate': datetime.date.today().strftime('%Y-%m-%dT%H:%M:%S'),\n        }\n        if ('charset' in r.headers.get('content-type', '')):\n            mentionData['content'] = r.text\n        else:\n            mentionData['content'] = r.content\n        if ((vouchDomain is not None) and cfg['require_vouch']):\n            mentionData['vouched'] = processVouch(sourceURL, targetURL, vouchDomain)\n            result = mentionData['vouched']\n            app.logger.info(('result of vouch? %s' % result))\n        else:\n            result = (not cfg['require_vouch'])\n            app.logger.info(('no vouch domain, result %s' % result))\n        mf2Data = Parser(doc=mentionData['content']).to_dict()\n        hcard = extractHCard(mf2Data)\n        mentionData['hcardName'] = hcard['name']\n        mentionData['hcardURL'] = hcard['url']\n        mentionData['mf2data'] = mf2Data\n    return result\n", "label": 0}
{"function": "\n\ndef get_foreign_key(self, source, dest, field=None):\n    if (isinstance(source, SelectQuery) or isinstance(dest, SelectQuery)):\n        return (None, None)\n    fk_field = source._meta.rel_for_model(dest, field)\n    if (fk_field is not None):\n        return (fk_field, False)\n    reverse_rel = source._meta.reverse_rel_for_model(dest, field)\n    if (reverse_rel is not None):\n        return (reverse_rel, True)\n    return (None, None)\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('exname,colno', [(exname, colno) for exname in examples.keys() for colno in range(1, 3)])\ndef test_onecolumn(exname, colno):\n    if ((exname == 't0') and (colno > 0)):\n        pytest.skip(('Not enough columns in %s.' % (exname,)))\n    if (exname.startswith('t1_sub') and (colno > 1)):\n        pytest.skip(('Not enough columns in %s.' % (exname,)))\n    with analyzed_bayesdb_generator(examples[exname](), 1, 1) as (bdb, generator_id):\n        bqlfn.bql_column_value_probability(bdb, generator_id, None, colno, 4)\n        bdb.sql_execute('select bql_column_value_probability(?, NULL, ?, 4)', (generator_id, colno)).fetchall()\n", "label": 0}
{"function": "\n\ndef discover_roku():\n    ' Search LAN for available Roku devices. Returns a Roku object. '\n    print('Searching for Roku devices within LAN ...')\n    rokus = Roku.discover()\n    if (not rokus):\n        print((('Unable to discover Roku devices. ' + 'Try again, or manually specify the IP address with ') + \"'roku <ipaddr>' (e.g. roku 192.168.1.130)\"))\n        return None\n    print('Found the following Roku devices:')\n    for (i, r) in enumerate(rokus):\n        print(((((('[' + str((i + 1))) + ']   ') + str(r.host)) + ':') + str(r.port)))\n    print('')\n    if (len(rokus) == 1):\n        print('Selecting Roku 1 by default')\n        return rokus[0]\n    else:\n        print('Multiple Rokus found. Select the index of the Roku to control:')\n        while True:\n            try:\n                query = (('Select (1 to ' + str(len(rokus))) + ') > ')\n                sel = (int(raw_input(query)) - 1)\n                if (sel >= len(rokus)):\n                    raise ValueError\n                else:\n                    break\n            except ValueError:\n                print('Invalid selection')\n        return rokus[sel]\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.file_path == other.file_path) and (self.append_trailing_newlines == other.append_trailing_newlines) and (self.coder == other.coder) and (self.file_name_prefix == other.file_name_prefix) and (self.file_name_suffix == other.file_name_suffix) and (self.num_shards == other.num_shards) and (self.shard_name_template == other.shard_name_template) and (self.validate == other.validate))\n", "label": 0}
{"function": "\n\ndef _join_monotonic(self, other, how='left', return_indexers=False):\n    if self.equals(other):\n        ret_index = (other if (how == 'right') else self)\n        if return_indexers:\n            return (ret_index, None, None)\n        else:\n            return ret_index\n    sv = self.values\n    ov = other._values\n    if (self.is_unique and other.is_unique):\n        if (how == 'left'):\n            join_index = self\n            lidx = None\n            ridx = self._left_indexer_unique(sv, ov)\n        elif (how == 'right'):\n            join_index = other\n            lidx = self._left_indexer_unique(ov, sv)\n            ridx = None\n        elif (how == 'inner'):\n            (join_index, lidx, ridx) = self._inner_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n        elif (how == 'outer'):\n            (join_index, lidx, ridx) = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n    else:\n        if (how == 'left'):\n            (join_index, lidx, ridx) = self._left_indexer(sv, ov)\n        elif (how == 'right'):\n            (join_index, ridx, lidx) = self._left_indexer(ov, sv)\n        elif (how == 'inner'):\n            (join_index, lidx, ridx) = self._inner_indexer(sv, ov)\n        elif (how == 'outer'):\n            (join_index, lidx, ridx) = self._outer_indexer(sv, ov)\n        join_index = self._wrap_joined_index(join_index, other)\n    if return_indexers:\n        return (join_index, lidx, ridx)\n    else:\n        return join_index\n", "label": 1}
{"function": "\n\ndef findall(dir=os.curdir):\n    \"Find all files under 'dir' and return the list of full filenames\\n    (relative to 'dir').\\n    \"\n    all_files = []\n    for (base, dirs, files) in os.walk(dir):\n        if ((base == os.curdir) or base.startswith((os.curdir + os.sep))):\n            base = base[2:]\n        if base:\n            files = [os.path.join(base, f) for f in files]\n        all_files.extend(filter(os.path.isfile, files))\n    return all_files\n", "label": 0}
{"function": "\n\ndef get_config(self):\n    config = {\n        'output_dim': self.output_dim,\n        'init': self.init.__name__,\n        'activation': self.activation.__name__,\n        'W_regularizer': (self.W_regularizer.get_config() if self.W_regularizer else None),\n        'b_regularizer': (self.b_regularizer.get_config() if self.b_regularizer else None),\n        'activity_regularizer': (self.activity_regularizer.get_config() if self.activity_regularizer else None),\n        'W_constraint': (self.W_constraint.get_config() if self.W_constraint else None),\n        'b_constraint': (self.b_constraint.get_config() if self.b_constraint else None),\n        'bias': self.bias,\n        'input_dim': self.input_dim,\n        'input_length': self.input_length,\n    }\n    base_config = super(TimeDistributedDense, self).get_config()\n    return dict((list(base_config.items()) + list(config.items())))\n", "label": 0}
{"function": "\n\ndef call(self, x, mask=None):\n    input_shape = self.input_spec[0].shape\n    input_length = input_shape[1]\n    if (not input_length):\n        if hasattr(K, 'int_shape'):\n            input_length = K.int_shape(x)[1]\n            if (not input_length):\n                raise Exception((('Layer ' + self.name) + ' requires to know the length of its input, but it could not be inferred automatically. Specify it manually by passing an input_shape argument to the first layer in your model.'))\n        else:\n            input_length = K.shape(x)[1]\n    x = K.reshape(x, ((- 1), input_shape[(- 1)]))\n    y = K.dot(x, self.W)\n    if self.bias:\n        y += self.b\n    y = K.reshape(y, ((- 1), input_length, self.output_dim))\n    y = self.activation(y)\n    return y\n", "label": 0}
{"function": "\n\ndef ClauseD(self, rule, tokens, _parent=None):\n    _context = self.Context(_parent, self._scanner, 'ClauseD', [rule, tokens])\n    _token = self._peek('STR', 'ID', 'LP', 'LB', 'STMT', context=_context)\n    if (_token == 'STR'):\n        STR = self._scan('STR', context=_context)\n        t = (STR, eval(STR, {\n            \n        }, {\n            \n        }))\n        if (t not in tokens):\n            tokens.insert(0, t)\n        return parsetree.Terminal(rule, STR)\n    elif (_token == 'ID'):\n        ID = self._scan('ID', context=_context)\n        OptParam = self.OptParam(_context)\n        return resolve_name(rule, tokens, ID, OptParam)\n    elif (_token == 'LP'):\n        LP = self._scan('LP', context=_context)\n        ClauseA = self.ClauseA(rule, tokens, _context)\n        RP = self._scan('RP', context=_context)\n        return ClauseA\n    elif (_token == 'LB'):\n        LB = self._scan('LB', context=_context)\n        ClauseA = self.ClauseA(rule, tokens, _context)\n        RB = self._scan('RB', context=_context)\n        return parsetree.Option(rule, ClauseA)\n    else:\n        STMT = self._scan('STMT', context=_context)\n        return parsetree.Eval(rule, STMT[2:(- 2)])\n", "label": 0}
{"function": "\n\ndef include(elem, loader=None):\n    if (loader is None):\n        loader = default_loader\n    i = 0\n    while (i < len(elem)):\n        e = elem[i]\n        if (e.tag == XINCLUDE_INCLUDE):\n            href = e.get('href')\n            parse = e.get('parse', 'xml')\n            if (parse == 'xml'):\n                node = loader(href, parse)\n                if (node is None):\n                    raise FatalIncludeError(('cannot load %r as %r' % (href, parse)))\n                node = copy.copy(node)\n                if e.tail:\n                    node.tail = ((node.tail or '') + e.tail)\n                elem[i] = node\n            elif (parse == 'text'):\n                text = loader(href, parse, e.get('encoding'))\n                if (text is None):\n                    raise FatalIncludeError(('cannot load %r as %r' % (href, parse)))\n                if i:\n                    node = elem[(i - 1)]\n                    node.tail = (((node.tail or '') + text) + (e.tail or ''))\n                else:\n                    elem.text = (((elem.text or '') + text) + (e.tail or ''))\n                del elem[i]\n                continue\n            else:\n                raise FatalIncludeError(('unknown parse type in xi:include tag (%r)' % parse))\n        elif (e.tag == XINCLUDE_FALLBACK):\n            raise FatalIncludeError(('xi:fallback tag must be child of xi:include (%r)' % e.tag))\n        else:\n            include(e, loader)\n        i = (i + 1)\n", "label": 1}
{"function": "\n\ndef _iter_funcs(self, func_name):\n    hooks = []\n    for plugin in self.instance.master.ctrl.plugins.values():\n        if ('get_hooks' not in plugin):\n            continue\n        hooks.extend(plugin['get_hooks']())\n    if ('hooks' in self.instance.config):\n        hooks.extend(self.instance.config['hooks'].hooks)\n    for hook in hooks:\n        func = getattr(hook, func_name, None)\n        if (func is not None):\n            (yield func)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef combine_profiles(profile_dir, outfile, sortby='cumulative'):\n    s = StringIO.StringIO()\n    stat_files = [f for f in os.listdir(profile_dir) if (os.path.isfile(os.path.join(profile_dir, f)) and f.endswith('.stats'))]\n    ps = pstats.Stats(os.path.join(profile_dir, stat_files[0]), stream=s)\n    if (len(stat_files) > 1):\n        for stat in stat_files[1:]:\n            ps.add(os.path.join(profile_dir, stat))\n    profile_name = os.path.join(profile_dir, '{}.profile'.format(outfile.replace('.profile', '')))\n    with open(profile_name, 'w') as f:\n        ps.strip_dirs()\n        ps.sort_stats(sortby)\n        ps.print_stats()\n        f.write(s.getvalue())\n", "label": 0}
{"function": "\n\n@app.route('/direct/<path:url>')\ndef direct(url):\n    kwargs = {\n        \n    }\n    for key in ('width', 'height', 'mode', 'quality', 'transform'):\n        value = (request.args.get(key) or request.args.get(key[0]))\n        if (value is not None):\n            value = (int(value) if value.isdigit() else value)\n            kwargs[key] = value\n    return redirect(images.build_url(url, **kwargs))\n", "label": 0}
{"function": "\n\ndef validate_stats(self):\n    '\\n        Making sure all stats are within range.\\n        Nevermind that we have modernized_stats!\\n        If the modernized stat is bigger than the difference between max and base, we have to adjust.\\n        '\n    diff = self.ship.max_stats.diff(self.ship.base_stats)\n    mapper = inspect(diff)\n    for column in mapper.attrs:\n        if (column.key == 'id'):\n            continue\n        modern_value = getattr(self.modernized_stats, column.key)\n        if (modern_value is not None):\n            diff_value = getattr(diff, column.key)\n            if (modern_value > diff_value):\n                setattr(self.stats, column.key, getattr(self.ship.max_stats, column.key))\n                setattr(self.modernized_stats, column.key, diff_value)\n", "label": 0}
{"function": "\n\ndef get_proc_family_model():\n    'Extracts the family and model based on vendorId\\n\\n        :rtype: (ComputedFamily, ComputedModel)\\n    '\n    cpuid_res = do_cpuid(1)\n    if is_intel_proc():\n        format = X86IntelCpuidFamilly\n    elif is_amd_proc():\n        format = X86AmdCpuidFamilly\n    else:\n        raise NotImplementedError('Cannot get familly information of proc <{0}>'.format(get_vendor_id()))\n    infos = format.from_buffer_copy(struct.pack('<I', cpuid_res.EAX))\n    if ((infos.FamilyID == 6) or (infos.FamilyID == 15)):\n        ComputedModel = (infos.ModelID + (infos.ExtendedModel << 4))\n    else:\n        ComputedModel = infos.ModelID\n    if (infos.FamilyID == 15):\n        ComputedFamily = (infos.FamilyID + infos.ExtendedFamily)\n    else:\n        ComputedFamily = infos.FamilyID\n    return (ComputedFamily, ComputedModel)\n", "label": 0}
{"function": "\n\ndef buttonClick(self, event):\n    curr = self._window._currentFeature.getValue()\n    if (curr is None):\n        nextt = self._app._allFeatures.firstItemId()\n    else:\n        nextt = self._app._allFeatures.nextItemId(curr)\n    while ((nextt is not None) and isinstance(nextt, FeatureSet)):\n        nextt = self._app._allFeatures.nextItemId(nextt)\n    if (nextt is not None):\n        self._window._currentFeature.setValue(nextt)\n    else:\n        self.showNotification('Last sample')\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _archive_cls(file):\n    cls = None\n    if isinstance(file, six.string_types):\n        filename = file\n    else:\n        try:\n            filename = file.name\n        except AttributeError:\n            raise UnrecognizedArchiveFormat('File object not a recognized archive format.')\n    (base, tail_ext) = os.path.splitext(filename.lower())\n    cls = extension_map.get(tail_ext)\n    if (not cls):\n        (base, ext) = os.path.splitext(base)\n        cls = extension_map.get(ext)\n    if (not cls):\n        raise UnrecognizedArchiveFormat(('Path not a recognized archive format: %s' % filename))\n    return cls\n", "label": 0}
{"function": "\n\n@util.debuglog\ndef info(self, extended=False):\n    result = dict([(proc.pid, proc.info()) for proc in self.processes.values()])\n    if (extended and ('extended_stats' in self.hooks)):\n        for (pid, stats) in result.items():\n            self.hooks['extended_stats'](self, self.arbiter, 'extended_stats', pid=pid, stats=stats)\n    return result\n", "label": 0}
{"function": "\n\ndef createReservation(self, function_name, **kwargs):\n    LOGGER.debug(('%s Called' % function_name))\n    if (function_name == 'schedulerserver/remoteaddtoheap'):\n        LOGGER.debug('remoteaddtoheap has been called')\n        LOGGER.debug(('kwargs: %s' % repr(kwargs)))\n        if set(('uuid', 'type')).issubset(set(kwargs)):\n            LOGGER.debug(('\\tUUID: %s\\n\\tType: %s' % (kwargs['uuid'], kwargs['type'])))\n            if kwargs['uuid']:\n                self.addToHeap(kwargs['uuid'], kwargs['type'])\n            return {\n                \n            }\n        else:\n            return {\n                'error': 'Required parameters are uuid and type',\n            }\n    elif (function_name == 'schedulerserver/remoteremovefromheap'):\n        LOGGER.debug('remoteremovefromheap has been called')\n        LOGGER.debug(('kwargs: %s' % repr(kwargs)))\n        if ('uuid' in kwargs):\n            LOGGER.debug(('UUID: %s' % kwargs['uuid']))\n            if kwargs['uuid']:\n                self.removeFromHeap(kwargs['uuid'])\n            return {\n                \n            }\n        else:\n            return {\n                'error': 'Required parameters are uuid',\n            }\n    return\n", "label": 0}
{"function": "\n\ndef make_node(self, x, index, gz):\n    x = as_sparse_variable(x)\n    gz = as_sparse_variable(gz)\n    assert (x.format in ['csr', 'csc'])\n    assert (gz.format in ['csr', 'csc'])\n    ind = tensor.as_tensor_variable(index)\n    assert (ind.ndim == 1)\n    assert ('int' in ind.dtype)\n    scipy_ver = [int(n) for n in scipy.__version__.split('.')[:2]]\n    if (not (scipy_ver >= [0, 13])):\n        raise NotImplementedError('Scipy version is to old')\n    return gof.Apply(self, [x, ind, gz], [x.type()])\n", "label": 0}
{"function": "\n\n@json_view(permission='upload_tender_documents', validators=(validate_file_update,))\ndef put(self):\n    'Tender Document Update'\n    if (((self.request.authenticated_role != 'auction') and (self.request.validated['tender_status'] != 'active.enquiries')) or ((self.request.authenticated_role == 'auction') and (self.request.validated['tender_status'] not in ['active.auction', 'active.qualification']))):\n        self.request.errors.add('body', 'data', \"Can't update document in current ({}) tender status\".format(self.request.validated['tender_status']))\n        self.request.errors.status = 403\n        return\n    document = upload_file(self.request)\n    self.request.validated['tender'].documents.append(document)\n    if save_tender(self.request):\n        self.LOGGER.info('Updated tender document {}'.format(self.request.context.id), extra=context_unpack(self.request, {\n            'MESSAGE_ID': 'tender_document_put',\n        }))\n        return {\n            'data': document.serialize('view'),\n        }\n", "label": 0}
{"function": "\n\n@expose('/lab/availability/local', methods=['POST'])\ndef change_accessibility(self):\n    lab_id = int(request.form['lab_id'])\n    activate = (request.form['activate'] == 'true')\n    lab = self.session.query(Laboratory).filter_by(id=lab_id).first()\n    if (lab is not None):\n        if activate:\n            lab.available = (not activate)\n            lab.default_local_identifier = ''\n        else:\n            local_id = request.form['default_local_identifier']\n            local_id = local_id.lstrip(' ')\n            local_id = local_id.strip(' ')\n            if ((not activate) and (len(local_id) == 0)):\n                flash(gettext('Invalid local identifier (empty)'))\n                return redirect(url_for('.index_view'))\n            existing_labs = self.session.query(Laboratory).filter_by(default_local_identifier=local_id).all()\n            if ((len(existing_labs) > 0) and (lab not in existing_labs)):\n                flash(gettext(\"Local identifier '%(localidentifier)s' already exists\", localidentifier=local_id))\n                return redirect(url_for('.index_view'))\n            lab.available = (not activate)\n            lab.default_local_identifier = local_id\n        self.session.add(lab)\n        self.session.commit()\n    return redirect(url_for('.index_view'))\n", "label": 0}
{"function": "\n\n@property\ndef average(self):\n    from .quantize import get_color_index\n    if (self._avg is None):\n        total = 0\n        mult = (1 << (8 - SIGBITS))\n        r_sum = 0\n        g_sum = 0\n        b_sum = 0\n        for i in range(self.r1, (self.r2 + 1)):\n            for j in range(self.g1, (self.g2 + 1)):\n                for k in range(self.b1, (self.b2 + 1)):\n                    index = get_color_index(i, j, k)\n                    hval = self.histo[index]\n                    total += hval\n                    r_sum += ((hval * (i + 0.5)) * mult)\n                    g_sum += ((hval * (j + 0.5)) * mult)\n                    b_sum += ((hval * (k + 0.5)) * mult)\n        if total:\n            r_avg = (~ (~ int((r_sum / total))))\n            g_avg = (~ (~ int((g_sum / total))))\n            b_avg = (~ (~ int((b_sum / total))))\n        else:\n            r_avg = (~ (~ int(((mult * ((self.r1 + self.r2) + 1)) / 2))))\n            g_avg = (~ (~ int(((mult * ((self.g1 + self.g2) + 1)) / 2))))\n            b_avg = (~ (~ int(((mult * ((self.b1 + self.b2) + 1)) / 2))))\n        self._avg = (r_avg, g_avg, b_avg)\n    return self._avg\n", "label": 0}
{"function": "\n\ndef send_metric(self, name, type, metric, keys, snapshot_keys=None):\n    if (snapshot_keys is None):\n        snapshot_keys = []\n    base_name = re.sub('\\\\s+', '_', name)\n    if self.prefix:\n        base_name = '{0}.{1}'.format(self.prefix, base_name)\n    for name in keys:\n        value = True\n        value = getattr(metric, name)\n        self._buffered_send_metric(base_name, name, value, now())\n    if hasattr(metric, 'snapshot'):\n        snapshot = metric.snapshot\n        for name in snapshot_keys:\n            value = True\n            value = getattr(snapshot, name)\n            self._buffered_send_metric(base_name, name, value, now())\n", "label": 0}
{"function": "\n\ndef test_add_column(self):\n    some_dataset_map = {column.name: column for column in SomeDataSet.columns}\n    sub_dataset_new_col_map = {column.name: column for column in SubDataSetNewCol.columns}\n    sub_col_names = {column.name for column in SubDataSetNewCol.columns}\n    self.assertIn('qux', sub_col_names)\n    self.assertEqual(sub_dataset_new_col_map['qux'].dtype, float64_dtype)\n    self.assertEqual({column.name for column in SomeDataSet.columns}, (sub_col_names - {'qux'}))\n    for (k, some_dataset_column) in some_dataset_map.items():\n        sub_dataset_column = sub_dataset_new_col_map[k]\n        self.assertIsNot(some_dataset_column, sub_dataset_column, ('subclass column %r should not have the same identity as the parent' % k))\n        self.assertEqual(some_dataset_column.dtype, sub_dataset_column.dtype, ('subclass column %r should have the same dtype as the parent' % k))\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    order = self._ordering\n    if (order is None):\n        order = self._sections\n    try:\n        return super(Namespace, self).__getitem__(key)\n    except KeyError:\n        pass\n    for section in order:\n        if isinstance(section, dict):\n            try:\n                return section[key]\n            except KeyError:\n                pass\n        else:\n            try:\n                return self[section][key]\n            except KeyError:\n                pass\n    raise KeyError(('Key %s not found in namespace' % key))\n", "label": 0}
{"function": "\n\ndef __init__(self, address=None, Body=None):\n    self.address = address\n    if (not Body):\n        self.rawBody = '{}'\n    else:\n        self.rawBody = Body\n    data = json.loads(self.rawBody)\n    self.cmdid = None\n    self.timestamp = int(time.time())\n    self.params = dict()\n    if data.has_key('cmdid'):\n        self.cmdid = int(data['cmdid'])\n    if data.has_key('timestamp'):\n        self.timestamp = int(data['timestamp'])\n    if data.has_key('params'):\n        self.params = data['params']\n", "label": 0}
{"function": "\n\ndef git_root(directory):\n    global git_root_cache\n    retval = False\n    leaf_dir = directory\n    if ((leaf_dir in git_root_cache) and (git_root_cache[leaf_dir]['expires'] > time.time())):\n        return git_root_cache[leaf_dir]['retval']\n    while directory:\n        if os.path.exists(os.path.join(directory, '.git')):\n            retval = directory\n            break\n        parent = os.path.realpath(os.path.join(directory, os.path.pardir))\n        if (parent == directory):\n            retval = False\n            break\n        directory = parent\n    git_root_cache[leaf_dir] = {\n        'retval': retval,\n        'expires': (time.time() + 5),\n    }\n    return retval\n", "label": 0}
{"function": "\n\ndef _load_universal_map(fileid):\n    contents = load(join(_UNIVERSAL_DATA, (fileid + '.map')), format='text')\n    _MAPPINGS[fileid]['universal'].default_factory = (lambda : 'X')\n    for line in contents.splitlines():\n        line = line.strip()\n        if (line == ''):\n            continue\n        (fine, coarse) = line.split('\\t')\n        assert (coarse in _UNIVERSAL_TAGS), 'Unexpected coarse tag: {}'.format(coarse)\n        assert (fine not in _MAPPINGS[fileid]['universal']), 'Multiple entries for original tag: {}'.format(fine)\n        _MAPPINGS[fileid]['universal'][fine] = coarse\n", "label": 0}
{"function": "\n\ndef get_filters(self, request):\n    _search = request.GET.get('_search')\n    filters = None\n    if (_search == 'true'):\n        _filters = request.GET.get('filters')\n        try:\n            filters = (_filters and json.loads(_filters))\n        except ValueError:\n            return None\n        if (filters is None):\n            field = request.GET.get('searchField')\n            op = request.GET.get('searchOper')\n            data = request.GET.get('searchString')\n            if all([field, op, data]):\n                filters = {\n                    'groupOp': 'AND',\n                    'rules': [{\n                        'op': op,\n                        'field': field,\n                        'data': data,\n                    }],\n                }\n    return filters\n", "label": 0}
{"function": "\n\n@staticmethod\ndef from_full_message(commit_msg_str):\n    '  Parses a full git commit message by parsing a given string into the different parts of a commit message '\n    lines = [line for line in commit_msg_str.split('\\n') if (not line.startswith('#'))]\n    full = '\\n'.join(lines)\n    title = (lines[0] if (len(lines) > 0) else '')\n    body = (lines[1:] if (len(lines) > 1) else [])\n    return GitCommitMessage(original=commit_msg_str, full=full, title=title, body=body)\n", "label": 0}
{"function": "\n\ndef import_pgp(keyring, keydata):\n    '\\n    keydata should be public key data to be imported to the keyring.\\n    The return value will be the sha1 fingerprint of the public key added.\\n    '\n    (out, err, ret) = run_command(['gpg', '--batch', '--status-fd', '1--no-default-keyring', '--keyring', keyring, '--import'], input=keydata)\n    fingerprint = None\n    for line in out.split('\\n'):\n        data = line.split()\n        if ((not data) or (data[0] != '[GNUPG:]')):\n            continue\n        if (data[1] == 'IMPORT_OK'):\n            fingerprint = data[3]\n            break\n    else:\n        raise ValueError('GPG failed to import pgp public key')\n    return fingerprint\n", "label": 0}
{"function": "\n\ndef _parse_list_data(self, raw_data):\n    for entry in raw_data:\n        if ('type' not in entry):\n            raise BootstrapSourceError((self.ERROR_PREFIX + 'No type defined for entry'))\n        entry_type = entry['type']\n        if (entry_type == 'host'):\n            self._parse_host_entry(entry)\n        elif (entry_type == 'cdn'):\n            self._parse_cdn_entry(entry)\n        else:\n            raise BootstrapSourceError((self.ERROR_PREFIX + (\"Invalid type: '%s'\" % entry_type)))\n", "label": 0}
{"function": "\n\ndef output(self, key, obj):\n    value = get_value((key if (self.attribute is None) else self.attribute), obj)\n    if (value is None):\n        if self.allow_null:\n            return None\n        elif (self.default is not None):\n            return self.default\n    return marshal(value, self.nested)\n", "label": 0}
{"function": "\n\ndef should_return_304(self):\n    'Returns True if the headers indicate that we should return 304.\\n\\n        .. versionadded:: 3.1\\n        '\n    if self.check_etag_header():\n        return True\n    ims_value = self.request.headers.get('If-Modified-Since')\n    if (ims_value is not None):\n        date_tuple = email.utils.parsedate(ims_value)\n        if (date_tuple is not None):\n            if_since = datetime.datetime(*date_tuple[:6])\n            if (if_since >= self.modified):\n                return True\n    return False\n", "label": 0}
{"function": "\n\ndef extract_time(element):\n    ' return a datetime object based on a gml text string\\n\\nex:\\n<gml:beginPosition>2006-07-27T21:10:00Z</gml:beginPosition>\\n<gml:endPosition indeterminatePosition=\"now\"/>\\n\\nIf there happens to be a strange element with both attributes and text,\\nuse the text.\\nex: <gml:beginPosition indeterminatePosition=\"now\">2006-07-27T21:10:00Z</gml:beginPosition>\\nWould be 2006-07-27T21:10:00Z, not \\'now\\'\\n\\n'\n    if (element is None):\n        return None\n    try:\n        dt = parser.parse(element.text)\n    except Exception:\n        att = testXMLValue(element.attrib.get('indeterminatePosition'), True)\n        if (att and (att == 'now')):\n            dt = datetime.utcnow()\n            dt.replace(tzinfo=pytz.utc)\n        else:\n            dt = None\n    return dt\n", "label": 0}
{"function": "\n\ndef get_tmy3_data(self, station, station_fallback=True):\n    if (self.stations is None):\n        self.stations = self._load_stations()\n    if (not (station in self.stations)):\n        warnings.warn('Station {} is not a TMY3 station. See self.stations for a complete list.'.format(station))\n        if station_fallback:\n            station = self._find_nearby_station(station)\n        else:\n            station = None\n    if (station is None):\n        return None\n    url = 'http://rredc.nrel.gov/solar/old_data/nsrdb/1991-2005/data/tmy3/{}TYA.CSV'.format(station)\n    r = requests.get(url)\n    if (r.status_code == 200):\n        hours = []\n        for line in r.text.splitlines()[2:]:\n            row = line.split(',')\n            year = row[0][6:10]\n            month = row[0][0:2]\n            day = row[0][3:5]\n            hour = (int(row[1][0:2]) - 1)\n            date_string = '{}{}{}{:02d}'.format(year, month, day, hour)\n            dt = datetime.strptime(date_string, '%Y%m%d%H')\n            temp_C = float(row[31])\n            hours.append({\n                'temp_C': temp_C,\n                'dt': dt,\n            })\n        return hours\n    else:\n        warnings.warn('Station {} was not found. Tried url {}.'.format(station, url))\n        return None\n", "label": 0}
{"function": "\n\ndef _get_division_orientation(self, one, two, splitter=False):\n    ' Returns whether there is a division between two visible QWidgets.\\n\\n        Divided in context means that the widgets are adjacent and aligned along\\n        the direction of the adjaceny.\\n        '\n    united = one.united(two)\n    if splitter:\n        sep = self.control.style().pixelMetric(QtGui.QStyle.PM_DockWidgetSeparatorExtent, None, self.control)\n        united.adjust(0, 0, (- sep), (- sep))\n    if ((one.x() == two.x()) and (one.width() == two.width()) and (united.height() == (one.height() + two.height()))):\n        return QtCore.Qt.Horizontal\n    elif ((one.y() == two.y()) and (one.height() == two.height()) and (united.width() == (one.width() + two.width()))):\n        return QtCore.Qt.Vertical\n    return 0\n", "label": 0}
{"function": "\n\ndef is_filtering_value_found(self, filter_value, value):\n    if isinstance(filter_value, dict):\n        for tag_pair in value:\n            if ((not isinstance(tag_pair, dict)) or (filter_value.get('key') != tag_pair.get('key'))):\n                continue\n            for filter_dict_value in filter_value.get('value'):\n                if super(TaggableItemsDescriber, self).is_filtering_value_found(filter_dict_value, tag_pair.get('value')):\n                    return True\n        return False\n    return super(TaggableItemsDescriber, self).is_filtering_value_found(filter_value, value)\n", "label": 0}
{"function": "\n\ndef header_store_parse(self, name, value):\n    \"+\\n        The name is returned unchanged.  If the input value has a 'name'\\n        attribute and it matches the name ignoring case, the value is returned\\n        unchanged.  Otherwise the name and value are passed to header_factory\\n        method, and the resulting custom header object is returned as the\\n        value.  In this case a ValueError is raised if the input value contains\\n        CR or LF characters.\\n\\n        \"\n    if (hasattr(value, 'name') and (value.name.lower() == name.lower())):\n        return (name, value)\n    if (isinstance(value, str) and (len(value.splitlines()) > 1)):\n        raise ValueError('Header values may not contain linefeed or carriage return characters')\n    return (name, self.header_factory(name, value))\n", "label": 0}
{"function": "\n\ndef _mktime(time_struct):\n    \"Custom mktime because Windows can't be arsed to properly do pre-Epoch\\n    dates, probably because it's busy counting all its chromosomes.\\n    \"\n    try:\n        return time.mktime(time_struct)\n    except OverflowError:\n        dt = datetime.datetime(*time_struct[:6])\n        ep = datetime.datetime(1970, 1, 1)\n        diff = (dt - ep)\n        ts = ((((diff.days * 24) * 3600) + diff.seconds) + time.timezone)\n        if (time_struct.tm_isdst == 1):\n            ts -= 3600\n        if ((time_struct.tm_isdst == (- 1)) and _isdst(dt)):\n            ts -= 3600\n        return ts\n", "label": 0}
{"function": "\n\n@contextmanager\ndef as_user(self, username=None):\n    'Run nested commands as the given user. For example::\\n\\n            head = local[\"head\"]\\n            head(\"-n1\", \"/dev/sda1\")    # this will fail...\\n            with local.as_user():\\n                head(\"-n1\", \"/dev/sda1\")\\n\\n        :param username: The user to run commands as. If not given, root (or Administrator) is assumed\\n        '\n    if IS_WIN32:\n        if (username is None):\n            username = 'Administrator'\n        self._as_user_stack.append((lambda argv: (['runas', '/savecred', ('/user:%s' % (username,)), (('\"' + ' '.join((str(a) for a in argv))) + '\"')], self.which('runas'))))\n    elif (username is None):\n        self._as_user_stack.append((lambda argv: ((['sudo'] + list(argv)), self.which('sudo'))))\n    else:\n        self._as_user_stack.append((lambda argv: ((['sudo', '-u', username] + list(argv)), self.which('sudo'))))\n    try:\n        (yield)\n    finally:\n        self._as_user_stack.pop((- 1))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef getDescNames(arr, root, exclude):\n    for c in root.__subclasses__():\n        if (c.__name__ in exclude):\n            for b in TestSet.getBaseNames({\n                \n            }, c):\n                if (b.__name__ in arr):\n                    del arr[b.__name__]\n        else:\n            arr[c.__name__] = c.__name__\n            if (len(c.__subclasses__()) > 0):\n                TestSet.getDescNames(arr, c, exclude)\n    return arr\n", "label": 0}
{"function": "\n\ndef ex_list_network_domains(self, location=None, name=None, service_plan=None, state=None):\n    '\\n        List networks domains deployed across all data center locations\\n        for your organization.\\n        The response includes the location of each network domain.\\n\\n        :param      location: Only network domains in the location (optional)\\n        :type       location: :class:`NodeLocation` or ``str``\\n\\n        :param      name: Only network domains of this name (optional)\\n        :type       name: ``str``\\n\\n        :param      service_plan: Only network domains of this type (optional)\\n        :type       service_plan: ``str``\\n\\n        :param      state: Only network domains in this state (optional)\\n        :type       state: ``str``\\n\\n        :return: a list of `DimensionDataNetwork` objects\\n        :rtype: ``list`` of :class:`DimensionDataNetwork`\\n        '\n    params = {\n        \n    }\n    if (location is not None):\n        params['datacenterId'] = self._location_to_location_id(location)\n    if (name is not None):\n        params['name'] = name\n    if (service_plan is not None):\n        params['type'] = service_plan\n    if (state is not None):\n        params['state'] = state\n    response = self.connection.request_with_orgId_api_2('network/networkDomain', params=params).object\n    return self._to_network_domains(response)\n", "label": 0}
{"function": "\n\ndef _is_valid_dev_type(device_info, vg):\n    \"Returns bool value if we should use device based on different rules:\\n\\n    1. Should have approved MAJOR number\\n    2. Shouldn't be nbd/ram/loop device\\n    3. Should contain DEVNAME itself\\n    4. Should be compared with vg value\\n\\n    :param device_info: A dict of properties which we get from udevadm.\\n    :param vg: determine if we need LVM devices or not.\\n    :returns: bool if we should use this device.\\n    \"\n    if (('E: MAJOR' in device_info) and (int(device_info['E: MAJOR']) not in VALID_MAJORS)):\n        return False\n    if any((os.path.basename(device_info['E: DEVNAME']).startswith(n) for n in ('nbd', 'ram', 'loop'))):\n        return False\n    if ('E: DEVNAME' not in device_info):\n        return False\n    if ((vg and ('E: DM_VG_NAME' in device_info)) or ((not vg) and ('E: DM_VG_NAME' not in device_info))):\n        return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef __call__(self, i, model, localvars):\n    'Returns True if the model should stop early.\\n\\n        Otherwise, returns a status string.\\n\\n        '\n    assert (i == (self._prev_iter + 1))\n    self._prev_iter = i\n    if isinstance(model, AdditiveModel):\n        if (self._y_pred is None):\n            self._y_pred = model.predict(self.X)\n        else:\n            self._y_pred += model.iter_y_delta(i, self.X)\n        y_pred = self._y_pred\n    else:\n        y_pred = model.predict(self.X)\n    score = 0.0\n    for (qid, a, b) in self._query_groups:\n        sorted_y = get_sorted_y(self.y[a:b], y_pred[a:b])\n        score += self.metric.evaluate(qid, sorted_y)\n    score /= len(self._query_groups)\n    if ((self._best_score is None) or (score > self._best_score)):\n        self._best_score = score\n        self._best_score_i = i\n    since = (i - self._best_score_i)\n    if (self.trim_on_stop and ((since >= self.stop_after) or ((i + 1) == model.n_estimators))):\n        model.trim((self._best_score_i + 1))\n        return True\n    return 'C:{:12.4f} B:{:12.4f} S:{:3d}'.format(score, self._best_score, since)\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    '\\n        Return null for keys beyond the range of the column. This allows for columns to be of uneven length and still be merged into rows cleanly.\\n        '\n    l = len(self)\n    if isinstance(key, slice):\n        indices = six.moves.range(*key.indices(l))\n        return [(list.__getitem__(self, i) if (i < l) else None) for i in indices]\n    if (key >= l):\n        return None\n    return list.__getitem__(self, key)\n", "label": 0}
{"function": "\n\ndef export(self, lwrite, level, namespace_='DNSQueryObj:', name_='DNSQueryObjectType', namespacedef_='', pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    showIndent(lwrite, level, pretty_print)\n    lwrite(('<%s%s%s' % (namespace_, name_, ((namespacedef_ and (' ' + namespacedef_)) or ''))))\n    already_processed = set()\n    self.exportAttributes(lwrite, level, already_processed, namespace_, name_='DNSQueryObjectType')\n    if self.hasContent_():\n        lwrite(('>%s' % (eol_,)))\n        self.exportChildren(lwrite, (level + 1), namespace_, name_, pretty_print=pretty_print)\n        showIndent(lwrite, level, pretty_print)\n        lwrite(('</%s%s>%s' % (namespace_, name_, eol_)))\n    else:\n        lwrite(('/>%s' % (eol_,)))\n", "label": 0}
{"function": "\n\ndef __call__(self, value):\n    value = force_text(value)\n    if ((not value) or ('@' not in value)):\n        raise ValidationError(self.message, code=self.code)\n    (user_part, domain_part) = value.rsplit('@', 1)\n    if (not self.user_regex.match(user_part)):\n        raise ValidationError(self.message, code=self.code)\n    if ((not (domain_part in self.domain_whitelist)) and (not self.domain_regex.match(domain_part))):\n        try:\n            domain_part = domain_part.encode('idna').decode('ascii')\n            if (not self.domain_regex.match(domain_part)):\n                raise ValidationError(self.message, code=self.code)\n            else:\n                return\n        except UnicodeError:\n            pass\n        raise ValidationError(self.message, code=self.code)\n", "label": 0}
{"function": "\n\ndef best_models(self, nb_models, model, data, max_evals):\n    trials_list = self.compute_trials(model, data, max_evals)\n    num_trials = sum((len(trials) for trials in trials_list))\n    if (num_trials < nb_models):\n        nb_models = len(trials)\n    scores = []\n    for trials in trials_list:\n        scores = (scores + [trial.get('result').get('loss') for trial in trials])\n    cut_off = sorted(scores, reverse=True)[(nb_models - 1)]\n    model_list = []\n    for trials in trials_list:\n        for trial in trials:\n            if (trial.get('result').get('loss') >= cut_off):\n                model = model_from_yaml(trial.get('result').get('model'))\n                model.set_weights(pickle.loads(trial.get('result').get('weights')))\n                model_list.append(model)\n    return model_list\n", "label": 0}
{"function": "\n\ndef find_up(name, path=None):\n    'Search upward from the starting path (or the current directory)\\n    until the given file or directory is found. The given name is\\n    assumed to be a basename, not a path.  Returns the absolute path\\n    of the file or directory if found, or None otherwise.\\n\\n    Args\\n    ----\\n    name : str\\n        Base name of the file or directory being searched for.\\n\\n    path : str, optional\\n        Starting directory.  If not supplied, current directory is used.\\n    '\n    if (not path):\n        path = os.getcwd()\n    if (not exists(path)):\n        return None\n    while path:\n        if exists(join(path, name)):\n            return abspath(join(path, name))\n        else:\n            pth = path\n            path = dirname(path)\n            if (path == pth):\n                return None\n    return None\n", "label": 0}
{"function": "\n\ndef _parse_args(func, args, usage=None):\n    'Parse the arguments given to a magic function'\n    if isinstance(args, list):\n        args = ' '.join(args)\n    args = _split_args(args)\n    kwargs = dict()\n    if getattr(func, 'has_options', False):\n        parser = MagicOptionParser(usage=usage)\n        parser.add_options(func.options)\n        left = []\n        value = None\n        if ('--' in args):\n            left = args[:args.index('--')]\n            (value, args) = parser.parse_args(args[(args.index('--') + 1):])\n        else:\n            while args:\n                try:\n                    (value, args) = parser.parse_args(args)\n                except Exception:\n                    left.append(args.pop(0))\n                else:\n                    break\n        args = (left + args)\n        if value:\n            kwargs = value.__dict__\n    new_args = []\n    for arg in args:\n        try:\n            new_args.append(eval(arg))\n        except:\n            new_args.append(arg)\n    for (key, value) in kwargs.items():\n        try:\n            kwargs[key] = eval(value)\n        except:\n            pass\n    return (new_args, kwargs)\n", "label": 1}
{"function": "\n\ndef asString(self):\n    if self.isSigned:\n        prefix = self.config.iprefix\n    else:\n        prefix = self.config.uprefix\n    if (self.width <= 8):\n        name = (prefix + '8')\n    elif (self.width <= 16):\n        name = (prefix + '16')\n    elif (self.width <= 32):\n        name = (prefix + '32')\n    else:\n        name = 'char*'\n    return name\n", "label": 0}
{"function": "\n\n@classmethod\ndef parse(cls, api, json):\n    status = cls(api)\n    for (k, v) in json.items():\n        if (k == 'user'):\n            user_model = (getattr(api.parser.model_factory, 'user') if api else User)\n            user = user_model.parse(api, v)\n            setattr(status, 'author', user)\n            setattr(status, 'user', user)\n        elif (k == 'created_at'):\n            setattr(status, k, parse_datetime(v))\n        elif (k == 'source'):\n            if ('<' in v):\n                setattr(status, k, parse_html_value(v))\n                setattr(status, 'source_url', parse_a_href(v))\n            else:\n                setattr(status, k, v)\n                setattr(status, 'source_url', None)\n        elif (k == 'retweeted_status'):\n            setattr(status, k, Status.parse(api, v))\n        elif (k == 'place'):\n            if (v is not None):\n                setattr(status, k, Place.parse(api, v))\n            else:\n                setattr(status, k, None)\n        else:\n            setattr(status, k, v)\n    return status\n", "label": 1}
{"function": "\n\ndef _real_extract(self, url):\n    mobj = re.match(self._VALID_URL, url)\n    playlist_id = mobj.group('id')\n    pllist_url = ('http://everyonesmixtape.com/mixtape.php?a=getMixes&u=-1&linked=%s&explore=' % playlist_id)\n    pllist_req = compat_urllib_request.Request(pllist_url)\n    pllist_req.add_header('X-Requested-With', 'XMLHttpRequest')\n    playlist_list = self._download_json(pllist_req, playlist_id, note='Downloading playlist metadata')\n    try:\n        playlist_no = next((playlist['id'] for playlist in playlist_list if (playlist['code'] == playlist_id)))\n    except StopIteration:\n        raise ExtractorError('Playlist id not found')\n    pl_url = ('http://everyonesmixtape.com/mixtape.php?a=getMix&id=%s&userId=null&code=' % playlist_no)\n    pl_req = compat_urllib_request.Request(pl_url)\n    pl_req.add_header('X-Requested-With', 'XMLHttpRequest')\n    playlist = self._download_json(pl_req, playlist_id, note='Downloading playlist info')\n    entries = [{\n        '_type': 'url',\n        'url': t['url'],\n        'title': t['title'],\n    } for t in playlist['tracks']]\n    if mobj.group('songnr'):\n        songnr = (int(mobj.group('songnr')) - 1)\n        return entries[songnr]\n    playlist_title = playlist['mixData']['name']\n    return {\n        '_type': 'playlist',\n        'id': playlist_id,\n        'title': playlist_title,\n        'entries': entries,\n    }\n", "label": 0}
{"function": "\n\ndef teardown(host=None, port=None):\n    'Remove an installed WSGI hook for ``host`` and ```port``.\\n\\n    If no host or port is passed, the default values will be assumed.\\n    If no hook is installed for the defaults, and both the host and\\n    port are missing, the last hook installed will be removed.\\n\\n    Returns True if a hook was removed, otherwise False.\\n    '\n    both_missing = ((not host) and (not port))\n    host = (host or DEFAULT_HOST)\n    port = (port or DEFAULT_PORT)\n    key = (host, port)\n    key_to_delete = None\n    if (key in INSTALLED):\n        key_to_delete = key\n    if ((not (key in INSTALLED)) and both_missing and (len(INSTALLED) > 0)):\n        (host, port) = key_to_delete = INSTALLED.keys()[(- 1)]\n    if key_to_delete:\n        (_, old_propagate) = INSTALLED[key_to_delete]\n        del INSTALLED[key_to_delete]\n        result = True\n        if (old_propagate is not None):\n            settings.DEBUG_PROPAGATE_EXCEPTIONS = old_propagate\n    else:\n        result = False\n    twill.remove_wsgi_intercept(host, port)\n    return result\n", "label": 1}
{"function": "\n\ndef __init__(self, pwm_pin=None, pwm_freq=50, min_ms=0.5, max_ms=2.4):\n    assert (pwm_freq > 0), ('pwm_freq must be positive, given: %s' % str(pwm_freq))\n    assert (min_ms > 0), ('0 min_ms must be positive, given: %s' % str(min_ms))\n    assert (max_ms > 0), ('max_ms must be positive, given: %s' % str(max_ms))\n    self.pwm_freq = pwm_freq\n    self.min_ms = min_ms\n    self.max_ms = max_ms\n    self.pwm_pin = None\n    if pwm_pin:\n        self.attach(pwm_pin)\n    self.angle = None\n", "label": 0}
{"function": "\n\ndef __call__(self, env, start_response):\n    '\\n        WSGI entry point.\\n        Wraps env in swob.Request object and passes it down.\\n\\n        :param env: WSGI environment dictionary\\n        :param start_response: WSGI callable\\n        '\n    req = Request(env)\n    if (self.memcache_client is None):\n        self.memcache_client = cache_from_env(env)\n    if (not self.memcache_client):\n        self.logger.warning(_('Warning: Cannot ratelimit without a memcached client'))\n        return self.app(env, start_response)\n    try:\n        (version, account, container, obj) = req.split_path(1, 4, True)\n    except ValueError:\n        return self.app(env, start_response)\n    ratelimit_resp = self.handle_ratelimit(req, account, container, obj)\n    if (ratelimit_resp is None):\n        return self.app(env, start_response)\n    else:\n        return ratelimit_resp(env, start_response)\n", "label": 0}
{"function": "\n\ndef delete(name, remove=False, force=False):\n    \"\\n    Remove a user from the minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' user.delete name remove=True force=True\\n    \"\n    if salt.utils.contains_whitespace(name):\n        raise SaltInvocationError('Username cannot contain whitespace')\n    if (not info(name)):\n        return True\n    if force:\n        log.warn('force option is unsupported on MacOS, ignoring')\n    if remove:\n        __salt__['file.remove'](info(name)['home'])\n    chgroups(name, ())\n    return (_dscl(['/Users/{0}'.format(name)], ctype='delete')['retcode'] == 0)\n", "label": 0}
{"function": "\n\ndef testOnClassData(self, dataset=None, verbose=False, return_targets=False):\n    'Return winner-takes-all classification output on a given dataset.\\n\\n        If no dataset is given, the dataset passed during Trainer\\n        initialization is used. If return_targets is set, also return\\n        corresponding target classes.\\n        '\n    if (dataset == None):\n        dataset = self.ds\n    dataset.reset()\n    out = []\n    targ = []\n    for seq in dataset._provideSequences():\n        self.module.reset()\n        for (input, target) in seq:\n            res = self.module.activate(input)\n            out.append(argmax(res))\n            targ.append(argmax(target))\n    if return_targets:\n        return (out, targ)\n    else:\n        return out\n", "label": 0}
{"function": "\n\ndef __init__(self, compiled_grammar, default_token=None, lexers=None):\n    assert isinstance(compiled_grammar, _CompiledGrammar)\n    assert ((default_token is None) or isinstance(default_token, tuple))\n    assert ((lexers is None) or all((isinstance(v, Lexer) for (k, v) in lexers.items())))\n    assert ((lexers is None) or isinstance(lexers, dict))\n    self.compiled_grammar = compiled_grammar\n    self.default_token = (default_token or Token)\n    self.lexers = (lexers or {\n        \n    })\n", "label": 0}
{"function": "\n\ndef url_for(self, fn, **kwargs):\n    if (not callable(fn)):\n        raise RouterException('router url_for method only accept callable object.')\n    for (rule, v) in self.rulesMap.items():\n        if (v == fn):\n            if (len(rule.variables) > 0):\n                return rule.build_url(kwargs)\n            return rule.build_url()\n    raise RouterException(\"callable object doesn't matched any routing rule.\")\n", "label": 0}
{"function": "\n\ndef readline(self):\n    if (not self._lines):\n        if self._closed:\n            return ''\n        return NeedMoreData\n    line = self._lines.pop()\n    for ateof in self._eofstack[::(- 1)]:\n        if ateof(line):\n            self._lines.append(line)\n            return ''\n    return line\n", "label": 0}
{"function": "\n\n@timed\ndef gen_predicates(num):\n    res = []\n    for x in range(num):\n        r = random.randint(0, 5)\n        if (r == 0):\n            p_str = (\"name is '%s' and not test\" % random.choice(SELECT_WORDS))\n        elif (r == 1):\n            gender = ('Male' if (random.random() > 0.5) else 'Female')\n            age = random.randint(1, 100)\n            p_str = (\"gender is '%s' and age > %d\" % (gender, age))\n        elif (r == 2):\n            city_letter = chr((97 + random.randint(0, 25)))\n            city_reg = ('^%s.*' % city_letter)\n            age = random.randint(1, 100)\n            p_str = (\"age > %d and city matches '%s'\" % (age, city_reg))\n        elif (r == 3):\n            interest = random.choice(SELECT_WORDS)\n            p_str = (\"interests contains '%s' and test\" % interest)\n        elif (r == 4):\n            gender = ('Male' if random.random() else 'Female')\n            p_str = (\"name is '%s' or gender is '%s'\" % (random.choice(SELECT_WORDS), gender))\n        elif (r == 5):\n            gender = ('Male' if random.random() else 'Female')\n            age = random.randint(1, 100)\n            p_str = (\"(age > %d and gender is '%s')\" % (age, gender))\n            gender = ('Male' if random.random() else 'Female')\n            age = random.randint(1, 100)\n            p_str += (\" or (age < %d and gender is '%s')\" % (age, gender))\n        p = Predicate(p_str)\n        res.append(p)\n    return res\n", "label": 1}
{"function": "\n\ndef gen_date(min_date=None, max_date=None):\n    'Returns a random date value\\n\\n    :param min_date: A valid ``datetime.date`` object.\\n    :param max_date: A valid ``datetime.date`` object.\\n    :raises: ``ValueError`` if arguments are not valid ``datetime.date``\\n        objects.\\n    :returns: Random ``datetime.date`` object.\\n\\n    '\n    _min_value = (datetime.date.today() - datetime.timedelta((365 * MIN_YEARS)))\n    _max_value = (datetime.date.today() + datetime.timedelta((365 * MAX_YEARS)))\n    if (min_date is None):\n        min_date = _min_value\n    if (max_date is None):\n        max_date = _max_value\n    if (not isinstance(min_date, datetime.date)):\n        raise ValueError('%s is not a valid datetime.date object')\n    if (not isinstance(max_date, datetime.date)):\n        raise ValueError('%s is not a valid datetime.date object')\n    assert (min_date < max_date)\n    random.seed()\n    diff = (max_date - min_date)\n    days = random.randint(0, diff.days)\n    date = (min_date + datetime.timedelta(days=days))\n    return date\n", "label": 0}
{"function": "\n\ndef _check_file(self):\n    '\\n        Checks watched file moficiation time and permission changes.\\n        '\n    try:\n        self.editor.toPlainText()\n    except RuntimeError:\n        self._timer.stop()\n        return\n    if (self.editor and self.editor.file.path):\n        if ((not os.path.exists(self.editor.file.path)) and self._mtime):\n            self._notify_deleted_file()\n        else:\n            mtime = os.path.getmtime(self.editor.file.path)\n            if (mtime > self._mtime):\n                self._mtime = mtime\n                self._notify_change()\n            writeable = os.access(self.editor.file.path, os.W_OK)\n            self.editor.setReadOnly((not writeable))\n", "label": 0}
{"function": "\n\ndef _GetLibyalGoogleDriveLatestVersion(library_name):\n    'Retrieves the latest version number of a libyal library on Google Drive.\\n\\n  Args:\\n    library_name: the name of the libyal library.\\n\\n  Returns:\\n    The latest version for a given libyal library on Google Drive\\n    or 0 on error.\\n  '\n    download_url = 'https://code.google.com/p/{0:s}/'.format(library_name)\n    page_content = _DownloadPageContent(download_url)\n    if (not page_content):\n        return 0\n    page_content = page_content.decode('utf-8')\n    expression_string = b'<a href=\"(https://googledrive.com/host/[^/]*/)\"[^>]*>Downloads</a>'\n    matches = re.findall(expression_string, page_content)\n    if ((not matches) or (len(matches) != 1)):\n        return 0\n    page_content = _DownloadPageContent(matches[0])\n    if (not page_content):\n        return 0\n    page_content = page_content.decode('utf-8')\n    expression_string = b'/host/[^/]*/{0:s}-[a-z-]*([0-9]+)[.]tar[.]gz'.format(library_name)\n    matches = re.findall(expression_string, page_content)\n    if (not matches):\n        return 0\n    return int(max(matches))\n", "label": 0}
{"function": "\n\ndef _write_enums(enum_descriptors, out):\n    'Write nested and non-nested Enum types.\\n\\n  Args:\\n    enum_descriptors: List of EnumDescriptor objects from which to generate\\n      enums.\\n    out: Indent writer used for generating text.\\n  '\n    for enum in (enum_descriptors or []):\n        (out << '')\n        (out << '')\n        (out << ('class %s(messages.Enum):' % enum.name))\n        (out << '')\n        with out.indent():\n            if (not enum.values):\n                (out << 'pass')\n            else:\n                for enum_value in enum.values:\n                    (out << ('%s = %s' % (enum_value.name, enum_value.number)))\n", "label": 0}
{"function": "\n\ndef _response_parser(self, r, expect_json=True, ignore_content=False):\n    '\\n        :param :class:`requests.Response` r: a response object of the Requests\\n            library\\n        :param bool expect_json: if True, raise :class:`.InvalidAPIAccess` if\\n            response is not in JSON format\\n        :param bool ignore_content: whether to ignore setting content of the\\n            Response object\\n        '\n    if r.ok:\n        try:\n            j = r.json()\n            return Response(j.get('state'), j)\n        except ValueError:\n            if expect_json:\n                logger = logging.getLogger(conf.LOGGING_API_LOGGER)\n                logger.debug(r.text)\n                raise InvalidAPIAccess('Invalid API access.')\n            if ignore_content:\n                res = Response(True, None)\n            else:\n                res = Response(True, r.text)\n            return res\n    else:\n        r.raise_for_status()\n", "label": 0}
{"function": "\n\ndef count(typename, objects=None):\n    \"Count objects tracked by the garbage collector with a given class name.\\n\\n    Example:\\n\\n        >>> count('dict')\\n        42\\n        >>> count('MyClass', get_leaking_objects())\\n        3\\n        >>> count('mymodule.MyClass')\\n        2\\n\\n    Note that the GC does not track simple objects like int or str.\\n\\n    .. versionchanged:: 1.7\\n       New parameter: ``objects``.\\n\\n    .. versionchanged:: 1.8\\n       Accepts fully-qualified type names (i.e. 'package.module.ClassName')\\n       as well as short type names (i.e. 'ClassName').\\n\\n    \"\n    if (objects is None):\n        objects = gc.get_objects()\n    try:\n        if ('.' in typename):\n            return sum((1 for o in objects if (_long_typename(o) == typename)))\n        else:\n            return sum((1 for o in objects if (_short_typename(o) == typename)))\n    finally:\n        del objects\n", "label": 0}
{"function": "\n\ndef __init__(self, store):\n    from lino.core.gfks import GenericForeignKey\n    SpecialStoreField.__init__(self, store)\n    self.always_disabled = set()\n    for f in self.store.all_fields:\n        if (f.field is not None):\n            if isinstance(f, VirtStoreField):\n                if (not f.vf.editable):\n                    if (not isinstance(f.vf.return_type, fields.DisplayField)):\n                        self.always_disabled.add(f.name)\n            elif (not isinstance(f.field, GenericForeignKey)):\n                if (not f.field.editable):\n                    self.always_disabled.add(f.name)\n", "label": 0}
{"function": "\n\ndef _validate(self, configuration):\n    for port in configuration.ports:\n        if (port_is_in_trunk_mode(port) and ((port.trunk_vlans is None) or (len(port.trunk_vlans) == 0))):\n            raise FailingCommitResults([TrunkShouldHaveVlanMembers(interface=port.name), ConfigurationCheckOutFailed()])\n    return super(JuniperQfxCopperNetconfDatastore, self)._validate(configuration)\n", "label": 0}
{"function": "\n\ndef read_credentials_file(file):\n    user = ''\n    token = ''\n    with open(file, 'r') as f:\n        content = [x.strip('\\n') for x in f.readlines()]\n        for line in content:\n            if ('user=' in line):\n                str_line = line.strip()\n                user = str_line[len('user='):].strip()\n            elif ('password=' in line):\n                str_line = line.strip()\n                token = str_line[len('password='):].strip()\n    if (user == ''):\n        show_error_and_exit((\"Could not resolve github username from the file: %s. Please make sure that it's supplied in its own line as,\\nuser= $USERNAME\" % file))\n    if (token == ''):\n        show_error_and_exit((\"Could not resolve github token from the file: %s. Please make sure that it's supplied in its own line as,\\npassword= $TOKEN\" % file))\n    return (user, token)\n", "label": 0}
{"function": "\n\n@register_vcs_handler('git', 'keywords')\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    if (not keywords):\n        raise NotThisMethod('no keywords at all, weird')\n    refnames = keywords['refnames'].strip()\n    if refnames.startswith('$Format'):\n        if verbose:\n            print('keywords are unexpanded, not using')\n        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')\n    refs = set([r.strip() for r in refnames.strip('()').split(',')])\n    TAG = 'tag: '\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if (not tags):\n        tags = set([r for r in refs if re.search('\\\\d', r)])\n        if verbose:\n            print((\"discarding '%s', no digits\" % ','.join((refs - tags))))\n    if verbose:\n        print(('likely tags: %s' % ','.join(sorted(tags))))\n    for ref in sorted(tags):\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(('picking %s' % r))\n            return {\n                'version': r,\n                'full-revisionid': keywords['full'].strip(),\n                'dirty': False,\n                'error': None,\n            }\n    if verbose:\n        print('no suitable tags, using unknown + full revision id')\n    return {\n        'version': '0+unknown',\n        'full-revisionid': keywords['full'].strip(),\n        'dirty': False,\n        'error': 'no suitable tags',\n    }\n", "label": 1}
{"function": "\n\ndef _fractional_matrix_power(A, p):\n    '\\n    Compute the fractional power of a matrix.\\n\\n    See the fractional_matrix_power docstring in matfuncs.py for more info.\\n\\n    '\n    A = np.asarray(A)\n    if ((len(A.shape) != 2) or (A.shape[0] != A.shape[1])):\n        raise ValueError('expected a square matrix')\n    if (p == int(p)):\n        return np.linalg.matrix_power(A, int(p))\n    s = svdvals(A)\n    if s[(- 1)]:\n        k2 = (s[0] / s[(- 1)])\n        p1 = (p - np.floor(p))\n        p2 = (p - np.ceil(p))\n        if ((p1 * (k2 ** (1 - p1))) <= ((- p2) * k2)):\n            a = int(np.floor(p))\n            b = p1\n        else:\n            a = int(np.ceil(p))\n            b = p2\n        try:\n            R = _remainder_matrix_power(A, b)\n            Q = np.linalg.matrix_power(A, a)\n            return Q.dot(R)\n        except np.linalg.LinAlgError:\n            pass\n    if (p < 0):\n        X = np.empty_like(A)\n        X.fill(np.nan)\n        return X\n    else:\n        p1 = (p - np.floor(p))\n        a = int(np.floor(p))\n        b = p1\n        (R, info) = funm(A, (lambda x: pow(x, b)), disp=False)\n        Q = np.linalg.matrix_power(A, a)\n        return Q.dot(R)\n", "label": 0}
{"function": "\n\ndef get(self, name):\n    if (not may(CREATE)):\n        raise Forbidden()\n    try:\n        item = current_app.storage.open(name)\n    except (OSError, IOError) as e:\n        if (e.errno == errno.ENOENT):\n            return ('No file found.', 404)\n        raise\n    if item.meta['complete']:\n        error = 'Upload complete. Cannot delete fileupload garbage.'\n    else:\n        error = None\n    if error:\n        return (error, 409)\n    try:\n        item = current_app.storage.remove(name)\n    except (OSError, IOError) as e:\n        if (e.errno == errno.ENOENT):\n            raise NotFound()\n        raise\n    return 'Upload aborted'\n", "label": 0}
{"function": "\n\ndef parse(readable, *args, **kwargs):\n    'Parse HTML or XML readable.'\n    if (not hasattr(readable, 'read')):\n        readable = open(readable, 'rb')\n    mdp = MicroDOMParser(*args, **kwargs)\n    mdp.filename = getattr(readable, 'name', '<xmlfile />')\n    mdp.makeConnection(None)\n    if hasattr(readable, 'getvalue'):\n        mdp.dataReceived(readable.getvalue())\n    else:\n        r = readable.read(1024)\n        while r:\n            mdp.dataReceived(r)\n            r = readable.read(1024)\n    mdp.connectionLost(None)\n    if (not mdp.documents):\n        raise ParseError(mdp.filename, 0, 0, 'No top-level Nodes in document')\n    if mdp.beExtremelyLenient:\n        if (len(mdp.documents) == 1):\n            d = mdp.documents[0]\n            if (not isinstance(d, Element)):\n                el = Element('html')\n                el.appendChild(d)\n                d = el\n        else:\n            d = Element('html')\n            for child in mdp.documents:\n                d.appendChild(child)\n    else:\n        d = mdp.documents[0]\n    doc = Document(d)\n    doc.doctype = mdp._mddoctype\n    return doc\n", "label": 1}
{"function": "\n\ndef test_recalc_on_state_changes(self):\n    ct = self.setup_hist_calc_counter()\n    assert (ct.call_count == 0)\n    self.artist.update()\n    assert (ct.call_count == 1)\n    self.artist.lo -= 1\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 2)\n    self.artist.hi -= 1\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 3)\n    self.artist.nbins += 1\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 4)\n    self.artist.xlog ^= True\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 5)\n    self.artist.ylog ^= True\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 5)\n    self.artist.cumulative ^= True\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 5)\n    self.artist.normed ^= True\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 5)\n    self.artist.layer.style.color = '#00ff00'\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 5)\n    self.artist.layer.subset_state = (self.artist.layer.data.id['x'] > 10)\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 6)\n", "label": 1}
{"function": "\n\ndef guessFieldFormatters(self, generator):\n    formatters = {\n        \n    }\n    model = self.model\n    nameGuesser = Name(generator)\n    fieldTypeGuesser = FieldTypeGuesser(generator)\n    for field in model._meta.fields:\n        fieldName = field.name\n        if isinstance(field, (ForeignKey, ManyToManyField, OneToOneField)):\n            relatedModel = field.rel.to\n\n            def build_relation(inserted):\n                if ((relatedModel in inserted) and inserted[relatedModel]):\n                    return relatedModel.objects.get(pk=random.choice(inserted[relatedModel]))\n                if (not field.null):\n                    try:\n                        relatedModel.objects.order_by('?')[0]\n                    except IndexError:\n                        raise Exception(('Relation \"%s.%s\" with \"%s\" cannot be null, check order of addEntity list' % (field.model.__name__, field.name, relatedModel.__name__)))\n                return None\n            formatters[fieldName] = build_relation\n            continue\n        if isinstance(field, AutoField):\n            continue\n        formatter = nameGuesser.guessFormat(fieldName)\n        if formatter:\n            formatters[fieldName] = formatter\n            continue\n        formatter = fieldTypeGuesser.guessFormat(field)\n        if formatter:\n            formatters[fieldName] = formatter\n            continue\n    return formatters\n", "label": 0}
{"function": "\n\ndef haskell_type(view, filename, module_name, line, column, cabal=None):\n    result = None\n    if hsdev.hsdev_enabled():\n\n        def to_file_pos(r):\n            return FilePosition((int(r['line']) - 1), (int(r['column']) - 1))\n\n        def to_region_type(r):\n            return RegionType(r['type'], to_file_pos(r['region']['from']), to_file_pos(r['region']['to']))\n        ts = autocomplete.hsdev_client.ghcmod_type(filename, (line + 1), (column + 1), ghc=get_ghc_opts(filename))\n        if ts:\n            return [to_region_type(r) for r in ts]\n        return None\n    column = sublime_column_to_ghc_column(view, line, column)\n    line = (line + 1)\n    if hdevtools_enabled():\n        result = hdevtools_type(filename, line, column, cabal=cabal)\n    if ((not result) and module_name and ghcmod_enabled()):\n        result = ghcmod_type(filename, module_name, line, column)\n    return (parse_type_output(view, result) if result else None)\n", "label": 1}
{"function": "\n\ndef Finalize(self, state):\n    'Perform all checks that need to occur after all lines are processed.'\n    super(JavaScriptLintRules, self).Finalize(state)\n    if error_check.ShouldCheck(Rule.UNUSED_PRIVATE_MEMBERS):\n        unused_private_members = (self._declared_private_members - self._used_private_members)\n        for variable in unused_private_members:\n            token = self._declared_private_member_tokens[variable]\n            self._HandleError(errors.UNUSED_PRIVATE_MEMBER, ('Unused private member: %s.' % token.string), token)\n        self._declared_private_member_tokens = {\n            \n        }\n        self._declared_private_members = set()\n        self._used_private_members = set()\n    namespaces_info = self._namespaces_info\n    if (namespaces_info is not None):\n        if ((not namespaces_info.GetProvidedNamespaces()) and (not namespaces_info.GetRequiredNamespaces())):\n            missing_provides = namespaces_info.GetMissingProvides()\n            if missing_provides:\n                self._ReportMissingProvides(missing_provides, state.GetFirstToken(), None)\n            (missing_requires, illegal_alias) = namespaces_info.GetMissingRequires()\n            if missing_requires:\n                self._ReportMissingRequires(missing_requires, state.GetFirstToken(), None)\n            if illegal_alias:\n                self._ReportIllegalAliasStatement(illegal_alias)\n    self._CheckSortedRequiresProvides(state.GetFirstToken())\n", "label": 1}
{"function": "\n\ndef __new__(cls, e, z, z0, dir='+'):\n    e = sympify(e)\n    z = sympify(z)\n    z0 = sympify(z0)\n    if (z0 is S.Infinity):\n        dir = '-'\n    elif (z0 is S.NegativeInfinity):\n        dir = '+'\n    if isinstance(dir, string_types):\n        dir = Symbol(dir)\n    elif (not isinstance(dir, Symbol)):\n        raise TypeError(('direction must be of type basestring or Symbol, not %s' % type(dir)))\n    if (str(dir) not in ('+', '-')):\n        raise ValueError((\"direction must be either '+' or '-', not %s\" % dir))\n    obj = Expr.__new__(cls)\n    obj._args = (e, z, z0, dir)\n    return obj\n", "label": 0}
{"function": "\n\ndef validate(self, parser, options, args, cluster_name=False, node_name=False, load_cluster=False, load_node=True):\n    self.options = options\n    self.args = args\n    if (options.config_dir is None):\n        self.path = common.get_default_path()\n    else:\n        self.path = options.config_dir\n    if cluster_name:\n        if (len(args) == 0):\n            print_('Missing cluster name', file=sys.stderr)\n            parser.print_help()\n            exit(1)\n        self.name = args[0]\n    if node_name:\n        if (len(args) == 0):\n            print_('Missing node name', file=sys.stderr)\n            parser.print_help()\n            exit(1)\n        self.name = args[0]\n    if load_cluster:\n        self.cluster = self._load_current_cluster()\n        if (node_name and load_node):\n            try:\n                self.node = self.cluster.nodes[self.name]\n            except KeyError:\n                print_(('Unknown node %s in cluster %s' % (self.name, self.cluster.name)), file=sys.stderr)\n                exit(1)\n", "label": 1}
{"function": "\n\ndef attach_skins_and_materials(asset, definitions, material_name, default=True):\n    'Find all skins in the ``definitions`` asset which remap the material ``material_name``. Attach the found skin\\n    and references materials to the target ``asset``.'\n    skins = definitions.retrieve_skins()\n    for (skin, materials) in skins.iteritems():\n        attach_skin = False\n        for (v, k) in materials.iteritems():\n            if (v == material_name):\n                attach_skin = True\n                material = definitions.retrieve_material(k, default)\n                asset.attach_material(k, raw=material)\n        if attach_skin:\n            asset.attach_skin(skin, materials)\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    try:\n        if self._semlock._is_mine():\n            name = current_process().name\n            if (threading.current_thread().name != 'MainThread'):\n                name += ('|' + threading.current_thread().name)\n            count = self._semlock._count()\n        elif (self._semlock._get_value() == 1):\n            (name, count) = ('None', 0)\n        elif (self._semlock._count() > 0):\n            (name, count) = ('SomeOtherThread', 'nonzero')\n        else:\n            (name, count) = ('SomeOtherProcess', 'nonzero')\n    except Exception:\n        (name, count) = ('unknown', 'unknown')\n    return ('<RLock(%s, %s)>' % (name, count))\n", "label": 0}
{"function": "\n\ndef transform_data(self, data, request=None, response=None):\n    'Runs the transforms specified on this endpoint with the provided data, returning the data modified'\n    if (self.transform and (not (isinstance(self.transform, type) and isinstance(data, self.transform)))):\n        if self._params_for_transform:\n            return self.transform(data, **self._arguments(self._params_for_transform, request, response))\n        else:\n            return self.transform(data)\n    return data\n", "label": 0}
{"function": "\n\ndef chunkize(text, in_tag):\n    start = 0\n    idx = 0\n    chunks = []\n    for c in text:\n        if (c == '<'):\n            in_tag = True\n            if (start != idx):\n                chunks.append(('text', text[start:idx]))\n            start = idx\n        elif (c == '>'):\n            in_tag = False\n            if (start != (idx + 1)):\n                chunks.append(('tag', text[start:(idx + 1)]))\n            start = (idx + 1)\n        idx += 1\n    if (start != idx):\n        chunks.append((('tag' if in_tag else 'text'), text[start:idx]))\n    return (chunks, in_tag)\n", "label": 0}
{"function": "\n\ndef test_content_subreddit(reddit, terminal):\n    submissions = reddit.get_front_page(limit=5)\n    content = SubredditContent('front', submissions, terminal.loader)\n    assert (len(content._submission_data) == 1)\n    assert (content.get(0)['type'] == 'Submission')\n    for data in content.iterate(0, 1):\n        assert all(((k in data) for k in ('object', 'n_rows', 'offset', 'type', 'index', 'title', 'split_title', 'hidden')))\n        for val in data.values():\n            assert (not isinstance(val, six.binary_type))\n    with pytest.raises(IndexError):\n        content.get((- 1))\n    with pytest.raises(IndexError):\n        content.get(5)\n", "label": 0}
{"function": "\n\ndef check_can_access(self, request, can_retry=True, **kwargs):\n    'Check can user access'\n    project = self.get_project(**kwargs)\n    if (not project.can_access(request.user)):\n        if (project.organization and can_retry and request.user.is_authenticated()):\n            Project.objects.update_user_projects(request.user)\n            return self.check_can_access(request, False)\n        elif (not request.user.is_authenticated()):\n            return redirect_to_login(request.get_full_path(), self.get_login_url(), self.get_redirect_field_name())\n        raise PermissionDenied\n", "label": 0}
{"function": "\n\n@flow.StateHandler(next_state='ParseFiles')\ndef Start(self):\n    'Determine the Firefox history directory.'\n    self.state.Register('hist_count', 0)\n    self.state.Register('history_paths', [])\n    if self.args.history_path:\n        self.state.history_paths.append(self.args.history_path)\n    else:\n        self.state.history_paths = self.GuessHistoryPaths(self.args.username)\n        if (not self.state.history_paths):\n            raise flow.FlowError('Could not find valid History paths.')\n    if (self.runner.output is not None):\n        self.runner.output = aff4.FACTORY.Create(self.runner.output.urn, 'VFSAnalysisFile', token=self.token)\n    filename = 'places.sqlite'\n    for path in self.state.history_paths:\n        self.CallFlow('FileFinder', paths=[os.path.join(path, '**2', filename)], pathtype=self.state.args.pathtype, action=file_finder.FileFinderAction(action_type=file_finder.FileFinderAction.Action.DOWNLOAD), next_state='ParseFiles')\n", "label": 0}
{"function": "\n\ndef _step(self, dt):\n    \"pumps all the actions in the node actions container\\n\\n            The actions scheduled to be removed are removed.\\n            Then a :meth:`.Action.step` is called for each action in the\\n            node actions container, and if the action doesn't need any more step\\n            calls, it will be scheduled to be removed. When scheduled to be\\n            removed, the :meth:`.Action.stop` method for the action is called.\\n\\n        Arguments:\\n            dt (float):\\n                The time in seconds that elapsed since that last time this \\n                function was called.\\n        \"\n    for x in self.to_remove:\n        if (x in self.actions):\n            self.actions.remove(x)\n    self.to_remove = []\n    if self.skip_frame:\n        self.skip_frame = False\n        return\n    if (len(self.actions) == 0):\n        self.scheduled = False\n        pyglet.clock.unschedule(self._step)\n    for action in self.actions:\n        if (not action.scheduled_to_remove):\n            action.step(dt)\n            if action.done():\n                self.remove_action(action)\n", "label": 0}
{"function": "\n\ndef defaultColor(self, s):\n    if (s == QsciLexerJavaScript.Keyword):\n        return QColor('#514CA6')\n    elif ((s == QsciLexerJavaScript.Comment) or (s == QsciLexerJavaScript.CommentLine)):\n        return QColor('#29A349')\n    elif (s == QsciLexerJavaScript.DoubleQuotedString):\n        return QColor('#DB0909')\n    elif (s == QsciLexerJavaScript.Number):\n        return QColor('#961212')\n    return QColor('black')\n", "label": 0}
{"function": "\n\ndef truncate_paths(paths, max_samples):\n    '\\n    Truncate the list of paths so that the total number of samples is exactly equal to max_samples. This is done by\\n    removing extra paths at the end of the list, and make the last path shorter if necessary\\n    :param paths: a list of paths\\n    :param max_samples: the absolute maximum number of samples\\n    :return: a list of paths, truncated so that the number of samples adds up to max-samples\\n    '\n    paths = list(paths)\n    total_n_samples = sum((len(path['rewards']) for path in paths))\n    while ((len(paths) > 0) and ((total_n_samples - len(paths[(- 1)]['rewards'])) >= max_samples)):\n        total_n_samples -= len(paths.pop((- 1))['rewards'])\n    if (len(paths) > 0):\n        last_path = paths.pop((- 1))\n        truncated_last_path = dict()\n        truncated_len = (len(last_path['rewards']) - (total_n_samples - max_samples))\n        for (k, v) in last_path.iteritems():\n            if (k in ['observations', 'actions', 'rewards']):\n                truncated_last_path[k] = tensor_utils.truncate_tensor_list(v, truncated_len)\n            elif (k in ['env_infos', 'agent_infos']):\n                truncated_last_path[k] = tensor_utils.truncate_tensor_dict(v, truncated_len)\n            else:\n                raise NotImplementedError\n        paths.append(truncated_last_path)\n    return paths\n", "label": 0}
{"function": "\n\ndef get_active_filters(self):\n    ' Returns a list of tuples with all active filters contained in\\n        this manager and their names.\\n        '\n    ret = []\n    for (i_name, i) in self.filters.iteritems():\n        if isinstance(i, self.FilterGroup):\n            for (f_name, f) in i.filters.iteritems():\n                if f.active:\n                    ret.append((f, f_name))\n        elif i.active:\n            ret.append((i, i_name))\n    return ret\n", "label": 0}
{"function": "\n\ndef stop_server(jboss_config, host=None):\n    '\\n    Stop running jboss instance\\n\\n    jboss_config\\n        Configuration dictionary with properties specified above.\\n    host\\n        The name of the host. JBoss domain mode only - and required if running in domain mode.\\n        The host name is the \"name\" attribute of the \"host\" element in host.xml\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt \\'*\\' jboss7.stop_server \\'{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}\\'\\n\\n       '\n    log.debug('======================== MODULE FUNCTION: jboss7.stop_server')\n    if (host is None):\n        operation = ':shutdown'\n    else:\n        operation = '/host=\"{host}\"/:shutdown'.format(host=host)\n    shutdown_result = __salt__['jboss7_cli.run_operation'](jboss_config, operation, fail_on_error=False)\n    if (shutdown_result['success'] or ((not shutdown_result['success']) and ('Operation failed: Channel closed' in shutdown_result['stdout']))):\n        return shutdown_result\n    else:\n        raise Exception(\"Cannot handle error, return code={retcode}, stdout='{stdout}', stderr='{stderr}' \".format(**shutdown_result))\n", "label": 0}
{"function": "\n\n@weblab_api.route_web('/ilab/')\ndef ilab():\n    action = request.headers.get('SOAPAction')\n    if (action is None):\n        return 'No SOAPAction provided'\n    if (weblab_api.ctx.session_id is None):\n        return 'Not logged in!'\n    if (weblab_api.ctx.reservation_id is None):\n        try:\n            reservation_id_str = weblab_api.api.get_reservation_id_by_session_id()\n            weblab_api.ctx.reservation_id = reservation_id_str\n        except:\n            traceback.print_exc()\n    methods = {\n        '\"http://ilab.mit.edu/GetLabConfiguration\"': process_GetLabConfiguration,\n        '\"http://ilab.mit.edu/Submit\"': process_Submit,\n        '\"http://ilab.mit.edu/GetExperimentStatus\"': process_GetExperimentStatus,\n        '\"http://ilab.mit.edu/RetrieveResult\"': process_RetrieveResult,\n        '\"http://ilab.mit.edu/SaveAnnotation\"': process_SaveAnnotation,\n        '\"http://ilab.mit.edu/ListAllClientItems\"': process_ListAllClientItems,\n        '\"http://ilab.mit.edu/LoadClientItem\"': process_LoadClientItem,\n        '\"http://ilab.mit.edu/SaveClientItem\"': process_SaveClientItem,\n        '\"http://ilab.mit.edu/GetExperimentInformation\"': process_GetExperimentInformation,\n    }\n    if (not (action in methods)):\n        return 'Action not found'\n    response = make_response(methods[action]())\n    response.content_type = 'text/xml'\n    if hasattr(weblab_api.ctx, 'other_cookies'):\n        for (name, value) in weblab_api.ctx.other_cookies:\n            response.set_cookie(name, value, path=weblab_api.ctx.location)\n    return response\n", "label": 0}
{"function": "\n\ndef record_metadata(self, root):\n    ' Record metadata for all variables of interest.\\n\\n        Args\\n        ----\\n        root : `System`\\n           System containing variables.\\n        '\n    for recorder in self._recorders:\n        if (recorder._parallel or (self.rank == 0)):\n            if recorder.options['record_metadata']:\n                recorder.record_metadata(root)\n", "label": 0}
{"function": "\n\ndef xmlToAddrList(xml_file):\n    tree = etree.parse(xml_file)\n    root = tree.getroot()\n    addr_list = []\n    for element in root:\n        if ((element.tag == 'node') or (element.tag == 'way')):\n            address = {\n                \n            }\n            for x in element.iter('tag'):\n                addr = ast.literal_eval(str(x.attrib))\n                address[addr['k']] = addr['v']\n            addr_list.append(address)\n    return addr_list\n", "label": 0}
{"function": "\n\ndef get_language(self, language_code, site_id=None):\n    '\\n        Return the language settings for the current site\\n\\n        This function can be used with other settings variables\\n        to support modules which create their own variation of the ``PARLER_LANGUAGES`` setting.\\n        For an example, see :func:`~parler.appsettings.add_default_language_settings`.\\n        '\n    if (language_code is None):\n        if (get_language() is None):\n            raise ValueError(\"language_code can't be null, use translation.activate(..) when accessing translated models outside the request/response loop.\")\n        else:\n            raise ValueError(\"language_code can't be null\")\n    if (site_id is None):\n        site_id = getattr(settings, 'SITE_ID', None)\n    for lang_dict in self.get(site_id, ()):\n        if (lang_dict['code'] == language_code):\n            return lang_dict\n    for lang_dict in self.get(site_id, ()):\n        if (lang_dict['code'].split('-')[0] == language_code.split('-')[0]):\n            return lang_dict\n    return self['default']\n", "label": 0}
{"function": "\n\ndef __iter__(self):\n    table = self.table\n    source = self.source\n    encoding = self.encoding\n    errors = self.errors\n    lineterminator = self.lineterminator\n    caption = self.caption\n    index_header = self.index_header\n    tr_style = self.tr_style\n    td_styles = self.td_styles\n    vrepr = self.vrepr\n    truncate = self.truncate\n    with source.open('wb') as buf:\n        if PY2:\n            codec = getcodec(encoding)\n            f = codec.streamwriter(buf, errors=errors)\n        else:\n            f = io.TextIOWrapper(buf, encoding=encoding, errors=errors, newline='')\n        try:\n            it = iter(table)\n            hdr = next(it)\n            _write_begin(f, hdr, lineterminator, caption, index_header, truncate)\n            (yield hdr)\n            if (tr_style and callable(tr_style)):\n                it = (Record(row, hdr) for row in it)\n            for row in it:\n                _write_row(f, hdr, row, lineterminator, vrepr, tr_style, td_styles, truncate)\n                (yield row)\n            _write_end(f, lineterminator)\n            f.flush()\n        finally:\n            if (not PY2):\n                f.detach()\n", "label": 0}
{"function": "\n\ndef heartbeat_reply(self, stats):\n    if self.shut_down:\n        return\n    if (not self.online):\n        return\n    if (stats == None):\n        self.active_sessions = None\n        self.go_offline()\n    else:\n        sessions_created = active_sessions = active_streams = preceived = ptransmitted = 0\n        for line in stats.splitlines():\n            line_parts = line.split(':', 1)\n            if (line_parts[0] == 'sessions created'):\n                sessions_created = int(line_parts[1])\n            elif (line_parts[0] == 'active sessions'):\n                active_sessions = int(line_parts[1])\n            elif (line_parts[0] == 'active streams'):\n                active_streams = int(line_parts[1])\n            elif (line_parts[0] == 'packets received'):\n                preceived = int(line_parts[1])\n            elif (line_parts[0] == 'packets transmitted'):\n                ptransmitted = int(line_parts[1])\n            self.update_active(active_sessions, sessions_created, active_streams, preceived, ptransmitted)\n    Timeout(self.heartbeat, randomize(self.hrtb_ival, 0.1))\n", "label": 1}
{"function": "\n\ndef clean(self):\n    data = super(CreateNamespaceForm, self).clean()\n    metadef_file = data.get('metadef_file', None)\n    metadata_raw = data.get('direct_input', None)\n    if (metadata_raw and metadef_file):\n        raise ValidationError(_('Cannot specify both file and direct input.'))\n    if ((not metadata_raw) and (not metadef_file)):\n        raise ValidationError(_('No input was provided for the namespace content.'))\n    try:\n        if metadef_file:\n            ns_str = self.files['metadef_file'].read()\n        else:\n            ns_str = data['direct_input']\n        namespace = json.loads(ns_str)\n        if data['public']:\n            namespace['visibility'] = 'public'\n        else:\n            namespace['visibility'] = 'private'\n        namespace['protected'] = data['protected']\n        for protected_prop in constants.METADEFS_PROTECTED_PROPS:\n            namespace.pop(protected_prop, None)\n        data['namespace'] = namespace\n    except Exception as e:\n        msg = (_('There was a problem loading the namespace: %s.') % e)\n        raise forms.ValidationError(msg)\n    return data\n", "label": 1}
{"function": "\n\n@team_required\n@login_required\ndef team_leave(request):\n    team = request.team\n    state = team.state_for(request.user)\n    if ((team.manager_access == Team.MEMBER_ACCESS_INVITATION) and (state is None) and (not request.user.is_staff)):\n        raise Http404()\n    if (team.can_leave(request.user) and (request.method == 'POST')):\n        membership = Membership.objects.get(team=team, user=request.user)\n        membership.delete()\n        messages.success(request, MESSAGE_STRINGS['left-team'])\n        return redirect('dashboard')\n    else:\n        return redirect('team_detail', slug=team.slug)\n", "label": 0}
{"function": "\n\n@display_hook\ndef map_display(vmap, max_frames, max_branches):\n    if (not isinstance(vmap, (HoloMap, DynamicMap))):\n        return None\n    if ((len(vmap) == 0) and ((not isinstance(vmap, DynamicMap)) or vmap.sampled)):\n        return sanitize_HTML(vmap)\n    elif (len(vmap) > max_frames):\n        max_frame_warning(max_frames)\n        return sanitize_HTML(vmap)\n    return render(vmap)\n", "label": 0}
{"function": "\n\ndef _do_one_outer_iteration(self, **kwargs):\n    '\\n        One iteration of an outer iteration loop for an algorithm\\n        (e.g. time or parametric study)\\n        '\n    nan_tol = sp.isnan(self['pore.source_tol'])\n    nan_max = sp.isnan(self['pore.source_maxiter'])\n    self._tol_for_all = sp.amin(self['pore.source_tol'][(~ nan_tol)])\n    self._maxiter_for_all = sp.amax(self['pore.source_maxiter'][(~ nan_max)])\n    if (self._guess is None):\n        self._guess = sp.zeros(self._coeff_dimension)\n    t = 1\n    step = 0\n    while ((t > self._tol_for_all) and (step <= self._maxiter_for_all)):\n        (X, t, A, b) = self._do_inner_iteration_stage(guess=self._guess, **kwargs)\n        logger.info(((('tol for Picard source_algorithm in step ' + str(step)) + ' : ') + str(t)))\n        self._guess = X\n        step += 1\n    self._steps = step\n    if ((t >= self._tol_for_all) and (step > self._maxiter_for_all)):\n        raise Exception(((('Iterative algorithm for the source term reached to the maxiter: ' + str(self._maxiter_for_all)) + ' without achieving tol: ') + str(self._tol_for_all)))\n    logger.info('Picard algorithm for source term converged!')\n    self.A = A\n    self.b = b\n    self._tol_reached = t\n    return X\n", "label": 0}
{"function": "\n\ndef star_result(cluster, logdir, cmdline, *args):\n    'Re-print stdout/stderr for last run job(s)'\n    if (not args):\n        args = sorted(cluster.connections.keys())\n    for x in args:\n        job = cluster.last_result.get(cluster.locate(x), None)\n        if job:\n            res = job.result\n            running_time = (job.end_time - job.start_time)\n            if isinstance(res, CommandResult):\n                if res.stdout:\n                    cluster.console.q.put(((x, False), res.stdout.decode(cluster.defaults['character_encoding'], 'replace')))\n                if res.stderr:\n                    cluster.console.q.put(((x, True), res.stderr.decode(cluster.defaults['character_encoding'], 'replace')))\n            else:\n                cluster.console.q.put(((x, True), repr(res)))\n            cluster.console.join()\n", "label": 0}
{"function": "\n\ndef _contains(self, other):\n    from sympy.functions import arg, Abs\n    from sympy.core.containers import Tuple\n    other = sympify(other)\n    isTuple = isinstance(other, Tuple)\n    if (isTuple and (len(other) != 2)):\n        raise ValueError('expecting Tuple of length 2')\n    if (not self.polar):\n        (re, im) = (other if isTuple else other.as_real_imag())\n        for element in self.psets:\n            if And(element.args[0]._contains(re), element.args[1]._contains(im)):\n                return True\n        return False\n    elif self.polar:\n        if isTuple:\n            (r, theta) = other\n        elif other.is_zero:\n            (r, theta) = (S.Zero, S.Zero)\n        else:\n            (r, theta) = (Abs(other), arg(other))\n        for element in self.psets:\n            if And(element.args[0]._contains(r), element.args[1]._contains(theta)):\n                return True\n            return False\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_request(cls):\n    '\\n        Get the HTTPRequest object from thread storage or from a callee by searching\\n        each frame in the call stack.\\n        '\n    request = cls.get_global('request')\n    if request:\n        return request\n    try:\n        stack = inspect.stack()\n    except IndexError:\n        return\n    for (frame, _, _, _, _, _) in stack:\n        if ('request' in frame.f_locals):\n            if isinstance(frame.f_locals['request'], HttpRequest):\n                request = frame.f_locals['request']\n                cls.set_global('request', request)\n                return request\n", "label": 0}
{"function": "\n\ndef doMode(self, irc, msg):\n    if self.disabled(irc):\n        return\n    chanserv = self.registryValue('ChanServ')\n    on = ('on %s' % irc.network)\n    if ircutils.strEqual(msg.nick, chanserv):\n        channel = msg.args[0]\n        if (len(msg.args) == 3):\n            if ircutils.strEqual(msg.args[2], irc.nick):\n                mode = msg.args[1]\n                info = self.log.info\n                if (mode == '+o'):\n                    info('Received op from ChanServ in %s %s.', channel, on)\n                elif (mode == '+h'):\n                    info('Received halfop from ChanServ in %s %s.', channel, on)\n                elif (mode == '+v'):\n                    info('Received voice from ChanServ in %s %s.', channel, on)\n", "label": 0}
{"function": "\n\ndef do(self, *args):\n    'Run Django with ImportD.'\n    for bp in self.blueprint_list:\n        self._apply_blueprint(bp)\n    if (not args):\n        args = sys.argv[1:]\n    if (len(args) == 0):\n        return self._handle_management_command('runserver', '8000')\n    if ('livereload' in sys.argv):\n        if (not hasattr(self, 'lr')):\n            print('Livereload setting, lr not configured.')\n            return\n        from livereload import Server\n        server = Server(self)\n        for (pat, cmd) in self.lr.items():\n            parts = pat.split(',')\n            for part in parts:\n                server.watch(part, cmd)\n        server.serve(port=8000)\n        return\n    return self._act_as_manage(*args)\n", "label": 0}
{"function": "\n\ndef normalize(self):\n    subqueries = [q for q in self.subqueries if (q is not NullQuery)]\n    if (not subqueries):\n        return NullQuery\n    subqs = []\n    seenterms = set()\n    for s in subqueries:\n        s = s.normalize()\n        if (s is NullQuery):\n            continue\n        if isinstance(s, Term):\n            term = (s.fieldname, s.text)\n            if (term in seenterms):\n                continue\n            seenterms.add(term)\n        if isinstance(s, self.__class__):\n            subqs += s.subqueries\n        else:\n            subqs.append(s)\n    if (not subqs):\n        return NullQuery\n    if (len(subqs) == 1):\n        return subqs[0]\n    return self.__class__(subqs, boost=self.boost)\n", "label": 1}
{"function": "\n\ndef getWindow(title, exact=False):\n    \"Return Window object if 'title' or its part found in visible windows titles, else return None\\n\\n    Return only 1 window found first\\n    Args:\\n        title: unicode string\\n        exact (bool): True if search only exact match\\n    \"\n    titles = getWindows()\n    hwnd = titles.get(title, None)\n    if ((not hwnd) and (not exact)):\n        for (k, v) in titles.items():\n            if (title in k):\n                hwnd = v\n                break\n    if hwnd:\n        return Window(hwnd)\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    val = self._getval()\n    if isNamedClass(val, Parameter):\n        val = val._getval()\n    w = []\n    if (self.name is not None):\n        w.append((\"name='%s'\" % self.name))\n    if ((self.decimals is not None) and (val is not None)):\n        fmtstr = (('value=%.' + str(self.decimals)) + 'f')\n        string = (fmtstr % float(repr(val)))\n        if (self.stderr is not None):\n            fmtstr = ((' +/- %.' + str(self.decimals)) + 'f')\n            string = (string + (fmtstr % self.stderr))\n        if (self.units is not None):\n            string = (string + (' %s' % self.units))\n        w.append(string)\n    else:\n        w.append(('value=%s' % repr(val)))\n    w.append(('vary=%s' % repr(self.vary)))\n    if (self._expr is not None):\n        w.append((\"expr='%s'\" % self._expr))\n    if (self.min not in (None, (- inf))):\n        w.append(('min=%s' % repr(self.min)))\n    if (self.max not in (None, inf)):\n        w.append(('max=%s' % repr(self.max)))\n    return ('param(%s)' % ', '.join(w))\n", "label": 1}
{"function": "\n\ndef __call__(self, *args, **kw):\n    'Call instance class.'\n    if args:\n        if (not hasattr(self, '_configured')):\n            self._configure_django(DEBUG=True)\n        if (isinstance(args[0], dict) and (len(args) == 2)):\n            for bp in self.blueprint_list:\n                self.apply_blueprint(bp)\n            return self.wsgi_application(*args)\n        if self._is_management_command(args[0]):\n            self._handle_management_command(*args, **kw)\n            return self\n        if isinstance(args[0], list):\n            self.update_urls(args[0])\n            return self\n        if isinstance(args[0], Callable):\n            self.add_view('/{}/'.format(args[0].__name__), args[0])\n            return args[0]\n\n        def ddecorator(candidate):\n            from django.forms import forms\n            if isinstance(candidate, forms.DeclarativeFieldsMetaclass):\n                self.add_form(args[0], candidate, *args[1:], **kw)\n                return candidate\n            self.add_view(args[0], candidate, *args[1:], **kw)\n            return candidate\n        return ddecorator\n    else:\n        self._configure_django(**kw)\n    return self\n", "label": 1}
{"function": "\n\ndef get_notification_data(self):\n    '\\n        Provides custom notification data that is used by the transport layer\\n        when the feed is updated.\\n        '\n    notification_data = dict()\n    if (self.track_unseen and self.track_unread):\n        (unseen_count, unread_count) = self.feed_markers.count('unseen', 'unread')\n        notification_data['unseen_count'] = unseen_count\n        notification_data['unread_count'] = unread_count\n    elif self.track_unseen:\n        unseen_count = self.feed_markers.count('unseen')\n        notification_data['unseen_count'] = unseen_count\n    elif self.track_unread:\n        unread_count = self.feed_markers.count('unread')\n        notification_data['unread_count'] = unread_count\n    return notification_data\n", "label": 0}
{"function": "\n\ndef testGetArtifacts(self):\n    self._LoadAllArtifacts()\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows')\n    for result in results:\n        self.assertTrue((('Windows' in result.supported_os) or (not result.supported_os)))\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', name_list=['TestAggregationArtifact', 'TestFileArtifact'])\n    self.assertItemsEqual([x.name for x in results], ['TestAggregationArtifact'])\n    for result in results:\n        self.assertTrue((('Windows' in result.supported_os) or (not result.supported_os)))\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', source_type=artifact_registry.ArtifactSource.SourceType.REGISTRY_VALUE, name_list=['DepsProvidesMultiple'])\n    self.assertEqual(results.pop().name, 'DepsProvidesMultiple')\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', name_list=['RekallPsList'])\n    self.assertEqual(len(results), 1)\n    self.assertEqual(results.pop().name, 'RekallPsList')\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', exclude_dependents=True)\n    for result in results:\n        self.assertFalse(result.GetArtifactPathDependencies())\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', provides=['users.homedir', 'domain'])\n    for result in results:\n        self.assertTrue((len(set(result.provides).union(set(['users.homedir', 'domain']))) >= 1))\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', provides=['nothingprovidesthis'])\n    self.assertEqual(len(results), 0)\n", "label": 0}
{"function": "\n\ndef do_field(self, indent, value, **kwargs):\n    field_name = kwargs.pop('name')\n    field = getattr(self.form, field_name)\n    error = field.error\n    obj = self.form.fields[field_name]\n    help_string = (kwargs.pop('help_string', None) or field.help_string)\n    if ('label' in kwargs):\n        label = kwargs.pop('label')\n    else:\n        label = obj.label\n    if label:\n        obj.label = label\n        label_text = obj.get_label(_class='control-label')\n    else:\n        label_text = ''\n    _class = (self.get_class(obj) + ' control-group')\n    if (label_text == ''):\n        _class = (_class + ' nolabel')\n    if error:\n        _class = (_class + ' error')\n    field.field.html_attrs.update(kwargs)\n    if self.is_hidden(obj):\n        return str(field)\n    div_group = Tag('div', _class=_class, id=('div_' + obj.id))\n    with div_group:\n        if (self.get_widget_name(obj) == 'Checkbox'):\n            (div_group << '')\n        else:\n            (div_group << label_text)\n        div = Tag('div', _class='controls')\n        with div:\n            if (self.get_widget_name(obj) == 'Checkbox'):\n                (div << ((('<label>' + str(field)) + label) + '</label>'))\n            else:\n                (div << field)\n            (div << Tag('div', _class='help help-block', _value=help_string))\n            if error:\n                (div << Tag('div', _class='message help-block', _value=error))\n        (div_group << str(div))\n    return ((indent * ' ') + str(div_group))\n", "label": 1}
{"function": "\n\ndef _get_body_content_string(soup, comments=True):\n    if (not isinstance(soup, bs4.BeautifulSoup)):\n        soup = bs4.BeautifulSoup(soup, HTML_PARSER)\n    return ''.join(((('<!--{}-->'.format(C) if comments else '') if isinstance(C, bs4.Comment) else str(C)) for C in soup.body.contents))\n", "label": 0}
{"function": "\n\ndef handle(self, request, context):\n    try:\n        if (context['subnet_id'] == ''):\n            del context['subnet_id']\n        if (context['cidr'] == ''):\n            del context['cidr']\n        routes = context['external_routes']\n        p = []\n        if (len(routes) > 0):\n            for item in routes:\n                values = [i.split(':')[1] for i in item.split(',')]\n                values = {\n                    'destination': values[0],\n                    'nexthop': values[1],\n                }\n                p.append(values)\n        context['external_routes'] = p\n        external_segment = client.create_externalconnectivity(request, **context)\n        msg = _('External Connectivity successfully created.')\n        LOG.debug(msg)\n        messages.success(request, msg)\n        return external_segment\n    except Exception as e:\n        msg = str(e)\n        LOG.error(msg)\n        exceptions.handle(request, msg, redirect=shortcuts.redirect)\n", "label": 0}
{"function": "\n\ndef _removeSecurityGroup(self, input_dict, req, instance_id):\n    context = req.environ['nova.context']\n    try:\n        body = input_dict['removeSecurityGroup']\n        group_name = body['name']\n        instance_id = int(instance_id)\n    except ValueError:\n        msg = _('Server id should be integer')\n        raise exc.HTTPBadRequest(explanation=msg)\n    except TypeError:\n        msg = _('Missing parameter dict')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    except KeyError:\n        msg = _('Security group not specified')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    if ((not group_name) or (group_name.strip() == '')):\n        msg = _('Security group name cannot be empty')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        self.compute_api.remove_security_group(context, instance_id, group_name)\n    except exception.SecurityGroupNotFound as exp:\n        return exc.HTTPNotFound(explanation=unicode(exp))\n    except exception.InstanceNotFound as exp:\n        return exc.HTTPNotFound(explanation=unicode(exp))\n    except exception.Invalid as exp:\n        return exc.HTTPBadRequest(explanation=unicode(exp))\n    return exc.HTTPAccepted()\n", "label": 1}
{"function": "\n\ndef dget(self, section, option, default=None, type=str):\n    if (not self.has_option(section, option)):\n        return default\n    value = self.get(section, option)\n    if (type is int):\n        value = int(value)\n    elif (type is bool):\n        value = to_bool(value)\n    elif (type is float):\n        value = float(value)\n    elif (type is not str):\n        raise NotImplementedError()\n    return value\n", "label": 0}
{"function": "\n\ndef __call__(self, state, inst, state_dict):\n    state.manager = manager = manager_of_class(self.class_)\n    if (manager is None):\n        raise exc.UnmappedInstanceError(inst, ('Cannot deserialize object of type %r - no mapper() has been configured for this class within the current Python process!' % self.class_))\n    elif (manager.is_mapped and (not manager.mapper.configured)):\n        manager.mapper._configure_all()\n    if (inst is not None):\n        manager.setup_instance(inst, state)\n    manager.dispatch.unpickle(state, state_dict)\n", "label": 0}
{"function": "\n\ndef __defaultValueExtractor(plug, topLevelPlug, value):\n    with IECore.IgnoredExceptions(AttributeError):\n        value = value.value\n    if plug.isSame(topLevelPlug):\n        return value\n    for name in plug.relativeName(topLevelPlug).split('.'):\n        try:\n            value = getattr(value, name)\n        except AttributeError:\n            accessor = getattr(value, (('get' + name[0].upper()) + name[1:]), None)\n            if (accessor is not None):\n                value = accessor()\n            else:\n                return None\n    return value\n", "label": 0}
{"function": "\n\ndef _assert_fields_equal(self, a, b, exclude=None):\n    exclude = (exclude or [])\n    fields = {k: v for (k, v) in six.iteritems(self.db_type._fields) if (k not in exclude)}\n    assert_funcs = {\n        'mongoengine.fields.DictField': self.assertDictEqual,\n        'mongoengine.fields.ListField': self.assertListEqual,\n        'mongoengine.fields.SortedListField': self.assertListEqual,\n    }\n    for (k, v) in six.iteritems(fields):\n        assert_func = assert_funcs.get(str(v), self.assertEqual)\n        assert_func(getattr(a, k, None), getattr(b, k, None))\n", "label": 0}
{"function": "\n\n@serialized\ndef __init__(self, session, uri=None, sp_playlist=None, add_ref=True):\n    super(Playlist, self).__init__()\n    assert (uri or sp_playlist), 'uri or sp_playlist is required'\n    self._session = session\n    if (uri is not None):\n        sp_playlist = spotify.Link(self._session, uri)._as_sp_playlist()\n        if (sp_playlist is None):\n            raise spotify.Error(('Failed to get playlist from Spotify URI: %r' % uri))\n        session._cache[sp_playlist] = self\n        add_ref = False\n    if add_ref:\n        lib.sp_playlist_add_ref(sp_playlist)\n    self._sp_playlist = ffi.gc(sp_playlist, lib.sp_playlist_release)\n    self._sp_playlist_callbacks = None\n    self._lib = lib\n", "label": 0}
{"function": "\n\ndef __init__(self, disk_spec, vm_name, machine_type):\n    super(AzureDisk, self).__init__(disk_spec)\n    self.host_caching = FLAGS.azure_host_caching\n    self.name = None\n    self.vm_name = vm_name\n    self.lun = None\n    if (self.disk_type == PREMIUM_STORAGE):\n        self.metadata = PREMIUM_STORAGE_METADATA\n    elif (self.disk_type == STANDARD_DISK):\n        self.metadata = {\n            disk.MEDIA: disk.HDD,\n            disk.REPLICATION: AZURE_REPLICATION_MAP[FLAGS.azure_storage_type],\n            disk.LEGACY_DISK_TYPE: disk.STANDARD,\n        }\n    elif (self.disk_type == disk.LOCAL):\n        media = (disk.SSD if LocalDiskIsSSD(machine_type) else disk.HDD)\n        self.metadata = {\n            disk.MEDIA: media,\n            disk.REPLICATION: disk.NONE,\n            disk.LEGACY_DISK_TYPE: disk.LOCAL,\n        }\n", "label": 0}
{"function": "\n\ndef _list_subfolders(project, path, cached_folder_lists, recurse=True):\n    if (project not in cached_folder_lists):\n        cached_folder_lists[project] = dxpy.get_handler(project).describe(input_params={\n            'folders': True,\n        })['folders']\n    if recurse:\n        return (f for f in cached_folder_lists[project] if f.startswith(path))\n    else:\n        return (f for f in cached_folder_lists[project] if (f.startswith(path) and ('/' not in f[(len(path) + 1):])))\n", "label": 0}
{"function": "\n\n@classdef.method('inspect')\ndef method_inspect(self, space):\n    string_format = ((not self.symbol) or (not self.symbol[0].isalpha()) or (not self.symbol.isalnum()))\n    if string_format:\n        result = [':', '\"']\n        for c in self.symbol:\n            if (c == '\"'):\n                result.append('\\\\')\n            result.append(c)\n        result.append('\"')\n        return space.newstr_fromchars(result)\n    else:\n        return space.newstr_fromstr((':%s' % self.symbol))\n", "label": 0}
{"function": "\n\n@property\ndef case_filter(self):\n    now = datetime.datetime.utcnow()\n    fromdate = (now - timedelta(days=42))\n    filters = BaseHNBCReport.base_filters(self)\n    filters.append({\n        'term': {\n            'pp_case_filter.#value': '1',\n        },\n    })\n    filters.append({\n        'range': {\n            'date_birth.#value': {\n                'gte': json_format_date(fromdate),\n            },\n        },\n    })\n    status = self.request_params.get('PNC_status', '')\n    or_stmt = []\n    if status:\n        if (status == 'On Time'):\n            for i in range(1, 8):\n                filters.append({\n                    'term': {\n                        ('case_pp_%s_done.#value' % i): 'yes',\n                    },\n                })\n        else:\n            for i in range(1, 8):\n                or_stmt.append({\n                    'not': {\n                        'term': {\n                            ('case_pp_%s_done.#value' % i): 'yes',\n                        },\n                    },\n                })\n            or_stmt = {\n                'or': or_stmt,\n            }\n            filters.append(or_stmt)\n    return ({\n        'and': filters,\n    } if filters else {\n        \n    })\n", "label": 0}
{"function": "\n\ndef test_ModuleOrder():\n    o1 = ModuleOrder(lex, grlex, False)\n    o2 = ModuleOrder(ilex, lex, False)\n    assert (o1 == ModuleOrder(lex, grlex, False))\n    assert ((o1 != ModuleOrder(lex, grlex, False)) is False)\n    assert (o1 != o2)\n    assert (o1((1, 2, 3)) == (1, (5, (2, 3))))\n    assert (o2((1, 2, 3)) == ((- 1), (2, 3)))\n", "label": 0}
{"function": "\n\ndef test_Add_is_algebraic():\n    a = Symbol('a', algebraic=True)\n    b = Symbol('a', algebraic=True)\n    na = Symbol('na', algebraic=False)\n    nb = Symbol('nb', algebraic=False)\n    x = Symbol('x')\n    assert (a + b).is_algebraic\n    assert ((na + nb).is_algebraic is None)\n    assert ((a + na).is_algebraic is False)\n    assert ((a + x).is_algebraic is None)\n    assert ((na + x).is_algebraic is None)\n", "label": 0}
{"function": "\n\ndef tokenize(text):\n    tokenized = []\n    for c in punc:\n        text = text.replace(c, ((' ' + c) + ' '))\n    for c in contractions:\n        text = text.replace(c, (' |' + c))\n    text = text.replace(\" |'\", ' `')\n    text = text.replace(\"'\", \" ' \")\n    tokens = text.split(' ')\n    tokens = [token for token in tokens if token]\n    return tokens\n", "label": 0}
{"function": "\n\ndef get_ical(self, obj, request):\n    ' Returns a populated iCalendar instance. '\n    cal = vobject.iCalendar()\n    cal.add('method').value = 'PUBLISH'\n    items = self.__get_dynamic_attr('items', obj)\n    cal_name = self.__get_dynamic_attr('cal_name', obj)\n    cal_desc = self.__get_dynamic_attr('cal_desc', obj)\n    if cal_name:\n        cal.add('x-wr-calname').value = cal_name\n    if cal_desc:\n        cal.add('x-wr-caldesc').value = cal_desc\n    if get_current_site:\n        current_site = get_current_site(request)\n    else:\n        current_site = None\n    for item in items:\n        event = cal.add('vevent')\n        for (vkey, key) in EVENT_ITEMS:\n            value = self.__get_dynamic_attr(key, item)\n            if value:\n                if (vkey == 'rruleset'):\n                    event.rruleset = value\n                else:\n                    if ((vkey == 'url') and current_site):\n                        value = add_domain(current_site.domain, value, request.is_secure())\n                    event.add(vkey).value = value\n    return cal\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.I32):\n                self.success = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef initialization_test(self):\n    'Test the BaseErrorMeasure initialization.'\n    BaseErrorMeasure()\n    for percentage in [(- 1.2), (- 0.1), 100.1, 123.9]:\n        try:\n            BaseErrorMeasure(percentage)\n        except ValueError:\n            pass\n        else:\n            assert False\n    for percentage in [0.0, 12.3, 53.4, 100.0]:\n        try:\n            BaseErrorMeasure(percentage)\n        except ValueError:\n            assert False\n", "label": 0}
{"function": "\n\ndef configure_input(inp, op):\n    ' Configure the inp using op.'\n    if is_old_pipeline():\n        if op.is_a('vtkDataSet'):\n            inp.input = op\n        else:\n            inp.input = op.output\n    elif hasattr(op, 'output_port'):\n        if hasattr(inp, 'input_connection'):\n            inp.input_connection = op.output_port\n        elif hasattr(inp, 'set_input_connection'):\n            inp.set_input_connection(op.output_port)\n    elif op.is_a('vtkAlgorithmOutput'):\n        inp.input_connection = op\n    elif op.is_a('vtkDataSet'):\n        inp.set_input_data(op)\n    else:\n        raise ValueError(('Unknown input type for object %s' % op))\n", "label": 0}
{"function": "\n\n@ensure_valid_username\n@ensure_valid_groupname\ndef group_command(args):\n    session = make_session()\n    group = session.query(Group).filter_by(groupname=args.groupname).scalar()\n    if (not group):\n        logging.error('No such group %s'.format(args.groupname))\n        return\n    for username in args.username:\n        user = User.get(session, name=username)\n        if (not user):\n            logging.error(\"no such user '{}'\".format(username))\n            return\n        if (args.subcommand == 'add_member'):\n            if args.member:\n                role = 'member'\n            elif args.owner:\n                role = 'owner'\n            elif args.np_owner:\n                role = 'np-owner'\n            elif args.manager:\n                role = 'manager'\n            assert role\n            logging.info('Adding {} as {} to group {}'.format(username, role, args.groupname))\n            group.add_member(user, user, 'grouper-ctl join', status='actioned', role=role)\n            AuditLog.log(session, user.id, 'join_group', '{} manually joined via grouper-ctl'.format(username), on_group_id=group.id)\n            session.commit()\n        elif (args.subcommand == 'remove_member'):\n            logging.info('Removing {} from group {}'.format(username, args.groupname))\n            group.revoke_member(user, user, 'grouper-ctl remove')\n            AuditLog.log(session, user.id, 'leave_group', '{} manually left via grouper-ctl'.format(username), on_group_id=group.id)\n            session.commit()\n", "label": 1}
{"function": "\n\ndef _regenerate_derived_data():\n    global SUPPORTED_REGIONS, COUNTRY_CODES_FOR_NON_GEO_REGIONS, _NANPA_REGIONS\n    SUPPORTED_REGIONS.clear()\n    COUNTRY_CODES_FOR_NON_GEO_REGIONS.clear()\n    for (cc, region_codes) in COUNTRY_CODE_TO_REGION_CODE.items():\n        if ((len(region_codes) == 1) and (region_codes[0] == REGION_CODE_FOR_NON_GEO_ENTITY)):\n            COUNTRY_CODES_FOR_NON_GEO_REGIONS.add(cc)\n        else:\n            SUPPORTED_REGIONS.update(region_codes)\n    if (REGION_CODE_FOR_NON_GEO_ENTITY in SUPPORTED_REGIONS):\n        SUPPORTED_REGIONS.remove(REGION_CODE_FOR_NON_GEO_ENTITY)\n    _NANPA_REGIONS.clear()\n    _NANPA_REGIONS.update(COUNTRY_CODE_TO_REGION_CODE[_NANPA_COUNTRY_CODE])\n", "label": 0}
{"function": "\n\ndef generate_joins(self, joins, model_class, alias_map):\n    clauses = []\n    seen = set()\n    q = [model_class]\n    while q:\n        curr = q.pop()\n        if ((curr not in joins) or (curr in seen)):\n            continue\n        seen.add(curr)\n        for join in joins[curr]:\n            src = curr\n            dest = join.dest\n            if isinstance(join.on, (Expression, Func, Clause, Entity)):\n                constraint = join.on.clone().alias()\n            else:\n                metadata = join.metadata\n                if metadata.is_backref:\n                    fk_model = join.dest\n                    pk_model = join.src\n                else:\n                    fk_model = join.src\n                    pk_model = join.dest\n                fk = metadata.foreign_key\n                if fk:\n                    lhs = getattr(fk_model, fk.name)\n                    rhs = getattr(pk_model, fk.to_field.name)\n                    if metadata.is_backref:\n                        (lhs, rhs) = (rhs, lhs)\n                    constraint = (lhs == rhs)\n                else:\n                    raise ValueError('Missing required join predicate.')\n            if isinstance(dest, Node):\n                dest_n = dest\n            else:\n                q.append(dest)\n                dest_n = dest.as_entity().alias(alias_map[dest])\n            join_type = join.get_join_type()\n            if (join_type in self.join_map):\n                join_sql = SQL(self.join_map[join_type])\n            else:\n                join_sql = SQL(join_type)\n            clauses.append(Clause(join_sql, dest_n, SQL('ON'), constraint))\n    return clauses\n", "label": 1}
{"function": "\n\ndef read(config, args):\n    pp = pprint.PrettyPrinter()\n    url = (config['url'] + '/api/recording/')\n    if ((args['all'] is False) and (args['recording'] is None)):\n        print('No recording(s) specified to read. Use -h to see usage.')\n        return\n    params = {\n        \n    }\n    if ((args['recording'] is not None) and (args['all'] is False)):\n        url += (args['recording'] + '/')\n        if (args['traffic'] is True):\n            url += 'traffic/'\n        if (args['start'] is not None):\n            params['start'] = args['start']\n        if (args['offset'] is not None):\n            params['offset'] = args['offset']\n    token = auth.get_token(config)\n    r = requests.get(url, params=params, headers={\n        'X-Auth-Token': token,\n    })\n    if (r.status_code != 200):\n        print(('API returned status code %s' % r.status_code))\n        sys.exit(1)\n    else:\n        pp.pprint(json.loads(r.content))\n", "label": 1}
{"function": "\n\ndef test_full_flow(self):\n    rv = self.client.post(authorize_url, data={\n        'confirm': 'yes',\n    })\n    rv = self.client.get(clean_url(rv.location))\n    assert (b'access_token' in rv.data)\n    rv = self.client.get('/')\n    assert (b'username' in rv.data)\n    rv = self.client.get('/address')\n    assert (rv.status_code == 401)\n    assert (b'message' in rv.data)\n    rv = self.client.get('/method/post')\n    assert (b'POST' in rv.data)\n    rv = self.client.get('/method/put')\n    assert (b'PUT' in rv.data)\n    rv = self.client.get('/method/delete')\n    assert (b'DELETE' in rv.data)\n", "label": 0}
{"function": "\n\ndef build(self):\n    self.opt.log('Checking Root CA...')\n    self.checkRootCA()\n    self.opt.log('Checking Test Directory...')\n    self.initDirectory()\n    if self.opt.compFunc:\n        self.opt.log('Building Functionality Test Cases...')\n        cases = TestFunctionality(self.fqdn, self.info).build().getTestCases()\n        for test in cases:\n            self.addTestCase(test, self.opt.replace)\n    if self.opt.compCert:\n        self.opt.log('Building X509 Test Cases...')\n        cases = self.getAllTestCases(TestCase)\n        for test in cases:\n            self.addTestCase(test, self.opt.replace)\n    if self.opt.compOverflow:\n        self.opt.log('Building Overflow Test Cases...')\n        cases = TestOverflow(self.fqdn, self.info, self.opt.overflowLen).build().getTestCases()\n        for test in cases:\n            self.addTestCase(test, self.opt.replace)\n    self.baseCase = ValidCert(self.fqdn, self.info)\n    self.baseCase.testBuild(False)\n    saveSerial()\n    return self\n", "label": 0}
{"function": "\n\ndef clean(self):\n    external_segment_dict = {\n        \n    }\n    cleaned_data = super(UpdateL3PolicyForm, self).clean()\n    if self.is_valid():\n        ipversion = int(cleaned_data['ip_version'])\n        subnet_prefix_length = int(cleaned_data['subnet_prefix_length'])\n        msg = _('Subnet prefix out of range.')\n        if ((ipversion == 4) and (subnet_prefix_length not in range(2, 31))):\n            raise forms.ValidationError(msg)\n        if ((ipversion == 6) and (subnet_prefix_length not in range(2, 128))):\n            raise forms.ValidationError(msg)\n        if cleaned_data['external_segments']:\n            dic = {\n                \n            }\n            for external_segment in cleaned_data['external_segments']:\n                values = [i.split(':')[1] for i in external_segment.split(',')]\n                dic[values[0]] = [values[1]]\n                external_segment_dict.update(dic)\n            cleaned_data['external_segments'] = external_segment_dict\n        else:\n            cleaned_data['external_segments'] = {\n                \n            }\n        updated_data = {d: cleaned_data[d] for d in cleaned_data if (d in self.changed_data)}\n        cleaned_data = updated_data\n    return cleaned_data\n", "label": 1}
{"function": "\n\ndef update_subclass(self, target):\n    if (target not in self._clslevel):\n        self._clslevel[target] = collections.deque()\n    clslevel = self._clslevel[target]\n    for cls in target.__mro__[1:]:\n        if (cls in self._clslevel):\n            clslevel.extend([fn for fn in self._clslevel[cls] if (fn not in clslevel)])\n", "label": 0}
{"function": "\n\ndef authorize(self, cli, ev_msg):\n    if (('key' in self.props) and ((len(ev_msg['params']) < 2) or (self.props['key'] != ev_msg['params'][1]))):\n        cli.dump_numeric('475', [self.name, 'Cannot join channel (+k) - bad key'])\n        return False\n    if ('exempt' in self.props):\n        for e in self.props['exempt']:\n            if match(0, e, cli.hostmask):\n                return True\n    if ('ban' in self.props):\n        for b in self.props['ban']:\n            if match(0, b, cli.hostmask):\n                cli.dump_numeric('474', [self.name, 'You are banned.'])\n                return False\n    if (('invite' in self.props) and ('invite-exemption' in self.props)):\n        for i in self.props['invite-exemption']:\n            if match(0, i, cli.hostmask):\n                return True\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, offset, name=None):\n    if isinstance(offset, numbers.Integral):\n        offset = datetime.timedelta(minutes=offset)\n    elif (not isinstance(offset, datetime.timedelta)):\n        raise TypeError('offset must be an integer or datetime.timedelta')\n    if ((offset < self.MIN_PRECISION) and (total_seconds(offset) > 0)):\n        raise ValueError('the minimum precision of offset is minute')\n    elif (offset > self.MAX_PRECISION):\n        raise ValueError('the maximum precision of offset is hour')\n    if (name is None):\n        seconds = int(total_seconds(offset))\n        if (seconds != 0):\n            name = '{0:+03d}:{1:02d}'.format((seconds // 3600), ((abs(seconds) % 3600) // 60))\n        else:\n            name = 'UTC'\n    elif (not isinstance(name, basestring)):\n        raise TypeError(('name must be a string, not ' + repr(name)))\n    self.offset = offset\n    self.name = name\n", "label": 1}
{"function": "\n\n@staticmethod\ndef Mul(expr, assumptions):\n    '\\n        As long as there is at most only one noncommutative term:\\n        Hermitian*Hermitian         -> !Antihermitian\\n        Hermitian*Antihermitian     -> Antihermitian\\n        Antihermitian*Antihermitian -> !Antihermitian\\n        '\n    if expr.is_number:\n        return AskImaginaryHandler._number(expr, assumptions)\n    nccount = 0\n    result = False\n    for arg in expr.args:\n        if ask(Q.antihermitian(arg), assumptions):\n            result = (result ^ True)\n        elif (not ask(Q.hermitian(arg), assumptions)):\n            break\n        if ask((~ Q.commutative(arg)), assumptions):\n            nccount += 1\n            if (nccount > 1):\n                break\n    else:\n        return result\n", "label": 0}
{"function": "\n\n@staticmethod\ndef from_directory(path, verbosity=1):\n    'Static factory method for reading an EegSubject object from a subdirectory of \"eeg_full.\"\\n\\n        Keyword arguments:\\n        path -- path to a UCI EEG database subject folder\\n        verbosity -- level of verbosity\\n        '\n    if (verbosity > 0):\n        sys.stdout.write(('Reading EegSubject from directory ' + path))\n        if (verbosity > 1):\n            sys.stdout.write('\\n')\n        else:\n            sys.stdout.write(':')\n    m = re.match('.*/?(co\\\\d(a|c)\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d)', path)\n    subject_id = m.group(1)\n    alcoholic = (m.group(2) == 'a')\n    filenames = os.listdir(path)\n    trials_data = []\n    for filename in filenames:\n        if (verbosity > 1):\n            sys.stdout.write((('  reading EegTrial from ' + filename) + '...'))\n        else:\n            sys.stdout.write('.')\n        try:\n            trials_data.append(EegTrial.from_file(os.path.join(path, filename)))\n        except Exception as e:\n            sys.stdout.write((('ERROR: could not read EegTrial from file: ' + str(e)) + '\\n'))\n        else:\n            if (verbosity > 1):\n                sys.stdout.write('SUCCESS\\n')\n    sys.stdout.write('\\n')\n    return EegSubject(subject_id=subject_id, alcoholic=alcoholic, trials_data=trials_data)\n", "label": 0}
{"function": "\n\ndef defaultExpression(self, plug):\n    parentNode = plug.node().ancestor(Gaffer.Node)\n    if (parentNode is None):\n        return ''\n    if (not hasattr(plug, 'getValue')):\n        return ''\n    value = plug.getValue()\n    objectVector = IECore.ObjectVector()\n    try:\n        objectVector.append(value)\n    except:\n        return ''\n\n    def canExtractValue(plug, topLevelPlug, value):\n        if (not len(plug)):\n            return (_extractPlugValue(plug, topLevelPlug, value) is not None)\n        else:\n            for child in plug.children():\n                if (not canExtractValue(child, topLevelPlug, value)):\n                    return False\n            return True\n    if (not canExtractValue(plug, plug, objectVector[0])):\n        return ''\n    result = ''\n    modulePath = Gaffer.Serialisation.modulePath(value)\n    if (modulePath not in ('IECore', '')):\n        result += (('import ' + modulePath) + '\\n\\n')\n    result += 'parent[\"'\n    result += plug.relativeName(parentNode).replace('.', '\"][\"')\n    result += '\"] = '\n    result += repr(value)\n    return result\n", "label": 0}
{"function": "\n\ndef __getitem__(self, seq):\n    'return nlmsaID,NLMSASequence,offset for a given seq'\n    if (not hasattr(seq, 'annotationType')):\n        try:\n            return self._cache[seq.pathForward]\n        except AttributeError:\n            raise KeyError('key must be a sequence interval!')\n        except KeyError:\n            pass\n    seqID = self.getSeqID(seq)\n    try:\n        (nlmsaID, nsID, offset) = self.seqIDdict[seqID]\n    except KeyError:\n        raise KeyError('seq not found in this alignment')\n    v = (nlmsaID, self.seqlist[nsID], offset)\n    if (not hasattr(seq, 'annotationType')):\n        self._cache[seq.pathForward] = _NLMSASeqDict_ValueWrapper(*v)\n    return v\n", "label": 0}
{"function": "\n\ndef parse_sc_packet(self, packet):\n    try:\n        (_, data_and_metadata) = packet.split('|', 1)\n        if (data_and_metadata.count('|') == 1):\n            (check_name, status) = data_and_metadata.split('|')\n            metadata = ''\n        else:\n            (check_name, status, metadata) = data_and_metadata.split('|', 2)\n        service_check = {\n            'check_name': check_name,\n            'status': int(status),\n        }\n        message_delimiter = ('|m:' if ('|m:' in metadata) else 'm:')\n        if (message_delimiter in metadata):\n            (meta, message) = metadata.rsplit(message_delimiter, 1)\n            service_check['message'] = self._unescape_sc_content(message)\n        else:\n            meta = metadata\n        if (not meta):\n            return service_check\n        meta = unicode(meta)\n        for m in meta.split('|'):\n            if (m[0] == 'd'):\n                service_check['timestamp'] = float(m[2:])\n            elif (m[0] == 'h'):\n                service_check['hostname'] = m[2:]\n            elif (m[0] == '#'):\n                service_check['tags'] = sorted(m[1:].split(','))\n        return service_check\n    except (IndexError, ValueError):\n        raise Exception(('Unparseable service check packet: %s' % packet))\n", "label": 1}
{"function": "\n\ndef install(packages, update=False, options=None, version=None):\n    \"\\n    Install one or more packages.\\n\\n    If *update* is ``True``, the package definitions will be updated\\n    first, using :py:func:`~fabtools.deb.update_index`.\\n\\n    Extra *options* may be passed to ``apt-get`` if necessary.\\n\\n    Example::\\n\\n        import fabtools\\n\\n        # Update index, then install a single package\\n        fabtools.deb.install('build-essential', update=True)\\n\\n        # Install multiple packages\\n        fabtools.deb.install([\\n            'python-dev',\\n            'libxml2-dev',\\n        ])\\n\\n        # Install a specific version\\n        fabtools.deb.install('emacs', version='23.3+1-1ubuntu9')\\n\\n    \"\n    manager = MANAGER\n    if update:\n        update_index()\n    if (options is None):\n        options = []\n    if (version is None):\n        version = ''\n    if (version and (not isinstance(packages, list))):\n        version = ('=' + version)\n    if (not isinstance(packages, basestring)):\n        packages = ' '.join(packages)\n    options.append('--quiet')\n    options.append('--assume-yes')\n    options = ' '.join(options)\n    cmd = ('%(manager)s install %(options)s %(packages)s%(version)s' % locals())\n    run_as_root(cmd, pty=False)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef log(expr, assumptions):\n    if ask(Q.real(expr.args[0]), assumptions):\n        if ask(Q.positive(expr.args[0]), assumptions):\n            return False\n        return\n    if (expr.args[0].func == exp):\n        if (expr.args[0].args[0] in [I, (- I)]):\n            return True\n    im = ask(Q.imaginary(expr.args[0]), assumptions)\n    if (im is False):\n        return False\n", "label": 0}
{"function": "\n\n@cache\ndef search(query, results=10, suggestion=False):\n    '\\n  Do a Wikipedia search for `query`.\\n\\n  Keyword arguments:\\n\\n  * results - the maxmimum number of results returned\\n  * suggestion - if True, return results and suggestion (if any) in a tuple\\n  '\n    search_params = {\n        'list': 'search',\n        'srprop': '',\n        'srlimit': results,\n        'limit': results,\n        'srsearch': query,\n    }\n    if suggestion:\n        search_params['srinfo'] = 'suggestion'\n    raw_results = _wiki_request(search_params)\n    if ('error' in raw_results):\n        if (raw_results['error']['info'] in ('HTTP request timed out.', 'Pool queue is full')):\n            raise HTTPTimeoutError(query)\n        else:\n            raise WikipediaException(raw_results['error']['info'])\n    search_results = (d['title'] for d in raw_results['query']['search'])\n    if suggestion:\n        if raw_results['query'].get('searchinfo'):\n            return (list(search_results), raw_results['query']['searchinfo']['suggestion'])\n        else:\n            return (list(search_results), None)\n    return list(search_results)\n", "label": 0}
{"function": "\n\n@click.command()\n@click.option('--ip-version', help='Display only IPv4', type=click.Choice(['v4', 'v6']))\n@environment.pass_env\ndef cli(env, ip_version):\n    'List all global IPs.'\n    mgr = SoftLayer.NetworkManager(env.client)\n    table = formatting.Table(['id', 'ip', 'assigned', 'target'])\n    version = None\n    if (ip_version == 'v4'):\n        version = 4\n    elif (ip_version == 'v6'):\n        version = 6\n    ips = mgr.list_global_ips(version=version)\n    for ip_address in ips:\n        assigned = 'No'\n        target = 'None'\n        if ip_address.get('destinationIpAddress'):\n            dest = ip_address['destinationIpAddress']\n            assigned = 'Yes'\n            target = dest['ipAddress']\n            virtual_guest = dest.get('virtualGuest')\n            if virtual_guest:\n                target += (' (%s)' % virtual_guest['fullyQualifiedDomainName'])\n            elif ip_address['destinationIpAddress'].get('hardware'):\n                target += (' (%s)' % dest['hardware']['fullyQualifiedDomainName'])\n        table.add_row([ip_address['id'], ip_address['ipAddress']['ipAddress'], assigned, target])\n    env.fout(table)\n", "label": 0}
{"function": "\n\ndef run(self):\n    self.log = logging.getLogger()\n    for x in self.log.handlers:\n        self.log.removeHandler(x)\n    configure_logging(level=self.log_level, format=('%%(asctime)-15s %%(process)d %s %%(levelname).1s: %%(message)s' % self.name), filename=self.log_filename)\n    self.log.debug('Starting')\n    signal.signal(signal.SIGTERM, signal.SIG_DFL)\n    backend = get_backend()\n    self.log.info('Loaded backend %s', backend)\n    time_item_last_processed = datetime.datetime.utcnow()\n    for item_count in itertools.count():\n        if (not self.running.value):\n            break\n        if self.idle_time_reached(time_item_last_processed):\n            self.log.info('Exiting due to reaching idle time limit')\n            break\n        if (item_count > 1000):\n            self.log.info('Exiting due to reaching item limit')\n            break\n        try:\n            item_processed = self.process(backend)\n            if item_processed:\n                time_item_last_processed = datetime.datetime.utcnow()\n        except KeyboardInterrupt:\n            sys.exit(1)\n    self.log.info('Exiting')\n", "label": 0}
{"function": "\n\n@method_decorator(catch_and_raise_exceptions)\ndef get_total_row(self):\n\n    def _clean_total_row(val, col):\n        if isinstance(val, numbers.Number):\n            return val\n        elif col.calculate_total:\n            return 0\n        return ''\n\n    def _get_relevant_column_ids(col, column_id_to_expanded_column_ids):\n        return column_id_to_expanded_column_ids.get(col.column_id, [col.column_id])\n    expanded_columns = get_expanded_columns(self.column_configs, self.config)\n    qc = self.query_context()\n    for c in self.columns:\n        qc.append_column(c.view)\n    session = connection_manager.get_scoped_session(self.engine_id)\n    totals = qc.totals(session.connection(), [column_id for col in self.column_configs for column_id in _get_relevant_column_ids(col, expanded_columns) if col.calculate_total], self.filter_values)\n    total_row = [_clean_total_row(totals.get(column_id), col) for col in self.column_configs for column_id in _get_relevant_column_ids(col, expanded_columns)]\n    if (total_row and (total_row[0] is '')):\n        total_row[0] = ugettext('Total')\n    return total_row\n", "label": 1}
{"function": "\n\ndef _configure(self, as_defaults, kw):\n    if self._started:\n        raise TypeError('this TransactionFactory is already started')\n    not_supported = []\n    for (k, v) in kw.items():\n        for dict_ in (self._url_cfg, self._engine_cfg, self._maker_cfg, self._ignored_cfg, self._facade_cfg, self._transaction_ctx_cfg):\n            if (k in dict_):\n                dict_[k] = (_Default(v) if as_defaults else v)\n                break\n        else:\n            not_supported.append(k)\n    if not_supported:\n        warnings.warn(('Configuration option(s) %r not supported' % sorted(not_supported)), exception.NotSupportedWarning)\n", "label": 0}
{"function": "\n\ndef additional_validators(self, key, column):\n    '\\n        Returns additional validators for given column\\n\\n        :param key: String key of the column property\\n        :param column: SQLAlchemy Column object\\n        '\n    validators = []\n    if (key in self.meta.validators):\n        try:\n            validators.extend(self.meta.validators[key])\n        except TypeError:\n            validators.append(self.meta.validators[key])\n    if (('validators' in column.info) and column.info['validators']):\n        try:\n            validators.extend(column.info['validators'])\n        except TypeError:\n            validators.append(column.info['validators'])\n    return validators\n", "label": 0}
{"function": "\n\ndef add_group(self, name, exclusive, group_filters=None, overwrite=False):\n    ' Adds a filter group.\\n\\n        :param str name: The name of the new filter group.\\n        :param bool exclusive: Determines if the new group is exclusive, i.e.\\n            only one of its items can be active at the same time.\\n        :param bool overwrite: If ``True``, an existing group with the same\\n            name will be overwritten. If ``False`` and a group with the same\\n            name exists, a value error is raised.\\n            Default: False\\n        '\n    if (name in self.filters):\n        prev = self.filters[name]\n        if isinstance(prev, FilterManager.Filter):\n            raise ValueError('A filter with this name already exists!')\n        if (not overwrite):\n            raise ValueError('A filter group with this name already exists!')\n    g = self.FilterGroup(exclusive)\n    if group_filters:\n        if exclusive:\n            for f in group_filters.itervalues():\n                f.active = False\n            group_filters.values()[0].active = True\n        g.filters = group_filters\n    else:\n        g.filters = OrderedDict()\n    self.filters[name] = g\n", "label": 0}
{"function": "\n\ndef validate(self, value):\n    if self.validator:\n        value = self.validator.validate(value)\n    if (self.min is not None):\n        if ((value < self.min) or ((value == self.min) and (self.include_min is False))):\n            self.error(value)\n    if (self.max is not None):\n        if ((value > self.max) or ((value == self.max) and (self.include_max is False))):\n            self.error(value)\n    return value\n", "label": 1}
{"function": "\n\ndef invalid_example_number(region_code):\n    'Gets an invalid number for the specified region.\\n\\n    This is useful for unit-testing purposes, where you want to test what\\n    will happen with an invalid number. Note that the number that is\\n    returned will always be able to be parsed and will have the correct\\n    country code. It may also be a valid *short* number/code for this\\n    region. Validity checking such numbers is handled with shortnumberinfo.\\n\\n    Arguments:\\n    region_code -- The region for which an example number is needed.\\n\\n\\n    Returns an invalid number for the specified region. Returns None when an\\n    unsupported region or the region 001 (Earth) is passed in.\\n    '\n    if (not _is_valid_region_code(region_code)):\n        return None\n    metadata = PhoneMetadata.metadata_for_region(region_code.upper())\n    desc = _number_desc_for_type(metadata, PhoneNumberType.FIXED_LINE)\n    if (desc.example_number is None):\n        return None\n    example_number = desc.example_number\n    phone_number_length = (len(example_number) - 1)\n    while (phone_number_length >= _MIN_LENGTH_FOR_NSN):\n        number_to_try = example_number[:phone_number_length]\n        try:\n            possibly_valid_number = parse(number_to_try, region_code)\n            if (not is_valid_number(possibly_valid_number)):\n                return possibly_valid_number\n        except NumberParseException:\n            pass\n        phone_number_length -= 1\n    return None\n", "label": 0}
{"function": "\n\ndef get_ports_for_vlan(self, vlan):\n    ports = []\n    for port in self.switch_configuration.ports:\n        if (not isinstance(port, VlanPort)):\n            if ((port.trunk_vlans and (vlan.number in port.trunk_vlans)) or (port.access_vlan == vlan.number)):\n                ports.append(port)\n    return ports\n", "label": 0}
{"function": "\n\ndef _resourceOverlap(self, res1_start_time, res1_end_time, res2_start_time, res2_end_time):\n    now = datetime.datetime.utcnow()\n    forever = datetime.datetime(9999, 1, 1)\n    r1s = (res1_start_time or now)\n    r2s = (res2_start_time or now)\n    r1e = (res1_end_time or forever)\n    r2e = (res2_end_time or forever)\n    assert (r1s < r1e), 'Cannot detect overlap for backwards reservation (1)'\n    assert (r2s < r2e), 'Cannot detect overlap for backwards reservation (2)'\n    if (r2e < r1s):\n        return False\n    if (r2s > r1e):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef slice_locs(self, start=None, end=None, step=None, kind=None):\n    \"\\n        Compute slice locations for input labels.\\n\\n        Parameters\\n        ----------\\n        start : label, default None\\n            If None, defaults to the beginning\\n        end : label, default None\\n            If None, defaults to the end\\n        step : int, defaults None\\n            If None, defaults to 1\\n        kind : {'ix', 'loc', 'getitem'} or None\\n\\n        Returns\\n        -------\\n        start, end : int\\n\\n        \"\n    inc = ((step is None) or (step >= 0))\n    if (not inc):\n        (start, end) = (end, start)\n    start_slice = None\n    if (start is not None):\n        start_slice = self.get_slice_bound(start, 'left', kind)\n    if (start_slice is None):\n        start_slice = 0\n    end_slice = None\n    if (end is not None):\n        end_slice = self.get_slice_bound(end, 'right', kind)\n    if (end_slice is None):\n        end_slice = len(self)\n    if (not inc):\n        (end_slice, start_slice) = ((start_slice - 1), (end_slice - 1))\n        if (end_slice == (- 1)):\n            end_slice -= len(self)\n        if (start_slice == (- 1)):\n            start_slice -= len(self)\n    return (start_slice, end_slice)\n", "label": 1}
{"function": "\n\n@classmethod\ndef parse(cls, buf):\n    'Parse DHCP Packet.\\n\\n        1. To get client IP Address(ciaddr).\\n        2. To get relaying gateway IP Address(giaddr).\\n        3. To get DHCP Relay Agent Information Option Suboption\\n        such as Link Selection, VSS, Server Identifier override.\\n        '\n    pkt = DhcpPacket()\n    (pkt.ciaddr,) = cls.struct('4s').unpack_from(buf, 12)\n    (pkt.giaddr,) = cls.struct('4s').unpack_from(buf, 24)\n    cls.struct('4s').pack_into(buf, 24, b'')\n    pos = 240\n    while (pos < len(buf)):\n        (opttag,) = cls.struct('B').unpack_from(buf, pos)\n        if (opttag == 0):\n            pos += 1\n            continue\n        if (opttag == END):\n            pkt.end = pos\n            break\n        (optlen,) = cls.struct('B').unpack_from(buf, (pos + 1))\n        startpos = pos\n        pos += 2\n        if (opttag != RELAY_AGENT_INFO):\n            pos += optlen\n            continue\n        optend = (pos + optlen)\n        while (pos < optend):\n            (subopttag, suboptlen) = cls.struct('BB').unpack_from(buf, pos)\n            fmt = ('%is' % (suboptlen,))\n            (val,) = cls.struct(fmt).unpack_from(buf, (pos + 2))\n            pkt.relay_options[subopttag] = val\n            pos += (suboptlen + 2)\n        cls.struct(('%is' % (optlen + 2))).pack_into(buf, startpos, b'')\n    pkt.buf = buf\n    return pkt\n", "label": 0}
{"function": "\n\ndef handle_starttag(self, tag, attributes):\n    name = [v for (k, v) in attributes if (k == 'id')]\n    if name:\n        self.names.append(name[0])\n    if (tag == 'a'):\n        name = [v for (k, v) in attributes if (k == 'name')]\n        if name:\n            self.names.append(name[0])\n", "label": 0}
{"function": "\n\ndef build_message_from_gitlog(user_profile, name, ref, commits, before, after, url, pusher, forced=None, created=None):\n    short_ref = re.sub('^refs/heads/', '', ref)\n    subject = name\n    if re.match('^0+$', after):\n        content = ('%s deleted branch %s' % (pusher, short_ref))\n    elif ((forced and (not created)) or ((forced is None) and (len(commits) == 0))):\n        content = ('%s [force pushed](%s) to branch %s.  Head is now %s' % (pusher, url, short_ref, after[:7]))\n    else:\n        content = build_commit_list_content(commits, short_ref, url, pusher)\n    return (subject, content)\n", "label": 0}
{"function": "\n\ndef __new__(mcs, *args, **kwargs):\n    \"\\n        Handles the url_map generation based on the rules from the base classes\\n        Added to the class' own rules\\n        \"\n    name = args[0]\n    bases = args[1]\n    members = args[2].copy()\n    rules = members.get('rules', [])\n    for base in (base for base in bases if hasattr(base, 'rules')):\n        for base_rule in base.rules:\n            if (not any((((base_rule.rule == rule.rule) and set(base_rule.methods).intersection(set(rule.methods))) for rule in rules))):\n                rules.append(base_rule)\n    url_map = members.get('url_map', Map())\n    for rule in rules:\n        url_map.add(Rule(rule.rule, methods=rule.methods, endpoint=rule.endpoint))\n    members['rules'] = list(url_map.iter_rules())\n    members['url_map'] = url_map\n    return super(mcs, mcs).__new__(mcs, *(name, bases, members), **kwargs)\n", "label": 1}
{"function": "\n\ndef test_expando_update():\n    d1 = {\n        'a': 123,\n        'b': 'abc',\n    }\n    x = Expando(d1)\n    assert (x.a == d1['a'])\n    assert (x.b == d1['b'])\n    d = {\n        'b': {\n            'c': 456,\n            'd': {\n                'e': 'abc',\n            },\n        },\n        'f': 'lmn',\n    }\n    x.update(d)\n    assert (x.a == d1['a'])\n    assert (x.b.c == d['b']['c'])\n    assert (x.b.d.e == d['b']['d']['e'])\n    assert (x.f == d['f'])\n    d2 = {\n        'a': 789,\n        'f': 'opq',\n    }\n    y = Expando(d2)\n    x.update(y)\n    assert (x.a == 789)\n    assert (x.f == 'opq')\n", "label": 1}
{"function": "\n\ndef _getActivity(self, serviceRecord, dbcl, path):\n    activityData = None\n    try:\n        (f, metadata) = dbcl.get_file_and_metadata(path)\n    except rest.ErrorResponse as e:\n        self._raiseDbException(e)\n    if (not activityData):\n        activityData = f.read()\n    try:\n        if path.lower().endswith('.tcx'):\n            act = TCXIO.Parse(activityData)\n        else:\n            act = GPXIO.Parse(activityData)\n    except ValueError as e:\n        raise APIExcludeActivity(('Invalid GPX/TCX ' + str(e)), activity_id=path, user_exception=UserException(UserExceptionType.Corrupt))\n    except lxml.etree.XMLSyntaxError as e:\n        raise APIExcludeActivity(('LXML parse error ' + str(e)), activity_id=path, user_exception=UserException(UserExceptionType.Corrupt))\n    return (act, metadata['rev'])\n", "label": 0}
{"function": "\n\ndef paired_tests(paired_test, options, test_labels):\n    state = setup(int(options.verbosity), test_labels)\n    if (not test_labels):\n        print('')\n        from django.db.models.loading import get_apps\n        test_labels = [app.__name__.split('.')[(- 2)] for app in get_apps()]\n    print('***** Trying paired execution')\n    for label in [paired_test, 'model_inheritance_same_model_name']:\n        try:\n            test_labels.remove(label)\n        except ValueError:\n            pass\n    subprocess_args = [sys.executable, upath(__file__), ('--settings=%s' % options.settings)]\n    if options.failfast:\n        subprocess_args.append('--failfast')\n    if options.verbosity:\n        subprocess_args.append(('--verbosity=%s' % options.verbosity))\n    if (not options.interactive):\n        subprocess_args.append('--noinput')\n    for (i, label) in enumerate(test_labels):\n        print(('***** %d of %d: Check test pairing with %s' % ((i + 1), len(test_labels), label)))\n        failures = subprocess.call((subprocess_args + [label, paired_test]))\n        if failures:\n            print(('***** Found problem pair with %s' % label))\n            return\n    print('***** No problem pair found')\n    teardown(state)\n", "label": 1}
{"function": "\n\ndef match_hostname(cert, hostname):\n    'Verify that *cert* (in decoded format as returned by\\n    SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 rules\\n    are mostly followed, but IP addresses are not accepted for *hostname*.\\n\\n    CertificateError is raised on failure. On success, the function\\n    returns nothing.\\n    '\n    if (not cert):\n        raise ValueError('empty or no certificate')\n    dnsnames = []\n    san = cert.get('subjectAltName', ())\n    for (key, value) in san:\n        if (key == 'DNS'):\n            if _dnsname_to_pat(value).match(hostname):\n                return\n            dnsnames.append(value)\n    if (not san):\n        for sub in cert.get('subject', ()):\n            for (key, value) in sub:\n                if (key == 'commonName'):\n                    if _dnsname_to_pat(value).match(hostname):\n                        return\n                    dnsnames.append(value)\n    if (len(dnsnames) > 1):\n        raise CertificateError((\"hostname %r doesn't match either of %s\" % (hostname, ', '.join(map(repr, dnsnames)))))\n    elif (len(dnsnames) == 1):\n        raise CertificateError((\"hostname %r doesn't match %r\" % (hostname, dnsnames[0])))\n    else:\n        raise CertificateError('no appropriate commonName or subjectAltName fields were found')\n", "label": 1}
{"function": "\n\ndef __init__(self, host=None, port=None, auth_mech=None):\n    if (host is not None):\n        self.host = host\n    elif ('IMPYLA_TEST_HOST' in os.environ):\n        self.host = os.environ['IMPYLA_TEST_HOST']\n    else:\n        sys.stderr.write(\"IMPYLA_TEST_HOST not set; using 'localhost'\")\n        self.host = 'localhost'\n    if (port is not None):\n        self.port = port\n    elif ('IMPYLA_TEST_PORT' in os.environ):\n        self.port = int(os.environ['IMPYLA_TEST_PORT'])\n    else:\n        sys.stderr.write('IMPYLA_TEST_PORT not set; using 21050')\n        self.port = 21050\n    if (auth_mech is not None):\n        self.auth_mech = auth_mech\n    elif ('IMPYLA_TEST_AUTH_MECH' in os.environ):\n        self.auth_mech = os.environ['IMPYLA_TEST_AUTH_MECH']\n    else:\n        sys.stderr.write(\"IMPYLA_TEST_AUTH_MECH not set; using 'NOSASL'\")\n        self.auth_mech = 'NOSASL'\n", "label": 0}
{"function": "\n\ndef off(self, event=None, *handlers):\n    'unregister an event or handler from an event'\n    if (not event):\n        self.events.clear()\n        return True\n    if (not (event in self.events)):\n        raise EventNotFound(event)\n    if (not handlers):\n        self.events.pop(event, None)\n        return True\n    for callback in handlers:\n        if (not (callback in self.events[event])):\n            raise HandlerNotFound(event, callback)\n        while (callback in self.events[event]):\n            self.events[event].remove(callback)\n    return True\n", "label": 0}
{"function": "\n\ndef standardize_Y(shape, Y):\n    if (not numpy_array(Y)):\n        Y = np.asarray(Y)\n    if (len(Y.shape) == 1):\n        Y = Y.reshape((- 1), 1)\n    if ((len(Y.shape) == 2) and (len(shape) == 2)):\n        if (shape[(- 1)] != Y.shape[(- 1)]):\n            return one_hot(Y, n=shape[(- 1)])\n        else:\n            return Y\n    else:\n        return Y\n", "label": 0}
{"function": "\n\ndef reindex(self, target, method=None, level=None, limit=None, tolerance=None):\n    \"\\n        Create index with target's values (move/add/delete values as necessary)\\n\\n        Parameters\\n        ----------\\n        target : an iterable\\n\\n        Returns\\n        -------\\n        new_index : pd.Index\\n            Resulting index\\n        indexer : np.ndarray or None\\n            Indices of output values in original index\\n\\n        \"\n    preserve_names = (not hasattr(target, 'name'))\n    target = _ensure_has_len(target)\n    if ((not isinstance(target, Index)) and (len(target) == 0)):\n        attrs = self._get_attributes_dict()\n        attrs.pop('freq', None)\n        target = self._simple_new(None, dtype=self.dtype, **attrs)\n    else:\n        target = _ensure_index(target)\n    if (level is not None):\n        if (method is not None):\n            raise TypeError('Fill method not supported if level passed')\n        (_, indexer, _) = self._join_level(target, level, how='right', return_indexers=True)\n    elif self.equals(target):\n        indexer = None\n    elif self.is_unique:\n        indexer = self.get_indexer(target, method=method, limit=limit, tolerance=tolerance)\n    else:\n        if ((method is not None) or (limit is not None)):\n            raise ValueError('cannot reindex a non-unique index with a method or limit')\n        (indexer, missing) = self.get_indexer_non_unique(target)\n    if (preserve_names and (target.nlevels == 1) and (target.name != self.name)):\n        target = target.copy()\n        target.name = self.name\n    return (target, indexer)\n", "label": 1}
{"function": "\n\ndef on_message(self, msg):\n    \"Callback: web socket has received a message.\\n        \\n        Note that message is a string here. The UTF-8 bytes have been decoded,\\n        but it hasn't been de-jsoned.\\n        \"\n    if ((not self.twconn) or (not self.twconn.available)):\n        self.application.twlog.warning('Websocket connection is not available')\n        self.write_tw_error('Your connection is not registered.')\n        return\n    self.twconn.lastmsgtime = twcommon.misc.now()\n    if (not self.application.twservermgr.tworldavailable):\n        self.application.twlog.warning('Tworld is not available.')\n        self.write_tw_error('Tworld service is not available.')\n        return\n    if (not msg.startswith('{')):\n        self.application.twlog.warning('Message from client appeared invalid: %s', msg[0:50])\n        self.write_tw_error('Message format appeared to be invalid.')\n        return\n    if (len(msg) > 1000):\n        self.application.twlog.warning('Message from client was too long: %s', msg[0:50])\n        self.write_tw_error('Message was too long.')\n        return\n    try:\n        self.application.twservermgr.tworld_write(self.twconnid, msg)\n    except Exception as ex:\n        self.application.twlog.error('Could not pass message to tworld socket: %s', ex)\n        self.write_tw_error(('Unable to pass command to service: %s' % (ex,)))\n", "label": 0}
{"function": "\n\n@pytest.mark.xfail\ndef test_warp_reproject_check_invert(runner, tmpdir):\n    srcname = 'tests/data/world.rgb.tif'\n    outputname = str(tmpdir.join('test.tif'))\n    result = runner.invoke(warp.warp, [srcname, outputname, '--check-invert-proj', 'yes', '--dst-crs', 'EPSG:3759'])\n    assert (result.exit_code == 0)\n    assert os.path.exists(outputname)\n    with rasterio.open(outputname) as output:\n        assert (output.crs == {\n            'init': 'epsg:3759',\n        })\n        shape1 = output.shape\n    output2name = str(tmpdir.join('test2.tif'))\n    result = runner.invoke(warp.warp, [srcname, output2name, '--check-invert-proj', 'no', '--dst-crs', 'EPSG:3759'])\n    assert (result.exit_code == 0)\n    assert os.path.exists(output2name)\n    with rasterio.open(output2name) as output:\n        assert (output.crs == {\n            'init': 'epsg:3759',\n        })\n        assert (output.shape != shape1)\n", "label": 0}
{"function": "\n\ndef _introspect_inline_admin(self, inline):\n    'Introspects the given inline admin, returning a tuple of (inline_model, follow_field).'\n    inline_model = None\n    follow_field = None\n    fk_name = None\n    if issubclass(inline, GenericInlineModelAdmin):\n        inline_model = inline.model\n        ct_field = inline.ct_field\n        fk_name = inline.ct_fk_field\n        for field in self.model._meta.virtual_fields:\n            if (isinstance(field, GenericRelation) and (remote_model(field) == inline_model) and (field.object_id_field_name == fk_name) and (field.content_type_field_name == ct_field)):\n                follow_field = field.name\n                break\n    elif issubclass(inline, options.InlineModelAdmin):\n        inline_model = inline.model\n        fk_name = inline.fk_name\n        if (not fk_name):\n            for field in inline_model._meta.fields:\n                if (isinstance(field, (models.ForeignKey, models.OneToOneField)) and issubclass(self.model, remote_model(field))):\n                    fk_name = field.name\n                    break\n        if (fk_name and (not remote_field(inline_model._meta.get_field(fk_name)).is_hidden())):\n            field = inline_model._meta.get_field(fk_name)\n            accessor = remote_field(field).get_accessor_name()\n            follow_field = accessor\n    return (inline_model, follow_field, fk_name)\n", "label": 1}
{"function": "\n\ndef _build_rate_limits(self, rate_limits):\n    limits = []\n    for rate_limit in rate_limits:\n        _rate_limit_key = None\n        _rate_limit = self._build_rate_limit(rate_limit)\n        for limit in limits:\n            if ((limit['uri'] == rate_limit['URI']) and (limit['regex'] == rate_limit['regex'])):\n                _rate_limit_key = limit\n                break\n        if (not _rate_limit_key):\n            _rate_limit_key = {\n                'uri': rate_limit['URI'],\n                'regex': rate_limit['regex'],\n                'limit': [],\n            }\n            limits.append(_rate_limit_key)\n        _rate_limit_key['limit'].append(_rate_limit)\n    return limits\n", "label": 0}
{"function": "\n\ndef xml_to_dict(node):\n    'Recursively convert minidom XML node to dictionary'\n    node_data = {\n        \n    }\n    if (node.nodeType == node.TEXT_NODE):\n        node_data = node.data\n    elif (node.nodeType not in (node.DOCUMENT_NODE, node.DOCUMENT_TYPE_NODE)):\n        node_data.update(node.attributes.items())\n    if (node.nodeType not in (node.TEXT_NODE, node.DOCUMENT_TYPE_NODE)):\n        for child in node.childNodes:\n            (child_name, child_data) = xml_to_dict(child)\n            if (not child_data):\n                child_data = ''\n            if (child_name not in node_data):\n                node_data[child_name] = child_data\n            else:\n                if (not isinstance(node_data[child_name], list)):\n                    node_data[child_name] = [node_data[child_name]]\n                node_data[child_name].append(child_data)\n        if (node_data.keys() == ['#text']):\n            node_data = node_data['#text']\n    if (node.nodeType == node.DOCUMENT_NODE):\n        return node_data\n    else:\n        return (node.nodeName, node_data)\n", "label": 1}
{"function": "\n\ndef _make_status(self):\n    line = self._responseline\n    try:\n        [version, status, reason] = line.split(None, 2)\n    except ValueError:\n        try:\n            [version, status] = line.split(None, 1)\n            reason = ''\n        except ValueError:\n            raise BadStatusLineError(line)\n    try:\n        status = int(status)\n        if ((status < 100) or (status > 999)):\n            raise BadStatusLine(line)\n    except ValueError:\n        raise BadStatusLineError(line)\n    try:\n        version = float(version.split('/')[1])\n    except (IndexError, ValueError):\n        version = 0.9\n    reason = reason.strip()\n    reason = (reason or httputils.STATUSCODES.get(status, ''))\n    self._responseline = None\n    return ResponseLine(version, status, reason)\n", "label": 0}
{"function": "\n\ndef stem(word, cached=True, history=10000, **kwargs):\n    ' Returns the stem of the given word: ponies => poni.\\n        Note: it is often taken to be a crude error \\n        that a stemming algorithm does not leave a real word after removing the stem. \\n        But the purpose of stemming is to bring variant forms of a word together, \\n        not to map a word onto its \"paradigm\" form. \\n    '\n    stem = word.lower()\n    if (cached and (stem in cache)):\n        return case_sensitive(cache[stem], word)\n    if (cached and (len(cache) > history)):\n        cache.clear()\n    if (len(stem) <= 2):\n        return case_sensitive(stem, word)\n    if (stem in exceptions):\n        return case_sensitive(exceptions[stem], word)\n    if (stem in uninflected):\n        return case_sensitive(stem, word)\n    stem = upper_consonant_y(stem)\n    for f in (step_1a, step_1b, step_1c, step_2, step_3, step_4, step_5a, step_5b):\n        stem = f(stem)\n    stem = stem.lower()\n    stem = case_sensitive(stem, word)\n    if cached:\n        cache[word.lower()] = stem.lower()\n    return stem\n", "label": 1}
{"function": "\n\ndef files_changed(files):\n    global _mtimes, _win\n    for filename in files:\n        if (filename.endswith('.pyc') or filename.endswith('.pyo')):\n            filename = filename[:(- 1)]\n        if (not os.path.exists(filename)):\n            continue\n        stat = os.stat(filename)\n        mtime = stat.st_mtime\n        if _win:\n            mtime -= stat.st_ctime\n        if (filename not in _mtimes):\n            _mtimes[filename] = mtime\n            continue\n        if (mtime != _mtimes[filename]):\n            _mtimes = {\n                \n            }\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef __call__(self, connection):\n    \"Check a TLSConnection.\\n\\n        When a Checker is passed to a handshake function, this will\\n        be called at the end of the function.\\n\\n        @type connection: L{tlslite.tlsconnection.TLSConnection}\\n        @param connection: The TLSConnection to examine.\\n\\n        @raise tlslite.errors.TLSAuthenticationError: If the other\\n        party's certificate chain is missing or bad.\\n        \"\n    if ((not self.checkResumedSession) and connection.resumed):\n        return\n    if self.x509Fingerprint:\n        if connection._client:\n            chain = connection.session.serverCertChain\n        else:\n            chain = connection.session.clientCertChain\n        if self.x509Fingerprint:\n            if isinstance(chain, X509CertChain):\n                if self.x509Fingerprint:\n                    if (chain.getFingerprint() != self.x509Fingerprint):\n                        raise TLSFingerprintError(('X.509 fingerprint mismatch: %s, %s' % (chain.getFingerprint(), self.x509Fingerprint)))\n            elif chain:\n                raise TLSAuthenticationTypeError()\n            else:\n                raise TLSNoAuthenticationError()\n", "label": 1}
{"function": "\n\ndef LabelValueTable(self, label_list=None):\n    'Returns whole table as rows of name/value pairs.\\n\\n    One (or more) column entries are used for the row prefix label.\\n    The remaining columns are each displayed as a row entry with the\\n    prefix labels appended.\\n\\n    Use the first column as the label if label_list is None.\\n\\n    Args:\\n      label_list: A list of prefix labels to use.\\n\\n    Returns:\\n      Label/Value formatted table.\\n\\n    Raises:\\n      TableError: If specified label is not a column header of the table.\\n    '\n    label_list = (label_list or self._Header()[0])\n    for label in label_list:\n        if (label not in self._Header()):\n            raise TableError(('Invalid label prefix: %s.' % label))\n    sorted_list = []\n    for header in self._Header():\n        if (header in label_list):\n            sorted_list.append(header)\n    label_str = ('# LABEL %s\\n' % '.'.join(sorted_list))\n    body = []\n    for row in self:\n        label_prefix = []\n        value_list = []\n        for (key, value) in row.items():\n            if (key in sorted_list):\n                label_prefix.append(value)\n            else:\n                value_list.append(('%s %s' % (key, value)))\n        body.append(''.join([('%s.%s\\n' % ('.'.join(label_prefix), v)) for v in value_list]))\n    return ('%s%s' % (label_str, ''.join(body)))\n", "label": 1}
{"function": "\n\ndef get_system_users():\n    retlist = []\n    rawlist = _psutil_osx.get_system_users()\n    for item in rawlist:\n        (user, tty, hostname, tstamp) = item\n        if (tty == '~'):\n            continue\n        if (not tstamp):\n            continue\n        nt = nt_user(user, (tty or None), (hostname or None), tstamp)\n        retlist.append(nt)\n    return retlist\n", "label": 0}
{"function": "\n\n@mock_ec2\ndef test_eip_describe():\n    'Listing of allocated Elastic IP Addresses.'\n    conn = boto.connect_ec2('the_key', 'the_secret')\n    eips = []\n    number_of_classic_ips = 2\n    number_of_vpc_ips = 2\n    for _ in range(number_of_classic_ips):\n        eips.append(conn.allocate_address())\n    for _ in range(number_of_vpc_ips):\n        eips.append(conn.allocate_address(domain='vpc'))\n    len(eips).should.be.equal((number_of_classic_ips + number_of_vpc_ips))\n    for eip in eips:\n        if eip.allocation_id:\n            lookup_addresses = conn.get_all_addresses(allocation_ids=[eip.allocation_id])\n        else:\n            lookup_addresses = conn.get_all_addresses(addresses=[eip.public_ip])\n        len(lookup_addresses).should.be.equal(1)\n        lookup_addresses[0].public_ip.should.be.equal(eip.public_ip)\n    lookup_addresses = conn.get_all_addresses(addresses=[eips[0].public_ip, eips[1].public_ip])\n    len(lookup_addresses).should.be.equal(2)\n    lookup_addresses[0].public_ip.should.be.equal(eips[0].public_ip)\n    lookup_addresses[1].public_ip.should.be.equal(eips[1].public_ip)\n    for eip in eips:\n        eip.release()\n    len(conn.get_all_addresses()).should.be.equal(0)\n", "label": 0}
{"function": "\n\ndef get_record_children(self, record):\n    ret = {\n        \n    }\n    if (not record.is_directory()):\n        return ret.values()\n    try:\n        indx_alloc_attr = record.attribute(ATTR_TYPE.INDEX_ALLOCATION)\n        indx_alloc = INDEX_ALLOCATION(self.get_attribute_data(indx_alloc_attr), 0)\n        indx = indx_alloc\n        for block in indx.blocks():\n            for entry in block.index().entries():\n                ref = MREF(entry.header().mft_reference())\n                if ((ref == INODE_ROOT) and (entry.filename_information().filename() == '.')):\n                    continue\n                ret[ref] = self._enumerator.get_record(ref)\n    except AttributeNotFoundError:\n        indx_root_attr = record.attribute(ATTR_TYPE.INDEX_ROOT)\n        indx_root = INDEX_ROOT(self.get_attribute_data(indx_root_attr), 0)\n        indx = indx_root\n        for entry in indx.index().entries():\n            ref = MREF(entry.header().mft_reference())\n            if ((ref == INODE_ROOT) and (entry.filename_information().filename() == '.')):\n                continue\n            ret[ref] = self._enumerator.get_record(ref)\n    return ret.values()\n", "label": 1}
{"function": "\n\ndef waitforkeypress(fps=None):\n    if (fps is not None):\n        clock = pygame.time.Clock()\n    while True:\n        for event in pygame.event.get():\n            if (event.type == KEYDOWN):\n                continue\n            elif (event.type == QUIT):\n                pygame.quit()\n                sys.exit()\n            elif (event.type == KEYUP):\n                return interpretkeyevent(event)\n        pygame.display.update()\n        if (fps is not None):\n            clock.tick(fps)\n", "label": 0}
{"function": "\n\ndef daemonize(self):\n    'Daemonize process by doing Stevens double fork.'\n    if (self.stdout is not DEVNULL):\n        self.stdout.flush()\n    if (self.stderr is not DEVNULL):\n        self.stderr.flush()\n    for f in (sys.stdout, sys.stderr):\n        f.flush()\n    self._fork()\n    os.chdir('/')\n    os.setsid()\n    os.umask(0)\n    self._fork()\n    with open(os.devnull, 'w+') as devnull:\n        stdin = (devnull if (self.stdin is DEVNULL) else self.stdin)\n        stdout = (devnull if (self.stdout is DEVNULL) else self.stdout)\n        stderr = (devnull if (self.stderr is DEVNULL) else self.stderr)\n        os.dup2(stdin.fileno(), STDIN_FILENO)\n        os.dup2(stdout.fileno(), STDOUT_FILENO)\n        os.dup2(stderr.fileno(), STDERR_FILENO)\n    if (self.pidfile is not None):\n        atexit.register(self.delete_pid)\n        signal.signal(signal.SIGTERM, self.handle_sigterm)\n        self.pidfile.write(os.getpid())\n", "label": 0}
{"function": "\n\ndef load_config(domain):\n    '\\n    Parses and loads up the Galah configuration file and extracts the\\n    configuration data for the given domain (ex: \"web\" or \"sheep\").\\n\\n    Global configuration options will always be returned, no matter the domain.\\n\\n    The configuration file will only be loaded once per instance of the\\n    interpreter, so feel free to call this function multiple times in many files\\n    to avoid ugly dependency issues.\\n\\n    '\n    global loaded\n    first_load = (loaded is None)\n    config_file = os.environ.get('GALAH_CONFIG_PATH', '/etc/galah/galah.config')\n    if (not loaded):\n        if os.path.isfile(config_file):\n            loaded = imp.load_source('user_config_file', config_file)\n    local_config = {\n        \n    }\n    user_config = dict(defaults)\n    if loaded:\n        user_config.update(loaded.config)\n    prefix = ('%s/' % domain)\n    global_prefix = 'global/'\n    for (k, v) in user_config.items():\n        if (k.startswith(prefix) and (len(k) != len(prefix))):\n            local_key = k[len(prefix):]\n            local_config[local_key] = v\n        elif (k.startswith(global_prefix) and (len(k) != len(global_prefix))):\n            global_key = k[len(global_prefix):]\n            local_config[global_key] = v\n        elif ('/' in k):\n            local_config[k] = v\n    return local_config\n", "label": 1}
{"function": "\n\ndef ask_gerrit_username(server, ssh_port=29418):\n    ' Run a wizard to try to configure gerrit access\\n\\n    If that fails, ask the user for its username\\n    If that fails, give up and suggest upload the public key\\n\\n    '\n    ui.info(ui.green, 'Configuring gerrit ssh access ...')\n    username = qisys.sh.username()\n    if (not username):\n        username = qisys.interact.ask_string('Please enter your username')\n        if (not username):\n            return\n    ui.info(('Checking gerrit connection with %s@%s:%i' % (username, server, ssh_port)))\n    if check_gerrit_connection(username, server, ssh_port=ssh_port):\n        ui.info('Success')\n        return username\n    ui.warning('Could not connect to ssh using username', username)\n    try_other = qisys.interact.ask_yes_no('Do you want to try with another username?')\n    if (not try_other):\n        return\n    username = qisys.interact.ask_string('Please enter your username')\n    if (not username):\n        return\n    if check_gerrit_connection(username, server, ssh_port=ssh_port):\n        return username\n", "label": 0}
{"function": "\n\ndef collections(action):\n    if (not hasattr(request, 'auth_collections')):\n        request.auth_collections = {\n            READ: set(),\n            WRITE: set(),\n        }\n        if is_admin():\n            q = Collection.all_ids().filter((Collection.deleted_at == None))\n            for (col_id,) in q:\n                request.auth_collections[READ].add(col_id)\n                request.auth_collections[WRITE].add(col_id)\n        else:\n            q = Permission.all()\n            q = q.filter(Permission.role_id.in_(request.auth_roles))\n            q = q.filter((Permission.resource_type == Permission.COLLECTION))\n            for perm in q:\n                if perm.read:\n                    request.auth_collections[READ].add(perm.resource_id)\n                if (perm.write and request.logged_in):\n                    request.auth_collections[WRITE].add(perm.resource_id)\n    return list(request.auth_collections.get(action, []))\n", "label": 0}
{"function": "\n\ndef get_items(self, block):\n    ' Break a block into list items. '\n    items = []\n    for line in block.split('\\n'):\n        m = self.CHILD_RE.match(line)\n        if m:\n            if ((not items) and (self.TAG == 'ol')):\n                INTEGER_RE = re.compile('(\\\\d+)')\n                self.STARTSWITH = INTEGER_RE.match(m.group(1)).group()\n            items.append(m.group(3))\n        elif self.INDENT_RE.match(line):\n            if items[(- 1)].startswith((' ' * self.tab_length)):\n                items[(- 1)] = ('%s\\n%s' % (items[(- 1)], line))\n            else:\n                items.append(line)\n        else:\n            items[(- 1)] = ('%s\\n%s' % (items[(- 1)], line))\n    return items\n", "label": 0}
{"function": "\n\ndef write_error(self, status_code, exc_info=None, error_text=None):\n    '\\n        Render a custom error page. This is invoked if a handler throws\\n        an exception. We also call it manually, in some places.\\n        '\n    if (status_code == 404):\n        self.render('404.html')\n        return\n    if (status_code == 403):\n        if (not error_text):\n            error_text = 'Not permitted'\n        if exc_info:\n            error_text = str(exc_info[1])\n        self.render('error.html', status_code=403, exctitle=None, exception=error_text)\n        return\n    exception = ''\n    exctitle = None\n    if error_text:\n        exception = error_text\n    if exc_info:\n        exctitle = str(exc_info[1])\n        ls = [ln for ln in traceback.format_exception(*exc_info)]\n        if exception:\n            exception = (exception + '\\n')\n        exception = (exception + ''.join(ls))\n    self.render('error.html', status_code=status_code, exctitle=exctitle, exception=exception)\n", "label": 1}
{"function": "\n\ndef parse_title_page(lines):\n    'Parse the title page.\\n\\n    Spec: http://fountain.io/syntax#section-titlepage\\n    Returns None if the document does not have a title page section,\\n    otherwise a dictionary with the data.\\n    '\n    result = {\n        \n    }\n    it = iter(lines)\n    try:\n        line = it.next()\n        while True:\n            key_match = title_page_key_re.match(line)\n            if (not key_match):\n                return None\n            (key, value) = key_match.groups()\n            if value:\n                result.setdefault(key, []).append(value)\n                line = it.next()\n            else:\n                for line in it:\n                    value_match = title_page_value_re.match(line)\n                    if (not value_match):\n                        break\n                    result.setdefault(key, []).append(value_match.group(1))\n                else:\n                    break\n    except StopIteration:\n        pass\n    return result\n", "label": 0}
{"function": "\n\ndef related_models(self, backrefs=False):\n    models = []\n    stack = [self.model_class]\n    while stack:\n        model = stack.pop()\n        if (model in models):\n            continue\n        models.append(model)\n        for fk in model._meta.rel.values():\n            stack.append(fk.rel_model)\n        if backrefs:\n            for fk in model._meta.reverse_rel.values():\n                stack.append(fk.model_class)\n    return models\n", "label": 0}
{"function": "\n\ndef visit_completion_counter(case):\n    mother_counter = 0\n    child_counter = 0\n    case_obj = CommCareCase.get(case['_id'])\n    baby_case = [c for c in case_obj.get_subcases().all() if (c.type == 'baby')]\n    for i in range(1, 8):\n        if (('pp_%s_done' % i) in case):\n            val = case[('pp_%s_done' % i)]\n            try:\n                if (val.lower() == 'yes'):\n                    mother_counter += 1\n                elif (int(float(val)) == 1):\n                    mother_counter += 1\n            except ValueError:\n                pass\n        if (baby_case and (('bb_pp_%s_done' % i) in baby_case[0])):\n            val = baby_case[0][('bb_pp_%s_done' % i)]\n            try:\n                if (val.lower() == 'yes'):\n                    child_counter += 1\n                elif (int(float(val)) == 1):\n                    child_counter += 1\n            except ValueError:\n                pass\n    return (mother_counter if (mother_counter > child_counter) else child_counter)\n", "label": 1}
{"function": "\n\ndef send_frame(self, frame):\n    \"\\n        Send a single frame. If there is no transport or we're not connected\\n        yet, append to the output buffer, else send immediately to the socket.\\n        This is called from within the MethodFrames.\\n        \"\n    if self._closed:\n        if (self._close_info and (len(self._close_info['reply_text']) > 0)):\n            raise ConnectionClosed(('connection is closed: %s : %s' % (self._close_info['reply_code'], self._close_info['reply_text'])))\n        raise ConnectionClosed('connection is closed')\n    if ((self._transport is None) or ((not self._connected) and (frame.channel_id != 0))):\n        self._output_frame_buffer.append(frame)\n        return\n    if (self._debug > 1):\n        self.logger.debug('WRITE: %s', frame)\n    buf = bytearray()\n    frame.write_frame(buf)\n    if (len(buf) > self._frame_max):\n        self.close(reply_code=501, reply_text=('attempted to send frame of %d bytes, frame max %d' % (len(buf), self._frame_max)), class_id=0, method_id=0, disconnect=True)\n        raise ConnectionClosed(('connection is closed: %s : %s' % (self._close_info['reply_code'], self._close_info['reply_text'])))\n    self._transport.write(buf)\n    self._frames_written += 1\n", "label": 1}
{"function": "\n\ndef _delete_files(self):\n    for file_path in self.to_delete_files:\n        if (not os.path.isfile(file_path)):\n            continue\n        try:\n            os.remove(file_path)\n        except Exception:\n            pass\n    for file_path in self.to_delete_directories:\n        if (not os.path.isdir(file_path)):\n            continue\n        try:\n            shutil.rmtree(file_path)\n        except Exception:\n            pass\n", "label": 0}
{"function": "\n\ndef get_process_exe(self):\n    try:\n        exe = os.readlink(('/proc/%s/exe' % self.pid))\n    except (OSError, IOError):\n        err = sys.exc_info()[1]\n        if (err.errno == errno.ENOENT):\n            if os.path.lexists(('/proc/%s/exe' % self.pid)):\n                return ''\n            else:\n                raise NoSuchProcess(self.pid, self._process_name)\n        if (err.errno in (errno.EPERM, errno.EACCES)):\n            raise AccessDenied(self.pid, self._process_name)\n        raise\n    exe = exe.replace('\\x00', '')\n    if (exe.endswith(' (deleted)') and (not os.path.exists(exe))):\n        exe = exe[:(- 10)]\n    return exe\n", "label": 0}
{"function": "\n\ndef __init__(self, domain, config_or_config_id, filters, aggregation_columns, columns, order_by):\n    self.lang = None\n    self.domain = domain\n    if isinstance(config_or_config_id, DataSourceConfiguration):\n        self._config = config_or_config_id\n        self._config_id = self._config._id\n    else:\n        assert isinstance(config_or_config_id, basestring)\n        self._config = None\n        self._config_id = config_or_config_id\n    self._filters = {f.slug: f for f in filters}\n    self._filter_values = {\n        \n    }\n    self._deferred_filters = {\n        \n    }\n    self._order_by = order_by\n    self._aggregation_columns = aggregation_columns\n    self._column_configs = OrderedDict()\n    for column in columns:\n        assert (column.column_id not in self._column_configs), 'Report {} in domain {} has more than one {} column defined!'.format(self._config_id, self.domain, column.column_id)\n        self._column_configs[column.column_id] = column\n", "label": 0}
{"function": "\n\ndef test_security_group_rule_create(self):\n    sg_rule = [r for r in self.api_q_secgroup_rules.list() if ((r['protocol'] == 'tcp') and r['remote_ip_prefix'])][0]\n    sg_id = sg_rule['security_group_id']\n    secgroup = [sg for sg in self.api_q_secgroups.list() if (sg['id'] == sg_id)][0]\n    post_rule = copy.deepcopy(sg_rule)\n    del post_rule['id']\n    del post_rule['tenant_id']\n    post_body = {\n        'security_group_rule': post_rule,\n    }\n    self.qclient.create_security_group_rule(post_body).AndReturn({\n        'security_group_rule': copy.deepcopy(sg_rule),\n    })\n    self.qclient.list_security_groups(id=set([sg_id]), fields=['id', 'name']).AndReturn({\n        'security_groups': [copy.deepcopy(secgroup)],\n    })\n    self.mox.ReplayAll()\n    ret = api.network.security_group_rule_create(self.request, sg_rule['security_group_id'], sg_rule['direction'], sg_rule['ethertype'], sg_rule['protocol'], sg_rule['port_range_min'], sg_rule['port_range_max'], sg_rule['remote_ip_prefix'], sg_rule['remote_group_id'])\n    self._cmp_sg_rule(sg_rule, ret)\n", "label": 0}
{"function": "\n\ndef load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):\n    'Load translations from the given directory.\\n\\n        :param dirname: the directory containing the ``MO`` files\\n        :param locales: the list of locales in order of preference (items in\\n                        this list can be either `Locale` objects or locale\\n                        strings)\\n        :param domain: the message domain\\n        :return: the loaded catalog, or a ``NullTranslations`` instance if no\\n                 matching translations were found\\n        :rtype: `Translations`\\n        '\n    if (locales is not None):\n        if (not isinstance(locales, (list, tuple))):\n            locales = [locales]\n        locales = [str(locale) for locale in locales]\n    if (not domain):\n        domain = cls.DEFAULT_DOMAIN\n    filename = gettext.find(domain, dirname, locales)\n    if (not filename):\n        return gettext.NullTranslations()\n    return cls(fileobj=open(filename, 'rb'), domain=domain)\n", "label": 0}
{"function": "\n\ndef analyze_manifest(self):\n    self.manifest_files = mf = {\n        \n    }\n    if (not self.distribution.include_package_data):\n        return\n    src_dirs = {\n        \n    }\n    for package in (self.packages or ()):\n        src_dirs[assert_relative(self.get_package_dir(package))] = package\n    self.run_command('egg_info')\n    ei_cmd = self.get_finalized_command('egg_info')\n    for path in ei_cmd.filelist.files:\n        (d, f) = os.path.split(assert_relative(path))\n        prev = None\n        oldf = f\n        while (d and (d != prev) and (d not in src_dirs)):\n            prev = d\n            (d, df) = os.path.split(d)\n            f = os.path.join(df, f)\n        if (d in src_dirs):\n            if (path.endswith('.py') and (f == oldf)):\n                continue\n            mf.setdefault(src_dirs[d], []).append(path)\n", "label": 1}
{"function": "\n\ndef eval(self):\n    '\\n        Return the Labeled Attachment Score (LAS) and Unlabeled Attachment Score (UAS)\\n\\n        :return : tuple(float,float)\\n        '\n    if (len(self._parsed_sents) != len(self._gold_sents)):\n        raise ValueError(' Number of parsed sentence is different with number of gold sentence.')\n    corr = 0\n    corrL = 0\n    total = 0\n    for i in range(len(self._parsed_sents)):\n        parsed_sent_nodes = self._parsed_sents[i].nodes\n        gold_sent_nodes = self._gold_sents[i].nodes\n        if (len(parsed_sent_nodes) != len(gold_sent_nodes)):\n            raise ValueError('Sentences must have equal length.')\n        for (parsed_node_address, parsed_node) in parsed_sent_nodes.items():\n            gold_node = gold_sent_nodes[parsed_node_address]\n            if (parsed_node['word'] is None):\n                continue\n            if (parsed_node['word'] != gold_node['word']):\n                raise ValueError('Sentence sequence is not matched.')\n            if (self._remove_punct(parsed_node['word']) == ''):\n                continue\n            total += 1\n            if (parsed_node['head'] == gold_node['head']):\n                corr += 1\n                if (parsed_node['rel'] == gold_node['rel']):\n                    corrL += 1\n    return ((corr / total), (corrL / total))\n", "label": 1}
{"function": "\n\ndef close(self):\n    if self._closed:\n        return\n    snames = [n for n in dir(self) if n.endswith('socket')]\n    for socket in [getattr(self, name) for name in snames]:\n        if (isinstance(socket, zmq.Socket) and (not socket.closed)):\n            socket.close()\n    self._closed = True\n", "label": 0}
{"function": "\n\ndef _setup_joins(self, pieces, opts, alias):\n    '\\n        A helper method for get_ordering and get_distinct. This method will\\n        call query.setup_joins, handle refcounts and then promote the joins.\\n\\n        Note that get_ordering and get_distinct must produce same target\\n        columns on same input, as the prefixes of get_ordering and get_distinct\\n        must match. Executing SQL where this is not true is an error.\\n        '\n    if (not alias):\n        alias = self.query.get_initial_alias()\n    (field, targets, opts, joins, _) = self.query.setup_joins(pieces, opts, alias)\n    joins_to_promote = [j for j in joins if (self.query.alias_refcount[j] < 2)]\n    alias = joins[(- 1)]\n    cols = [target.column for target in targets]\n    if (not field.rel):\n        self.query.ref_alias(alias)\n    self.query.promote_joins(joins_to_promote)\n    return (field, cols, alias, joins, opts)\n", "label": 0}
{"function": "\n\ndef _build_targets(self, targets):\n    \"Turn valid target IDs or 'all' into two lists:\\n        (int_ids, uuids).\\n        \"\n    if (not self._ids):\n        if (not self.ids):\n            raise error.NoEnginesRegistered(\"Can't build targets without any engines\")\n    if (targets is None):\n        targets = self._ids\n    elif isinstance(targets, str):\n        if (targets.lower() == 'all'):\n            targets = self._ids\n        else:\n            raise TypeError((\"%r not valid str target, must be 'all'\" % targets))\n    elif isinstance(targets, int):\n        if (targets < 0):\n            targets = self.ids[targets]\n        if (targets not in self._ids):\n            raise IndexError(('No such engine: %i' % targets))\n        targets = [targets]\n    if isinstance(targets, slice):\n        indices = list(range(len(self._ids)))[targets]\n        ids = self.ids\n        targets = [ids[i] for i in indices]\n    if (not isinstance(targets, (tuple, list, xrange))):\n        raise TypeError(('targets by int/slice/collection of ints only, not %s' % type(targets)))\n    return ([util.asbytes(self._engines[t]) for t in targets], list(targets))\n", "label": 1}
{"function": "\n\n@app.route('/success', methods=['GET'])\ndef handleLoginSuccess():\n    app.logger.info(('handleLoginSuccess [%s]' % request.method))\n    me = request.args.get('me')\n    code = request.args.get('code')\n    app.logger.info(('me [%s] code [%s]' % (me, code)))\n    if (db is not None):\n        app.logger.info('getting data to validate auth code')\n        key = ('login-%s' % me)\n        data = db.hgetall(key)\n        if data:\n            r = ninka.indieauth.validateAuthCode(code=code, client_id=me, redirect_uri=data['redirect_uri'])\n            if (r['status'] == requests.codes.ok):\n                app.logger.info('login code verified')\n                scope = r['response']['scope']\n                from_uri = data['from_uri']\n                token = str(uuid.uuid4())\n                db.hset(key, 'code', code)\n                db.hset(key, 'token', token)\n                db.expire(key, cfg['auth_timeout'])\n                db.set(('token-%s' % token), key)\n                db.expire(('token-%s' % code), cfg['auth_timeout'])\n                session['indieauth_token'] = token\n                session['indieauth_scope'] = scope\n                session['indieauth_id'] = me\n            else:\n                app.logger.info('login invalid')\n                clearAuth()\n        else:\n            app.logger.info(('nothing found for [%s]' % me))\n    if scope:\n        if from_uri:\n            return redirect(from_uri)\n        else:\n            return redirect('/')\n    else:\n        return ('authentication failed', 403)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(AddL3PolicyForm, self).clean()\n    if self.is_valid():\n        ipversion = int(cleaned_data['ip_version'])\n        subnet_prefix_length = int(cleaned_data['subnet_prefix_length'])\n        msg = _('Subnet prefix out of range.')\n        if ((ipversion == 4) and (subnet_prefix_length not in range(2, 31))):\n            raise forms.ValidationError(msg)\n        if ((ipversion == 6) and (subnet_prefix_length not in range(2, 128))):\n            raise forms.ValidationError(msg)\n    return cleaned_data\n", "label": 0}
{"function": "\n\ndef intersection(self, other):\n    \"\\n        Form the intersection of two Index objects.\\n\\n        This returns a new Index with elements common to the index and `other`.\\n        Sortedness of the result is not guaranteed.\\n\\n        Parameters\\n        ----------\\n        other : Index or array-like\\n\\n        Returns\\n        -------\\n        intersection : Index\\n\\n        Examples\\n        --------\\n\\n        >>> idx1 = pd.Index([1, 2, 3, 4])\\n        >>> idx2 = pd.Index([3, 4, 5, 6])\\n        >>> idx1.intersection(idx2)\\n        Int64Index([3, 4], dtype='int64')\\n\\n        \"\n    self._assert_can_do_setop(other)\n    other = _ensure_index(other)\n    if self.equals(other):\n        return self\n    if (not com.is_dtype_equal(self.dtype, other.dtype)):\n        this = self.astype('O')\n        other = other.astype('O')\n        return this.intersection(other)\n    if (self.is_monotonic and other.is_monotonic):\n        try:\n            result = self._inner_indexer(self.values, other._values)[0]\n            return self._wrap_union_result(other, result)\n        except TypeError:\n            pass\n    try:\n        indexer = Index(self.values).get_indexer(other._values)\n        indexer = indexer.take((indexer != (- 1)).nonzero()[0])\n    except:\n        indexer = Index(self.values).get_indexer_non_unique(other._values)[0].unique()\n        indexer = indexer[(indexer != (- 1))]\n    taken = self.take(indexer)\n    if (self.name != other.name):\n        taken.name = None\n    return taken\n", "label": 0}
{"function": "\n\n@pytest.hookimpl(tryfirst=True, hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = (yield)\n    rep = outcome.get_result()\n    if ((rep.when == 'call') and rep.failed):\n        if ('browser' in item.fixturenames):\n            browser = item.funcargs['browser']\n            for log_type in (set(browser.log_types) - {'har'}):\n                data = '\\n\\n'.join(filter(None, (l.get('message') for l in browser.get_log(log_type))))\n                if data:\n                    rep.sections.append(('Captured {} log'.format(log_type), data))\n", "label": 0}
{"function": "\n\n@require_instance_manager\ndef similar_objects(self):\n    lookup_kwargs = self._lookup_kwargs()\n    lookup_keys = sorted(lookup_kwargs)\n    qs = self.through.objects.values(*lookup_kwargs.keys())\n    qs = qs.annotate(n=models.Count('pk'))\n    qs = qs.exclude(**lookup_kwargs)\n    qs = qs.filter(tag__in=self.all())\n    qs = qs.order_by('-n')\n    items = {\n        \n    }\n    if (len(lookup_keys) == 1):\n        f = self.through._meta.get_field_by_name(lookup_keys[0])[0]\n        objs = f.rel.to._default_manager.filter(**{\n            ('%s__in' % f.rel.field_name): [r['content_object'] for r in qs],\n        })\n        for obj in objs:\n            items[(getattr(obj, f.rel.field_name),)] = obj\n    else:\n        preload = {\n            \n        }\n        for result in qs:\n            preload.setdefault(result['content_type'], set())\n            preload[result['content_type']].add(result['object_id'])\n        for (ct, obj_ids) in preload.iteritems():\n            ct = ContentType.objects.get_for_id(ct)\n            for obj in ct.model_class()._default_manager.filter(pk__in=obj_ids):\n                items[(ct.pk, obj.pk)] = obj\n    results = []\n    for result in qs:\n        obj = items[tuple((result[k] for k in lookup_keys))]\n        obj.similar_tags = result['n']\n        results.append(obj)\n    return results\n", "label": 1}
{"function": "\n\ndef signup(request, template='accounts/account_signup.html', extra_context=None):\n    '\\n    Signup form.\\n    '\n    profile_form = get_profile_form()\n    form = profile_form((request.POST or None), (request.FILES or None))\n    if ((request.method == 'POST') and form.is_valid()):\n        new_user = form.save()\n        if (not new_user.is_active):\n            if settings.ACCOUNTS_APPROVAL_REQUIRED:\n                send_approve_mail(request, new_user)\n                info(request, _(\"Thanks for signing up! You'll receive an email when your account is activated.\"))\n            else:\n                send_verification_mail(request, new_user, 'signup_verify')\n                info(request, _('A verification email has been sent with a link for activating your account.'))\n            return redirect((next_url(request) or '/'))\n        else:\n            info(request, _('Successfully signed up'))\n            auth_login(request, new_user)\n            return login_redirect(request)\n    context = {\n        'form': form,\n        'title': _('Sign up'),\n    }\n    context.update((extra_context or {\n        \n    }))\n    return TemplateResponse(request, template, context)\n", "label": 1}
{"function": "\n\ndef do_GET(self):\n    self.send_response(200, 'OK')\n    acs = self.headers['Accept'].split(',')\n    acq = [x.split(';') for x in acs if (';' in x)]\n    acn = [(x, 'q=1') for x in acs if (';' not in x)]\n    acs = [(x[0], float(x[1].strip()[2:])) for x in (acq + acn)]\n    ac = sorted(acs, key=(lambda x: x[1]))\n    ct = ac[(- 1)]\n    if ('application/rdf+xml' in ct):\n        rct = 'application/rdf+xml'\n        content = xmltestdoc\n    elif ('text/n3' in ct):\n        rct = 'text/n3'\n        content = n3testdoc\n    elif ('text/plain' in ct):\n        rct = 'text/plain'\n        content = nttestdoc\n    self.send_header('Content-type', rct)\n    self.end_headers()\n    self.wfile.write(content.encode('utf-8'))\n", "label": 1}
{"function": "\n\ndef placeholders(self, fill_missing=None):\n    '\\n        Generate placeholder dict.\\n        :param fill_missing: A function taking a missing placeholder name & returning a temporary fill value.\\n                May be `None` indicating not to return such replacements.\\n        :return: Dict of placeholder names to values.\\n        '\n    result = {K: self._format_placeholder(K, V) for (K, V) in self.values.items() if self._should_make_placeholder(K)}\n    print('Making result!')\n    if (fill_missing is not None):\n        for key in self.required:\n            print('Checking out {}'.format(key))\n            if (self._should_make_placeholder(key) and (not result.get(key))):\n                result[key] = self._format_placeholder(key, fill_missing(key))\n                print('Added!', key, result[key])\n    return result\n", "label": 0}
{"function": "\n\ndef get_episode(self, url, imdb, tvdb, title, date, season, episode):\n    try:\n        if (url == None):\n            return\n        (year, month) = re.compile('(\\\\d{4})-(\\\\d{2})').findall(date)[(- 1)]\n        if (int(year) <= 2008):\n            raise Exception()\n        cat = urlparse.urljoin(self.base_link, url)\n        cat = cat.split('category/', 1)[(- 1)].rsplit('/')[0]\n        url = urlparse.urljoin(self.base_link, ('/episode/%s-s%02de%02d' % (cat, int(season), int(episode))))\n        result = client.source(url, output='response', error=True)\n        if ('404' in result[0]):\n            url = urlparse.urljoin(self.base_link, ('/%s/%s/%s-s%02de%02d' % (year, month, cat, int(season), int(episode))))\n            result = client.source(url, output='response', error=True)\n        if ('404' in result[0]):\n            url = urlparse.urljoin(self.base_link, ('/%s/%s/%s-%01dx%01d' % (year, month, cat, int(season), int(episode))))\n            result = client.source(url, output='response', error=True)\n        if ('404' in result[0]):\n            raise Exception()\n        try:\n            url = re.compile('//.+?(/.+)').findall(url)[0]\n        except:\n            url = result\n        url = client.replaceHTMLCodes(url)\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.input = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_user(self, request, user_id=None, token_id=None, username=None, email=None, auth_key=None, **kw):\n    'Securely fetch a user by id, username, email or auth key\\n\\n        Returns user or nothing\\n        '\n    odm = request.app.odm()\n    now = datetime.utcnow()\n    if (token_id and user_id):\n        with odm.begin() as session:\n            query = session.query(odm.token)\n            query = query.filter_by(user_id=user_id, id=token_id)\n            query.update({\n                'last_access': now,\n            }, synchronize_session=False)\n            if (not query.count()):\n                return\n    if auth_key:\n        with odm.begin() as session:\n            query = session.query(odm.registration)\n            reg = query.get(auth_key)\n            if (reg and (reg.expiry > now)):\n                if (not reg.confirmed):\n                    user_id = reg.user_id\n            else:\n                return\n    with odm.begin() as session:\n        query = session.query(odm.user)\n        try:\n            if user_id:\n                user = query.get(user_id)\n            elif username:\n                user = query.filter_by(username=username).one()\n            elif email:\n                user = query.filter_by(email=normalise_email(email)).one()\n            else:\n                return\n        except NoResultFound:\n            return\n    return user\n", "label": 1}
{"function": "\n\ndef enable(app_id, enabled=True):\n    \"\\n    Enable or disable an existing assistive access application.\\n\\n    app_id\\n        The bundle ID or command to set assistive access status.\\n\\n    enabled\\n        Sets enabled or disabled status. Default is ``True``.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' assistive.enable /usr/bin/osascript\\n        salt '*' assistive.enable com.smileonmymac.textexpander enabled=False\\n    \"\n    enable_str = ('1' if enabled else '0')\n    for a in _get_assistive_access():\n        if (app_id == a[0]):\n            cmd = 'sqlite3 \"/Library/Application Support/com.apple.TCC/TCC.db\" \"UPDATE access SET allowed=\\'{0}\\' WHERE client=\\'{1}\\'\"'.format(enable_str, app_id)\n            call = __salt__['cmd.run_all'](cmd, output_loglevel='debug', python_shell=False)\n            if (call['retcode'] != 0):\n                comment = ''\n                if ('stderr' in call):\n                    comment += call['stderr']\n                if ('stdout' in call):\n                    comment += call['stdout']\n                raise CommandExecutionError('Error enabling app: {0}'.format(comment))\n            return True\n    return False\n", "label": 0}
{"function": "\n\n@discover.register(Collection)\ndef discover_pymongo_collection(coll, n=50):\n    items = list(take(n, coll.find()))\n    if (not items):\n        return (var * Record([]))\n    oid_cols = [k for (k, v) in items[0].items() if isinstance(v, ObjectId)]\n    for item in items:\n        for col in oid_cols:\n            del item[col]\n    ds = discover(items)\n    if isdimension(ds[0]):\n        return (coll.count() * ds.subshape[0])\n    else:\n        raise ValueError('Consistent datashape not found')\n", "label": 0}
{"function": "\n\ndef send_messages(self, messages):\n    'Write all messages to the stream in a thread-safe way.'\n    if (not messages):\n        return\n    self._lock.acquire()\n    try:\n        try:\n            stream_created = self.open()\n            for message in messages:\n                self.stream.write(render_message(message))\n                self.stream.write('\\n')\n                self.stream.write(('-' * 79))\n                self.stream.write('\\n')\n                self.stream.flush()\n            if stream_created:\n                self.close()\n        except:\n            if (not self.fail_silently):\n                raise\n    finally:\n        self._lock.release()\n    return len(messages)\n", "label": 0}
{"function": "\n\ndef parse_properties(file):\n    props = {\n        \n    }\n    with open(file, 'rb') as fp:\n        for (key, value) in jprops.iter_properties(fp):\n            if (key == 'AndroidVersion.ApiLevel'):\n                props['api'] = value\n            elif (key == 'Pkg.LicenseRef'):\n                props['license'] = value\n            elif (key == 'Pkg.Revision'):\n                props['revision'] = value\n            elif (key == 'SystemImage.Abi'):\n                props['abi'] = value\n            elif (key == 'SystemImage.TagId'):\n                props['tag'] = value\n    return props\n", "label": 0}
{"function": "\n\ndef convert_md_to_rst(source, destination=None, backup_dir=None):\n    \"Try to convert the source, an .md (markdown) file, to an .rst\\n    (reStructuredText) file at the destination. If the destination isn't\\n    provided, it defaults to be the same as the source path except for the\\n    filename extension. If the destination file already exists, it will be\\n    overwritten. In the event of an error, the destination file will be\\n    left untouched.\"\n    try:\n        import pypandoc\n    except ImportError:\n        os.system('pip install pypandoc')\n        import pypandoc\n    destination = (destination or (os.path.splitext(source)[0] + '.rst'))\n    backup_dir = (backup_dir or os.path.join(os.path.dirname(destination), 'bak'))\n    bak_name = (os.path.basename(destination) + time.strftime('.%Y%m%d%H%M%S.bak'))\n    bak_path = os.path.join(backup_dir, bak_name)\n    if os.path.isfile(destination):\n        if (not os.path.isdir(os.path.dirname(bak_path))):\n            os.mkdir(os.path.dirname(bak_path))\n        os.rename(destination, bak_path)\n    try:\n        pypandoc.convert(source, 'rst', format='md', outputfile=destination)\n    except:\n        if os.path.isfile(destination):\n            os.remove(destination)\n        if os.path.isfile(bak_path):\n            os.rename(bak_path, destination)\n        raise\n", "label": 1}
{"function": "\n\ndef _get_changed_data(self):\n    '\\n        The standard modelform thinks the Translation PKs are the initial\\n        values.  We need to dig deeper to assert whether there are indeed\\n        changes.\\n        '\n    Model = self._meta.model\n    if (self._changed_data is None):\n        changed = copy(forms.ModelForm.changed_data.__get__(self))\n        fieldnames = [f.name for f in Model._meta.fields]\n        fields = [(name, Model._meta.get_field(name)) for name in changed if (name in fieldnames)]\n        trans = [name for (name, field) in fields if isinstance(field, TranslatedField)]\n        if trans:\n            try:\n                orig = Model.objects.get(pk=self.instance.pk)\n            except Model.DoesNotExist:\n                return self._changed_data\n            for field in trans:\n                if (getattr(orig, field) == getattr(self.instance, field)):\n                    self._changed_data.remove(field)\n    return self._changed_data\n", "label": 1}
{"function": "\n\ndef _parse_arguments(self, method, parameters):\n    'Parse arguments to method, returning a dictionary.'\n    arguments = _fetch_arguments(self, method)\n    arg_dict = {\n        \n    }\n    errors = []\n    for (key, properties) in parameters:\n        if (key in arguments):\n            value = arguments[key]\n            try:\n                arg_dict[key] = _apply_validator_chain(properties.get('validators', []), value, self)\n            except validators.ValidationError as err:\n                errors.append(err)\n        elif properties.get('required', False):\n            raise web.HTTPError(400, ('Missing required parameter: %s' % (key,)))\n        elif (properties.get('default', None) is not None):\n            arg_dict[key] = properties['default']\n        else:\n            arg_dict[key] = None\n    if errors:\n        raise web.HTTPError(400, ('There were %s errors' % len(errors)))\n    return arg_dict\n", "label": 0}
{"function": "\n\ndef is_last_li(li, meta_data, current_numId):\n    '\\n    Determine if ``li`` is the last list item for a given list\\n    '\n    if (not is_li(li, meta_data)):\n        return False\n    w_namespace = get_namespace(li, 'w')\n    next_el = li\n    while True:\n        if (next_el is None):\n            return True\n        next_el = next_el.getnext()\n        if (not is_li(next_el, meta_data)):\n            continue\n        new_numId = get_numId(next_el, w_namespace)\n        if (current_numId != new_numId):\n            return True\n        return False\n", "label": 0}
{"function": "\n\ndef _handle_apply_reply(self, msg):\n    'Save the reply to an apply_request into our results.'\n    parent = msg['parent_header']\n    msg_id = parent['msg_id']\n    if (msg_id not in self.outstanding):\n        if (msg_id in self.history):\n            print(('got stale result: %s' % msg_id))\n            print(self.results[msg_id])\n            print(msg)\n        else:\n            print(('got unknown result: %s' % msg_id))\n    else:\n        self.outstanding.remove(msg_id)\n    content = msg['content']\n    header = msg['header']\n    md = self.metadata[msg_id]\n    md.update(self._extract_metadata(header, parent, content))\n    self.metadata[msg_id] = md\n    e_outstanding = self._outstanding_dict[md['engine_uuid']]\n    if (msg_id in e_outstanding):\n        e_outstanding.remove(msg_id)\n    if (content['status'] == 'ok'):\n        self.results[msg_id] = util.unserialize_object(msg['buffers'])[0]\n    elif (content['status'] == 'aborted'):\n        self.results[msg_id] = error.TaskAborted(msg_id)\n    elif (content['status'] == 'resubmitted'):\n        pass\n    else:\n        self.results[msg_id] = self._unwrap_exception(content)\n", "label": 0}
{"function": "\n\ndef get_output_for(self, input, deterministic=False, **kwargs):\n    input_mean = input.mean(self.axes)\n    input_std = TT.sqrt((input.var(self.axes) + self.epsilon))\n    use_averages = kwargs.get('batch_norm_use_averages', deterministic)\n    if use_averages:\n        mean = self.mean\n        std = self.std\n    else:\n        mean = input_mean\n        std = input_std\n    update_averages = kwargs.get('batch_norm_update_averages', (not deterministic))\n    if update_averages:\n        running_mean = theano.clone(self.mean, share_inputs=False)\n        running_std = theano.clone(self.std, share_inputs=False)\n        running_mean.default_update = (((1 - self.alpha) * running_mean) + (self.alpha * input_mean))\n        running_std.default_update = (((1 - self.alpha) * running_std) + (self.alpha * input_std))\n        mean += (0 * running_mean)\n        std += (0 * running_std)\n    param_axes = iter(range((input.ndim - len(self.axes))))\n    pattern = [('x' if (input_axis in self.axes) else next(param_axes)) for input_axis in range(input.ndim)]\n    beta = (0 if (self.beta is None) else self.beta.dimshuffle(pattern))\n    gamma = (1 if (self.gamma is None) else self.gamma.dimshuffle(pattern))\n    mean = mean.dimshuffle(pattern)\n    std = std.dimshuffle(pattern)\n    normalized = (((input - mean) * (gamma * TT.inv(std))) + beta)\n    return normalized\n", "label": 0}
{"function": "\n\ndef _set_hostport(self, host, port):\n    if (port is None):\n        i = host.rfind(':')\n        j = host.rfind(']')\n        if (i > j):\n            try:\n                port = int(host[(i + 1):])\n            except ValueError:\n                if (host[(i + 1):] == ''):\n                    port = self.default_port\n                else:\n                    raise InvalidURL((\"nonnumeric port: '%s'\" % host[(i + 1):]))\n            host = host[:i]\n        else:\n            port = self.default_port\n        if (host and (host[0] == '[') and (host[(- 1)] == ']')):\n            host = host[1:(- 1)]\n    self.host = host\n    self.port = port\n", "label": 0}
{"function": "\n\ndef is_disk(dev, bspec=None, uspec=None):\n    'Checks if given device is a disk.\\n\\n    :param dev: A device file, e.g. /dev/sda.\\n    :param bspec: A dict of properties which we get from blockdev.\\n    :param uspec: A dict of properties which we get from udevadm.\\n\\n    :returns: True if device is disk else False.\\n    '\n    if (uspec is None):\n        uspec = udevreport(dev)\n    if (uspec.get('ID_CDROM') == '1'):\n        return False\n    if (uspec.get('DEVTYPE') == 'partition'):\n        return False\n    if (('MAJOR' in uspec) and (int(uspec['MAJOR']) not in VALID_MAJORS)):\n        return False\n    if (bspec is None):\n        bspec = blockdevreport(dev)\n    if (bspec.get('ro') == '1'):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef _move_ordered_dict_item(o_dict, item_key, new_position):\n    ' Move an item in an ordered dictionary to a new position\\n\\n    :param OrderedDict o_dict: The dictionary on which the move takes place\\n    :param key item_key: The key of the item to move. If the key does not\\n        exist in the dictionary, this function will do nothing.\\n    :param key new_position: The item to move will be inserted after the\\n        item with this key. If this key does not exist in the dictionary,\\n        the item will be moved to the first position.\\n    '\n    if ((not o_dict) or (not (item_key in o_dict))):\n        return\n    item = o_dict[item_key]\n    items = o_dict.items()\n    in_there = (new_position in o_dict)\n    o_dict.clear()\n    if (not in_there):\n        o_dict[item_key] = item\n    for i in items:\n        if (i[0] != item_key):\n            o_dict[i[0]] = i[1]\n        if (i[0] == new_position):\n            o_dict[item_key] = item\n", "label": 0}
{"function": "\n\ndef test_QuotientModule():\n    R = QQ.old_poly_ring(x)\n    F = R.free_module(3)\n    N = F.submodule([1, x, (x ** 2)])\n    M = (F / N)\n    assert (M != F)\n    assert (M != N)\n    assert (M == (F / [(1, x, (x ** 2))]))\n    assert (not M.is_zero())\n    assert (F / F.basis()).is_zero()\n    SQ = (F.submodule([1, x, (x ** 2)], [2, 0, 0]) / N)\n    assert (SQ == M.submodule([2, x, (x ** 2)]))\n    assert (SQ != M.submodule([2, 1, 0]))\n    assert (SQ != M)\n    assert M.is_submodule(SQ)\n    assert (not SQ.is_full_module())\n    raises(ValueError, (lambda : (N / F)))\n    raises(ValueError, (lambda : (F.submodule([2, 0, 0]) / N)))\n    raises(ValueError, (lambda : (R.free_module(2) / F)))\n    raises(CoercionFailed, (lambda : F.convert(M.convert([1, x, (x ** 2)]))))\n    M1 = (F / [[1, 1, 1]])\n    M2 = M1.submodule([1, 0, 0], [0, 1, 0])\n    assert (M1 == M2)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.prefix == other.prefix) and (self.mnemonic == other.mnemonic) and (self.operands == other.operands) and (self.bytes == other.bytes) and (self.size == other.size) and (self.address == other.address))\n", "label": 0}
{"function": "\n\ndef wait_for_status_change(self, parameters, conn, state_requested, max_wait_time=60, poll_interval=10):\n    ' After we have sent a signal to the cloud infrastructure to change the state\\n      of the instances (unsually from runnning to either stoppped or \\n      terminated), wait for the status to change.  If all the instances change\\n      successfully, return True, if not return False.\\n\\n    Args:\\n      parameters: A dictionary of parameters.\\n      conn: A connection object returned from self.open_connection().\\n      state_requrested: String of the requested final state of the instances.\\n      max_wait_time: int of maximum amount of time (in seconds)  to wait for the\\n        state change.\\n      poll_interval: int of the number of seconds to wait between checking of\\n        the state.\\n    '\n    time_start = time.time()\n    instance_ids = parameters[self.PARAM_INSTANCE_IDS]\n    instances_in_state = {\n        \n    }\n    while True:\n        time.sleep(poll_interval)\n        reservations = conn.get_all_instances(instance_ids)\n        instances = [i for r in reservations for i in r.instances]\n        for i in instances:\n            if ((i.state == state_requested) and (i.key_name == parameters[self.PARAM_KEYNAME])):\n                if (i.id not in instances_in_state.keys()):\n                    instances_in_state[i.id] = 1\n        if (len(instances_in_state.keys()) >= len(instance_ids)):\n            return True\n        if ((time.time() - time_start) > max_wait_time):\n            return False\n", "label": 1}
{"function": "\n\ndef _index_date(date, dates):\n    '\\n    Gets the index number of a date in a date index.\\n\\n    Works in-sample and will return one past the end of the dates since\\n    prediction can start one out.\\n\\n    Currently used to validate prediction start dates.\\n\\n    If there dates are not of a fixed-frequency and date is not on the\\n    existing dates, then a ValueError is raised.\\n    '\n    if isinstance(date, string_types):\n        date = date_parser(date)\n    try:\n        if hasattr(dates, 'indexMap'):\n            return dates.indexMap[date]\n        else:\n            date = dates.get_loc(date)\n            try:\n                len(date)\n                return np.where(date)[0].item()\n            except TypeError:\n                return date\n    except KeyError as err:\n        freq = _infer_freq(dates)\n        if (freq is None):\n            raise ValueError(('There is no frequency for these dates and date %s is not in dates index. Try giving a date that is in the dates index or use an integer' % date))\n        if (_idx_from_dates(dates[(- 1)], date, freq) == 1):\n            return len(dates)\n        raise ValueError(('date %s not in date index. Try giving a date that is in the dates index or use an integer' % date))\n", "label": 0}
{"function": "\n\ndef _confirm_launched_at(self, block, exists):\n    if (exists.get('state') != 'active'):\n        return\n    (apb, ape) = self._get_audit_period(exists)\n    launched_at = self._extract_launched_at(exists)\n    if (apb and ape and (apb <= launched_at <= ape) and (len(block) == 0)):\n        raise UsageException('U8', '.exists launched_at in audit period, but no related events found.')\n", "label": 0}
{"function": "\n\ndef handle(self):\n    'Handles a request ignoring dropped connections.'\n    rv = None\n    try:\n        rv = BaseHTTPRequestHandler.handle(self)\n    except (socket.error, socket.timeout) as e:\n        self.connection_dropped(e)\n    except Exception:\n        if ((self.server.ssl_context is None) or (not is_ssl_error())):\n            raise\n    if self.server.shutdown_signal:\n        self.initiate_shutdown()\n    return rv\n", "label": 0}
{"function": "\n\ndef _check_string(data):\n    val = data.lower()\n    return ((val.startswith('select ') and (' from ' in val)) or val.startswith('insert into') or (val.startswith('update ') and (' set ' in val)) or val.startswith('delete from '))\n", "label": 0}
{"function": "\n\ndef transform(self):\n    '\\n        Apply ModelView transformations.\\n\\n        You will most likely want to wrap calls to this function with\\n        ``glPushMatrix()``/``glPopMatrix()``\\n        '\n    (x, y) = director.get_window_size()\n    if (not (self.grid and self.grid.active)):\n        self.camera.locate()\n    gl.glTranslatef(self.position[0], self.position[1], 0)\n    gl.glTranslatef(self.transform_anchor_x, self.transform_anchor_y, 0)\n    if (self.rotation != 0.0):\n        gl.glRotatef((- self._rotation), 0, 0, 1)\n    if ((self.scale != 1.0) or (self.scale_x != 1.0) or (self.scale_y != 1.0)):\n        gl.glScalef((self._scale * self._scale_x), (self._scale * self._scale_y), 1)\n    if (self.transform_anchor != (0, 0)):\n        gl.glTranslatef((- self.transform_anchor_x), (- self.transform_anchor_y), 0)\n", "label": 0}
{"function": "\n\ndef _convert_expr(self, lj, expr):\n    try:\n        if (len(expr) == 2):\n            tmp_exprs = expr[0]\n            final_exprs = expr[1]\n            if ((len(final_exprs) != 1) and (self.signature.ret_type == ctypes.c_double)):\n                raise NotImplementedError('Return of multiple expressions not supported for this callback')\n            for (name, e) in tmp_exprs:\n                val = lj._print(e)\n                lj._add_tmp_var(name, val)\n    except TypeError:\n        final_exprs = [expr]\n    vals = [lj._print(e) for e in final_exprs]\n    return vals\n", "label": 0}
{"function": "\n\ndef _make_instance_list(context, inst_list, db_inst_list, expected_attrs):\n    get_fault = (expected_attrs and ('fault' in expected_attrs))\n    inst_faults = {\n        \n    }\n    if get_fault:\n        expected_attrs.remove('fault')\n        instance_uuids = [inst['uuid'] for inst in db_inst_list]\n        faults = objects.InstanceFaultList.get_by_instance_uuids(context, instance_uuids)\n        for fault in faults:\n            if (fault.instance_uuid not in inst_faults):\n                inst_faults[fault.instance_uuid] = fault\n    inst_cls = objects.Instance\n    inst_list.objects = []\n    for db_inst in db_inst_list:\n        inst_obj = inst_cls._from_db_object(context, inst_cls(context), db_inst, expected_attrs=expected_attrs)\n        if get_fault:\n            inst_obj.fault = inst_faults.get(inst_obj.uuid, None)\n        inst_list.objects.append(inst_obj)\n    inst_list.obj_reset_changes()\n    return inst_list\n", "label": 0}
{"function": "\n\ndef login(self, username=None, password=None, section='default'):\n    '\\n        Created the passport with ``username`` and ``password`` and log in.\\n        If either ``username`` or ``password`` is None or omitted, the\\n        credentials file will be parsed.\\n\\n        :param str username: username to login (email, phone number or user ID)\\n        :param str password: password\\n        :param str section: section name in the credential file\\n        :raise: raises :class:`.AuthenticationError` if failed to login\\n        '\n    if self.has_logged_in:\n        return True\n    if ((username is None) or (password is None)):\n        credential = conf.get_credential(section)\n        username = credential['username']\n        password = credential['password']\n    passport = Passport(username, password)\n    r = self.http.post(LOGIN_URL, passport.form)\n    if (r.state is True):\n        self.passport = passport\n        passport.data = r.content['data']\n        self._user_id = r.content['data']['USER_ID']\n        return True\n    else:\n        msg = None\n        if ('err_name' in r.content):\n            if (r.content['err_name'] == 'account'):\n                msg = 'Account does not exist.'\n            elif (r.content['err_name'] == 'passwd'):\n                msg = 'Password is incorrect.'\n        raise AuthenticationError(msg)\n", "label": 0}
{"function": "\n\ndef get_version():\n    '\\n    Returns the version from the generated version file without actually\\n    loading it (and thus trying to load the extension module).\\n    '\n    if (not os.path.exists(VERSION_FILE)):\n        raise VersionNotFound((VERSION_FILE + ' does not exist'))\n    fp = open(VERSION_FILE, 'r')\n    vline = None\n    for x in fp.readlines():\n        x = x.rstrip()\n        if (not x):\n            continue\n        if (not x.startswith('__version__')):\n            continue\n        vline = x.split('=')[1]\n        break\n    if (not vline):\n        raise VersionNotFound('version file present but has no contents')\n    return vline.strip().rstrip().replace(\"'\", '')\n", "label": 0}
{"function": "\n\ndef setup(self):\n    self._jira_url = self._config['url']\n    rsa_cert_file = self._config['rsa_cert_file']\n    if (not os.path.exists(rsa_cert_file)):\n        raise Exception(('Cert file for JIRA OAuth not found at %s.' % rsa_cert_file))\n    self._rsa_key = self._read_cert(rsa_cert_file)\n    self._poll_interval = self._config.get('poll_interval', self._poll_interval)\n    oauth_creds = {\n        'access_token': self._config['oauth_token'],\n        'access_token_secret': self._config['oauth_secret'],\n        'consumer_key': self._config['consumer_key'],\n        'key_cert': self._rsa_key,\n    }\n    self._jira_client = JIRA(options={\n        'server': self._jira_url,\n    }, oauth=oauth_creds)\n    if (self._projects_available is None):\n        self._projects_available = set()\n        for proj in self._jira_client.projects():\n            self._projects_available.add(proj.key)\n    self._project = self._config.get('project', None)\n    if ((not self._project) or (self._project not in self._projects_available)):\n        raise Exception(('Invalid project (%s) to track.' % self._project))\n    self._jql_query = ('project=%s' % self._project)\n    all_issues = self._jira_client.search_issues(self._jql_query, maxResults=None)\n    self._issues_in_project = {issue.key: issue for issue in all_issues}\n", "label": 0}
{"function": "\n\ndef main():\n    INFO = 'Verilog code parser'\n    VERSION = pyverilog.utils.version.VERSION\n    USAGE = 'Usage: python example_parser.py file ...'\n\n    def showVersion():\n        print(INFO)\n        print(VERSION)\n        print(USAGE)\n        sys.exit()\n    optparser = OptionParser()\n    optparser.add_option('-v', '--version', action='store_true', dest='showversion', default=False, help='Show the version')\n    optparser.add_option('-I', '--include', dest='include', action='append', default=[], help='Include path')\n    optparser.add_option('-D', dest='define', action='append', default=[], help='Macro Definition')\n    (options, args) = optparser.parse_args()\n    filelist = args\n    if options.showversion:\n        showVersion()\n    for f in filelist:\n        if (not os.path.exists(f)):\n            raise IOError(('file not found: ' + f))\n    if (len(filelist) == 0):\n        showVersion()\n    (ast, directives) = parse(filelist, preprocess_include=options.include, preprocess_define=options.define)\n    ast.show()\n    for (lineno, directive) in directives:\n        print(('Line %d : %s' % (lineno, directive)))\n", "label": 0}
{"function": "\n\ndef win32GetLinkLocalIPv6Addresses():\n    '\\n    Return a list of strings in colon-hex format representing all the link local\\n    IPv6 addresses available on the system, as reported by\\n    I{WSAIoctl}/C{SIO_ADDRESS_LIST_QUERY}.\\n    '\n    s = socket(AF_INET6, SOCK_STREAM)\n    size = 4096\n    retBytes = c_int()\n    for i in range(2):\n        buf = create_string_buffer(size)\n        ret = WSAIoctl(s.fileno(), SIO_ADDRESS_LIST_QUERY, 0, 0, buf, size, byref(retBytes), 0, 0)\n        if (ret and retBytes.value):\n            size = retBytes.value\n        else:\n            break\n    if ret:\n        raise RuntimeError('WSAIoctl failure')\n    addrList = cast(buf, POINTER(make_SAL(0)))\n    addrCount = addrList[0].iAddressCount\n    addrList = cast(buf, POINTER(make_SAL(addrCount)))\n    addressStringBufLength = 1024\n    addressStringBuf = create_string_buffer(addressStringBufLength)\n    retList = []\n    for i in range(addrList[0].iAddressCount):\n        retBytes.value = addressStringBufLength\n        address = addrList[0].Address[i]\n        ret = WSAAddressToString(address.lpSockaddr, address.iSockaddrLength, 0, addressStringBuf, byref(retBytes))\n        if ret:\n            raise RuntimeError('WSAAddressToString failure')\n        retList.append(string_at(addressStringBuf))\n    return [addr for addr in retList if ('%' in addr)]\n", "label": 1}
{"function": "\n\ndef render(self, context):\n    if (self.page_number_variable is None):\n        default_number = self.page_number\n    else:\n        default_number = int(self.page_number_variable.resolve(context))\n    if (self.per_page_variable is None):\n        per_page = self.per_page\n    else:\n        per_page = int(self.per_page_variable.resolve(context))\n    if (self.first_page_variable is None):\n        first_page = (self.first_page or per_page)\n    else:\n        first_page = int(self.first_page_variable.resolve(context))\n    if (self.querystring_key_variable is None):\n        querystring_key = self.querystring_key\n    else:\n        querystring_key = self.querystring_key_variable.resolve(context)\n    if (self.override_path_variable is None):\n        override_path = self.override_path\n    else:\n        override_path = self.override_path_variable.resolve(context)\n    objects = self.objects.resolve(context)\n    paginator = self.paginator(objects, per_page, first_page=first_page, orphans=settings.ORPHANS)\n    if (default_number < 0):\n        default_number = utils.normalize_page_number(default_number, paginator.page_range)\n    page_number = utils.get_page_number_from_request(context['request'], querystring_key, default=default_number)\n    try:\n        page = paginator.page(page_number)\n    except EmptyPage:\n        page = paginator.page(1)\n    data = {\n        'default_number': default_number,\n        'override_path': override_path,\n        'page': page,\n        'querystring_key': querystring_key,\n    }\n    context.update({\n        'endless': data,\n        self.var_name: page.object_list,\n    })\n    return ''\n", "label": 1}
{"function": "\n\ndef configure(self, in_obj):\n    \"\\n        Must receive a list of shapes for configurations\\n        Need both the layer container and roi dataset to configure shapes\\n        'in_obj' will include be [image_shape, roi_shape] (e.g [(3, 600, 1000), 5])\\n        \"\n    self.prev_layer = None\n    if (not isinstance(in_obj, list)):\n        assert (hasattr(in_obj, 'shape') and isinstance(in_obj.shape, list))\n        assert hasattr(in_obj, 'rois_per_img')\n        self.rois_per_img = in_obj.rois_per_img\n        assert hasattr(in_obj, 'rois_per_batch')\n        self.rois_per_batch = in_obj.rois_per_batch\n        in_obj = in_obj.shape\n    assert isinstance(in_obj, list), 'ROI pool layer must have interpretable input shapes'\n    in_obj_img = in_obj[0]\n    self.in_shape_img = in_obj_img\n    if self.layers:\n        for l in self.layers:\n            in_obj_img = l.configure(in_obj_img)\n        self.in_shape = in_obj_img.out_shape\n        (self.fm_channel, self.fm_height, self.fm_width) = self.in_shape\n        self.fm_reshape_shape = (self.fm_channel, (self.fm_height * self.fm_width), self.be.bsz)\n        self.error_in_reshape = (self.fm_channel, (- 1))\n    self.out_shape = (((self.fm_channel * self.roi_H) * self.roi_W), self.rois_per_img)\n    return self\n", "label": 0}
{"function": "\n\ndef get_view_from_another_group(self, window, filename):\n    if self.calling_view_index:\n        calling_group = self.calling_view_index[0]\n    else:\n        calling_group = window.get_view_index(window.active_view())[0]\n    for group in range(window.num_groups()):\n        if (group == calling_group):\n            continue\n        for view in window.views_in_group(group):\n            if (view.file_name() == filename):\n                return view\n", "label": 0}
{"function": "\n\ndef uncomment_json(lines):\n    new_lines = []\n    for line in lines:\n        if ('//' in line):\n            if (('\"' in line) or (\"'\" in line)):\n                single_quote_open = False\n                double_quote_open = False\n                previous_slash = False\n                counter = 0\n                comment_found = False\n                last_c = ''\n                for c in line:\n                    if (c == '/'):\n                        if (previous_slash and (not single_quote_open) and (not double_quote_open)):\n                            comment_found = True\n                            break\n                        previous_slash = True\n                    else:\n                        previous_slash = False\n                    if ((c == '\"') and (last_c != '\\\\')):\n                        double_quote_open = (not double_quote_open)\n                    if ((c == \"'\") and (last_c != '\\\\')):\n                        single_quote_open = (not single_quote_open)\n                    last_c = c\n                    counter += 1\n                if comment_found:\n                    new_lines.append((line[:(counter - 1)] + '\\n'))\n                else:\n                    new_lines.append(line)\n            else:\n                new_lines.append(line.split('//')[0])\n        else:\n            new_lines.append(line)\n    return new_lines\n", "label": 1}
{"function": "\n\ndef backport_makefile(self, mode='r', buffering=None, encoding=None, errors=None, newline=None):\n    '\\n    Backport of ``socket.makefile`` from Python 3.5.\\n    '\n    if (not (set(mode) <= set(['r', 'w', 'b']))):\n        raise ValueError(('invalid mode %r (only r, w, b allowed)' % (mode,)))\n    writing = ('w' in mode)\n    reading = (('r' in mode) or (not writing))\n    assert (reading or writing)\n    binary = ('b' in mode)\n    rawmode = ''\n    if reading:\n        rawmode += 'r'\n    if writing:\n        rawmode += 'w'\n    raw = SocketIO(self, rawmode)\n    self._makefile_refs += 1\n    if (buffering is None):\n        buffering = (- 1)\n    if (buffering < 0):\n        buffering = io.DEFAULT_BUFFER_SIZE\n    if (buffering == 0):\n        if (not binary):\n            raise ValueError('unbuffered streams must be binary')\n        return raw\n    if (reading and writing):\n        buffer = io.BufferedRWPair(raw, raw, buffering)\n    elif reading:\n        buffer = io.BufferedReader(raw, buffering)\n    else:\n        assert writing\n        buffer = io.BufferedWriter(raw, buffering)\n    if binary:\n        return buffer\n    text = io.TextIOWrapper(buffer, encoding, errors, newline)\n    text.mode = mode\n    return text\n", "label": 1}
{"function": "\n\ndef _decode_signed_value_v2(secret, name, value, max_age_days, clock):\n\n    def _consume_field(s):\n        (length, _, rest) = s.partition(b':')\n        n = int(length)\n        field_value = rest[:n]\n        if (rest[n:(n + 1)] != b'|'):\n            raise ValueError('malformed v2 signed value field')\n        rest = rest[(n + 1):]\n        return (field_value, rest)\n    rest = value[2:]\n    try:\n        (key_version, rest) = _consume_field(rest)\n        (timestamp, rest) = _consume_field(rest)\n        (name_field, rest) = _consume_field(rest)\n        (value_field, rest) = _consume_field(rest)\n    except ValueError:\n        return None\n    passed_sig = rest\n    signed_string = value[:(- len(passed_sig))]\n    expected_sig = _create_signature_v2(secret, signed_string)\n    if (not _time_independent_equals(passed_sig, expected_sig)):\n        return None\n    if (name_field != utf8(name)):\n        return None\n    timestamp = int(timestamp)\n    if (timestamp < (clock() - (max_age_days * 86400))):\n        return None\n    try:\n        return base64.b64decode(value_field)\n    except Exception:\n        return None\n", "label": 0}
{"function": "\n\ndef __init__(self):\n    for (name, url) in URL_MAP.items():\n        local_path = PATH_MAP[name]\n        if (not os.path.exists(local_path)):\n            logging.info('downloading %s dataset of binarized MNIST')\n            np.save(local_path, np.loadtxt(urllib.urlretrieve(url)[0]))\n    train_set = [(x,) for x in np.load(PATH_MAP['train'])]\n    valid_set = [(x,) for x in np.load(PATH_MAP['valid'])]\n    test_set = [(x,) for x in np.load(PATH_MAP['test'])]\n    super(BinarizedMnistDataset, self).__init__(train_set, valid=valid_set, test=test_set)\n", "label": 0}
{"function": "\n\ndef _shrink(self):\n    order = sorted((x for x in self.items() if (x[1][1] is not None)), (lambda a, b: cmp(b[1][1], a[1][1])))\n    toRemove = order[0:(self.limit / 10)]\n    if ((len(self) - len(toRemove)) > (self.limit * 0.95)):\n        toRemove += [x for x in self.items()][0:(self.limit / 10)]\n    for x in toRemove:\n        del self[x[0]]\n", "label": 0}
{"function": "\n\n@timed(2)\ndef test_watchdog_send_logs(self):\n    'Test if the watchdog send logs'\n    m = Main()\n    watchdogsock = m.get_watchdogsocket()\n    m.tcp_connect()\n    m.send_connect([('login', '4.test'), ('passcode', 'test')])\n    m.get_frame()\n    msg = str(uuid.uuid4())\n    watchdogsock.settimeout(0.1)\n    try:\n        while watchdogsock.recvfrom(4096):\n            pass\n    except socket.timeout:\n        pass\n    m.send_frame('SEND', [('destination', config.TEST_WATCHDOG_TOPIC[0][0])], msg)\n    start = time.time()\n    watchdogsock.settimeout(1)\n    while (time.time() < (start + 3)):\n        (frame, _) = watchdogsock.recvfrom(4096)\n        if (frame != ''):\n            jframe = json.loads(frame)\n            if (jframe['type'] == 'log'):\n                if ((jframe['topic'] == config.TEST_WATCHDOG_TOPIC[0][0]) and (jframe['message'] == msg)):\n                    watchdogsock.close()\n                    return\n    watchdogsock.close()\n    raise Exception('No logs')\n", "label": 0}
{"function": "\n\ndef add_option(self, mask):\n    'Set arbitrary query flags using a bitmask.\\n\\n        To set the tailable flag:\\n        cursor.add_option(2)\\n        '\n    if (not isinstance(mask, int)):\n        raise TypeError('mask must be an int')\n    self.__check_okay_to_chain()\n    if (mask & _QUERY_OPTIONS['slave_okay']):\n        self.__slave_okay = True\n    if (mask & _QUERY_OPTIONS['exhaust']):\n        if self.__limit:\n            raise InvalidOperation(\"Can't use limit and exhaust together.\")\n        if self.__collection.database.connection.is_mongos:\n            raise InvalidOperation('Exhaust cursors are not supported by mongos')\n        self.__exhaust = True\n    self.__query_flags |= mask\n    return self\n", "label": 0}
{"function": "\n\ndef _create_table(self, model_class, safe=False):\n    statement = ('CREATE TABLE IF NOT EXISTS' if safe else 'CREATE TABLE')\n    meta = model_class._meta\n    (columns, constraints) = ([], [])\n    if meta.composite_key:\n        pk_cols = [meta.fields[f].as_entity() for f in meta.primary_key.field_names]\n        constraints.append(Clause(SQL('PRIMARY KEY'), EnclosedClause(*pk_cols)))\n    for field in meta.sorted_fields:\n        columns.append(self.field_definition(field))\n        if (isinstance(field, ForeignKeyField) and (not field.deferred)):\n            constraints.append(self.foreign_key_constraint(field))\n    if model_class._meta.constraints:\n        for constraint in model_class._meta.constraints:\n            if (not isinstance(constraint, Node)):\n                constraint = SQL(constraint)\n            constraints.append(constraint)\n    return Clause(SQL(statement), model_class.as_entity(), EnclosedClause(*(columns + constraints)))\n", "label": 1}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    (xshp, yshp) = shapes\n    (x, y) = node.inputs\n    if ((x.ndim == 2) and (y.ndim == 2)):\n        return [(xshp[0], yshp[1])]\n    if ((x.ndim == 1) and (y.ndim == 2)):\n        return [(yshp[1],)]\n    if ((x.ndim == 2) and (y.ndim == 1)):\n        return [(xshp[0],)]\n    if ((x.ndim == 1) and (y.ndim == 1)):\n        return [()]\n    raise NotImplementedError()\n", "label": 1}
{"function": "\n\ndef perform_request(self, method, url, params=None, body=None, timeout=None, ignore=()):\n    mc = self._get_connection()\n    url = (self.url_prefix + url)\n    if params:\n        url = ('%s?%s' % (url, urlencode((params or {\n            \n        }))))\n    full_url = (self.host + url)\n    mc_method = self.method_map.get(method, 'get')\n    start = time.time()\n    try:\n        status = 200\n        if (mc_method == 'set'):\n            response = ''\n            if (not json.dumps(mc.set(url, body))):\n                status = 500\n        else:\n            response = mc.get(url)\n        duration = (time.time() - start)\n        if response:\n            response = response.decode('utf-8')\n    except Exception as e:\n        self.log_request_fail(method, full_url, body, (time.time() - start), exception=e)\n        raise ConnectionError('N/A', str(e), e)\n    finally:\n        self._release_connection(mc)\n    if (response and (response[0] == '{') and (('\"status\"' in response) or ('\"error\"' in response))):\n        data = json.loads(response)\n        if (('status' in data) and isinstance(data['status'], int)):\n            status = data['status']\n        elif ('error' in data):\n            raise TransportError('N/A', data['error'])\n    if ((not (200 <= status < 300)) and (status not in ignore)):\n        self.log_request_fail(method, url, body, duration, status)\n        self._raise_error(status, response)\n    self.log_request_success(method, full_url, url, body, status, response, duration)\n    return (status, {\n        \n    }, response)\n", "label": 1}
{"function": "\n\ndef GetInstallRequires():\n    'Returns the install_requires for setup.py'\n    install_requires = []\n    for values in PYTHON_DEPENDENCIES:\n        module_name = values[0]\n        module_version = values[2]\n        if (module_name == 'yaml'):\n            module_name = 'PyYAML'\n        elif (module_name == 'sqlite3'):\n            module_name = 'pysqlite'\n            module_version = None\n        if (not module_version):\n            install_requires.append(module_name)\n        else:\n            install_requires.append('{0:s} >= {1:s}'.format(module_name, module_version))\n    install_requires.append('pytsk3 >= 4.1.2')\n    for (module_name, module_version) in sorted(LIBYAL_DEPENDENCIES.items()):\n        if (not module_version):\n            install_requires.append(module_name)\n        else:\n            install_requires.append('{0:s} >= {1:d}'.format(module_name, module_version))\n    return sorted(install_requires)\n", "label": 0}
{"function": "\n\ndef test_none_entity(self):\n    (s, (u1, u2, u3, u4)) = self._fixture()\n    User = self.classes.User\n    ua = aliased(User)\n    q = s.query(User, ua)\n    kt = (lambda *x: KeyedTuple(x, ['User', 'useralias']))\n    collection = [kt(u1, u2), kt(u1, None), kt(u2, u3)]\n    it = loading.merge_result(q, collection)\n    eq_([(((x and x.id) or None), ((y and y.id) or None)) for (x, y) in it], [(u1.id, u2.id), (u1.id, None), (u2.id, u3.id)])\n", "label": 0}
{"function": "\n\ndef _nthroot_solve(p, n, prec):\n    '\\n     helper function for ``nthroot``\\n     It denests ``p**Rational(1, n)`` using its minimal polynomial\\n    '\n    from sympy.polys.numberfields import _minimal_polynomial_sq\n    from sympy.solvers import solve\n    while ((n % 2) == 0):\n        p = sqrtdenest(sqrt(p))\n        n = (n // 2)\n    if (n == 1):\n        return p\n    pn = (p ** Rational(1, n))\n    x = Symbol('x')\n    f = _minimal_polynomial_sq(p, n, x)\n    if (f is None):\n        return None\n    sols = solve(f, x)\n    for sol in sols:\n        if (abs((sol - pn)).n() < (1.0 / (10 ** prec))):\n            sol = sqrtdenest(sol)\n            if (_mexpand((sol ** n)) == p):\n                return sol\n", "label": 0}
{"function": "\n\ndef fit(self, X, y, sample_weight=None):\n    if (self.uniform_variables is None):\n        raise ValueError('Please set uniform variables')\n    if (len(self.uniform_variables) == 0):\n        raise ValueError('The set of uniform variables cannot be empty')\n    if (len(X) != len(y)):\n        raise ValueError('Different size of X and y')\n    assert np.in1d(y, [0, 1]).all(), 'only two-class classification is implemented'\n    X_train_vars = self.get_train_vars(X)\n    if (self.smoothing is None):\n        self.smoothing = (10.0 / self.efficiency_steps)\n    neighbours_matrix = computeKnnIndicesOfSameClass(self.uniform_variables, X, y, n_neighbours=self.knn)\n    self.target_efficiencies = np.linspace(0, 1, (self.efficiency_steps + 2))[1:(- 1)]\n    self.classifiers = []\n    for efficiency in self.target_efficiencies:\n        classifier = uBoostBDT(uniform_variables=self.uniform_variables, uniform_label=self.uniform_label, train_variables=None, target_efficiency=efficiency, n_neighbors=self.knn, n_estimators=self.n_estimators, base_estimator=self.base_estimator, random_state=self.random_state, bagging=self.bagging, smoothing=self.smoothing, algorithm=self.algorithm)\n        self.classifiers.append(classifier)\n    self.classifiers = map_on_cluster(self.ipc_profile, _train_classifier, self.classifiers, (self.efficiency_steps * [X_train_vars]), (self.efficiency_steps * [y]), (self.efficiency_steps * [sample_weight]), (self.efficiency_steps * [neighbours_matrix]))\n    return self\n", "label": 0}
{"function": "\n\ndef visit_literal(self, node):\n    classes = node.get('classes', [])\n    if ('code' in classes):\n        node['classes'] = [cls for cls in classes if (cls != 'code')]\n        self.body.append(self.starttag(node, 'code', ''))\n        return\n    self.body.append(self.starttag(node, 'code', '', CLASS='docutils literal'))\n    text = node.astext()\n    for token in self.words_and_spaces.findall(text):\n        if token.strip():\n            if self.sollbruchstelle.search(token):\n                self.body.append(('<span class=\"pre\">%s</span>' % self.encode(token)))\n            else:\n                self.body.append(self.encode(token))\n        elif (token in ('\\n', ' ')):\n            self.body.append(token)\n        else:\n            self.body.append((('&nbsp;' * (len(token) - 1)) + ' '))\n    self.body.append('</code>')\n    raise nodes.SkipNode\n", "label": 0}
{"function": "\n\ndef test_category(self):\n    'Test category variations'\n    data = dict(self.default)\n    del data['category']\n    assert (len(verify_video_data(data, None)) == 1)\n    assert (len(verify_video_data(data, 'Test Category')) == 0)\n    data = dict(self.default)\n    assert (len(verify_video_data(data, None)) == 0)\n    assert (len(verify_video_data(data, data['category'])) == 0)\n    assert (len(verify_video_data(data, (data['category'] + 'abc'))) == 1)\n", "label": 0}
{"function": "\n\ndef fill_in_job(self, job):\n    obj_info = None\n    if (job['type'] == ssbench.DELETE_OBJECT):\n        try:\n            obj_info = self.objs_by_size[job['size_str']].popleft()\n        except IndexError:\n            return None\n    elif (job['type'] != ssbench.CREATE_OBJECT):\n        try:\n            obj_info = self.objs_by_size[job['size_str']][0]\n            self.objs_by_size[job['size_str']].rotate((- 1))\n        except IndexError:\n            return None\n    if obj_info:\n        (job['container'], job['name'], _) = obj_info\n    return job\n", "label": 0}
{"function": "\n\ndef xbrlLoaded(cntlr, options, modelXbrl, entryPoint, *args, **kwargs):\n    modelManager = cntlr.modelManager\n    if hasattr(modelManager, 'efmFiling'):\n        if ((modelXbrl.modelDocument.type == ModelDocument.Type.INSTANCE) or (modelXbrl.modelDocument.type == ModelDocument.Type.INLINEXBRL)):\n            efmFiling = modelManager.efmFiling\n            efmFiling.addReport(modelXbrl)\n            _report = efmFiling.reports[(- 1)]\n            _report.entryPoint = entryPoint\n            if (('accessionNumber' in entryPoint) and (not hasattr(efmFiling, 'accessionNumber'))):\n                efmFiling.accessionNumber = entryPoint['accessionNumber']\n            if (('exhibitType' in entryPoint) and (not hasattr(_report, 'exhibitType'))):\n                _report.exhibitType = entryPoint['exhibitType']\n            efmFiling.arelleUnitTests = modelXbrl.arelleUnitTests.copy()\n        elif (modelXbrl.modelDocument.type == ModelDocument.Type.RSSFEED):\n            testcasesStart(cntlr, options, modelXbrl)\n", "label": 1}
{"function": "\n\ndef bubble_sort(items):\n    num_items = len(items)\n    if (num_items < 2):\n        return items\n    while (num_items > 0):\n        for k in range(num_items):\n            try:\n                if (items[k] > items[(k + 1)]):\n                    copy = items[k]\n                    copy_next = items[(k + 1)]\n                    items[k] = copy_next\n                    items[(k + 1)] = copy\n                elif (items[k] == items[(k + 1)]):\n                    continue\n            except IndexError:\n                continue\n        num_items -= 1\n    return items\n", "label": 0}
{"function": "\n\ndef circle_data1(offset):\n    r1 = 1.5\n    center = (eu.Vector2(0.0, 0.0) + offset)\n    center_circle = create_obj_with_circle('center', center, r1)\n    r2 = 0.3\n    d1 = ((r1 + r2) - 0.1)\n    angles = [a for a in range(360, 1, ((- 360) // 12))]\n    ring_touching = set()\n    for a in angles:\n        center = ((d1 * eu.Vector2(cos(radians(a)), sin(radians(a)))) + offset)\n        circle = create_obj_with_circle(('ring_touching, distance 0.0, angle %3s' % a), center, r2)\n        ring_touching.add(circle)\n    near_distance = 0.1\n    d2 = ((r1 + r2) + near_distance)\n    angles = [a for a in range(360, 1, ((- 360) // 12))]\n    ring_near = set()\n    for a in angles:\n        center = ((d2 * eu.Vector2(cos(radians(a)), sin(radians(a)))) + offset)\n        circle = create_obj_with_circle(('ring_near, distance 0.1, angle %3s' % a), center, r2)\n        ring_near.add(circle)\n    far_distance = 0.2\n    d3 = ((r1 + r2) + far_distance)\n    angles = [a for a in range(360, 1, ((- 360) // 12))]\n    ring_far = set()\n    for a in angles:\n        center = ((d3 * eu.Vector2(cos(radians(a)), sin(radians(a)))) + offset)\n        circle = create_obj_with_circle(('ring_far, distance 0.2, angle %3s' % a), center, r2)\n        ring_far.add(circle)\n    return (center_circle, ring_touching, ring_near, ring_far, near_distance, far_distance, angles)\n", "label": 0}
{"function": "\n\ndef parse_etags(value):\n    'Parse an etag header.\\n\\n    :param value: the tag header to parse\\n    :return: an :class:`~werkzeug.datastructures.ETags` object.\\n    '\n    if (not value):\n        return ETags()\n    strong = []\n    weak = []\n    end = len(value)\n    pos = 0\n    while (pos < end):\n        match = _etag_re.match(value, pos)\n        if (match is None):\n            break\n        (is_weak, quoted, raw) = match.groups()\n        if (raw == '*'):\n            return ETags(star_tag=True)\n        elif quoted:\n            raw = quoted\n        if is_weak:\n            weak.append(raw)\n        else:\n            strong.append(raw)\n        pos = match.end()\n    return ETags(strong, weak)\n", "label": 0}
{"function": "\n\ndef render_pep440_old(pieces):\n    if pieces['closest-tag']:\n        rendered = pieces['closest-tag']\n        if (pieces['distance'] or pieces['dirty']):\n            rendered += ('.post%d' % pieces['distance'])\n            if pieces['dirty']:\n                rendered += '.dev0'\n    else:\n        rendered = ('0.post%d' % pieces['distance'])\n        if pieces['dirty']:\n            rendered += '.dev0'\n    return rendered\n", "label": 0}
{"function": "\n\n@register.function\ndef inline_css(bundle, media=False, debug=None):\n    '\\n    If we are in debug mode, just output a single style tag for each css file.\\n    If we are not in debug mode, return a style that contains bundle-min.css.\\n    Forces a regular css() call for external URLs (no inline allowed).\\n    '\n    if (debug is None):\n        debug = getattr(settings, 'TEMPLATE_DEBUG', False)\n    if debug:\n        items = [_get_compiled_css_url(i) for i in settings.MINIFY_BUNDLES['css'][bundle]]\n    else:\n        items = [('css/%s-min.css' % bundle)]\n    if (not media):\n        media = getattr(settings, 'CSS_MEDIA_DEFAULT', 'screen,projection,tv')\n    contents = []\n    for css in items:\n        if is_external(css):\n            return _build_html([css], ('<link rel=\"stylesheet\" media=\"%s\" href=\"%%s\" />' % media))\n        with open(get_path(css), 'r') as f:\n            contents.append(f.read())\n    return _build_html(contents, ('<style type=\"text/css\" media=\"%s\">%%s</style>' % media))\n", "label": 0}
{"function": "\n\ndef __init__(self, app, context, old_key=None, new_key=None, old='', new=''):\n    super(SideDiffCommentEdit, self).__init__([])\n    self.app = app\n    self.context = context\n    self.old_key = old_key\n    self.new_key = new_key\n    self.old = mywid.MyEdit(edit_text=old, multiline=True, ring=app.ring)\n    self.new = mywid.MyEdit(edit_text=new, multiline=True, ring=app.ring)\n    self.contents.append((urwid.Text(''), ('given', LN_COL_WIDTH, False)))\n    if (context.old_file_key and ((context.old_ln is not None) or context.header)):\n        self.contents.append((urwid.AttrMap(self.old, 'draft-comment'), ('weight', 1, False)))\n    else:\n        self.contents.append((urwid.Text(''), ('weight', 1, False)))\n    self.contents.append((urwid.Text(''), ('given', LN_COL_WIDTH, False)))\n    if (context.new_file_key and ((context.new_ln is not None) or context.header)):\n        self.contents.append((urwid.AttrMap(self.new, 'draft-comment'), ('weight', 1, False)))\n        new_editable = True\n    else:\n        self.contents.append((urwid.Text(''), ('weight', 1, False)))\n        new_editable = False\n    if new_editable:\n        self.focus_position = 3\n    else:\n        self.focus_position = 1\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('inport_ret_val,outport_ret_val,expected', [(False, False, False), (False, True, False), (True, False, False), (True, True, True)])\ndef test_did_connect(actor, inport_ret_val, outport_ret_val, expected):\n    for port in actor.inports.values():\n        port.is_connected = Mock(return_value=inport_ret_val)\n    for port in actor.outports.values():\n        port.is_connected = Mock(return_value=outport_ret_val)\n    actor.fsm = Mock()\n    actor.did_connect(None)\n    if expected:\n        actor.fsm.transition_to.assert_called_with(Actor.STATUS.ENABLED)\n        assert actor._calvinsys.scheduler_wakeup.called\n    else:\n        assert (not actor.fsm.transition_to.called)\n        assert (not actor._calvinsys.scheduler_wakeup.called)\n", "label": 0}
{"function": "\n\ndef python_value(self, value):\n    if value:\n        if isinstance(value, basestring):\n            pp = (lambda x: x.time())\n            return format_date_time(value, self.formats, pp)\n        elif isinstance(value, datetime.datetime):\n            return value.time()\n    if ((value is not None) and isinstance(value, datetime.timedelta)):\n        return (datetime.datetime.min + value).time()\n    return value\n", "label": 0}
{"function": "\n\ndef attach_iso(self, name, iso):\n    (dom, host) = self.get_active_domain(name)\n    if (not dom):\n        dom = self.get_inactive_domain(name)\n        host = dom['host']\n    for x in self.hosts:\n        if (host == x['name']):\n            conn = x['conn']\n    dom = conn.lookupByName(name)\n    cdrom = None\n    desc = fromstring(dom.XMLDesc(libvirt.VIR_DOMAIN_XML_SECURE))\n    for disk in desc.findall('.//disk'):\n        if (disk.get('device') == 'cdrom'):\n            cdrom = disk\n            if cdrom.find('.//source'):\n                cdrom.find('.//source').set('file', iso)\n            else:\n                desc.find('.//devices').remove(cdrom)\n                cdrom = None\n    if (not cdrom):\n        xml = '\\n<disk type=\"file\" device=\"cdrom\">\\n  <driver name=\"qemu\"/>\\n  <source file=\"%s\" />\\n  <target dev=\"hdc\" bus=\"ide\"/>\\n  <readonly/>\\n</disk>\\n   '\n    xml = (xml % iso)\n    desc.find('.//devices').insert((- 1), fromstring(xml))\n    conn.defineXML(tostring(desc))\n    return True\n", "label": 0}
{"function": "\n\ndef __init__(self, value, context):\n    if value.startswith('r'):\n        value = value[1:]\n        raw = True\n    else:\n        raw = False\n    if value.startswith(\"'''\"):\n        assert value.endswith(\"'''\")\n        value = value[3:(- 3)]\n    elif value.startswith('\"\"\"'):\n        assert value.endswith('\"\"\"')\n        value = value[3:(- 3)]\n    elif value.startswith('\"'):\n        assert value.endswith('\"')\n        value = value[1:(- 1)]\n    elif value.startswith(\"'\"):\n        assert value.endswith(\"'\")\n        value = value[1:(- 1)]\n    else:\n        raise NotImplementedError(value)\n    if (not raw):\n        value = '\\\\'.join(map(self._rep, value.split('\\\\\\\\')))\n    super().__init__(value, context)\n", "label": 1}
{"function": "\n\ndef read(self, filename):\n    'Read only one filename. In contrast to the original ConfigParser of\\n        Python, this one is able to read only one file at a time. The last\\n        read file will be used for the :meth:`write` method.\\n\\n        .. versionchanged:: 1.9.0\\n            :meth:`read` now calls the callbacks if read changed any values.\\n\\n        '\n    if (not isinstance(filename, string_types)):\n        raise Exception('Only one filename is accepted ({})'.format(string_types.__name__))\n    self.filename = filename\n    old_vals = {sect: {k: v for (k, v) in self.items(sect)} for sect in self.sections()}\n    PythonConfigParser.read(self, filename)\n    f = self._do_callbacks\n    for section in self.sections():\n        if (section not in old_vals):\n            for (k, v) in self.items(section):\n                f(section, k, v)\n            continue\n        old_keys = old_vals[section]\n        for (k, v) in self.items(section):\n            if ((k not in old_keys) or (v != old_keys[k])):\n                f(section, k, v)\n", "label": 1}
{"function": "\n\ndef follow_joins(self, collected):\n    prepared = [collected[self.model]]\n    for metadata in self.join_list:\n        inst = collected[metadata.src]\n        try:\n            joined_inst = collected[metadata.dest]\n        except KeyError:\n            joined_inst = collected[metadata.dest_model]\n        mpk = (metadata.primary_key is not None)\n        can_populate_joined_pk = (mpk and (metadata.attr in inst._data) and (getattr(joined_inst, metadata.primary_key.name) is None))\n        if can_populate_joined_pk:\n            setattr(joined_inst, metadata.primary_key.name, inst._data[metadata.attr])\n        if metadata.is_backref:\n            can_populate_joined_fk = (mpk and (metadata.foreign_key is not None) and (getattr(inst, metadata.primary_key.name) is not None) and (joined_inst._data.get(metadata.foreign_key.name) is None))\n            if can_populate_joined_fk:\n                setattr(joined_inst, metadata.foreign_key.name, inst)\n        setattr(inst, metadata.attr, joined_inst)\n        prepared.append(joined_inst)\n    return prepared\n", "label": 1}
{"function": "\n\ndef _dmp_inner_gcd(f, g, u, K):\n    'Helper function for `dmp_inner_gcd()`. '\n    if (not K.is_Exact):\n        try:\n            exact = K.get_exact()\n        except DomainError:\n            return (dmp_one(u, K), f, g)\n        f = dmp_convert(f, u, K, exact)\n        g = dmp_convert(g, u, K, exact)\n        (h, cff, cfg) = _dmp_inner_gcd(f, g, u, exact)\n        h = dmp_convert(h, u, exact, K)\n        cff = dmp_convert(cff, u, exact, K)\n        cfg = dmp_convert(cfg, u, exact, K)\n        return (h, cff, cfg)\n    elif K.has_Field:\n        if (K.is_QQ and query('USE_HEU_GCD')):\n            try:\n                return dmp_qq_heu_gcd(f, g, u, K)\n            except HeuristicGCDFailed:\n                pass\n        return dmp_ff_prs_gcd(f, g, u, K)\n    else:\n        if (K.is_ZZ and query('USE_HEU_GCD')):\n            try:\n                return dmp_zz_heu_gcd(f, g, u, K)\n            except HeuristicGCDFailed:\n                pass\n        return dmp_rr_prs_gcd(f, g, u, K)\n", "label": 1}
{"function": "\n\ndef _number_type_helper(national_number, metadata):\n    'Return the type of the given number against the metadata'\n    if (not _is_number_matching_desc(national_number, metadata.general_desc)):\n        return PhoneNumberType.UNKNOWN\n    if _is_number_matching_desc(national_number, metadata.premium_rate):\n        return PhoneNumberType.PREMIUM_RATE\n    if _is_number_matching_desc(national_number, metadata.toll_free):\n        return PhoneNumberType.TOLL_FREE\n    if _is_number_matching_desc(national_number, metadata.shared_cost):\n        return PhoneNumberType.SHARED_COST\n    if _is_number_matching_desc(national_number, metadata.voip):\n        return PhoneNumberType.VOIP\n    if _is_number_matching_desc(national_number, metadata.personal_number):\n        return PhoneNumberType.PERSONAL_NUMBER\n    if _is_number_matching_desc(national_number, metadata.pager):\n        return PhoneNumberType.PAGER\n    if _is_number_matching_desc(national_number, metadata.uan):\n        return PhoneNumberType.UAN\n    if _is_number_matching_desc(national_number, metadata.voicemail):\n        return PhoneNumberType.VOICEMAIL\n    if _is_number_matching_desc(national_number, metadata.fixed_line):\n        if metadata.same_mobile_and_fixed_line_pattern:\n            return PhoneNumberType.FIXED_LINE_OR_MOBILE\n        elif _is_number_matching_desc(national_number, metadata.mobile):\n            return PhoneNumberType.FIXED_LINE_OR_MOBILE\n        return PhoneNumberType.FIXED_LINE\n    if ((not metadata.same_mobile_and_fixed_line_pattern) and _is_number_matching_desc(national_number, metadata.mobile)):\n        return PhoneNumberType.MOBILE\n    return PhoneNumberType.UNKNOWN\n", "label": 1}
{"function": "\n\n@contextfunction\ndef permission_block(context, object):\n    'Block with objects permissions'\n    request = context['request']\n    response_format = 'html'\n    if ('response_format' in context):\n        response_format = context['response_format']\n    response_format_tags = response_format\n    if ('response_format_tags' in context):\n        response_format_tags = context['response_format_tags']\n    if ('permission' in request.GET):\n        if request.user.profile.has_permission(object, mode='w'):\n            if request.POST:\n                if ('cancel' in request.POST):\n                    request.redirect = request.path\n                    return Markup(render_to_string('core/tags/permission_block', {\n                        'object': object,\n                        'path': request.path,\n                    }, context_instance=RequestContext(request), response_format=response_format))\n                form = PermissionForm(request.POST, instance=object)\n                if form.is_valid():\n                    form.save()\n                    request.redirect = request.path\n                    return Markup(render_to_string('core/tags/permission_block', {\n                        'object': object,\n                        'path': request.path,\n                    }, context_instance=RequestContext(request), response_format=response_format))\n            else:\n                form = PermissionForm(instance=object)\n            context = {\n                'object': object,\n                'path': request.path,\n                'form': form,\n            }\n            if ('ajax' in response_format_tags):\n                context = converter.preprocess_context(context)\n            return Markup(render_to_string('core/tags/permission_block_edit', context, context_instance=RequestContext(request), response_format=response_format))\n    return Markup(render_to_string('core/tags/permission_block', {\n        'object': object,\n        'path': request.path,\n    }, context_instance=RequestContext(request), response_format=response_format))\n", "label": 1}
{"function": "\n\ndef should_sync(domain, last_sync, utcnow=None):\n    if ((not last_sync) or (not last_sync.date)):\n        return True\n    utcnow = (utcnow or datetime.utcnow())\n    try:\n        timezone = domain.get_default_timezone()\n    except pytz.UnknownTimeZoneError:\n        timezone = utc\n    _assert = soft_assert(to=[(('droberts' + '@') + 'dimagi.com')])\n    last_sync_utc = last_sync.date\n    if (not _assert((last_sync_utc.tzinfo is None), 'last_sync.date should be an offset-naive dt')):\n        last_sync_utc = UserTime(last_sync_utc).server_time().done()\n    last_sync_local = ServerTime(last_sync_utc).user_time(timezone).done()\n    current_date_local = ServerTime(utcnow).user_time(timezone).done()\n    if (current_date_local.date() != last_sync_local.date()):\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef insert_filename(self, dirname, filename, ext='.py'):\n    'Ensure that a specific filename exists in the breakpoint tree'\n    full_filename = os.path.join(dirname, filename)\n    if (not self.exists(nodify(full_filename))):\n        if (full_filename == self.normalizer(full_filename)):\n            if self.root:\n                return\n            self.insert_dirname(dirname)\n        elif (self.root is None):\n            return\n        files = sorted(self.get_children(nodify(dirname)), reverse=False)\n        index = len([item for item in files if (item < nodify(full_filename))])\n        self.insert(nodify(dirname), index, nodify(os.path.join(dirname, filename)), text=filename, open=True, tags=((['file'] + ['code']) if (ext == '.py') else ['non_code']))\n", "label": 0}
{"function": "\n\ndef __getitem__(self, getitem_args):\n    'Rel selector and traversor for navigators'\n    traversal = utils.normalize_getitem_args(getitem_args)\n    intermediates = [self]\n    val = self\n    for (i, arg) in enumerate(traversal):\n        try:\n            if isinstance(arg, six.string_types):\n                val()\n                if (val._embedded and (arg in val._embedded)):\n                    val = val._embedded[arg]\n                else:\n                    val = val.links()[arg]\n            elif isinstance(arg, tuple):\n                val = val.get_by(*arg, raise_exc=True)\n            elif (isinstance(arg, int) and isinstance(val, list)):\n                val = val[arg]\n            else:\n                raise TypeError(\"{0!r} doesn't accept a traversor of {1!r}\".format(val, arg))\n        except Exception as e:\n            raise exc.OffTheRailsException(traversal, i, intermediates, e)\n        intermediates.append(val)\n    return val\n", "label": 1}
{"function": "\n\ndef _plot_topomap_multi_cbar(data, pos, ax, title=None, unit=None, vmin=None, vmax=None, cmap='RdBu_r', colorbar=False, cbar_fmt='%3.3f'):\n    'Aux Function'\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n    _hide_frame(ax)\n    vmin = (np.min(data) if (vmin is None) else vmin)\n    vmax = (np.max(data) if (vmax is None) else vmax)\n    if (title is not None):\n        ax.set_title(title, fontsize=10)\n    (im, _) = plot_topomap(data, pos, vmin=vmin, vmax=vmax, axes=ax, cmap=cmap, image_interp='bilinear', contours=False, show=False)\n    if (colorbar is True):\n        divider = make_axes_locatable(ax)\n        cax = divider.append_axes('right', size='10%', pad=0.25)\n        cbar = plt.colorbar(im, cax=cax, format=cbar_fmt)\n        cbar.set_ticks((vmin, vmax))\n        if (unit is not None):\n            cbar.ax.set_title(unit, fontsize=8)\n        cbar.ax.tick_params(labelsize=8)\n", "label": 0}
{"function": "\n\ndef unregister(self, vote_model, content_models, Provider):\n    provider_instance = Provider()\n    for signal in provider_instance.rate_signals:\n        if isinstance(signal, str):\n            sig_class_name = signal.split('.')[(- 1)]\n            sig_instance = import_from_classname(signal)\n            listener = getattr(provider_instance, sig_class_name, False)\n            if listener:\n                for model in content_models:\n                    sig_instance.disconnect(listener, sender=model)\n    new_set = [i for i in self.providers if (not isinstance(i, Provider))]\n    self.providers = set(new_set)\n    for model in content_models:\n        del self._content_providers[model_path(model)]\n    del self._vote_providers[model_path(vote_model)]\n", "label": 0}
{"function": "\n\ndef list(self, other_pks=None, filters=None):\n    \"\\n        List returns back list of URI\\n        In case of multiple pks, We guess the pk\\n        by using '<modelname>_id' as the field convention\\n        \"\n    try:\n        pk = primary_key(self.cls)\n    except KeyNotFoundException:\n        pk = None\n    filter_dict = dict()\n    if filters:\n        filter_dict = {k: v for (k, v) in filters.items()}\n        filter_dict.pop('format', None)\n        filter_dict.pop('page', None)\n        filter_dict.pop('sort_by', None)\n    if other_pks:\n        query_pks = dict()\n        for key in class_keys(self.cls):\n            if (key in other_pks):\n                query_pks[key] = other_pks[key]\n        query_pks.update(filter_dict)\n        if pk:\n            queryset = self.session.query(self.cls.__dict__[pk]).filter_by(**query_pks).all()\n        else:\n            queryset = self.session.query(self.cls).filter_by(**query_pks).all()\n    elif filter_dict:\n        if pk:\n            queryset = self.session.query(self.cls.__dict__[pk]).filter_by(**filter_dict).all()\n        else:\n            queryset = self.session.query(self.cls).filter_by(**filter_dict).all()\n    elif pk:\n        queryset = self.session.query(self.cls.__dict__[pk]).all()\n    else:\n        queryset = self.session.query(self.cls).limit(1000).all()\n    return queryset\n", "label": 1}
{"function": "\n\ndef filter_attributes(self, attrs):\n    '\\n        Filter set of model attributes based on only, exclude and include\\n        meta parameters.\\n\\n        :param attrs: Set of attributes\\n        '\n    if self.meta.only:\n        attrs = OrderedDict([(key, prop) for (key, prop) in map(self.validate_attribute, self.meta.only) if key])\n    else:\n        if self.meta.include:\n            attrs.update([(key, prop) for (key, prop) in map(self.validate_attribute, self.meta.include) if key])\n        if self.meta.exclude:\n            for key in self.meta.exclude:\n                try:\n                    del attrs[key]\n                except KeyError:\n                    if self.meta.attr_errors:\n                        raise InvalidAttributeException(key)\n    return attrs\n", "label": 1}
{"function": "\n\ndef run_job(name, force=False):\n    \"\\n    Run a scheduled job on the minion immediately\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' schedule.run_job job1\\n\\n        salt '*' schedule.run_job job1 force=True\\n        Force the job to run even if it is disabled.\\n    \"\n    ret = {\n        'comment': [],\n        'result': True,\n    }\n    if (not name):\n        ret['comment'] = 'Job name is required.'\n        ret['result'] = False\n    schedule = list_(show_all=True, return_yaml=False)\n    if (name in schedule):\n        data = schedule[name]\n        if (('enabled' in data) and (not data['enabled']) and (not force)):\n            ret['comment'] = 'Job {0} is disabled.'.format(name)\n        else:\n            out = __salt__['event.fire']({\n                'name': name,\n                'func': 'run_job',\n            }, 'manage_schedule')\n            if out:\n                ret['comment'] = 'Scheduling Job {0} on minion.'.format(name)\n            else:\n                ret['comment'] = 'Failed to run job {0} on minion.'.format(name)\n                ret['result'] = False\n    else:\n        ret['comment'] = 'Job {0} does not exist.'.format(name)\n        ret['result'] = False\n    return ret\n", "label": 0}
{"function": "\n\ndef to_pairs(dictlike):\n    'Yield (key, value) pairs from any dict-like object.\\n\\n    Implements an optimized version of the dict.update() definition of\\n    \"dictlike\".\\n\\n    '\n    if hasattr(dictlike, 'iteritems'):\n        return dictlike.iteritems()\n    elif hasattr(dictlike, 'keys'):\n        return ((key, dictlike[key]) for key in dictlike.keys())\n    elif hasattr(dictlike, '_asdict'):\n        return dictlike._asdict().iteritems()\n    else:\n        return ((key, value) for (key, value) in dictlike)\n", "label": 0}
{"function": "\n\ndef iterfieldmap(source, mappings, failonerror, errorvalue):\n    it = iter(source)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    outhdr = mappings.keys()\n    (yield tuple(outhdr))\n    mapfuns = dict()\n    for (outfld, m) in mappings.items():\n        if (m in hdr):\n            mapfuns[outfld] = operator.itemgetter(m)\n        elif (isinstance(m, int) and (m < len(hdr))):\n            mapfuns[outfld] = operator.itemgetter(m)\n        elif isinstance(m, string_types):\n            mapfuns[outfld] = expr(m)\n        elif callable(m):\n            mapfuns[outfld] = m\n        elif (isinstance(m, (tuple, list)) and (len(m) == 2)):\n            srcfld = m[0]\n            fm = m[1]\n            if callable(fm):\n                mapfuns[outfld] = composefun(fm, srcfld)\n            elif isinstance(fm, dict):\n                mapfuns[outfld] = composedict(fm, srcfld)\n            else:\n                raise ArgumentError('expected callable or dict')\n        else:\n            raise ArgumentError(('invalid mapping %r: %r' % (outfld, m)))\n    it = (Record(row, flds) for row in it)\n    for row in it:\n        outrow = list()\n        for outfld in outhdr:\n            try:\n                val = mapfuns[outfld](row)\n            except Exception as e:\n                if failonerror:\n                    raise e\n                else:\n                    val = errorvalue\n            outrow.append(val)\n        (yield tuple(outrow))\n", "label": 1}
{"function": "\n\ndef _gen_reference(self):\n\n    def get_scope_data(scope):\n        ret = []\n        for si in ScopeInfoIterator(self.context.options.known_scope_to_info).iterate([scope]):\n            help_info = HelpInfoExtracter(si.scope).get_option_scope_help_info_from_parser(self.context.options.get_parser(si.scope))\n            ret.append({\n                'scope_info': si,\n                'help_info': help_info._asdict(),\n            })\n        return ret\n    all_global_data = get_scope_data(GLOBAL_SCOPE)\n    global_scope_data = all_global_data[0:1]\n    global_subsystem_data = all_global_data[1:]\n    goal_scopes = sorted([si.scope for si in self.context.options.known_scope_to_info.values() if (si.scope and ('.' not in si.scope) and (si.category != ScopeInfo.SUBSYSTEM))])\n    goal_data = []\n    for scope in goal_scopes:\n        goal_data.append({\n            'goal': scope,\n            'goal_description': Goal.by_name(scope).description,\n            'task_data': get_scope_data(scope)[1:],\n        })\n    self._do_render(self.get_options().pants_reference_template, {\n        'global_scope_data': global_scope_data,\n        'global_subsystem_data': global_subsystem_data,\n        'goal_data': goal_data,\n    })\n", "label": 0}
{"function": "\n\ndef check_delete_headers(request):\n    \"\\n    Validate if 'x-delete' headers are have correct values\\n    values should be positive integers and correspond to\\n    a time in the future.\\n\\n    :param request: the swob request object\\n\\n    :returns: HTTPBadRequest in case of invalid values\\n              or None if values are ok\\n    \"\n    if ('x-delete-after' in request.headers):\n        try:\n            x_delete_after = int(request.headers['x-delete-after'])\n        except ValueError:\n            raise HTTPBadRequest(request=request, content_type='text/plain', body='Non-integer X-Delete-After')\n        actual_del_time = (time.time() + x_delete_after)\n        if (actual_del_time < time.time()):\n            raise HTTPBadRequest(request=request, content_type='text/plain', body='X-Delete-After in past')\n        request.headers['x-delete-at'] = utils.normalize_delete_at_timestamp(actual_del_time)\n    if ('x-delete-at' in request.headers):\n        try:\n            x_delete_at = int(utils.normalize_delete_at_timestamp(int(request.headers['x-delete-at'])))\n        except ValueError:\n            raise HTTPBadRequest(request=request, content_type='text/plain', body='Non-integer X-Delete-At')\n        if (x_delete_at < time.time()):\n            raise HTTPBadRequest(request=request, content_type='text/plain', body='X-Delete-At in past')\n    return request\n", "label": 0}
{"function": "\n\ndef min_max(obj, val, is_max):\n    ' min/max validator for float and integer\\n    '\n    n = getattr(obj, ('maximum' if is_max else 'minimum'), None)\n    if (n == None):\n        return\n    _eq = getattr(obj, ('exclusiveMaximum' if is_max else 'exclusiveMinimum'), False)\n    if is_max:\n        to_raise = ((val >= n) if _eq else (val > n))\n    else:\n        to_raise = ((val <= n) if _eq else (val < n))\n    if to_raise:\n        raise ValidationError('condition failed: {0}, v:{1} compared to o:{2}'.format(('maximum' if is_max else 'minimum'), val, n))\n", "label": 1}
{"function": "\n\ndef parse_authorization_header(value):\n    'Parse an HTTP basic/digest authorization header transmitted by the web\\n    browser.  The return value is either `None` if the header was invalid or\\n    not given, otherwise an :class:`~werkzeug.datastructures.Authorization`\\n    object.\\n\\n    :param value: the authorization header to parse.\\n    :return: a :class:`~werkzeug.datastructures.Authorization` object or `None`.\\n    '\n    if (not value):\n        return\n    value = wsgi_to_bytes(value)\n    try:\n        (auth_type, auth_info) = value.split(None, 1)\n        auth_type = auth_type.lower()\n    except ValueError:\n        return\n    if (auth_type == b'basic'):\n        try:\n            (username, password) = base64.b64decode(auth_info).split(b':', 1)\n        except Exception as e:\n            return\n        return Authorization('basic', {\n            'username': bytes_to_wsgi(username),\n            'password': bytes_to_wsgi(password),\n        })\n    elif (auth_type == b'digest'):\n        auth_map = parse_dict_header(auth_info)\n        for key in ('username', 'realm', 'nonce', 'uri', 'response'):\n            if (not (key in auth_map)):\n                return\n        if ('qop' in auth_map):\n            if ((not auth_map.get('nc')) or (not auth_map.get('cnonce'))):\n                return\n        return Authorization('digest', auth_map)\n", "label": 1}
{"function": "\n\ndef build_instances(self, ctxt, **kwargs):\n    'Build instances.'\n    build_inst_kwargs = kwargs\n    instances = build_inst_kwargs['instances']\n    build_inst_kwargs['image'] = jsonutils.to_primitive(build_inst_kwargs['image'])\n    version = '1.34'\n    if self.client.can_send_version('1.34'):\n        build_inst_kwargs.pop('legacy_bdm', None)\n    else:\n        bdm_p = objects_base.obj_to_primitive(build_inst_kwargs['block_device_mapping'])\n        build_inst_kwargs['block_device_mapping'] = bdm_p\n        version = '1.32'\n    if (not self.client.can_send_version('1.32')):\n        instances_p = [jsonutils.to_primitive(inst) for inst in instances]\n        build_inst_kwargs['instances'] = instances_p\n        version = '1.30'\n    if (not self.client.can_send_version('1.30')):\n        if ('filter_properties' in build_inst_kwargs):\n            filter_properties = build_inst_kwargs['filter_properties']\n            flavor = filter_properties['instance_type']\n            flavor_p = objects_base.obj_to_primitive(flavor)\n            filter_properties['instance_type'] = flavor_p\n        version = '1.8'\n    cctxt = self.client.prepare(version=version)\n    cctxt.cast(ctxt, 'build_instances', build_inst_kwargs=build_inst_kwargs)\n", "label": 0}
{"function": "\n\ndef W(self):\n    'ISO-8601 week number of year, weeks starting on Monday'\n    week_number = None\n    jan1_weekday = (self.data.replace(month=1, day=1).weekday() + 1)\n    weekday = (self.data.weekday() + 1)\n    day_of_year = self.z()\n    if ((day_of_year <= (8 - jan1_weekday)) and (jan1_weekday > 4)):\n        if ((jan1_weekday == 5) or ((jan1_weekday == 6) and calendar.isleap((self.data.year - 1)))):\n            week_number = 53\n        else:\n            week_number = 52\n    else:\n        if calendar.isleap(self.data.year):\n            i = 366\n        else:\n            i = 365\n        if ((i - day_of_year) < (4 - weekday)):\n            week_number = 1\n        else:\n            j = ((day_of_year + (7 - weekday)) + (jan1_weekday - 1))\n            week_number = (j // 7)\n            if (jan1_weekday > 4):\n                week_number -= 1\n    return week_number\n", "label": 1}
{"function": "\n\ndef topurl(url, positions=''):\n    'Return the top performing URL for a site given a positions object.'\n    if (not url):\n        return 'lite value required in column or Config tab.'\n    apexdom = apex(url)\n    if positions:\n        urldict = json.loads(positions)\n        if urldict:\n            for (thepos, aurl) in urldict.iteritems():\n                if (apexdom in aurl):\n                    return aurl\n        else:\n            return None\n", "label": 0}
{"function": "\n\ndef assign_index(self, lhs, rhs):\n    if (isinstance(lhs.index.type, ScalarT) and isinstance(rhs.type, ScalarT)):\n        self.assign(lhs, rhs)\n        return\n    (scalar_indices, scalar_index_positions, slices, slice_positions) = self.dissect_index_expr(lhs)\n    assert (len(scalar_indices) == len(scalar_index_positions))\n    assert (len(slices) == len(slice_positions))\n    if (len(slices) == 0):\n        self.setidx(lhs.value, self.tuple(scalar_indices), rhs)\n        return\n    bounds = self.tuple([self.div(self.sub(stop, start), step) for (start, stop, step) in slices])\n    setidx_fn = self.make_setidx_fn(lhs.value.type, rhs.type, scalar_index_positions, slice_positions)\n    starts = [start for (start, _, _) in slices]\n    steps = [step for (_, _, step) in slices]\n    closure = self.closure(setidx_fn, ((([lhs.value, rhs] + scalar_indices) + starts) + steps))\n    self.parfor(closure, bounds)\n", "label": 1}
{"function": "\n\ndef chfullname(name, fullname):\n    \"\\n    Change the user's Full Name\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' user.chfullname foo 'Foo Bar'\\n    \"\n    if isinstance(fullname, string_types):\n        fullname = _sdecode(fullname)\n    pre_info = info(name)\n    if (not pre_info):\n        raise CommandExecutionError(\"User '{0}' does not exist\".format(name))\n    if isinstance(pre_info['fullname'], string_types):\n        pre_info['fullname'] = _sdecode(pre_info['fullname'])\n    if (fullname == pre_info['fullname']):\n        return True\n    _dscl(['/Users/{0}'.format(name), 'RealName', fullname], ctype='create')\n    time.sleep(1)\n    current = info(name).get('fullname')\n    if isinstance(current, string_types):\n        current = _sdecode(current)\n    return (current == fullname)\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('InvalidOperation')\n    if (self.what is not None):\n        oprot.writeFieldBegin('what', TType.I32, 1)\n        oprot.writeI32(self.what)\n        oprot.writeFieldEnd()\n    if (self.why is not None):\n        oprot.writeFieldBegin('why', TType.STRING, 2)\n        oprot.writeString(self.why)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\n@login_notrequired\ndef show_login_page(request, login_errors=False):\n    'Used by the non-jframe login'\n    redirect_to = request.REQUEST.get('next', '/')\n    is_first_login_ever = OAuthBackend.is_first_login_ever()\n    request.session.set_test_cookie()\n    return render('oauth-login.mako', request, {\n        'action': urlresolvers.reverse('oauth_login'),\n        'next': redirect_to,\n        'first_login_ever': is_first_login_ever,\n        'login_errors': ((request.method == 'POST') or login_errors),\n        'socialGoogle': ((liboauth.conf.CONSUMER_KEY_GOOGLE.get() != '') and (liboauth.conf.CONSUMER_SECRET_GOOGLE.get() != '')),\n        'socialFacebook': ((liboauth.conf.CONSUMER_KEY_FACEBOOK.get() != '') and (liboauth.conf.CONSUMER_SECRET_FACEBOOK.get() != '')),\n        'socialLinkedin': ((liboauth.conf.CONSUMER_KEY_LINKEDIN.get() != '') and (liboauth.conf.CONSUMER_SECRET_LINKEDIN.get() != '')),\n        'socialTwitter': ((liboauth.conf.CONSUMER_KEY_TWITTER.get() != '') and (liboauth.conf.CONSUMER_SECRET_TWITTER.get() != '')),\n    })\n", "label": 0}
{"function": "\n\ndef get_attribute_value(obj, bit):\n    try:\n        value = getattr(obj, bit)\n    except (AttributeError, ObjectDoesNotExist):\n        value = None\n    else:\n        if (callable(value) and (not isinstance(value, Manager))):\n            if ((not hasattr(value, 'alters_data')) or (value.alters_data is not True)):\n                value = value()\n    return value\n", "label": 0}
{"function": "\n\ndef inject(context, injection_list):\n    'Helper function to inject the payload and to collect the results\\n\\n    :param context: The Behave context\\n    :param injection_list: An anomaly dictionary, see dictwalker.py\\n    '\n    if hasattr(context, 'injection_methods'):\n        methods = context.injection_methods\n    else:\n        methods = ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS', 'HEAD', 'PATCH']\n    responses = []\n    for injection in injection_list:\n        for injected_submission in dictwalk(context.submission[0], injection):\n            for method in methods:\n                if (context.type == 'urlencode'):\n                    form_string = serialise_to_url(injected_submission, encode=True)\n                    if (method == 'GET'):\n                        form_string = ('?' + form_string)\n                if (context.type == 'url-parameters'):\n                    form_string = dict_to_urlparams(injected_submission)\n                if (context.type == 'json'):\n                    form_string = serialise_to_json(injected_submission, encode=True)\n                if (hasattr(context, 'proxy_address') is False):\n                    context.proxy_address = None\n                responses += send_http(context, form_string, timeout=context.timeout, proxy=context.proxy_address, method=method, content_type=context.content_type, scenario_id=context.scenario_id, auth=authenticate(context, context.authentication_id))\n                if hasattr(context, 'valid_case_instrumentation'):\n                    test_valid_submission(context, injected_submission)\n    return responses\n", "label": 1}
{"function": "\n\ndef test_FreeModuleElement():\n    M = QQ.old_poly_ring(x).free_module(3)\n    e = M.convert([1, x, (x ** 2)])\n    f = [QQ.old_poly_ring(x).convert(1), QQ.old_poly_ring(x).convert(x), QQ.old_poly_ring(x).convert((x ** 2))]\n    assert (list(e) == f)\n    assert (f[0] == e[0])\n    assert (f[1] == e[1])\n    assert (f[2] == e[2])\n    raises(IndexError, (lambda : e[3]))\n    g = M.convert([x, 0, 0])\n    assert ((e + g) == M.convert([(x + 1), x, (x ** 2)]))\n    assert ((f + g) == M.convert([(x + 1), x, (x ** 2)]))\n    assert ((- e) == M.convert([(- 1), (- x), (- (x ** 2))]))\n    assert ((e - g) == M.convert([(1 - x), x, (x ** 2)]))\n    assert (e != g)\n    assert ((M.convert([x, x, x]) / QQ.old_poly_ring(x).convert(x)) == [1, 1, 1])\n    R = QQ.old_poly_ring(x, order='ilex')\n    assert ((R.free_module(1).convert([x]) / R.convert(x)) == [1])\n", "label": 1}
{"function": "\n\ndef scan(node, env, libpath=()):\n    '\\n    This scanner scans program files for static-library\\n    dependencies.  It will search the LIBPATH environment variable\\n    for libraries specified in the LIBS variable, returning any\\n    files it finds as dependencies.\\n    '\n    try:\n        libs = env['LIBS']\n    except KeyError:\n        return []\n    if SCons.Util.is_String(libs):\n        libs = libs.split()\n    else:\n        libs = SCons.Util.flatten(libs)\n    try:\n        prefix = env['LIBPREFIXES']\n        if (not SCons.Util.is_List(prefix)):\n            prefix = [prefix]\n    except KeyError:\n        prefix = ['']\n    try:\n        suffix = env['LIBSUFFIXES']\n        if (not SCons.Util.is_List(suffix)):\n            suffix = [suffix]\n    except KeyError:\n        suffix = ['']\n    pairs = []\n    for suf in map(env.subst, suffix):\n        for pref in map(env.subst, prefix):\n            pairs.append((pref, suf))\n    result = []\n    if callable(libpath):\n        libpath = libpath()\n    find_file = SCons.Node.FS.find_file\n    adjustixes = SCons.Util.adjustixes\n    for lib in libs:\n        if SCons.Util.is_String(lib):\n            lib = env.subst(lib)\n            for (pref, suf) in pairs:\n                l = adjustixes(lib, pref, suf)\n                l = find_file(l, libpath, verbose=print_find_libs)\n                if l:\n                    result.append(l)\n        else:\n            result.append(lib)\n    return result\n", "label": 1}
{"function": "\n\ndef building_marker_fn(record):\n    '\\n        Function to decide which Marker to use for Building Assessments Map\\n        @ToDo: Legend\\n        @ToDo: Move to Templates\\n    '\n    if (record.asbestos == 1):\n        marker = 'hazmat'\n    else:\n        marker = 'residence'\n    priority = record.priority\n    if (priority == 1):\n        marker = ('%s_red' % marker)\n    elif (priority == 2):\n        marker = ('%s_yellow' % marker)\n    elif (priority == 3):\n        marker = ('%s_green' % marker)\n    mtable = db.gis_marker\n    try:\n        marker = db((mtable.name == marker)).select(mtable.image, mtable.height, mtable.width, cache=s3db.cache, limitby=(0, 1)).first()\n    except:\n        marker = db((mtable.name == 'residence')).select(mtable.image, mtable.height, mtable.width, cache=s3db.cache, limitby=(0, 1)).first()\n    return marker\n", "label": 0}
{"function": "\n\ndef validate_url(self, url):\n    'Validate the :class:`~urllib.parse.ParseResult` object.\\n\\n        This method will make sure the :meth:`~brownant.app.BrownAnt.parse_url`\\n        could work as expected even meet a unexpected URL string.\\n\\n        :param url: the parsed url.\\n        :type url: :class:`~urllib.parse.ParseResult`\\n        '\n    url_path = to_bytes_safe(url.path)\n    url_path = urllib.parse.quote(url_path, safe=b'/%')\n    url_query = to_bytes_safe(url.query)\n    url_query = urllib.parse.quote(url_query, safe=b'?=&')\n    url = urllib.parse.ParseResult(url.scheme, url.netloc, url_path, url.params, url_query, url.fragment)\n    has_hostname = ((url.hostname is not None) and (len(url.hostname) > 0))\n    has_http_scheme = (url.scheme in ('http', 'https'))\n    has_path = ((not len(url.path)) or url.path.startswith('/'))\n    if (not (has_hostname and has_http_scheme and has_path)):\n        raise NotSupported(('invalid url: %s' % repr(url)))\n    return url\n", "label": 0}
{"function": "\n\ndef test_python_headers():\n    cells = _exec_test_file('ex2')\n    converted = convert(cells, to='python', to_kwargs={\n        'keep_markdown': 'h1',\n    })\n    assert converted.startswith('# # Test notebook\\n\\n# some code in python')\n    assert (len(converted.splitlines()) == 30)\n    converted = convert(cells, to='python', to_kwargs={\n        'keep_markdown': 'h2,h3',\n    })\n    assert (not converted.startswith('# # Test notebook\\n\\n# some code'))\n    assert (len(converted.splitlines()) == 30)\n    converted = convert(cells, to='python', to_kwargs={\n        'keep_markdown': 'headers',\n    })\n    assert converted.startswith('# # Test notebook\\n\\n# some code in python')\n    assert (len(converted.splitlines()) == 32)\n    converted = convert(cells, to='python', to_kwargs={\n        'keep_markdown': 'all',\n    })\n    assert (len(converted.splitlines()) == 72)\n    converted = convert(cells, to='python', to_kwargs={\n        'keep_markdown': False,\n    })\n    assert (len(converted.splitlines()) == 28)\n", "label": 1}
{"function": "\n\ndef _pk_trace(self, value, prep_func, lookup_type, **kwargs):\n    v = value\n    if isinstance(v, self.rel.to):\n        field_name = getattr(self.rel, 'field_name', None)\n    else:\n        field_name = None\n    try:\n        while True:\n            if (field_name is None):\n                field_name = v._meta.pk.name\n            v = getattr(v, field_name)\n            field_name = None\n    except AttributeError:\n        pass\n    except exceptions.ObjectDoesNotExist:\n        v = None\n    field = self\n    while field.rel:\n        if hasattr(field.rel, 'field_name'):\n            field = field.rel.to._meta.get_field(field.rel.field_name)\n        else:\n            field = field.rel.to._meta.pk\n    if (lookup_type in ('range', 'in')):\n        v = [v]\n    v = getattr(field, prep_func)(lookup_type, v, **kwargs)\n    if isinstance(v, list):\n        v = v[0]\n    return v\n", "label": 1}
{"function": "\n\ndef before(self, state):\n    resource = state.request.context.get('resource')\n    items = state.request.context.get('resources')\n    if ((state.request.method != 'POST') or (not resource) or (not items)):\n        return\n    plugin = manager.NeutronManager.get_plugin_for_resource(resource)\n    deltas = collections.Counter(map((lambda x: x['tenant_id']), items))\n    reservations = []\n    neutron_context = state.request.context.get('neutron_context')\n    for (tenant_id, delta) in deltas.items():\n        try:\n            reservation = quota.QUOTAS.make_reservation(neutron_context, tenant_id, {\n                resource: delta,\n            }, plugin)\n            LOG.debug('Made reservation on behalf of %(tenant_id)s for: %(delta)s', {\n                'tenant_id': tenant_id,\n                'delta': {\n                    resource: delta,\n                },\n            })\n            reservations.append(reservation)\n        except exceptions.QuotaResourceUnknown as e:\n            LOG.debug(e)\n    state.request.context['reservations'] = reservations\n", "label": 0}
{"function": "\n\ndef circular_max_sum(self, A):\n    '\\n        dp:\\n        left: max sum for index 0..i\\n        right: max sum for index i..(n-1)\\n\\n        :param A:\\n        :return:\\n        '\n    n = len(A)\n    left = [None for _ in A]\n    right = [None for _ in A]\n    (cur, max_sum, idx) = (0, A[0], 0)\n    for i in xrange(n):\n        cur += A[i]\n        if (cur > max_sum):\n            idx = i\n            max_sum = cur\n        left[i] = (max_sum, idx)\n    (cur, max_sum, idx) = (0, A[(n - 1)], (n - 1))\n    for i in xrange((n - 1), (- 1), (- 1)):\n        cur += A[i]\n        if (cur > max_sum):\n            idx = i\n            max_sum = cur\n        right[i] = (max_sum, idx)\n    ret = Sum(A[0], 0, 0)\n    for i in xrange(1, n):\n        r = right[i]\n        l = left[(i - 1)]\n        if (ret.sum < (r[0] + l[0])):\n            ret = Sum((r[0] + l[0]), r[1], l[1])\n    return ret\n", "label": 1}
{"function": "\n\ndef coerce(self, column):\n    '\\n        Returns coerce callable for given column\\n\\n        :param column: SQLAlchemy Column object\\n        '\n    if ('coerce' in column.info):\n        return column.info['coerce']\n    if isinstance(column.type, types.ChoiceType):\n        return choice_type_coerce_factory(column.type)\n    try:\n        python_type = column.type.python_type\n    except NotImplementedError:\n        return null_or_unicode\n    if (column.nullable and issubclass(python_type, six.string_types)):\n        return null_or_unicode\n    return python_type\n", "label": 0}
{"function": "\n\ndef main():\n    MAX_FPS = 20\n    time_per_frame = (lambda : (1.0 / MAX_FPS))\n    with FullscreenWindow() as window:\n        with Input() as input_generator:\n            snake = Snake(window.height, window.width)\n            while True:\n                c = None\n                t0 = time.time()\n                while True:\n                    t = time.time()\n                    temp_c = input_generator.send(max(0, (t - (t0 + time_per_frame()))))\n                    if (temp_c == '<ESC>'):\n                        return\n                    elif (temp_c == '+'):\n                        MAX_FPS += 1\n                    elif (temp_c == '-'):\n                        MAX_FPS = max(1, (MAX_FPS - 1))\n                    elif (temp_c is not None):\n                        c = temp_c\n                    if (time_per_frame() < (t - t0)):\n                        break\n                if snake.tick(c):\n                    return\n                window.render_to_terminal(snake.render())\n", "label": 1}
{"function": "\n\ndef test_service_completed():\n    package_install('cassandra', True, ['--package-version=0.2.0-1'])\n    time.sleep(5)\n    services = get_services(3)\n    cassandra_id = None\n    for service in services:\n        if (service['name'] == 'cassandra.dcos'):\n            cassandra_id = service['id']\n            break\n    assert (cassandra_id is not None)\n    assert_command(['dcos', 'marathon', 'group', 'remove', '/cassandra'])\n    service_shutdown(cassandra_id)\n    delete_zk_node('cassandra-mesos')\n    services = get_services(2)\n    assert (not any(((service['id'] == cassandra_id) for service in services)))\n    services = get_services(args=['--completed'])\n    assert (len(services) >= 3)\n    assert any(((service['id'] == cassandra_id) for service in services))\n    package_uninstall('cassandra')\n", "label": 0}
{"function": "\n\ndef mosaic_some(self, paths, image_loader=None):\n    if (image_loader is None):\n        image_loader = self.fv.load_image\n    for url in paths:\n        if self.ev_intr.isSet():\n            break\n        mosaic_hdus = self.settings.get('mosaic_hdus', False)\n        if mosaic_hdus:\n            self.logger.debug('mosaicing hdus')\n            with pyfits.open(url, 'readonly') as in_f:\n                i = 0\n                for hdu in in_f:\n                    i += 1\n                    if (hdu.data is None):\n                        continue\n                    self.logger.debug(('ingesting hdu #%d' % i))\n                    image = AstroImage.AstroImage(logger=self.logger)\n                    image.load_hdu(hdu)\n                    image.set(name=('hdu%d' % i))\n                    image = self.preprocess(image)\n                    self.ingest_one(image)\n        else:\n            image = image_loader(url)\n            image = self.preprocess(image)\n            self.ingest_one(image)\n", "label": 0}
{"function": "\n\ndef write(self, x86_operand, value):\n    if isinstance(x86_operand, barf.arch.x86.x86base.X86RegisterOperand):\n        reil_operand = ReilRegisterOperand(x86_operand.name, x86_operand.size)\n        if ((self._arch_info.architecture_mode == ARCH_X86_MODE_64) and (x86_operand.size == 32)):\n            if (x86_operand.name in self._regs_mapper):\n                (base_reg, offset) = self._regs_mapper[x86_operand.name]\n                reil_operand_base = ReilRegisterOperand(base_reg, self._regs_size[base_reg])\n                reil_immediate = ReilImmediateOperand(0, self._regs_size[base_reg])\n                self.add(self._builder.gen_str(reil_immediate, reil_operand_base))\n        self.add(self._builder.gen_str(value, reil_operand))\n    elif isinstance(x86_operand, barf.arch.x86.x86base.X86MemoryOperand):\n        addr = self._compute_memory_address(x86_operand)\n        if (value.size != x86_operand.size):\n            tmp = self.temporal(x86_operand.size)\n            self.add(self._builder.gen_str(value, tmp))\n            self.add(self._builder.gen_stm(tmp, addr))\n        else:\n            self.add(self._builder.gen_stm(value, addr))\n    else:\n        raise Exception()\n", "label": 0}
{"function": "\n\ndef run(self, parent, blocks):\n    items = self.get_items(blocks.pop(0))\n    sibling = self.lastChild(parent)\n    if (sibling and (sibling.tag in ['ol', 'ul'])):\n        lst = sibling\n        if lst[(- 1)].text:\n            p = util.etree.Element('p')\n            p.text = lst[(- 1)].text\n            lst[(- 1)].text = ''\n            lst[(- 1)].insert(0, p)\n        lch = self.lastChild(lst[(- 1)])\n        if ((lch is not None) and lch.tail):\n            p = util.etree.SubElement(lst[(- 1)], 'p')\n            p.text = lch.tail.lstrip()\n            lch.tail = ''\n        li = util.etree.SubElement(lst, 'li')\n        self.parser.state.set('looselist')\n        firstitem = items.pop(0)\n        self.parser.parseBlocks(li, [firstitem])\n        self.parser.state.reset()\n    elif (parent.tag in ['ol', 'ul']):\n        lst = parent\n    else:\n        lst = util.etree.SubElement(parent, self.TAG)\n        if ((not self.parser.markdown.lazy_ol) and (self.STARTSWITH != '1')):\n            lst.attrib['start'] = self.STARTSWITH\n    self.parser.state.set('list')\n    for item in items:\n        if item.startswith((' ' * self.tab_length)):\n            self.parser.parseBlocks(lst[(- 1)], [item])\n        else:\n            li = util.etree.SubElement(lst, 'li')\n            self.parser.parseBlocks(li, [item])\n    self.parser.state.reset()\n", "label": 1}
{"function": "\n\ndef get_stories(self, story_type='', limit=30):\n    \"\\n        Yields a list of stories from the passed page\\n        of HN.\\n        'story_type' can be:\\n        \\t'' = top stories (homepage) (default)\\n        \\t'news2' = page 2 of top stories\\n        \\t'newest' = most recent stories\\n        \\t'best' = best stories\\n\\n        'limit' is the number of stories required from the given page.\\n        Defaults to 30. Cannot be more than 30.\\n        \"\n    if ((limit is None) or (limit < 1) or (limit > 30)):\n        limit = 30\n    stories_found = 0\n    while (stories_found < limit):\n        soup = get_soup(page=story_type)\n        all_rows = self._get_zipped_rows(soup)\n        stories = self._build_story(all_rows)\n        for story in stories:\n            (yield story)\n            stories_found += 1\n            if (stories_found == limit):\n                return\n", "label": 0}
{"function": "\n\ndef __call__(self, environ, start_response):\n    cleaned_path = environ.get('PATH_INFO', '').strip('/')\n    for sep in (os.sep, os.altsep):\n        if (sep and (sep != '/')):\n            cleaned_path = cleaned_path.replace(sep, '/')\n    path = '/'.join(([''] + [x for x in cleaned_path.split('/') if (x and (x != '..'))]))\n    (real_filename, file_loader) = self.loader(path[1:])\n    if (file_loader is None):\n        return self.app(environ, start_response)\n    guessed_type = mimetypes.guess_type(real_filename)\n    mime_type = (guessed_type[0] or self.fallback_mimetype)\n    (f, mtime, file_size) = file_loader()\n    headers = [('Date', http_date())]\n    headers.append(('Cache-Control', 'public'))\n    headers.extend((('Content-Type', mime_type), ('Content-Length', str(file_size)), ('Last-Modified', http_date(mtime))))\n    start_response('200 OK', headers)\n    return wrap_file(environ, f)\n", "label": 1}
{"function": "\n\ndef test_ReqPackage_render_as_branch():\n    mks1 = find_req('markupsafe', 'jinja2')\n    jinja = find_dist('jinja2')\n    assert (mks1.project_name == 'markupsafe')\n    assert (mks1.installed_version == '0.18')\n    assert (mks1.version_spec is None)\n    assert (mks1.render_as_branch(jinja, False) == 'markupsafe [installed: 0.18]')\n    assert (mks1.render_as_branch(jinja, True) == 'MarkupSafe==0.18')\n    mks2 = find_req('markupsafe', 'mako')\n    mako = find_dist('mako')\n    assert (mks2.project_name == 'MarkupSafe')\n    assert (mks2.installed_version == '0.18')\n    assert (mks2.version_spec == '>=0.9.2')\n    assert (mks2.render_as_branch(mako, False) == 'MarkupSafe [required: >=0.9.2, installed: 0.18]')\n    assert (mks2.render_as_branch(mako, True) == 'MarkupSafe==0.18')\n", "label": 1}
{"function": "\n\ndef _check_close(self):\n    conn = self.headers.get('connection')\n    if (self.version == 11):\n        conn = self.headers.get('connection')\n        if (conn and ('close' in conn.lower())):\n            return True\n        return False\n    if self.headers.get('keep-alive'):\n        return False\n    if (conn and ('keep-alive' in conn.lower())):\n        return False\n    pconn = self.headers.get('proxy-connection')\n    if (pconn and ('keep-alive' in pconn.lower())):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef _find_package_by_freq(self, linkset, package_freq):\n    package_names = [p[0] for p in package_freq]\n    size = len(package_freq)\n    best_index = size\n    best_element = None\n    for link in linkset.links.all():\n        package = find_package(link.code_element)\n        if (package is not None):\n            try:\n                index = package_names.index(package)\n                if (index < best_index):\n                    if ((index == (size - 1)) or (best_index == size)):\n                        best_index = index\n                        best_element = link.code_element\n                    elif (package_freq[index][1] > package_freq[best_index][1]):\n                        best_index = index\n                        best_element = link.code_element\n                    else:\n                        depth_best = len(package_names[best_index].split('.'))\n                        depth_index = len(package_names[index].split('.'))\n                        if (depth_best < depth_index):\n                            continue\n                        elif (depth_index < depth_best):\n                            best_index = index\n                            best_element = link.code_element\n                        else:\n                            best_index = size\n                            best_element = None\n            except Exception:\n                pass\n    return best_element\n", "label": 1}
{"function": "\n\ndef test_walk(self):\n    sc = ServiceCalendar()\n    sc.add_period(0, ((1 * 3600) * 24), ['WKDY'])\n    tz = Timezone()\n    tz.add_period(TimezonePeriod(0, ((1 * 3600) * 24), 0))\n    hb = HeadwayBoard('WKDY', sc, tz, 0, 'tr1', 200, 1000, 50)\n    s0 = State(1, 0)\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.time == 250)\n    assert (s1.weight == 251)\n    s0 = State(1, 200)\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.time == 250)\n    assert (s1.weight == 51)\n    s0 = State(1, 500)\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.time == 550)\n    assert (s1.weight == 51)\n    s0 = State(1, 1000)\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.time == 1050)\n    assert (s1.weight == 51)\n    s0 = State(1, 1001)\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1 == None)\n", "label": 1}
{"function": "\n\ndef _deepcopy(self, x, memo=None):\n    \"Deepcopy helper for the data dictionary or list.\\n\\n        Regular expressions cannot be deep copied but as they are immutable we\\n        don't have to copy them when cloning.\\n        \"\n    if (not hasattr(x, 'items')):\n        (y, is_list, iterator) = ([], True, enumerate(x))\n    else:\n        (y, is_list, iterator) = ({\n            \n        }, False, x.iteritems())\n    if (memo is None):\n        memo = {\n            \n        }\n    val_id = id(x)\n    if (val_id in memo):\n        return memo.get(val_id)\n    memo[val_id] = y\n    for (key, value) in iterator:\n        if (isinstance(value, (dict, list)) and (not isinstance(value, SON))):\n            value = self._deepcopy(value, memo)\n        elif (not isinstance(value, RE_TYPE)):\n            value = copy.deepcopy(value, memo)\n        if is_list:\n            y.append(value)\n        else:\n            if (not isinstance(key, RE_TYPE)):\n                key = copy.deepcopy(key, memo)\n            y[key] = value\n    return y\n", "label": 1}
{"function": "\n\ndef main():\n    usage = 'git restack [BRANCH]'\n    parser = argparse.ArgumentParser(usage=usage, description=COPYRIGHT)\n    parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', help=\"Output more information about what's going on\")\n    parser.add_argument('--license', dest='license', action='store_true', help='Print the license and exit')\n    parser.add_argument('--version', action='version', version=('%s version %s' % (os.path.split(sys.argv[0])[(- 1)], get_version())))\n    parser.add_argument('branch', nargs='?')\n    parser.set_defaults(verbose=False)\n    try:\n        (top_dir, git_dir) = git_directories()\n    except GitDirectoriesException as no_git_dir:\n        pass\n    else:\n        no_git_dir = False\n        config = Config(os.path.join(top_dir, '.gitreview'))\n    options = parser.parse_args()\n    if no_git_dir:\n        raise no_git_dir\n    if options.license:\n        print(COPYRIGHT)\n        sys.exit(0)\n    global VERBOSE\n    VERBOSE = options.verbose\n    if (options.branch is None):\n        branch = config['branch']\n    else:\n        branch = options.branch\n    if (branch is None):\n        branch = 'master'\n    status = 0\n    cmd = ('git merge-base HEAD origin/%s' % branch)\n    base = run_command_exc(GitMergeBaseException, cmd)\n    run_command_foreground(('git rebase -i %s' % base), stdin=sys.stdin)\n    sys.exit(status)\n", "label": 0}
{"function": "\n\ndef _read_status(self):\n    line = str(self.fp.readline((_MAXLINE + 1)), 'iso-8859-1')\n    if (len(line) > _MAXLINE):\n        raise LineTooLong('status line')\n    if (self.debuglevel > 0):\n        print('reply:', repr(line))\n    if (not line):\n        raise BadStatusLine(line)\n    try:\n        (version, status, reason) = line.split(None, 2)\n    except ValueError:\n        try:\n            (version, status) = line.split(None, 1)\n            reason = ''\n        except ValueError:\n            version = ''\n    if (not version.startswith('HTTP/')):\n        self._close_conn()\n        raise BadStatusLine(line)\n    try:\n        status = int(status)\n        if ((status < 100) or (status > 999)):\n            raise BadStatusLine(line)\n    except ValueError:\n        raise BadStatusLine(line)\n    return (version, status, reason)\n", "label": 1}
{"function": "\n\ndef submit_metric(self, name, value, mtype, tags=None, hostname=None, device_name=None, timestamp=None, sample_rate=1):\n    hostname = (hostname if (hostname is not None) else self.hostname)\n    if (tags is None):\n        context = (name, tuple(), hostname, device_name)\n    else:\n        context = (name, tuple(sorted(set(tags))), hostname, device_name)\n    cur_time = time()\n    if ((timestamp is not None) and ((cur_time - int(timestamp)) > self.recent_point_threshold)):\n        log.debug(('Discarding %s - ts = %s , current ts = %s ' % (name, timestamp, cur_time)))\n        self.num_discarded_old_points += 1\n    else:\n        timestamp = (timestamp or cur_time)\n        bucket_start_timestamp = self.calculate_bucket_start(timestamp)\n        if (bucket_start_timestamp == self.current_bucket):\n            metric_by_context = self.current_mbc\n        else:\n            if (bucket_start_timestamp not in self.metric_by_bucket):\n                self.metric_by_bucket[bucket_start_timestamp] = {\n                    \n                }\n            metric_by_context = self.metric_by_bucket[bucket_start_timestamp]\n            self.current_bucket = bucket_start_timestamp\n            self.current_mbc = metric_by_context\n        if (context not in metric_by_context):\n            metric_class = self.metric_type_to_class[mtype]\n            metric_by_context[context] = metric_class(self.formatter, name, tags, hostname, device_name, self.metric_config.get(metric_class))\n        metric_by_context[context].sample(value, sample_rate, timestamp)\n", "label": 1}
{"function": "\n\ndef checkAuth():\n    'Check if a valid Session cookie is found and the auth token within is valid\\n    '\n    authed = False\n    indieauth_id = None\n    if (('indieauth_id' in session) and ('indieauth_token' in session)):\n        indieauth_id = session['indieauth_id']\n        indieauth_token = session['indieauth_token']\n        app.logger.info('session cookie found')\n        if (db is not None):\n            key = db.get(('token-%s' % indieauth_token))\n            if key:\n                data = db.hgetall(key)\n                if (data and (data['token'] == indieauth_token)):\n                    authed = True\n    return (authed, indieauth_id)\n", "label": 0}
{"function": "\n\ndef _register_model(admin, model):\n    if (not hasattr(admin, 'revision_manager')):\n        admin.revision_manager = default_revision_manager\n    if (not hasattr(admin, 'reversion_format')):\n        admin.reversion_format = 'json'\n    if (not admin.revision_manager.is_registered(model)):\n        inline_fields = []\n        for inline in getattr(admin, 'inlines', []):\n            inline_model = inline.model\n            if getattr(inline, 'generic_inline', False):\n                ct_field = getattr(inline, 'ct_field', 'content_type')\n                ct_fk_field = getattr(inline, 'ct_fk_field', 'object_id')\n                for field in model._meta.many_to_many:\n                    if (isinstance(field, GenericRelation) and (field.rel.to == inline_model) and (field.object_id_field_name == ct_fk_field) and (field.content_type_field_name == ct_field)):\n                        inline_fields.append(field.name)\n                _autoregister(admin, inline_model)\n            else:\n                fk_name = getattr(inline, 'fk_name', None)\n                if (not fk_name):\n                    for field in inline_model._meta.fields:\n                        if (isinstance(field, (models.ForeignKey, models.OneToOneField)) and issubclass(model, field.rel.to)):\n                            fk_name = field.name\n                _autoregister(admin, inline_model, follow=[fk_name])\n                if (not inline_model._meta.get_field(fk_name).rel.is_hidden()):\n                    accessor = inline_model._meta.get_field(fk_name).remote_field.get_accessor_name()\n                    inline_fields.append(accessor)\n        _autoregister(admin, model, inline_fields)\n", "label": 1}
{"function": "\n\ndef _updateFromPlug(self):\n    self.__menuButton.setEnabled(self._editable())\n    text = ''\n    mode = 'standard'\n    if (self.getPlug() is not None):\n        mode = Gaffer.Metadata.plugValue(self.getPlug(), 'formatPlugValueWidget:mode')\n        with self.getContext():\n            fmt = self.getPlug().getValue()\n        text = self.__formatLabel(fmt)\n        if (fmt == GafferImage.Format()):\n            mode = 'standard'\n        elif (not GafferImage.Format.name(fmt)):\n            mode = 'custom'\n    self.__menuButton.setText((text if (mode != 'custom') else 'Custom'))\n    nonZeroOrigin = (fmt.getDisplayWindow().min != IECore.V2i(0))\n    for widget in (self.__minLabel, self.__minWidget):\n        widget.setVisible(((mode == 'custom') and nonZeroOrigin))\n    for widget in (self.__maxLabel, self.__maxWidget, self.__pixelAspectLabel, self.__pixelAspectWidget):\n        widget.setVisible((mode == 'custom'))\n    self.__maxLabel.setText(('Max' if nonZeroOrigin else 'Size'))\n", "label": 1}
{"function": "\n\ndef report_failure(self, item):\n    item_dict = model_to_dict(item)\n    url = settings.MY_EQUIPMENT_REPORT_FAILURE_URL\n    if url:\n        placeholders = [k[1] for k in Formatter().parse(url) if (k[1] is not None)]\n        item_dict.update({k: getattr_dunder(item, k) for k in placeholders})\n        if (self.request and ('username' not in item_dict)):\n            item_dict['username'] = self.request.user.username\n\n        def escape_param(p):\n            '\\n                Escape URL param and replace quotation by unicode inches sign\\n                '\n            return quote(str(p).replace('\"', '\u2033'))\n        return '<a href=\"{}\" target=\"_blank\">{}</a><br />'.format(url.format(**{k: escape_param(v) for (k, v) in item_dict.items()}), _('Report failure'))\n    return ''\n", "label": 0}
{"function": "\n\ndef getUpperLeftX(self, width, x, xPrev, xNext, xMin, xMax, xMid, xMouse):\n    if (AnnotationLocation.AT_THE_MOUSE == self.location):\n        if (GChartConsts.NAI == xMouse):\n            result = Double.NaN\n        else:\n            result = xMouse\n    elif (AnnotationLocation.AT_THE_MOUSE_SNAP_TO_X == self.location):\n        if (GChartConsts.NAI == xMouse):\n            result = Double.NaN\n        else:\n            result = x\n    elif (AnnotationLocation.AT_THE_MOUSE_SNAP_TO_Y == self.location):\n        if (GChartConsts.NAI == xMouse):\n            result = Double.NaN\n        else:\n            result = xMouse\n    elif ((AnnotationLocation.NORTHWEST == self.location) or (AnnotationLocation.WEST == self.location) or (AnnotationLocation.SOUTHWEST == self.location)):\n        result = xMin\n    elif ((AnnotationLocation.NORTHEAST == self.location) or (AnnotationLocation.EAST == self.location) or (AnnotationLocation.SOUTHEAST == self.location)):\n        result = xMax\n    else:\n        result = ((xMin + xMax) / 2)\n    return result\n", "label": 1}
{"function": "\n\ndef get_language_from_request(request, current_page=None):\n    '\\n    Return the most obvious language according the request\\n    '\n    language = None\n    if hasattr(request, 'POST'):\n        language = request.POST.get('language', None)\n    if (hasattr(request, 'GET') and (not language)):\n        language = request.GET.get('language', None)\n    site_id = (current_page.site_id if current_page else None)\n    if language:\n        language = get_language_code(language)\n        if (not (language in get_language_list(site_id))):\n            language = None\n    if (language is None):\n        language = get_language_code(getattr(request, 'LANGUAGE_CODE', None))\n    if language:\n        if (not (language in get_language_list(site_id))):\n            language = None\n    if ((language is None) and current_page):\n        languages = current_page.get_languages()\n        if (len(languages) > 0):\n            language = languages[0]\n    if (language is None):\n        language = get_default_language(site_id=site_id)\n    return language\n", "label": 1}
{"function": "\n\ndef make_factory(self, cls, count):\n    ' Get the generators from the Scaffolding class within the model.\\n        '\n    field_names = cls._meta.get_all_field_names()\n    fields = {\n        \n    }\n    text = []\n    finalizer = None\n    scaffold = scaffolding.scaffold_for_model(cls)\n    for field_name in field_names:\n        generator = getattr(scaffold, field_name, None)\n        if generator:\n            if hasattr(generator, 'set_up'):\n                generator.set_up(cls, count)\n            fields[field_name] = generator\n            text.append(('%s: %s; ' % (field_name, fields[field_name])))\n    try:\n        self.stdout.write(('Generator for %s: %s\\n' % (cls, ''.join(text))))\n    except models.ObjectDoesNotExist:\n        self.stdout.write(('Generator for %s\\n' % ''.join(text)))\n    if (hasattr(scaffold, 'finalize') and hasattr(scaffold.finalize, '__call__')):\n        finalizer = scaffold.finalize\n    return (fields, finalizer)\n", "label": 0}
{"function": "\n\ndef send(self, request, **kwargs):\n    if (self._is_cache_disabled or (request.method not in self._cache_allowable_methods)):\n        response = super(CachedSession, self).send(request, **kwargs)\n        response.from_cache = False\n        return response\n    cache_key = self.cache.create_key(request)\n\n    def send_request_and_cache_response():\n        response = super(CachedSession, self).send(request, **kwargs)\n        if (response.status_code in self._cache_allowable_codes):\n            self.cache.save_response(cache_key, response)\n        response.from_cache = False\n        return response\n    (response, timestamp) = self.cache.get_response_and_time(cache_key)\n    if (response is None):\n        return send_request_and_cache_response()\n    if (self._cache_expire_after is not None):\n        is_expired = ((datetime.utcnow() - timestamp) > self._cache_expire_after)\n        if is_expired:\n            if (not self._return_old_data_on_error):\n                self.cache.delete(cache_key)\n                return send_request_and_cache_response()\n            try:\n                new_response = send_request_and_cache_response()\n            except Exception:\n                return response\n            else:\n                if (new_response.status_code not in self._cache_allowable_codes):\n                    return response\n                return new_response\n    response.from_cache = True\n    response = dispatch_hook('response', request.hooks, response, **kwargs)\n    return response\n", "label": 1}
{"function": "\n\ndef test_diff3():\n    (a, b, c) = map(Symbol, 'abc')\n    p = Rational(5)\n    e = ((a * b) + sin((b ** p)))\n    assert (e == ((a * b) + sin((b ** 5))))\n    assert (e.diff(a) == b)\n    assert (e.diff(b) == (a + ((5 * (b ** 4)) * cos((b ** 5)))))\n    e = tan(c)\n    assert (e == tan(c))\n    assert (e.diff(c) in [(cos(c) ** (- 2)), (1 + ((sin(c) ** 2) / (cos(c) ** 2))), (1 + (tan(c) ** 2))])\n    e = ((c * log(c)) - c)\n    assert (e == ((- c) + (c * log(c))))\n    assert (e.diff(c) == log(c))\n    e = log(sin(c))\n    assert (e == log(sin(c)))\n    assert (e.diff(c) in [((sin(c) ** (- 1)) * cos(c)), cot(c)])\n    e = ((Rational(2) ** a) / log(Rational(2)))\n    assert (e == ((2 ** a) * (log(Rational(2)) ** (- 1))))\n    assert (e.diff(a) == (2 ** a))\n", "label": 1}
{"function": "\n\ndef step_3(w):\n    ' Step 3 replaces -ic, -ful, -ness etc. suffixes.\\n        This only happens if there is at least one vowel-consonant pair before the suffix.\\n    '\n    for (suffix, rules) in suffixes3:\n        if w.endswith(suffix):\n            for (A, B) in rules:\n                if w.endswith(A):\n                    return ((R1(w).endswith(A) and (w[:(- len(A))] + B)) or w)\n    return w\n", "label": 0}
{"function": "\n\ndef run(self, refresh_only=False):\n    repo = self.get_repo(silent=(True if refresh_only else False))\n    if (not repo):\n        return\n    title = (GIT_STATUS_VIEW_TITLE_PREFIX + os.path.basename(repo))\n    view = find_view_by_settings(self.window, git_view='status', git_repo=repo)\n    if ((not view) and (not refresh_only)):\n        view = self.window.new_file()\n        view.set_name(title)\n        view.set_syntax_file(GIT_STATUS_VIEW_SYNTAX)\n        view.set_scratch(True)\n        view.set_read_only(True)\n        view.settings().set('git_view', 'status')\n        view.settings().set('git_repo', repo)\n        view.settings().set('__vi_external_disable', (get_setting('git_status_disable_vintageous') is True))\n        for (key, val) in list(GIT_STATUS_VIEW_SETTINGS.items()):\n            view.settings().set(key, val)\n    if (view is not None):\n        self.window.focus_view(view)\n        view.run_command('git_status_refresh')\n", "label": 0}
{"function": "\n\n@ensure_tag(['p'])\ndef get_single_list_nodes_data(li, meta_data):\n    '\\n    Find consecutive li tags that have content that have the same list id.\\n    '\n    (yield li)\n    w_namespace = get_namespace(li, 'w')\n    current_numId = get_numId(li, w_namespace)\n    starting_ilvl = get_ilvl(li, w_namespace)\n    el = li\n    while True:\n        el = el.getnext()\n        if (el is None):\n            break\n        if (not has_text(el)):\n            continue\n        if _is_top_level_upper_roman(el, meta_data):\n            break\n        if (is_li(el, meta_data) and (starting_ilvl > get_ilvl(el, w_namespace))):\n            break\n        new_numId = get_numId(el, w_namespace)\n        if ((new_numId is None) or (new_numId == (- 1))):\n            (yield el)\n            continue\n        if (current_numId != new_numId):\n            break\n        if is_last_li(el, meta_data, current_numId):\n            (yield el)\n            break\n        (yield el)\n", "label": 1}
{"function": "\n\ndef func(self):\n    '\\n        Creates the object.\\n        '\n    caller = self.caller\n    if (not self.args):\n        string = 'Usage: @create[/drop] <newname>[;alias;alias...] [:typeclass_path]'\n        caller.msg(string)\n        return\n    for objdef in self.lhs_objs:\n        string = ''\n        name = objdef['name']\n        aliases = objdef['aliases']\n        typeclass = objdef['option']\n        lockstring = ('control:id(%s);delete:id(%s) or perm(Wizards)' % (caller.id, caller.id))\n        obj = create.create_object(typeclass, name, caller, home=caller, aliases=aliases, locks=lockstring, report_to=caller)\n        if (not obj):\n            continue\n        if aliases:\n            string = 'You create a new %s: %s (aliases: %s).'\n            string = (string % (obj.typename, obj.name, ', '.join(aliases)))\n        else:\n            string = 'You create a new %s: %s.'\n            string = (string % (obj.typename, obj.name))\n        if (not obj.db.desc):\n            obj.db.desc = 'You see nothing special.'\n        if ('drop' in self.switches):\n            if caller.location:\n                obj.home = caller.location\n                obj.move_to(caller.location, quiet=True)\n    if string:\n        caller.msg(string)\n", "label": 1}
{"function": "\n\ndef iter_segments(self):\n    while (not self.closed):\n        for sequence in filter(self.valid_sequence, self.playlist_sequences):\n            self.logger.debug('Adding segment {0} to queue', sequence.num)\n            (yield sequence)\n            stream_end = (self.playlist_end and (sequence.num >= self.playlist_end))\n            if (self.closed or stream_end):\n                return\n            self.playlist_sequence = (sequence.num + 1)\n        if self.wait(self.playlist_reload_time):\n            try:\n                self.reload_playlist()\n            except StreamError as err:\n                self.logger.warning('Failed to reload playlist: {0}', err)\n", "label": 0}
{"function": "\n\ndef _match_pattern(self):\n    buf = []\n    while ((self.c != EOF) and (self.c != self.delimiter)):\n        if (self.c == '\\\\'):\n            buf.append(self.c)\n            self.consume()\n            if (self.c == self.delimiter):\n                buf[(- 1)] = self.delimiter\n                self.consume()\n            if (self.c in '\\\\'):\n                buf.append(self.c)\n                self.consume()\n            if (self.c == EOF):\n                break\n        else:\n            buf.append(self.c)\n            self.consume()\n    return ''.join(buf)\n", "label": 0}
{"function": "\n\ndef star_help(cluster=None, logdir=None, cmdline=None, *args):\n    'Help on *commands'\n    if args:\n        for cmd in args:\n            if (not (cmd[0] == '*')):\n                cmd = ('*' + cmd)\n            if (cmd in commands):\n                print('Help for', cmd)\n                print(commands[cmd].help_text)\n            else:\n                print('Unknown command:', cmd)\n        return\n    for cmd in sorted(commands.keys()):\n        func = commands[cmd]\n        if (not isinstance(func, StarCommand)):\n            print(('%s (old-style) - %s' % (cmd, func.__doc__)))\n            continue\n        if func.version:\n            print(('%s (%s) - %s' % (cmd, func.version, func.synopsis)))\n        else:\n            print(('%s - %s' % (cmd, func.synopsis)))\n", "label": 0}
{"function": "\n\ndef list_all(request):\n    suggestions = Suggestion.objects.order_by('-rating_score')\n    if ('nqs' in request.GET):\n        qs = request.GET['nqs'].replace('+', ' ')\n        suggestions = suggestions.filter(text__icontains=qs)\n    if ('filter' in request.GET):\n        try:\n            f = request.GET['filter']\n            if (f == 'mine'):\n                user = User.objects.get(username=request.user)\n                for s in suggestions:\n                    voted = s.rating.get_rating_for_user(user=user, ip_address=request.META['REMOTE_ADDR'])\n                    if (not voted):\n                        suggestions = suggestions.exclude(pk=s.id)\n            if (f == 'done'):\n                suggestions = suggestions.exclude(completed=False)\n        except:\n            pass\n    form = SuggestionForm()\n    return render_to_response('suggestions/list.html', {\n        'suggestions': suggestions,\n        'form': form,\n    }, context_instance=RequestContext(request))\n", "label": 0}
{"function": "\n\n@register.simple_tag(takes_context=True)\ndef render(context, model_instance, attribute_name):\n    '\\n    This filter applies all renderers specified in admin2.py to the field.\\n    '\n    value = utils.get_attr(model_instance, attribute_name)\n    admin = context['view'].model_admin\n    renderer = admin.field_renderers.get(attribute_name, False)\n    if (renderer is None):\n        return value\n    if (not renderer):\n        if isinstance(value, bool):\n            renderer = renderers.boolean_renderer\n        elif isinstance(value, (date, time, datetime)):\n            renderer = renderers.datetime_renderer\n        elif isinstance(value, Number):\n            renderer = renderers.number_renderer\n        else:\n            return value\n    try:\n        field = model_instance._meta.get_field_by_name(attribute_name)[0]\n    except FieldDoesNotExist:\n        field = None\n    return renderer(value, field)\n", "label": 0}
{"function": "\n\ndef replaceImportedModule(old, new):\n    for (key, value) in imported_by_name.items():\n        if (value == old):\n            imported_by_name[key] = new\n            break\n    else:\n        assert False\n    for (key, value) in imported_modules.items():\n        if (value == old):\n            imported_modules[key] = new\n            break\n    else:\n        assert False\n", "label": 1}
{"function": "\n\ndef rekey(dikt):\n    'Rekey a dict that has been forced to use str keys where there should be\\n    ints by json.'\n    for k in dikt.keys():\n        if isinstance(k, str):\n            ik = fk = None\n            try:\n                ik = int(k)\n            except ValueError:\n                try:\n                    fk = float(k)\n                except ValueError:\n                    continue\n            if (ik is not None):\n                nk = ik\n            else:\n                nk = fk\n            if (nk in dikt):\n                raise KeyError(('already have key %r' % nk))\n            dikt[nk] = dikt.pop(k)\n    return dikt\n", "label": 0}
{"function": "\n\ndef _send_request(self, method, url, body, headers):\n    header_names = dict.fromkeys([k.lower() for k in headers])\n    skips = {\n        \n    }\n    if ('host' in header_names):\n        skips['skip_host'] = 1\n    if ('accept-encoding' in header_names):\n        skips['skip_accept_encoding'] = 1\n    self.putrequest(method, url, **skips)\n    if ((body is not None) and ('content-length' not in header_names)):\n        self._set_content_length(body)\n    for (hdr, value) in headers.items():\n        self.putheader(hdr, value)\n    if isinstance(body, str):\n        body = body.encode('iso-8859-1')\n    self.endheaders(body)\n", "label": 0}
{"function": "\n\ndef ensure_time(time):\n    '\\n    Attempts to convert an object to a time (of day).\\n\\n    @rtype\\n      `datetime.time`\\n    '\n    if isinstance(time, datetime.datetime):\n        return date.time()\n    if isinstance(time, datetime.time):\n        return time\n    if (time == 'local-now'):\n        return datetime.datetime.now().time()\n    if (time == 'utc-now'):\n        return datetime.datetime.utcnow().time()\n\n    def from_parts(h, m, s=0):\n        try:\n            return datetime.time(h, m, s)\n        except ValueError:\n            raise TypeError('not a time: {!r}'.format(time))\n    if isinstance(time, str):\n        match = re.match('(\\\\d?\\\\d):(\\\\d\\\\d):(\\\\d\\\\d)', time)\n        if (match is None):\n            match = re.match('(\\\\d?\\\\d):(\\\\d\\\\d)', time)\n        if (match is not None):\n            return from_parts(*[int(g) for g in match.groups()])\n    raise TypeError('not a time: {!r}'.format(time))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef create_component(collection):\n    'Present user with a dialog to define and add new components.\\n\\n        Parameters\\n        ----------\\n        collection : A `DataCollection` to edit\\n        '\n    widget = CustomComponentWidget(collection)\n    while True:\n        widget.ui.show()\n        if (widget.ui.exec_() == QtGui.QDialog.Accepted):\n            if (len(str(widget.ui.expression.toPlainText())) == 0):\n                QtGui.QMessageBox.critical(widget.ui, 'Error', 'No expression set', buttons=QtGui.QMessageBox.Ok)\n            elif (widget._number_targets == 0):\n                QtGui.QMessageBox.critical(widget.ui, 'Error', 'Please specify the target dataset(s)', buttons=QtGui.QMessageBox.Ok)\n            elif (len(widget.ui.new_label.text()) == 0):\n                QtGui.QMessageBox.critical(widget.ui, 'Error', 'Please specify the new component name', buttons=QtGui.QMessageBox.Ok)\n            else:\n                link = widget._create_link()\n                if link:\n                    widget._add_link_to_targets(link)\n                break\n        else:\n            break\n", "label": 0}
{"function": "\n\ndef world_gen(name, width, height, seed, temps=[0.874, 0.765, 0.594, 0.439, 0.366, 0.124], humids=[0.941, 0.778, 0.507, 0.236, 0.073, 0.014, 0.002], num_plates=10, ocean_level=1.0, step=Step.full(), gamma_curve=1.25, curve_offset=0.2, fade_borders=True, verbose=get_verbose()):\n    if verbose:\n        start_time = time.time()\n    world = _plates_simulation(name, width, height, seed, temps, humids, gamma_curve, curve_offset, num_plates, ocean_level, step, verbose)\n    center_land(world)\n    if verbose:\n        elapsed_time = (time.time() - start_time)\n        print(((('...plates.world_gen: set_elevation, set_plates, center_land ' + 'complete. Elapsed time ') + str(elapsed_time)) + ' seconds.'))\n    if verbose:\n        start_time = time.time()\n    add_noise_to_elevation(world, numpy.random.randint(0, 4096))\n    if verbose:\n        elapsed_time = (time.time() - start_time)\n        print((('...plates.world_gen: elevation noise added. Elapsed time ' + str(elapsed_time)) + ' seconds.'))\n    if verbose:\n        start_time = time.time()\n    if fade_borders:\n        place_oceans_at_map_borders(world)\n    initialize_ocean_and_thresholds(world)\n    if verbose:\n        elapsed_time = (time.time() - start_time)\n        print((('...plates.world_gen: oceans initialized. Elapsed time ' + str(elapsed_time)) + ' seconds.'))\n    return generate_world(world, step)\n", "label": 0}
{"function": "\n\ndef list_security_groups(self, **_params):\n    ret = []\n    for security_group in self._fake_security_groups.values():\n        names = _params.get('name')\n        if names:\n            if (not isinstance(names, list)):\n                names = [names]\n            for name in names:\n                if (security_group.get('name') == name):\n                    ret.append(security_group)\n        ids = _params.get('id')\n        if ids:\n            if (not isinstance(ids, list)):\n                ids = [ids]\n            for id in ids:\n                if (security_group.get('id') == id):\n                    ret.append(security_group)\n        elif (not (names or ids)):\n            ret.append(security_group)\n    return {\n        'security_groups': ret,\n    }\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.what = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.why = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __menuDefinition(self):\n    result = IECore.MenuDefinition()\n    if (self.getPlug() is None):\n        return result\n    formats = [GafferImage.Format.format(n) for n in GafferImage.Format.registeredFormats()]\n    if (not self.getPlug().ancestor(Gaffer.ScriptNode).isSame(self.getPlug().node())):\n        formats.insert(0, GafferImage.Format())\n    currentFormat = self.getPlug().getValue()\n    modeIsCustom = (Gaffer.Metadata.plugValue(self.getPlug(), 'formatPlugValueWidget:mode') == 'custom')\n    for fmt in formats:\n        result.append(('/' + self.__formatLabel(fmt)), {\n            'command': functools.partial(Gaffer.WeakMethod(self.__applyFormat), fmt=fmt),\n            'checkBox': ((fmt == currentFormat) and (not modeIsCustom)),\n        })\n    result.append('/CustomDivider', {\n        'divider': True,\n    })\n    result.append('/Custom', {\n        'command': Gaffer.WeakMethod(self.__applyCustomFormat),\n        'checkBox': (modeIsCustom or (currentFormat not in formats)),\n    })\n    return result\n", "label": 0}
{"function": "\n\ndef holidays(self, start=None, end=None, return_name=False):\n    '\\n        Returns a curve with holidays between start_date and end_date\\n\\n        Parameters\\n        ----------\\n        start : starting date, datetime-like, optional\\n        end : ending date, datetime-like, optional\\n        return_names : bool, optional\\n            If True, return a series that has dates and holiday names.\\n            False will only return a DatetimeIndex of dates.\\n\\n        Returns\\n        -------\\n            DatetimeIndex of holidays\\n        '\n    if (self.rules is None):\n        raise Exception(('Holiday Calendar %s does not have any rules specified' % self.name))\n    if (start is None):\n        start = AbstractHolidayCalendar.start_date\n    if (end is None):\n        end = AbstractHolidayCalendar.end_date\n    start = Timestamp(start)\n    end = Timestamp(end)\n    holidays = None\n    if ((self._cache is None) or (start < self._cache[0]) or (end > self._cache[1])):\n        for rule in self.rules:\n            rule_holidays = rule.dates(start, end, return_name=True)\n            if (holidays is None):\n                holidays = rule_holidays\n            else:\n                holidays = holidays.append(rule_holidays)\n        self._cache = (start, end, holidays.sort_index())\n    holidays = self._cache[2]\n    holidays = holidays[start:end]\n    if return_name:\n        return holidays\n    else:\n        return holidays.index\n", "label": 1}
{"function": "\n\ndef __init__(self, authority=None, type=None, name=None, urn=None):\n    if (not (urn is None)):\n        if (not is_valid_urn(urn)):\n            raise ValueError(('Invalid URN %s' % urn))\n        spl = urn.split('+')\n        self.authority = urn_to_string_format(spl[1])\n        self.type = urn_to_string_format(spl[2])\n        self.name = urn_to_string_format('+'.join(spl[3:]))\n        self.urn = urn\n    else:\n        if ((not authority) or (not type) or (not name)):\n            raise ValueError('Must provide either all of authority, type, and name, or a urn must be provided')\n        for i in [authority, type, name]:\n            if (i.strip() == ''):\n                raise ValueError('Parameter to create_urn was empty string')\n        self.authority = authority\n        self.type = type\n        self.name = name\n        if (not is_valid_urn_string(authority)):\n            authority = string_to_urn_format(authority)\n        if (not is_valid_urn_string(type)):\n            type = string_to_urn_format(type)\n        if (not is_valid_urn_string(name)):\n            name = string_to_urn_format(name)\n        self.urn = ('%s+%s+%s+%s' % (Xrn.URN_PREFIX, authority, type, name))\n        if (not is_valid_urn(self.urn)):\n            raise ValueError(('Failed to create valid URN from args %s, %s, %s' % (self.authority, self.type, self.name)))\n", "label": 1}
{"function": "\n\ndef nits(self):\n    with_contexts = set(self.iter_ast_types(ast.With))\n    with_context_calls = set((node.context_expr for node in with_contexts if isinstance(node.context_expr, ast.Call)))\n    for call in self.iter_ast_types(ast.Call):\n        if (isinstance(call.func, ast.Name) and (call.func.id == 'open') and (call not in with_context_calls)):\n            (yield self.warning('T802', 'open() calls should be made within a contextmanager.', call))\n", "label": 0}
{"function": "\n\ndef __getitem__(self, args):\n    if (not isinstance(args, tuple)):\n        args = (args,)\n    if (len(args) == 2):\n        scalar_arg_1 = (numpy.isscalar(args[0]) or (getattr(args[0], 'type', None) == tensor.iscalar))\n        scalar_arg_2 = (numpy.isscalar(args[1]) or (getattr(args[1], 'type', None) == tensor.iscalar))\n        if (scalar_arg_1 and scalar_arg_2):\n            ret = get_item_scalar(self, args)\n        elif isinstance(args[0], list):\n            ret = get_item_2lists(self, args[0], args[1])\n        else:\n            ret = get_item_2d(self, args)\n    elif isinstance(args[0], list):\n        ret = get_item_list(self, args[0])\n    else:\n        ret = get_item_2d(self, args)\n    return ret\n", "label": 1}
{"function": "\n\ndef scan_directory(pym, directory, sentinel, installed, depth=0):\n    'Entry point scan that creates a PyModule instance if needed.\\n    '\n    if (not pym):\n        d = os.path.abspath(directory)\n        basename = os.path.basename(d)\n        pym = utils.find_package(basename, installed)\n        if (not pym):\n            version = 'DIRECTORY'\n            if os.path.isfile(os.path.join(d, '__init__.py')):\n                version = 'MODULE'\n            pym = PyModule(basename, version, d)\n            installed.insert(0, pym)\n        else:\n            pym.is_scan = True\n    bad_scans = 0\n    for item in _scan_directory(directory, sentinel, depth):\n        if os.path.isfile(item):\n            if (bad_scans > 100):\n                log.debug('Stopping scan of directory since it looks like a data dump: %s', directory)\n                break\n            if (not scan_file(pym, item, sentinel, installed)):\n                bad_scans += 1\n            else:\n                bad_scans = 0\n        elif os.path.isdir(item):\n            scan_directory(pym, item, sentinel, installed, (depth + 1))\n    return pym\n", "label": 1}
{"function": "\n\ndef locked_get(self):\n    'Retrieve Credential from datastore.\\n\\n    Returns:\\n      oauth2client.Credentials\\n    '\n    if self._cache:\n        json = self._cache.get(self._key_name)\n        if json:\n            return Credentials.new_from_json(json)\n    credential = None\n    entity = self._model.get_by_key_name(self._key_name)\n    if (entity is not None):\n        credential = getattr(entity, self._property_name)\n        if (credential and hasattr(credential, 'set_store')):\n            credential.set_store(self)\n            if self._cache:\n                self._cache.set(self._key_name, credentials.to_json())\n    return credential\n", "label": 0}
{"function": "\n\ndef _textNodes(self, recurse=False):\n    if (self.text and (getattr(self, 'xValid', 0) != VALID_NO_CONTENT)):\n        (yield self.text)\n    for c in self.iterchildren():\n        if (recurse and isinstance(c, ModelObject)):\n            for nestedText in c._textNodes(recurse):\n                (yield nestedText)\n        if (c.tail and (getattr(self, 'xValid', 0) != VALID_NO_CONTENT)):\n            (yield c.tail)\n", "label": 1}
{"function": "\n\ndef load_extension(self, ext_module):\n    \"\\n        Given an extension module name, load or in other-words 'import' the\\n        extension.\\n\\n        :param ext_module: The extension module name.  For example:\\n            'cement.ext.ext_logging'.\\n        :type ext_module: ``str``\\n        :raises: cement.core.exc.FrameworkError\\n\\n        \"\n    if (ext_module.find('.') == (- 1)):\n        ext_module = ('cement.ext.ext_%s' % ext_module)\n    if (ext_module in self._loaded_extensions):\n        LOG.debug((\"framework extension '%s' already loaded\" % ext_module))\n        return\n    LOG.debug((\"loading the '%s' framework extension\" % ext_module))\n    try:\n        if (ext_module not in sys.modules):\n            __import__(ext_module, globals(), locals(), [], 0)\n        if hasattr(sys.modules[ext_module], 'load'):\n            sys.modules[ext_module].load(self.app)\n        if (ext_module not in self._loaded_extensions):\n            self._loaded_extensions.append(ext_module)\n    except ImportError as e:\n        raise exc.FrameworkError(e.args[0])\n", "label": 0}
{"function": "\n\ndef test_issue_10302():\n    x = Symbol('x')\n    r = Symbol('r', real=True)\n    u = ((- ((3 * (2 ** pi)) ** (1 / pi))) + (2 * (3 ** (1 / pi))))\n    i = (u + (u * I))\n    assert (i.is_real is None)\n    assert ((u + i).is_zero is None)\n    assert ((1 + i).is_zero is False)\n    a = Dummy('a', zero=True)\n    assert ((a + I).is_zero is False)\n    assert ((a + (r * I)).is_zero is None)\n    assert (a + I).is_imaginary\n    assert (((a + x) + I).is_imaginary is None)\n    assert (((a + (r * I)) + I).is_imaginary is None)\n", "label": 1}
{"function": "\n\ndef _validate_counts_matrix(counts, ids=None, suppress_cast=False):\n    results = []\n    if ((len(counts) == 0) or (not isinstance(counts[0], collections.Iterable))):\n        counts = [counts]\n    counts = np.asarray(counts)\n    if (counts.ndim > 2):\n        raise ValueError(('Only 1-D and 2-D array-like objects can be provided as input. Provided object has %d dimensions.' % counts.ndim))\n    if ((ids is not None) and (len(counts) != len(ids))):\n        raise ValueError('Number of rows in ``counts`` must be equal to number of provided ``ids``.')\n    lens = []\n    for v in counts:\n        results.append(_validate_counts_vector(v, suppress_cast))\n        lens.append(len(v))\n    if (len(set(lens)) > 1):\n        raise ValueError('All rows in ``counts`` must be of equal length.')\n    return np.asarray(results)\n", "label": 0}
{"function": "\n\n@classmethod\ndef parse(cls, value, kwds):\n    if isinstance(value, str):\n        kwds['parse_tree'] = address_list = cls.value_parser(value)\n        groups = []\n        for addr in address_list.addresses:\n            groups.append(Group(addr.display_name, [Address((mb.display_name or ''), (mb.local_part or ''), (mb.domain or '')) for mb in addr.all_mailboxes]))\n        defects = list(address_list.all_defects)\n    else:\n        if (not hasattr(value, '__iter__')):\n            value = [value]\n        groups = [(Group(None, [item]) if (not hasattr(item, 'addresses')) else item) for item in value]\n        defects = []\n    kwds['groups'] = groups\n    kwds['defects'] = defects\n    kwds['decoded'] = ', '.join([str(item) for item in groups])\n    if ('parse_tree' not in kwds):\n        kwds['parse_tree'] = cls.value_parser(kwds['decoded'])\n", "label": 1}
{"function": "\n\ndef test_function_score_from_dict():\n    d = {\n        'function_score': {\n            'filter': {\n                'term': {\n                    'tags': 'python',\n                },\n            },\n            'functions': [{\n                'filter': {\n                    'terms': {\n                        'tags': 'python',\n                    },\n                },\n                'script_score': {\n                    'script': \"doc['comment_count'] * _score\",\n                },\n            }, {\n                'boost_factor': 6,\n            }],\n        },\n    }\n    q = query.Q(d)\n    assert isinstance(q, query.FunctionScore)\n    assert isinstance(q.filter, query.Term)\n    assert (len(q.functions) == 2)\n    sf = q.functions[0]\n    assert isinstance(sf, function.ScriptScore)\n    assert isinstance(sf.filter, query.Terms)\n    sf = q.functions[1]\n    assert isinstance(sf, function.BoostFactor)\n    assert (6 == sf.value)\n    assert ({\n        'boost_factor': 6,\n    } == sf.to_dict())\n", "label": 1}
{"function": "\n\ndef parse_args(argv, defaults=None, expected_argc=0):\n    opts = {\n        \n    }\n    if defaults:\n        opts.update(defaults)\n    args = []\n    i = 0\n    while (i < len(argv)):\n        if (argv[i][0] == '-'):\n            arg = argv[i].lstrip('-')\n            if ('=' in arg):\n                (k, v) = arg.split('=', 2)\n                if (k in defaults):\n                    if (type(defaults[k]) == float):\n                        opts[k] = float(v)\n                    elif (type(defaults[k]) == int):\n                        opts[k] = int(v)\n                    else:\n                        opts[k] = v\n            else:\n                opts[arg] = True\n        else:\n            args.append(argv[i])\n        i += 1\n    while (len(args) < expected_argc):\n        args.append(None)\n    return (opts, args)\n", "label": 1}
{"function": "\n\ndef assocunify(u, v, s, eq=core.eq, n=None):\n    ' Associative Unification\\n\\n    See Also:\\n        eq_assoccomm\\n    '\n    (uop, uargs) = op_args(u)\n    (vop, vargs) = op_args(v)\n    if ((not uop) and (not vop)):\n        res = unify(u, v, s)\n        if (res is not False):\n            return (res,)\n    if (uop and vop):\n        s = unify(uop, vop, s)\n        if (s is False):\n            raise StopIteration()\n        op = walk(uop, s)\n        (sm, lg) = ((uargs, vargs) if (len(uargs) <= len(vargs)) else (vargs, uargs))\n        ops = assocsized(op, lg, len(sm))\n        goal = condeseq(([(eq, a, b) for (a, b) in zip(sm, lg2)] for lg2 in ops))\n        return goaleval(goal)(s)\n    if uop:\n        (op, tail) = (uop, uargs)\n        b = v\n    if vop:\n        (op, tail) = (vop, vargs)\n        b = u\n    ns = ([n] if n else range(2, (len(tail) + 1)))\n    knowns = (build(op, x) for n in ns for x in assocsized(op, tail, n))\n    goal = condeseq(([(core.eq, b, k)] for k in knowns))\n    return goaleval(goal)(s)\n", "label": 1}
{"function": "\n\ndef _get_call_class(method):\n    'Find the call class for method if it exists else create one.'\n    (call_base, call_name) = method.split('.', 1)\n    mod = __import__('ubersmith.calls.{0}'.format(call_base), fromlist=[''])\n    gen = (getattr(mod, x) for x in dir(mod) if (not x.startswith('_')))\n    gen = (x for x in gen if ((type(x) is type) and issubclass(x, BaseCall)))\n    for call_class in gen:\n        if (call_class.method == method):\n            return call_class\n    else:\n\n        class GenericCall(BaseCall):\n            method = '.'.join((call_base, call_name))\n        return GenericCall\n", "label": 1}
{"function": "\n\ndef _get_params(mapper_spec, allowed_keys=None):\n    'Obtain output writer parameters.\\n\\n  Utility function for output writer implementation. Fetches parameters\\n  from mapreduce specification giving appropriate usage warnings.\\n\\n  Args:\\n    mapper_spec: The MapperSpec for the job\\n    allowed_keys: set of all allowed keys in parameters as strings. If it is not\\n      None, then parameters are expected to be in a separate \"output_writer\"\\n      subdictionary of mapper_spec parameters.\\n\\n  Returns:\\n    mapper parameters as dict\\n\\n  Raises:\\n    BadWriterParamsError: if parameters are invalid/missing or not allowed.\\n  '\n    if ('output_writer' not in mapper_spec.params):\n        message = \"Output writer's parameters should be specified in output_writer subdictionary.\"\n        if allowed_keys:\n            raise errors.BadWriterParamsError(message)\n        params = mapper_spec.params\n        params = dict(((str(n), v) for (n, v) in params.iteritems()))\n    else:\n        if (not isinstance(mapper_spec.params.get('output_writer'), dict)):\n            raise errors.BadWriterParamsError('Output writer parameters should be a dictionary')\n        params = mapper_spec.params.get('output_writer')\n        params = dict(((str(n), v) for (n, v) in params.iteritems()))\n        if allowed_keys:\n            params_diff = (set(params.keys()) - allowed_keys)\n            if params_diff:\n                raise errors.BadWriterParamsError(('Invalid output_writer parameters: %s' % ','.join(params_diff)))\n    return params\n", "label": 0}
{"function": "\n\ndef post(self):\n    if (not may(CREATE)):\n        raise Forbidden()\n    f = request.files.get('file')\n    t = request.form.get('text')\n    if (f and f.filename):\n        if ContentRange.from_request():\n            abort(416)\n        content_type = (f.headers.get('Content-Type') or request.headers.get('Content-Type'))\n        content_type_hint = 'application/octet-stream'\n        filename = f.filename\n        f.seek(0, os.SEEK_END)\n        size = f.tell()\n        f.seek(0)\n    elif (t is not None):\n        t = t.encode('utf-8')\n        content_type = request.form.get('contenttype')\n        content_type_hint = 'text/plain'\n        size = len(t)\n        f = BytesIO(t)\n        filename = request.form.get('filename')\n    else:\n        raise NotImplementedError\n    maxlife_unit = request.form.get('maxlife-unit', 'forever').upper()\n    maxlife_value = int(request.form.get('maxlife-value', 1))\n    maxtime = time_unit_to_sec(maxlife_value, maxlife_unit)\n    maxlife_timestamp = ((int(time.time()) + maxtime) if (maxtime > 0) else maxtime)\n    name = create_item(f, filename, size, content_type, content_type_hint, maxlife_stamp=maxlife_timestamp)\n    return redirect_next('bepasty.display', name=name, _anchor=url_quote(filename))\n", "label": 0}
{"function": "\n\ndef __virtual__():\n    short_name = __name__.rsplit('.')[(- 1)]\n    mod_opts = __opts__.get(short_name, {\n        \n    })\n    if mod_opts:\n        if ((not cpy_error) and ('port' in mod_opts)):\n            return __virtualname__\n        if cpy_error:\n            from distutils.version import LooseVersion as V\n            if (('cherrypy' in globals()) and (V(cherrypy.__version__) < V(cpy_min))):\n                error_msg = 'Required version of CherryPy is {0} or greater.'.format(cpy_min)\n            else:\n                error_msg = cpy_error\n            logger.error(\"Not loading '%s'. Error loading CherryPy: %s\", __name__, error_msg)\n        if ('port' not in mod_opts):\n            logger.error(\"Not loading '%s'. 'port' not specified in config\", __name__)\n    return False\n", "label": 0}
{"function": "\n\ndef build(self, input_shape):\n    input_dim = input_shape[1]\n    self.input_spec = [InputSpec(dtype=K.floatx(), shape=(None, input_dim))]\n    self.W = self.init((input_dim, input_dim), name='{}_W'.format(self.name))\n    self.W_carry = self.init((input_dim, input_dim), name='{}_W_carry'.format(self.name))\n    if self.bias:\n        self.b = K.zeros((input_dim,), name='{}_b'.format(self.name))\n        self.b_carry = K.variable((np.ones((input_dim,)) * self.transform_bias), name='{}_b_carry'.format(self.name))\n        self.trainable_weights = [self.W, self.b, self.W_carry, self.b_carry]\n    else:\n        self.trainable_weights = [self.W, self.W_carry]\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if (self.bias and self.b_regularizer):\n        self.b_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    if self.activity_regularizer:\n        self.activity_regularizer.set_layer(self)\n        self.regularizers.append(self.activity_regularizer)\n    self.constraints = {\n        \n    }\n    if self.W_constraint:\n        self.constraints[self.W] = self.W_constraint\n    if (self.bias and self.b_constraint):\n        self.constraints[self.b] = self.b_constraint\n    if (self.initial_weights is not None):\n        self.set_weights(self.initial_weights)\n        del self.initial_weights\n", "label": 1}
{"function": "\n\n@pytest.fixture(scope='module')\ndef rs_auth():\n    global is_authorized, tried\n    if ((not is_authorized) and (not tried)):\n        if (not tried):\n            try:\n                conn = boto.connect_redshift()\n            except boto.exception.NoAuthHandlerFound as e:\n                pytest.skip(('authorization to access redshift cluster failed %s' % e))\n            try:\n                conn.authorize_cluster_security_group_ingress('default', cidrip=cidrip)\n            except boto.redshift.exceptions.AuthorizationAlreadyExists:\n                is_authorized = True\n            except Exception as e:\n                pytest.skip(('authorization to access redshift cluster failed %s' % e))\n            else:\n                is_authorized = True\n            finally:\n                tried = True\n        else:\n            pytest.skip('authorization to access redshift cluster failed')\n", "label": 0}
{"function": "\n\ndef add(self, sig=None, argtypes=None, restype=None):\n    if (argtypes is not None):\n        warnings.warn('Keyword argument argtypes is deprecated', DeprecationWarning)\n        assert (sig is None)\n        if (restype is None):\n            sig = tuple(argtypes)\n        else:\n            sig = restype(*argtypes)\n    del argtypes\n    del restype\n    indims = [len(x) for x in self.inputsig]\n    outdims = [len(x) for x in self.outputsig]\n    funcname = self.py_func.__name__\n    src = expand_gufunc_template(self._kernel_template, indims, outdims, funcname)\n    glbls = self._get_globals(sig)\n    _exec(src, glbls)\n    fnobj = glbls['__gufunc_{name}'.format(name=funcname)]\n    (args, return_type) = sigutils.normalize_signature(sig)\n    outertys = list(_determine_gufunc_outer_types(args, (indims + outdims)))\n    kernel = self._compile_kernel(fnobj, sig=tuple(outertys))\n    dtypes = tuple((np.dtype(str(t.dtype)) for t in outertys))\n    self.kernelmap[tuple(dtypes[:(- 1)])] = (dtypes[(- 1)], kernel)\n", "label": 0}
{"function": "\n\n@classmethod\ndef _validate(cls, name, config_dict):\n    '\\n        Raises a ConfigValidationError in case of an invalid configuration.\\n\\n        :type name: str\\n        :type config_dict: dict\\n        :rtype: None\\n        '\n    required_fields = {COMMANDS, ATOMIZERS}\n    allowed_fields_expected_types = {\n        SETUP_BUILD: [(list, str)],\n        TEARDOWN_BUILD: [(list, str)],\n        COMMANDS: [(list, str)],\n        ATOMIZERS: [(list, dict)],\n        MAX_EXECUTORS: [int],\n        MAX_EXECUTORS_PER_SLAVE: [int],\n    }\n    if (not isinstance(config_dict, dict)):\n        raise ConfigValidationError('Passed in configuration is not a dictionary for job: \"{}\".'.format(name))\n    missing_required_fields = (required_fields - config_dict.keys())\n    if missing_required_fields:\n        raise ConfigValidationError('Definition for job \"{}\" is missing required config sections: {}'.format(name, missing_required_fields))\n    for (config_section_name, config_section_value) in config_dict.items():\n        if (config_section_name not in allowed_fields_expected_types):\n            raise ConfigValidationError('Definition for job \"{}\" contains an invalid config section \"{}\".'.format(name, config_section_name))\n        expected_section_types = allowed_fields_expected_types[config_section_name]\n        actual_section_type = type(config_section_value)\n        if (actual_section_type is list):\n            actual_section_type = (list, type(config_section_value[0]))\n        if (actual_section_type not in expected_section_types):\n            raise ConfigValidationError('Definition for job \"{}\" contains an invalid value for config section \"{}\". Parser expected one of {} but found {}.'.format(name, config_section_name, expected_section_types, actual_section_type))\n", "label": 0}
{"function": "\n\ndef _handle_request_exception(self, e):\n    if isinstance(e, Finish):\n        if (not self._finished):\n            self.finish()\n        return\n    self.log_exception(*sys.exc_info())\n    if self._finished:\n        return\n    if isinstance(e, HTTPError):\n        if ((e.status_code not in httputil.responses) and (not e.reason)):\n            gen_log.error('Bad HTTP status code: %d', e.status_code)\n            self.send_error(500, exc_info=sys.exc_info())\n        else:\n            self.send_error(e.status_code, exc_info=sys.exc_info())\n    else:\n        self.send_error(500, exc_info=sys.exc_info())\n", "label": 0}
{"function": "\n\ndef test_method1(self):\n    code = py2js(self.method1)\n    lines = [line for line in code.split('\\n') if line]\n    assert (len(lines) == 4)\n    assert (lines[1] == 'method1 = function () {')\n    assert lines[2].startswith('  ')\n    assert (lines[3] == '};')\n", "label": 0}
{"function": "\n\ndef read(s, cc):\n    f = cc(s)\n    fd = f.params[0]\n    buf = f.params[1]\n    size = f.params[2]\n    s.log.function_call(f, 'read(fd={}, ptr={}, size={})', fd, buf, size)\n    output = OutputBuffer(s, buf)\n    if fd.symbolic:\n        raise ValueError('wtf')\n    if (fd.value > len(s.files)):\n        return f.ret(value=0)\n    else:\n        file = s.files[fd.value]\n        offset = file['offset']\n        output = OutputBuffer(s, buf)\n        real_fd = None\n        if (file['path'] not in ['stdin', 'stdout', 'stderr']):\n            real_fd = open(file['path'], 'rb')\n        if size.symbolic:\n            raise NotImplementedError()\n        elif (real_fd is None):\n            for i in xrange(0, size.value):\n                b = bv.Symbol(8, 'file_{}_{:x}'.format(fd.value, offset))\n                output.append(b)\n                file['bytes'][offset] = b\n                offset += 1\n        else:\n            real_fd.seek(offset, 0)\n            for i in range(0, size.value):\n                byte = real_fd.read(1)\n                if (len(byte) == 1):\n                    if (byte == '#'):\n                        b = bv.Symbol(8, 'file_{}_{:x}'.format(fd.value, offset))\n                    else:\n                        b = bv.Constant(8, ord(byte))\n                    output.append(b)\n                    file['bytes'][offset] = b\n                    offset += 1\n                else:\n                    break\n        file['offset'] = offset\n        if (real_fd is not None):\n            real_fd.close()\n    return f.ret(value=size)\n", "label": 1}
{"function": "\n\ndef findall(root, xpath, attribute_name=None, attribute_value=None):\n    'Find elements recursively from given root element based on\\n    xpath and possibly given attribute\\n\\n    :param root: Element root element where to start search\\n    :param xpath: xpath defintion, like {http://foo/bar/namespace}ElementName\\n    :param attribute_name: name of possible attribute of given element\\n    :param attribute_value: value of the attribute\\n    :return: list of elements or None\\n    '\n    found_elements = []\n    if (((2, 6) == sys.version_info[0:2]) and (which_etree() != 'lxml.etree')):\n        elements = root.getiterator(xpath)\n        if ((attribute_name is not None) and (attribute_value is not None)):\n            for element in elements:\n                if (element.attrib.get(attribute_name) == attribute_value):\n                    found_elements.append(element)\n        else:\n            found_elements = elements\n    else:\n        if ((attribute_name is not None) and (attribute_value is not None)):\n            xpath = ('%s[@%s=\"%s\"]' % (xpath, attribute_name, attribute_value))\n        found_elements = root.findall(('.//' + xpath))\n    if (found_elements == []):\n        found_elements = None\n    return found_elements\n", "label": 1}
{"function": "\n\ndef GenerateCommand(self):\n    'Generate the actual commands to execute.\\n\\n    Some special casing is needed here per-class type because of the original\\n    design of CocoaDialog itself being inconsistent unfortunately.\\n\\n    SubClasses will normally call super and extend the array with their own\\n    parameters.\\n\\n    Returns:\\n      an array of commands.\\n    '\n    cmds = [self._cocoadialog]\n    runmode = self.__class__.__name__.lower().replace('_', '-')\n    cmds.append(runmode)\n    cmds.append('--string-output')\n    if self.title:\n        cmds.extend(['--title', self._title])\n    if self.debug:\n        cmds.append('--debug')\n    if (not self._timeout):\n        if (runmode == 'bubble'):\n            cmds.append('--no-timeout')\n    else:\n        cmds.extend(['--timeout', self._timeout])\n    if self._width:\n        cmds.extend(['--width', self._width])\n    if self._height:\n        cmds.extend(['--height', self._height])\n    return cmds\n", "label": 0}
{"function": "\n\ndef collect(self):\n    hosts = self.config.get('hosts')\n    if isinstance(hosts, basestring):\n        hosts = [hosts]\n    for host in hosts:\n        matches = re.search('((.+)\\\\@)?([^:]+)(:(\\\\d+))?', host)\n        alias = matches.group(2)\n        hostname = matches.group(3)\n        port = matches.group(5)\n        if (alias is None):\n            alias = hostname\n        stats = self.get_stats(hostname, port)\n        desired = self.config.get('publish', stats.keys())\n        for stat in desired:\n            if (stat in stats):\n                if (stat in self.GAUGES):\n                    self.publish_gauge(((alias + '.') + stat), stats[stat])\n                else:\n                    self.publish_counter(((alias + '.') + stat), stats[stat])\n            else:\n                self.log.error(\"No such key '%s' available, issue 'stats' for a full list\", stat)\n", "label": 0}
{"function": "\n\ndef read(self, callback, grpos_range, frames=None):\n    if grpos_range:\n        end_grpos = (self._grpos + grpos_range)\n        pos = self._pos\n        grpos = self._grpos\n        while 1:\n            idx = (pos - self._s.data_offset)\n            if (idx < 0):\n                pos -= idx\n                idx = 0\n            try:\n                f = self._s.data[idx]\n            except IndexError:\n                break\n            grpos = f[0]\n            if (grpos >= end_grpos):\n                grpos = end_grpos\n                break\n            callback(*f)\n            pos += 1\n        self._pos = pos\n        self._grpos = grpos\n    elif frames:\n        pos = self._pos\n        grpos = self._grpos\n        while 1:\n            idx = (pos - self._s.data_offset)\n            if (idx < 0):\n                pos -= idx\n                frames += idx\n            if (frames < 1):\n                break\n            try:\n                f = self._s.data[idx]\n            except IndexError:\n                break\n            grpos = f[0]\n            callback(*f)\n            pos += 1\n            frames -= 1\n        self._pos = pos\n        self._grpos = grpos\n    return (None, defer.succeed(None))\n", "label": 1}
{"function": "\n\ndef SeriesFactory(values, xvalues=None, zvalues=None, title=None, title_from_data=False):\n    '\\n    Convenience Factory for creating chart data series.\\n    '\n    if (not isinstance(values, Reference)):\n        values = Reference(range_string=values)\n    if title_from_data:\n        cell = values.pop()\n        title = '{0}!{1}'.format(values.sheetname, cell)\n        title = SeriesLabel(strRef=StrRef(title))\n    elif (title is not None):\n        title = SeriesLabel(v=title)\n    source = NumDataSource(numRef=NumRef(f=values))\n    if (xvalues is not None):\n        if (not isinstance(xvalues, Reference)):\n            xvalues = Reference(range_string=xvalues)\n        series = XYSeries()\n        series.yVal = source\n        series.xVal = AxDataSource(numRef=NumRef(f=xvalues))\n        if (zvalues is not None):\n            if (not isinstance(zvalues, Reference)):\n                zvalues = Reference(range_string=zvalues)\n            series.zVal = NumDataSource(NumRef(f=zvalues))\n    else:\n        series = Series()\n        series.val = source\n    if (title is not None):\n        series.title = title\n    return series\n", "label": 1}
{"function": "\n\ndef on_done(self, result):\n    if (result['okay'] is True):\n        status = 'Compilation Succeeded'\n    else:\n        status = 'Compilation FAILED '\n        error_list = []\n        print(result['err'])\n        for line in result['err'].split('\\n'):\n            if (len(line.split(':')) < 3):\n                continue\n            try:\n                message = line.split(':')[2]\n                lineNum = line.split('(')[1].split(',')[0]\n                rowNum = line.split('(')[1].split(',')[1].split(')')[0]\n            except:\n                print(('Cannot parse: ' + line))\n                continue\n            try:\n                error_list.append({\n                    'message': message,\n                    'line': (int(lineNum) - 1),\n                    'col': int(rowNum),\n                })\n            except:\n                continue\n        if len(error_list):\n            popup_error_list(self.view, error_list)\n    later = (lambda : sublime.status_message(status))\n    sublime.set_timeout(later, 300)\n", "label": 0}
{"function": "\n\ndef test_random_complex_exact(self):\n    for dtype in COMPLEX_DTYPES:\n        for n in (20, 200):\n            for lapack_driver in TestLstsq.lapack_drivers:\n                for overwrite in (True, False):\n                    a = np.asarray((random([n, n]) + (1j * random([n, n]))), dtype=dtype)\n                    for i in range(n):\n                        a[(i, i)] = (20 * (0.1 + a[(i, i)]))\n                    for i in range(2):\n                        b = np.asarray(random([n, 3]), dtype=dtype)\n                        a1 = a.copy()\n                        b1 = b.copy()\n                        out = lstsq(a1, b1, lapack_driver=lapack_driver, overwrite_a=overwrite, overwrite_b=overwrite)\n                        x = out[0]\n                        r = out[2]\n                        assert_((r == n), ('expected efficient rank %s, got %s' % (n, r)))\n                        if (dtype is np.complex64):\n                            assert_allclose(dot(a, x), b, rtol=(400 * np.finfo(a1.dtype).eps), atol=(400 * np.finfo(a1.dtype).eps), err_msg=('driver: %s' % lapack_driver))\n                        else:\n                            assert_allclose(dot(a, x), b, rtol=(1000 * np.finfo(a1.dtype).eps), atol=(1000 * np.finfo(a1.dtype).eps), err_msg=('driver: %s' % lapack_driver))\n", "label": 0}
{"function": "\n\ndef __init__(self, width, height, color='black', emphasis=None, highlight=0):\n    if (((width == 0) and (height == 0) and (color == 'red') and (emphasis == 'strong')) or (highlight > 100)):\n        raise ValueError('sorry, you lose')\n    if ((width == 0) and (height == 0) and ((color == 'red') or (emphasis is None))):\n        raise ValueError((\"I don't think so -- values are %s, %s\" % (width, height)))\n    Blob.__init__(self, width, height, color, emphasis, highlight)\n", "label": 1}
{"function": "\n\ndef _process_commands(self, data):\n    success = True\n    defaults = self._context.defaults().get('shell', {\n        \n    })\n    with open(os.devnull, 'w') as devnull:\n        for item in data:\n            stdin = stdout = stderr = devnull\n            if isinstance(item, dict):\n                cmd = item['command']\n                msg = item.get('description', None)\n                if (item.get('stdin', defaults.get('stdin', False)) is True):\n                    stdin = None\n                if (item.get('stdout', defaults.get('stdout', False)) is True):\n                    stdout = None\n                if (item.get('stderr', defaults.get('stderr', False)) is True):\n                    stderr = None\n            elif isinstance(item, list):\n                cmd = item[0]\n                msg = (item[1] if (len(item) > 1) else None)\n            else:\n                cmd = item\n                msg = None\n            if (msg is None):\n                self._log.lowinfo(cmd)\n            else:\n                self._log.lowinfo(('%s [%s]' % (msg, cmd)))\n            ret = subprocess.call(cmd, shell=True, stdin=stdin, stdout=stdout, stderr=stderr, cwd=self._context.base_directory())\n            if (ret != 0):\n                success = False\n                self._log.warning(('Command [%s] failed' % cmd))\n    if success:\n        self._log.info('All commands have been executed')\n    else:\n        self._log.error('Some commands were not successfully executed')\n    return success\n", "label": 1}
{"function": "\n\ndef add_entity(self, entity, platform=None):\n    'Add entity to component.'\n    if ((entity is None) or (entity in self.entities.values())):\n        return False\n    entity.hass = self.hass\n    if (getattr(entity, 'entity_id', None) is None):\n        object_id = (entity.name or DEVICE_DEFAULT_NAME)\n        if ((platform is not None) and (platform.entity_namespace is not None)):\n            object_id = '{} {}'.format(platform.entity_namespace, object_id)\n        entity.entity_id = generate_entity_id(self.entity_id_format, object_id, self.entities.keys())\n    self.entities[entity.entity_id] = entity\n    entity.update_ha_state()\n    return True\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _read_depgraph(node, depgraph, label_counter=None, parent=None):\n    if (not label_counter):\n        label_counter = Counter()\n    if (node['rel'].lower() in ['spec', 'punct']):\n        return (node['word'], node['tag'])\n    else:\n        fstruct = FStructure()\n        fstruct.pred = None\n        fstruct.label = FStructure._make_label(label_counter.get())\n        fstruct.parent = parent\n        (word, tag) = (node['word'], node['tag'])\n        if (tag[:2] == 'VB'):\n            if (tag[2:3] == 'D'):\n                fstruct.safeappend('tense', ('PAST', 'tense'))\n            fstruct.pred = (word, tag[:2])\n        if (not fstruct.pred):\n            fstruct.pred = (word, tag)\n        children = [depgraph.nodes[idx] for idx in sum(list(node['deps'].values()), [])]\n        for child in children:\n            fstruct.safeappend(child['rel'], FStructure._read_depgraph(child, depgraph, label_counter, fstruct))\n        return fstruct\n", "label": 0}
{"function": "\n\ndef transform_inventory_targeting_to_dfp(targeting):\n    '\\n    Convert inventory targeting model to dfp dictionary\\n\\n    @param adunit: TargetingCriterion\\n    @return: dict\\n    '\n    targeted_placements = []\n    targeted_adunits = []\n    excluded_adunits = []\n    (includes, excludes) = targeting.get_includes_and_excludes()\n    for i in includes:\n        if isinstance(i, AdUnit):\n            targeted_adunits.append(_adunit_to_dfp(i))\n        elif isinstance(i, Placement):\n            targeted_placements.append(i.id)\n    for e in excludes:\n        if isinstance(e, AdUnit):\n            excluded_adunits.append(_adunit_to_dfp(e))\n    target_dict = {\n        \n    }\n    if targeted_placements:\n        target_dict['targetedPlacementIds'] = targeted_placements\n    if targeted_adunits:\n        target_dict['targetedAdUnits'] = targeted_adunits\n    if excluded_adunits:\n        target_dict['excludedAdUnits'] = excluded_adunits\n    return target_dict\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('exname,source,target,colnos', [(exname, source, target, list(colnos)) for exname in examples.keys() for source in range(1, 3) for target in range(2, 4) for colnos in powerset(range(1, 3))])\ndef test_row_similarity(exname, source, target, colnos):\n    if ((exname == 't0') and any(((colno > 0) for colno in colnos))):\n        pytest.skip('Not enough columns in t0.')\n    if (exname.startswith('t1_sub') and any(((colno > 1) for colno in colnos))):\n        pytest.skip(('Not enough columns in %s.' % (exname,)))\n    with analyzed_bayesdb_generator(examples[exname](), 1, 1) as (bdb, generator_id):\n        bqlfn.bql_row_similarity(bdb, generator_id, None, source, target, *colnos)\n        sql = ('select bql_row_similarity(?, NULL, ?, ?%s%s)' % (('' if (0 == len(colnos)) else ', '), ', '.join(map(str, colnos))))\n        bdb.sql_execute(sql, (generator_id, source, target)).fetchall()\n", "label": 0}
{"function": "\n\ndef _filter(self, include_private, filter):\n    childnodes = {\n        \n    }\n    for name in dir(self.node.__class__):\n        if ((not name.startswith('__')) and (name != 'parent')):\n            if (include_private or (not name.startswith('_'))):\n                attr = getattr(self.node, name)\n                if filter(attr):\n                    childnodes[name] = attr\n    return childnodes\n", "label": 0}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    completions = []\n    if (not view.match_selector(locations[0], 'meta.scope.between-output-tags.cfml - meta.tag - comment - string,                 text.html.cfm - meta - source - comment - string,                 text.html.cfm.embedded.cfml - meta - source.cfscript.embedded.cfml - comment - string,                 punctuation.definition.tag.cf.begin,                 source.sql.embedded.cfml - string - comment - meta.name.interpolated.hash')):\n        return\n    if SETTINGS.get('verbose_tag_completions'):\n        return\n    sel = view.sel()[0]\n    if (view.substr((sel.begin() - 1)) == '.'):\n        return []\n    pt = ((locations[0] - len(prefix)) - 1)\n    if any(((s in view.scope_name(pt)) for s in ['meta.tag.block.cf', 'meta.tag.inline.cf', 'string', 'comment'])):\n        return\n    for s in self.cflib.completions.keys():\n        completions.extend([((s + '\\tTag (cmfl)'), s)])\n    if (view.substr(pt) != '<'):\n        completions = [(list(item)[(- 2)], ('<' + list(item)[1])) for item in completions]\n    return sorted(completions)\n", "label": 1}
{"function": "\n\ndef terminate(pid, sig, timeout):\n    'Terminates process with PID `pid` and returns True if process finished\\n    during `timeout`. Current user must have permission to access process\\n    information.'\n    os.kill(pid, sig)\n    start = time.time()\n    while True:\n        try:\n            (_, status) = os.waitpid(pid, os.WNOHANG)\n        except OSError as exc:\n            if (exc.errno != errno.ECHILD):\n                raise\n        else:\n            if status:\n                return True\n        if (not is_running(pid)):\n            return True\n        if ((time.time() - start) >= timeout):\n            return False\n        time.sleep(0.1)\n", "label": 0}
{"function": "\n\ndef _get_td_css(h, v, td_styles):\n    if td_styles:\n        if isinstance(td_styles, string_types):\n            return td_styles\n        elif callable(td_styles):\n            return td_styles(v)\n        elif isinstance(td_styles, dict):\n            if (h in td_styles):\n                s = td_styles[h]\n                if isinstance(s, string_types):\n                    return s\n                elif callable(s):\n                    return s(v)\n                else:\n                    raise ArgumentError(('expected string or callable, got %r' % s))\n        else:\n            raise ArgumentError(('expected string, callable or dict, got %r' % td_styles))\n    if (isinstance(v, numeric_types) and (not isinstance(v, bool))):\n        return 'text-align: right'\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef submit_packets(self, packets):\n    if self.utf8_decoding:\n        packets = unicode(packets, 'utf-8', errors='replace')\n    for packet in packets.splitlines():\n        if (not packet.strip()):\n            continue\n        if packet.startswith('_e'):\n            self.event_count += 1\n            event = self.parse_event_packet(packet)\n            self.event(**event)\n        elif packet.startswith('_sc'):\n            self.service_check_count += 1\n            service_check = self.parse_sc_packet(packet)\n            self.service_check(**service_check)\n        else:\n            self.count += 1\n            parsed_packets = self.parse_metric_packet(packet)\n            for (name, value, mtype, tags, sample_rate) in parsed_packets:\n                (hostname, device_name, tags) = self._extract_magic_tags(tags)\n                self.submit_metric(name, value, mtype, tags=tags, hostname=hostname, device_name=device_name, sample_rate=sample_rate)\n", "label": 0}
{"function": "\n\ndef documentation(self, add_to=None):\n    'Produces general documentation for the interface'\n    doc = (OrderedDict if (add_to is None) else add_to)\n    usage = self.interface.spec.__doc__\n    if usage:\n        doc['usage'] = usage\n    if getattr(self, 'requires', None):\n        doc['requires'] = [getattr(requirement, '__doc__', requirement.__name__) for requirement in self.requires]\n    doc['outputs'] = OrderedDict()\n    doc['outputs']['format'] = self.outputs.__doc__\n    doc['outputs']['content_type'] = self.outputs.content_type\n    parameters = [param for param in self.parameters if ((not (param in ('request', 'response', 'self'))) and (not param.startswith('hug_')) and (not hasattr(param, 'directive')))]\n    if parameters:\n        inputs = doc.setdefault('inputs', OrderedDict())\n        types = self.interface.spec.__annotations__\n        for argument in parameters:\n            kind = types.get(argument, text)\n            if (getattr(kind, 'directive', None) is True):\n                continue\n            input_definition = inputs.setdefault(argument, OrderedDict())\n            input_definition['type'] = (kind if isinstance(kind, str) else kind.__doc__)\n            default = self.defaults.get(argument, None)\n            if (default is not None):\n                input_definition['default'] = default\n    return doc\n", "label": 1}
{"function": "\n\ndef env(key, default='', factory=None):\n    if ((default is RaiseException) and (key not in os.environ)):\n        raise KeyError(key)\n    val = os.environ.get(key, default)\n    if isinstance(val, basestring):\n        val = val.strip()\n    if (val == default == NotSet):\n        return NotSet\n    if (factory != NotSet):\n        if ((not factory) and (default != NotSet)):\n            factory = type(default)\n        if factory:\n            if (factory == bool):\n                if (isinstance(val, basestring) and (val.lower() in ['no', 'false', 'off', '0'])):\n                    val = False\n            val = factory(val)\n    return val\n", "label": 1}
{"function": "\n\ndef test_can_process(self):\n\n    class _CovHandler(ArtifactHandler):\n        FILENAMES = ('coverage.xml',)\n\n    class _OtherHandler(ArtifactHandler):\n        FILENAMES = ('/other.xml', 'foo/*/weird.json')\n    manager = Manager([_CovHandler, _OtherHandler])\n    assert manager.can_process('foo/coverage.xml')\n    assert manager.can_process('other.xml')\n    assert manager.can_process('artifactstore/other.xml')\n    assert manager.can_process('foo/bar/weird.json')\n    assert manager.can_process('artifactstore/foo/bar/weird.json')\n    assert (not manager.can_process('foo/other.xml'))\n    assert (not manager.can_process('bar/foo/baz/weird.json'))\n    assert (not manager.can_process('service.log'))\n", "label": 1}
{"function": "\n\ndef log_actor_migrate(self, actor_id, dest_node_id):\n    ' Trace actor migrate\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_ACTOR_MIGRATE in logger.events)):\n            if ((not logger.actors) or (actor_id in logger.actors)):\n                data = {\n                    \n                }\n                data['timestamp'] = time.time()\n                data['node_id'] = self.node.id\n                data['type'] = 'actor_migrate'\n                data['actor_id'] = actor_id\n                data['dest_node_id'] = dest_node_id\n                if (logger.connection is not None):\n                    if (not logger.connection.connection_lost):\n                        logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                    else:\n                        disconnected.append(user_id)\n                elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                    msg = {\n                        'cmd': 'logevent',\n                        'msgid': logger.handle,\n                        'header': None,\n                        'data': ('data: %s\\n\\n' % json.dumps(data)),\n                    }\n                    self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef _format_nsn_using_pattern(national_number, formatting_pattern, number_format, carrier_code=None):\n    number_format_rule = formatting_pattern.format\n    m_re = re.compile(formatting_pattern.pattern)\n    formatted_national_number = U_EMPTY_STRING\n    if ((number_format == PhoneNumberFormat.NATIONAL) and (carrier_code is not None) and (len(carrier_code) > 0) and (formatting_pattern.domestic_carrier_code_formatting_rule is not None) and (len(formatting_pattern.domestic_carrier_code_formatting_rule) > 0)):\n        cc_format_rule = formatting_pattern.domestic_carrier_code_formatting_rule\n        cc_format_rule = re.sub(_CC_PATTERN, carrier_code, cc_format_rule, count=1)\n        number_format_rule = re.sub(_FIRST_GROUP_PATTERN, cc_format_rule, number_format_rule, count=1)\n        formatted_national_number = re.sub(m_re, number_format_rule, national_number)\n    else:\n        national_prefix_formatting_rule = formatting_pattern.national_prefix_formatting_rule\n        if ((number_format == PhoneNumberFormat.NATIONAL) and (national_prefix_formatting_rule is not None) and (len(national_prefix_formatting_rule) > 0)):\n            first_group_rule = re.sub(_FIRST_GROUP_PATTERN, national_prefix_formatting_rule, number_format_rule, count=1)\n            formatted_national_number = re.sub(m_re, first_group_rule, national_number)\n        else:\n            formatted_national_number = re.sub(m_re, number_format_rule, national_number)\n    if (number_format == PhoneNumberFormat.RFC3966):\n        m = _SEPARATOR_PATTERN.match(formatted_national_number)\n        if m:\n            formatted_national_number = re.sub(_SEPARATOR_PATTERN, U_EMPTY_STRING, formatted_national_number, count=1)\n        formatted_national_number = re.sub(_SEPARATOR_PATTERN, U_DASH, formatted_national_number)\n    return formatted_national_number\n", "label": 1}
{"function": "\n\ndef handle(self, request, data):\n    image_id = data['image_id']\n    error_updating = _('Unable to update image \"%s\".')\n    if (data['disk_format'] in ['aki', 'ari', 'ami']):\n        container_format = data['disk_format']\n    else:\n        container_format = 'bare'\n    meta = {\n        'is_public': data['public'],\n        'protected': data['protected'],\n        'disk_format': data['disk_format'],\n        'container_format': container_format,\n        'name': data['name'],\n        'properties': {\n            \n        },\n    }\n    if data['description']:\n        meta['properties']['description'] = data['description']\n    if data['kernel']:\n        meta['properties']['kernel_id'] = data['kernel']\n    if data['ramdisk']:\n        meta['properties']['ramdisk_id'] = data['ramdisk']\n    if data['architecture']:\n        meta['properties']['architecture'] = data['architecture']\n    meta['purge_props'] = False\n    try:\n        image = api.glance.image_update(request, image_id, **meta)\n        messages.success(request, _('Image was successfully updated.'))\n        return image\n    except Exception:\n        exceptions.handle(request, (error_updating % image_id))\n", "label": 0}
{"function": "\n\ndef run(self):\n    '\\n        Perform the actual VASP run.\\n\\n        Returns:\\n            (subprocess.Popen) Used for monitoring.\\n        '\n    cmd = list(self.vasp_cmd)\n    if self.auto_gamma:\n        vi = VaspInput.from_directory('.')\n        kpts = vi['KPOINTS']\n        if ((kpts.style == Kpoints.supported_modes.Gamma) and (tuple(kpts.kpts[0]) == (1, 1, 1))):\n            if ((self.gamma_vasp_cmd is not None) and which(self.gamma_vasp_cmd[(- 1)])):\n                cmd = self.gamma_vasp_cmd\n            elif which((cmd[(- 1)] + '.gamma')):\n                cmd[(- 1)] += '.gamma'\n    logging.info('Running {}'.format(' '.join(cmd)))\n    with open(self.output_file, 'w') as f:\n        p = subprocess.Popen(cmd, stdout=f)\n    return p\n", "label": 0}
{"function": "\n\ndef __init__(self):\n    '\\n            Constructor\\n        '\n    settings = current.deployment_settings\n    log_level = settings.get_log_level()\n    if (log_level is None):\n        self.critical = self.error = self.warning = self.info = self.debug = self.ignore\n        self.log_level = 100\n    else:\n        try:\n            level = getattr(logging, log_level.upper())\n        except AttributeError:\n            raise SyntaxError(('Invalid settings.log.level: %s' % log_level))\n        self.log_level = level\n        self.critical = (self._critical if (level <= logging.CRITICAL) else self.ignore)\n        self.error = (self._error if (level <= logging.ERROR) else self.ignore)\n        self.warning = (self._warning if (level <= logging.WARNING) else self.ignore)\n        self.info = (self._info if (level <= logging.INFO) else self.ignore)\n        self.debug = (self._debug if (level <= logging.DEBUG) else self.ignore)\n    self.configure_logger()\n", "label": 0}
{"function": "\n\ndef get_subj_alt_name(peer_cert):\n    dns_name = []\n    if (not SUBJ_ALT_NAME_SUPPORT):\n        return dns_name\n    general_names = SubjectAltName()\n    for i in range(peer_cert.get_extension_count()):\n        ext = peer_cert.get_extension(i)\n        ext_name = ext.get_short_name()\n        if (ext_name != 'subjectAltName'):\n            continue\n        ext_dat = ext.get_data()\n        decoded_dat = der_decoder.decode(ext_dat, asn1Spec=general_names)\n        for name in decoded_dat:\n            if (not isinstance(name, SubjectAltName)):\n                continue\n            for entry in range(len(name)):\n                component = name.getComponentByPosition(entry)\n                if (component.getName() != 'dNSName'):\n                    continue\n                dns_name.append(str(component.getComponent()))\n    return dns_name\n", "label": 0}
{"function": "\n\ndef move_to_stash(self, which, where=None):\n    if ((which is not None) and where):\n        which = str(which)\n        stash_regions = self.get_all_stash_regions()\n        if stash_regions:\n            prev_regions = [r for r in stash_regions if (self.view.substr(r) < which)]\n            next_regions = [r for r in stash_regions if (self.view.substr(r) >= which)]\n            if next_regions:\n                next = next_regions[0]\n            else:\n                next = prev_regions[(- 1)]\n            self.move_to_region(next)\n        else:\n            self.move_to_file(1)\n    elif isinstance(which, int):\n        stashes = self.get_all_stash_regions()\n        if stashes:\n            if (len(stashes) >= which):\n                self.move_to_region(self.view.line(stashes[(which - 1)]))\n            else:\n                self.move_to_region(self.view.line(stashes[(- 1)]))\n", "label": 1}
{"function": "\n\ndef evaluate_srl_1step(find_preds_automatically=False, gold_file=None):\n    '\\n    Evaluates the network on the SRL task performed with one step for\\n    id + class.\\n    '\n    md = Metadata.load_from_file('srl')\n    nn = taggers.load_network(md)\n    r = taggers.create_reader(md, gold_file=gold_file)\n    itd = r.get_inverse_tag_dictionary()\n    if find_preds_automatically:\n        tagger = taggers.SRLTagger()\n    else:\n        iter_predicates = iter(r.predicates)\n    for sent in iter(r.sentences):\n        actual_sent = sent[0]\n        if find_preds_automatically:\n            pred_positions = tagger.find_predicates(sent)\n        else:\n            pred_positions = iter_predicates.next()\n        verbs = [(position, actual_sent[position].word) for position in pred_positions]\n        sent_codified = np.array([r.converter.convert(token) for token in actual_sent])\n        answers = nn.tag_sentence(sent_codified, pred_positions)\n        tags = [convert_iob_to_iobes([itd[x] for x in pred_answer]) for pred_answer in answers]\n        print(prop_conll(verbs, tags, len(actual_sent)))\n", "label": 0}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    sel = view.sel()[0].a\n    completions = []\n    pt = ((locations[0] - len(prefix)) - 1)\n    if any(((s in view.scope_name(pt)) for s in ['string', 'comment'])):\n        return\n    if any(((s in view.scope_name(sel)) for s in self.valid_scopes_tags)):\n        for region in view.sel():\n            pos = region.begin()\n            tagdata = view.substr(sublime.Region(0, pos)).split('<')\n            tagdata.reverse()\n            tagdata = tagdata.pop(0).split(' ')\n            tagname = tagdata[0]\n        if (tagname in self.cflib.completions.keys()):\n            completions = self.cflib.completions[tagname]['completions']\n    if (completions == []):\n        return\n    return (completions, (sublime.INHIBIT_WORD_COMPLETIONS | sublime.INHIBIT_EXPLICIT_COMPLETIONS))\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.num1 = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.num2 = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.op = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.comment = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef associate_dhcp_options(context, dhcp_options_id, vpc_id):\n    vpc = ec2utils.get_db_item(context, vpc_id)\n    rollback_dhcp_options_id = vpc.get('dhcp_options_id')\n    if (dhcp_options_id == 'default'):\n        dhcp_options_id = None\n        dhcp_options = None\n    else:\n        dhcp_options = ec2utils.get_db_item(context, dhcp_options_id)\n        dhcp_options_id = dhcp_options['id']\n    neutron = clients.neutron(context)\n    os_ports = neutron.list_ports(tenant_id=context.project_id)['ports']\n    network_interfaces = db_api.get_items(context, 'eni')\n    rollback_dhcp_options_object = (db_api.get_item_by_id(context, rollback_dhcp_options_id) if (dhcp_options_id is not None) else None)\n    with common.OnCrashCleaner() as cleaner:\n        _associate_vpc_item(context, vpc, dhcp_options_id)\n        cleaner.addCleanup(_associate_vpc_item, context, vpc, rollback_dhcp_options_id)\n        for network_interface in network_interfaces:\n            os_port = next((p for p in os_ports if (p['id'] == network_interface['os_id'])), None)\n            if (not os_port):\n                continue\n            _add_dhcp_opts_to_port(context, dhcp_options, network_interface, os_port, neutron)\n            cleaner.addCleanup(_add_dhcp_opts_to_port, context, rollback_dhcp_options_object, network_interface, os_port, neutron)\n    return True\n", "label": 0}
{"function": "\n\ndef go(self, room, tell_new=None, tell_old=None):\n    'Go to a specific room.'\n    if (self.position[0] == 'standing'):\n        if room:\n            if self.location:\n                prev = ('%s_%s' % (self.location.id, self.location.area.name))\n                if tell_old:\n                    self.location.tell_room(tell_old, [self.name])\n                self.location.remove_char(self)\n            else:\n                prev = 'void'\n            if (self.location and (self.location == room)):\n                self.update_output(\"You're already there.\\n\")\n            else:\n                self.location = room\n                self.update_output(self.look_at_room())\n                self.location.add_char(self, prev)\n                if tell_new:\n                    self.location.tell_room(tell_new, [self.name])\n        else:\n            self.world.log.debug(('We gave %s a nonexistant room.' % self.name))\n    else:\n        self.update_output('You better stand up first.')\n", "label": 0}
{"function": "\n\ndef _post_replicate_hook(self, broker, info, responses):\n    if (info['account'] == MISPLACED_OBJECTS_ACCOUNT):\n        return\n    try:\n        self.sync_store.update_sync_store(broker)\n    except Exception:\n        self.logger.exception(('Failed to update sync_store %s' % broker.db_file))\n    point = broker.get_reconciler_sync()\n    if ((not broker.has_multiple_policies()) and (info['max_row'] != point)):\n        broker.update_reconciler_sync(info['max_row'])\n        return\n    max_sync = self.dump_to_reconciler(broker, point)\n    success = (responses.count(True) >= quorum_size(len(responses)))\n    if ((max_sync > point) and success):\n        broker.update_reconciler_sync(max_sync)\n", "label": 0}
{"function": "\n\n@logging.log_deploy_wrapper(LOG.info, _('Create containers on host'))\ndef create_servers(self):\n    host_provider = self.get_host_provider()\n    name_prefix = self.config['container_name_prefix']\n    hosts = []\n    if ('start_lxc_network' in self.config):\n        network = netaddr.IPNetwork(self.config['start_lxc_network'])\n    else:\n        network = None\n    distribution = self.config.get('distribution', 'ubuntu')\n    release = self.config.get('release')\n    for server in host_provider.create_servers():\n        config = {\n            'tunnel_to': self.config.get('tunnel_to', []),\n            'forward_ssh': self.config.get('forward_ssh', False),\n        }\n        if network:\n            config['network'] = str(network)\n        host = LxcHost(server, config)\n        host.prepare()\n        ip = (str(network.ip).replace('.', '-') if network else '0')\n        first_name = ('%s-000-%s' % (name_prefix, ip))\n        host.create_container(first_name, distribution, release)\n        for i in range(1, self.config.get('containers_per_host', 1)):\n            name = ('%s-%03d-%s' % (name_prefix, i, ip))\n            host.create_clone(name, first_name)\n        host.start_containers()\n        hosts.append(host)\n        if network:\n            network += 1\n    servers = []\n    for host in hosts:\n        for server in host.get_server_objects():\n            servers.append(server)\n        info = {\n            'host': host.server.get_credentials(),\n            'config': host.config,\n            'forwarded_ports': host._port_cache.items(),\n            'container_names': host.containers,\n        }\n        self.resources.create(info)\n    return servers\n", "label": 1}
{"function": "\n\ndef osmNaturalToTraining(xml_file):\n    address_list = xmlToAddrList(xml_file)\n    train_addr_list = etree.Element('AddressCollection')\n    trainFileName = (('../training_data/' + re.sub('\\\\W+', '_', xml_file)) + '.xml')\n    punc_list = ',.'\n    osm_tags_to_addr_tags = {\n        'addr:housenumber': 'AddressNumber',\n        'addr:street:prefix': 'StreetNamePreDirectional',\n        'addr:street:name': 'StreetName',\n        'addr:street:type': 'StreetNamePostType',\n        'addr:city': 'PlaceName',\n        'addr:state': 'StateName',\n        'addr:postcode': 'ZipCode',\n    }\n    for address in address_list:\n        addr_tokens = address['addr:full'].split()\n        train_addr = etree.Element('AddressString')\n        is_addr_taggable = True\n        for token in addr_tokens:\n            is_token_taggable = False\n            for (key, value) in list(address.items()):\n                if ((key in list(osm_tags_to_addr_tags.keys())) and (key != 'addr:full') and (token in value.split())):\n                    is_taggable = True\n                    token_xml = etree.Element(osm_tags_to_addr_tags[key])\n                    token_xml.text = token\n                    if (token[(- 1)] in punc_list):\n                        token_xml.text = token[0:(- 1)]\n                        token_xml.tail = token[(- 1)]\n                    train_addr.append(token_xml)\n            if (is_token_taggable == False):\n                is_addr_taggable = False\n        if (is_addr_taggable == True):\n            train_addr_list.append(train_addr)\n    output = etree.tostring(train_addr_list, pretty_print=True)\n    with open(trainFileName, 'w') as f:\n        f.write(output)\n", "label": 1}
{"function": "\n\ndef test_expand_func():\n    from sympy.abc import a, b, c\n    from sympy import gamma, expand_func\n    (a1, b1, c1) = (randcplx(), randcplx(), (randcplx() + 5))\n    assert (expand_func(hyper([a, b], [c], 1)) == ((gamma(c) * gamma((((- a) - b) + c))) / (gamma(((- a) + c)) * gamma(((- b) + c)))))\n    assert (abs((expand_func(hyper([a1, b1], [c1], 1)).n() - hyper([a1, b1], [c1], 1).n())) < 1e-10)\n    assert (expand_func(hyper([], [], z)) == exp(z))\n    assert (expand_func(hyper([1, 2, 3], [], z)) == hyper([1, 2, 3], [], z))\n    assert (expand_func(meijerg([[1, 1], []], [[1], [0]], z)) == log((z + 1)))\n    assert (expand_func(meijerg([[1, 1], []], [[], []], z)) == meijerg([[1, 1], []], [[], []], z))\n", "label": 0}
{"function": "\n\ndef compute_tensor(self, input_var, mask=None, additional_inputs=None, steps=None, backward=False):\n    backward = (backward if backward else self._go_backwards)\n    steps = (steps if steps else self._steps)\n    mask = (mask if mask else self._mask)\n    if (mask and (self._input_type == 'one')):\n        raise Exception('Mask only works with sequence input')\n    init_state_map = self.get_initial_states(input_var)\n    if (self._input_type == 'sequence'):\n        input_var = input_var.dimshuffle((1, 0, 2))\n        seq_map = self.get_step_inputs(input_var, mask=mask, additional_inputs=additional_inputs)\n    else:\n        init_state_map[self.main_state] = input_var\n        seq_map = self.get_step_inputs(None, mask=mask, additional_inputs=additional_inputs)\n    (retval_map, _) = Scanner(self.step, sequences=seq_map, outputs_info=init_state_map, n_steps=steps, go_backwards=backward).compute()\n    main_states = retval_map[self.main_state]\n    if (self._output_type == 'one'):\n        return main_states[(- 1)]\n    elif (self._output_type == 'sequence'):\n        main_states = main_states.dimshuffle((1, 0, 2))\n        if mask:\n            main_states *= mask.dimshuffle((0, 1, 'x'))\n        return main_states\n", "label": 1}
{"function": "\n\ndef handle_bit(self, bit):\n    '\\n        Handle the current bit\\n        '\n    breakpoint = False\n    if (self.forced_next is not None):\n        if (bit != self.forced_next):\n            raise BreakpointExpected(self.tagname, [self.forced_next], bit)\n    elif (bit in self.options.reversed_combined_breakpoints):\n        expected = self.options.reversed_combined_breakpoints[bit]\n        raise BreakpointExpected(self.tagname, [expected], bit)\n    if (bit == self.options.next_breakpoint):\n        self.handle_next_breakpoint(bit)\n        breakpoint = bit\n    elif (bit in self.options.breakpoints):\n        self.handle_breakpoints(bit)\n        breakpoint = bit\n    else:\n        self.handle_argument(bit)\n    if (bit in self.options.combined_breakpoints):\n        self.forced_next = self.options.combined_breakpoints[bit]\n    else:\n        self.forced_next = None\n    del self.todo[0]\n    return breakpoint\n", "label": 0}
{"function": "\n\ndef update_track(track_dict):\n    for (key, value) in DEFAULT_TEMPLATE.items():\n        if (key in ['actions']):\n            if (track_dict[key] != DEFAULT_TEMPLATE[key]):\n                warning(\"Your track's '{0}' configuration is not the same as the default.\".format(key))\n                default = 'n'\n                if (key == 'actions'):\n                    default = 'y'\n                    warning(\"Unless you have manually modified your 'actions' (the commands which get run for a release), you should update to the new default.\")\n                warning('Should it be updated to the default setting?')\n                if maybe_continue(default):\n                    track_dict[key] = DEFAULT_TEMPLATE[key]\n        elif (key not in track_dict):\n            value = (value.default if isinstance(value, PromptEntry) else value)\n            track_dict[key] = value\n    return track_dict\n", "label": 0}
{"function": "\n\ndef getpygamecolor(value):\n    \"Returns a pygame.Color object of the argument passed in. The argument can be a RGB/RGBA tuple, pygame.Color object, or string in the colornames dict (such as 'blue' or 'gray').\"\n    if (type(value) in (tuple, list)):\n        alpha = (((len(value) > 3) and value[3]) or 255)\n        return pygame.Color(value[0], value[1], value[2], alpha)\n    elif (str(type(value)) in (\"<class 'pygame.Color'>\", \"<type 'pygame.Color'>\")):\n        return value\n    elif (value in colornames):\n        return colornames[value]\n    else:\n        raise Exception(('Color set to invalid value: %s' % repr(value)))\n    if (type(color) in (tuple, list)):\n        return pygame.Color(*color)\n    return color\n", "label": 0}
{"function": "\n\ndef initialize(self):\n    (self.handle, self.close_file) = base.open_resource(self.resource, 'w')\n    if self.html_header:\n        self.handle.write(self.html_header)\n    attr_string = ''\n    if self.table_attributes:\n        for attr_value in self.table_attributes.items():\n            attr_string += (' %s=\"%s\"\\n' % attr_value)\n    string = ('<table%s>\\n' % attr_string)\n    if self.write_headers:\n        string += '<tr>'\n        for field in self.fields:\n            if field.label:\n                header = field.label\n            else:\n                header = field.name\n            string += ('  <th>%s</th>\\n' % header)\n        string += '</tr>\\n'\n    self.handle.write(string)\n", "label": 0}
{"function": "\n\ndef build_files_status(self, repo):\n    status = ''\n    (untracked, unstaged, staged) = self.get_files_status(repo)\n    if ((not untracked) and (not unstaged) and (not staged)):\n        status += (GIT_WORKING_DIR_CLEAN + '\\n')\n    if untracked:\n        status += SECTIONS[UNTRACKED_FILES]\n        for (s, f) in untracked:\n            status += ('\\t%s\\n' % f.strip())\n        status += '\\n'\n    if unstaged:\n        status += (SECTIONS[UNSTAGED_CHANGES] if staged else SECTIONS[CHANGES])\n        for (s, f) in unstaged:\n            status += ('\\t%s %s\\n' % (STATUS_LABELS[s], f))\n        status += '\\n'\n    if staged:\n        status += SECTIONS[STAGED_CHANGES]\n        for (s, f) in staged:\n            status += ('\\t%s %s\\n' % (STATUS_LABELS[s], f))\n        status += '\\n'\n    return status\n", "label": 1}
{"function": "\n\ndef test_order_rewrite_literal(self):\n    'Tests rewrite of literal values'\n    l = ast.Literal('foo')\n    v = ast.Literal('bar')\n    v2 = ast.Literal('zip')\n    cmp1 = ast.CompareOperator('<', l, v)\n    cmp2 = ast.CompareOperator('>=', l, v)\n    cmp3 = ast.CompareOperator('>', l, v2)\n    cmp4 = ast.CompareOperator('<=', l, v2)\n    or1 = ast.LogicalOperator('or', cmp1, cmp2)\n    or2 = ast.LogicalOperator('or', cmp3, cmp4)\n    or3 = ast.LogicalOperator('or', or1, or2)\n    name = merge.node_name(cmp1, True)\n    r = compare.order_rewrite(ast.dup(or3), name, cmp1, True)\n    assert isinstance(r.left.left, ast.Constant)\n    assert (r.left.left.value == True)\n    assert isinstance(r.left.right, ast.Constant)\n    assert (r.left.right.value == False)\n    assert ASTPattern(or2).matches(r.right)\n    name = merge.node_name(cmp1, True)\n    r = compare.order_rewrite(ast.dup(or3), name, cmp1, False)\n    assert isinstance(r.left.left, ast.Constant)\n    assert (r.left.left.value == False)\n    assert isinstance(r.left.right, ast.Constant)\n    assert (r.left.right.value == True)\n    assert ASTPattern(or2).matches(r.right)\n", "label": 1}
{"function": "\n\ndef invitation_retrieve(request, response_format='html'):\n    'Retrieve invitation and create account'\n    if request.user.username:\n        return HttpResponseRedirect('/')\n    email = request.REQUEST.get('email', None)\n    key = request.REQUEST.get('key', None)\n    if (email and key):\n        try:\n            invitation = Invitation.objects.get(email=email, key=key)\n        except:\n            raise Http404\n    else:\n        raise Http404\n    if request.POST:\n        form = InvitationForm(invitation, request.POST)\n        if form.is_valid():\n            profile = form.save()\n            username = profile.user.username\n            password = form.cleaned_data['password']\n            user = authenticate(username=username, password=password)\n            if user:\n                invitation.delete()\n                login(request, user)\n                return HttpResponseRedirect('/')\n    else:\n        form = InvitationForm(invitation)\n    return render_to_response('core/invitation_retrieve', {\n        'invitation': invitation,\n        'form': form,\n    }, context_instance=RequestContext(request), response_format=response_format)\n", "label": 0}
{"function": "\n\n@contextfunction\ndef tags_box(context, object):\n    request = context['request']\n    response_format = 'html'\n    if ('response_format' in context):\n        response_format = context['response_format']\n    response_format_tags = response_format\n    if ('response_format_tags' in context):\n        response_format_tags = context['response_format_tags']\n    tags = object.tags.all()\n    form = None\n    if ('tags-edit' in request.GET):\n        if (request.POST.get('tags_object', 0) == unicode(object.id)):\n            form = TagsForm(tags, request.POST)\n            if form.is_valid():\n                if ('multicomplete_tags' in request.POST):\n                    tag_names = request.POST.get('multicomplete_tags').split(',')\n                    new_tags = []\n                    for name in tag_names:\n                        name = name.strip()\n                        if name:\n                            try:\n                                tag = Tag.objects.get(name=name)\n                            except Tag.DoesNotExist:\n                                tag = Tag(name=name)\n                                tag.save()\n                            new_tags.append(tag)\n                else:\n                    new_tags = form.is_valid()\n                object.tags.clear()\n                for tag in new_tags:\n                    object.tags.add(tag)\n                tags = object.tags.all()\n                form = None\n        else:\n            form = TagsForm(tags)\n    context = {\n        'object': object,\n        'tags': tags,\n        'form': form,\n        'editlink': (request.path + '?tags-edit'),\n    }\n    if ('ajax' in response_format_tags):\n        context = converter.preprocess_context(context)\n    return Markup(render_to_string('core/tags/tags_box', context, context_instance=RequestContext(request), response_format=response_format))\n", "label": 1}
{"function": "\n\n@spin_first\ndef shutdown(self, targets=None, restart=False, hub=False, block=None):\n    'Terminates one or more engine processes, optionally including the hub.'\n    block = (self.block if (block is None) else block)\n    if hub:\n        targets = 'all'\n    targets = self._build_targets(targets)[0]\n    for t in targets:\n        self.session.send(self._control_socket, 'shutdown_request', content={\n            'restart': restart,\n        }, ident=t)\n    error = False\n    if (block or hub):\n        self._flush_ignored_control()\n        for i in range(len(targets)):\n            (idents, msg) = self.session.recv(self._control_socket, 0)\n            if self.debug:\n                pprint(msg)\n            if (msg['content']['status'] != 'ok'):\n                error = self._unwrap_exception(msg['content'])\n    else:\n        self._ignored_control_replies += len(targets)\n    if hub:\n        time.sleep(0.25)\n        self.session.send(self._query_socket, 'shutdown_request')\n        (idents, msg) = self.session.recv(self._query_socket, 0)\n        if self.debug:\n            pprint(msg)\n        if (msg['content']['status'] != 'ok'):\n            error = self._unwrap_exception(msg['content'])\n    if error:\n        raise error\n", "label": 1}
{"function": "\n\ndef reset(self, url, data=None, query=None, method='GET', encoding=DEFAULT_ENCODING, language=DEFAULT_LANGUAGE, useragent=DEFAULT_UA, accept=DEFAULT_ACCEPT, extraheaders=None, strict=True, proxy=None, cookiejar=None, accept_encoding=None):\n    self.clear_state()\n    self._url = urlparse.UniversalResourceLocator(url, strict)\n    self._proxy = proxy\n    if accept_encoding:\n        assert (accept_encoding in ('identity', 'gzip', 'deflate'))\n        self._accept_encoding = accept_encoding\n    h = httputils.Headers()\n    if cookiejar:\n        c = cookiejar.get_cookie(self._url)\n        if c:\n            h.append(c)\n    h.append(httputils.UserAgent(useragents.USER_AGENTS.get(useragent, useragent)))\n    h.append(httputils.Accept(accept))\n    h.append(httputils.AcceptLanguage(language))\n    h.append(httputils.AcceptCharset(('%s,*;q=0.7' % (encoding,))))\n    self._encoding = encoding\n    self._language = language\n    self._query = query\n    self._headers = h\n    if (extraheaders and isinstance(extraheaders, list)):\n        self._headers.extend(map(httputils.get_header, extraheaders))\n    if data:\n        self.set_data(data)\n    else:\n        self._data = {\n            \n        }\n    self.set_method(method)\n", "label": 0}
{"function": "\n\ndef search(self):\n    sqs = super(AdvancedSearchForm, self).search()\n    if (not self.is_valid()):\n        return sqs\n    if self.cleaned_data['language']:\n        sqs = sqs.filter(language=self.cleaned_data['language'].name)\n    if self.cleaned_data['version']:\n        sqs = sqs.filter(version__in=self.cleaned_data['version'])\n    if self.cleaned_data['minimum_pub_date']:\n        sqs = sqs.filter(pub_date__gte=self.cleaned_data['minimum_pub_date'])\n    if self.cleaned_data['minimum_bookmark_count']:\n        sqs = sqs.filter(bookmark_count__gte=self.cleaned_data['minimum_bookmark_count'])\n    if self.cleaned_data['minimum_rating_score']:\n        sqs = sqs.filter(rating_score__gte=self.cleaned_data['minimum_rating_score'])\n    return sqs\n", "label": 0}
{"function": "\n\ndef test_Add_is_pos_neg():\n    n = Symbol('n', negative=True, infinite=True)\n    nn = Symbol('n', nonnegative=True, infinite=True)\n    np = Symbol('n', nonpositive=True, infinite=True)\n    p = Symbol('p', positive=True, infinite=True)\n    r = Dummy(real=True, finite=False)\n    x = Symbol('x')\n    xf = Symbol('xb', finite=True)\n    assert ((n + p).is_positive is None)\n    assert ((n + x).is_positive is None)\n    assert ((p + x).is_positive is None)\n    assert ((n + p).is_negative is None)\n    assert ((n + x).is_negative is None)\n    assert ((p + x).is_negative is None)\n    assert ((n + xf).is_positive is False)\n    assert ((p + xf).is_positive is True)\n    assert ((n + xf).is_negative is True)\n    assert ((p + xf).is_negative is False)\n    assert ((x - S.Infinity).is_negative is None)\n    assert (p + nn).is_positive\n    assert (n + np).is_negative\n    assert ((p + r).is_positive is None)\n", "label": 1}
{"function": "\n\ndef to_python(value):\n    if (value in validators.EMPTY_VALUES):\n        phone_number = value\n    elif (value and isinstance(value, string_types)):\n        try:\n            phone_number = PhoneNumber.from_string(phone_number=value)\n        except NumberParseException:\n            phone_number = PhoneNumber(raw_input=value)\n    elif (isinstance(value, phonenumbers.PhoneNumber) and (not isinstance(value, PhoneNumber))):\n        phone_number = PhoneNumber()\n        phone_number.merge_from(value)\n    elif isinstance(value, PhoneNumber):\n        phone_number = value\n    else:\n        phone_number = None\n    return phone_number\n", "label": 0}
{"function": "\n\ndef test_issues_8632_8633_8638_8675_8992():\n    p = Dummy(integer=True, positive=True)\n    nn = Dummy(integer=True, nonnegative=True)\n    assert (p - S.Half).is_positive\n    assert (p - 1).is_nonnegative\n    assert (nn + 1).is_positive\n    assert ((- p) + 1).is_nonpositive\n    assert ((- nn) - 1).is_negative\n    prime = Dummy(prime=True)\n    assert (prime - 2).is_nonnegative\n    assert ((prime - 3).is_nonnegative is None)\n    even = Dummy(positive=True, even=True)\n    assert (even - 2).is_nonnegative\n    p = Dummy(positive=True)\n    assert ((p / (p + 1)) - 1).is_negative\n    assert (((p + 2) ** 3) - S.Half).is_positive\n    n = Dummy(negative=True)\n    assert (n - 3).is_nonpositive\n", "label": 1}
{"function": "\n\ndef test_setValue(self, spinbox):\n    assert spinbox.setValue(10)\n    assert (spinbox.value() == 10)\n    assert spinbox.setValue((18446744073709551615 + 1))\n    assert (spinbox.value() == spinbox.maximum())\n    assert spinbox.setValue(((- 18446744073709551616) - 1))\n    assert (spinbox.value() == spinbox.minimum())\n", "label": 0}
{"function": "\n\ndef visit_QDockArea(self, area):\n    ' Visit a QDockArea node.\\n\\n        This visitor generates an AreaLayout for the area and pushes\\n        it onto the stack.\\n\\n        '\n    central = area.centralWidget()\n    if (central is None):\n        layout = AreaLayout()\n    else:\n        self.visit(central)\n        layout = AreaLayout(self.stack.pop())\n    bar_data = defaultdict(list)\n    for (container, position) in area.dockBarContainers():\n        bar_data[position].append(container.objectName())\n    for (bar_pos, names) in bar_data.iteritems():\n        bar = DockBarLayout(*names, position=self.BAR_POSITIONS[bar_pos])\n        layout.dock_bars.append(bar)\n    maxed = area.maximizedWidget()\n    if (maxed is not None):\n        name = maxed.objectName()\n        for item in layout.find_all(ItemLayout):\n            if (item.name == name):\n                item.maximized = True\n                break\n    self.stack.append(layout)\n", "label": 0}
{"function": "\n\ndef generate_mount_url(self, regex, v_or_f, mod):\n    \"The self.mounts can be None, which means no url generation.\\n\\n        url is being managed by urlpatterns.\\n        else self.mounts is a dict, containing app name and where to mount\\n        if where it mount is None then again don't mount this fellow.\\n        \"\n    if (getattr(self, 'mounts', None) is None):\n        return\n    if (not regex.startswith('/')):\n        return regex\n    if (not mod):\n        if isinstance(v_or_f, basestring):\n            mod = v_or_f\n        else:\n            mod = v_or_f.__module__\n    (best_k, best_v) = ('', None)\n    for (k, v) in tuple(self.mounts.items()):\n        if (mod.startswith(k) and (len(k) > len(best_k))):\n            (best_k, best_v) = (k, v)\n    if best_k:\n        if (not best_v):\n            return\n        if (not best_v.endswith('/')):\n            best_k += '/'\n        if (best_v != '/'):\n            regex = (best_v[:(- 1)] + regex)\n    return regex\n", "label": 1}
{"function": "\n\ndef _split_mul(f, x):\n    '\\n    Split expression ``f`` into fac, po, g, where fac is a constant factor,\\n    po = x**s for some s independent of s, and g is \"the rest\".\\n\\n    >>> from sympy.integrals.meijerint import _split_mul\\n    >>> from sympy import sin\\n    >>> from sympy.abc import s, x\\n    >>> _split_mul((3*x)**s*sin(x**2)*x, x)\\n    (3**s, x*x**s, sin(x**2))\\n    '\n    from sympy import polarify, unpolarify\n    fac = S(1)\n    po = S(1)\n    g = S(1)\n    f = expand_power_base(f)\n    args = Mul.make_args(f)\n    for a in args:\n        if (a == x):\n            po *= x\n        elif (x not in a.free_symbols):\n            fac *= a\n        else:\n            if (a.is_Pow and (x not in a.exp.free_symbols)):\n                (c, t) = a.base.as_coeff_mul(x)\n                if (t != (x,)):\n                    (c, t) = expand_mul(a.base).as_coeff_mul(x)\n                if (t == (x,)):\n                    po *= (x ** a.exp)\n                    fac *= unpolarify(polarify((c ** a.exp), subs=False))\n                    continue\n            g *= a\n    return (fac, po, g)\n", "label": 0}
{"function": "\n\ndef generate_column_map(self):\n    column_map = []\n    models = set([self.model])\n    for (i, node) in enumerate(self.column_meta):\n        attr = conv = None\n        if isinstance(node, Field):\n            if isinstance(node, FieldProxy):\n                key = node._model_alias\n                constructor = node.model\n                conv = node.field_instance.python_value\n            else:\n                key = constructor = node.model_class\n                conv = node.python_value\n            attr = (node._alias or node.name)\n        else:\n            if (node._bind_to is None):\n                key = constructor = self.model\n            else:\n                key = constructor = node._bind_to\n            if (isinstance(node, Node) and node._alias):\n                attr = node._alias\n            elif isinstance(node, Entity):\n                attr = node.path[(- 1)]\n        column_map.append((key, constructor, attr, conv))\n        models.add(key)\n    return (column_map, models)\n", "label": 1}
{"function": "\n\ndef makeLines(self, diff, lines_to_add, comment_lists):\n    lines = []\n    for (old, new) in lines_to_add:\n        context = self.makeContext(diff, old[0], new[0])\n        lines.append(SideDiffLine(self.app, context, old, new, callback=self.onSelect))\n        key = ('old-%s-%s' % (old[0], diff.oldname))\n        old_list = comment_lists.pop(key, [])\n        key = ('new-%s-%s' % (new[0], diff.newname))\n        new_list = comment_lists.pop(key, [])\n        while (old_list or new_list):\n            old_comment_key = new_comment_key = None\n            old_comment = new_comment = ''\n            if old_list:\n                (old_comment_key, old_comment) = old_list.pop(0)\n            if new_list:\n                (new_comment_key, new_comment) = new_list.pop(0)\n            lines.append(SideDiffComment(context, old_comment, new_comment))\n        key = ('olddraft-%s-%s' % (old[0], diff.oldname))\n        old_list = comment_lists.pop(key, [])\n        key = ('newdraft-%s-%s' % (new[0], diff.newname))\n        new_list = comment_lists.pop(key, [])\n        while (old_list or new_list):\n            old_comment_key = new_comment_key = None\n            old_comment = new_comment = ''\n            if old_list:\n                (old_comment_key, old_comment) = old_list.pop(0)\n            if new_list:\n                (new_comment_key, new_comment) = new_list.pop(0)\n            lines.append(SideDiffCommentEdit(self.app, context, old_comment_key, new_comment_key, old_comment, new_comment))\n    return lines\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('i', range(10))\ndef test_simple_size_estimation(self, space, i):\n    class_node = mapdict.ClassNode(i)\n    assert (class_node.size_estimate.object_size_estimate() == 0)\n    assert (class_node.size_estimate.unboxed_size_estimate() == 0)\n    for j in range(1000):\n        w_obj = FakeObject(class_node)\n        for a in 'abcdefghij'[:i]:\n            w_obj.map = w_obj.map.add(space, mapdict.ObjectAttributeNode, a, w_obj)\n    assert (class_node.size_estimate.object_size_estimate() == i)\n    assert (class_node.size_estimate.unboxed_size_estimate() == 0)\n", "label": 0}
{"function": "\n\ndef begin_text_resource(self, resource, text):\n    'Event hook for processing an individual resource.\\n\\n        If the input resource is a sphinx input file, this method will replace\\n        replace the text of the file with the sphinx-generated documentation.\\n\\n        Sphinx itself is run lazily the first time this method is called.\\n        This means that if no sphinx-related resources need updating, then\\n        we entirely avoid running sphinx.\\n        '\n    suffix = self.sphinx_config.get('source_suffix', '.rst')\n    if (not resource.source_file.path.endswith(suffix)):\n        return text\n    if (self.sphinx_build_dir is None):\n        self._run_sphinx()\n    output = []\n    settings = self.settings\n    sphinx_output = self._get_sphinx_output(resource)\n    if (not settings.block_map):\n        output.append(sphinx_output['body'])\n    else:\n        for (nm, content) in iteritems(sphinx_output):\n            try:\n                block = getattr(settings.block_map, nm)\n            except AttributeError:\n                pass\n            else:\n                output.append(('{%% block %s %%}' % (block,)))\n                output.append(content)\n                output.append('{% endblock %}')\n    return '\\n'.join(output)\n", "label": 0}
{"function": "\n\ndef write_test_serialisation(self, tests, datafile):\n    arr = []\n    for t in tests:\n        exe = t.get_exe()\n        if isinstance(exe, dependencies.ExternalProgram):\n            fname = exe.fullpath\n        else:\n            fname = [os.path.join(self.environment.get_build_dir(), self.get_target_filename(t.get_exe()))]\n        is_cross = (self.environment.is_cross_build() and self.environment.cross_info.need_cross_compiler())\n        if is_cross:\n            exe_wrapper = self.environment.cross_info.config['binaries'].get('exe_wrapper', None)\n        else:\n            exe_wrapper = None\n        if mesonlib.is_windows():\n            extra_paths = self.determine_windows_extra_paths(exe)\n        else:\n            extra_paths = []\n        cmd_args = []\n        for a in t.cmd_args:\n            if isinstance(a, mesonlib.File):\n                a = os.path.join(self.environment.get_build_dir(), a.rel_to_builddir(self.build_to_src))\n            cmd_args.append(a)\n        ts = TestSerialisation(t.get_name(), t.suite, fname, is_cross, exe_wrapper, t.is_parallel, cmd_args, t.env, t.should_fail, t.valgrind_args, t.timeout, t.workdir, extra_paths)\n        arr.append(ts)\n    pickle.dump(arr, datafile)\n", "label": 0}
{"function": "\n\ndef _fetch_cdn_data(self):\n    \"Fetches the container's CDN data from the CDN service\"\n    if (self._cdn_enabled is FAULT):\n        headers = self.manager.fetch_cdn_data(self)\n    else:\n        headers = {\n            \n        }\n    self._set_cdn_defaults()\n    if (not headers):\n        return\n    else:\n        self._cdn_enabled = True\n    for (key, value) in headers.items():\n        low_key = key.lower()\n        if (low_key == 'x-cdn-uri'):\n            self._cdn_uri = value\n        elif (low_key == 'x-ttl'):\n            self._cdn_ttl = int(value)\n        elif (low_key == 'x-cdn-ssl-uri'):\n            self._cdn_ssl_uri = value\n        elif (low_key == 'x-cdn-streaming-uri'):\n            self._cdn_streaming_uri = value\n        elif (low_key == 'x-cdn-ios-uri'):\n            self._cdn_ios_uri = value\n        elif (low_key == 'x-log-retention'):\n            self._cdn_log_retention = (value == 'True')\n", "label": 1}
{"function": "\n\ndef _hijack_tty(self, pumps):\n    with tty.Terminal(self.operation.stdin, raw=self.operation.israw()):\n        self.resize()\n        while True:\n            read_pumps = [p for p in pumps if (not p.eof)]\n            write_streams = [p.to_stream for p in pumps if p.to_stream.needs_write()]\n            (read_ready, write_ready) = io.select(read_pumps, write_streams, timeout=60)\n            try:\n                for write_stream in write_ready:\n                    write_stream.do_write()\n                for pump in read_ready:\n                    pump.flush()\n                if all([p.is_done() for p in pumps]):\n                    break\n            except SSLError as e:\n                if ('The operation did not complete' not in e.strerror):\n                    raise e\n", "label": 1}
{"function": "\n\ndef _do_ssl_handshake(self):\n    try:\n        self.socket.do_handshake()\n    except ssl.SSLError as err:\n        if (err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE)):\n            return\n        elif (err.args[0] == ssl.SSL_ERROR_EOF):\n            return self.handle_close()\n        raise\n    except socket.error as err:\n        if (err.args[0] == errno.ECONNABORTED):\n            return self.handle_close()\n    else:\n        self._ssl_accepting = False\n", "label": 0}
{"function": "\n\ndef reload_constraints():\n    '\\n    Parse SWIFT_CONF_FILE and reset module level global contraint attrs,\\n    populating OVERRIDE_CONSTRAINTS AND EFFECTIVE_CONSTRAINTS along the way.\\n    '\n    global SWIFT_CONSTRAINTS_LOADED, OVERRIDE_CONSTRAINTS\n    SWIFT_CONSTRAINTS_LOADED = False\n    OVERRIDE_CONSTRAINTS = {\n        \n    }\n    constraints_conf = ConfigParser()\n    if constraints_conf.read(utils.SWIFT_CONF_FILE):\n        SWIFT_CONSTRAINTS_LOADED = True\n        for name in DEFAULT_CONSTRAINTS:\n            try:\n                value = constraints_conf.get('swift-constraints', name)\n            except NoOptionError:\n                pass\n            except NoSectionError:\n                break\n            else:\n                try:\n                    value = int(value)\n                except ValueError:\n                    value = utils.list_from_csv(value)\n                OVERRIDE_CONSTRAINTS[name] = value\n    for (name, default) in DEFAULT_CONSTRAINTS.items():\n        value = OVERRIDE_CONSTRAINTS.get(name, default)\n        EFFECTIVE_CONSTRAINTS[name] = value\n        globals()[name.upper()] = value\n", "label": 0}
{"function": "\n\ndef run(self):\n    events = []\n    while (not self._stopping):\n        asap = False\n        try:\n            events = self.poll(TIMEOUT_PRECISION)\n        except (OSError, IOError) as e:\n            if (errno_from_exception(e) in (errno.EPIPE, errno.EINTR)):\n                asap = True\n                logging.debug('poll:%s', e)\n            else:\n                logging.error('poll:%s', e)\n                import traceback\n                traceback.print_exc()\n                continue\n        for (sock, fd, event) in events:\n            handler = self._fdmap.get(fd, None)\n            if (handler is not None):\n                handler = handler[1]\n                try:\n                    handler.handle_event(sock, fd, event)\n                except (OSError, IOError) as e:\n                    shell.print_exception(e)\n        now = time.time()\n        if (asap or ((now - self._last_time) >= TIMEOUT_PRECISION)):\n            for callback in self._periodic_callbacks:\n                callback()\n            self._last_time = now\n", "label": 1}
{"function": "\n\ndef append_dialog(self, paragraphs):\n    if (len(self.lines) < 2):\n        return False\n    character = self.lines[0]\n    if character.endswith(TWOSPACE):\n        return False\n    if (character.startswith('@') and (len(character) >= 2)):\n        character = character[1:]\n    elif (not character.isupper()):\n        return False\n    if (paragraphs and isinstance(paragraphs[(- 1)], Dialog)):\n        dual_match = dual_dialog_re.match(character)\n        if dual_match:\n            previous = paragraphs.pop()\n            dialog = self._create_dialog(dual_match.group(1))\n            paragraphs.append(DualDialog(previous, dialog))\n            return True\n    paragraphs.append(self._create_dialog(character))\n    return True\n", "label": 1}
{"function": "\n\ndef checkstyle(self, targets, sources):\n    union_classpath = OrderedSet(self.tool_classpath('checkstyle'))\n    if self.get_options().include_user_classpath:\n        runtime_classpaths = self.context.products.get_data('runtime_classpath')\n        for target in targets:\n            runtime_classpath = runtime_classpaths.get_for_targets(target.closure(bfs=True))\n            union_classpath.update((jar for (conf, jar) in runtime_classpath if (conf in self.get_options().confs)))\n    args = ['-c', self.get_options().configuration, '-f', 'plain']\n    if self.get_options().properties:\n        properties_file = os.path.join(self.workdir, 'checkstyle.properties')\n        with safe_open(properties_file, 'w') as pf:\n            for (k, v) in self.get_options().properties.items():\n                pf.write('{key}={value}\\n'.format(key=k, value=v))\n        args.extend(['-p', properties_file])\n\n    def call(xargs):\n        return self.runjava(classpath=union_classpath, main=self._CHECKSTYLE_MAIN, jvm_options=self.get_options().jvm_options, args=(args + xargs), workunit_name='checkstyle')\n    checks = Xargs(call)\n    return checks.execute(sources)\n", "label": 0}
{"function": "\n\ndef fix_requirements(f):\n    requirements = []\n    before = []\n    after = []\n    for line in f:\n        before.append(line)\n        if ((not len(requirements)) or (requirements[(- 1)].value is not None)):\n            requirements.append(Requirement())\n        requirement = requirements[(- 1)]\n        if ((len(requirements) == 1) and (line.strip() == b'')):\n            if (len(requirement.comments) and requirement.comments[0].startswith(b'#')):\n                requirement.value = b'\\n'\n            else:\n                requirement.comments.append(line)\n        elif (line.startswith(b'#') or (line.strip() == b'')):\n            requirement.comments.append(line)\n        else:\n            requirement.value = line\n    for requirement in sorted(requirements):\n        for comment in requirement.comments:\n            after.append(comment)\n        after.append(requirement.value)\n    before_string = b''.join(before)\n    after_string = b''.join(after)\n    if (before_string == after_string):\n        return 0\n    else:\n        f.seek(0)\n        f.write(after_string)\n        f.truncate()\n        return 1\n", "label": 1}
{"function": "\n\n@classmethod\ndef _get_task_data(cls, slug):\n    game = _get_game(slug)\n    user = _get_user_name()\n    try:\n        data = _json_decoder.decode(request.POST['data'])\n    except KeyError:\n        raise BadRequest('Missing parameter \"data\"')\n    except JSONDecodeError as e:\n        raise BadRequest(('Data-parameter JSON error: %s' % str(e)))\n    if (not isinstance(data, dict)):\n        raise BadRequest('Data-parameter is not a dict')\n    get_data = data.get\n    key = get_data('key')\n    if (not key):\n        raise BadRequest('No notification-key given')\n    if (key not in GameNotificationKeysList.get(game).to_dict()):\n        raise BadRequest((('Unknown key \"' + key) + '\" given.'))\n    msg = get_data('msg')\n    if (not msg):\n        raise BadRequest('No message given')\n    if (not msg.get('text')):\n        raise BadRequest('No text-attribute in msg')\n    try:\n        delay = int((get_data('time') or 0))\n    except ValueError:\n        raise BadRequest('Incorrect format for time')\n    recipient = (get_data('recipient', '').strip() or user)\n    return (create_id(), key, user, recipient, msg, game, delay)\n", "label": 1}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef kill_process(self, process, stop_signal=None, graceful_timeout=None):\n    'Kill process (stop_signal, graceful_timeout then SIGKILL)\\n        '\n    if (stop_signal is None):\n        stop_signal = self.stop_signal\n    if (graceful_timeout is None):\n        graceful_timeout = self.graceful_timeout\n    if process.stopping:\n        raise gen.Return(False)\n    try:\n        logger.debug('%s: kill process %s', self.name, process.pid)\n        if self.stop_children:\n            self.send_signal_process(process, stop_signal)\n        else:\n            self.send_signal(process.pid, stop_signal)\n            self.notify_event('kill', {\n                'process_pid': process.pid,\n                'time': time.time(),\n            })\n    except NoSuchProcess:\n        raise gen.Return(False)\n    process.stopping = True\n    waited = 0\n    while (waited < graceful_timeout):\n        if (not process.is_alive()):\n            break\n        (yield tornado_sleep(0.1))\n        waited += 0.1\n    if (waited >= graceful_timeout):\n        if hasattr(signal, 'SIGKILL'):\n            self.send_signal_process(process, signal.SIGKILL)\n    if self.stream_redirector:\n        self.stream_redirector.remove_redirections(process)\n    process.stopping = False\n    process.stop()\n    raise gen.Return(True)\n", "label": 1}
{"function": "\n\n@classdef.method('[]=')\n@check_frozen()\ndef method_subscript_assign(self, space, w_idx, w_count_or_obj, w_obj=None):\n    w_count = None\n    if w_obj:\n        w_count = w_count_or_obj\n    else:\n        w_obj = w_count_or_obj\n    (start, end, as_range, _) = space.subscript_access(self.length(), w_idx, w_count=w_count)\n    if (w_count and (end < start)):\n        raise space.error(space.w_IndexError, ('negative length (%d)' % (end - start)))\n    elif (start < 0):\n        raise space.error(space.w_IndexError, ('index %d too small for array; minimum: %d' % ((start - self.length()), (- self.length()))))\n    elif as_range:\n        w_converted = space.convert_type(w_obj, space.w_array, 'to_ary', raise_error=False)\n        if (w_converted is space.w_nil):\n            rep_w = [w_obj]\n        else:\n            rep_w = space.listview(w_converted)\n        self._subscript_assign_range(space, start, end, rep_w)\n    elif (start >= self.length()):\n        self.items_w += ([space.w_nil] * ((start - self.length()) + 1))\n        self.items_w[start] = w_obj\n    else:\n        self.items_w[start] = w_obj\n    return w_obj\n", "label": 0}
{"function": "\n\ndef _compare(a, b):\n    global last_keys\n    skip_types = ['whitener', 'proj', 'reginv', 'noisenorm', 'nchan', 'command_line', 'working_dir', 'mri_file', 'mri_id']\n    try:\n        if isinstance(a, (dict, Info)):\n            assert_true(isinstance(b, (dict, Info)))\n            for (k, v) in six.iteritems(a):\n                if ((k not in b) and (k not in skip_types)):\n                    raise ValueError((\"First one had one second one didn't:\\n%s not in %s\" % (k, b.keys())))\n                if (k not in skip_types):\n                    last_keys.pop()\n                    last_keys = ([k] + last_keys)\n                    _compare(v, b[k])\n            for (k, v) in six.iteritems(b):\n                if ((k not in a) and (k not in skip_types)):\n                    raise ValueError((\"Second one had one first one didn't:\\n%s not in %s\" % (k, a.keys())))\n        elif isinstance(a, list):\n            assert_true((len(a) == len(b)))\n            for (i, j) in zip(a, b):\n                _compare(i, j)\n        elif isinstance(a, sparse.csr.csr_matrix):\n            assert_array_almost_equal(a.data, b.data)\n            assert_equal(a.indices, b.indices)\n            assert_equal(a.indptr, b.indptr)\n        elif isinstance(a, np.ndarray):\n            assert_array_almost_equal(a, b)\n        else:\n            assert_true((a == b))\n    except Exception as exptn:\n        print(last_keys)\n        raise exptn\n", "label": 1}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef manage_processes(self):\n    'Manage processes.'\n    if self.is_stopped():\n        return\n    for process in list(self.processes.values()):\n        if (process.status in (DEAD_OR_ZOMBIE, UNEXISTING)):\n            self.processes.pop(process.pid)\n    if self.max_age:\n        (yield self.remove_expired_processes())\n    if ((len(self.processes) < self.numprocesses) and (not self.is_stopping())):\n        if self.respawn:\n            (yield self.spawn_processes())\n        elif ((not len(self.processes)) and (not self.on_demand)):\n            (yield self._stop())\n    if (len(self.processes) > self.numprocesses):\n        processes_to_kill = []\n        for process in sorted(self.processes.values(), key=(lambda process: process.started), reverse=True)[self.numprocesses:]:\n            if (process.status in (DEAD_OR_ZOMBIE, UNEXISTING)):\n                self.processes.pop(process.pid)\n            else:\n                processes_to_kill.append(process)\n        removes = (yield [self.kill_process(process) for process in processes_to_kill])\n        for (i, process) in enumerate(processes_to_kill):\n            if removes[i]:\n                self.processes.pop(process.pid)\n", "label": 1}
{"function": "\n\ndef stripEmptyGeometry(mesh):\n    to_delete = []\n    geoms_to_delete = []\n    for (i, geom) in enumerate(mesh.geometries):\n        if (len(geom.primitives) == 0):\n            to_delete.append(i)\n            geoms_to_delete.append(geom)\n    for scene in mesh.scenes:\n        nodes_to_check = []\n        nodes_to_check.extend(scene.nodes)\n        while (len(nodes_to_check) > 0):\n            curnode = nodes_to_check.pop()\n            scene_nodes_to_delete = []\n            for (i, node) in enumerate(curnode.children):\n                if isinstance(node, collada.scene.Node):\n                    nodes_to_check.append(node)\n                elif isinstance(node, collada.scene.GeometryNode):\n                    if (node.geometry in geoms_to_delete):\n                        scene_nodes_to_delete.append(i)\n            scene_nodes_to_delete.sort(reverse=True)\n            for i in scene_nodes_to_delete:\n                del curnode.children[i]\n    to_delete.sort(reverse=True)\n    for i in to_delete:\n        del mesh.geometries[i]\n", "label": 1}
{"function": "\n\ndef execute(self, service, shared_data):\n    osutils = osutils_factory.get_os_utils()\n    network_details = service.get_network_details()\n    if (not network_details):\n        return (plugin_base.PLUGIN_EXECUTION_DONE, False)\n    network_adapters = osutils.get_network_adapters()\n    network_details = _preprocess_nics(network_details, network_adapters)\n    macnics = {\n        \n    }\n    for nic in network_details:\n        macnics[nic.mac] = nic\n    adapter_macs = [pair[1] for pair in network_adapters]\n    reboot_required = False\n    configured = False\n    for mac in adapter_macs:\n        nic = macnics.pop(mac, None)\n        if (not nic):\n            LOG.warn('Missing details for adapter %s', mac)\n            continue\n        LOG.info('Configuring network adapter %s', mac)\n        reboot = osutils.set_static_network_config(mac, nic.address, nic.netmask, nic.broadcast, nic.gateway, nic.dnsnameservers)\n        reboot_required = (reboot or reboot_required)\n        if (nic.address6 and nic.netmask6):\n            osutils.set_static_network_config_v6(mac, nic.address6, nic.netmask6, nic.gateway6)\n        configured = True\n    for mac in macnics:\n        LOG.warn('Details not used for adapter %s', mac)\n    if (not configured):\n        LOG.error('No adapters were configured')\n    return (plugin_base.PLUGIN_EXECUTION_DONE, reboot_required)\n", "label": 1}
{"function": "\n\ndef save(self, verbosity=1, *args, **kwargs):\n    is_new = (self.pk is None)\n    if (is_new and (connection.schema_name != get_public_schema_name())):\n        raise Exception((\"Can't create tenant outside the public schema. Current schema is %s.\" % connection.schema_name))\n    elif ((not is_new) and (connection.schema_name not in (self.schema_name, get_public_schema_name()))):\n        raise Exception((\"Can't update tenant outside it's own schema or the public schema. Current schema is %s.\" % connection.schema_name))\n    super(TenantMixin, self).save(*args, **kwargs)\n    if (is_new and self.auto_create_schema):\n        try:\n            self.create_schema(check_if_exists=True, verbosity=verbosity)\n            post_schema_sync.send(sender=TenantMixin, tenant=self)\n        except Exception as e:\n            self.delete(force_drop=True)\n            raise\n    elif is_new:\n        schema_needs_to_be_sync.send(sender=TenantMixin, tenant=self)\n", "label": 1}
{"function": "\n\ndef query(query_file_name, days=DEFAULT_NUMBER_OF_DAYS, es_url=er.ES_URL, quantity=DEFAULT_MAX_QUANTITY, verbose=False):\n    es = er_results.SearchEngine(es_url)\n    with open(query_file_name) as f:\n        query_file = yaml.load(f.read())\n        query = query_file['query']\n    r = es.search(query, days=days)\n    print(('total hits: %s' % r.hits['total']))\n    attributes = {\n        \n    }\n    for hit in r.hits['hits']:\n        for (key, value) in hit['_source'].iteritems():\n            value_hash = json.dumps(value)\n            attributes.setdefault(key, {\n                \n            }).setdefault(value_hash, 0)\n            attributes[key][value_hash] += 1\n    analysis = analyze_attributes(attributes)\n    for (attribute, results) in sorted(analysis.iteritems()):\n        if ((not verbose) and (attribute in IGNORED_ATTRIBUTES)):\n            continue\n        print(attribute)\n        for (percentage, value) in itertools.islice(results, quantity):\n            if isinstance(value, list):\n                value = ' '.join((unicode(x) for x in value))\n            print(('  %d%% %s' % (percentage, value)))\n", "label": 1}
{"function": "\n\ndef test_reductions():\n    ((chunk, chunk_expr), (agg, agg_expr)) = split(t, t.amount.nunique())\n    assert (chunk.schema == t.schema)\n    assert chunk_expr.isidentical(chunk.amount.distinct())\n    assert isscalar(agg.dshape.measure)\n    assert agg_expr.isidentical(agg.distinct().count())\n    ((chunk, chunk_expr), (agg, agg_expr)) = split(t, t.amount.nunique(keepdims=True))\n    assert (chunk.schema == t.schema)\n    assert chunk_expr.isidentical(chunk.amount.distinct())\n    assert isscalar(agg.dshape.measure)\n    assert agg_expr.isidentical(agg.distinct().count(keepdims=True))\n", "label": 1}
{"function": "\n\ndef __init__(self, name_or_email, email=None, encoding='utf-8'):\n    self.encoding = encoding\n    if (email is None):\n        if isinstance(name_or_email, AddressList):\n            if (not (0 < len(name_or_email) < 2)):\n                raise ValueError('AddressList to convert must only contain a single Address.')\n            name_or_email = unicode(name_or_email[0])\n        if isinstance(name_or_email, (tuple, list)):\n            self.name = unicodestr(name_or_email[0], encoding)\n            self.address = unicodestr(name_or_email[1], encoding)\n        elif isinstance(name_or_email, bytes):\n            (self.name, self.address) = parseaddr(unicodestr(name_or_email, encoding))\n        elif isinstance(name_or_email, unicode):\n            (self.name, self.address) = parseaddr(name_or_email)\n        else:\n            raise TypeError('Expected string, tuple or list, got {0} instead'.format(repr(type(name_or_email))))\n    else:\n        self.name = unicodestr(name_or_email, encoding)\n        self.address = unicodestr(email, encoding)\n    (email, err) = EmailValidator().validate_email(self.address)\n    if err:\n        raise ValueError('\"{0}\" is not a valid e-mail address: {1}'.format(email, err))\n", "label": 0}
{"function": "\n\ndef extract(self, to_path):\n    namelist = self._archive.namelist()\n    leading = self.has_leading_dir(namelist)\n    for name in namelist:\n        data = self._archive.read(name)\n        if leading:\n            name = self.split_leading_dir(name)[1]\n        filename = os.path.join(to_path, name)\n        dirname = os.path.dirname(filename)\n        if (dirname and (not os.path.exists(dirname))):\n            os.makedirs(dirname)\n        if filename.endswith(('/', '\\\\')):\n            if (not os.path.exists(filename)):\n                os.makedirs(filename)\n        else:\n            with open(filename, 'wb') as outfile:\n                outfile.write(data)\n", "label": 0}
{"function": "\n\n@wrap_exceptions\ndef set_process_ionice(self, ioclass, value):\n    if (ioclass in (IOPRIO_CLASS_NONE, None)):\n        if value:\n            raise ValueError(\"can't specify value with IOPRIO_CLASS_NONE\")\n        ioclass = IOPRIO_CLASS_NONE\n        value = 0\n    if (ioclass in (IOPRIO_CLASS_RT, IOPRIO_CLASS_BE)):\n        if (value is None):\n            value = 4\n    elif (ioclass == IOPRIO_CLASS_IDLE):\n        if value:\n            raise ValueError(\"can't specify value with IOPRIO_CLASS_IDLE\")\n        value = 0\n    else:\n        value = 0\n    if (not (0 <= value <= 8)):\n        raise ValueError('value argument range expected is between 0 and 8')\n    return _psutil_linux.ioprio_set(self.pid, ioclass, value)\n", "label": 0}
{"function": "\n\ndef make_node(self, x, index):\n    x = as_sparse_variable(x)\n    assert (x.format in ['csr', 'csc'])\n    assert (len(index) == 2)\n    input_op = [x]\n    for ind in index:\n        if isinstance(ind, slice):\n            raise Exception('GetItemScalar called with a slice as index!')\n        elif isinstance(ind, int):\n            ind = theano.tensor.constant(ind)\n            input_op += [ind]\n        elif (ind.ndim == 0):\n            input_op += [ind]\n        else:\n            raise NotImplemented()\n    return gof.Apply(self, input_op, [tensor.scalar(dtype=x.dtype)])\n", "label": 0}
{"function": "\n\ndef _translate_lods_suffix(self, tb, instruction, suffix):\n    if (self._arch_mode == ARCH_X86_MODE_32):\n        src = ReilRegisterOperand('esi', 32)\n    elif (self._arch_mode == ARCH_X86_MODE_64):\n        src = ReilRegisterOperand('rsi', 64)\n    else:\n        raise Exception('Invalid architecture mode: %d', self._arch_mode)\n    if (suffix == 'b'):\n        dst = ReilRegisterOperand('al', 8)\n    elif (suffix == 'w'):\n        dst = ReilRegisterOperand('ax', 16)\n    elif (suffix == 'd'):\n        dst = ReilRegisterOperand('eax', 32)\n    elif (suffix == 'q'):\n        dst = ReilRegisterOperand('rax', 64)\n    else:\n        raise Exception(('Invalid instruction suffix: %s' % suffix))\n    if instruction.prefix:\n        (counter, loop_start_lbl) = self._rep_prefix_begin(tb, instruction)\n    tb.add(self._builder.gen_ldm(src, dst))\n    self._update_strings_src(tb, src, dst.size, instruction)\n    if instruction.prefix:\n        self._rep_prefix_end(tb, instruction, counter, loop_start_lbl)\n", "label": 1}
{"function": "\n\ndef varbind_pretty_value(varbind):\n    output = varbind.value\n    objid = ObjectId(varbind.oid)\n    if (varbind.value_type == 'ipaddress'):\n        try:\n            name = socket.gethostbyaddr(varbind.value)[0]\n            output = ('%s (%s)' % (name, output))\n        except socket.error:\n            pass\n    elif (varbind.value_type == 'oid'):\n        output = to_mibname(varbind.value)\n    elif (varbind.value_type == 'octet'):\n        if (objid.textual == 'DateAndTime'):\n            output = decode_date(varbind.value)\n    if (objid.enums and varbind.value.isdigit()):\n        val = int(varbind.value)\n        output = objid.enums.get(val, val)\n    if objid.units:\n        output = ('%s %s' % (output, objid.units))\n    return output\n", "label": 1}
{"function": "\n\ndef clean(self, value):\n    super(CommaSeparatedUserField, self).clean(value)\n    names = set(value.split(','))\n    names_set = set([name.strip() for name in names])\n    users = list(get_user_model().objects.filter(username__in=names_set))\n    unknown_names = (names_set ^ set([user.username for user in users]))\n    recipient_filter = self._recipient_filter\n    invalid_users = []\n    if (recipient_filter is not None):\n        for r in users:\n            if (recipient_filter(r) is False):\n                users.remove(r)\n                invalid_users.append(r.username)\n    if (unknown_names or invalid_users):\n        humanized_usernames = ', '.join((list(unknown_names) + invalid_users))\n        raise forms.ValidationError((_('The following usernames are incorrect: %(users)s.') % {\n            'users': humanized_usernames,\n        }))\n    return users\n", "label": 0}
{"function": "\n\ndef process_sequences(self, playlist, sequences):\n    (first_sequence, last_sequence) = (sequences[0], sequences[(- 1)])\n    if (first_sequence.segment.key and (first_sequence.segment.key.method != 'NONE')):\n        self.logger.debug('Segments in this playlist are encrypted')\n        if (not CAN_DECRYPT):\n            raise StreamError('Need pyCrypto installed to decrypt this stream')\n    self.playlist_changed = ([s.num for s in self.playlist_sequences] != [s.num for s in sequences])\n    self.playlist_reload_time = (playlist.target_duration or last_sequence.segment.duration)\n    self.playlist_sequences = sequences\n    if (not self.playlist_changed):\n        self.playlist_reload_time = max((self.playlist_reload_time / 2), 1)\n    if playlist.is_endlist:\n        self.playlist_end = last_sequence.num\n    if (self.playlist_sequence < 0):\n        if (self.playlist_end is None):\n            edge_index = (- min(len(sequences), max(int(self.live_edge), 1)))\n            edge_sequence = sequences[edge_index]\n            self.playlist_sequence = edge_sequence.num\n        else:\n            self.playlist_sequence = first_sequence.num\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_or_create(cls, **kwargs):\n    defaults = kwargs.pop('defaults', {\n        \n    })\n    query = cls.select()\n    for (field, value) in kwargs.items():\n        if ('__' in field):\n            query = query.filter(**{\n                field: value,\n            })\n        else:\n            query = query.where((getattr(cls, field) == value))\n    try:\n        return (query.get(), False)\n    except cls.DoesNotExist:\n        try:\n            params = dict(((k, v) for (k, v) in kwargs.items() if ('__' not in k)))\n            params.update(defaults)\n            with cls._meta.database.atomic():\n                return (cls.create(**params), True)\n        except IntegrityError as exc:\n            try:\n                return (query.get(), False)\n            except cls.DoesNotExist:\n                raise exc\n", "label": 0}
{"function": "\n\ndef _custom_target_list_to_child_list(custom_targets, is_operator):\n    \"\\n    Convert a list of custom targets to a list of dictionaries\\n    formatted for use in DFP\\n\\n    @param custom_targets: list(Custom)\\n    @param is_operator: str, 'IS' or 'IS_NOT'\\n    @return: list(dict)\\n    \"\n    _children = defaultdict((lambda : defaultdict(list)))\n    for value in custom_targets:\n        _children[value.parent_id][is_operator].append(value)\n    dfp_children = []\n    for (_parent_id, _operations) in _children.iteritems():\n        for (_operator, _values) in _operations.iteritems():\n            _value_target_ids = []\n            _node_type = None\n            _node_value_type = None\n            for _custom_value in _values:\n                _this_node_type = _custom_value.node_key\n                _this_node_value_type = _custom_value.id_key\n                if ((not _node_type) and (not _node_value_type)):\n                    _node_type = _this_node_type\n                    _node_value_type = _this_node_value_type\n                if ((_this_node_type != _custom_value.node_key) or (_this_node_value_type != _custom_value.id_key)):\n                    raise ParselmouthException(('Could not serialize Custom Target: %s' % _custom_value))\n                _value_target_ids.append(_custom_value.id)\n            _doc = {\n                'operator': _operator,\n                'xsi_type': _node_type,\n                _node_value_type: _value_target_ids,\n            }\n            if _parent_id:\n                _doc['keyId'] = _parent_id\n            dfp_children.append(_doc)\n    return dfp_children\n", "label": 1}
{"function": "\n\ndef convert_to_py2():\n    if ((source_dir == 'python2_source') and (not PY2_CONVERTED)):\n        try:\n            subprocess.check_output(['3to2', '--help'])\n            subprocess.check_output(['pasteurize', '--help'])\n        except OSError as e:\n            if (e.errno != errno.ENOENT):\n                raise e\n            if (not os.path.exists(os.path.join(source_dir, 'pylatex'))):\n                raise ImportError('3to2 and future need to be installed before installing when PyLaTeX for Python 2.7 when it is not installed using one of the pip releases.')\n        else:\n            converter = (os.path.dirname(os.path.realpath(__file__)) + '/convert_to_py2.sh')\n            subprocess.check_call([converter])\n            global PY2_CONVERTED\n            PY2_CONVERTED = True\n", "label": 0}
{"function": "\n\n@attr(migrated_tenant=['admin', 'tenant1', 'tenant2'])\ndef test_tenant_key_exists_on_dst(self):\n    \"Validate deleted tenant's keypairs were migrated.\"\n    unused_keypairs = []\n    for (tenant_name, tenant) in self.deleted_tenants:\n        migrated_keys = [key['name'] for key in config.keypairs if (key['user'] in self._get_tenant_users_with_keypair(tenant_name))]\n        keys_list = []\n        for dst_vm in self.dst_vm_list:\n            for src_vm in tenant['vms']:\n                if (dst_vm.name == src_vm['name']):\n                    if ('key_name' not in src_vm):\n                        continue\n                    if (dst_vm.key_name not in migrated_keys):\n                        keys_list.append(dst_vm.key_name)\n        if keys_list:\n            unused_keypairs.append({\n                tenant_name: keys_list,\n            })\n    if unused_keypairs:\n        msg = \"Tenant's key_pairs {0} exist on destination, but should be deleted!\"\n        self.fail(msg.format(unused_keypairs))\n", "label": 1}
{"function": "\n\ndef required_validator(self, column):\n    '\\n        Returns required / optional validator for given column based on column\\n        nullability and form configuration.\\n\\n        :param column: SQLAlchemy Column object\\n        '\n    if ((not self.meta.all_fields_optional) and (not column.default) and (not column.nullable)):\n        type_map = self.meta.not_null_validator_type_map\n        try:\n            return type_map[column.type]\n        except KeyError:\n            if isinstance(column.type, sa.types.TypeDecorator):\n                type_ = column.type.impl\n                try:\n                    return type_map[type_]\n                except KeyError:\n                    pass\n            if (self.meta.not_null_validator is not None):\n                return self.meta.not_null_validator\n    return self.get_validator('optional')\n", "label": 0}
{"function": "\n\ndef unescape(item):\n    if (not isinstance(item, basestring)):\n        return item\n    result = []\n    unprocessed = item\n    while True:\n        res = _ESCAPE_RE.search(unprocessed)\n        if (res is None):\n            result.append(unprocessed)\n            break\n        result.append(unprocessed[:res.start()])\n        escapes = res.group(1)\n        nextchars = res.group(2)\n        unprocessed = unprocessed[res.end():]\n        result.append(('\\\\' * (len(escapes) / 2)))\n        if (((len(escapes) % 2) == 0) or (len(nextchars) == 0) or (nextchars[0] not in ['n', 'r', 't'])):\n            result.append(nextchars)\n        elif (nextchars[0] == 'n'):\n            if ((len(nextchars) == 1) or (nextchars[1] == ' ')):\n                result.append('\\n')\n            else:\n                result.append(('\\n' + nextchars[1]))\n        elif (nextchars[0] == 'r'):\n            result.append(('\\r' + nextchars[1:]))\n        else:\n            result.append(('\\t' + nextchars[1:]))\n    return ''.join(result)\n", "label": 1}
{"function": "\n\ndef test_Mul_is_algebraic():\n    a = Symbol('a', algebraic=True)\n    b = Symbol('a', algebraic=True)\n    na = Symbol('na', algebraic=False)\n    an = Symbol('an', algebraic=True, nonzero=True)\n    nb = Symbol('nb', algebraic=False)\n    x = Symbol('x')\n    assert (a * b).is_algebraic\n    assert ((na * nb).is_algebraic is None)\n    assert ((a * na).is_algebraic is None)\n    assert ((an * na).is_algebraic is False)\n    assert ((a * x).is_algebraic is None)\n    assert ((na * x).is_algebraic is None)\n", "label": 0}
{"function": "\n\ndef _check_mixed_int(df, dtype=None):\n    dtypes = dict(A='int32', B='uint64', C='uint8', D='int64')\n    if isinstance(dtype, compat.string_types):\n        dtypes = dict([(k, dtype) for (k, v) in dtypes.items()])\n    elif isinstance(dtype, dict):\n        dtypes.update(dtype)\n    if dtypes.get('A'):\n        assert (df.dtypes['A'] == dtypes['A'])\n    if dtypes.get('B'):\n        assert (df.dtypes['B'] == dtypes['B'])\n    if dtypes.get('C'):\n        assert (df.dtypes['C'] == dtypes['C'])\n    if dtypes.get('D'):\n        assert (df.dtypes['D'] == dtypes['D'])\n", "label": 1}
{"function": "\n\ndef reload_playlist(self):\n    if self.closed:\n        return\n    self.reader.buffer.wait_free()\n    self.logger.debug('Reloading playlist')\n    res = self.session.http.get(self.stream.url, exception=StreamError, **self.reader.request_params)\n    try:\n        playlist = hls_playlist.load(res.text, res.url)\n    except ValueError as err:\n        raise StreamError(err)\n    if playlist.is_master:\n        raise StreamError(\"Attempted to play a variant playlist, use 'hlsvariant://{0}' instead\".format(self.stream.url))\n    if playlist.iframes_only:\n        raise StreamError('Streams containing I-frames only is not playable')\n    media_sequence = (playlist.media_sequence or 0)\n    sequences = [Sequence((media_sequence + i), s) for (i, s) in enumerate(playlist.segments)]\n    if sequences:\n        self.process_sequences(playlist, sequences)\n", "label": 0}
{"function": "\n\ndef gethtml(url):\n    'Return HTML text from given URL. Makes attempt to recycle pre-fetched\\n  HTML from current if any of them used the same URL input parameter.'\n    if globs.html:\n        out('Recycling HTML.')\n        return globs.html\n    else:\n        out('Doing first HTML fetch for row.')\n        path = urlparse.urlparse(url).path\n        ext = os.path.splitext(path)[1]\n        if (ext not in globs.texttypes):\n            try:\n                defend = requests.head(url)\n            except:\n                return None\n            if ('Content-Type' in defend.headers):\n                ct = defend.headers['Content-Type']\n            else:\n                return None\n            if ((ct != '') and ('text' not in ct)):\n                return None\n        try:\n            globs.hobj = requests.get(url, timeout=(5, 10))\n        except:\n            return None\n        globs.html = globs.hobj.text\n    return globs.html\n", "label": 0}
{"function": "\n\ndef _human_st_mode(self, mode):\n    'Convert the st_mode returned by stat in human readable (ls-like) format'\n    hr = ''\n    if os.path.stat.S_ISREG(mode):\n        hr = '-'\n    elif os.path.stat.S_ISLNK(mode):\n        hr = 'l'\n    elif os.path.stat.S_ISSOCK(mode):\n        hr = 's'\n    elif os.path.stat.S_ISDIR(mode):\n        hr = 'd'\n    elif os.path.stat.S_ISBLK(mode):\n        hr = 'b'\n    elif os.path.stat.S_ISFIFO(mode):\n        hr = 'p'\n    elif os.path.stat.S_ISCHR(mode):\n        hr = 'c'\n    else:\n        hr = '-'\n    for who in ('USR', 'GRP', 'OTH'):\n        for perm in ('R', 'W', 'X'):\n            if (mode & getattr(os.path.stat, (('S_I' + perm) + who))):\n                hr += perm.lower()\n            else:\n                hr += '-'\n    return hr\n", "label": 1}
{"function": "\n\n@ensure_tag(['tbl'])\ndef get_rowspan_data(table):\n    w_namespace = get_namespace(table, 'w')\n    tr_index = 0\n    td_index = 0\n    tr_rows = list(table.xpath('.//w:tr', namespaces=table.nsmap))\n    for tr in table.xpath('.//w:tr', namespaces=table.nsmap):\n        for td in tr.xpath('.//w:tc', namespaces=tr.nsmap):\n            v_merge = get_v_merge(td)\n            if (v_merge is None):\n                td_index += get_grid_span(td)\n                continue\n            if (v_merge.get(('%sval' % w_namespace)) == 'restart'):\n                row_span = 1\n                for tr_el in tr_rows[(tr_index + 1):]:\n                    td_el = get_td_at_index(tr_el, td_index)\n                    td_el_v_merge = get_v_merge(td_el)\n                    if (td_el_v_merge is None):\n                        break\n                    val = td_el_v_merge.get(('%sval' % w_namespace))\n                    if (val == 'restart'):\n                        break\n                    row_span += 1\n                (yield row_span)\n            td_index += get_grid_span(td)\n        tr_index += 1\n        td_index = 0\n", "label": 0}
{"function": "\n\ndef substitute(t, subs):\n    '\\n    Return the term obtained from t by simultaneous substitution given\\n    by subs\\n\\n    subs is either a dictionary or a mapping given by an iterable of\\n    (key, value) pairs\\n\\n    Both keys and values in subs can be either Var or Const.\\n    All keys in subs will be substituted by their values in subs.\\n\\n    For variables, only free occurances will be substituted.\\n\\n    If the substitution will create capturing of a free variable, the\\n    substitution will fail with an error.\\n    '\n    if (not isinstance(subs, dict)):\n        subs = dict(subs)\n    if (type(t) in (Var, Const)):\n        if (t in subs):\n            return subs[t]\n        else:\n            return t\n    elif (type(t) in (Apply, Eq, Ite, Not, And, Or, Implies, Iff)):\n        return type(t)(*(substitute(x, subs) for x in t))\n    elif (type(t) in (ForAll, Exists)):\n        forbidden_variables = free_variables(*subs.values())\n        if forbidden_variables.isdisjoint(t.variables):\n            return type(t)(t.variables, substitute(t.body, ((k, v) for (k, v) in subs.iteritems() if (k not in t.variables))))\n        else:\n            assert False, (t, subs)\n    else:\n        assert False, type(e)\n", "label": 1}
{"function": "\n\n@eventmgr_rfc1459.message('NAMES', min_params=1)\ndef m_NAMES(cli, ev_msg):\n    chanlist = ev_msg['params'][0].split(',')\n    for chan in chanlist:\n        if (not validate_chan(chan)):\n            cli.dump_numeric('479', [chan, 'Illegal channel name'])\n            return\n        ch = cli.ctx.chmgr.get(ev_msg['params'][0], create=False)\n        if (not ch):\n            cli.dump_numeric('403', [chan, 'No such channel'])\n            return\n        names_f = (lambda x: True)\n        if (not ch.has_member(cli)):\n            names_f = (lambda x: ('user:invisible' not in x.client.props))\n        if ('userhost-in-names' in cli.caps):\n            cli.dump_numeric('353', [ch.classification, ch.name, ' '.join([m.hostmask for m in filter(names_f, ch.members)])])\n        else:\n            cli.dump_numeric('353', [ch.classification, ch.name, ' '.join([m.name for m in filter(names_f, ch.members)])])\n        cli.dump_numeric('366', [ch.name, 'End of /NAMES list.'])\n", "label": 0}
{"function": "\n\ndef yesno(question, default=None, all=False):\n    if (default is True):\n        question = ('%s [Yes/no' % question)\n        answers = {\n            False: ('n', 'no'),\n            True: ('', 'y', 'yes'),\n        }\n    elif (default is False):\n        question = ('%s [yes/No' % question)\n        answers = {\n            False: ('', 'n', 'no'),\n            True: ('y', 'yes'),\n        }\n    else:\n        question = ('%s [yes/no' % question)\n        answers = {\n            False: ('n', 'no'),\n            True: ('y', 'yes'),\n        }\n    if all:\n        if (default is 'all'):\n            answers['all'] = ('', 'a', 'all')\n            question = ('%s/All' % question)\n        else:\n            answers['all'] = ('a', 'all')\n            question = ('%s/all' % question)\n    question = ('%s] ' % question)\n    while 1:\n        answer = get_input(question).lower()\n        for option in answers:\n            if (answer in answers[option]):\n                return option\n        if all:\n            print('You have to answer with y, yes, n, no, a or all.', file=sys.stderr)\n        else:\n            print('You have to answer with y, yes, n or no.', file=sys.stderr)\n", "label": 1}
{"function": "\n\ndef filter_queryset(self, request, queryset, view):\n    self.request = request\n    self.view = view\n    for (name, idents) in self.filter_options.items():\n        if (name == 'extra'):\n            continue\n        ids = [ident.object_id for ident in idents]\n        fn = getattr(self, ('filter_by_%s' % name), None)\n        if (not fn):\n            fn = getattr(self.view, ('filter_by_%s' % name), None)\n        if fn:\n            queryset = fn(queryset, ids)\n        else:\n            raise Exception((\"Don't know how to filter by %s\" % name))\n    if ('extra' in self.filter_options):\n        fn = getattr(self.view, 'filter_by_extra', self.filter_by_extra)\n        queryset = fn(queryset, self.filter_options['extra'])\n    return queryset\n", "label": 0}
{"function": "\n\ndef get_rank_score(variant_line=None, variant_dict=None, family_id=0):\n    '\\n    Return the rank score priority for a certain family.\\n    \\n    If no family is given the first family found is used\\n    \\n    Arguments:\\n        variant_line (str): A vcf variant line\\n        variant_dict (dict): A variant dictionary\\n        family_id (str): A family id\\n    \\n    Return:\\n        rank_score (str): The rank score for this variant\\n    '\n    rank_score = (- 100)\n    raw_entry = None\n    if variant_line:\n        variant_line = variant_line.split('\\t')\n        for info_annotation in variant_line[7].split(';'):\n            info_annotation = info_annotation.split('=')\n            if (len(info_annotation) == 2):\n                key = info_annotation[0]\n                value = info_annotation[1]\n            if (key == 'RankScore'):\n                raw_entry = value\n                break\n    elif variant_dict:\n        raw_entry = variant_dict['info_dict'].get('RankScore')\n    if raw_entry:\n        for family_annotation in raw_entry.split(','):\n            family_annotation = family_annotation.split(':')\n            if family_id:\n                if (family_id == family_annotation[0]):\n                    rank_score = float(family_annotation[1])\n            else:\n                rank_score = float(family_annotation[1])\n    return str(rank_score)\n", "label": 1}
{"function": "\n\ndef test_expected_profile_url(request, app, entity):\n    'Test that urls are built correctly, based on the user_mode'\n    url = profile_url_for(entity)\n    if (app.user_mode == 'single'):\n        assert (url == '/profile')\n    elif (app.user_mode == 'multiple'):\n        assert (url == (('/' + entity.name) + '/profile'))\n    elif (app.user_mode == 'subdomain'):\n        assert (url == 'http://{}.example.com/profile'.format(entity.name))\n", "label": 0}
{"function": "\n\n@classmethod\ndef setUpClass(cls):\n    TestShowLatency.rc = controller.RootController()\n    actual_out = util.capture_stdout(TestShowLatency.rc.execute, ['show', 'latency'])\n    TestShowLatency.output_list = test_util.get_separate_output(actual_out, 'Latency')\n    for item in TestShowLatency.output_list:\n        if ('~~~proxy Latency~~' in item):\n            TestShowLatency.proxy_latency = item\n        elif ('~~query Latency~~' in item):\n            TestShowLatency.query_latency = item\n        elif ('~~reads Latency~~' in item):\n            TestShowLatency.reads_latency = item\n        elif ('~~udf Latency~~' in item):\n            TestShowLatency.udf_latency = item\n        elif ('~~writes_master Latency~~' in item):\n            TestShowLatency.writes_master_latency = item\n        elif ('~~writes_reply Latency~~' in item):\n            TestShowLatency.writes_reply_latency = item\n", "label": 0}
{"function": "\n\ndef iterate_open_nodes(self, node=None):\n    'Generator to iterate over all the expended nodes starting from\\n        `node` and down. If `node` is `None`, the generator start with\\n        :attr:`root`.\\n\\n        To get all the open nodes::\\n\\n            treeview = TreeView()\\n            # ... add nodes ...\\n            for node in treeview.iterate_open_nodes():\\n                print(node)\\n\\n        '\n    if (not node):\n        node = self.root\n    if (self.hide_root and (node is self.root)):\n        pass\n    else:\n        (yield node)\n    if (not node.is_open):\n        return\n    f = self.iterate_open_nodes\n    for cnode in node.nodes:\n        for ynode in f(cnode):\n            (yield ynode)\n", "label": 0}
{"function": "\n\ndef execute_sql(self, sql, params=None, require_commit=True):\n    logger.debug((sql, params))\n    with self.exception_wrapper():\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(sql, (params or ()))\n        except Exception:\n            if (self.get_autocommit() and self.autorollback):\n                self.rollback()\n            raise\n        else:\n            if (require_commit and self.get_autocommit()):\n                self.commit()\n    return cursor\n", "label": 0}
{"function": "\n\ndef get_page(self, user, max_page_size, is_above, score, score_time):\n    self._read_leaderboard()\n    scores = self.scores\n    leaderboard = []\n    player = None\n    query_complete = False\n    if (not is_above):\n        scores = reversed(scores)\n    for s in scores:\n        if is_above:\n            if (((self.sort_by * s.score) < (self.sort_by * score)) or ((s.score == score) and (s.score_time >= score_time))):\n                query_complete = True\n        elif (((self.sort_by * s.score) > (self.sort_by * score)) or ((s.score == score) and (s.score_time <= score_time))):\n            query_complete = True\n        if (query_complete and (len(leaderboard) >= max_page_size)):\n            break\n        username = s.user\n        row = self._get_row(username, s)\n        if (username == user.username):\n            player = row\n        leaderboard.append(row)\n    leaderboard = leaderboard[(- max_page_size):]\n    if (not is_above):\n        leaderboard = list(reversed(leaderboard))\n    if (player is None):\n        player = self._get_user_row(user)\n    if (len(leaderboard) > 0):\n        self._rank_leaderboard(leaderboard, self._get_rank(leaderboard[0]['score']))\n        top = (self.scores[0].user == leaderboard[0]['user']['username'])\n        bottom = (self.scores[(- 1)].user == leaderboard[(- 1)]['user']['username'])\n    else:\n        top = True\n        bottom = True\n    return self.create_response(top, bottom, leaderboard, player)\n", "label": 1}
{"function": "\n\ndef fix_path():\n    current_folder = os.path.abspath(os.path.dirname(__file__))\n    lib_path = os.path.join(current_folder, 'libs')\n    djangae_path = os.path.abspath(os.path.join(current_folder, os.pardir))\n    if (lib_path not in sys.path):\n        sys.path.insert(0, lib_path)\n    if (djangae_path not in sys.path):\n        sys.path.insert(0, djangae_path)\n    base_django_path = os.path.join(current_folder, 'submodules', 'django')\n    django_path = os.path.join(base_django_path, 'django')\n    django_tests_path = os.path.join(base_django_path, 'tests')\n    if (base_django_path not in sys.path):\n        sys.path.insert(0, base_django_path)\n    if (django_path not in sys.path):\n        sys.path.insert(0, django_path)\n    if (django_tests_path not in sys.path):\n        sys.path.insert(0, django_tests_path)\n    os.environ['DJANGAE_APP_YAML_LOCATION'] = current_folder\n    os.environ['PYTHONPATH'] = ''\n    try:\n        import wrapper_util\n    except ImportError:\n        appengine_path = os.path.join(lib_path, 'google_appengine')\n        sys.path.insert(0, appengine_path)\n        simplejson_path = os.path.join(appengine_path, 'lib', 'simplejson')\n        sys.path.insert(0, simplejson_path)\n", "label": 0}
{"function": "\n\ndef build_schedule(self):\n    created_items = []\n    reader = csv.DictReader(self.cleaned_data.get('filename'))\n    data = [dict(((k.strip(), v.strip()) for (k, v) in x.items())) for x in reader]\n    created_items.extend(self._build_rooms(data))\n    created_items.extend(self._build_days(data))\n    for row in data:\n        room = Room.objects.get(schedule=self.schedule, name=row[self.ROOM_KEY])\n        date = datetime.strptime(row[self.DATE_KEY], '%m/%d/%Y')\n        day = Day.objects.get(schedule=self.schedule, date=date)\n        (start, end) = self._get_start_end_times(row)\n        (slot_kind, created) = SlotKind.objects.get_or_create(label=row[self.KIND], schedule=self.schedule)\n        if created:\n            created_items.append(slot_kind)\n        if (row[self.KIND] == 'plenary'):\n            (slot, created) = Slot.objects.get_or_create(kind=slot_kind, day=day, start=start, end=end)\n            if created:\n                created_items.append(slot)\n        else:\n            slot = Slot.objects.create(kind=slot_kind, day=day, start=start, end=end)\n            created_items.append(slot)\n        try:\n            with transaction.atomic():\n                SlotRoom.objects.create(slot=slot, room=room)\n        except IntegrityError:\n            for x in created_items:\n                x.delete()\n            return (messages.ERROR, 'An overlap occurred; the import was cancelled.')\n    return (messages.SUCCESS, 'Your schedule has been imported.')\n", "label": 1}
{"function": "\n\ndef in_edges_iter(self, nbunch=None, data=False):\n    'Return an iterator over the incoming edges.\\n\\n        Parameters\\n        ----------\\n        nbunch : iterable container, optional (default= all nodes)\\n            A container of nodes.  The container will be iterated\\n            through once.\\n        data : bool, optional (default=False)\\n            If True, return edge attribute dict in 3-tuple (u,v,data).\\n\\n        Returns\\n        -------\\n        in_edge_iter : iterator\\n            An iterator of (u,v) or (u,v,d) tuples of incoming edges.\\n\\n        See Also\\n        --------\\n        edges_iter : return an iterator of edges\\n        '\n    if (nbunch is None):\n        nodes_nbrs = self.pred.items()\n    else:\n        nodes_nbrs = ((n, self.pred[n]) for n in self.nbunch_iter(nbunch))\n    if data:\n        for (n, nbrs) in nodes_nbrs:\n            for (nbr, data) in nbrs.items():\n                (yield (nbr, n, data))\n    else:\n        for (n, nbrs) in nodes_nbrs:\n            for nbr in nbrs:\n                (yield (nbr, n))\n", "label": 0}
{"function": "\n\ndef attrs(attrs=[], terse=False, undefined=None):\n    buf = []\n    if bool(attrs):\n        buf.append('')\n        for (k, v) in attrs:\n            if ((undefined is not None) and isinstance(v, undefined)):\n                continue\n            if ((v != None) and ((v != False) or (type(v) != bool))):\n                if ((k == 'class') and isinstance(v, (list, tuple))):\n                    v = ' '.join(map(str, flatten(v)))\n                t = ((v == True) and (type(v) == bool))\n                if (t and (not terse)):\n                    v = k\n                buf.append((('%s' % k) if (terse and t) else ('%s=\"%s\"' % (k, escape(v)))))\n    return ' '.join(buf)\n", "label": 1}
{"function": "\n\ndef __get__(self, instance, instance_type=None):\n    if (instance is None):\n        return self\n    try:\n        rel_obj = getattr(instance, self.cache_name)\n    except AttributeError:\n        related_pk = instance._get_pk_val()\n        if (related_pk is None):\n            rel_obj = None\n        else:\n            params = {\n                ('%s__pk' % self.related.field.name): related_pk,\n            }\n            try:\n                rel_obj = self.get_query_set(instance=instance).get(**params)\n            except self.related.model.DoesNotExist:\n                rel_obj = None\n            else:\n                setattr(rel_obj, self.related.field.get_cache_name(), instance)\n        setattr(instance, self.cache_name, rel_obj)\n    if (rel_obj is None):\n        raise self.related.model.DoesNotExist\n    else:\n        return rel_obj\n", "label": 0}
{"function": "\n\ndef run(self, cmdargv, cb):\n    self._purge_cache()\n    argv = self._build_argv(cmdargv)\n    args = ' '.join(argv)\n    m = hashlib.sha256()\n    m.update(args.encode('UTF-8'))\n    LOG.debug((\"Finding cache for args '%s'\" % args))\n    file = (((self.cachedir + '/') + m.hexdigest()) + '.json')\n    if ((not os.path.exists(file)) or self.refresh):\n        sp = self._run_async(argv)\n        try:\n            with open(file, 'wb') as f:\n                while True:\n                    line = sp.stdout.readline()\n                    if (not line):\n                        break\n                    f.write(line)\n        except:\n            os.unlink(file)\n            raise\n        sp.wait()\n        if (sp.returncode != 0):\n            os.unlink(file)\n            lines = []\n            while True:\n                line = sp.stderr.readline()\n                if (not line):\n                    break\n                lines.append(line.decode('UTF-8'))\n            msg = ''.join(lines)\n            raise Exception(('Error running command %s: %s' % (args, msg)))\n    sp = self._run_async(['cat', file])\n    self._process(sp, argv, cb)\n", "label": 1}
{"function": "\n\ndef shutdown(self):\n    LOGGER.critical('Shutting down.')\n    self.job_queue = []\n    self.queued_jobs = {\n        \n    }\n    deferreds = []\n    LOGGER.debug(('%s stopping on main HTTP interface.' % self.name))\n    d = self.site_port.stopListening()\n    if isinstance(d, Deferred):\n        deferreds.append(d)\n    if (self.reportjobspeedloop is not None):\n        LOGGER.debug('Stopping report job speed loop.')\n        d = self.reportjobspeedloop.stop()\n        if isinstance(d, Deferred):\n            deferreds.append(d)\n    if (self.jobsloop is not None):\n        LOGGER.debug('Stopping jobs loop.')\n        d = self.jobsloop.stop()\n        if isinstance(d, Deferred):\n            deferreds.append(d)\n    if (self.queryloop is not None):\n        LOGGER.debug('Stopping query loop.')\n        d = self.queryloop.stop()\n        if isinstance(d, Deferred):\n            deferreds.append(d)\n    if (self.coordinateloop is not None):\n        LOGGER.debug('Stopping coordinating loop.')\n        d = self.coordinateloop.stop()\n        if isinstance(d, Deferred):\n            deferreds.append(d)\n        LOGGER.debug('Removing data from SDB coordination domain.')\n        d = self.sdb.delete(self.aws_sdb_coordination_domain, self.uuid)\n        d.addCallback(self.peerCheckRequest)\n        deferreds.append(d)\n    if (len(deferreds) > 0):\n        d = DeferredList(deferreds)\n        d.addCallback(self._shutdownCallback)\n        return d\n    else:\n        return self._shutdownCallback(None)\n", "label": 1}
{"function": "\n\ndef write_enum(self, fobj, name, value, type_='uint'):\n    if isinstance(value, basestring):\n        if (value.startswith('0x') and type_.startswith('u')):\n            value += 'U'\n        if ((len(value) > 12) and type_.startswith('u')):\n            value += 'L'\n    try:\n        v = int(value)\n        if (v < 0):\n            type_ = 'int'\n    except ValueError:\n        pass\n    if (isinstance(value, basestring) and ('\"' in value)):\n        type_ = 'const(char)*'\n    fobj.write('enum {} {} = {};\\n'.format(type_, name, value))\n", "label": 1}
{"function": "\n\ndef get_metadata(self, engine, noisy=False, force=False, do_reflection=True):\n    'Fetch metadata for an sqlalchemy engine'\n    (db_key, ipydb_engine) = get_metadata_engine(engine)\n    create_schema(ipydb_engine)\n    db = self.databases[db_key]\n    if db.reflecting:\n        log.debug('Is already reflecting')\n        return db\n    if (not do_reflection):\n        return db\n    if force:\n        log.debug('was foreced to re-reflect')\n        db = self.read_expunge(ipydb_engine)\n        self.databases[db_key] = db\n        if noisy:\n            print('ipydb is fetching database metadata')\n        self.spawn_reflection_thread(db_key, db, engine.url)\n        return db\n    if (db.age > MAX_CACHE_AGE):\n        log.debug('Cache expired age:%s reading from sqlite', db.age)\n        db = self.read_expunge(ipydb_engine)\n        self.databases[db_key] = db\n        if (db.age > MAX_CACHE_AGE):\n            log.debug('Sqlite data too old: %s, re-reflecting', db.age)\n            if noisy:\n                print('ipydb is fetching database metadata')\n            self.spawn_reflection_thread(db_key, db, engine.url)\n    return db\n", "label": 0}
{"function": "\n\n@log_debug\ndef get_target_in_direction(level, location, direction, attack_range=100):\n    '\\n    Get target of the attack\\n\\n    :param level: level to operate\\n    :type level: Level\\n    :param location: start location\\n    :type location: (int, int)\\n    :param direction: direction to follow\\n    :type direction: int\\n    :returns: target character if found, otherwise None\\n    :rtype: Character\\n    '\n    target = None\n    target_location = location\n    target_data = None\n    if (direction == 9):\n        return TargetData('void', None, None, None)\n    off_sets = [(0, 0), (0, (- 1)), (1, (- 1)), (1, 0), (1, 1), (0, 1), ((- 1), 1), ((- 1), 0), ((- 1), (- 1))]\n    while ((target is None) and (distance_between(location, target_location) <= attack_range)):\n        target_location = tuple([x for x in map(sum, zip(target_location, off_sets[direction]))])\n        if blocks_movement(level, target_location):\n            target_data = TargetData('wall', target_location, None, target_data)\n            target = target_data\n        else:\n            target = get_character(level, target_location)\n            if target:\n                target_data = TargetData('character', target_location, target, target_data)\n                target = target_data\n            else:\n                target_data = TargetData('void', target_location, None, target_data)\n    return target_data\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self._options = options\n    for ts in TagSynonym.objects.all():\n        if ((ts.tag != ts.synonym_tag) and (ts.synonym_tag.items.count() > 0)):\n            self.stdout.write('')\n            self.stdout.write('copying tagged items from tag:')\n            self.stdout.write(((str(ts.synonym_tag.id) + ': ') + ts.synonym_tag.name))\n            self.stdout.write('to tag:')\n            self.stdout.write(((str(ts.tag.id) + ': ') + ts.tag.name))\n            self.stdout.write('objects:')\n            for ti in ts.synonym_tag.items.all():\n                self.stdout.write(((ti.content_type.name + ': ') + unicode(ti.object_id)))\n                ti.tag = ts.tag\n                ok = True\n                delti = False\n                try:\n                    ti.full_clean()\n                except ValidationError as e:\n                    if (e.message_dict == {\n                        '__all__': ['Tagged item with this Tag, Content type and Object id already exists.'],\n                    }):\n                        self.stdout.write('object is already tagged on proper tag, delete from synonym')\n                        delti = True\n                    else:\n                        self.stdout.write('validation error:')\n                        self.stdout.write(str(e.message_dict))\n                        ok = False\n                if (ok and options['nodryrun']):\n                    try:\n                        if delti:\n                            ti.delete()\n                        else:\n                            ti.save()\n                        self.stdout.write('done')\n                    except IntegrityError as e:\n                        self.stdout.write('IntegrityError:')\n                        self.stdout.write(str(e))\n", "label": 1}
{"function": "\n\ndef get_value(self, series, key):\n    \"\\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\\n        know what you're doing\\n        \"\n    s = getattr(series, '_values', None)\n    if (isinstance(s, Index) and lib.isscalar(key)):\n        try:\n            return s[key]\n        except (IndexError, ValueError):\n            pass\n    s = _values_from_object(series)\n    k = _values_from_object(key)\n    k = self._convert_scalar_indexer(k, kind='getitem')\n    try:\n        return self._engine.get_value(s, k, tz=getattr(series.dtype, 'tz', None))\n    except KeyError as e1:\n        if ((len(self) > 0) and (self.inferred_type in ['integer', 'boolean'])):\n            raise\n        try:\n            return tslib.get_value_box(s, key)\n        except IndexError:\n            raise\n        except TypeError:\n            if is_iterator(key):\n                raise InvalidIndexError(key)\n            else:\n                raise e1\n        except Exception:\n            raise e1\n    except TypeError:\n        if lib.isscalar(key):\n            raise IndexError(key)\n        raise InvalidIndexError(key)\n", "label": 1}
{"function": "\n\ndef import_data(self, user, upload, external_id_field_index=None):\n    '\\n        Import data into this ``Dataset`` from a given ``DataUpload``.\\n        '\n    self.lock()\n    try:\n        if upload.imported:\n            raise DataImportError(_('This file has already been imported.'))\n        task_type = get_import_task_type_for_upload(upload)\n        if (not task_type):\n            raise DataImportError(_('This file type is not supported for data import.'))\n        if self.column_schema:\n            if (upload.columns != [c['name'] for c in self.column_schema]):\n                raise DataImportError(_('The columns in this file do not match those in the dataset.'))\n        else:\n            self.column_schema = make_column_schema(upload.columns, types=upload.guessed_types)\n        if (self.sample_data is None):\n            self.sample_data = upload.sample_data\n        if ((self.initial_upload is None) and (self.row_count is None)):\n            self.initial_upload = upload\n        self.current_task = TaskStatus.objects.create(task_name=task_type.name, task_description=(_('Import data from %(filename)s into %(slug)s.') % {\n            'filename': upload.filename,\n            'slug': self.slug,\n        }), creator=user)\n        self.save()\n        task_type.apply_async(args=[self.slug, upload.id], kwargs={\n            'external_id_field_index': external_id_field_index,\n        }, task_id=self.current_task.id)\n    except:\n        self.unlock()\n        raise\n", "label": 1}
{"function": "\n\ndef _handle_standalone_auth(self, request, **kwargs):\n    authorization = pecan.request.authorization\n    auth_backend = self._auth_backend.__class__.__name__\n    remote_addr = pecan.request.remote_addr\n    extra = {\n        'auth_backend': auth_backend,\n        'remote_addr': remote_addr,\n    }\n    if (not authorization):\n        LOG.audit('Authorization header not provided', extra=extra)\n        self._abort_request()\n        return\n    (auth_type, auth_value) = authorization\n    if (auth_type.lower() not in ['basic']):\n        extra['auth_type'] = auth_type\n        LOG.audit(('Unsupported authorization type: %s' % auth_type), extra=extra)\n        self._abort_request()\n        return\n    try:\n        auth_value = base64.b64decode(auth_value)\n    except Exception:\n        LOG.audit('Invalid authorization header', extra=extra)\n        self._abort_request()\n        return\n    split = auth_value.split(':')\n    if (len(split) != 2):\n        LOG.audit('Invalid authorization header', extra=extra)\n        self._abort_request()\n        return\n    (username, password) = split\n    result = self._auth_backend\n    result = self._auth_backend.authenticate(username=username, password=password)\n    if (result is True):\n        ttl = getattr(request, 'ttl', None)\n        try:\n            token = self._create_token_for_user(username=username, ttl=ttl)\n            return self._process_successful_response(token=token)\n        except TTLTooLargeException as e:\n            self._abort_request(status_code=http_client.BAD_REQUEST, message=e.message)\n            return\n    LOG.audit('Invalid credentials provided', extra=extra)\n    self._abort_request()\n", "label": 0}
{"function": "\n\ndef posix_shell(chan, encoding='UTF-8'):\n    try:\n        chan.settimeout(0.0)\n        while True:\n            (r, w, e) = select.select([chan, sys.stdin], [], [])\n            if (chan in r):\n                try:\n                    x = chan.recv(1024)\n                    if (len(x) == 0):\n                        sys.stdout.write('\\r\\n*** EOF ***\\r\\n')\n                        break\n                    print(x.decode(encoding), end='')\n                    sys.stdout.flush()\n                except socket.timeout:\n                    pass\n            if (sys.stdin in r):\n                x = sys.stdin.read()\n                chan.send(x.encode(encoding))\n    except Exception as e:\n        print(('Exception in TTY session\\n%r\\n' % e))\n", "label": 0}
{"function": "\n\ndef is_resource_modified(environ, etag=None, data=None, last_modified=None):\n    'Convenience method for conditional requests.\\n\\n    :param environ: the WSGI environment of the request to be checked.\\n    :param etag: the etag for the response for comparison.\\n    :param data: or alternatively the data of the response to automatically\\n                 generate an etag using :func:`generate_etag`.\\n    :param last_modified: an optional date of the last modification.\\n    :return: `True` if the resource was modified, otherwise `False`.\\n    '\n    if ((etag is None) and (data is not None)):\n        etag = generate_etag(data)\n    elif (data is not None):\n        raise TypeError('both data and etag given')\n    if (environ['REQUEST_METHOD'] not in ('GET', 'HEAD')):\n        return False\n    unmodified = False\n    if isinstance(last_modified, string_types):\n        last_modified = parse_date(last_modified)\n    if (last_modified is not None):\n        last_modified = last_modified.replace(microsecond=0)\n    modified_since = parse_date(environ.get('HTTP_IF_MODIFIED_SINCE'))\n    if (modified_since and last_modified and (last_modified <= modified_since)):\n        unmodified = True\n    if etag:\n        if_none_match = parse_etags(environ.get('HTTP_IF_NONE_MATCH'))\n        if if_none_match:\n            unmodified = if_none_match.contains_raw(etag)\n    return (not unmodified)\n", "label": 1}
{"function": "\n\ndef _validate_temp_url_config(self):\n    'Validate the required settings for a temporary URL.'\n    if (not CONF.glance.swift_temp_url_key):\n        raise exc.MissingParameterValue(_('Swift temporary URLs require a shared secret to be created. You must provide \"swift_temp_url_key\" as a config option.'))\n    if (not CONF.glance.swift_endpoint_url):\n        raise exc.MissingParameterValue(_('Swift temporary URLs require a Swift endpoint URL. You must provide \"swift_endpoint_url\" as a config option.'))\n    if ((not CONF.glance.swift_account) and (CONF.glance.temp_url_endpoint_type == 'swift')):\n        raise exc.MissingParameterValue(_('Swift temporary URLs require a Swift account string. You must provide \"swift_account\" as a config option.'))\n    if (CONF.glance.swift_temp_url_duration < CONF.glance.swift_temp_url_expected_download_start_delay):\n        raise exc.InvalidParameterValue(_('\"swift_temp_url_duration\" must be greater than or equal to \"[glance]swift_temp_url_expected_download_start_delay\" option, otherwise the Swift temporary URL may expire before the download starts.'))\n    seed_num_chars = CONF.glance.swift_store_multiple_containers_seed\n    if ((seed_num_chars is None) or (seed_num_chars < 0) or (seed_num_chars > 32)):\n        raise exc.InvalidParameterValue(_('An integer value between 0 and 32 is required for swift_store_multiple_containers_seed.'))\n", "label": 1}
{"function": "\n\ndef __init__(self, author=None, category=None, content=None, atom_id=None, link=None, published=None, title=None, updated=None, organization=None, phone_number=None, nickname=None, occupation=None, gender=None, birthday=None, postal_address=None, structured_postal_address=None, email=None, im=None, relation=None, user_defined_field=None, website=None, external_id=None, event=None, batch_operation=None, batch_id=None, batch_status=None, text=None, extension_elements=None, extension_attributes=None, etag=None):\n    gdata.BatchEntry.__init__(self, author=author, category=category, content=content, atom_id=atom_id, link=link, published=published, batch_operation=batch_operation, batch_id=batch_id, batch_status=batch_status, title=title, updated=updated)\n    self.organization = (organization or [])\n    self.phone_number = (phone_number or [])\n    self.nickname = nickname\n    self.occupation = occupation\n    self.gender = gender\n    self.birthday = birthday\n    self.postal_address = (postal_address or [])\n    self.structured_postal_address = (structured_postal_address or [])\n    self.email = (email or [])\n    self.im = (im or [])\n    self.relation = (relation or [])\n    self.user_defined_field = (user_defined_field or [])\n    self.website = (website or [])\n    self.external_id = (external_id or [])\n    self.event = (event or [])\n    self.text = text\n    self.extension_elements = (extension_elements or [])\n    self.extension_attributes = (extension_attributes or {\n        \n    })\n    self.etag = etag\n", "label": 1}
{"function": "\n\ndef match(self, daytime):\n    match = self.pattern.match(daytime)\n    if (not match):\n        raise ValueError('format mismatch')\n    get = match.groupdict().get\n    tm = ([0] * 9)\n    year = get('year')\n    if year:\n        year = int(year)\n        if (year < 68):\n            year = (2000 + year)\n        elif (year < 100):\n            year = (1900 + year)\n        tm[0] = year\n    month = get('month')\n    if month:\n        if (month in MONTHS):\n            month = (MONTHS.index(month) + 1)\n        tm[1] = int(month)\n    day = get('day')\n    if day:\n        tm[2] = int(day)\n    hour = get('hour')\n    if hour:\n        tm[3] = int(hour)\n    else:\n        hour = get('hour12')\n        if hour:\n            hour = int(hour)\n            if (get('ampm12', '').lower() == 'pm'):\n                hour = (hour + 12)\n            tm[3] = hour\n    minute = get('minute')\n    if minute:\n        tm[4] = int(minute)\n    second = get('second')\n    if second:\n        tm[5] = int(second)\n    return tuple(tm)\n", "label": 1}
{"function": "\n\ndef update_models(new_obj, current_table, tables, relations):\n    ' Update the state of the parsing. '\n    _update_check_inputs(current_table, tables, relations)\n    _check_no_current_table(new_obj, current_table)\n    if isinstance(new_obj, Table):\n        tables_names = [t.name for t in tables]\n        _check_not_creating_duplicates(new_obj.name, tables_names, 'table', DuplicateTableException)\n        return (new_obj, (tables + [new_obj]), relations)\n    if isinstance(new_obj, Relation):\n        tables_names = [t.name for t in tables]\n        _check_colname_in_lst(new_obj.right_col, tables_names)\n        _check_colname_in_lst(new_obj.left_col, tables_names)\n        return (current_table, tables, (relations + [new_obj]))\n    if isinstance(new_obj, Column):\n        columns_names = [c.name for c in current_table.columns]\n        _check_not_creating_duplicates(new_obj.name, columns_names, 'column', DuplicateColumnException)\n        current_table.columns.append(new_obj)\n        return (current_table, tables, relations)\n    msg = 'new_obj cannot be of type {}'\n    raise ValueError(msg.format(new_obj.__class__.__name__))\n", "label": 0}
{"function": "\n\ndef handle_actor_migrate(self, handle, connection, match, data, hdr):\n    ' Migrate actor\\n        '\n    status = calvinresponse.OK\n    if ('peer_node_id' in data):\n        try:\n            self.node.am.migrate(match.group(1), data['peer_node_id'], callback=CalvinCB(self.actor_migrate_cb, handle, connection))\n        except:\n            _log.exception('Migration failed')\n            status = calvinresponse.INTERNAL_ERROR\n    elif ('requirements' in data):\n        try:\n            self.node.am.update_requirements(match.group(1), data['requirements'], extend=(data['extend'] if ('extend' in data) else False), move=(data['move'] if ('move' in data) else False), callback=CalvinCB(self.actor_migrate_cb, handle, connection))\n        except:\n            _log.exception('Migration failed')\n            status = calvinresponse.INTERNAL_ERROR\n    else:\n        status = calvinresponse.BAD_REQUEST\n    if (status != calvinresponse.OK):\n        self.send_response(handle, connection, None, status=status)\n", "label": 0}
{"function": "\n\ndef is_gde_post(activity):\n    'Identify gde post.'\n    content = activity['object']['content']\n    if ('annotation' in activity):\n        content += (' ' + activity['annotation'])\n    if (not is_valid_expert_tag(content)):\n        return False\n    if ('verb' in activity):\n        if (activity['verb'] == 'checkin'):\n            return False\n        if (activity['verb'] == 'post'):\n            return True\n        elif (activity['object']['actor']['id'] == activity['actor']['id']):\n            return True\n        else:\n            return False\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef results_iter(self):\n    '\\n        Returns an iterator over the results from executing this query.\\n        '\n    resolve_columns = hasattr(self, 'resolve_columns')\n    if resolve_columns:\n        from django.db.models.fields import DateField\n        fields = [DateField()]\n    else:\n        from django.db.backends.util import typecast_date\n        needs_string_cast = self.connection.features.needs_datetime_string_cast\n    offset = len(self.query.extra_select)\n    for rows in self.execute_sql(MULTI):\n        for row in rows:\n            date = row[offset]\n            if resolve_columns:\n                date = self.resolve_columns(row, fields)[offset]\n            elif needs_string_cast:\n                date = typecast_date(str(date))\n            if isinstance(date, datetime.datetime):\n                date = date.date()\n            (yield date)\n", "label": 0}
{"function": "\n\ndef collect_fields(self, fields, *layouts):\n    '`fields` is a pointer to either `self.detail_fields` or\\n        `self.list_fields`.  Each of these must contain a primary key\\n        field.\\n\\n        '\n    pk_found = False\n    for layout in layouts:\n        for df in layout._store_fields:\n            assert (df is not None)\n            self.add_field_for(fields, df)\n            if df.primary_key:\n                self.primary_keys.add(df)\n                pk_found = True\n                if (self.pk is None):\n                    self.pk = df\n    if (self.pk is None):\n        self.pk = self.actor.get_pk_field()\n    if (self.pk is not None):\n        if (not pk_found):\n            self.add_field_for(fields, self.pk)\n", "label": 1}
{"function": "\n\ndef _case_changed(case_id, handler_ids):\n    subcases = None\n    case = CommCareCase.get(case_id)\n    for handler in CaseReminderHandler.get_handlers_from_ids(handler_ids):\n        if (handler.start_condition_type == CASE_CRITERIA):\n            kwargs = {\n                \n            }\n            if handler.uses_time_case_property:\n                kwargs = {\n                    'schedule_changed': True,\n                    'prev_definition': handler,\n                }\n            handler.case_changed(case, **kwargs)\n            if handler.uses_parent_case_property:\n                if (subcases is None):\n                    subcases = get_subcases(case)\n                for subcase in subcases:\n                    handler.case_changed(subcase, **kwargs)\n", "label": 0}
{"function": "\n\ndef test_Idx_bounds():\n    (i, a, b) = symbols('i a b', integer=True)\n    assert (Idx(i).lower is None)\n    assert (Idx(i).upper is None)\n    assert (Idx(i, a).lower == 0)\n    assert (Idx(i, a).upper == (a - 1))\n    assert (Idx(i, 5).lower == 0)\n    assert (Idx(i, 5).upper == 4)\n    assert (Idx(i, oo).lower == 0)\n    assert (Idx(i, oo).upper == oo)\n    assert (Idx(i, (a, b)).lower == a)\n    assert (Idx(i, (a, b)).upper == b)\n    assert (Idx(i, (1, 5)).lower == 1)\n    assert (Idx(i, (1, 5)).upper == 5)\n    assert (Idx(i, ((- oo), oo)).lower == (- oo))\n    assert (Idx(i, ((- oo), oo)).upper == oo)\n", "label": 1}
{"function": "\n\ndef test_rescale_on_state_changes(self):\n    ct = self.setup_hist_scale_counter()\n    assert (ct.call_count == 0)\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 1)\n    self.artist.lo -= 1\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 2)\n    self.artist.hi -= 1\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 3)\n    self.artist.nbins += 1\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 4)\n    self.artist.xlog ^= True\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 5)\n    self.artist.ylog ^= True\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 6)\n    self.artist.cumulative ^= True\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 7)\n    self.artist.normed ^= True\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 8)\n    self.artist.layer.subset_state = (self.artist.layer.data.id['x'] > 10)\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 9)\n    self.artist.layer.style.color = '#00ff00'\n    self.artist.update()\n    self.artist.update()\n    assert (ct.call_count == 9)\n", "label": 1}
{"function": "\n\ndef json_api_exception_handler(exc, context):\n    '\\n    Custom exception handler that returns errors object as an array\\n    '\n    from rest_framework.views import exception_handler\n    response = exception_handler(exc, context)\n    errors = []\n    if response:\n        message = response.data\n        if isinstance(exc, TwoFactorRequiredError):\n            response['X-OSF-OTP'] = 'required; app'\n        if isinstance(exc, JSONAPIException):\n            errors.extend([{\n                'source': (exc.source or {\n                    \n                }),\n                'detail': exc.detail,\n                'meta': (exc.meta or {\n                    \n                }),\n            }])\n        elif isinstance(message, dict):\n            errors.extend(dict_error_formatting(message, None))\n        else:\n            if isinstance(message, basestring):\n                message = [message]\n            for (index, error) in enumerate(message):\n                if isinstance(error, dict):\n                    errors.extend(dict_error_formatting(error, index))\n                else:\n                    errors.append({\n                        'detail': error,\n                    })\n        response.data = {\n            'errors': errors,\n        }\n    return response\n", "label": 1}
{"function": "\n\ndef goto(self, goto):\n    (what, which, where) = self.parse_goto(goto)\n    if (what == 'section'):\n        self.move_to_section(which, where)\n    elif (what == 'item'):\n        self.move_to_item(which, where)\n    elif (what == 'file'):\n        self.move_to_file(which, where)\n    elif (what == 'stash'):\n        self.move_to_stash(which, where)\n    elif (what == 'point'):\n        try:\n            point = int(which)\n            self.move_to_point(point)\n        except ValueError:\n            pass\n", "label": 0}
{"function": "\n\ndef test_random_overdet(self):\n    for dtype in REAL_DTYPES:\n        for (n, m) in ((20, 15), (200, 2)):\n            for lapack_driver in TestLstsq.lapack_drivers:\n                for overwrite in (True, False):\n                    a = np.asarray(random([n, m]), dtype=dtype)\n                    for i in range(m):\n                        a[(i, i)] = (20 * (0.1 + a[(i, i)]))\n                    for i in range(4):\n                        b = np.asarray(random([n, 3]), dtype=dtype)\n                        a1 = a.copy()\n                        b1 = b.copy()\n                        try:\n                            out = lstsq(a1, b1, lapack_driver=lapack_driver, overwrite_a=overwrite, overwrite_b=overwrite)\n                        except LstsqLapackError:\n                            if (lapack_driver is None):\n                                mesg = 'LstsqLapackError raised with lapack_driver being None.'\n                                raise AssertionError(mesg)\n                            else:\n                                continue\n                        x = out[0]\n                        r = out[2]\n                        assert_((r == m), ('expected efficient rank %s, got %s' % (m, r)))\n                        assert_allclose(x, direct_lstsq(a, b, cmplx=0), rtol=(25 * np.finfo(a1.dtype).eps), atol=(25 * np.finfo(a1.dtype).eps), err_msg=('driver: %s' % lapack_driver))\n", "label": 1}
{"function": "\n\ndef validate(self):\n    if ('direction' in self.resource):\n        direction = self.resource['direction']\n        if (direction not in ['ingress', 'egress']):\n            raise ValueError('The direction attribute must be either ingress or egress')\n    if ('ethertype' in self.resource):\n        ethertype = self.resource['ethertype']\n        if (ethertype not in ['IPv4', 'IPv6']):\n            raise ValueError('The ethertype attribute must be either IPv4 or IPv6')\n    if ('protocol' in self.resource):\n        protocol = self.resource['protocol']\n        if (protocol not in ['tcp', 'udp', 'icmp']):\n            raise ValueError('The protocol attribute must be either tcp, udp or icmp')\n    if ('remote_mode' in self.resource):\n        remote_mode = self.resource['remote_mode']\n        if (remote_mode not in ['remote_ip_prefix', 'remote_group_id']):\n            raise ValueError('The remote_mode attribute must be either remote_ip_prefix or remote_group_id')\n    return True\n", "label": 1}
{"function": "\n\ndef testcaseVariationXbrlLoaded(testcaseModelXbrl, instanceModelXbrl, modelTestcaseVariation, *args, **kwargs):\n    modelManager = instanceModelXbrl.modelManager\n    if (hasattr(testcaseModelXbrl, 'efmOptions') and modelManager.validateDisclosureSystem and getattr(modelManager.disclosureSystem, 'EFMplugin', False) and ((instanceModelXbrl.modelDocument.type == ModelDocument.Type.INSTANCE) or (instanceModelXbrl.modelDocument.type == ModelDocument.Type.INLINEXBRL))):\n        cntlr = modelManager.cntlr\n        options = testcaseModelXbrl.efmOptions\n        entrypointFiles = [{\n            'file': instanceModelXbrl.modelDocument.uri,\n        }]\n        if (not hasattr(modelManager, 'efmFiling')):\n            modelManager.efmFiling = Filing(cntlr, options, instanceModelXbrl.fileSource, entrypointFiles, None, None, instanceModelXbrl.errorCaptureLevel)\n            for pluginXbrlMethod in pluginClassMethods('EdgarRenderer.Filing.Start'):\n                pluginXbrlMethod(cntlr, options, entrypointFiles, modelManager.efmFiling)\n        modelManager.efmFiling.addReport(instanceModelXbrl)\n        _report = modelManager.efmFiling.reports[(- 1)]\n        _report.entryPoint = entrypointFiles[0]\n        modelManager.efmFiling.arelleUnitTests = instanceModelXbrl.arelleUnitTests.copy()\n        for _instanceElt in XmlUtil.descendants(modelTestcaseVariation, '*', 'instance', 'readMeFirst', 'true', False):\n            if instanceModelXbrl.modelDocument.uri.endswith(_instanceElt.text):\n                if _instanceElt.get('exhibitType'):\n                    _report.entryPoint['exhibitType'] = _report.exhibitType = _instanceElt.get('exhibitType')\n                break\n", "label": 1}
{"function": "\n\ndef __init__(self, timestr=None, timezone=LOCALTZ, allowpast=True, allowfuture=True):\n    ' Converts input to UTC timestamp. '\n    if (timestr is None):\n        timestr = time.time()\n    self.timezone = timezone\n    if (type(timestr) == str):\n        self.__timestamp__ = self.__fromstring__(timestr)\n    elif (type(timestr) in [int, float]):\n        self.__timestamp__ = (timestr + self.timezone)\n    elif (type(timestr) in [datetime.datetime, datetime.date, datetime.time]):\n        self.__timestamp__ = (_mktime(timestr.timetuple()) + self.timezone)\n    elif (type(timestr) == time.struct_time):\n        self.__timestamp__ = (_mktime(timestr) + self.timezone)\n    else:\n        raise TypeError('Failed to recognize given type.')\n    if ((not allowpast) and (self.__timestamp__ < currentutc())):\n        raise DateRangeError('Values from the past are not allowed.')\n    if ((not allowfuture) and (self.__timestamp__ > currentutc())):\n        raise DateRangeError('Values from the future are not allowed.')\n", "label": 1}
{"function": "\n\ndef test_warp_reproject_dst_bounds(runner, tmpdir):\n    '--x-dst-bounds option works.'\n    srcname = 'tests/data/shade.tif'\n    outputname = str(tmpdir.join('test.tif'))\n    out_bounds = [(- 106.45036), 39.6138, (- 106.44136), 39.6278]\n    result = runner.invoke(warp.warp, ([srcname, outputname, '--dst-crs', 'EPSG:4326', '--res', 0.001, '--x-dst-bounds'] + out_bounds))\n    assert (result.exit_code == 0)\n    assert os.path.exists(outputname)\n    with rasterio.open(srcname) as src:\n        with rasterio.open(outputname) as output:\n            assert (output.crs == {\n                'init': 'epsg:4326',\n            })\n            assert numpy.allclose(output.bounds[0::3], [(- 106.45036), 39.6278])\n            assert numpy.allclose([0.001, 0.001], [output.affine.a, (- output.affine.e)])\n            assert numpy.allclose([(output.bounds[2] - 0.001), (output.bounds[1] + 0.001)], [(- 106.44136), 39.6138])\n            assert (output.width == 10)\n            assert (output.height == 15)\n", "label": 1}
{"function": "\n\ndef dict_flat_generator(value, attname=None, splitter=JSPLITTER, dumps=None, prefix=None, error=ValueError, recursive=True):\n    'Convert a nested dictionary into a flat dictionary representation'\n    if ((not isinstance(value, dict)) or (not recursive)):\n        if (not prefix):\n            raise error('Cannot assign a non dictionary to a JSON field')\n        else:\n            name = (('%s%s%s' % (attname, splitter, prefix)) if attname else prefix)\n            (yield (name, (dumps(value) if dumps else value)))\n    else:\n        for field in value:\n            val = value[field]\n            key = prefix\n            if field:\n                key = (('%s%s%s' % (prefix, splitter, field)) if prefix else field)\n            for (k, v2) in dict_flat_generator(val, attname, splitter, dumps, key, error, field):\n                (yield (k, v2))\n", "label": 1}
{"function": "\n\ndef _print_phantomjs_output(result):\n    errors = result['errors']\n    messages = result['messages']\n    resources = result['resources']\n    for message in messages:\n        msg = message['msg']\n        line = message.get('line')\n        source = message.get('source')\n        if (source is None):\n            write(msg)\n        elif (line is None):\n            write(('%s: %s' % (source, msg)))\n        else:\n            write(('%s:%s: %s' % (source, line, msg)))\n    for resource in resources:\n        url = resource['url']\n        if url.endswith('.png'):\n            ok(('%s: %s (%s)' % (url, yellow(resource['status']), resource['statusText'])))\n        else:\n            warn(('Resource error:: %s: %s (%s)' % (url, red(resource['status']), resource['statusText'])))\n    for error in errors:\n        warn(('%s: %s' % (red('PhatomJS Error: '), error['msg'])))\n        for item in error['trace']:\n            write(('    %s: %d' % (item['file'], item['line'])))\n", "label": 0}
{"function": "\n\ndef getEditFileCommand(filePath, lineNum):\n    (editor, _editor_path) = getEditorAndPath()\n    if ((editor in ['vim', 'vim -p']) and (lineNum != 0)):\n        return (\"'%s' +%d\" % (filePath, lineNum))\n    elif ((editor in ['vi', 'nvim', 'nano', 'joe', 'emacs', 'emacsclient']) and (lineNum != 0)):\n        return (\"+%d '%s'\" % (lineNum, filePath))\n    elif ((editor in ['subl', 'sublime', 'atom']) and (lineNum != 0)):\n        return (\"'%s:%d'\" % (filePath, lineNum))\n    else:\n        return (\"'%s'\" % filePath)\n", "label": 0}
{"function": "\n\n@name.setter\ndef name(self, value):\n    old_name = self._name\n    if (value is old_name):\n        return\n    self._name = value\n    configs = ConfigParser._named_configs\n    if old_name:\n        (_, props) = configs.get(old_name, (None, []))\n        for (widget, prop) in props:\n            widget = widget()\n            if widget:\n                widget.property(prop).set_config(None)\n        configs[old_name] = (None, props)\n    if (not value):\n        return\n    try:\n        (config, props) = configs[value]\n    except KeyError:\n        configs[value] = (ref(self), [])\n        return\n    if (config is not None):\n        raise ValueError('A parser named {} already exists'.format(value))\n    for (widget, prop) in props:\n        widget = widget()\n        if widget:\n            widget.property(prop).set_config(self)\n    configs[value] = (ref(self), props)\n", "label": 1}
{"function": "\n\ndef add_many_rows(self, user, data):\n    '\\n        Shortcut for adding rows in bulk. \\n\\n        ``data`` must be an array of tuples in the format (data_array, external_id)\\n        '\n    self.lock()\n    try:\n        data_typer = DataTyper(self.column_schema)\n        solr_rows = [utils.solr.make_data_row(self, d[0], external_id=d[1]) for d in data]\n        solr_rows = [data_typer(s, d[0]) for (s, d) in zip(solr_rows, data)]\n        solr.add(settings.SOLR_DATA_CORE, solr_rows, commit=True)\n        self.schema = data_typer.schema\n        if (not self.sample_data):\n            self.sample_data = []\n        if (len(self.sample_data) < 5):\n            needed = (5 - len(self.sample_data))\n            self.sample_data.extend([d[0] for d in data[:needed]])\n        old_row_count = self.row_count\n        self.row_count = self._count_rows()\n        added = (self.row_count - (old_row_count or 0))\n        updated = (len(data) - added)\n        self.last_modified = now()\n        self.last_modified_by = user\n        if (added and updated):\n            self.last_modification = (_('%(added)i rows added and %(updated)i updated') % {\n                'added': added,\n                'updated': updated,\n            })\n        elif added:\n            self.last_modification = (_('%i rows added') % added)\n        else:\n            self.last_modification = (_('%i rows updated') % updated)\n        self.save()\n        return solr_rows\n    finally:\n        self.unlock()\n", "label": 1}
{"function": "\n\ndef generate_data_center(self):\n    self.stdout.write('Generating Data Center assets')\n    data_center_status = DataCenterAssetStatus()\n    parent_category = DataCenterCategoryFactory(name='DATA CENTER', imei_required=False)\n    for i in range(2):\n        server_room = ServerRoomFactory()\n        visualization_col = 1\n        visualization_row = 1\n        for j in range(10):\n            rack = RackFactory(server_room=server_room, visualization_row=visualization_row, visualization_col=visualization_col)\n            visualization_row += 1\n            if (visualization_row > server_room.data_center.visualization_rows_num):\n                visualization_row = 1\n                visualization_col += 1\n            accessory = AccessoryFactory()\n            RackAccessoryFactory(rack=rack, accessory=accessory)\n            position = 1\n            for (status_id, name) in data_center_status:\n                for i in range(2):\n                    asset_model = DataCenterAssetModelFactory(category=DataCenterCategoryFactory(parent=parent_category))\n                    DataCenterAssetFactory(rack=rack, status=status_id, position=position, slot_no='', service_env=ServiceEnvironmentFactory(), model=asset_model)\n                    position += asset_model.height_of_device\n                    if (position > rack.max_u_height):\n                        position = 1\n        chassis = DataCenterAssetFactory(rack=rack, status=DataCenterAssetStatus.used.id, position=38, slot_no=None, service_env=ServiceEnvironmentFactory(), model=DataCenterAssetModelFactory(name='Chassis', category=DataCenterCategoryFactory(parent=parent_category), height_of_device=5))\n        for i in range(5):\n            DataCenterAssetFactory(rack=rack, status=DataCenterAssetStatus.used.id, position=None, service_env=ServiceEnvironmentFactory(), slot_no=i, parent=chassis, model=DataCenterAssetModelFactory(name='Blade', has_parent=True, category=DataCenterCategoryFactory(parent=parent_category)))\n", "label": 0}
{"function": "\n\ndef _fold(self, name, value, refold_binary=False):\n    if hasattr(value, 'name'):\n        return value.fold(policy=self)\n    maxlen = (self.max_line_length if self.max_line_length else float('inf'))\n    lines = value.splitlines()\n    refold = ((self.refold_source == 'all') or ((self.refold_source == 'long') and ((lines and (((len(lines[0]) + len(name)) + 2) > maxlen)) or any(((len(x) > maxlen) for x in lines[1:])))))\n    if (refold or (refold_binary and _has_surrogates(value))):\n        return self.header_factory(name, ''.join(lines)).fold(policy=self)\n    return (((name + ': ') + self.linesep.join(lines)) + self.linesep)\n", "label": 1}
{"function": "\n\ndef test_caching():\n    vcf_fn = 'fixture/sample.vcf.gz'\n    cache_fn = vcfnp.array._mk_cache_fn(vcf_fn, array_type='variants')\n    debug(cache_fn)\n    if os.path.exists(cache_fn):\n        os.remove(cache_fn)\n    a = variants(vcf_fn, cache=True, verbose=True)\n    a2 = np.load(cache_fn)\n    assert np.all((a == a2))\n    cache_fn = vcfnp.array._mk_cache_fn(vcf_fn, array_type='calldata')\n    debug(cache_fn)\n    if os.path.exists(cache_fn):\n        os.remove(cache_fn)\n    a = calldata(vcf_fn, cache=True, verbose=True)\n    a2 = np.load(cache_fn)\n    assert np.all((a == a2))\n    cache_fn = vcfnp.array._mk_cache_fn(vcf_fn, array_type='calldata_2d')\n    debug(cache_fn)\n    if os.path.exists(cache_fn):\n        os.remove(cache_fn)\n    a = calldata_2d(vcf_fn, cache=True, verbose=True)\n    a2 = np.load(cache_fn)\n    assert np.all((a == a2))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef Add(expr, assumptions):\n    '\\n        Imaginary + Imaginary -> Imaginary\\n        Imaginary + Complex   -> ?\\n        Imaginary + Real      -> !Imaginary\\n        '\n    if expr.is_number:\n        return AskImaginaryHandler._number(expr, assumptions)\n    reals = 0\n    for arg in expr.args:\n        if ask(Q.imaginary(arg), assumptions):\n            pass\n        elif ask(Q.real(arg), assumptions):\n            reals += 1\n        else:\n            break\n    else:\n        if (reals == 0):\n            return True\n        if ((reals == 1) or (len(expr.args) == reals)):\n            return False\n", "label": 1}
{"function": "\n\ndef _init_toolkit():\n    ' Initialise the current toolkit. '\n\n    def import_toolkit(tk):\n        try:\n            be = ('pyface.ui.%s.' % tk)\n            __import__((be + 'init'))\n        except:\n            raise\n        return be\n    if ETSConfig.toolkit:\n        be = import_toolkit(ETSConfig.toolkit)\n    else:\n        known_toolkits = ('qt4', 'wx', 'null')\n        for tk in known_toolkits:\n            try:\n                with provisional_toolkit(tk):\n                    be = import_toolkit(tk)\n                break\n            except ImportError as exc:\n                msg = \"Could not import Pyface backend '{0}'\"\n                logger.info(msg.format(tk))\n                if (logger.getEffectiveLevel() <= logging.INFO):\n                    logger.exception(exc)\n        else:\n            try:\n                be = import_toolkit('null')\n                import warnings\n                msg = (\"Unable to import the '{0}' backend for pyface; \" + \"using the 'null' backend instead.\")\n                warnings.warn(msg.format(toolkit_name), RuntimeWarning)\n            except ImportError as exc:\n                logger.exception(exc)\n                raise ImportError(('Unable to import a pyface backend for any of the %s toolkits' % ', '.join(known_toolkits)))\n    global _toolkit_backend\n    _toolkit_backend = be\n", "label": 0}
{"function": "\n\ndef testOnData(self, dataset=None, verbose=False):\n    'Compute the MSE of the module performance on the given dataset.\\n\\n        If no dataset is supplied, the one passed upon Trainer initialization is\\n        used.'\n    if (dataset == None):\n        dataset = self.ds\n    dataset.reset()\n    if verbose:\n        print('\\nTesting on data:')\n    errors = []\n    importances = []\n    ponderatedErrors = []\n    for seq in dataset._provideSequences():\n        self.module.reset()\n        (e, i) = dataset._evaluateSequence(self.module.activate, seq, verbose)\n        importances.append(i)\n        errors.append(e)\n        ponderatedErrors.append((e / i))\n    if verbose:\n        print(('All errors:', ponderatedErrors))\n    assert (sum(importances) > 0)\n    avgErr = (sum(errors) / sum(importances))\n    if verbose:\n        print(('Average error:', avgErr))\n        print(('Max error:', max(ponderatedErrors), 'Median error:', sorted(ponderatedErrors)[(len(errors) // 2)]))\n    return avgErr\n", "label": 0}
{"function": "\n\ndef wait(self, jobs=None, timeout=(- 1)):\n    'waits on one or more `jobs`, for up to `timeout` seconds.\\n        \\n        Parameters\\n        ----------\\n        \\n        jobs : int, str, or list of ints and/or strs, or one or more AsyncResult objects\\n                ints are indices to self.history\\n                strs are msg_ids\\n                default: wait on all outstanding messages\\n        timeout : float\\n                a time in seconds, after which to give up.\\n                default is -1, which means no timeout\\n        \\n        Returns\\n        -------\\n        \\n        True : when all msg_ids are done\\n        False : timeout reached, some msg_ids still outstanding\\n        '\n    tic = time.time()\n    if (jobs is None):\n        theids = self.outstanding\n    else:\n        if isinstance(jobs, (int, str, AsyncResult)):\n            jobs = [jobs]\n        theids = set()\n        for job in jobs:\n            if isinstance(job, int):\n                job = self.history[job]\n            elif isinstance(job, AsyncResult):\n                list(map(theids.add, job.msg_ids))\n                continue\n            theids.add(job)\n    if (not theids.intersection(self.outstanding)):\n        return True\n    self.spin()\n    while theids.intersection(self.outstanding):\n        if ((timeout >= 0) and ((time.time() - tic) > timeout)):\n            break\n        time.sleep(0.001)\n        self.spin()\n    return (len(theids.intersection(self.outstanding)) == 0)\n", "label": 1}
{"function": "\n\ndef get_expr_list_by_hackathon_id(self, hackathon, context):\n    user_name = (context.user_name if ('user_name' in context) else None)\n    status = (context.status if ('status' in context) else None)\n    page = (int(context.page) if ('page' in context) else 1)\n    per_page = (int(context.per_page) if ('per_page' in context) else 10)\n    users = (User.objects(name=user_name).all() if user_name else [])\n    if (user_name and status):\n        experiments_pagi = Experiment.objects(hackathon=hackathon, status=status, user__in=users).paginate(page, per_page)\n    elif (user_name and (not status)):\n        experiments_pagi = Experiment.objects(hackathon=hackathon, user__in=users).paginate(page, per_page)\n    elif ((not user_name) and status):\n        experiments_pagi = Experiment.objects(hackathon=hackathon, status=status).paginate(page, per_page)\n    else:\n        experiments_pagi = Experiment.objects(hackathon=hackathon).paginate(page, per_page)\n    return self.util.paginate(experiments_pagi, self.__get_expr_with_detail)\n", "label": 1}
{"function": "\n\ndef log_application_new(self, application_id, application_name):\n    ' Trace application new\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_APPLICATION_NEW in logger.events)):\n            data = {\n                \n            }\n            data['timestamp'] = time.time()\n            data['node_id'] = self.node.id\n            data['type'] = 'application_new'\n            data['application_id'] = application_id\n            data['application_name'] = application_name\n            if (logger.connection is not None):\n                if (not logger.connection.connection_lost):\n                    logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                else:\n                    disconnected.append(user_id)\n            elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                msg = {\n                    'cmd': 'logevent',\n                    'msgid': logger.handle,\n                    'header': None,\n                    'data': ('data: %s\\n\\n' % json.dumps(data)),\n                }\n                self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef build(self, input_shape):\n    assert (len(input_shape) == 2)\n    input_dim = input_shape[1]\n    self.input_spec = [InputSpec(dtype=K.floatx(), shape=(None, input_dim))]\n    self.W = self.init((input_dim, self.output_dim), name='{}_W'.format(self.name))\n    if self.bias:\n        self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n        self.trainable_weights = [self.W, self.b]\n    else:\n        self.trainable_weights = [self.W]\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if (self.bias and self.b_regularizer):\n        self.b_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    if self.activity_regularizer:\n        self.activity_regularizer.set_layer(self)\n        self.regularizers.append(self.activity_regularizer)\n    self.constraints = {\n        \n    }\n    if self.W_constraint:\n        self.constraints[self.W] = self.W_constraint\n    if (self.bias and self.b_constraint):\n        self.constraints[self.b] = self.b_constraint\n    if (self.initial_weights is not None):\n        self.set_weights(self.initial_weights)\n        del self.initial_weights\n", "label": 1}
{"function": "\n\ndef iterate(self, prod_, rule_):\n    newProduction = ''\n    for i in range(len(prod_)):\n        step = self.production[i]\n        if (step == 'W'):\n            newProduction = (newProduction + self.ruleW)\n        elif (step == 'X'):\n            newProduction = (newProduction + self.ruleX)\n        elif (step == 'Y'):\n            newProduction = (newProduction + self.ruleY)\n        elif (step == 'Z'):\n            newProduction = (newProduction + self.ruleZ)\n        elif (step != 'F'):\n            newProduction = (newProduction + step)\n    self.drawLength = (self.drawLength * 0.5)\n    self.generations += 1\n    return newProduction\n", "label": 0}
{"function": "\n\ndef test_groupdisposable_remove():\n    disp1 = [False]\n    disp2 = [False]\n\n    def action1():\n        disp1[0] = True\n    d1 = Disposable(action1)\n\n    def action2():\n        disp2[0] = True\n    d2 = Disposable(action2)\n    g = CompositeDisposable(d1, d2)\n    assert (g.length == 2)\n    assert g.contains(d1)\n    assert g.contains(d2)\n    assert g.remove(d1)\n    assert (g.length == 1)\n    assert (not g.contains(d1))\n    assert g.contains(d2)\n    assert disp1[0]\n    assert g.remove(d2)\n    assert (not g.contains(d1))\n    assert (not g.contains(d2))\n    assert disp2[0]\n    disp3 = [False]\n\n    def action3():\n        disp3[0] = True\n    d3 = Disposable(action3)\n    assert (not g.remove(d3))\n    assert (not disp3[0])\n", "label": 1}
{"function": "\n\ndef time_period_str(value):\n    'Validate and transform time offset.'\n    if isinstance(value, int):\n        raise vol.Invalid('Make sure you wrap time values in quotes')\n    elif (not isinstance(value, str)):\n        raise vol.Invalid(TIME_PERIOD_ERROR.format(value))\n    negative_offset = False\n    if value.startswith('-'):\n        negative_offset = True\n        value = value[1:]\n    elif value.startswith('+'):\n        value = value[1:]\n    try:\n        parsed = [int(x) for x in value.split(':')]\n    except ValueError:\n        raise vol.Invalid(TIME_PERIOD_ERROR.format(value))\n    if (len(parsed) == 2):\n        (hour, minute) = parsed\n        second = 0\n    elif (len(parsed) == 3):\n        (hour, minute, second) = parsed\n    else:\n        raise vol.Invalid(TIME_PERIOD_ERROR.format(value))\n    offset = timedelta(hours=hour, minutes=minute, seconds=second)\n    if negative_offset:\n        offset *= (- 1)\n    return offset\n", "label": 1}
{"function": "\n\ndef describe_diff(a, b):\n    \"\\n    Takes two strings and calculates the difference between them.\\n\\n    Output format is a list of::\\n\\n        (line_number, diff_type, line)\\n\\n    ``diff_type`` can be:\\n        * ``' '``\\n        * ``'-'``\\n        * ``'+'``\\n\\n    Example::\\n\\n        >>> list(describe_diff('a\\\\nb\\\\nc', 'a\\\\nc\\\\nd'))\\n        [(1, ' ', 'a'),\\n        (2, '-', 'b'),\\n        (2, ' ', 'c'),\\n        (3, '+', 'd')]\\n    \"\n    a = a.splitlines()\n    b = b.splitlines()\n    for (tag, i1, i2, j1, j2) in SequenceMatcher(None, a, b).get_opcodes():\n        if (tag == 'equal'):\n            for (idx, line) in enumerate(b[j1:j2]):\n                (yield (((idx + j1) + 1), ' ', _make_unicode(line)))\n            continue\n        if ((tag == 'replace') or (tag == 'delete')):\n            for (idx, line) in enumerate(a[i1:i2]):\n                (yield (((idx + i1) + 1), '-', _make_unicode(line)))\n        if ((tag == 'replace') or (tag == 'insert')):\n            for (idx, line) in enumerate(b[j1:j2]):\n                (yield (((idx + j1) + 1), '+', _make_unicode(line)))\n", "label": 1}
{"function": "\n\ndef before(base=_datetime, diff=None):\n    '\\n    count datetime before `base` time\\n    :param base:  minuend -> str/datetime/date\\n    :param diff:  str\\n    :return: datetime\\n    '\n    _base = parse(base)\n    if isinstance(_base, datetime.date):\n        _base = midnight(_base)\n    if (not diff):\n        return _base\n    result_dict = dp(diff)\n    for unit in result_dict:\n        _val = result_dict[unit]\n        if (not _val):\n            continue\n        if (unit == 'years'):\n            _base = _base.replace(year=(_base.year - _val))\n        elif (unit == 'months'):\n            if (_base.month <= _val):\n                _month_diff = (12 - (_val - _base.month))\n                _base = _base.replace(year=(_base.year - 1)).replace(month=_month_diff)\n            else:\n                _base = _base.replace(month=(_base.month - _val))\n        elif (unit in ['days', 'hours', 'minutes', 'seconds']):\n            _base = (_base - datetime.timedelta(**{\n                unit: _val,\n            }))\n    return _base\n", "label": 1}
{"function": "\n\ndef add_namespaces(root, ns_keys):\n    if isinstance(ns_keys, six.string_types):\n        ns_keys = [ns_keys]\n    namespaces = Namespaces()\n    ns_keys = [(x, namespaces.get_namespace(x)) for x in ns_keys]\n    if (etree.__name__ != 'lxml.etree'):\n        existing_namespaces = set()\n        for elem in root.getiterator():\n            if (elem.tag[0] == '{'):\n                (uri, tag) = elem.tag[1:].split('}')\n                existing_namespaces.add(namespaces.get_namespace_from_url(uri))\n        for (key, link) in ns_keys:\n            if ((link is not None) and (key not in existing_namespaces)):\n                root.set(('xmlns:%s' % key), link)\n        return root\n    else:\n        new_map = root.nsmap\n        for (key, link) in ns_keys:\n            if (link is not None):\n                new_map[key] = link\n        new_root = etree.Element(root.tag, nsmap=new_map)\n        for (a, v) in list(root.items()):\n            new_root.set(a, v)\n        for child in root:\n            new_root.append(deepcopy(child))\n        return new_root\n", "label": 1}
{"function": "\n\ndef run(self, edit):\n    sel = self.view.sel()[0]\n    for region in self.view.sel():\n        self.view.insert(edit, region.end(), '>')\n    self.view.run_command('hide_auto_complete')\n    if (not SETTINGS.get('auto_close_cfml')):\n        return\n    if (self.view.match_selector(sel.end(), 'string') or self.view.match_selector(sel.end(), 'source.cfscript.embedded.cfml') or (not self.view.match_selector(sel.end(), 'meta.tag.block.cf'))):\n        return\n    for region in self.view.sel():\n        pos = region.begin()\n        tagdata = self.view.substr(sublime.Region(0, pos)).split('<')\n        tagdata.reverse()\n        tagdata = tagdata.pop(0).split(' ')\n        tagname = tagdata[0]\n    if (self.view.match_selector(sel.end(), 'meta.tag.block.cf') and (not (self.view.substr((sel.end() - 1)) == '/')) and (not (tagname[0] == '/'))):\n        if (not (tagname[(- 1)] == '>')):\n            tagname = (tagname + '>')\n        if ((not SETTINGS.get('auto_indent_on_close')) or (tagname == 'cfoutput>')):\n            self.view.run_command('insert_snippet', {\n                'contents': ('$0</' + tagname),\n            })\n        else:\n            self.view.run_command('insert_snippet', {\n                'contents': ('\\n\\t$0\\n</' + tagname),\n            })\n", "label": 1}
{"function": "\n\ndef _confirm_delete(self, exists, delete_events, fields):\n    deleted_at = exists.get('deleted_at')\n    state = exists.get('state')\n    (apb, ape) = self._get_audit_period(exists)\n    if ((not deleted_at) and delete_events):\n        raise UsageException('U6', '.deleted events found but .exists has no deleted_at value.')\n    if (deleted_at and (state != 'deleted')):\n        raise UsageException('U3', \".exists state not 'deleted' but .exists deleted_at is set.\")\n    if (deleted_at and (not delete_events)):\n        launched_at = exists.get('launched_at')\n        if (deleted_at < launched_at):\n            raise UsageException('U4', '.exists deleted_at < .exists launched_at.')\n        if (apb and ape and (deleted_at >= apb) and (deleted_at <= ape)):\n            raise UsageException('U5', '.exists deleted_at in audit period, but no matching .delete event found.')\n    if (len(delete_events) > 1):\n        raise UsageException('U7', 'Multiple .delete.end events')\n    if delete_events:\n        self._verify_fields(exists, delete_events[(- 1)], fields)\n", "label": 1}
{"function": "\n\ndef get_browser_locale(self, default='en_US'):\n    \"Determines the user's locale from ``Accept-Language`` header.\\n\\n        See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4\\n        \"\n    if ('Accept-Language' in self.request.headers):\n        languages = self.request.headers['Accept-Language'].split(',')\n        locales = []\n        for language in languages:\n            parts = language.strip().split(';')\n            if ((len(parts) > 1) and parts[1].startswith('q=')):\n                try:\n                    score = float(parts[1][2:])\n                except (ValueError, TypeError):\n                    score = 0.0\n            else:\n                score = 1.0\n            locales.append((parts[0], score))\n        if locales:\n            locales.sort(key=(lambda pair: pair[1]), reverse=True)\n            codes = [l[0] for l in locales]\n            return locale.get(*codes)\n    return locale.get(default)\n", "label": 0}
{"function": "\n\ndef do_mask(img, mask_path, mask_source, mask_mode=None):\n    if (not mask_path):\n        return img\n    if (mask_source == 'media'):\n        mask = pil.open(MEDIA_STORAGE.open(mask_path)).convert('RGBA')\n    else:\n        mask = pil.open(STATIC_STORAGE.open(mask_path)).convert('RGBA')\n    if (mask_mode == 'distort'):\n        (iw, ih) = img.size\n        (mw, mh) = mask.size\n        if ((mw != iw) or (mh != ih)):\n            mask = mask.resize((iw, ih), pil.ANTIALIAS)\n    else:\n        (iw, ih) = img.size\n        (ow, oh) = mask.size\n        overlay_ratio = (float(ow) / float(oh))\n        have_to_scale = False\n        if (ow > iw):\n            ow = iw\n            oh = int((float(iw) / overlay_ratio))\n        if (oh > ih):\n            ow = int((float(ih) * overlay_ratio))\n            oh = ih\n        if ((ow != iw) or (oh != ih)):\n            have_to_scale = True\n        if have_to_scale:\n            nmask = mask.resize((ow, oh), pil.ANTIALIAS)\n            mask = pil.new('RGBA', (iw, ih))\n            do_paste(mask, nmask, (int(((iw - ow) / 2)), int(((ih - oh) / 2))))\n            (ow, oh) = mask.size\n    (r, g, b, a) = mask.split()\n    img.putalpha(a)\n", "label": 1}
{"function": "\n\ndef eq_comm(u, v, eq=None):\n    \" Goal for commutative equality\\n\\n    >>> from logpy import run, var, fact\\n    >>> from logpy.assoccomm import eq_comm as eq\\n    >>> from logpy.assoccomm import commutative, associative\\n\\n    >>> fact(commutative, 'add')    # declare that 'add' is commutative\\n    >>> fact(associative, 'add')    # declare that 'add' is associative\\n\\n    >>> x = var()\\n    >>> run(0, x, eq(('add', 1, 2, 3), ('add', 2, x, 1)))\\n    (3,)\\n    \"\n    eq = (eq or eq_comm)\n    op = var()\n    utail = var()\n    vtail = var()\n    if (isvar(u) and isvar(v)):\n        return (core.eq, u, v)\n        raise EarlyGoalError()\n    (uop, uargs) = op_args(u)\n    (vop, vargs) = op_args(v)\n    if ((not uop) and (not vop)):\n        return (core.eq, u, v)\n    if (vop and (not uop)):\n        (uop, uargs) = (vop, vargs)\n        (v, u) = (u, v)\n    return (conde, ((core.eq, u, v),), ((commutative, uop), (buildo, uop, vtail, v), (permuteq, uargs, vtail, eq)))\n", "label": 0}
{"function": "\n\ndef get_args(self, mtype, name, code, args):\n    self.code = code\n    old_args = args\n    mtype = mtype.replace('sticky', 'cell')\n    func = getattr(self, ((mtype + '_') + name))\n    try:\n        (args, kwargs) = _parse_args(func, args, usage=self.get_help(mtype, name))\n    except Exception as e:\n        self.kernel.Error(str(e))\n        return self\n    arg_spec = inspect.getargspec(func)\n    fargs = arg_spec.args\n    if (fargs[0] == 'self'):\n        fargs = fargs[1:]\n    fargs = [f for f in fargs if (not (f in kwargs.keys()))]\n    if ((len(args) > len(fargs)) and (not arg_spec.varargs)):\n        extra = ' '.join((str(s) for s in args[(len(fargs) - 1):]))\n        args = (args[:(len(fargs) - 1)] + [extra])\n    return (args, kwargs, old_args)\n", "label": 0}
{"function": "\n\ndef sources(action):\n    if (not hasattr(request, 'auth_sources')):\n        request.auth_sources = {\n            READ: set(),\n            WRITE: set(),\n        }\n        if is_admin():\n            for (source_id,) in Source.all_ids():\n                request.auth_sources[READ].add(source_id)\n                request.auth_sources[WRITE].add(source_id)\n        else:\n            q = Permission.all()\n            q = q.filter(Permission.role_id.in_(request.auth_roles))\n            q = q.filter((Permission.resource_type == Permission.SOURCE))\n            for perm in q:\n                if perm.read:\n                    request.auth_sources[READ].add(perm.resource_id)\n                if (perm.write and request.logged_in):\n                    request.auth_sources[WRITE].add(perm.resource_id)\n    return list(request.auth_sources.get(action, []))\n", "label": 0}
{"function": "\n\ndef _number_desc_for_type(metadata, num_type):\n    'Return the PhoneNumberDesc of the metadata for the given number type'\n    if (num_type == PhoneNumberType.PREMIUM_RATE):\n        return metadata.premium_rate\n    elif (num_type == PhoneNumberType.TOLL_FREE):\n        return metadata.toll_free\n    elif (num_type == PhoneNumberType.MOBILE):\n        return metadata.mobile\n    elif ((num_type == PhoneNumberType.FIXED_LINE) or (num_type == PhoneNumberType.FIXED_LINE_OR_MOBILE)):\n        return metadata.fixed_line\n    elif (num_type == PhoneNumberType.SHARED_COST):\n        return metadata.shared_cost\n    elif (num_type == PhoneNumberType.VOIP):\n        return metadata.voip\n    elif (num_type == PhoneNumberType.PERSONAL_NUMBER):\n        return metadata.personal_number\n    elif (num_type == PhoneNumberType.PAGER):\n        return metadata.pager\n    elif (num_type == PhoneNumberType.UAN):\n        return metadata.uan\n    elif (num_type == PhoneNumberType.VOICEMAIL):\n        return metadata.voicemail\n    else:\n        return metadata.general_desc\n", "label": 1}
{"function": "\n\ndef get_oauth_params(self, request):\n    'Get the basic OAuth parameters to be used in generating a signature.\\n        '\n    nonce = (generate_nonce() if (self.nonce is None) else self.nonce)\n    timestamp = (generate_timestamp() if (self.timestamp is None) else self.timestamp)\n    params = [('oauth_nonce', nonce), ('oauth_timestamp', timestamp), ('oauth_version', '1.0'), ('oauth_signature_method', self.signature_method), ('oauth_consumer_key', self.client_key)]\n    if self.resource_owner_key:\n        params.append(('oauth_token', self.resource_owner_key))\n    if self.callback_uri:\n        params.append(('oauth_callback', self.callback_uri))\n    if self.verifier:\n        params.append(('oauth_verifier', self.verifier))\n    content_type = request.headers.get('Content-Type', None)\n    content_type_eligible = (content_type and (content_type.find('application/x-www-form-urlencoded') < 0))\n    if ((request.body is not None) and content_type_eligible):\n        params.append(('oauth_body_hash', base64.b64encode(hashlib.sha1(request.body.encode('utf-8')).digest()).decode('utf-8')))\n    return params\n", "label": 1}
{"function": "\n\ndef main(in_args):\n    port = default_port.get()\n    host = 'localhost'\n    auto_start = True\n    before_args = []\n    after_args = None\n    i = 1\n    while (i < len(in_args)):\n        if in_args[i].startswith('--host='):\n            host = in_args[i][7:]\n            i += 1\n            continue\n        if in_args[i].startswith('--port='):\n            port = in_args[i][7:]\n            i += 1\n            continue\n        if (in_args[i] == '--no_auto_start'):\n            auto_start = False\n            i += 1\n            continue\n        if (in_args[i] == 'prelaunch'):\n            after_args = in_args[(i + 1):]\n            break\n        before_args.append(in_args[i])\n        i += 1\n    if (after_args == None):\n        raise Exception('Expected: prelaunch')\n    assert (len(before_args) == 0)\n    if (len(after_args) == 0):\n        after_args.append('search')\n    try:\n        sys.stdout.write(run_command_in_existing(host, port, after_args, auto_start))\n        return 0\n    except Exception as e:\n        sys.stdout.write((str(e) + '\\n'))\n        return (- 1)\n", "label": 1}
{"function": "\n\ndef convertFromStr(self, value, type):\n    if (value is not None):\n        if (type == 'str'):\n            return str(value)\n        elif (value.strip() != ''):\n            if (type == 'long'):\n                return long(value)\n            elif (type == 'float'):\n                return float(value)\n            elif (type == 'int'):\n                return int(value)\n            elif (type == 'date'):\n                return date(*time_strptime(value, '%Y-%m-%d')[0:3])\n            elif (type == 'datetime'):\n                return datetime(*time_strptime(value, '%Y-%m-%d %H:%M:%S')[0:6])\n    return None\n", "label": 1}
{"function": "\n\ndef _check_peers(peers):\n    'Checks whether the input is a valid list of peers and transforms domain names into IP Addresses'\n    if (not isinstance(peers, list)):\n        return False\n    for peer in peers:\n        if (not isinstance(peer, str)):\n            return False\n    ip_only_peers = list()\n    for peer in peers:\n        try:\n            ip_only_peers.append(str(IPAddress(peer)))\n        except AddrFormatError:\n            dns_reply = list()\n            try:\n                dns_reply = dns.resolver.query(peer)\n            except dns.resolver.NoAnswer:\n                continue\n            for dns_ip in dns_reply:\n                ip_only_peers.append(str(dns_ip))\n    peers = ip_only_peers\n    return True\n", "label": 0}
{"function": "\n\ndef minCostII(self, costs):\n    '\\n        Lef F[i][j] be the total min costs when the houses BEFORE i are painted, with (i-1)-th house pained as color j\\n        F[i][j] = \\\\min(F[i-1][k] + cost[i-1][j] \\x0corall k, k != j\\n\\n        edge case handling for i\\n        :type costs: List[List[int]]\\n        :rtype: int\\n        '\n    if (not costs):\n        return 0\n    n = len(costs)\n    m = len(costs[0])\n    F = [[0 for _ in xrange(m)] for _ in xrange((n + 1))]\n    for i in xrange(1, (n + 1)):\n        for k1 in xrange(m):\n            F[i][k1] = min(((F[(i - 1)][k0] + costs[(i - 1)][k1]) for k0 in xrange(m) if ((i == 1) or (k1 != k0))))\n    return min((F[n][i] for i in xrange(m)))\n", "label": 1}
{"function": "\n\ndef _format_table(self, expr):\n    ctx = self.context\n    ref_expr = expr\n    op = ref_op = expr.op()\n    if isinstance(op, ops.SelfReference):\n        ref_expr = op.table\n        ref_op = ref_expr.op()\n    if isinstance(ref_op, ops.PhysicalTable):\n        name = ref_op.name\n        if (name is None):\n            raise com.RelationError('Table did not have a name: {0!r}'.format(expr))\n        result = quote_identifier(name)\n        is_subquery = False\n    else:\n        if ctx.is_extracted(ref_expr):\n            alias = ctx.get_ref(expr)\n            if isinstance(op, ops.SelfReference):\n                return '{0} {1}'.format(ctx.get_ref(ref_expr), alias)\n            else:\n                return alias\n        subquery = ctx.get_compiled_expr(expr)\n        result = '(\\n{0}\\n)'.format(util.indent(subquery, self.indent))\n        is_subquery = True\n    if (is_subquery or ctx.need_aliases()):\n        result += ' {0}'.format(ctx.get_ref(expr))\n    return result\n", "label": 0}
{"function": "\n\ndef _make_kwargs(self, **kwargs):\n    'Creates the keyword arguments for the single run handling'\n    result_dict = {\n        'traj': self._traj,\n        'logging_manager': self._logging_manager,\n        'runfunc': self._runfunc,\n        'runargs': self._args,\n        'runkwargs': self._kwargs,\n        'clean_up_runs': self._clean_up_runs,\n        'automatic_storing': self._automatic_storing,\n        'wrap_mode': self._wrap_mode,\n        'niceness': self._niceness,\n        'graceful_exit': self._graceful_exit,\n    }\n    result_dict.update(kwargs)\n    if self._multiproc:\n        if (self._use_pool or self._use_scoop):\n            if self._use_scoop:\n                del result_dict['graceful_exit']\n            if self._freeze_input:\n                result_dict['full_copy'] = self.traj.v_full_copy\n                if self._map_arguments:\n                    del result_dict['runargs']\n                    del result_dict['runkwargs']\n            else:\n                result_dict['clean_up_runs'] = False\n                if self._use_pool:\n                    del result_dict['logging_manager']\n                    del result_dict['niceness']\n        else:\n            result_dict['clean_up_runs'] = False\n    return result_dict\n", "label": 0}
{"function": "\n\ndef test_warp_no_reproject_bounds_res(runner, tmpdir):\n    srcname = 'tests/data/shade.tif'\n    outputname = str(tmpdir.join('test.tif'))\n    out_bounds = [(- 11850000), 4810000, (- 11849000), 4812000]\n    result = runner.invoke(warp.warp, ([srcname, outputname, '--res', 30, '--bounds'] + out_bounds))\n    assert (result.exit_code == 0)\n    assert os.path.exists(outputname)\n    with rasterio.open(srcname) as src:\n        with rasterio.open(outputname) as output:\n            assert (output.crs == src.crs)\n            assert numpy.allclose(output.bounds, out_bounds)\n            assert numpy.allclose([30, 30], [output.affine.a, (- output.affine.e)])\n            assert (output.width == 34)\n            assert (output.height == 67)\n", "label": 0}
{"function": "\n\ndef check_hyperparams_dict(hypers):\n    if (type(hypers) is not dict):\n        raise TypeError('hypers should be a dict')\n    keys = ['a', 'b', 'kappa']\n    for key in keys:\n        if (key not in hypers):\n            raise KeyError(('missing key in hypers: %r' % (key,)))\n    for (key, value) in six.iteritems(hypers):\n        if (key not in keys):\n            raise KeyError(('invalid hypers key: %r' % (key,)))\n        if (not isinstance(value, (float, numpy.float64))):\n            raise TypeError(('%r should be float' % (key,)))\n        if (key in ['a', 'kappa']):\n            if (value <= 0.0):\n                raise ValueError(('hypers[%r] should be greater than 0' % (key,)))\n        if (key == 'b'):\n            if ((value <= 0.0) or (value >= (2 * pi))):\n                raise ValueError(('hypers[%r] should be in [0,2*pi]' % (key,)))\n", "label": 1}
{"function": "\n\ndef init(self, parent):\n    ' Finishes initializing the editor by creating the underlying toolkit\\n            widget.\\n        '\n    factory = self.factory\n    style = self.base_style\n    self.evaluate = factory.evaluate\n    self.sync_value(factory.evaluate_name, 'evaluate', 'from')\n    if ((not factory.multi_line) or factory.password):\n        style &= (~ wx.TE_MULTILINE)\n    if factory.password:\n        style |= wx.TE_PASSWORD\n    multi_line = ((style & wx.TE_MULTILINE) != 0)\n    if multi_line:\n        self.scrollable = True\n    if (factory.enter_set and (not multi_line)):\n        control = wx.TextCtrl(parent, (- 1), self.str_value, style=(style | wx.TE_PROCESS_ENTER))\n        wx.EVT_TEXT_ENTER(parent, control.GetId(), self.update_object)\n    else:\n        control = wx.TextCtrl(parent, (- 1), self.str_value, style=style)\n    wx.EVT_KILL_FOCUS(control, self.update_object)\n    if factory.auto_set:\n        wx.EVT_TEXT(parent, control.GetId(), self.update_object)\n    self.control = control\n    self.set_error_state(False)\n    self.set_tooltip()\n", "label": 0}
{"function": "\n\ndef _lookup_param_1_3(self):\n    lookup_params = self.params.copy()\n    for i in (ALL_VAR, ORDER_VAR, ORDER_TYPE_VAR, SEARCH_VAR, IS_POPUP_VAR, TO_FIELD_VAR):\n        if (i in lookup_params):\n            del lookup_params[i]\n    for (key, value) in lookup_params.items():\n        if (not isinstance(key, str)):\n            del lookup_params[key]\n            lookup_params[smart_str(key)] = value\n        if key.endswith('__in'):\n            value = value.split(',')\n            lookup_params[key] = value\n        if key.endswith('__isnull'):\n            if (value.lower() in ('', 'false')):\n                value = False\n            else:\n                value = True\n            lookup_params[key] = value\n        if (not self.model_admin.lookup_allowed(key, value)):\n            raise SuspiciousOperation(('Filtering by %s not allowed' % key))\n    return lookup_params\n", "label": 1}
{"function": "\n\ndef _is_name_allowed_in_acl(self, req, path_parts, identity):\n    if (not self.allow_names_in_acls):\n        return False\n    user_domain_id = identity['user_domain'][0]\n    if (user_domain_id and (user_domain_id != self.default_domain_id)):\n        return False\n    proj_domain_id = identity['project_domain'][0]\n    if (proj_domain_id and (proj_domain_id != self.default_domain_id)):\n        return False\n    (tenant_id, tenant_name) = identity['tenant']\n    (version, account, container, obj) = path_parts\n    if self._account_matches_tenant(account, tenant_id):\n        allow = True\n    else:\n        (exists, acc_domain_id) = self._get_project_domain_id(req.environ)\n        allow = (exists and (acc_domain_id in [self.default_domain_id, None]))\n    if allow:\n        self.logger.debug('Names allowed in acls.')\n    return allow\n", "label": 1}
{"function": "\n\n@jsonwrap\ndef render_POST(self, request):\n    send_cors(request)\n    err = require_args(request, ('sid', 'mxid'))\n    if err:\n        return err\n    sid = request.args['sid'][0]\n    mxid = request.args['mxid'][0]\n    if ('client_secret' in request.args):\n        clientSecret = request.args['client_secret'][0]\n    elif ('clientSecret' in request.args):\n        clientSecret = request.args['clientSecret'][0]\n    else:\n        request.setResponseCode(400)\n        return {\n            'errcode': 'M_MISSING_PARAM',\n            'error': 'No client_secret',\n        }\n    noMatchError = {\n        'errcode': 'M_NO_VALID_SESSION',\n        'error': 'No valid session was found matching that sid and client secret',\n    }\n    try:\n        res = self.sydent.threepidBinder.addBinding(sid, clientSecret, mxid)\n        return res\n    except IncorrectClientSecretException:\n        return noMatchError\n    except SessionExpiredException:\n        return {\n            'errcode': 'M_SESSION_EXPIRED',\n            'error': 'This validation session has expired: call requestToken again',\n        }\n    except InvalidSessionIdException:\n        return noMatchError\n    except SessionNotValidatedException:\n        return {\n            'errcode': 'M_SESSION_NOT_VALIDATED',\n            'error': 'This validation session has not yet been completed',\n        }\n", "label": 0}
{"function": "\n\n@staticmethod\ndef Mul(expr, assumptions):\n    '\\n        As long as there is at most only one noncommutative term:\\n        Hermitian*Hermitian         -> Hermitian\\n        Hermitian*Antihermitian     -> !Hermitian\\n        Antihermitian*Antihermitian -> Hermitian\\n        '\n    if expr.is_number:\n        return AskRealHandler._number(expr, assumptions)\n    nccount = 0\n    result = True\n    for arg in expr.args:\n        if ask(Q.antihermitian(arg), assumptions):\n            result = (result ^ True)\n        elif (not ask(Q.hermitian(arg), assumptions)):\n            break\n        if ask((~ Q.commutative(arg)), assumptions):\n            nccount += 1\n            if (nccount > 1):\n                break\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef _find_targets(self, targets):\n    found_target = False\n    for target in targets:\n        if found_target:\n            break\n        for region in target['regions']:\n            if ('subtarget' in region):\n                if region['subtarget'].startswith('left_plate'):\n                    self._targets_on_left.append(region['subtarget'])\n                    found_target = True\n                elif region['subtarget'].startswith('right_plate'):\n                    self._targets_on_right.append(region['subtarget'])\n                    found_target = True\n    if (not found_target):\n        self._operations.say('This training protocol requires a dueling tree target')\n    else:\n        self._operations.show_text_on_feed('left score: 0\\nright score: 0')\n    return found_target\n", "label": 0}
{"function": "\n\ndef get_show(self, imdb, tvdb, tvshowtitle, year):\n    try:\n        tvshowtitle = cleantitle.tv(tvshowtitle)\n        query = urlparse.urljoin(self.base_link, self.search_link)\n        result = client.source(query)\n        result = re.compile('(<li>.+?</li>)').findall(result)\n        result = [re.compile('href=\"(.+?)\">(.+?)<').findall(i) for i in result]\n        result = [i[0] for i in result if (len(i[0]) > 0)]\n        result = [i for i in result if (tvshowtitle == cleantitle.tv(i[1]))]\n        result = [i[0] for i in result][0]\n        try:\n            url = re.compile('//.+?(/.+)').findall(result)[0]\n        except:\n            url = result\n        url = client.replaceHTMLCodes(url)\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef _read(self, name, start_range=None, end_range=None):\n    name = self._path(name)\n    (headers, range_) = ({\n        \n    }, None)\n    if ((start_range is not None) and (end_range is not None)):\n        range_ = ('%s-%s' % (start_range, end_range))\n    elif (start_range is not None):\n        range_ = ('%s' % start_range)\n    if (range_ is not None):\n        headers = {\n            'Range': ('bytes=%s' % range_),\n        }\n    response = self.connection.get(self.bucket, name, headers)\n    valid_responses = [200]\n    if ((start_range is not None) or (end_range is not None)):\n        valid_responses.append(206)\n    if (response.http_response.status not in valid_responses):\n        raise S3Error(response.message)\n    headers = response.http_response.msg\n    data = response.object.data\n    if (headers.get('Content-Encoding') == 'gzip'):\n        gzf = GzipFile(mode='rb', fileobj=StringIO(data))\n        data = gzf.read()\n        gzf.close()\n    return (data, headers.get('etag', None), headers.get('content-range', None))\n", "label": 1}
{"function": "\n\ndef setup(etcdir, vardir):\n    ' Prompts the user if necessary to set up directories '\n    is_root = (os.geteuid() == 0)\n    default_etc = {\n        True: '/etc/squadron',\n        False: os.path.join(os.path.expanduser('~'), '.squadron'),\n    }\n    default_var = {\n        True: '/var/squadron',\n        False: os.path.join(os.path.expanduser('~'), '.squadron', 'state'),\n    }\n    if (not etcdir):\n        result = raw_input('Location for config [{}]: '.format(default_etc[is_root]))\n        etcdir = (result if result else default_etc[is_root])\n    if (not vardir):\n        result = raw_input('Location for state [{}]: '.format(default_var[is_root]))\n        vardir = (result if result else default_var[is_root])\n    ret = init_system(etcdir, vardir)\n    if is_root:\n        result = raw_input('Install init script? [N/y] ')\n        if ((result == 'Y') or (result == 'y')):\n            init_script = resource_string(__name__, os.path.join('init', 'init-script-ubuntu'))\n            with open('/etc/init.d/squadron', 'w') as init_file:\n                init_file.write(init_script.format(vardir))\n                os.fchmod(init_file.fileno(), ((stat.S_IRWXU | stat.S_IRGRP) | stat.S_IROTH))\n    return ret\n", "label": 0}
{"function": "\n\ndef excludeStr(self, longname, buildShort=False):\n    '\\n        Generate an \"exclusion string\" for the given option\\n\\n        @type longname: C{str}\\n        @param longname: The long option name (e.g. \"verbose\" instead of \"v\")\\n\\n        @type buildShort: C{bool}\\n        @param buildShort: May be True to indicate we\\'re building an excludes\\n            string for the short option that correspondes to the given long opt.\\n\\n        @return: The generated C{str}\\n        '\n    if (longname in self.excludes):\n        exclusions = self.excludes[longname].copy()\n    else:\n        exclusions = set()\n    if (longname not in self.multiUse):\n        if (buildShort is False):\n            short = self.getShortOption(longname)\n            if (short is not None):\n                exclusions.add(short)\n        else:\n            exclusions.add(longname)\n    if (not exclusions):\n        return ''\n    strings = []\n    for optName in exclusions:\n        if (len(optName) == 1):\n            strings.append(('-' + optName))\n        else:\n            strings.append(('--' + optName))\n    strings.sort()\n    return ('(%s)' % ' '.join(strings))\n", "label": 0}
{"function": "\n\ndef parse(self):\n    '\\n        We need to expand the default parsing to get all\\n        the cases, see the module doc.\\n        '\n    super(ObjManipCommand, self).parse()\n    obj_defs = ([], [])\n    obj_attrs = ([], [])\n    for (iside, arglist) in enumerate((self.lhslist, self.rhslist)):\n        for objdef in arglist:\n            (aliases, option, attrs) = ([], None, [])\n            if (':' in objdef):\n                (objdef, option) = [part.strip() for part in objdef.rsplit(':', 1)]\n            if (';' in objdef):\n                (objdef, aliases) = [part.strip() for part in objdef.split(';', 1)]\n                aliases = [alias.strip() for alias in aliases.split(';') if alias.strip()]\n            if ('/' in objdef):\n                (objdef, attrs) = [part.strip() for part in objdef.split('/', 1)]\n                attrs = [part.strip().lower() for part in attrs.split('/') if part.strip()]\n            obj_defs[iside].append({\n                'name': objdef,\n                'option': option,\n                'aliases': aliases,\n            })\n            obj_attrs[iside].append({\n                'name': objdef,\n                'attrs': attrs,\n            })\n    self.lhs_objs = obj_defs[0]\n    self.rhs_objs = obj_defs[1]\n    self.lhs_objattr = obj_attrs[0]\n    self.rhs_objattr = obj_attrs[1]\n", "label": 1}
{"function": "\n\ndef handleresponses(self):\n    self.rpc.fetch_responses()\n    ans = self.rpc.first_response()\n    if (not ans):\n        return False\n    (req_id, msg_id, msg_body) = ans\n    handled = False\n    for i in range(self.nolevels):\n        for responder in self.resp_idx[i]:\n            if (responder.isReqIdMatch(req_id) and responder.isMsgIdMatch(msg_id)):\n                handled = responder.handlemsg(req_id, msg_id, msg_body, self.rpc, self.parser)\n            if handled:\n                break\n        if handled:\n            break\n    if (not handled):\n        for responder in self.gen_responders:\n            handled = responder.handlemsg(req_id, msg_id, msg_body, self.rpc, self.parser)\n            if handled:\n                break\n    self.other_msgs.append((req_id, msg_id, msg_body))\n    return True\n", "label": 1}
{"function": "\n\ndef get_tree(self, name=None, backup=None, root=None, agent=None):\n    'See :func:`burpui.misc.backend.interface.BUIbackend.get_tree`'\n    res = []\n    if ((not name) or (not backup)):\n        return res\n    if (not root):\n        top = ''\n    else:\n        try:\n            top = root.decode('utf-8', 'replace')\n        except UnicodeDecodeError:\n            top = root\n    filemap = self.status('c:{0}:b:{1}:p:{2}\\n'.format(name, backup, top))\n    useful = False\n    for line in filemap:\n        if ((not useful) and re.match('^-list begin-$', line)):\n            useful = True\n            continue\n        if (useful and re.match('^-list end-$', line)):\n            useful = False\n            continue\n        if useful:\n            tree = {\n                \n            }\n            match = re.search('^(.{10})\\\\s', line)\n            if match:\n                if re.match('^(d|l)', match.group(1)):\n                    tree['type'] = 'd'\n                else:\n                    tree['type'] = 'f'\n                spl = re.split('\\\\s+', line, 7)\n                tree['mode'] = spl[0]\n                tree['inodes'] = spl[1]\n                tree['uid'] = spl[2]\n                tree['gid'] = spl[3]\n                tree['size'] = '{0:.1eM}'.format(_hr(spl[4]))\n                tree['date'] = '{0} {1}'.format(spl[5], spl[6])\n                tree['name'] = spl[7]\n                tree['parent'] = top\n                res.append(tree)\n    return res\n", "label": 1}
{"function": "\n\n@state(True)\ndef turn_on(self, transition_time, pipeline, **kwargs):\n    'Turn on (or adjust property of) a group.'\n    from limitlessled.presets import COLORLOOP\n    if (ATTR_BRIGHTNESS in kwargs):\n        self._brightness = kwargs[ATTR_BRIGHTNESS]\n    if (ATTR_RGB_COLOR in kwargs):\n        self._color = kwargs[ATTR_RGB_COLOR]\n    if (min(self._color) > (256 - RGB_BOUNDARY)):\n        pipeline.white()\n        self._color = WHITE\n    pipeline.transition(transition_time, brightness=_from_hass_brightness(self._brightness), color=_from_hass_color(self._color))\n    if (ATTR_FLASH in kwargs):\n        duration = 0\n        if (kwargs[ATTR_FLASH] == FLASH_LONG):\n            duration = 1\n        pipeline.flash(duration=duration)\n    if (ATTR_EFFECT in kwargs):\n        if (kwargs[ATTR_EFFECT] == EFFECT_COLORLOOP):\n            self.repeating = True\n            pipeline.append(COLORLOOP)\n        if (kwargs[ATTR_EFFECT] == EFFECT_WHITE):\n            pipeline.white()\n            self._color = WHITE\n", "label": 1}
{"function": "\n\ndef test_macro_api(self):\n    tmpl = self.env.from_string('{% macro foo(a, b) %}{% endmacro %}{% macro bar() %}{{ varargs }}{{ kwargs }}{% endmacro %}{% macro baz() %}{{ caller() }}{% endmacro %}')\n    assert (tmpl.module.foo.arguments == ('a', 'b'))\n    assert (tmpl.module.foo.defaults == ())\n    assert (tmpl.module.foo.name == 'foo')\n    assert (not tmpl.module.foo.caller)\n    assert (not tmpl.module.foo.catch_kwargs)\n    assert (not tmpl.module.foo.catch_varargs)\n    assert (tmpl.module.bar.arguments == ())\n    assert (tmpl.module.bar.defaults == ())\n    assert (not tmpl.module.bar.caller)\n    assert tmpl.module.bar.catch_kwargs\n    assert tmpl.module.bar.catch_varargs\n    assert tmpl.module.baz.caller\n", "label": 1}
{"function": "\n\ndef print_doc(provider_or_field=None, args=None, lang=DEFAULT_LOCALE, output=None, includes=None):\n    args = (args or [])\n    output = (output or sys.stdout)\n    fake = Faker(locale=lang, includes=includes)\n    from faker.providers import BaseProvider\n    base_provider_formatters = [f for f in dir(BaseProvider)]\n    if provider_or_field:\n        if ('.' in provider_or_field):\n            parts = provider_or_field.split('.')\n            locale = (parts[(- 2)] if (parts[(- 2)] in AVAILABLE_LOCALES) else lang)\n            fake = Factory.create(locale, providers=[provider_or_field], includes=includes)\n            doc = documentor.Documentor(fake)\n            doc.already_generated = base_provider_formatters\n            print_provider(doc, fake.get_providers()[0], doc.get_provider_formatters(fake.get_providers()[0]), output=output)\n        else:\n            try:\n                print(fake.format(provider_or_field, *args), end='', file=output)\n            except AttributeError:\n                raise ValueError('No faker found for \"{0}({1})\"'.format(provider_or_field, args))\n    else:\n        doc = documentor.Documentor(fake)\n        formatters = doc.get_formatters(with_args=True, with_defaults=True)\n        for (provider, fakers) in formatters:\n            print_provider(doc, provider, fakers, output=output)\n        for language in AVAILABLE_LOCALES:\n            if (language == lang):\n                continue\n            print(file=output)\n            print('## LANGUAGE {0}'.format(language), file=output)\n            fake = Faker(locale=language)\n            d = documentor.Documentor(fake)\n            for (p, fs) in d.get_formatters(with_args=True, with_defaults=True, locale=language, excludes=base_provider_formatters):\n                print_provider(d, p, fs, output=output)\n", "label": 1}
{"function": "\n\ndef run_migrations_online():\n    \"Run migrations in 'online' mode.\\n\\n    In this scenario we need to create an Engine\\n    and associate a connection with the context.\\n\\n    \"\n    engines = {\n        \n    }\n    for name in re.split(',\\\\s*', db_names):\n        engines[name] = rec = {\n            \n        }\n        rec['engine'] = engine_from_config(context.config.get_section(name), prefix='sqlalchemy.', poolclass=pool.NullPool)\n    for (name, rec) in engines.items():\n        engine = rec['engine']\n        rec['connection'] = conn = engine.connect()\n        if USE_TWOPHASE:\n            rec['transaction'] = conn.begin_twophase()\n        else:\n            rec['transaction'] = conn.begin()\n    try:\n        for (name, rec) in engines.items():\n            logger.info(('Migrating database %s' % name))\n            context.configure(connection=rec['connection'], upgrade_token=('%s_upgrades' % name), downgrade_token=('%s_downgrades' % name), target_metadata=target_metadata.get(name))\n            context.run_migrations(engine_name=name)\n        if USE_TWOPHASE:\n            for rec in engines.values():\n                rec['transaction'].prepare()\n        for rec in engines.values():\n            rec['transaction'].commit()\n    except:\n        for rec in engines.values():\n            rec['transaction'].rollback()\n        raise\n    finally:\n        for rec in engines.values():\n            rec['connection'].close()\n", "label": 1}
{"function": "\n\ndef getresponse(self):\n    'Get the response from the server.\\n\\n        If the HTTPConnection is in the correct state, returns an\\n        instance of HTTPResponse or of whatever object is returned by\\n        class the response_class variable.\\n\\n        If a request has not been sent or if a previous response has\\n        not be handled, ResponseNotReady is raised.  If the HTTP\\n        response indicates that the connection should be closed, then\\n        it will be closed before the response is returned.  When the\\n        connection is closed, the underlying socket is closed.\\n        '\n    if (self.__response and self.__response.isclosed()):\n        self.__response = None\n    if ((self.__state != _CS_REQ_SENT) or self.__response):\n        raise ResponseNotReady(self.__state)\n    if (self.debuglevel > 0):\n        response = self.response_class(self.sock, self.debuglevel, method=self._method)\n    else:\n        response = self.response_class(self.sock, method=self._method)\n    response.begin()\n    assert (response.will_close != _UNKNOWN)\n    self.__state = _CS_IDLE\n    if response.will_close:\n        self.close()\n    else:\n        self.__response = response\n    return response\n", "label": 0}
{"function": "\n\ndef as_json(self):\n    ' Serializes to JSON by inspecting `elementProperties()` and creating\\n        a JSON dictionary of all registered properties.\\n        '\n    js = {\n        \n    }\n    found = set()\n    nonoptionals = set()\n    for (name, jsname, typ, is_list, of_many, not_optional) in self.elementProperties():\n        if not_optional:\n            nonoptionals.add((of_many or jsname))\n        val = getattr(self, name)\n        if (val is None):\n            continue\n        if is_list:\n            if (len(val) > 0):\n                found.add((of_many or jsname))\n            js[jsname] = [(v.as_json() if hasattr(v, 'as_json') else v) for v in val]\n        else:\n            found.add((of_many or jsname))\n            js[jsname] = (val.as_json() if hasattr(val, 'as_json') else val)\n    if (len((nonoptionals - found)) > 0):\n        for nonop in (nonoptionals - found):\n            logging.warning(\"Element '{}' is not optional, you should provide a value for it on {}\".format(nonop, self))\n    return js\n", "label": 1}
{"function": "\n\ndef __sub__(self, other):\n    if isinstance(other, BiHemiLabel):\n        if (self.hemi == 'lh'):\n            return (self - other.lh)\n        else:\n            return (self - other.rh)\n    elif isinstance(other, Label):\n        if (self.subject != other.subject):\n            raise ValueError(('Label subject parameters must match, got \"%s\" and \"%s\". Consider setting the subject parameter on initialization, or setting label.subject manually before combining labels.' % (self.subject, other.subject)))\n    else:\n        raise TypeError(('Need: Label or BiHemiLabel. Got: %r' % other))\n    if (self.hemi == other.hemi):\n        keep = in1d(self.vertices, other.vertices, True, invert=True)\n    else:\n        keep = np.arange(len(self.vertices))\n    name = ('%s - %s' % ((self.name or 'unnamed'), (other.name or 'unnamed')))\n    return Label(self.vertices[keep], self.pos[keep], self.values[keep], self.hemi, self.comment, name, None, self.subject, self.color, self.verbose)\n", "label": 0}
{"function": "\n\ndef read(self, size=(- 1)):\n    rbufsize = max(self._rbufsize, self.default_bufsize)\n    buf = self._rbuf\n    buf.seek(0, 2)\n    if (size < 0):\n        self._rbuf = StringIO()\n        while True:\n            try:\n                data = self._sock.recv(rbufsize)\n            except OpenSSL.SSL.WantReadError:\n                continue\n            if (not data):\n                break\n            buf.write(data)\n        return buf.getvalue()\n    else:\n        buf_len = buf.tell()\n        if (buf_len >= size):\n            buf.seek(0)\n            rv = buf.read(size)\n            self._rbuf = StringIO()\n            self._rbuf.write(buf.read())\n            return rv\n        self._rbuf = StringIO()\n        while True:\n            left = (size - buf_len)\n            try:\n                data = self._sock.recv(left)\n            except OpenSSL.SSL.WantReadError:\n                continue\n            if (not data):\n                break\n            n = len(data)\n            if ((n == size) and (not buf_len)):\n                return data\n            if (n == left):\n                buf.write(data)\n                del data\n                break\n            assert (n <= left), ('recv(%d) returned %d bytes' % (left, n))\n            buf.write(data)\n            buf_len += n\n            del data\n        return buf.getvalue()\n", "label": 1}
{"function": "\n\ndef write(self, file, depth=0):\n    'parse XML structure recursively and append to the output fileID,\\n        increasing the offset (tabs) while descending into the tree'\n    if ('myName' not in self.tag):\n        print('Error parsing XML structure: Tag name missing!')\n        sys.exit(1)\n    nAttr = self.nbAttributes()\n    endmark = '/>'\n    if self.hasSubtag():\n        endmark = '>'\n    if (nAttr > 0):\n        file.write((((((((self._tab * depth) + '<') + self.tag['myName']) + ' ') + ' '.join([(((name + '=\"') + str(val)) + '\"') for (name, val) in self.tag.items() if ((name != 'myName') and (name != 'Icontain'))])) + endmark) + '\\n'))\n    else:\n        file.write(((((self._tab * depth) + '<') + self.tag['myName']) + '>\\n'))\n    if self.hasSubtag():\n        for subtag in self.tag['Icontain']:\n            subtag.write(file, depth=(depth + 1))\n        file.write(((((self._tab * depth) + '</') + self.tag['myName']) + '>\\n'))\n", "label": 1}
{"function": "\n\ndef setup(self, context):\n    self.cyclictest_on_device = 'cyclictest'\n    self.cyclictest_result = os.path.join(self.device.working_directory, TXT_RESULT_NAME)\n    self.cyclictest_command = '{} --clock={} --duration={}s --thread={} --latency={} {} {} > {}'\n    self.device_binary = None\n    if (not self.device.is_rooted):\n        raise WorkloadError('This workload requires a device with root premissions to run')\n    host_binary = context.resolver.get(Executable(self, self.device.abi, 'cyclictest'))\n    self.device_binary = self.device.install(host_binary)\n    self.cyclictest_command = self.cyclictest_command.format(self.device_binary, (0 if (self.clock == 'monotonic') else 1), self.duration, self.thread, self.latency, ('--quiet' if self.quiet else ''), self.extra_parameters, self.cyclictest_result)\n    if self.clear_file_cache:\n        self.device.execute('sync')\n        self.device.set_sysfile_value('/proc/sys/vm/drop_caches', 3)\n    if (self.device.platform == 'android'):\n        if (self.screen_off and self.device.is_screen_on):\n            self.device.execute('input keyevent 26')\n", "label": 0}
{"function": "\n\n@app.route('/micropub', methods=['GET', 'POST', 'PATCH', 'PUT', 'DELETE'])\ndef handleMicroPub():\n    app.logger.info(('handleMicroPub [%s]' % request.method))\n    access_token = request.headers.get('Authorization')\n    if access_token:\n        access_token = access_token.replace('Bearer ', '')\n    (me, client_id, scope) = checkAccessToken(access_token)\n    app.logger.info(('micropub %s [%s] [%s, %s, %s]' % (request.method, access_token, me, client_id, scope)))\n    if ((me is None) or (client_id is None)):\n        return ('Invalid access_token', 400, {\n            \n        })\n    elif (request.method == 'POST'):\n        domain = baseDomain(me, includeScheme=False)\n        if (domain == cfg.our_domain):\n            data = {\n                \n            }\n            for key in ('h', 'name', 'summary', 'content', 'published', 'updated', 'category', 'slug', 'location', 'in-reply-to', 'repost-of', 'syndication', 'syndicate-to'):\n                data[key] = request.form.get(key)\n            return processMicropub(me, client_id, scope, data)\n        else:\n            return ('unauthorized', 401)\n    elif (request.method == 'GET'):\n        return ('not implemented', 501)\n", "label": 0}
{"function": "\n\ndef pretty_date(time=None):\n    \"\\n    Get a datetime object or a int() Epoch timestamp and return a\\n    pretty string like 'an hour ago', 'Yesterday', '3 months ago',\\n    'just now', etc\\n    :param time:\\n    \"\n    from datetime import datetime\n    now = datetime.now()\n    if (type(time) is int):\n        diff = (now - datetime.fromtimestamp(time))\n    elif isinstance(time, datetime):\n        diff = (now - time)\n    else:\n        diff = (now - now)\n    second_diff = diff.seconds\n    day_diff = diff.days\n    if (day_diff < 0):\n        return ''\n    if (day_diff == 0):\n        if (second_diff < 10):\n            return 'just now'\n        if (second_diff < 60):\n            return (str(second_diff) + ' seconds ago')\n        if (second_diff < 120):\n            return 'a minute ago'\n        if (second_diff < 3600):\n            return (str((second_diff / 60)) + ' minutes ago')\n        if (second_diff < 7200):\n            return 'an hour ago'\n        if (second_diff < 86400):\n            return (str((second_diff / 3600)) + ' hours ago')\n    if (day_diff == 1):\n        return 'Yesterday'\n    if (day_diff < 7):\n        return (str(day_diff) + ' days ago')\n    if (day_diff < 31):\n        return (str((day_diff / 7)) + ' weeks ago')\n    if (day_diff < 365):\n        return (str((day_diff / 30)) + ' months ago')\n    return (str((day_diff / 365)) + ' years ago')\n", "label": 1}
{"function": "\n\ndef lineReceived(self, line):\n    'a line has been received.'\n    parts = line.split(None, 1)\n    if ((len(parts) == 2) and line.startswith(parts[0])):\n        (cmd, rest) = parts\n        offset = (len(cmd) + 1)\n        cmd = cmd.rstrip(self._colon_sym)\n        if (cmd in self._test_sym):\n            self.startTest(offset, line)\n        elif (cmd in self._error_sym):\n            self.addError(offset, line)\n        elif (cmd in self._failure_sym):\n            self.addFailure(offset, line)\n        elif (cmd in self._progress_sym):\n            self.parser._handleProgress(offset, line)\n        elif (cmd in self._skip_sym):\n            self.addSkip(offset, line)\n        elif (cmd in self._success_sym):\n            self.addSuccess(offset, line)\n        elif (cmd in self._tags_sym):\n            self.parser._handleTags(offset, line)\n            self.parser.subunitLineReceived(line)\n        elif (cmd in self._time_sym):\n            self.parser._handleTime(offset, line)\n            self.parser.subunitLineReceived(line)\n        elif (cmd in self._xfail_sym):\n            self.addExpectedFail(offset, line)\n        elif (cmd in self._uxsuccess_sym):\n            self.addUnexpectedSuccess(offset, line)\n        else:\n            self.parser.stdOutLineReceived(line)\n    else:\n        self.parser.stdOutLineReceived(line)\n", "label": 1}
{"function": "\n\ndef step_2(w):\n    ' Step 2 replaces double suffixes (singularization => singularize).\\n        This only happens if there is at least one vowel-consonant pair before the suffix.\\n    '\n    for (suffix, rules) in suffixes2:\n        if w.endswith(suffix):\n            for (A, B) in rules:\n                if w.endswith(A):\n                    return ((R1(w).endswith(A) and (w[:(- len(A))] + B)) or w)\n    if (w.endswith('li') and (R1(w)[(- 3):(- 2)] in VALID_LI)):\n        return w[:(- 2)]\n    return w\n", "label": 1}
{"function": "\n\ndef test_content_subreddit_from_name(reddit, terminal):\n    name = '/r/python'\n    content = SubredditContent.from_name(reddit, name, terminal.loader)\n    assert (content.name == '/r/python')\n    assert (content.order is None)\n    name = 'python/top/'\n    content = SubredditContent.from_name(reddit, name, terminal.loader)\n    assert (content.name == '/r/python')\n    assert (content.order == 'top')\n    name = '/r/python/top'\n    content = SubredditContent.from_name(reddit, name, terminal.loader, order='new')\n    assert (content.name == '/r/python')\n    assert (content.order == 'new')\n    name = '/r/python/fake'\n    with terminal.loader():\n        SubredditContent.from_name(reddit, name, terminal.loader)\n    assert isinstance(terminal.loader.exception, exceptions.SubredditError)\n    name = '/r/front/rising'\n    content = SubredditContent.from_name(reddit, name, terminal.loader)\n    assert (content.name == '/r/front')\n    assert (content.order == 'rising')\n    SubredditContent.from_name(reddit, 'front', terminal.loader, query='pea')\n    SubredditContent.from_name(reddit, 'python', terminal.loader, query='pea')\n", "label": 1}
{"function": "\n\ndef test_sync_with_push_conflict(self):\n    upstream = 'origin'\n    branch = 'master'\n    credentials = 'credentials'\n    mocked_repo = MagicMock()\n    mocked_merge = MagicMock()\n    mocked_sync_done = MagicMock()\n    mocked_syncing = MagicMock()\n    mocked_push_successful = MagicMock()\n    mocked_fetch = MagicMock()\n    mocked_strategy = MagicMock()\n    mocked_repo.behind = True\n    mocked_repo.ahead = MagicMock(1)\n    mocked_repo.push.side_effect = [GitError('Mocked error'), None]\n    with patch.multiple('gitfs.worker.sync', sync_done=mocked_sync_done, syncing=mocked_syncing, push_successful=mocked_push_successful, fetch=mocked_fetch):\n        worker = SyncWorker('name', 'email', 'name', 'email', repository=mocked_repo, strategy=mocked_strategy, credentials=credentials, upstream=upstream, branch=branch)\n        worker.merge = mocked_merge\n        while (not worker.sync()):\n            pass\n        assert (mocked_syncing.clear.call_count == 1)\n        assert (mocked_push_successful.clear.call_count == 1)\n        assert (mocked_sync_done.clear.call_count == 2)\n        assert (mocked_sync_done.set.call_count == 1)\n        assert (mocked_fetch.set.call_count == 1)\n        assert (mocked_push_successful.set.call_count == 1)\n        assert (mocked_repo.behind is False)\n        assert (mocked_repo.ahead.call_count == 2)\n        mocked_repo.push.assert_has_calls([call(upstream, branch, credentials), call(upstream, branch, credentials)])\n", "label": 1}
{"function": "\n\n@Appender(_index_shared_docs['copy'])\ndef copy(self, name=None, deep=False, dtype=None, **kwargs):\n    names = kwargs.get('names')\n    if ((names is not None) and (name is not None)):\n        raise TypeError('Can only provide one of `names` and `name`')\n    if deep:\n        from copy import deepcopy\n        new_index = self._shallow_copy(self._data.copy())\n        name = (name or deepcopy(self.name))\n    else:\n        new_index = self._shallow_copy()\n        name = self.name\n    if (name is not None):\n        names = [name]\n    if names:\n        new_index = new_index.set_names(names)\n    if dtype:\n        new_index = new_index.astype(dtype)\n    return new_index\n", "label": 0}
{"function": "\n\ndef step_4(w):\n    ' Step 4 strips -ant, -ent etc. suffixes.\\n        This only happens if there is more than one vowel-consonant pair before the suffix.\\n    '\n    for (suffix, rules) in suffixes4:\n        if w.endswith(suffix):\n            for A in rules:\n                if w.endswith(A):\n                    return ((R2(w).endswith(A) and w[:(- len(A))]) or w)\n    if (R2(w).endswith('ion') and w[:(- 3)].endswith(('s', 't'))):\n        return w[:(- 3)]\n    return w\n", "label": 1}
{"function": "\n\ndef _vpn_dict(self, context, project_id, instance):\n    elevated = context.elevated()\n    rv = {\n        'project_id': project_id,\n    }\n    if (not instance):\n        rv['state'] = 'pending'\n        return rv\n    rv['instance_id'] = instance.uuid\n    rv['created_at'] = utils.isotime(instance.created_at)\n    nw_info = compute_utils.get_nw_info_for_instance(instance)\n    if (not nw_info):\n        return rv\n    vif = nw_info[0]\n    ips = [ip for ip in vif.fixed_ips() if (ip['version'] == 4)]\n    if ips:\n        rv['internal_ip'] = ips[0]['address']\n    elevated.project_id = project_id\n    network = self.network_api.get(elevated, vif['network']['id'])\n    if network:\n        vpn_ip = network['vpn_public_address']\n        vpn_port = network['vpn_public_port']\n        rv['public_ip'] = vpn_ip\n        rv['public_port'] = vpn_port\n        if (vpn_ip and vpn_port):\n            if utils.vpn_ping(vpn_ip, vpn_port):\n                rv['state'] = 'running'\n            else:\n                rv['state'] = 'down'\n        else:\n            rv['state'] = 'invalid'\n    return rv\n", "label": 1}
{"function": "\n\ndef build_html_tree(self):\n    from grab.base import GLOBAL_STATE\n    if (self._lxml_tree is None):\n        fix_setting = self.grab.config['fix_special_entities']\n        body = self.unicode_body(fix_special_entities=fix_setting).strip()\n        if self.grab.config['lowercased_tree']:\n            body = body.lower()\n        if self.grab.config['strip_null_bytes']:\n            body = body.replace(NULL_BYTE, '')\n        if six.PY3:\n            body = RE_UNICODE_XML_DECLARATION.sub('', body)\n        else:\n            body = RE_XML_DECLARATION.sub('', body)\n        if (not body):\n            body = '<html></html>'\n        start = time.time()\n        try:\n            self._lxml_tree = self._build_dom(body, 'html')\n        except Exception as ex:\n            if (isinstance(ex, ParserError) and ('Document is empty' in str(ex)) and ('<html' not in body)):\n                body = '<html>%s</html>'.format(body)\n                self._lxml_tree = self._build_dom(body, 'html')\n            elif (isinstance(ex, TypeError) and (\"object of type 'NoneType' has no len\" in str(ex)) and ('<html' not in body)):\n                body = '<html>%s</html>'.format(body)\n                self._lxml_tree = self._build_dom(body, 'html')\n            else:\n                raise\n        GLOBAL_STATE['dom_build_time'] += (time.time() - start)\n    return self._lxml_tree\n", "label": 1}
{"function": "\n\ndef __cmp__(self, other):\n    if (other is None):\n        return 1\n    if (not isinstance(other, Version)):\n        other = Version(other)\n    for (ind, el) in enumerate(self.as_list):\n        if ((ind + 1) > len(other.as_list)):\n            return 1\n        if ((not isinstance(el, int)) and isinstance(other.as_list[ind], int)):\n            return (- 1)\n        elif ((not isinstance(other.as_list[ind], int)) and isinstance(el, int)):\n            return 1\n        elif (el == other.as_list[ind]):\n            continue\n        elif (el > other.as_list[ind]):\n            return 1\n        else:\n            return (- 1)\n    if (len(other.as_list) > len(self.as_list)):\n        return (- 1)\n    else:\n        return 0\n", "label": 1}
{"function": "\n\ndef extract(self, to_path):\n    members = [member for member in self._archive.getmembers() if (member.name != 'pax_global_header')]\n    leading = self.has_leading_dir(members)\n    for member in members:\n        name = member.name\n        if leading:\n            name = self.split_leading_dir(name)[1]\n        filename = os.path.join(to_path, name)\n        if member.isdir():\n            if (filename and (not os.path.exists(filename))):\n                os.makedirs(filename)\n        else:\n            try:\n                extracted = self._archive.extractfile(member)\n            except (KeyError, AttributeError) as exc:\n                print(('In the tar file %s the member %s is invalid: %s' % (name, member.name, exc)))\n            else:\n                dirname = os.path.dirname(filename)\n                if (dirname and (not os.path.exists(dirname))):\n                    os.makedirs(dirname)\n                with open(filename, 'wb') as outfile:\n                    shutil.copyfileobj(extracted, outfile)\n            finally:\n                if extracted:\n                    extracted.close()\n", "label": 1}
{"function": "\n\ndef fetch():\n    retval = {\n        \n    }\n    content = retrieve_content(__url__)\n    if (__check__ in content):\n        for line in content.split('\\n'):\n            line = line.strip()\n            if ((not line) or line.startswith('#') or ('.' not in line) or ('Shunlist' in line)):\n                continue\n            line = line.lower()\n            if ('malware distribution' in line):\n                info = 'malware distribution'\n            elif (' ek ' in line):\n                info = 'malicious'\n            elif any(((_ in line) for _ in ('c&c', 'malware', 'botnet', 'virus'))):\n                info = 'malware'\n            else:\n                info = 'known attacker'\n            retval[line.split(',')[0]] = (info, __reference__)\n    return retval\n", "label": 1}
{"function": "\n\ndef convert_image(target, image_size):\n    (_, extension) = os.path.splitext(os.path.basename(target))\n    if (image_size and (not all(image_size))):\n        return target\n    invalid_extensions = ('.bmp', '.dib', '.tiff', '.tif')\n    try:\n        image = Image.open(target)\n    except IOError:\n        return target\n    image_format = image.format\n    image_file_name = target\n    if ((image_size is not None) and (image.size != image_size)):\n        try:\n            image = image.resize(image_size, Image.ANTIALIAS)\n        except IOError:\n            pass\n    if (extension.lower() in invalid_extensions):\n        image_format = 'GIF'\n        image_file_name = replace_ext(target, '.gif')\n    try:\n        image.save(image_file_name, image_format)\n    except IOError:\n        return target\n    return image_file_name\n", "label": 1}
{"function": "\n\ndef format_attributes(self, obj, attrname=None, crop=True):\n    '\\n        Helper function that returns info about attributes and/or\\n        non-persistent data stored on object\\n        '\n    if attrname:\n        db_attr = [(attrname, obj.attributes.get(attrname))]\n        try:\n            ndb_attr = [(attrname, object.__getattribute__(obj.ndb, attrname))]\n        except Exception:\n            ndb_attr = None\n    else:\n        db_attr = [(attr.key, attr.value) for attr in obj.db_attributes.all()]\n        try:\n            ndb_attr = obj.nattributes.all(return_tuples=True)\n        except Exception:\n            ndb_attr = None\n    string = ''\n    if (db_attr and db_attr[0]):\n        string += '\\n{wPersistent attributes{n:'\n        for (attr, value) in db_attr:\n            string += self.list_attribute(crop, attr, value)\n    if (ndb_attr and ndb_attr[0]):\n        string += '\\n{wNon-Persistent attributes{n:'\n        for (attr, value) in ndb_attr:\n            string += self.list_attribute(crop, attr, value)\n    return string\n", "label": 1}
{"function": "\n\ndef _truncate_timestamps(self, tags):\n    lengths = [10, 16, 19]\n    for length in lengths:\n        considered_tags = [t for t in tags if (len(t.timestamp) > length)]\n        grouped_by_timestamp = {\n            \n        }\n        for t in considered_tags:\n            truncated_timestamp = t.timestamp[:length]\n            if (truncated_timestamp not in grouped_by_timestamp):\n                grouped_by_timestamp[truncated_timestamp] = []\n            grouped_by_timestamp[truncated_timestamp].append(t)\n        for (truncated_timestamp, similar_tags) in grouped_by_timestamp.items():\n            if (len(similar_tags) == 1):\n                similar_tags[0].timestamp = truncated_timestamp\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    kwargs = parse(self.queryset)\n    pargs = parse(self.params)\n    for i in kwargs:\n        if (kwargs[i] == 'False'):\n            kwargs[i] = False\n        elif (kwargs[i] == 'True'):\n            kwargs[i] = True\n        else:\n            kwargs[i] = resolve_variable(kwargs[i], context)\n    kwargs = dict([(str(x), kwargs[x]) for x in kwargs.keys()])\n    new_queryset = self.object.objects.filter(**kwargs)\n    if ('order_by' in pargs):\n        new_queryset = new_queryset.order_by(pargs['order_by'])\n    if ('limit' in pargs):\n        l = pargs['limit']\n        i = l.split(':')[0]\n        if i:\n            i = int(i)\n        else:\n            i = 0\n        j = l.split(':')[1]\n        if j:\n            j = int(j)\n        else:\n            j = 0\n        new_queryset = new_queryset[i:j]\n    if ('template' in pargs):\n        template_name = pargs['template']\n    else:\n        model = new_queryset.model\n        template_name = ('%s/%s_list.html' % (model._meta.app_label, model._meta.object_name.lower()))\n    context['object_list'] = new_queryset\n    context['today'] = datetime.date.today()\n    return template.loader.get_template(template_name).render(context)\n", "label": 1}
{"function": "\n\ndef execute(self):\n    insert_with_loop = (self._is_multi_row_insert and (self._query is None) and (self._returning is None) and (not self.database.insert_many))\n    if insert_with_loop:\n        return self._insert_with_loop()\n    if ((self._returning is not None) and (self._qr is None)):\n        return self._execute_with_result_wrapper()\n    elif (self._qr is not None):\n        return self._qr\n    else:\n        cursor = self._execute()\n        if (not self._is_multi_row_insert):\n            if self.database.insert_returning:\n                pk_row = cursor.fetchone()\n                meta = self.model_class._meta\n                clean_data = [field.python_value(column) for (field, column) in zip(meta.get_primary_key_fields(), pk_row)]\n                if self.model_class._meta.composite_key:\n                    return clean_data\n                return clean_data[0]\n            return self.database.last_insert_id(cursor, self.model_class)\n        elif self._return_id_list:\n            return map(operator.itemgetter(0), cursor.fetchall())\n        else:\n            return True\n", "label": 1}
{"function": "\n\ndef __init__(self, data):\n    self.buffer = (ctypes.c_byte * len(data))()\n    ctypes.memmove(self.buffer, data, len(data))\n    ft_library = ft_get_library()\n    self.face = FT_Face()\n    r = FT_New_Memory_Face(ft_library, self.buffer, len(self.buffer), 0, self.face)\n    if (r != 0):\n        raise base.FontException('Could not load font data')\n    self.name = self.face.contents.family_name\n    self.bold = ((self.face.contents.style_flags & FT_STYLE_FLAG_BOLD) != 0)\n    self.italic = ((self.face.contents.style_flags & FT_STYLE_FLAG_ITALIC) != 0)\n    if (self.face.contents.face_flags & FT_FACE_FLAG_SFNT):\n        name = FT_SfntName()\n        for i in range(FT_Get_Sfnt_Name_Count(self.face)):\n            result = FT_Get_Sfnt_Name(self.face, i, name)\n            if (result != 0):\n                continue\n            if (not ((name.platform_id == TT_PLATFORM_MICROSOFT) and (name.encoding_id == TT_MS_ID_UNICODE_CS))):\n                continue\n            if (name.name_id == TT_NAME_ID_FONT_FAMILY):\n                string = string_at(name.string, name.string_len)\n                self.name = string.decode('utf-16be', 'ignore')\n", "label": 0}
{"function": "\n\ndef run(self):\n    '\\n        Tries to continually dispatch messages to consumers.\\n        '\n    if self.signal_handlers:\n        self.install_signal_handler()\n    channels = self.channel_layer.router.channels\n    while (not self.termed):\n        self.in_job = False\n        (channel, content) = self.channel_layer.receive_many(channels, block=True)\n        self.in_job = True\n        if (channel is None):\n            time.sleep(0.01)\n            continue\n        logger.debug('Worker got message on %s: repl %s', channel, content.get('reply_channel', 'none'))\n        message = Message(content=content, channel_name=channel, channel_layer=self.channel_layer)\n        if (content.get('__retries__', 0) == self.message_retries):\n            message.__doomed__ = True\n        match = self.channel_layer.router.match(message)\n        if (match is None):\n            logger.exception('Could not find match for message on %s! Check your routing.', channel)\n            continue\n        else:\n            (consumer, kwargs) = match\n        if self.callback:\n            self.callback(channel, message)\n        try:\n            consumer(message, **kwargs)\n        except ConsumeLater:\n            content['__retries__'] = (content.get('__retries__', 0) + 1)\n            if (content['__retries__'] > self.message_retries):\n                logger.warning('Exceeded number of retries for message on channel %s: %s', channel, repr(content)[:100])\n                continue\n            self.channel_layer.send(channel, content)\n        except:\n            logger.exception('Error processing message with consumer %s:', name_that_thing(consumer))\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('command_type', ['command', 'response', 'error'])\ndef test_encodings(self, command_type):\n    command = {\n        command_type: 'write',\n        'none': None,\n        'bool': True,\n        'str': string.printable,\n        'int': 2271560481,\n        'list': range(100),\n    }\n    encoding = encode(command)\n    assert isinstance(encoding, str)\n    decoding = decode(encoding)\n    assert isinstance(decoding, dict)\n    assert (decoding == command)\n    assert (decoding['none'] is None)\n    assert isinstance(decoding['bool'], bool)\n    assert isinstance(decoding['str'], basestring)\n    assert isinstance(decoding['int'], Integral)\n    assert isinstance(decoding['list'], list)\n", "label": 1}
{"function": "\n\ndef test_operations():\n    q = Quantity(10, m)\n    p = Quantity(5, s)\n    assert ((- q) == Quantity((- 10), m))\n    assert (q.add(Quantity(20, m)) == Quantity(30, m))\n    assert (q.add(Quantity(20, Unit(m.dim, 10))) == Quantity(30, Unit(m.dim, 11)))\n    assert (q.sub(Quantity(20, m)) == Quantity((- 10), m))\n    assert (q.sub(Quantity(20, Unit(m.dim, 10))) == Quantity((- 10), Unit(m.dim, (- 9))))\n    assert (q.pow(2) == Quantity((10 ** 2), m.pow(2)))\n    assert (q.mul(p) == Quantity((5 * 10), m.mul(s)))\n    assert (q.div(p) == Quantity((10 / 5), m.div(s)))\n", "label": 1}
{"function": "\n\ndef _indent(elem, level=0):\n    i = ('\\n' + (level * '  '))\n    if len(elem):\n        if ((not elem.text) or (not elem.text.strip())):\n            elem.text = (i + '  ')\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n        for elem in elem:\n            _indent(elem, (level + 1))\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n    elif (level and ((not elem.tail) or (not elem.tail.strip()))):\n        elem.tail = i\n", "label": 1}
{"function": "\n\n@classmethod\ndef _fix_up_properties(cls):\n    'Fix up the properties by calling their _fix_up() method.\\n\\n    Note: This is called by MetaModel, but may also be called manually\\n    after dynamically updating a model class.\\n    '\n    kind = cls._get_kind()\n    if (not isinstance(kind, basestring)):\n        raise KindError(('Class %s defines a _get_kind() method that returns a non-string (%r)' % (cls.__name__, kind)))\n    if (not isinstance(kind, str)):\n        try:\n            kind = kind.encode('ascii')\n        except UnicodeEncodeError:\n            raise KindError(('Class %s defines a _get_kind() method that returns a Unicode string (%r); please encode using utf-8' % (cls.__name__, kind)))\n    cls._properties = {\n        \n    }\n    if (cls.__module__ == __name__):\n        return\n    for name in set(dir(cls)):\n        attr = getattr(cls, name, None)\n        if (isinstance(attr, ModelAttribute) and (not isinstance(attr, ModelKey))):\n            if name.startswith('_'):\n                raise TypeError(('ModelAttribute %s cannot begin with an underscore character. _ prefixed attributes are reserved for temporary Model instance values.' % name))\n            attr._fix_up(cls, name)\n            if isinstance(attr, Property):\n                if (attr._repeated or (isinstance(attr, StructuredProperty) and attr._modelclass._has_repeated)):\n                    cls._has_repeated = True\n                cls._properties[attr._name] = attr\n    cls._update_kind_map()\n", "label": 1}
{"function": "\n\ndef wrap_class_based_view(fun, injector):\n    cls = fun.view_class\n    name = fun.__name__\n    closure_contents = (c.cell_contents for c in fun.__closure__)\n    fun_closure = dict(zip(fun.__code__.co_freevars, closure_contents))\n    try:\n        class_kwargs = fun_closure['class_kwargs']\n    except KeyError:\n        flask_restful_api = fun_closure['self']\n        fun = fun_closure['resource']\n        fun_closure = {\n            \n        }\n        class_kwargs = {\n            \n        }\n    else:\n        flask_restful_api = None\n        class_args = fun_closure.get('class_args')\n        assert (not class_args), 'Class args are not supported, use kwargs instead'\n    if (flask_restful_api and flask_restplus and isinstance(flask_restful_api, flask_restplus.Api)):\n        if ('api' in class_kwargs):\n            raise AssertionError('api keyword argument is reserved')\n        class_kwargs['api'] = flask_restful_api\n\n    def view(*args, **kwargs):\n        self = injector.create_object(cls, additional_kwargs=class_kwargs)\n        return self.dispatch_request(*args, **kwargs)\n    if cls.decorators:\n        view.__name__ = name\n        view.__module__ = cls.__module__\n        for decorator in cls.decorators:\n            view = decorator(view)\n    view.view_class = cls\n    view.__name__ = name\n    view.__doc__ = cls.__doc__\n    view.__module__ = cls.__module__\n    view.methods = cls.methods\n    fun = view\n    if flask_restful_api:\n        return wrap_flask_restful_resource(fun, flask_restful_api, injector)\n    return fun\n", "label": 1}
{"function": "\n\ndef __init__(self, paginator_class, objects, first_page=None, per_page=None, var_name=None, number=None, key=None, override_path=None):\n    self.paginator = (paginator_class or DefaultPaginator)\n    self.objects = template.Variable(objects)\n    self.var_name = (objects if (var_name is None) else var_name)\n    self.per_page_variable = None\n    if (per_page is None):\n        self.per_page = settings.PER_PAGE\n    elif per_page.isdigit():\n        self.per_page = int(per_page)\n    else:\n        self.per_page_variable = template.Variable(per_page)\n    self.first_page_variable = None\n    if (first_page is None):\n        self.first_page = None\n    elif first_page.isdigit():\n        self.first_page = int(first_page)\n    else:\n        self.first_page_variable = template.Variable(first_page)\n    self.page_number_variable = None\n    if (number is None):\n        self.page_number = 1\n    else:\n        try:\n            self.page_number = int(number)\n        except ValueError:\n            self.page_number_variable = template.Variable(number)\n    self.querystring_key_variable = None\n    if (key is None):\n        self.querystring_key = settings.PAGE_LABEL\n    elif ((key[0] in ('\"', \"'\")) and (key[(- 1)] == key[0])):\n        self.querystring_key = key[1:(- 1)]\n    else:\n        self.querystring_key_variable = template.Variable(key)\n    self.override_path_variable = None\n    if (override_path is None):\n        self.override_path = None\n    elif ((override_path[0] in ('\"', \"'\")) and (override_path[(- 1)] == override_path[0])):\n        self.override_path = override_path[1:(- 1)]\n    else:\n        self.override_path_variable = template.Variable(override_path)\n", "label": 1}
{"function": "\n\ndef get_log_entries(self, from_tag, to_tag, skip_merges=False):\n    cmd = [self._executable, 'log']\n    if (from_tag or to_tag):\n        cmd.append(('%s%s' % ((('%s..' % to_tag) if to_tag else ''), (from_tag if from_tag else ''))))\n    cmd.append('--format=format:%H')\n    if skip_merges:\n        cmd.append('--no-merges')\n    result = self._run_command(cmd)\n    if result['returncode']:\n        raise RuntimeError(('Could not fetch commit hashes:\\n%s' % result['output']))\n    log_entries = []\n    if result['output']:\n        hashes = result['output'].splitlines()\n        for hash_ in hashes:\n            cmd = [self._executable, 'log', hash_, '-n', '1', '--format=format:%B']\n            result = self._run_command(cmd)\n            if result['returncode']:\n                raise RuntimeError(('Could not fetch commit message:\\n%s' % result['output']))\n            if (result['output'] == from_tag):\n                continue\n            msg = result['output']\n            cmd = [self._executable, 'show', hash_, '--name-only', '--format=format:\"\"']\n            result = self._run_command(cmd)\n            if result['returncode']:\n                raise RuntimeError(('Could not fetch affected paths:\\n%s' % result['output']))\n            affected_paths = result['output'].splitlines()\n            log_entries.append(LogEntry(msg, affected_paths, self._get_author(hash_)))\n    return log_entries\n", "label": 1}
{"function": "\n\ndef eventFilter(self, obj, event):\n    ' Reimplemented to hide on certain key presses and on text edit focus\\n            changes.\\n        '\n    if (obj == self._text_edit):\n        etype = event.type()\n        if (etype == QtCore.QEvent.KeyPress):\n            key = event.key()\n            if (key in (QtCore.Qt.Key_Enter, QtCore.Qt.Key_Return)):\n                self.hide()\n            elif (key == QtCore.Qt.Key_Escape):\n                self.hide()\n                return True\n        elif (etype == QtCore.QEvent.FocusOut):\n            self.hide()\n        elif (etype == QtCore.QEvent.Enter):\n            self._hide_timer.stop()\n        elif (etype == QtCore.QEvent.Leave):\n            self._leave_event_hide()\n    return super(CallTipWidget, self).eventFilter(obj, event)\n", "label": 0}
{"function": "\n\ndef get_log_args(conf, log_file_name, **kwargs):\n    cmd_args = []\n    if conf.debug:\n        cmd_args.append('--debug')\n    if conf.verbose:\n        cmd_args.append('--verbose')\n    if (conf.log_dir or conf.log_file):\n        cmd_args.append(('--log-file=%s' % log_file_name))\n        log_dir = None\n        if (conf.log_dir and conf.log_file):\n            log_dir = os.path.dirname(os.path.join(conf.log_dir, conf.log_file))\n        elif conf.log_dir:\n            log_dir = conf.log_dir\n        elif conf.log_file:\n            log_dir = os.path.dirname(conf.log_file)\n        if log_dir:\n            cmd_args.append(('--log-dir=%s' % log_dir))\n        if (kwargs.get('metadata_proxy_watch_log') is False):\n            cmd_args.append('--nometadata_proxy_watch_log')\n    elif conf.use_syslog:\n        cmd_args.append('--use-syslog')\n        if conf.syslog_log_facility:\n            cmd_args.append(('--syslog-log-facility=%s' % conf.syslog_log_facility))\n    return cmd_args\n", "label": 1}
{"function": "\n\ndef parse_node(self, node, alias_map=None, conv=None):\n    (sql, params, unknown) = self._parse(node, alias_map, conv)\n    if (unknown and conv and params):\n        params = [conv.db_value(i) for i in params]\n    if isinstance(node, Node):\n        if node._negated:\n            sql = ('NOT %s' % sql)\n        if node._alias:\n            sql = ' '.join((sql, 'AS', node._alias))\n        if node._ordering:\n            sql = ' '.join((sql, node._ordering))\n    return (sql, params)\n", "label": 1}
{"function": "\n\ndef read(self, num_bytes=None):\n    if (self.start_range and (self.start_range >= self.size)):\n        return self._empty_read()\n    args = []\n    if num_bytes:\n        if (self.start_range < 0):\n            offset = self.size\n        else:\n            offset = 0\n        args = [(self.start_range + offset), (((self.start_range + num_bytes) - 1) + offset)]\n    elif self.start_range:\n        args = [self.start_range, '']\n    try:\n        (data, etags, content_range) = self._storage._read(self.name, *args)\n    except S3Error as e:\n        if ('<Code>InvalidRange</Code>' in ('%s' % e)):\n            return self._empty_read()\n        raise\n    if (content_range is not None):\n        (current_range, size) = content_range.split(' ', 1)[1].split('/', 1)\n        (start_range, end_range) = current_range.split('-', 1)\n        (self._size, self.start_range) = (int(size), (int(end_range) + 1))\n    self.file = StringIO(data)\n    return self.file.getvalue()\n", "label": 1}
{"function": "\n\ndef unify_users(request):\n    user = request.user\n    conf = app_by_application_name('molly.auth')\n    users = set()\n    for identifier in user.useridentifier_set.all():\n        if (not (identifier.namespace in conf.unify_identifiers)):\n            continue\n        identifiers = UserIdentifier.objects.filter(namespace=identifier.namespace, value=identifier.value)\n        users |= set((i.user for i in identifiers))\n    token_namespaces = set((t.namespace for t in user.externalservicetoken_set.all()))\n    identifier_namespaces = set((i.namespace for i in user.useridentifier_set.all()))\n    root_user = min(users, key=(lambda u: u.date_joined))\n    users_without_root = users.copy()\n    users_without_root = users_without_root.remove(root_user)\n    unifying_users.send_robust(sender=request, users=users_without_root, into=root_user)\n    for u in sorted(users, cmp=(lambda x, y: ((- 1) if (x == root_user) else (1 if (y == root_user) else 0)))):\n        u.usersession_set.all().update(user=root_user)\n        for token in u.externalservicetoken_set.all():\n            if ((u != user) and (token.namespace in token_namespaces)):\n                token.delete()\n            else:\n                token.user = root_user\n                token.save()\n        for identifier in u.useridentifier_set.all():\n            if ((u != user) and (identifier.namespace in identifier_namespaces)):\n                identifier.delete()\n            else:\n                identifier.user = root_user\n                identifier.save()\n        if (u != root_user):\n            u.delete()\n", "label": 1}
{"function": "\n\ndef getAllTestCases(self, root):\n    cases = []\n    counter = 0\n    q = queue.PriorityQueue()\n    q.put((0, counter, root))\n    while (not q.empty()):\n        c = q.get()[2]\n        if self.isExcluded(c):\n            continue\n        if (not hasattr(c, '__subclasses__')):\n            cases.append(c)\n        else:\n            for r in c.__subclasses__():\n                expansions = []\n                s = 0\n                counter += 1\n                if self.exp.isExpansion(r):\n                    continue\n                if (len(r.__subclasses__()) != 0):\n                    s += 100000\n                else:\n                    rc = r\n                    r = rc(self.fqdn, self.info)\n                    if (not r.getCritical()):\n                        if (r.getSeverity() == SEV_HIGH):\n                            s += 100\n                        elif (r.getSeverity() == SEV_MED):\n                            s += 200\n                        else:\n                            s += 300\n                    s += len(rc.__bases__)\n                    expansions = self.exp.getExpansions(r)\n                q.put((s, counter, r))\n                for e in expansions:\n                    s += 1\n                    q.put((s, counter, e))\n    counter = 0\n    for c in cases:\n        s = 1\n        counter += 1\n        if c.getCritical():\n            s = 0\n        q.put((s, counter, c))\n    cases = []\n    while (not q.empty()):\n        cases.append(q.get()[2])\n    return cases\n", "label": 1}
{"function": "\n\ndef _get_raw_config(self):\n    raw_config = {\n        \n    }\n    try:\n        config = ConfigParser.ConfigParser()\n        config_path = os.path.dirname(get_config_path())\n        if os.path.exists(os.path.join(config_path, 'plugins.d')):\n            config_path = os.path.join(config_path, 'plugins.d')\n        else:\n            config_path = os.path.join(config_path, 'plugins.cfg')\n        if (not os.access(config_path, os.R_OK)):\n            self.logger.error(('Unable to read the config file at ' + config_path))\n            self.logger.error('Using no config...')\n            return raw_config\n        if os.path.isdir(config_path):\n            for config_file in glob(os.path.join(config_path, '*.cfg')):\n                config.read(config_file)\n        else:\n            config.read(config_path)\n        for section in config.sections():\n            raw_config[section] = {\n                \n            }\n            for option in config.options(section):\n                raw_config[section][option] = config.get(section, option)\n    except ConfigParser.ParsingError:\n        self.logger.error('v1 Plugins config file not found or incorrectly formatted.')\n    return raw_config\n", "label": 0}
{"function": "\n\n@staticmethod\ndef requirements(fname):\n    '\\n        Create a list of requirements from the output of the pip freeze command\\n        saved in a text file.\\n        '\n    packages = Setup.read(fname, fail_silently=True).split('\\n')\n    packages = (p.strip() for p in packages)\n    packages = (p for p in packages if (p and (not p.startswith('#'))))\n    packages = (p for p in packages if (p and (not p.startswith('https://'))))\n    packages = (p for p in packages if (p and (not p.startswith('-r '))))\n    return list(packages)\n", "label": 1}
{"function": "\n\ndef parse_dmidecode(type):\n    'Parses `dmidecode` output.\\n\\n    :param type: A string with type of entity to display.\\n\\n    :returns: A list with dictionaries of entities for specified type.\\n    '\n    output = utils.execute('dmidecode', '-q', '--type', type)\n    lines = output[0].split('\\n')\n    info = []\n    multiline_values = None\n    section = 0\n    for line in lines:\n        if ((len(line) != 0) and (len(line.strip()) == len(line))):\n            info.append({\n                \n            })\n            section = (len(info) - 1)\n        try:\n            (k, v) = (l.strip() for l in line.split(':', 1))\n        except ValueError:\n            k = line.strip()\n            if (not k):\n                multiline_values = None\n            if multiline_values:\n                info[section][multiline_values].append(k)\n        else:\n            if (not v):\n                multiline_values = k.lower()\n                info[section][multiline_values] = []\n            else:\n                info[section][k.lower()] = v\n    return info\n", "label": 1}
{"function": "\n\n@display_hook\ndef layout_display(layout, max_frames, max_branches):\n    if isinstance(layout, AdjointLayout):\n        layout = Layout.from_values(layout)\n    if (not isinstance(layout, (Layout, NdLayout))):\n        return None\n    nframes = len(unique_dimkeys(layout)[1])\n    if isinstance(layout, Layout):\n        if (layout._display == 'auto'):\n            branches = len(set([path[0] for path in list(layout.data.keys())]))\n            if (branches > max_branches):\n                return (('<tt>' + sanitize_HTML(layout)) + '</tt>')\n            elif ((len(layout.data) * nframes) > max_frames):\n                max_frame_warning(max_frames)\n                return (('<tt>' + sanitize_HTML(layout)) + '</tt>')\n    return render(layout)\n", "label": 0}
{"function": "\n\ndef setup_fields_processors(config, model_cls, schema):\n    \" Set up model fields' processors.\\n\\n    :param config: Pyramid Configurator instance.\\n    :param model_cls: Model class for field of which processors should be\\n        set up.\\n    :param schema: Dict of model JSON schema.\\n    \"\n    properties = schema.get('properties', {\n        \n    })\n    for (field_name, props) in properties.items():\n        if (not props):\n            continue\n        processors = props.get('_processors')\n        backref_processors = props.get('_backref_processors')\n        if processors:\n            processors = [resolve_to_callable(val) for val in processors]\n            setup_kwargs = {\n                'model': model_cls,\n                'field': field_name,\n            }\n            config.add_field_processors(processors, **setup_kwargs)\n        if backref_processors:\n            db_settings = props.get('_db_settings', {\n                \n            })\n            is_relationship = (db_settings.get('type') == 'relationship')\n            document = db_settings.get('document')\n            backref_name = db_settings.get('backref_name')\n            if (not (is_relationship and document and backref_name)):\n                continue\n            backref_processors = [resolve_to_callable(val) for val in backref_processors]\n            setup_kwargs = {\n                'model': engine.get_document_cls(document),\n                'field': backref_name,\n            }\n            config.add_field_processors(backref_processors, **setup_kwargs)\n", "label": 1}
{"function": "\n\ndef process_response(self, request, response):\n    'Sets the cache, if needed.'\n    if (not self._should_update_cache(request, response)):\n        return response\n    if (response.streaming or (response.status_code != 200)):\n        return response\n    if ((not request.COOKIES) and response.cookies and has_vary_header(response, 'Cookie')):\n        return response\n    timeout = get_max_age(response)\n    if (timeout is None):\n        timeout = self.cache_timeout\n    elif (timeout == 0):\n        return response\n    patch_response_headers(response, timeout)\n    if timeout:\n        cache_key = learn_cache_key(request, response, timeout, self.key_prefix, cache=self.cache)\n        if (hasattr(response, 'render') and callable(response.render)):\n            response.add_post_render_callback((lambda r: self.cache.set(cache_key, r, timeout)))\n        else:\n            self.cache.set(cache_key, response, timeout)\n    return response\n", "label": 1}
{"function": "\n\n@cuda.jit(argtypes=[f4[:, :], f4[:, :], f4[:, :]])\ndef cu_square_matrix_mul(A, B, C):\n    sA = cuda.shared.array(shape=(tpb, tpb), dtype=f4)\n    sB = cuda.shared.array(shape=(tpb, tpb), dtype=f4)\n    tx = cuda.threadIdx.x\n    ty = cuda.threadIdx.y\n    bx = cuda.blockIdx.x\n    by = cuda.blockIdx.y\n    bw = cuda.blockDim.x\n    bh = cuda.blockDim.y\n    x = (tx + (bx * bw))\n    y = (ty + (by * bh))\n    acc = 0.0\n    for i in range(bpg):\n        if ((x < n) and (y < n)):\n            sA[(ty, tx)] = A[(y, (tx + (i * tpb)))]\n            sB[(ty, tx)] = B[((ty + (i * tpb)), x)]\n        cuda.syncthreads()\n        if ((x < n) and (y < n)):\n            for j in range(tpb):\n                acc += (sA[(ty, j)] * sB[(j, tx)])\n        cuda.syncthreads()\n    if ((x < n) and (y < n)):\n        C[(y, x)] = acc\n", "label": 1}
{"function": "\n\ndef __init__(self, left_size, right_size, out_size, nobias=False, initialW=None, initial_bias=None):\n    super(Bilinear, self).__init__(W=(left_size, right_size, out_size))\n    self.in_sizes = (left_size, right_size)\n    self.nobias = nobias\n    if (initialW is not None):\n        assert (initialW.shape == self.W.data.shape)\n        self.W.data[...] = initialW\n    else:\n        in_size = ((left_size * right_size) * out_size)\n        self.W.data[...] = numpy.random.normal(0, numpy.sqrt((1.0 / in_size)), self.W.data.shape)\n    if (not self.nobias):\n        self.add_param('V1', (left_size, out_size))\n        self.add_param('V2', (right_size, out_size))\n        self.add_param('b', out_size)\n        if (initial_bias is not None):\n            (V1, V2, b) = initial_bias\n            assert (V1.shape == self.V1.data.shape)\n            assert (V2.shape == self.V2.data.shape)\n            assert (b.shape == self.b.data.shape)\n            self.V1.data[...] = V1\n            self.V2.data[...] = V2\n            self.b.data[...] = b\n        else:\n            self.V1.data[...] = numpy.random.normal(0, numpy.sqrt((1.0 / left_size)), (left_size, out_size))\n            self.V2.data[...] = numpy.random.normal(0, numpy.sqrt((1.0 / right_size)), (right_size, out_size))\n            self.b.data.fill(0)\n", "label": 0}
{"function": "\n\ndef GetApplicationPath(file=None):\n    import re, os, platform\n    if (not hasattr(GetApplicationPath, 'dir')):\n        if hasattr(sys, 'frozen'):\n            dir = os.path.dirname(sys.executable)\n        elif ('__file__' in globals()):\n            dir = os.path.dirname(os.path.realpath(__file__))\n        else:\n            dir = os.getcwd()\n        GetApplicationPath.dir = dir\n    if (file is None):\n        file = ''\n    if ((not file.startswith('/')) and (not file.startswith('\\\\')) and (not re.search('^[\\\\w-]+:', file))):\n        path = ((GetApplicationPath.dir + os.sep) + file)\n        if (platform.system() == 'Windows'):\n            path = re.sub('[/\\\\\\\\]+', re.escape(os.sep), path)\n        path = re.sub('[/\\\\\\\\]+$', '', path)\n        return path\n    return str(file)\n", "label": 1}
{"function": "\n\ndef take_action(self, parsed_args):\n    self.log.debug('take_action(%s)', parsed_args)\n    senlin_client = self.app.client_manager.clustering\n    try:\n        if ((not parsed_args.force) and sys.stdin.isatty()):\n            sys.stdout.write(_('Are you sure you want to delete this node(s) [y/N]?'))\n            prompt_response = sys.stdin.readline().lower()\n            if (not prompt_response.startswith('y')):\n                return\n    except KeyboardInterrupt:\n        self.log.info(_LI('Ctrl-c detected.'))\n        return\n    except EOFError:\n        self.log.info(_LI('Ctrl-d detected'))\n        return\n    failure_count = 0\n    for nid in parsed_args.node:\n        try:\n            senlin_client.delete_node(nid, False)\n        except Exception as ex:\n            failure_count += 1\n            print(ex)\n    if failure_count:\n        raise exc.CommandError((_('Failed to delete %(count)s of the %(total)s specified node(s).') % {\n            'count': failure_count,\n            'total': len(parsed_args.node),\n        }))\n    print('Request accepted')\n", "label": 1}
{"function": "\n\ndef readinto(self, b):\n    if (self.fp is None):\n        return 0\n    if (self._method == 'HEAD'):\n        self._close_conn()\n        return 0\n    if self.chunked:\n        return self._readinto_chunked(b)\n    if (self.length is not None):\n        if (len(b) > self.length):\n            b = memoryview(b)[0:self.length]\n    if PY2:\n        data = self.fp.read(len(b))\n        n = len(data)\n        b[:n] = data\n    else:\n        n = self.fp.readinto(b)\n    if ((not n) and b):\n        self._close_conn()\n    elif (self.length is not None):\n        self.length -= n\n        if (not self.length):\n            self._close_conn()\n    return n\n", "label": 1}
{"function": "\n\ndef find_package(name, installed, package=False):\n    'Finds a package in the installed list.\\n\\n    If `package` is true, match package names, otherwise, match import paths.\\n    '\n    if package:\n        name = name.lower()\n        tests = ((lambda x: (x.user and (name == x.name.lower()))), (lambda x: (x.local and (name == x.name.lower()))), (lambda x: (name == x.name.lower())))\n    else:\n        tests = ((lambda x: (x.user and (name in x.import_names))), (lambda x: (x.local and (name in x.import_names))), (lambda x: (name in x.import_names)))\n    for t in tests:\n        try:\n            found = list(filter(t, installed))\n            if (found and (not found[0].is_scan)):\n                return found[0]\n        except StopIteration:\n            pass\n    return None\n", "label": 1}
{"function": "\n\ndef __init__(self, name, length=None, decimalCount=None, start=None, stop=None, ignoreErrors=False):\n    'Initialize instance.'\n    assert (self.typeCode is not None), 'Type code must be overriden'\n    assert (self.defaultValue is not None), 'Default value must be overriden'\n    if (len(name) > 10):\n        raise ValueError(('Field name \"%s\" is too long' % name))\n    name = str(name).upper()\n    if (self.__class__.length is None):\n        if (length is None):\n            raise ValueError((\"[%s] Length isn't specified\" % name))\n        length = int(length)\n        if (length <= 0):\n            raise ValueError(('[%s] Length must be a positive integer' % name))\n    else:\n        length = self.length\n    if (decimalCount is None):\n        decimalCount = 0\n    self.name = name\n    self.length = length\n    self.decimalCount = decimalCount\n    self.ignoreErrors = ignoreErrors\n    self.start = start\n    self.end = stop\n", "label": 0}
{"function": "\n\ndef positions(keyword='', serps='', topkeyword=''):\n    'Return a position/url paired JSON dict of all results for keyword.'\n    if topkeyword:\n        keyword = topkeyword\n    if (not serps):\n\n        def gserps(keyword):\n            global serps\n            return serps(keyword)\n        serps = gserps(keyword)\n    elif serps:\n        serps = json.loads(serps)\n        easydict = {\n            \n        }\n        serpos = 1\n        rd = serps[0]['responseData']\n        if (rd == None):\n            return 'API quota likely exceeded'\n        for serpage in serps:\n            try:\n                serpage = serpage['responseData']['results']\n                for result in serpage:\n                    easydict[serpos] = result['url']\n                    serpos += 1\n            except:\n                pass\n        return json.dumps(easydict)\n    else:\n        return 'Error'\n", "label": 0}
{"function": "\n\ndef log_actor_new(self, actor_id, actor_name, actor_type, is_shadow):\n    ' Trace actor new\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_ACTOR_NEW in logger.events)):\n            if ((not logger.actors) or (actor_id in logger.actors)):\n                data = {\n                    \n                }\n                data['timestamp'] = time.time()\n                data['node_id'] = self.node.id\n                data['type'] = 'actor_new'\n                data['actor_id'] = actor_id\n                data['actor_name'] = actor_name\n                data['actor_type'] = actor_type\n                data['is_shadow'] = is_shadow\n                if (logger.connection is not None):\n                    if (not logger.connection.connection_lost):\n                        logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                    else:\n                        disconnected.append(user_id)\n                elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                    msg = {\n                        'cmd': 'logevent',\n                        'msgid': logger.handle,\n                        'header': None,\n                        'data': ('data: %s\\n\\n' % json.dumps(data)),\n                    }\n                    self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef handle_task(self, task, **kwargs):\n    log_prefix = ('%s - ' % (task.id,))\n    task_expiration = kwargs.get('task_expiration', None)\n    if self.expire_task(task, task_expiration):\n        return None\n    monitor = Monitor.registry[task.context['monitor']['name']]\n    command = monitor.command\n    task.context = command.build_context(task.context)\n    previous_state = self.get_state(task.id)\n    timeout = task.context.get('monitor_timeout', kwargs.get('monitor_timeout'))\n    max_retries = task.context.get('max_retries', kwargs.get('max_retries'))\n    last_attempt = int(task.attempt)\n    current_attempt = (last_attempt + 1)\n    result = self.execute_task(task, timeout, **kwargs)\n    if (result.state == types.STATE_OK):\n        if (previous_state and (not (previous_state.state == types.STATE_OK)) and (previous_state.state_type == types.STATE_TYPE_SOFT)):\n            result.state_type = types.STATE_TYPE_SOFT\n    else:\n        logger.debug((log_prefix + 'current_attempt: %d, max_retries: %d'), current_attempt, max_retries)\n        if (current_attempt <= max_retries):\n            if ((not previous_state) or (previous_state.state_type == types.STATE_TYPE_SOFT) or (previous_state.state == types.STATE_OK)):\n                result.state_type = types.STATE_TYPE_SOFT\n                delay = task.context.get('retry_delay', kwargs.get('retry_delay'))\n                delay = max(delay, 0)\n                logger.debug('Resubmitting task with %ds delay.', delay)\n                self.resubmit_task(task, delay, **kwargs)\n        else:\n            logger.debug('Retry limit hit, not resubmitting.')\n    result.validate()\n    return result\n", "label": 1}
{"function": "\n\ndef RunCommand(self):\n    'Command entry point for the cat command.'\n    show_header = False\n    request_range = None\n    start_byte = 0\n    end_byte = None\n    if self.sub_opts:\n        for (o, a) in self.sub_opts:\n            if (o == '-h'):\n                show_header = True\n            elif (o == '-r'):\n                request_range = a.strip()\n                range_matcher = re.compile('^(?P<start>[0-9]+)-(?P<end>[0-9]*)$|^(?P<endslice>-[0-9]+)$')\n                range_match = range_matcher.match(request_range)\n                if (not range_match):\n                    raise CommandException(('Invalid range (%s)' % request_range))\n                if range_match.group('start'):\n                    start_byte = long(range_match.group('start'))\n                if range_match.group('end'):\n                    end_byte = long(range_match.group('end'))\n                if range_match.group('endslice'):\n                    start_byte = long(range_match.group('endslice'))\n            else:\n                self.RaiseInvalidArgumentException()\n    return CatHelper(self).CatUrlStrings(self.args, show_header=show_header, start_byte=start_byte, end_byte=end_byte)\n", "label": 1}
{"function": "\n\ndef run(self):\n    if (not self.action):\n        return\n    try:\n        S.SESSION_BUSY = True\n        if (self.action == ACTION_EVALUATE):\n            self.evaluate(self.get_option('expression'))\n        elif (self.action == ACTION_EXECUTE):\n            self.execute(self.get_option('command'))\n        elif (self.action == ACTION_INIT):\n            self.init()\n        elif (self.action == ACTION_REMOVE_BREAKPOINT):\n            self.remove_breakpoint(self.get_option('breakpoint_id'))\n        elif (self.action == ACTION_SET_BREAKPOINT):\n            self.set_breakpoint(self.get_option('filename'), self.get_option('lineno'), self.get_option('expression'))\n        elif (self.action == ACTION_STATUS):\n            self.status()\n        elif (self.action == ACTION_USER_EXECUTE):\n            self.user_execute(self.get_option('command'), self.get_option('args'))\n        elif (self.action == ACTION_WATCH):\n            self.watch_expression()\n    except ProtocolConnectionException:\n        e = sys.exc_info()[1]\n        self.timeout((lambda : connection_error(('%s' % e))))\n    finally:\n        S.SESSION_BUSY = False\n", "label": 1}
{"function": "\n\ndef move(self, entries, directory):\n    '\\n        Move one or more entries (file or directory) to the destination\\n        directory\\n\\n        :param list entries: a list of source entries (:class:`.BaseFile`\\n            object)\\n        :param directory: destination directory\\n        :return: whether the action is successful\\n        :raise: :class:`.APIError` if something bad happened\\n        '\n    fcids = []\n    for entry in entries:\n        if isinstance(entry, File):\n            fcid = entry.fid\n        elif isinstance(entry, Directory):\n            fcid = entry.cid\n        else:\n            raise APIError('Invalid BaseFile instance for an entry.')\n        fcids.append(fcid)\n    if (not isinstance(directory, Directory)):\n        raise APIError('Invalid destination directory.')\n    if self._req_files_move(directory.cid, fcids):\n        for entry in entries:\n            if isinstance(entry, File):\n                entry.cid = directory.cid\n            entry.reload()\n        return True\n    else:\n        raise APIError('Error moving entries.')\n", "label": 0}
{"function": "\n\ndef short_contests(platforms):\n    'Gets all the short contests(less than or equal to 4 hours of duration)'\n    contests_data = get_contests_data()\n    active_contests = contests_data['active']\n    upcoming_contests = contests_data['pending']\n    platform_filter = get_platform_filter(platforms)\n    get_challenge_duration = (lambda x: (int(x.split(':')[0]) if ('days' not in x) else float('inf')))\n    short_contests = [contest for contest in active_contests if ((get_challenge_duration(contest['duration']) <= 4) and (contest['host_name'] in platform_filter))]\n    short_contests += [contest for contest in upcoming_contests if ((get_challenge_duration(contest['duration']) <= 4) and (contest['host_name'] in platform_filter))]\n    return short_contests\n", "label": 0}
{"function": "\n\ndef write(self):\n    for (name, metric) in self.registry:\n        if isinstance(metric, Meter):\n            self.send_metric(name, 'meter', metric, ['count', 'one_minute_rate', 'five_minute_rate', 'fifteen_minute_rate', 'mean_rate'])\n        if isinstance(metric, Gauge):\n            self.send_metric(name, 'gauge', metric, ['value'])\n        if isinstance(metric, UtilizationTimer):\n            self.send_metric(name, 'timer', metric, ['count', 'one_minute_rate', 'five_minute_rate', 'fifteen_minute_rate', 'mean_rate', 'min', 'max', 'mean', 'stddev', 'one_minute_utilization', 'five_minute_utilization', 'fifteen_minute_utilization', 'mean_utilization'], ['median', 'percentile_95th'])\n        if isinstance(metric, Timer):\n            self.send_metric(name, 'timer', metric, ['count', 'one_minute_rate', 'five_minute_rate', 'fifteen_minute_rate', 'mean_rate', 'min', 'max', 'mean', 'stddev'], ['median', 'percentile_95th'])\n        if isinstance(metric, Counter):\n            self.send_metric(name, 'counter', metric, ['count'])\n        if isinstance(metric, Histogram):\n            self.send_metric(name, 'histogram', metric, ['count', 'min', 'max', 'mean', 'stddev'], ['median', 'percentile_95th'])\n    self._send()\n", "label": 0}
{"function": "\n\ndef process_request(self, request):\n    if (not hasattr(request, 'user')):\n        raise ImproperlyConfigured(\"The Django remote user auth middleware requires the authentication middleware to be installed.  Edit your MIDDLEWARE_CLASSES setting to insert 'django.contrib.auth.middleware.AuthenticationMiddleware' before the RemoteUserMiddleware class.\")\n    try:\n        username = request.META[self.header]\n    except KeyError:\n        if (self.force_logout_if_no_header and request.user.is_authenticated):\n            self._remove_invalid_user(request)\n        return\n    if request.user.is_authenticated:\n        if (request.user.get_username() == self.clean_username(username, request)):\n            return\n        else:\n            self._remove_invalid_user(request)\n    user = auth.authenticate(remote_user=username)\n    if user:\n        request.user = user\n        auth.login(request, user)\n", "label": 0}
{"function": "\n\ndef update_state(bug):\n    log.debug('Starting bug \"{}\" update'.format(bug.externalId))\n    now = datetime.utcnow()\n    td = (now - datetime.replace(bug.updated, tzinfo=None))\n    diff = ((td.microseconds + ((td.seconds + ((td.days * 24) * 3600)) * (10 ** 6))) / (10 ** 6))\n    if (bug.state in settings.BUG_STATE_EXPIRED):\n        old_state = bug.state\n        new_state = get_issue_fields_from_bts(bug.externalId)['status']['name']\n        log.debug('Comparing bug state,\"{0}\" and \"{1}\"'.format(old_state, new_state))\n        if ((old_state == new_state) and (diff > float(settings.BUG_TIME_EXPIRED))):\n            log.debug('Bug \"{}\" expired, deleting it from DB'.format(bug.externalId))\n            bug.delete()\n        elif ((old_state == new_state) and (diff < float(settings.BUG_TIME_EXPIRED))):\n            log.debug('Bug \"{}\" not updated, because {} seconds not expired'.format(bug.externalId, settings.BUG_TIME_EXPIRED))\n        else:\n            bug.state = new_state\n            bug.updated = now\n            log.debug('Saving bug \"{}\"'.format(bug.externalId))\n            bug.save()\n    if ((bug.state not in settings.BUG_STATE_EXPIRED) and (diff > float(settings.TIME_BEFORE_UPDATE_BUG_INFO))):\n        log.debug('%s > %s time to update bug state.', diff, settings.TIME_BEFORE_UPDATE_BUG_INFO)\n        bug.updated = now\n        bug.state = get_issue_fields_from_bts(bug.externalId)['status']['name']\n        log.debug('Saving bug \"{}\"'.format(bug.externalId))\n        bug.save()\n", "label": 0}
{"function": "\n\n@staticmethod\ndef Mul(expr, assumptions):\n    '\\n        Integer*Integer      -> Integer\\n        Integer*Irrational   -> !Integer\\n        Odd/Even             -> !Integer\\n        Integer*Rational     -> ?\\n        '\n    if expr.is_number:\n        return AskIntegerHandler._number(expr, assumptions)\n    _output = True\n    for arg in expr.args:\n        if (not ask(Q.integer(arg), assumptions)):\n            if arg.is_Rational:\n                if (arg.q == 2):\n                    return ask(Q.even((2 * expr)), assumptions)\n                if (~ (arg.q & 1)):\n                    return None\n            elif ask(Q.irrational(arg), assumptions):\n                if _output:\n                    _output = False\n                else:\n                    return\n            else:\n                return\n    else:\n        return _output\n", "label": 1}
{"function": "\n\ndef compute_whdr(self, r, delta=0.1):\n    ' Compute the Weighted Human Disagreement for a reflectance image\\n        ``r``.  '\n    error_sum = 0.0\n    weight_sum = 0.0\n    for c in self.comparisons:\n        point1 = self.id_to_points[c['point1']]\n        point2 = self.id_to_points[c['point2']]\n        darker = c['darker']\n        weight = c['darker_score']\n        if ((not point1['opaque']) or (not point2['opaque'])):\n            continue\n        if ((weight < 0) or (weight is None)):\n            raise ValueError(('Invalid darker_score: %s' % weight))\n        if (darker not in ('1', '2', 'E')):\n            raise ValueError(('Invalid darker: %s' % darker))\n        l1 = np.mean(r[(int((point1['y'] * r.shape[0])), int((point1['x'] * r.shape[1])), ...)])\n        l2 = np.mean(r[(int((point2['y'] * r.shape[0])), int((point2['x'] * r.shape[1])), ...)])\n        l1 = max(l1, 1e-10)\n        l2 = max(l2, 1e-10)\n        if ((l2 / l1) > (1.0 + delta)):\n            alg_darker = '1'\n        elif ((l1 / l2) > (1.0 + delta)):\n            alg_darker = '2'\n        else:\n            alg_darker = 'E'\n        if (darker != alg_darker):\n            error_sum += weight\n        weight_sum += weight\n    if weight_sum:\n        return (error_sum / weight_sum)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef _find_changed_targets(self):\n    changed = self._directly_changed_targets()\n    if (not changed):\n        return changed\n    if (self._include_dependees == 'none'):\n        return changed\n    for address in self._address_mapper.scan_addresses():\n        self._build_graph.inject_address_closure(address)\n    if (self._include_dependees == 'direct'):\n        return changed.union(*[self._build_graph.dependents_of(addr) for addr in changed])\n    if (self._include_dependees == 'transitive'):\n        return set((t.address for t in self._build_graph.transitive_dependees_of_addresses(changed)))\n    raise ValueError('Unknown dependee inclusion: \"{}\"'.format(self._include_dependees))\n", "label": 0}
{"function": "\n\n@internationalizeDocstring\ndef decode(self, irc, msg, args, encoding, text):\n    '<encoding> <text>\\n\\n        Returns an un-encoded form of the given text; the valid encodings are\\n        available in the documentation of the Python codecs module:\\n        <http://docs.python.org/library/codecs.html#standard-encodings>.\\n        '\n    if (encoding in 'base64 bz2 hex quopri uu zlib'):\n        encoding += '_codec'\n    if (encoding.endswith('_codec') and (encoding != 'base64_codec')):\n        text = codecs.getdecoder('base64_codec')(text.encode())[0]\n    try:\n        decoder = codecs.getdecoder(encoding)\n    except LookupError:\n        irc.errorInvalid(_('encoding'), encoding)\n    if (minisix.PY3 and (not isinstance(text, bytes))):\n        text = text.encode()\n    try:\n        text = decoder(text)[0]\n    except binascii.Error:\n        irc.errorInvalid(_('base64 string'), s=_(\"Base64 strings must be a multiple of 4 in length, padded with '=' if necessary.\"))\n        return\n    if (minisix.PY2 and isinstance(text, unicode)):\n        text = text.encode('utf-8')\n    elif (minisix.PY3 and isinstance(text, bytes)):\n        try:\n            text = text.decode()\n        except UnicodeDecodeError:\n            pass\n    irc.reply(text)\n", "label": 1}
{"function": "\n\ndef parseGnmap(inFile):\n    targets = {\n        \n    }\n    for hostLine in inFile:\n        currentTarget = []\n        fields = hostLine.split(' ')\n        ip = fields[1]\n        for item in fields:\n            if ((item.find('http') != (- 1)) and re.findall('\\\\d+/open', item)):\n                port = None\n                https = False\n                \"\\n\\t\\t\\t\\tnmap has a bunch of ways to list HTTP like services, for example:\\n\\t\\t\\t\\t8089/open/tcp//ssl|http\\n\\t\\t\\t\\t8000/closed/tcp//http-alt///\\n\\t\\t\\t\\t8008/closed/tcp//http///\\n\\t\\t\\t\\t8080/closed/tcp//http-proxy//\\n\\t\\t\\t\\t443/open/tcp//ssl|https?///\\n\\t\\t\\t\\t8089/open/tcp//ssl|http\\n\\t\\t\\t\\tSince we want to detect them all, let's just match on the word http\\n\\t\\t\\t\\tand make special cases for things containing https and ssl when we\\n\\t\\t\\t\\tconstruct the URLs.\\n\\t\\t\\t\\t\"\n                port = item.split('/')[0]\n                if ((item.find('https') != (- 1)) or (item.find('ssl') != (- 1))):\n                    https = True\n                currentTarget.append([port, https])\n        if (len(currentTarget) > 0):\n            targets[ip] = currentTarget\n    return targets\n", "label": 0}
{"function": "\n\ndef handle_osx(self, lbry_name):\n    lbry_process = [d for d in subprocess.Popen(['ps', 'aux'], stdout=subprocess.PIPE).stdout.readlines() if (('LBRY.app' in d) and ('LBRYURIHandler' not in d))]\n    try:\n        status = self.daemon.is_running()\n    except:\n        status = None\n    if (lbry_process or status):\n        self.check_status()\n        started = False\n    else:\n        os.system('open /Applications/LBRY.app')\n        self.check_status()\n        started = True\n    if ((lbry_name == 'lbry') or ((lbry_name == '') and (not started))):\n        webbrowser.open(UI_ADDRESS)\n    else:\n        webbrowser.open(((UI_ADDRESS + '/view?name=') + lbry_name))\n", "label": 1}
{"function": "\n\n@task(default_retry_delay=5, max_retries=3)\ndef import_feed(self, feed):\n    from molly.apps.feeds.models import Item\n    feed_data = feedparser.parse(feed.rss_url)\n    try:\n        feed.last_modified = struct_to_datetime(feed_data.feed.updated_parsed)\n    except:\n        feed.last_modified = parse_date(feed_data.headers.get('last-modified', datetime.now().strftime('%a, %d %b %Y %H:%M:%S +0000')))\n    feed.save(update_last_modified=True)\n    items = set()\n    for x_item in feed_data.entries:\n        if hasattr(x_item, 'id'):\n            guid = x_item.id\n        else:\n            guid = x_item.link\n        try:\n            last_modified = datetime(*x_item.date_parsed[:7])\n        except:\n            last_modified = None\n        for i in items:\n            if (i.guid == guid):\n                item = i\n                break\n        else:\n            try:\n                item = Item.objects.get(guid=guid, feed=feed)\n            except Item.DoesNotExist:\n                item = Item(guid=guid, last_modified=datetime(1900, 1, 1), feed=feed)\n        if (True or (item.last_modified < last_modified)):\n            item.title = x_item.title\n            item.description = sanitise_html(x_item.get('description', ''))\n            item.link = x_item.link\n            item.last_modified = last_modified\n            item.save()\n        items.add(item)\n    for item in Item.objects.filter(feed=feed):\n        if (item not in items):\n            item.delete()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_stream(body, previous_fragment_data=''):\n    (content_type, version_major, version_minor, length) = struct.unpack_from('!BBBH', body, 0)\n    if ((version_major != 3) or (version_minor > 3)):\n        raise ValueError('Bad TLS Version for SSL3-TLS1.2 parsing')\n    if (content_type not in name_map):\n        raise ValueError(('Unknown content type %d' % content_type))\n    fragment = body[5:(5 + length)]\n    original_fragment = fragment\n    fragment = (previous_fragment_data + fragment)\n    if (len(fragment) < length):\n        raise TlsRecordIncompleteError(len(fragment), length)\n    type = TlsRecord.type_map.get(content_type, OpaqueFragment)\n    objs = []\n    try:\n        if (fragment == ''):\n            (obj, size) = type.from_stream(fragment)\n            objs.append(obj)\n        while (fragment != ''):\n            (obj, size) = type.from_stream(fragment)\n            objs.append(obj)\n            fragment = fragment[size:]\n    except TlsNotEnoughDataError:\n        raise TlsMessageFragmentedError(original_fragment, (5 + length))\n    return (TlsRecord(content_type, Version(version_major, version_minor), objs), (5 + length))\n", "label": 0}
{"function": "\n\ndef get_message(data):\n    '\\n    Extract status and message from the given dictionary.\\n    The message is translated according to the hash map.\\n\\n    :param data: status and message hash/key pair\\n    :type data: dict\\n    :returns: (success, message) tuple\\n    '\n    success = data['success']\n    message = ''\n    if success:\n        if (('message' in data) and (len(data['message']) == 2)):\n            hash_map = data['message'][0]\n            message_code = data['message'][1]\n            message = getattr(g, hash_map)(message_code)\n    elif (('errors' in data) and (len(data['errors']) > 0)):\n        error_tuple = data['errors'][0].get('code')\n        if (error_tuple and (len(error_tuple) == 2)):\n            hash_map = error_tuple[0]\n            error_code = error_tuple[1]\n            message = getattr(g, hash_map)(error_code)\n    return (success, message)\n", "label": 0}
{"function": "\n\ndef get_constraint_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances')\n    constraint_matrix = [[True for j in xrange(num_instances)] for i in xrange(num_hosts)]\n    instance_type = (filter_properties.get('instance_type') or {\n        \n    })\n    requested_ram = instance_type.get('memory_mb', 0)\n    if ('memory_mb' not in instance_type):\n        LOG.warn(_LW(\"No information about requested instances' RAM size was found, default value (0) is used.\"))\n    if (requested_ram <= 0):\n        LOG.warn(_LW('RamConstraint is skipped because requested instance RAM size is 0 or invalid.'))\n        return constraint_matrix\n    for i in xrange(num_hosts):\n        ram_allocation_ratio = self._get_ram_allocation_ratio(hosts[i], filter_properties)\n        free_ram_mb = hosts[i].free_ram_mb\n        total_usable_ram_mb = hosts[i].total_usable_ram_mb\n        memory_mb_limit = (total_usable_ram_mb * ram_allocation_ratio)\n        used_ram_mb = (total_usable_ram_mb - free_ram_mb)\n        usable_ram = (memory_mb_limit - used_ram_mb)\n        acceptable_num_instances = int((usable_ram / requested_ram))\n        if (acceptable_num_instances < num_instances):\n            inacceptable_num = (num_instances - acceptable_num_instances)\n            constraint_matrix[i] = ([True for j in xrange(acceptable_num_instances)] + [False for j in xrange(inacceptable_num)])\n        LOG.debug('%(host)s can accept %(num)s requested instances according to RamConstraint.', {\n            'host': hosts[i],\n            'num': acceptable_num_instances,\n        })\n        hosts[i].limits['memory_mb'] = memory_mb_limit\n    return constraint_matrix\n", "label": 1}
{"function": "\n\ndef find_gopaths(self):\n    'search for potential GOPATHs.'\n    goroot = set((os.path.normpath(s) for s in os.environ.get('GOROOT', '').split(os.pathsep)))\n    gopath = set((os.path.normpath(s) for s in os.environ.get('GOPATH', '').split(os.pathsep)))\n    if ('.' in gopath):\n        gopath.remove('.')\n    gopath = list(gopath)\n    dirparts = os.path.dirname(self.filename).split(os.sep)\n    for i in range((len(dirparts) - 1), 1, (- 1)):\n        if (dirparts[i].lower() != 'src'):\n            continue\n        p = os.path.normpath(os.sep.join(dirparts[:i]))\n        if ((p not in goroot) and (p not in gopath)):\n            gopath.append(p)\n    if persist.debug_mode():\n        persist.printf('{}: {} {}'.format(self.name, os.path.basename((self.filename or '<unsaved>')), ('guessed GOPATH=' + os.pathsep.join(gopath))))\n    return os.pathsep.join(gopath)\n", "label": 1}
{"function": "\n\ndef star_status(cluster, logdir, cmdline, *args):\n    'Print result itemization'\n    if (not args):\n        args = sorted(cluster.connections.keys())\n    if (not cluster.last_result):\n        print('Cluster status not availble / no command has been run')\n        return\n    missing_results = []\n    for x in args:\n        job = cluster.last_result.get(cluster.locate(x), None)\n        if job:\n            res = job.result\n            running_time = (job.end_time - job.start_time)\n            if isinstance(res, CommandResult):\n                print(('%s: %s - Return Code [%s] took %0.4g seconds' % (x, res.status, res.return_code, running_time)))\n            else:\n                print(('%s: *** Error *** [%s] took %0.4g seconds' % (x, repr(res), running_time)))\n        else:\n            missing_results.append(x)\n    if missing_results:\n        if (len(missing_results) < 10):\n            print('Missing results from:', ', '.join(missing_results))\n        else:\n            print(('Missing results from %d hosts' % len(missing_results)))\n", "label": 0}
{"function": "\n\ndef __call__(self, irc, msg):\n    self.__parent.__call__(irc, msg)\n    if self.disabled(irc):\n        return\n    nick = self._getNick(irc.network)\n    if (nick not in self.registryValue('nicks')):\n        return\n    nickserv = self.registryValue('NickServ')\n    password = self._getNickServPassword(nick)\n    ghostDelay = self.registryValue('ghostDelay')\n    if (not ghostDelay):\n        return\n    if (nick and nickserv and password and (not ircutils.strEqual(nick, irc.nick))):\n        if (irc.afterConnect and ((self.sentGhost is None) or ((self.sentGhost + ghostDelay) < time.time()))):\n            if (nick in irc.state.nicksToHostmasks):\n                self._doGhost(irc)\n            else:\n                irc.sendMsg(ircmsgs.nick(nick))\n", "label": 1}
{"function": "\n\n@ensure_tag(['p'])\ndef is_header(el, meta_data):\n    if _is_top_level_upper_roman(el, meta_data):\n        return 'h2'\n    el_is_natural_header = is_natural_header(el, meta_data.styles_dict)\n    if el_is_natural_header:\n        return el_is_natural_header\n    if _is_li(el):\n        return False\n    w_namespace = get_namespace(el, 'w')\n    if (el.tag == ('%stbl' % w_namespace)):\n        return False\n    if DETECT_FONT_SIZE:\n        font_size = get_font_size(el, meta_data.styles_dict)\n        if (font_size is not None):\n            if meta_data.font_sizes_dict[font_size]:\n                return meta_data.font_sizes_dict[font_size]\n    num_words = len(etree.tostring(el, encoding=unicode, method='text').split(' '))\n    if (num_words > 8):\n        return False\n    (whole_line_bold, whole_line_italics) = whole_line_styled(el)\n    if (whole_line_bold or whole_line_italics):\n        return 'h2'\n    return False\n", "label": 1}
{"function": "\n\ndef test_basic_metadata(standard_graph, session, users, groups, permissions):\n    ' Test basic metadata functionality. '\n    graph = standard_graph\n    user_id = users['zorkian@a.co'].id\n    assert (len(users['zorkian@a.co'].my_metadata()) == 0), 'No metadata yet'\n    set_user_metadata(session, user_id, 'foo', 1)\n    md = get_user_metadata(session, user_id)\n    assert (len(md) == 1), 'One metadata item'\n    assert ([d.data_value for d in md if (d.data_key == 'foo')] == ['1']), 'foo is 1'\n    set_user_metadata(session, user_id, 'bar', 'test string')\n    md = get_user_metadata(session, user_id)\n    assert (len(md) == 2), 'Two metadata items'\n    assert ([d.data_value for d in md if (d.data_key == 'bar')] == ['test string']), 'bar is test string'\n    set_user_metadata(session, user_id, 'foo', 'test2')\n    md = get_user_metadata(session, user_id)\n    assert (len(md) == 2), 'Two metadata items'\n    assert ([d.data_value for d in md if (d.data_key == 'foo')] == ['test2']), 'foo is test2'\n    set_user_metadata(session, user_id, 'foo', None)\n    md = get_user_metadata(session, user_id)\n    assert (len(md) == 1), 'One metadata item'\n    assert ([d.data_value for d in md if (d.data_key == 'foo')] == []), 'foo is not found'\n    set_user_metadata(session, user_id, 'baz', None)\n    md = get_user_metadata(session, user_id)\n    assert (len(md) == 1), 'One metadata item'\n", "label": 1}
{"function": "\n\n@recursivereader\ndef pools_get_vs(self, pools=None, minimal=False):\n    'Returns VirtualServers associated with a list of Pools'\n    if (pools is None):\n        pools = f5.Pool._get_list(self)\n    elif isinstance(pools[0], f5.Pool):\n        pools = [pool.name for pool in pools]\n    result = {pool: [] for pool in pools}\n    vss = f5.VirtualServer._get(self, minimal=minimal)\n    if (minimal is True):\n        vss = f5.VirtualServer._refresh_default_pool(self, vss)\n    for pool in pools:\n        for vs in vss:\n            print(vs._default_pool)\n            if (pool == vs._default_pool.name):\n                result[pool].append(vs)\n    return result\n", "label": 1}
{"function": "\n\ndef snapshot(self, conf, no_metadata=False, disk_only=False, reuse_ext=False, quiesce=False):\n    'Creates a guest snapshot.\\n\\n        :param conf: libvirt.LibvirtConfigGuestSnapshotDisk\\n        :param no_metadata: Make snapshot without remembering it\\n        :param disk_only: Disk snapshot, no system checkpoint\\n        :param reuse_ext: Reuse any existing external files\\n        :param quiesce: Use QGA to quiece all mounted file systems\\n        '\n    flags = (no_metadata and (libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_NO_METADATA or 0))\n    flags |= (disk_only and (libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_DISK_ONLY or 0))\n    flags |= (reuse_ext and (libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_REUSE_EXT or 0))\n    flags |= ((quiesce and libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_QUIESCE) or 0)\n    self._domain.snapshotCreateXML(conf.to_xml(), flags=flags)\n", "label": 1}
{"function": "\n\ndef sum_add(self, other, method=0):\n    'Helper function for Sum simplification'\n    from sympy.concrete.summations import Sum\n    if (type(self) == type(other)):\n        if (method == 0):\n            if (self.limits == other.limits):\n                return Sum((self.function + other.function), *self.limits)\n        elif (method == 1):\n            if (simplify((self.function - other.function)) == 0):\n                if (len(self.limits) == len(other.limits) == 1):\n                    i = self.limits[0][0]\n                    x1 = self.limits[0][1]\n                    y1 = self.limits[0][2]\n                    j = other.limits[0][0]\n                    x2 = other.limits[0][1]\n                    y2 = other.limits[0][2]\n                    if (i == j):\n                        if (x2 == (y1 + 1)):\n                            return Sum(self.function, (i, x1, y2))\n                        elif (x1 == (y2 + 1)):\n                            return Sum(self.function, (i, x2, y1))\n    return Add(self, other)\n", "label": 1}
{"function": "\n\ndef perform(self, node, inputs, outputs):\n    (a, b) = inputs\n    (out,) = outputs\n    if (a.shape[1] != b.shape[0]):\n        raise ValueError('shape mismatch in StructuredDot.perform', (a.shape, b.shape))\n    variable = (a * b)\n    if isinstance(node.outputs[0].type, SparseType):\n        assert _is_sparse(variable)\n        out[0] = variable\n        return\n    assert _is_dense(variable)\n    if (variable.ndim == 1):\n        variable = numpy.expand_dims(variable, 1)\n    elif (variable.ndim != 2):\n        raise Exception('Output of structured dot should be a matrix (ndim=2)')\n    assert (variable.ndim == 2)\n    if (variable.shape != (a.shape[0], b.shape[1])):\n        if (b.shape[0] == 1):\n            raise Exception(('a.shape=%s, b.shape=%s, variable.shape=%s ??? This is probably because scipy.csc_matrix dot has a bug with singleton dimensions (i.e. b.shape[0]=1), for scipy 0.6. Use scipy 0.7. NB you have scipy version %s' % (a.shape, b.shape, variable.shape, scipy.__version__)))\n        else:\n            raise Exception('a.shape=%s, b.shape=%s, variable.shape=%s  ??? I have no idea why')\n    out[0] = theano._asarray(variable, str(variable.dtype))\n", "label": 1}
{"function": "\n\ndef make_node(self, alpha, x, y, z):\n    if ((not _is_sparse_variable(x)) and (not _is_sparse_variable(y))):\n        raise TypeError(x)\n    dtype_out = scalar.upcast(alpha.type.dtype, x.type.dtype, y.type.dtype, z.type.dtype)\n    alpha = tensor.as_tensor_variable(alpha)\n    z = tensor.as_tensor_variable(z)\n    assert (z.ndim == 2)\n    assert (alpha.type.broadcastable == ((True,) * alpha.ndim))\n    if (not _is_sparse_variable(x)):\n        x = tensor.as_tensor_variable(x)\n        assert (y.format in ['csr', 'csc'])\n        assert (x.ndim == 2)\n    if (not _is_sparse_variable(y)):\n        y = tensor.as_tensor_variable(y)\n        assert (x.format in ['csr', 'csc'])\n        assert (y.ndim == 2)\n    return gof.Apply(self, [alpha, x, y, z], [tensor.tensor(dtype=dtype_out, broadcastable=(False, False))])\n", "label": 1}
{"function": "\n\ndef build_policy(config=None, update=None, replace=None):\n    'Builds the policy as a string from the settings.'\n    if (config is None):\n        config = from_settings()\n    if (update is not None):\n        for (key, value) in update.items():\n            if (not isinstance(value, (list, tuple))):\n                value = (value,)\n            if (config[key] is not None):\n                config[key] += value\n            else:\n                config[key] = value\n    if (replace is not None):\n        for (key, value) in replace.items():\n            if ((value is not None) and (not isinstance(value, (list, tuple)))):\n                value = [value]\n            config[key] = value\n    report_uri = config.pop('report-uri', None)\n    policy = [('%s %s' % (k, ' '.join(v))) for (k, v) in sorted(config.items()) if (v is not None)]\n    if report_uri:\n        policy.append(('report-uri %s' % report_uri))\n    return '; '.join(policy)\n", "label": 1}
{"function": "\n\ndef _translate_stos_suffix(self, tb, instruction, suffix):\n    if (suffix == 'b'):\n        src = ReilRegisterOperand('al', 8)\n    elif (suffix == 'w'):\n        src = ReilRegisterOperand('ax', 16)\n    elif (suffix == 'd'):\n        src = ReilRegisterOperand('eax', 32)\n    elif (suffix == 'q'):\n        src = ReilRegisterOperand('rax', 64)\n    else:\n        raise Exception(('Invalid instruction suffix: %s' % suffix))\n    if (self._arch_mode == ARCH_X86_MODE_32):\n        dst = ReilRegisterOperand('edi', 32)\n    elif (self._arch_mode == ARCH_X86_MODE_64):\n        dst = ReilRegisterOperand('rdi', 64)\n    else:\n        raise Exception('Invalid architecture mode: %d', self._arch_mode)\n    if instruction.prefix:\n        (counter, loop_start_lbl) = self._rep_prefix_begin(tb, instruction)\n    tb.add(self._builder.gen_stm(src, dst))\n    self._update_strings_dst(tb, dst, src.size, instruction)\n    if instruction.prefix:\n        self._rep_prefix_end(tb, instruction, counter, loop_start_lbl)\n", "label": 1}
{"function": "\n\ndef parse(self, path, a_file=True):\n    output = None\n    tosca = ToscaTemplate(path, None, a_file)\n    version = tosca.version\n    if tosca.version:\n        print(('\\nversion: ' + version))\n    if hasattr(tosca, 'description'):\n        description = tosca.description\n        if description:\n            print(('\\ndescription: ' + description))\n    if hasattr(tosca, 'inputs'):\n        inputs = tosca.inputs\n        if inputs:\n            print('\\ninputs:')\n            for input in inputs:\n                print(('\\t' + input.name))\n    if hasattr(tosca, 'nodetemplates'):\n        nodetemplates = tosca.nodetemplates\n        if nodetemplates:\n            print('\\nnodetemplates:')\n            for node in nodetemplates:\n                print(('\\t' + node.name))\n    if hasattr(tosca, 'outputs'):\n        outputs = tosca.outputs\n        if outputs:\n            print('\\noutputs:')\n            for output in outputs:\n                print(('\\t' + output.name))\n", "label": 1}
{"function": "\n\ndef get_user_toggle(request):\n    ufilter = group = individual = show_commtrack = None\n    try:\n        request_obj = (request.POST if (request.method == 'POST') else request.GET)\n        if request_obj.get('ufilter', ''):\n            ufilter = request_obj.getlist('ufilter')\n        group = request_obj.get('group', '')\n        individual = request_obj.get('individual', '')\n        show_commtrack = request.project.commtrack_enabled\n    except (KeyError, AttributeError):\n        pass\n    show_filter = True\n    toggle = (HQUserType.commtrack_defaults() if show_commtrack else HQUserType.use_defaults())\n    if (ufilter and (not (group or individual))):\n        toggle = HQUserType.use_filter(ufilter)\n    elif (group or individual):\n        show_filter = False\n    return (toggle, show_filter)\n", "label": 1}
{"function": "\n\ndef save(self, request, contact_type=None):\n    'Process form and create DB objects as required'\n    if self.instance:\n        contact = self.instance\n    else:\n        contact = Contact()\n        contact.contact_type = contact_type\n    contact.name = unicode(self.cleaned_data['name'])\n    if ('parent' in self.cleaned_data):\n        contact.parent = self.cleaned_data['parent']\n    if ('related_user' in self.cleaned_data):\n        contact.related_user = self.cleaned_data['related_user']\n    contact.save()\n    if self.instance:\n        contact.contactvalue_set.all().delete()\n    for field in contact.contact_type.fields.all():\n        for form_name in self.cleaned_data:\n            if re.match(str((('^' + field.name) + '___\\\\d+$')), form_name):\n                if isinstance(self.fields[form_name], forms.FileField):\n                    value = ContactValue(field=field, contact=contact, value=self._handle_uploaded_file(form_name))\n                    if isinstance(self.fields[form_name], forms.ImageField):\n                        self._image_resize(value.value)\n                elif ((field.field_type == 'picture') and isinstance(self.fields[form_name], forms.ChoiceField)):\n                    if (self.cleaned_data[form_name] != 'delete'):\n                        value = ContactValue(field=field, contact=contact, value=self.cleaned_data[form_name])\n                else:\n                    value = ContactValue(field=field, contact=contact, value=self.cleaned_data[form_name])\n                value.save()\n    return contact\n", "label": 1}
{"function": "\n\ndef lineReceived(self, line):\n    '\\n        IMC2 -> Evennia\\n\\n        Triggered when text is received from the IMC2 network. Figures out\\n        what to do with the packet. This deals with the following\\n\\n        Args:\\n            line (str): Incoming text.\\n\\n        '\n    line = line.strip()\n    if (not self.is_authenticated):\n        self._imc_login(line)\n        return\n    packet = pck.IMC2Packet(self.mudname, packet_str=line)\n    if (packet.packet_type == 'is-alive'):\n        self.imc2_mudlist.update_mud_from_packet(packet)\n    elif (packet.packet_type == 'keepalive-request'):\n        self.send_packet(pck.IMC2PacketIsAlive())\n    elif (packet.packet_type == 'ice-msg-b'):\n        self.data_out(text=line, packettype='broadcast')\n    elif (packet.packet_type == 'whois-reply'):\n        self._whois_reply(packet)\n    elif (packet.packet_type == 'close-notify'):\n        self.imc2_mudlist.remove_mud_from_packet(packet)\n    elif (packet.packet_type == 'ice-update'):\n        self.imc2_chanlist.update_channel_from_packet(packet)\n    elif (packet.packet_type == 'ice-destroy'):\n        self.imc2_chanlist.remove_channel_from_packet(packet)\n    elif (packet.packet_type == 'tell'):\n        pass\n", "label": 1}
{"function": "\n\ndef parse_event_packet(self, packet):\n    try:\n        name_and_metadata = packet.split(':', 1)\n        if (len(name_and_metadata) != 2):\n            raise Exception(('Unparseable event packet: %s' % packet))\n        name = name_and_metadata[0]\n        metadata = name_and_metadata[1]\n        (title_length, text_length) = name.split(',')\n        title_length = int(title_length[3:])\n        text_length = int(text_length[:(- 1)])\n        event = {\n            'title': metadata[:title_length],\n            'text': self._unescape_event_text(metadata[(title_length + 1):((title_length + text_length) + 1)]),\n        }\n        meta = metadata[((title_length + text_length) + 1):]\n        for m in meta.split('|')[1:]:\n            if (m[0] == 't'):\n                event['alert_type'] = m[2:]\n            elif (m[0] == 'k'):\n                event['aggregation_key'] = m[2:]\n            elif (m[0] == 's'):\n                event['source_type_name'] = m[2:]\n            elif (m[0] == 'd'):\n                event['date_happened'] = int(m[2:])\n            elif (m[0] == 'p'):\n                event['priority'] = m[2:]\n            elif (m[0] == 'h'):\n                event['hostname'] = m[2:]\n            elif (m[0] == '#'):\n                event['tags'] = sorted(m[1:].split(','))\n        return event\n    except (IndexError, ValueError):\n        raise Exception(('Unparseable event packet: %s' % packet))\n", "label": 1}
{"function": "\n\ndef _process(self, p, element, ranges={\n    \n}):\n    if isinstance(element.data, np.ndarray):\n        if ('dataframe' in Dataset.datatype):\n            el_data = element.table('dataframe')\n        else:\n            el_data = element.table('ndelement')\n    el_data = element.data\n    dims = [d for d in element.dimensions() if _is_number(element.range(d)[0])]\n    permuted_dims = [(d1, d2) for d1 in dims for d2 in dims[::(- 1)]]\n    data = {\n        \n    }\n    for (d1, d2) in permuted_dims:\n        if (d1 == d2):\n            if (p.diagonal_type is Histogram):\n                bin_range = ranges.get(d1.name, element.range(d1))\n                el = element.hist(dimension=d1.name, bin_range=bin_range, adjoin=False)\n            else:\n                values = element.dimension_values(d1)\n                el = p.diagonal_type(values, kdims=[d1])\n        else:\n            el = p.chart_type(el_data, kdims=[d1], vdims=[d2])\n        data[(d1.name, d2.name)] = el\n    return data\n", "label": 1}
{"function": "\n\ndef _run_cleanups(self, cleanups):\n    for (function, args, kwargs) in reversed(cleanups):\n        try:\n            function(*args, **kwargs)\n        except Exception:\n            if inspect.ismethod(function):\n                if six.PY2:\n                    cmodule = function.im_class.__module__\n                    cname = function.im_class.__name__\n                else:\n                    cmodule = function.__self__.__class__.__module__\n                    cname = function.__self__.__class__.__name__\n                name = ('%s.%s.%s' % (cmodule, cname, function.__name__))\n            elif inspect.isfunction(function):\n                name = ('%s.%s' % (function.__module__, function.__name__))\n            else:\n                name = ('%s.%s' % (function.__class__.__module__, function.__class__.__name__))\n            formatted_args = ''\n            args_string = ', '.join([repr(arg) for arg in args])\n            kwargs_string = ', '.join([('%s=%r' % (key, value)) for (key, value) in kwargs.items()])\n            if args_string:\n                formatted_args = args_string\n            if kwargs_string:\n                if formatted_args:\n                    formatted_args += ', '\n                formatted_args += kwargs_string\n            LOG.warning((_LW('Error cleaning up %(name)s(%(args)s)') % {\n                'name': name,\n                'args': formatted_args,\n            }), exc_info=True)\n            pass\n", "label": 1}
{"function": "\n\ndef normalize_quantifiers(t):\n    '\\n    Push universals inside conjunctions and existentials inside\\n    disjunctions, and flatten conjunctions and disjunctions\\n    '\n    if (type(t) in (Var, Const)):\n        return t\n    elif (type(t) in (Apply, Eq, Ite, Not, Implies, Iff)):\n        return type(t)(*(normalize_quantifiers(x) for x in t))\n    elif (type(t) in (And, Or)):\n        return type(t)(*(z for x in t for y in [normalize_quantifiers(x)] for z in (y if (type(y) is type(t)) else [y])))\n    elif (((type(t) is ForAll) and (type(t.body) is And)) or ((type(t) is Exists) and (type(t.body) is Or))):\n        return normalize_quantifiers(type(t.body)(*(type(t)(t.variables, x) for x in t.body)))\n    elif (type(t) in (ForAll, Exists)):\n        return type(t)((free_variables(t.body) & frozenset(t.variables)), normalize_quantifiers(t.body))\n    else:\n        assert False, type(t)\n", "label": 1}
{"function": "\n\ndef fields_for_document(document, fields=None, exclude=None, formfield_callback=None):\n    '\\n    Returns a ``SortedDict`` containing form fields for the given model.\\n\\n    ``fields`` is an optional list of field names. If provided, only the named\\n    fields will be included in the returned fields.\\n\\n    ``exclude`` is an optional list of field names. If provided, the named\\n    fields will be excluded from the returned fields, even if they are listed\\n    in the ``fields`` argument.\\n    '\n    field_list = []\n    structure = document.structure\n    for (field_name, field_type) in structure.items():\n        if (fields and (not (field_name in fields))):\n            continue\n        if (exclude and (field_name in exclude)):\n            continue\n        form_field = None\n        if formfield_callback:\n            form_field = formfield_callback(document, field_name)\n        if (not form_field):\n            form_field = formfield_for_document_field(document, field_name)\n        if form_field:\n            field_list.append((field_name, form_field))\n    field_dict = SortedDict(field_list)\n    if fields:\n        field_dict = SortedDict([(f, field_dict.get(f)) for f in fields if ((not exclude) or (exclude and (f not in exclude)))])\n    return field_dict\n", "label": 1}
{"function": "\n\ndef _guess_method(self):\n    'Set `args.method` if not specified to either POST or GET\\n        based on whether the request has data or not.\\n\\n        '\n    if (self.args.method is None):\n        assert (not self.args.items)\n        if ((not self.args.ignore_stdin) and (not self.env.stdin_isatty)):\n            self.args.method = HTTP_POST\n        else:\n            self.args.method = HTTP_GET\n    elif (not re.match('^[a-zA-Z]+$', self.args.method)):\n        try:\n            self.args.items.insert(0, KeyValueArgType(*SEP_GROUP_ITEMS).__call__(self.args.url))\n        except ArgumentTypeError as e:\n            if self.args.traceback:\n                raise\n            self.error(e.message)\n        else:\n            self.args.url = self.args.method\n            has_data = (((not self.args.ignore_stdin) and (not self.env.stdin_isatty)) or any(((item.sep in SEP_GROUP_DATA_ITEMS) for item in self.args.items)))\n            self.args.method = (HTTP_POST if has_data else HTTP_GET)\n", "label": 1}
{"function": "\n\ndef _broadcast(self, arys):\n    'Perform numpy ufunc broadcasting\\n        '\n    shapelist = [a.shape for a in arys]\n    shape = _multi_broadcast(*shapelist)\n    for (i, ary) in enumerate(arys):\n        if (ary.shape == shape):\n            pass\n        elif self.is_device_array(ary):\n            arys[i] = self.broadcast_device(ary, shape)\n        else:\n            ax_differs = [ax for ax in range(len(shape)) if ((ax >= ary.ndim) or (ary.shape[ax] != shape[ax]))]\n            missingdim = (len(shape) - len(ary.shape))\n            strides = (([0] * missingdim) + list(ary.strides))\n            for ax in ax_differs:\n                strides[ax] = 0\n            strided = np.lib.stride_tricks.as_strided(ary, shape=shape, strides=strides)\n            arys[i] = self.force_array_layout(strided)\n    return arys\n", "label": 1}
{"function": "\n\ndef handle_starttag(self, tag, attrs):\n    if (tag == 'meta'):\n        http_equiv = content = None\n        for (k, v) in attrs:\n            if (k == 'http-equiv'):\n                http_equiv = string.lower(v)\n            elif (k == 'content'):\n                content = v\n        if ((http_equiv == 'content-type') and content):\n            header = mimetools.Message(StringIO.StringIO(('%s: %s\\n\\n' % (http_equiv, content))))\n            encoding = header.getparam('charset')\n            if encoding:\n                self.encoding = encoding\n    if (tag in AUTOCLOSE):\n        if (self.__stack and (self.__stack[(- 1)] == tag)):\n            self.handle_endtag(tag)\n    self.__stack.append(tag)\n    attrib = {\n        \n    }\n    if attrs:\n        for (k, v) in attrs:\n            attrib[string.lower(k)] = v\n    self.__builder.start(tag, attrib)\n    if (tag in IGNOREEND):\n        self.__stack.pop()\n        self.__builder.end(tag)\n", "label": 1}
{"function": "\n\ndef _get_tokens(self, cli, text):\n    m = self.compiled_grammar.match_prefix(text)\n    if m:\n        characters = [[self.default_token, c] for c in text]\n        for v in m.variables():\n            lexer = self.lexers.get(v.varname)\n            if lexer:\n                document = Document(text[v.start:v.stop])\n                lexer_tokens_for_line = lexer.lex_document(cli, document)\n                lexer_tokens = []\n                for i in range(len(document.lines)):\n                    lexer_tokens.extend(lexer_tokens_for_line(i))\n                    lexer_tokens.append((Token, '\\n'))\n                if lexer_tokens:\n                    lexer_tokens.pop()\n                i = v.start\n                for (t, s) in lexer_tokens:\n                    for c in s:\n                        if (characters[i][0] == self.default_token):\n                            characters[i][0] = t\n                        i += 1\n        trailing_input = m.trailing_input()\n        if trailing_input:\n            for i in range(trailing_input.start, trailing_input.stop):\n                characters[i][0] = Token.TrailingInput\n        return characters\n    else:\n        return [(Token, text)]\n", "label": 1}
{"function": "\n\n@classmethod\ndef _from_pb(cls, pb, set_key=True, ent=None, key=None):\n    'Internal helper to create an entity from an EntityProto protobuf.'\n    if (not isinstance(pb, entity_pb.EntityProto)):\n        raise TypeError(('pb must be a EntityProto; received %r' % pb))\n    if (ent is None):\n        ent = cls()\n    if ((key is None) and pb.key().path().element_size()):\n        key = Key(reference=pb.key())\n    if ((key is not None) and (set_key or key.id() or key.parent())):\n        ent._key = key\n    projection = []\n    for (indexed, plist) in ((True, pb.property_list()), (False, pb.raw_property_list())):\n        for p in plist:\n            if (p.meaning() == entity_pb.Property.INDEX_VALUE):\n                projection.append(p.name())\n            ent._get_property_for(p, indexed)._deserialize(ent, p)\n    ent._set_projection(projection)\n    return ent\n", "label": 1}
{"function": "\n\ndef dict_error_formatting(errors, index=None):\n    '\\n    Formats all dictionary error messages for both single and bulk requests\\n    '\n    formatted_error_list = []\n    top_level_error_keys = ['links', 'status', 'code', 'detail', 'source', 'meta']\n    resource_object_identifiers = ['type', 'id']\n    if (index is None):\n        index = ''\n    else:\n        index = (str(index) + '/')\n    for (error_key, error_description) in errors.iteritems():\n        if isinstance(error_description, basestring):\n            error_description = [error_description]\n        if (error_key in top_level_error_keys):\n            formatted_error_list.extend(({\n                error_key: description,\n            } for description in error_description))\n        elif (error_key in resource_object_identifiers):\n            formatted_error_list.extend([{\n                'source': {\n                    'pointer': ('/data/{}'.format(index) + error_key),\n                },\n                'detail': reason,\n            } for reason in error_description])\n        elif (error_key == 'non_field_errors'):\n            formatted_error_list.extend([{'detail': description for description in error_description}])\n        else:\n            formatted_error_list.extend([{\n                'source': {\n                    'pointer': ('/data/{}attributes/'.format(index) + error_key),\n                },\n                'detail': reason,\n            } for reason in error_description])\n    return formatted_error_list\n", "label": 1}
{"function": "\n\ndef log_actor_firing(self, actor_id, action_method, tokens_produced, tokens_consumed, production):\n    ' Trace actor firing\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_ACTOR_FIRING in logger.events)):\n            if ((not logger.actors) or (actor_id in logger.actors)):\n                data = {\n                    \n                }\n                data['timestamp'] = time.time()\n                data['node_id'] = self.node.id\n                data['type'] = 'actor_fire'\n                data['actor_id'] = actor_id\n                data['action_method'] = action_method\n                data['produced'] = tokens_produced\n                data['consumed'] = tokens_consumed\n                if (self.LOG_ACTION_RESULT in logger.events):\n                    data['action_result'] = production\n                if (logger.connection is not None):\n                    if (not logger.connection.connection_lost):\n                        logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                    else:\n                        disconnected.append(user_id)\n                elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                    msg = {\n                        'cmd': 'logevent',\n                        'msgid': logger.handle,\n                        'header': None,\n                        'data': ('data: %s\\n\\n' % json.dumps(data)),\n                    }\n                    self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\n@wrap_exceptions\ndef get_open_files(self):\n    retlist = []\n    files = os.listdir(('/proc/%s/fd' % self.pid))\n    hit_enoent = False\n    for fd in files:\n        file = ('/proc/%s/fd/%s' % (self.pid, fd))\n        if os.path.islink(file):\n            try:\n                file = os.readlink(file)\n            except OSError:\n                err = sys.exc_info()[1]\n                if (err.errno == errno.ENOENT):\n                    hit_enoent = True\n                    continue\n                raise\n            else:\n                if (file.startswith('/') and isfile_strict(file)):\n                    ntuple = nt_openfile(file, int(fd))\n                    retlist.append(ntuple)\n    if hit_enoent:\n        os.stat(('/proc/%s' % self.pid))\n    return retlist\n", "label": 1}
{"function": "\n\ndef _get_backfill_events(self, txn, room_id, event_list, limit):\n    logger.debug('_get_backfill_events: %s, %s, %s', room_id, repr(event_list), limit)\n    event_results = set()\n    query = 'SELECT depth, prev_event_id FROM event_edges INNER JOIN events ON prev_event_id = events.event_id AND event_edges.room_id = events.room_id WHERE event_edges.room_id = ? AND event_edges.event_id = ? AND event_edges.is_state = ? LIMIT ?'\n    queue = PriorityQueue()\n    for event_id in event_list:\n        depth = self._simple_select_one_onecol_txn(txn, table='events', keyvalues={\n            'event_id': event_id,\n        }, retcol='depth', allow_none=True)\n        if depth:\n            queue.put(((- depth), event_id))\n    while ((not queue.empty()) and (len(event_results) < limit)):\n        try:\n            (_, event_id) = queue.get_nowait()\n        except Empty:\n            break\n        if (event_id in event_results):\n            continue\n        event_results.add(event_id)\n        txn.execute(query, (room_id, event_id, False, (limit - len(event_results))))\n        for row in txn.fetchall():\n            if (row[1] not in event_results):\n                queue.put(((- row[0]), row[1]))\n    return event_results\n", "label": 1}
{"function": "\n\ndef do_overlays(img, overlays, overlay_tints, overlay_sources, overlay_sizes, overlay_positions):\n    overlay_index = 0\n    for overlay in overlays:\n        try:\n            overlay_tint = overlay_tints[overlay_index]\n        except (IndexError, TypeError):\n            overlay_tint = None\n        if (overlay_tint == 'None'):\n            overlay_tint = None\n        try:\n            overlay_source = overlay_sources[overlay_index]\n        except (IndexError, TypeError):\n            overlay_source = 'static'\n        try:\n            overlay_size = overlay_sizes[overlay_index]\n        except (IndexError, TypeError):\n            overlay_size = None\n        if (overlay_size == 'None'):\n            overlay_size = None\n        try:\n            overlay_position = overlay_positions[overlay_index]\n        except (IndexError, TypeError):\n            overlay_position = None\n        if (overlay_position == 'None'):\n            overlay_position = None\n        do_overlay(img, overlay, overlay_source, overlay_tint, overlay_size, overlay_position)\n        overlay_index += 1\n", "label": 1}
{"function": "\n\ndef genLabel(self, role=None, fallbackToQname=False, fallbackToXlinkLabel=False, lang=None, strip=False, linkrole=None):\n    from arelle import XbrlConst\n    if (role is None):\n        role = XbrlConst.genStandardLabel\n    if (role == XbrlConst.conceptNameLabelRole):\n        return str(self.qname)\n    labelsRelationshipSet = self.modelXbrl.relationshipSet(XbrlConst.elementLabel, linkrole)\n    if labelsRelationshipSet:\n        label = labelsRelationshipSet.label(self, role, lang)\n        if (label is not None):\n            if strip:\n                return label.strip()\n            return Locale.rtlString(label, lang=lang)\n    if fallbackToQname:\n        return str(self.qname)\n    elif (fallbackToXlinkLabel and hasattr(self, 'xlinkLabel')):\n        return self.xlinkLabel\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef startup(self, root):\n    ' Initialization during setup.\\n\\n        Args\\n        ----\\n        root : `System`\\n           System containing variables.\\n        '\n    pathname = root.pathname\n    if (MPI and root.is_active()):\n        rrank = root.comm.rank\n        rowned = root._owning_ranks\n    self._record_p = self._record_u = self._record_r = False\n    for recorder in self._recorders:\n        recorder.startup(root)\n        if (not recorder._parallel):\n            self._has_serial_recorders = True\n        pnames = recorder._filtered[pathname]['p']\n        unames = recorder._filtered[pathname]['u']\n        rnames = recorder._filtered[pathname]['r']\n        if pnames:\n            self._record_p = True\n        if unames:\n            self._record_u = True\n        if rnames:\n            self._record_r = True\n        if MPI:\n            pnames = {n for n in pnames if (rrank == rowned[n])}\n            unames = {n for n in unames if (rrank == rowned[n])}\n            rnames = {n for n in rnames if (rrank == rowned[n])}\n            if recorder._parallel:\n                recorder._filtered[pathname]['p'] = pnames\n                recorder._filtered[pathname]['u'] = unames\n                recorder._filtered[pathname]['r'] = rnames\n        self._vars_to_record['pnames'].update(pnames)\n        self._vars_to_record['unames'].update(unames)\n        self._vars_to_record['rnames'].update(rnames)\n", "label": 1}
{"function": "\n\ndef __calculate_view_index(self, window, history_entry):\n    group = history_entry['group']\n    if ((group < 0) or (group >= window.num_groups())):\n        group = self.calling_view_index[0]\n    max_index = len(window.views_in_group(group))\n    saved_index = history_entry['index']\n    if (self.USE_SAVED_POSITION and (saved_index >= 0) and (saved_index <= max_index)):\n        index = saved_index\n    elif (self.NEW_TAB_POSITION == 'first'):\n        index = 0\n    elif (self.NEW_TAB_POSITION == 'last'):\n        index = max_index\n    elif self.calling_view_index:\n        index = (self.calling_view_index[1] + 1)\n    else:\n        index = 0\n    return (group, index)\n", "label": 1}
{"function": "\n\ndef fprop(self, inputs, inference=False, beta=0.0):\n    \"\\n        TODO:  Handle final layers that don't own their own outputs (bias, activation)\\n        \"\n    x = inputs\n    if inference:\n        inference_revert_list = []\n    for l in self.layers:\n        l.revert_list = []\n        altered_tensor = l.be.distribute_data(x, l.parallelism)\n        if altered_tensor:\n            if inference:\n                inference_revert_list.append((l, altered_tensor))\n            else:\n                l.revert_list.append(altered_tensor)\n        if ((l is self.layers[(- 1)]) and (beta != 0)):\n            x = l.fprop(x, inference, beta=beta)\n        else:\n            x = l.fprop(x, inference)\n    if inference:\n        for (layer, tensor) in inference_revert_list:\n            layer.be.revert_tensor(tensor)\n    return x\n", "label": 1}
{"function": "\n\ndef prepare_value(self, key, value):\n    if isinstance(value, bool):\n        type_prefix = 'b'\n    elif isinstance(value, six.integer_types):\n        type_prefix = 'i'\n    elif isinstance(value, six.string_types):\n        type_prefix = 's'\n    elif isinstance(value, float):\n        type_prefix = 'f'\n    elif isinstance(value, dict):\n        type_prefix = 'o'\n        value = self.prepare_object(value)\n    elif isinstance(value, list):\n        type_prefix = 'l'\n    elif isinstance(value, (datetime, date)):\n        type_prefix = 'd'\n    elif isinstance(value, uuid.UUID):\n        type_prefix = 'u'\n        value = value.hex\n    else:\n        raise TypeError(('cannot index values of type %s' % type(value)))\n    return (('%s_%s' % (type_prefix, key)), value)\n", "label": 1}
{"function": "\n\ndef loadTestsFromName(self, name, module=None):\n    root = self.getRootSuite()\n    if (name == 'suite'):\n        return root\n    all_tests = []\n    for (testcase, testname) in find_all_tests(root):\n        if ((testname == name) or testname.endswith(('.' + name)) or ((('.' + name) + '.') in testname) or testname.startswith((name + '.'))):\n            all_tests.append(testcase)\n    if (not all_tests):\n        raise LookupError(('could not find test case for \"%s\"' % name))\n    if (len(all_tests) == 1):\n        return all_tests[0]\n    rv = unittest.TestSuite()\n    for test in all_tests:\n        rv.addTest(test)\n    return rv\n", "label": 1}
{"function": "\n\n@classdef.method('divmod')\ndef method_divmod(self, space, w_other):\n    if (math.isnan(self.floatvalue) or math.isinf(self.floatvalue)):\n        raise space.error(space.w_FloatDomainError, space.obj_to_s(space.getclass(w_other)))\n    if (space.is_kind_of(w_other, space.w_fixnum) or space.is_kind_of(w_other, space.w_bignum) or space.is_kind_of(w_other, space.w_float)):\n        y = space.float_w(w_other)\n        if math.isnan(y):\n            raise space.error(space.w_FloatDomainError, space.obj_to_s(space.getclass(w_other)))\n        x = self.floatvalue\n        mod = space.float_w(self.method_mod_float_impl(space, y))\n        div = ((x - mod) / y)\n        if mod:\n            if ((y < 0.0) != (mod < 0.0)):\n                mod += y\n                div -= 1.0\n        else:\n            mod *= mod\n            if (y < 0.0):\n                mod = (- mod)\n        if div:\n            floordiv = math.floor(div)\n            if ((div - floordiv) > 0.5):\n                floordiv += 1.0\n        else:\n            div *= div\n            floordiv = ((div * x) / y)\n        try:\n            return space.newarray([self.float_to_w_int(space, round_away(div)), space.newfloat(mod)])\n        except OverflowError:\n            return space.newarray([space.newbigint_fromfloat(div), space.newfloat(mod)])\n    else:\n        raise space.error(space.w_TypeError, (\"%s can't be coerced into Float\" % space.obj_to_s(space.getclass(w_other))))\n", "label": 1}
{"function": "\n\ndef _connect(self, *args, **kwargs):\n    while True:\n        try:\n            (ts, conn) = heapq.heappop(self._connections)\n            key = self.conn_key(conn)\n        except IndexError:\n            ts = conn = None\n            logger.debug('No connection available in pool.')\n            break\n        else:\n            if self._is_closed(key, conn):\n                logger.debug('Connection %s was closed.', key)\n                ts = conn = None\n                self._closed.discard(key)\n            elif (self.stale_timeout and self._is_stale(ts)):\n                logger.debug('Connection %s was stale, closing.', key)\n                self._close(conn, True)\n                self._closed.discard(key)\n                ts = conn = None\n            else:\n                break\n    if (conn is None):\n        if (self.max_connections and (len(self._in_use) >= self.max_connections)):\n            raise ValueError('Exceeded maximum connections.')\n        conn = super(PooledDatabase, self)._connect(*args, **kwargs)\n        ts = time.time()\n        key = self.conn_key(conn)\n        logger.debug('Created new connection %s.', key)\n    self._in_use[key] = ts\n    return conn\n", "label": 1}
{"function": "\n\ndef execute(self, args):\n    self.set_up_output_directory(args)\n    add_log_file(settings.log_file)\n    if os.path.isfile(args.agenda):\n        agenda = Agenda(args.agenda)\n        settings.agenda = args.agenda\n        shutil.copy(args.agenda, settings.meta_directory)\n        if (len(agenda.workloads) == 0):\n            raise ConfigError('No workloads specified')\n    elif (('.' in args.agenda) or (os.sep in args.agenda)):\n        raise ConfigError('Agenda \"{}\" does not exist.'.format(args.agenda))\n    else:\n        self.logger.debug('{} is not a file; assuming workload name.'.format(args.agenda))\n        agenda = Agenda()\n        agenda.add_workload_entry(args.agenda)\n    if args.instruments_to_disable:\n        if ('instrumentation' not in agenda.config):\n            agenda.config['instrumentation'] = []\n        for itd in args.instruments_to_disable:\n            self.logger.debug('Updating agenda to disable {}'.format(itd))\n            agenda.config['instrumentation'].append('~{}'.format(itd))\n    basename = 'config_'\n    for (file_number, path) in enumerate(settings.get_config_paths(), 1):\n        file_ext = os.path.splitext(path)[1]\n        shutil.copy(path, os.path.join(settings.meta_directory, ((basename + str(file_number)) + file_ext)))\n    executor = Executor()\n    executor.execute(agenda, selectors={\n        'ids': args.only_run_ids,\n    })\n", "label": 1}
{"function": "\n\ndef unconvert(values, dtype, compress=None):\n    as_is_ext = (isinstance(values, ExtType) and (values.code == 0))\n    if as_is_ext:\n        values = values.data\n    if is_categorical_dtype(dtype):\n        return values\n    elif is_object_dtype(dtype):\n        return np.array(values, dtype=object)\n    dtype = pandas_dtype(dtype).base\n    if (not as_is_ext):\n        values = values.encode('latin1')\n    if compress:\n        if (compress == 'zlib'):\n            _check_zlib()\n            decompress = zlib.decompress\n        elif (compress == 'blosc'):\n            _check_blosc()\n            decompress = blosc.decompress\n        else:\n            raise ValueError(\"compress must be one of 'zlib' or 'blosc'\")\n        try:\n            return np.frombuffer(_move_into_mutable_buffer(decompress(values)), dtype=dtype)\n        except _BadMove as e:\n            values = e.args[0]\n            if (len(values) > 1):\n                warnings.warn('copying data after decompressing; this may mean that decompress is caching its result', PerformanceWarning)\n    return np.fromstring(values, dtype=dtype)\n", "label": 1}
{"function": "\n\n@utils.positional(1)\ndef _to_dict(self, include=None, exclude=None):\n    \"Return a dict containing the entity's property values.\\n\\n    Args:\\n      include: Optional set of property names to include, default all.\\n      exclude: Optional set of property names to skip, default none.\\n        A name contained in both include and exclude is excluded.\\n    \"\n    if ((include is not None) and (not isinstance(include, (list, tuple, set, frozenset)))):\n        raise TypeError('include should be a list, tuple or set')\n    if ((exclude is not None) and (not isinstance(exclude, (list, tuple, set, frozenset)))):\n        raise TypeError('exclude should be a list, tuple or set')\n    values = {\n        \n    }\n    for prop in self._properties.itervalues():\n        name = prop._code_name\n        if ((include is not None) and (name not in include)):\n            continue\n        if ((exclude is not None) and (name in exclude)):\n            continue\n        try:\n            values[name] = prop._get_for_dict(self)\n        except UnprojectedPropertyError:\n            pass\n    return values\n", "label": 1}
{"function": "\n\ndef result_headers(cl):\n    lookup_opts = cl.lookup_opts\n    for (i, field_name) in enumerate(lookup_opts.admin.list_display):\n        try:\n            f = lookup_opts.get_field(field_name)\n        except models.FieldDoesNotExist:\n            if (field_name == '__str__'):\n                header = lookup_opts.verbose_name\n            else:\n                attr = getattr(cl.model, field_name)\n                try:\n                    header = attr.short_description\n                except AttributeError:\n                    header = field_name.replace('_', ' ')\n            if (not getattr(getattr(cl.model, field_name), 'admin_order_field', None)):\n                (yield {\n                    'text': header,\n                })\n                continue\n        else:\n            if (isinstance(f.rel, models.ManyToOneRel) and f.null):\n                (yield {\n                    'text': f.verbose_name,\n                })\n                continue\n            else:\n                header = f.verbose_name\n        th_classes = []\n        new_order_type = 'asc'\n        if (field_name == cl.order_field):\n            th_classes.append(('sorted %sending' % cl.order_type.lower()))\n            new_order_type = {\n                'asc': 'desc',\n                'desc': 'asc',\n            }[cl.order_type.lower()]\n        (yield {\n            'text': header,\n            'sortable': True,\n            'url': cl.get_query_string({\n                ORDER_VAR: i,\n                ORDER_TYPE_VAR: new_order_type,\n            }),\n            'class_attrib': ((th_classes and (' class=\"%s\"' % ' '.join(th_classes))) or ''),\n        })\n", "label": 1}
{"function": "\n\ndef _do_one_inner_iteration(self, A, b, **kwargs):\n    '\\n        This method solves AX = b and returns the result to the corresponding\\n        algorithm.\\n        '\n    logger.info('Solving AX = b for the sparse matrices')\n    if (A is None):\n        A = self.A\n    if (b is None):\n        b = self.b\n    if (self._iterative_solver is None):\n        X = sprslin.spsolve(A, b)\n    else:\n        if (self._iterative_solver not in ['cg', 'gmres']):\n            raise Exception(('GenericLinearTransport does not support the' + ' requested iterative solver!'))\n        params = kwargs.copy()\n        solver_params = ['x0', 'tol', 'maxiter', 'xtype', 'M', 'callback']\n        [params.pop(item, None) for item in kwargs.keys() if (item not in solver_params)]\n        tol = kwargs.get('tol')\n        if (tol is None):\n            tol = 1e-20\n        params['tol'] = tol\n        if (self._iterative_solver == 'cg'):\n            result = sprslin.cg(A, b, **params)\n        elif (self._iterative_solver == 'gmres'):\n            result = sprslin.gmres(A, b, **params)\n        X = result[0]\n        self._iterative_solver_info = result[1]\n    return X\n", "label": 1}
{"function": "\n\ndef _visitor(self, data, dirname, filesindir):\n    prune = []\n    self.insert_dirname(dirname)\n    for filename in filesindir:\n        if os.path.isdir(os.path.join(dirname, filename)):\n            if ((filename in ('.git', '.hg', '_build')) or filename.endswith('.egg-info')):\n                prune.append(filename)\n        else:\n            (name, ext) = os.path.splitext(filename)\n            if (not ((ext in ('.pyc',)) or filename.startswith('.'))):\n                (root, ext) = os.path.splitext(filename)\n                if (ext == '.py'):\n                    self.insert_filename(dirname, filename, ext)\n                else:\n                    prune.append(filename)\n            else:\n                prune.append(filename)\n    for filename in prune:\n        filesindir.remove(filename)\n", "label": 1}
{"function": "\n\ndef grad(self, inputs, gout):\n    (x,) = inputs\n    (gz,) = gout\n    if (x.dtype not in continuous_dtypes):\n        return [x.zeros_like(dtype=theano.config.floatX)]\n    if self.structured:\n        if (self.axis is None):\n            r = (gz * theano.sparse.sp_ones_like(x))\n        elif (self.axis == 0):\n            r = col_scale(theano.sparse.sp_ones_like(x), gz)\n        elif (self.axis == 1):\n            r = row_scale(theano.sparse.sp_ones_like(x), gz)\n        else:\n            raise ValueError('Illegal value for self.axis.')\n    else:\n        o_format = x.format\n        x = dense_from_sparse(x)\n        if _is_sparse_variable(gz):\n            gz = dense_from_sparse(gz)\n        if (self.axis is None):\n            r = tensor.second(x, gz)\n        else:\n            ones = tensor.ones_like(x)\n            if (self.axis == 0):\n                r = (tensor.addbroadcast(gz.dimshuffle('x', 0), 0) * ones)\n            elif (self.axis == 1):\n                r = (tensor.addbroadcast(gz.dimshuffle(0, 'x'), 1) * ones)\n            else:\n                raise ValueError('Illegal value for self.axis.')\n        r = SparseFromDense(o_format)(r)\n    return [r]\n", "label": 1}
{"function": "\n\ndef _validate_float_string(what):\n    if ((what[0] == '-') or (what[0] == '+')):\n        what = what[1:]\n    if what.isdigit():\n        return\n    (left, right) = what.split('.')\n    if ((left == '') and (right == '')):\n        raise dns.exception.FormError\n    if ((not (left == '')) and (not left.isdigit())):\n        raise dns.exception.FormError\n    if ((not (right == '')) and (not right.isdigit())):\n        raise dns.exception.FormError\n", "label": 1}
{"function": "\n\ndef _addSecurityGroup(self, input_dict, req, instance_id):\n    context = req.environ['nova.context']\n    try:\n        body = input_dict['addSecurityGroup']\n        group_name = body['name']\n        instance_id = int(instance_id)\n    except ValueError:\n        msg = _('Server id should be integer')\n        raise exc.HTTPBadRequest(explanation=msg)\n    except TypeError:\n        msg = _('Missing parameter dict')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    except KeyError:\n        msg = _('Security group not specified')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    if ((not group_name) or (group_name.strip() == '')):\n        msg = _('Security group name cannot be empty')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        self.compute_api.add_security_group(context, instance_id, group_name)\n    except exception.SecurityGroupNotFound as exp:\n        return exc.HTTPNotFound(explanation=unicode(exp))\n    except exception.InstanceNotFound as exp:\n        return exc.HTTPNotFound(explanation=unicode(exp))\n    except exception.Invalid as exp:\n        return exc.HTTPBadRequest(explanation=unicode(exp))\n    return exc.HTTPAccepted()\n", "label": 1}
{"function": "\n\ndef wait_on_done(self, interval=2, timeout=((3600 * 24) * 7), **kwargs):\n    \"\\n        :param interval: Number of seconds between queries to the analysis's state\\n        :type interval: integer\\n        :param timeout: Maximum amount of time to wait, in seconds, until the analysis is done (or at least partially failed)\\n        :type timeout: integer\\n        :raises: :exc:`~dxpy.exceptions.DXError` if the timeout is reached before the analysis has finished running, or :exc:`~dxpy.exceptions.DXJobFailureError` if some job in the analysis has failed\\n\\n        Waits until the analysis has finished running.\\n        \"\n    elapsed = 0\n    while True:\n        state = self._get_state(**kwargs)\n        if (state == 'done'):\n            break\n        if (state in ['failed', 'partially_failed']):\n            desc = self.describe(**kwargs)\n            err_msg = 'Analysis has failed because of {failureReason}: {failureMessage}'.format(**desc)\n            if ((desc.get('failureFrom') != None) and (desc['failureFrom']['id'] != desc['id'])):\n                err_msg += ' (failure from {id})'.format(id=desc['failureFrom']['id'])\n            raise DXJobFailureError(err_msg)\n        if (state == 'terminated'):\n            raise DXJobFailureError('Analysis was terminated.')\n        if ((elapsed >= timeout) or (elapsed < 0)):\n            raise DXJobFailureError('Reached timeout while waiting for the analysis to finish')\n        time.sleep(interval)\n        elapsed += interval\n", "label": 1}
{"function": "\n\ndef value_repr(self, value, top_level=False):\n    if (unicode_types and isinstance(value, unicode_types)):\n        try:\n            value = repr(value.encode('ascii'))\n        except UnicodeEncodeError:\n            pass\n    if isinstance(value, dict):\n        if (not value):\n            (yield ('none', 'gray'))\n            raise StopIteration()\n        if (not top_level):\n            (yield ('{', None))\n        for (i, (key, value)) in enumerate(sorted(value.items())):\n            if (i > 0):\n                (yield (', ', None))\n            (yield (key, 'key'))\n            (yield (':', None))\n            for output in self.value_repr(value):\n                (yield output)\n        if (not top_level):\n            (yield ('}', None))\n    elif isinstance(value, str):\n        (yield (value, 'str'))\n    elif isinstance(value, bool):\n        (yield (str(value), 'bool'))\n    elif isinstance(value, int_types):\n        (yield (str(value), 'int'))\n    else:\n        (yield (repr(value), 'red'))\n", "label": 1}
{"function": "\n\ndef __replace(node, repl):\n    modified = False\n    reduction = 0\n    if ((node.type == 'identifier') and getattr(node, 'parent', None)):\n        if ((node.parent.type in ('dot', 'property_init')) and (type(node.value) is str) and __matcher.match(node.value)):\n            if (node.value in repl):\n                reduction = ((reduction + len(node.value)) - len(repl[node.value]))\n                node.value = repl[node.value]\n                modified = True\n            elif node.value.endswith('__'):\n                pass\n            else:\n                raise Error(node.value, node.line)\n    for child in node:\n        if (child != None):\n            (subModified, subReduction) = __replace(child, repl)\n            modified = (modified or subModified)\n            reduction = (reduction + subReduction)\n    return (modified, reduction)\n", "label": 1}
{"function": "\n\ndef _get_field(self, question):\n    '\\n        Create a form field based on a question.\\n        '\n    if (question.type == 'M'):\n        choices = ([('', '')] + [(c, c) for c in question.get_possible_answers()])\n        return forms.ChoiceField(label=question.text, required=question.required, widget=forms.Select(attrs={\n            'class': 'select_choice_field',\n        }), choices=choices)\n    if (question.type == 'R'):\n        choices = [(c, c) for c in question.get_possible_answers()]\n        return forms.ChoiceField(label=question.text, required=question.required, widget=forms.RadioSelect(attrs={\n            'class': 'radio_choice_field',\n        }), choices=choices)\n    if (question.type == 'C'):\n        choices = [(c, c) for c in question.get_possible_answers()]\n        return forms.MultipleChoiceField(label=question.text, required=question.required, widget=forms.CheckboxSelectMultiple(attrs={\n            'class': 'checkbox_choice_field',\n        }), choices=choices)\n    elif (question.type == 'S'):\n        return forms.CharField(label=question.text, required=question.required, widget=forms.TextInput(attrs={\n            'class': 'short_text_field',\n        }))\n    elif (question.type == 'L'):\n        return forms.CharField(label=question.text, required=question.required, widget=forms.Textarea(attrs={\n            'class': 'long_text_field',\n        }))\n", "label": 1}
{"function": "\n\ndef is_short_syllable(w, before=None):\n    ' A short syllable in a word is either:\\n        - a vowel followed by a non-vowel other than w, x or Y and preceded by a non-vowel\\n        - a vowel at the beginning of the word followed by a non-vowel. \\n        Checks the three characters before the given index in the word (or entire word if None).\\n    '\n    if (before != None):\n        i = (((before < 0) and (len(w) + before)) or before)\n        return is_short_syllable(w[max(0, (i - 3)):i])\n    if ((len(w) == 3) and is_consonant(w[0]) and is_vowel(w[1]) and is_consonant(w[2]) and (w[2] not in 'wxY')):\n        return True\n    if ((len(w) == 2) and is_vowel(w[0]) and is_consonant(w[1])):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef rel_for_model(self, model, field_obj=None, multi=False):\n    is_field = isinstance(field_obj, Field)\n    is_node = ((not is_field) and isinstance(field_obj, Node))\n    if multi:\n        accum = []\n    for field in self.sorted_fields:\n        if (isinstance(field, ForeignKeyField) and (field.rel_model == model)):\n            is_match = ((field_obj is None) or (is_field and (field_obj.name == field.name)) or (is_node and (field_obj._alias == field.name)))\n            if is_match:\n                if (not multi):\n                    return field\n                accum.append(field)\n    if multi:\n        return accum\n", "label": 1}
{"function": "\n\ndef testvalidators(self):\n    ev = validators.Email()\n    assert ev('test@example.com')\n    assert (not ev('adsf@.asdf.asdf'))\n    assert validators.Length()('a')\n    assert (not validators.Length(2)('a'))\n    assert validators.Length(max_length=10)('abcdegf')\n    assert (not validators.Length(max_length=3)('abcdegf'))\n    n = validators.Number(1, 5)\n    assert n(2)\n    assert (not n(6))\n    assert validators.Number(1)(100.0)\n    assert (not validators.Number()('rawr!'))\n    vc = validators.ValidatorChain(validators.Length(8), validators.Email())\n    assert vc('test@example.com')\n    assert (not vc('a@a.com'))\n    assert (not vc('asdfasdfasdfasdfasdf'))\n", "label": 1}
{"function": "\n\ndef get_template_from_request(request, obj=None, no_current_page=False):\n    '\\n    Gets a valid template from different sources or falls back to the default\\n    template.\\n    '\n    template = None\n    if (len(get_cms_setting('TEMPLATES')) == 1):\n        return get_cms_setting('TEMPLATES')[0][0]\n    if (hasattr(request, 'POST') and ('template' in request.POST)):\n        template = request.POST['template']\n    elif (hasattr(request, 'GET') and ('template' in request.GET)):\n        template = request.GET['template']\n    if ((not template) and (obj is not None)):\n        template = obj.get_template()\n    if ((not template) and (not no_current_page) and hasattr(request, 'current_page')):\n        current_page = request.current_page\n        if hasattr(current_page, 'get_template'):\n            template = current_page.get_template()\n    if ((template is not None) and (template in dict(get_cms_setting('TEMPLATES')).keys())):\n        if ((template == constants.TEMPLATE_INHERITANCE_MAGIC) and obj):\n            return obj.get_template()\n        return template\n    return get_cms_setting('TEMPLATES')[0][0]\n", "label": 1}
{"function": "\n\ndef _join_multi(self, other, how, return_indexers=True):\n    from .multi import MultiIndex\n    self_is_mi = isinstance(self, MultiIndex)\n    other_is_mi = isinstance(other, MultiIndex)\n    self_names = [n for n in self.names if (n is not None)]\n    other_names = [n for n in other.names if (n is not None)]\n    overlap = list((set(self_names) & set(other_names)))\n    if (not len(overlap)):\n        raise ValueError('cannot join with no level specified and no overlapping names')\n    if (len(overlap) > 1):\n        raise NotImplementedError('merging with more than one level overlap on a multi-index is not implemented')\n    jl = overlap[0]\n    if (not (self_is_mi and other_is_mi)):\n        flip_order = False\n        if self_is_mi:\n            (self, other) = (other, self)\n            flip_order = True\n            how = {\n                'right': 'left',\n                'left': 'right',\n            }.get(how, how)\n        level = other.names.index(jl)\n        result = self._join_level(other, level, how=how, return_indexers=return_indexers)\n        if flip_order:\n            if isinstance(result, tuple):\n                return (result[0], result[2], result[1])\n        return result\n    raise NotImplementedError('merging with both multi-indexes is not implemented')\n", "label": 1}
{"function": "\n\ndef form2obj(self, ar, form_values, instance, is_new):\n    'Store the `form_values` into the `instance` by calling\\n        :meth:`form2obj` for every store field.\\n\\n        '\n    disabled_fields = set(self.actor.disabled_fields(instance, ar))\n    changed_triggers = []\n    for f in self.all_fields:\n        if (f.name not in disabled_fields):\n            try:\n                if f.form2obj(ar, instance, form_values, is_new):\n                    m = getattr(instance, (f.name + '_changed'), None)\n                    if (m is not None):\n                        changed_triggers.append(m)\n            except exceptions.ValidationError as e:\n                raise exceptions.ValidationError({\n                    f.name: e.messages,\n                })\n            except ValueError as e:\n                raise exceptions.ValidationError({\n                    f.name: (_('Invalid value for this field (%s).') % e),\n                })\n            except Exception as e:\n                logger.warning('Exception during Store.form2obj (field %s) : %s', f.name, e)\n                logger.exception(e)\n                raise\n    for m in changed_triggers:\n        m(ar)\n", "label": 1}
{"function": "\n\ndef get_movie(self, imdb, title, year):\n    try:\n        query = (self.search_link % urllib.quote_plus(title))\n        query = urlparse.urljoin(self.base_link, query)\n        result = client.source(query)\n        result = client.parseDOM(result, 'div', attrs={\n            'class': 'home_post_cont.+?',\n        })\n        title = cleantitle.movie(title)\n        years = [('(%s)' % str(year)), ('(%s)' % str((int(year) + 1))), ('(%s)' % str((int(year) - 1)))]\n        result = [(client.parseDOM(i, 'a', ret='href')[0], client.parseDOM(i, 'img', ret='title')[0]) for i in result]\n        result = [(i[0], client.replaceHTMLCodes(i[1])) for i in result]\n        result = [(i[0], client.parseDOM(i[1], 'a')) for i in result]\n        result = [(i[0], i[1][0]) for i in result if (len(i[1]) > 0)]\n        result = [i for i in result if (title == cleantitle.movie(i[1]))]\n        result = [i[0] for i in result if any(((x in i[1]) for x in years))][0]\n        try:\n            url = re.compile('//.+?(/.+)').findall(result)[0]\n        except:\n            url = result\n        url = client.replaceHTMLCodes(url)\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef config_for_set(uset, app, defaults=None):\n    '\\n    This is a helper function for `configure_uploads` that extracts the\\n    configuration for a single set.\\n\\n    :param uset: The upload set.\\n    :param app: The app to load the configuration from.\\n    :param defaults: A dict with keys `url` and `dest` from the\\n                     `UPLOADS_DEFAULT_DEST` and `DEFAULT_UPLOADS_URL`\\n                     settings.\\n    '\n    config = app.config\n    prefix = ('UPLOADED_%s_' % uset.name.upper())\n    using_defaults = False\n    if (defaults is None):\n        defaults = dict(dest=None, url=None)\n    allow_extns = tuple(config.get((prefix + 'ALLOW'), ()))\n    deny_extns = tuple(config.get((prefix + 'DENY'), ()))\n    destination = config.get((prefix + 'DEST'))\n    base_url = config.get((prefix + 'URL'))\n    if (destination is None):\n        if uset.default_dest:\n            destination = uset.default_dest(app)\n        if (destination is None):\n            if (defaults['dest'] is not None):\n                using_defaults = True\n                destination = os.path.join(defaults['dest'], uset.name)\n            else:\n                raise RuntimeError(('no destination for set %s' % uset.name))\n    if ((base_url is None) and using_defaults and defaults['url']):\n        base_url = ((addslash(defaults['url']) + uset.name) + '/')\n    return UploadConfiguration(destination, base_url, allow_extns, deny_extns)\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    if (len(args) != 0):\n        raise CommandError(\"Command doesn't accept any arguments\")\n    locale = options.get('locale')\n    domain = options.get('domain')\n    verbosity = int(options.get('verbosity'))\n    process_all = options.get('all')\n    extensions = (options.get('extensions') or ['html'])\n    if (domain == 'djangojs'):\n        extensions = []\n    else:\n        extensions = handle_extensions(extensions)\n    if ('.js' in extensions):\n        raise CommandError(\"JavaScript files should be examined by using the special 'djangojs' domain only.\")\n    if process_all:\n        if os.path.isdir(os.path.join('conf', 'locale')):\n            localedir = os.path.abspath(os.path.join('conf', 'locale'))\n        elif os.path.isdir('locale'):\n            localedir = os.path.abspath('locale')\n        else:\n            raise CommandError('This script should be run from the Transifex project tree.')\n        locale_dirs = filter(os.path.isdir, glob.glob(('%s/*' % localedir)))\n        for locale_dir in locale_dirs:\n            locale = os.path.basename(locale_dir)\n            make_messages(locale, domain, verbosity, False, extensions)\n    else:\n        make_messages(locale, domain, verbosity, process_all, extensions)\n", "label": 1}
{"function": "\n\ndef get_query_set(self, request=None):\n    qs = self.root_query_set.clone()\n    try:\n        (self.filter_specs, self.has_filters, remaining_lookup_params, use_distinct) = self.get_filters(request)\n        for filter_spec in self.filter_specs:\n            new_qs = filter_spec.queryset(request, qs)\n            if (new_qs is not None):\n                qs = new_qs\n    except ValueError:\n        remaining_lookup_params = self._lookup_param_1_3()\n    try:\n        qs = qs.filter(**remaining_lookup_params)\n    except (SuspiciousOperation, ImproperlyConfigured):\n        raise\n    except Exception as e:\n        raise IncorrectLookupParameters(e)\n    ordering = self.get_ordering(request, qs)\n    qs = qs.order_by(*ordering)\n\n    def construct_search(field_name):\n        if field_name.startswith('^'):\n            return ('%s__istartswith' % field_name[1:])\n        elif field_name.startswith('='):\n            return ('%s__iexact' % field_name[1:])\n        else:\n            return ('%s__icontains' % field_name)\n    if (self.search_fields and self.query):\n        orm_lookups = [construct_search(str(search_field)) for search_field in self.search_fields]\n        for bit in self.query.split():\n            or_queries = [Q(**{\n                orm_lookup: bit,\n            }) for orm_lookup in orm_lookups]\n            qs = qs.filter(reduce(operator.or_, or_queries))\n    return qs\n", "label": 1}
{"function": "\n\ndef drawline(self, start_pos, end_pos, char=' ', fgcolor=None, bgcolor=None):\n    if (fgcolor is None):\n        fgcolor = self._fgcolor\n    else:\n        fgcolor = getpygamecolor(fgcolor)\n    if (bgcolor is None):\n        bgcolor = self._bgcolor\n    else:\n        bgcolor = getpygamecolor(bgcolor)\n    (x0, y0) = start_pos\n    (x1, y1) = end_pos\n    isSteep = (abs((y1 - y0)) > abs((x1 - x0)))\n    if isSteep:\n        (x0, y0) = (y0, x0)\n        (x1, y1) = (y1, x1)\n    if (x0 > x1):\n        (x0, x1) = (x1, x0)\n        (y0, y1) = (y1, y0)\n    if (y0 < y1):\n        ystep = 1\n    else:\n        ystep = (- 1)\n    xdelta = (x1 - x0)\n    ydelta = abs((y1 - y0))\n    error = ((- xdelta) / 2)\n    y = y0\n    for x in range(x0, (x1 + 1)):\n        if isSteep:\n            self.putchar(char, y, x, fgcolor, bgcolor)\n        else:\n            self.putchar(char, x, y, fgcolor, bgcolor)\n        error = (error + ydelta)\n        if (error > 0):\n            y = (y + ystep)\n            error = (error - xdelta)\n", "label": 1}
{"function": "\n\ndef is_git_dir(d):\n    \" This is taken from the git setup.c:is_git_directory\\n    function.\\n\\n    @throws WorkTreeRepositoryUnsupported if it sees a worktree directory. It's quite hacky to do that here,\\n            but at least clearly indicates that we don't support it.\\n            There is the unlikely danger to throw if we see directories which just look like a worktree dir,\\n            but are none.\"\n    if isdir(d):\n        if (isdir(join(d, 'objects')) and isdir(join(d, 'refs'))):\n            headref = join(d, 'HEAD')\n            return (isfile(headref) or (os.path.islink(headref) and os.readlink(headref).startswith('refs')))\n        elif (isfile(join(d, 'gitdir')) and isfile(join(d, 'commondir')) and isfile(join(d, 'gitfile'))):\n            raise WorkTreeRepositoryUnsupported(d)\n    return False\n", "label": 1}
{"function": "\n\ndef filter_queryset(self, request, queryset, view):\n    '\\n        This applies ordering to the result set\\n        Eg: ?order=title\\n\\n        It also supports reverse ordering\\n        Eg: ?order=-title\\n\\n        And random ordering\\n        Eg: ?order=random\\n        '\n    if ('order' in request.GET):\n        if ('search' in request.GET):\n            raise BadRequestError('ordering with a search query is not supported')\n        order_by = request.GET['order']\n        if (order_by == 'random'):\n            if ('offset' in request.GET):\n                raise BadRequestError('random ordering with offset is not supported')\n            return queryset.order_by('?')\n        if order_by.startswith('-'):\n            reverse_order = True\n            order_by = order_by[1:]\n        else:\n            reverse_order = False\n        if ((order_by == 'id') or (order_by in view.get_api_fields(queryset.model))):\n            queryset = queryset.order_by(order_by)\n        else:\n            raise BadRequestError((\"cannot order by '%s' (unknown field)\" % order_by))\n        if reverse_order:\n            queryset = queryset.reverse()\n    return queryset\n", "label": 1}
{"function": "\n\ndef test_SubModulePolyRing_nontriv_local():\n    R = QQ.old_poly_ring(x, y, z, order=ilex)\n    F = R.free_module(1)\n\n    def contains(I, f):\n        return F.submodule(*[[g] for g in I]).contains([f])\n    assert contains([x, y], x)\n    assert contains([x, y], (x + y))\n    assert (not contains([x, y], 1))\n    assert (not contains([x, y], z))\n    assert contains([((x ** 2) + y), ((x ** 2) + x)], (x - y))\n    assert (not contains([((x + y) + z), (((x * y) + (x * z)) + (y * z)), ((x * y) * z)], (x ** 2)))\n    assert contains([(x * ((1 + x) + y)), (y * (1 + z))], x)\n    assert contains([(x * ((1 + x) + y)), (y * (1 + z))], (x + y))\n", "label": 1}
{"function": "\n\ndef format_multimatches(self, caller, matches):\n    '\\n        Format multiple command matches to a useful error.\\n\\n        This is copied directly from the default method in\\n        evennia.commands.cmdhandler.\\n\\n        '\n    string = 'There were multiple matches:'\n    for (num, match) in enumerate(matches):\n        (candidate, cmd) = match\n        is_channel = (hasattr(cmd, 'is_channel') and cmd.is_channel)\n        if is_channel:\n            is_channel = ' (channel)'\n        else:\n            is_channel = ''\n        is_exit = (hasattr(cmd, 'is_exit') and cmd.is_exit)\n        if (is_exit and cmd.destination):\n            is_exit = (' (exit to %s)' % cmd.destination)\n        else:\n            is_exit = ''\n        id1 = ''\n        id2 = ''\n        if ((not (is_channel or is_exit)) and (hasattr(cmd, 'obj') and (cmd.obj != caller))):\n            id1 = ('%s-' % cmd.obj.name)\n            id2 = (' (%s-%s)' % ((num + 1), candidate.cmdname))\n        else:\n            id1 = ('%s-' % (num + 1))\n            id2 = ''\n        string += ('\\n  %s%s%s%s%s' % (id1, candidate.cmdname, id2, is_channel, is_exit))\n    return string\n", "label": 1}
{"function": "\n\ndef _reload(mod, _larch=None, **kws):\n    'reload a module, either larch or python'\n    if (_larch is None):\n        raise Warning((\"cannot reload module '%s' -- larch broken?\" % mod))\n    modname = None\n    if (mod in _larch.symtable._sys.modules.values()):\n        for (k, v) in _larch.symtable._sys.modules.items():\n            if (v == mod):\n                modname = k\n    elif (mod in sys.modules.values()):\n        for (k, v) in sys.modules.items():\n            if (v == mod):\n                modname = k\n    elif ((mod in _larch.symtable._sys.modules.keys()) or (mod in sys.modules.keys())):\n        modname = mod\n    if (modname is not None):\n        return _larch.import_module(modname, do_reload=True)\n", "label": 1}
{"function": "\n\ndef patch_sourcelines(fname, in_line_start, out_line, endline='\\n', silent_mode=False):\n    'Replace the middle of lines between in_line_start and endline '\n    import io\n    import os.path as osp\n    if osp.isfile(fname):\n        with io.open(fname, 'r') as fh:\n            contents = fh.readlines()\n            content = ''.join(contents)\n            for l in range(len(contents)):\n                if contents[l].startswith(in_line_start):\n                    (begining, middle) = (in_line_start, contents[l][len(in_line_start):])\n                    ending = ''\n                    if (middle.find(endline) > 0):\n                        ending = (endline + endline.join(middle.split(endline)[1:]))\n                        middle = middle.split(endline)[0]\n                    middle = out_line\n                    new_line = ((begining + middle) + ending)\n                    if (not (new_line == contents[l])):\n                        if (not silent_mode):\n                            print('patching ', fname, ' from\\n', contents[l], '\\nto\\n', new_line)\n                    contents[l] = new_line\n            new_content = ''.join(contents)\n        if (not (new_content == content)):\n            with io.open(fname, 'wt') as fh:\n                try:\n                    fh.write(new_content)\n                except:\n                    print('impossible to patch', fname, 'from', content, 'to', new_content)\n", "label": 1}
{"function": "\n\ndef write(self):\n    q = {\n        \n    }\n    if (self._host_name is not None):\n        Util.set_by_path(q, 'HostName', self._host_name)\n    if (self._password is not None):\n        Util.set_by_path(q, 'Password', self._password)\n    if (len(self._ssh_keys) > 0):\n        Util.set_by_path(q, 'SSHKey.PublicKey', '\\n'.join(self._ssh_keys))\n    if (self._ip_address is not None):\n        Util.set_by_path(q, 'UserIPAddress', self._ip_address)\n    if (self._default_route is not None):\n        Util.set_by_path(q, 'UserSubnet.DefaultRoute', self._default_route)\n    if (self._network_mask_len is not None):\n        Util.set_by_path(q, 'UserSubnet.NetworkMaskLen', self._network_mask_len)\n    if (0 < len(self._scripts)):\n        notes = []\n        for script in self._scripts:\n            notes.append({\n                'ID': script._id(),\n            })\n        Util.set_by_path(q, 'Notes', notes)\n    path = (('/disk/' + self._disk_id) + '/config')\n    self._client.request('PUT', path, q)\n    return self\n", "label": 1}
{"function": "\n\ndef parse_journal(foldername):\n    'Return a list of Entry objects, sorted by date'\n    journal = dict()\n    for filename in os.listdir(os.path.join(foldername, 'entries')):\n        if (os.path.splitext(filename)[1] == '.doentry'):\n            try:\n                entry = Entry(os.path.join(foldername, 'entries', filename))\n            except KeyError as err:\n                pass\n            journal[entry['UUID']] = entry\n    if (len(journal) == 0):\n        raise Exception(('No journal entries found in ' + foldername))\n    try:\n        photos = os.listdir(os.path.join(foldername, 'photos'))\n    except OSError:\n        pass\n    else:\n        for filename in photos:\n            base = os.path.splitext(filename)[0]\n            try:\n                journal[base].set_photo(os.path.join('photos', filename))\n            except KeyError:\n                pass\n    journal = list(journal.values())\n    journal.sort(key=itemgetter('Creation Date'))\n    newest_tz = 'utc'\n    for entry in reversed(journal):\n        if ('Time Zone' in entry):\n            newest_tz = entry['Time Zone']\n            break\n    tz = newest_tz\n    for entry in reversed(journal):\n        if ('Time Zone' in entry):\n            tz = entry['Time Zone']\n        else:\n            entry.set_time_zone(tz)\n        entry.set_localized_date(tz)\n    return journal\n", "label": 1}
{"function": "\n\ndef _notify_agents_router_rescheduled(self, context, router_id, old_agents, new_agents):\n    l3_notifier = self.agent_notifiers.get(constants.AGENT_TYPE_L3)\n    if (not l3_notifier):\n        return\n    old_hosts = [agent['host'] for agent in old_agents]\n    new_hosts = [agent['host'] for agent in new_agents]\n    for host in (set(old_hosts) - set(new_hosts)):\n        l3_notifier.router_removed_from_agent(context, router_id, host)\n    for agent in new_agents:\n        for attempt in range(AGENT_NOTIFY_MAX_ATTEMPTS):\n            try:\n                l3_notifier.router_added_to_agent(context, [router_id], agent['host'])\n                break\n            except oslo_messaging.MessagingException:\n                LOG.warning(_LW('Failed to notify L3 agent on host %(host)s about added router. Attempt %(attempt)d out of %(max_attempts)d'), {\n                    'host': agent['host'],\n                    'attempt': (attempt + 1),\n                    'max_attempts': AGENT_NOTIFY_MAX_ATTEMPTS,\n                })\n        else:\n            self._unbind_router(context, router_id, agent['id'])\n            raise l3agentscheduler.RouterReschedulingFailed(router_id=router_id)\n", "label": 1}
{"function": "\n\ndef _dmp_rr_trivial_gcd(f, g, u, K):\n    'Handle trivial cases in GCD algorithm over a ring. '\n    zero_f = dmp_zero_p(f, u)\n    zero_g = dmp_zero_p(g, u)\n    if_contain_one = (dmp_one_p(f, u, K) or dmp_one_p(g, u, K))\n    if (zero_f and zero_g):\n        return tuple(dmp_zeros(3, u, K))\n    elif zero_f:\n        if K.is_nonnegative(dmp_ground_LC(g, u, K)):\n            return (g, dmp_zero(u), dmp_one(u, K))\n        else:\n            return (dmp_neg(g, u, K), dmp_zero(u), dmp_ground((- K.one), u))\n    elif zero_g:\n        if K.is_nonnegative(dmp_ground_LC(f, u, K)):\n            return (f, dmp_one(u, K), dmp_zero(u))\n        else:\n            return (dmp_neg(f, u, K), dmp_ground((- K.one), u), dmp_zero(u))\n    elif if_contain_one:\n        return (dmp_one(u, K), f, g)\n    elif query('USE_SIMPLIFY_GCD'):\n        return _dmp_simplify_gcd(f, g, u, K)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef load_stdlib():\n    'Scans sys.path for standard library modules.\\n    '\n    if _stdlib:\n        return _stdlib\n    prefixes = tuple({os.path.abspath(p) for p in (sys.prefix, getattr(sys, 'real_prefix', sys.prefix), getattr(sys, 'base_prefix', sys.prefix))})\n    for sp in sys.path:\n        if (not sp):\n            continue\n        _import_paths.append(os.path.abspath(sp))\n    stdpaths = tuple({p for p in _import_paths if (p.startswith(prefixes) and ('site-packages' not in p))})\n    _stdlib.update(sys.builtin_module_names)\n    for stdpath in stdpaths:\n        if (not os.path.isdir(stdpath)):\n            continue\n        for item in os.listdir(stdpath):\n            if (item.startswith('.') or (item == 'site-packages')):\n                continue\n            p = os.path.join(stdpath, item)\n            if ((not os.path.isdir(p)) and (not item.endswith(('.py', '.so')))):\n                continue\n            _stdlib.add(item.split('.', 1)[0])\n    return _stdlib\n", "label": 1}
{"function": "\n\ndef update_report(self, base64bz2report):\n    try:\n        base64bz2report = base64bz2report.replace(' ', '+')\n        plist = self.b64bz_decode(base64bz2report)\n        self.report = plistlib.writePlistToString(plist)\n    except:\n        plist = None\n        self.report = ''\n    if (plist is None):\n        self.activity = None\n        self.errors = 0\n        self.warnings = 0\n        self.console_user = '<None>'\n        return\n    activity = dict()\n    for section in ('ItemsToInstall', 'InstallResults', 'ItemsToRemove', 'RemovalResults', 'AppleUpdates'):\n        if ((section in plist) and len(plist[section])):\n            activity[section] = plist[section]\n    if activity:\n        self.activity = plistlib.writePlistToString(activity)\n    else:\n        self.activity = None\n    if ('Errors' in plist):\n        self.errors = len(plist['Errors'])\n    else:\n        self.errors = 0\n    if ('Warnings' in plist):\n        self.warnings = len(plist['Warnings'])\n    else:\n        self.warnings = 0\n    self.console_user = 'unknown'\n    if ('ConsoleUser' in plist):\n        self.console_user = unicode(plist['ConsoleUser'])\n", "label": 1}
{"function": "\n\ndef loadConfig(configFilename, host=None, port=None, basepath=None, logpath=None):\n    result = Config()\n    result.fromJson(configFilename)\n    if ((host is not None) and ('host' not in result)):\n        result.host = host\n    if ((port is not None) and ('port' not in result)):\n        result.port = port\n    if ((basepath is not None) and ('basepath' not in result)):\n        result.basepath = basepath\n    if ((logpath is not None) and ('logpath' not in result)):\n        result.logpath = logpath\n    if ('auth_timeout' not in result):\n        result.auth_timeout = 300\n    if ('require_vouch' not in result):\n        result.require_vouch = False\n    return result\n", "label": 1}
{"function": "\n\ndef authenticate(self, request, user_id=None, username=None, email=None, user=None, password=None, **kw):\n    odm = request.app.odm()\n    try:\n        if (not user):\n            with odm.begin() as session:\n                query = session.query(odm.user)\n                if user_id:\n                    user = query.get(user_id)\n                elif username:\n                    user = query.filter_by(username=username).one()\n                elif email:\n                    email = normalise_email(email)\n                    user = query.filter_by(email=email).one()\n                else:\n                    raise AuthenticationError('Invalid credentials')\n        if (user and self.crypt_verify(user.password, password)):\n            return user\n        else:\n            raise NoResultFound\n    except NoResultFound:\n        if username:\n            raise AuthenticationError('Invalid username or password')\n        elif email:\n            raise AuthenticationError('Invalid email or password')\n        else:\n            raise AuthenticationError('Invalid credentials')\n", "label": 1}
{"function": "\n\ndef parse_content_range_header(value, on_update=None):\n    'Parses a range header into a\\n    :class:`~werkzeug.datastructures.ContentRange` object or `None` if\\n    parsing is not possible.\\n\\n    .. versionadded:: 0.7\\n\\n    :param value: a content range header to be parsed.\\n    :param on_update: an optional callable that is called every time a value\\n                      on the :class:`~werkzeug.datastructures.ContentRange`\\n                      object is changed.\\n    '\n    if (value is None):\n        return None\n    try:\n        (units, rangedef) = (value or '').strip().split(None, 1)\n    except ValueError:\n        return None\n    if ('/' not in rangedef):\n        return None\n    (rng, length) = rangedef.split('/', 1)\n    if (length == '*'):\n        length = None\n    elif length.isdigit():\n        length = int(length)\n    else:\n        return None\n    if (rng == '*'):\n        return ContentRange(units, None, None, length, on_update=on_update)\n    elif ('-' not in rng):\n        return None\n    (start, stop) = rng.split('-', 1)\n    try:\n        start = int(start)\n        stop = (int(stop) + 1)\n    except ValueError:\n        return None\n    if is_byte_range_valid(start, stop, length):\n        return ContentRange(units, start, stop, length, on_update=on_update)\n", "label": 1}
{"function": "\n\ndef transform_first_chunk(self, status_code, headers, chunk, finishing):\n    if ('Vary' in headers):\n        headers['Vary'] += b', Accept-Encoding'\n    else:\n        headers['Vary'] = b'Accept-Encoding'\n    if self._gzipping:\n        ctype = _unicode(headers.get('Content-Type', '')).split(';')[0]\n        self._gzipping = (self._compressible_type(ctype) and ((not finishing) or (len(chunk) >= self.MIN_LENGTH)) and ('Content-Encoding' not in headers))\n    if self._gzipping:\n        headers['Content-Encoding'] = 'gzip'\n        self._gzip_value = BytesIO()\n        self._gzip_file = gzip.GzipFile(mode='w', fileobj=self._gzip_value)\n        chunk = self.transform_chunk(chunk, finishing)\n        if ('Content-Length' in headers):\n            if finishing:\n                headers['Content-Length'] = str(len(chunk))\n            else:\n                del headers['Content-Length']\n    return (status_code, headers, chunk)\n", "label": 1}
{"function": "\n\ndef _meijerint_definite_3(f, x):\n    '\\n    Try to integrate f dx from zero to infinity.\\n\\n    This function calls _meijerint_definite_4 to try to compute the\\n    integral. If this fails, it tries using linearity.\\n    '\n    res = _meijerint_definite_4(f, x)\n    if (res and (res[1] != False)):\n        return res\n    if f.is_Add:\n        _debug('Expanding and evaluating all terms.')\n        ress = [_meijerint_definite_4(g, x) for g in f.args]\n        if all(((r is not None) for r in ress)):\n            conds = []\n            res = S(0)\n            for (r, c) in ress:\n                res += r\n                conds += [c]\n            c = And(*conds)\n            if (c != False):\n                return (res, c)\n", "label": 1}
{"function": "\n\ndef validate_b2_file_name(name):\n    '\\n    Raises a ValueError if the name is not a valid B2 file name.\\n\\n    :param name: a string\\n    :return: None\\n    '\n    if (not isinstance(name, six.string_types)):\n        raise ValueError('file name must be a string, not bytes')\n    name_utf8 = name.encode('utf-8')\n    if (len(name_utf8) < 1):\n        raise ValueError('file name too short (0 utf-8 bytes)')\n    if (1000 < len(name_utf8)):\n        raise ValueError('file name too long (more than 1000 utf-8 bytes)')\n    if (name[0] == '/'):\n        raise ValueError(\"file names must not start with '/'\")\n    if (name[(- 1)] == '/'):\n        raise ValueError(\"file names must not end with '/'\")\n    if ('\\\\' in name):\n        raise ValueError(\"file names must not contain '\\\\'\")\n    if ('//' in name):\n        raise ValueError(\"file names must not contain '//'\")\n    if (chr(127) in name):\n        raise ValueError('file names must not contain DEL')\n    if any(((250 < len(segment)) for segment in name_utf8.split(six.b('/')))):\n        raise ValueError(\"file names segments (between '/') can be at most 250 utf-8 bytes\")\n", "label": 1}
{"function": "\n\ndef clear_attachments(schema_or_doc):\n    if (schema_or_doc and ('_attachments' in schema_or_doc)):\n        del schema_or_doc['_attachments']\n    if (schema_or_doc and ('case_attachments' in schema_or_doc)):\n        del schema_or_doc['case_attachments']\n    if schema_or_doc:\n        for action in schema_or_doc.get('actions', []):\n            if (('attachments' in action) and ('updated_unknown_properties' in action)):\n                del action['attachments']\n    return schema_or_doc\n", "label": 1}
{"function": "\n\ndef _SetValues(self, values):\n    \"Set values from supplied dictionary or list.\\n\\n    Args:\\n      values: A Row, dict indexed by column name, or list.\\n\\n    Raises:\\n      TypeError: Argument is not a list or dict, or list is not equal row\\n      length or dictionary keys don't match.\\n    \"\n\n    def _ToStr(value):\n        'Convert individul list entries to string.'\n        if isinstance(value, (list, tuple)):\n            result = []\n            for val in value:\n                result.append(str(val))\n            return result\n        else:\n            return str(value)\n    if isinstance(values, Row):\n        if (self._keys != values.header):\n            raise TypeError('Attempt to append row with mismatched header.')\n        self._values = copy.deepcopy(values.values)\n    elif isinstance(values, dict):\n        for key in self._keys:\n            if (key not in values):\n                raise TypeError('Dictionary key mismatch with row.')\n        for key in self._keys:\n            self[key] = _ToStr(values[key])\n    elif (isinstance(values, list) or isinstance(values, tuple)):\n        if (len(values) != len(self._values)):\n            raise TypeError('Supplied list length != row length')\n        for (index, value) in enumerate(values):\n            self._values[index] = _ToStr(value)\n    else:\n        raise TypeError('Supplied argument must be Row, dict or list, not %s', type(values))\n", "label": 1}
{"function": "\n\ndef form_fields(self):\n    '\\n        Return fields of default form.\\n\\n        Fill some fields with reasonable values.\\n        '\n    fields = dict(self.form.fields)\n    for elem in self.form.inputs:\n        if (not elem.get('name')):\n            continue\n        if elem.get('disabled'):\n            if (elem.name in fields):\n                del fields[elem.name]\n        elif (elem.tag == 'select'):\n            if (fields[elem.name] is None):\n                if len(elem.value_options):\n                    fields[elem.name] = elem.value_options[0]\n        elif (getattr(elem, 'type', None) == 'radio'):\n            if (fields[elem.name] is None):\n                fields[elem.name] = elem.get('value')\n        elif (getattr(elem, 'type', None) == 'checkbox'):\n            if (not elem.checked):\n                if (elem.name is not None):\n                    if (elem.name in fields):\n                        del fields[elem.name]\n    return fields\n", "label": 1}
{"function": "\n\ndef _refresh(self):\n    'Refreshes the cursor with more data from Mongo.\\n\\n        Returns the length of self.__data after refresh. Will exit early if\\n        self.__data is already non-empty. Raises OperationFailure when the\\n        cursor cannot be refreshed due to an error on the query.\\n        '\n    if (len(self.__data) or self.__killed):\n        return len(self.__data)\n    if (self.__id is None):\n        ntoreturn = self.__batch_size\n        if self.__limit:\n            if self.__batch_size:\n                ntoreturn = min(self.__limit, self.__batch_size)\n            else:\n                ntoreturn = self.__limit\n        self.__send_message(message.query(self.__query_options(), self.__collection.full_name, self.__skip, ntoreturn, self.__query_spec(), self.__fields, self.__uuid_subtype))\n        if (not self.__id):\n            self.__killed = True\n    elif self.__id:\n        if self.__limit:\n            limit = (self.__limit - self.__retrieved)\n            if self.__batch_size:\n                limit = min(limit, self.__batch_size)\n        else:\n            limit = self.__batch_size\n        if self.__exhaust:\n            self.__send_message(None)\n        else:\n            self.__send_message(message.get_more(self.__collection.full_name, limit, self.__id))\n    else:\n        self.__killed = True\n    return len(self.__data)\n", "label": 1}
{"function": "\n\ndef _iter_rows(self):\n    model_meta = self.model_class._meta\n    if self._validate_fields:\n        valid_fields = (set(model_meta.fields.keys()) | set(model_meta.fields.values()))\n\n        def validate_field(field):\n            if (field not in valid_fields):\n                raise KeyError(('\"%s\" is not a recognized field.' % field))\n    defaults = model_meta._default_dict\n    callables = model_meta._default_callables\n    for row_dict in self._rows:\n        field_row = defaults.copy()\n        seen = set()\n        for key in row_dict:\n            if self._validate_fields:\n                validate_field(key)\n            if (key in model_meta.fields):\n                field = model_meta.fields[key]\n            else:\n                field = key\n            field_row[field] = row_dict[key]\n            seen.add(field)\n        if callables:\n            for field in callables:\n                if (field not in seen):\n                    field_row[field] = callables[field]()\n        (yield field_row)\n", "label": 1}
{"function": "\n\ndef virtual_memory():\n    (total, free, buffers, shared, _, _) = _psutil_linux.get_sysinfo()\n    cached = active = inactive = None\n    f = open('/proc/meminfo', 'r')\n    try:\n        for line in f:\n            if line.startswith('Cached:'):\n                cached = (int(line.split()[1]) * 1024)\n            elif line.startswith('Active:'):\n                active = (int(line.split()[1]) * 1024)\n            elif line.startswith('Inactive:'):\n                inactive = (int(line.split()[1]) * 1024)\n            if ((cached is not None) and (active is not None) and (inactive is not None)):\n                break\n        else:\n            raise RuntimeError('line(s) not found')\n    finally:\n        f.close()\n    avail = ((free + buffers) + cached)\n    used = (total - free)\n    percent = usage_percent((total - avail), total, _round=1)\n    return nt_virtmem_info(total, avail, percent, used, free, active, inactive, buffers, cached)\n", "label": 1}
{"function": "\n\ndef annotations(self, imageset=None, classes=None, nodifficult=False):\n    if (imageset is None):\n        imageset = self.imageset()\n    elif isinstance(imageset, str):\n        imageset = self.imageset(imageset)\n    for file in imageset:\n        logger.info('Reading {0}'.format(file))\n        file = os.path.join(self.root, 'Annotations', '{0}.xml'.format(file))\n        if (not file.endswith('.xml')):\n            continue\n        tree = ElementTree.parse(file)\n        filename = tree.find('filename').text.strip()\n        for object in tree.findall('object'):\n            label = object.find('name').text.strip()\n            if (classes and (label not in classes)):\n                continue\n            if nodifficult:\n                difficult = object.find('difficult').text\n                difficult = bool(int(difficult))\n                if difficult:\n                    continue\n            xtl = int(object.find('bndbox/xmin').text)\n            ytl = int(object.find('bndbox/ymin').text)\n            xbr = int(object.find('bndbox/xmax').text)\n            ybr = int(object.find('bndbox/ymax').text)\n            (yield Box(xtl, ytl, xbr, ybr, label=label, image=filename))\n", "label": 1}
{"function": "\n\ndef test_equality_rewrite_literals(self):\n    'Test an equality rewrite with different literals'\n    l = ast.Literal('foo')\n    v = ast.Literal('bar')\n    v2 = ast.Literal('baz')\n    cmp1 = ast.CompareOperator('=', l, v)\n    cmp2 = ast.CompareOperator('=', l, v2)\n    or1 = ast.LogicalOperator('or', cmp1, cmp2)\n    name = merge.node_name(cmp1, True)\n    r = compare.equality_rewrite(ast.dup(or1), name, cmp1, True)\n    assert isinstance(r.left, ast.Constant)\n    assert (r.left.value == True)\n    assert ASTPattern(cmp2).matches(r.right)\n    name = merge.node_name(cmp1, True)\n    r = compare.equality_rewrite(ast.dup(or1), name, cmp1, False)\n    assert isinstance(r.left, ast.Constant)\n    assert (r.left.value == False)\n    assert ASTPattern(cmp2).matches(r.right)\n    name = merge.node_name(cmp2, True)\n    r = compare.equality_rewrite(ast.dup(or1), name, cmp2, True)\n    assert ASTPattern(cmp1).matches(r.left)\n    assert isinstance(r.right, ast.Constant)\n    assert (r.right.value == True)\n    name = merge.node_name(cmp2, True)\n    r = compare.equality_rewrite(ast.dup(or1), name, cmp2, False)\n    assert ASTPattern(cmp1).matches(r.left)\n    assert isinstance(r.right, ast.Constant)\n    assert (r.right.value == False)\n", "label": 1}
{"function": "\n\ndef dump_message(self, msg, exclusion_list=None, local_only=True, cap=None, exclude_cap=None):\n    if (not exclusion_list):\n        exclusion_list = list()\n    if local_only:\n        ctx = get_context()\n    for m in self.members:\n        if (m.client in exclusion_list):\n            continue\n        if (local_only and (m.client.servername != ctx.conf.name)):\n            continue\n        if (cap and (cap not in m.client.caps)):\n            continue\n        if (exclude_cap and (exclude_cap in m.client.caps)):\n            continue\n        m.client.dump_message(msg)\n", "label": 1}
{"function": "\n\ndef _build_droot_impact(destroy_handler):\n    droot = {\n        \n    }\n    impact = {\n        \n    }\n    root_destroyer = {\n        \n    }\n    for app in destroy_handler.destroyers:\n        for (output_idx, input_idx_list) in app.op.destroy_map.items():\n            if (len(input_idx_list) != 1):\n                raise NotImplementedError()\n            input_idx = input_idx_list[0]\n            input = app.inputs[input_idx]\n            view_i = destroy_handler.view_i\n            _r = input\n            while (_r is not None):\n                r = _r\n                _r = view_i.get(r)\n            input_root = r\n            if (input_root in droot):\n                raise InconsistencyError(('Multiple destroyers of %s' % input_root))\n            droot[input_root] = input_root\n            root_destroyer[input_root] = app\n            input_impact = OrderedSet()\n            queue = Queue()\n            queue.put(input_root)\n            while (not queue.empty()):\n                v = queue.get()\n                for n in destroy_handler.view_o.get(v, []):\n                    input_impact.add(n)\n                    queue.put(n)\n            for v in input_impact:\n                assert (v not in droot)\n                droot[v] = input_root\n            impact[input_root] = input_impact\n            impact[input_root].add(input_root)\n    return (droot, impact, root_destroyer)\n", "label": 1}
{"function": "\n\ndef ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None, ca_certs=None, server_hostname=None, ssl_version=None):\n    ctx = OpenSSL.SSL.Context(_openssl_versions[ssl_version])\n    if certfile:\n        ctx.use_certificate_file(certfile)\n    if keyfile:\n        ctx.use_privatekey_file(keyfile)\n    if (cert_reqs != ssl.CERT_NONE):\n        ctx.set_verify(_openssl_verify[cert_reqs], _verify_callback)\n    if ca_certs:\n        try:\n            ctx.load_verify_locations(ca_certs, None)\n        except OpenSSL.SSL.Error as e:\n            raise ssl.SSLError(('bad ca_certs: %r' % ca_certs), e)\n    OP_NO_COMPRESSION = 131072\n    ctx.set_options(OP_NO_COMPRESSION)\n    ctx.set_cipher_list(DEFAULT_SSL_CIPHER_LIST)\n    cnx = OpenSSL.SSL.Connection(ctx, sock)\n    cnx.set_tlsext_host_name(server_hostname)\n    cnx.set_connect_state()\n    while True:\n        try:\n            cnx.do_handshake()\n        except OpenSSL.SSL.WantReadError:\n            select.select([sock], [], [])\n            continue\n        except OpenSSL.SSL.Error as e:\n            raise ssl.SSLError('bad handshake', e)\n        break\n    return WrappedSocket(cnx, sock)\n", "label": 1}
{"function": "\n\n@transform(buildLCA, suffix('.taxa.gz'), '.taxa.readcounts')\ndef countContributingReads(infile, outfile):\n    '\\n    count number of reads with a taxnomic assignment\\n    '\n    levels = ['phylum', 'class', 'order', 'family', 'genus', 'species']\n    result = collections.OrderedDict()\n    for level in levels:\n        result[level] = 0\n    inf = IOTools.openFile(infile)\n    header = inf.readline().split('\\t')\n    indices = [3, 5, 7, 9, 11, 13]\n    total = 0\n    for line in inf.readlines():\n        total += 1\n        data = line[:(- 1)].split('\\t')\n        (phylum, _class, order, family, genus, species) = [data[i] for i in indices]\n        if (phylum != 'NA'):\n            result['phylum'] += 1\n        if (_class != 'NA'):\n            result['class'] += 1\n        if (order != 'NA'):\n            result['order'] += 1\n        if (family != 'NA'):\n            result['family'] += 1\n        if (genus != 'NA'):\n            result['genus'] += 1\n        if (species != 'NA'):\n            result['species'] += 1\n    outf = open(outfile, 'w')\n    outf.write('level\\tn_reads\\tpct_reads\\n')\n    for (level, count) in result.iteritems():\n        (nreads, prop) = (count, (float(count) / total))\n        outf.write(('\\t'.join([level, str(nreads), str((prop * 100))]) + '\\n'))\n    outf.close()\n", "label": 1}
{"function": "\n\ndef check_package(self, package, package_dir):\n    \"Check namespace packages' __init__ for declare_namespace\"\n    try:\n        return self.packages_checked[package]\n    except KeyError:\n        pass\n    init_py = _build_py.check_package(self, package, package_dir)\n    self.packages_checked[package] = init_py\n    if ((not init_py) or (not self.distribution.namespace_packages)):\n        return init_py\n    for pkg in self.distribution.namespace_packages:\n        if ((pkg == package) or pkg.startswith((package + '.'))):\n            break\n    else:\n        return init_py\n    f = open(init_py, 'rU')\n    if ('declare_namespace' not in f.read()):\n        from distutils import log\n        log.warn('WARNING: %s is a namespace package, but its __init__.py does\\nnot declare_namespace(); setuptools 0.7 will REQUIRE this!\\n(See the setuptools manual under \"Namespace Packages\" for details.)\\n', package)\n    f.close()\n    return init_py\n", "label": 1}
{"function": "\n\ndef extract_docstring(self):\n    ' Extract a module-level docstring\\n        '\n    lines = open(self.filename).readlines()\n    start_row = 0\n    if lines[0].startswith('#!'):\n        lines.pop(0)\n        start_row = 1\n    docstring = ''\n    first_par = ''\n    tokens = tokenize.generate_tokens(lines.__iter__().next)\n    for (tok_type, tok_content, _, (erow, _), _) in tokens:\n        tok_type = token.tok_name[tok_type]\n        if (tok_type in ('NEWLINE', 'COMMENT', 'NL', 'INDENT', 'DEDENT')):\n            continue\n        elif (tok_type == 'STRING'):\n            docstring = eval(tok_content)\n            paragraphs = '\\n'.join((line.rstrip() for line in docstring.split('\\n'))).split('\\n\\n')\n            if (len(paragraphs) > 0):\n                first_par = paragraphs[0]\n        break\n    thumbloc = None\n    for (i, line) in enumerate(docstring.split('\\n')):\n        m = re.match('^_thumb: (\\\\.\\\\d+),\\\\s*(\\\\.\\\\d+)', line)\n        if m:\n            thumbloc = (float(m.group(1)), float(m.group(2)))\n            break\n    if (thumbloc is not None):\n        self.thumbloc = thumbloc\n        docstring = '\\n'.join([l for l in docstring.split('\\n') if (not l.startswith('_thumb'))])\n    self.docstring = docstring\n    self.short_desc = first_par\n    self.end_line = ((erow + 1) + start_row)\n", "label": 1}
{"function": "\n\ndef save(self):\n    '\\n        Save the order, create or update the contact information\\n        (if available) and return the saved order instance\\n        '\n    order = super(BaseCheckoutForm, self).save(commit=False)\n    contact = self.shop.contact_from_user(self.request.user)\n    if contact:\n        order.user = contact.user\n    elif self.request.user.is_authenticated():\n        order.user = self.request.user\n    if ((self.cleaned_data.get('create_account') and (not contact)) or ((not contact) and self.request.user.is_authenticated())):\n        password = None\n        email = self.cleaned_data.get('email')\n        if (not self.request.user.is_authenticated()):\n            password = User.objects.make_random_password()\n            params = {\n                'email': email,\n                'password': password,\n            }\n            if (getattr(User, 'USERNAME_FIELD', 'username') == 'username'):\n                params['username'] = email[:30]\n            user = User.objects.create_user(**params)\n            user = auth.authenticate(username=email, password=password)\n            auth.login(self.request, user)\n        else:\n            user = self.request.user\n        contact = self.shop.contact_model(user=user)\n        order.user = user\n        signals.contact_created.send(sender=self.shop, user=user, contact=contact, password=password, request=self.request)\n    order.save()\n    if contact:\n        contact.update_from_order(order, request=self.request)\n        contact.save()\n    return order\n", "label": 1}
{"function": "\n\ndef parse_args(args, options):\n    'Parse arguments from command-line to set options.'\n    long_opts = ['help', 'oauth', 'save-dir=', 'api-rate', 'timeline=', 'mentions=', 'favorites', 'follow-redirects', 'redirect-sites=', 'dms=', 'isoformat']\n    short_opts = 'hos:at:m:vfr:d:i'\n    (opts, extra_args) = getopt(args, short_opts, long_opts)\n    for (opt, arg) in opts:\n        if (opt in ('-h', '--help')):\n            print(__doc__)\n            raise SystemExit(0)\n        elif (opt in ('-o', '--oauth')):\n            options['oauth'] = True\n        elif (opt in ('-s', '--save-dir')):\n            options['save-dir'] = arg\n        elif (opt in ('-a', '--api-rate')):\n            options['api-rate'] = True\n        elif (opt in ('-t', '--timeline')):\n            options['timeline'] = arg\n        elif (opt in ('-m', '--mentions')):\n            options['mentions'] = arg\n        elif (opt in ('-v', '--favorites')):\n            options['favorites'] = True\n        elif (opt in ('-f', '--follow-redirects')):\n            options['follow-redirects'] = True\n        elif (opt in ('-r', '--redirect-sites')):\n            options['redirect-sites'] = arg\n        elif (opt in ('-d', '--dms')):\n            options['dms'] = arg\n        elif (opt in ('-i', '--isoformat')):\n            options['isoformat'] = True\n    options['extra_args'] = extra_args\n", "label": 1}
{"function": "\n\ndef get_primitive(self, primitive):\n    if (primitive == 'USER'):\n        auth = (self.request.authorization and self.request.authorization[1])\n        userpass = ((auth and base64.b64decode(auth)) or None)\n        return ((userpass and userpass.split(':')[0]) or None)\n    if (primitive == 'ALL_DATA'):\n        return self.get_raw_data()\n    if (primitive == 'LOGGED_IN_USER'):\n        return self.request.user\n    if (primitive == 'RAW_INVOCATION_ARGS'):\n        return unquote('/'.join(self.path_args))\n    raise Exception('Primitive not supported')\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    if (name == 'CustomViewer'):\n        return type.__new__(cls, name, bases, attrs)\n    ui = {\n        \n    }\n    for (key, value) in list(attrs.items()):\n        if (key.startswith('_') or (key in CustomViewer.__dict__)):\n            continue\n        if (not isinstance(value, (MethodType, FunctionType))):\n            ui[key] = attrs.pop(key)\n    attrs['ui'] = ui\n    attrs.setdefault('name', name)\n    udfs = {\n        \n    }\n    for (nm, value) in list(attrs.items()):\n        dscr = CustomViewer.__dict__.get(nm, None)\n        if isinstance(dscr, UserDefinedFunction):\n            udfs[nm] = attrs.pop(nm)\n    result = type.__new__(cls, name, bases, attrs)\n    for (k, v) in udfs.items():\n        udf_decorator = getattr(result, k)\n        udf_decorator(v)\n    result._build_widget_subclass()\n    return result\n", "label": 1}
{"function": "\n\n@verbose\ndef __init__(self, vertices, pos=None, values=None, hemi=None, comment='', name=None, filename=None, subject=None, color=None, verbose=None):\n    if (not isinstance(hemi, string_types)):\n        raise ValueError(('hemi must be a string, not %s' % type(hemi)))\n    vertices = np.asarray(vertices)\n    if np.any((np.diff(vertices.astype(int)) <= 0)):\n        raise ValueError('Vertices must be ordered in increasing order.')\n    if (color is not None):\n        from matplotlib.colors import colorConverter\n        color = colorConverter.to_rgba(color)\n    if (values is None):\n        values = np.ones(len(vertices))\n    else:\n        values = np.asarray(values)\n    if (pos is None):\n        pos = np.zeros((len(vertices), 3))\n    else:\n        pos = np.asarray(pos)\n    if (not (len(vertices) == len(values) == len(pos))):\n        raise ValueError('vertices, values and pos need to have same length (number of vertices)')\n    if ((name is None) and (filename is not None)):\n        name = op.basename(filename[:(- 6)])\n    self.vertices = vertices\n    self.pos = pos\n    self.values = values\n    self.hemi = hemi\n    self.comment = comment\n    self.verbose = verbose\n    self.subject = _check_subject(None, subject, False)\n    self.color = color\n    self.name = name\n    self.filename = filename\n", "label": 1}
{"function": "\n\ndef strip_parens(s):\n    if ((not s) or (s[0] != '(')):\n        return s\n    ct = i = 0\n    l = len(s)\n    while (i < l):\n        if ((s[i] == '(') and (s[(l - 1)] == ')')):\n            ct += 1\n            i += 1\n            l -= 1\n        else:\n            break\n    if ct:\n        unbalanced_ct = 0\n        required = 0\n        for i in range(ct, (l - ct)):\n            if (s[i] == '('):\n                unbalanced_ct += 1\n            elif (s[i] == ')'):\n                unbalanced_ct -= 1\n            if (unbalanced_ct < 0):\n                required += 1\n                unbalanced_ct = 0\n            if (required == ct):\n                break\n        ct -= required\n    if (ct > 0):\n        return s[ct:(- ct)]\n    return s\n", "label": 1}
{"function": "\n\ndef run(self):\n    client = self.client\n    container = self.container\n    object_names = self.object_names\n    cname = utils.get_name(container)\n    ident = self.client.identity\n    headers = {\n        'X-Auth-Token': ident.token,\n        'Content-Type': 'text/plain',\n    }\n    uri = '/?bulk-delete=1'\n    key_map = {\n        'Number Not Found': 'not_found',\n        'Response Status': 'status',\n        'Errors': 'errors',\n        'Number Deleted': 'deleted',\n        'Response Body': None,\n    }\n    batch = []\n    while object_names:\n        batch[:] = object_names[:MAX_BULK_DELETE]\n        del object_names[:MAX_BULK_DELETE]\n        obj_paths = (('%s/%s' % (cname, nm)) for nm in batch)\n        body = '\\n'.join(obj_paths)\n        (resp, resp_body) = self.client.method_delete(uri, data=body, headers=headers)\n        for (k, v) in six.iteritems(resp_body):\n            if (key_map[k] == 'errors'):\n                status_code = int(resp_body.get('Response Status', '200')[:3])\n                if ((status_code != 200) and (not v)):\n                    self.results['errors'].extend([[resp_body.get('Response Body'), resp_body.get('Response Status')]])\n                else:\n                    self.results['errors'].extend(v)\n            elif (key_map[k] in ('deleted', 'not_found')):\n                self.results[key_map[k]] += int(v)\n            elif key_map[k]:\n                self.results[key_map[k]] = v\n    self.completed = True\n", "label": 1}
{"function": "\n\ndef getdoc(object):\n    'Get the documentation string for an object.\\n\\n    All tabs are expanded to spaces.  To clean up docstrings that are\\n    indented to line up with blocks of code, any whitespace than can be\\n    uniformly removed from the second line onwards is removed.'\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if (not isinstance(doc, types.StringTypes)):\n        return None\n    try:\n        lines = string.split(string.expandtabs(doc), '\\n')\n    except UnicodeError:\n        return None\n    else:\n        margin = sys.maxint\n        for line in lines[1:]:\n            content = len(string.lstrip(line))\n            if content:\n                indent = (len(line) - content)\n                margin = min(margin, indent)\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if (margin < sys.maxint):\n            for i in range(1, len(lines)):\n                lines[i] = lines[i][margin:]\n        while (lines and (not lines[(- 1)])):\n            lines.pop()\n        while (lines and (not lines[0])):\n            lines.pop(0)\n        return string.join(lines, '\\n')\n", "label": 1}
{"function": "\n\ndef on_change_input(self, fgraph, app, i, old_r, new_r, reason):\n    '\\n        app.inputs[i] changed from old_r to new_r.\\n\\n        '\n    if (app == 'output'):\n        pass\n    else:\n        if (app not in self.debug_all_apps):\n            raise ProtocolError('change without import')\n        self.clients[old_r][app] -= 1\n        if (self.clients[old_r][app] == 0):\n            del self.clients[old_r][app]\n        self.clients.setdefault(new_r, OrderedDict()).setdefault(app, 0)\n        self.clients[new_r][app] += 1\n        for (o_idx, i_idx_list) in iteritems(getattr(app.op, 'view_map', OrderedDict())):\n            if (len(i_idx_list) > 1):\n                raise NotImplementedError()\n            i_idx = i_idx_list[0]\n            output = app.outputs[o_idx]\n            if (i_idx == i):\n                if (app.inputs[i_idx] is not new_r):\n                    raise ProtocolError('wrong new_r on change')\n                self.view_i[output] = new_r\n                self.view_o[old_r].remove(output)\n                if (not self.view_o[old_r]):\n                    del self.view_o[old_r]\n                self.view_o.setdefault(new_r, OrderedSet()).add(output)\n    self.stale_droot = True\n", "label": 1}
{"function": "\n\ndef decompress(self, value):\n    '\\n        Receives an instance of `MultiLingualText` (or a properly-formatted\\n        block of XML) and returns a list of values corresponding in position\\n        to the current ordering of settings.LANGUAGES.\\n        '\n    text_dict = {\n        \n    }\n    if value:\n        if isinstance(value, datastructures.MultiLingualText):\n            text_dict = dict(((code, getattr(value, code)) for (code, verbose) in LANGUAGES))\n        else:\n            try:\n                xml_as_python_object = objectify.fromstring(value)\n            except XMLSyntaxError:\n                raise Exception(('%s MultiLingualTextFieldWidget.decompress()!' % INVALID_XML_ERROR))\n            else:\n                text_dict = dict(((unicode(l.code), unicode((l.language_text or ''))) for l in xml_as_python_object.language))\n    return [text_dict.get(code, '') for (code, verbose) in LANGUAGES]\n", "label": 1}
{"function": "\n\ndef _format_node(self, op):\n    formatted_args = []\n\n    def visit(what, extra_indents=0):\n        if isinstance(what, ir.Expr):\n            result = self._format_subexpr(what)\n        else:\n            result = self._indent(str(what))\n        if (extra_indents > 0):\n            result = util.indent(result, self.indent_size)\n        formatted_args.append(result)\n    arg_names = getattr(op, '_arg_names', None)\n    if (arg_names is None):\n        for arg in op.args:\n            if isinstance(arg, list):\n                for x in arg:\n                    visit(x)\n            else:\n                visit(arg)\n    else:\n        for (arg, name) in zip(op.args, arg_names):\n            if (name is not None):\n                name = self._indent('{0}:'.format(name))\n            if isinstance(arg, list):\n                if ((name is not None) and (len(arg) > 0)):\n                    formatted_args.append(name)\n                    indents = 1\n                else:\n                    indents = 0\n                for x in arg:\n                    visit(x, extra_indents=indents)\n            else:\n                if (name is not None):\n                    formatted_args.append(name)\n                    indents = 1\n                else:\n                    indents = 0\n                visit(arg, extra_indents=indents)\n    opname = type(op).__name__\n    type_display = self._get_type_display(op)\n    opline = ('%s[%s]' % (opname, type_display))\n    return '\\n'.join(([opline] + formatted_args))\n", "label": 1}
{"function": "\n\ndef __call__(self, func=None, timeout=None, wait=None, message=None):\n    if (func is None):\n        return (lambda func: self(func, timeout, wait, message))\n    if func():\n        return\n    now = self.getnow()\n    sleep = self.getsleep()\n    if (timeout is None):\n        timeout = self.timeout\n    if (wait is None):\n        wait = self.wait\n    wait = float(wait)\n    deadline = (now() + timeout)\n    while 1:\n        sleep(wait)\n        if func():\n            return\n        if (now() > deadline):\n            raise self.TimeOutWaitingFor((message or getattr(func, '__doc__') or getattr(func, '__name__')))\n", "label": 1}
{"function": "\n\ndef update_active_tasks(self):\n    self.metric_sources = []\n    for (service_name, service) in self.collector_config.services.iteritems():\n        (service_record, created) = Service.objects.get_or_create(name=service_name, defaults={\n            'metric_url': service.metric_url,\n        })\n        if (not created):\n            service_record.active = True\n            service_record.save()\n        for (cluster_name, cluster) in service.clusters.iteritems():\n            (cluster_record, created) = Cluster.objects.get_or_create(service=service_record, name=cluster_name)\n            if (not created):\n                cluster_record.active = True\n                cluster_record.save()\n            for job_name in service.jobs:\n                (job_record, created) = Job.objects.get_or_create(cluster=cluster_record, name=job_name)\n                if (not created):\n                    job_record.active = True\n                    job_record.save()\n                job = cluster.jobs[job_name]\n                port = (job.base_port + 1)\n                hosts = job.hosts\n                for (host_id, host) in hosts.iteritems():\n                    host_name = job.hostnames[host_id]\n                    for instance_id in range(host.instance_num):\n                        task_id = deploy_utils.get_task_id(hosts, host_id, instance_id)\n                        instance_port = deploy_utils.get_base_port(port, instance_id)\n                        (task_record, created) = Task.objects.get_or_create(job=job_record, task_id=task_id, defaults={\n                            'host': host_name,\n                            'port': instance_port,\n                        })\n                        if ((not created) or (task_record.host != host_name) or (task_record.port != instance_port)):\n                            task_record.active = True\n                            task_record.host = host_name\n                            task_record.port = instance_port\n                            task_record.save()\n                        self.metric_sources.append(MetricSource(self.collector_config, task_record))\n", "label": 1}
{"function": "\n\ndef test_content_flatten_comments(reddit):\n    url = 'https://www.reddit.com/r/AskReddit/comments/cmwov'\n    submission = reddit.get_submission(url, comment_sort='top')\n    more_comment = submission.comments[(- 1)]\n    assert isinstance(more_comment, praw.objects.MoreComments)\n    comments = more_comment.comments()\n    top_level_comments = []\n    for comment in comments[:(- 1)]:\n        if (comment.parent_id == more_comment.parent_id):\n            top_level_comments.append(comment.id)\n        else:\n            assert comment.parent_id.endswith(top_level_comments[(- 1)])\n    top_level_comments.append(comments[(- 1)].id)\n    assert isinstance(comments[(- 1)], praw.objects.MoreComments)\n    assert (comments[(- 1)].parent_id == more_comment.parent_id)\n    flattened = Content.flatten_comments(comments, root_level=2)\n    assert (len(flattened) == len(comments))\n    for (i, comment) in enumerate(flattened):\n        assert (comment.id == comments[i].id)\n        if (comment.id in top_level_comments):\n            assert (comment.nested_level == 2)\n        else:\n            assert (comment.nested_level > 2)\n", "label": 1}
{"function": "\n\ndef scan_file(self, fp):\n    current_file_offset = 0\n    while True:\n        (data, dlen) = fp.read_block()\n        if (not data):\n            break\n        current_block_offset = 0\n        block_start = (fp.tell() - dlen)\n        self.status.completed = (block_start - fp.offset)\n        for r in self.magic.scan(data, dlen):\n            if (r.offset < current_block_offset):\n                continue\n            relative_offset = (r.offset + r.adjust)\n            r.offset = (block_start + relative_offset)\n            r.file = fp\n            self.result(r=r)\n            if (r.valid and (r.jump > 0) and (not self.dumb_scan)):\n                absolute_jump_offset = (r.offset + r.jump)\n                current_block_offset = (relative_offset + r.jump)\n                if (absolute_jump_offset >= fp.tell()):\n                    fp.seek((r.offset + r.jump))\n                    break\n", "label": 1}
{"function": "\n\ndef get_sources(self, url, hosthdDict, hostDict, locDict):\n    try:\n        sources = []\n        if (url == None):\n            return sources\n        url = urlparse.urljoin(self.base_link, url)\n        result = client.source(url)\n        quality = re.compile('Quality *: *(.+)').findall(result)\n        quality = ('SD' if (len(quality) == 0) else quality[0])\n        quality = re.sub('<.+?>', '', quality).strip().upper()\n        if (quality == 'SD'):\n            quality = 'SD'\n        elif (quality == 'HD'):\n            quality = 'HD'\n        else:\n            quality = 'CAM'\n        url = client.parseDOM(result, 'iframe', ret='src')\n        url = [i for i in url if ('filepup' in i)][0]\n        url = filepup.resolve(url)\n        if (url == None):\n            raise Exception()\n        sources.append({\n            'source': 'Filepup',\n            'quality': quality,\n            'provider': 'Movienight',\n            'url': url,\n        })\n        return sources\n    except:\n        return sources\n", "label": 1}
{"function": "\n\ndef get_file_generator(environ, root_path, config=None):\n    logname = log_name(environ)\n    logpath = safe_path(root_path, logname)\n    if (logpath is None):\n        raise UnsafePath()\n    file_generator = None\n    use_files = (util.parse_param(environ, 'source', default='all') != 'swift')\n    if (use_files and does_file_exist(logpath)):\n        file_generator = DiskIterableBuffer(logname, logpath, config)\n    elif (logname and (logname[(- 1)] == '/')):\n        file_generator = SwiftIterableBuffer(os.path.join(logname, 'index.html'), config)\n        if (not file_generator.obj):\n            file_generator = SwiftIterableBuffer(logname, config)\n    else:\n        file_generator = SwiftIterableBuffer(logname, config)\n        if (not file_generator.obj):\n            file_generator = SwiftIterableBuffer(os.path.join(logname, 'index.html'), config)\n    if ((not file_generator) or (not file_generator.obj)):\n        if (config.has_section('general') and config.has_option('general', 'generate_folder_index') and config.getboolean('general', 'generate_folder_index')):\n            index_generator = IndexIterableBuffer(logname, logpath, config)\n            if (len(index_generator.file_list) > 0):\n                return index_generator\n        raise NoSuchFile()\n    return file_generator\n", "label": 1}
{"function": "\n\ndef test_equality_rewrite_diff_static(self):\n    'Test an equality rewrite with different static values'\n    l = ast.Literal('foo')\n    s = ast.Literal('\"test\"')\n    s.static = True\n    s.static_val = 'test'\n    s1 = ast.Literal('\"other\"')\n    s1.static = True\n    s1.static_val = 'other'\n    cmp1 = ast.CompareOperator('=', l, s)\n    cmp2 = ast.CompareOperator('=', l, s1)\n    or1 = ast.LogicalOperator('or', cmp1, cmp2)\n    name = merge.node_name(cmp1, True)\n    r = compare.equality_rewrite(ast.dup(or1), name, cmp1, True)\n    assert isinstance(r.left, ast.Constant)\n    assert (r.left.value == True)\n    assert isinstance(r.right, ast.Constant)\n    assert (r.right.value == False)\n    name = merge.node_name(cmp1, True)\n    r = compare.equality_rewrite(ast.dup(or1), name, cmp1, False)\n    assert isinstance(r.left, ast.Constant)\n    assert (r.left.value == False)\n    assert ASTPattern(cmp2).matches(r.right)\n    name = merge.node_name(cmp2, True)\n    r = compare.equality_rewrite(ast.dup(or1), name, cmp2, True)\n    assert isinstance(r.left, ast.Constant)\n    assert (r.left.value == False)\n    assert isinstance(r.right, ast.Constant)\n    assert (r.right.value == True)\n    name = merge.node_name(cmp2, True)\n    r = compare.equality_rewrite(ast.dup(or1), name, cmp2, False)\n    assert ASTPattern(cmp1).matches(r.left)\n    assert isinstance(r.right, ast.Constant)\n    assert (r.right.value == False)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef generate_discrete_support(params, support=0.95, nbins=100):\n    \"\\n        returns a set of intervals over which the component model pdf is\\n        supported.\\n        Inputs:\\n            params: a dict with entries 'mu' and 'kappa'\\n            nbins: cardinality of the set or the number of grid points in the\\n                approximation\\n            support: a float in (0,1) that describes the amount of probability\\n                we want in the range of support\\n        \"\n    if (type(nbins) is not int):\n        raise TypeError('nbins should be an int')\n    if (nbins <= 0):\n        raise ValueError('nbins should be greater than 0')\n    support = check_type_force_float(support, 'support')\n    if ((support <= 0.0) or (support >= 1.0)):\n        raise ValueError('support is a float st: 0 < support < 1')\n    check_model_params_dict(params)\n    mu = params['mu']\n    kappa = params['kappa']\n    assert ((mu >= 0) and (mu <= (2 * math.pi)))\n    (a, b) = vonmises.interval(support, kappa)\n    a += mu\n    b += mu\n    assert ((- math.pi) <= a < b <= (3 * math.pi))\n    assert ((b - a) <= (2 * math.pi))\n    support_range = (b - a)\n    support_bin_size = (support_range / (nbins - 1.0))\n    bins = [(a + (i * support_bin_size)) for i in range(nbins)]\n    return bins\n", "label": 1}
{"function": "\n\ndef _set_foreground(self, foreground):\n    color = None\n    flags = 0\n    for part in foreground.split(','):\n        part = part.strip()\n        if (part in _ATTRIBUTES):\n            if (flags & _ATTRIBUTES[part]):\n                raise AttrSpecError((('Setting %s specified more than' + 'once in foreground (%s)') % (repr(part), repr(foreground))))\n            flags |= _ATTRIBUTES[part]\n            continue\n        if (part in ('', 'default')):\n            scolor = 0\n        elif (part in _BASIC_COLORS):\n            scolor = _BASIC_COLORS.index(part)\n            flags |= _FG_BASIC_COLOR\n        elif (self._value & _HIGH_88_COLOR):\n            scolor = _parse_color_88(part)\n            flags |= _FG_HIGH_COLOR\n        else:\n            scolor = _parse_color_256(part)\n            flags |= _FG_HIGH_COLOR\n        if (scolor is None):\n            raise AttrSpecError((('Unrecognised color specification %s' + 'in foreground (%s)') % (repr(part), repr(foreground))))\n        if (color is not None):\n            raise AttrSpecError((('More than one color given for ' + 'foreground (%s)') % (repr(foreground),)))\n        color = scolor\n    if (color is None):\n        color = 0\n    self._value = (((self._value & (~ _FG_MASK)) | color) | flags)\n", "label": 1}
{"function": "\n\ndef select_related_descend(field, restricted, requested, reverse=False):\n    '\\n    Returns True if this field should be used to descend deeper for\\n    select_related() purposes. Used by both the query construction code\\n    (sql.query.fill_related_selections()) and the model instance creation code\\n    (query.get_cached_row()).\\n\\n    Arguments:\\n     * field - the field to be checked\\n     * restricted - a boolean field, indicating if the field list has been\\n       manually restricted using a requested clause)\\n     * requested - The select_related() dictionary.\\n     * reverse - boolean, True if we are checking a reverse select related\\n    '\n    if (not field.rel):\n        return False\n    if (field.rel.parent_link and (not reverse)):\n        return False\n    if restricted:\n        if (reverse and (field.related_query_name() not in requested)):\n            return False\n        if ((not reverse) and (field.name not in requested)):\n            return False\n    if ((not restricted) and field.null):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef render_content(self, content, request, response, **kwargs):\n    if (hasattr(content, 'interface') and ((content.interface is True) or hasattr(content.interface, 'http'))):\n        if (content.interface is True):\n            content(request, response, api_version=None, **kwargs)\n        else:\n            content.interface.http(request, response, api_version=None, **kwargs)\n        return\n    content = self.transform_data(content, request, response)\n    content = self.outputs(content, **self._arguments(self._params_for_outputs, request, response))\n    if hasattr(content, 'read'):\n        size = None\n        if (hasattr(content, 'name') and os.path.isfile(content.name)):\n            size = os.path.getsize(content.name)\n        if (request.range and size):\n            (start, end) = request.range\n            if (end < 0):\n                end = (size + end)\n            end = min(end, size)\n            length = ((end - start) + 1)\n            content.seek(start)\n            response.data = content.read(length)\n            response.status = falcon.HTTP_206\n            response.content_range = (start, end, size)\n            content.close()\n        else:\n            response.stream = content\n            if size:\n                response.stream_len = size\n    else:\n        response.data = content\n", "label": 1}
{"function": "\n\ndef receiveMessage(self, msg, sender):\n    if (msg == 'Best Friend'):\n        if (not self.bestFriend):\n            self.bestFriend = self.createActor(Lassie)\n        self.send(self.bestFriend, msg)\n        return\n    if (msg == 'Best Friend Says'):\n        if (not self.bestFriend):\n            self.send(sender, 'no best friend')\n        else:\n            self.waiter = sender\n            self.send(self.bestFriend, 'what')\n    elif (self.bestFriend and (self.bestFriend == sender)):\n        if self.waiter:\n            if (sender != self.bestFriend):\n                self.send(self.waiter, 'ERROR: lcl/rmt address comparison not epimorphic.')\n            else:\n                self.send(self.waiter, msg)\n            self.waiter = None\n        else:\n            pass\n    elif (sender != self.myAddress):\n        self.send(sender, 'Greetings.')\n", "label": 1}
{"function": "\n\ndef test_warp_no_reproject(runner, tmpdir):\n    ' When called without parameters, output should be same as source '\n    srcname = 'tests/data/shade.tif'\n    outputname = str(tmpdir.join('test.tif'))\n    result = runner.invoke(warp.warp, [srcname, outputname])\n    assert (result.exit_code == 0)\n    assert os.path.exists(outputname)\n    with rasterio.open(srcname) as src:\n        with rasterio.open(outputname) as output:\n            assert (output.count == src.count)\n            assert (output.crs == src.crs)\n            assert (output.nodata == src.nodata)\n            assert numpy.allclose(output.bounds, src.bounds)\n            assert output.affine.almost_equals(src.affine)\n            assert numpy.allclose(output.read(1), src.read(1))\n", "label": 1}
{"function": "\n\ndef format_option(self, option):\n    old_help = option.help\n    default = option.default\n    if (isinstance(default, str) and (' ' in default)):\n        default = repr(default)\n    if (option.help is None):\n        option.help = ('Specify a %s.' % option.type)\n    if (option.type == 'choice'):\n        choices = []\n        for choice in option.choices:\n            if (choice == option.default):\n                if (' ' in choice):\n                    choice = repr(choice)\n                choice = (('[' + choice) + ']')\n            elif (' ' in choice):\n                choice = repr(choice)\n            choices.append(choice)\n        option.help = ('%s Choices: %s.' % (option.help, ', '.join(choices)))\n    elif (default != NO_DEFAULT):\n        if (option.action == 'store_false'):\n            option.help = ('%s Default: %s.' % (option.help, (not default)))\n        else:\n            option.help = ('%s Default: %s.' % (option.help, default))\n    result = TitledHelpFormatter.format_option(self, option)\n    option.help = old_help\n    return result\n", "label": 1}
{"function": "\n\ndef add(name, uid=None, gid=None, groups=None, home=None, shell=None, fullname=None, createhome=True, **kwargs):\n    \"\\n    Add a user to the minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' user.add name <uid> <gid> <groups> <home> <shell>\\n    \"\n    if info(name):\n        raise CommandExecutionError(\"User '{0}' already exists\".format(name))\n    if salt.utils.contains_whitespace(name):\n        raise SaltInvocationError('Username cannot contain whitespace')\n    if (uid is None):\n        uid = _first_avail_uid()\n    if (gid is None):\n        gid = 20\n    if (home is None):\n        home = '/Users/{0}'.format(name)\n    if (shell is None):\n        shell = '/bin/bash'\n    if (fullname is None):\n        fullname = ''\n    if (not isinstance(uid, int)):\n        raise SaltInvocationError('uid must be an integer')\n    if (not isinstance(gid, int)):\n        raise SaltInvocationError('gid must be an integer')\n    name_path = '/Users/{0}'.format(name)\n    _dscl([name_path, 'UniqueID', uid])\n    _dscl([name_path, 'PrimaryGroupID', gid])\n    _dscl([name_path, 'UserShell', shell])\n    _dscl([name_path, 'NFSHomeDirectory', home])\n    _dscl([name_path, 'RealName', fullname])\n    if createhome:\n        __salt__['file.mkdir'](home, user=uid, group=gid)\n    time.sleep(1)\n    if groups:\n        chgroups(name, groups)\n    return True\n", "label": 1}
{"function": "\n\n@property\ndef legacy_modes(self):\n    args = ['+']\n    for i in self.props.keys():\n        if ((self.props[i] != False) and (i in channel_property_items) and (not ((i == 'ban') or (i == 'quiet') or (i == 'invite-exemption') or (i == 'exemption')))):\n            args[0] += channel_property_items[i]\n            if (self.props[i] != True):\n                args.append(self.props[i])\n    return ' '.join(args)\n", "label": 1}
{"function": "\n\ndef test_Idx_fixed_bounds():\n    (i, a, b, x) = symbols('i a b x', integer=True)\n    assert (Idx(x).lower is None)\n    assert (Idx(x).upper is None)\n    assert (Idx(x, a).lower == 0)\n    assert (Idx(x, a).upper == (a - 1))\n    assert (Idx(x, 5).lower == 0)\n    assert (Idx(x, 5).upper == 4)\n    assert (Idx(x, oo).lower == 0)\n    assert (Idx(x, oo).upper == oo)\n    assert (Idx(x, (a, b)).lower == a)\n    assert (Idx(x, (a, b)).upper == b)\n    assert (Idx(x, (1, 5)).lower == 1)\n    assert (Idx(x, (1, 5)).upper == 5)\n    assert (Idx(x, ((- oo), oo)).lower == (- oo))\n    assert (Idx(x, ((- oo), oo)).upper == oo)\n", "label": 1}
{"function": "\n\ndef _verts_within_dist(graph, sources, max_dist):\n    'Find all vertices wihin a maximum geodesic distance from source\\n\\n    Parameters\\n    ----------\\n    graph : scipy.sparse.csr_matrix\\n        Sparse matrix with distances between adjacent vertices.\\n    sources : list of int\\n        Source vertices.\\n    max_dist : float\\n        Maximum geodesic distance.\\n\\n    Returns\\n    -------\\n    verts : array\\n        Vertices within max_dist.\\n    dist : array\\n        Distances from source vertex.\\n    '\n    dist_map = {\n        \n    }\n    verts_added_last = []\n    for source in sources:\n        dist_map[source] = 0\n        verts_added_last.append(source)\n    while (len(verts_added_last) > 0):\n        verts_added = []\n        for i in verts_added_last:\n            v_dist = dist_map[i]\n            row = graph[i, :]\n            neighbor_vert = row.indices\n            neighbor_dist = row.data\n            for (j, d) in zip(neighbor_vert, neighbor_dist):\n                n_dist = (v_dist + d)\n                if (j in dist_map):\n                    if (n_dist < dist_map[j]):\n                        dist_map[j] = n_dist\n                elif (n_dist <= max_dist):\n                    dist_map[j] = n_dist\n                    verts_added.append(j)\n        verts_added_last = verts_added\n    verts = np.sort(np.array(list(dist_map.keys()), dtype=np.int))\n    dist = np.array([dist_map[v] for v in verts])\n    return (verts, dist)\n", "label": 1}
{"function": "\n\ndef prompt_for_var(prompt_str, default=None, allow_empty=False, choices=None):\n    prompt = prompt_str\n    if (default is not None):\n        prompt += ((' [' + default) + ']: ')\n    else:\n        prompt += ': '\n    while True:\n        try:\n            value = input(prompt)\n        except KeyboardInterrupt:\n            print('')\n            exit(1)\n        except EOFError:\n            print('')\n            exit(1)\n        if (value != ''):\n            if ((choices is not None) and (value not in choices)):\n                print(('Error: unrecognized response, expected one of ' + json.dumps(choices)))\n            else:\n                return value\n        elif (default is not None):\n            return default\n        elif allow_empty:\n            return value\n", "label": 1}
{"function": "\n\ndef _parse(self, node, alias_map, conv):\n    node_type = getattr(node, '_node_type', None)\n    unknown = False\n    if (node_type in self._parse_map):\n        (sql, params) = self._parse_map[node_type](node, alias_map, conv)\n        unknown = (node_type in self._unknown_types)\n    elif isinstance(node, (list, tuple)):\n        (sql, params) = self.parse_node_list(node, alias_map, conv)\n        sql = ('(%s)' % sql)\n    elif isinstance(node, Model):\n        sql = self.interpolation\n        if (conv and isinstance(conv, ForeignKeyField) and (not isinstance(conv.to_field, ForeignKeyField))):\n            params = [conv.to_field.db_value(getattr(node, conv.to_field.name))]\n        else:\n            params = [node._get_pk_value()]\n    elif ((isclass(node) and issubclass(node, Model)) or isinstance(node, ModelAlias)):\n        entity = node.as_entity().alias(alias_map[node])\n        (sql, params) = self.parse_node(entity, alias_map, conv)\n    else:\n        (sql, params) = self._parse_default(node, alias_map, conv)\n        unknown = True\n    return (sql, params, unknown)\n", "label": 1}
{"function": "\n\ndef _sock_state_cb(self, fd, readable, writable):\n    if (readable or writable):\n        if readable:\n            self.loop.add_reader(fd, self._handle_event, fd, READ)\n            self._read_fds.add(fd)\n        if writable:\n            self.loop.add_writer(fd, self._handle_event, fd, WRITE)\n            self._write_fds.add(fd)\n        if (self._timer is None):\n            self._timer = self.loop.call_later(1.0, self._timer_cb)\n    else:\n        if (fd in self._read_fds):\n            self._read_fds.discard(fd)\n            self.loop.remove_reader(fd)\n        if (fd in self._write_fds):\n            self._write_fds.discard(fd)\n            self.loop.remove_writer(fd)\n        if ((not self._read_fds) and (not self._write_fds) and (self._timer is not None)):\n            self._timer.cancel()\n            self._timer = None\n", "label": 1}
{"function": "\n\ndef test_issue_4149():\n    assert (3 + I).is_complex\n    assert ((3 + I).is_imaginary is False)\n    assert ((3 * I) + (S.Pi * I)).is_imaginary\n    y = Symbol('y', real=True)\n    assert ((((3 * I) + (S.Pi * I)) + (y * I)).is_imaginary is None)\n    p = Symbol('p', positive=True)\n    assert (((3 * I) + (S.Pi * I)) + (p * I)).is_imaginary\n    n = Symbol('n', negative=True)\n    assert ((((- 3) * I) - (S.Pi * I)) + (n * I)).is_imaginary\n    i = Symbol('i', imaginary=True)\n    assert ([(i ** a).is_imaginary for a in range(4)] == [False, True, False, True])\n    e = S('-sqrt(3)*I/2 + 0.866025403784439*I')\n    assert (e.is_real is False)\n    assert e.is_imaginary\n", "label": 1}
{"function": "\n\ndef test_new_projection(self):\n    rec = {\n        '@Package': {\n            'name': 'foo',\n            'version': '1.0.0',\n            'rid': 'this_is_fake',\n        },\n    }\n    x = self.client.record_create(9, rec)\n    assert (x._rid == '#9:0')\n    import re\n    assert re.match('[0-1]', str(x._version))\n    assert (x._class == 'Package')\n    assert (x.name == 'foo')\n    assert (x.version == '1.0.0')\n    assert (x.rid == 'this_is_fake')\n    assert (x.oRecordData['name'] == 'foo')\n    assert (x.oRecordData['version'] == '1.0.0')\n    assert (x.oRecordData['rid'] == 'this_is_fake')\n", "label": 1}
{"function": "\n\ndef get_language(tree, headers, domain=None):\n    lang = None\n    if (headers and ('content-language' in headers)):\n        lang = headers['content-language']\n    if ((lang is None) and ('lang' in tree.attrib)):\n        lang = tree.attrib['lang']\n    if (lang is None):\n        page_txt = (' '.join(tree.xpath('//p/text()')) + ' '.join(tree.xpath('//div/text()')))\n        if (not page_txt):\n            page_txt = tree.text_content()\n        lang = langdetect.detect(page_txt)\n    if ((lang is None) and (domain is not None)):\n        lang = domain.split('.')[(- 1)]\n        if (lang == 'com'):\n            return 'en'\n    return (lang[:2] or 'en')\n", "label": 1}
{"function": "\n\ndef heuristics(e, z, z0, dir):\n    rv = None\n    if (abs(z0) is S.Infinity):\n        rv = limit(e.subs(z, (1 / z)), z, S.Zero, ('+' if (z0 is S.Infinity) else '-'))\n        if isinstance(rv, Limit):\n            return\n    elif (e.is_Mul or e.is_Add or e.is_Pow or e.is_Function):\n        r = []\n        for a in e.args:\n            l = limit(a, z, z0, dir)\n            if (l.has(S.Infinity) and (l.is_finite is None)):\n                return\n            elif isinstance(l, Limit):\n                return\n            elif (l is S.NaN):\n                return\n            else:\n                r.append(l)\n        if r:\n            rv = e.func(*r)\n            if (rv is S.NaN):\n                return\n    return rv\n", "label": 1}
{"function": "\n\ndef func(self):\n    'Set the aliases.'\n    caller = self.caller\n    if (not self.lhs):\n        string = 'Usage: @alias <obj> [= [alias[,alias ...]]]'\n        self.caller.msg(string)\n        return\n    objname = self.lhs\n    obj = caller.search(objname)\n    if (not obj):\n        return\n    if (self.rhs is None):\n        aliases = obj.aliases.all()\n        if aliases:\n            caller.msg((\"Aliases for '%s': %s\" % (obj.get_display_name(caller), ', '.join(aliases))))\n        else:\n            caller.msg((\"No aliases exist for '%s'.\" % obj.get_display_name(caller)))\n        return\n    if (not obj.access(caller, 'edit')):\n        caller.msg(\"You don't have permission to do that.\")\n        return\n    if (not self.rhs):\n        old_aliases = obj.aliases.all()\n        if old_aliases:\n            caller.msg(('Cleared aliases from %s: %s' % (obj.get_display_name(caller), ', '.join(old_aliases))))\n            obj.aliases.clear()\n        else:\n            caller.msg('No aliases to clear.')\n        return\n    old_aliases = obj.aliases.all()\n    new_aliases = [alias.strip().lower() for alias in self.rhs.split(',') if alias.strip()]\n    old_aliases.extend(new_aliases)\n    aliases = list(set(old_aliases))\n    obj.aliases.add(aliases)\n    obj.at_cmdset_get(force_init=True)\n    caller.msg((\"Alias(es) for '%s' set to %s.\" % (obj.get_display_name(caller), str(obj.aliases))))\n", "label": 1}
{"function": "\n\ndef _remove_hstore_virtual_fields(self):\n    ' remove hstore virtual fields from class '\n    cls = self.model\n    if hasattr(cls, '_hstore_virtual_fields'):\n        for field_name in cls._hstore_virtual_fields.keys():\n            delattr(cls, field_name)\n        delattr(cls, '_hstore_virtual_fields')\n    if (parse_version(get_version()[0:3]) >= parse_version('1.8')):\n        hstore_fields = []\n        for field in getattr(cls._meta, 'virtual_fields'):\n            if hasattr(field, 'hstore_field_name'):\n                hstore_fields.append(field)\n        for field in hstore_fields:\n            getattr(cls._meta, 'virtual_fields').remove(field)\n        fields = [f for f in cls._meta.fields if (not hasattr(f, 'hstore_field_name'))]\n        cls._meta.fields = cls._meta.fields.__class__(fields)\n    else:\n        for meta_fields in ['fields', 'local_fields', 'virtual_fields']:\n            hstore_fields = []\n            for field in getattr(cls._meta, meta_fields):\n                if hasattr(field, 'hstore_field_name'):\n                    hstore_fields.append(field)\n            for field in hstore_fields:\n                getattr(cls._meta, meta_fields).remove(field)\n", "label": 1}
{"function": "\n\ndef render(self, name, value, attrs=None, choices=(), *args, **kwargs):\n    if (value is None):\n        value = ''\n    final_attrs = self.build_attrs(attrs, name=name)\n    css_class = final_attrs.get('class', 'list')\n    output = ('<ul class=\"%s\">' % css_class)\n    options = None\n    if value:\n        if getattr(value, '__iter__', None):\n            options = [(index, string) for (index, string) in self.choices if (index in value)]\n        else:\n            options = [(index, string) for (index, string) in self.choices if (index == value)]\n    elif self.choices:\n        if ((self.choices[0] != ('', '---------')) and (value != [])):\n            options = [(index, string) for (index, string) in self.choices]\n    if options:\n        for (index, string) in options:\n            if self.queryset:\n                try:\n                    output += ('<li><a href=\"%s\">%s</a></li>' % (self.queryset.get(pk=index).get_absolute_url(), string))\n                except AttributeError:\n                    output += ('<li>%s</li>' % string)\n            else:\n                output += ('<li>%s</li>' % string)\n    else:\n        output += ('<li>%s</li>' % _('None'))\n    return mark_safe((output + '</ul>\\n'))\n", "label": 1}
{"function": "\n\ndef main():\n    arguments = docopt(__doc__)\n    ctx = app.test_request_context()\n    ctx.push()\n    if arguments['user']:\n        if arguments['create']:\n            create_user(arguments['<email>'], interactive=True)\n        elif arguments['activate']:\n            activate_user(arguments['<email_or_id>'])\n        elif arguments['deactivate']:\n            deactivate_user(arguments['<email_or_id>'])\n        elif arguments['delete']:\n            delete_user(arguments['<email_or_id>'])\n    elif arguments['db']:\n        if arguments['create']:\n            create_db()\n        if arguments['destroy']:\n            destroy_db()\n    ctx.pop()\n", "label": 1}
{"function": "\n\ndef save(self):\n    if (not self.data['choice']):\n        return\n    t = self.report\n    model = loads(t.model)\n    classobj = model.get_class_object()\n    xfield = classobj._meta.get_field_by_name(self.field_name)[0]\n    field = model.get_field(self.field_name)\n    if (not field.filters):\n        field.filters = []\n    c = self.data['choice']\n    type = xfield.get_internal_type()\n    if (type == 'ManyToManyField'):\n        c = xfield.related.parent_model.objects.filter(pk=c)[0]\n    if (type == 'ForeignKey'):\n        c = xfield.related.parent_model.objects.filter(pk=c)[0]\n    display_choice = c\n    if hasattr(self.fields['choice'], 'choices'):\n        for (choice, dc) in self.fields['choice'].choices:\n            if (unicode(c) == unicode(choice)):\n                display_choice = dc\n                break\n    for (choice, dc) in self.fields['operand'].choices:\n        if (unicode(self.data['operand']) == unicode(choice)):\n            display_operand = dc\n            break\n    display = ('%s %s %s' % (field.get_human_name(), display_operand, unicode(display_choice)))\n    field.filters.append({\n        'choice': c,\n        'operand': self.data['operand'],\n        'display': display,\n    })\n    t.model = dumps(model)\n    t.save()\n", "label": 1}
{"function": "\n\ndef __menuDefinition(self):\n    with self.getContext():\n        try:\n            selectedChannels = self.getPlug().getValue()\n        except:\n            selectedChannels = IECore.StringVectorData()\n        try:\n            availableChannels = self.__imagePlug['channelNames'].getValue()\n        except:\n            availableChannels = IECore.StringVectorData()\n    result = IECore.MenuDefinition()\n    for channel in availableChannels:\n        channelSelected = (channel in selectedChannels)\n        if channelSelected:\n            newValue = IECore.StringVectorData([c for c in selectedChannels if (c != channel)])\n        else:\n            newValue = IECore.StringVectorData([c for c in availableChannels if ((c in selectedChannels) or (c == channel))])\n        result.append(('/' + channel.replace('.', '/')), {\n            'command': functools.partial(Gaffer.WeakMethod(self.__setValue), value=newValue),\n            'checkBox': channelSelected,\n        })\n    return result\n", "label": 1}
{"function": "\n\ndef write_to_hdf5(group, name, value):\n    filename = group.file.filename\n    if isinstance(value, dict):\n        dict_grp = group[name]\n        for (k, v) in value.items():\n            write_to_hdf5(dict_grp, k, v)\n    elif isinstance(value, VariableTree):\n        vtree_grp = group[name]\n        for k in value.list_vars():\n            write_to_hdf5(vtree_grp, k, value.get(k))\n    elif isinstance(value, np.ndarray):\n        dset = group[name]\n        dset[:] = value[:]\n    elif isinstance(value, list):\n        if (len(value) > 0):\n            if isinstance(value[0], str):\n                dset = group[name]\n                for (i, v) in enumerate(value):\n                    dset[i] = value[i]\n        else:\n            pass\n    elif (value == None):\n        pass\n    elif isinstance(value, (np.float64, float)):\n        dset = group[name]\n        dset[()] = value\n    elif isinstance(value, int):\n        dset = group[name]\n        dset[()] = value\n    elif isinstance(value, str):\n        dset = group[name]\n        dset[()] = value\n        sys.stdout.flush()\n    elif isinstance(value, bool):\n        dset = group[name]\n        dset[()] = value\n", "label": 1}
{"function": "\n\ndef test_random_exact(self):\n    for dtype in REAL_DTYPES:\n        for n in (20, 200):\n            for lapack_driver in TestLstsq.lapack_drivers:\n                for overwrite in (True, False):\n                    a = np.asarray(random([n, n]), dtype=dtype)\n                    for i in range(n):\n                        a[(i, i)] = (20 * (0.1 + a[(i, i)]))\n                    for i in range(4):\n                        b = np.asarray(random([n, 3]), dtype=dtype)\n                        a1 = a.copy()\n                        b1 = b.copy()\n                        try:\n                            out = lstsq(a1, b1, lapack_driver=lapack_driver, overwrite_a=overwrite, overwrite_b=overwrite)\n                        except LstsqLapackError:\n                            if (lapack_driver is None):\n                                mesg = 'LstsqLapackError raised with lapack_driver being None.'\n                                raise AssertionError(mesg)\n                            else:\n                                continue\n                        x = out[0]\n                        r = out[2]\n                        assert_((r == n), ('expected efficient rank %s, got %s' % (n, r)))\n                        if (dtype is np.float32):\n                            assert_allclose(dot(a, x), b, rtol=(500 * np.finfo(a1.dtype).eps), atol=(500 * np.finfo(a1.dtype).eps), err_msg=('driver: %s' % lapack_driver))\n                        else:\n                            assert_allclose(dot(a, x), b, rtol=(1000 * np.finfo(a1.dtype).eps), atol=(1000 * np.finfo(a1.dtype).eps), err_msg=('driver: %s' % lapack_driver))\n", "label": 1}
{"function": "\n\ndef checkPrivileges(self, irc, channel):\n    if self.disabled(irc):\n        return\n    chanserv = self.registryValue('ChanServ')\n    on = ('on %s' % irc.network)\n    if (chanserv and self.registryValue('ChanServ.op', channel)):\n        if (irc.nick not in irc.state.channels[channel].ops):\n            self.log.info('Requesting op from %s in %s %s.', chanserv, channel, on)\n            irc.sendMsg(ircmsgs.privmsg(chanserv, ('op %s' % channel)))\n    if (chanserv and self.registryValue('ChanServ.halfop', channel)):\n        if (irc.nick not in irc.state.channels[channel].halfops):\n            self.log.info('Requesting halfop from %s in %s %s.', chanserv, channel, on)\n            irc.sendMsg(ircmsgs.privmsg(chanserv, ('halfop %s' % channel)))\n    if (chanserv and self.registryValue('ChanServ.voice', channel)):\n        if (irc.nick not in irc.state.channels[channel].voices):\n            self.log.info('Requesting voice from %s in %s %s.', chanserv, channel, on)\n            irc.sendMsg(ircmsgs.privmsg(chanserv, ('voice %s' % channel)))\n", "label": 1}
{"function": "\n\ndef add(x, y):\n    '\\n    Add two matrices, at least one of which is sparse.\\n\\n    This method will provide the right op according\\n    to the inputs.\\n\\n    Parameters\\n    ----------\\n    x\\n        A matrix variable.\\n    y\\n        A matrix variable.\\n\\n    Returns\\n    -------\\n    A sparse matrix\\n        `x` + `y`\\n\\n    Notes\\n    -----\\n    At least one of `x` and `y` must be a sparse matrix.\\n\\n    The grad will be structured only when one of the variable will be a dense\\n    matrix.\\n\\n    '\n    if hasattr(x, 'getnnz'):\n        x = as_sparse_variable(x)\n    if hasattr(y, 'getnnz'):\n        y = as_sparse_variable(y)\n    if (not isinstance(x, theano.Variable)):\n        x = theano.tensor.as_tensor_variable(x)\n    if (not isinstance(y, theano.Variable)):\n        y = theano.tensor.as_tensor_variable(y)\n    x_is_sparse_variable = _is_sparse_variable(x)\n    y_is_sparse_variable = _is_sparse_variable(y)\n    assert (x_is_sparse_variable or y_is_sparse_variable)\n    if (x_is_sparse_variable and y_is_sparse_variable):\n        return add_s_s(x, y)\n    elif (x_is_sparse_variable and (not y_is_sparse_variable)):\n        return add_s_d(x, y)\n    elif (y_is_sparse_variable and (not x_is_sparse_variable)):\n        return add_s_d(y, x)\n    else:\n        raise NotImplementedError()\n", "label": 1}
{"function": "\n\ndef _parse_items(self):\n    'Parse `args.items` into `args.headers`, `args.data`, `args.params`,\\n         and `args.files`.\\n\\n        '\n    self.args.headers = CaseInsensitiveDict()\n    self.args.data = (ParamDict() if self.args.form else OrderedDict())\n    self.args.files = OrderedDict()\n    self.args.params = ParamDict()\n    try:\n        parse_items(items=self.args.items, headers=self.args.headers, data=self.args.data, files=self.args.files, params=self.args.params)\n    except ParseError as e:\n        if self.args.traceback:\n            raise\n        self.error(e.message)\n    if (self.args.files and (not self.args.form)):\n        file_fields = list(self.args.files.keys())\n        if (file_fields != ['']):\n            self.error(('Invalid file fields (perhaps you meant --form?): %s' % ','.join(file_fields)))\n        (fn, fd) = self.args.files['']\n        self.args.files = {\n            \n        }\n        self._body_from_file(fd)\n        if ('Content-Type' not in self.args.headers):\n            (mime, encoding) = mimetypes.guess_type(fn, strict=False)\n            if mime:\n                content_type = mime\n                if encoding:\n                    content_type = ('%s; charset=%s' % (mime, encoding))\n                self.args.headers['Content-Type'] = content_type\n", "label": 1}
{"function": "\n\ndef init(args):\n    encoding = args.get('--encoding')\n    extra_ignore_dirs = args.get('--ignore')\n    if extra_ignore_dirs:\n        extra_ignore_dirs = extra_ignore_dirs.split(',')\n    candidates = get_all_imports(args['<path>'], encoding=encoding, extra_ignore_dirs=extra_ignore_dirs)\n    candidates = get_pkg_names(candidates)\n    logging.debug(('Found imports: ' + ', '.join(candidates)))\n    pypi_server = 'https://pypi.python.org/pypi/'\n    proxy = None\n    if args['--pypi-server']:\n        pypi_server = args['--pypi-server']\n    if args['--proxy']:\n        proxy = {\n            'http': args['--proxy'],\n            'https': args['--proxy'],\n        }\n    if args['--use-local']:\n        logging.debug('Getting package information ONLY from local installation.')\n        imports = get_import_local(candidates, encoding=encoding)\n    else:\n        logging.debug('Getting packages information from Local/PyPI')\n        local = get_import_local(candidates, encoding=encoding)\n        difference = [x for x in candidates if (x.lower() not in [z['name'].lower() for z in local])]\n        imports = (local + get_imports_info(difference, proxy=proxy, pypi_server=pypi_server))\n    path = (args['--savepath'] if args['--savepath'] else os.path.join(args['<path>'], 'requirements.txt'))\n    if ((not args['--savepath']) and (not args['--force']) and os.path.exists(path)):\n        logging.warning('Requirements.txt already exists, use --force to overwrite it')\n        return\n    generate_requirements_file(path, imports)\n    logging.info(('Successfully saved requirements file in ' + path))\n", "label": 1}
{"function": "\n\ndef _replacement(self, node):\n    if isinstance(node.left, ast.Constant):\n        if ((node.type == 'and') and (node.left.value == True)):\n            return node.right\n        if ((node.type == 'or') and (node.left.value == False)):\n            return node.right\n    elif isinstance(node.right, ast.Constant):\n        if ((node.type == 'and') and (node.right.value == True)):\n            return node.left\n        if ((node.type == 'or') and (node.right.value == False)):\n            return node.left\n    return None\n", "label": 1}
{"function": "\n\ndef test_Pow_is_algebraic():\n    e = Symbol('e', algebraic=True)\n    assert Pow(1, e, evaluate=False).is_algebraic\n    assert Pow(0, e, evaluate=False).is_algebraic\n    a = Symbol('a', algebraic=True)\n    na = Symbol('na', algebraic=False)\n    ia = Symbol('ia', algebraic=True, irrational=True)\n    ib = Symbol('ib', algebraic=True, irrational=True)\n    r = Symbol('r', rational=True)\n    x = Symbol('x')\n    assert (a ** r).is_algebraic\n    assert ((a ** x).is_algebraic is None)\n    assert ((na ** r).is_algebraic is None)\n    assert (ia ** r).is_algebraic\n    assert ((ia ** ib).is_algebraic is False)\n    assert ((a ** e).is_algebraic is None)\n    assert (Pow(2, sqrt(2), evaluate=False).is_algebraic is False)\n    assert (Pow(S.GoldenRatio, sqrt(3), evaluate=False).is_algebraic is False)\n    t = Symbol('t', real=True, transcendental=True)\n    n = Symbol('n', integer=True)\n    assert ((t ** n).is_algebraic is None)\n    assert ((t ** n).is_integer is None)\n", "label": 1}
{"function": "\n\ndef test_urls(self):\n    api = Api()\n    api.register(NoteResource())\n    api.register(UserResource())\n    patterns = api.urls\n    self.assertEqual(len(patterns), 3)\n    self.assertEqual(sorted([pattern.name for pattern in patterns if hasattr(pattern, 'name')]), ['api_v1_top_level'])\n    self.assertEqual([[pattern.name for pattern in include.url_patterns if hasattr(pattern, 'name')] for include in patterns if hasattr(include, 'reverse_dict')], [['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail'], ['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail']])\n    api = Api(api_name='v2')\n    api.register(NoteResource())\n    api.register(UserResource())\n    patterns = api.urls\n    self.assertEqual(len(patterns), 3)\n    self.assertEqual(sorted([pattern.name for pattern in patterns if hasattr(pattern, 'name')]), ['api_v2_top_level'])\n    self.assertEqual([[pattern.name for pattern in include.url_patterns if hasattr(pattern, 'name')] for include in patterns if hasattr(include, 'reverse_dict')], [['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail'], ['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail']])\n", "label": 1}
{"function": "\n\ndef _get_objects(self, show=None, order_by=None, name=None, func_name=None, silk_request=None, filters=None):\n    if (not filters):\n        filters = []\n    if (not show):\n        show = self.default_show\n    manager = Profile.objects\n    if silk_request:\n        query_set = manager.filter(request=silk_request)\n    else:\n        query_set = manager.all()\n    if (not order_by):\n        order_by = self.defualt_order_by\n    if (order_by == 'Recent'):\n        query_set = query_set.order_by('-start_time')\n    elif (order_by == 'Name'):\n        query_set = query_set.order_by('-name')\n    elif (order_by == 'Function Name'):\n        query_set = query_set.order_by('-func_name')\n    elif (order_by == 'Num. Queries'):\n        query_set = query_set.annotate(num_queries=Count('queries')).order_by('-num_queries')\n    elif (order_by == 'Time'):\n        query_set = query_set.order_by('-time_taken')\n    elif (order_by == 'Time on queries'):\n        query_set = query_set.annotate(db_time=Sum('queries__time_taken')).order_by('-db_time')\n    elif order_by:\n        raise RuntimeError(('Unknown order_by: \"%s\"' % order_by))\n    if func_name:\n        query_set = query_set.filter(func_name=func_name)\n    if name:\n        query_set = query_set.filter(name=name)\n    for f in filters:\n        query_set = f.contribute_to_query_set(query_set)\n        query_set = query_set.filter(f)\n    return list(query_set[:show])\n", "label": 1}
{"function": "\n\ndef _doGhost(self, irc, nick=None):\n    if self.disabled(irc):\n        return\n    if (nick is None):\n        nick = self._getNick(irc.network)\n    if (nick not in self.registryValue('nicks')):\n        return\n    nickserv = self.registryValue('NickServ')\n    password = self._getNickServPassword(nick)\n    ghostDelay = self.registryValue('ghostDelay')\n    if (not ghostDelay):\n        return\n    if ((not nickserv) or (not password)):\n        s = 'Tried to ghost without a NickServ or password set.'\n        self.log.warning(s)\n        return\n    if (self.sentGhost and (time.time() < (self.sentGhost + ghostDelay))):\n        self.log.warning(('Refusing to send GHOST more than once every %s seconds.' % ghostDelay))\n    elif (not password):\n        self.log.warning('Not ghosting: no password set.')\n        return\n    else:\n        self.log.info('Sending ghost (current nick: %s; ghosting: %s)', irc.nick, nick)\n        ghost = ('GHOST %s %s' % (nick, password))\n        irc.sendMsg(ircmsgs.privmsg(nickserv, ghost))\n        self.sentGhost = time.time()\n", "label": 1}
{"function": "\n\ndef _infer_freq(dates):\n    maybe_freqstr = getattr(dates, 'freqstr', None)\n    if (maybe_freqstr is not None):\n        return maybe_freqstr\n    try:\n        from pandas.tseries.api import infer_freq\n        freq = infer_freq(dates)\n        return freq\n    except ImportError:\n        pass\n    timedelta = datetime.timedelta\n    nobs = min(len(dates), 6)\n    if (nobs == 1):\n        raise ValueError('Cannot infer frequency from one date')\n    if hasattr(dates, 'values'):\n        dates = dates.values\n    diff = np.diff(dates[:nobs])\n    delta = _add_datetimes(diff)\n    nobs -= 1\n    if (delta == timedelta(nobs)):\n        return 'D'\n    elif (delta == timedelta((nobs + 2))):\n        return 'B'\n    elif (delta == timedelta((7 * nobs))):\n        return 'W'\n    elif ((delta >= timedelta((28 * nobs))) and (delta <= timedelta((31 * nobs)))):\n        return 'M'\n    elif ((delta >= timedelta((90 * nobs))) and (delta <= timedelta((92 * nobs)))):\n        return 'Q'\n    elif ((delta >= timedelta((365 * nobs))) and (delta <= timedelta((366 * nobs)))):\n        return 'A'\n    else:\n        return\n", "label": 1}
{"function": "\n\ndef get_driver_info(self):\n    ' Return list of driver info dictionaries. '\n    top = self._cfg_map.keys()[0].parent\n    while top.parent:\n        top = top.parent\n    prefix_drop = 0\n    driver_info = []\n    for (driver, (ins, outs)) in sorted(self._cfg_map.items(), key=(lambda item: item[0].get_pathname())):\n        name = driver.get_pathname()[prefix_drop:]\n        info = dict(name=name, _id=id(driver), recording=(ins + outs))\n        if hasattr(driver, 'get_parameters'):\n            info['parameters'] = [str(param) for param in driver.get_parameters().values()]\n        if hasattr(driver, 'eval_objectives'):\n            info['objectives'] = [key for key in driver.get_objectives()]\n        if hasattr(driver, 'eval_responses'):\n            info['responses'] = [key for key in driver.get_responses()]\n        if hasattr(driver, 'get_eq_constraints'):\n            info['eq_constraints'] = [str(con) for con in driver.get_eq_constraints().values()]\n        if hasattr(driver, 'get_ineq_constraints'):\n            info['ineq_constraints'] = [str(con) for con in driver.get_ineq_constraints().values()]\n        driver_info.append(info)\n    return driver_info\n", "label": 1}
{"function": "\n\ndef cli():\n    args = arg_setup()\n    ez = EZMomi(**vars(args))\n    kwargs = vars(args)\n    if (kwargs['mode'] == 'list'):\n        ez.list_objects()\n    elif (kwargs['mode'] == 'clone'):\n        ez.clone()\n    elif (kwargs['mode'] == 'destroy'):\n        ez.destroy()\n    elif (kwargs['mode'] == 'listSnapshots'):\n        ez.listSnapshots()\n    elif (kwargs['mode'] == 'createSnapshot'):\n        ez.createSnapshot()\n    elif (kwargs['mode'] == 'removeSnapshot'):\n        ez.removeSnapshot()\n    elif (kwargs['mode'] == 'revertSnapshot'):\n        ez.revertSnapshot()\n    elif (kwargs['mode'] == 'status'):\n        ez.status()\n    elif (kwargs['mode'] == 'shutdown'):\n        ez.shutdown()\n    elif (kwargs['mode'] == 'powerOff'):\n        ez.powerOff()\n    elif (kwargs['mode'] == 'powerOn'):\n        ez.powerOn()\n", "label": 1}
{"function": "\n\ndef _scan_file(filename, sentinel, source_type='import'):\n    'Generator that performs the actual scanning of files.\\n\\n    Yeilds a tuple containing import type, import path, and an extra file\\n    that should be scanned. Extra file scans should be the file or directory\\n    that relates to the import name.\\n    '\n    filename = os.path.abspath(filename)\n    real_filename = os.path.realpath(filename)\n    if (os.path.getsize(filename) <= max_file_size):\n        if ((real_filename not in sentinel) and os.path.isfile(filename)):\n            sentinel.add(real_filename)\n            basename = os.path.basename(filename)\n            (scope, imports) = ast_scan_file(filename)\n            if ((scope is not None) and (imports is not None)):\n                for imp in imports:\n                    (yield (source_type, imp.module, None))\n                if (('INSTALLED_APPS' in scope) and (basename == 'settings.py')):\n                    log.info('Found Django settings: %s', filename)\n                    for item in django.handle_django_settings(filename):\n                        (yield item)\n            else:\n                log.warn('Could not scan imports from: %s', filename)\n    else:\n        log.warn('File size too large: %s', filename)\n", "label": 1}
{"function": "\n\n@property\ndef crop_thumb(self):\n    from cropduster.models import Thumb\n    try:\n        pil_img = PIL.Image.open(self.file_path)\n    except:\n        return None\n    (orig_w, orig_h) = pil_img.size\n    dimensions = self.get('Regions', {\n        \n    }).get('AppliedToDimensions', None)\n    if (not isinstance(dimensions, dict)):\n        return None\n    (w, h) = (dimensions.get('w'), dimensions.get('h'))\n    if (not all(map((lambda v: isinstance(v, int)), [w, h]))):\n        return None\n    region_list = self.get('Regions', {\n        \n    }).get('RegionList', [])\n    if (not isinstance(region_list, list)):\n        return None\n    try:\n        crop_region = [r for r in region_list if (r['Name'] == 'Crop')][0]\n    except IndexError:\n        return None\n    if ((not isinstance(crop_region, dict)) or (not isinstance(crop_region.get('Area'), dict))):\n        return None\n    area = crop_region.get('Area')\n    if (not all([isinstance(v, float) for (k, v) in area.items() if (k in ('w', 'h', 'x', 'y'))])):\n        return None\n    return Thumb(name='crop', crop_x=(area['x'] * w), crop_y=(area['y'] * h), crop_w=(area['w'] * w), crop_h=(area['h'] * h), width=orig_w, height=orig_h)\n", "label": 1}
{"function": "\n\ndef set_cookie(self, name, value, domain=None, expires=None, path='/', expires_days=None, **kwargs):\n    'Sets the given cookie name/value with the given options.\\n\\n        Additional keyword arguments are set on the Cookie.Morsel\\n        directly.\\n        See http://docs.python.org/library/cookie.html#morsel-objects\\n        for available attributes.\\n        '\n    name = escape.native_str(name)\n    value = escape.native_str(value)\n    if re.search('[\\\\x00-\\\\x20]', (name + value)):\n        raise ValueError(('Invalid cookie %r: %r' % (name, value)))\n    if (not hasattr(self, '_new_cookie')):\n        self._new_cookie = Cookie.SimpleCookie()\n    if (name in self._new_cookie):\n        del self._new_cookie[name]\n    self._new_cookie[name] = value\n    morsel = self._new_cookie[name]\n    if domain:\n        morsel['domain'] = domain\n    if ((expires_days is not None) and (not expires)):\n        expires = (datetime.datetime.utcnow() + datetime.timedelta(days=expires_days))\n    if expires:\n        morsel['expires'] = httputil.format_timestamp(expires)\n    if path:\n        morsel['path'] = path\n    for (k, v) in kwargs.items():\n        if (k == 'max_age'):\n            k = 'max-age'\n        morsel[k] = v\n", "label": 1}
{"function": "\n\ndef get_branches(local=True, remote_branches=True):\n    'Returns a list of local and remote branches.'\n    repo_check()\n    branches = []\n    if remote_branches:\n        try:\n            for b in remote.refs:\n                name = '/'.join(b.name.split('/')[1:])\n                if (name not in settings.forbidden_branches):\n                    branches.append(Branch(name, is_published=True))\n        except (IndexError, AssertionError):\n            pass\n    if local:\n        for b in [h.name for h in repo.heads]:\n            if ((b not in [br.name for br in branches]) or (not remote_branches)):\n                if (b not in settings.forbidden_branches):\n                    branches.append(Branch(b, is_published=False))\n    return sorted(branches, key=attrgetter('name'))\n", "label": 1}
{"function": "\n\ndef clean_target_dict(data):\n    \"\\n    There are many unimplemented fields throughout parselmouth,\\n    for example some types of targeting are not yet implemented.\\n    In these cases, we must clean the dictionary before we pass\\n    it back to DFP. Fields like X.Type exist in many places, and these\\n    must be replaced with 'xsi_type' for exaample. Also, unneeded fields\\n    sometimes raise errors when passed back to DFP.\\n\\n    @param data: dict\\n    @return: dict\\n    \"\n    if (not isinstance(data, dict)):\n        return data\n    at_bottom = True\n    cln_dict = {\n        \n    }\n    for (_key, _val) in data.items():\n        if isinstance(_val, dict):\n            at_bottom = False\n            cln_dict[_key] = clean_target_dict(_val)\n        elif isinstance(_val, list):\n            at_bottom = False\n            cln_dict[_key] = [clean_target_dict(d) for d in _val]\n        elif (_key.find('_Type') >= 0):\n            cln_dict['xsi_type'] = _val\n        else:\n            cln_dict[_key] = _val\n    if at_bottom:\n        clean_dict = {\n            \n        }\n        for (_key, _val) in cln_dict.items():\n            if ((_key.find('id') >= 0) or (_key.find('Id') >= 0)):\n                clean_dict[_key] = _val\n            elif (_key == 'xsi_type'):\n                clean_dict[_key] = _val\n    else:\n        clean_dict = cln_dict\n    return clean_dict\n", "label": 1}
{"function": "\n\ndef finish_task(dsk, key, state, results, sortkey, delete=True, release_data=release_data):\n    '\\n    Update execution state after a task finishes\\n\\n    Mutates.  This should run atomically (with a lock).\\n    '\n    for dep in sorted(state['dependents'][key], key=sortkey, reverse=True):\n        s = state['waiting'][dep]\n        s.remove(key)\n        if (not s):\n            del state['waiting'][dep]\n            state['ready'].append(dep)\n    for dep in state['dependencies'][key]:\n        if (dep in state['waiting_data']):\n            s = state['waiting_data'][dep]\n            s.remove(key)\n            if ((not s) and (dep not in results)):\n                if DEBUG:\n                    from chest.core import nbytes\n                    print(('Key: %s\\tDep: %s\\t NBytes: %.2f\\t Release' % (key, dep, sum((map(nbytes, state['cache'].values()) / 1000000.0)))))\n                release_data(dep, state, delete=delete)\n        elif (delete and (dep not in results)):\n            release_data(dep, state, delete=delete)\n    state['finished'].add(key)\n    state['running'].remove(key)\n    return state\n", "label": 1}
{"function": "\n\ndef get_db_prep_lookup(self, lookup_type, value, connection, prepared=False):\n    if (not prepared):\n        value = self.get_prep_lookup(lookup_type, value)\n    if hasattr(value, 'get_compiler'):\n        value = value.get_compiler(connection=connection)\n    if (hasattr(value, 'as_sql') or hasattr(value, '_as_sql')):\n        if hasattr(value, 'relabel_aliases'):\n            return value\n        if hasattr(value, 'as_sql'):\n            (sql, params) = value.as_sql()\n        else:\n            (sql, params) = value._as_sql(connection=connection)\n        return QueryWrapper(('(%s)' % sql), params)\n    if (lookup_type in ['exact', 'gt', 'lt', 'gte', 'lte']):\n        return [self._pk_trace(value, 'get_db_prep_lookup', lookup_type, connection=connection, prepared=prepared)]\n    if (lookup_type in ('range', 'in')):\n        return [self._pk_trace(v, 'get_db_prep_lookup', lookup_type, connection=connection, prepared=prepared) for v in value]\n    elif (lookup_type == 'isnull'):\n        return []\n    raise TypeError(('Related Field has invalid lookup: %s' % lookup_type))\n", "label": 1}
{"function": "\n\n@dispatch(Slice, (Select, Selectable, ColumnElement))\ndef compute_up(expr, data, **kwargs):\n    index = expr.index[0]\n    if isinstance(index, slice):\n        start = (index.start or 0)\n        if (start < 0):\n            raise ValueError('start value of slice cannot be negative with a SQL backend')\n        stop = index.stop\n        if ((stop is not None) and (stop < 0)):\n            raise ValueError('stop value of slice cannot be negative with a SQL backend.')\n        if ((index.step is not None) and (index.step != 1)):\n            raise ValueError('step parameter in slice objects not supported with SQL backend')\n    elif isinstance(index, (np.integer, numbers.Integral)):\n        if (index < 0):\n            raise ValueError('integer slice cannot be negative for the SQL backend')\n        start = index\n        stop = (start + 1)\n    else:\n        raise TypeError(('type %r not supported for slicing wih SQL backend' % type(index).__name__))\n    warnings.warn('The order of the result set from a Slice expression computed against the SQL backend is not deterministic.')\n    if (stop is None):\n        return select(data).offset(start)\n    else:\n        return select(data).offset(start).limit((stop - start))\n", "label": 1}
{"function": "\n\ndef test_basic_support(self):\n    c = SecureCookie(secret_key=b'foo')\n    assert c.new\n    assert (not c.modified)\n    assert (not c.should_save)\n    c['x'] = 42\n    assert c.modified\n    assert c.should_save\n    s = c.serialize()\n    c2 = SecureCookie.unserialize(s, b'foo')\n    assert (c is not c2)\n    assert (not c2.new)\n    assert (not c2.modified)\n    assert (not c2.should_save)\n    self.assert_equal(c2, c)\n    c3 = SecureCookie.unserialize(s, b'wrong foo')\n    assert (not c3.modified)\n    assert (not c3.new)\n    self.assert_equal(c3, {\n        \n    })\n", "label": 1}
{"function": "\n\ndef _send_event(self, atom):\n    headers = {\n        'Content-Type': 'application/atom+xml',\n    }\n    headers = self._get_auth(headers=headers)\n    attempts = 0\n    status = 0\n    while True:\n        try:\n            res = requests.post(self.url, data=atom, headers=headers, timeout=self.http_timeout)\n            status = res.status_code\n            if ((status >= 200) and (status < 300)):\n                break\n            if (status == 401):\n                logger.info('Auth expired, reauthorizing...')\n                headers = self._get_auth(headers=headers, force=True)\n                continue\n            if (status == 409):\n                logger.debug(('Duplicate message: \\n%s' % atom))\n                break\n            if (status == 400):\n                logger.error(('Invalid Content: Server rejected content: \\n%s' % atom))\n                break\n        except requests.exceptions.ConnectionError:\n            logger.exception(('Connection error talking to %s' % self.url))\n        except requests.exceptions.Timeout:\n            logger.exception(('HTTP timeout talking to %s' % self.url))\n        except requests.exceptions.HTTPError:\n            logger.exception(('HTTP protocol error talking to %s' % self.url))\n        except requests.exceptions.RequestException:\n            logger.exception(('Unknown exeption talking to %s' % self.url))\n        attempts += 1\n        wait = min((attempts * self.wait_interval), self.max_wait)\n        logger.error(('Message delivery failed, going to sleep, will try again in %s seconds' % str(wait)))\n        time.sleep(wait)\n    return status\n", "label": 1}
{"function": "\n\ndef fill(self, char=' ', fgcolor=None, bgcolor=None, region=None):\n    (x, y, width, height) = self.getregion(region)\n    if ((x, y, width, height) == (None, None, None, None)):\n        return\n    fgcolor = (((fgcolor is not None) and getpygamecolor(fgcolor)) or self._fgcolor)\n    bgcolor = (((bgcolor is not None) and getpygamecolor(bgcolor)) or self._bgcolor)\n    for ix in range(x, (x + width)):\n        for iy in range(y, (y + height)):\n            if (char is not None):\n                self._screenchar[ix][iy] = char\n            if (fgcolor is not None):\n                self._screenfgcolor[ix][iy] = fgcolor\n            if (bgcolor is not None):\n                self._screenbgcolor[ix][iy] = bgcolor\n            self._screendirty[ix][iy] = True\n    if self._autoupdate:\n        self.update()\n", "label": 1}
{"function": "\n\ndef getUpperLeftY(self, height, y, yPrev, yNext, yMin, yMax, yMid, yMouse):\n    if (AnnotationLocation.AT_THE_MOUSE == self.location):\n        if (GChartConsts.NAI == yMouse):\n            result = Double.NaN\n        else:\n            result = yMouse\n    elif (AnnotationLocation.AT_THE_MOUSE_SNAP_TO_X == self.location):\n        if (GChartConsts.NAI == yMouse):\n            result = Double.NaN\n        else:\n            result = yMouse\n    elif (AnnotationLocation.AT_THE_MOUSE_SNAP_TO_Y == self.location):\n        if (GChartConsts.NAI == yMouse):\n            result = Double.NaN\n        else:\n            result = y\n    elif ((AnnotationLocation.NORTHWEST == self.location) or (AnnotationLocation.NORTH == self.location) or (AnnotationLocation.NORTHEAST == self.location)):\n        result = yMin\n    elif ((AnnotationLocation.SOUTHWEST == self.location) or (AnnotationLocation.SOUTH == self.location) or (AnnotationLocation.SOUTHEAST == self.location)):\n        result = yMax\n    else:\n        result = ((yMin + yMax) / 2)\n    return result\n", "label": 1}
{"function": "\n\ndef updateFiles(dirs, map_old2new, counter, suffixes, regex_restrict=None, dry_run=False):\n    'iterate through all files in dirs and\\n    replace patterns is map_old2new'\n    if regex_restrict:\n        rx = re.compile(regex_restrict)\n    else:\n        rx = None\n    for d in dirs:\n        for (root, dirs, files) in os.walk(d):\n            for f in files:\n                (_, ext) = os.path.splitext(f)\n                if (rx and (not rx.search(f))):\n                    continue\n                if (ext not in suffixes):\n                    continue\n                counter.files_examined += 1\n                fn = os.path.join(root, f)\n                with IOTools.openFile(fn, 'r') as inf:\n                    old_data = inf.read()\n                changed = False\n                for (old_name, new_name) in map_old2new.items():\n                    old_name += '([\\'`\\\\s\"=])'\n                    new_name += '\\\\1'\n                    new_data = re.sub(old_name, new_name, old_data)\n                    if (old_data != new_data):\n                        changed = True\n                        E.info(('changed: %s : %s to %s' % (fn, old_name, new_name)))\n                    old_data = new_data\n                if changed:\n                    counter.files_changed += 1\n                    if (not dry_run):\n                        with IOTools.openFile(fn, 'w') as outf:\n                            outf.write(new_data)\n", "label": 1}
{"function": "\n\ndef select(self, ensemble, x):\n    if ensemble.in_agreement(x):\n        return (Ensemble([ensemble.classifiers[0]]), None)\n    classifiers = ensemble.classifiers\n    [idx] = self.knn.kneighbors(x, return_distance=False)\n    (X, y) = (self.Xval[idx], self.yval[idx])\n    d = {\n        \n    }\n    scores = [clf.score(X, y) for clf in ensemble.classifiers]\n    for (i, scr) in enumerate(scores):\n        d[scr] = ((d[scr] + [i]) if (scr in d) else [i])\n    best_scores = sorted([k for k in d.iterkeys()], reverse=True)\n    if (len(d[best_scores[0]]) == 1):\n        i = d[best_scores[0]][0]\n        return (Ensemble([classifiers[i]]), None)\n    options = None\n    for (j, score) in enumerate(best_scores):\n        pred = [classifiers[i].predict(x) for i in d[score]]\n        pred = np.asarray(pred).flatten()\n        bincount = np.bincount(pred)\n        if (options != None):\n            for i in range(len(bincount)):\n                bincount[i] = (bincount[i] if (i in options) else 0)\n        imx = np.argmax(bincount)\n        votes = np.argwhere((bincount == bincount[imx])).flatten()\n        count = len(votes)\n        if (count == 1):\n            return (Ensemble([classifiers[np.argmax((pred == imx))]]), None)\n        elif (options == None):\n            options = votes\n    return (Ensemble([classifiers[np.argmax(scores)]]), None)\n", "label": 1}
{"function": "\n\ndef print_dict(dct, dict_property='Property', wrap=0, dict_value='Value', json_flag=False):\n    'Print a `dict` as a table of two columns.\\n\\n    :param dct: `dict` to print\\n    :param dict_property: name of the first column\\n    :param wrap: wrapping for the second column\\n    :param dict_value: header label for the value (second) column\\n    :param json_flag: print `dict` as JSON instead of table\\n    '\n    if json_flag:\n        print(json.dumps(dct, indent=4, separators=(',', ': ')))\n        return\n    pt = prettytable.PrettyTable([dict_property, dict_value])\n    pt.align = 'l'\n    for (k, v) in sorted(dct.items()):\n        if isinstance(v, dict):\n            v = six.text_type(v)\n        if (wrap > 0):\n            v = textwrap.fill(six.text_type(v), wrap)\n        if (v and isinstance(v, six.string_types) and ('\\\\n' in v)):\n            lines = v.strip().split('\\\\n')\n            col1 = k\n            for line in lines:\n                pt.add_row([col1, line])\n                col1 = ''\n        else:\n            pt.add_row([k, v])\n    if six.PY3:\n        print(encodeutils.safe_encode(pt.get_string()).decode())\n    else:\n        print(encodeutils.safe_encode(pt.get_string()))\n", "label": 1}
{"function": "\n\ndef putchar(self, char, x=None, y=None, fgcolor=None, bgcolor=None):\n    '\\n        Print a single character to the coordinates on the surface. This function does not move the cursor.\\n        '\n    if (type(char) not in (str, unicode)):\n        raise Exception(('Argument 1 must be str or unicode, not %s' % type(char)))\n    if (char == ''):\n        return\n    if (x is None):\n        x = self._cursorx\n    if (y is None):\n        y = self._cursory\n    if ((x < 0) or (y < 0) or (x >= self._width) or (y >= self._height)):\n        return None\n    if (fgcolor is not None):\n        self._screenfgcolor[x][y] = getpygamecolor(fgcolor)\n    if (bgcolor is not None):\n        self._screenbgcolor[x][y] = getpygamecolor(bgcolor)\n    self._screenchar[x][y] = char[0]\n    self._screendirty[x][y] = True\n    if self._autoupdate:\n        self.update()\n    return char\n", "label": 1}
{"function": "\n\ndef analyse_action(func):\n    'Analyse a function.'\n    description = (inspect.getdoc(func) or 'undocumented action')\n    arguments = []\n    (args, varargs, kwargs, defaults) = inspect.getargspec(func)\n    if (varargs or kwargs):\n        raise TypeError('variable length arguments for action not allowed.')\n    if (len(args) != len((defaults or ()))):\n        raise TypeError('not all arguments have proper definitions')\n    for (idx, (arg, definition)) in enumerate(zip(args, (defaults or ()))):\n        if arg.startswith('_'):\n            raise TypeError('arguments may not start with an underscore')\n        if (not isinstance(definition, tuple)):\n            shortcut = None\n            default = definition\n        else:\n            (shortcut, default) = definition\n        argument_type = argument_types[type(default)]\n        if (isinstance(default, bool) and (default is True)):\n            arg = ('no-' + arg)\n        arguments.append((arg.replace('_', '-'), shortcut, default, argument_type))\n    return (func, description, arguments)\n", "label": 1}
{"function": "\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    GITS = ['git']\n    if (sys.platform == 'win32'):\n        GITS = ['git.cmd', 'git.exe']\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if (me.endswith('.pyc') or me.endswith('.pyo')):\n            me = (os.path.splitext(me)[0] + '.py')\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = 'versioneer.py'\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open('.gitattributes', 'r')\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ('export-subst' in line.strip().split()[1:]):\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if (not present):\n        f = open('.gitattributes', 'a+')\n        f.write(('%s export-subst\\n' % versionfile_source))\n        f.close()\n        files.append('.gitattributes')\n    run_command(GITS, (['add', '--'] + files))\n", "label": 1}
{"function": "\n\ndef angle(self, x, y):\n    result = Double.NaN\n    if (x == 0):\n        if (y > 0):\n            result = (math.pi / 2.0)\n        elif (y < 0):\n            result = ((3 * math.pi) / 2.0)\n    elif ((x > 0) and (y >= 0)):\n        result = math.atan((y / x))\n    elif ((x < 0) and (y >= 0)):\n        result = (math.pi - math.atan(((- y) / x)))\n    elif ((x < 0) and (y < 0)):\n        result = (math.pi + math.atan((y / x)))\n    elif ((x > 0) and (y < 0)):\n        result = ((2 * math.pi) - math.atan(((- y) / x)))\n    return result\n", "label": 1}
{"function": "\n\ndef build(self, input_shape):\n    input_dim = input_shape[1]\n    self.input_spec = [InputSpec(dtype=K.floatx(), shape=(None, input_dim))]\n    self.W = self.init((self.nb_feature, input_dim, self.output_dim), name='{}_W'.format(self.name))\n    if self.bias:\n        self.b = K.zeros((self.nb_feature, self.output_dim), name='{}_b'.format(self.name))\n        self.trainable_weights = [self.W, self.b]\n    else:\n        self.trainable_weights = [self.W]\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if (self.bias and self.b_regularizer):\n        self.b_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    if self.activity_regularizer:\n        self.activity_regularizer.set_layer(self)\n        self.regularizers.append(self.activity_regularizer)\n    self.constraints = {\n        \n    }\n    if self.W_constraint:\n        self.constraints[self.W] = self.W_constraint\n    if (self.bias and self.b_constraint):\n        self.constraints[self.b] = self.b_constraint\n    if (self.initial_weights is not None):\n        self.set_weights(self.initial_weights)\n        del self.initial_weights\n", "label": 1}
{"function": "\n\ndef get_extended_cost_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances')\n    extended_cost_matrix = [[0 for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n    project_id = filter_properties['project_id']\n    host_racks_map = solver_utils.get_host_racks_map(hosts)\n    affinity_racks = set([])\n    affinity_hosts = set([])\n    for i in xrange(num_hosts):\n        host_name = hosts[i].host\n        host_racks = host_racks_map.get(host_name, set([]))\n        if (project_id in hosts[i].projects):\n            affinity_hosts.add(host_name)\n            affinity_racks = affinity_racks.union(host_racks)\n    for i in xrange(num_hosts):\n        host_name = hosts[i].host\n        host_racks = host_racks_map.get(host_name, set([]))\n        if ((not any([(rack in affinity_racks) for rack in host_racks])) and (host_name not in affinity_hosts)):\n            extended_cost_matrix[i] = [1 for j in xrange((num_instances + 1))]\n        else:\n            LOG.debug(_('%(host)s is in tenant affinity rack.'), {\n                'host': host_name,\n            })\n    return extended_cost_matrix\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_content(cls, abspath, start=None, end=None):\n    'Retrieve the content of the requested resource which is located\\n        at the given absolute path.\\n\\n        This class method may be overridden by subclasses.  Note that its\\n        signature is different from other overridable class methods\\n        (no ``settings`` argument); this is deliberate to ensure that\\n        ``abspath`` is able to stand on its own as a cache key.\\n\\n        This method should either return a byte string or an iterator\\n        of byte strings.  The latter is preferred for large files\\n        as it helps reduce memory fragmentation.\\n\\n        .. versionadded:: 3.1\\n        '\n    with open(abspath, 'rb') as file:\n        if (start is not None):\n            file.seek(start)\n        if (end is not None):\n            remaining = (end - (start or 0))\n        else:\n            remaining = None\n        while True:\n            chunk_size = (64 * 1024)\n            if ((remaining is not None) and (remaining < chunk_size)):\n                chunk_size = remaining\n            chunk = file.read(chunk_size)\n            if chunk:\n                if (remaining is not None):\n                    remaining -= len(chunk)\n                (yield chunk)\n            else:\n                if (remaining is not None):\n                    assert (remaining == 0)\n                return\n", "label": 1}
{"function": "\n\ndef obj_load_attr(self, attrname):\n    if (attrname not in INSTANCE_OPTIONAL_ATTRS):\n        raise exception.ObjectActionError(action='obj_load_attr', reason=('attribute %s not lazy-loadable' % attrname))\n    if (not self._context):\n        raise exception.OrphanedObjectError(method='obj_load_attr', objtype=self.obj_name())\n    LOG.debug(\"Lazy-loading '%(attr)s' on %(name)s uuid %(uuid)s\", {\n        'attr': attrname,\n        'name': self.obj_name(),\n        'uuid': self.uuid,\n    })\n    if (attrname == 'fault'):\n        self._load_fault()\n    elif (attrname == 'numa_topology'):\n        self._load_numa_topology()\n    elif (attrname == 'pci_requests'):\n        self._load_pci_requests()\n    elif (attrname == 'vcpu_model'):\n        self._load_vcpu_model()\n    elif (attrname == 'ec2_ids'):\n        self._load_ec2_ids()\n    elif (attrname == 'migration_context'):\n        self._load_migration_context()\n    elif (attrname == 'security_groups'):\n        self._load_security_groups()\n    elif (attrname == 'pci_devices'):\n        self._load_pci_devices()\n    elif ('flavor' in attrname):\n        self._load_flavor()\n    elif ((attrname == 'services') and self.deleted):\n        self.services = objects.ServiceList(self._context)\n    else:\n        self._load_generic(attrname)\n    self.obj_reset_changes([attrname])\n", "label": 1}
{"function": "\n\ndef handle_starttag(self, tag, attrs):\n    if (tag == 'a'):\n        href = [v for (k, v) in attrs if (k == 'href')]\n        if href:\n            self.in_a = True\n            self.url = href[0]\n    elif ((tag == 'img') and self.in_a):\n        src = [v for (k, v) in attrs if (k == 'src')]\n        if src:\n            self.text += (' [image:%s] ' % src[0])\n", "label": 1}
{"function": "\n\ndef test_func_calls(self):\n    assert (py2js('foo()') == 'foo();')\n    assert (py2js('foo(3, 4)') == 'foo(3, 4);')\n    assert (py2js('foo(3, 4+1)') == 'foo(3, 4 + 1);')\n    assert py2js('foo(3, *args)')\n    assert py2js('a.foo(3, *args)')\n    raises(JSError, py2js, 'foo(x=1, y=2)')\n    raises(JSError, py2js, 'foo(**kwargs)')\n    code = \"def foo(x): return x + 1\\nd = {'foo':foo}\\n\"\n    assert (evalpy((code + 'foo(3)')) == '4')\n    assert (evalpy((code + 'd.foo(3)')) == '4')\n    code = \"def foo(x, *xx): return x + sum(xx)\\nd = {'foo':foo}\\nfive=[2, 3]\\n\"\n    assert (evalpy((code + 'foo(1, 2, 3)')) == '6')\n    assert (evalpy((code + 'd.foo(1, 2, 3)')) == '6')\n    assert (evalpy((code + 'foo(1, *five)')) == '6')\n    assert (evalpy((code + 'd.foo(1, *five)')) == '6')\n", "label": 1}
{"function": "\n\ndef skip_column(self, column):\n    '\\n        Whether or not to skip column in the generation process.\\n\\n        :param column_property: SQLAlchemy Column object\\n        '\n    if ((not self.meta.include_foreign_keys) and column.foreign_keys):\n        return True\n    if ((not self.meta.include_primary_keys) and column.primary_key):\n        return True\n    if ((not self.meta.include_datetimes_with_default) and isinstance(column.type, sa.types.DateTime) and column.default):\n        return True\n    if isinstance(column.type, types.TSVectorType):\n        return True\n    if (self.meta.only_indexed_fields and (not self.has_index(column))):\n        return True\n    if (not isinstance(column, sa.Column)):\n        return True\n    return False\n", "label": 1}
{"function": "\n\n@register.simple_tag\ndef pygments(filename):\n    fqfn = os.path.abspath(os.path.join(settings.PROJECT_DIR, filename))\n    with open(fqfn, 'r') as f:\n        readlines = f.readlines()\n    startfrom = 0\n    prevline = True\n    content = []\n    for (lno, line) in enumerate(readlines):\n        if ('start tutorial' in line):\n            startfrom = (lno + 1)\n        if ('end tutorial' in line):\n            break\n        if (bool(line) and (not line.isspace())):\n            content.append(line)\n            prevline = True\n        else:\n            if prevline:\n                content.append(line)\n            prevline = False\n    code = ''.join(content[startfrom:])\n    if filename.endswith('.py'):\n        lexer = PythonLexer()\n    elif filename.endswith('.html'):\n        lexer = HtmlDjangoLexer()\n    elif filename.endswith('.js'):\n        lexer = JavascriptLexer()\n    return highlight(code, lexer, HtmlFormatter())\n", "label": 1}
{"function": "\n\ndef norm_words(words):\n    if (not args.no_lowercase):\n        words = (w.lower() for w in words)\n    if (not args.punctuation):\n        words = (w.strip(string.punctuation) for w in words)\n        words = (w for w in words if w)\n    if stopset:\n        words = (w for w in words if (w.lower() not in stopset))\n    if (not isinstance(words, list)):\n        words = list(words)\n    if args.ngrams:\n        return functools.reduce(operator.add, [(words if (n == 1) else list(ngrams(words, n))) for n in args.ngrams])\n    else:\n        return words\n", "label": 1}
{"function": "\n\ndef test_pendingCallbacksClearQueueAndMoreRunsRunAdditionalQueuedOnMoreCompletions(self):\n    numExtras = len(self.extraTransmitIds)\n    self.test_extraPendingCallbackCompletionsDoNothing()\n    for _moreExtras in range((numExtras + 3)):\n        self.testTrans.forTestingCompleteAPendingIntent(SendStatus.Sent)\n    expectedCBCount = (2 * (numExtras + 3))\n    self.assertEqual(self.successCBcalls, expectedCBCount)\n    self.assertEqual((MAX_PENDING_TRANSMITS + numExtras), len(self.testTrans.intents))\n    self.assertEqual(numExtras, len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n    for count in self.extraTransmitIds:\n        self.testTrans.scheduleTransmit(None, TransmitIntent(ActorAddress(3.5), count, self.successCB, self.failureCB))\n    self.assertEqual(self.successCBcalls, expectedCBCount)\n    self.assertEqual((MAX_PENDING_TRANSMITS + (2 * numExtras)), len(self.testTrans.intents))\n    self.assertEqual((2 * numExtras), len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n    for _extras in range(numExtras):\n        self.testTrans.forTestingCompleteAPendingIntent(SendStatus.Sent)\n    expectedCBCount += numExtras\n    self.assertEqual(self.successCBcalls, expectedCBCount)\n    self.assertEqual(((MAX_PENDING_TRANSMITS + numExtras) + numExtras), len(self.testTrans.intents))\n    self.assertEqual((2 * numExtras), len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n", "label": 1}
{"function": "\n\ndef __init__(self, branch=None, upstream=None, import_branch=None, extra_branches=None, *args, **kwargs):\n    if (not extra_branches):\n        extra_branches = []\n    self._branch = branch\n    self._upstream = upstream\n    self._import_branch = import_branch\n    self._extra_branches = extra_branches\n    super(ImportUpstream, self).__init__(*args, **kwargs)\n    if self.is_detached():\n        raise ImportUpstreamError(\"In 'detached HEAD' state\")\n    if self.repo.bare:\n        raise ImportUpstreamError('Cannot perform imports in bare repos')\n    if (self.branch == 'HEAD'):\n        self._branch = str(self.repo.active_branch)\n    branches = [self.branch, self.upstream]\n    branches.extend(self.extra_branches)\n    invalid_ref = False\n    for branch in branches:\n        if (not any((head for head in self.repo.heads if (head.name == branch)))):\n            msg = \"Specified ref does not exist: '%s'\"\n            self.log.error(msg, branch)\n            invalid_ref = True\n    if invalid_ref:\n        raise ImportUpstreamError('Invalid ref')\n", "label": 1}
{"function": "\n\ndef render_plugin(self, context=None, placeholder=None, admin=False, processors=None):\n    (instance, plugin) = self.get_plugin_instance()\n    request = None\n    current_app = None\n    if context:\n        request = context.get('request', None)\n        if request:\n            current_app = getattr(request, 'current_app', None)\n        if (not current_app):\n            current_app = (context.current_app if context else None)\n    if (instance and (not (admin and (not plugin.admin_preview)))):\n        if ((not placeholder) or (not isinstance(placeholder, Placeholder))):\n            placeholder = instance.placeholder\n        placeholder_slot = placeholder.slot\n        context = PluginContext(context, instance, placeholder, current_app=current_app)\n        context = plugin.render(context, instance, placeholder_slot)\n        page = None\n        if request:\n            page = request.current_page\n        plugin.cms_plugin_instance = instance\n        context['allowed_child_classes'] = plugin.get_child_classes(placeholder_slot, page)\n        context['allowed_parent_classes'] = plugin.get_parent_classes(placeholder_slot, page)\n        if plugin.render_plugin:\n            template = plugin._get_render_template(context, instance, placeholder)\n            if (not template):\n                raise ValidationError(('plugin has no render_template: %s' % plugin.__class__))\n        else:\n            template = None\n        return render_plugin(context, instance, placeholder, template, processors, current_app)\n    else:\n        from cms.middleware.toolbar import toolbar_plugin_processor\n        if (processors and (toolbar_plugin_processor in processors)):\n            if (not placeholder):\n                placeholder = self.placeholder\n            context = PluginContext(context, self, placeholder, current_app=current_app)\n            template = None\n            return render_plugin(context, self, placeholder, template, processors, current_app)\n    return ''\n", "label": 1}
{"function": "\n\ndef iterunpack(source, field, newfields, include_original, missing):\n    it = iter(source)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    if (field in flds):\n        field_index = flds.index(field)\n    elif (isinstance(field, int) and (field < len(flds))):\n        field_index = field\n        field = flds[field_index]\n    else:\n        raise ArgumentError('field invalid: must be either field name or index')\n    outhdr = list(flds)\n    if (not include_original):\n        outhdr.remove(field)\n    if isinstance(newfields, (list, tuple)):\n        outhdr.extend(newfields)\n        nunpack = len(newfields)\n    elif isinstance(newfields, int):\n        nunpack = newfields\n        newfields = [(text_type(field) + text_type((i + 1))) for i in range(newfields)]\n        outhdr.extend(newfields)\n    elif (newfields is None):\n        nunpack = 0\n    else:\n        raise ArgumentError('newfields argument must be list or tuple of field names, or int (number of values to unpack)')\n    (yield tuple(outhdr))\n    for row in it:\n        value = row[field_index]\n        if include_original:\n            out_row = list(row)\n        else:\n            out_row = [v for (i, v) in enumerate(row) if (i != field_index)]\n        nvals = len(value)\n        if (nunpack > 0):\n            if (nvals >= nunpack):\n                newvals = value[:nunpack]\n            else:\n                newvals = (list(value) + ([missing] * (nunpack - nvals)))\n            out_row.extend(newvals)\n        (yield tuple(out_row))\n", "label": 1}
{"function": "\n\ndef move_to_section(self, which, where=None):\n    if (which in range(1, 5)):\n        sections = self.get_sections()\n        if (sections and (len(sections) >= which)):\n            section = sections[(which - 1)]\n            self.move_to_region(section)\n    elif (which in list(SECTIONS.keys())):\n        sections = self.get_sections()\n        for section in sections:\n            if (self.section_at_region(section) == which):\n                self.move_to_region(section)\n                return\n    elif (which in ('next', 'prev')):\n        point = self.get_first_point()\n        sections = self.get_sections()\n        if (point and sections):\n            next = self.next_or_prev_region(which, sections, point)\n            self.move_to_region(next)\n", "label": 1}
{"function": "\n\ndef __search(node, coll=None):\n    if (coll is None):\n        coll = set()\n    if ((node.type == 'assign') and (node[0].type == 'dot')):\n        if (node[0][1].type == 'identifier'):\n            name = node[0][1].value\n            if ((type(name) is str) and __matcher.match(name)):\n                coll.add(name)\n    elif (node.type == 'property_init'):\n        name = node[0].value\n        if ((type(name) is str) and __matcher.match(name)):\n            coll.add(name)\n    for child in node:\n        if (child != None):\n            __search(child, coll)\n    return coll\n", "label": 1}
{"function": "\n\ndef render(self):\n    translate((width / 2), (height / 2))\n    pushes = 0\n    repeats = 1\n    self.steps += 12\n    if (self.steps > len(self.production)):\n        self.steps = len(self.production)\n    for i in range(self.steps):\n        step = self.production[i]\n        if (step == 'F'):\n            stroke(255, 60)\n            for j in range(repeats):\n                line(0, 0, 0, (- self.drawLength))\n                noFill()\n                translate(0, (- self.drawLength))\n            repeats = 1\n        elif (step == '+'):\n            for j in range(repeats):\n                rotate(self.theta)\n            repeats = 1\n        elif (step == '-'):\n            for j in range(repeats):\n                rotate((- self.theta))\n            repeats = 1\n        elif (step == '['):\n            pushes += 1\n            pushMatrix()\n        elif (step == ']'):\n            popMatrix()\n            pushes -= 1\n        elif ((ord(step) >= 48) and (ord(step) <= 57)):\n            repeats = (ord(step) - 48)\n    while (pushes > 0):\n        popMatrix()\n        pushes -= 1\n", "label": 1}
{"function": "\n\ndef convert_html_entities(s):\n    import htmlentitydefs\n    matches = re.findall('&#\\\\d+;', s)\n    if (len(matches) > 0):\n        hits = set(matches)\n        for hit in hits:\n            name = hit[2:(- 1)]\n            try:\n                entnum = int(name)\n                s = s.replace(hit, unichr(entnum))\n            except ValueError:\n                pass\n    matches = re.findall('&#[xX][0-9a-fA-F]+;', s)\n    if (len(matches) > 0):\n        hits = set(matches)\n        for hit in hits:\n            hex = hit[3:(- 1)]\n            try:\n                entnum = int(hex, 16)\n                s = s.replace(hit, unichr(entnum))\n            except ValueError:\n                pass\n    matches = re.findall('&\\\\w+;', s)\n    hits = set(matches)\n    amp = '&'\n    if (amp in hits):\n        hits.remove(amp)\n    for hit in hits:\n        name = hit[1:(- 1)]\n        if htmlentitydefs.name2codepoint.has_key(name):\n            s = s.replace(hit, unichr(htmlentitydefs.name2codepoint[name]))\n    s = s.replace(amp, '&')\n    return s\n", "label": 1}
{"function": "\n\ndef updateuptime(self, server):\n    now = time.mktime(datetime.datetime.now().timetuple())\n    servercameback = time.mktime(server.timeservercameback.timetuple())\n    difference = (now - servercameback)\n    MINUTE = 60\n    HOUR = (MINUTE * 60)\n    DAY = (HOUR * 24)\n    days = int((difference / DAY))\n    hours = int(((difference % DAY) / HOUR))\n    minutes = int(((difference % HOUR) / MINUTE))\n    seconds = int((difference % MINUTE))\n    string = ''\n    if (days > 0):\n        string += (((str(days) + ' ') + (((days == 1) and 'day') or 'days')) + ', ')\n    if ((len(string) > 0) or (hours > 0)):\n        string += (((str(hours) + ' ') + (((hours == 1) and 'hour') or 'hours')) + ', ')\n    if ((len(string) > 0) or (minutes > 0)):\n        string += (((str(minutes) + ' ') + (((minutes == 1) and 'minute') or 'minutes')) + ', ')\n    string += ((str(seconds) + ' ') + (((seconds == 1) and 'second') or 'seconds'))\n    server.uptime = string\n    server.put()\n", "label": 1}
{"function": "\n\ndef test_pow(self, space):\n    w_res = space.execute('return 1.0 ** 2')\n    assert (self.unwrap(space, w_res) == 1.0)\n    w_res = space.execute('return 2.0 ** 2')\n    assert (self.unwrap(space, w_res) == 4.0)\n    w_res = space.execute('return 2.0 ** 0')\n    assert (self.unwrap(space, w_res) == 1.0)\n    w_res = space.execute('return 4.0 ** 4.0')\n    assert (self.unwrap(space, w_res) == 256.0)\n    w_res = space.execute('return 0.0 ** (-1.0)')\n    assert (self.unwrap(space, w_res) == float('inf'))\n    w_res = space.execute('return (-2.0) ** 2')\n    assert (self.unwrap(space, w_res) == 4)\n    w_res = space.execute('return (-2.0) ** 3')\n    assert (self.unwrap(space, w_res) == (- 8))\n    w_res = space.execute('return (-2.0) ** -2')\n    assert (self.unwrap(space, w_res) == 0.25)\n    w_res = space.execute('return (-2.0) ** -3')\n    assert (self.unwrap(space, w_res) == (- 0.125))\n    with self.raises(space, 'TypeError', \"String can't be coerced into Bignum\"):\n        space.execute(\"18446744073709551628 ** 'hallo'\")\n", "label": 1}
{"function": "\n\ndef prep_search_value(self, term, lookup_type):\n    if (lookup_type in ('exact', 'in', 'range')):\n        try:\n            date_obj = dateutil.parser.parse(term)\n        except ValueError:\n            pass\n        except TypeError:\n            pass\n        else:\n            return date_obj\n    if (lookup_type not in ('exact', 'in', 'range')):\n        test_term = term\n        if (lookup_type == 'week_day'):\n            try:\n                test_term = (int(test_term) - 1)\n            except:\n                return None\n            else:\n                test_term = str(test_term)\n        for test_format in STRPTIME_PLACEHOLDERS[lookup_type]:\n            try:\n                date_obj = datetime.strptime(test_term, test_format)\n            except ValueError:\n                pass\n            else:\n                if (lookup_type == 'week_day'):\n                    term = (date_obj.weekday() + 1)\n                else:\n                    term = getattr(date_obj, lookup_type)\n                return str(term)\n    return super(DateColumn, self).prep_search_value(term, lookup_type)\n", "label": 1}
{"function": "\n\ndef get_l3_agent_candidates(self, context, sync_router, l3_agents, ignore_admin_state=False):\n    \"Get the valid l3 agents for the router from a list of l3_agents.\\n\\n        It will not return agents in 'dvr' mode for a dvr router as dvr\\n        routers are not explicitly scheduled to l3 agents on compute nodes\\n        \"\n    candidates = []\n    is_router_distributed = sync_router.get('distributed', False)\n    for l3_agent in l3_agents:\n        if ((not ignore_admin_state) and (not l3_agent.admin_state_up)):\n            continue\n        agent_conf = self.get_configuration_dict(l3_agent)\n        agent_mode = agent_conf.get(n_const.L3_AGENT_MODE, n_const.L3_AGENT_MODE_LEGACY)\n        if ((agent_mode == n_const.L3_AGENT_MODE_DVR) or ((agent_mode == n_const.L3_AGENT_MODE_LEGACY) and is_router_distributed)):\n            continue\n        router_id = agent_conf.get('router_id', None)\n        if (router_id and (router_id != sync_router['id'])):\n            continue\n        handle_internal_only_routers = agent_conf.get('handle_internal_only_routers', True)\n        gateway_external_network_id = agent_conf.get('gateway_external_network_id', None)\n        ex_net_id = (sync_router['external_gateway_info'] or {\n            \n        }).get('network_id')\n        if (((not ex_net_id) and (not handle_internal_only_routers)) or (ex_net_id and gateway_external_network_id and (ex_net_id != gateway_external_network_id))):\n            continue\n        candidates.append(l3_agent)\n    return candidates\n", "label": 1}
{"function": "\n\ndef pagination(cl):\n    (paginator, page_num) = (cl.paginator, cl.page_num)\n    pagination_required = (((not cl.show_all) or (not cl.can_show_all)) and cl.multi_page)\n    if (not pagination_required):\n        page_range = []\n    else:\n        ON_EACH_SIDE = 3\n        ON_ENDS = 2\n        if (paginator.pages <= 10):\n            page_range = range(paginator.pages)\n        else:\n            page_range = []\n            if (page_num > (ON_EACH_SIDE + ON_ENDS)):\n                page_range.extend(range(0, (ON_EACH_SIDE - 1)))\n                page_range.append(DOT)\n                page_range.extend(range((page_num - ON_EACH_SIDE), (page_num + 1)))\n            else:\n                page_range.extend(range(0, (page_num + 1)))\n            if (page_num < (((paginator.pages - ON_EACH_SIDE) - ON_ENDS) - 1)):\n                page_range.extend(range((page_num + 1), ((page_num + ON_EACH_SIDE) + 1)))\n                page_range.append(DOT)\n                page_range.extend(range((paginator.pages - ON_ENDS), paginator.pages))\n            else:\n                page_range.extend(range((page_num + 1), paginator.pages))\n    need_show_all_link = (cl.can_show_all and (not cl.show_all) and cl.multi_page)\n    return {\n        'cl': cl,\n        'pagination_required': pagination_required,\n        'show_all_url': (need_show_all_link and cl.get_query_string({\n            ALL_VAR: '',\n        })),\n        'page_range': page_range,\n        'ALL_VAR': ALL_VAR,\n        '1': 1,\n    }\n", "label": 1}
{"function": "\n\ndef update_with_json(self, jsondict):\n    ' Update the receiver with data in a JSON dictionary.\\n        '\n    if (jsondict is None):\n        return\n    if (not isinstance(jsondict, dict)):\n        logging.warning('Non-dict type {} fed to `update_with_json` on {}'.format(type(jsondict), type(self)))\n        return\n    found = set(['resourceType', 'fhir_comments'])\n    nonoptionals = set()\n    for (name, jsname, typ, is_list, of_many, not_optional) in self.elementProperties():\n        if (not (jsname in jsondict)):\n            if not_optional:\n                nonoptionals.add((of_many or jsname))\n            continue\n        if hasattr(typ, 'with_json_and_owner'):\n            setattr(self, name, typ.with_json_and_owner(jsondict[jsname], self))\n        else:\n            setattr(self, name, jsondict[jsname])\n        found.add(jsname)\n        found.add(('_' + jsname))\n        if (of_many is not None):\n            found.add(of_many)\n    if (len((nonoptionals - found)) > 0):\n        for miss in (nonoptionals - found):\n            logging.warning(\"Non-optional property '{}' on {} is missing from JSON\".format(miss, self))\n    if (len((set(jsondict.keys()) - found)) > 0):\n        for supflu in (set(jsondict.keys()) - found):\n            logging.warning(\"Superfluous entry '{}' in JSON for {}\".format(supflu, self))\n", "label": 1}
{"function": "\n\ndef is_forest(edges, n_vertices=None):\n    if ((n_vertices is not None) and (len(edges) > (n_vertices - 1))):\n        return False\n    n_vertices = (np.max(edges) + 1)\n    parents = (- np.ones(n_vertices))\n    visited = np.zeros(n_vertices, dtype=np.bool)\n    neighbors = [[] for i in range(n_vertices)]\n    for edge in edges:\n        neighbors[edge[0]].append(edge[1])\n        neighbors[edge[1]].append(edge[0])\n    lonely = 0\n    while (lonely < n_vertices):\n        for i in range(lonely, n_vertices):\n            if (not visited[i]):\n                queue = [i]\n                lonely = (i + 1)\n                visited[i] = True\n                break\n            lonely = n_vertices\n        while queue:\n            node = queue.pop()\n            for neighbor in neighbors[node]:\n                if (not visited[neighbor]):\n                    parents[neighbor] = node\n                    queue.append(neighbor)\n                    visited[neighbor] = True\n                elif (not (parents[node] == neighbor)):\n                    return False\n    return True\n", "label": 1}
{"function": "\n\ndef postprocess(self):\n    '\\n        Postprocessing includes renaming and gzipping where necessary.\\n        Also copies the magmom to the incar if necessary\\n        '\n    for f in (VASP_OUTPUT_FILES + [self.output_file]):\n        if os.path.exists(f):\n            if (self.final and (self.suffix != '')):\n                shutil.move(f, '{}{}'.format(f, self.suffix))\n            elif (self.suffix != ''):\n                shutil.copy(f, '{}{}'.format(f, self.suffix))\n    if (self.copy_magmom and (not self.final)):\n        try:\n            outcar = Outcar('OUTCAR')\n            magmom = [m['tot'] for m in outcar.magnetization]\n            incar = Incar.from_file('INCAR')\n            incar['MAGMOM'] = magmom\n            incar.write_file('INCAR')\n        except:\n            logging.error('MAGMOM copy from OUTCAR to INCAR failed')\n", "label": 1}
{"function": "\n\ndef list_dir(root_dir, dirs_only=False, include_special=False):\n    '\\n    List directory.\\n    :param root_dir: string: directory to list\\n    :param dirs_only: boolean\\n    :param include_special: boolean\\n    :return: list\\n    '\n    root_dir = ('.' if (not root_dir) else root_dir)\n    res = []\n    if ('~' in root_dir):\n        root_dir = os.path.expanduser(root_dir)\n    if (not os.path.exists(root_dir)):\n        (root_dir, _) = os.path.split(root_dir)\n    if os.path.exists(root_dir):\n        for name in os.listdir(root_dir):\n            path = os.path.join(root_dir, name)\n            if ((not include_special) and name.startswith('.')):\n                continue\n            if (dirs_only and (not os.path.isdir(path))):\n                continue\n            res.append(name)\n    return res\n", "label": 1}
{"function": "\n\ndef main(numpreds=100, numdocs=2000, printp=0):\n    preds = gen_predicates(numpreds)\n    docs = gen_docs(numdocs)\n    s1 = make_set(preds)\n    s2 = make_set_optimized(preds)\n    if printp:\n        print('Predicates:')\n        for p in preds:\n            print('\\t', p.predicate)\n    start = time.time()\n    total = 0\n    for d in docs:\n        total += len(s1.evaluate(d))\n    end = time.time()\n    print(('(Naive) Evaluated %d docs across %d predicates in %0.3f seconds' % (numdocs, numpreds, (end - start))))\n    print(('(Naive) Total of %d predicates matched' % total))\n    start = time.time()\n    total_o = 0\n    for d in docs:\n        total_o += len(s2.evaluate(d))\n    end = time.time()\n    print(('(Opt) Evaluated %d docs across %d predicates in %0.3f seconds' % (numdocs, numpreds, (end - start))))\n    print(('(Opt) Total of %d predicates matched' % total_o))\n    if (total != total_o):\n        print('Mismatch! Differing inputs:')\n        for d in docs:\n            r1 = s1.evaluate(d)\n            r2 = s2.evaluate(d)\n            if (r1 != r2):\n                print('Input:', repr(d))\n                print('Naive:')\n                for p in r1:\n                    print('\\t', p.predicate)\n                print('Opt:')\n                for p in r2:\n                    print('\\t', p.predicate)\n                print()\n        sys.exit(1)\n", "label": 1}
{"function": "\n\ndef parseSite(self):\n    print(('Beginning EatManga check: %s' % self.manga))\n    url = ('http://eatmanga.com/Manga-Scan/%s' % self.fixFormatting(self.manga))\n    if self.verbose_FLAG:\n        print(url)\n    source = getSourceCode(url, self.proxy)\n    self.chapters = EatManga.re_getChapters.findall(source)\n    self.chapters.reverse()\n    if (not self.chapters):\n        raise self.MangaNotFound\n    lowerRange = 0\n    for i in range(0, len(self.chapters)):\n        if ('upcoming' in self.chapters[i][0]):\n            del self.chapters[i]\n            continue\n        self.chapters[i] = (('http://eatmanga.com%s' % self.chapters[i][0]), self.chapters[i][2], self.chapters[i][2])\n        if (not self.auto):\n            print(('(%i) %s' % ((i + 1), self.chapters[i][1])))\n        elif (self.lastDownloaded == self.chapters[i][1]):\n            lowerRange = (i + 1)\n    upperRange = len(self.chapters)\n    if (not self.auto):\n        self.chapters_to_download = self.selectChapters(self.chapters)\n    else:\n        if (lowerRange == upperRange):\n            raise self.NoUpdates\n        for i in range(lowerRange, upperRange):\n            self.chapters_to_download.append(i)\n    self.isPrependMangaName = True\n    return\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    ext = os.path.splitext(self.logname)[1]\n    if (ext == '.gz'):\n        d = zlib.decompressobj((16 + zlib.MAX_WBITS))\n    if isinstance(self.obj, types.GeneratorType):\n        buf = next(self.obj)\n        partial = ''\n        while buf:\n            if (ext == '.gz'):\n                string = (partial + d.decompress(buf))\n            else:\n                string = (partial + buf)\n            split = string.split('\\n')\n            for line in split[:(- 1)]:\n                (yield (line + '\\n'))\n            partial = split[(- 1)]\n            try:\n                buf = next(self.obj)\n            except StopIteration:\n                break\n        if (partial != ''):\n            (yield partial)\n    else:\n        output = self.obj\n        if (ext == '.gz'):\n            output = d.decompress(output)\n        split = output.split('\\n')\n        for line in split[:(- 1)]:\n            (yield (line + '\\n'))\n        partial = split[(- 1)]\n        if (partial != ''):\n            (yield partial)\n", "label": 1}
{"function": "\n\ndef get_config_paths():\n    'Return the paths to each :file:`.reviewboardrc` influencing the cwd.\\n\\n    A list of paths to :file:`.reviewboardrc` files will be returned, where\\n    each subsequent list entry should have lower precedence than the previous.\\n    i.e. configuration found in files further up the list will take precedence.\\n\\n    Configuration in the paths set in :envvar:`$RBTOOLS_CONFIG_PATH` will take\\n    precedence over files found in the current working directory or its\\n    parents.\\n    '\n    config_paths = []\n    for path in os.environ.get('RBTOOLS_CONFIG_PATH', '').split(os.pathsep):\n        if (not path):\n            continue\n        filename = os.path.realpath(os.path.join(path, CONFIG_FILE))\n        if (os.path.exists(filename) and (filename not in config_paths)):\n            config_paths.append(filename)\n    for path in walk_parents(os.getcwd()):\n        filename = os.path.realpath(os.path.join(path, CONFIG_FILE))\n        if (os.path.exists(filename) and (filename not in config_paths)):\n            config_paths.append(filename)\n    home_config_path = os.path.realpath(os.path.join(get_home_path(), CONFIG_FILE))\n    if (os.path.exists(home_config_path) and (home_config_path not in config_paths)):\n        config_paths.append(home_config_path)\n    return config_paths\n", "label": 1}
{"function": "\n\ndef diff_map(self, inlanguages):\n    'Generate SQL to transform existing languages\\n\\n        :param input_map: a YAML map defining the new languages\\n        :return: list of SQL statements\\n\\n        Compares the existing language definitions, as fetched from the\\n        catalogs, to the input map and generates SQL statements to\\n        transform the languages accordingly.\\n        '\n    stmts = []\n    for lng in inlanguages:\n        inlng = inlanguages[lng]\n        if (lng in self):\n            if (not hasattr(inlng, '_ext')):\n                stmts.append(self[lng].diff_map(inlng))\n        elif hasattr(inlng, 'oldname'):\n            oldname = inlng.oldname\n            try:\n                stmts.append(self[oldname].rename(inlng.name))\n                del self[oldname]\n            except KeyError as exc:\n                exc.args = ((\"Previous name '%s' for language '%s' not found\" % (oldname, inlng.name)),)\n                raise\n        else:\n            stmts.append(inlng.create())\n    for lng in self:\n        if (lng not in inlanguages):\n            if ((self.dbconn.version >= 90000) and (self[lng].name == 'plpgsql')):\n                continue\n            self[lng].dropped = True\n    return stmts\n", "label": 1}
{"function": "\n\ndef print_env_short_list(ctx, param, value):\n    if ((not value) or ctx.resilient_parsing):\n        return\n    nova_creds = config.run_config()\n    row = 1\n    for nova_env in nova_creds.keys():\n        executable = 'nova'\n        auth_url = ''\n        for (param, value) in sorted(nova_creds[nova_env].items()):\n            if (param.upper() == 'OS_EXECUTABLE'):\n                executable = value\n            if ((param.upper() == 'OS_AUTH_URL') and ('inova' in value)):\n                executable = 'inova'\n            if (param.upper() == 'OS_AUTH_URL'):\n                auth_url = value\n        color = 'green'\n        if ((row % 2) == 0):\n            color = 'red'\n        env = click.style(nova_env, fg=color)\n        click.echo(('%s (%s) @ %s' % (env, executable, auth_url)))\n        row += 1\n    ctx.exit()\n", "label": 1}
{"function": "\n\n@classmethod\ndef process(c, request, name=None):\n    '\\n        process uses the current request to determine which menus\\n        should be visible, which are selected, etc.\\n        '\n    c.load_menus()\n    c.sort_menus()\n    if (name is None):\n        items = {\n            \n        }\n        for name in c.items:\n            items[name] = c.process(request, name)\n        return items\n    if (name not in c.items):\n        return []\n    items = copy.deepcopy(c.items[name])\n    curitem = None\n    for item in items:\n        item.process(request)\n        if item.visible:\n            item.selected = False\n            if item.match_url(request):\n                if ((curitem is None) or (len(curitem.url) < len(item.url))):\n                    curitem = item\n    if (curitem is not None):\n        curitem.selected = True\n    visible = [item for item in items if item.visible]\n    if getattr(settings, 'MENU_SELECT_PARENTS', False):\n\n        def is_child_selected(item):\n            for child in item.children:\n                if (child.selected or is_child_selected(child)):\n                    return True\n        for item in visible:\n            if is_child_selected(item):\n                item.selected = True\n    return visible\n", "label": 1}
{"function": "\n\ndef validate(self, r):\n    '\\n        Called automatically by self.result.\\n        '\n    if self.show_invalid:\n        r.valid = True\n    elif r.valid:\n        if (not r.description):\n            r.valid = False\n        if (r.size and ((r.size + r.offset) > r.file.size)):\n            r.valid = False\n        if (r.jump and ((r.jump + r.offset) > r.file.size)):\n            r.valid = False\n    if r.valid:\n        if (r.id == self.one_of_many):\n            r.display = False\n        elif r.many:\n            self.one_of_many = r.id\n        else:\n            self.one_of_many = None\n", "label": 1}
{"function": "\n\ndef get_parler_languages_from_django_cms(cms_languages=None):\n    \"\\n    Converts django CMS' setting CMS_LANGUAGES into PARLER_LANGUAGES. Since\\n    CMS_LANGUAGES is a strict superset of PARLER_LANGUAGES, we do a bit of\\n    cleansing to remove irrelevant items.\\n    \"\n    valid_keys = ['code', 'fallbacks', 'hide_untranslated', 'redirect_on_fallback']\n    if cms_languages:\n        if (sys.version_info < (3, 0, 0)):\n            int_types = (int, long)\n        else:\n            int_types = int\n        parler_languages = copy.deepcopy(cms_languages)\n        for (site_id, site_config) in cms_languages.items():\n            if (site_id and ((not isinstance(site_id, int_types)) and (site_id != 'default'))):\n                del parler_languages[site_id]\n                continue\n            if (site_id == 'default'):\n                for (key, value) in site_config.items():\n                    if (key not in valid_keys):\n                        del parler_languages['default'][key]\n            else:\n                for (i, lang_config) in enumerate(site_config):\n                    for (key, value) in lang_config.items():\n                        if (key not in valid_keys):\n                            del parler_languages[site_id][i][key]\n        return parler_languages\n    return None\n", "label": 1}
{"function": "\n\ndef unstash_index(sync=False, branch=None):\n    'Returns an unstash index if one is available.'\n    repo_check()\n    stash_list = repo.git.execute([git, 'stash', 'list'])\n    if (branch is None):\n        branch = get_current_branch_name()\n    for stash in stash_list.splitlines():\n        verb = ('syncing' if sync else 'switching')\n        if ((('Legit' in stash) and ('On {0}:'.format(branch) in stash) and (verb in stash)) or (('GitHub' in stash) and ('On {0}:'.format(branch) in stash) and (verb in stash))):\n            return stash[7]\n", "label": 1}
{"function": "\n\ndef setup(self):\n    if (not self.square_matrix):\n        source = self.values._data[self.origin]\n        target = self.values._data[self.destination]\n        union = source.append(target).unique()\n        N = union.shape[0]\n        m = pd.DataFrame(np.zeros((N, N)), columns=union, index=union)\n        if (not self.label):\n            self.label = list(union)\n        if (self.value is None):\n            for (_, row) in self.values._data.iterrows():\n                m[row[self.origin]][row[self.destination]] += 1\n            self.matrix = m.get_values()\n        if (self.value is not None):\n            if (isinstance(self.value, int) or isinstance(self.value, float)):\n                for (_, row) in self.values._data.iterrows():\n                    m[row[self.origin]][row[self.destination]] = self.value\n                self.matrix = m.get_values()\n            elif isinstance(self.value, str):\n                for (_, row) in self.values._data.iterrows():\n                    m[row[self.origin]][row[self.destination]] = row[self.value]\n                self.matrix = m.get_values().T\n    else:\n        self.matrix = self._data.df.get_values()\n    if self.label:\n        assert (len(self.label) == self.matrix.shape[0])\n", "label": 1}
{"function": "\n\ndef _intersect(self, other):\n    from sympy.solvers.diophantine import diophantine\n    if (self.base_set is S.Integers):\n        g = None\n        if (isinstance(other, ImageSet) and (other.base_set is S.Integers)):\n            g = other.lamda.expr\n            m = other.lamda.variables[0]\n        elif (other is S.Integers):\n            m = g = Dummy('x')\n        if (g is not None):\n            f = self.lamda.expr\n            n = self.lamda.variables[0]\n            (a, b) = (Dummy('a'), Dummy('b'))\n            (f, g) = (f.subs(n, a), g.subs(m, b))\n            solns_set = diophantine((f - g))\n            if (solns_set == set()):\n                return EmptySet()\n            solns = list(diophantine((f - g)))\n            if (len(solns) != 1):\n                return\n            nsol = solns[0][0]\n            t = nsol.free_symbols.pop()\n            return imageset(Lambda(n, f.subs(a, nsol.subs(t, n))), S.Integers)\n    if (other == S.Reals):\n        from sympy.solvers.solveset import solveset_real\n        from sympy.core.function import expand_complex\n        if (len(self.lamda.variables) > 1):\n            return None\n        f = self.lamda.expr\n        n = self.lamda.variables[0]\n        n_ = Dummy(n.name, real=True)\n        f_ = f.subs(n, n_)\n        (re, im) = f_.as_real_imag()\n        im = expand_complex(im)\n        return imageset(Lambda(n_, re), self.base_set.intersect(solveset_real(im, n_)))\n", "label": 1}
{"function": "\n\ndef __init__(self, K, n, C=None, sigma_C=1, mu=None, mu_pi=None):\n    '\\n        Create a PGMultinomial distribution with mean and covariance for psi.\\n\\n        :param K:       Dimensionality of the multinomial distribution\\n        :param mu_C:    Mean of the matrix normal distribution over C\\n        '\n    assert (isinstance(K, int) and (K >= 2)), 'K must be an integer >= 2'\n    self.K = K\n    assert (isinstance(n, int) and (n >= 1)), 'n must be an integer >= 1'\n    self.n = n\n    self.sigma_C = sigma_C\n    if (C is None):\n        self.C = (self.sigma_C * np.random.randn((self.K - 1), self.n))\n    else:\n        assert (C.shape == ((self.K - 1), self.n))\n        self.C = C\n    if ((mu is None) and (mu_pi is None)):\n        self.mu = np.zeros((self.K - 1))\n    elif (mu is not None):\n        assert (mu.shape == ((self.K - 1),))\n        self.mu = mu\n    else:\n        assert (mu_pi.shape == (self.K,))\n        self.mu = pi_to_psi(mu_pi)\n    self.ppgs = initialize_polya_gamma_samplers()\n", "label": 1}
{"function": "\n\ndef create_html(tree, meta_data):\n    new_html = etree.Element('html')\n    w_namespace = get_namespace(tree, 'w')\n    visited_nodes = []\n    _strip_tag(tree, ('%ssectPr' % w_namespace))\n    for el in tree.iter():\n        if (el in visited_nodes):\n            continue\n        header_value = is_header(el, meta_data)\n        if is_header(el, meta_data):\n            p_text = get_element_content(el, meta_data)\n            if (p_text == ''):\n                continue\n            new_html.append(etree.XML(('<%s>%s</%s>' % (header_value, p_text, header_value))))\n        elif (el.tag == ('%sp' % w_namespace)):\n            if is_title(el):\n                continue\n            if is_li(el, meta_data):\n                li_nodes = get_single_list_nodes_data(el, meta_data)\n                (new_el, list_visited_nodes) = build_list(li_nodes, meta_data)\n                visited_nodes.extend(list_visited_nodes)\n            else:\n                p_text = get_element_content(el, meta_data)\n                if (p_text == ''):\n                    continue\n                new_el = etree.XML(('<p>%s</p>' % p_text))\n            new_html.append(new_el)\n        elif (el.tag == ('%stbl' % w_namespace)):\n            (table_el, table_visited_nodes) = build_table(el, meta_data)\n            visited_nodes.extend(table_visited_nodes)\n            new_html.append(table_el)\n            continue\n        visited_nodes.append(el)\n    result = etree.tostring(new_html, method='html', with_tail=True)\n    return _make_void_elements_self_close(result)\n", "label": 1}
{"function": "\n\ndef create_email_msg(self, emails, attachments=None, from_email=None, lang=None, message_class=EmailMultiRelated, headers=None):\n    'Create an email message instance.'\n    from_email = (from_email or settings.DEFAULT_FROM_EMAIL)\n    subject = self._render_part('subject.txt', lang=lang).strip()\n    try:\n        body = self._render_part('body.txt', lang=lang)\n    except TemplateDoesNotExist:\n        body = None\n    try:\n        html_content = self._render_part('body.html', lang=lang)\n    except TemplateDoesNotExist:\n        html_content = None\n    if ((html_content is None) and (body is None)):\n        raise TemplateDoesNotExist('Txt and html templates have not been found')\n    if ((html_content is not None) and (body is None)):\n        h = html2text.HTML2Text()\n        body = h.handle(html_content)\n    if (headers is None):\n        reply_to = getattr(settings, 'NO_REPLY_EMAIL', None)\n        if (not reply_to):\n            reply_to = getattr(settings, 'SUPPORT_EMAIL', settings.DEFAULT_FROM_EMAIL)\n        headers = {\n            'Reply-To': reply_to,\n        }\n    msg = message_class(subject, body, from_email, emails, headers=headers)\n    if html_content:\n        msg.attach_alternative(html_content, 'text/html')\n    attachments = self.get_attachments(attachments)\n    if attachments:\n        for (filepath, filename, mimetype) in attachments:\n            with open(filepath, 'rb') as attachment:\n                if mimetype.startswith('image'):\n                    msg.attach_related_file(filepath, mimetype, filename)\n                else:\n                    msg.attach(filename, attachment.read(), mimetype)\n    return msg\n", "label": 1}
{"function": "\n\n@ensure_tag(['p'])\ndef get_font_size(p, styles_dict):\n    w_namespace = get_namespace(p, 'w')\n    r = p.find(('%sr' % w_namespace))\n    if (r is None):\n        return None\n    rpr = r.find(('%srPr' % w_namespace))\n    if (rpr is None):\n        return None\n    size = rpr.find(('%ssz' % w_namespace))\n    if (size is None):\n        pPr = p.find(('%spPr' % w_namespace))\n        if (pPr is None):\n            return None\n        pStyle = pPr.find(('%spStyle' % w_namespace))\n        if (pStyle is None):\n            return None\n        pStyle = pStyle.get(('%sval' % w_namespace))\n        font_size = None\n        style_value = styles_dict.get(pStyle, None)\n        if (style_value is None):\n            return None\n        if ('font_size' in style_value):\n            font_size = styles_dict[pStyle]['font_size']\n        while (font_size is None):\n            old_pStyle = pStyle\n            if (pStyle not in styles_dict):\n                break\n            if ('based_on' not in styles_dict[pStyle]):\n                break\n            pStyle = styles_dict[pStyle]['based_on']\n            if (old_pStyle == pStyle):\n                break\n            if (pStyle not in styles_dict):\n                break\n            font_size = styles_dict[pStyle]['font_size']\n        return font_size\n    return size.get(('%sval' % w_namespace))\n", "label": 1}
{"function": "\n\ndef create_mult(self, res_list, actor_id=None):\n    'Creates a list of resources from objects. Objects may have _id in it to predetermine their ID.\\n        Returns a list of 2-tuples (resource_id, rev)'\n    cur_time = get_ion_ts()\n    id_list = []\n    for resobj in res_list:\n        lcsm = get_restype_lcsm(resobj.type_)\n        resobj.lcstate = (lcsm.initial_state if lcsm else LCS.DEPLOYED)\n        resobj.availability = (lcsm.initial_availability if lcsm else AS.AVAILABLE)\n        resobj.ts_created = cur_time\n        resobj.ts_updated = cur_time\n        id_list.append((resobj._id if ('_id' in resobj) else create_unique_resource_id()))\n    res = self.rr_store.create_mult(res_list, id_list, allow_ids=True)\n    rid_list = [(rid, rrv) for (success, rid, rrv) in res]\n    if (actor_id and (actor_id != 'anonymous')):\n        assoc_list = []\n        for (resobj, (rid, rrv)) in zip(res_list, rid_list):\n            resobj._id = rid\n            assoc_list.append((resobj, PRED.hasOwner, actor_id))\n        self.create_association_mult(assoc_list)\n    for (resobj, (rid, rrv)) in zip(res_list, rid_list):\n        self.event_pub.publish_event(event_type='ResourceModifiedEvent', origin=rid, origin_type=resobj.type_, mod_type=ResourceModificationType.CREATE)\n    return rid_list\n", "label": 1}
{"function": "\n\ndef get_near(self, user, size):\n    self._read_leaderboard()\n    scores = self.scores\n    if (len(scores) == 0):\n        return self.create_response(True, True, [])\n    if (not (user.username in self.user_scores)):\n        return self.get_top_players(user, size)\n    index = None\n    for (i, r) in enumerate(scores):\n        if (r.user == user.username):\n            index = i\n            break\n    start = (index - int(floor((size * 0.5))))\n    end = (index + int(ceil((size * 0.5))))\n    num_scores = len(scores)\n    if (start < 0):\n        end -= start\n        start = 0\n        if (end > num_scores):\n            end = num_scores\n    elif (end > num_scores):\n        start -= (end - num_scores)\n        end = num_scores\n        if (start < 0):\n            start = 0\n    leaderboard = []\n    player = None\n    for i in xrange(start, end, 1):\n        s = scores[i]\n        username = s.user\n        row = self._get_row(username, s)\n        if (username == user.username):\n            player = row\n        leaderboard.append(row)\n    if (player is None):\n        player = self._get_user_row(user)\n    self._rank_leaderboard(leaderboard, self._get_rank(leaderboard[0]['score']))\n    top = (start == 0)\n    bottom = (end == num_scores)\n    return self.create_response(top, bottom, leaderboard, player)\n", "label": 1}
{"function": "\n\ndef get_l3_agents(self, context, active=None, filters=None):\n    query = context.session.query(agents_db.Agent)\n    query = query.filter((agents_db.Agent.agent_type == constants.AGENT_TYPE_L3))\n    if (active is not None):\n        query = query.filter((agents_db.Agent.admin_state_up == active))\n    if filters:\n        for (key, value) in six.iteritems(filters):\n            column = getattr(agents_db.Agent, key, None)\n            if column:\n                if (not value):\n                    return []\n                query = query.filter(column.in_(value))\n        agent_modes = filters.get('agent_modes', [])\n        if agent_modes:\n            agent_mode_key = '\"agent_mode\": \"'\n            configuration_filter = [agents_db.Agent.configurations.contains(('%s%s\"' % (agent_mode_key, agent_mode))) for agent_mode in agent_modes]\n            query = query.filter(or_(*configuration_filter))\n    return [l3_agent for l3_agent in query if agentschedulers_db.AgentSchedulerDbMixin.is_eligible_agent(active, l3_agent)]\n", "label": 1}
{"function": "\n\ndef addmul_number_dicts(series):\n    'Multiply dictionaries by a numeric values and add them together.\\n\\n:parameter series: a tuple of two elements tuples. Each serie is of the form::\\n\\n        (weight,dictionary)\\n\\n    where ``weight`` is a number and ``dictionary`` is a dictionary with\\n    numeric values.\\n:parameter skip: optional list of field names to skip.\\n\\nOnly common fields are aggregated. If a field has a non-numeric value it is\\nnot included either.'\n    if (not series):\n        return\n    vtype = value_type((s[1] for s in series))\n    if (vtype == 1):\n        return sum(((weight * float(d)) for (weight, d) in series))\n    elif (vtype == 3):\n        keys = set(series[0][1])\n        for serie in series[1:]:\n            keys.intersection_update(serie[1])\n        results = {\n            \n        }\n        for key in keys:\n            key_series = tuple(((weight, d[key]) for (weight, d) in series))\n            result = addmul_number_dicts(key_series)\n            if (result is not None):\n                results[key] = result\n        return results\n", "label": 1}
{"function": "\n\ndef Q(*predicates, **query):\n    '\\n    Handles situations where :class:`hunter.Query` objects (or other callables) are passed in as positional arguments.\\n    Conveniently converts that to an :class:`hunter.And` predicate.\\n    '\n    optional_actions = query.pop('actions', [])\n    if ('action' in query):\n        optional_actions.append(query.pop('action'))\n    for p in predicates:\n        if (not callable(p)):\n            raise TypeError('Predicate {0!r} is not callable.'.format(p))\n    for a in optional_actions:\n        if (not callable(a)):\n            raise TypeError('Action {0!r} is not callable.'.format(a))\n    if predicates:\n        predicates = tuple(((p() if (inspect.isclass(p) and issubclass(p, Action)) else p) for p in predicates))\n        if any((isinstance(p, CodePrinter) for p in predicates)):\n            if (CodePrinter in optional_actions):\n                optional_actions.remove(CodePrinter)\n        if query:\n            predicates += (Query(**query),)\n        result = And(*predicates)\n    else:\n        result = Query(**query)\n    if optional_actions:\n        result = When(result, *optional_actions)\n    return result\n", "label": 1}
{"function": "\n\ndef __init__(self, rdclass, rdtype, latitude, longitude, altitude):\n    super(GPOS, self).__init__(rdclass, rdtype)\n    if (isinstance(latitude, float) or isinstance(latitude, int) or isinstance(latitude, long)):\n        latitude = str(latitude)\n    if (isinstance(longitude, float) or isinstance(longitude, int) or isinstance(longitude, long)):\n        longitude = str(longitude)\n    if (isinstance(altitude, float) or isinstance(altitude, int) or isinstance(altitude, long)):\n        altitude = str(altitude)\n    _validate_float_string(latitude)\n    _validate_float_string(longitude)\n    _validate_float_string(altitude)\n    self.latitude = latitude\n    self.longitude = longitude\n    self.altitude = altitude\n", "label": 1}
{"function": "\n\ndef test_rrlyrae_lightcurves():\n    for partial in [True, False]:\n        try:\n            rrlyrae = fetch_rrlyrae(partial=partial)\n        except (URLError, ConnectionError):\n            raise SkipTest('No internet connection: data download test skipped')\n        lcid = rrlyrae.ids[0]\n        if (not partial):\n            (t, y, dy) = rrlyrae.get_lightcurve(lcid, return_1d=False)\n            assert (t.ndim == 2)\n            assert (t.shape == y.shape)\n            assert (t.shape == dy.shape)\n        (t, y, dy, filts) = rrlyrae.get_lightcurve(lcid)\n        assert (t.ndim == 1)\n        assert (t.shape == y.shape)\n        assert (t.shape == dy.shape)\n        assert (t.shape == filts.shape)\n        assert (not np.any(np.isnan(((t + y) + dy))))\n        meta = rrlyrae.get_metadata(lcid)\n        assert (meta['id'] == int(lcid))\n        meta = rrlyrae.get_obsmeta(lcid)\n        assert (meta['id'] == int(lcid))\n", "label": 1}
{"function": "\n\ndef step_1b(w):\n    ' Step 1b handles -ed and -ing suffixes (or -edly and -ingly).\\n        Removes double consonants at the end of the stem and adds -e to some words.\\n    '\n    if (w.endswith('y') and w.endswith(('edly', 'ingly'))):\n        w = w[:(- 2)]\n    if w.endswith(('ed', 'ing')):\n        if w.endswith('ied'):\n            return (((len(w) == 4) and w[:(- 1)]) or w[:(- 2)])\n        if w.endswith('eed'):\n            return ((R1(w).endswith('eed') and w[:(- 1)]) or w)\n        for suffix in ('ed', 'ing'):\n            if (w.endswith(suffix) and has_vowel(w[:(- len(suffix))])):\n                w = w[:(- len(suffix))]\n                if w.endswith(('at', 'bl', 'iz')):\n                    return (w + 'e')\n                if is_double_consonant(w[(- 2):]):\n                    return w[:(- 1)]\n                if is_short(w):\n                    return (w + 'e')\n    return w\n", "label": 1}
{"function": "\n\ndef perform(self, node, ins, outs):\n    '\\n        .. todo::\\n\\n            WRITEME\\n        '\n    (x,) = ins\n    outs[0][0] = z = x.copy()\n    (B, M, N, K) = x.shape\n    for level in range(1, self.n_levels):\n        z0 = z[0]\n        if ((z0.shape[0] <= 2) or (z0.shape[1] <= 2)):\n            raise ValueError('Cannot downsample an image smaller than 3x3', z0.shape)\n        logger.info('{0} {1} {2}'.format(z0.shape, z0.dtype, z0.strides))\n        out0 = cv.pyrDown(z0)\n        assert (out0.dtype == x.dtype)\n        if (out0.ndim == 3):\n            assert (out0.shape[2] == x.shape[3])\n        else:\n            assert (K == 1)\n        out = numpy.empty((x.shape[0], out0.shape[0], out0.shape[1], K), dtype=out0.dtype)\n        if (K == 1):\n            out[0][:, :, 0] = out0\n        else:\n            out[0] = out0\n        for (i, zi) in enumerate(z[1:]):\n            if (K == 1):\n                out[i][:, :, 0] = cv.pyrDown(z[i])\n            else:\n                out[i] = cv.pyrDown(z[i])\n        outs[level][0] = out\n        z = out\n", "label": 1}
{"function": "\n\ndef evalModify(ctx, u):\n    originalctx = ctx\n    if u.using:\n        otherDefault = False\n        for d in u.using:\n            if d.default:\n                if (not otherDefault):\n                    dg = Graph()\n                    ctx = ctx.pushGraph(dg)\n                    otherDefault = True\n                ctx.load(d.default, default=True)\n            elif d.named:\n                g = d.named\n                ctx.load(g, default=False)\n    if ((not u.using) and u.withClause):\n        g = ctx.dataset.get_context(u.withClause)\n        ctx = ctx.pushGraph(g)\n    res = evalPart(ctx, u.where)\n    if u.using:\n        if otherDefault:\n            ctx = originalctx\n        if u.withClause:\n            g = ctx.dataset.get_context(u.withClause)\n            ctx = ctx.pushGraph(g)\n    for c in res:\n        dg = ctx.graph\n        if u.delete:\n            dg -= _fillTemplate(u.delete.triples, c)\n            for (g, q) in u.delete.quads.iteritems():\n                cg = ctx.dataset.get_context(c.get(g))\n                cg -= _fillTemplate(q, c)\n        if u.insert:\n            dg += _fillTemplate(u.insert.triples, c)\n            for (g, q) in u.insert.quads.iteritems():\n                cg = ctx.dataset.get_context(c.get(g))\n                cg += _fillTemplate(q, c)\n", "label": 1}
{"function": "\n\ndef build(self, input_shape):\n    self.input_spec = [InputSpec(dtype=K.floatx(), shape=((None,) + input_shape[1:]))]\n    input_dim = input_shape[2]\n    self.W = self.init((input_dim, self.output_dim), name='{}_W'.format(self.name))\n    if self.bias:\n        self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n        self.trainable_weights = [self.W, self.b]\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if (self.bias and self.b_regularizer):\n        self.b_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    if self.activity_regularizer:\n        self.activity_regularizer.set_layer(self)\n        self.regularizers.append(self.activity_regularizer)\n    self.constraints = {\n        \n    }\n    if self.W_constraint:\n        self.constraints[self.W] = self.W_constraint\n    if (self.bias and self.b_constraint):\n        self.constraints[self.b] = self.b_constraint\n    if (self.initial_weights is not None):\n        self.set_weights(self.initial_weights)\n        del self.initial_weights\n", "label": 1}
{"function": "\n\ndef highlight_html_differences(s1, s2):\n    differ = diff_match_patch()\n    ops = differ.diff_main(s1, s2)\n    differ.diff_cleanupSemantic(ops)\n    retval = ''\n    in_tag = False\n    idx = 0\n    while (idx < len(ops)):\n        (op, text) = ops[idx]\n        next_op = None\n        if (idx != (len(ops) - 1)):\n            (next_op, next_text) = ops[(idx + 1)]\n        if ((op == diff_match_patch.DIFF_DELETE) and (next_op == diff_match_patch.DIFF_INSERT)):\n            (chunks, in_tag) = chunkize(next_text, in_tag)\n            retval += highlight_chunks(chunks, highlight_replaced)\n            idx += 1\n        elif ((op == diff_match_patch.DIFF_INSERT) and (next_op == diff_match_patch.DIFF_DELETE)):\n            (chunks, in_tag) = chunkize(text, in_tag)\n            retval += highlight_chunks(chunks, highlight_replaced)\n            idx += 1\n        elif (op == diff_match_patch.DIFF_DELETE):\n            retval += highlight_deleted('&nbsp;')\n        elif (op == diff_match_patch.DIFF_INSERT):\n            (chunks, in_tag) = chunkize(text, in_tag)\n            retval += highlight_chunks(chunks, highlight_inserted)\n        elif (op == diff_match_patch.DIFF_EQUAL):\n            (chunks, in_tag) = chunkize(text, in_tag)\n            retval += text\n        idx += 1\n    if (not verify_html(retval)):\n        from zerver.lib.actions import internal_send_message\n        logging.getLogger('').error('HTML diff produced mal-formed HTML')\n        if (settings.ERROR_BOT is not None):\n            subject = ('HTML diff failure on %s' % (platform.node(),))\n            internal_send_message(settings.ERROR_BOT, 'stream', 'errors', subject, 'HTML diff produced malformed HTML')\n        return s2\n    return retval\n", "label": 1}
{"function": "\n\ndef _PrettyIndent(self, elem, level=0):\n    'Prettifies an element tree in-place'\n    i = ('\\n' + (level * '  '))\n    if len(elem):\n        if ((not elem.text) or (not elem.text.strip())):\n            elem.text = (i + '  ')\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n        for elem in elem:\n            self._PrettyIndent(elem, (level + 1))\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n    elif (level and ((not elem.tail) or (not elem.tail.strip()))):\n        elem.tail = i\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    sep = ''\n    string = ''\n    if self._modifier:\n        string = (self._modifier + ' ')\n    if self._segment:\n        string += (self._segment + ':')\n    string += '['\n    if self._base:\n        string += self._base\n    if self._index:\n        if self._base:\n            string += ((sep + '+') + sep)\n        string += self._index\n        string += (((sep + '*') + sep) + str(self._scale))\n    if (self._displacement != 0):\n        if (self._base or self._index):\n            string += ((sep + '+') + sep)\n        imm_hex = hex((self._displacement & ((2 ** 32) - 1)))\n        string += (imm_hex[:(- 1)] if (imm_hex[(- 1)] == 'L') else imm_hex)\n    string += ']'\n    return string\n", "label": 1}
{"function": "\n\ndef _proc_pax(self, filetar):\n    'Process an extended or global header as described in POSIX.1-2001.'\n    buf = filetar.fileobj.read(self._block(self.size))\n    if (self.type == tarfile.XGLTYPE):\n        pax_headers = filetar.pax_headers\n    else:\n        pax_headers = filetar.pax_headers.copy()\n    regex = re.compile('(\\\\d+) ([^=]+)=', re.U)\n    pos = 0\n    while True:\n        match = regex.match(buf, pos)\n        if (not match):\n            break\n        (length, keyword) = match.groups()\n        length = int(length)\n        value = buf[(match.end(2) + 1):((match.start(1) + length) - 1)]\n        try:\n            keyword = keyword.decode('utf8')\n        except Exception:\n            pass\n        try:\n            value = value.decode('utf8')\n        except Exception:\n            pass\n        pax_headers[keyword] = value\n        pos += length\n    try:\n        next = self.fromtarfile(filetar)\n    except tarfile.HeaderError:\n        raise tarfile.SubsequentHeaderError('missing or bad subsequent header')\n    if (self.type in (tarfile.XHDTYPE, tarfile.SOLARIS_XHDTYPE)):\n        next._apply_pax_info(pax_headers, filetar.encoding, filetar.errors)\n        next.offset = self.offset\n        if ('size' in pax_headers):\n            offset = next.offset_data\n            if (next.isreg() or (next.type not in tarfile.SUPPORTED_TYPES)):\n                offset += next._block(next.size)\n            filetar.offset = offset\n    return next\n", "label": 1}
{"function": "\n\ndef _get_format_from_style(self, token, style):\n    ' Returns a QTextCharFormat for token by reading a Pygments style.\\n        '\n    result = QtGui.QTextCharFormat()\n    for (key, value) in list(style.style_for_token(token).items()):\n        if value:\n            if (key == 'color'):\n                result.setForeground(self._get_brush(value))\n            elif (key == 'bgcolor'):\n                result.setBackground(self._get_brush(value))\n            elif (key == 'bold'):\n                result.setFontWeight(QtGui.QFont.Bold)\n            elif (key == 'italic'):\n                result.setFontItalic(True)\n            elif (key == 'underline'):\n                result.setUnderlineStyle(QtGui.QTextCharFormat.SingleUnderline)\n            elif (key == 'sans'):\n                result.setFontStyleHint(QtGui.QFont.SansSerif)\n            elif (key == 'roman'):\n                result.setFontStyleHint(QtGui.QFont.Times)\n            elif (key == 'mono'):\n                result.setFontStyleHint(QtGui.QFont.TypeWriter)\n    return result\n", "label": 1}
{"function": "\n\ndef visit(self):\n    '\\n        This function *visits* its children in a recursive\\n        way.\\n\\n        It will first *visit* the children that\\n        that have a z-order value less than 0.\\n\\n        Then it will call the :meth:`draw` method to\\n        draw itself.\\n\\n        And finally it will *visit* the rest of the\\n        children (the ones with a z-value bigger\\n        or equal than 0)\\n\\n        Before *visiting* any children it will call\\n        the :meth:`transform` method to apply any possible\\n        transformations.\\n        '\n    if (not self.visible):\n        return\n    position = 0\n    if (self.grid and self.grid.active):\n        self.grid.before_draw()\n    if (self.children and (self.children[0][0] < 0)):\n        gl.glPushMatrix()\n        self.transform()\n        for (z, c) in self.children:\n            if (z >= 0):\n                break\n            position += 1\n            c.visit()\n        gl.glPopMatrix()\n    self.draw()\n    if (position < len(self.children)):\n        gl.glPushMatrix()\n        self.transform()\n        for (z, c) in self.children[position:]:\n            c.visit()\n        gl.glPopMatrix()\n    if (self.grid and self.grid.active):\n        self.grid.after_draw(self.camera)\n", "label": 1}
{"function": "\n\ndef get_result(self):\n    what = self.expr.op()\n    if self.memoize:\n        self._memoize_tables()\n    if (isinstance(what, ops.TableNode) and what.has_schema()):\n        if ((not self.memoize) and (what in self.memo)):\n            text = ('Table: %s' % self.memo.get_alias(what))\n        elif isinstance(what, ops.PhysicalTable):\n            text = self._format_table(what)\n        else:\n            text = self._format_node(what)\n    elif isinstance(what, ops.TableColumn):\n        text = self._format_column(self.expr)\n    elif isinstance(what, ir.Node):\n        text = self._format_node(what)\n    elif isinstance(what, ops.Literal):\n        text = ('Literal[%s] %s' % (self._get_type_display(), str(what.value)))\n    if (isinstance(self.expr, ir.ValueExpr) and (self.expr._name is not None)):\n        text = '{0} = {1}'.format(self.expr.get_name(), text)\n    if self.memoize:\n        alias_to_text = [(self.memo.aliases[x], self.memo.formatted[x], self.memo.ops[x]) for x in self.memo.formatted]\n        alias_to_text.sort()\n        refs = [((x + '\\n') + y) for (x, y, op) in alias_to_text if (not op.equals(what))]\n        text = '\\n\\n'.join((refs + [text]))\n    return self._indent(text, self.base_level)\n", "label": 1}
{"function": "\n\n@eventmgr_rfc1459.message('TOPIC', min_params=1, update_idle=True)\ndef m_TOPIC(cli, ev_msg):\n    chanlist = ev_msg['params'][0].split(',')\n    for chan in chanlist:\n        if (not validate_chan(chan)):\n            cli.dump_numeric('479', [chan, 'Illegal channel name'])\n            return\n        ch = cli.ctx.chmgr.get(chan, create=False)\n        if (not ch):\n            cli.dump_numeric('403', [chan, 'No such channel'])\n            continue\n        if (not ch.has_member(cli)):\n            cli.dump_numeric('442', [ch.name, \"You're not on that channel\"])\n            continue\n        if (len(ev_msg['params']) == 1):\n            if ch.topic:\n                cli.dump_numeric('332', [ch.name, ch.topic])\n                cli.dump_numeric('333', [ch.name, ch.topic_setter, ch.topic_ts])\n                continue\n            cli.dump_numeric('331', [ch.name, 'No topic is set'])\n            continue\n        if (not ch.props.get('op-topic')):\n            cli.dump_numeric('482', [ch.name, \"You're not a channel operator\"])\n            continue\n        topic = ev_msg['params'][1]\n        topiclen = cli.ctx.conf.limits.get('topic', None)\n        if (topiclen and (len(ev_msg['params'][1]) > topiclen)):\n            topic = topic[:topiclen]\n        ch.topic = topic\n        ch.topic_setter = cli.hostmask\n        ch.topic_ts = cli.ctx.current_ts\n        ch.dump_message(RFC1459Message.from_data('TOPIC', source=cli, params=[ch.name, ch.topic]))\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    super_new = super(CategoryMetaclass, cls).__new__\n    parents = [b for b in bases if (isinstance(b, CategoryMetaclass) and (not ((b.__name__ == 'NewBase') and (b.__mro__ == (b, object)))))]\n    if (not parents):\n        return super_new(cls, name, bases, attrs)\n    new_cls = super_new(cls, name, bases, attrs)\n    if (not hasattr(new_cls, 'name')):\n        raise ImproperlyConfigured(('No name attribute defined in %s.' % new_cls))\n    if (not hasattr(new_cls, 'slug')):\n        raise ImproperlyConfigured(('No slug attribute defined in %s.' % new_cls))\n    if (not hasattr(new_cls, 'dashboard')):\n        raise ImproperlyConfigured(('No dashboard attribute defined in %s.' % new_cls))\n    if (not re.match('^[\\\\w-]+$', new_cls.slug)):\n        raise ImproperlyConfigured(('The slug attribute defined in %s contains invalid chars.' % new_cls))\n    return new_cls\n", "label": 1}
{"function": "\n\ndef phys_tokens(toks):\n    \"Return all physical tokens, even line continuations.\\n\\n    tokenize.generate_tokens() doesn't return a token for the backslash that\\n    continues lines.  This wrapper provides those tokens so that we can\\n    re-create a faithful representation of the original source.\\n\\n    Returns the same values as generate_tokens()\\n\\n    \"\n    last_line = None\n    last_lineno = (- 1)\n    last_ttype = None\n    for (ttype, ttext, (slineno, scol), (elineno, ecol), ltext) in toks:\n        if (last_lineno != elineno):\n            if (last_line and last_line.endswith('\\\\\\n')):\n                inject_backslash = True\n                if (last_ttype == tokenize.COMMENT):\n                    inject_backslash = False\n                elif (ttype == token.STRING):\n                    if (('\\n' in ttext) and (ttext.split('\\n', 1)[0][(- 1)] == '\\\\')):\n                        inject_backslash = False\n                if inject_backslash:\n                    ccol = (len(last_line.split('\\n')[(- 2)]) - 1)\n                    (yield (99999, '\\\\\\n', (slineno, ccol), (slineno, (ccol + 2)), last_line))\n            last_line = ltext\n            last_ttype = ttype\n        (yield (ttype, ttext, (slineno, scol), (elineno, ecol), ltext))\n        last_lineno = elineno\n", "label": 1}
{"function": "\n\ndef _validate_for_numeric_binop(self, other, op, opstr):\n    '\\n        return valid other, evaluate or raise TypeError\\n        if we are not of the appropriate type\\n\\n        internal method called by ops\\n        '\n    from pandas.tseries.offsets import DateOffset\n    if (not self._is_numeric_dtype):\n        raise TypeError('cannot evaluate a numeric op {opstr} for type: {typ}'.format(opstr=opstr, typ=type(self)))\n    if isinstance(other, Index):\n        if (not other._is_numeric_dtype):\n            raise TypeError('cannot evaluate a numeric op {opstr} with type: {typ}'.format(opstr=type(self), typ=type(other)))\n    elif (isinstance(other, np.ndarray) and (not other.ndim)):\n        other = other.item()\n    if isinstance(other, (Index, ABCSeries, np.ndarray)):\n        if (len(self) != len(other)):\n            raise ValueError('cannot evaluate a numeric op with unequal lengths')\n        other = _values_from_object(other)\n        if (other.dtype.kind not in ['f', 'i']):\n            raise TypeError('cannot evaluate a numeric op with a non-numeric dtype')\n    elif isinstance(other, (DateOffset, np.timedelta64, Timedelta, datetime.timedelta)):\n        pass\n    elif isinstance(other, (Timestamp, np.datetime64)):\n        pass\n    elif (not (is_float(other) or is_integer(other))):\n        raise TypeError('can only perform ops with scalar values')\n    return other\n", "label": 1}
{"function": "\n\n@contextlib.contextmanager\ndef _transaction_scope(self, context):\n    new_transaction = self._independent\n    transaction_contexts_by_thread = _transaction_contexts_by_thread(context)\n    current = restore = getattr(transaction_contexts_by_thread, 'current', None)\n    use_factory = self._factory\n    global_factory = None\n    if self._replace_global_factory:\n        use_factory = global_factory = self._replace_global_factory\n    elif ((current is not None) and current.global_factory):\n        global_factory = current.global_factory\n        if self._root._is_global_manager:\n            use_factory = global_factory\n    if ((current is not None) and (new_transaction or (current.factory is not use_factory))):\n        current = None\n    if (current is None):\n        current = transaction_contexts_by_thread.current = _TransactionContext(use_factory, global_factory=global_factory, **use_factory._transaction_ctx_cfg)\n    try:\n        if (self._mode is not None):\n            with current._produce_block(mode=self._mode, connection=self._connection, savepoint=self._savepoint, allow_async=self._allow_async) as resource:\n                (yield resource)\n        else:\n            (yield)\n    finally:\n        if (restore is None):\n            del transaction_contexts_by_thread.current\n        elif (current is not restore):\n            transaction_contexts_by_thread.current = restore\n", "label": 1}
{"function": "\n\ndef handle_response(self, payload):\n    ' Handle a tunnel response\\n        '\n    if ('msgid' in payload):\n        msgid = payload['msgid']\n        if (msgid in self.connections):\n            if (('cmd' in payload) and ('header' in payload) and ('data' in payload)):\n                cmd = payload['cmd']\n                if (cmd == 'httpresp'):\n                    self.send_response(msgid, payload['header'], payload['data'], True)\n                    return\n                elif (cmd == 'logresp'):\n                    self.send_response(msgid, payload['header'], payload['data'], False)\n                    return\n                elif (cmd == 'logevent'):\n                    result = self.send_response(msgid, payload['header'], payload['data'], False)\n                    if (not result):\n                        msg = {\n                            'cmd': 'logclose',\n                        }\n                        self.tunnel.send(msg)\n                    return\n    _log.error(('Unknown control proxy response %s' % payload))\n", "label": 1}
{"function": "\n\ndef test_tripboard_over_midnight(self):\n    sc = ServiceCalendar()\n    sc.add_period(0, ((1 * 3600) * 24), ['WKDY'])\n    sc.add_period(((1 * 3600) * 24), ((2 * 3600) * 24), ['SAT'])\n    tz = Timezone()\n    tz.add_period(TimezonePeriod(0, ((2 * 3600) * 24), 0))\n    hb = HeadwayBoard('WKDY', sc, tz, 0, 'owl', (23 * 3600), (26 * 3600), 100)\n    s0 = State(1, 0)\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.weight == 82901)\n    assert (s1.service_period(0).service_ids == [0])\n    s0 = State(1, (23 * 3600))\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.weight == 101)\n    assert (s1.service_period(0).service_ids == [0])\n    s0 = State(1, (24 * 3600))\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.weight == 101)\n    assert (s1.service_period(0).service_ids == [1])\n    s0 = State(1, (25 * 3600))\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.time == ((25 * 3600) + 100))\n    assert (s1.weight == 101)\n    assert (s1.service_period(0).service_ids == [1])\n    s0 = State(1, (26 * 3600))\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1.time == ((26 * 3600) + 100))\n    assert (s1.weight == 101)\n    assert (s1.service_period(0).service_ids == [1])\n    s0 = State(1, ((26 * 3600) + 1))\n    s1 = hb.walk(s0, WalkOptions())\n    assert (s1 == None)\n", "label": 1}
{"function": "\n\ndef trace(*predicates, **options):\n    '\\n    Starts tracing. Can be used as a context manager (with slightly incorrect semantics - it starts tracing\\n    before ``__enter__`` is called).\\n\\n    Parameters:\\n        *predicates (callables): Runs actions if **all** of the given predicates match.\\n    Keyword Args:\\n        clear_env_var: Disables tracing in subprocess. Default: ``False``.\\n        threading_support: Enable tracing *new* threads. Default: ``False``.\\n        action: Action to run if all the predicates return ``True``. Default: ``CodePrinter``.\\n        actions: Actions to run (in case you want more than 1).\\n    '\n    global _last_tracer\n    clear_env_var = options.pop('clear_env_var', False)\n    threading_support = (options.pop('threading_support', False) or options.pop('threads_support', False) or options.pop('thread_support', False) or options.pop('threadingsupport', False) or options.pop('threadssupport', False) or options.pop('threadsupport', False) or options.pop('threading', False) or options.pop('threads', False) or options.pop('thread', False))\n    predicate = _prepare_predicate(*predicates, **options)\n    if clear_env_var:\n        os.environ.pop('PYTHONHUNTER', None)\n    _last_tracer = Tracer(threading_support)\n\n    @atexit.register\n    def atexit_cleanup(ref=weakref.ref(_last_tracer)):\n        maybe_tracer = ref()\n        if (maybe_tracer is not None):\n            maybe_tracer.stop()\n    return _last_tracer.trace(predicate)\n", "label": 1}
{"function": "\n\ndef all(self, category=None, return_key_and_category=False):\n    '\\n        Get all tags in this handler.\\n\\n        Args:\\n            category (str, optional): The Tag category to limit the\\n                request to. Note that `None` is the valid, default\\n                category.\\n            return_key_and_category (bool, optional): Return a list of\\n                tuples `[(key, category), ...]`.\\n\\n        Returns:\\n            tags (list): A list of tag keys `[tagkey, tagkey, ...]` or\\n                a list of tuples `[(key, category), ...]` if\\n                `return_key_and_category` is set.\\n\\n        '\n    if ((self._cache is None) or (not _TYPECLASS_AGGRESSIVE_CACHE)):\n        self._recache()\n    if category:\n        category = (category.strip().lower() if (category is not None) else None)\n        matches = [tag for tag in self._cache.values() if (tag.db_category == category)]\n    else:\n        matches = self._cache.values()\n    if matches:\n        matches = sorted(matches, key=(lambda o: o.id))\n        if return_key_and_category:\n            return [(to_str(p.db_key), to_str(p.db_category)) for p in matches]\n        else:\n            return [to_str(p.db_key) for p in matches]\n    return []\n", "label": 1}
{"function": "\n\ndef dictify(values, colnames):\n    \"\\n    Convert a list of values into a dictionary based upon given column names.\\n\\n    If the column name starts with an '@', the value is assumed to be a comma\\n    separated list.\\n\\n    If the name starts with a '#', the value is assumed to be an int.\\n\\n    If the name starts with '@#', the value is assumed to  a comma separated\\n    list of ints.\\n\\n    \"\n    d = {\n        \n    }\n    for i in xrange(len(colnames)):\n        key = colnames[i]\n        split = False\n        num = False\n        if (key[0] == '@'):\n            key = key[1:]\n            split = True\n        if (key[0] == '#'):\n            key = key[1:]\n            num = True\n        if (i < len(values)):\n            if (num and split):\n                val = [int(x) for x in values[i].rstrip(',').split(',')]\n            elif num:\n                val = int(values[i])\n            elif split:\n                val = values[i].rstrip(',').split(',')\n            else:\n                val = values[i]\n            d[key] = val\n        else:\n            d[key] = None\n    return d\n", "label": 1}
{"function": "\n\ndef __init__(self, function):\n    self.spec = getattr(function, 'original', function)\n    self.arguments = introspect.arguments(function)\n    self._function = function\n    self.is_coroutine = introspect.is_coroutine(self.spec)\n    if self.is_coroutine:\n        self.spec = getattr(self.spec, '__wrapped__', self.spec)\n    self.takes_kargs = introspect.takes_kargs(self.spec)\n    self.takes_kwargs = introspect.takes_kwargs(self.spec)\n    self.parameters = introspect.arguments(self.spec, (1 if self.takes_kargs else 0))\n    if self.takes_kargs:\n        self.karg = self.parameters[(- 1)]\n    self.defaults = {\n        \n    }\n    for (index, default) in enumerate(reversed((self.spec.__defaults__ or ()))):\n        self.defaults[self.parameters[(- (index + 1))]] = default\n    self.required = self.parameters[:((- len((self.spec.__defaults__ or ()))) or None)]\n    if (introspect.is_method(self.spec) or introspect.is_method(function)):\n        self.required = self.required[1:]\n        self.parameters = self.parameters[1:]\n    self.transform = self.spec.__annotations__.get('return', None)\n    self.directives = {\n        \n    }\n    self.input_transformations = {\n        \n    }\n    for (name, transformer) in self.spec.__annotations__.items():\n        if isinstance(transformer, str):\n            continue\n        elif hasattr(transformer, 'directive'):\n            self.directives[name] = transformer\n            continue\n        if hasattr(transformer, 'load'):\n            transformer = MarshmallowSchema(transformer)\n        elif hasattr(transformer, 'deserialize'):\n            transformer = transformer.deserialize\n        self.input_transformations[name] = transformer\n", "label": 1}
{"function": "\n\ndef __init__(self, log_category='jcli'):\n    CmdProtocol.__init__(self, log_category)\n    self.authentication = {\n        'username': None,\n        'password': None,\n        'printedPassword': None,\n        'auth': False,\n    }\n    if ('persist' not in self.commands):\n        self.commands.append('persist')\n    if ('load' not in self.commands):\n        self.commands.append('load')\n    if ('user' not in self.commands):\n        self.commands.append('user')\n    if ('group' not in self.commands):\n        self.commands.append('group')\n    if ('filter' not in self.commands):\n        self.commands.append('filter')\n    if ('mointerceptor' not in self.commands):\n        self.commands.append('mointerceptor')\n    if ('mtinterceptor' not in self.commands):\n        self.commands.append('mtinterceptor')\n    if ('morouter' not in self.commands):\n        self.commands.append('morouter')\n    if ('mtrouter' not in self.commands):\n        self.commands.append('mtrouter')\n    if ('smppccm' not in self.commands):\n        self.commands.append('smppccm')\n    if ('httpccm' not in self.commands):\n        self.commands.append('httpccm')\n    if ('stats' not in self.commands):\n        self.commands.append('stats')\n", "label": 1}
{"function": "\n\ndef _separatevars_dict(expr, symbols):\n    if symbols:\n        if (not all((t.is_Atom for t in symbols))):\n            raise ValueError('symbols must be Atoms.')\n        symbols = list(symbols)\n    elif (symbols is None):\n        return {\n            'coeff': expr,\n        }\n    else:\n        symbols = list(expr.free_symbols)\n        if (not symbols):\n            return None\n    ret = dict(((i, []) for i in (symbols + ['coeff'])))\n    for i in Mul.make_args(expr):\n        expsym = i.free_symbols\n        intersection = set(symbols).intersection(expsym)\n        if (len(intersection) > 1):\n            return None\n        if (len(intersection) == 0):\n            ret['coeff'].append(i)\n        else:\n            ret[intersection.pop()].append(i)\n    for (k, v) in ret.items():\n        ret[k] = Mul(*v)\n    return ret\n", "label": 1}
{"function": "\n\ndef run(self, config, args):\n    server = config.get('server')\n    stream = args['--stream']\n    if (stream == 'stdin'):\n        stream = None\n    if ('.' in args['<pid>']):\n        (pid, stream) = args['<pid>'].split('.', 1)\n    else:\n        pid = args['<pid>']\n    if (not pid.isdigit()):\n        raise RuntimeError('invalid <pid> value')\n    try:\n        p = server.get_process(int(pid))\n    except GafferNotFound:\n        print(('process %r not found' % pid))\n        return\n    if (not stream):\n        if (not p.redirect_input):\n            raise RuntimeError(\"can't write on this process, (no stdin)\")\n    elif (stream not in p.custom_streams):\n        raise RuntimeError(('stream %r not found' % stream))\n    self.pid = int(pid)\n    self.stream = stream\n    self.socket = p.socket(mode=pyuv.UV_WRITABLE, stream=stream)\n    self.socket.start()\n    data = ' '.join(args['<data>'])\n    self.args = args\n    self.tty = None\n    self.should_close = False\n    if (data.strip() == '-'):\n        self.tty = pyuv.TTY(server.loop, sys.stdin.fileno(), True)\n        self.tty.start_read(self._on_tty_read)\n        self.sig_handler = SigHandler(self)\n        self.sig_handler.start(server.loop)\n    else:\n        data = data.strip()\n        if (not data.endswith('\\n')):\n            data = (data + os.linesep)\n        self.socket.write(data, self._on_done)\n    server.loop.run()\n", "label": 1}
{"function": "\n\ndef initialize(self, description):\n    model = self.model\n    conv = []\n    identity = (lambda x: x)\n    for i in range(len(description)):\n        func = identity\n        column = description[i][0]\n        found = False\n        if (self.column_meta is not None):\n            try:\n                select_column = self.column_meta[i]\n            except IndexError:\n                pass\n            else:\n                if isinstance(select_column, Field):\n                    func = select_column.python_value\n                    column = (select_column._alias or select_column.name)\n                    found = True\n                elif (isinstance(select_column, Func) and len(select_column.arguments) and isinstance(select_column.arguments[0], Field)):\n                    if select_column._coerce:\n                        func = select_column.arguments[0].python_value\n                    found = True\n        if ((not found) and (column in model._meta.columns)):\n            field_obj = model._meta.columns[column]\n            column = field_obj.name\n            func = field_obj.python_value\n        conv.append((i, column, func))\n    self.conv = conv\n", "label": 1}
{"function": "\n\ndef parse_items(items, data=None, headers=None, files=None, params=None):\n    'Parse `KeyValue` `items` into `data`, `headers`, `files`,\\n    and `params`.\\n\\n    '\n    if (headers is None):\n        headers = CaseInsensitiveDict()\n    if (data is None):\n        data = OrderedDict()\n    if (files is None):\n        files = OrderedDict()\n    if (params is None):\n        params = ParamDict()\n    for item in items:\n        value = item.value\n        key = item.key\n        if (item.sep == SEP_HEADERS):\n            target = headers\n        elif (item.sep == SEP_QUERY):\n            target = params\n        elif (item.sep == SEP_FILES):\n            try:\n                with open(os.path.expanduser(value), 'rb') as f:\n                    value = (os.path.basename(value), BytesIO(f.read()))\n            except IOError as e:\n                raise ParseError(('Invalid argument \"%s\": %s' % (item.orig, e)))\n            target = files\n        elif (item.sep in [SEP_DATA, SEP_DATA_RAW_JSON]):\n            if (item.sep == SEP_DATA_RAW_JSON):\n                try:\n                    value = json.loads(item.value)\n                except ValueError:\n                    raise ParseError(('\"%s\" is not valid JSON' % item.orig))\n            target = data\n        else:\n            raise TypeError(item)\n        target[key] = value\n    return (headers, data, files, params)\n", "label": 1}
{"function": "\n\n@task\ndef deploy(type=None):\n    'deploy your site, support rsync / ftp / github pages\\n\\n    run deploy:\\n        $ fab deploy\\n\\n    run deploy with specific type(not supported specify multiple types):\\n        $ fab deploy:type=rsync\\n\\n    '\n    if (('deploy' not in configs) or (not isinstance(configs['deploy'], list))):\n        do_exit('Warning: deploy not set right in _config.yml')\n    if (type and (type not in SUPPORTED_DEPLOY_TYPES)):\n        do_exit('Warning: supported deploy type: {0}'.format(', '.join(SUPPORTED_DEPLOY_TYPES)))\n    deploy_configs = configs['deploy']\n    done = False\n    for deploy_item in deploy_configs:\n        deploy_type = deploy_item.pop('type')\n        if (type and (deploy_type != type)):\n            continue\n        func_name = 'deploy_{0}'.format(deploy_type)\n        func = globals().get(func_name)\n        if (not func):\n            do_exit('Warning: not supprt {0} deploy method'.format(deploy_type))\n        func(deploy_item)\n        done = True\n    if (not done):\n        if type:\n            do_exit('Warning: specific deploy type not configured yet')\n        else:\n            print(blue('do nothing...'))\n", "label": 1}
{"function": "\n\ndef _resolve_signature(self):\n    'Resolve signature.\\n        May have ambiguous case.\\n        '\n    matches = []\n    if self.scalarpos:\n        for formaltys in self.typemap:\n            match_map = []\n            for (i, (formal, actual)) in enumerate(zip(formaltys, self.argtypes)):\n                if (actual is None):\n                    actual = np.asarray(self.args[i]).dtype\n                match_map.append((actual == formal))\n            if all(match_map):\n                matches.append(formaltys)\n    if (not matches):\n        matches = []\n        for formaltys in self.typemap:\n            all_matches = all((((actual is None) or (formal == actual)) for (formal, actual) in zip(formaltys, self.argtypes)))\n            if all_matches:\n                matches.append(formaltys)\n    if (not matches):\n        raise TypeError(\"No matching version.  GPU ufunc requires array arguments to have the exact types.  This behaves like regular ufunc with casting='no'.\")\n    if (len(matches) > 1):\n        raise TypeError('Failed to resolve ufunc due to ambiguous signature. Too many untyped scalars. Use numpy dtype object to type tag.')\n    self.argtypes = matches[0]\n", "label": 1}
{"function": "\n\ndef determine_linker(self, target, src):\n    if isinstance(target, build.StaticLibrary):\n        return self.build.static_linker\n    if (len(self.build.compilers) == 1):\n        return self.build.compilers[0]\n    cpp = None\n    for c in self.build.compilers:\n        if (c.get_language() == 'cpp'):\n            cpp = c\n            break\n    if (cpp is not None):\n        for s in src:\n            if c.can_compile(s):\n                return cpp\n    for c in self.build.compilers:\n        if (c.get_language() == 'vala'):\n            continue\n        for s in src:\n            if c.can_compile(s):\n                return c\n    raise RuntimeError('Unreachable code')\n", "label": 1}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    if self.IsValidScope(view):\n        settings = SublimePapyrus.GetSettings()\n        if (settings and settings.get('intelligent_code_completion', True)):\n            if self.completionRunning:\n                return\n            elif self.linterRunning:\n                return\n            self.completionRunning = True\n            completions = None\n            if (not view.find('scriptname', 0, sublime.IGNORECASE)):\n                path = view.file_name()\n                if path:\n                    (_, name) = os.path.split(path)\n                    completions = [('scriptname\\tscript header', ('ScriptName %s' % name[:name.rfind('.')]))]\n                else:\n                    completions = [('scriptname\\tscript header', 'ScriptName ')]\n            else:\n                completions = self.Completions(view, prefix, locations)\n            if completions:\n                completions = list(set(completions))\n            elif (completions == None):\n                completions = []\n            completions = (completions, (sublime.INHIBIT_WORD_COMPLETIONS | sublime.INHIBIT_EXPLICIT_COMPLETIONS))\n            self.completionRunning = False\n            return completions\n", "label": 1}
{"function": "\n\ndef create_cluster(self, api=True, exclude=None, **kwargs):\n    cluster_data = {\n        'name': ('cluster-api-' + str(randint(0, 1000000))),\n    }\n    editable_attributes = kwargs.pop('editable_attributes', None)\n    vmware_attributes = kwargs.pop('vmware_attributes', None)\n    if kwargs:\n        cluster_data.update(kwargs)\n    if ('release_id' not in cluster_data):\n        cluster_data['release_id'] = self.create_release(api=False).id\n    if (exclude and isinstance(exclude, list)):\n        for ex in exclude:\n            try:\n                del cluster_data[ex]\n            except KeyError as err:\n                logger.warning(err)\n    if api:\n        resp = self.app.post(reverse('ClusterCollectionHandler'), jsonutils.dumps(cluster_data), headers=self.default_headers, expect_errors=True)\n        self.tester.assertEqual(resp.status_code, 201, resp.body)\n        cluster = resp.json_body\n        cluster_db = Cluster.get_by_uid(cluster['id'])\n    else:\n        cluster = Cluster.create(cluster_data)\n        cluster_db = cluster\n        db().commit()\n    self.clusters.append(cluster_db)\n    if editable_attributes:\n        Cluster.patch_attributes(cluster_db, {\n            'editable': editable_attributes,\n        })\n    if vmware_attributes:\n        Cluster.update_vmware_attributes(cluster_db, vmware_attributes)\n    return cluster_db\n", "label": 1}
{"function": "\n\ndef _get_result_wrapper(self):\n    if self._tuples:\n        return self.database.get_result_wrapper(RESULTS_TUPLES)\n    elif self._dicts:\n        return self.database.get_result_wrapper(RESULTS_DICTS)\n    elif self._aggregate_rows:\n        return self.database.get_result_wrapper(RESULTS_AGGREGATE_MODELS)\n    has_joins = (self.lhs._joins or self.rhs._joins)\n    is_naive = (self.lhs._naive or self.rhs._naive or self._naive)\n    if (is_naive or (not has_joins) or self.verify_naive()):\n        return self.database.get_result_wrapper(RESULTS_NAIVE)\n    else:\n        return self.database.get_result_wrapper(RESULTS_MODELS)\n", "label": 1}
{"function": "\n\ndef get_font_sizes_dict(tree, styles_dict):\n    font_sizes_dict = defaultdict(int)\n    for p in tree.xpath('//w:p', namespaces=tree.nsmap):\n        if is_natural_header(p, styles_dict):\n            continue\n        if _is_li(p):\n            continue\n        font_size = get_font_size(p, styles_dict)\n        if (font_size is None):\n            continue\n        font_sizes_dict[font_size] += 1\n    most_used_font_size = (- 1)\n    highest_count = (- 1)\n    for (size, count) in font_sizes_dict.items():\n        if (count > highest_count):\n            highest_count = count\n            most_used_font_size = size\n    result = {\n        \n    }\n    for size in font_sizes_dict:\n        if (size is None):\n            continue\n        if (int(size) > int(most_used_font_size)):\n            result[size] = 'h2'\n        else:\n            result[size] = None\n    return result\n", "label": 1}
{"function": "\n\n@classmethod\ndef _session_flush(cls, session, flush_context, instances):\n    for obj in session.deleted:\n        class_ = obj.__class__\n        tracked_columns = cls.mapped_entities.get(class_, tuple())\n        for col in tracked_columns:\n            value = getattr(obj, col)\n            if (value is not None):\n                session._depot_old = getattr(session, '_depot_old', set())\n                session._depot_old.update(value.files)\n    for obj in session.new.union(session.dirty):\n        class_ = obj.__class__\n        tracked_columns = cls.mapped_entities.get(class_, tuple())\n        for col in tracked_columns:\n            history = get_history(obj, col)\n            added_files = itertools.chain(*(f.files for f in history.added if (f is not None)))\n            deleted_files = itertools.chain(*(f.files for f in history.deleted if (f is not None)))\n            session._depot_new = getattr(session, '_depot_new', set())\n            session._depot_new.update(added_files)\n            session._depot_old = getattr(session, '_depot_old', set())\n            session._depot_old.update(deleted_files)\n", "label": 1}
{"function": "\n\ndef get_ordering(self, request=None, queryset=None):\n    \"\\n        Returns the list of ordering fields for the change list.\\n        First we check the get_ordering() method in model admin, then we check\\n        the object's default ordering. Then, any manually-specified ordering\\n        from the query string overrides anything. Finally, a deterministic\\n        order is guaranteed by ensuring the primary key is used as the last\\n        ordering field.\\n        \"\n    if (queryset is None):\n        return super(DocumentChangeList, self).get_ordering()\n    params = self.params\n    ordering = list((self.model_admin.get_ordering(request) or self._get_default_ordering()))\n    if (ORDER_VAR in params):\n        ordering = []\n        order_params = params[ORDER_VAR].split('.')\n        for p in order_params:\n            try:\n                (none, pfx, idx) = p.rpartition('-')\n                field_name = self.list_display[int(idx)]\n                order_field = self.get_ordering_field(field_name)\n                if (not order_field):\n                    continue\n                ordering.append((pfx + order_field))\n            except (IndexError, ValueError):\n                continue\n    sign = (lambda t: (((t[1] > 0) and '+') or '-'))\n    qs_ordering = [(sign(t) + t[0]) for t in queryset._ordering]\n    ordering.extend(qs_ordering)\n    pk_name = self.lookup_opts.pk.name\n    if (not (set(ordering) & set(['pk', '-pk', pk_name, ('-' + pk_name)]))):\n        ordering.append('pk')\n    return ordering\n", "label": 1}
{"function": "\n\ndef test_conflicting_deps():\n    (_, _, conflicting_tree) = venv_fixture('tests/virtualenvs/unsatisfiedenv.pickle')\n    flask = next((x for x in conflicting_tree.keys() if (x.key == 'flask')))\n    jinja = next((x for x in conflicting_tree[flask] if (x.key == 'jinja2')))\n    uritemplate = next((x for x in conflicting_tree.keys() if (x.key == 'uritemplate')))\n    simplejson = next((x for x in conflicting_tree[uritemplate] if (x.key == 'simplejson')))\n    assert jinja\n    assert flask\n    assert uritemplate\n    assert simplejson\n    unsatisfied = conflicting_deps(conflicting_tree)\n    assert (unsatisfied == {\n        flask: [jinja],\n        uritemplate: [simplejson],\n    })\n", "label": 1}
{"function": "\n\ndef plugin_loaded():\n    'Called directly from sublime on plugin load.'\n    try:\n        os.makedirs(((sublime.cache_path() + os.path.sep) + 'Xkcd'))\n    except OSError as e:\n        if (e.errno != errno.EEXIST):\n            raise IOError('Error encountered during file creation.')\n    settings = sublime.load_settings('Package Control.sublime-settings')\n    http_proxy = settings.get('http_proxy')\n    https_proxy = settings.get('https_proxy')\n    proxies = {\n        \n    }\n    if (http_proxy or https_proxy):\n        if http_proxy:\n            proxies['http'] = http_proxy\n        if https_proxy:\n            proxies['https'] = https_proxy\n    proxy_username = settings.get('proxy_username')\n    proxy_password = settings.get('proxy_password')\n    if (proxy_username and proxy_password):\n        if http_proxy:\n            proxies['http'] = ((((proxy_username + ':') + proxy_password) + '@') + http_proxy)\n        if https_proxy:\n            proxies['https'] = ((((proxy_username + ':') + proxy_password) + '@') + https_proxy)\n    global opener\n    opener = urllib.request.build_opener(urllib.request.ProxyHandler(proxies))\n    urllib.request.install_opener(opener)\n", "label": 1}
{"function": "\n\ndef check_operands_size(instr, arch_size):\n    if (instr.mnemonic in [ReilMnemonic.ADD, ReilMnemonic.SUB, ReilMnemonic.MUL, ReilMnemonic.DIV, ReilMnemonic.MOD, ReilMnemonic.BSH, ReilMnemonic.AND, ReilMnemonic.OR, ReilMnemonic.XOR]):\n        assert (instr.operands[0].size == instr.operands[1].size), ('Invalid operands size: %s' % instr)\n    elif (instr.mnemonic in [ReilMnemonic.LDM]):\n        assert (instr.operands[0].size == arch_size), ('Invalid operands size: %s' % instr)\n    elif (instr.mnemonic in [ReilMnemonic.STM]):\n        assert (instr.operands[2].size == arch_size), ('Invalid operands size: %s' % instr)\n    elif (instr.mnemonic in [ReilMnemonic.STR]):\n        pass\n    elif (instr.mnemonic in [ReilMnemonic.BISZ]):\n        pass\n    elif (instr.mnemonic in [ReilMnemonic.JCC]):\n        assert (instr.operands[2].size == (arch_size + 8)), ('Invalid operands size: %s' % instr)\n        pass\n    elif (instr.mnemonic in [ReilMnemonic.UNKN]):\n        pass\n    elif (instr.mnemonic in [ReilMnemonic.UNDEF]):\n        pass\n    elif (instr.mnemonic in [ReilMnemonic.NOP]):\n        pass\n    elif (instr.mnemonic in [ReilMnemonic.SEXT]):\n        assert (instr.operands[0].size <= instr.operands[2].size), ('Invalid operands size: %s' % instr)\n", "label": 1}
{"function": "\n\ndef _scan_from_end(self, grpos_range, frames=None, flag_mask=0):\n    if (not self._s.data):\n        return None\n    pos = (len(self._s.data) - 1)\n    if (grpos_range > 0):\n        grpos = self._s.data[pos][0]\n        target_grpos = (grpos - grpos_range)\n        while (pos > 0):\n            pos -= 1\n            f = self._s.data[pos]\n            if (f[0] < target_grpos):\n                pos += 1\n                break\n    elif (frames > 0):\n        pos = max(0, (len(self._s.data) - frames))\n    if (flag_mask < 0):\n        mask = (- flag_mask)\n        fpos = pos\n        while (fpos >= 0):\n            f = self._s.data[fpos]\n            if (f[1] & mask):\n                break\n            fpos -= 1\n        if (fpos >= 0):\n            pos = fpos\n    elif (flag_mask > 0):\n        mask = flag_mask\n        fpos = pos\n        end_pos = (len(self._s.data) - 1)\n        while (fpos <= end_pos):\n            f = self._s.data[fpos]\n            if (f[1] & mask):\n                break\n            fpos += 1\n        if (fpos <= end_pos):\n            pos = fpos\n    return (self._s.data_offset + pos)\n", "label": 1}
{"function": "\n\ndef get_backoff(self, ngram):\n    assert (len(ngram) == 3)\n    ngram = tuple(ngram)\n    if (ngram in self.lm):\n        return 1.0\n    elif ((ngram[:2] in self.lm) and (ngram[1:] in self.lm)):\n        return 0.8\n    elif (ngram[1:] in self.lm):\n        return 0.6\n    elif ((ngram[:2] in self.lm) and (ngram[2:] in self.lm)):\n        return 0.4\n    elif ((ngram[1:2] in self.lm) and (ngram[2:] in self.lm)):\n        return 0.3\n    elif (ngram[2:] in self.lm):\n        return 0.2\n    else:\n        return 0.1\n", "label": 1}
{"function": "\n\ndef description_meta(missing_files, counter, url, meta_path):\n    description = ''\n    if ('description' in missing_files):\n        try:\n            description = request.json[('description' + str(counter))]\n        except:\n            pass\n        if ((not ('.' in url)) or (not ('git' in url))):\n            if (description == ''):\n                description = request.json['indexDesc']\n            if (not path.exists(meta_path)):\n                mkdir(meta_path)\n            with open(((meta_path + '/') + app.config['SERVICE_DICT']['description']), 'w') as f:\n                f.write(description)\n        elif (url.rsplit('.', 1)[1] == 'git'):\n            with open(((meta_path + '/') + app.config['SERVICE_DICT']['description']), 'w') as f:\n                f.write(description)\n    elif ((not ('.' in url)) or (not ('git' in url))):\n        if (description == ''):\n            description = request.json['indexDesc']\n        if (not path.exists(meta_path)):\n            mkdir(meta_path)\n        with open(((meta_path + '/') + app.config['SERVICE_DICT']['description']), 'w') as f:\n            f.write(description)\n", "label": 1}
{"function": "\n\n@unittest.skip(\"Disabled: orphan subnets don't migrate because tenant is specified in filter.\")\ndef test_subnets_exist_on_dst(self):\n    \"Validate deleted tenant's subnets were migrated.\"\n    tenants_subnets = []\n    for (tenant_name, tenant) in self.deleted_tenants:\n        all_subnets = self.dst_cloud.neutronclient.list_subnets()\n        dst_admin_subnets = [subnet for subnet in all_subnets['subnets'] if (subnet['tenant_id'] == self.dst_tenants[self.dst_cloud.tenant])]\n        net_list = []\n        for network in tenant['networks']:\n            net_list.append(network['subnets'])\n        src_tenant_subnets_list = sum(net_list, [])\n        migrated_subnets = []\n        for src_subnet in src_tenant_subnets_list:\n            for dst_subnet in dst_admin_subnets:\n                if (src_subnet['name'] == dst_subnet['name']):\n                    migrated_subnets.append(src_subnet['name'])\n        src_tenant_net_names = [subnet['name'] for subnet in src_tenant_subnets_list]\n        non_migrated_subnets = (set(src_tenant_net_names) ^ set(migrated_subnets))\n        if non_migrated_subnets:\n            tenants_subnets.append({\n                tenant_name: non_migrated_subnets,\n            })\n    if tenants_subnets:\n        msg = \"Tenant's subnets do not exist on destination, but should be!\"\n        self.fail(msg.format(tenants_subnets))\n", "label": 1}
{"function": "\n\ndef __init__(self):\n    if is_port_listening(TEST_PORT):\n        for i in range(2):\n            logging.warn('Existing daemon found. Asking it to exit')\n            try:\n                conn = httplib.HTTPConnection('localhost', TEST_PORT, True)\n                conn.request('GET', '/exit')\n            except:\n                break\n            res = conn.getresponse()\n            if (res.status != 200):\n                break\n            else:\n                time.sleep(0.2)\n    if is_port_listening(TEST_PORT):\n        raise Exception('Daemon running')\n    self.daemon_settings_file = tempfile.NamedTemporaryFile()\n    args = ['./quickopend', 'run', '--settings', self.daemon_settings_file.name, '--port', str(TEST_PORT)]\n    args.append('--test')\n    self.proc = subprocess.Popen(args)\n    ok = False\n    for i in range(10):\n        try:\n            conn = httplib.HTTPConnection('localhost', TEST_PORT, True)\n            conn.request('GET', '/ping')\n        except:\n            time.sleep(0.1)\n            continue\n        res = conn.getresponse()\n        if (res.status != 200):\n            ok = False\n            break\n        if (json.loads(res.read()) != 'pong'):\n            ok = False\n            break\n        ok = True\n        break\n    if (not ok):\n        raise Exception('Daemon did not come up')\n    self._conn = None\n    self._db_proxy = None\n", "label": 1}
{"function": "\n\ndef test_should_return_list_of_params(self):\n    ret = sdg.generate_separated_model_parameters('continuous', 0.5, self.num_clusters, self.get_next_seed)\n    assert isinstance(ret, list)\n    assert (len(ret) == self.num_clusters)\n    for entry in ret:\n        assert isinstance(entry, dict)\n        for key in entry.keys():\n            assert (key in ['mu', 'rho'])\n        assert (len(entry.keys()) == 2)\n    ret = sdg.generate_separated_model_parameters('multinomial', 0.5, self.num_clusters, self.get_next_seed, distargs=self.distargs_multinomial)\n    assert isinstance(ret, list)\n    assert (len(ret) == self.num_clusters)\n    for entry in ret:\n        assert isinstance(entry, dict)\n        for key in entry.keys():\n            assert (key in ['weights'])\n        assert (len(entry.keys()) == 1)\n", "label": 1}
{"function": "\n\ndef _read_rrd(self, rrd_path, hostname, device_name):\n    ' Main metric fetching method '\n    metric_count = 0\n    try:\n        info = rrdtool.info(rrd_path)\n    except Exception:\n        self.log.exception(('Unable to read RRD file at %s' % rrd_path))\n        return metric_count\n    c_funcs = set([v for (k, v) in info.items() if k.endswith('.cf')])\n    for c in list(c_funcs):\n        last_ts_key = ('%s.%s' % (rrd_path, c))\n        if (last_ts_key not in self.last_ts):\n            self.last_ts[last_ts_key] = int(time.time())\n            continue\n        start = self.last_ts[last_ts_key]\n        last_ts = start\n        try:\n            fetched = rrdtool.fetch(rrd_path, c, '--start', str(start))\n        except rrdtool.error:\n            self.log.warn(('Time %s out of range for %s' % (rrd_path, start)))\n            return metric_count\n        (start_ts, end_ts, interval) = fetched[0]\n        metric_names = fetched[1]\n        points = fetched[2]\n        for (k, m_name) in enumerate(metric_names):\n            m_name = self._format_metric_name(m_name, c)\n            for (i, p) in enumerate(points):\n                ts = (start_ts + (i * interval))\n                if (p[k] is None):\n                    continue\n                val = self._transform_metric(m_name, p[k])\n                self.gauge(m_name, val, hostname=hostname, device_name=device_name, timestamp=ts)\n                metric_count += 1\n                last_ts = (ts + interval)\n        self.last_ts[last_ts_key] = last_ts\n    return metric_count\n", "label": 1}
{"function": "\n\ndef product_mul(self, other, method=0):\n    'Helper function for Product simplification'\n    from sympy.concrete.products import Product\n    if (type(self) == type(other)):\n        if (method == 0):\n            if (self.limits == other.limits):\n                return Product((self.function * other.function), *self.limits)\n        elif (method == 1):\n            if (simplify((self.function - other.function)) == 0):\n                if (len(self.limits) == len(other.limits) == 1):\n                    i = self.limits[0][0]\n                    x1 = self.limits[0][1]\n                    y1 = self.limits[0][2]\n                    j = other.limits[0][0]\n                    x2 = other.limits[0][1]\n                    y2 = other.limits[0][2]\n                    if (i == j):\n                        if (x2 == (y1 + 1)):\n                            return Product(self.function, (i, x1, y2))\n                        elif (x1 == (y2 + 1)):\n                            return Product(self.function, (i, x2, y1))\n    return Mul(self, other)\n", "label": 1}
{"function": "\n\ndef items(self, srs=None, profile=None):\n    'supports dict-like items() access'\n    items = []\n    if ((not srs) and (not profile)):\n        for item in self.contents:\n            items.append((item, self.contents[item]))\n    elif (srs and profile):\n        for item in self.contents:\n            if ((self.contents[item].srs == srs) and (self.contents[item].profile == profile)):\n                items.append((item, self.contents[item]))\n    elif srs:\n        for item in self.contents:\n            if (self.contents[item].srs == srs):\n                items.append((item, self.contents[item]))\n    elif profile:\n        for item in self.contents:\n            if (self.contents[item].profile == profile):\n                items.append((item, self.contents[item]))\n    return items\n", "label": 1}
{"function": "\n\ndef _comparison(self, op, value):\n    if (op != '='):\n        raise datastore_errors.BadFilterError('StructuredProperty filter can only use ==')\n    if (not self._indexed):\n        raise datastore_errors.BadFilterError(('Cannot query for unindexed StructuredProperty %s' % self._name))\n    from .query import ConjunctionNode, PostFilterNode\n    from .query import RepeatedStructuredPropertyPredicate\n    if (value is None):\n        from .query import FilterNode\n        return FilterNode(self._name, op, value)\n    value = self._do_validate(value)\n    value = self._call_to_base_type(value)\n    filters = []\n    match_keys = []\n    for prop in self._modelclass._properties.itervalues():\n        vals = prop._get_base_value_unwrapped_as_list(value)\n        if prop._repeated:\n            if vals:\n                raise datastore_errors.BadFilterError(('Cannot query for non-empty repeated property %s' % prop._name))\n            continue\n        assert (isinstance(vals, list) and (len(vals) == 1)), repr(vals)\n        val = vals[0]\n        if (val is not None):\n            altprop = getattr(self, prop._code_name)\n            filt = altprop._comparison(op, val)\n            filters.append(filt)\n            match_keys.append(altprop._name)\n    if (not filters):\n        raise datastore_errors.BadFilterError('StructuredProperty filter without any values')\n    if (len(filters) == 1):\n        return filters[0]\n    if self._repeated:\n        pb = value._to_pb(allow_partial=True)\n        pred = RepeatedStructuredPropertyPredicate(match_keys, pb, (self._name + '.'))\n        filters.append(PostFilterNode(pred))\n    return ConjunctionNode(*filters)\n", "label": 1}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef _start(self):\n    'Start.\\n        '\n    if self.pending_socket_event:\n        return\n    if (not self.is_stopped()):\n        if (len(self.processes) < self.numprocesses):\n            self.reap_processes()\n            (yield self.spawn_processes())\n        return\n    found_wids = len(self._found_wids)\n    if ((not self._found_wids) and (not self.call_hook('before_start'))):\n        logger.debug('Aborting startup')\n        return\n    self._status = 'starting'\n    if (self.stdout_stream and hasattr(self.stdout_stream, 'open')):\n        self.stdout_stream.open()\n    if (self.stderr_stream and hasattr(self.stderr_stream, 'open')):\n        self.stderr_stream.open()\n    self._create_redirectors()\n    self.reap_processes()\n    (yield self.spawn_processes())\n    if ((not self.processes) or (not self.call_hook('after_start'))):\n        logger.debug('Aborting startup')\n        (yield self._stop(True))\n        return\n    self._status = 'active'\n    if found_wids:\n        logger.info(('%s already running' % self.name))\n    else:\n        logger.info(('%s started' % self.name))\n    self.notify_event('start', {\n        'time': time.time(),\n    })\n", "label": 1}
{"function": "\n\n@internationalizeDocstring\ndef encode(self, irc, msg, args, encoding, text):\n    '<encoding> <text>\\n\\n        Returns an encoded form of the given text; the valid encodings are\\n        available in the documentation of the Python codecs module:\\n        <http://docs.python.org/library/codecs.html#standard-encodings>.\\n        '\n    if (encoding in 'base64 bz2 hex quopri uu zlib'):\n        encoding += '_codec'\n    if encoding.endswith('_codec'):\n        text = text.encode()\n    try:\n        encoder = codecs.getencoder(encoding)\n    except LookupError:\n        irc.errorInvalid(_('encoding'), encoding)\n    text = encoder(text)[0]\n    if (encoding.endswith('_codec') and (encoding != 'base64_codec')):\n        text = codecs.getencoder('base64_codec')(text)[0].decode()\n    if (minisix.PY2 and isinstance(text, unicode)):\n        text = text.encode('utf-8')\n    elif (minisix.PY3 and isinstance(text, bytes)):\n        text = text.decode()\n    if (encoding in ('base64', 'base64_codec')):\n        text = text.replace('\\n', '')\n    irc.reply(text.rstrip('\\n'))\n", "label": 1}
{"function": "\n\ndef get_tree(self, name=None, backup=None, root=None, agent=None):\n    'See :func:`burpui.misc.backend.interface.BUIbackend.get_tree`'\n    ret = []\n    if ((not name) or (not backup)):\n        return ret\n    if (not root):\n        top = ''\n    else:\n        try:\n            top = root.decode('utf-8', 'replace')\n        except UnicodeDecodeError:\n            top = root\n    query = self.status('c:{0}:b:{1}:p:{2}\\n'.format(name, backup, top))\n    if (not query):\n        return ret\n    try:\n        backup = query['clients'][0]['backups'][0]\n    except KeyError as e:\n        return ret\n    for entry in backup['browse']['entries']:\n        data = {\n            \n        }\n        if (entry['name'] == '.'):\n            continue\n        else:\n            data['name'] = entry['name']\n        data['mode'] = self._human_st_mode(entry['mode'])\n        if re.match('^(d|l)', data['mode']):\n            data['type'] = 'd'\n        else:\n            data['type'] = 'f'\n        data['inodes'] = entry['nlink']\n        data['uid'] = entry['uid']\n        data['gid'] = entry['gid']\n        data['parent'] = top\n        data['size'] = '{0:.1eM}'.format(_hr(entry['size']))\n        data['date'] = entry['mtime']\n        ret.append(data)\n    return ret\n", "label": 1}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef _reload(self, graceful=True, sequential=False):\n    ' reload\\n        '\n    if ((not graceful) and sequential):\n        logger.warn('with graceful=False, sequential=True is ignored')\n    if (self.prereload_fn is not None):\n        self.prereload_fn(self)\n    if (not graceful):\n        (yield self._restart())\n        return\n    if self.is_stopped():\n        (yield self._start())\n    elif self.send_hup:\n        for process in self.processes.values():\n            logger.info(('SENDING HUP to %s' % process.pid))\n            process.send_signal(signal.SIGHUP)\n    elif sequential:\n        active_processes = self.get_active_processes()\n        for process in active_processes:\n            (yield self.kill_process(process))\n            self.reap_process(process.pid)\n            self.spawn_process()\n            (yield tornado_sleep(self.warmup_delay))\n    else:\n        for i in range(self.numprocesses):\n            self.spawn_process()\n        (yield self.manage_processes())\n    self.notify_event('reload', {\n        'time': time.time(),\n    })\n    logger.info('%s reloaded', self.name)\n", "label": 1}
{"function": "\n\ndef url(self, name, force=False):\n    '\\n        Returns the real URL in DEBUG mode.\\n        '\n    if (settings.DEBUG and (not force)):\n        (hashed_name, fragment) = (name, '')\n    else:\n        (clean_name, fragment) = urldefrag(name)\n        cache_key = self.cache_key(name)\n        hashed_name = self.cache.get(cache_key)\n        if (hashed_name is None):\n            hashed_name = self.hashed_name(clean_name).replace('\\\\', '/')\n            self.cache.set(cache_key, hashed_name)\n    final_url = super(CachedFilesMixin, self).url(hashed_name)\n    query_fragment = ('?#' in name)\n    if (fragment or query_fragment):\n        urlparts = list(urlsplit(final_url))\n        if (fragment and (not urlparts[4])):\n            urlparts[4] = fragment\n        if (query_fragment and (not urlparts[3])):\n            urlparts[2] += '?'\n        final_url = urlunsplit(urlparts)\n    return unquote(final_url)\n", "label": 1}
{"function": "\n\ndef installed_packages(local=False):\n    installed = []\n    for dist in get_installed_distributions(local_only=local):\n        pym = PyModule(dist.project_name, dist.version, dist.location)\n        if dist.has_metadata('top_level.txt'):\n            pym.set_import_names(list(dist.get_metadata_lines('top_level.txt')))\n        pym.local = dist_is_local(dist)\n        pym.user = dist_in_usersite(dist)\n        pym._dependencies = [dep.project_name for dep in dist.requires()]\n        for filename in iter_dist_files(dist):\n            if (not filename.startswith(dist.location)):\n                if is_script(filename):\n                    pym.installed_scripts.append(filename)\n                else:\n                    pym.installed_files.append(filename)\n        if (pym.installed_scripts or (pym.name in ignore_packages)):\n            pym.hidden = True\n        installed.append(pym)\n    for pym in installed[:]:\n        for dep in pym._dependencies:\n            if (dep == 'argparse'):\n                continue\n            pymc = find_package(dep, installed, True)\n            if (not pymc):\n                pymc = PyModule(dep, 'MISSING', missing=True)\n                installed.append(pymc)\n            pymc.add_dependant(pym)\n            pym.add_dependency(pymc)\n    return installed\n", "label": 1}
{"function": "\n\ndef print_usage(actions):\n    'Print the usage information.  (Help screen)'\n    actions = actions.items()\n    actions.sort()\n    print(('usage: %s <action> [<options>]' % basename(sys.argv[0])))\n    print(('       %s --help' % basename(sys.argv[0])))\n    print()\n    print('actions:')\n    for (name, (func, doc, arguments)) in actions:\n        print(('  %s:' % name))\n        for line in doc.splitlines():\n            print(('    %s' % line))\n        if arguments:\n            print()\n        for (arg, shortcut, default, argtype) in arguments:\n            if isinstance(default, bool):\n                print(('    %s' % ((((shortcut and ('-%s, ' % shortcut)) or '') + '--') + arg)))\n            else:\n                print(('    %-30s%-10s%s' % (((((shortcut and ('-%s, ' % shortcut)) or '') + '--') + arg), argtype, default)))\n        print()\n", "label": 1}
{"function": "\n\ndef thread_worker(self):\n    self.idle_event.clear()\n    while True:\n        try:\n            (action, data) = self.input_queue.get(block=False)\n        except Empty:\n            self.idle_event.set()\n            time.sleep(0.1)\n            self.idle_event.clear()\n        else:\n            assert (action in ('load', 'save'))\n            if (action == 'load'):\n                (task, grab) = data\n                result = None\n                if self.is_cache_loading_allowed(task, grab):\n                    result = self.load_from_cache(task, grab)\n                if result:\n                    self.result_queue.put(('network_result', result))\n                else:\n                    self.result_queue.put(('task', task))\n            elif (action == 'save'):\n                (task, grab) = data\n                if self.is_cache_saving_allowed(task, grab):\n                    with self.spider.timer.log_time('cache'):\n                        with self.spider.timer.log_time('cache.write'):\n                            self.cache.save_response(task.url, grab)\n", "label": 1}
{"function": "\n\ndef check_model_params_dict(params):\n    if (type(params) is not dict):\n        raise TypeError('params should be a dict')\n    keys = ['mu', 'kappa']\n    for key in keys:\n        if (key not in params):\n            raise KeyError(('missing key in params: %r' % (key,)))\n    for (key, value) in six.iteritems(params):\n        if (key not in keys):\n            raise KeyError(('invalid params key: %r' % (key,)))\n        if (not isinstance(value, (float, numpy.float64))):\n            raise TypeError(('%r should be float' % (key,)))\n        if (key == 'kappa'):\n            if (value <= 0.0):\n                raise ValueError('kappa should be greater than 0')\n        elif (key != 'mu'):\n            raise KeyError(('Invalid params key: %r' % (key,)))\n        elif ((value < 0.0) or (value > (2 * pi))):\n            raise ValueError('mu should be in [0,2*pi]')\n", "label": 1}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    'Defines how calling the function locally should be handled'\n    for requirement in self.requires:\n        lacks_requirement = self.check_requirements()\n        if lacks_requirement:\n            return (self.outputs(lacks_requirement) if self.outputs else lacks_requirement)\n    for (index, argument) in enumerate(args):\n        kwargs[self.parameters[index]] = argument\n    if (not getattr(self, 'skip_directives', False)):\n        for (parameter, directive) in self.directives.items():\n            if (parameter in kwargs):\n                continue\n            arguments = ((self.defaults[parameter],) if (parameter in self.defaults) else ())\n            kwargs[parameter] = directive(*arguments, api=self.api, api_version=self.version, interface=self)\n    if (not getattr(self, 'skip_validation', False)):\n        errors = self.validate(kwargs)\n        if errors:\n            errors = {\n                'errors': errors,\n            }\n            if getattr(self, 'on_invalid', False):\n                errors = self.on_invalid(errors)\n            outputs = getattr(self, 'invalid_outputs', self.outputs)\n            return (outputs(errors) if outputs else errors)\n    result = self.interface(**kwargs)\n    if self.transform:\n        result = self.transform(result)\n    return (self.outputs(result) if self.outputs else result)\n", "label": 1}
{"function": "\n\ndef gather_parameters(self, request, response, api_version=None, **input_parameters):\n    'Gathers and returns all parameters that will be used for this endpoint'\n    input_parameters.update(request.params)\n    if (self.parse_body and (request.content_length is not None)):\n        body = request.stream\n        (content_type, encoding) = separate_encoding(request.content_type)\n        body_formatter = (body and self.api.http.input_format(content_type))\n        if body_formatter:\n            body = (body_formatter(body, encoding) if (encoding is not None) else body_formatter(body))\n        if ('body' in self.parameters):\n            input_parameters['body'] = body\n        if isinstance(body, dict):\n            input_parameters.update(body)\n    elif ('body' in self.parameters):\n        input_parameters['body'] = None\n    if ('request' in self.parameters):\n        input_parameters['request'] = request\n    if ('response' in self.parameters):\n        input_parameters['response'] = response\n    if ('api_version' in self.parameters):\n        input_parameters['api_version'] = api_version\n    for (parameter, directive) in self.directives.items():\n        arguments = ((self.defaults[parameter],) if (parameter in self.defaults) else ())\n        input_parameters[parameter] = directive(*arguments, response=response, request=request, api=self.api, api_version=api_version, interface=self)\n    return input_parameters\n", "label": 1}
{"function": "\n\ndef _parse_headers(self, lines):\n    lastheader = ''\n    lastvalue = []\n    for (lineno, line) in enumerate(lines):\n        if (line[0] in ' \\t'):\n            if (not lastheader):\n                defect = errors.FirstHeaderLineIsContinuationDefect(line)\n                self._cur.defects.append(defect)\n                continue\n            lastvalue.append(line)\n            continue\n        if lastheader:\n            lhdr = EMPTYSTRING.join(lastvalue)[:(- 1)].rstrip('\\r\\n')\n            self._cur[lastheader] = lhdr\n            (lastheader, lastvalue) = ('', [])\n        if line.startswith('From '):\n            if (lineno == 0):\n                mo = NLCRE_eol.search(line)\n                if mo:\n                    line = line[:(- len(mo.group(0)))]\n                self._cur.set_unixfrom(line)\n                continue\n            elif (lineno == (len(lines) - 1)):\n                self._input.unreadline(line)\n                return\n            else:\n                defect = errors.MisplacedEnvelopeHeaderDefect(line)\n                self._cur.defects.append(defect)\n                continue\n        i = line.find(':')\n        if (i < 0):\n            defect = errors.MalformedHeaderDefect(line)\n            self._cur.defects.append(defect)\n            continue\n        lastheader = line[:i]\n        lastvalue = [line[(i + 1):].lstrip()]\n    if lastheader:\n        self._cur[lastheader] = EMPTYSTRING.join(lastvalue).rstrip('\\r\\n')\n", "label": 1}
{"function": "\n\ndef _find_parenthesis(self, position, forward=True):\n    \" If 'forward' is True (resp. False), proceed forwards\\n            (resp. backwards) through the line that contains 'position' until an\\n            unmatched closing (resp. opening) parenthesis is found. Returns a\\n            tuple containing the position of this parenthesis (or -1 if it is\\n            not found) and the number commas (at depth 0) found along the way.\\n        \"\n    commas = depth = 0\n    document = self._text_edit.document()\n    char = document.characterAt(position)\n    while ((category(char) != 'Cc') and (position > 0)):\n        if ((char == ',') and (depth == 0)):\n            commas += 1\n        elif (char == ')'):\n            if (forward and (depth == 0)):\n                break\n            depth += 1\n        elif (char == '('):\n            if ((not forward) and (depth == 0)):\n                break\n            depth -= 1\n        position += (1 if forward else (- 1))\n        char = document.characterAt(position)\n    else:\n        position = (- 1)\n    return (position, commas)\n", "label": 1}
{"function": "\n\ndef method_for(self):\n    for i in range(5):\n        for j in range(5):\n            if (j == 4):\n                break\n        else:\n            print('this should not show')\n    else:\n        print('ok')\n    for i in range(5):\n        if (i == 1):\n            break\n        for j in range(5):\n            pass\n        else:\n            print('ok')\n    else:\n        print('this should not show')\n", "label": 1}
{"function": "\n\ndef _check_mixed_float(df, dtype=None):\n    dtypes = dict(A='float32', B='float32', C='float16', D='float64')\n    if isinstance(dtype, compat.string_types):\n        dtypes = dict([(k, dtype) for (k, v) in dtypes.items()])\n    elif isinstance(dtype, dict):\n        dtypes.update(dtype)\n    if dtypes.get('A'):\n        assert (df.dtypes['A'] == dtypes['A'])\n    if dtypes.get('B'):\n        assert (df.dtypes['B'] == dtypes['B'])\n    if dtypes.get('C'):\n        assert (df.dtypes['C'] == dtypes['C'])\n    if dtypes.get('D'):\n        assert (df.dtypes['D'] == dtypes['D'])\n", "label": 1}
{"function": "\n\ndef _join_metadata(self):\n    src = self.model_from_alias(self.src)\n    dest = self.model_from_alias(self.dest)\n    join_alias = ((isinstance(self.on, Node) and self.on._alias) or None)\n    is_expression = isinstance(self.on, (Expression, Func, SQL))\n    on_field = ((isinstance(self.on, (Field, FieldProxy)) and self.on) or None)\n    if on_field:\n        fk_field = on_field\n        is_backref = (on_field.name not in src._meta.fields)\n    else:\n        (fk_field, is_backref) = self.get_foreign_key(src, dest, self.on)\n        if ((fk_field is None) and (self.on is not None)):\n            (fk_field, is_backref) = self.get_foreign_key(src, dest)\n    if (fk_field is not None):\n        primary_key = fk_field.to_field\n    else:\n        primary_key = None\n    if (not join_alias):\n        if (fk_field is not None):\n            if is_backref:\n                target_attr = dest._meta.db_table\n            else:\n                target_attr = fk_field.name\n        else:\n            try:\n                target_attr = self.on.lhs.name\n            except AttributeError:\n                target_attr = dest._meta.db_table\n    else:\n        target_attr = None\n    return JoinMetadata(src_model=src, dest_model=dest, src=self.src, dest=self.dest, attr=(join_alias or target_attr), primary_key=primary_key, foreign_key=fk_field, is_backref=is_backref, alias=join_alias, is_self_join=(src is dest), is_expression=is_expression)\n", "label": 1}
{"function": "\n\ndef badge_badge(request, format, nick):\n    view = api.actor_get(request.user, nick)\n    presence = api.presence_get(request.user, view.nick)\n    if (not presence):\n        line = 'Offline'\n        light = 'gray'\n        location = ''\n    else:\n        line = presence.extra.get('status', 'Offline')\n        light = presence.extra.get('light', 'gray')\n        location = presence.extra.get('location', '')\n    if (format == 'image'):\n        return http.HttpResponseRedirect(('/images/badge_%s.gif' % light))\n    if (format == 'js-small'):\n        multiline = (len(line) > 17)\n        truncated_line = (((len(line) > 30) and ('%s...' % line[:27])) or line)\n        content_type = 'text/javascript'\n        template_path = 'js_small.js'\n    elif ((format == 'js-medium') or (format == 'js-large')):\n        truncated_line = (((len(line) > 40) and ('%s...' % line[:27])) or line)\n        content_type = 'text/javascript'\n        template_path = ('%s.js' % format.replace('-', '_'))\n    elif (format == 'json'):\n        content_type = 'text/javascript'\n        template_path = 'badge.json'\n    elif (format == 'xml'):\n        content_type = 'application/xml'\n        template_path = 'badge.xml'\n    c = template.RequestContext(request, locals())\n    t = loader.get_template(('badge/templates/%s' % template_path))\n    r = http.HttpResponse(t.render(c))\n    r['Content-type'] = content_type\n    return r\n", "label": 1}
{"function": "\n\ndef _test_redis(what, verbose, redis_port, redis_passwd, redis_db, redis_host, stdout, stderr, exit_func):\n    if verbose:\n        print(('Checking redis connection for %s...' % what), end='', file=stdout)\n        stdout.flush()\n    kwargs = {\n        \n    }\n    if (redis_port is not None):\n        kwargs['port'] = redis_port\n    if (redis_passwd is not None):\n        kwargs['password'] = redis_passwd\n    if (redis_db is not None):\n        kwargs['db'] = redis_db\n    if (redis_host is not None):\n        kwargs['host'] = redis_host\n    try:\n        import redis\n    except ImportError:\n        print((\"redis selected for %s; but redis module is not available. Try installing it with 'pip install redis'\" % what), file=stderr)\n        exit_func((- 1))\n    else:\n        try:\n            client = redis.Redis(**kwargs)\n            client.get('this.should.not.exist')\n        except:\n            print(('redis selected for %s; but could not use the provided configuration' % what), file=stderr)\n            traceback.print_exc(file=stderr)\n            exit_func((- 1))\n        else:\n            if verbose:\n                print('[done]', file=stdout)\n", "label": 1}
{"function": "\n\ndef get_client(self, name=None, agent=None):\n    'See :func:`burpui.misc.backend.interface.BUIbackend.get_client`'\n    ret = []\n    if (not name):\n        return ret\n    query = self.status('c:{0}\\n'.format(name))\n    if (not query):\n        return ret\n    try:\n        backups = query['clients'][0]['backups']\n    except KeyError as e:\n        self._logger('warning', 'Client not found')\n        return ret\n    for backup in backups:\n        back = {\n            \n        }\n        if (('flags' in backup) and ('working' in backup['flags'])):\n            continue\n        back['number'] = backup['number']\n        if (('flags' in backup) and ('deletable' in backup['flags'])):\n            back['deletable'] = True\n        else:\n            back['deletable'] = False\n        back['date'] = backup['timestamp']\n        log = self.get_backup_logs(backup['number'], name)\n        try:\n            back['encrypted'] = log['encrypted']\n            try:\n                back['received'] = log['received']\n            except KeyError as e:\n                back['received'] = 0\n            try:\n                back['size'] = log['totsize']\n            except KeyError as e:\n                back['size'] = 0\n            back['end'] = log['end']\n            back['date'] = log['start']\n            ret.append(back)\n        except Exception as e:\n            self._logger('warning', 'Unable to parse logs')\n            pass\n    ret.reverse()\n    return ret\n", "label": 1}
{"function": "\n\ndef __call__(self, request, response, api_version=None, **kwargs):\n    'Call the wrapped function over HTTP pulling information as needed'\n    api_version = (int(api_version) if (api_version is not None) else api_version)\n    if (not self.catch_exceptions):\n        exception_types = ()\n    else:\n        exception_types = self.api.http.exception_handlers(api_version)\n        exception_types = (tuple(exception_types.keys()) if exception_types else ())\n    try:\n        self.set_response_defaults(response, request)\n        lacks_requirement = self.check_requirements(request, response)\n        if lacks_requirement:\n            response.data = self.outputs(lacks_requirement, **self._arguments(self._params_for_outputs, request, response))\n            return\n        input_parameters = self.gather_parameters(request, response, api_version, **kwargs)\n        errors = self.validate(input_parameters)\n        if errors:\n            return self.render_errors(errors, request, response)\n        self.render_content(self.call_function(**input_parameters), request, response, **kwargs)\n    except falcon.HTTPNotFound:\n        return self.api.http.not_found(request, response, **kwargs)\n    except exception_types as exception:\n        handler = None\n        if (type(exception) in exception_types):\n            handler = self.api.http.exception_handlers(api_version)[type(exception)]\n        else:\n            for (exception_type, exception_handler) in tuple(self.api.http.exception_handlers(api_version).items())[::(- 1)]:\n                if isinstance(exception, exception_type):\n                    handler = exception_handler\n        handler(request=request, response=response, exception=exception, **kwargs)\n", "label": 1}
{"function": "\n\ndef get(self, key, default=None, category=None, return_tagobj=False):\n    '\\n        Get the tag for the given key or list of tags.\\n\\n        Args:\\n            key (str or list): The tag or tags to retrieve.\\n            default (any, optional): The value to return in case of no match.\\n            category (str, optional): The Tag category to limit the\\n                request to. Note that `None` is the valid, default\\n                category.\\n            return_tagobj (bool, optional): Return the Tag object itself\\n                instead of a string representation of the Tag.\\n\\n        Returns:\\n            tags (str, TagObject or list): The matches, either string\\n                representations of the tags or the Tag objects themselves\\n                depending on `return_tagobj`.\\n\\n        '\n    if ((self._cache is None) or (not _TYPECLASS_AGGRESSIVE_CACHE)):\n        self._recache()\n    ret = []\n    category = (category.strip().lower() if (category is not None) else None)\n    searchkey = [(('%s-%s' % (key.strip().lower(), category)) if (key is not None) else None) for key in make_iter(key)]\n    ret = [val for val in (self._cache.get(keystr) for keystr in searchkey) if val]\n    ret = ([to_str(tag.db_data) for tag in ret] if return_tagobj else ret)\n    return (ret[0] if (len(ret) == 1) else (ret if ret else default))\n", "label": 1}
{"function": "\n\ndef filter(self, *args, **kwargs):\n    dq_node = Node()\n    if args:\n        dq_node &= reduce(operator.and_, [a.clone() for a in args])\n    if kwargs:\n        dq_node &= DQ(**kwargs)\n    q = deque([dq_node])\n    dq_joins = set()\n    while q:\n        curr = q.popleft()\n        if (not isinstance(curr, Expression)):\n            continue\n        for (side, piece) in (('lhs', curr.lhs), ('rhs', curr.rhs)):\n            if isinstance(piece, DQ):\n                (query, joins) = self.convert_dict_to_node(piece.query)\n                dq_joins.update(joins)\n                expression = reduce(operator.and_, query)\n                expression._negated = piece._negated\n                expression._alias = piece._alias\n                setattr(curr, side, expression)\n            else:\n                q.append(piece)\n    dq_node = dq_node.rhs\n    query = self.clone()\n    for field in dq_joins:\n        if isinstance(field, ForeignKeyField):\n            (lm, rm) = (field.model_class, field.rel_model)\n            field_obj = field\n        elif isinstance(field, ReverseRelationDescriptor):\n            (lm, rm) = (field.field.rel_model, field.rel_model)\n            field_obj = field.field\n        query = query.ensure_join(lm, rm, field_obj)\n    return query.where(dq_node)\n", "label": 1}
{"function": "\n\ndef toposort(data):\n    'Dependencies are expressed as a dictionary whose keys are items\\nand whose values are a set of dependent items. Output is a list of\\nsets in topological order. The first set consists of items with no\\ndependences, each subsequent set consists of items that depend upon\\nitems in the preceeding sets.\\n'\n    if (len(data) == 0):\n        return\n    data = data.copy()\n    for (k, v) in data.items():\n        v.discard(k)\n    extra_items_in_deps = (_reduce(set.union, data.values()) - set(data.keys()))\n    data.update({item: set() for item in extra_items_in_deps})\n    while True:\n        ordered = set((item for (item, dep) in data.items() if (len(dep) == 0)))\n        if (not ordered):\n            break\n        (yield ordered)\n        data = {item: (dep - ordered) for (item, dep) in data.items() if (item not in ordered)}\n    if (len(data) != 0):\n        raise ValueError('Cyclic dependencies exist among these items: {}'.format(', '.join((repr(x) for x in data.items()))))\n", "label": 1}
{"function": "\n\ndef update_job_status(db, job_id, job_url, filename, run_status, results, github_auth):\n    '\\n    '\n    try:\n        job = read_job(db, job_id)\n    except TypeError:\n        raise Exception('Job {} not found'.format(job_id))\n    if (filename not in job.states):\n        raise Exception('Unknown file from job {}: \"{}\"'.format(job.id, filename))\n    filenames = list(job.task_files.values())\n    job.states[filename] = run_status\n    job.file_results[filename] = results\n    if (False in job.states.values()):\n        job.status = False\n    elif (None in job.states.values()):\n        job.status = None\n    else:\n        job.status = True\n    write_job(db, job.id, job.status, job.task_files, job.states, job.file_results, job.github_owner, job.github_repository, job.github_status_url)\n    if (not job.github_status_url):\n        _L.warning('No status_url to tell about {} status of job {}'.format(job.status, job.id))\n        return\n    if (job.status is False):\n        bad_files = [name for (name, state) in job.states.items() if (state is False)]\n        update_failing_status(job.github_status_url, job_url, bad_files, filenames, github_auth)\n    elif (job.status is None):\n        update_pending_status(job.github_status_url, job_url, filenames, github_auth)\n    elif (job.status is True):\n        update_success_status(job.github_status_url, job_url, filenames, github_auth)\n", "label": 1}
{"function": "\n\ndef _configure_merge(self):\n    '\\n        Helper function for configuring shapes depending on the merge concatenation type\\n        '\n    in_shapes = [l.out_shape for l in self.layers]\n    if (self.merge == 'recurrent'):\n        catdims = [xs[1] for xs in in_shapes]\n        self.out_shape = (in_shapes[0][0], sum(catdims))\n        stride_size = self.be.bsz\n    elif (self.merge == 'depth'):\n        catdims = [xs[0] for xs in in_shapes]\n        self.out_shape = ((sum(catdims),) + in_shapes[0][1:])\n        stride_size = np.prod(in_shapes[0][1:])\n    elif (self.merge == 'stack'):\n        catdims = [(xs if isinstance(xs, int) else np.prod(xs)) for xs in in_shapes]\n        self.out_shape = sum(catdims)\n        stride_size = 1\n    end_idx = [(idx * stride_size) for idx in np.cumsum(catdims)]\n    start_idx = ([0] + end_idx[:(- 1)])\n    self.slices = [slice(s, e) for (s, e) in zip(start_idx, end_idx)]\n", "label": 1}
{"function": "\n\ndef filter_agents(agents, stack_outputs, override=None):\n    deployed_agents = {\n        \n    }\n    for agent in agents.values():\n        stack_values = _get_stack_values(stack_outputs, agent['id'], ['ip'])\n        if override:\n            stack_values.update(override(agent))\n        if (not stack_values.get('ip')):\n            LOG.info('Ignore non-deployed agent: %s', agent)\n            continue\n        agent.update(stack_values)\n        if ((agent.get('mode') == 'slave') and (not agent.get('ip'))):\n            LOG.info('IP address is missing in agent: %s', agent)\n            continue\n        deployed_agents[agent['id']] = agent\n    result = {\n        \n    }\n    for agent in deployed_agents.values():\n        if ((agent.get('mode') == 'alone') or ((agent.get('mode') == 'master') and (agent.get('slave_id') in deployed_agents)) or ((agent.get('mode') == 'slave') and (agent.get('master_id') in deployed_agents))):\n            result[agent['id']] = agent\n    return result\n", "label": 1}
{"function": "\n\ndef finish(self, chunk=None):\n    'Finishes this response, ending the HTTP request.'\n    if self._finished:\n        raise RuntimeError('finish() called twice.  May be caused by using async operations without the @asynchronous decorator.')\n    if (chunk is not None):\n        self.write(chunk)\n    if (not self._headers_written):\n        if ((self._status_code == 200) and (self.request.method in ('GET', 'HEAD')) and ('Etag' not in self._headers)):\n            self.set_etag_header()\n            if self.check_etag_header():\n                self._write_buffer = []\n                self.set_status(304)\n        if (self._status_code == 304):\n            assert (not self._write_buffer), 'Cannot send body with 304'\n            self._clear_headers_for_304()\n        elif ('Content-Length' not in self._headers):\n            content_length = sum((len(part) for part in self._write_buffer))\n            self.set_header('Content-Length', content_length)\n    if hasattr(self.request, 'connection'):\n        self.request.connection.set_close_callback(None)\n    self.flush(include_footers=True)\n    self.request.finish()\n    self._log()\n    self._finished = True\n    self.on_finish()\n    self.ui = None\n", "label": 1}
{"function": "\n\ndef save(self, force_insert=False, only=None):\n    field_dict = dict(self._data)\n    if (self._meta.primary_key is not False):\n        pk_field = self._meta.primary_key\n        pk_value = self._get_pk_value()\n    else:\n        pk_field = pk_value = None\n    if only:\n        field_dict = self._prune_fields(field_dict, only)\n    elif (self._meta.only_save_dirty and (not force_insert)):\n        field_dict = self._prune_fields(field_dict, self.dirty_fields)\n        if (not field_dict):\n            self._dirty.clear()\n            return False\n    self._populate_unsaved_relations(field_dict)\n    if ((pk_value is not None) and (not force_insert)):\n        if self._meta.composite_key:\n            for pk_part_name in pk_field.field_names:\n                field_dict.pop(pk_part_name, None)\n        else:\n            field_dict.pop(pk_field.name, None)\n        rows = self.update(**field_dict).where(self._pk_expr()).execute()\n    elif (pk_field is None):\n        self.insert(**field_dict).execute()\n        rows = 1\n    else:\n        pk_from_cursor = self.insert(**field_dict).execute()\n        if (pk_from_cursor is not None):\n            pk_value = pk_from_cursor\n        self._set_pk_value(pk_value)\n        rows = 1\n    self._dirty.clear()\n    return rows\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    vmatch = self.vmatch\n    vdict = self.vdict\n    with self.source.open('rb') as xmlf:\n        tree = etree.parse(xmlf)\n        if (not hasattr(tree, 'iterfind')):\n            tree.iterfind = tree.findall\n        if (vmatch is not None):\n            for rowelm in tree.iterfind(self.rmatch):\n                if (self.attr is None):\n                    getv = attrgetter('text')\n                else:\n                    getv = (lambda e: e.get(self.attr))\n                if isinstance(vmatch, string_types):\n                    velms = rowelm.findall(vmatch)\n                else:\n                    velms = itertools.chain(*[rowelm.findall(enm) for enm in vmatch])\n                (yield tuple((getv(velm) for velm in velms)))\n        else:\n            flds = tuple(sorted(map(text_type, vdict.keys())))\n            (yield flds)\n            vmatches = dict()\n            vgetters = dict()\n            for f in flds:\n                vmatch = self.vdict[f]\n                if isinstance(vmatch, string_types):\n                    vmatches[f] = vmatch\n                    vgetters[f] = element_text_getter(self.missing)\n                else:\n                    vmatches[f] = vmatch[0]\n                    attr = vmatch[1]\n                    vgetters[f] = attribute_text_getter(attr, self.missing)\n            for rowelm in tree.iterfind(self.rmatch):\n                (yield tuple((vgetters[f](rowelm.findall(vmatches[f])) for f in flds)))\n", "label": 1}
{"function": "\n\ndef request(url):\n    try:\n        if ('</regex>' in url):\n            import regex\n            url = regex.resolve(url)\n        rd = realdebrid.resolve(url)\n        if (not (rd == None)):\n            return rd\n        pz = premiumize.resolve(url)\n        if (not (pz == None)):\n            return pz\n        if url.startswith('rtmp'):\n            if (len(re.compile('\\\\s*timeout=(\\\\d*)').findall(url)) == 0):\n                url += ' timeout=10'\n            return url\n        u = client.shrink_host(url)\n        u = u.lower()\n        r = [i['class'] for i in info() if (u in i['netloc'])][0]\n        r = __import__(r, globals(), locals(), [], (- 1))\n        r = r.resolve(url)\n        if (r == None):\n            return r\n        elif (type(r) == list):\n            return r\n        try:\n            h = dict(urlparse.parse_qsl(r.rsplit('|', 1)[1]))\n        except:\n            h = dict('')\n        if (not ('User-Agent' in h)):\n            h['User-Agent'] = client.agent()\n        if (not ('Referer' in h)):\n            h['Referer'] = url\n        r = ('%s|%s' % (r.split('|')[0], urllib.urlencode(h)))\n        return r\n    except:\n        return url\n", "label": 1}
{"function": "\n\ndef make_temp(temp_dir, prefix, keep_this_file=None, keep_num=5):\n\n    def parse_num(path):\n        '\\n        parse the number out of a path (if it matches the prefix)\\n\\n        Borrowed from py.path\\n        '\n        if path.startswith(prefix):\n            try:\n                return int(path[len(prefix):])\n            except ValueError:\n                pass\n    lockfile = os.path.join(temp_dir, (prefix + 'lock'))\n    with FileLock(temp_dir, lockfile):\n        files = os.listdir(temp_dir)\n        oldest = None\n        youngest = None\n        for f in files:\n            num = parse_num(f)\n            if num:\n                if ((oldest is None) or (oldest > num)):\n                    oldest = num\n                if ((youngest is None) or (youngest < num)):\n                    youngest = num\n        if ((youngest is None) or (oldest is None)):\n            index = '1'\n        elif ((youngest - oldest) < keep_num):\n            index = str((youngest + 1))\n        else:\n            index = str((youngest + 1))\n            for i in range(oldest, (youngest - keep_num)):\n                to_delete = os.path.join(temp_dir, (prefix + str(i)))\n                if (to_delete != keep_this_file):\n                    rmtree(os.path.join(temp_dir, (prefix + str(i))), True)\n        temp = os.path.join(temp_dir, (prefix + index))\n        mkdirp(temp)\n        return temp\n", "label": 1}
{"function": "\n\n@spin_first\ndef abort(self, jobs=None, targets=None, block=None):\n    'Abort specific jobs from the execution queues of target(s).\\n        \\n        This is a mechanism to prevent jobs that have already been submitted\\n        from executing.\\n        \\n        Parameters\\n        ----------\\n        \\n        jobs : msg_id, list of msg_ids, or AsyncResult\\n            The jobs to be aborted\\n        \\n        \\n        '\n    block = (self.block if (block is None) else block)\n    targets = self._build_targets(targets)[0]\n    msg_ids = []\n    if isinstance(jobs, (str, AsyncResult)):\n        jobs = [jobs]\n    bad_ids = [obj for obj in jobs if (not isinstance(obj, (str, AsyncResult)))]\n    if bad_ids:\n        raise TypeError(('Invalid msg_id type %r, expected str or AsyncResult' % bad_ids[0]))\n    for j in jobs:\n        if isinstance(j, AsyncResult):\n            msg_ids.extend(j.msg_ids)\n        else:\n            msg_ids.append(j)\n    content = dict(msg_ids=msg_ids)\n    for t in targets:\n        self.session.send(self._control_socket, 'abort_request', content=content, ident=t)\n    error = False\n    if block:\n        self._flush_ignored_control()\n        for i in range(len(targets)):\n            (idents, msg) = self.session.recv(self._control_socket, 0)\n            if self.debug:\n                pprint(msg)\n            if (msg['content']['status'] != 'ok'):\n                error = self._unwrap_exception(msg['content'])\n    else:\n        self._ignored_control_replies += len(targets)\n    if error:\n        raise error\n", "label": 1}
{"function": "\n\n@classmethod\ndef text(cls, max_nb_chars=200):\n    \"\\n        Generate a text string.\\n        Depending on the $maxNbChars, returns a string made of words, sentences, or paragraphs.\\n        :example 'Sapiente sunt omnis. Ut pariatur ad autem ducimus et. Voluptas rem voluptas sint modi dolorem amet.'\\n        :param max_nb_chars Maximum number of characters the text should contain (minimum 5)\\n        :return string\\n        \"\n    text = []\n    if (max_nb_chars < 5):\n        raise ValueError('text() can only generate text of at least 5 characters')\n    if (max_nb_chars < 25):\n        while (not text):\n            size = 0\n            while (size < max_nb_chars):\n                word = ((' ' if size else '') + cls.word())\n                text.append(word)\n                size += len(word)\n            text.pop()\n        text[0] = (text[0][0].upper() + text[0][1:])\n        last_index = (len(text) - 1)\n        text[last_index] += '.'\n    elif (max_nb_chars < 100):\n        while (not text):\n            size = 0\n            while (size < max_nb_chars):\n                sentence = ((' ' if size else '') + cls.sentence())\n                text.append(sentence)\n                size += len(sentence)\n            text.pop()\n    else:\n        while (not text):\n            size = 0\n            while (size < max_nb_chars):\n                paragraph = (('\\n' if size else '') + cls.paragraph())\n                text.append(paragraph)\n                size += len(paragraph)\n            text.pop()\n    return ''.join(text)\n", "label": 1}
{"function": "\n\ndef pp_list(out, table, labels):\n    assert (0 < len(labels))\n    colwidths = [len(label) for label in labels]\n    for row in table:\n        for (colno, v) in enumerate(row):\n            colwidths[colno] = max(colwidths[colno], len(unicode(v)))\n    first = True\n    for (colno, label) in enumerate(labels):\n        if first:\n            first = False\n        else:\n            out.write(' | ')\n        out.write(('%*s' % (colwidths[colno], label)))\n    out.write('\\n')\n    first = True\n    for (colno, label) in enumerate(labels):\n        if first:\n            first = False\n        else:\n            out.write('-+-')\n        out.write(('%s' % ('-' * colwidths[colno])))\n    out.write('\\n')\n    for row in table:\n        first = True\n        for (colno, v) in enumerate(row):\n            if first:\n                first = False\n            else:\n                out.write(' | ')\n            out.write(('%*s' % (colwidths[colno], unicode(v))))\n        out.write('\\n')\n", "label": 1}
{"function": "\n\ndef _testlist(self, classes):\n    sess = create_session()\n    count = 1\n    obj = None\n    for c in classes:\n        if isinstance(c, type):\n            newobj = c(('item %d' % count))\n            count += 1\n        else:\n            newobj = c\n        if (obj is not None):\n            obj.nxt = newobj\n        else:\n            t = newobj\n        obj = newobj\n    sess.add(t)\n    sess.flush()\n    assertlist = []\n    node = t\n    while node:\n        assertlist.append(node)\n        n = node.nxt\n        if (n is not None):\n            assert (n.prev is node)\n        node = n\n    original = repr(assertlist)\n    sess.expunge_all()\n    node = sess.query(Table1).order_by(Table1.id).filter((Table1.id == t.id)).first()\n    assertlist = []\n    while node:\n        assertlist.append(node)\n        n = node.nxt\n        if (n is not None):\n            assert (n.prev is node)\n        node = n\n    forwards = repr(assertlist)\n    sess.expunge_all()\n    node = sess.query(Table1).order_by(Table1.id).filter((Table1.id == obj.id)).first()\n    assertlist = []\n    while node:\n        assertlist.insert(0, node)\n        n = node.prev\n        if (n is not None):\n            assert (n.nxt is node)\n        node = n\n    backwards = repr(assertlist)\n    assert (original == forwards == backwards)\n", "label": 1}
{"function": "\n\ndef _file_gen(dname, fmatch=bool, dmatch=None):\n    'A generator returning files under the given directory, with optional\\n    file and directory filtering.\\n\\n    Args\\n    ----\\n    fmatch : predicate funct\\n        A predicate function that returns True on a match.\\n        This is used to match files only.\\n\\n    dmatch : predicate funct\\n        A predicate function that returns True on a match.\\n        This is used to match directories only.\\n    '\n    if ((dmatch is not None) and (not dmatch(dname))):\n        return\n    for (path, dirlist, filelist) in os.walk(dname):\n        if (dmatch is not None):\n            newdl = [d for d in dirlist if dmatch(d)]\n            if (len(newdl) != len(dirlist)):\n                dirlist[:] = newdl\n        for name in [f for f in filelist if fmatch(f)]:\n            (yield join(path, name))\n", "label": 1}
{"function": "\n\ndef mul(x, y):\n    '\\n    Multiply elementwise two matrices, at least one of which is sparse.\\n\\n    This method will provide the right op according to the inputs.\\n\\n    Parameters\\n    ----------\\n    x\\n        A matrix variable.\\n    y\\n        A matrix variable.\\n\\n    Returns\\n    -------\\n    A sparse matrix\\n        `x` + `y`\\n\\n    Notes\\n    -----\\n    At least one of `x` and `y` must be a sparse matrix.\\n    The grad is regular, i.e. not structured.\\n\\n    '\n    x = as_sparse_or_tensor_variable(x)\n    y = as_sparse_or_tensor_variable(y)\n    x_is_sparse_variable = _is_sparse_variable(x)\n    y_is_sparse_variable = _is_sparse_variable(y)\n    assert (x_is_sparse_variable or y_is_sparse_variable)\n    if (x_is_sparse_variable and y_is_sparse_variable):\n        if ((y.dtype == 'float64') and (x.dtype == 'float32')):\n            x = x.astype('float64')\n        return mul_s_s(x, y)\n    elif (x_is_sparse_variable and (not y_is_sparse_variable)):\n        if ((y.dtype == 'float64') and (x.dtype == 'float32')):\n            x = x.astype('float64')\n        return mul_s_d(x, y)\n    elif (y_is_sparse_variable and (not x_is_sparse_variable)):\n        return mul_s_d(y, x)\n    else:\n        raise NotImplementedError()\n", "label": 1}
{"function": "\n\n@util.debuglog\n@gen.coroutine\ndef _stop(self, close_output_streams=False, for_shutdown=False):\n    if self.is_stopped():\n        return\n    self._status = 'stopping'\n    skip = (for_shutdown and self.use_papa)\n    if (not skip):\n        logger.debug(('stopping the %s watcher' % self.name))\n        logger.debug(('gracefully stopping processes [%s] for %ss' % (self.name, self.graceful_timeout)))\n        self.call_hook('before_stop')\n        (yield self.kill_processes())\n        self.reap_processes()\n    if self.stream_redirector:\n        self.stream_redirector.stop()\n        self.stream_redirector = None\n    if close_output_streams:\n        if (self.stdout_stream and hasattr(self.stdout_stream, 'close')):\n            self.stdout_stream.close()\n        if (self.stderr_stream and hasattr(self.stderr_stream, 'close')):\n            self.stderr_stream.close()\n    if skip:\n        logger.info('%s left running in papa', self.name)\n    else:\n        if (self.evpub_socket is not None):\n            self.notify_event('stop', {\n                'time': time.time(),\n            })\n        self._status = 'stopped'\n        self.call_hook('after_stop')\n        logger.info('%s stopped', self.name)\n", "label": 1}
{"function": "\n\ndef __new__(cls, sets, polar=False):\n    from sympy import sin, cos\n    (x, y, r, theta) = symbols('x, y, r, theta', cls=Dummy)\n    I = S.ImaginaryUnit\n    polar = sympify(polar)\n    if (polar == False):\n        if (all((_a.is_FiniteSet for _a in sets.args)) and (len(sets.args) == 2)):\n            complex_num = []\n            for x in sets.args[0]:\n                for y in sets.args[1]:\n                    complex_num.append((x + (I * y)))\n            obj = FiniteSet(*complex_num)\n        else:\n            obj = ImageSet.__new__(cls, Lambda((x, y), (x + (I * y))), sets)\n        obj._variables = (x, y)\n        obj._expr = (x + (I * y))\n    elif (polar == True):\n        new_sets = []\n        if (not sets.is_ProductSet):\n            for k in sets.args:\n                new_sets.append(k)\n        else:\n            new_sets.append(sets)\n        for (k, v) in enumerate(new_sets):\n            from sympy.sets import ProductSet\n            new_sets[k] = ProductSet(v.args[0], normalize_theta_set(v.args[1]))\n        sets = Union(*new_sets)\n        obj = ImageSet.__new__(cls, Lambda((r, theta), (r * (cos(theta) + (I * sin(theta))))), sets)\n        obj._variables = (r, theta)\n        obj._expr = (r * (cos(theta) + (I * sin(theta))))\n    else:\n        raise ValueError('polar should be either True or False')\n    obj._sets = sets\n    obj._polar = polar\n    return obj\n", "label": 1}
{"function": "\n\ndef run(self, edit, discard='item'):\n    repo = self.get_repo()\n    if (not repo):\n        return\n    goto = None\n    if (discard == 'section'):\n        points = self.get_all_points()\n        sections = set([self.section_at_point(p) for p in points])\n        all_files = self.get_all_files()\n        if (STASHES in sections):\n            self.discard_all_stashes(repo)\n        if (UNTRACKED_FILES in sections):\n            self.discard_all_untracked(repo)\n        files = [i for i in all_files if (i[0] in (UNSTAGED_CHANGES, STAGED_CHANGES))]\n        if files:\n            self.discard_files(repo, files)\n    elif (discard == 'item'):\n        files = self.get_selected_files()\n        stashes = self.get_selected_stashes()\n        if files:\n            self.discard_files(repo, files)\n        if stashes:\n            self.discard_stashes(repo, stashes)\n        goto = self.logical_goto_next_file()\n    elif (discard == 'all'):\n        self.discard_all(repo)\n    self.update_status(goto)\n", "label": 1}
{"function": "\n\ndef parse_range_header(value, make_inclusive=True):\n    'Parses a range header into a :class:`~werkzeug.datastructures.Range`\\n    object.  If the header is missing or malformed `None` is returned.\\n    `ranges` is a list of ``(start, stop)`` tuples where the ranges are\\n    non-inclusive.\\n\\n    .. versionadded:: 0.7\\n    '\n    if ((not value) or ('=' not in value)):\n        return None\n    ranges = []\n    last_end = 0\n    (units, rng) = value.split('=', 1)\n    units = units.strip().lower()\n    for item in rng.split(','):\n        item = item.strip()\n        if ('-' not in item):\n            return None\n        if item.startswith('-'):\n            if (last_end < 0):\n                return None\n            begin = int(item)\n            end = None\n            last_end = (- 1)\n        elif ('-' in item):\n            (begin, end) = item.split('-', 1)\n            begin = int(begin)\n            if ((begin < last_end) or (last_end < 0)):\n                return None\n            if end:\n                end = (int(end) + 1)\n                if (begin >= end):\n                    return None\n            else:\n                end = None\n            last_end = end\n        ranges.append((begin, end))\n    return Range(units, ranges)\n", "label": 1}
{"function": "\n\ndef log_actor_destroy(self, actor_id):\n    ' Trace actor destroy\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_ACTOR_DESTROY in logger.events)):\n            if ((not logger.actors) or (actor_id in logger.actors)):\n                data = {\n                    \n                }\n                data['timestamp'] = time.time()\n                data['node_id'] = self.node.id\n                data['type'] = 'actor_destroy'\n                data['actor_id'] = actor_id\n                if (logger.connection is not None):\n                    if (not logger.connection.connection_lost):\n                        logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                    else:\n                        disconnected.append(user_id)\n                elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                    msg = {\n                        'cmd': 'logevent',\n                        'msgid': logger.handle,\n                        'header': None,\n                        'data': ('data: %s\\n\\n' % json.dumps(data)),\n                    }\n                    self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef process(self, request):\n    '\\n        process determines if this item should visible, if its selected, etc...\\n        '\n    self.check(request)\n    if (not self.visible):\n        return\n    if callable(self.title):\n        self.title = self.title(request)\n    if (self.slug is None):\n        if (sys.version_info > (3, 0)):\n            self.slug = slugify(self.title)\n        else:\n            self.slug = slugify(unicode(self.title))\n    if callable(self.children):\n        children = list(self.children(request))\n    else:\n        children = list(self.children)\n    for child in children:\n        child.parent = self\n        child.process(request)\n    self.children = [child for child in children if child.visible]\n    self.children.sort(key=(lambda child: child.weight))\n    hide_empty = getattr(settings, 'MENU_HIDE_EMPTY', False)\n    if (hide_empty and (len(self.children) == 0)):\n        self.visible = False\n        return\n    curitem = None\n    for item in self.children:\n        item.selected = False\n        if item.match_url(request):\n            if ((curitem is None) or (len(curitem.url) < len(item.url))):\n                curitem = item\n    if (curitem is not None):\n        curitem.selected = True\n", "label": 1}
{"function": "\n\ndef perform(self, logfile=None):\n    'Fetch all of the added Request objects.\\n\\n        Return two lists. The first list is a list of responses that\\n        completed. The second is list of the requests that errored.\\n        '\n    m = pycurl.CurlMulti()\n    requests = []\n    num_q = num_urls = len(self._requests)\n    reqs = self._requests\n    self._requests = []\n    for req in reqs:\n        (c, resp) = req.get_requester()\n        c.resp = resp\n        m.add_handle(c)\n        requests.append(c)\n    del reqs\n    while 1:\n        (ret, num_handles) = m.perform()\n        if (ret != pycurl.E_CALL_MULTI_PERFORM):\n            break\n    num_handles = num_urls\n    while num_handles:\n        ret = m.select(5.0)\n        if (ret == (- 1)):\n            continue\n        while 1:\n            (ret, num_handles) = m.perform()\n            if (ret != pycurl.E_CALL_MULTI_PERFORM):\n                break\n    goodlist = []\n    errlist = []\n    while 1:\n        (num_q, ok_list, err_list) = m.info_read(num_urls)\n        for c in ok_list:\n            resp = c.resp\n            del c.resp\n            resp.error = None\n            m.remove_handle(c)\n            resp.finalize(c)\n            goodlist.append(resp)\n        for (c, errno, errmsg) in err_list:\n            resp = c.resp\n            del c.resp\n            resp.error = (errno, errmsg)\n            m.remove_handle(c)\n            errlist.append(resp)\n        if (num_q == 0):\n            break\n    m.close()\n    return (goodlist, errlist)\n", "label": 1}
{"function": "\n\ndef test_default_configuration(self):\n    c = Config(sitepath=TEST_SITE, config_dict={\n        \n    })\n    for root in ['content', 'layout']:\n        name = (root + '_root')\n        path = (name + '_path')\n        assert hasattr(c, name)\n        assert (getattr(c, name) == root)\n        assert hasattr(c, path)\n        assert (getattr(c, path) == TEST_SITE.child_folder(root))\n    assert (c.media_root_path == c.content_root_path.child_folder('media'))\n    assert hasattr(c, 'plugins')\n    assert (len(c.plugins) == 0)\n    assert hasattr(c, 'ignore')\n    assert (c.ignore == ['*~', '*.bak', '.hg', '.git', '.svn'])\n    assert (c.deploy_root_path == TEST_SITE.child_folder('deploy'))\n    assert (c.not_found == '404.html')\n    assert (c.meta.nodemeta == 'meta.yaml')\n", "label": 1}
{"function": "\n\ndef __init__(self, bucket=None, access_key=None, secret_key=None, headers=None, calling_format=None, cache=None, base_url=None):\n    if (bucket is None):\n        bucket = settings.AWS_STORAGE_BUCKET_NAME\n    if (calling_format is None):\n        calling_format = getattr(settings, 'AWS_CALLING_FORMAT', CallingFormat.SUBDOMAIN)\n    self.bucket = bucket\n    if ((not access_key) and (not secret_key)):\n        (access_key, secret_key) = self._get_access_keys()\n    self.connection = AWSAuthConnection(access_key, secret_key, calling_format=calling_format)\n    default_headers = getattr(settings, HEADERS, [])\n    if isinstance(default_headers, dict):\n        default_headers = [('.*', default_headers)]\n    if headers:\n        default_headers = (list(headers) + list(default_headers))\n    self.headers = []\n    for value in default_headers:\n        self.headers.append((re.compile(value[0]), value[1]))\n    if (cache is not None):\n        self.cache = cache\n    else:\n        cache = getattr(settings, 'CUDDLYBUDDLY_STORAGE_S3_CACHE', None)\n        if (cache is not None):\n            self.cache = self._get_cache_class(cache)()\n        else:\n            self.cache = None\n    if (base_url is None):\n        if (not self.static):\n            base_url = settings.MEDIA_URL\n        else:\n            base_url = settings.STATIC_URL\n    self.base_url = base_url\n", "label": 1}
{"function": "\n\ndef get_or_create(self, data):\n    descriptor = self.get_descriptor(data)\n    urls = self.get_urls(data)\n    read_response = self.requester.get(urls['read'])\n    if (read_response.status_code != 200):\n        message = 'Read error GET ({}): {}\\n{}'.format(read_response.status_code, urls['read'], read_response.text)\n        raise ValueError(message)\n    read_response_data = read_response.json()\n    pks = [item['id'] for item in read_response_data]\n    if (pks == []):\n        create_response = self.requester.post(urls['create'], data)\n        if (create_response.status_code != 201):\n            message = 'Create error POST ({}): {}\\n{}\\n{}'.format(create_response.status_code, urls['create'], data, create_response.text)\n            raise ValueError(message)\n        create_response_data = create_response.json()\n        if self.many:\n            pks = [response_data['id'] for response_data in create_response_data]\n        else:\n            pk = create_response_data['id']\n        if self.verbose:\n            if self.many:\n                print('Created {} ({}, pks={})'.format(self.item_name, descriptor, pks))\n            else:\n                print('Created {} ({}, pk={})'.format(self.item_name, descriptor, pk))\n        return (create_response_data, True)\n    else:\n        if ((len(pks) > 1) and (not self.many)):\n            message = 'Found multiple {} instances ({}) for {}'.format(self.item_name, pks, descriptor)\n            warnings.warn(message)\n        if self.verbose:\n            print('Existing {} ({}, pks={})'.format(self.item_name, descriptor, pks))\n        return (read_response_data, False)\n", "label": 1}
{"function": "\n\ndef isitemsimilar(v1, v2, verb=False, dtol=DTOL):\n    '\\n    Compare two values for differences\\n\\n    See :py:func:`isdicsimilar` for Parameters.\\n\\n    '\n    r = True\n    if (type(v1) != type(v2)):\n        r = False\n        if verb:\n            print('Item has different type', type(v1), type(v2))\n    elif isinstance(v1, dict):\n        r = (r and isdicsimilar(v1, v2, verb=verb, dtol=dtol))\n    elif isinstance(v1, list):\n        r = (r and islistsimilar(v1, v2, verb=verb, dtol=dtol))\n    elif isinstance(v1, (int, float)):\n        if (abs((v1 - v2)) > dtol):\n            r = False\n            if verb:\n                print('Key mismatch:', v1, v2)\n    elif (v1 != v2):\n        r = False\n        if verb:\n            print('Key mismatch:', v1, v2)\n    return r\n", "label": 1}
{"function": "\n\ndef datadiff(self, data1, data2, path=None, ignore_keys=[], compare_sorted=False):\n    if (path is None):\n        path = []\n\n    def fail(msg, failed_path):\n        self.fail('Path \"{0}\": {1}'.format('->'.join(failed_path), msg))\n    if ((not isinstance(data1, dict)) or (not isinstance(data2, dict))):\n        if isinstance(data1, (list, tuple)):\n            newpath = path[:]\n            if compare_sorted:\n                data1 = sorted(data1)\n                data2 = sorted(data2)\n            for (i, keys) in enumerate(izip(data1, data2)):\n                newpath.append(str(i))\n                self.datadiff(keys[0], keys[1], newpath, ignore_keys, compare_sorted)\n                newpath.pop()\n        elif (data1 != data2):\n            err = 'Values differ: {0} != {1}'.format(str(data1), str(data2))\n            fail(err, path)\n    else:\n        newpath = path[:]\n        if (len(data1) != len(data2)):\n            fail('Dicts have different keys number: {0} != {1}'.format(len(data1), len(data2)), path)\n        for (key1, key2) in zip(sorted(data1), sorted(data2)):\n            if (key1 != key2):\n                err = 'Keys differ: {0} != {1}'.format(str(key1), str(key2))\n                fail(err, path)\n            if (key1 in ignore_keys):\n                continue\n            newpath.append(key1)\n            self.datadiff(data1[key1], data2[key2], newpath, ignore_keys, compare_sorted)\n            newpath.pop()\n", "label": 1}
{"function": "\n\ndef validate(self, input_parameters):\n    'Runs all set type transformers / validators against the provided input parameters and returns any errors'\n    errors = {\n        \n    }\n    for (key, type_handler) in self.interface.input_transformations.items():\n        if self.raise_on_invalid:\n            if (key in input_parameters):\n                input_parameters[key] = type_handler(input_parameters[key])\n        else:\n            try:\n                if (key in input_parameters):\n                    input_parameters[key] = type_handler(input_parameters[key])\n            except InvalidTypeData as error:\n                errors[key] = (error.reasons or str(error.message))\n            except Exception as error:\n                if (hasattr(error, 'args') and error.args):\n                    errors[key] = error.args[0]\n                else:\n                    errors[key] = str(error)\n    for require in self.interface.required:\n        if (not (require in input_parameters)):\n            errors[require] = \"Required parameter '{}' not supplied\".format(require)\n    if ((not errors) and getattr(self, 'validate_function', False)):\n        errors = self.validate_function(input_parameters)\n    return errors\n", "label": 1}
{"function": "\n\ndef test_hyper():\n    raises(TypeError, (lambda : hyper(1, 2, z)))\n    assert (hyper((1, 2), (1,), z) == hyper(Tuple(1, 2), Tuple(1), z))\n    h = hyper((1, 2), (3, 4, 5), z)\n    assert (h.ap == Tuple(1, 2))\n    assert (h.bq == Tuple(3, 4, 5))\n    assert (h.argument == z)\n    assert (h.is_commutative is True)\n    assert tn(hyper(Tuple(), Tuple(), z), exp(z), z)\n    assert tn((z * hyper((1, 1), Tuple(2), (- z))), log((1 + z)), z)\n    h = hyper((randcplx(), randcplx(), randcplx()), (randcplx(), randcplx()), z)\n    assert td(h, z)\n    (a1, a2, b1, b2, b3) = symbols('a1:3, b1:4')\n    assert (hyper((a1, a2), (b1, b2, b3), z).diff(z) == (((a1 * a2) / ((b1 * b2) * b3)) * hyper(((a1 + 1), (a2 + 1)), ((b1 + 1), (b2 + 1), (b3 + 1)), z)))\n    assert (hyper([z], [], z).diff(z) == Derivative(hyper([z], [], z), z))\n    from sympy import polar_lift\n    assert (hyper([polar_lift(z)], [polar_lift(k)], polar_lift(x)) == hyper([z], [k], polar_lift(x)))\n", "label": 1}
{"function": "\n\ndef test_meijerg_derivative():\n    assert (meijerg([], [1, 1], [0, 0, x], [], z).diff(x) == ((log(z) * meijerg([], [1, 1], [0, 0, x], [], z)) + (2 * meijerg([], [1, 1, 1], [0, 0, x, 0], [], z))))\n    y = randcplx()\n    a = 5\n    assert td(meijerg([x], [], [], [], y), x)\n    assert td(meijerg([(x ** 2)], [], [], [], y), x)\n    assert td(meijerg([], [x], [], [], y), x)\n    assert td(meijerg([], [], [x], [], y), x)\n    assert td(meijerg([], [], [], [x], y), x)\n    assert td(meijerg([x], [a], [(a + 1)], [], y), x)\n    assert td(meijerg([x], [(a + 1)], [a], [], y), x)\n    assert td(meijerg([x, a], [], [], [(a + 1)], y), x)\n    assert td(meijerg([x, (a + 1)], [], [], [a], y), x)\n    b = (S(3) / 2)\n    assert td(meijerg([(a + 2)], [b], [(b - 3), x], [a], y), x)\n", "label": 1}
{"function": "\n\ndef on_key_press(self, event):\n    if (event.key == 'q'):\n        self.show(False, False)\n        self.app.quit()\n        return\n    elif ((event.key == 'p') or (event.key == ' ')):\n        self._paused = (not self._paused)\n        self.update_timer_state()\n    elif (event.key == 's'):\n        img = _screenshot()\n        self.write_img(img)\n    elif (event.key == 'a'):\n        print(('Size/pos args: --size %dx%d --pos %d,%d' % (self.physical_size[0], self.physical_size[1], self.position[0], self.position[1])))\n    elif (event.key == 'f'):\n        self._profile = (not self._profile)\n        if self._profile:\n\n            def print_profile(fps):\n                print(('%.2f ms/frame' % (1000.0 / float(fps))))\n                return False\n            self.measure_fps(1.0, print_profile)\n        else:\n            self.measure_fps(1.0, False)\n        self.update_timer_state()\n    elif ((event.key == keys.LEFT) or (event.key == keys.RIGHT)):\n        self._paused = True\n        self.update_timer_state()\n        step = (1.0 / 60.0)\n        if (keys.ALT in event.modifiers):\n            step *= 0.1\n            if (keys.SHIFT in event.modifiers):\n                step *= 0.1\n        else:\n            if (keys.SHIFT in event.modifiers):\n                step *= 10.0\n            if (keys.CONTROL in event.modifiers):\n                step *= 100.0\n        if (event.key == keys.LEFT):\n            step *= (- 1.0)\n        self.program['iGlobalTime'] += step\n        self.print_t()\n        self.update()\n", "label": 1}
{"function": "\n\ndef gettile(self, x, y, z, id=None, title=None, srs=None, mimetype=None, timeout=None):\n    if ((not id) and (not title) and (not srs)):\n        raise ValueError('either id or title and srs must be specified')\n    if id:\n        return self._gettilefromset(self.contents[id].tilemap.tilesets, x, y, z, self.contents[id].tilemap.extension, timeout=timeout)\n    elif (title and srs):\n        for tm in self.contents.values():\n            if ((tm.title == title) and (tm.srs == srs)):\n                if mimetype:\n                    if (tm.tilemap.mimetype == mimetype):\n                        return self._gettilefromset(tm.tilemap.tilesets, x, y, z, tm.tilemap.extension, timeout=timeout)\n                else:\n                    return self._gettilefromset(tm.tilemap.tilesets, x, y, z, tm.tilemap.extension, timeout=timeout)\n        else:\n            raise ValueError(('cannot find %s with projection %s for zoomlevel %i' % (title, srs, z)))\n    elif (title or srs):\n        ValueError('both title and srs must be specified')\n    raise ValueError(('Specified Tile with id %s, title %s\\n                projection %s format %s at zoomlevel %i cannot be found' % (id, title, srs, format, z)))\n", "label": 1}
{"function": "\n\ndef test_if(self):\n    assert (evalpy('if True: 4\\nelse: 5') == '4')\n    assert (evalpy('if False: 4\\nelse: 5') == '5')\n    assert (evalpy('x=4\\nif x>3: 13\\nelif x > 2: 12\\nelse: 10') == '13')\n    assert (evalpy('x=3\\nif x>3: 13\\nelif x > 2: 12\\nelse: 10') == '12')\n    assert (evalpy('x=1\\nif x>3: 13\\nelif x > 2: 12\\nelse: 10') == '10')\n    line = py2js('3 if True else 4').replace(')', '').replace('(', '')\n    assert (line == 'true? 3 : 4;')\n    assert (evalpy('4 if True else 5') == '4')\n    assert (evalpy('4 if False else 5') == '5')\n    assert (evalpy('3+1 if 0+2/1 else 4+1') == '4')\n    assert (evalpy('3+1 if 4/2-2 else 4+1') == '5')\n", "label": 1}
{"function": "\n\ndef get_show(self, imdb, tvdb, tvshowtitle, year):\n    try:\n        query = self.search_link\n        post = {\n            'searchquery': tvshowtitle,\n            'searchin': '2',\n        }\n        result = ''\n        links = [self.link_1, self.link_3]\n        for base_link in links:\n            result = client.source(urlparse.urljoin(base_link, query), post=post, headers=self.headers)\n            if ('widget search-page' in str(result)):\n                break\n        result = client.parseDOM(result, 'div', attrs={\n            'class': 'widget search-page',\n        })[0]\n        result = client.parseDOM(result, 'td')\n        tvshowtitle = cleantitle.tv(tvshowtitle)\n        years = [('(%s)' % str(year)), ('(%s)' % str((int(year) + 1))), ('(%s)' % str((int(year) - 1)))]\n        result = [(client.parseDOM(i, 'a', ret='href')[(- 1)], client.parseDOM(i, 'a')[(- 1)]) for i in result]\n        result = [i for i in result if (tvshowtitle == cleantitle.tv(i[1]))]\n        result = [i[0] for i in result if any(((x in i[1]) for x in years))][0]\n        url = client.replaceHTMLCodes(result)\n        try:\n            url = urlparse.parse_qs(urlparse.urlparse(url).query)['u'][0]\n        except:\n            pass\n        url = urlparse.urlparse(url).path\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef test_write_job3():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'write': ['test.test', 'test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\n@classmethod\ndef size(cls, val, str_cache, object_cache, traits_cache):\n    if (val in object_cache):\n        index = object_cache.index(val)\n        return AMF3Integer.size((index << 1))\n    else:\n        object_cache.append(val)\n        size = 0\n        traits = type(val)\n        if (traits in traits_cache):\n            index = traits_cache.index(traits)\n            size += AMF3Integer.size(((index << 2) | 1))\n        else:\n            header = 3\n            if traits.__dynamic__:\n                header |= (2 << 2)\n            if traits.__externalizable__:\n                header |= (1 << 2)\n            header |= (len(traits.__members__) << 4)\n            size += AMF3Integer.size(header)\n            if isinstance(val, AMF3Object):\n                size += U8.size\n            else:\n                size += AMF3String.size(traits.__name__, cache=str_cache)\n                traits_cache.append(traits)\n            for member in traits.__members__:\n                size += AMF3String.size(member, cache=str_cache)\n        for member in traits.__members__:\n            value = getattr(val, member)\n            size += AMF3Value.size(value, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n        if traits.__dynamic__:\n            if isinstance(val, AMF3Object):\n                iterator = val.items()\n            else:\n                iterator = val.__dict__.items()\n            for (key, value) in iterator:\n                if (key in traits.__members__):\n                    continue\n                size += AMF3String.size(key, cache=str_cache)\n                size += AMF3Value.size(value, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n            size += U8.size\n        return size\n", "label": 1}
{"function": "\n\ndef build(self, skip_features=False, managed=False):\n    if hasattr(self.session._internal, 'namespace'):\n        namespace_instance = self.session._internal.namespace['instance']\n        if hasattr(namespace_instance, 'after_request'):\n            getattr(namespace_instance, 'after_request')(self, self.session)\n    if (self.namespace and (not skip_features)):\n        for feature in self.namespace['features']:\n            feature._handle_response(self)\n    if (not isinstance(self.result, UnformattedResponse)):\n        if self.function:\n            self.result = self.function['format'](self.result)\n        if isinstance(self.output_formatter, type):\n            self.output_formatter = self.output_formatter(sapi_request=self.sapi_request, callback=self.callback)\n        if isinstance(self.wrapper, type):\n            self.wrapper = self.wrapper(sapi_request=self.sapi_request)\n        wrapper_result = self.wrapper._build(errors=self.errors, result=self.result)\n        formatter_result = self.output_formatter.build(wrapper_result)\n    else:\n        self.mimetype = self.result.mimetype\n        formatter_result = self.result.content\n    result = {\n        'result': formatter_result,\n        'mimetype': self.mimetype,\n    }\n    if managed:\n        return result\n    else:\n        return self._build_response_obj(self.sapi_request, result)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.LIST):\n                self.column_names = []\n                (_etype10, _size7) = iprot.readListBegin()\n                for _i11 in xrange(_size7):\n                    _elem12 = iprot.readString()\n                    self.column_names.append(_elem12)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.slice_range = SliceRange()\n                self.slice_range.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    quiet = options.get('quiet', False)\n    codes = self.get_codes()\n    if (not quiet):\n        if codes:\n            self.stdout.write(('Will now delete codes: %s \\n' % codes))\n        else:\n            self.stdout.write('No Object codes to delete. \\n')\n    revisions = self.get_revisions()\n    if (not quiet):\n        if codes:\n            self.stdout.write((\"Will now delete additional doc's revisions in docs: %s \\n\" % [d[0] for d in revisions]))\n        else:\n            self.stdout.write('No additional revision files to delete. \\n')\n    if (codes or revisions):\n        processor = core.document_processor.DocumentProcessor()\n        user = User.objects.filter(is_superuser=True)[0]\n        for code in codes:\n            processor.delete(code, {\n                'user': user,\n            })\n            if (not processor.errors):\n                if (not quiet):\n                    self.stdout.write(('Permanently deleted object with code: %s' % code))\n            else:\n                if (not quiet):\n                    self.stdout.write(processor.errors)\n                raise (Exception, processor.errors)\n        for rev in revisions:\n            processor.delete(rev[0], {\n                'user': user,\n                'delete_revision': rev[1],\n            })\n", "label": 1}
{"function": "\n\ndef test_diff_nans(self):\n    'Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/204'\n    arr = np.empty((10, 10), dtype=np.float64)\n    arr[:5] = 1.0\n    arr[5:] = np.nan\n    arr2 = arr.copy()\n    table = np.rec.array([(1.0, 2.0), (3.0, np.nan), (np.nan, np.nan)], names=['cola', 'colb']).view(fits.FITS_rec)\n    table2 = table.copy()\n    assert ImageDataDiff(arr, arr2).identical\n    assert TableDataDiff(table, table2).identical\n    arr2[0][0] = 2.0\n    arr2[5][0] = 2.0\n    table2[0][0] = 2.0\n    table2[1][1] = 2.0\n    diff = ImageDataDiff(arr, arr2)\n    assert (not diff.identical)\n    assert (diff.diff_pixels[0] == ((0, 0), (1.0, 2.0)))\n    assert (diff.diff_pixels[1][0] == (5, 0))\n    assert np.isnan(diff.diff_pixels[1][1][0])\n    assert (diff.diff_pixels[1][1][1] == 2.0)\n    diff = TableDataDiff(table, table2)\n    assert (not diff.identical)\n    assert (diff.diff_values[0] == (('cola', 0), (1.0, 2.0)))\n    assert (diff.diff_values[1][0] == ('colb', 1))\n    assert np.isnan(diff.diff_values[1][1][0])\n    assert (diff.diff_values[1][1][1] == 2.0)\n", "label": 1}
{"function": "\n\ndef test_RosdepDatabase():\n    from rosdep2.model import RosdepDatabase\n    db = RosdepDatabase()\n    assert (not db.is_loaded('foo'))\n    data = {\n        'a': 1,\n    }\n    db.set_view_data('foo', data, [], 'origin1')\n    assert db.is_loaded('foo')\n    entry = db.get_view_data('foo')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin1')\n    assert (entry.view_dependencies == [])\n    data['a'] = 2\n    assert (entry.rosdep_data != data)\n    data = {\n        'b': 2,\n    }\n    db.set_view_data('bar', data, ['foo'], 'origin2')\n    assert db.is_loaded('bar')\n    entry = db.get_view_data('bar')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin2')\n    assert (entry.view_dependencies == ['foo'])\n    data = {\n        'b': 3,\n    }\n    assert db.is_loaded('bar')\n    db.set_view_data('bar', data, ['baz', 'blah'], 'origin3')\n    assert db.is_loaded('bar')\n    entry = db.get_view_data('bar')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin3')\n    assert (set(entry.view_dependencies) == set(['baz', 'blah']))\n", "label": 1}
{"function": "\n\ndef Check(self):\n    'Assertion verification for options.'\n    try:\n        assert (self.m0 >= 0), 'margin0'\n        assert (self.m1 >= self.m0), 'margin1'\n        assert (self.c0 >= 0), 'cost0'\n        assert (self.c1 >= 0), 'cost1'\n        assert (self.cb >= 0), 'costb'\n        assert (self.ind >= 0), 'indent'\n        assert (self.adj_comment >= 0), 'adj_comment'\n        assert (self.adj_flow >= 0), 'adj_flow'\n        assert (self.adj_call >= 0), 'adj_call'\n        assert (self.adj_arg >= 0), 'adj_arg'\n        assert (self.cpack >= 0), 'cpack'\n    except AssertionError as e:\n        raise Error((\"Illegal option value for '%s'\" % e.args[0]))\n", "label": 1}
{"function": "\n\n@classmethod\ndef _from_line(cls, remote, line):\n    'Create a new PushInfo instance as parsed from line which is expected to be like\\n            refs/heads/master:refs/heads/master 05d2687..1d0568e as bytes'\n    (control_character, from_to, summary) = line.split('\\t', 3)\n    flags = 0\n    try:\n        flags |= cls._flag_map[control_character]\n    except KeyError:\n        raise ValueError(('Control character %r unknown as parsed from line %r' % (control_character, line)))\n    (from_ref_string, to_ref_string) = from_to.split(':')\n    if (flags & cls.DELETED):\n        from_ref = None\n    else:\n        from_ref = Reference.from_path(remote.repo, from_ref_string)\n    old_commit = None\n    if summary.startswith('['):\n        if ('[rejected]' in summary):\n            flags |= cls.REJECTED\n        elif ('[remote rejected]' in summary):\n            flags |= cls.REMOTE_REJECTED\n        elif ('[remote failure]' in summary):\n            flags |= cls.REMOTE_FAILURE\n        elif ('[no match]' in summary):\n            flags |= cls.ERROR\n        elif ('[new tag]' in summary):\n            flags |= cls.NEW_TAG\n        elif ('[new branch]' in summary):\n            flags |= cls.NEW_HEAD\n    else:\n        split_token = '...'\n        if (control_character == ' '):\n            split_token = '..'\n        (old_sha, new_sha) = summary.split(' ')[0].split(split_token)\n        old_commit = remote.repo.commit(old_sha)\n    return PushInfo(flags, from_ref, to_ref_string, remote, old_commit, summary)\n", "label": 1}
{"function": "\n\ndef _AtNonLeaf(self, attr_value, path):\n    'Called when at a non-leaf value. Should recurse and yield values.'\n    try:\n        if isinstance(attr_value, collections.Mapping):\n            sub_obj = attr_value.get(path[1])\n            if (len(path) > 2):\n                sub_obj = self.Expand(sub_obj, path[2:])\n            if isinstance(sub_obj, basestring):\n                (yield sub_obj)\n            elif isinstance(sub_obj, collections.Mapping):\n                for (k, v) in sub_obj.items():\n                    (yield {\n                        k: v,\n                    })\n            else:\n                for value in sub_obj:\n                    (yield value)\n        else:\n            for sub_obj in attr_value:\n                for value in self.Expand(sub_obj, path[1:]):\n                    (yield value)\n    except TypeError:\n        for value in self.Expand(attr_value, path[1:]):\n            (yield value)\n", "label": 1}
{"function": "\n\ndef conceptsUsed(self):\n    conceptsUsed = set((f.qname for f in self.modelXbrl.factsInInstance))\n    for cntx in self.modelXbrl.contexts.values():\n        for dim in cntx.qnameDims.values():\n            conceptsUsed.add(dim.dimensionQname)\n            if dim.isExplicit:\n                conceptsUsed.add(dim.memberQname)\n            else:\n                conceptsUsed.add(dim.typedMember.qname)\n    for (defaultDim, defaultDimMember) in self.modelXbrl.qnameDimensionDefaults.items():\n        conceptsUsed.add(defaultDim)\n        conceptsUsed.add(defaultDimMember)\n    for roleTypes in (self.modelXbrl.roleTypes, self.modelXbrl.arcroleTypes):\n        for modelRoleTypes in roleTypes.values():\n            for modelRoleType in modelRoleTypes:\n                for qn in modelRoleType.usedOns:\n                    conceptsUsed.add(qn)\n    for relationshipSetKey in self.relationshipSets:\n        relationshipSet = self.modelXbrl.relationshipSet(*relationshipSetKey)\n        for rel in relationshipSet.modelRelationships:\n            if isinstance(rel.fromModelObject, ModelConcept):\n                conceptsUsed.add(rel.fromModelObject)\n            if isinstance(rel.toModelObject, ModelConcept):\n                conceptsUsed.add(rel.toModelObject)\n    for qn in (XbrlConst.qnXbrliIdentifier, XbrlConst.qnXbrliPeriod, XbrlConst.qnXbrliUnit):\n        conceptsUsed.add(qn)\n    conceptsUsed -= {None}\n    return conceptsUsed\n", "label": 1}
{"function": "\n\ndef validate(self):\n    'Validate arguments, raises UrlArgsValidationError if something is wrong'\n    args = self.request.args\n    if (len(args) == 0):\n        raise UrlArgsValidationError('Mandatory arguments not found, please refer to the HTTPAPI specifications.')\n    for arg in args:\n        if (arg not in self.fields):\n            raise UrlArgsValidationError(('Argument [%s] is unknown.' % arg))\n        for field in self.fields:\n            fieldData = self.fields[field]\n            if (field in args):\n                if (isinstance(args[field][0], int) or isinstance(args[field][0], float)):\n                    value = str(args[field][0])\n                else:\n                    value = args[field][0]\n                if (('pattern' in self.fields[field]) and (self.fields[field]['pattern'].match(value) is None)):\n                    raise UrlArgsValidationError(('Argument [%s] has an invalid value: [%s].' % (field, value)))\n            elif (not fieldData['optional']):\n                raise UrlArgsValidationError(('Mandatory argument [%s] is not found.' % field))\n    return True\n", "label": 1}
{"function": "\n\ndef _regenerate(self, dev_mode=False):\n    if self._compiled:\n        for (module_name, (mtime, content, hash)) in self._compiled.items():\n            if ((module_name not in self._collected) or (not os.path.exists(self._collected[module_name])) or (os.path.getmtime(self._collected[module_name]) != mtime)):\n                self._compiled = {\n                    \n                }\n                break\n        else:\n            return\n    modules = [self.main_module, 'pyjslib']\n    while True:\n        if (not modules):\n            break\n        module_name = modules.pop()\n        path = self._collected[module_name]\n        mtime = os.path.getmtime(path)\n        source = read_text_file(path)\n        try:\n            (content, py_deps, js_deps) = self._compile(module_name, source, dev_mode=dev_mode)\n        except:\n            self._compiled = {\n                \n            }\n            raise\n        hash = sha1(smart_str(content)).hexdigest()\n        self._compiled[module_name] = (mtime, content, hash)\n        for name in py_deps:\n            if (name not in self._collected):\n                if (('.' in name) and (name.rsplit('.', 1)[0] in self._collected)):\n                    name = name.rsplit('.', 1)[0]\n                else:\n                    raise ImportError(('The pyjs module %s could not find the dependency %s' % (module_name, name)))\n            if (name not in self._compiled):\n                modules.append(name)\n", "label": 1}
{"function": "\n\ndef SetRepo(self, repo):\n    if (self.repo and (self.repo == self.mainRepo)):\n        self.mainRepo = self.repo\n        self.mainRepoSelection = [self.rows[row][0].commit.sha1 for row in self.selection]\n    repo_changed = (self.repo != repo)\n    if repo_changed:\n        self.selection = []\n        self.Scroll(0, 0)\n    if (not repo.parent):\n        self.mainRepo = repo\n    self.repo = repo\n    self.commits = self.repo.get_log(['--topo-order', '--all'])\n    self.CreateLogGraph()\n    if (repo_changed and (self.repo != self.mainRepo)):\n        for version in self.mainRepoSelection:\n            submodule_version = self.repo.parent.get_submodule_version(self.repo.name, version)\n            if submodule_version:\n                rows = [r for r in self.rows if (r[0].commit.sha1 == submodule_version)]\n                if rows:\n                    self.selection.append(self.rows.index(rows[0]))\n    self.SetVirtualSize(((- 1), ((len(self.rows) + 1) * LINH)))\n    self.SetScrollRate(LINH, LINH)\n    self.Refresh()\n", "label": 1}
{"function": "\n\ndef apply(self):\n    start_tags = []\n    for node in self.document.traverse(docutils.nodes.raw):\n        if (node['format'] != 'html'):\n            continue\n        start_match = self._start_re.match(node.astext())\n        if (not start_match):\n            continue\n        class_match = self._class_re.match(start_match.group(2))\n        if (not class_match):\n            continue\n        admonition_class = class_match.group(1)\n        if (admonition_class == 'info'):\n            admonition_class = 'note'\n        start_tags.append((node, admonition_class))\n    for (node, admonition_class) in reversed(start_tags):\n        content = []\n        for sibling in node.traverse(include_self=False, descend=False, siblings=True, ascend=False):\n            end_tag = (isinstance(sibling, docutils.nodes.raw) and (sibling['format'] == 'html') and self._end_re.match(sibling.astext()))\n            if end_tag:\n                admonition_node = AdmonitionNode(classes=['admonition', admonition_class])\n                admonition_node.extend(content)\n                parent = node.parent\n                parent.replace(node, admonition_node)\n                for n in content:\n                    parent.remove(n)\n                parent.remove(sibling)\n                break\n            else:\n                content.append(sibling)\n", "label": 1}
{"function": "\n\ndef test_read_job2():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test.test', 'test1'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef __init__(self, channel, body_value, properties=None, auto_id=False, opinionated=False):\n    'Create a new instance of the Message object.'\n    super(Message, self).__init__(channel, 'Message')\n    self.properties = (properties or {\n        \n    })\n    if isinstance(body_value, memoryview):\n        self.body = bytes(body_value)\n    else:\n        self.body = self._auto_serialize(body_value)\n    if ((opinionated or auto_id) and ('message_id' not in self.properties)):\n        if auto_id:\n            raise DeprecationWarning('Use opinionated instead of auto_id')\n        self._add_auto_message_id()\n    if opinionated:\n        if ('timestamp' not in self.properties):\n            self._add_timestamp()\n    if ('timestamp' in self.properties):\n        self.properties['timestamp'] = self._as_datetime(self.properties['timestamp'])\n    if self._invalid_properties:\n        msg = ('Invalid property: %s' % self._invalid_properties[0])\n        raise KeyError(msg)\n", "label": 1}
{"function": "\n\ndef set_visible_views(self, indices, data, viewport):\n    view_opts = self.view_opts\n    (new, remaining, old) = self.recycleview.view_adapter.set_visible_views(indices, data, view_opts)\n    remove = self.remove_widget\n    view_indices = self.view_indices\n    for (_, widget) in old:\n        remove(widget)\n        del view_indices[widget]\n    refresh_view_layout = self.refresh_view_layout\n    for (index, widget) in new:\n        opt = view_opts[index]\n        refresh_view_layout(index, opt['pos'], opt['pos_hint'], opt['size'], opt['size_hint'], widget, viewport)\n    add = self.add_widget\n    for (index, widget) in new:\n        view_indices[widget] = index\n        if (widget.parent is None):\n            add(widget)\n    changed = False\n    for (index, widget) in new:\n        opt = view_opts[index]\n        if (changed or ((widget.size == opt['size']) and (widget.size_hint == opt['size_hint']) and (widget.pos_hint == opt['pos_hint']))):\n            continue\n        changed = True\n    if changed:\n        self._size_needs_update = True\n        self.recycleview.refresh_from_layout(view_size=True)\n", "label": 1}
{"function": "\n\ndef DownloadActivity(self, serviceRecord, activity):\n    workoutID = activity.ServiceData['WorkoutID']\n    logger.debug(('DownloadActivity for %s' % workoutID))\n    session = self._get_session(record=serviceRecord)\n    resp = session.get((self._urlRoot + ('/api/workout/%d' % workoutID)))\n    try:\n        res = resp.json()\n    except ValueError:\n        raise APIException(('Parse failure in Motivato activity (%d) download: %s' % (workoutID, res.text)))\n    lap = Lap(stats=activity.Stats, startTime=activity.StartTime, endTime=activity.EndTime)\n    activity.Laps = [lap]\n    activity.GPS = False\n    if (('track' in res) and ('points' in res['track'])):\n        for pt in res['track']['points']:\n            wp = Waypoint()\n            if ('moment' not in pt):\n                continue\n            wp.Timestamp = self._parseDateTime(pt['moment'])\n            if ((('lat' in pt) and ('lon' in pt)) or ('ele' in pt)):\n                wp.Location = Location()\n                if (('lat' in pt) and ('lon' in pt)):\n                    wp.Location.Latitude = pt['lat']\n                    wp.Location.Longitude = pt['lon']\n                    activity.GPS = True\n                if ('ele' in pt):\n                    wp.Location.Altitude = float(pt['ele'])\n            if ('bpm' in pt):\n                wp.HR = pt['bpm']\n            lap.Waypoints.append(wp)\n    activity.Stationary = (len(lap.Waypoints) == 0)\n    return activity\n", "label": 1}
{"function": "\n\ndef AddLine(self, line):\n    'Adds a line of text to the block. Paragraph type is auto-determined.'\n    if self.paragraphs.IsType(paragraph.CodeBlock):\n        if line.startswith('<'):\n            self.paragraphs.Close()\n            line = line[1:].lstrip()\n            if line:\n                self.AddLine(line)\n            return\n        if (line[:1] not in ' \\t'):\n            self.paragraphs.Close()\n            self.AddLine(line)\n            return\n        self.paragraphs.AddLine(line)\n        return\n    self._ParseArgs(line)\n    if (not line.strip()):\n        self.paragraphs.SetType(paragraph.BlankLine)\n        return\n    match = regex.list_item.match((line or ''))\n    if match:\n        leader = match.group(1)\n        self.paragraphs.Close()\n        line = regex.list_item.sub('', line)\n        self.paragraphs.SetType(paragraph.ListItem, leader)\n        self.paragraphs.AddLine(line)\n        return\n    if (line and (line[:1] in ' \\t')):\n        if self.paragraphs.IsType(paragraph.ListItem):\n            self.paragraphs.AddLine(line.lstrip())\n            return\n    elif self.paragraphs.IsType(paragraph.ListItem):\n        self.paragraphs.Close()\n    self.paragraphs.SetType(paragraph.TextParagraph)\n    if ((line == '>') or line.endswith(' >')):\n        line = line[:(- 1)].rstrip()\n        if line:\n            self.paragraphs.AddLine(line)\n        self.paragraphs.SetType(paragraph.CodeBlock)\n        return\n    self.paragraphs.AddLine(line)\n", "label": 1}
{"function": "\n\ndef convertPyClassOrFunctionDefinitionToForaFunctionExpression(self, classOrFunctionDefinition, objectIdToObjectDefinition):\n    pyAst = self.convertClassOrFunctionDefinitionToNativePyAst(classOrFunctionDefinition, objectIdToObjectDefinition)\n    assert (pyAst is not None)\n    sourcePath = objectIdToObjectDefinition[classOrFunctionDefinition.sourceFileId].path\n    tr = None\n    if isinstance(classOrFunctionDefinition, TypeDescription.FunctionDefinition):\n        if (isinstance(pyAst, ForaNative.PythonAstStatement) and pyAst.isFunctionDef()):\n            tr = self.nativeConverter.convertPythonAstFunctionDefToForaOrParseError(pyAst.asFunctionDef, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]))\n        else:\n            assert pyAst.isLambda()\n            tr = self.nativeConverter.convertPythonAstLambdaToForaOrParseError(pyAst.asLambda, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]))\n    elif isinstance(classOrFunctionDefinition, TypeDescription.ClassDefinition):\n        objectIdToFreeVar = {v: k for (k, v) in classOrFunctionDefinition.freeVariableMemberAccessChainsToId.iteritems()}\n        baseClasses = [objectIdToFreeVar[baseId].split('.') for baseId in classOrFunctionDefinition.baseClassIds]\n        tr = self.nativeConverter.convertPythonAstClassDefToForaOrParseError(pyAst.asClassDef, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]), baseClasses)\n    else:\n        assert False\n    if isinstance(tr, ForaNative.PythonToForaConversionError):\n        raise convertNativePythonToForaConversionError(tr, sourcePath)\n    return tr\n", "label": 1}
{"function": "\n\ndef __init__(self, identifier, buckets=None, **kwargs):\n    super(SongProxy, self).__init__()\n    buckets = (buckets or [])\n    self.id = identifier\n    self._object_type = 'song'\n    kwargs = dict(((str(k), v) for (k, v) in kwargs.iteritems()))\n    if kwargs.has_key('track_id'):\n        self.track_id = kwargs['track_id']\n    if kwargs.has_key('tag'):\n        self.tag = kwargs['tag']\n    if kwargs.has_key('score'):\n        self.score = kwargs['score']\n    if kwargs.has_key('audio'):\n        self.audio = kwargs['audio']\n    if kwargs.has_key('release_image'):\n        self.release_image = kwargs['release_image']\n    core_attrs = ['title', 'artist_name', 'artist_id']\n    if (not all(((ca in kwargs) for ca in core_attrs))):\n        profile = self.get_attribute('profile', **{\n            'id': self.id,\n            'bucket': buckets,\n        })\n        kwargs.update(profile.get('songs')[0])\n    [self.__dict__.update({\n        ca: kwargs.pop(ca),\n    }) for ca in core_attrs]\n    self.cache.update(kwargs)\n", "label": 1}
{"function": "\n\ndef test_manage_job():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == False)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef init_app(self, app, dsn=None, logging=None, level=None, logging_exclusions=None, wrap_wsgi=None, register_signal=None):\n    if (dsn is not None):\n        self.dsn = dsn\n    if (level is not None):\n        self.level = level\n    if (wrap_wsgi is not None):\n        self.wrap_wsgi = wrap_wsgi\n    elif (self.wrap_wsgi is None):\n        if (app and app.debug):\n            self.wrap_wsgi = False\n        else:\n            self.wrap_wsgi = True\n    if (register_signal is not None):\n        self.register_signal = register_signal\n    if (logging is not None):\n        self.logging = logging\n    if (logging_exclusions is not None):\n        self.logging_exclusions = logging_exclusions\n    if (not self.client):\n        self.client = make_client(self.client_cls, app, self.dsn)\n    if self.logging:\n        kwargs = {\n            \n        }\n        if (self.logging_exclusions is not None):\n            kwargs['exclude'] = self.logging_exclusions\n        setup_logging(SentryHandler(self.client, level=self.level), **kwargs)\n    if self.wrap_wsgi:\n        app.wsgi_app = SentryMiddleware(app.wsgi_app, self.client)\n    app.before_request(self.before_request)\n    if self.register_signal:\n        got_request_exception.connect(self.handle_exception, sender=app)\n        request_finished.connect(self.after_request, sender=app)\n    if (not hasattr(app, 'extensions')):\n        app.extensions = {\n            \n        }\n    app.extensions['sentry'] = self\n", "label": 1}
{"function": "\n\ndef test_numpy_geometric(self):\n    geom = jit_unary('np.random.geometric')\n    self.assertRaises(ValueError, geom, (- 1.0))\n    self.assertRaises(ValueError, geom, 0.0)\n    self.assertRaises(ValueError, geom, 1.001)\n    N = 200\n    r = [geom(1.0) for i in range(N)]\n    self.assertPreciseEqual(r, ([1] * N))\n    r = [geom(0.9) for i in range(N)]\n    n = r.count(1)\n    self.assertGreaterEqual(n, (N // 2))\n    self.assertLess(n, N)\n    self.assertFalse([i for i in r if (i > 1000)])\n    r = [geom(0.4) for i in range(N)]\n    self.assertTrue([i for i in r if (i > 4)])\n    r = [geom(0.01) for i in range(N)]\n    self.assertTrue([i for i in r if (i > 50)])\n    r = [geom(1e-15) for i in range(N)]\n    self.assertTrue([i for i in r if (i > (2 ** 32))])\n", "label": 1}
{"function": "\n\n@synchronizedDeferred(busy)\n@deferredAsThread\ndef garbageCheck(self):\n    'Check for file patterns that are removeable'\n    watchDict = copy.deepcopy(self.watchDict)\n    for (directory, garbageList) in watchDict.iteritems():\n        if (not os.path.exists(directory)):\n            continue\n        for (pattern, limit) in garbageList:\n            self.cleanupLinks(directory)\n            files = [os.path.join(directory, f) for f in os.listdir(directory) if re.search(pattern, f)]\n            files = sorted(files)\n            if (len(files) > int(limit)):\n                log(('These files matched:\\n\\t%s' % '\\n\\t'.join(files)))\n            while (len(files) > int(limit)):\n                oldfile = files.pop(0)\n                log(('Deleting %s' % oldfile))\n                if os.path.islink(oldfile):\n                    continue\n                if os.path.isdir(oldfile):\n                    for (base, dirs, myfiles) in os.walk(oldfile, topdown=False):\n                        for name in myfiles:\n                            os.remove(os.path.join(base, name))\n                        for name in dirs:\n                            os.rmdir(os.path.join(base, name))\n                    os.rmdir(oldfile)\n                else:\n                    os.unlink(oldfile)\n        self.cleanupLinks(directory)\n", "label": 1}
{"function": "\n\ndef _insert(self, data):\n    if isinstance(data, list):\n        return [self._insert(item) for item in data]\n    if (not all((isinstance(k, string_types) for k in data))):\n        raise ValueError('Document keys must be strings')\n    if ('_id' not in data):\n        data['_id'] = ObjectId()\n    object_id = data['_id']\n    if isinstance(object_id, dict):\n        object_id = helpers.hashdict(object_id)\n    if (object_id in self._documents):\n        raise DuplicateKeyError('Duplicate Key Error', 11000)\n    for unique in self._uniques:\n        find_kwargs = {\n            \n        }\n        for (key, direction) in unique:\n            if (key in data):\n                find_kwargs[key] = data[key]\n        answer = self.find(find_kwargs)\n        if (answer.count() > 0):\n            raise DuplicateKeyError('Duplicate Key Error', 11000)\n    self._documents[object_id] = self._internalize_dict(data)\n    return data['_id']\n", "label": 1}
{"function": "\n\ndef suggest(self):\n    pattern = QtCore.QRegExp('\\\\w+$')\n    cursor = self._editor.textCursor()\n    block = cursor.block()\n    text = block.text()\n    if ((not self._room) or (not self._room.users) or (not text) or cursor.hasSelection()):\n        return False\n    blockText = QtCore.QString(text[:(cursor.position() - block.position())])\n    matchPosition = blockText.indexOf(pattern)\n    if (matchPosition < 0):\n        return False\n    word = blockText[matchPosition:]\n    if word.trimmed().isEmpty():\n        return False\n    matchingUserNames = []\n    for user in self._room.users:\n        if QtCore.QString(user['name']).startsWith(word, QtCore.Qt.CaseInsensitive):\n            matchingUserNames.append(user['name'])\n    if (len(matchingUserNames) == 1):\n        self._replace(cursor, word, (matchingUserNames[0] + (': ' if (matchPosition == 0) else ' ')))\n    else:\n        menu = QtGui.QMenu('Suggestions', self._editor)\n        for userName in matchingUserNames:\n            action = QtGui.QAction(userName, menu)\n            action.setData((cursor, word, userName, matchPosition))\n            self.connect(action, QtCore.SIGNAL('triggered()'), self._userSelected)\n            menu.addAction(action)\n        menu.popup(self._editor.mapToGlobal(self._editor.cursorRect().center()))\n", "label": 1}
{"function": "\n\ndef test_option():\n    kernel = get_kernel()\n    d = Dummy(kernel)\n    assert ('Options:' in d.line_dummy.__doc__)\n    assert ('--size' in d.line_dummy.__doc__)\n    ret = d.call_magic('line', 'dummy', '', 'hey -s400,200')\n    assert (ret == d)\n    assert (d.foo == 'hey'), d.foo\n    assert (d.size == (400, 200))\n    ret = d.call_magic('line', 'dummy', '', 'hey there')\n    assert (d.foo == 'hey there')\n    ret = d.call_magic('line', 'dummy', '', 'range(1, 10)')\n    assert (d.foo == range(1, 10))\n    ret = d.call_magic('line', 'dummy', '', '[1, 2, 3]')\n    assert (d.foo == [1, 2, 3])\n    ret = d.call_magic('line', 'dummy', '', 'hey -l -s400,200')\n    assert (d.size == (400, 200))\n    assert (d.foo == 'hey -l')\n    ret = d.call_magic('line', 'dummy', '', 'hey -s -- -s400,200')\n    assert (d.size == (400, 200))\n    assert (d.foo == 'hey -s')\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_line(line):\n    line = line.rstrip()\n    fields = line.split('\\t')\n    (contig, start, stop) = (fields[0], int(fields[1]), int(fields[2]))\n    n = len(fields)\n    name = ((fields[3] or None) if (n >= 4) else None)\n    score = ((fields[4] or None) if (n >= 5) else None)\n    strand = ((fields[5] or None) if (n >= 6) else None)\n    thick_start = ((fields[6] or None) if (n >= 7) else None)\n    thick_end = ((fields[7] or None) if (n >= 8) else None)\n    item_rgb = ((fields[8] or None) if (n >= 9) else None)\n    return BedRecord(contig, start, stop, name, score, strand, thick_start, thick_end, item_rgb)\n", "label": 1}
{"function": "\n\ndef _plot_timeseries(ax, ch_idx, tmin, tmax, vmin, vmax, ylim, data, color, times, vline=None, x_label=None, y_label=None, colorbar=False, hline=None):\n    'Aux function to show time series on topo split across multiple axes'\n    import matplotlib.pyplot as plt\n    picker_flag = False\n    for (data_, color_) in zip(data, color):\n        if (not picker_flag):\n            ax.plot(times, data_[ch_idx], color_, picker=1000000000.0)\n            picker_flag = True\n        else:\n            ax.plot(times, data_[ch_idx], color_)\n    if vline:\n        for x in vline:\n            plt.axvline(x, color='w', linewidth=0.5)\n    if hline:\n        for y in hline:\n            plt.axhline(y, color='w', linewidth=0.5)\n    if (x_label is not None):\n        plt.xlabel(x_label)\n    if (y_label is not None):\n        if isinstance(y_label, list):\n            plt.ylabel(y_label[ch_idx])\n        else:\n            plt.ylabel(y_label)\n    if colorbar:\n        plt.colorbar()\n", "label": 1}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_freq():\n    ' reading/writing of 3D RNMRTK frequency domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_3d', 'freq_3d.sec'))\n    assert (data.shape == (128, 128, 4096))\n    assert (np.abs((data[(0, 1, 2)] - 3.23)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)] - 1.16)) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef get_fields_to_translatable_models(model):\n    if (model in _F2TM_CACHE):\n        return _F2TM_CACHE[model]\n    results = []\n    if NEW_META_API:\n        for f in model._meta.get_fields():\n            if (f.is_relation and f.related_model):\n                if (get_translatable_fields_for_model(f.related_model) is not None):\n                    results.append((f.name, f.related_model))\n    else:\n        for field_name in model._meta.get_all_field_names():\n            (field_object, modelclass, direct, m2m) = model._meta.get_field_by_name(field_name)\n            if (direct and isinstance(field_object, RelatedField)):\n                if (get_translatable_fields_for_model(field_object.related.parent_model) is not None):\n                    results.append((field_name, field_object.related.parent_model))\n            if isinstance(field_object, RelatedObject):\n                if (get_translatable_fields_for_model(field_object.model) is not None):\n                    results.append((field_name, field_object.model))\n    _F2TM_CACHE[model] = dict(results)\n    return _F2TM_CACHE[model]\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = Archive()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = InvalidOperation()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef serialize(self, previous):\n    params = {\n        \n    }\n    unsaved_keys = (self._unsaved_values or set())\n    previous = (previous or self._previous or {\n        \n    })\n    for (k, v) in self.items():\n        if ((k == 'id') or (isinstance(k, str) and k.startswith('_'))):\n            continue\n        elif isinstance(v, APIResource):\n            continue\n        elif hasattr(v, 'serialize'):\n            params[k] = v.serialize(previous.get(k, None))\n        elif (k in unsaved_keys):\n            params[k] = _compute_diff(v, previous.get(k, None))\n        elif ((k == 'additional_owners') and (v is not None)):\n            params[k] = _serialize_list(v, previous.get(k, None))\n    return params\n", "label": 1}
{"function": "\n\ndef _extract_monitor_data(self, monitor):\n    if (self._monitor_type is not None):\n        raise TypeError('Your result `%s` already extracted data from a `%s` monitor. Please use a new empty result for a new monitor.')\n    self._monitor_type = monitor.__class__.__name__\n    if isinstance(monitor, SpikeCounter):\n        self._extract_spike_counter(monitor)\n    elif isinstance(monitor, VanRossumMetric):\n        self._extract_van_rossum_metric(monitor)\n    elif isinstance(monitor, PopulationSpikeCounter):\n        self._extract_population_spike_counter(monitor)\n    elif isinstance(monitor, StateSpikeMonitor):\n        self._extract_state_spike_monitor(monitor)\n    elif isinstance(monitor, PopulationRateMonitor):\n        self._extract_population_rate_monitor(monitor)\n    elif isinstance(monitor, ISIHistogramMonitor):\n        self._extract_isi_hist_monitor(monitor)\n    elif isinstance(monitor, SpikeMonitor):\n        self._extract_spike_monitor(monitor)\n    elif isinstance(monitor, MultiStateMonitor):\n        self._extract_multi_state_monitor(monitor)\n    elif isinstance(monitor, StateMonitor):\n        self._extract_state_monitor(monitor)\n    else:\n        raise ValueError(('Monitor Type %s is not supported (yet)' % str(type(monitor))))\n", "label": 1}
{"function": "\n\ndef Execute(self, opt, args):\n    self.opt = opt\n    project_list = self.GetProjects(args)\n    pending = []\n    branch = None\n    if opt.branch:\n        branch = opt.branch\n    for project in project_list:\n        if opt.current_branch:\n            cbr = project.CurrentBranch\n            up_branch = project.GetUploadableBranch(cbr)\n            if up_branch:\n                avail = [up_branch]\n            else:\n                avail = None\n                print(('ERROR: Current branch (%s) not pushable. You may be able to type \"git branch --set-upstream-to m/master\" to fix your branch.' % str(cbr)), file=sys.stderr)\n        else:\n            avail = project.GetUploadableBranches(branch)\n        if avail:\n            pending.append((project, avail))\n    if (pending and (not opt.bypass_hooks)):\n        hook = RepoHook('pre-push', self.manifest.repo_hooks_project, self.manifest.topdir, abort_if_user_denies=True)\n        pending_proj_names = [project.name for (project, avail) in pending]\n        pending_worktrees = [project.worktree for (project, avail) in pending]\n        try:\n            hook.Run(opt.allow_all_hooks, project_list=pending_proj_names, worktree_list=pending_worktrees)\n        except HookError as e:\n            print(('ERROR: %s' % str(e)), file=sys.stderr)\n            return\n    if (not pending):\n        print('no branches ready for push', file=sys.stderr)\n    elif ((len(pending) == 1) and (len(pending[0][1]) == 1)):\n        self._SingleBranch(opt, pending[0][1][0])\n    else:\n        self._MultipleBranches(opt, pending)\n", "label": 1}
{"function": "\n\ndef to_json(self, user):\n    ret = super(GitHubNodeSettings, self).to_json(user)\n    user_settings = user.get_addon('github')\n    ret.update({\n        'user_has_auth': (user_settings and user_settings.has_auth),\n        'is_registration': self.owner.is_registration,\n    })\n    if (self.user_settings and self.user_settings.has_auth):\n        valid_credentials = False\n        owner = self.user_settings.owner\n        connection = GitHubClient(external_account=self.external_account)\n        valid_credentials = True\n        try:\n            repos = itertools.chain.from_iterable((connection.repos(), connection.my_org_repos()))\n            repo_names = ['{0} / {1}'.format(repo.owner.login, repo.name) for repo in repos]\n        except GitHubError:\n            repo_names = []\n            valid_credentials = False\n        if (owner == user):\n            ret.update({\n                'repo_names': repo_names,\n            })\n        ret.update({\n            'node_has_auth': True,\n            'github_user': (self.user or ''),\n            'github_repo': (self.repo or ''),\n            'github_repo_full_name': ('{0} / {1}'.format(self.user, self.repo) if (self.user and self.repo) else ''),\n            'auth_osf_name': owner.fullname,\n            'auth_osf_url': owner.url,\n            'auth_osf_id': owner._id,\n            'github_user_name': self.external_account.display_name,\n            'github_user_url': self.external_account.profile_url,\n            'is_owner': (owner == user),\n            'valid_credentials': valid_credentials,\n            'addons_url': web_url_for('user_addons'),\n            'files_url': self.owner.web_url_for('collect_file_trees'),\n        })\n    return ret\n", "label": 1}
{"function": "\n\ndef _children_updated(self, object, name, event):\n    ' Handles the children of a node being changed.\\n        '\n    name = name[:(- 6)]\n    self.log_change(self._get_undo_item, object, name, event)\n    start = event.index\n    n = len(event.added)\n    end = (start + len(event.removed))\n    tree = self._tree\n    for (expanded, node, nid) in self._object_info_for(object, name):\n        children = node.get_children(object)\n        if expanded:\n            for cnid in self._nodes_for(nid)[start:end]:\n                self._delete_node(cnid)\n            remaining = (len(children) - len(event.removed))\n            child_index = 0\n            for child in event.added:\n                (child, child_node) = self._node_for(child)\n                if (child_node is not None):\n                    insert_index = ((start + child_index) if (start <= remaining) else None)\n                    self._insert_node(nid, insert_index, child_node, child)\n                    child_index += 1\n        else:\n            dummy = getattr(nid, '_dummy', None)\n            if ((dummy is None) and (len(children) > 0)):\n                nid._dummy = QtGui.QTreeWidgetItem(nid)\n            elif ((dummy is not None) and (len(children) == 0)):\n                nid.removeChild(dummy)\n                del nid._dummy\n        if node.can_auto_open(object):\n            nid.setExpanded(True)\n", "label": 1}
{"function": "\n\n@setup_cache\ndef test_set_get_delete(cache):\n    for value in range(100):\n        cache.set(value, value)\n    cache.check()\n    for value in range(100):\n        assert (cache.get(value) == value)\n    cache.check()\n    for value in range(100):\n        assert (value in cache)\n    cache.check()\n    for value in range(100):\n        assert cache.delete(value)\n    assert (cache.delete(100) == False)\n    cache.check()\n    for value in range(100):\n        cache[value] = value\n    cache.check()\n    for value in range(100):\n        assert (cache[value] == value)\n    cache.check()\n    cache.clear()\n    assert (len(cache) == 0)\n    cache.check()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.settings = BootstrapSettings()\n                self.settings.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef setup_app(app, config):\n    \"Configure flask app.\\n\\n    :param app: Flask app object\\n    :param config: api's config\\n    :type  config: dict\\n    \"\n    flask_config = (config.get('flask') or {\n        \n    })\n    for (key, value) in flask_config.items():\n        if (key == 'PERMANENT_SESSION_LIFETIME'):\n            app.config[key] = datetime.timedelta(days=int(value))\n        else:\n            app.config[key] = value\n    app_config = (config.get('app') or {\n        \n    })\n    for (key, value) in app_config.items():\n        app.config[key] = value\n    app.config['plugins'] = (app.config.get('plugins') or {\n        \n    })\n    for (name, config) in app.config['plugins'].items():\n        plugin_class = get_cls_with_path(config.pop('class'))\n        plugin = plugin_class(config)\n        app.config['plugins'][name] = plugin\n    default_class_sign = ''\n    app.config['responses'] = (app.config.get('responses') or {\n        \n    })\n    for (name, class_path) in app.config['responses'].items():\n        if (name == 'default'):\n            default_class_sign = class_path\n        else:\n            response_class = get_cls_with_path(class_path)\n            app.config['responses'][name] = response_class\n    app.config['responses']['default'] = app.config['responses'][default_class_sign]\n", "label": 1}
{"function": "\n\ndef envs(ignore_cache=False):\n    '\\n    Return a list of refs that can be used as environments\\n    '\n    if (not ignore_cache):\n        env_cache = os.path.join(__opts__['cachedir'], 'hgfs/envs.p')\n        cache_match = salt.fileserver.check_env_cache(__opts__, env_cache)\n        if (cache_match is not None):\n            return cache_match\n    ret = set()\n    for repo in init():\n        repo['repo'].open()\n        if (repo['branch_method'] in ('branches', 'mixed')):\n            for branch in _all_branches(repo['repo']):\n                branch_name = branch[0]\n                if (branch_name == repo['base']):\n                    branch_name = 'base'\n                ret.add(branch_name)\n        if (repo['branch_method'] in ('bookmarks', 'mixed')):\n            for bookmark in _all_bookmarks(repo['repo']):\n                bookmark_name = bookmark[0]\n                if (bookmark_name == repo['base']):\n                    bookmark_name = 'base'\n                ret.add(bookmark_name)\n        ret.update([x[0] for x in _all_tags(repo['repo'])])\n        repo['repo'].close()\n    return [x for x in sorted(ret) if _env_is_exposed(x)]\n", "label": 1}
{"function": "\n\ndef get_stack(message=None, stack_first_frame=1, max_frames=15):\n    try:\n        stack = inspect.stack()\n        frame_num = stack_first_frame\n        context = []\n        while ((len(stack) > frame_num) and (frame_num < (max_frames + stack_first_frame))):\n            exec_line = ('%s:%s:%s' % (stack[frame_num][1], stack[frame_num][2], stack[frame_num][3]))\n            context.insert(0, exec_line)\n            if (exec_line.endswith('_control_flow') or exec_line.endswith('load_ion') or exec_line.endswith('spawn_process') or exec_line.endswith(':main') or exec_line.endswith(':dispatch_request')):\n                break\n            frame_num += 1\n        stack_str = '\\n '.join(context)\n        if message:\n            stack_str = ((message + '\\n') + stack_str)\n        return stack_str\n    except Exception as ex:\n        stack_str = ('ERROR: ' + str(ex))\n        if message:\n            stack_str = ((message + '\\n') + stack_str)\n        return stack_str\n", "label": 1}
{"function": "\n\ndef _check_workers(self):\n    while (not self.stopped.is_set()):\n        for (worker, info) in self.worker_tracker.workers.iteritems():\n            if ((int(time.time()) - info.last_update) > HEARTBEAT_CHECK_INTERVAL):\n                info.continous_register = 0\n                if (info.status == RUNNING):\n                    info.status = HANGUP\n                elif (info.status == HANGUP):\n                    info.status = STOPPED\n                    self.black_list.append(worker)\n                    for job in self.job_tracker.running_jobs:\n                        self.job_tracker.remove_worker(job, worker)\n            elif (info.continous_register >= CONTINOUS_HEARTBEAT):\n                if (info.status != RUNNING):\n                    info.status = RUNNING\n                if (worker in self.black_list):\n                    self.black_list.remove(worker)\n                for job in self.job_tracker.running_jobs:\n                    if (not client_call(worker, 'has_job')):\n                        client_call(worker, 'prepare', job)\n                        client_call(worker, 'run_job', job)\n                    self.job_tracker.add_worker(job, worker)\n        self.stopped.wait(HEARTBEAT_CHECK_INTERVAL)\n", "label": 1}
{"function": "\n\ndef addAndFixActions(startDict, actions):\n    curDict = copy.copy(startDict)\n    for action in actions:\n        new_ops = []\n        for op in action.db_operations:\n            if (op.vtType == 'add'):\n                if ((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId))):\n                    curDict[(op.db_what, op.db_objectId)] = op\n                    new_ops.append(op)\n            elif (op.vtType == 'change'):\n                if (curDict.has_key((op.db_what, op.db_oldObjId)) and ((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId)))):\n                    del curDict[(op.db_what, op.db_oldObjId)]\n                    curDict[(op.db_what, op.db_newObjId)] = op\n                    new_ops.append(op)\n            elif (op.vtType == 'delete'):\n                if (((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId))) and curDict.has_key((op.db_what, op.db_objectId))):\n                    del curDict[(op.db_what, op.db_objectId)]\n                    new_ops.append(op)\n        action.db_operations = new_ops\n    return curDict\n", "label": 1}
{"function": "\n\ndef test_time_dep_bra():\n    b = TimeDepBra(0, t)\n    assert isinstance(b, TimeDepBra)\n    assert isinstance(b, BraBase)\n    assert isinstance(b, StateBase)\n    assert isinstance(b, QExpr)\n    assert (b.label == (Integer(0),))\n    assert (b.args == (Integer(0), t))\n    assert (b.time == t)\n    assert (b.dual_class() == TimeDepKet)\n    assert (b.dual == TimeDepKet(0, t))\n    k = TimeDepBra(x, 0.5)\n    assert (k.label == (x,))\n    assert (k.args == (x, sympify(0.5)))\n    assert (TimeDepBra() == TimeDepBra('psi', 't'))\n", "label": 1}
{"function": "\n\ndef handle_text(self):\n    '\\n        Takes care of converting body text to unicode, if its text at all.\\n        Sets self.original_encoding to original char encoding, and converts body\\n        to unicode if possible. Must come after handle_compression, and after\\n        self.mediaType is valid.\\n        '\n    self.encoding = None\n    if (self.mediaType and ((self.mediaType.type == 'text') or ((self.mediaType.type == 'application') and ('xml' in self.mediaType.subtype)))):\n        if ('charset' in self.mediaType.params):\n            override_encodings = [self.mediaType.params['charset']]\n        else:\n            override_encodings = []\n        if (self.body != ''):\n            if UnicodeDammit:\n                dammit = UnicodeDammit(self.body, override_encodings)\n                if dammit.unicode:\n                    self.text = dammit.unicode\n                    self.originalEncoding = dammit.originalEncoding\n                else:\n                    pass\n            else:\n                u = None\n                for e in (override_encodings + ['utf8', 'iso-8859-1']):\n                    try:\n                        u = self.body.decode(e, 'strict')\n                        self.originalEncoding = e\n                        break\n                    except UnicodeError:\n                        pass\n                if (not u):\n                    u = self.body.decode('utf8', 'replace')\n                    self.originalEncoding = None\n                self.text = (u or None)\n    else:\n        self.text = b64encode(self.body)\n        self.encoding = 'base64'\n", "label": 1}
{"function": "\n\ndef GetHandlerType(self):\n    'Get handler type of mapping.\\n\\n    Returns:\\n      Handler type determined by which handler id attribute is set.\\n\\n    Raises:\\n      UnknownHandlerType: when none of the no handler id attributes are set.\\n\\n      UnexpectedHandlerAttribute: when an unexpected attribute is set for the\\n        discovered handler type.\\n\\n      HandlerTypeMissingAttribute: when the handler is missing a\\n        required attribute for its handler type.\\n\\n      MissingHandlerAttribute: when a URL handler is missing an attribute\\n    '\n    if (getattr(self, HANDLER_API_ENDPOINT) is not None):\n        mapping_type = HANDLER_API_ENDPOINT\n    else:\n        for id_field in URLMap.ALLOWED_FIELDS.iterkeys():\n            if (getattr(self, id_field) is not None):\n                mapping_type = id_field\n                break\n        else:\n            raise appinfo_errors.UnknownHandlerType(('Unknown url handler type.\\n%s' % str(self)))\n    allowed_fields = URLMap.ALLOWED_FIELDS[mapping_type]\n    for attribute in self.ATTRIBUTES.iterkeys():\n        if ((getattr(self, attribute) is not None) and (not ((attribute in allowed_fields) or (attribute in URLMap.COMMON_FIELDS) or (attribute == mapping_type)))):\n            raise appinfo_errors.UnexpectedHandlerAttribute(('Unexpected attribute \"%s\" for mapping type %s.' % (attribute, mapping_type)))\n    if ((mapping_type == HANDLER_STATIC_FILES) and (not self.upload)):\n        raise appinfo_errors.MissingHandlerAttribute(('Missing \"%s\" attribute for URL \"%s\".' % (UPLOAD, self.url)))\n    return mapping_type\n", "label": 1}
{"function": "\n\ndef test_include_1():\n    sub_ffi = FFI()\n    sub_ffi.cdef('static const int k2 = 121212;')\n    sub_ffi.include(original_ffi)\n    assert ('macro FOOBAR' in original_ffi._parser._declarations)\n    assert ('macro FOOBAZ' in original_ffi._parser._declarations)\n    sub_ffi.set_source('re_python_pysrc', None)\n    sub_ffi.emit_python_code(str(tmpdir.join('_re_include_1.py')))\n    if (sys.version_info[:2] >= (3, 3)):\n        import importlib\n        importlib.invalidate_caches()\n    from _re_include_1 import ffi\n    assert (ffi.integer_const('FOOBAR') == (- 42))\n    assert (ffi.integer_const('FOOBAZ') == (- 43))\n    assert (ffi.integer_const('k2') == 121212)\n    lib = ffi.dlopen(extmod)\n    assert (lib.FOOBAR == (- 42))\n    assert (lib.FOOBAZ == (- 43))\n    assert (lib.k2 == 121212)\n    p = ffi.new('bar_t *', [5, b'foobar'])\n    assert (p.a[4] == ord('a'))\n", "label": 1}
{"function": "\n\ndef parse_long(tokens, options):\n    (raw, eq, value) = tokens.move().partition('=')\n    value = (None if (eq == value == '') else value)\n    opt = [o for o in options if (o.long and o.long.startswith(raw))]\n    if (len(opt) < 1):\n        if (tokens.error is DocoptExit):\n            raise tokens.error(('%s is not recognized' % raw))\n        else:\n            o = Option(None, raw, (1 if (eq == '=') else 0))\n            options.append(o)\n            return [o]\n    if (len(opt) > 1):\n        raise tokens.error(('%s is not a unique prefix: %s?' % (raw, ', '.join((('%s' % o.long) for o in opt)))))\n    opt = copy(opt[0])\n    if (opt.argcount == 1):\n        if (value is None):\n            if (tokens.current() is None):\n                raise tokens.error(('%s requires argument' % opt.name))\n            value = tokens.move()\n    elif (value is not None):\n        raise tokens.error(('%s must not have an argument' % opt.name))\n    opt.value = (value or True)\n    return [opt]\n", "label": 1}
{"function": "\n\ndef initialize_locksets(self):\n    log.debug('initializing locksets')\n    v = self.sign(VoteBlock(0, 0, self.chainservice.chain.genesis.hash))\n    self.add_vote(v)\n    head_proposal = self.load_proposal(self.head.hash)\n    if head_proposal:\n        assert (head_proposal.blockhash == self.head.hash)\n        for v in head_proposal.signing_lockset:\n            self.add_vote(v)\n        assert self.heights[(self.head.header.number - 1)].has_quorum\n    last_committing_lockset = self.load_last_committing_lockset()\n    if last_committing_lockset:\n        assert (last_committing_lockset.has_quorum == self.head.hash)\n        for v in last_committing_lockset.votes:\n            self.add_vote(v)\n        assert self.heights[self.head.header.number].has_quorum\n    else:\n        assert (self.head.header.number == 0)\n    assert self.highest_committing_lockset\n    assert self.last_committing_lockset\n    assert self.last_valid_lockset\n", "label": 1}
{"function": "\n\ndef distrib_id():\n    '\\n    Get the OS distribution ID.\\n\\n    Returns a string such as ``\"Debian\"``, ``\"Ubuntu\"``, ``\"RHEL\"``,\\n    ``\"CentOS\"``, ``\"SLES\"``, ``\"Fedora\"``, ``\"Arch\"``, ``\"Gentoo\"``,\\n    ``\"SunOS\"``...\\n\\n    Example::\\n\\n        from fabtools.system import distrib_id\\n\\n        if distrib_id() != \\'Debian\\':\\n            abort(u\"Distribution is not supported\")\\n\\n    '\n    with settings(hide('running', 'stdout')):\n        kernel = run('uname -s')\n        if (kernel == 'Linux'):\n            if is_file('/usr/bin/lsb_release'):\n                id_ = run('lsb_release --id --short')\n                if (id in ['arch', 'Archlinux']):\n                    id_ = 'Arch'\n                return id_\n            elif is_file('/etc/debian_version'):\n                return 'Debian'\n            elif is_file('/etc/fedora-release'):\n                return 'Fedora'\n            elif is_file('/etc/arch-release'):\n                return 'Arch'\n            elif is_file('/etc/redhat-release'):\n                release = run('cat /etc/redhat-release')\n                if release.startswith('Red Hat Enterprise Linux'):\n                    return 'RHEL'\n                elif release.startswith('CentOS'):\n                    return 'CentOS'\n                elif release.startswith('Scientific Linux'):\n                    return 'SLES'\n            elif is_file('/etc/gentoo-release'):\n                return 'Gentoo'\n        elif (kernel == 'SunOS'):\n            return 'SunOS'\n", "label": 1}
{"function": "\n\ndef handle_msg(self, proto, msg):\n    if (msg[0:3] not in ['GET', 'SET', 'ADD', 'RES', 'UPD']):\n        return\n    if (msg[0:3] in ['GET']):\n        (_, key) = msg.split(' ', 1)\n        key = json.loads(key)\n        if (key in self.dict):\n            item = self.dict[key]\n            proto.send(('RES ' + json.dumps([key, item])))\n        else:\n            proto.send(('RES ' + json.dumps([key, None])))\n    if (msg[0:3] in ['SET']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        self.dict[key] = value\n    if (msg[0:3] in ['ADD']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if (key not in self.dict):\n            self.dict[key] = []\n        self.dict[key].append(value)\n    if (msg[0:3] in ['RES']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if (key in self.queue):\n            self.queue[key].put(value)\n    if (msg[0:3] in ['UPD']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if ((key not in self.dict) or (self.dict[key] is not value)):\n            self.dict[key] = value\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _from_db_object(context, fixedip, db_fixedip, expected_attrs=None):\n    if (expected_attrs is None):\n        expected_attrs = []\n    for field in fixedip.fields:\n        if (field == 'default_route'):\n            continue\n        if (field not in FIXED_IP_OPTIONAL_ATTRS):\n            fixedip[field] = db_fixedip[field]\n    if ('instance' in expected_attrs):\n        fixedip.instance = (objects.Instance._from_db_object(context, objects.Instance(context), db_fixedip['instance']) if db_fixedip['instance'] else None)\n    if ('network' in expected_attrs):\n        fixedip.network = (objects.Network._from_db_object(context, objects.Network(context), db_fixedip['network']) if db_fixedip['network'] else None)\n    if ('virtual_interface' in expected_attrs):\n        db_vif = db_fixedip['virtual_interface']\n        vif = (objects.VirtualInterface._from_db_object(context, objects.VirtualInterface(context), db_fixedip['virtual_interface']) if db_vif else None)\n        fixedip.virtual_interface = vif\n    if ('floating_ips' in expected_attrs):\n        fixedip.floating_ips = obj_base.obj_make_list(context, objects.FloatingIPList(context), objects.FloatingIP, db_fixedip['floating_ips'])\n    fixedip._context = context\n    fixedip.obj_reset_changes()\n    return fixedip\n", "label": 1}
{"function": "\n\ndef clean(self):\n    cleaned = super(AuthorizeRequestTokenForm, self).clean()\n    t = Token.objects.get(id=cleaned.get('obj_id'))\n    default_scopes = t.scope.split(' ')\n    scopes = cleaned.get('scopes')\n    if (not scopes):\n        raise forms.ValidationError('You need to select permissions for the client')\n    if (('statements/read/mine' in scopes) and ('statements/read' in scopes)):\n        raise forms.ValidationError(\"'statements/read/mine' and 'statements/read' are conflicting scope values. choose one.\")\n    if ('all' in default_scopes):\n        return cleaned\n    elif ('all' in scopes):\n        raise forms.ValidationError(\"Can't raise permissions beyond what the consumer registered.\")\n    if (set(scopes) != set(default_scopes)):\n        nomatch = [k for k in scopes if (k not in default_scopes)]\n        if (not (('all/read' in nomatch) or (('statements/read' in nomatch) and ('all/read' in default_scopes)) or (('statements/read/mine' in nomatch) and (('all/read' in default_scopes) or ('statements/read' in default_scopes))))):\n            raise forms.ValidationError(\"Can't raise permissions beyond what the consumer registered.\")\n    return cleaned\n", "label": 1}
{"function": "\n\ndef elemwise(op, *args, **kwargs):\n    ' Elementwise operation for dask.Dataframes '\n    columns = kwargs.pop('columns', no_default)\n    _name = ('elemwise-' + tokenize(op, kwargs, *args))\n    args = _maybe_from_pandas(args)\n    from .multi import _maybe_align_partitions\n    args = _maybe_align_partitions(args)\n    dasks = [arg for arg in args if isinstance(arg, (_Frame, Scalar))]\n    dfs = [df for df in dasks if isinstance(df, _Frame)]\n    divisions = dfs[0].divisions\n    n = (len(divisions) - 1)\n    other = [(i, arg) for (i, arg) in enumerate(args) if (not isinstance(arg, (_Frame, Scalar)))]\n    if other:\n        op2 = partial_by_order(op, other)\n    else:\n        op2 = op\n    keys = [((d._keys() * n) if isinstance(d, Scalar) else d._keys()) for d in dasks]\n    dsk = dict((((_name, i), ((op2,) + frs)) for (i, frs) in enumerate(zip(*keys))))\n    dsk = merge(dsk, *[d.dask for d in dasks])\n    if (columns is no_default):\n        if ((len(dfs) >= 2) and (len(dasks) != len(dfs))):\n            msg = 'elemwise with 2 or more DataFrames and Scalar is not supported'\n            raise NotImplementedError(msg)\n        columns = _emulate(op, *args, **kwargs)\n    return _Frame(dsk, _name, columns, divisions)\n", "label": 1}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01, gamma=10, cof=4000):\n    ''\n    samplerate = self.audio.samplerate\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for y in super(Y4, self).walk(N, freq_base, freq_max, start, end, each=False, combine=False, gamma=gamma, cof=cof):\n        y = np.maximum(0, y)\n        if (not combine):\n            if each:\n                for v in y:\n                    (yield v)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\ndef data(self, index, role=QtCore.Qt.DisplayRole):\n    if (not index.isValid()):\n        return QtCore.QVariant()\n    obj = index.internalPointer()\n    obj_type = obj.get_type()\n    if (role == QtCore.Qt.DisplayRole):\n        if (obj.get_type() in (Object.GROUP, Object.USER)):\n            return QtCore.QVariant(obj.get_display_name())\n    elif (role == QtCore.Qt.UserRole):\n        if (obj_type == Object.GROUP):\n            return QtCore.QVariant(obj.gid)\n        elif (obj_type == Object.USER):\n            return QtCore.QVariant(obj.uri)\n    elif (role == QtCore.Qt.ToolTipRole):\n        if (obj_type == Object.USER):\n            tool_tip = ('%s (URI: %s)' % (obj.get_display_name(), obj.uri))\n            return QtCore.QVariant(tool_tip)\n    elif (role == QtCore.Qt.DecorationRole):\n        if (obj_type == Object.USER):\n            return QtGui.QPixmap('../icons/exit.png')\n    return QtCore.QVariant()\n", "label": 1}
{"function": "\n\ndef _ch_neighbor_connectivity(ch_names, neighbors):\n    'Compute sensor connectivity matrix\\n\\n    Parameters\\n    ----------\\n    ch_names : list of str\\n        The channel names.\\n    neighbors : list of list\\n        A list of list of channel names. The neighbors to\\n        which the channels in ch_names are connected with.\\n        Must be of the same length as ch_names.\\n\\n    Returns\\n    -------\\n    ch_connectivity : scipy.sparse matrix\\n        The connectivity matrix.\\n    '\n    if (len(ch_names) != len(neighbors)):\n        raise ValueError('`ch_names` and `neighbors` must have the same length')\n    set_neighbors = set([c for d in neighbors for c in d])\n    rest = (set(ch_names) - set_neighbors)\n    if (len(rest) > 0):\n        raise ValueError('Some of your neighbors are not present in the list of channel names')\n    for neigh in neighbors:\n        if ((not isinstance(neigh, list)) and (not all((isinstance(c, string_types) for c in neigh)))):\n            raise ValueError('`neighbors` must be a list of lists of str')\n    ch_connectivity = np.eye(len(ch_names), dtype=bool)\n    for (ii, neigbs) in enumerate(neighbors):\n        ch_connectivity[(ii, [ch_names.index(i) for i in neigbs])] = True\n    ch_connectivity = sparse.csr_matrix(ch_connectivity)\n    return ch_connectivity\n", "label": 1}
{"function": "\n\ndef pre_build_check():\n    '\\n    Try to verify build tools\\n    '\n    if os.environ.get('CASS_DRIVER_NO_PRE_BUILD_CHECK'):\n        return True\n    try:\n        from distutils.ccompiler import new_compiler\n        from distutils.sysconfig import customize_compiler\n        from distutils.dist import Distribution\n        be = build_ext(Distribution())\n        be.initialize_options()\n        be.finalize_options()\n        have_python_include = any((os.path.isfile(os.path.join(p, 'Python.h')) for p in be.include_dirs))\n        if (not have_python_include):\n            sys.stderr.write((\"Did not find 'Python.h' in %s.\\n\" % (be.include_dirs,)))\n            return False\n        compiler = new_compiler(compiler=be.compiler)\n        customize_compiler(compiler)\n        executables = []\n        if (compiler.compiler_type in ('unix', 'cygwin')):\n            executables = [compiler.executables[exe][0] for exe in ('compiler_so', 'linker_so')]\n        elif (compiler.compiler_type == 'nt'):\n            executables = [getattr(compiler, exe) for exe in ('cc', 'linker')]\n        if executables:\n            from distutils.spawn import find_executable\n            for exe in executables:\n                if (not find_executable(exe)):\n                    sys.stderr.write(('Failed to find %s for compiler type %s.\\n' % (exe, compiler.compiler_type)))\n                    return False\n    except Exception as exc:\n        sys.stderr.write(('%s\\n' % str(exc)))\n        sys.stderr.write('Failed pre-build check. Attempting anyway.\\n')\n    return True\n", "label": 1}
{"function": "\n\n@command\ndef module(self, input='', search_type='prefix', project=None, file=None, module=None, deps=None, sandbox=None, cabal=False, db=None, package=None, source=False, standalone=False):\n    q = {\n        'input': input,\n        'type': search_type,\n    }\n    fs = []\n    if project:\n        fs.append({\n            'project': project,\n        })\n    if file:\n        fs.append({\n            'file': file,\n        })\n    if module:\n        fs.append({\n            'module': module,\n        })\n    if deps:\n        fs.append({\n            'deps': deps,\n        })\n    if sandbox:\n        fs.append({\n            'cabal': {\n                'sandbox': sandbox,\n            },\n        })\n    if cabal:\n        fs.append({\n            'cabal': 'cabal',\n        })\n    if db:\n        fs.append({\n            'db': encode_package_db(db),\n        })\n    if package:\n        fs.append({\n            'package': package,\n        })\n    if source:\n        fs.append('sourced')\n    if standalone:\n        fs.append('standalone')\n    return cmd('module', {\n        'query': q,\n        'filters': fs,\n    }, parse_modules)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if (not isinstance(other, Vulnerability)):\n        raise TypeError((\"Expected Vulnerability, got '%s' instead\" % type(other)))\n    if (other.cves != self.cves):\n        return False\n    if (other.threat != self.threat):\n        return False\n    if (other.name != self.name):\n        return False\n    if (other.cvss != self.cvss):\n        return False\n    if (other.description != self.description):\n        return False\n    if (other.id != self.id):\n        return False\n    if (other.level != self.level):\n        return False\n    if (other.references != self.references):\n        return False\n    for (host, port) in self.hosts:\n        for (o_host, o_port) in other.hosts:\n            if ((o_host == host) and (o_port == port)):\n                break\n        else:\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef check_char_lookup(self, lookup):\n    lname = ('field__' + lookup)\n    mymodel = CharListModel.objects.create(field=['mouldy', 'rotten'])\n    mouldy = CharListModel.objects.filter(**{\n        lname: 'mouldy',\n    })\n    assert (mouldy.count() == 1)\n    assert (mouldy[0] == mymodel)\n    rotten = CharListModel.objects.filter(**{\n        lname: 'rotten',\n    })\n    assert (rotten.count() == 1)\n    assert (rotten[0] == mymodel)\n    clean = CharListModel.objects.filter(**{\n        lname: 'clean',\n    })\n    assert (clean.count() == 0)\n    with pytest.raises(ValueError):\n        list(CharListModel.objects.filter(**{\n            lname: ['a', 'b'],\n        }))\n    both = CharListModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) & Q(**{\n        lname: 'rotten',\n    })))\n    assert (both.count() == 1)\n    assert (both[0] == mymodel)\n    either = CharListModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) | Q(**{\n        lname: 'clean',\n    })))\n    assert (either.count() == 1)\n    not_clean = CharListModel.objects.exclude(**{\n        lname: 'clean',\n    })\n    assert (not_clean.count() == 1)\n    not_mouldy = CharListModel.objects.exclude(**{\n        lname: 'mouldy',\n    })\n    assert (not_mouldy.count() == 0)\n", "label": 1}
{"function": "\n\ndef enqueue(self, pdu):\n    self.log('enqueue {pdu.name} PDU'.format(pdu=pdu))\n    if (not (pdu.type in connection_mode_pdu_types)):\n        self.err('non connection mode pdu on data link connection')\n        pdu = FrameReject.from_pdu(pdu, flags='W', dlc=self)\n        self.close()\n        self.send_queue.append(pdu)\n        return\n    if self.state.CLOSED:\n        pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=1)\n        self.send_queue.append(pdu)\n    if self.state.LISTEN:\n        if isinstance(pdu, Connect):\n            if (super(DataLinkConnection, self).enqueue(pdu) == False):\n                log.warn('full backlog on listening socket')\n                pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=32)\n                self.send_queue.append(pdu)\n                return False\n            return True\n    if self.state.CONNECT:\n        if (isinstance(pdu, ConnectionComplete) or isinstance(pdu, DisconnectedMode)):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.DISCONNECT:\n        if isinstance(pdu, DisconnectedMode):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.ESTABLISHED:\n        return self._enqueue_state_established(pdu)\n", "label": 1}
{"function": "\n\ndef receive(self, category=None, timeout=None, alarm_value=None):\n    \"Similar to 'receive' of Coro, except it retrieves (waiting,\\n        if necessary) messages in given 'category'.\\n        \"\n    c = self._categories.get(category, None)\n    if c:\n        msg = c.popleft()\n        raise StopIteration(msg)\n    if timeout:\n        start = _time()\n    while 1:\n        msg = (yield self._coro.receive(timeout=timeout, alarm_value=alarm_value))\n        if (msg == alarm_value):\n            raise StopIteration(msg)\n        for categorize in reversed(self._categorize):\n            c = categorize(msg)\n            if (c == category):\n                raise StopIteration(msg)\n            if (c is not None):\n                bucket = self._categories.get(c, None)\n                if (not bucket):\n                    bucket = self._categories[c] = collections.deque()\n                bucket.append(msg)\n                break\n        else:\n            self._categories[None].append(msg)\n        if timeout:\n            now = _time()\n            timeout -= (now - start)\n            start = now\n", "label": 1}
{"function": "\n\ndef _check_failure_put_connections(self, conns, req, nodes, min_conns):\n    '\\n        Identify any failed connections and check minimum connection count.\\n        '\n    if ((req.if_none_match is not None) and ('*' in req.if_none_match)):\n        statuses = [conn.resp.status for conn in conns if conn.resp]\n        if (HTTP_PRECONDITION_FAILED in statuses):\n            self.app.logger.debug(_('Object PUT returning 412, %(statuses)r'), {\n                'statuses': statuses,\n            })\n            raise HTTPPreconditionFailed(request=req)\n    if any((conn for conn in conns if (conn.resp and (conn.resp.status == HTTP_CONFLICT)))):\n        status_times = [('%(status)s (%(timestamp)s)' % {\n            'status': conn.resp.status,\n            'timestamp': HeaderKeyDict(conn.resp.getheaders()).get('X-Backend-Timestamp', 'unknown'),\n        }) for conn in conns if conn.resp]\n        self.app.logger.debug(_('Object PUT returning 202 for 409: %(req_timestamp)s <= %(timestamps)r'), {\n            'req_timestamp': req.timestamp.internal,\n            'timestamps': ', '.join(status_times),\n        })\n        raise HTTPAccepted(request=req)\n    self._check_min_conn(req, conns, min_conns)\n", "label": 1}
{"function": "\n\ndef _cleanse_tags(self, message):\n    'Using BeautifulSoup, remove or modify improper tags & attributes'\n    soup = BeautifulSoup(message, 'lxml')\n    for tag in soup.findAll():\n        if (tag.name not in self.VALID_TAGS):\n            tag.hidden = True\n        for (attr, value) in dict(tag.attrs).iteritems():\n            if (attr not in self.VALID_ATTRS):\n                del tag.attrs[attr]\n            if (attr == 'src'):\n                parsed_src = urlparse(value)\n                valid_netlocs = settings.ALLOWED_HOSTS\n                valid_netlocs.append('')\n                if (parsed_src.netloc not in valid_netlocs):\n                    tag.hidden = True\n            if (attr == 'href'):\n                parsed_src = urlparse(value)\n                if (parsed_src.scheme not in self.VALID_SCHEMES):\n                    tag.hidden = True\n        if (self.ADD_MAX_WIDTH and (tag.name == 'img')):\n            tag.attrs['style'] = 'max-width: 100%;'\n    return unicode(soup).encode('utf-8', errors='ignore')\n", "label": 1}
{"function": "\n\ndef __rebuildAsAssignment(node, firstVarStatement):\n    'Rebuilds the items of a var statement into a assignment list and moves declarations to the given var statement'\n    assignment = Node.Node(node.tokenizer, 'semicolon')\n    assignmentList = Node.Node(node.tokenizer, 'comma')\n    assignment.append(assignmentList, 'expression')\n    for child in list(node):\n        if hasattr(child, 'name'):\n            if hasattr(child, 'initializer'):\n                assign = __createSimpleAssignment(child.name, child.initializer)\n                assignmentList.append(assign)\n            firstVarStatement.append(child)\n        else:\n            for identifier in child.names:\n                firstVarStatement.append(__createDeclaration(identifier.value))\n            if hasattr(child, 'initializer'):\n                assign = __createMultiAssignment(child.names, child.initializer)\n                assignmentList.append(assign)\n            node.remove(child)\n    if (len(assignmentList) > 0):\n        node.parent.replace(node, assignment)\n    elif (getattr(node, 'rel', None) == 'iterator'):\n        if hasattr(child, 'name'):\n            node.parent.replace(node, __createIdentifier(child.name))\n        else:\n            node.parent.replace(node, child.names)\n    else:\n        if hasattr(node, 'rel'):\n            Console.warn(('Remove related node (%s) from parent: %s' % (node.rel, node)))\n        node.parent.remove(node)\n    if (len(assignmentList) == 1):\n        assignment.replace(assignmentList, assignmentList[0])\n", "label": 1}
{"function": "\n\ndef _post_parse(self):\n    for test_id in self._unknown_entities:\n        matcher = (lambda i: ((i == test_id) or i.startswith(('%s.' % test_id))))\n        known_ids = filter(matcher, self._tests)\n        for id_ in known_ids:\n            if (self._tests[id_]['status'] == 'init'):\n                self._tests[id_]['status'] = self._unknown_entities[test_id]['status']\n            if self._unknown_entities[test_id].get('reason'):\n                self._tests[id_]['reason'] = self._unknown_entities[test_id]['reason']\n            elif self._unknown_entities[test_id].get('traceback'):\n                self._tests[id_]['traceback'] = self._unknown_entities[test_id]['traceback']\n    for test_id in self._expected_failures:\n        if self._tests.get(test_id):\n            if (self._tests[test_id]['status'] == 'fail'):\n                self._tests[test_id]['status'] = 'xfail'\n                if self._expected_failures[test_id]:\n                    self._tests[test_id]['reason'] = self._expected_failures[test_id]\n            elif (self._tests[test_id]['status'] == 'success'):\n                self._tests[test_id]['status'] = 'uxsuccess'\n    for test_id in self._tests:\n        for file_name in ['traceback', 'reason']:\n            if (file_name in self._tests[test_id]):\n                self._tests[test_id][file_name] = encodeutils.safe_decode(self._tests[test_id][file_name])\n    self._is_parsed = True\n", "label": 1}
{"function": "\n\ndef _restore(self, obj):\n    if has_tag(obj, tags.B64):\n        restore = self._restore_base64\n    elif has_tag(obj, tags.BYTES):\n        restore = self._restore_quopri\n    elif has_tag(obj, tags.ID):\n        restore = self._restore_id\n    elif has_tag(obj, tags.REF):\n        restore = self._restore_ref\n    elif has_tag(obj, tags.ITERATOR):\n        restore = self._restore_iterator\n    elif has_tag(obj, tags.TYPE):\n        restore = self._restore_type\n    elif has_tag(obj, tags.REPR):\n        restore = self._restore_repr\n    elif has_tag(obj, tags.REDUCE):\n        restore = self._restore_reduce\n    elif has_tag(obj, tags.OBJECT):\n        restore = self._restore_object\n    elif has_tag(obj, tags.FUNCTION):\n        restore = self._restore_function\n    elif util.is_list(obj):\n        restore = self._restore_list\n    elif has_tag(obj, tags.TUPLE):\n        restore = self._restore_tuple\n    elif has_tag(obj, tags.SET):\n        restore = self._restore_set\n    elif util.is_dictionary(obj):\n        restore = self._restore_dict\n    else:\n        restore = (lambda x: x)\n    return restore(obj)\n", "label": 1}
{"function": "\n\ndef listdir(self, path, start_time=None, end_time=None):\n    '\\n        Get an iterable with S3 folder contents.\\n        Iterable contains paths relative to queried path.\\n\\n        :param start_time: Optional argument to copy files with modified dates after start_time\\n        :param end_time: Optional argument to copy files with modified dates before end_time\\n        '\n    (bucket, key) = self._path_to_bucket_and_key(path)\n    s3_bucket = self.s3.get_bucket(bucket, validate=True)\n    key_path = self._add_path_delimiter(key)\n    key_path_len = len(key_path)\n    for item in s3_bucket.list(prefix=key_path):\n        last_modified_date = time.strptime(item.last_modified, '%Y-%m-%dT%H:%M:%S.%fZ')\n        if (((not start_time) and (not end_time)) or (start_time and (not end_time) and (start_time < last_modified_date)) or ((not start_time) and end_time and (last_modified_date < end_time)) or (start_time and end_time and (start_time < last_modified_date < end_time))):\n            (yield (self._add_path_delimiter(path) + item.key[key_path_len:]))\n", "label": 1}
{"function": "\n\ndef write(self, data):\n    if (self.paused or ((not self.iAmStreaming) and (not self.outstandingPull))):\n        self._buffer.append(data)\n    elif (self.consumer is not None):\n        assert (not self._buffer), 'Writing fresh data to consumer before my buffer is empty!'\n        bytesSent = self._writeSomeData(data)\n        self.outstandingPull = False\n        if (not (bytesSent == len(data))):\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer.append(data[bytesSent:])\n    if ((self.producer is not None) and self.producerIsStreaming):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (bytesBuffered >= self.bufferSize):\n            self.producer.pauseProducing()\n            self.producerPaused = True\n", "label": 1}
{"function": "\n\ndef test_as_coeff_exponent():\n    assert ((3 * (x ** 4)).as_coeff_exponent(x) == (3, 4))\n    assert ((2 * (x ** 3)).as_coeff_exponent(x) == (2, 3))\n    assert ((4 * (x ** 2)).as_coeff_exponent(x) == (4, 2))\n    assert ((6 * (x ** 1)).as_coeff_exponent(x) == (6, 1))\n    assert ((3 * (x ** 0)).as_coeff_exponent(x) == (3, 0))\n    assert ((2 * (x ** 0)).as_coeff_exponent(x) == (2, 0))\n    assert ((1 * (x ** 0)).as_coeff_exponent(x) == (1, 0))\n    assert ((0 * (x ** 0)).as_coeff_exponent(x) == (0, 0))\n    assert (((- 1) * (x ** 0)).as_coeff_exponent(x) == ((- 1), 0))\n    assert (((- 2) * (x ** 0)).as_coeff_exponent(x) == ((- 2), 0))\n    assert (((2 * (x ** 3)) + (pi * (x ** 3))).as_coeff_exponent(x) == ((2 + pi), 3))\n    assert (((x * log(2)) / ((2 * x) + (pi * x))).as_coeff_exponent(x) == ((log(2) / (2 + pi)), 0))\n    D = Derivative\n    f = Function('f')\n    fx = D(f(x), x)\n    assert (fx.as_coeff_exponent(f(x)) == (fx, 0))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 3):\n            if (ftype == TType.STRING):\n                self.column_family = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.super_column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_task(self, taskname):\n    task = getattr(self.pavement, taskname, None)\n    if (not task):\n        for finder in self.task_finders:\n            task = finder.get_task(taskname)\n            if task:\n                break\n    if (not task):\n        task = _import_task(taskname)\n    if (not task):\n        all_tasks = self.get_tasks()\n        matches = [t for t in all_tasks if (t.shortname == taskname)]\n        if (len(matches) > 1):\n            matched_names = [t.name for t in matches]\n            raise BuildFailure(('Ambiguous task name %s (%s)' % (taskname, matched_names)))\n        elif matches:\n            task = matches[0]\n    return task\n", "label": 1}
{"function": "\n\n@setup_cache\ndef test_stats(cache):\n    for value in range(100):\n        cache[value] = value\n    assert (cache.stats(enable=True) == (0, 0))\n    for value in range(100):\n        cache[value]\n    for value in range(100, 110):\n        cache.get(value)\n    assert (cache.stats(reset=True) == (100, 10))\n    assert (cache.stats(enable=False) == (0, 0))\n    for value in range(100):\n        cache[value]\n    for value in range(100, 110):\n        cache.get(value)\n    assert (cache.stats() == (0, 0))\n    assert (len(cache.check()) == 0)\n", "label": 1}
{"function": "\n\ndef test_path_in_several_ways(self):\n    alice = Node(name='Alice')\n    bob = Node(name='Bob')\n    carol = Node(name='Carol')\n    dave = Node(name='Dave')\n    path = Path(alice, 'LOVES', bob, Relationship(carol, 'HATES', bob), carol, 'KNOWS', dave)\n    assert path.__bool__()\n    assert path.__nonzero__()\n    assert (path[0] == Relationship(alice, 'LOVES', bob))\n    assert (path[1] == Relationship(carol, 'HATES', bob))\n    assert (path[2] == Relationship(carol, 'KNOWS', dave))\n    assert (path[(- 1)] == Relationship(carol, 'KNOWS', dave))\n    assert (path[0:1] == Path(alice, 'LOVES', bob))\n    assert (path[0:2] == Path(alice, 'LOVES', bob, Relationship(carol, 'HATES', bob), carol))\n    try:\n        _ = path[7]\n    except IndexError:\n        assert True\n    else:\n        assert False\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    if (':' in request.get_host()):\n        (domain, port) = request.get_host().split(':')\n        if (int(port) not in (80, 443)):\n            domain = request.get_host()\n    else:\n        domain = request.get_host().split(':')[0]\n    domain = domain.lower()\n    cache_key = ('Site:domain:%s' % domain)\n    site = cache.get(cache_key)\n    if site:\n        SITE_ID.value = site\n    else:\n        try:\n            site = Site.objects.get(domain=domain)\n        except Site.DoesNotExist:\n            site = None\n        if (not site):\n            if domain.startswith('www.'):\n                fallback_domain = domain[4:]\n            else:\n                fallback_domain = ('www.' + domain)\n            try:\n                site = Site.objects.get(domain=fallback_domain)\n            except Site.DoesNotExist:\n                site = None\n        if ((not site) and getattr(settings, 'CREATE_SITES_AUTOMATICALLY', True)):\n            site = Site(domain=domain, name=domain)\n            site.save()\n        if site:\n            SITE_ID.value = site.pk\n        else:\n            SITE_ID.value = _default_site_id\n        cache.set(cache_key, SITE_ID.value, (5 * 60))\n", "label": 1}
{"function": "\n\ndef _update_document_fields_positional(self, doc, fields, spec, updater, subdocument=None):\n    'Implements the $set behavior on an existing document'\n    for (k, v) in iteritems(fields):\n        if ('$' in k):\n            field_name_parts = k.split('.')\n            if (not subdocument):\n                current_doc = doc\n                subspec = spec\n                for part in field_name_parts[:(- 1)]:\n                    if (part == '$'):\n                        subspec = subspec.get('$elemMatch', subspec)\n                        for item in current_doc:\n                            if filter_applies(subspec, item):\n                                current_doc = item\n                                break\n                        continue\n                    new_spec = {\n                        \n                    }\n                    for el in subspec:\n                        if el.startswith(part):\n                            if (len(el.split('.')) > 1):\n                                new_spec['.'.join(el.split('.')[1:])] = subspec[el]\n                            else:\n                                new_spec = subspec[el]\n                    subspec = new_spec\n                    current_doc = current_doc[part]\n                subdocument = current_doc\n                if ((field_name_parts[(- 1)] == '$') and isinstance(subdocument, list)):\n                    for (i, doc) in enumerate(subdocument):\n                        if filter_applies(subspec, doc):\n                            subdocument[i] = v\n                            break\n                    continue\n            updater(subdocument, field_name_parts[(- 1)], v)\n            continue\n        self._update_document_single_field(doc, k, v, updater)\n    return subdocument\n", "label": 1}
{"function": "\n\ndef data(self):\n    if (not hasattr(self, '_data')):\n        request = URLRequest(self.url)\n        if (self.env and self.env.cache):\n            headers = self.env.cache.get(('url', 'headers', self.url))\n            if headers:\n                (etag, lmod) = headers\n                if etag:\n                    request.add_header('If-None-Match', etag)\n                if lmod:\n                    request.add_header('If-Modified-Since', lmod)\n        try:\n            response = urlopen(request)\n        except HTTPError as e:\n            if (e.code != 304):\n                raise\n            self._data = self.env.cache.get(('url', 'contents', self.url))\n        else:\n            with contextlib.closing(response):\n                self._data = response.read()\n            if (self.env and self.env.cache):\n                self.env.cache.set(('url', 'headers', self.url), (response.headers.get('ETag'), response.headers.get('Last-Modified')))\n                self.env.cache.set(('url', 'contents', self.url), self._data)\n    return self._data\n", "label": 1}
{"function": "\n\ndef get_field_parameters(self):\n    params = {\n        \n    }\n    if self.nullable:\n        params['null'] = True\n    if ((self.field_class is ForeignKeyField) or (self.name != self.db_column)):\n        params['db_column'] = (\"'%s'\" % self.db_column)\n    if (self.primary_key and (self.field_class is not PrimaryKeyField)):\n        params['primary_key'] = True\n    if self.is_foreign_key():\n        params['rel_model'] = self.rel_model\n        if self.to_field:\n            params['to_field'] = (\"'%s'\" % self.to_field)\n        if self.related_name:\n            params['related_name'] = (\"'%s'\" % self.related_name)\n    if (not self.is_primary_key()):\n        if self.unique:\n            params['unique'] = 'True'\n        elif (self.index and (not self.is_foreign_key())):\n            params['index'] = 'True'\n    return params\n", "label": 1}
{"function": "\n\ndef build_suite(self, test_labels=None, extra_tests=None, **kwargs):\n    suite = TestSuite()\n    test_labels = (test_labels or ['.'])\n    extra_tests = (extra_tests or [])\n    discover_kwargs = {\n        \n    }\n    if (self.pattern is not None):\n        discover_kwargs['pattern'] = self.pattern\n    if (self.top_level is not None):\n        discover_kwargs['top_level_dir'] = self.top_level\n    for label in test_labels:\n        kwargs = discover_kwargs.copy()\n        tests = None\n        label_as_path = os.path.abspath(label)\n        if (not os.path.exists(label_as_path)):\n            tests = self.test_loader.loadTestsFromName(label)\n        elif (os.path.isdir(label_as_path) and (not self.top_level)):\n            top_level = label_as_path\n            while True:\n                init_py = os.path.join(top_level, '__init__.py')\n                if os.path.exists(init_py):\n                    try_next = os.path.dirname(top_level)\n                    if (try_next == top_level):\n                        break\n                    top_level = try_next\n                    continue\n                break\n            kwargs['top_level_dir'] = top_level\n        if (not (tests and tests.countTestCases())):\n            tests = self.test_loader.discover(start_dir=label, **kwargs)\n            self.test_loader._top_level_dir = None\n        suite.addTests(tests)\n    for test in extra_tests:\n        suite.addTest(test)\n    return reorder_suite(suite, self.reorder_by)\n", "label": 1}
{"function": "\n\ndef mpf_exp(x, prec, rnd=round_fast):\n    (sign, man, exp, bc) = x\n    if man:\n        mag = (bc + exp)\n        wp = (prec + 14)\n        if sign:\n            man = (- man)\n        if ((prec > 600) and (exp >= 0)):\n            e = mpf_e((wp + int((1.45 * mag))))\n            return mpf_pow_int(e, (man << exp), prec, rnd)\n        if (mag < (- wp)):\n            return mpf_perturb(fone, sign, prec, rnd)\n        if (mag > 1):\n            wpmod = (wp + mag)\n            offset = (exp + wpmod)\n            if (offset >= 0):\n                t = (man << offset)\n            else:\n                t = (man >> (- offset))\n            lg2 = ln2_fixed(wpmod)\n            (n, t) = divmod(t, lg2)\n            n = int(n)\n            t >>= mag\n        else:\n            offset = (exp + wp)\n            if (offset >= 0):\n                t = (man << offset)\n            else:\n                t = (man >> (- offset))\n            n = 0\n        man = exp_basecase(t, wp)\n        return from_man_exp(man, (n - wp), prec, rnd)\n    if (not exp):\n        return fone\n    if (x == fninf):\n        return fzero\n    return x\n", "label": 1}
{"function": "\n\ndef login(request, template=None, next_url=None):\n    feedback = ''\n    action_url = '.'\n    if ('user' in request.session.keys()):\n        if next_url:\n            return HttpResponseRedirect(next_url)\n        else:\n            return HttpResponseRedirect('/')\n    try:\n        if request.REQUEST['next_url']:\n            next_url = request.REQUEST['next_url']\n            action_url = ('%s?next_url=%s' % (action_url, next_url))\n    except:\n        pass\n    if (request.method == 'POST'):\n        form = LoginForm(request.POST)\n        if form.is_valid():\n            email = form.cleaned_data['email']\n            password = form.cleaned_data['password']\n            query = db.Query(User)\n            user = query.filter('email =', email).get()\n            if user:\n                success = user.login(email=email, password=password, request=request)\n                if success:\n                    if (next_url is not None):\n                        return HttpResponseRedirect(next_url)\n                    else:\n                        return HttpResponseRedirect('/')\n                else:\n                    feedback = 'Bad username and password.  Please try again.'\n            else:\n                feedback = 'Bad username and password.  Please try again.'\n    else:\n        form = LoginForm()\n    c = RequestContext(request, {\n        'form': form,\n        'feedback': feedback,\n        'action_url': action_url,\n    })\n    if (template is None):\n        template = 'login.html'\n    return render_to_response(template, c)\n", "label": 1}
{"function": "\n\ndef gpx_fields_to_xml(instance, tag, version, custom_attributes=None):\n    fields = instance.gpx_10_fields\n    if (version == '1.1'):\n        fields = instance.gpx_11_fields\n    tag_open = bool(tag)\n    body = ''\n    if tag:\n        body = ('\\n<' + tag)\n        if custom_attributes:\n            for (key, value) in custom_attributes.items():\n                body += (' %s=\"%s\"' % (key, mod_utils.make_str(value)))\n    for gpx_field in fields:\n        if isinstance(gpx_field, str):\n            if tag_open:\n                body += '>'\n                tag_open = False\n            if (gpx_field[0] == '/'):\n                body += ('<%s>' % gpx_field)\n            else:\n                body += ('\\n<%s' % gpx_field)\n                tag_open = True\n        else:\n            value = getattr(instance, gpx_field.name)\n            if gpx_field.attribute:\n                body += (' ' + gpx_field.to_xml(value, version))\n            elif value:\n                if tag_open:\n                    body += '>'\n                    tag_open = False\n                xml_value = gpx_field.to_xml(value, version)\n                if xml_value:\n                    body += xml_value\n    if tag:\n        if tag_open:\n            body += '>'\n        body += (('</' + tag) + '>')\n    return body\n", "label": 1}
{"function": "\n\ndef load_directives(self, inherits):\n    \"there has got to be a better way\\n           plugin pattern away!!!\\n           \\n           #1 this could be a comprehension but it'd be hard\\n              to read.\\n           #2 maybe we should put a dummy sys.modules in place\\n              while doing this to limit junk in the name space.\\n        \"\n    m = self.__class__.__module__\n    exclude = ['__init__.py']\n    relpath = sys.modules[m].__file__\n    reldir = os.path.split(relpath)[0]\n    fulldir = os.path.realpath(reldir)\n    pyfiles = []\n    for f in os.listdir(fulldir):\n        ff = os.path.join(fulldir, f)\n        if os.path.isfile(ff):\n            pyfiles.append(f)\n    pyfiles = [f for f in pyfiles if f.endswith('.py')]\n    pyfiles = [f for f in pyfiles if (f not in exclude)]\n    pyfiles = [f[:(- 3)] for f in pyfiles]\n    candidates = []\n    for py in pyfiles:\n        m = __import__(py, globals(), locals(), 'romeo.directives')\n        for i in dir(m):\n            attr = getattr(m, i)\n            if (not (type(attr) == types.TypeType)):\n                continue\n            matches = [cls for cls in inherits if issubclass(attr, cls) if (attr.__name__ != cls.__name__)]\n            if (len(matches) == len(inherits)):\n                candidates.append(attr)\n    for c in candidates:\n        cinst = c(c.name, self.root, *c.init_args(self), **c.init_kwargs(self))\n        self.directives.append(cinst)\n", "label": 1}
{"function": "\n\ndef filter(self, iterable, prereleases=None):\n    if (prereleases is None):\n        prereleases = self.prereleases\n    if self._specs:\n        for spec in self._specs:\n            iterable = spec.filter(iterable, prereleases=bool(prereleases))\n        return iterable\n    else:\n        filtered = []\n        found_prereleases = []\n        for item in iterable:\n            if (not isinstance(item, (LegacyVersion, Version))):\n                parsed_version = parse(item)\n            else:\n                parsed_version = item\n            if isinstance(parsed_version, LegacyVersion):\n                continue\n            if (parsed_version.is_prerelease and (not prereleases)):\n                if (not filtered):\n                    found_prereleases.append(item)\n            else:\n                filtered.append(item)\n        if ((not filtered) and found_prereleases and (prereleases is None)):\n            return found_prereleases\n        return filtered\n", "label": 1}
{"function": "\n\ndef read_events(self, timeout=None):\n    timeout_ms = 2147483647\n    if (timeout is not None):\n        timeout_ms = int((timeout * 1000))\n        if ((timeout_ms < 0) or (timeout_ms >= 2147483647)):\n            raise ValueError('Timeout value out of range')\n    try:\n        events = []\n        (rc, num, key, _) = win32file.GetQueuedCompletionStatus(self.__cphandle, timeout_ms)\n        if (rc == 0):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled and (not watch._removed)):\n                    events.extend(process_events(watch, num))\n        elif (rc == 5):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled):\n                    close_watch(watch)\n                    del self.__key_to_watch[key]\n                    events.append(FSEvent(watch, FSEvent.DeleteSelf))\n        return events\n    except pywintypes.error as e:\n        raise FSMonitorWindowsError(*e.args)\n", "label": 1}
{"function": "\n\ndef reset(self, **traits):\n    'Creates the dataset afresh or resets existing data source.'\n    self.set(trait_change_notify=False, **traits)\n    points = self.points\n    scalars = self.scalars\n    (x, y, z) = (self.x, self.y, self.z)\n    points = np.c_[(x.ravel(), y.ravel(), z.ravel())].ravel()\n    points.shape = ((- 1), 3)\n    self.set(points=points, trait_change_notify=False)\n    triangles = self.triangles\n    assert (triangles.shape[1] == 3), 'The shape of the triangles array must be (X, 3)'\n    assert (triangles.max() < len(points)), 'The triangles indices must be smaller that the number of points'\n    assert (triangles.min() >= 0), 'The triangles indices must be positive or null'\n    if (self.dataset is None):\n        pd = tvtk.PolyData()\n    else:\n        pd = self.dataset\n    pd.set(points=points)\n    pd.set(polys=triangles)\n    if ((not ('scalars' in traits)) and (scalars is not None) and (scalars.shape != x.shape)):\n        scalars = z\n    if ((scalars is not None) and (len(scalars) > 0)):\n        if (not scalars.flags.contiguous):\n            scalars = scalars.copy()\n            self.set(scalars=scalars, trait_change_notify=False)\n        assert (x.shape == scalars.shape)\n        pd.point_data.scalars = scalars.ravel()\n        pd.point_data.scalars.name = 'scalars'\n    self.dataset = pd\n", "label": 1}
{"function": "\n\n@login_required\ndef editor(request, id):\n    '\\n    Display the article editor.\\n    '\n    article = (get_object_or_404(Article, pk=id) if id else None)\n    if (article and (not article.can_edit(request))):\n        raise Http404\n    if (request.method == 'POST'):\n        form = EditorForm(instance=article, data=request.POST)\n        if form.is_valid():\n            creating = (article is None)\n            article = form.save(commit=False)\n            if creating:\n                article.author = request.user\n            article.save()\n            messages.info(request, ('The article has been saved.' if creating else 'Your changes to the article have been saved.'))\n            if (('action' in request.POST) and (request.POST['action'] == 'continue')):\n                return redirect('articles:editor', article.id)\n            else:\n                return redirect(article)\n    else:\n        form = EditorForm(instance=article)\n    return render(request, 'articles/editor.html', {\n        'title': (('Edit \"%s\"' % article) if article else 'New Article'),\n        'form': form,\n        'description': ('Use the form below to %s.' % ('edit the article' if article else 'create an article')),\n    })\n", "label": 1}
{"function": "\n\ndef parse(self, node):\n    '\\n        Get tag name, indentantion and correct attributes of a node according\\n        to its class\\n        '\n    node_class_name = node.__class__.__name__\n    spec = self.rst_terms[node_class_name]\n    tag_name = (spec[0] or node_class_name)\n    use_name_in_class = ((len(spec) > 3) and spec[3])\n    indent = (spec[4] if (len(spec) > 4) else True)\n    if use_name_in_class:\n        node['classes'].insert(0, node_class_name)\n    attributes = {\n        \n    }\n    replacements = {\n        'refuri': 'href',\n        'uri': 'src',\n        'refid': 'href',\n        'morerows': 'rowspan',\n        'morecols': 'colspan',\n        'classes': 'class',\n        'ids': 'id',\n    }\n    ignores = ('names', 'dupnames', 'bullet', 'enumtype', 'colwidth', 'stub', 'backrefs', 'auto', 'anonymous')\n    for (k, v) in node.attributes.items():\n        if ((k in ignores) or (not v)):\n            continue\n        if (k in replacements):\n            k = replacements[k]\n        if isinstance(v, list):\n            v = ' '.join(v)\n        attributes[k] = v\n    if getattr(self, 'next_elem_attr', None):\n        attributes.update(self.next_elem_attr)\n        del self.next_elem_attr\n    return (tag_name, indent, attributes)\n", "label": 1}
{"function": "\n\ndef find_referenced_templates(ast):\n    'Finds all the referenced templates from the AST.  This will return an\\n    iterator over all the hardcoded template extensions, inclusions and\\n    imports.  If dynamic inheritance or inclusion is used, `None` will be\\n    yielded.\\n\\n    >>> from jinja2 import Environment, meta\\n    >>> env = Environment()\\n    >>> ast = env.parse(\\'{% extends \"layout.html\" %}{% include helper %}\\')\\n    >>> list(meta.find_referenced_templates(ast))\\n    [\\'layout.html\\', None]\\n\\n    This function is useful for dependency tracking.  For example if you want\\n    to rebuild parts of the website after a layout template has changed.\\n    '\n    for node in ast.find_all((nodes.Extends, nodes.FromImport, nodes.Import, nodes.Include)):\n        if (not isinstance(node.template, nodes.Const)):\n            if isinstance(node.template, (nodes.Tuple, nodes.List)):\n                for template_name in node.template.items:\n                    if isinstance(template_name, nodes.Const):\n                        if isinstance(template_name.value, string_types):\n                            (yield template_name.value)\n                    else:\n                        (yield None)\n            else:\n                (yield None)\n            continue\n        if isinstance(node.template.value, string_types):\n            (yield node.template.value)\n        elif (isinstance(node, nodes.Include) and isinstance(node.template.value, (tuple, list))):\n            for template_name in node.template.value:\n                if isinstance(template_name, string_types):\n                    (yield template_name)\n        else:\n            (yield None)\n", "label": 1}
{"function": "\n\ndef __init__(self, op_tree, ad):\n    '\\n        op_tree: the op_tree at this grad_node\\n        ad: the autodiff object with global op_tree, next_error and dicts\\n        '\n    assert (op_tree is not None)\n    self.op_tree = op_tree\n    self.grad_op_tree = None\n    self.ad = ad\n    self.left = None\n    self.right = None\n    if isinstance(op_tree, Tensor):\n        if (op_tree._original_base not in ad.map_tensor_grad_node):\n            ad.map_tensor_grad_node[op_tree._original_base] = self\n    elif (type(op_tree) == OpTreeNode):\n        if (op_tree[1] is not None):\n            if (isinstance(op_tree[1], Tensor) and (op_tree[1]._original_base in ad.map_tensor_grad_node)):\n                self.left = ad.map_tensor_grad_node[op_tree[1]._original_base]\n            else:\n                self.left = GradNode(op_tree[1], ad)\n        if (op_tree[2] is not None):\n            if (isinstance(op_tree[2], Tensor) and (op_tree[2]._original_base in ad.map_tensor_grad_node)):\n                self.right = ad.map_tensor_grad_node[op_tree[2]._original_base]\n            else:\n                self.right = GradNode(op_tree[2], ad)\n", "label": 1}
{"function": "\n\ndef test_04_ore_init_with_statement(self):\n    s = Ore_Sword_Statement(ORE_TEST_STATEMENT)\n    assert (len(s.states) == 1)\n    assert (len(s.original_deposits) == 1)\n    assert (len(s.resources) == 1)\n    assert (s.xml_document != None)\n    assert (s.dom != None)\n    assert s.parsed\n    assert s.valid\n    (uri, description) = s.states[0]\n    assert (uri == 'http://purl.org/net/sword/terms/state/Testing')\n    assert (description == 'The work has passed through review and is now in the archive')\n    t = datetime.strptime('2011-03-02T20:50:06Z', '%Y-%m-%dT%H:%M:%SZ')\n    entry = s.resources[0]\n    assert (len(entry.packaging) == 1)\n    assert (entry.deposited_by == 'sword')\n    assert (entry.deposited_on_behalf_of == 'jbloggs')\n    assert (entry.deposited_on == t)\n    assert (entry.uri == 'http://localhost:8080/part-IRI/43/my_deposit/example.zip')\n    assert (entry.packaging[0] == 'http://purl.org/net/sword/package/SimpleZip')\n", "label": 1}
{"function": "\n\ndef make_node(self, *inputs):\n    assert (self.nout == 1)\n    assert (len(inputs) == 2)\n    _inputs = [gpu_contiguous(as_cuda_ndarray_variable(i)) for i in inputs]\n    if ((self.nin > 0) and (len(_inputs) != self.nin)):\n        raise TypeError('Wrong argument count', (self.nin, len(_inputs)))\n    for i in _inputs[1:]:\n        if (i.type.ndim != inputs[0].type.ndim):\n            raise TypeError('different ranks among inputs')\n    if any([any(i.type.broadcastable) for i in inputs]):\n        raise Exception(\"pycuda don't support broadcasted dimensions\")\n    otype = CudaNdarrayType(broadcastable=([False] * _inputs[0].type.ndim))\n    out_node = Apply(self, _inputs, [otype() for o in xrange(self.nout)])\n    return out_node\n", "label": 1}
{"function": "\n\ndef sufficient_options(self):\n    'Check if all required options are present.\\n\\n        :raises: AuthPluginOptionsMissing\\n        '\n    has_token = (self.opts.get('token') or self.opts.get('auth_token'))\n    no_auth = (has_token and self.opts.get('endpoint'))\n    has_tenant = (self.opts.get('tenant_id') or self.opts.get('tenant_name'))\n    has_credential = (self.opts.get('username') and has_tenant and self.opts.get('password') and self.opts.get('auth_url'))\n    missing = (not (no_auth or has_credential))\n    if missing:\n        missing_opts = []\n        opts = ['token', 'endpoint', 'username', 'password', 'auth_url', 'tenant_id', 'tenant_name']\n        for opt in opts:\n            if (not self.opts.get(opt)):\n                missing_opts.append(opt)\n        raise exceptions.AuthPluginOptionsMissing(missing_opts)\n", "label": 1}
{"function": "\n\ndef test_posts_atom(client, silly_posts):\n    rv = client.get('/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('Probably a &lt;i&gt;dumb&lt;/i&gt; joke' in content)\n    assert ('First interesting article' in content)\n    rv = client.get('/everything/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('mal.colm/reynolds' in content)\n    assert ('First interesting article' in content)\n    rv = client.get('/notes/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('Probably a &lt;i&gt;dumb&lt;/i&gt; joke' in content)\n    assert ('First interesting article' not in content)\n", "label": 1}
{"function": "\n\n@utils.synchronized('dhcp-agent')\ndef sync_state(self, networks=None):\n    \"Sync the local DHCP state with Neutron. If no networks are passed,\\n        or 'None' is one of the networks, sync all of the networks.\\n        \"\n    only_nets = set(([] if ((not networks) or (None in networks)) else networks))\n    LOG.info(_LI('Synchronizing state'))\n    pool = eventlet.GreenPool(self.conf.num_sync_threads)\n    known_network_ids = set(self.cache.get_network_ids())\n    try:\n        active_networks = self.plugin_rpc.get_active_networks_info()\n        LOG.info(_LI('All active networks have been fetched through RPC.'))\n        active_network_ids = set((network.id for network in active_networks))\n        for deleted_id in (known_network_ids - active_network_ids):\n            try:\n                self.disable_dhcp_helper(deleted_id)\n            except Exception as e:\n                self.schedule_resync(e, deleted_id)\n                LOG.exception(_LE('Unable to sync network state on deleted network %s'), deleted_id)\n        for network in active_networks:\n            if ((not only_nets) or (network.id not in known_network_ids) or (network.id in only_nets)):\n                pool.spawn(self.safe_configure_dhcp_for_network, network)\n        pool.waitall()\n        LOG.info(_LI('Synchronizing state complete'))\n    except Exception as e:\n        if only_nets:\n            for network_id in only_nets:\n                self.schedule_resync(e, network_id)\n        else:\n            self.schedule_resync(e)\n        LOG.exception(_LE('Unable to sync network state.'))\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kwargs):\n    evaluate = kwargs.get('evaluate', global_evaluate[0])\n    if iterable(args[0]):\n        if (isinstance(args[0], Point) and (not evaluate)):\n            return args[0]\n        args = args[0]\n    coords = Tuple(*args)\n    if any(((a.is_number and im(a)) for a in coords)):\n        raise ValueError('Imaginary coordinates not permitted.')\n    if evaluate:\n        coords = coords.xreplace(dict([(f, simplify(nsimplify(f, rational=True))) for f in coords.atoms(Float)]))\n    if (len(coords) == 2):\n        return Point2D(coords, **kwargs)\n    if (len(coords) == 3):\n        return Point3D(coords, **kwargs)\n    return GeometryEntity.__new__(cls, *coords)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _tensAdd_flatten(args):\n    if (not all((isinstance(x, TensExpr) for x in args))):\n        args1 = []\n        for x in args:\n            if isinstance(x, TensExpr):\n                if isinstance(x, TensAdd):\n                    args1.extend(list(x.args))\n                else:\n                    args1.append(x)\n        args1 = [x for x in args1 if (isinstance(x, TensExpr) and x.coeff)]\n        args2 = [x for x in args if (not isinstance(x, TensExpr))]\n        t1 = TensMul.from_data(Add(*args2), [], [], [])\n        args = ([t1] + args1)\n    a = []\n    for x in args:\n        if isinstance(x, TensAdd):\n            a.extend(list(x.args))\n        else:\n            a.append(x)\n    args = [x for x in a if x.coeff]\n    return args\n", "label": 1}
{"function": "\n\ndef the_local_prediction_is(step, prediction):\n    if (isinstance(world.local_prediction, list) or isinstance(world.local_prediction, tuple)):\n        local_prediction = world.local_prediction[0]\n    elif isinstance(world.local_prediction, dict):\n        local_prediction = world.local_prediction['prediction']\n    else:\n        local_prediction = world.local_prediction\n    try:\n        local_model = world.local_model\n        if (not isinstance(world.local_model, LogisticRegression)):\n            if isinstance(local_model, MultiModel):\n                local_model = local_model.models[0]\n            if local_model.tree.regression:\n                local_prediction = round(float(local_prediction), 4)\n                prediction = round(float(prediction), 4)\n    except AttributeError:\n        local_model = world.local_ensemble.multi_model.models[0]\n        if local_model.tree.regression:\n            local_prediction = round(float(local_prediction), 4)\n            prediction = round(float(prediction), 4)\n    if (local_prediction == prediction):\n        assert True\n    else:\n        assert False, ('found: %s, expected %s' % (local_prediction, prediction))\n", "label": 1}
{"function": "\n\ndef test_updating_state(db, tmpdir):\n    if (not db.startswith('mysql')):\n        db = os.path.join(str(tmpdir), db)\n    jip.db.init(db)\n    j = jip.db.Job()\n    jip.db.save(j)\n    j = jip.db.get(j.id)\n    assert (j is not None)\n    assert (j.create_date is not None)\n    assert (j.start_date is None)\n    assert (j.finish_date is None)\n    assert (j.job_id is None)\n    assert (j.state == jip.db.STATE_HOLD)\n    assert (len(j.pipe_to) == 0)\n    assert (len(j.pipe_from) == 0)\n    date = datetime.datetime.now()\n    j.job_id = 10\n    j.start_date = date\n    j.finish_date = date\n    j.state = jip.db.STATE_DONE\n    jip.db.update_job_states(j)\n    fresh = jip.db.get(j.id)\n    assert (fresh is not None)\n    assert (fresh.job_id == '10')\n    assert (fresh.state == jip.db.STATE_DONE)\n    assert (len(jip.db.get_all()) == 1)\n", "label": 1}
{"function": "\n\ndef test_tweet_ordering():\n    now = datetime.now(timezone.utc)\n    tweet_1 = Tweet('A', now)\n    tweet_2 = Tweet('B', (now + timedelta(hours=1)))\n    tweet_3 = Tweet('C', (now + timedelta(hours=2)))\n    tweet_4 = Tweet('D', (now + timedelta(hours=2)))\n    tweet_5 = Tweet('D', (now + timedelta(hours=2)))\n    source = Source('foo', 'bar')\n    with pytest.raises(TypeError):\n        (tweet_1 < source)\n    with pytest.raises(TypeError):\n        (tweet_1 <= source)\n    with pytest.raises(TypeError):\n        (tweet_1 > source)\n    with pytest.raises(TypeError):\n        (tweet_1 >= source)\n    assert (tweet_1 != source)\n    assert (tweet_1 < tweet_2)\n    assert (tweet_1 <= tweet_2)\n    assert (tweet_2 > tweet_1)\n    assert (tweet_2 >= tweet_1)\n    assert (tweet_3 != tweet_4)\n    assert (tweet_5 == tweet_4)\n    assert (tweet_5 >= tweet_4)\n    assert (tweet_5 <= tweet_4)\n    assert (not (tweet_3 <= tweet_4))\n    assert (not (tweet_3 >= tweet_4))\n", "label": 1}
{"function": "\n\ndef io_connection_pattern(inputs, outputs):\n    '\\n    Returns the connection pattern of a subgraph defined by given\\n    inputs and outputs.\\n\\n    '\n    inner_nodes = io_toposort(inputs, outputs)\n    connect_pattern_by_var = {\n        \n    }\n    nb_inputs = len(inputs)\n    for i in range(nb_inputs):\n        input = inputs[i]\n        inp_connection_pattern = [(i == j) for j in range(nb_inputs)]\n        connect_pattern_by_var[input] = inp_connection_pattern\n    for n in inner_nodes:\n        try:\n            op_connection_pattern = n.op.connection_pattern(n)\n        except AttributeError:\n            op_connection_pattern = ([([True] * len(n.outputs))] * len(n.inputs))\n        for out_idx in range(len(n.outputs)):\n            out = n.outputs[out_idx]\n            out_connection_pattern = ([False] * nb_inputs)\n            for inp_idx in range(len(n.inputs)):\n                inp = n.inputs[inp_idx]\n                if (inp in connect_pattern_by_var):\n                    inp_connection_pattern = connect_pattern_by_var[inp]\n                    if op_connection_pattern[inp_idx][out_idx]:\n                        out_connection_pattern = [(out_connection_pattern[i] or inp_connection_pattern[i]) for i in range(nb_inputs)]\n            connect_pattern_by_var[out] = out_connection_pattern\n    global_connection_pattern = [[] for o in range(len(inputs))]\n    for out in outputs:\n        out_connection_pattern = connect_pattern_by_var[out]\n        for i in range(len(inputs)):\n            global_connection_pattern[i].append(out_connection_pattern[i])\n    return global_connection_pattern\n", "label": 1}
{"function": "\n\n@skipif(('GPy' not in sys.modules), 'this test requires hyperopt')\ndef test_gp():\n    searchspace = SearchSpace()\n    searchspace.add_float('x', (- 10), 10)\n    searchspace.add_float('y', 1, 10, warp='log')\n    searchspace.add_int('z', (- 10), 10)\n    searchspace.add_enum('w', ['opt1', 'opt2'])\n    history = [(searchspace.rvs(), np.random.random(), 'SUCCEEDED') for _ in range(4)]\n    params = GP().suggest(history, searchspace)\n    for (k, v) in iteritems(params):\n        assert (k in searchspace.variables)\n        if isinstance(searchspace[k], EnumVariable):\n            assert (v in searchspace[k].choices)\n        elif isinstance(searchspace[k], FloatVariable):\n            assert (searchspace[k].min <= v <= searchspace[k].max)\n        elif isinstance(searchspace[k], IntVariable):\n            assert (searchspace[k].min <= v <= searchspace[k].max)\n        else:\n            assert False\n", "label": 1}
{"function": "\n\ndef communicate(self, input=None):\n    '\\n        Interact with process: Enqueue data to be sent to stdin.  Return data\\n        read from stdout and stderr as a tuple (stdoutdata, stderrdata).  Do\\n        NOT wait for process to terminate.\\n        '\n    if (self.use_stdin and input):\n        self.stdin_lock.acquire()\n        self.stdin_queue.append(input)\n        self.stdin_lock.release()\n    stdoutdata = None\n    stderrdata = None\n    if self.use_stdout:\n        data = b''\n        self.stderr_lock.acquire()\n        try:\n            while (len(self.stdout_queue) > 0):\n                data += self.stdout_queue.popleft()\n        except:\n            self.stderr_lock.release()\n            raise\n        self.stderr_lock.release()\n        if data:\n            stdoutdata = data\n    if self.use_stderr:\n        data = b''\n        self.stderr_lock.acquire()\n        try:\n            while (len(self.stderr_queue) > 0):\n                data += self.stderr_queue.popleft()\n        except:\n            self.stderr_lock.release()\n            raise\n        self.stderr_lock.release()\n        if data:\n            stderrdata = data\n    return (stdoutdata, stderrdata)\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    assert name.endswith('PropMap'), 'Please use convention: ___PropMap, e.g. ElectromagneticPropMap'\n    _properties = {\n        \n    }\n    for base in bases:\n        for baseProp in getattr(base, '_properties', {\n            \n        }):\n            _properties[baseProp] = base._properties[baseProp]\n    keys = [key for key in attrs]\n    for attr in keys:\n        if isinstance(attrs[attr], Property):\n            attrs[attr].name = attr\n            attrs[(attr + 'Map')] = attrs[attr]._getMapProperty()\n            attrs[(attr + 'Index')] = attrs[attr]._getIndexProperty()\n            _properties[attr] = attrs[attr]\n            attrs.pop(attr)\n    attrs['_properties'] = _properties\n    defaultInvProps = []\n    for p in _properties:\n        prop = _properties[p]\n        if prop.defaultInvProp:\n            defaultInvProps += [p]\n        if (prop.propertyLink is not None):\n            assert (prop.propertyLink[0] in _properties), (\"You can only link to things that exist: '%s' is trying to link to '%s'\" % (prop.name, prop.propertyLink[0]))\n    if (len(defaultInvProps) > 1):\n        raise Exception(('You have more than one default inversion property: %s' % defaultInvProps))\n    newClass = super(_PropMapMetaClass, cls).__new__(cls, name, bases, attrs)\n    newClass.PropModel = cls.createPropModelClass(newClass, name, _properties)\n    _PROPMAPCLASSREGISTRY[name] = newClass\n    return newClass\n", "label": 1}
{"function": "\n\n@classmethod\ndef delete(cls, repo, path):\n    'Delete the reference at the given path\\n\\n        :param repo:\\n            Repository to delete the reference from\\n\\n        :param path:\\n            Short or full path pointing to the reference, i.e. refs/myreference\\n            or just \"myreference\", hence \\'refs/\\' is implied.\\n            Alternatively the symbolic reference to be deleted'\n    full_ref_path = cls.to_full_path(path)\n    abs_path = join(repo.git_dir, full_ref_path)\n    if exists(abs_path):\n        os.remove(abs_path)\n    else:\n        pack_file_path = cls._get_packed_refs_path(repo)\n        try:\n            reader = open(pack_file_path, 'rb')\n        except (OSError, IOError):\n            pass\n        else:\n            new_lines = list()\n            made_change = False\n            dropped_last_line = False\n            for line in reader:\n                line = line.decode(defenc)\n                if ((line.startswith('#') or (full_ref_path not in line)) and ((not dropped_last_line) or (dropped_last_line and (not line.startswith('^'))))):\n                    new_lines.append(line)\n                    dropped_last_line = False\n                    continue\n                made_change = True\n                dropped_last_line = True\n            reader.close()\n            if made_change:\n                open(pack_file_path, 'wb').writelines((l.encode(defenc) for l in new_lines))\n    reflog_path = RefLog.path(cls(repo, full_ref_path))\n    if os.path.isfile(reflog_path):\n        os.remove(reflog_path)\n", "label": 1}
{"function": "\n\ndef test_remove_role():\n    db = SQLAlchemy('sqlite:///:memory:')\n    auth = authcode.Auth(SECRET_KEY, db=db, roles=True)\n    User = auth.User\n    Role = auth.Role\n    db.create_all()\n    user = User(login='meh', password='foobar')\n    db.session.add(user)\n    db.session.commit()\n    assert hasattr(auth, 'Role')\n    assert hasattr(User, 'roles')\n    user.add_role('admin')\n    db.session.commit()\n    assert user.has_role('admin')\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('foobar')\n    db.session.commit()\n    assert (db.query(Role).count() == 1)\n", "label": 1}
{"function": "\n\ndef configure_dhcp_for_network(self, network):\n    if (not network.admin_state_up):\n        return\n    enable_metadata = self.dhcp_driver_cls.should_enable_metadata(self.conf, network)\n    dhcp_network_enabled = False\n    for subnet in network.subnets:\n        if subnet.enable_dhcp:\n            if self.call_driver('enable', network):\n                dhcp_network_enabled = True\n                self.cache.put(network)\n            break\n    if (enable_metadata and dhcp_network_enabled):\n        for subnet in network.subnets:\n            if ((subnet.ip_version == 4) and subnet.enable_dhcp):\n                self.enable_isolated_metadata_proxy(network)\n                break\n    elif ((not self.conf.force_metadata) and (not self.conf.enable_isolated_metadata)):\n        self.disable_isolated_metadata_proxy(network)\n", "label": 1}
{"function": "\n\ndef lex(s, name=None, trim_whitespace=True, line_offset=0, delimeters=None):\n    if (delimeters is None):\n        delimeters = (Template.default_namespace['start_braces'], Template.default_namespace['end_braces'])\n    in_expr = False\n    chunks = []\n    last = 0\n    last_pos = ((line_offset + 1), 1)\n    token_re = re.compile(('%s|%s' % (re.escape(delimeters[0]), re.escape(delimeters[1]))))\n    for match in token_re.finditer(s):\n        expr = match.group(0)\n        pos = find_position(s, match.end(), last, last_pos)\n        if ((expr == delimeters[0]) and in_expr):\n            raise TemplateError(('%s inside expression' % delimeters[0]), position=pos, name=name)\n        elif ((expr == delimeters[1]) and (not in_expr)):\n            raise TemplateError(('%s outside expression' % delimeters[1]), position=pos, name=name)\n        if (expr == delimeters[0]):\n            part = s[last:match.start()]\n            if part:\n                chunks.append(part)\n            in_expr = True\n        else:\n            chunks.append((s[last:match.start()], last_pos))\n            in_expr = False\n        last = match.end()\n        last_pos = pos\n    if in_expr:\n        raise TemplateError(('No %s to finish last expression' % delimeters[1]), name=name, position=last_pos)\n    part = s[last:]\n    if part:\n        chunks.append(part)\n    if trim_whitespace:\n        chunks = trim_lex(chunks)\n    return chunks\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('grid_type', ALL_GRID_TYPES)\ndef test_append_two_quantities_invalid(self, grid_type):\n    g = self.grid[grid_type]\n    g['density'] = []\n    g['energy'] = []\n    g['density'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    assert g['density'][0]\n    assert g['density'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['density'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert g['energy'][0]\n    assert g['energy'][1]\n    assert g['energy'][(- 1)]\n    assert g['energy'][(- 2)]\n    with pytest.raises(IndexError) as exc:\n        assert g['energy'][2]\n    assert (exc_msg(exc) == 'list index out of range')\n    with pytest.raises(ValueError) as exc:\n        g.n_dust\n    assert (exc_msg(exc) == 'Not all dust lists in the grid have the same size')\n", "label": 1}
{"function": "\n\ndef commit_dirtiness_flags(self):\n    '\\n        Updates any dirtiness flags in the database.\\n        '\n    if self.domain:\n        flags_to_save = self.get_flags_to_save()\n        if should_create_flags_on_submission(self.domain):\n            assert settings.UNIT_TESTING\n            all_touched_ids = (set(flags_to_save.keys()) | self.get_clean_owner_ids())\n            to_update = {f.owner_id: f for f in OwnershipCleanlinessFlag.objects.filter(domain=self.domain, owner_id__in=list(all_touched_ids))}\n            for owner_id in all_touched_ids:\n                if (owner_id not in to_update):\n                    flag = OwnershipCleanlinessFlag(domain=self.domain, owner_id=owner_id, is_clean=True)\n                    if (owner_id in flags_to_save):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                    flag.save()\n                else:\n                    flag = to_update[owner_id]\n                    if ((owner_id in flags_to_save) and (flag.is_clean or (not flag.hint))):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                        flag.save()\n        else:\n            flags_to_update = OwnershipCleanlinessFlag.objects.filter(Q(domain=self.domain), Q(owner_id__in=flags_to_save.keys()), (Q(is_clean=True) | Q(hint__isnull=True)))\n            for flag in flags_to_update:\n                flag.is_clean = False\n                flag.hint = flags_to_save[flag.owner_id]\n                flag.save()\n", "label": 1}
{"function": "\n\ndef _compute_generator_info(self):\n    \"\\n        Compute the generator's state variables as the union of live variables\\n        at all yield points.\\n        \"\n    gi = self.generator_info\n    for yp in gi.get_yield_points():\n        live_vars = set(self.block_entry_vars[yp.block])\n        weak_live_vars = set()\n        stmts = iter(yp.block.body)\n        for stmt in stmts:\n            if isinstance(stmt, ir.Assign):\n                if (stmt.value is yp.inst):\n                    break\n                live_vars.add(stmt.target.name)\n            elif isinstance(stmt, ir.Del):\n                live_vars.remove(stmt.value)\n        else:\n            assert 0, \"couldn't find yield point\"\n        for stmt in stmts:\n            if isinstance(stmt, ir.Del):\n                name = stmt.value\n                if (name in live_vars):\n                    live_vars.remove(name)\n                    weak_live_vars.add(name)\n            else:\n                break\n        yp.live_vars = live_vars\n        yp.weak_live_vars = weak_live_vars\n    st = set()\n    for yp in gi.get_yield_points():\n        st |= yp.live_vars\n        st |= yp.weak_live_vars\n    gi.state_vars = sorted(st)\n", "label": 1}
{"function": "\n\ndef test05_ManyActorsUniqueAddress(self):\n    asys = ActorSystem()\n    addresses = [asys.createActor(Juliet) for n in range(100)]\n    uniqueAddresses = set(addresses)\n    if (len(addresses) != len(uniqueAddresses)):\n        duplicates = [A for A in uniqueAddresses if (len([X for X in addresses if (X == A)]) > 1)]\n        print(('Duplicates: %s' % map(str, duplicates)))\n        if duplicates:\n            for each in duplicates:\n                print(('... %s at: %s' % (str(each), str([N for (N, A) in enumerate(addresses) if (A == each)]))))\n        print('Note: if this is a UDPTransport test, be advised that Linux occasionally does seem to assign the same UDP port multiple times.  Linux bug?')\n    self.assertEqual(len(addresses), len(uniqueAddresses))\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if (self is other):\n        return True\n    return (isinstance(other, self.__class__) and (self._numeric_id == other._numeric_id) and (self._id == other._id) and (self._cells_table == other._cells_table) and (self._contents_table == other._contents_table) and (self._deps_table == other._deps_table) and (self._renames == other._renames) and (self._deltas == other._deltas) and (self._cell_count == other._cell_count) and (self._content_count == other._content_count))\n", "label": 1}
{"function": "\n\ndef get_dot_completions(view, prefix, position, info):\n    if ((not get_setting(view, 'fw1_enabled')) or (len(info['dot_context']) == 0)):\n        return None\n    if extends_fw1(view):\n        if (info['dot_context'][(- 1)].name == 'variables'):\n            key = '.'.join([symbol.name for symbol in reversed(info['dot_context'])])\n            if (key in fw1['settings']):\n                return CompletionList(fw1['settings'][key], 1, False)\n        if (info['dot_context'][(- 1)].name in ['renderdata', 'renderer']):\n            return CompletionList(fw1['methods']['renderdata'], 1, False)\n    if (get_file_type(view) == 'controller'):\n        if ((len(info['dot_context']) > 1) and (info['dot_context'][(- 2)].name in ['renderdata', 'renderer'])):\n            return CompletionList(fw1['methods']['renderdata'], 1, False)\n        if (info['dot_context'][(- 1)].name in ['fw', 'framework']):\n            return CompletionList(fw1['methods']['calls'], 1, False)\n    return None\n", "label": 1}
{"function": "\n\ndef test_tb_option(self, testdir, option):\n    testdir.makepyfile('\\n            import pytest\\n            def g():\\n                raise IndexError\\n            def test_func():\\n                print (6*7)\\n                g()  # --calling--\\n        ')\n    for tbopt in ['long', 'short', 'no']:\n        print(('testing --tb=%s...' % tbopt))\n        result = testdir.runpytest(('--tb=%s' % tbopt))\n        s = result.stdout.str()\n        if (tbopt == 'long'):\n            assert ('print (6*7)' in s)\n        else:\n            assert ('print (6*7)' not in s)\n        if (tbopt != 'no'):\n            assert ('--calling--' in s)\n            assert ('IndexError' in s)\n        else:\n            assert ('FAILURES' not in s)\n            assert ('--calling--' not in s)\n            assert ('IndexError' not in s)\n", "label": 1}
{"function": "\n\n@register.simple_tag\ndef aggregate_section_totals(section_name, results_arr, daily):\n    'Hackish function to do a summation of a section in the org_report'\n    startindex = (- 1)\n    endindex = (- 1)\n    for itemarr in results_arr:\n        if (itemarr[1] == section_name):\n            startindex = results_arr.index(itemarr)\n            continue\n        if (startindex >= 0):\n            if (itemarr[1] != None):\n                endindex = results_arr.index(itemarr)\n                break\n    summation = []\n    section_arr = []\n    if (endindex == (- 1)):\n        section_arr = results_arr[startindex:]\n    else:\n        section_arr = results_arr[startindex:(endindex + 1)]\n    for itemarr in section_arr:\n        if (summation == []):\n            summation = (summation + itemarr[(- 1)])\n        else:\n            for i in range(0, len(itemarr[(- 1)])):\n                summation[i] += itemarr[(- 1)][i]\n    ret = ''\n    if daily:\n        for item in summation:\n            ret += ('<td style=\"background:#99FFFF\"><strong>%d</strong></td>' % item)\n    else:\n        sum = 0\n        for item in summation:\n            sum += item\n        ret = ('<td>%d</td>' % sum)\n    return ret\n", "label": 1}
{"function": "\n\ndef test_log_pdf(self):\n    X_1 = numpy.array([[1], [0]])\n    parameters = dict(mu=0.0, rho=1.0)\n    log_pdf = self.component_class.log_pdf(X_1, parameters)\n    assert (len(log_pdf) == 2)\n    assert (math.fabs(((- 1.4189385332046727) - log_pdf[(0, 0)])) < 1e-08)\n    assert (math.fabs(((- 0.9189385332046727) - log_pdf[(1, 0)])) < 1e-08)\n    parameters = dict(mu=2.2, rho=12.1)\n    log_pdf = self.component_class.log_pdf(X_1, parameters)\n    assert (len(log_pdf) == 2)\n    assert (math.fabs(((- 8.38433580690333) - log_pdf[(0, 0)])) < 1e-08)\n    assert (math.fabs(((- 28.954335806903334) - log_pdf[(1, 0)])) < 1e-08)\n    parameters = dict(mu=0.0, rho=1.0)\n    lspc = numpy.linspace(0, 10, num=20)\n    X_2 = numpy.array([[x] for x in lspc])\n    log_pdf = self.component_class.log_pdf(X_2, parameters)\n    assert (len(log_pdf) == 20)\n    for n in range(1, 20):\n        assert (log_pdf[((n - 1), 0)] > log_pdf[(n, 0)])\n", "label": 1}
{"function": "\n\ndef _construct_keymaps(self, config):\n    '\\n        Construct keymaps for handling input\\n        '\n    keymap = {\n        \n    }\n    move_keymap = {\n        \n    }\n    for key in config.move_left:\n        keymap[key] = self._move\n        move_keymap[key] = 7\n    for key in config.move_up:\n        keymap[key] = self._move\n        move_keymap[key] = 1\n    for key in config.move_right:\n        keymap[key] = self._move\n        move_keymap[key] = 3\n    for key in config.move_down:\n        keymap[key] = self._move\n        move_keymap[key] = 5\n    for key in config.start:\n        keymap[key] = self._menu\n    for key in config.action_a:\n        keymap[key] = self._action_a\n    for key in config.back:\n        keymap[key] = self._back\n    for key in config.left_shoulder:\n        keymap[key] = self._shoulder_left\n    for key in config.right_shoulder:\n        keymap[key] = self._shoulder_right\n    for key in config.mode_1:\n        keymap[key] = self._zoom_out\n    for key in config.mode_2:\n        keymap[key] = self._zoom_in\n    return (keymap, move_keymap)\n", "label": 1}
{"function": "\n\ndef load(self, filename):\n    if (not filename):\n        import traceback\n        traceback.print_stack()\n        return\n    try:\n        im = None\n        if self._inline:\n            im = pygame.image.load(filename, 'x.{}'.format(self._ext))\n        elif isfile(filename):\n            with open(filename, 'rb') as fd:\n                im = pygame.image.load(fd)\n        elif isinstance(filename, bytes):\n            try:\n                fname = filename.decode()\n                if isfile(fname):\n                    with open(fname, 'rb') as fd:\n                        im = pygame.image.load(fd)\n            except UnicodeDecodeError:\n                pass\n        if (im is None):\n            im = pygame.image.load(filename)\n    except:\n        raise\n    fmt = ''\n    if (im.get_bytesize() == 3):\n        fmt = 'rgb'\n    elif (im.get_bytesize() == 4):\n        fmt = 'rgba'\n    if (fmt not in ('rgb', 'rgba')):\n        try:\n            imc = im.convert(32)\n            fmt = 'rgba'\n        except:\n            try:\n                imc = im.convert_alpha()\n                fmt = 'rgba'\n            except:\n                Logger.warning(('Image: Unable to convert image %r to rgba (was %r)' % (filename, im.fmt)))\n                raise\n        im = imc\n    if (not self._inline):\n        self.filename = filename\n    data = pygame.image.tostring(im, fmt.upper())\n    return [ImageData(im.get_width(), im.get_height(), fmt, data, source=filename)]\n", "label": 1}
{"function": "\n\ndef test_file_metadata_drive(basepath):\n    item = fixtures.list_file['items'][0]\n    path = basepath.child(item['title'])\n    parsed = GoogleDriveFileMetadata(item, path)\n    assert (parsed.provider == 'googledrive')\n    assert (parsed.id == item['id'])\n    assert (path.name == item['title'])\n    assert (parsed.name == item['title'])\n    assert (parsed.size == item['fileSize'])\n    assert (parsed.modified == item['modifiedDate'])\n    assert (parsed.content_type == item['mimeType'])\n    assert (parsed.extra == {\n        'revisionId': item['version'],\n        'webView': item['alternateLink'],\n    })\n    assert (parsed.path == ('/' + os.path.join(*[x.raw for x in path.parts])))\n    assert (parsed.materialized_path == str(path))\n    assert (parsed.is_google_doc == False)\n    assert (parsed.export_name == item['title'])\n", "label": 1}
{"function": "\n\ndef extend_list(self, data, parsed_args):\n    'Add subnet information to a network list.'\n    neutron_client = self.get_client()\n    search_opts = {\n        'fields': ['id', 'cidr'],\n    }\n    if self.pagination_support:\n        page_size = parsed_args.page_size\n        if page_size:\n            search_opts.update({\n                'limit': page_size,\n            })\n    subnet_ids = []\n    for n in data:\n        if ('subnets' in n):\n            subnet_ids.extend(n['subnets'])\n\n    def _get_subnet_list(sub_ids):\n        search_opts['id'] = sub_ids\n        return neutron_client.list_subnets(**search_opts).get('subnets', [])\n    try:\n        subnets = _get_subnet_list(subnet_ids)\n    except exceptions.RequestURITooLong as uri_len_exc:\n        subnet_count = len(subnet_ids)\n        max_size = ((self.subnet_id_filter_len * subnet_count) - uri_len_exc.excess)\n        chunk_size = (max_size // self.subnet_id_filter_len)\n        subnets = []\n        for i in range(0, subnet_count, chunk_size):\n            subnets.extend(_get_subnet_list(subnet_ids[i:(i + chunk_size)]))\n    subnet_dict = dict([(s['id'], s) for s in subnets])\n    for n in data:\n        if ('subnets' in n):\n            n['subnets'] = [(subnet_dict.get(s) or {\n                'id': s,\n            }) for s in n['subnets']]\n", "label": 1}
{"function": "\n\ndef create_table_constraints(self, table):\n    constraints = []\n    if table.primary_key:\n        constraints.append(table.primary_key)\n    constraints.extend([c for c in table._sorted_constraints if (c is not table.primary_key)])\n    return ', \\n\\t'.join((p for p in (self.process(constraint) for constraint in constraints if (((constraint._create_rule is None) or constraint._create_rule(self)) and ((not self.dialect.supports_alter) or (not getattr(constraint, 'use_alter', False))))) if (p is not None)))\n", "label": 1}
{"function": "\n\ndef __dir__(self):\n    'return list of member names'\n    cls_members = []\n    cname = self.__class__.__name__\n    if ((cname != 'SymbolTable') and hasattr(self, '__class__')):\n        cls_members = dir(self.__class__)\n    dict_keys = [key for key in self.__dict__ if (key not in cls_members)]\n    return [key for key in (cls_members + dict_keys) if ((not key.startswith('_SymbolTable_')) and (not key.startswith('_Group_')) and (not key.startswith(('_%s_' % cname))) and (not (key.startswith('__') and key.endswith('__'))) and (key not in self.__private))]\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.LIST):\n                self.expressions = []\n                (_etype17, _size14) = iprot.readListBegin()\n                for _i18 in xrange(_size14):\n                    _elem19 = IndexExpression()\n                    _elem19.read(iprot)\n                    self.expressions.append(_elem19)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.start_key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef run(self, show_trees=False):\n    '\\n        Sentences in the test suite are divided into two classes:\\n         - grammatical (``accept``) and\\n         - ungrammatical (``reject``).\\n        If a sentence should parse accordng to the grammar, the value of\\n        ``trees`` will be a non-empty list. If a sentence should be rejected\\n        according to the grammar, then the value of ``trees`` will be None.\\n        '\n    for test in self.suite:\n        print((test['doc'] + ':'), end=' ')\n        for key in ['accept', 'reject']:\n            for sent in test[key]:\n                tokens = sent.split()\n                trees = list(self.cp.parse(tokens))\n                if (show_trees and trees):\n                    print()\n                    print(sent)\n                    for tree in trees:\n                        print(tree)\n                if (key == 'accept'):\n                    if (trees == []):\n                        raise ValueError((\"Sentence '%s' failed to parse'\" % sent))\n                    else:\n                        accepted = True\n                elif trees:\n                    raise ValueError((\"Sentence '%s' received a parse'\" % sent))\n                else:\n                    rejected = True\n        if (accepted and rejected):\n            print('All tests passed!')\n", "label": 1}
{"function": "\n\ndef test_sqrt_rounding():\n    for i in [2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15]:\n        i = from_int(i)\n        for dps in [7, 15, 83, 106, 2000]:\n            mp.dps = dps\n            a = mpf_pow_int(mpf_sqrt(i, mp.prec, round_down), 2, mp.prec, round_down)\n            b = mpf_pow_int(mpf_sqrt(i, mp.prec, round_up), 2, mp.prec, round_up)\n            assert mpf_lt(a, i)\n            assert mpf_gt(b, i)\n    random.seed(1234)\n    prec = 100\n    for rnd in [round_down, round_nearest, round_ceiling]:\n        for i in range(100):\n            a = mpf_rand(prec)\n            b = mpf_mul(a, a)\n            assert (mpf_sqrt(b, prec, rnd) == a)\n    mp.dps = 100\n    a = (mpf(9) + 1e-90)\n    b = (mpf(9) - 1e-90)\n    mp.dps = 15\n    assert (sqrt(a, rounding='d') == 3)\n    assert (sqrt(a, rounding='n') == 3)\n    assert (sqrt(a, rounding='u') > 3)\n    assert (sqrt(b, rounding='d') < 3)\n    assert (sqrt(b, rounding='n') == 3)\n    assert (sqrt(b, rounding='u') == 3)\n    assert (sqrt(mpf('7.0503726185518891')) == mpf('2.655253776675949'))\n", "label": 1}
{"function": "\n\ndef maximalSquare(self, matrix):\n    if (not matrix):\n        return 0\n    (H, W) = (0, 1)\n    table = [[[0, 0] for j in xrange(len(matrix[0]))] for i in xrange(len(matrix))]\n    for i in reversed(xrange(len(matrix))):\n        for j in reversed(xrange(len(matrix[i]))):\n            if (matrix[i][j] == '1'):\n                (h, w) = (1, 1)\n                if ((i + 1) < len(matrix)):\n                    h = (table[(i + 1)][j][H] + 1)\n                if ((j + 1) < len(matrix[i])):\n                    w = (table[i][(j + 1)][W] + 1)\n                table[i][j] = [h, w]\n    s = [[0 for j in xrange(len(matrix[0]))] for i in xrange(len(matrix))]\n    max_square_area = 0\n    for i in reversed(xrange(len(matrix))):\n        for j in reversed(xrange(len(matrix[i]))):\n            side = min(table[i][j][H], table[i][j][W])\n            if (matrix[i][j] == '1'):\n                if (((i + 1) < len(matrix)) and ((j + 1) < len(matrix[(i + 1)]))):\n                    side = min((s[(i + 1)][(j + 1)] + 1), side)\n                s[i][j] = side\n                max_square_area = max(max_square_area, (side * side))\n    return max_square_area\n", "label": 1}
{"function": "\n\ndef flatten_data(self, resource_object, parser_context, is_list):\n    '\\n        Flattens data objects, making attributes and relationships fields the same level as id and type.\\n        '\n    relationships = resource_object.get('relationships')\n    is_relationship = parser_context.get('is_relationship')\n    request_method = parser_context['request'].method\n    if (is_relationship and (request_method == 'POST')):\n        if (not relationships):\n            raise JSONAPIException(source={\n                'pointer': '/data/relationships',\n            }, detail=NO_RELATIONSHIPS_ERROR)\n    elif (('attributes' not in resource_object) and (request_method != 'DELETE')):\n        raise JSONAPIException(source={\n            'pointer': '/data/attributes',\n        }, detail=NO_ATTRIBUTES_ERROR)\n    object_id = resource_object.get('id')\n    object_type = resource_object.get('type')\n    if (is_list and (request_method == 'DELETE')):\n        if (object_id is None):\n            raise JSONAPIException(source={\n                'pointer': '/data/id',\n            }, detail=NO_ID_ERROR)\n        if (object_type is None):\n            raise JSONAPIException(source={\n                'pointer': '/data/type',\n            }, detail=NO_TYPE_ERROR)\n    attributes = resource_object.get('attributes')\n    parsed = {\n        'id': object_id,\n        'type': object_type,\n    }\n    if attributes:\n        parsed.update(attributes)\n    if relationships:\n        relationships = self.flatten_relationships(relationships)\n        parsed.update(relationships)\n    return parsed\n", "label": 1}
{"function": "\n\ndef parse_options(self, arg):\n    '\\n        Parse options with the argv\\n\\n        :param arg: one arg from argv\\n        '\n    if (not arg.startswith('-')):\n        return False\n    value = None\n    if ('=' in arg):\n        (arg, value) = arg.split('=')\n    for option in self._option_list:\n        if (arg not in (option.shortname, option.longname)):\n            continue\n        action = option.action\n        if action:\n            action()\n        if (option.key == option.shortname):\n            self._results[option.key] = True\n            return True\n        if (option.boolean and option.default):\n            self._results[option.key] = False\n            return True\n        if option.boolean:\n            self._results[option.key] = True\n            return True\n        if (not value):\n            if self._argv:\n                value = self._argv[0]\n                self._argv = self._argv[1:]\n        if (not value):\n            raise RuntimeError(('Missing value for: %s' % option.name))\n        self._results[option.key] = option.to_python(value)\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I64):\n                self.latency_ns = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I64):\n                self.cpu_time_ns = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.cardinality = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I64):\n                self.memory_used = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef resolve(url):\n    try:\n        url = url.split('/preview', 1)[0]\n        url = url.replace('drive.google.com', 'docs.google.com')\n        result = client.request(url)\n        result = re.compile('\"fmt_stream_map\",(\".+?\")').findall(result)[0]\n        u = json.loads(result)\n        u = [i.split('|')[(- 1)] for i in u.split(',')]\n        u = sum([tag(i) for i in u], [])\n        url = []\n        try:\n            url += [[i for i in u if (i['quality'] == '1080p')][0]]\n        except:\n            pass\n        try:\n            url += [[i for i in u if (i['quality'] == 'HD')][0]]\n        except:\n            pass\n        try:\n            url += [[i for i in u if (i['quality'] == 'SD')][0]]\n        except:\n            pass\n        if (url == []):\n            return\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    if ('REQUEST_URI' not in environ):\n        environ['REQUEST_URI'] = (quote(environ.get('SCRIPT_NAME', '')) + quote(environ.get('PATH_INFO', '')))\n    if self.include_os_environ:\n        cgi_environ = os.environ.copy()\n    else:\n        cgi_environ = {\n            \n        }\n    for name in environ:\n        if ((name.upper() == name) and isinstance(environ[name], str)):\n            cgi_environ[name] = environ[name]\n    if (self.query_string is not None):\n        old = cgi_environ.get('QUERY_STRING', '')\n        if old:\n            old += '&'\n        cgi_environ['QUERY_STRING'] = (old + self.query_string)\n    cgi_environ['SCRIPT_FILENAME'] = self.script\n    proc = subprocess.Popen([self.script], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=cgi_environ, cwd=os.path.dirname(self.script))\n    writer = CGIWriter(environ, start_response)\n    if (select and (sys.platform != 'win32')):\n        proc_communicate(proc, stdin=StdinReader.from_environ(environ), stdout=writer, stderr=environ['wsgi.errors'])\n    else:\n        (stdout, stderr) = proc.communicate(StdinReader.from_environ(environ).read())\n        if stderr:\n            environ['wsgi.errors'].write(stderr)\n        writer.write(stdout)\n    if (not writer.headers_finished):\n        start_response(writer.status, writer.headers)\n    return []\n", "label": 1}
{"function": "\n\ndef collect(self):\n    '\\n        Collect metrics\\n        '\n    collected = {\n        \n    }\n    files = []\n    if isinstance(self.config['dir'], basestring):\n        dirs = [d.strip() for d in self.config['dir'].split(',')]\n    elif isinstance(self.config['dir'], list):\n        dirs = self.config['dir']\n    if isinstance(self.config['files'], basestring):\n        files = [f.strip() for f in self.config['files'].split(',')]\n    elif isinstance(self.config['files'], list):\n        files = self.config['files']\n    for sdir in dirs:\n        for sfile in files:\n            if sfile.endswith('conntrack_count'):\n                metric_name = 'ip_conntrack_count'\n            elif sfile.endswith('conntrack_max'):\n                metric_name = 'ip_conntrack_max'\n            else:\n                self.log.error('Unknown file for collection: %s', sfile)\n                continue\n            fpath = os.path.join(sdir, sfile)\n            if (not os.path.exists(fpath)):\n                continue\n            try:\n                with open(fpath, 'r') as fhandle:\n                    metric = float(fhandle.readline().rstrip('\\n'))\n                    collected[metric_name] = metric\n            except Exception as exception:\n                self.log.error(\"Failed to collect from '%s': %s\", fpath, exception)\n    if (not collected):\n        self.log.error('No metric was collected, looks like nf_conntrack/ip_conntrack kernel module was not loaded')\n    else:\n        for key in collected.keys():\n            self.publish(key, collected[key])\n", "label": 1}
{"function": "\n\ndef arg2nodes(self, args, node_factory=_null, lookup_list=_null, **kw):\n    if (node_factory is _null):\n        node_factory = self.fs.File\n    if (lookup_list is _null):\n        lookup_list = self.lookup_list\n    if (not args):\n        return []\n    args = SCons.Util.flatten(args)\n    nodes = []\n    for v in args:\n        if SCons.Util.is_String(v):\n            n = None\n            for l in lookup_list:\n                n = l(v)\n                if (n is not None):\n                    break\n            if (n is not None):\n                if SCons.Util.is_String(n):\n                    kw['raw'] = 1\n                    n = self.subst(n, **kw)\n                    if node_factory:\n                        n = node_factory(n)\n                if SCons.Util.is_List(n):\n                    nodes.extend(n)\n                else:\n                    nodes.append(n)\n            elif node_factory:\n                kw['raw'] = 1\n                v = node_factory(self.subst(v, **kw))\n                if SCons.Util.is_List(v):\n                    nodes.extend(v)\n                else:\n                    nodes.append(v)\n        else:\n            nodes.append(v)\n    return nodes\n", "label": 1}
{"function": "\n\ndef download_default_pages(names, prefix):\n    from httplib import HTTPSConnection\n    host = 'trac.edgewall.org'\n    if (prefix and (not prefix.endswith('/'))):\n        prefix += '/'\n    conn = HTTPSConnection(host)\n    for name in names:\n        if (name in ('WikiStart', 'SandBox')):\n            continue\n        sys.stdout.write(('Downloading %s%s' % (prefix, name)))\n        conn.request('GET', ('/wiki/%s%s?format=txt' % (prefix, name)))\n        response = conn.getresponse()\n        content = response.read()\n        if (prefix and ((response.status != 200) or (not content))):\n            sys.stdout.write((' %s' % name))\n            conn.request('GET', ('/wiki/%s?format=txt' % name))\n            response = conn.getresponse()\n            content = response.read()\n        if ((response.status == 200) and content):\n            with open(('trac/wiki/default-pages/' + name), 'w') as f:\n                lines = content.replace('\\r\\n', '\\n').splitlines(True)\n                f.write(''.join((line for line in lines if (line.strip() != '[[TranslatedPages]]'))))\n            sys.stdout.write('\\tdone.\\n')\n        else:\n            sys.stdout.write('\\tmissing or empty.\\n')\n    conn.close()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.groupName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_versions(default=DEFAULT, verbose=False):\n    assert (versionfile_source is not None), 'please set versioneer.versionfile_source'\n    assert (tag_prefix is not None), 'please set versioneer.tag_prefix'\n    assert (parentdir_prefix is not None), 'please set versioneer.parentdir_prefix'\n    root = get_root()\n    versionfile_abs = os.path.join(root, versionfile_source)\n    variables = get_expanded_variables(versionfile_abs)\n    if variables:\n        ver = versions_from_expanded_variables(variables, tag_prefix)\n        if ver:\n            if verbose:\n                print(('got version from expanded variable %s' % ver))\n            return ver\n    ver = versions_from_file(versionfile_abs)\n    if ver:\n        if verbose:\n            print(('got version from file %s %s' % (versionfile_abs, ver)))\n        return ver\n    ver = versions_from_vcs(tag_prefix, root, verbose)\n    if ver:\n        if verbose:\n            print(('got version from git %s' % ver))\n        return ver\n    ver = versions_from_parentdir(parentdir_prefix, root, verbose)\n    if ver:\n        if verbose:\n            print(('got version from parentdir %s' % ver))\n        return ver\n    if verbose:\n        print(('got version from default %s' % ver))\n    return default\n", "label": 1}
{"function": "\n\ndef checkNode(graph, superpipeline, source, target, nodeType, expectedDataType, values, isInput):\n    errors = er.consistencyErrors()\n    data = superpipeline.pipelineConfigurationData[superpipeline.pipeline]\n    longFormArgument = graph.CM_getArgumentAttribute(graph.graph, source, target, 'longFormArgument')\n    dataType = graph.CM_getArgumentAttribute(graph.graph, source, target, 'dataType')\n    isLinkOnly = graph.CM_getArgumentAttribute(graph.graph, source, target, 'isLinkOnly')\n    if (not isLinkOnly):\n        if (not expectedDataType):\n            expectedDataType = dataType\n        elif (expectedDataType != dataType):\n            print('dataConsistency.checkNode - 1', dataType, expectedDataType)\n            exit(0)\n        for value in values:\n            if (not isCorrectDataType(value, expectedDataType)):\n                print('dataConsistency.checkNode - 2', longFormArgument, value, dataType, type(value))\n                exit(0)\n            if (nodeType == 'file'):\n                if (not graph.CM_getArgumentAttribute(graph.graph, source, target, 'isStub')):\n                    extensions = graph.CM_getArgumentAttribute(graph.graph, source, target, 'extensions')\n                    if extensions:\n                        task = (target if isInput else source)\n                        fileNodeId = (source if isInput else target)\n                        if (not checkExtensions(value, extensions)):\n                            if (longFormArgument in data.longFormArguments.keys()):\n                                shortFormArgument = data.longFormArguments[longFormArgument].shortFormArgument\n                                errors.invalidExtensionPipeline(longFormArgument, shortFormArgument, value, extensions)\n                            else:\n                                shortFormArgument = graph.CM_getArgumentAttribute(graph.graph, source, target, 'shortFormArgument')\n                                errors.invalidExtension(task, longFormArgument, shortFormArgument, value, extensions)\n        return expectedDataType\n", "label": 1}
{"function": "\n\ndef _write_viewing_info(self, group):\n    if ((self.peeloff_origin is not None) and (self.inside_observer is not None)):\n        raise Exception('Cannot specify inside observer and peeloff origin at the same time')\n    if (self.inside_observer is not None):\n        group.attrs['inside_observer'] = bool2str(True)\n        self._write_inside_observer(group)\n        if (self.viewing_angles == []):\n            self.set_viewing_angles([90.0], [0.0])\n        if (self.image and (self.xmin < self.xmax)):\n            raise ValueError('longitudes should increase towards the left for inside observers')\n        if (self.d_min < 0.0):\n            if (self.d_min != (- np.inf)):\n                raise ValueError('Lower limit of depth should be positive for inside observer')\n            self.d_min = 0.0\n        if (self.d_max < 0.0):\n            raise ValueError('Upper limit of depth should be positive for inside observer')\n    elif (len(self.viewing_angles) > 0):\n        group.attrs['inside_observer'] = bool2str(False)\n        if (self.peeloff_origin is None):\n            self.set_peeloff_origin((0.0, 0.0, 0.0))\n        self._write_peeloff_origin(group)\n    else:\n        raise Exception('Need to specify either observer position, or viewing angles')\n    self._write_ignore_optical_depth(group)\n    self._write_viewing_angles(group)\n    self._write_depth(group)\n", "label": 1}
{"function": "\n\ndef line_contains_open_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)/\\\\*', line) and (not re.match('^(\\\\+|\\\\-)(.*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(.*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef _get_container_description(self, state, name, network_state=True, ip_partitions=None):\n    state_container = state.containers[name]\n    container_id = state_container['id']\n    try:\n        container = self.docker_client.inspect_container(container_id)\n    except docker.errors.APIError as err:\n        if (err.response.status_code == 404):\n            return Container(name, container_id, ContainerState.MISSING)\n        else:\n            raise\n    state_dict = container.get('State')\n    if (state_dict and state_dict.get('Running')):\n        container_state = ContainerState.UP\n    else:\n        container_state = ContainerState.DOWN\n    extras = {\n        \n    }\n    network = container.get('NetworkSettings')\n    ip = None\n    if network:\n        ip = network.get('IPAddress')\n        if ip:\n            extras['ip_address'] = ip\n    if (network_state and (name in state.containers) and (container_state == ContainerState.UP)):\n        device = state_container['device']\n        extras['device'] = device\n        extras['network_state'] = self.network.network_state(device)\n        if (ip_partitions and ip):\n            extras['partition'] = ip_partitions.get(ip)\n    else:\n        extras['network_state'] = NetworkState.UNKNOWN\n        extras['device'] = None\n    cfg_container = self.config.containers.get(name)\n    extras['neutral'] = (cfg_container.neutral if cfg_container else False)\n    extras['holy'] = (cfg_container.holy if cfg_container else False)\n    return Container(name, container_id, container_state, **extras)\n", "label": 1}
{"function": "\n\ndef _filter_metadata(metadata, **kwargs):\n    if (not isinstance(metadata, dict)):\n        return metadata\n    filtered_metadata = {\n        \n    }\n    for (key, value) in metadata.items():\n        if (key == '_self'):\n            default_value = value.get('default_value', None)\n            if (default_value is None):\n                default_callback_params = value.get('default_callback_params', {\n                    \n                })\n                callback_params = dict(kwargs)\n                if default_callback_params:\n                    callback_params.update(default_callback_params)\n                default_callback = value.get('default_callback', None)\n                if default_callback:\n                    default_value = default_callback(key, **callback_params)\n            options = value.get('options', None)\n            if (options is None):\n                options_callback_params = value.get('options_callback_params', {\n                    \n                })\n                callback_params = dict(kwargs)\n                if options_callback_params:\n                    callback_params.update(options_callback_params)\n                options_callback = value.get('options_callback', None)\n                if options_callback:\n                    options = options_callback(key, **callback_params)\n            filtered_metadata[key] = value\n            if (default_value is not None):\n                filtered_metadata[key]['default_value'] = default_value\n            if (options is not None):\n                filtered_metadata[key]['options'] = options\n        else:\n            filtered_metadata[key] = _filter_metadata(value, **kwargs)\n    return filtered_metadata\n", "label": 1}
{"function": "\n\ndef query_library(self, query, tie_breaker=no_tiebreak, modifiers=None, auto=False):\n    'Queries the library for songs.\\n        returns a list of matches, or None.\\n        '\n    if (not modifiers):\n        modifiers = []\n    try:\n        if (not auto):\n            return self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, modifiers, auto))\n        else:\n            current_mods = modifiers[:]\n            future_mods = (m for m in self.auto_modifiers if (m not in modifiers))\n            while True:\n                results = self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, current_mods, auto))\n                if (not results):\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        return results\n                elif (len(results) == 1):\n                    return results\n                else:\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        raise self.TieBroken(tie_breaker(query, results))\n                    next_results = self.query_library(query, tie_breaker, current_mods, auto)\n                    if (not next_results):\n                        raise self.TieBroken(tie_breaker(query, results))\n                    else:\n                        return next_results\n    except self.TieBroken as tie:\n        return tie.results\n", "label": 1}
{"function": "\n\ndef find_corpus_fileids(root, regexp):\n    if (not isinstance(root, PathPointer)):\n        raise TypeError('find_corpus_fileids: expected a PathPointer')\n    regexp += '$'\n    if isinstance(root, ZipFilePathPointer):\n        fileids = [name[len(root.entry):] for name in root.zipfile.namelist() if (not name.endswith('/'))]\n        items = [name for name in fileids if re.match(regexp, name)]\n        return sorted(items)\n    elif isinstance(root, FileSystemPathPointer):\n        items = []\n        kwargs = {\n            \n        }\n        if (not py25()):\n            kwargs = {\n                'followlinks': True,\n            }\n        for (dirname, subdirs, fileids) in os.walk(root.path, **kwargs):\n            prefix = ''.join((('%s/' % p) for p in _path_from(root.path, dirname)))\n            items += [(prefix + fileid) for fileid in fileids if re.match(regexp, (prefix + fileid))]\n            if ('.svn' in subdirs):\n                subdirs.remove('.svn')\n        return sorted(items)\n    else:\n        raise AssertionError((\"Don't know how to handle %r\" % root))\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef action_for_event_by_user(self, event, handler, current_state):\n    actions_by_user = {\n        \n    }\n    users_dict = (yield self.store.are_guests(self.rules_by_user.keys()))\n    filtered_by_user = (yield handler.filter_events_for_clients(users_dict.items(), [event], {\n        event.event_id: current_state,\n    }))\n    room_members = (yield self.store.get_users_in_room(self.room_id))\n    evaluator = PushRuleEvaluatorForEvent(event, len(room_members))\n    condition_cache = {\n        \n    }\n    display_names = {\n        \n    }\n    for ev in current_state.values():\n        nm = ev.content.get('displayname', None)\n        if (nm and (ev.type == EventTypes.Member)):\n            display_names[ev.state_key] = nm\n    for (uid, rules) in self.rules_by_user.items():\n        display_name = display_names.get(uid, None)\n        filtered = filtered_by_user[uid]\n        if (len(filtered) == 0):\n            continue\n        if (filtered[0].sender == uid):\n            continue\n        for rule in rules:\n            if (('enabled' in rule) and (not rule['enabled'])):\n                continue\n            matches = _condition_checker(evaluator, rule['conditions'], uid, display_name, condition_cache)\n            if matches:\n                actions = [x for x in rule['actions'] if (x != 'dont_notify')]\n                if (actions and ('notify' in actions)):\n                    actions_by_user[uid] = actions\n                break\n    defer.returnValue(actions_by_user)\n", "label": 1}
{"function": "\n\ndef create_tree_from_coverage(cov, strip_prefix=None, path_aliases=None, cover=[], exclude=[]):\n    'Create a tree with coverage statistics.\\n\\n    Takes a coverage.coverage() instance.\\n\\n    Returns the root node of the tree.\\n    '\n    root = CoverageNode()\n    if path_aliases:\n        apply_path_aliases(cov, dict([alias.partition('=')[::2] for alias in path_aliases]))\n    for filename in cov.data.measured_files():\n        if (not any(((pattern in filename.replace('/', '.')) for pattern in cover))):\n            continue\n        if any(((pattern in filename.replace('/', '.')) for pattern in exclude)):\n            continue\n        if (strip_prefix and filename.startswith(strip_prefix)):\n            short_name = filename[len(strip_prefix):]\n            short_name = short_name.replace('/', os.path.sep)\n            short_name = short_name.lstrip(os.path.sep)\n        else:\n            short_name = cov.file_locator.relative_filename(filename)\n        tree_index = filename_to_list(short_name.replace(os.path.sep, '.'))\n        if (('tests' in tree_index) or ('ftests' in tree_index)):\n            continue\n        root.set_at(tree_index, CoverageCoverageNode(cov, filename))\n    return root\n", "label": 1}
{"function": "\n\ndef communicate(params, request):\n\n    def authorize_request():\n        request['client_time'] = current_milli_time()\n        request['2fa_token'] = (params.mfa_token,)\n        request['2fa_type'] = params.mfa_type\n        request['session_token'] = params.session_token\n        request['username'] = params.user\n    if (not params.session_token):\n        try:\n            login(params)\n        except:\n            raise\n    authorize_request()\n    if params.debug:\n        print(('payload: ' + str(request)))\n    try:\n        r = requests.post(params.server, json=request)\n    except:\n        raise CommunicationError(sys.exc_info()[0])\n    response_json = r.json()\n    if params.debug:\n        debug_response(params, request, r)\n    if (response_json['result_code'] == 'auth_failed'):\n        if params.debug:\n            print('Re-authorizing.')\n        try:\n            login(params)\n        except:\n            raise\n        authorize_request()\n        try:\n            r = requests.post(params.server, json=request)\n        except:\n            print('Comm error during re-auth')\n            raise CommunicationError(sys.exc_info()[0])\n        response_json = r.json()\n        if params.debug:\n            debug_response(params, request, r)\n    if (response_json['result'] != 'success'):\n        if response_json['result_code']:\n            raise CommunicationError(('Unexpected problem: ' + response_json['result_code']))\n    return response_json\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.threadName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.threadStringRepresentation = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.isDaemon = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.LIST):\n                self.stackTrace = []\n                (_etype3, _size0) = iprot.readListBegin()\n                for _i4 in xrange(_size0):\n                    _elem5 = StackTraceElement()\n                    _elem5.read(iprot)\n                    self.stackTrace.append(_elem5)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _prettifyETree(self, elem):\n    ' Recursively add linebreaks to ElementTree children. '\n    i = '\\n'\n    if (markdown.isBlockLevel(elem.tag) and (elem.tag not in ['code', 'pre'])):\n        if (((not elem.text) or (not elem.text.strip())) and len(elem) and markdown.isBlockLevel(elem[0].tag)):\n            elem.text = i\n        for e in elem:\n            if markdown.isBlockLevel(e.tag):\n                self._prettifyETree(e)\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n    if ((not elem.tail) or (not elem.tail.strip())):\n        elem.tail = i\n", "label": 1}
{"function": "\n\ndef write(self, s):\n    self._checkWritable()\n    if self.closed:\n        raise ValueError('write to closed file')\n    if (not isinstance(s, unicode)):\n        raise TypeError((\"can't write %s to text stream\" % s.__class__.__name__))\n    length = len(s)\n    haslf = ((self._writetranslate or self._line_buffering) and ('\\n' in s))\n    if (haslf and self._writetranslate and (self._writenl != '\\n')):\n        s = s.replace('\\n', self._writenl)\n    encoder = (self._encoder or self._get_encoder())\n    b = encoder.encode(s)\n    self.buffer.write(b)\n    if (self._line_buffering and (haslf or ('\\r' in s))):\n        self.flush()\n    self._snapshot = None\n    if self._decoder:\n        self._decoder.reset()\n    return length\n", "label": 1}
{"function": "\n\ndef dfs(self, s, cur, left, pi, i, rmcnt, ret):\n    '\\n        Remove parenthesis\\n        backtracking, post-check\\n        :param s: original string\\n        :param cur: current string builder\\n        :param left: number of remaining left parentheses in s[0..i] not consumed by \")\"\\n        :param pi: last removed char\\n        :param i: current index\\n        :param rmcnt: number of remaining removals needed\\n        :param ret: results\\n        '\n    if ((left < 0) or (rmcnt < 0) or (i > len(s))):\n        return\n    if (i == len(s)):\n        if ((rmcnt == 0) and (left == 0)):\n            ret.append(cur)\n        return\n    if (s[i] not in ('(', ')')):\n        self.dfs(s, (cur + s[i]), left, None, (i + 1), rmcnt, ret)\n    elif (pi == s[i]):\n        while ((i < len(s)) and pi and (pi == s[i])):\n            (i, rmcnt) = ((i + 1), (rmcnt - 1))\n        self.dfs(s, cur, left, pi, i, rmcnt, ret)\n    else:\n        self.dfs(s, cur, left, s[i], (i + 1), (rmcnt - 1), ret)\n        L = ((left + 1) if (s[i] == '(') else (left - 1))\n        self.dfs(s, (cur + s[i]), L, None, (i + 1), rmcnt, ret)\n", "label": 1}
{"function": "\n\ndef convert(self, model, field, field_args, multiple=False):\n    kwargs = {\n        'label': unicode((field.verbose_name or field.name or '')),\n        'description': (field.help_text or ''),\n        'validators': [],\n        'filters': [],\n        'default': field.default,\n    }\n    if field_args:\n        kwargs.update(field_args)\n    if field.required:\n        if (isinstance(field, IntField) or isinstance(field, FloatField)):\n            kwargs['validators'].append(validators.InputRequired())\n        else:\n            kwargs['validators'].append(validators.Required())\n    else:\n        kwargs['validators'].append(validators.Optional())\n    if field.choices:\n        kwargs['choices'] = field.choices\n        if isinstance(field, IntField):\n            kwargs['coerce'] = int\n        if (not multiple):\n            return f.SelectField(**kwargs)\n        else:\n            return f.SelectMultipleField(**kwargs)\n    ftype = type(field).__name__\n    if hasattr(field, 'to_form_field'):\n        return field.to_form_field(model, kwargs)\n    if (ftype in self.converters):\n        return self.converters[ftype](model, field, kwargs)\n", "label": 1}
{"function": "\n\n@classmethod\ndef update_next(cls, seconds, now):\n    'Setup theme for next update.'\n    cls.next_change = None\n    cls.day = None\n    closest = None\n    lowest = None\n    for t in cls.themes:\n        if ((seconds < t.time) and ((closest is None) or (t.time < closest.time))):\n            closest = t\n        if ((lowest is None) or (t.time < lowest.time)):\n            lowest = t\n    if (closest is not None):\n        cls.next_change = closest\n    elif (lowest is not None):\n        cls.next_change = lowest\n    if (lowest is not None):\n        cls.lowest = lowest\n    if ((cls.next_change.time == cls.lowest.time) and (seconds < cls.lowest.time)):\n        cls.day = (- 1)\n    else:\n        cls.day = now.day\n    debug_log(('Today = %d' % cls.day))\n    debug_log(('%s - Next Change @ %s' % (time.ctime(), str(cls.next_change))))\n", "label": 1}
{"function": "\n\n@classmethod\ndef read(cls, fd, str_cache=None, object_cache=None, traits_cache=None):\n    type_ = U8.read(fd)\n    if (str_cache is None):\n        str_cache = []\n    if (object_cache is None):\n        object_cache = []\n    if (traits_cache is None):\n        traits_cache = []\n    if ((type_ == AMF3_TYPE_UNDEFINED) or (type_ == AMF3_TYPE_NULL)):\n        return None\n    elif (type_ == AMF3_TYPE_FALSE):\n        return False\n    elif (type_ == AMF3_TYPE_TRUE):\n        return True\n    elif (type_ == AMF3_TYPE_STRING):\n        return AMF3String.read(fd, cache=str_cache)\n    elif (type_ == AMF3_TYPE_ARRAY):\n        return AMF3ArrayPacker.read(fd, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif (type_ == AMF3_TYPE_OBJECT):\n        return AMF3ObjectPacker.read(fd, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif (type_ == AMF3_TYPE_DATE):\n        return AMF3DatePacker.read(fd, cache=object_cache)\n    elif (type_ in cls.Readers):\n        return cls.Readers[type_].read(fd)\n    else:\n        raise IOError('Unhandled AMF3 type: {0}'.format(hex(type_)))\n", "label": 1}
{"function": "\n\ndef __delitem__(self, key):\n    if (isinstance(key, slice) or self._haswildcard(key)):\n        if isinstance(key, slice):\n            indices = range(*key.indices(len(self)))\n            if (key.step and (key.step < 0)):\n                indices = reversed(indices)\n        else:\n            indices = self._wildcardmatch(key)\n        for idx in reversed(indices):\n            del self[idx]\n        return\n    elif isinstance(key, string_types):\n        key = Card.normalize_keyword(key)\n        indices = self._keyword_indices\n        if (key not in self._keyword_indices):\n            indices = self._rvkc_indices\n        if (key not in indices):\n            raise KeyError((\"Keyword '%s' not found.\" % key))\n        for idx in reversed(indices[key]):\n            del self[idx]\n        return\n    idx = self._cardindex(key)\n    card = self._cards[idx]\n    keyword = card.keyword\n    del self._cards[idx]\n    indices = self._keyword_indices[keyword]\n    indices.remove(idx)\n    if (not indices):\n        del self._keyword_indices[keyword]\n    if (card.field_specifier is not None):\n        indices = self._rvkc_indices[card.rawkeyword]\n        indices.remove(idx)\n        if (not indices):\n            del self._rvkc_indices[card.rawkeyword]\n    self._updateindices(idx, increment=False)\n    self._modified = True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef assignment_onvalidation(form):\n    '\\n            Validation callback for work assignments:\\n            - a worker can only be assigned once to the same job\\n\\n            @param form: the FORM\\n        '\n    db = current.db\n    s3db = current.s3db\n    table = s3db.work_assignment\n    form_vars = form.vars\n    if ('id' in form_vars):\n        record_id = form_vars.id\n    elif hasattr(form, 'record_id'):\n        record_id = form.record_id\n    else:\n        record_id = None\n    try:\n        job_id = form_vars.job_id\n    except AttributeError:\n        job_id = None\n    try:\n        person_id = form_vars.person_id\n    except AttributeError:\n        person_id = None\n    if ((job_id is None) or (person_id is None)):\n        if record_id:\n            query = ((table.id == record_id) & (table.deleted != True))\n            record = db(query).select(table.job_id, table.person_id, limitby=(0, 1)).first()\n            if record:\n                job_id = record.job_id\n                person_id = record.person_id\n        else:\n            if (job_id is None):\n                job_id = table.job_id.default\n            if (person_id is None):\n                person_id = table.person_id.default\n    if (job_id and person_id):\n        query = (((table.job_id == job_id) & (table.person_id == person_id)) & (table.deleted != True))\n        if record_id:\n            query = ((table.id != record_id) & query)\n        duplicate = db(query).select(table.id, limitby=(0, 1)).first()\n        if duplicate:\n            msg = current.T('This person is already assigned to the job')\n            form.errors['person_id'] = msg\n", "label": 1}
{"function": "\n\ndef _collect(self):\n    LOG.debug(('collecting arguments/commands for %s' % self))\n    arguments = []\n    commands = []\n    arguments = list(self._meta.arguments)\n    for member in dir(self.__class__):\n        if member.startswith('_'):\n            continue\n        try:\n            func = getattr(self.__class__, member).__cement_meta__\n        except AttributeError:\n            continue\n        else:\n            func['controller'] = self\n            commands.append(func)\n    for contr in handler.list('controller'):\n        if (contr == self.__class__):\n            continue\n        contr = contr()\n        contr._setup(self.app)\n        if (contr._meta.stacked_on == self._meta.label):\n            if (contr._meta.stacked_type == 'embedded'):\n                (contr_arguments, contr_commands) = contr._collect()\n                for arg in contr_arguments:\n                    arguments.append(arg)\n                for func in contr_commands:\n                    commands.append(func)\n            elif (contr._meta.stacked_type == 'nested'):\n                metadict = {\n                    \n                }\n                metadict['label'] = re.sub('_', '-', contr._meta.label)\n                metadict['func_name'] = '_dispatch'\n                metadict['exposed'] = True\n                metadict['hide'] = contr._meta.hide\n                metadict['help'] = contr._meta.description\n                metadict['aliases'] = contr._meta.aliases\n                metadict['aliases_only'] = contr._meta.aliases_only\n                metadict['controller'] = contr\n                commands.append(metadict)\n    return (arguments, commands)\n", "label": 1}
{"function": "\n\ndef _raise_ssl_error(self, ssl, result):\n    if (self._context._verify_helper is not None):\n        self._context._verify_helper.raise_if_problem()\n    if (self._context._npn_advertise_helper is not None):\n        self._context._npn_advertise_helper.raise_if_problem()\n    if (self._context._npn_select_helper is not None):\n        self._context._npn_select_helper.raise_if_problem()\n    if (self._context._alpn_select_helper is not None):\n        self._context._alpn_select_helper.raise_if_problem()\n    error = _lib.SSL_get_error(ssl, result)\n    if (error == _lib.SSL_ERROR_WANT_READ):\n        raise WantReadError()\n    elif (error == _lib.SSL_ERROR_WANT_WRITE):\n        raise WantWriteError()\n    elif (error == _lib.SSL_ERROR_ZERO_RETURN):\n        raise ZeroReturnError()\n    elif (error == _lib.SSL_ERROR_WANT_X509_LOOKUP):\n        raise WantX509LookupError()\n    elif (error == _lib.SSL_ERROR_SYSCALL):\n        if (_lib.ERR_peek_error() == 0):\n            if (result < 0):\n                if (platform == 'win32'):\n                    errno = _ffi.getwinerror()[0]\n                else:\n                    errno = _ffi.errno\n                raise SysCallError(errno, errorcode.get(errno))\n            else:\n                raise SysCallError((- 1), 'Unexpected EOF')\n        else:\n            _raise_current_error()\n    elif (error == _lib.SSL_ERROR_NONE):\n        pass\n    else:\n        _raise_current_error()\n", "label": 1}
{"function": "\n\ndef wait_for_services(self, instances, callback=None):\n    assert self.connected\n    pending = set(instances)\n    ready = []\n    failed = []\n    while len(pending):\n        toRemove = []\n        for i in pending:\n            i.update()\n            if ('status' not in i.tags):\n                continue\n            if i.tags['status'].endswith('failed'):\n                toRemove.append(i)\n                failed.append(i)\n            elif (i.tags['status'] == 'ready'):\n                toRemove.append(i)\n                ready.append(i)\n        for i in toRemove:\n            pending.remove(i)\n        status = {\n            \n        }\n        for i in itertools.chain(pending, ready, failed):\n            if ('status' not in i.tags):\n                status_name = 'installing dependencies'\n            else:\n                status_name = i.tags['status']\n            if (status_name not in status):\n                status[status_name] = []\n            status[status_name].append(i)\n        if callback:\n            callback(status)\n        if len(pending):\n            time.sleep(1)\n    return (len(failed) == 0)\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw_args):\n    args = [sympify(arg) for arg in args]\n    args = TensMul._flatten(args)\n    is_canon_bp = kw_args.get('is_canon_bp', False)\n    if (not any([isinstance(arg, TensExpr) for arg in args])):\n        tids = TIDS([], [], [])\n    else:\n        tids_list = [arg._tids for arg in args if isinstance(arg, (Tensor, TensMul))]\n        if (len(tids_list) == 1):\n            for arg in args:\n                if (not isinstance(arg, Tensor)):\n                    continue\n                is_canon_bp = kw_args.get('is_canon_bp', arg._is_canon_bp)\n        tids = reduce((lambda a, b: (a * b)), tids_list)\n    if any([isinstance(arg, TensAdd) for arg in args]):\n        add_args = TensAdd._tensAdd_flatten(args)\n        return TensAdd(*add_args)\n    coeff = reduce((lambda a, b: (a * b)), ([S.One] + [arg for arg in args if (not isinstance(arg, TensExpr))]))\n    args = tids.get_tensors()\n    if (coeff != 1):\n        args = ([coeff] + args)\n    if (len(args) == 1):\n        return args[0]\n    obj = Basic.__new__(cls, *args)\n    obj._types = []\n    for t in tids.components:\n        obj._types.extend(t._types)\n    obj._tids = tids\n    obj._ext_rank = (len(obj._tids.free) + (2 * len(obj._tids.dum)))\n    obj._coeff = coeff\n    obj._is_canon_bp = is_canon_bp\n    return obj\n", "label": 1}
{"function": "\n\ndef processArgs(self, parsedArguments):\n    \"Update the Setup's config to reflect parsedArguments.\\n\\n        parsedArguments - the result of an argparse parse action.\\n        \"\n    parsed = parsedArguments\n    if (('stackLogfile' in parsed) and parsed.stackLogfile):\n        self.config.backgroundStackTraceLoopFilename = parsed.stackLogfile\n    if (('memoryLogfile' in parsed) and parsed.memoryLogfile):\n        self.config.backgroundMemoryUsageLoopFilename = parsed.memoryLogfile\n    if (('logging' in parsed) and parsed.logging):\n        self.config.setLoggingLevel(parsed.logging, parsed.background_logging)\n    if (('target' in parsed) and parsed.target):\n        self.config.target = parsed.target\n    if (('dataroot' in parsed) and parsed.dataroot):\n        path = os.path.expanduser(parsed.dataroot)\n        self.config.setRootDataDir(path)\n    if (('datarootsubdir' in parsed) and parsed.datarootsubdir):\n        path = os.path.join(self.config.rootDataDir, parsed.datarootsubdir)\n        self.config.setRootDataDir(path)\n    if (('baseport' in parsed) and parsed.baseport):\n        self.config.setAllPorts(parsed.baseport)\n    self.parsedArguments = parsedArguments\n", "label": 1}
{"function": "\n\ndef parse(self, argv=None):\n    '\\n        Parse argv of terminal\\n\\n        :param argv: default is sys.argv\\n        '\n    if (not argv):\n        argv = sys.argv\n    elif isinstance(argv, str):\n        argv = argv.split()\n    self._argv = argv[1:]\n    if (not self._argv):\n        self.validate_options()\n        if self._command_func:\n            self._command_func(**self._results)\n            return True\n        return False\n    cmd = self._argv[0]\n    if (not cmd.startswith('-')):\n        for command in self._command_list:\n            if (isinstance(command, Command) and (command._name == cmd)):\n                command._parent = self\n                return command.parse(self._argv)\n    _positional_index = 0\n    while self._argv:\n        arg = self._argv[0]\n        self._argv = self._argv[1:]\n        if (not self.parse_options(arg)):\n            self._args_results.append(arg)\n            if (len(self._positional_list) > _positional_index):\n                key = self._positional_list[_positional_index]\n                self._results[key] = arg\n                _positional_index += 1\n    self.validate_options()\n    if (self._parent and isinstance(self._parent, Command)):\n        self._parent._args_results = self._args_results\n    if self._command_func:\n        self._command_func(**self._results)\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef _key_opts(self):\n    '\\n        Return options for the ssh command base for Salt to call\\n        '\n    options = ['KbdInteractiveAuthentication=no']\n    if self.passwd:\n        options.append('PasswordAuthentication=yes')\n    else:\n        options.append('PasswordAuthentication=no')\n    if (self.opts.get('_ssh_version', (0,)) > (4, 9)):\n        options.append('GSSAPIAuthentication=no')\n    options.append('ConnectTimeout={0}'.format(self.timeout))\n    if self.opts.get('ignore_host_keys'):\n        options.append('StrictHostKeyChecking=no')\n    if self.opts.get('no_host_keys'):\n        options.extend(['StrictHostKeyChecking=no', 'UserKnownHostsFile=/dev/null'])\n    known_hosts = self.opts.get('known_hosts_file')\n    if (known_hosts and os.path.isfile(known_hosts)):\n        options.append('UserKnownHostsFile={0}'.format(known_hosts))\n    if self.port:\n        options.append('Port={0}'.format(self.port))\n    if self.priv:\n        options.append('IdentityFile={0}'.format(self.priv))\n    if self.user:\n        options.append('User={0}'.format(self.user))\n    if self.identities_only:\n        options.append('IdentitiesOnly=yes')\n    ret = []\n    for option in options:\n        ret.append('-o {0} '.format(option))\n    return ''.join(ret)\n", "label": 1}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a time. Returns a\\n        Python datetime.time object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value.time()\n    if isinstance(value, datetime.time):\n        return value\n    if isinstance(value, list):\n        if (len(value) != 2):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES)):\n            return None\n        start_value = value[0]\n        end_value = value[1]\n    start_time = None\n    end_time = None\n    for format in (self.input_formats or formats.get_format('TIME_INPUT_FORMATS')):\n        try:\n            start_time = datetime.datetime(*time.strptime(start_value, format)[:6]).time()\n        except ValueError:\n            if start_time:\n                continue\n            else:\n                raise ValidationError(self.error_messages['invalid'])\n    for format in (self.input_formats or formats.get_format('TIME_INPUT_FORMATS')):\n        try:\n            end_time = datetime.datetime(*time.strptime(end_value, format)[:6]).time()\n        except ValueError:\n            if end_time:\n                continue\n            else:\n                raise ValidationError(self.error_messages['invalid'])\n    return (start_time, end_time)\n", "label": 1}
{"function": "\n\ndef format_progress_line(self):\n    show_percent = self.show_percent\n    info_bits = []\n    if self.length_known:\n        bar_length = int((self.pct * self.width))\n        bar = (self.fill_char * bar_length)\n        bar += (self.empty_char * (self.width - bar_length))\n        if (show_percent is None):\n            show_percent = (not self.show_pos)\n    elif self.finished:\n        bar = (self.fill_char * self.width)\n    else:\n        bar = list((self.empty_char * (self.width or 1)))\n        if (self.time_per_iteration != 0):\n            bar[int((((math.cos((self.pos * self.time_per_iteration)) / 2.0) + 0.5) * self.width))] = self.fill_char\n        bar = ''.join(bar)\n    if self.show_pos:\n        info_bits.append(self.format_pos())\n    if show_percent:\n        info_bits.append(self.format_pct())\n    if (self.show_eta and self.eta_known and (not self.finished)):\n        info_bits.append(self.format_eta())\n    if (self.item_show_func is not None):\n        item_info = self.item_show_func(self.current_item)\n        if (item_info is not None):\n            info_bits.append(item_info)\n    return (self.bar_template % {\n        'label': self.label,\n        'bar': bar,\n        'info': self.info_sep.join(info_bits),\n    }).rstrip()\n", "label": 1}
{"function": "\n\ndef __init__(self, kernel, itspace, *args, **kwargs):\n    base.ParLoop.__init__(self, kernel, itspace, *args, **kwargs)\n    self.__unwound_args = []\n    self.__unique_args = []\n    self._arg_dict = {\n        \n    }\n    seen = set()\n    c = 0\n    for arg in self._actual_args:\n        if arg._is_mat:\n            for a in arg:\n                self.__unwound_args.append(a)\n        elif (arg._is_vec_map or arg._uses_itspace):\n            for (d, m) in zip(arg.data, arg.map):\n                for i in range(m.arity):\n                    a = d(arg.access, m[i])\n                    a.position = arg.position\n                    self.__unwound_args.append(a)\n        else:\n            for a in arg:\n                self.__unwound_args.append(a)\n        if arg._is_dat:\n            key = (arg.data, arg.map)\n            if arg._is_indirect:\n                arg._which_indirect = c\n                if (arg._is_vec_map or arg._flatten):\n                    c += arg.map.arity\n                elif arg._uses_itspace:\n                    c += self._it_space.extents[arg.idx.index]\n                else:\n                    c += 1\n            if (key not in seen):\n                self.__unique_args.append(arg)\n                seen.add(key)\n        else:\n            self.__unique_args.append(arg)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse_content(node):\n    \" Parse content from input node and returns ContentHandler object\\n        it'll look like:\\n\\n            - template:\\n                - file:\\n                    - temple: path\\n\\n            or something\\n\\n        \"\n    output = ContentHandler()\n    is_template_path = False\n    is_template_content = False\n    is_file = False\n    is_done = False\n    while (node and (not is_done)):\n        if isinstance(node, basestring):\n            output.content = node\n            output.setup(node, is_file=is_file, is_template_path=is_template_path, is_template_content=is_template_content)\n            return output\n        elif ((not isinstance(node, dict)) and (not isinstance(node, list))):\n            raise TypeError('Content must be a string, dictionary, or list of dictionaries')\n        is_done = True\n        flat = lowercase_keys(flatten_dictionaries(node))\n        for (key, value) in flat.items():\n            if (key == 'template'):\n                if isinstance(value, basestring):\n                    if is_file:\n                        value = os.path.abspath(value)\n                    output.content = value\n                    is_template_content = (is_template_content or (not is_file))\n                    output.is_template_content = is_template_content\n                    output.is_template_path = is_file\n                    output.is_file = is_file\n                    return output\n                else:\n                    is_template_content = True\n                    node = value\n                    is_done = False\n                    break\n            elif (key == 'file'):\n                if isinstance(value, basestring):\n                    output.content = os.path.abspath(value)\n                    output.is_file = True\n                    output.is_template_content = is_template_content\n                    return output\n                else:\n                    is_file = True\n                    node = value\n                    is_done = False\n                    break\n    raise Exception('Invalid configuration for content.')\n", "label": 1}
{"function": "\n\ndef step(self, dependency_states, step_context):\n    input_node = self._input_node()\n    input_state = dependency_states.get(input_node, None)\n    if ((input_state is None) or (type(input_state) == Waiting)):\n        return Waiting([input_node])\n    elif (type(input_state) == Throw):\n        return input_state\n    elif (type(input_state) == Noop):\n        return Noop('Could not compute {} in order to project its fields.'.format(input_node))\n    elif (type(input_state) != Return):\n        State.raise_unrecognized(input_state)\n    input_product = input_state.value\n    values = []\n    for field in self.fields:\n        values.append(getattr(input_product, field))\n    if ((len(values) == 1) and (type(values[0]) is self.projected_subject)):\n        projected_subject = values[0]\n    else:\n        projected_subject = self.projected_subject(*values)\n    output_node = self._output_node(step_context, projected_subject)\n    output_state = dependency_states.get(output_node, None)\n    if ((output_state is None) or (type(output_state) == Waiting)):\n        return Waiting([input_node, output_node])\n    elif (type(output_state) == Noop):\n        return Noop('Successfully projected, but no source of output product for {}.'.format(output_node))\n    elif (type(output_state) in [Throw, Return]):\n        return output_state\n    else:\n        raise State.raise_unrecognized(output_state)\n", "label": 1}
{"function": "\n\ndef extract_conf_from(mod, conf=ModuleConfig(CONF_SPEC), depth=0, max_depth=2):\n    'recursively extract keys from module or object\\n    by passed config scheme\\n    '\n    for (key, default_value) in six.iteritems(conf):\n        conf[key] = _get_key_from_module(mod, key, default_value)\n    try:\n        filtered_apps = [app for app in conf['apps'] if (app not in BLACKLIST)]\n    except TypeError:\n        pass\n    except Exception as e:\n        warnings.warn(('Error %s during loading %s' % (e, conf['apps'])))\n    for app in filtered_apps:\n        try:\n            app_module = import_module(app)\n            if (app_module != mod):\n                app_module = _get_correct_module(app_module)\n                if (depth < max_depth):\n                    mod_conf = extract_conf_from(app_module, depth=(depth + 1))\n                    for (k, v) in six.iteritems(mod_conf):\n                        if (k == 'config'):\n                            continue\n                        if isinstance(v, dict):\n                            conf[k].update(v)\n                        elif isinstance(v, (list, tuple)):\n                            conf[k] = merge(conf[k], v)\n        except Exception as e:\n            pass\n    return conf\n", "label": 1}
{"function": "\n\ndef parse(self, lines):\n    'parse the input lines from a robots.txt file.\\n           We allow that a user-agent: line is not preceded by\\n           one or more blank lines.'\n    state = 0\n    linenumber = 0\n    entry = Entry()\n    for line in lines:\n        linenumber += 1\n        if (not line):\n            if (state == 1):\n                entry = Entry()\n                state = 0\n            elif (state == 2):\n                self._add_entry(entry)\n                entry = Entry()\n                state = 0\n        i = line.find('#')\n        if (i >= 0):\n            line = line[:i]\n        line = line.strip()\n        if (not line):\n            continue\n        line = line.split(':', 1)\n        if (len(line) == 2):\n            line[0] = line[0].strip().lower()\n            line[1] = urllib.unquote(line[1].strip())\n            if (line[0] == 'user-agent'):\n                if (state == 2):\n                    self._add_entry(entry)\n                    entry = Entry()\n                entry.useragents.append(line[1])\n                state = 1\n            elif (line[0] == 'disallow'):\n                if (state != 0):\n                    entry.rulelines.append(RuleLine(line[1], False))\n                    state = 2\n            elif (line[0] == 'allow'):\n                if (state != 0):\n                    entry.rulelines.append(RuleLine(line[1], True))\n                    state = 2\n    if (state == 2):\n        self._add_entry(entry)\n", "label": 1}
{"function": "\n\ndef test_getitem(session):\n    set_ = session.set(key('test_sortedset_getitem'), S('abc'), SortedSet)\n    assert (set_['a'] == 1)\n    assert (set_['b'] == 1)\n    assert (set_['c'] == 1)\n    with raises(KeyError):\n        set_['d']\n    with raises(TypeError):\n        set_[123]\n    set_.update(a=2.1, c=(- 2))\n    assert (set_['a'] == 3.1)\n    assert (set_['b'] == 1)\n    assert (set_['c'] == (- 1))\n    setx = session.set(key('test_sortedsetx_getitem'), S([1, 2, 3]), IntSet)\n    assert (setx[1] == 1)\n    assert (setx[2] == 1)\n    assert (setx[3] == 1)\n    with raises(KeyError):\n        setx[4]\n    with raises(TypeError):\n        setx['a']\n    setx.update({\n        1: 2.1,\n        3: (- 2),\n    })\n    assert (setx[1] == 3.1)\n    assert (setx[2] == 1)\n    assert (setx[3] == (- 1))\n", "label": 1}
{"function": "\n\ndef __new__(mcs, cls_name, cls_bases, cls_attrs):\n    for (name, attr) in cls_attrs.items():\n        if (getattr(attr, '_unguarded', False) or (name in mcs.ALWAYS_UNGUARDED)):\n            continue\n        if name.startswith('_'):\n            continue\n        if isinstance(attr, type):\n            continue\n        is_property = isinstance(attr, property)\n        if (not (callable(attr) or is_property)):\n            continue\n        if is_property:\n            property_methods = defaultdict(None)\n            for fn_name in ('fdel', 'fset', 'fget'):\n                prop_fn = getattr(cls_attrs[name], fn_name, None)\n                if (prop_fn is not None):\n                    if getattr(prop_fn, '_unguarded', False):\n                        property_methods[fn_name] = prop_fn\n                    else:\n                        property_methods[fn_name] = pre_verify(prop_fn)\n            cls_attrs[name] = property(**property_methods)\n        else:\n            cls_attrs[name] = pre_verify(attr)\n    return super(_PageObjectMetaclass, mcs).__new__(mcs, cls_name, cls_bases, cls_attrs)\n", "label": 1}
{"function": "\n\ndef _get_desktop_streams(self, channel_id):\n    password = self.options.get('password')\n    channel = self._get_module_info('channel', channel_id, password, schema=_channel_schema)\n    if (not isinstance(channel.get('stream'), list)):\n        raise NoStreamsError(self.url)\n    streams = {\n        \n    }\n    for provider in channel['stream']:\n        if (provider['name'] == 'uhs_akamai'):\n            continue\n        provider_url = provider['url']\n        provider_name = provider['name']\n        for (stream_index, stream_info) in enumerate(provider['streams']):\n            stream = None\n            stream_height = int(stream_info.get('height', 0))\n            stream_name = stream_info.get('description')\n            if (not stream_name):\n                if (stream_height > 0):\n                    if (not stream_info.get('isTranscoded')):\n                        stream_name = '{0}p+'.format(stream_height)\n                    else:\n                        stream_name = '{0}p'.format(stream_height)\n                else:\n                    stream_name = 'live'\n            if (stream_name in streams):\n                provider_name_clean = provider_name.replace('uhs_', '')\n                stream_name += '_alt_{0}'.format(provider_name_clean)\n            if provider_name.startswith('uhs_'):\n                stream = UHSStream(self.session, channel_id, self.url, provider_name, stream_index, password)\n            elif provider_url.startswith('rtmp'):\n                playpath = stream_info['streamName']\n                stream = self._create_rtmp_stream(provider_url, playpath)\n            if stream:\n                streams[stream_name] = stream\n    return streams\n", "label": 1}
{"function": "\n\ndef test_operations():\n    per = SeqPer((1, 2), (n, 0, oo))\n    per2 = SeqPer((2, 4), (n, 0, oo))\n    form = SeqFormula((n ** 2))\n    form2 = SeqFormula((n ** 3))\n    assert (((per + form) + form2) == SeqAdd(per, form, form2))\n    assert (((per + form) - form2) == SeqAdd(per, form, (- form2)))\n    assert (((per + form) - S.EmptySequence) == SeqAdd(per, form))\n    assert (((per + per2) + form) == SeqAdd(SeqPer((3, 6), (n, 0, oo)), form))\n    assert ((S.EmptySequence - per) == (- per))\n    assert ((form + form) == SeqFormula((2 * (n ** 2))))\n    assert (((per * form) * form2) == SeqMul(per, form, form2))\n    assert ((form * form) == SeqFormula((n ** 4)))\n    assert ((form * (- form)) == SeqFormula((- (n ** 4))))\n    assert ((form * (per + form2)) == SeqMul(form, SeqAdd(per, form2)))\n    assert ((form * (per + per)) == SeqMul(form, per2))\n    assert (form.coeff_mul(m) == SeqFormula((m * (n ** 2)), (n, 0, oo)))\n    assert (per.coeff_mul(m) == SeqPer((m, (2 * m)), (n, 0, oo)))\n", "label": 1}
{"function": "\n\ndef rAssertAlmostEqual(self, a, b, rel_err=2e-15, abs_err=5e-323, msg=None):\n    'Fail if the two floating-point numbers are not almost equal.\\n\\n        Determine whether floating-point values a and b are equal to within\\n        a (small) rounding error.  The default values for rel_err and\\n        abs_err are chosen to be suitable for platforms where a float is\\n        represented by an IEEE 754 double.  They allow an error of between\\n        9 and 19 ulps.\\n        '\n    if math.isnan(a):\n        if math.isnan(b):\n            return\n        self.fail((msg or '{!r} should be nan'.format(b)))\n    if math.isinf(a):\n        if (a == b):\n            return\n        self.fail((msg or 'finite result where infinity expected: expected {!r}, got {!r}'.format(a, b)))\n    if ((not a) and (not b)):\n        if (math.copysign(1.0, a) != math.copysign(1.0, b)):\n            self.fail((msg or 'zero has wrong sign: expected {!r}, got {!r}'.format(a, b)))\n    try:\n        absolute_error = abs((b - a))\n    except OverflowError:\n        pass\n    else:\n        if (absolute_error <= max(abs_err, (rel_err * abs(a)))):\n            return\n    self.fail((msg or '{!r} and {!r} are not sufficiently close'.format(a, b)))\n", "label": 1}
{"function": "\n\ndef test_fetchable_get_item():\n    call_counter = mock.Mock()\n\n    def fetch():\n        for i in range(3):\n            for i in range(5):\n                (yield i)\n            call_counter()\n    fetcher = mock.Mock(fetch=fetch)\n    fetchable = util.Fetchable(fetcher)\n    assert (fetchable[:2] == [0, 1])\n    assert (fetchable[:3] == [0, 1, 2])\n    assert (fetchable[3] == 3)\n    assert (call_counter.call_count == 0)\n    assert (fetchable[9] == 4)\n    assert (call_counter.call_count == 1)\n    assert (fetchable[2:3] == [2])\n    assert (fetchable[:] == fetchable[:])\n    assert (len(fetchable[:]) == 15)\n    assert (fetchable[:100000] == fetchable[:])\n    assert (fetchable[100000:] == [])\n    assert (fetchable[100000:10000000] == [])\n    with pytest.raises(IndexError):\n        fetchable[12312312]\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    self.login_default_admin()\n    project = self.create_project()\n    plan = self.create_plan(project, label='Foo')\n    step = self.create_step(plan=plan)\n    self.login_default_admin()\n    path = '/api/0/steps/{0}/'.format(step.id.hex)\n    resp = self.client.post(path, data={\n        'order': 1,\n        'implementation': 'changes.buildsteps.dummy.DummyBuildStep',\n        'data': '{}',\n        'build.timeout': '1',\n    })\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (data['data'] == '{}')\n    assert (data['order'] == 1)\n    assert (data['implementation'] == 'changes.buildsteps.dummy.DummyBuildStep')\n    assert (data['options'] == {\n        'build.timeout': '1',\n    })\n    db.session.expire(step)\n    step = Step.query.get(step.id)\n    assert (step.data == {\n        \n    })\n    assert (step.order == 1)\n    assert (step.implementation == 'changes.buildsteps.dummy.DummyBuildStep')\n    options = list(ItemOption.query.filter((ItemOption.item_id == step.id)))\n    assert (len(options) == 1)\n    assert (options[0].name == 'build.timeout')\n    assert (options[0].value == '1')\n", "label": 1}
{"function": "\n\ndef __init__(self, im):\n    data = None\n    colortable = None\n    if hasattr(im, 'toUtf8'):\n        im = unicode(im.toUtf8(), 'utf-8')\n    if Image.isStringType(im):\n        im = Image.open(im)\n    if (im.mode == '1'):\n        format = QImage.Format_Mono\n    elif (im.mode == 'L'):\n        format = QImage.Format_Indexed8\n        colortable = []\n        for i in range(256):\n            colortable.append(rgb(i, i, i))\n    elif (im.mode == 'P'):\n        format = QImage.Format_Indexed8\n        colortable = []\n        palette = im.getpalette()\n        for i in range(0, len(palette), 3):\n            colortable.append(rgb(*palette[i:(i + 3)]))\n    elif (im.mode == 'RGB'):\n        data = im.tostring('raw', 'BGRX')\n        format = QImage.Format_RGB32\n    elif (im.mode == 'RGBA'):\n        try:\n            data = im.tostring('raw', 'BGRA')\n        except SystemError:\n            (r, g, b, a) = im.split()\n            im = Image.merge('RGBA', (b, g, r, a))\n        format = QImage.Format_ARGB32\n    else:\n        raise ValueError(('unsupported image mode %r' % im.mode))\n    self.__data = (data or im.tostring())\n    QImage.__init__(self, self.__data, im.size[0], im.size[1], format)\n    if colortable:\n        self.setColorTable(colortable)\n", "label": 1}
{"function": "\n\ndef initialize(self, context):\n    cmd_options = {\n        \n    }\n    if (context.device.get_sdk_version() >= 23):\n        if self.app_names:\n            cmd_options['-a'] = ','.join(self.app_names)\n        if self.buffer_size:\n            cmd_options['-b'] = self.buffer_size\n        if self.use_circular_buffer:\n            cmd_options['-c'] = None\n        if self.kernel_functions:\n            cmd_options['-k'] = ','.join(self.kernel_functions)\n        if self.ignore_signals:\n            cmd_options['-n'] = None\n        opt_string = ''.join(['{} {} '.format(name, (value or '')) for (name, value) in cmd_options.iteritems()])\n        self.start_cmd = 'atrace --async_start {} {}'.format(opt_string, ' '.join(self.categories))\n        self.output_file = os.path.join(self.device.working_directory, 'atrace.txt')\n        self.stop_cmd = 'atrace --async_stop {} > {}'.format(('-z' if self.compress_trace else ''), self.output_file)\n        available_categories = [cat.strip().split(' - ')[0] for cat in context.device.execute('atrace --list_categories').splitlines()]\n        for category in self.categories:\n            if (category not in available_categories):\n                raise ConfigError(\"Unknown category '{}'; Must be one of: {}\".format(category, available_categories))\n    else:\n        raise InstrumentError('Only android devices with an API level >= 23 can use systrace properly')\n", "label": 1}
{"function": "\n\ndef activate(self, test=False):\n    if (self.is_active and (not test)):\n        return True\n    logger.debug('Site activation started')\n    for dashboard in self.dashboards:\n        for report in dashboard.reports:\n            ct = ContentType.objects.get_for_model(report.model)\n            (report.object, created) = Report.objects.get_or_create(key=report.key, contenttype=ct)\n            if created:\n                logger.debug(('Reportobject for report %s created' % report.key))\n    register_settings = list(self.settings.keys())\n    for setting in Configuration.objects.all():\n        key = '.'.join((setting.app_label, setting.field_name))\n        if (key in self.settings):\n            if (not setting.active):\n                setting.active = True\n                setting.save()\n            register_settings.remove(key)\n        elif setting.active:\n            setting.active = False\n            setting.save()\n    if register_settings:\n        logger.debug('Need to register new settings')\n        for setting in register_settings:\n            (app, name) = setting.split('.', 1)\n            Configuration.objects.create(app_label=app, field_name=name)\n            logger.debug(('Registered setting %s' % setting))\n    self.is_active = True\n    logger.debug('Site is now active')\n    return True\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    a = self.msg\n    b = ''\n    if self.http_scheme:\n        b += ('%s://' % self.http_scheme)\n    if self.http_host:\n        b += self.http_host\n    if self.http_port:\n        b += (':%s' % self.http_port)\n    if self.http_path:\n        b += self.http_path\n    if self.http_query:\n        b += ('?%s' % self.http_query)\n    if self.http_status:\n        if b:\n            b = ('%s %s' % (b, self.http_status))\n        else:\n            b = str(self.http_status)\n    if self.http_reason:\n        if b:\n            b = ('%s %s' % (b, self.http_reason))\n        else:\n            b = ('- %s' % self.http_reason)\n    if self.http_device:\n        if b:\n            b = ('%s: device %s' % (b, self.http_device))\n        else:\n            b = ('device %s' % self.http_device)\n    if self.http_response_content:\n        if (len(self.http_response_content) <= 60):\n            b += ('   %s' % self.http_response_content)\n        else:\n            b += ('  [first 60 chars of response] %s' % self.http_response_content[:60])\n    return ((b and ('%s: %s' % (a, b))) or a)\n", "label": 1}
{"function": "\n\ndef __init__(self, parent=None, **traits):\n    super(TableEditorToolbar, self).__init__(**traits)\n    editor = self.editor\n    factory = editor.factory\n    actions = []\n    if (factory.sortable and (not factory.sort_model)):\n        actions.append(self.no_sort)\n    if ((not editor.in_column_mode) and factory.reorderable):\n        actions.append(self.move_up)\n        actions.append(self.move_down)\n    if (editor.in_row_mode and (factory.search is not None)):\n        actions.append(self.search)\n    if factory.editable:\n        if ((factory.row_factory is not None) and (not factory.auto_add)):\n            actions.append(self.add)\n        if ((factory.deletable != False) and (not editor.in_column_mode)):\n            actions.append(self.delete)\n    if factory.configurable:\n        actions.append(self.prefs)\n    if (len(actions) > 0):\n        toolbar = ToolBar(*actions, image_size=(16, 16), show_tool_names=False, show_divider=False)\n        self.control = toolbar.create_tool_bar(parent, self)\n        self.control.SetBackgroundColour(parent.GetBackgroundColour())\n        self.control.SetSize(wx.Size((23 * len(actions)), 16))\n", "label": 1}
{"function": "\n\ndef on_changed(self, which):\n    if (not hasattr(self, '_lpl')):\n        self.add_dterm('_lpl', maximum(multiply(a=multiply()), 0.0))\n    if (not hasattr(self, 'ldn')):\n        self.ldn = LightDotNormal((self.v.r.size / 3))\n    if (not hasattr(self, 'vn')):\n        logger.info('LambertianPointLight using auto-normals. This will be slow for derivative-free computations.')\n        self.vn = VertNormals(f=self.f, v=self.v)\n        self.vn.needs_autoupdate = True\n    if (('v' in which) and hasattr(self.vn, 'needs_autoupdate') and self.vn.needs_autoupdate):\n        self.vn.v = self.v\n    ldn_args = {k: getattr(self, k) for k in which if (k in ('light_pos', 'v', 'vn'))}\n    if (len(ldn_args) > 0):\n        self.ldn.set(**ldn_args)\n        self._lpl.a.a.a = self.ldn.reshape(((- 1), 1))\n    if (('num_verts' in which) or ('light_color' in which)):\n        self._lpl.a.a.b = self.light_color.reshape((1, self.num_channels))\n    if ('vc' in which):\n        self._lpl.a.b = self.vc.reshape(((- 1), self.num_channels))\n", "label": 1}
{"function": "\n\ndef print_help(self):\n    '\\n        Print the help menu.\\n        '\n    print(('\\n  %s %s' % ((self._title or self._name), (self._version or ''))))\n    if self._usage:\n        print(('\\n  %s' % self._usage))\n    else:\n        cmd = self._name\n        if (hasattr(self, '_parent') and isinstance(self._parent, Command)):\n            cmd = ('%s %s' % (self._parent._name, cmd))\n        if self._command_list:\n            usage = ('Usage: %s <command> [option]' % cmd)\n        else:\n            usage = ('Usage: %s [option]' % cmd)\n        pos = ' '.join([('<%s>' % name) for name in self._positional_list])\n        print(('\\n  %s %s' % (usage, pos)))\n    arglen = max((len(o.name) for o in self._option_list))\n    arglen += 2\n    self.print_title('\\n  Options:\\n')\n    for o in self._option_list:\n        print(('    %s %s' % (_pad(o.name, arglen), (o.description or ''))))\n    print('')\n    if self._command_list:\n        self.print_title('  Commands:\\n')\n        for cmd in self._command_list:\n            if isinstance(cmd, Command):\n                name = _pad(cmd._name, arglen)\n                desc = (cmd._description or '')\n                print(('    %s %s' % (_pad(name, arglen), desc)))\n        print('')\n    if self._help_footer:\n        print(self._help_footer)\n        print('')\n    return self\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    fake_author_id = uuid4()\n    project = self.create_project()\n    self.create_build(project)\n    path = '/api/0/authors/{0}/builds/'.format(fake_author_id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 404)\n    data = self.unserialize(resp)\n    assert (len(data) == 0)\n    author = Author(email=self.default_user.email, name='Foo Bar')\n    db.session.add(author)\n    build = self.create_build(project, author=author)\n    path = '/api/0/authors/{0}/builds/'.format(author.id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 401)\n    self.login(self.default_user)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    (username, domain) = self.default_user.email.split('@', 1)\n    author = self.create_author('{}+foo@{}'.format(username, domain))\n    self.create_build(project, author=author)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 2)\n", "label": 1}
{"function": "\n\ndef __unicode__(self):\n    qs = ['SELECT']\n    if self.distinct_fields:\n        if self.count:\n            qs += ['DISTINCT COUNT({0})'.format(', '.join(['\"{0}\"'.format(f) for f in self.distinct_fields]))]\n        else:\n            qs += ['DISTINCT {0}'.format(', '.join(['\"{0}\"'.format(f) for f in self.distinct_fields]))]\n    elif self.count:\n        qs += ['COUNT(*)']\n    else:\n        qs += [(', '.join(['\"{0}\"'.format(f) for f in self.fields]) if self.fields else '*')]\n    qs += ['FROM', self.table]\n    if self.where_clauses:\n        qs += [self._where]\n    if (self.order_by and (not self.count)):\n        qs += ['ORDER BY {0}'.format(', '.join((six.text_type(o) for o in self.order_by)))]\n    if self.limit:\n        qs += ['LIMIT {0}'.format(self.limit)]\n    if self.allow_filtering:\n        qs += ['ALLOW FILTERING']\n    return ' '.join(qs)\n", "label": 1}
{"function": "\n\ndef _get_python_variables(python_exe, variables, imports=['import sys']):\n    'Run a python interpreter and print some variables'\n    program = list(imports)\n    program.append('')\n    for v in variables:\n        program.append(('print(repr(%s))' % v))\n    os_env = dict(os.environ)\n    try:\n        del os_env['MACOSX_DEPLOYMENT_TARGET']\n    except KeyError:\n        pass\n    proc = Utils.pproc.Popen([python_exe, '-c', '\\n'.join(program)], stdout=Utils.pproc.PIPE, env=os_env)\n    output = proc.communicate()[0].split('\\n')\n    if proc.returncode:\n        if Options.options.verbose:\n            warn(('Python program to extract python configuration variables failed:\\n%s' % '\\n'.join([('line %03i: %s' % ((lineno + 1), line)) for (lineno, line) in enumerate(program)])))\n        raise RuntimeError\n    return_values = []\n    for s in output:\n        s = s.strip()\n        if (not s):\n            continue\n        if (s == 'None'):\n            return_values.append(None)\n        elif ((s[0] == \"'\") and (s[(- 1)] == \"'\")):\n            return_values.append(s[1:(- 1)])\n        elif s[0].isdigit():\n            return_values.append(int(s))\n        else:\n            break\n    return return_values\n", "label": 1}
{"function": "\n\ndef _run_server_as_subprocess():\n    args = sys.argv[2:]\n    (options, remainder) = getopt.getopt(args, '', ['debug', 'reload', 'host=', 'port=', 'server=', 'baseurl=', 'runtimepath='])\n    host = 'localhost'\n    port = 8080\n    debug = True\n    reloader = False\n    server = 'kokoro'\n    runtime_path = '.runtime/'\n    base_url = '/'\n    for (opt, arg) in options:\n        if (opt[0:2] == '--'):\n            opt = opt[2:]\n        if (opt == 'debug'):\n            debug = True\n        elif (opt == 'reload'):\n            reloader = True\n        elif (opt == 'host'):\n            host = arg\n        elif (opt == 'port'):\n            port = arg\n        elif (opt == 'server'):\n            server = arg\n        elif (opt == 'runtimepath'):\n            runtime_path = arg\n        elif (opt == 'baseurl'):\n            base_url = arg\n    SCRIPT_PATH = os.path.abspath(__file__)\n    RUN_COMMAND = ('%s %s' % (sys.executable, SCRIPT_PATH))\n    ARGUMENTS = ('run_server_once --host=%s --port=%d --server=%s --baseurl=%s --runtimepath=%s' % (host, port, server, base_url, runtime_path))\n    if reloader:\n        ARGUMENTS += ' --reload'\n    if debug:\n        ARGUMENTS += ' --debug'\n    RUN_COMMAND = ((RUN_COMMAND + ' ') + ARGUMENTS)\n    if hasattr(os, 'setsid'):\n        return subprocess.Popen(RUN_COMMAND, shell=True, preexec_fn=os.setsid)\n    else:\n        return subprocess.Popen(RUN_COMMAND, shell=True)\n", "label": 1}
{"function": "\n\ndef _parse_binary(stream, ptr=0):\n    i = ptr\n    deserialized = {\n        \n    }\n    while (i < len(stream)):\n        c = stream[i]\n        if (c == BIN_END):\n            return (deserialized, i)\n        (nodename, i) = _readtonull(stream, (i + 1))\n        if (c == BIN_NONE):\n            (deserialized[nodename], i) = _parse_binary(stream, (i + 1))\n        elif (c == BIN_STRING):\n            (deserialized[nodename], i) = _readtonull(stream, (i + 1))\n        elif (c == BIN_WIDESTRING):\n            raise Exception('NYI')\n        elif ((c == BIN_INT32) or (c == BIN_COLOR) or (c == BIN_POINTER)):\n            if ((len(stream) - i) < 4):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('i', stream, (i + 1))\n            (deserialized[nodename], i) = (value, (i + 4))\n        elif (c == BIN_UINT64):\n            if ((len(stream) - i) < 8):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('q', stream, (i + 1))\n            (deserialized[nodename], i) = (value, (i + 8))\n        elif (c == BIN_FLOAT32):\n            if ((len(stream) - i) < 4):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('f', stream, (i + 1))\n            (deserialized[nodename], i) = (0, (i + 4))\n        else:\n            raise Exception('Unknown KV type')\n        i += 1\n    return (deserialized, i)\n", "label": 1}
{"function": "\n\n@memoize_generator\ndef process(self, stack, stream):\n    for (token_type, value) in stream:\n        if ((token_type in Name) and (value.upper() == 'INCLUDE')):\n            self.detected = True\n            continue\n        elif self.detected:\n            if (token_type in Whitespace):\n                continue\n            if (token_type in String.Symbol):\n                path = join(self.dirpath, value[1:(- 1)])\n                try:\n                    f = open(path)\n                    raw_sql = f.read()\n                    f.close()\n                except IOError as err:\n                    if self.raiseexceptions:\n                        raise\n                    (yield (Comment, ('-- IOError: %s\\n' % err)))\n                else:\n                    try:\n                        filtr = IncludeStatement(self.dirpath, (self.maxRecursive - 1), self.raiseexceptions)\n                    except ValueError as err:\n                        if self.raiseexceptions:\n                            raise\n                        (yield (Comment, ('-- ValueError: %s\\n' % err)))\n                    stack = FilterStack()\n                    stack.preprocess.append(filtr)\n                    for tv in stack.run(raw_sql):\n                        (yield tv)\n                self.detected = False\n            continue\n        (yield (token_type, value))\n", "label": 1}
{"function": "\n\ndef update(self):\n    'Get the latest state of the sensor.'\n    data = ecobee.NETWORK\n    data.update()\n    for sensor in data.ecobee.get_remote_sensors(self.index):\n        for item in sensor['capability']:\n            if ((item['type'] == self.type) and (self.type == 'temperature') and (self.sensor_name == sensor['name'])):\n                self._state = (float(item['value']) / 10)\n            elif ((item['type'] == self.type) and (self.type == 'humidity') and (self.sensor_name == sensor['name'])):\n                self._state = item['value']\n            elif ((item['type'] == self.type) and (self.type == 'occupancy') and (self.sensor_name == sensor['name'])):\n                self._state = item['value']\n", "label": 1}
{"function": "\n\ndef create_slug(self, model_instance, add):\n    if (not isinstance(self._populate_from, (list, tuple))):\n        self._populate_from = (self._populate_from,)\n    slug_field = model_instance._meta.get_field(self.attname)\n    if (add or self.overwrite):\n        slug_for_field = (lambda field: self.slugify_func(getattr(model_instance, field)))\n        slug = self.separator.join(map(slug_for_field, self._populate_from))\n        next = 2\n    else:\n        slug = getattr(model_instance, self.attname)\n        return slug\n    slug_len = slug_field.max_length\n    if slug_len:\n        slug = slug[:slug_len]\n    slug = self._slug_strip(slug)\n    original_slug = slug\n    if self.allow_duplicates:\n        return slug\n    queryset = self.get_queryset(model_instance.__class__, slug_field)\n    if model_instance.pk:\n        queryset = queryset.exclude(pk=model_instance.pk)\n    kwargs = {\n        \n    }\n    for params in model_instance._meta.unique_together:\n        if (self.attname in params):\n            for param in params:\n                kwargs[param] = getattr(model_instance, param, None)\n    kwargs[self.attname] = slug\n    while ((not slug) or queryset.filter(**kwargs)):\n        slug = original_slug\n        end = ('%s%s' % (self.separator, next))\n        end_len = len(end)\n        if (slug_len and ((len(slug) + end_len) > slug_len)):\n            slug = slug[:(slug_len - end_len)]\n            slug = self._slug_strip(slug)\n        slug = ('%s%s' % (slug, end))\n        kwargs[self.attname] = slug\n        next += 1\n    return slug\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\n    'run controller method - old style'\n    if (not args):\n        args = (arg or '')\n    if dt:\n        if (not dn):\n            dn = dt\n        doc = frappe.get_doc(dt, dn)\n    else:\n        doc = frappe.get_doc(json.loads(docs))\n        doc._original_modified = doc.modified\n        doc.check_if_latest()\n    if (not doc.has_permission('read')):\n        frappe.msgprint(_('Not permitted'), raise_exception=True)\n    if doc:\n        try:\n            args = json.loads(args)\n        except ValueError:\n            args = args\n        (fnargs, varargs, varkw, defaults) = inspect.getargspec(getattr(doc, method))\n        if ((not fnargs) or ((len(fnargs) == 1) and (fnargs[0] == 'self'))):\n            r = doc.run_method(method)\n        elif (('args' in fnargs) or (not isinstance(args, dict))):\n            r = doc.run_method(method, args)\n        else:\n            r = doc.run_method(method, **args)\n        if r:\n            if cint(frappe.form_dict.get('as_csv')):\n                make_csv_output(r, doc.doctype)\n            else:\n                frappe.response['message'] = r\n        frappe.response.docs.append(doc)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_class_or_file_name_ != x.has_class_or_file_name_):\n        return 0\n    if (self.has_class_or_file_name_ and (self.class_or_file_name_ != x.class_or_file_name_)):\n        return 0\n    if (self.has_line_number_ != x.has_line_number_):\n        return 0\n    if (self.has_line_number_ and (self.line_number_ != x.line_number_)):\n        return 0\n    if (self.has_function_name_ != x.has_function_name_):\n        return 0\n    if (self.has_function_name_ and (self.function_name_ != x.function_name_)):\n        return 0\n    if (len(self.variables_) != len(x.variables_)):\n        return 0\n    for (e1, e2) in zip(self.variables_, x.variables_):\n        if (e1 != e2):\n            return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef on_response(self, response):\n    if (not self.first_server_chunk_received):\n        self.first_server_chunk_received = True\n        if ((not self.first_client_chunk_received) and (response.startswith('220 ') or response.startswith('220-'))):\n            self.smtp_detected = True\n    if (not self.smtp_detected):\n        return response\n    if self.ehlo_response_pending:\n        self.ehlo_response_pending = False\n        if (not response.startswith('250-')):\n            return response\n        lines = [l.rstrip() for l in response.splitlines()]\n        starttls_line_index = (- 1)\n        for i in range(len(lines)):\n            line = lines[i]\n            if line[4:].lower().startswith('starttls'):\n                starttls_line_index = i\n                break\n        else:\n            self.smtp_detected = False\n            self.log(logging.DEBUG, 'No STARTTLS in EHLO response')\n            return response\n        if (starttls_line_index == (len(lines) - 1)):\n            lines = lines[:starttls_line_index]\n            lines[(- 1)] = ((lines[(- 1)][0:3] + ' ') + lines[(- 1)][4:])\n        else:\n            lines = (lines[:starttls_line_index] + lines[(starttls_line_index + 1):])\n        response = ('\\r\\n'.join(lines) + '\\r\\n')\n        self.server_starttls_stripped = True\n        self.log(logging.DEBUG, 'Stripped STARTTLS from EHLO response')\n    return response\n", "label": 1}
{"function": "\n\ndef ParseDepends(self, filename, must_exist=None, only_one=0):\n    '\\n        Parse a mkdep-style file for explicit dependencies.  This is\\n        completely abusable, and should be unnecessary in the \"normal\"\\n        case of proper SCons configuration, but it may help make\\n        the transition from a Make hierarchy easier for some people\\n        to swallow.  It can also be genuinely useful when using a tool\\n        that can write a .d file, but for which writing a scanner would\\n        be too complicated.\\n        '\n    filename = self.subst(filename)\n    try:\n        fp = open(filename, 'r')\n    except IOError:\n        if must_exist:\n            raise\n        return\n    lines = SCons.Util.LogicalLines(fp).readlines()\n    lines = [l for l in lines if (l[0] != '#')]\n    tdlist = []\n    for line in lines:\n        try:\n            (target, depends) = line.split(':', 1)\n        except (AttributeError, ValueError):\n            pass\n        else:\n            tdlist.append((target.split(), depends.split()))\n    if only_one:\n        targets = []\n        for td in tdlist:\n            targets.extend(td[0])\n        if (len(targets) > 1):\n            raise SCons.Errors.UserError((\"More than one dependency target found in `%s':  %s\" % (filename, targets)))\n    for (target, depends) in tdlist:\n        self.Depends(target, depends)\n", "label": 1}
{"function": "\n\ndef root(self, request, url):\n    '\\n        DEPRECATED. This function is the old way of handling URL resolution, and\\n        is deprecated in favor of real URL resolution -- see ``get_urls()``.\\n\\n        This function still exists for backwards-compatibility; it will be\\n        removed in Django 1.3.\\n        '\n    import warnings\n    warnings.warn('AdminSite.root() is deprecated; use include(admin.site.urls) instead.', DeprecationWarning)\n    if ((request.method == 'GET') and (not request.path.endswith('/'))):\n        return http.HttpResponseRedirect((request.path + '/'))\n    if settings.DEBUG:\n        self.check_dependencies()\n    self.root_path = re.sub((re.escape(url) + '$'), '', request.path)\n    url = url.rstrip('/')\n    if (url == 'logout'):\n        return self.logout(request)\n    if (not self.has_permission(request)):\n        return self.login(request)\n    if (url == ''):\n        return self.index(request)\n    elif (url == 'password_change'):\n        return self.password_change(request)\n    elif (url == 'password_change/done'):\n        return self.password_change_done(request)\n    elif (url == 'jsi18n'):\n        return self.i18n_javascript(request)\n    elif url.startswith('r/'):\n        from django.contrib.contenttypes.views import shortcut\n        return shortcut(request, *url.split('/')[1:])\n    elif ('/' in url):\n        return self.model_page(request, *url.split('/', 2))\n    else:\n        return self.app_index(request, url)\n    raise http.Http404('The requested admin page does not exist.')\n", "label": 1}
{"function": "\n\ndef _select_range(self, multiselect, keep_anchor, node, idx):\n    'Selects a range between self._anchor and node or idx.\\n        If multiselect is True, it will be added to the selection, otherwise\\n        it will unselect everything before selecting the range. This is only\\n        called if self.multiselect is True.\\n        If keep anchor is False, the anchor is moved to node. This should\\n        always be True for keyboard selection.\\n        '\n    select = self.select_node\n    sister_nodes = self.get_selectable_nodes()\n    end = (len(sister_nodes) - 1)\n    last_node = self._anchor\n    last_idx = self._anchor_idx\n    if (last_node is None):\n        last_idx = end\n        last_node = sister_nodes[end]\n    elif ((last_idx > end) or (sister_nodes[last_idx] != last_node)):\n        try:\n            last_idx = self.get_index_of_node(last_node, sister_nodes)\n        except ValueError:\n            return\n    if ((idx > end) or (sister_nodes[idx] != node)):\n        try:\n            idx = self.get_index_of_node(node, sister_nodes)\n        except ValueError:\n            return\n    if (last_idx > idx):\n        (last_idx, idx) = (idx, last_idx)\n    if (not multiselect):\n        self.clear_selection()\n    for item in sister_nodes[last_idx:(idx + 1)]:\n        select(item)\n    if keep_anchor:\n        self._anchor = last_node\n        self._anchor_idx = last_idx\n    else:\n        self._anchor = node\n        self._anchor_idx = idx\n    self._last_selected_node = node\n    self._last_node_idx = idx\n", "label": 1}
{"function": "\n\ndef build(ctx):\n    if ctx.options.dump_state:\n        ctx.db.dump_database()\n        return 0\n    if ctx.options.delete_function:\n        if (not ctx.db.delete_function(ctx.options.delete_function)):\n            raise fbuild.Error(('function %r not cached' % ctx.options.delete_function))\n        return 0\n    if ctx.options.delete_file:\n        if (not ctx.db.delete_file(ctx.options.delete_file)):\n            raise fbuild.Error(('file %r not cached' % ctx.options.delete_file))\n        return 0\n    targets = (ctx.args or ['build'])\n    if ('install' in targets):\n        if (targets[(- 1)] != 'install'):\n            raise fbuild.Error('install must be last target')\n        if (not (set(targets) - {'configure', 'install'})):\n            targets.insert((targets.index('install') - 1), 'build')\n    for target_name in targets:\n        if (target_name == 'install'):\n            install_files(ctx)\n        else:\n            target = fbuild.target.find(target_name)\n            target.function(ctx)\n    return 0\n", "label": 1}
{"function": "\n\ndef formfield_for_dbfield(self, db_field, request, **kwargs):\n    \"\\n        Hook for specifying the form Field instance for a given database Field\\n        instance.\\n\\n        If kwargs are given, they're passed to the form Field's constructor.\\n        \"\n    if db_field.choices:\n        return self.formfield_for_choice_field(db_field, request, **kwargs)\n    if (db_field.many_to_many or isinstance(db_field, models.ForeignKey)):\n        if (db_field.__class__ in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[db_field.__class__], **kwargs)\n        if isinstance(db_field, models.ForeignKey):\n            formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)\n        elif db_field.many_to_many:\n            formfield = self.formfield_for_manytomany(db_field, request, **kwargs)\n        if (formfield and (db_field.name not in self.raw_id_fields)):\n            related_modeladmin = self.admin_site._registry.get(db_field.remote_field.model)\n            wrapper_kwargs = {\n                \n            }\n            if related_modeladmin:\n                wrapper_kwargs.update(can_add_related=related_modeladmin.has_add_permission(request), can_change_related=related_modeladmin.has_change_permission(request), can_delete_related=related_modeladmin.has_delete_permission(request))\n            formfield.widget = widgets.RelatedFieldWidgetWrapper(formfield.widget, db_field.remote_field, self.admin_site, **wrapper_kwargs)\n        return formfield\n    for klass in db_field.__class__.mro():\n        if (klass in self.formfield_overrides):\n            kwargs = dict(copy.deepcopy(self.formfield_overrides[klass]), **kwargs)\n            return db_field.formfield(**kwargs)\n    return db_field.formfield(**kwargs)\n", "label": 1}
{"function": "\n\ndef _build_http_request(url, method, headers=None, encoding=None, params=empty_params):\n    '\\n    Make an HTTP request and return an HTTP response.\\n    '\n    opts = {\n        'headers': (headers or {\n            \n        }),\n    }\n    if params.query:\n        opts['params'] = params.query\n    if ((params.body is not None) or params.data or params.files):\n        if (encoding == 'application/json'):\n            if (params.body is not None):\n                opts['json'] = params.body\n            else:\n                opts['json'] = params.data\n        elif (encoding == 'multipart/form-data'):\n            opts['data'] = params.data\n            opts['files'] = params.files\n        elif (encoding == 'application/x-www-form-urlencoded'):\n            opts['data'] = params.data\n        elif (encoding == 'application/octet-stream'):\n            opts['data'] = params.body\n            content_type = _get_content_type(params.body)\n            if content_type:\n                opts['headers']['content-type'] = content_type\n    request = requests.Request(method, url, **opts)\n    request = request.prepare()\n    return request\n", "label": 1}
{"function": "\n\n@classmethod\ndef target_info(cls, obj, ansi=False):\n    if isinstance(obj, type):\n        return ''\n    targets = obj.traverse(cls.get_target)\n    (elements, containers) = zip(*targets)\n    element_set = set((el for el in elements if (el is not None)))\n    container_set = set((c for c in containers if (c is not None)))\n    element_info = None\n    if (len(element_set) == 1):\n        element_info = ('Element: %s' % list(element_set)[0])\n    elif (len(element_set) > 1):\n        element_info = ('Elements:\\n   %s' % '\\n   '.join(sorted(element_set)))\n    container_info = None\n    if (len(container_set) == 1):\n        container_info = ('Container: %s' % list(container_set)[0])\n    elif (len(container_set) > 1):\n        container_info = ('Containers:\\n   %s' % '\\n   '.join(sorted(container_set)))\n    heading = cls.heading('Target Specifications', ansi=ansi, char='-')\n    target_header = '\\nTargets in this object available for customization:\\n'\n    if (element_info and container_info):\n        target_info = ('%s\\n\\n%s' % (element_info, container_info))\n    else:\n        target_info = (element_info if element_info else container_info)\n    target_footer = '\\nTo see the options info for one of these target specifications,\\nwhich are of the form {type}[.{group}[.{label}]], do holoviews.help({type}).'\n    return '\\n'.join([heading, target_header, target_info, target_footer])\n", "label": 1}
{"function": "\n\n@property\ndef type(self):\n    path = self.short_path\n    if any((path.startswith(prefix) for prefix in DOCKER_PREFIXES)):\n        return 'docker'\n    elif path.startswith('/lxc/'):\n        return 'lxc'\n    elif path.startswith('/user.slice/'):\n        (_, parent, name) = path.rsplit('/', 2)\n        if parent.endswith('.scope'):\n            if os.path.isdir(('/home/%s/.local/share/lxc/%s' % (self.owner, name))):\n                return 'lxc-user'\n        return 'systemd'\n    elif ((path == '/user.slice') or (path == '/system.slice') or path.startswith('/system.slice/')):\n        return 'systemd'\n    elif (regexp_ovz_container.match(path) and (path != '/0') and HAS_OPENVZ):\n        return 'openvz'\n    else:\n        return '-'\n", "label": 1}
{"function": "\n\ndef __init__(self, parser, path, source_resource=None):\n    self.parser = parser\n    self.path = path\n    self.source_resource = source_resource\n    self.entities = OrderedDict()\n    self.escape_quotes_on = (('mobile/android/base' in path) and (parser is DTDParser))\n    if source_resource:\n        for (key, entity) in source_resource.entities.items():\n            self.entities[key] = copy_source_entity(entity)\n    try:\n        self.structure = parser.get_structure(read_file(path, uncomment_moz_langpack=((parser is IncParser) and (not source_resource))))\n    except IOError:\n        if source_resource:\n            return\n        else:\n            raise\n    comments = []\n    current_order = 0\n    for obj in self.structure:\n        if isinstance(obj, silme.core.entity.Entity):\n            if self.escape_quotes_on:\n                obj.value = self.unescape_quotes(obj.value)\n            entity = SilmeEntity(obj, comments, current_order)\n            self.entities[entity.key] = entity\n            current_order += 1\n            comments = []\n        elif isinstance(obj, silme.core.structure.Comment):\n            for comment in obj:\n                lines = unicode(comment).strip().split('\\n')\n                comments += [line.strip() for line in lines]\n", "label": 1}
{"function": "\n\ndef process_handler_result(self, result, task=None):\n    '\\n        Process result received from the task handler.\\n\\n        Result could be:\\n        * None\\n        * Task instance\\n        * Data instance.\\n        '\n    if isinstance(result, Task):\n        self.add_task(result)\n    elif isinstance(result, Data):\n        handler = self.find_data_handler(result)\n        try:\n            data_result = handler(**result.storage)\n            if (data_result is None):\n                pass\n            else:\n                for something in data_result:\n                    self.process_handler_result(something, task)\n        except Exception as ex:\n            self.process_handler_error(('data_%s' % result.handler_key), ex, task)\n    elif (result is None):\n        pass\n    elif isinstance(result, Exception):\n        handler = self.find_task_handler(task)\n        handler_name = getattr(handler, '__name__', 'NONE')\n        self.process_handler_error(handler_name, result, task)\n    elif isinstance(result, dict):\n        if (result.get('type') == 'stat'):\n            for (name, count) in result['counters'].items():\n                self.stat.inc(name, count)\n            for (name, items) in result['collections'].items():\n                for item in items:\n                    self.stat.collect(name, item)\n        else:\n            raise SpiderError(('Unknown result type: %s' % result))\n    else:\n        raise SpiderError(('Unknown result type: %s' % result))\n", "label": 1}
{"function": "\n\ndef _load_00(b, classes):\n    identifier = b[0]\n    if isinstance(identifier, str):\n        identifier = ord(identifier)\n    if (identifier == _SPEC):\n        return _load_spec(b)\n    elif (identifier == _INT_32):\n        return _load_int_32(b)\n    elif ((identifier == _INT) or (identifier == _INT_NEG)):\n        return _load_int(b)\n    elif (identifier == _FLOAT):\n        return _load_float(b)\n    elif (identifier == _COMPLEX):\n        return _load_complex(b)\n    elif (identifier == _STR):\n        return _load_str(b)\n    elif (identifier == _BYTES):\n        return _load_bytes(b)\n    elif (identifier == _TUPLE):\n        return _load_tuple(b, classes)\n    elif (identifier == _NAMEDTUPLE):\n        return _load_namedtuple_00(b, classes)\n    elif (identifier == _LIST):\n        return _load_list(b, classes)\n    elif (identifier == _NPARRAY):\n        return _load_np_array(b)\n    elif (identifier == _DICT):\n        return _load_dict(b, classes)\n    elif (identifier == _GETSTATE):\n        return _load_getstate(b, classes)\n    else:\n        raise BFLoadError(\"unknown identifier '{}'\".format(hex(identifier)))\n", "label": 1}
{"function": "\n\ndef dropMimeData(self, mime_data, action, row, column, parent):\n    ' Reimplemented to allow items to be moved.\\n        '\n    if (action == QtCore.Qt.IgnoreAction):\n        return False\n    data = mime_data.data(tabular_mime_type)\n    if ((not data.isNull()) and (action == QtCore.Qt.MoveAction)):\n        id_and_rows = map(int, str(data).split(' '))\n        table_id = id_and_rows[0]\n        if (table_id == id(self)):\n            current_rows = id_and_rows[1:]\n            self.moveRows(current_rows, parent.row())\n            return True\n    data = PyMimeData.coerce(mime_data).instance()\n    if (data is not None):\n        if (not isinstance(data, list)):\n            data = [data]\n        editor = self._editor\n        object = editor.object\n        name = editor.name\n        adapter = editor.adapter\n        if ((row == (- 1)) and parent.isValid()):\n            row = parent.row()\n        if ((row == (- 1)) and (adapter.len(object, name) == 0)):\n            row = 0\n        if all((adapter.get_can_drop(object, name, row, item) for item in data)):\n            for item in reversed(data):\n                self.dropItem(item, row)\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef show_list(self, connection, app_names=None):\n    '\\n        Shows a list of all migrations on the system, or only those of\\n        some named apps.\\n        '\n    loader = MigrationLoader(connection, ignore_no_migrations=True)\n    graph = loader.graph\n    if app_names:\n        invalid_apps = []\n        for app_name in app_names:\n            if (app_name not in loader.migrated_apps):\n                invalid_apps.append(app_name)\n        if invalid_apps:\n            raise CommandError(('No migrations present for: %s' % ', '.join(invalid_apps)))\n    else:\n        app_names = sorted(loader.migrated_apps)\n    for app_name in app_names:\n        self.stdout.write(app_name, self.style.MIGRATE_LABEL)\n        shown = set()\n        for node in graph.leaf_nodes(app_name):\n            for plan_node in graph.forwards_plan(node):\n                if ((plan_node not in shown) and (plan_node[0] == app_name)):\n                    title = plan_node[1]\n                    if graph.nodes[plan_node].replaces:\n                        title += (' (%s squashed migrations)' % len(graph.nodes[plan_node].replaces))\n                    if (plan_node in loader.applied_migrations):\n                        self.stdout.write((' [X] %s' % title))\n                    else:\n                        self.stdout.write((' [ ] %s' % title))\n                    shown.add(plan_node)\n        if (not shown):\n            self.stdout.write(' (no migrations)', self.style.ERROR)\n", "label": 1}
{"function": "\n\ndef finddirs(pattern, path='.', exclude=None, recursive=True):\n    'Find directories that match *pattern* in *path*'\n    import fnmatch\n    import os\n    if recursive:\n        for (root, dirnames, filenames) in os.walk(path):\n            for pat in _to_list(pattern):\n                for dirname in fnmatch.filter(dirnames, pat):\n                    dirpath = join(abspath(root), dirname)\n                    for excl in _to_list(exclude):\n                        if (excl and fnmatch.fnmatch(dirpath, excl)):\n                            break\n                    else:\n                        (yield dirpath)\n    else:\n        for pat in _to_list(pattern):\n            for dirname in fnmatch.filter(listdirs(path), pat):\n                dirpath = join(abspath(path), dirname)\n                for excl in _to_list(exclude):\n                    if (excl and fnmatch.fnmatch(dirpath, excl)):\n                        break\n                else:\n                    (yield dirpath)\n", "label": 1}
{"function": "\n\ndef request(self, method, url, data=None, headers=None):\n    if (data and isinstance(data, bytes)):\n        data = data.decode()\n    if (data and (not isinstance(data, basestring))):\n        data = urlencode(data)\n    if (data is not None):\n        data = data.encode('utf8')\n    if (data and (not headers.get('Content-Type', None))):\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    request_headers = self.headers.copy()\n    if headers:\n        for (k, v) in headers.items():\n            if (v is None):\n                del request_headers[k]\n            else:\n                request_headers[k] = v\n    try:\n        func = getattr(requests, method.lower())\n    except AttributeError:\n        raise Exception(\"HTTP method '{}' is not supported\".format(method))\n    response = func(url, data=data, headers=request_headers)\n    if (response.status_code > 399):\n        raise HTTPError(response.status_code, '{}: {}'.format(response.status_code, response.content))\n    return response\n", "label": 1}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (new_r not in self.shape_of):\n        self.init_r(new_r)\n    self.update_shape(new_r, r)\n    for (shpnode, idx) in (r.clients + [(node, i)]):\n        if isinstance(getattr(shpnode, 'op', None), Shape_i):\n            idx = shpnode.op.i\n            repl = self.shape_of[new_r][idx]\n            if (repl.owner is shpnode):\n                continue\n            if (repl.owner and (repl.owner.inputs[0] is shpnode.inputs[0]) and isinstance(repl.owner.op, Shape_i) and (repl.owner.op.i == shpnode.op.i)):\n                continue\n            if (shpnode.outputs[0] in theano.gof.graph.ancestors([repl])):\n                raise InconsistencyError(('This substitution would insert a cycle in the graph:node: %s, i: %i, r: %s, new_r: %s' % (node, i, r, new_r)))\n            self.scheduled[shpnode] = new_r\n    unscheduled = [k for (k, v) in self.scheduled.items() if (v == r)]\n    for k in unscheduled:\n        del self.scheduled[k]\n    for v in self.shape_of_reverse_index.get(r, []):\n        for (ii, svi) in enumerate(self.shape_of.get(v, [])):\n            if (svi == r):\n                self.set_shape_i(v, ii, new_r)\n    self.shape_of_reverse_index[r] = set()\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Starts or resumes the generator, running until it reaches a\\n        yield point that is not ready.\\n        '\n    if (self.running or self.finished):\n        return\n    try:\n        self.running = True\n        while True:\n            future = self.future\n            if (not future.done()):\n                return\n            self.future = None\n            try:\n                orig_stack_contexts = stack_context._state.contexts\n                exc_info = None\n                try:\n                    value = future.result()\n                except Exception:\n                    self.had_exception = True\n                    exc_info = sys.exc_info()\n                if (exc_info is not None):\n                    yielded = self.gen.throw(*exc_info)\n                    exc_info = None\n                else:\n                    yielded = self.gen.send(value)\n                if (stack_context._state.contexts is not orig_stack_contexts):\n                    self.gen.throw(stack_context.StackContextInconsistentError('stack_context inconsistency (probably caused by yield within a \"with StackContext\" block)'))\n            except (StopIteration, Return) as e:\n                self.finished = True\n                self.future = _null_future\n                if (self.pending_callbacks and (not self.had_exception)):\n                    raise LeakedCallbackError(('finished without waiting for callbacks %r' % self.pending_callbacks))\n                self.result_future.set_result(_value_from_stopiteration(e))\n                self.result_future = None\n                self._deactivate_stack_context()\n                return\n            except Exception:\n                self.finished = True\n                self.future = _null_future\n                self.result_future.set_exc_info(sys.exc_info())\n                self.result_future = None\n                self._deactivate_stack_context()\n                return\n            if (not self.handle_yield(yielded)):\n                return\n    finally:\n        self.running = False\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw_args):\n    args = [sympify(x) for x in args if x]\n    args = TensAdd._tensAdd_flatten(args)\n    if (not args):\n        return S.Zero\n    if ((len(args) == 1) and (not isinstance(args[0], TensExpr))):\n        return args[0]\n    args = TensAdd._tensAdd_check_automatrix(args)\n    TensAdd._tensAdd_check(args)\n    if ((len(args) == 1) and isinstance(args[0], TensMul)):\n        obj = Basic.__new__(cls, *args, **kw_args)\n        return obj\n    args = [canon_bp(x) for x in args if x]\n    args = [x for x in args if x]\n    if (not args):\n        return S.Zero\n    if (len(args) == 1):\n        return args[0]\n\n    def sort_key(t):\n        x = get_tids(t)\n        return (x.components, x.free, x.dum)\n    args.sort(key=sort_key)\n    args = TensAdd._tensAdd_collect_terms(args)\n    if (not args):\n        return S.Zero\n    if (len(args) == 1):\n        return args[0]\n    obj = Basic.__new__(cls, *args, **kw_args)\n    return obj\n", "label": 1}
{"function": "\n\ndef _process(self):\n    '\\n        Coroutine implementing the key match algorithm. Key strokes are sent\\n        into this generator, and it calls the appropriate handlers.\\n        '\n    buffer = []\n    retry = False\n    while True:\n        if retry:\n            retry = False\n        else:\n            buffer.append((yield))\n        if buffer:\n            is_prefix_of_longer_match = self._is_prefix_of_longer_match(buffer)\n            matches = self._get_matches(buffer)\n            if (matches and matches[(- 1)].eager(self._cli_ref())):\n                is_prefix_of_longer_match = False\n            if ((not is_prefix_of_longer_match) and matches):\n                self._call_handler(matches[(- 1)], key_sequence=buffer)\n                buffer = []\n            elif ((not is_prefix_of_longer_match) and (not matches)):\n                retry = True\n                found = False\n                for i in range(len(buffer), 0, (- 1)):\n                    matches = self._get_matches(buffer[:i])\n                    if matches:\n                        self._call_handler(matches[(- 1)], key_sequence=buffer[:i])\n                        buffer = buffer[i:]\n                        found = True\n                if (not found):\n                    buffer = buffer[1:]\n", "label": 1}
{"function": "\n\ndef test_read_job3():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test.test', 'test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef handle_noargs(self, **options):\n    if ((not options['watch']) and (not options['initial_scan'])):\n        sys.exit('--no-initial-scan option should be used with --watch.')\n    scanned_dirs = get_scanned_dirs()\n    verbosity = int(options['verbosity'])\n    compilers = utils.get_compilers().values()\n    if ((not options['watch']) or options['initial_scan']):\n        for scanned_dir in scanned_dirs:\n            for (dirname, dirnames, filenames) in os.walk(scanned_dir):\n                for filename in filenames:\n                    path = os.path.join(dirname, filename)[len(scanned_dir):]\n                    if path.startswith('/'):\n                        path = path[1:]\n                    for compiler in compilers:\n                        if compiler.is_supported(path):\n                            try:\n                                compiler.handle_changed_file(path, verbosity=options['verbosity'])\n                            except (exceptions.StaticCompilationError, ValueError) as e:\n                                print(e)\n                            break\n    if options['watch']:\n        from static_precompiler.watch import watch_dirs\n        watch_dirs(scanned_dirs, verbosity)\n", "label": 1}
{"function": "\n\ndef acquire(self, timeout=None):\n    timeout = (((timeout is not None) and timeout) or self.timeout)\n    end_time = time.time()\n    if ((timeout is not None) and (timeout > 0)):\n        end_time += timeout\n    while True:\n        try:\n            os.symlink(self.unique_name, self.lock_file)\n        except OSError:\n            if self.i_am_locking():\n                return\n            else:\n                if ((timeout is not None) and (time.time() > end_time)):\n                    if (timeout > 0):\n                        raise LockTimeout(('Timeout waiting to acquire lock for %s' % self.path))\n                    else:\n                        raise AlreadyLocked(('%s is already locked' % self.path))\n                time.sleep(((timeout / 10) if (timeout is not None) else 0.1))\n        else:\n            return\n", "label": 1}
{"function": "\n\ndef _pick_drop_channels(self, idx):\n    from ..io.base import _BaseRaw\n    from ..epochs import _BaseEpochs\n    from ..evoked import Evoked\n    from ..time_frequency import AverageTFR\n    if isinstance(self, (_BaseRaw, _BaseEpochs)):\n        if (not self.preload):\n            raise RuntimeError('If Raw or Epochs, data must be preloaded to drop or pick channels')\n\n    def inst_has(attr):\n        return (getattr(self, attr, None) is not None)\n    if inst_has('picks'):\n        self.picks = self.picks[idx]\n    if inst_has('_cals'):\n        self._cals = self._cals[idx]\n    pick_info(self.info, idx, copy=False)\n    if inst_has('_projector'):\n        self._projector = self._projector[idx][:, idx]\n    if (isinstance(self, _BaseRaw) and inst_has('_data')):\n        self._data = self._data.take(idx, axis=0)\n    elif (isinstance(self, _BaseEpochs) and inst_has('_data')):\n        self._data = self._data.take(idx, axis=1)\n    elif (isinstance(self, AverageTFR) and inst_has('data')):\n        self.data = self.data.take(idx, axis=0)\n    elif isinstance(self, Evoked):\n        self.data = self.data.take(idx, axis=0)\n", "label": 1}
{"function": "\n\n@local_optimizer([GpuFromHost, GpuToGpu, host_from_gpu])\ndef local_cut_gpu_transfers(node):\n    if (isinstance(node.op, GpuFromHost) and node.inputs[0].owner and isinstance(node.inputs[0].owner.op, HostFromGpu)):\n        other = node.inputs[0].owner.inputs[0]\n        if (node.op.context_name == other.type.context_name):\n            return [other]\n        else:\n            return [GpuToGpu(node.op.context_name)(other)]\n    elif (isinstance(node.op, HostFromGpu) and node.inputs[0].owner):\n        n2 = node.inputs[0].owner\n        if isinstance(n2.op, GpuFromHost):\n            return [n2.inputs[0]]\n        if isinstance(n2.op, GpuToGpu):\n            return [host_from_gpu(n2.inputs[0])]\n    elif isinstance(node.op, GpuToGpu):\n        if (node.inputs[0].type.context_name == node.op.context_name):\n            return [node.inputs[0]]\n        if node.inputs[0].owner:\n            n2 = node.inputs[0].owner\n            if isinstance(n2.op, GpuFromHost):\n                return [GpuFromHost(node.op.context_name)(n2.inputs[0])]\n            if isinstance(n2.op, GpuToGpu):\n                if (node.op.context_name == n2.inputs[0].type.context_name):\n                    return [n2.inputs[0]]\n                else:\n                    return [node.op(n2.inputs[0])]\n", "label": 1}
{"function": "\n\n@classmethod\ndef _resolve_conflict(cls, existing, proposed):\n    if (existing.rev is None):\n        return proposed\n    if (proposed.rev is None):\n        return existing\n    if (proposed == existing):\n        if proposed.force:\n            return proposed\n        return existing\n    elif (existing.force and proposed.force):\n        raise cls.IvyResolveConflictingDepsError('Cannot force {}#{};{} to both rev {} and {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n    elif existing.force:\n        logger.debug('Ignoring rev {} for {}#{};{} already forced to {}'.format(proposed.rev, proposed.org, proposed.name, (proposed.classifier or ''), existing.rev))\n        return existing\n    elif proposed.force:\n        logger.debug('Forcing {}#{};{} from {} to {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n        return proposed\n    elif (Revision.lenient(proposed.rev) > Revision.lenient(existing.rev)):\n        logger.debug('Upgrading {}#{};{} from rev {}  to {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n        return proposed\n    else:\n        return existing\n", "label": 1}
{"function": "\n\ndef to_field_allowed(self, request, to_field):\n    '\\n        Returns True if the model associated with this admin should be\\n        allowed to be referenced by the specified field.\\n        '\n    opts = self.model._meta\n    try:\n        field = opts.get_field(to_field)\n    except FieldDoesNotExist:\n        return False\n    if field.primary_key:\n        return True\n    for many_to_many in opts.many_to_many:\n        if (many_to_many.m2m_target_field_name() == to_field):\n            return True\n    registered_models = set()\n    for (model, admin) in self.admin_site._registry.items():\n        registered_models.add(model)\n        for inline in admin.inlines:\n            registered_models.add(inline.model)\n    related_objects = (f for f in opts.get_fields(include_hidden=True) if (f.auto_created and (not f.concrete)))\n    for related_object in related_objects:\n        related_model = related_object.related_model\n        remote_field = related_object.field.remote_field\n        if (any((issubclass(model, related_model) for model in registered_models)) and hasattr(remote_field, 'get_related_field') and (remote_field.get_related_field() == field)):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    cleaver = Cleaver(environ, self._identity, self._backend, count_humans_only=self.count_humans_only)\n    environ[self.environ_key] = cleaver\n    if self.allow_override:\n        self._handle_variant_overrides(environ)\n    if (self.count_humans_only and (environ.get('REQUEST_METHOD', '') == 'POST') and (self.human_callback_token in environ.get('PATH_INFO', ''))):\n        (fp, length) = SplitMiddleware._copy_body_to_tempfile(environ)\n        environ.setdefault('CONTENT_LENGTH', length)\n        fs = cgi.FieldStorage(fp=fp, environ=environ, keep_blank_values=True)\n        try:\n            try:\n                x = int(fs.getlist('x')[0])\n            except (IndexError, ValueError):\n                x = 0\n            try:\n                y = int(fs.getlist('y')[0])\n            except (IndexError, ValueError):\n                y = 0\n            try:\n                z = int(fs.getlist('z')[0])\n            except (IndexError, ValueError):\n                z = 0\n            if (x and y and z and ((x + y) == z)):\n                self._backend.mark_human(cleaver.identity)\n                for e in self._backend.all_experiments():\n                    variant = self._backend.get_variant(cleaver.identity, e.name)\n                    if variant:\n                        self._backend.mark_participant(e.name, variant)\n                start_response('204 No Content', [('Content-Type', 'text/plain')])\n                return []\n        except (KeyError, ValueError):\n            pass\n        start_response('401 Unauthorized', [('Content-Type', 'text/plain')])\n        return []\n    return self.app(environ, start_response)\n", "label": 1}
{"function": "\n\ndef confirmation(self, client, pdu):\n    if _debug:\n        UDPMultiplexer._debug('confirmation %r %r', client, pdu)\n    if (pdu.pduSource == self.addrTuple):\n        if _debug:\n            UDPMultiplexer._debug('    - from us!')\n        return\n    src = Address(pdu.pduSource)\n    if (client is self.direct):\n        dest = self.address\n    elif (client is self.broadcast):\n        dest = LocalBroadcast()\n    else:\n        raise RuntimeError('confirmation mismatch')\n    if (not pdu.pduData):\n        if _debug:\n            UDPMultiplexer._debug('    - no data')\n        return\n    msg_type = struct.unpack('B', pdu.pduData[:1])[0]\n    if _debug:\n        UDPMultiplexer._debug('    - msg_type: %r', msg_type)\n    if (msg_type == 1):\n        if self.annexH.serverPeer:\n            self.annexH.response(PDU(pdu, source=src, destination=dest))\n    elif (msg_type == 129):\n        if self.annexJ.serverPeer:\n            self.annexJ.response(PDU(pdu, source=src, destination=dest))\n    else:\n        UDPMultiplexer._warning('unsupported message')\n", "label": 1}
{"function": "\n\ndef test_m2o_lazy_loader_on_persistent(self):\n    'Compare the behaviors from the lazyloader using\\n        the \"committed\" state in all cases, vs. the lazyloader\\n        using the \"current\" state in all cases except during flush.\\n\\n        '\n    for loadfk in (True, False):\n        for loadrel in (True, False):\n            for autoflush in (True, False):\n                for manualflush in (True, False):\n                    for fake_autoexpire in (True, False):\n                        sess.autoflush = autoflush\n                        if loadfk:\n                            c1.parent_id\n                        if loadrel:\n                            c1.parent\n                        c1.parent_id = p2.id\n                        if manualflush:\n                            sess.flush()\n                        if fake_autoexpire:\n                            sess.expire(c1, ['parent'])\n                        if (loadrel and (not fake_autoexpire)):\n                            assert (c1.parent is p1)\n                        else:\n                            assert (c1.parent is p2)\n                        sess.rollback()\n", "label": 1}
{"function": "\n\ndef _get_svn_url_rev(self, location):\n    from pip.exceptions import InstallationError\n    with open(os.path.join(location, self.dirname, 'entries')) as f:\n        data = f.read()\n    if (data.startswith('8') or data.startswith('9') or data.startswith('10')):\n        data = list(map(str.splitlines, data.split('\\n\\x0c\\n')))\n        del data[0][0]\n        url = data[0][3]\n        revs = ([int(d[9]) for d in data if ((len(d) > 9) and d[9])] + [0])\n    elif data.startswith('<?xml'):\n        match = _svn_xml_url_re.search(data)\n        if (not match):\n            raise ValueError(('Badly formatted data: %r' % data))\n        url = match.group(1)\n        revs = ([int(m.group(1)) for m in _svn_rev_re.finditer(data)] + [0])\n    else:\n        try:\n            xml = self.run_command(['info', '--xml', location], show_stdout=False)\n            url = _svn_info_xml_url_re.search(xml).group(1)\n            revs = [int(m.group(1)) for m in _svn_info_xml_rev_re.finditer(xml)]\n        except InstallationError:\n            (url, revs) = (None, [])\n    if revs:\n        rev = max(revs)\n    else:\n        rev = 0\n    return (url, rev)\n", "label": 1}
{"function": "\n\ndef test_proxy_snake_dict():\n    my_data = {\n        'one': 1,\n        'two': 2,\n        'none': None,\n        'threeOrFor': 3,\n        'inside': {\n            'otherCamelCase': 3,\n        },\n    }\n    p = ProxySnakeDict(my_data)\n    assert ('one' in p)\n    assert ('two' in p)\n    assert ('threeOrFor' in p)\n    assert ('none' in p)\n    assert (len(p) == len(my_data))\n    assert (p['none'] is None)\n    assert (p.get('none') is None)\n    assert (p.get('none_existent') is None)\n    assert ('three_or_for' in p)\n    assert (p.get('three_or_for') == 3)\n    assert ('inside' in p)\n    assert ('other_camel_case' in p['inside'])\n    assert (sorted(p.items()) == sorted(list([('inside', ProxySnakeDict({\n        'other_camel_case': 3,\n    })), ('none', None), ('three_or_for', 3), ('two', 2), ('one', 1)])))\n", "label": 1}
{"function": "\n\n@gof.local_optimizer([T.pow])\ndef local_pow_specialize(node):\n    if (node.op == T.pow):\n        odtype = node.outputs[0].dtype\n        xsym = node.inputs[0]\n        ysym = node.inputs[1]\n        y = local_mul_canonizer.get_constant(ysym)\n        if ((y is not None) and encompasses_broadcastable(xsym.type.broadcastable, ysym.type.broadcastable)):\n            rval = None\n            if N.all((y == 2)):\n                rval = [T.sqr(xsym)]\n            if N.all((y == 1)):\n                rval = [xsym]\n            if N.all((y == 0)):\n                rval = [T.fill(xsym, numpy.asarray(1, dtype=odtype))]\n            if N.all((y == 0.5)):\n                rval = [T.sqrt(xsym)]\n            if N.all((y == (- 0.5))):\n                rval = [T.inv(T.sqrt(xsym))]\n            if N.all((y == (- 1))):\n                rval = [T.inv(xsym)]\n            if N.all((y == (- 2))):\n                rval = [T.inv(T.sqr(xsym))]\n            if rval:\n                rval[0] = T.cast(rval[0], odtype)\n                assert (rval[0].type == node.outputs[0].type), (rval, node.outputs)\n                return rval\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.username = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.Pass = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.Remember = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef mpf_atan2(y, x, prec, rnd=round_fast):\n    (xsign, xman, xexp, xbc) = x\n    (ysign, yman, yexp, ybc) = y\n    if (not yman):\n        if ((y == fzero) and (x != fnan)):\n            if (mpf_sign(x) >= 0):\n                return fzero\n            return mpf_pi(prec, rnd)\n        if (y in (finf, fninf)):\n            if (x in (finf, fninf)):\n                return fnan\n            if (y == finf):\n                return mpf_shift(mpf_pi(prec, rnd), (- 1))\n            return mpf_neg(mpf_shift(mpf_pi(prec, negative_rnd[rnd]), (- 1)))\n        return fnan\n    if ysign:\n        return mpf_neg(mpf_atan2(mpf_neg(y), x, prec, negative_rnd[rnd]))\n    if (not xman):\n        if (x == fnan):\n            return fnan\n        if (x == finf):\n            return fzero\n        if (x == fninf):\n            return mpf_pi(prec, rnd)\n        if (y == fzero):\n            return fzero\n        return mpf_shift(mpf_pi(prec, rnd), (- 1))\n    tquo = mpf_atan(mpf_div(y, x, (prec + 4)), (prec + 4))\n    if xsign:\n        return mpf_add(mpf_pi((prec + 4)), tquo, prec, rnd)\n    else:\n        return mpf_pos(tquo, prec, rnd)\n", "label": 1}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_time():\n    ' reading/writing of 3D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_3d', 'time_3d.sec'))\n    assert (data.shape == (128, 88, 1250))\n    assert (np.abs((data[(0, 1, 2)].real - 7.98)) <= 0.01)\n    assert (np.abs((data[(0, 1, 2)].imag - 33.82)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].real - (- 9.36))) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].imag - (- 7.75))) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef __restore__(self):\n    super(Turtle, self).__restore__()\n    try:\n        [panda.setdefault(self.CREATOR, self.creator) for panda in self.pandas]\n        [panda.setdefault(self.NAME, self.name) for panda in self.pandas]\n        self.tigress.setdefault(self.CREATOR, self.creator)\n        self.tigress.setdefault(self.NAME, self.name)\n    except:\n        pass\n    self.pandas = crane.pandaStore.load_or_create_all(self.pandas)\n    self.tigress = crane.tigressStore.load_or_create(self.tigress)\n    self.pandaUids = set((p.uid for p in self.pandas))\n    self.invertedMapping = {tuple(v): k for (k, v) in self.mapping.iteritems()}\n    self.followers = set(self.followers)\n    if (self.FREQUIRES_UIDS in self.requires):\n        uids = self.requires[self.FREQUIRES_UIDS]\n        if isinstance(uids, basestring):\n            uids = eval(uids)\n        [panda.add_features(uids) for panda in self.pandas]\n    elif (self.FREQUIRES_TURTLES in self.requires):\n        turtles = self.store.load_or_create_all([{\n            'name': t,\n            'creator': self.creator,\n        } for t in self.requires[self.FREQUIRES_TURTLES]])\n        [panda.add_features(turtle.get_panda_uids()) for turtle in turtles for panda in self.pandas]\n    elif self.requires:\n        logger.error('dependent features are either in {0} or {1}, but not in {2}'.format(self.FREQUIRES_UIDS, self.FREQUIRES_TURTLES, self.requires))\n", "label": 1}
{"function": "\n\ndef test_has_multiple():\n    f = (((x ** 2) * y) + sin(((2 ** t) + log(z))))\n    assert f.has(x)\n    assert f.has(y)\n    assert f.has(z)\n    assert f.has(t)\n    assert (not f.has(u))\n    assert f.has(x, y, z, t)\n    assert f.has(x, y, z, t, u)\n    i = Integer(4400)\n    assert (not i.has(x))\n    assert (i * (x ** i)).has(x)\n    assert (not (i * (y ** i)).has(x))\n    assert (i * (y ** i)).has(x, y)\n    assert (not (i * (y ** i)).has(x, z))\n", "label": 1}
{"function": "\n\ndef run(self, cmd, code):\n    'Run the module checker or executable on code and return the output.'\n    if (self.module is not None):\n        use_module = False\n        if (not self.check_version):\n            use_module = True\n        else:\n            settings = self.get_view_settings()\n            version = settings.get('@python')\n            if (version is None):\n                use_module = ((cmd is None) or (cmd[0] == '<builtin>'))\n            else:\n                version = util.find_python(version=version, module=self.module)\n                use_module = (version[0] == '<builtin>')\n        if use_module:\n            if persist.debug_mode():\n                persist.printf('{}: {} <builtin>'.format(self.name, os.path.basename((self.filename or '<unsaved>'))))\n            try:\n                errors = self.check(code, os.path.basename((self.filename or '<unsaved>')))\n            except Exception as err:\n                persist.printf('ERROR: exception in {}.check: {}'.format(self.name, str(err)))\n                errors = ''\n            if isinstance(errors, (tuple, list)):\n                return '\\n'.join([str(e) for e in errors])\n            else:\n                return errors\n        else:\n            cmd = self._cmd\n    else:\n        cmd = (self.cmd or self._cmd)\n    cmd = self.build_cmd(cmd=cmd)\n    if cmd:\n        return super().run(cmd, code)\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef visit_bindparam(self, bindparam, within_columns_clause=False, literal_binds=False, skip_bind_expression=False, **kwargs):\n    if ((not skip_bind_expression) and bindparam.type._has_bind_expression):\n        bind_expression = bindparam.type.bind_expression(bindparam)\n        return self.process(bind_expression, skip_bind_expression=True)\n    if (literal_binds or (within_columns_clause and self.ansi_bind_rules)):\n        if ((bindparam.value is None) and (bindparam.callable is None)):\n            raise exc.CompileError((\"Bind parameter '%s' without a renderable value not allowed here.\" % bindparam.key))\n        return self.render_literal_bindparam(bindparam, within_columns_clause=True, **kwargs)\n    name = self._truncate_bindparam(bindparam)\n    if (name in self.binds):\n        existing = self.binds[name]\n        if (existing is not bindparam):\n            if ((existing.unique or bindparam.unique) and (not existing.proxy_set.intersection(bindparam.proxy_set))):\n                raise exc.CompileError((\"Bind parameter '%s' conflicts with unique bind parameter of the same name\" % bindparam.key))\n            elif (existing._is_crud or bindparam._is_crud):\n                raise exc.CompileError((\"bindparam() name '%s' is reserved for automatic usage in the VALUES or SET clause of this insert/update statement.   Please use a name other than column name when using bindparam() with insert() or update() (for example, 'b_%s').\" % (bindparam.key, bindparam.key)))\n    self.binds[bindparam.key] = self.binds[name] = bindparam\n    return self.bindparam_string(name, **kwargs)\n", "label": 1}
{"function": "\n\ndef handle_elt(self, elt, context):\n    titles = elt.findall('bibl/title')\n    title = []\n    if titles:\n        title = '\\n'.join((title.text.strip() for title in titles))\n    authors = elt.findall('bibl/author')\n    author = []\n    if authors:\n        author = '\\n'.join((author.text.strip() for author in authors))\n    dates = elt.findall('bibl/date')\n    date = []\n    if dates:\n        date = '\\n'.join((date.text.strip() for date in dates))\n    publishers = elt.findall('bibl/publisher')\n    publisher = []\n    if publishers:\n        publisher = '\\n'.join((publisher.text.strip() for publisher in publishers))\n    idnos = elt.findall('bibl/idno')\n    idno = []\n    if idnos:\n        idno = '\\n'.join((idno.text.strip() for idno in idnos))\n    notes = elt.findall('bibl/note')\n    note = []\n    if notes:\n        note = '\\n'.join((note.text.strip() for note in notes))\n    return {\n        'title': title,\n        'author': author,\n        'date': date,\n        'publisher': publisher,\n        'idno': idno,\n        'note': note,\n    }\n", "label": 1}
{"function": "\n\ndef pytest_cmdline_preparse(config, args):\n    if (('PYTEST_VERBOSE' in os.environ) and ('-v' not in args)):\n        args.insert(0, '-v')\n    if (('PYTEST_EXITFIRST' in os.environ) and ('-x' not in args)):\n        args.insert(0, '-x')\n    if (('PYTEST_NOCAPTURE' in os.environ) and ('-s' not in args)):\n        args.insert(0, '-s')\n    if (('PYTEST_TB' in os.environ) and (not any((('--tb' in a) for a in args)))):\n        args.insert(0, ('--tb=' + os.environ['PYTEST_TB']))\n    else:\n        args.insert(0, '--tb=short')\n    if (('PYTEST_NPROCS' in os.environ) and ('-n' not in args)):\n        args.insert(0, ('-n ' + os.environ['PYTEST_NPROCS']))\n    if (('PYTEST_WATCH' in os.environ) and ('-f' not in args)):\n        args.insert(0, '-f')\n    if ('PYTEST_LAZY' in os.environ):\n        args.insert(0, '--lazy')\n    if ('PYTEST_GREEDY' in os.environ):\n        args.insert(0, '--greedy')\n", "label": 1}
{"function": "\n\ndef fix_repeating_arguments(self):\n    'Fix elements that should accumulate/increment values.'\n    either = [list(c.children) for c in self.either.children]\n    for case in either:\n        for e in [c for c in case if (case.count(c) > 1)]:\n            if ((type(e) is Argument) or ((type(e) is Option) and e.argcount)):\n                if (e.value is None):\n                    e.value = []\n                elif (type(e.value) is not list):\n                    e.value = e.value.split()\n            if ((type(e) is Command) or ((type(e) is Option) and (e.argcount == 0))):\n                e.value = 0\n    return self\n", "label": 1}
{"function": "\n\ndef has_header(self, sample):\n    rdr = reader(StringIO(sample), self.sniff(sample))\n    header = next(rdr)\n    columns = len(header)\n    columnTypes = {\n        \n    }\n    for i in range(columns):\n        columnTypes[i] = None\n    checked = 0\n    for row in rdr:\n        if (checked > 20):\n            break\n        checked += 1\n        if (len(row) != columns):\n            continue\n        for col in list(columnTypes.keys()):\n            for thisType in [int, float, complex]:\n                try:\n                    thisType(row[col])\n                    break\n                except (ValueError, OverflowError):\n                    pass\n            else:\n                thisType = len(row[col])\n            if (thisType != columnTypes[col]):\n                if (columnTypes[col] is None):\n                    columnTypes[col] = thisType\n                else:\n                    del columnTypes[col]\n    hasHeader = 0\n    for (col, colType) in columnTypes.items():\n        if (type(colType) == type(0)):\n            if (len(header[col]) != colType):\n                hasHeader += 1\n            else:\n                hasHeader -= 1\n        else:\n            try:\n                colType(header[col])\n            except (ValueError, TypeError):\n                hasHeader += 1\n            else:\n                hasHeader -= 1\n    return (hasHeader > 0)\n", "label": 1}
{"function": "\n\ndef __init__(self, param, cursor, strings_only=False):\n    if (settings.USE_TZ and (isinstance(param, datetime.datetime) and (not isinstance(param, Oracle_datetime)))):\n        if timezone.is_aware(param):\n            warnings.warn(\"The Oracle database adapter received an aware datetime (%s), probably from cursor.execute(). Update your code to pass a naive datetime in the database connection's time zone (UTC by default).\", RemovedInDjango20Warning)\n            param = param.astimezone(timezone.utc).replace(tzinfo=None)\n        param = Oracle_datetime.from_datetime(param)\n    if isinstance(param, datetime.timedelta):\n        param = duration_string(param)\n        if (' ' not in param):\n            param = ('0 ' + param)\n    string_size = 0\n    if (param is True):\n        param = 1\n    elif (param is False):\n        param = 0\n    if hasattr(param, 'bind_parameter'):\n        self.force_bytes = param.bind_parameter(cursor)\n    elif isinstance(param, Database.Binary):\n        self.force_bytes = param\n    else:\n        self.force_bytes = convert_unicode(param, cursor.charset, strings_only)\n        if isinstance(self.force_bytes, six.string_types):\n            string_size = len(force_bytes(param, cursor.charset, strings_only))\n    if hasattr(param, 'input_size'):\n        self.input_size = param.input_size\n    elif (string_size > 4000):\n        self.input_size = Database.CLOB\n    else:\n        self.input_size = None\n", "label": 1}
{"function": "\n\ndef _update_checksum(self, checksum, checksum_keyword='CHECKSUM', datasum_keyword='DATASUM'):\n    \"Update the 'CHECKSUM' and 'DATASUM' keywords in the header (or\\n        keywords with equivalent semantics given by the ``checksum_keyword``\\n        and ``datasum_keyword`` arguments--see for example ``CompImageHDU``\\n        for an example of why this might need to be overridden).\\n        \"\n    modified = (self._header._modified or self._data_loaded)\n    if (checksum == 'remove'):\n        if (checksum_keyword in self._header):\n            del self._header[checksum_keyword]\n        if (datasum_keyword in self._header):\n            del self._header[datasum_keyword]\n    elif (modified or self._new or (checksum and (('CHECKSUM' not in self._header) or ('DATASUM' not in self._header)))):\n        if (checksum == 'datasum'):\n            self.add_datasum(datasum_keyword=datasum_keyword)\n        elif (checksum == 'nonstandard_datasum'):\n            self.add_datasum(blocking='nonstandard', datasum_keyword=datasum_keyword)\n        elif (checksum == 'test'):\n            self.add_datasum(self._datasum_comment, datasum_keyword=datasum_keyword)\n            self.add_checksum(self._checksum_comment, True, checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n        elif (checksum == 'nonstandard'):\n            self.add_checksum(blocking='nonstandard', checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n        elif checksum:\n            self.add_checksum(blocking='standard', checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n", "label": 1}
{"function": "\n\ndef skip(inbuf, ftype):\n    if ((ftype == TType.BOOL) or (ftype == TType.BYTE)):\n        inbuf.read(1)\n    elif (ftype == TType.I16):\n        inbuf.read(2)\n    elif (ftype == TType.I32):\n        inbuf.read(4)\n    elif (ftype == TType.I64):\n        inbuf.read(8)\n    elif (ftype == TType.DOUBLE):\n        inbuf.read(8)\n    elif (ftype == TType.STRING):\n        inbuf.read(unpack_i32(inbuf.read(4)))\n    elif ((ftype == TType.SET) or (ftype == TType.LIST)):\n        (v_type, sz) = read_list_begin(inbuf)\n        for i in range(sz):\n            skip(inbuf, v_type)\n    elif (ftype == TType.MAP):\n        (k_type, v_type, sz) = read_map_begin(inbuf)\n        for i in range(sz):\n            skip(inbuf, k_type)\n            skip(inbuf, v_type)\n    elif (ftype == TType.STRUCT):\n        while True:\n            (f_type, fid) = read_field_begin(inbuf)\n            if (f_type == TType.STOP):\n                break\n            skip(inbuf, f_type)\n", "label": 1}
{"function": "\n\ndef test_source_packages():\n    for ext in ('.tar.gz', '.tar', '.tgz', '.zip', '.tar.bz2'):\n        sl = SourcePackage(('a_p_r-3.1.3' + ext))\n        assert (sl._name == 'a_p_r')\n        assert (sl.name == 'a-p-r')\n        assert (sl.raw_version == '3.1.3')\n        assert (sl.version == parse_version(sl.raw_version))\n        for req in ('a_p_r', 'a_p_r>2', 'a_p_r>3', 'a_p_r>=3.1.3', 'a_p_r==3.1.3', 'a_p_r>3,<3.5'):\n            assert sl.satisfies(req)\n            assert sl.satisfies(Requirement.parse(req))\n        for req in ('foo', 'a_p_r==4.0.0', 'a_p_r>4.0.0', 'a_p_r>3.0.0,<3.0.3', 'a==3.1.3'):\n            assert (not sl.satisfies(req))\n    sl = SourcePackage('python-dateutil-1.5.tar.gz')\n    assert (sl.name == 'python-dateutil')\n    assert (sl.raw_version == '1.5')\n", "label": 1}
{"function": "\n\ndef console_output(self, targets):\n    concrete_targets = set()\n    for target in targets:\n        concrete_target = target.concrete_derived_from\n        concrete_targets.add(concrete_target)\n        if isinstance(concrete_target, ScalaLibrary):\n            concrete_targets.update(concrete_target.java_sources)\n    buildroot = get_buildroot()\n    files = set()\n    output_globs = self.get_options().globs\n    concrete_targets = set([target for target in concrete_targets if (not target.is_synthetic)])\n    for target in concrete_targets:\n        files.add(target.address.build_file.full_path)\n        if (output_globs or target.has_sources()):\n            if output_globs:\n                globs_obj = target.globs_relative_to_buildroot()\n                if globs_obj:\n                    files.update((os.path.join(buildroot, src) for src in globs_obj['globs']))\n            else:\n                files.update((os.path.join(buildroot, src) for src in target.sources_relative_to_buildroot()))\n        if (isinstance(target, JvmApp) and (not output_globs)):\n            files.update(itertools.chain(*[bundle.filemap.keys() for bundle in target.bundles]))\n    return files\n", "label": 1}
{"function": "\n\ndef test_rolling():\n    time = MockedTime()\n    percentile = RollingPercentile(time, 60000, 12, 1000, True)\n    percentile.add_value(1000)\n    percentile.add_value(1000)\n    percentile.add_value(1000)\n    percentile.add_value(2000)\n    assert (percentile.buckets.size == 1)\n    assert (percentile.percentile(50) == 0)\n    time.increment(6000)\n    assert (percentile.buckets.size == 1)\n    assert (percentile.percentile(50) == 1000)\n    assert (percentile.buckets.size == 2)\n    percentile.add_value(1000)\n    percentile.add_value(500)\n    assert (percentile.buckets.size == 2)\n    percentile.add_value(200)\n    percentile.add_value(200)\n    percentile.add_value(1600)\n    percentile.add_value(200)\n    percentile.add_value(1600)\n    percentile.add_value(1600)\n    assert (percentile.percentile(50) == 1000)\n    time.increment(6000)\n    snapshot = PercentileSnapshot(1000, 1000, 1000, 2000, 1000, 500, 200, 200, 1600, 200, 1600, 1600)\n    assert (snapshot.percentile(0.15) == percentile.percentile(0.15))\n    assert (snapshot.percentile(0.5) == percentile.percentile(0.5))\n    assert (snapshot.percentile(0.9) == percentile.percentile(0.9))\n    assert (snapshot.percentile(0.995) == percentile.percentile(0.995))\n    assert (snapshot.mean() == 991)\n", "label": 1}
{"function": "\n\ndef postgresql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None):\n    'PostgreSQL psycopg2 driver  accepts two syntaxes\\n\\n        Plus a string for .pgpass file\\n        '\n    dsn = []\n    if ((dsn_style is None) or (dsn_style == 'all') or (dsn_style == 'keyvalue')):\n        dsnstr = \"host='{0}' dbname='{2}' user='{3}' password='{4}'\"\n        if (dbport is not None):\n            dsnstr += \" port='{1}'\"\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'kwargs')):\n        dsnstr = \"host='{0}', database='{2}', user='{3}', password='{4}'\"\n        if (dbport is not None):\n            dsnstr += \", port='{1}'\"\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'uri')):\n        if (dbport is not None):\n            dsnstr = 'postgresql://{3}:{4}@{0}:{1}/{2}'\n        else:\n            dsnstr = 'postgresql://{3}:{4}@{0}/{2}'\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'pgpass')):\n        if (dbport is not None):\n            dbport = 5432\n        dsn.append('{0}:{1}:{2}:{3}:{4}'.format(dbhost, dbport, dbname, dbuser, dbpass))\n    return dsn\n", "label": 1}
{"function": "\n\ndef add_homology(self, seq, search, id=None, idFormat='%s_%d', autoIncrement=False, maxAnnot=999999, maxLoss=None, sliceInfo=None, **kwargs):\n    'find homology in our seq db and add as annotations'\n    try:\n        if (self.sliceAttrDict['id'] != 0):\n            raise KeyError\n    except KeyError:\n        sliceAttrDict['id'] = 0\n        sliceAttrDict['start'] = 1\n        sliceAttrDict['stop'] = 2\n    if autoIncrement:\n        id = len(self.sliceDB)\n    elif (id is None):\n        id = seq.id\n    if isinstance(search, str):\n        search = getattr(self.seqDB, search)\n    if isinstance(seq, str):\n        seq = Sequence(seq, str(id))\n    al = search(seq, **kwargs)\n    if (maxLoss is not None):\n        kwargs['minAlignSize'] = (len(seq) - maxLoss)\n    hits = al[seq].keys(**kwargs)\n    if (len(hits) > maxAnnot):\n        raise ValueError(('too many hits for %s: %d' % (id, len(hits))))\n    out = []\n    i = 0\n    k = id\n    for ival in hits:\n        if (len(hits) > 1):\n            if autoIncrement:\n                k = len(self.sliceDB)\n            else:\n                k = (idFormat % (id, i))\n            i += 1\n        if (sliceInfo is not None):\n            a = self.new_annotation(k, ((ival.id, ival.start, ival.stop) + sliceInfo))\n        else:\n            a = self.new_annotation(k, (ival.id, ival.start, ival.stop))\n        out.append(a)\n    return out\n", "label": 1}
{"function": "\n\ndef _eval_is_zero(self):\n    if self.function.is_zero:\n        return True\n    got_none = False\n    for l in self.limits:\n        if (len(l) == 3):\n            z = ((l[1] == l[2]) or (l[1] - l[2]).is_zero)\n            if z:\n                return True\n            elif (z is None):\n                got_none = True\n    free = self.function.free_symbols\n    for xab in self.limits:\n        if (len(xab) == 1):\n            free.add(xab[0])\n            continue\n        if ((len(xab) == 2) and (xab[0] not in free)):\n            if xab[1].is_zero:\n                return True\n            elif (xab[1].is_zero is None):\n                got_none = True\n        free.discard(xab[0])\n        for i in xab[1:]:\n            free.update(i.free_symbols)\n    if ((self.function.is_zero is False) and (got_none is False)):\n        return False\n", "label": 1}
{"function": "\n\ndef parse_command_line(args=None):\n    if (args is None):\n        args = sys.argv[1:]\n    usage = __doc__.format(cmd=os.path.basename(sys.argv[0]))\n    options = docopt(usage, args)\n    for opt_name in ('author', 'email', 'description'):\n        options[opt_name] = options.pop(('--' + opt_name))\n    options['command'] = options.pop('<command>')\n    options['project_name'] = options.pop('--project-name')\n    if (not RE_COMMAND_NAME.match(options['command'])):\n        die('command name must match regular expression {!r}', RE_COMMAND_NAME.pattern)\n    for (info, question, default_value) in (('author', 'Your name:  ', None), ('email', 'Your email: ', None), ('description', 'Description: ', None)):\n        if options.get(info, None):\n            continue\n        try:\n            options[info] = ask(question)\n        except EOFError:\n            pass\n        if (not options[info]):\n            if (default_value is None):\n                options[info] = ''\n            else:\n                options[info] = default_value\n    if (options['project_name'] is None):\n        default = 'OpenLMI {command} Script'.format(command=options['command'].capitalize())\n        try:\n            options['project_name'] = ask('Project name [{default}]: '.format(default=default))\n        except EOFError:\n            pass\n        if (not options['project_name']):\n            options['project_name'] = default\n    options = {k: (v.decode('utf-8') if isinstance(v, str) else v) for (k, v) in options.items()}\n    return options\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.settings = TrafficControlSetting()\n                self.settings.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.device = TrafficControlledDevice()\n                self.device.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.timeout = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _create_joins(self, source_polymorphic=False, source_selectable=None, dest_polymorphic=False, dest_selectable=None, of_type=None):\n    if (source_selectable is None):\n        if (source_polymorphic and self.parent.with_polymorphic):\n            source_selectable = self.parent._with_polymorphic_selectable\n    aliased = False\n    if (dest_selectable is None):\n        if (dest_polymorphic and self.mapper.with_polymorphic):\n            dest_selectable = self.mapper._with_polymorphic_selectable\n            aliased = True\n        else:\n            dest_selectable = self.mapper.mapped_table\n        if (self._is_self_referential and (source_selectable is None)):\n            dest_selectable = dest_selectable.alias()\n            aliased = True\n    else:\n        aliased = True\n    dest_mapper = (of_type or self.mapper)\n    single_crit = dest_mapper._single_table_criterion\n    aliased = (aliased or (source_selectable is not None))\n    (primaryjoin, secondaryjoin, secondary, target_adapter, dest_selectable) = self._join_condition.join_targets(source_selectable, dest_selectable, aliased, single_crit)\n    if (source_selectable is None):\n        source_selectable = self.parent.local_table\n    if (dest_selectable is None):\n        dest_selectable = self.mapper.local_table\n    return (primaryjoin, secondaryjoin, source_selectable, dest_selectable, secondary, target_adapter)\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_data_rows(self, ar):\n    if ar.param_values.show_callables:\n\n        def flt(v):\n            return True\n    else:\n\n        def flt(v):\n            if isinstance(v, (types.FunctionType, types.GeneratorType, types.UnboundMethodType, types.UnboundMethodType, types.BuiltinMethodType, types.BuiltinFunctionType)):\n                return False\n            return True\n    o = self.get_inspected(ar.param_values.inspected)\n    if isinstance(o, (list, tuple)):\n        for (i, v) in enumerate(o):\n            k = (('[' + str(i)) + ']')\n            (yield Inspected(o, '', k, v))\n    elif isinstance(o, AttrDict):\n        for (k, v) in list(o.items()):\n            (yield Inspected(o, '.', k, v))\n    elif isinstance(o, dict):\n        for (k, v) in list(o.items()):\n            k = (('[' + repr(k)) + ']')\n            (yield Inspected(o, '', k, v))\n    else:\n        for k in dir(o):\n            if (not k.startswith('__')):\n                if ((not ar.quick_search) or (ar.quick_search.lower() in k.lower())):\n                    v = getattr(o, k)\n                    if flt(v):\n                        (yield Inspected(o, '.', k, v))\n", "label": 1}
{"function": "\n\ndef __init__(self, method, uri, version='HTTP/1.0', headers=None, body=None, remote_ip=None, protocol=None, host=None, files=None, connection=None):\n    self.method = method\n    self.uri = uri\n    self.version = version\n    self.headers = (headers or httputil.HTTPHeaders())\n    self.body = (body or '')\n    self.remote_ip = remote_ip\n    if protocol:\n        self.protocol = protocol\n    elif (connection and isinstance(connection.stream, iostream.SSLIOStream)):\n        self.protocol = 'https'\n    else:\n        self.protocol = 'http'\n    if (connection and connection.xheaders):\n        ip = self.headers.get('X-Forwarded-For', self.remote_ip)\n        ip = ip.split(',')[(- 1)].strip()\n        ip = self.headers.get('X-Real-Ip', ip)\n        if netutil.is_valid_ip(ip):\n            self.remote_ip = ip\n        proto = self.headers.get('X-Scheme', self.headers.get('X-Forwarded-Proto', self.protocol))\n        if (proto in ('http', 'https')):\n            self.protocol = proto\n    self.host = (host or self.headers.get('Host') or '127.0.0.1')\n    self.files = (files or {\n        \n    })\n    self.connection = connection\n    self._start_time = time.time()\n    self._finish_time = None\n    (self.path, sep, self.query) = uri.partition('?')\n    self.arguments = parse_qs_bytes(self.query, keep_blank_values=True)\n", "label": 1}
{"function": "\n\ndef clean(self):\n    type = self.cleaned_data['recordType']\n    if (type in ('4', 'Z')):\n        for field in ADDRESS_FIELDS:\n            self.cleaned_data[field] = None\n    if (type == 'Z'):\n        self.cleaned_data['zipExtensionLow'] = None\n        self.cleaned_data['zipExtensionHigh'] = None\n        low = self.cleaned_data['zipCodeLow']\n        high = self.cleaned_data['zipCodeHigh']\n        if ((high is None) or (low is None)):\n            raise ValidationError(_('Zip-5 records need a high and a low.'))\n    elif (type == '4'):\n        low = self.cleaned_data['zipCodeLow']\n        high = self.cleaned_data['zipCodeHigh']\n        low_ext = self.cleaned_data['zipExtensionLow']\n        high_ext = self.cleaned_data['zipExtensionHigh']\n        if ((high is None) or (low is None) or (low_ext is None) or (high_ext is None)):\n            raise ValidationError(_('Zip+4 records need a high and a low for both parts.'))\n    else:\n        for field in ZIP_FIELDS:\n            self.cleaned_data[field] = None\n        for field in ('lowAddress', 'highAddress', 'streetName', 'cityName', 'zipCode', 'plus4'):\n            if (not self.cleaned_data[field]):\n                raise ValidationError(_('Address rocord needs: low, high, street, city, zip, zip+4'))\n    return super(TaxBoundryForm, self).clean()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef validate_field(value, field, new_record=True):\n    '\\n        Validates a field value against a field metadata dictionary. Note: this\\n        is not yet intended to be a full validation. There are sure to be\\n        missing validation cases. Returns a list of validation errors.\\n        '\n    errors = []\n    if new_record:\n        if ((not field['createable']) and (value is not None)):\n            errors.append('Cannot create this field')\n    elif ((not field['updateable']) and (value is not None)):\n        errors.append('Cannot update this field')\n    if ((value is not None) and field.get('restrictedPicklist')):\n        values = [i['value'] for i in field['picklistValues'] if i['active']]\n        if (value not in values):\n            errors.append('Bad value for restricted picklist field')\n    if (new_record and (value is None) and (not field['nillable']) and (not field['defaultedOnCreate']) and (field['type'] != 'boolean')):\n        errors.append('This field is required')\n    return errors\n", "label": 1}
{"function": "\n\ndef merge_undo(self, undo_item):\n    ' Merges two undo items if possible.\\n        '\n    if (isinstance(undo_item, self.__class__) and (self.object is undo_item.object) and (self.name == undo_item.name) and (self.index == undo_item.index)):\n        added = undo_item.added\n        removed = undo_item.removed\n        if ((len(self.added) == len(added)) and (len(self.removed) == len(removed))):\n            for (i, item) in enumerate(self.added):\n                if (item is not added[i]):\n                    break\n            else:\n                for (i, item) in enumerate(self.removed):\n                    if (item is not removed[i]):\n                        break\n                else:\n                    return True\n    return False\n", "label": 1}
{"function": "\n\ndef find_selected_files(schema, metadata):\n    targets = []\n    paths = [('', p) for p in schema.schema['pages']]\n    while len(paths):\n        (prefix, path) = paths.pop(0)\n        if path.get('questions'):\n            paths = (paths + [('', q) for q in path['questions']])\n        elif path.get('type'):\n            qid = path.get('qid', path.get('id'))\n            if (path['type'] == 'object'):\n                paths = (paths + [('{}.{}.value'.format(prefix, qid), p) for p in path['properties']])\n            elif (path['type'] == 'osf-upload'):\n                targets.append('{}.{}'.format(prefix, qid).lstrip('.'))\n    selected = {\n        \n    }\n    for t in targets:\n        parts = t.split('.')\n        value = metadata.get(parts.pop(0))\n        while (value and len(parts)):\n            value = value.get(parts.pop(0))\n        if value:\n            selected[t] = value\n    return selected\n", "label": 1}
{"function": "\n\ndef to_json_message(self):\n    json_message = {\n        'From': self.__sender,\n        'To': self.__to,\n        'Subject': self.__subject,\n    }\n    if self.__reply_to:\n        json_message['ReplyTo'] = self.__reply_to\n    if self.__cc:\n        json_message['Cc'] = self.__cc\n    if self.__bcc:\n        json_message['Bcc'] = self.__bcc\n    if self.__tag:\n        json_message['Tag'] = self.__tag\n    if self.__html_body:\n        json_message['HtmlBody'] = self.__html_body\n    if self.__text_body:\n        json_message['TextBody'] = self.__text_body\n    if self.__track_opens:\n        json_message['TrackOpens'] = True\n    if (len(self.__custom_headers) > 0):\n        cust_headers = []\n        for (key, value) in self.__custom_headers.items():\n            cust_headers.append({\n                'Name': key,\n                'Value': value,\n            })\n        json_message['Headers'] = cust_headers\n    if (len(self.__attachments) > 0):\n        attachments = []\n        for attachment in self.__attachments:\n            if isinstance(attachment, tuple):\n                attachments.append({\n                    'Name': attachment[0],\n                    'Content': attachment[1],\n                    'ContentType': attachment[2],\n                })\n            elif isinstance(attachment, MIMEBase):\n                attachments.append({\n                    'Name': attachment.get_filename(),\n                    'Content': attachment.get_payload(),\n                    'ContentType': attachment.get_content_type(),\n                })\n        json_message['Attachments'] = attachments\n    return json_message\n", "label": 1}
{"function": "\n\ndef _eval_expand_mul(self, **hints):\n    from sympy import fraction\n    expr = self\n    (n, d) = fraction(expr)\n    if d.is_Mul:\n        (n, d) = [(i._eval_expand_mul(**hints) if i.is_Mul else i) for i in (n, d)]\n        expr = (n / d)\n        if (not expr.is_Mul):\n            return expr\n    (plain, sums, rewrite) = ([], [], False)\n    for factor in expr.args:\n        if factor.is_Add:\n            sums.append(factor)\n            rewrite = True\n        elif factor.is_commutative:\n            plain.append(factor)\n        else:\n            sums.append(Basic(factor))\n    if (not rewrite):\n        return expr\n    else:\n        plain = self.func(*plain)\n        if sums:\n            terms = self.func._expandsums(sums)\n            args = []\n            for term in terms:\n                t = self.func(plain, term)\n                if (t.is_Mul and any((a.is_Add for a in t.args))):\n                    t = t._eval_expand_mul()\n                args.append(t)\n            return Add(*args)\n        else:\n            return plain\n", "label": 1}
{"function": "\n\ndef read(self, size=1):\n    '        Read size bytes from the serial port. If a timeout is set it may\\n        return less characters as requested. With no timeout it will block\\n        until the requested number of bytes is read.\\n        '\n    if (not self.is_open):\n        raise portNotOpenError\n    if ((self._timeout is not None) and (self._timeout != 0)):\n        timeout = (time.time() + self._timeout)\n    else:\n        timeout = None\n    data = bytearray()\n    while ((size > 0) and self.is_open):\n        try:\n            b = self.queue.get(timeout=self._timeout)\n        except queue.Empty:\n            if (self._timeout == 0):\n                break\n        else:\n            if (data is not None):\n                data += b\n                size -= 1\n            else:\n                break\n        if (timeout and (time.time() > timeout)):\n            if self.logger:\n                self.logger.info('read timeout')\n            break\n    return bytes(data)\n", "label": 1}
{"function": "\n\ndef before_after_sort(items):\n    \" Sort a sequence of items with 'before', 'after', and 'id' attributes.\\n        \\n    The sort is topological. If an item does not specify a 'before' or 'after',\\n    it is placed after the preceding item.\\n\\n    If a cycle is found in the dependencies, a warning is logged and the order\\n    of the items is undefined.\\n    \"\n    if (len(items) < 2):\n        return items\n    item_map = dict(((item.id, item) for item in items if item.id))\n    pairs = []\n    prev_item = None\n    for item in items:\n        new_pairs = []\n        if (hasattr(item, 'before') and item.before):\n            (parent, child) = (item, item_map.get(item.before))\n            if child:\n                new_pairs.append((parent, child))\n        if (hasattr(item, 'after') and item.after):\n            (parent, child) = (item_map.get(item.after), item)\n            if parent:\n                new_pairs.append((parent, child))\n        if new_pairs:\n            pairs.extend(new_pairs)\n        else:\n            if prev_item:\n                pairs.append((prev_item, item))\n            prev_item = item\n    (result, has_cycle) = topological_sort(pairs)\n    if has_cycle:\n        logger.warning('Cycle in before/after sort for items %r', items)\n    return result\n", "label": 1}
{"function": "\n\ndef test_goaa_happy_path(self):\n    (parser, opts, files) = self._run_check(['../gristle_file_converter.py', 'census.csv', '-d', ',', '-D', '|'], 'pass')\n    assert (opts.output is None)\n    assert (opts.recdelimiter is None)\n    assert (opts.delimiter == ',')\n    assert (opts.out_delimiter == '|')\n    assert (opts.recdelimiter is None)\n    assert (opts.out_recdelimiter is None)\n    assert (opts.quoting is False)\n    assert (opts.out_quoting is False)\n    assert (opts.quotechar == '\"')\n    assert (opts.hasheader is False)\n    assert (opts.out_hasheader is False)\n    assert (opts.stripfields is False)\n    assert (opts.verbose is True)\n    self._run_check(['../gristle_file_converter.py', 'census4.csv', '-D', '|'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census5.csv', '-d', ',', '-D', '|'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-d', ',', '-D', '|', '--hasheader', '--outhasheader'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-D', '|', '-r', '-R', '-q', '-Q', '--quotechar', '^'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-D', '|', '--stripfields'], 'pass')\n", "label": 1}
{"function": "\n\ndef linkSamplePredicate(subsample_size, predicate, items1, items2):\n    sample = []\n    predicate_function = predicate.func\n    field = predicate.field\n    red = defaultdict(list)\n    blue = defaultdict(list)\n    for (i, (index, record)) in enumerate(interleave(items1, items2)):\n        if (i == 20000):\n            if ((min(len(red), len(blue)) + len(sample)) < 10):\n                return sample\n        column = record[field]\n        if (not column):\n            (red, blue) = (blue, red)\n            continue\n        block_keys = predicate_function(column)\n        for block_key in block_keys:\n            if blue.get(block_key):\n                pair = sort_pair(blue[block_key].pop(), index)\n                sample.append(pair)\n                subsample_size -= 1\n                if subsample_size:\n                    break\n                else:\n                    return sample\n            else:\n                red[block_key].append(index)\n        (red, blue) = (blue, red)\n    for (index, record) in itertools.islice(items2, len(items1)):\n        column = record[field]\n        if (not column):\n            continue\n        block_keys = predicate_function(column)\n        for block_key in block_keys:\n            if red.get(block_key):\n                pair = sort_pair(red[block_key].pop(), index)\n                sample.append(pair)\n                subsample_size -= 1\n                if subsample_size:\n                    break\n                else:\n                    return sample\n    return sample\n", "label": 1}
{"function": "\n\ndef summary(self):\n    tpg = self.rtsnode\n    status = None\n    msg = []\n    if tpg.has_feature('nexus'):\n        msg.append(str(self.rtsnode.nexus))\n    if (not tpg.enable):\n        return ('disabled', False)\n    if tpg.has_feature('acls'):\n        if (('generate_node_acls' in tpg.list_attributes()) and int(tpg.get_attribute('generate_node_acls'))):\n            msg.append('gen-acls')\n        else:\n            msg.append('no-gen-acls')\n        if tpg.has_feature('auth'):\n            if (not int(tpg.get_attribute('authentication'))):\n                msg.append('no-auth')\n                if int(tpg.get_attribute('generate_node_acls')):\n                    status = True\n            elif (not int(tpg.get_attribute('generate_node_acls'))):\n                msg.append('auth per-acl')\n            else:\n                msg.append('tpg-auth')\n                status = True\n                if (not (tpg.chap_password and tpg.chap_userid)):\n                    status = False\n                if tpg.authenticate_target:\n                    msg.append('mutual auth')\n                else:\n                    msg.append('1-way auth')\n    return (', '.join(msg), status)\n", "label": 1}
{"function": "\n\ndef alignment_is_good(record, a, arange):\n    trimmed_slen = (arange.end - arange.start)\n    if (trimmed_slen < 0):\n        sys.exit('BUG: trimmed_slen bad value\\n')\n    qury_wiggle = (record.qlen * WIGGLE_PCT)\n    if (not (fabs((a.send - a.sstart)) > MIN_ALIGNMENT_SIZE)):\n        return False\n    if ((fabs((a.send - a.sstart)) >= (trimmed_slen * CONTAINED_PCT)) and (record.qlen > record.slen)):\n        return True\n    if ((fabs((a.qend - a.qstart)) > (record.qlen * CONTAINED_PCT)) and (record.slen > record.qlen)):\n        return True\n    if (((a.sstart == arange.start) or (arange.end == a.send)) and ((a.qstart < qury_wiggle) or (a.qend < qury_wiggle) or ((record.qlen - a.qstart) < qury_wiggle) or ((record.qlen - a.qend) < qury_wiggle))):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef check_if_doc_is_linked(doc, method='Delete'):\n    '\\n\\t\\tRaises excption if the given doc(dt, dn) is linked in another record.\\n\\t'\n    from frappe.model.rename_doc import get_link_fields\n    link_fields = get_link_fields(doc.doctype)\n    link_fields = [[lf['parent'], lf['fieldname'], lf['issingle']] for lf in link_fields]\n    for (link_dt, link_field, issingle) in link_fields:\n        if (not issingle):\n            item = frappe.db.get_value(link_dt, {\n                link_field: doc.name,\n            }, ['name', 'parent', 'parenttype', 'docstatus'], as_dict=True)\n            if (item and ((item.parent or item.name) != doc.name) and (((method == 'Delete') and (item.docstatus < 2)) or ((method == 'Cancel') and (item.docstatus == 1)))):\n                frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, (item.parenttype if item.parent else link_dt), (item.parent or item.name)), frappe.LinkExistsError)\n", "label": 1}
{"function": "\n\ndef match(self, interp, block, typemap, calltypes):\n    '\\n        Look for potential macros for expand and store their expansions.\\n        '\n    self.block = block\n    self.rewrites = rewrites = {\n        \n    }\n    for inst in block.body:\n        if isinstance(inst, ir.Assign):\n            rhs = inst.value\n            if (isinstance(rhs, ir.Expr) and (rhs.op == 'call') and isinstance(rhs.func, ir.Var)):\n                try:\n                    const = interp.infer_constant(rhs.func)\n                except errors.ConstantInferenceError:\n                    continue\n                if isinstance(const, Macro):\n                    assert const.callable\n                    new_expr = self._expand_callable_macro(interp, rhs, const, rhs.loc)\n                    rewrites[rhs] = new_expr\n            elif (isinstance(rhs, ir.Expr) and (rhs.op == 'getattr')):\n                try:\n                    const = interp.infer_constant(inst.target)\n                except errors.ConstantInferenceError:\n                    continue\n                if (isinstance(const, Macro) and (not const.callable)):\n                    new_expr = self._expand_non_callable_macro(const, rhs.loc)\n                    rewrites[rhs] = new_expr\n    return (len(rewrites) > 0)\n", "label": 1}
{"function": "\n\ndef temp_name(self, expr):\n    c = expr.__class__\n    if (c is PrimCall):\n        return expr.prim.name\n    elif (c is Attribute):\n        if (expr.value.__class__ is Var):\n            return ((names.original(expr.value.name) + '_') + expr.name)\n        else:\n            return expr.name\n    elif (c is Attribute):\n        if (expr.value.__class__ is Var):\n            return ('%s_%s' % (expr.value.name, expr.name))\n        else:\n            return expr.name\n    elif (c is Index):\n        idx_t = expr.index.type\n        if (isinstance(idx_t, SliceT) or (isinstance(idx_t, TupleT) and any((isinstance(elt_t, SliceT) for elt_t in idx_t.elt_types)))):\n            return 'slice'\n        else:\n            return 'elt'\n    elif (c is TupleProj):\n        if (expr.tuple.__class__ is Var):\n            original = names.original(expr.tuple.name)\n            return ('%s%d_elt%d' % (original, names.versions[original], expr.index))\n        else:\n            return ('tuple_elt%d' % expr.index)\n    elif (c is Var):\n        return names.refresh(expr.name)\n    else:\n        return 'temp'\n", "label": 1}
{"function": "\n\ndef on_ssl(self, client_hello):\n    anon_ciphers = [str(c) for c in client_hello.ciphers if ('_anon_' in str(c))]\n    if anon_ciphers:\n        self._handle_bad_ciphers(anon_ciphers, ('Client enabled anonymous TLS/SSL cipher suites %s' % ', '.join(anon_ciphers)))\n    null_ciphers = [str(c) for c in client_hello.ciphers if ('_WITH_NULL_' in str(c))]\n    if null_ciphers:\n        self._handle_bad_ciphers(null_ciphers, ('Client enabled NULL encryption TLS/SSL cipher suites %s' % ', '.join(null_ciphers)))\n    integ_ciphers = [str(c) for c in client_hello.ciphers if str(c).endswith('_NULL')]\n    if integ_ciphers:\n        self._handle_bad_ciphers(integ_ciphers, ('Client enabled NULL integrity TLS/SSL cipher suites %s' % ', '.join(integ_ciphers)))\n    export_ciphers = [str(c) for c in client_hello.ciphers if ('EXPORT' in str(c))]\n    if export_ciphers:\n        self._handle_bad_ciphers(integ_ciphers, ('Client enabled export TLS/SSL cipher suites %s' % ', '.join(export_ciphers)))\n", "label": 1}
{"function": "\n\ndef tostr(object, encoding=None):\n    ' get a unicode safe string representation of an object '\n    if isinstance(object, basestring):\n        if (encoding is None):\n            return object\n        else:\n            return object.encode(encoding)\n    if isinstance(object, tuple):\n        s = ['(']\n        for item in object:\n            if isinstance(item, basestring):\n                s.append(item)\n            else:\n                s.append(tostr(item))\n            s.append(', ')\n        s.append(')')\n        return ''.join(s)\n    if isinstance(object, list):\n        s = ['[']\n        for item in object:\n            if isinstance(item, basestring):\n                s.append(item)\n            else:\n                s.append(tostr(item))\n            s.append(', ')\n        s.append(']')\n        return ''.join(s)\n    if isinstance(object, dict):\n        s = ['{']\n        for item in object.items():\n            if isinstance(item[0], basestring):\n                s.append(item[0])\n            else:\n                s.append(tostr(item[0]))\n            s.append(' = ')\n            if isinstance(item[1], basestring):\n                s.append(item[1])\n            else:\n                s.append(tostr(item[1]))\n            s.append(', ')\n        s.append('}')\n        return ''.join(s)\n    try:\n        return unicode(object)\n    except:\n        return str(object)\n", "label": 1}
{"function": "\n\ndef test_create_alt(scratch_tree, scratch_pad):\n    sess = scratch_tree.edit('/', alt='de')\n    assert (sess.id == '')\n    assert (sess.path == '/')\n    assert (sess.record is not None)\n    assert (sess['_model'] == 'page')\n    assert (sess['title'] == 'Index')\n    assert (sess['body'] == 'Hello World!')\n    sess['body'] = 'Hallo Welt!'\n    sess.commit()\n    assert sess.closed\n    with open(sess.get_fs_path(alt='de')) as f:\n        assert (f.read().splitlines() == ['body: Hallo Welt!'])\n    scratch_pad.cache.flush()\n    item = scratch_pad.get('/', alt='de')\n    assert (item['_slug'] == '')\n    assert (item['title'] == 'Index')\n    assert (item['body'].source == 'Hallo Welt!')\n    assert (item['_model'] == 'page')\n", "label": 1}
{"function": "\n\ndef get_value(self, context, *tag_args, **tag_kwargs):\n    request = self.get_request(context)\n    output = None\n    (slot,) = tag_args\n    template_name = (tag_kwargs.get('template') or None)\n    cachable = is_true(tag_kwargs.get('cachable', (not bool(template_name))))\n    if (template_name and cachable and (not extract_literal(self.kwargs['template']))):\n        raise TemplateSyntaxError(\"{0} tag does not allow 'cachable' for variable template names!\".format(self.tag_name))\n    try_cache = (appsettings.FLUENT_CONTENTS_CACHE_OUTPUT and appsettings.FLUENT_CONTENTS_CACHE_PLACEHOLDER_OUTPUT and cachable)\n    if isinstance(slot, SharedContent):\n        sharedcontent = slot\n        if try_cache:\n            cache_key = get_shared_content_cache_key(sharedcontent)\n            output = cache.get(cache_key)\n    else:\n        site = Site.objects.get_current()\n        if try_cache:\n            cache_key_ptr = get_shared_content_cache_key_ptr(int(site.pk), slot, language_code=get_language())\n            cache_key = cache.get(cache_key_ptr)\n            if (cache_key is not None):\n                output = cache.get(cache_key)\n        if (output is None):\n            try:\n                sharedcontent = SharedContent.objects.parent_site(site).get(slug=slot)\n            except SharedContent.DoesNotExist:\n                return \"<!-- shared content '{0}' does not yet exist -->\".format(slot)\n            if (try_cache and (not cache_key)):\n                cache.set(cache_key_ptr, get_shared_content_cache_key(sharedcontent))\n    if (output is None):\n        output = self.render_shared_content(request, sharedcontent, template_name, cachable=cachable)\n    rendering.register_frontend_media(request, output.media)\n    return output.html\n", "label": 1}
{"function": "\n\ndef __lt__(self, other):\n    if (self.date == 'infinity'):\n        return False\n    if isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return True\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) < other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date < other.date.replace(tzinfo=self.tz))\n        return (self.date < other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            if (other.end.tz and (self.tz is None)):\n                return (self.date.replace(tzinfo=other.end.tz) < other.end.date)\n            elif (self.tz and (other.end.tz is None)):\n                return (self.date < other.end.date.replace(tzinfo=self.tz))\n            return (self.date < other.end.date)\n        else:\n            return self.__lt__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef log(self, sender, message, channel):\n    sender = sender[:10]\n    self.word_table.setdefault(sender, {\n        \n    })\n    if message.startswith('/'):\n        return\n    try:\n        say_something = (self.is_ping(message) or ((sender != self.conn.nick) and (random.random() < self.chattiness)))\n    except AttributeError:\n        say_something = False\n    messages = []\n    seed_key = None\n    if self.is_ping(message):\n        message = self.fix_ping(message)\n    for words in self.split_message(self.sanitize_message(message)):\n        key = tuple(words[:(- 1)])\n        if (key in self.word_table):\n            self.word_table[sender][key].append(words[(- 1)])\n        else:\n            self.word_table[sender][key] = [words[(- 1)]]\n        if ((self.stop_word not in key) and say_something):\n            for person in self.word_table:\n                if (person == sender):\n                    continue\n                if (key in self.word_table[person]):\n                    generated = self.generate_message(person, seed_key=key)\n                    if generated:\n                        messages.append((person, generated))\n    if len(messages):\n        (self.last, message) = random.choice(messages)\n        return message\n", "label": 1}
{"function": "\n\ndef get_line_content(self, patch_content, line_number):\n    content = ''\n    content_list = []\n    lines = patch_content.split('\\n')\n    original_line = 0\n    new_line = 0\n    original_end = 0\n    new_end = 0\n    last_content_added = ''\n    for line in lines:\n        if re.match('^@@(\\\\s|\\\\+|\\\\-|\\\\d|,)+@@', line, re.M):\n            begin = self.get_file_modification_begin(line)\n            original_line = begin[0]\n            new_line = begin[1]\n            end = self.get_file_modification_end(line)\n            deletion_end = end[0]\n            addition_end = end[1]\n        else:\n            if (deletion_end > line_number):\n                if (len(content_list) == 2):\n                    break\n                if ((original_line > line_number) and (new_line > line_number)):\n                    break\n            elif (new_line > line_number):\n                break\n            if re.match('^\\\\+.*', line, re.M):\n                if (new_line == line_number):\n                    content_list.append({\n                        'type': 'addition',\n                        'text': line[1:],\n                    })\n                    last_content_added = 'addition'\n                new_line += 1\n            elif re.match('^\\\\-.*', line, re.M):\n                if (original_line == line_number):\n                    content_list.append({\n                        'type': 'deletion',\n                        'text': line[1:],\n                    })\n                    last_content_added = 'deletion'\n                original_line += 1\n            elif ((line != '\\\\ No newline at end of file') and (line != '')):\n                original_line += 1\n                new_line += 1\n    if content_list:\n        content = content_list[(- 1)]\n    return content\n", "label": 1}
{"function": "\n\ndef execute_plugin_method_series(self, name, args=None, kwargs=None, single_response=False):\n    if (args is None):\n        args = []\n        use_kwargs = True\n    if (kwargs is None):\n        kwargs = {\n            \n        }\n        use_kwargs = False\n    if (use_kwargs and single_response):\n        raise RuntimeError('When executing plugins in series using `single` response mode, you must specify only args.')\n    elif (args and kwargs):\n        raise RuntimeError('Plugins can be ran in series using either args or kwargs, not both.')\n    for plugin in self.plugins:\n        if (not hasattr(plugin, name)):\n            continue\n        method = getattr(plugin, name)\n        plugin_result = method(*args, **kwargs)\n        if (plugin_result is not None):\n            if use_kwargs:\n                kwargs = plugin_result\n            elif single_response:\n                args = (plugin_result,)\n            else:\n                args = plugin_result\n    if use_kwargs:\n        return kwargs\n    elif single_response:\n        return args[0]\n    return args\n", "label": 1}
{"function": "\n\ndef visit_Module(self, node):\n    params = node.get_params()\n    for param in params.values():\n        if (param.width is not None):\n            param.width = self.replace_visitor.visit(param.width)\n        param.value = self.replace_visitor.visit(param.value)\n    localparams = node.get_localparams()\n    for localparam in localparams.values():\n        if (localparam.width is not None):\n            localparam.width = self.replace_visitor.visit(localparam.width)\n        localparam.value = self.replace_visitor.visit(localparam.value)\n    ports = node.get_ports()\n    for port in ports.values():\n        if (port.width is not None):\n            port.width = self.replace_visitor.visit(port.width)\n    vars = node.get_vars()\n    for var in vars.values():\n        if (var.width is not None):\n            var.width = self.replace_visitor.visit(var.width)\n    for asg in node.assign:\n        self.visit(asg)\n    for alw in node.always:\n        self.visit(alw)\n    for ini in node.initial:\n        self.visit(ini)\n    for ins in node.instance.values():\n        self.visit(ins)\n    return node\n", "label": 1}
{"function": "\n\ndef test_unix_domain_adapter_monkeypatch():\n    with UnixSocketServerThread() as usock_thread:\n        with requests_unixsocket.monkeypatch('http+unix://'):\n            urlencoded_usock = requests.compat.quote_plus(usock_thread.usock)\n            url = ('http+unix://%s/path/to/page' % urlencoded_usock)\n            for method in ['get', 'post', 'head', 'patch', 'put', 'delete', 'options']:\n                logger.debug('Calling session.%s(%r) ...', method, url)\n                r = getattr(requests, method)(url)\n                logger.debug('Received response: %r with text: %r and headers: %r', r, r.text, r.headers)\n                assert (r.status_code == 200)\n                assert (r.headers['server'] == 'waitress')\n                assert (r.headers['X-Transport'] == 'unix domain socket')\n                assert (r.headers['X-Requested-Path'] == '/path/to/page')\n                assert (r.headers['X-Socket-Path'] == usock_thread.usock)\n                assert isinstance(r.connection, requests_unixsocket.UnixAdapter)\n                assert (r.url == url)\n                if (method == 'head'):\n                    assert (r.text == '')\n                else:\n                    assert (r.text == 'Hello world!')\n    for method in ['get', 'post', 'head', 'patch', 'put', 'delete', 'options']:\n        with pytest.raises(requests.exceptions.InvalidSchema):\n            getattr(requests, method)(url)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = Results()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = QueryNotFoundException()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.error2 = BeeswaxException()\n                self.error2.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef eval_sum(f, limits):\n    from sympy.concrete.delta import deltasummation, _has_simple_delta\n    from sympy.functions import KroneckerDelta\n    (i, a, b) = limits\n    if (f is S.Zero):\n        return S.Zero\n    if (i not in f.free_symbols):\n        return (f * ((b - a) + 1))\n    if (a == b):\n        return f.subs(i, a)\n    if isinstance(f, Piecewise):\n        if (not any(((i in arg.args[1].free_symbols) for arg in f.args))):\n            newargs = []\n            for arg in f.args:\n                newexpr = eval_sum(arg.expr, limits)\n                if (newexpr is None):\n                    return None\n                newargs.append((newexpr, arg.cond))\n            return f.func(*newargs)\n    if (f.has(KroneckerDelta) and _has_simple_delta(f, limits[0])):\n        return deltasummation(f, limits)\n    dif = (b - a)\n    definite = dif.is_Integer\n    if (definite and (dif < 100)):\n        return eval_sum_direct(f, (i, a, b))\n    if isinstance(f, Piecewise):\n        return None\n    value = eval_sum_symbolic(f.expand(), (i, a, b))\n    if (value is not None):\n        return value\n    if definite:\n        return eval_sum_direct(f, (i, a, b))\n", "label": 1}
{"function": "\n\ndef _ContractAperture(self, force=False):\n    \"Attempt to contract the aperture.  By calling this it's assume the aperture\\n    needs to be contracted.\\n\\n    The aperture can be contracted if it's current size is larger than the\\n    min size.\\n    \"\n    if (self._pending_endpoints and (not force)):\n        return\n    num_healthy = len([c for c in self._heap[1:] if c.channel.is_open])\n    if (num_healthy > self._min_size):\n        least_loaded_endpoint = None\n        for n in self._heap[1:]:\n            if (n.channel.is_closed and (n.endpoint not in self._pending_endpoints)):\n                least_loaded_endpoint = n.endpoint\n                break\n        if (not least_loaded_endpoint):\n            for n in self._heap[1:]:\n                if (n.endpoint not in self._pending_endpoints):\n                    least_loaded_endpoint = n.endpoint\n                    break\n        if least_loaded_endpoint:\n            self._idle_endpoints.add(least_loaded_endpoint)\n            super(ApertureBalancerSink, self)._RemoveSink(least_loaded_endpoint)\n            self._log.debug(('Contracting aperture to remove %s' % str(least_loaded_endpoint)))\n            self._UpdateSizeVarz()\n", "label": 1}
{"function": "\n\ndef _parse_pre_yarn_history_log(lines):\n    'Collect useful info from a pre-YARN history file.\\n\\n    See :py:func:`_parse_yarn_history_log` for return format.\\n    '\n    result = {\n        \n    }\n    task_to_counters = {\n        \n    }\n    for record in _parse_pre_yarn_history_records(lines):\n        fields = record['fields']\n        if ((record['type'] == 'Job') and ('COUNTERS' in fields)):\n            result['counters'] = _parse_pre_yarn_counters(fields['COUNTERS'])\n        elif ((record['type'] == 'Task') and ('COUNTERS' in fields) and ('TASKID' in fields)):\n            task_id = fields['TASKID']\n            counters = _parse_pre_yarn_counters(fields['COUNTERS'])\n            task_to_counters[task_id] = counters\n        elif ((record['type'] in ('MapAttempt', 'ReduceAttempt')) and ('TASK_ATTEMPT_ID' in fields) and (fields.get('TASK_STATUS') == 'FAILED') and fields.get('ERROR')):\n            result.setdefault('errors', [])\n            result['errors'].append(dict(hadoop_error=dict(message=fields['ERROR'], start_line=record['start_line'], num_lines=record['num_lines']), attempt_id=fields['TASK_ATTEMPT_ID']))\n    if (('counters' not in result) and task_to_counters):\n        result['counters'] = _sum_counters(*task_to_counters.values())\n    return result\n", "label": 1}
{"function": "\n\ndef _non_dominated_front_old(iterable, key=(lambda x: x), allowequality=True):\n    'Return a subset of items from iterable which are not dominated by any\\n    other item in iterable.'\n    items = list(iterable)\n    keys = dict(((i, key(i)) for i in items))\n    dim = len(list(keys.values())[0])\n    if any(((dim != len(k)) for k in list(keys.values()))):\n        raise ValueError('Wrong tuple size.')\n    dominations = collections.defaultdict((lambda : []))\n    for i in items:\n        for j in items:\n            if allowequality:\n                if all(((keys[i][k] < keys[j][k]) for k in range(dim))):\n                    dominations[i].append(j)\n            elif all(((keys[i][k] <= keys[j][k]) for k in range(dim))):\n                dominations[i].append(j)\n    dominates = (lambda i, j: (j in dominations[i]))\n    res = set()\n    items = set(items)\n    for i in items:\n        res.add(i)\n        for j in list(res):\n            if (i is j):\n                continue\n            if dominates(j, i):\n                res.remove(i)\n                break\n            elif dominates(i, j):\n                res.remove(j)\n    return res\n", "label": 1}
{"function": "\n\n@dispatch((tuple, list, set, frozenset))\ndef discover(seq):\n    if (not seq):\n        return (var * string)\n    unite = do_one([unite_identical, unite_base, unite_merge_dimensions])\n    if (all((isinstance(item, (tuple, list)) for item in seq)) and (len(set(map(len, seq))) == 1)):\n        columns = list(zip(*seq))\n        try:\n            types = [unite([discover(data) for data in column]).subshape[0] for column in columns]\n            unite = do_one([unite_identical, unite_merge_dimensions, Tuple])\n            return (len(seq) * unite(types))\n        except AttributeError:\n            pass\n    if all((isinstance(item, dict) for item in seq)):\n        keys = sorted(set.union(*(set(d) for d in seq)))\n        columns = [[item.get(key) for item in seq] for key in keys]\n        try:\n            types = [unite([discover(data) for data in column]).subshape[0] for column in columns]\n            return (len(seq) * Record(list(zip(keys, types))))\n        except AttributeError:\n            pass\n    types = list(map(discover, seq))\n    return do_one([unite_identical, unite_merge_dimensions, Tuple])(types)\n", "label": 1}
{"function": "\n\ndef get_refs(genome_build, aligner, config):\n    'Retrieve reference genome data from a standard bcbio directory structure.\\n    '\n    ref_collection = tz.get_in(['arvados', 'reference'], config)\n    if (not ref_collection):\n        raise ValueError('Could not find reference collection in bcbio_system YAML for arvados.')\n    cfiles = collection_files(ref_collection, config['arvados'])\n    ref_prefix = None\n    for prefix in ['./%s', './genomes/%s']:\n        cur_prefix = (prefix % genome_build)\n        if any((x.startswith(cur_prefix) for x in cfiles)):\n            ref_prefix = cur_prefix\n            break\n    assert ref_prefix, ('Did not find genome files for %s:\\n%s' % (genome_build, pprint.pformat(cfiles)))\n    out = {\n        \n    }\n    base_targets = (('/%s.fa' % genome_build), '/mainIndex')\n    for dirname in ['seq', 'rtg', aligner]:\n        key = {\n            'seq': 'fasta',\n        }.get(dirname, dirname)\n        cur_files = [x for x in cfiles if x.startswith(('%s/%s/' % (ref_prefix, dirname)))]\n        cur_files = [('keep:%s' % os.path.normpath(os.path.join(ref_collection, x))) for x in cur_files]\n        base_files = [x for x in cur_files if x.endswith(base_targets)]\n        if (len(base_files) > 0):\n            assert (len(base_files) == 1), base_files\n            base_file = base_files[0]\n            del cur_files[cur_files.index(base_file)]\n            out[key] = {\n                'base': base_file,\n                'indexes': cur_files,\n            }\n        else:\n            out[key] = {\n                'indexes': cur_files,\n            }\n    return out\n", "label": 1}
{"function": "\n\ndef _hash_internal(method, salt, password):\n    'Internal password hash helper.  Supports plaintext without salt,\\n    unsalted and salted passwords.  In case salted passwords are used\\n    hmac is used.\\n    '\n    if (method == 'plain'):\n        return (password, method)\n    if isinstance(password, text_type):\n        password = password.encode('utf-8')\n    if method.startswith('pbkdf2:'):\n        args = method[7:].split(':')\n        if (len(args) not in (1, 2)):\n            raise ValueError('Invalid number of arguments for PBKDF2')\n        method = args.pop(0)\n        iterations = ((args and int((args[0] or 0))) or DEFAULT_PBKDF2_ITERATIONS)\n        is_pbkdf2 = True\n        actual_method = ('pbkdf2:%s:%d' % (method, iterations))\n    else:\n        is_pbkdf2 = False\n        actual_method = method\n    hash_func = _hash_funcs.get(method)\n    if (hash_func is None):\n        raise TypeError(('invalid method %r' % method))\n    if is_pbkdf2:\n        if (not salt):\n            raise ValueError('Salt is required for PBKDF2')\n        rv = pbkdf2_hex(password, salt, iterations, hashfunc=hash_func)\n    elif salt:\n        if isinstance(salt, text_type):\n            salt = salt.encode('utf-8')\n        rv = hmac.HMAC(salt, password, hash_func).hexdigest()\n    else:\n        h = hash_func()\n        h.update(password)\n        rv = h.hexdigest()\n    return (rv, actual_method)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _get_mixins_(bases):\n    'Returns the type for creating enum members, and the first inherited\\n        enum class.\\n\\n        bases: the tuple of bases that was given to __new__\\n\\n        '\n    if ((not bases) or (Enum is None)):\n        return (object, Enum)\n    member_type = first_enum = None\n    for base in bases:\n        if ((base is not Enum) and issubclass(base, Enum) and base._member_names_):\n            raise TypeError('Cannot extend enumerations')\n    if (not issubclass(base, Enum)):\n        raise TypeError('new enumerations must be created as `ClassName([mixin_type,] enum_type)`')\n    if (not issubclass(bases[0], Enum)):\n        member_type = bases[0]\n        first_enum = bases[(- 1)]\n    else:\n        for base in bases[0].__mro__:\n            if issubclass(base, Enum):\n                if (first_enum is None):\n                    first_enum = base\n            elif (member_type is None):\n                member_type = base\n    return (member_type, first_enum)\n", "label": 1}
{"function": "\n\ndef MoveFiles(rebalance, is_master):\n    'Commit the received files into the database.'\n    loc = data_store.DB.Location()\n    if (not os.path.exists(loc)):\n        return False\n    if (not os.path.isdir(loc)):\n        return False\n    tempdir = _CreateDirectory(loc, rebalance.id)\n    remove_file = _FileWithRemoveList(loc, rebalance)\n    to_remove = []\n    if os.path.exists(remove_file):\n        to_remove = [line.decode('utf8').rstrip('\\n') for line in open(remove_file, 'r')]\n    for fname in to_remove:\n        if (not fname.startswith(loc)):\n            logging.warning('Wrong file to remove: %s', fname)\n            continue\n        if (not os.path.exists(fname)):\n            logging.warning('File does not exist: %s', fname)\n            continue\n        if (not os.path.isfile(fname)):\n            logging.warning('Not a file: %s', fname)\n            continue\n        os.unlink(fname)\n        logging.info('Removing file %s', fname)\n    try:\n        os.unlink(remove_file)\n    except OSError:\n        pass\n    try:\n        _RecMoveFiles(tempdir, loc, '')\n    except OSError:\n        return False\n    if (not is_master):\n        if tempdir.startswith(loc):\n            shutil.rmtree(tempdir)\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, settings_module):\n    for setting in dir(global_settings):\n        if (setting == setting.upper()):\n            setattr(self, setting, getattr(global_settings, setting))\n    self.SETTINGS_MODULE = settings_module\n    try:\n        mod = importlib.import_module(self.SETTINGS_MODULE)\n    except ImportError as e:\n        raise ImportError((\"Could not import settings '%s' (Is it on sys.path?): %s\" % (self.SETTINGS_MODULE, e)))\n    tuple_settings = ('INSTALLED_APPS', 'TEMPLATE_DIRS')\n    for setting in dir(mod):\n        if (setting == setting.upper()):\n            setting_value = getattr(mod, setting)\n            if ((setting in tuple_settings) and isinstance(setting_value, six.string_types)):\n                warnings.warn(('The %s setting must be a tuple. Please fix your settings, as auto-correction is now deprecated.' % setting), PendingDeprecationWarning)\n                setting_value = (setting_value,)\n            setattr(self, setting, setting_value)\n    if (not self.SECRET_KEY):\n        raise ImproperlyConfigured('The SECRET_KEY setting must not be empty.')\n    if (hasattr(time, 'tzset') and self.TIME_ZONE):\n        zoneinfo_root = '/usr/share/zoneinfo'\n        if (os.path.exists(zoneinfo_root) and (not os.path.exists(os.path.join(zoneinfo_root, *self.TIME_ZONE.split('/'))))):\n            raise ValueError(('Incorrect timezone setting: %s' % self.TIME_ZONE))\n        os.environ['TZ'] = self.TIME_ZONE\n        time.tzset()\n", "label": 1}
{"function": "\n\ndef _validate_secure_origin(self, logger, location):\n    parsed = urllib_parse.urlparse(str(location))\n    origin = (parsed.scheme, parsed.hostname, parsed.port)\n    for secure_origin in (SECURE_ORIGINS + self.secure_origins):\n        if ((origin[0] != secure_origin[0]) and (secure_origin[0] != '*')):\n            continue\n        try:\n            addr = ipaddress.ip_address((origin[1] if (isinstance(origin[1], six.text_type) or (origin[1] is None)) else origin[1].decode('utf8')))\n            network = ipaddress.ip_network((secure_origin[1] if isinstance(secure_origin[1], six.text_type) else secure_origin[1].decode('utf8')))\n        except ValueError:\n            if ((origin[1] != secure_origin[1]) and (secure_origin[1] != '*')):\n                continue\n        else:\n            if (addr not in network):\n                continue\n        if ((origin[2] != secure_origin[2]) and (secure_origin[2] != '*') and (secure_origin[2] is not None)):\n            continue\n        return True\n    logger.warning(\"The repository located at %s is not a trusted or secure host and is being ignored. If this repository is available via HTTPS it is recommended to use HTTPS instead, otherwise you may silence this warning and allow it anyways with '--trusted-host %s'.\", parsed.hostname, parsed.hostname)\n    return False\n", "label": 1}
{"function": "\n\n@classmethod\ndef wrap(cls, data):\n    should_save = False\n    if ('original_doc' in data):\n        original_doc = data['original_doc']\n        del data['original_doc']\n        should_save = True\n        if original_doc:\n            original_doc = Domain.get_by_name(original_doc)\n            data['copy_history'] = [original_doc._id]\n    if ('license' in data):\n        if (data.get('license', None) == 'public'):\n            data['license'] = 'cc'\n            should_save = True\n    if (('slug' in data) and data['slug']):\n        data['hr_name'] = data['slug']\n        del data['slug']\n    if (('is_test' in data) and isinstance(data['is_test'], bool)):\n        data['is_test'] = ('true' if data['is_test'] else 'false')\n        should_save = True\n    if ('cloudcare_releases' not in data):\n        data['cloudcare_releases'] = 'nostars'\n    if ('location_types' in data):\n        data['obsolete_location_types'] = data.pop('location_types')\n    self = super(Domain, cls).wrap(data)\n    if (self.deployment is None):\n        self.deployment = Deployment()\n    if should_save:\n        self.save()\n    return self\n", "label": 1}
{"function": "\n\ndef find_description(soup):\n    '\\n    '\n    el = soup.find('h3')\n    if (not el):\n        return\n    while (el.name == 'h3'):\n        next_el = el.findNext('h3')\n        if next_el:\n            el = next_el\n        else:\n            break\n    subtitle = el.findNextSibling('b')\n    if subtitle:\n        el = subtitle\n        (yield el.string)\n        el = el.nextSibling.nextSibling\n    el = el.nextSibling\n    while True:\n        if (el.name is not None):\n            return\n        text = el\n        link = el.findNextSibling('a')\n        if link:\n            text += link.string\n            link = link.nextSibling\n            if (link.name is None):\n                text += link.string\n        if text.strip():\n            (yield unicode(text.strip()))\n        for i in range(2):\n            el = el.nextSibling\n            if (not el):\n                return\n            if (el.name != 'br'):\n                break\n        if (el.name == 'br'):\n            el = el.nextSibling\n        if ('Illustrative Examples:' in el):\n            return\n        if ('Cross-References.' in el):\n            return\n", "label": 1}
{"function": "\n\ndef subscribe(self, subscriber, timeout=None):\n    \"Must be used with 'yield', as, for example,\\n        'yield channel.subscribe(coro)'.\\n\\n        Subscribe to receive messages. Senders don't need to\\n        subscribe. A message sent to this channel is delivered to all\\n        subscribers.\\n\\n        Can also be used on remote channels.\\n        \"\n    if ((not isinstance(subscriber, Coro)) and (not isinstance(subscriber, Channel))):\n        logger.warning('invalid subscriber ignored')\n        raise StopIteration((- 1))\n    if (self._location == Channel._asyncoro._location):\n        if (subscriber._location != self._location):\n            if isinstance(subscriber, Coro):\n                subscriber._id = int(subscriber._id)\n                for s in self._subscribers:\n                    if (isinstance(s, Coro) and (s._id == subscriber._id) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n            elif isinstance(subscriber, Channel):\n                for s in self._subscribers:\n                    if (isinstance(s, Channel) and (s._name == subscriber._name) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n        self._subscribers.add(subscriber)\n        (yield self._subscribe_event.set())\n        reply = 0\n    else:\n        kwargs = {\n            'channel': self._name,\n        }\n        kwargs['subscriber'] = subscriber\n        request = _NetRequest('subscribe', kwargs=kwargs, dst=self._location, timeout=timeout)\n        reply = (yield Channel._asyncoro._sync_reply(request))\n    raise StopIteration(reply)\n", "label": 1}
{"function": "\n\ndef _update_defaults(self, defaults):\n    'Updates the given defaults with values from the config files and\\n        the environ. Does a little special handling for certain types of\\n        options (lists).'\n    config = {\n        \n    }\n    for section in ('global', self.name):\n        config.update(self.normalize_keys(self.get_config_section(section)))\n    if (not self.isolated):\n        config.update(self.normalize_keys(self.get_environ_vars()))\n    self.values = optparse.Values(self.defaults)\n    late_eval = set()\n    for (key, val) in config.items():\n        if (not val):\n            continue\n        option = self.get_option(key)\n        if (option is None):\n            continue\n        if (option.action in ('store_true', 'store_false', 'count')):\n            val = strtobool(val)\n        elif (option.action == 'append'):\n            val = val.split()\n            val = [self.check_default(option, key, v) for v in val]\n        elif (option.action == 'callback'):\n            late_eval.add(option.dest)\n            opt_str = option.get_opt_string()\n            val = option.convert_value(opt_str, val)\n            args = (option.callback_args or ())\n            kwargs = (option.callback_kwargs or {\n                \n            })\n            option.callback(option, opt_str, val, self, *args, **kwargs)\n        else:\n            val = self.check_default(option, key, val)\n        defaults[option.dest] = val\n    for key in late_eval:\n        defaults[key] = getattr(self.values, key)\n    self.values = None\n    return defaults\n", "label": 1}
{"function": "\n\ndef _find_candidate_bbs(self, start_address, end_address, mode=BARF_DISASM_RECURSIVE, symbols=None):\n    if (not symbols):\n        symbols = {\n            \n        }\n    bbs = []\n    addrs_to_process = Queue()\n    addrs_processed = set()\n    addrs_to_process.put(start_address)\n    while (not addrs_to_process.empty()):\n        addr_curr = addrs_to_process.get()\n        if ((addr_curr in addrs_processed) or (not ((addr_curr >= start_address) and (addr_curr <= end_address)))):\n            continue\n        bb = self._disassemble_bb(addr_curr, (end_address + 1), symbols)\n        if bb.empty():\n            continue\n        bbs += [bb]\n        addrs_processed.add(addr_curr)\n        if (mode == BARF_DISASM_LINEAR):\n            next_addr = (bb.address + bb.size)\n            if ((not self._bb_ends_in_direct_jmp(bb)) and (not self._bb_ends_in_return(bb)) and (not (next_addr in addrs_processed))):\n                addrs_to_process.put(next_addr)\n        if (mode == BARF_DISASM_RECURSIVE):\n            for (addr, _) in bb.branches:\n                if (not (addr in addrs_processed)):\n                    addrs_to_process.put(addr)\n    return bbs\n", "label": 1}
{"function": "\n\ndef slowloris(self, host, num, timeout, port=None):\n    port = (port or 80)\n    timeout = int(timeout)\n    conns = [Conn(host, int(port), 5) for i in range(int(num))]\n    failed = 0\n    packets = 0\n    while (not self.stop_flag.is_set()):\n        for conn in conns:\n            if self.stop_flag.is_set():\n                break\n            if (not conn.connected):\n                if conn.connect():\n                    packets += 3\n            if conn.connected:\n                query = ('?%d' % random.randint(1, 9999999999999))\n                payload = (self.primary_payload % (query, conn.host))\n                try:\n                    conn.send(payload)\n                    packets += 1\n                except socket.error:\n                    pass\n            else:\n                pass\n        for conn in conns:\n            if self.stop_flag.is_set():\n                break\n            if conn.connected:\n                try:\n                    conn.send('X-a: b\\r\\n')\n                    packets += 1\n                except socket.error:\n                    pass\n        gevent.sleep(timeout)\n    return ('%s failed, %s packets sent' % (failed, packets))\n", "label": 1}
{"function": "\n\ndef process_packets(self, transaction_id=None, invoked_method=None, timeout=None):\n    start = time()\n    while (self.connected and (transaction_id not in self._invoke_results)):\n        if (timeout and ((time() - start) >= timeout)):\n            raise RTMPTimeoutError('Timeout')\n        packet = self.read_packet()\n        if (packet.type == PACKET_TYPE_INVOKE):\n            try:\n                decoded = decode_amf(packet.body)\n            except IOError:\n                continue\n            try:\n                (method, transaction_id_, obj) = decoded[:3]\n                args = decoded[3:]\n            except ValueError:\n                continue\n            if (method == '_result'):\n                if (len(args) > 0):\n                    result = args[0]\n                else:\n                    result = None\n                self._invoke_results[transaction_id_] = result\n            else:\n                handler = self._invoke_handlers.get(method)\n                if handler:\n                    res = handler(*args)\n                    if (res is not None):\n                        self.call('_result', res, transaction_id=transaction_id_)\n                if (method == invoked_method):\n                    self._invoke_args[invoked_method] = args\n                    break\n            if (transaction_id_ == 1.0):\n                self._connect_result = packet\n            else:\n                self.handle_packet(packet)\n        else:\n            self.handle_packet(packet)\n    if transaction_id:\n        result = self._invoke_results.pop(transaction_id, None)\n        return result\n    if invoked_method:\n        args = self._invoke_args.pop(invoked_method, None)\n        return args\n", "label": 1}
{"function": "\n\ndef versions_from_expanded_variables(variables, tag_prefix, verbose=False):\n    refnames = variables['refnames'].strip()\n    if refnames.startswith('$Format'):\n        if verbose:\n            print('variables are unexpanded, not using')\n        return {\n            \n        }\n    refs = set([r.strip() for r in refnames.strip('()').split(',')])\n    TAG = 'tag: '\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if (not tags):\n        tags = set([r for r in refs if re.search('\\\\d', r)])\n        if verbose:\n            print((\"discarding '%s', no digits\" % ','.join((refs - tags))))\n    if verbose:\n        print(('likely tags: %s' % ','.join(sorted(tags))))\n    for ref in sorted(tags):\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(('picking %s' % r))\n            return {\n                'version': r,\n                'full': variables['full'].strip(),\n            }\n    if verbose:\n        print('no suitable tags, using full revision id')\n    return {\n        'version': variables['full'].strip(),\n        'full': variables['full'].strip(),\n    }\n", "label": 1}
{"function": "\n\ndef convert_to_dict(obj, ident=0, limit_ident=6):\n    ident += 1\n    if (type(obj) in primitive):\n        return obj\n    if (isinstance(obj, inspect.types.InstanceType) or (type(obj) not in (list, tuple, dict))):\n        if (ident <= limit_ident):\n            try:\n                obj = obj.convert_to_dict()\n            except AttributeError:\n                try:\n                    t = obj.__dict__\n                    t['_type_class'] = str(obj.__class__)\n                    obj = t\n                except AttributeError:\n                    return str((obj.__class__ if hasattr(obj, '__class__') else type(obj)))\n        else:\n            return str((obj.__class__ if hasattr(obj, '__class__') else type(obj)))\n    if (type(obj) is dict):\n        res = {\n            \n        }\n        for item in obj:\n            if (ident <= limit_ident):\n                res[item] = convert_to_dict(obj[item], ident)\n            else:\n                res[item] = str(obj[item])\n        return res\n    if (type(obj) in (list, tuple)):\n        res = []\n        for item in obj:\n            if (ident <= limit_ident):\n                res.append(convert_to_dict(item, ident))\n            else:\n                res.append(str(item))\n        return (res if (type(obj) is list) else tuple(res))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.finish = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.reversed = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef message_attr(self):\n    '\\n        Returns options dict as sent within WAMP messages.\\n        '\n    options = {\n        \n    }\n    if (self.acknowledge is not None):\n        options['acknowledge'] = self.acknowledge\n    if (self.exclude_me is not None):\n        options['exclude_me'] = self.exclude_me\n    if (self.exclude is not None):\n        options['exclude'] = (self.exclude if (type(self.exclude) == list) else [self.exclude])\n    if (self.exclude_authid is not None):\n        options['exclude_authid'] = (self.exclude_authid if (type(self.exclude_authid) == list) else self.exclude_authid)\n    if (self.exclude_authrole is not None):\n        options['exclude_authrole'] = (self.exclude_authrole if (type(self.exclude_authrole) == list) else self.exclude_authrole)\n    if (self.eligible is not None):\n        options['eligible'] = (self.eligible if (type(self.eligible) == list) else self.eligible)\n    if (self.eligible_authid is not None):\n        options['eligible_authid'] = (self.eligible_authid if (type(self.eligible_authid) == list) else self.eligible_authid)\n    if (self.eligible_authrole is not None):\n        options['eligible_authrole'] = (self.eligible_authrole if (type(self.eligible_authrole) == list) else self.eligible_authrole)\n    return options\n", "label": 1}
{"function": "\n\ndef test_manage_session2():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test', 'test1'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == True)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == True)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == True)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == True)\n        assert (key.can_manage('test1.test') == True)\n", "label": 1}
{"function": "\n\ndef _replace_heap(variable, heap):\n    if isinstance(variable, Pointer):\n        while isinstance(variable, Pointer):\n            if (variable.index == 0):\n                variable = None\n            elif (variable.index in heap):\n                variable = heap[variable.index]\n            else:\n                warnings.warn('Variable referenced by pointer not found in heap: variable will be set to None')\n                variable = None\n        (replace, new) = _replace_heap(variable, heap)\n        if replace:\n            variable = new\n        return (True, variable)\n    elif isinstance(variable, np.core.records.recarray):\n        for (ir, record) in enumerate(variable):\n            (replace, new) = _replace_heap(record, heap)\n            if replace:\n                variable[ir] = new\n        return (False, variable)\n    elif isinstance(variable, np.core.records.record):\n        for (iv, value) in enumerate(variable):\n            (replace, new) = _replace_heap(value, heap)\n            if replace:\n                variable[iv] = new\n        return (False, variable)\n    elif isinstance(variable, np.ndarray):\n        if (variable.dtype.type is np.object_):\n            for iv in range(variable.size):\n                (replace, new) = _replace_heap(variable.item(iv), heap)\n                if replace:\n                    variable.itemset(iv, new)\n        return (False, variable)\n    else:\n        return (False, variable)\n", "label": 1}
{"function": "\n\ndef __init__(self, toklist, name=None, asList=True, modal=True, isinstance=isinstance):\n    if self.__doinit:\n        self.__doinit = False\n        self.__name = None\n        self.__parent = None\n        self.__accumNames = {\n            \n        }\n        if isinstance(toklist, list):\n            self.__toklist = toklist[:]\n        elif isinstance(toklist, _generatorType):\n            self.__toklist = list(toklist)\n        else:\n            self.__toklist = [toklist]\n        self.__tokdict = dict()\n    if ((name is not None) and name):\n        if (not modal):\n            self.__accumNames[name] = 0\n        if isinstance(name, int):\n            name = _ustr(name)\n        self.__name = name\n        if (not (isinstance(toklist, (type(None), basestring, list)) and (toklist in (None, '', [])))):\n            if isinstance(toklist, basestring):\n                toklist = [toklist]\n            if asList:\n                if isinstance(toklist, ParseResults):\n                    self[name] = _ParseResultsWithOffset(toklist.copy(), 0)\n                else:\n                    self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]), 0)\n                self[name].__name = name\n            else:\n                try:\n                    self[name] = toklist[0]\n                except (KeyError, TypeError, IndexError):\n                    self[name] = toklist\n", "label": 1}
{"function": "\n\ndef get_value(self, var, cast=None, default=NOTSET, parse_default=False):\n    'Return value for given environment variable.\\n\\n        :param var: Name of variable.\\n        :param cast: Type to cast return value as.\\n        :param default: If var not present in environ, return this instead.\\n        :param parse_default: force to parse default..\\n\\n        :returns: Value from environment or default (if set)\\n        '\n    logger.debug(\"get '{0}' casted as '{1}' with default '{2}'\".format(var, cast, default))\n    if (var in self.scheme):\n        var_info = self.scheme[var]\n        try:\n            has_default = (len(var_info) == 2)\n        except TypeError:\n            has_default = False\n        if has_default:\n            if (not cast):\n                cast = var_info[0]\n            if (default is self.NOTSET):\n                try:\n                    default = var_info[1]\n                except IndexError:\n                    pass\n        elif (not cast):\n            cast = var_info\n    try:\n        value = self.ENVIRON[var]\n    except KeyError:\n        if (default is self.NOTSET):\n            error_msg = 'Set the {0} environment variable'.format(var)\n            raise ImproperlyConfigured(error_msg)\n        value = default\n    if (hasattr(value, 'startswith') and value.startswith('$')):\n        value = value.lstrip('$')\n        value = self.get_value(value, cast=cast, default=default)\n    if ((value != default) or (parse_default and value)):\n        value = self.parse_value(value, cast)\n    return value\n", "label": 1}
{"function": "\n\ndef _HandleImports(self, element, import_manager=None):\n    'Handles imports for the specified element.\\n\\n    Args:\\n      element: (Property|Parameter) The property we want to set the import for.\\n      import_manager: The import manager to import into if not the implied one.\\n\\n    '\n    element = getattr(element, 'referenced_schema', element)\n    if isinstance(element, data_types.ComplexDataType):\n        parent = element\n        data_type = element\n    else:\n        parent = element.schema\n        data_type = element.data_type\n    data_type = getattr(data_type, 'referenced_schema', data_type)\n    if (not import_manager):\n        if self._InnerModelClassesSupported():\n            while parent.parent:\n                parent = parent.parent\n        import_manager = cpp_import_manager.CppImportManager.ForElement(parent)\n    while isinstance(data_type, (data_types.ArrayDataType, data_types.MapDataType)):\n        data_type = data_type._base_type\n        data_type = getattr(data_type, 'referenced_schema', data_type)\n    if (not data_type):\n        return\n    json_type = data_type.json_type\n    json_format = data_type.values.get('format')\n    if (json_type == 'object'):\n        if (not data_type.parent):\n            import_manager.AddDataType(data_type)\n        return\n    datatype_and_imports = self.language_model.type_map.get((json_type, json_format))\n    if datatype_and_imports:\n        import_definition = datatype_and_imports[1]\n        for required_import in import_definition.imports:\n            if required_import:\n                import_manager.AddImport(required_import)\n        for template_value in import_definition.template_values:\n            element.SetTemplateValue(template_value, True)\n    elif data_type:\n        if (not data_type.parent):\n            import_manager.AddDataType(data_type)\n    return\n", "label": 1}
{"function": "\n\ndef __setattr__(self, name, value):\n    if ((name in self.__dict__.keys()) or ('_BaseAWSObject__initialized' not in self.__dict__)):\n        return dict.__setattr__(self, name, value)\n    elif (name in self.attributes):\n        self.resource[name] = value\n        return None\n    elif (name in self.propnames):\n        expected_type = self.props[name][0]\n        if isinstance(value, AWSHelperFn):\n            return self.properties.__setitem__(name, value)\n        elif isinstance(expected_type, types.FunctionType):\n            try:\n                value = expected_type(value)\n            except:\n                sys.stderr.write((\"%s: %s.%s function validator '%s' threw exception:\\n\" % (self.__class__, self.title, name, expected_type.__name__)))\n                raise\n            return self.properties.__setitem__(name, value)\n        elif isinstance(expected_type, list):\n            if (not isinstance(value, list)):\n                self._raise_type(name, value, expected_type)\n            for v in value:\n                if ((not isinstance(v, tuple(expected_type))) and (not isinstance(v, AWSHelperFn))):\n                    self._raise_type(name, v, expected_type)\n            return self.properties.__setitem__(name, value)\n        elif isinstance(value, expected_type):\n            return self.properties.__setitem__(name, value)\n        else:\n            self._raise_type(name, value, expected_type)\n    type_name = getattr(self, 'resource_type', self.__class__.__name__)\n    if ((type_name == 'AWS::CloudFormation::CustomResource') or type_name.startswith('Custom::')):\n        return self.properties.__setitem__(name, value)\n    raise AttributeError(('%s object does not support attribute %s' % (type_name, name)))\n", "label": 1}
{"function": "\n\n@get('/s/users/{user_id}')\ndef get_user_route(request, user_id):\n    '\\n    Get the user by their ID.\\n    '\n    db_conn = request['db_conn']\n    user = User.get(db_conn, id=user_id)\n    current_user = get_current_user(request)\n    if (not user):\n        return abort(404)\n    data = {\n        \n    }\n    data['user'] = user.deliver(access=('private' if (current_user and (user['id'] == current_user['id'])) else None))\n    if ('posts' in request['params']):\n        data['posts'] = [post.deliver() for post in get_posts_facade(db_conn, user_id=user['id'])]\n    if (('sets' in request['params']) and (user['settings']['view_sets'] == 'public')):\n        u_sets = UserSets.get(db_conn, user_id=user['id'])\n        data['sets'] = [set_.deliver() for set_ in u_sets.list_sets(db_conn)]\n    if (('follows' in request['params']) and (user['settings']['view_follows'] == 'public')):\n        data['follows'] = [follow.deliver() for follow in Follow.list(db_conn, user_id=user['id'])]\n    if ('avatar' in request['params']):\n        size = int(request['params']['avatar'])\n        data['avatar'] = user.get_avatar((size if size else None))\n    return (200, data)\n", "label": 1}
{"function": "\n\ndef uninstall(pkg, package_name, remove_all, app_id, cli, app):\n    'Uninstalls a package.\\n\\n    :param pkg: package manager to uninstall with\\n    :type pkg: PackageManager\\n    :param package_name: The package to uninstall\\n    :type package_name: str\\n    :param remove_all: Whether to remove all instances of the named app\\n    :type remove_all: boolean\\n    :param app_id: App ID of the app instance to uninstall\\n    :type app_id: str\\n    :param init_client: The program to use to run the app\\n    :type init_client: object\\n    :rtype: None\\n    '\n    if ((cli is False) and (app is False)):\n        cli = app = True\n    uninstalled = False\n    installed = installed_packages(pkg, app_id, package_name)\n    installed_cli = next((True for installed_pkg in installed if installed_pkg.get('command')), False)\n    installed_app = next((True for installed_pkg in installed if installed_pkg.get('apps')), False)\n    if (cli and installed_cli):\n        if subcommand.uninstall(package_name):\n            uninstalled = True\n    if (app and installed_app):\n        if pkg.uninstall_app(package_name, remove_all, app_id):\n            uninstalled = True\n    if uninstalled:\n        return None\n    else:\n        msg = 'Package [{}]'.format(package_name)\n        if (app_id is not None):\n            app_id = util.normalize_app_id(app_id)\n            msg += ' with id [{}]'.format(app_id)\n        msg += ' is not installed'\n        raise DCOSException(msg)\n", "label": 1}
{"function": "\n\ndef get_filters(self, request):\n    lookup_params = self.get_filters_params()\n    use_distinct = False\n    for (key, value) in lookup_params.items():\n        if (not self.model_admin.lookup_allowed(key, value)):\n            raise DisallowedModelAdminLookup(('Filtering by %s not allowed' % key))\n    filter_specs = []\n    if self.list_filter:\n        for list_filter in self.list_filter:\n            if callable(list_filter):\n                spec = list_filter(request, lookup_params, self.model, self.model_admin)\n            else:\n                field_path = None\n                if isinstance(list_filter, (tuple, list)):\n                    (field, field_list_filter_class) = list_filter\n                else:\n                    (field, field_list_filter_class) = (list_filter, FieldListFilter.create)\n                if (not isinstance(field, models.Field)):\n                    field_path = field\n                    field = get_fields_from_path(self.model, field_path)[(- 1)]\n                spec = field_list_filter_class(field, request, lookup_params, self.model, self.model_admin, field_path=field_path)\n                use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, field_path))\n            if (spec and spec.has_output()):\n                filter_specs.append(spec)\n    try:\n        for (key, value) in lookup_params.items():\n            lookup_params[key] = prepare_lookup_value(key, value)\n            use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, key))\n        return (filter_specs, bool(filter_specs), lookup_params, use_distinct)\n    except FieldDoesNotExist as e:\n        six.reraise(IncorrectLookupParameters, IncorrectLookupParameters(e), sys.exc_info()[2])\n", "label": 1}
{"function": "\n\ndef test_update_crossing_duration1_not_duration_not_at_1st_step(self, duration1, duration2):\n    global rec\n    if ((duration1 == 0.0) or (duration2 == 0.0)):\n        return\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    composite = ac.sequence(a1, a2)\n    rec = []\n    node.do(composite)\n    elapsed = 0.0\n    next_elapsed = (duration1 / 2.0)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    elapsed = next_elapsed\n    next_elapsed = ((duration1 + duration2) / 2.0)\n    dt = (next_elapsed - elapsed)\n    rec = []\n    node._step(dt)\n    recx = [e for e in rec if (e[1] != 'step')]\n    rec = [e for e in recx if (e[0] == name1)]\n    print('rec', rec)\n    assert ((rec[0][1] == 'update') and (rec[0][2] == 1.0))\n    assert (rec[1][1] == 'stop')\n    assert (len(rec) == 2)\n    rec = [e for e in recx if (e[0] == name2)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert (rec[1][1] == 'update')\n    assert (abs((rec[1][2] - ((next_elapsed - duration1) / duration2))) < fe)\n    assert (len(rec) == 2)\n", "label": 1}
{"function": "\n\ndef _inverse_binarize_thresholding(y, output_type, classes, threshold):\n    'Inverse label binarization transformation using thresholding.'\n    if ((output_type == 'binary') and (y.ndim == 2) and (y.shape[1] > 2)):\n        raise ValueError(\"output_type='binary', but y.shape = {0}\".format(y.shape))\n    if ((output_type != 'binary') and (y.shape[1] != len(classes))):\n        raise ValueError('The number of class is not equal to the number of dimension of y.')\n    classes = np.asarray(classes)\n    if sp.issparse(y):\n        if (threshold > 0):\n            if (y.format not in ('csr', 'csc')):\n                y = y.tocsr()\n            y.data = np.array((y.data > threshold), dtype=np.int)\n            y.eliminate_zeros()\n        else:\n            y = np.array((y.toarray() > threshold), dtype=np.int)\n    else:\n        y = np.array((y > threshold), dtype=np.int)\n    if (output_type == 'binary'):\n        if sp.issparse(y):\n            y = y.toarray()\n        if ((y.ndim == 2) and (y.shape[1] == 2)):\n            return classes[y[:, 1]]\n        elif (len(classes) == 1):\n            return np.repeat(classes[0], len(y))\n        else:\n            return classes[y.ravel()]\n    elif (output_type == 'multilabel-indicator'):\n        return y\n    else:\n        raise ValueError('{0} format is not supported'.format(output_type))\n", "label": 1}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01):\n    ''\n    N = int(N)\n    assert (N > 0)\n    samplerate = self.audio.samplerate\n    assert (1 <= freq_base <= freq_max <= (samplerate / 2.0))\n    step = 1024\n    win = step\n    assert (0 < step <= win)\n    coeffies = self.make_erb_filter_coeffiences(samplerate, N, freq_base, freq_max)\n    zi = None\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for samples in self.audio.walk(win, step, start, end, join_channels=True):\n        (y, zi) = self.filter(samples, coeffies, zi)\n        if (not combine):\n            if each:\n                for frame in y:\n                    (yield frame)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\ndef __getitem__(self, lst):\n    if (isinstance(lst, tuple) and (len(lst) < 5) and any(((Ellipsis is x) for x in lst))):\n        if ((len(lst) == 2) and (lst[1] is Ellipsis)):\n            return enumFrom(lst[0])\n        elif ((len(lst) == 3) and (lst[2] is Ellipsis)):\n            return enumFromThen(lst[0], lst[1])\n        elif ((len(lst) == 3) and (lst[1] is Ellipsis)):\n            return enumFromTo(lst[0], lst[2])\n        elif ((len(lst) == 4) and (lst[2] is Ellipsis)):\n            return enumFromThenTo(lst[0], lst[1], lst[3])\n        raise SyntaxError(('Invalid list comprehension: %s' % str(lst)))\n    elif (hasattr(lst, 'next') or hasattr(lst, '__next__')):\n        return List(tail=lst)\n    return List(head=lst)\n", "label": 1}
{"function": "\n\ndef parse_f(self, args):\n    if ((len(self.tex_coords) > 1) and (len(self.normals) == 1)):\n        raise PywavefrontException('Found texture coordinates, but no normals')\n    if (self.mesh is None):\n        self.mesh = mesh.Mesh()\n        self.wavefront.add_mesh(self.mesh)\n    if (self.material is None):\n        self.material = material.Material()\n    self.mesh.add_material(self.material)\n    v1 = None\n    vlast = None\n    points = []\n    for (i, v) in enumerate(args[0:]):\n        if (type(v) is bytes):\n            v = v.decode()\n        (v_index, t_index, n_index) = (list(map(int, [(j or 0) for j in v.split('/')])) + [0, 0])[:3]\n        if (v_index < 0):\n            v_index += (len(self.vertices) - 1)\n        if (t_index < 0):\n            t_index += (len(self.tex_coords) - 1)\n        if (n_index < 0):\n            n_index += (len(self.normals) - 1)\n        vertex = ((list(self.tex_coords[t_index]) + list(self.normals[n_index])) + list(self.vertices[v_index]))\n        if (i >= 3):\n            self.material.vertices += (v1 + vlast)\n        self.material.vertices += vertex\n        if (i == 0):\n            v1 = vertex\n        vlast = vertex\n", "label": 1}
{"function": "\n\ndef model_fields(model, fields=None, readonly_fields=None, exclude=None, field_args=None, converter=None):\n    '\\n    Generate a dictionary of WTForms fields for a given MongoEngine model.\\n\\n    See `model_form` docstring for description of parameters.\\n    '\n    from mongoengine.base import BaseDocument\n    if (BaseDocument not in inspect.getmro(model)):\n        raise TypeError('Model must be a MongoEngine Document schema')\n    readonly_fields = (readonly_fields or [])\n    exclude = (exclude or [])\n    converter = (converter or ModelConverter())\n    field_args = (field_args or {\n        \n    })\n    field_names = (fields if fields else model._fields.keys())\n    field_names = (x for x in field_names if (x not in exclude))\n    field_dict = {\n        \n    }\n    for name in field_names:\n        if ((name not in readonly_fields) and (name not in model._fields)):\n            raise KeyError(('\"%s\" is not read-only and does not appear to be a field on the document.' % name))\n        if ((name in model._fields) and (name not in readonly_fields)):\n            model_field = model._fields[name]\n            field = converter.convert(model, model_field, field_args.get(name))\n            if (field is not None):\n                field_dict[name] = field\n    return field_dict\n", "label": 1}
{"function": "\n\ndef process_urlencoded(entity):\n    'Read application/x-www-form-urlencoded data into entity.params.'\n    qs = entity.fp.read()\n    for charset in entity.attempt_charsets:\n        try:\n            params = {\n                \n            }\n            for aparam in qs.split(ntob('&')):\n                for pair in aparam.split(ntob(';')):\n                    if (not pair):\n                        continue\n                    atoms = pair.split(ntob('='), 1)\n                    if (len(atoms) == 1):\n                        atoms.append(ntob(''))\n                    key = unquote_plus(atoms[0]).decode(charset)\n                    value = unquote_plus(atoms[1]).decode(charset)\n                    if (key in params):\n                        if (not isinstance(params[key], list)):\n                            params[key] = [params[key]]\n                        params[key].append(value)\n                    else:\n                        params[key] = value\n        except UnicodeDecodeError:\n            pass\n        else:\n            entity.charset = charset\n            break\n    else:\n        raise cherrypy.HTTPError(400, ('The request entity could not be decoded. The following charsets were attempted: %s' % repr(entity.attempt_charsets)))\n    for (key, value) in params.items():\n        if (key in entity.params):\n            if (not isinstance(entity.params[key], list)):\n                entity.params[key] = [entity.params[key]]\n            entity.params[key].append(value)\n        else:\n            entity.params[key] = value\n", "label": 1}
{"function": "\n\ndef find(self, path):\n    '\\n        Generate filenames in path that satisfy criteria specified in\\n        the constructor.\\n        This method is a generator and should be repeatedly called\\n        until there are no more results.\\n        '\n    for (dirpath, dirs, files) in os.walk(path):\n        depth = dirpath[(len(path) + len(os.path.sep)):].count(os.path.sep)\n        if (depth >= self.mindepth):\n            for name in (dirs + files):\n                fstat = None\n                matches = True\n                fullpath = None\n                for criterion in self.criteria:\n                    if ((fstat is None) and (criterion.requires() & _REQUIRES_STAT)):\n                        fullpath = os.path.join(dirpath, name)\n                        fstat = os.stat(fullpath)\n                    if (not criterion.match(dirpath, name, fstat)):\n                        matches = False\n                        break\n                if matches:\n                    if (fullpath is None):\n                        fullpath = os.path.join(dirpath, name)\n                    for action in self.actions:\n                        if ((fstat is None) and (action.requires() & _REQUIRES_STAT)):\n                            fstat = os.stat(fullpath)\n                        result = action.execute(fullpath, fstat, test=self.test)\n                        if (result is not None):\n                            (yield result)\n        if (depth == self.maxdepth):\n            dirs[:] = []\n", "label": 1}
{"function": "\n\ndef _qname_matches(tag, namespace, qname):\n    \"Logic determines if a QName matches the desired local tag and namespace.\\n  \\n  This is used in XmlElement.get_elements and XmlElement.get_attributes to\\n  find matches in the element's members (among all expected-and-unexpected\\n  elements-and-attributes).\\n  \\n  Args:\\n    expected_tag: string\\n    expected_namespace: string\\n    qname: string in the form '{xml_namespace}localtag' or 'tag' if there is\\n           no namespace.\\n  \\n  Returns:\\n    boolean True if the member's tag and namespace fit the expected tag and\\n    namespace.\\n  \"\n    if (qname is None):\n        member_tag = None\n        member_namespace = None\n    elif qname.startswith('{'):\n        member_namespace = qname[1:qname.index('}')]\n        member_tag = qname[(qname.index('}') + 1):]\n    else:\n        member_namespace = None\n        member_tag = qname\n    return (((tag is None) and (namespace is None)) or ((namespace is None) and (member_tag == tag)) or ((tag is None) and (member_namespace == namespace)) or ((tag is None) and (namespace == '') and (member_namespace is None)) or ((tag == member_tag) and (namespace == member_namespace)) or ((tag == member_tag) and (namespace == '') and (member_namespace is None)))\n", "label": 1}
{"function": "\n\ndef get_instances(name, lifecycle_state='InService', health_status='Healthy', attribute='private_ip_address', attributes=None, region=None, key=None, keyid=None, profile=None):\n    '\\n    return attribute of all instances in the named autoscale group.\\n\\n    CLI example::\\n\\n        salt-call boto_asg.get_instances my_autoscale_group_name\\n\\n    '\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    ec2_conn = _get_ec2_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        asgs = conn.get_all_groups(names=[name])\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        return False\n    if (len(asgs) != 1):\n        log.debug(\"name '{0}' returns multiple ASGs: {1}\".format(name, [asg.name for asg in asgs]))\n        return False\n    asg = asgs[0]\n    instance_ids = []\n    for i in asg.instances:\n        if ((lifecycle_state is not None) and (i.lifecycle_state != lifecycle_state)):\n            continue\n        if ((health_status is not None) and (i.health_status != health_status)):\n            continue\n        instance_ids.append(i.instance_id)\n    instances = ec2_conn.get_only_instances(instance_ids=instance_ids)\n    if attributes:\n        return [[getattr(instance, attr).encode('ascii') for attr in attributes] for instance in instances]\n    else:\n        return [getattr(instance, attribute).encode('ascii') for instance in instances if getattr(instance, attribute)]\n    return [getattr(instance, attribute).encode('ascii') for instance in instances]\n", "label": 1}
{"function": "\n\ndef RC_calc(ctx, x, y, r, pv=True):\n    if (not (ctx.isnormal(x) and ctx.isnormal(y))):\n        if (ctx.isinf(x) or ctx.isinf(y)):\n            return (1 / (x * y))\n        if (y == 0):\n            return ctx.inf\n        if (x == 0):\n            return ((ctx.pi / ctx.sqrt(y)) / 2)\n        raise ValueError\n    if (pv and (ctx._im(y) == 0) and (ctx._re(y) < 0)):\n        return (ctx.sqrt((x / (x - y))) * RC_calc(ctx, (x - y), (- y), r))\n    if (x == y):\n        return (1 / ctx.sqrt(x))\n    extraprec = (2 * max(0, ((- ctx.mag((x - y))) + ctx.mag(x))))\n    ctx.prec += extraprec\n    if (ctx._is_real_type(x) and ctx._is_real_type(y)):\n        x = ctx._re(x)\n        y = ctx._re(y)\n        a = ctx.sqrt((x / y))\n        if (x < y):\n            b = ctx.sqrt((y - x))\n            v = (ctx.acos(a) / b)\n        else:\n            b = ctx.sqrt((x - y))\n            v = (ctx.acosh(a) / b)\n    else:\n        sx = ctx.sqrt(x)\n        sy = ctx.sqrt(y)\n        v = (ctx.acos((sx / sy)) / (ctx.sqrt((1 - (x / y))) * sy))\n    ctx.prec -= extraprec\n    return v\n", "label": 1}
{"function": "\n\ndef execute(self, fullpath, fstat, test=False):\n    result = []\n    for arg in self.fmt:\n        if (arg == 'path'):\n            result.append(fullpath)\n        elif (arg == 'name'):\n            result.append(os.path.basename(fullpath))\n        elif (arg == 'size'):\n            result.append(fstat[stat.ST_SIZE])\n        elif (arg == 'type'):\n            result.append(_FILE_TYPES.get(stat.S_IFMT(fstat[stat.ST_MODE]), '?'))\n        elif (arg == 'mode'):\n            result.append(int(oct(fstat[stat.ST_MODE])[(- 3):]))\n        elif (arg == 'mtime'):\n            result.append(fstat[stat.ST_MTIME])\n        elif (arg == 'user'):\n            uid = fstat[stat.ST_UID]\n            try:\n                result.append(pwd.getpwuid(uid).pw_name)\n            except KeyError:\n                result.append(uid)\n        elif (arg == 'group'):\n            gid = fstat[stat.ST_GID]\n            try:\n                result.append(grp.getgrgid(gid).gr_name)\n            except KeyError:\n                result.append(gid)\n        elif (arg == 'md5'):\n            if stat.S_ISREG(fstat[stat.ST_MODE]):\n                md5digest = salt.utils.get_hash(fullpath, 'md5')\n                result.append(md5digest)\n            else:\n                result.append('')\n    if (len(result) == 1):\n        return result[0]\n    else:\n        return result\n", "label": 1}
{"function": "\n\ndef _ApplyBarChartStyle(self, chart):\n    'If bar style is specified, fill in the missing data and apply it.'\n    if ((chart.style is None) or (not chart.data)):\n        return {\n            \n        }\n    (bar_thickness, bar_gap, group_gap) = (chart.style.bar_thickness, chart.style.bar_gap, chart.style.group_gap)\n    if ((bar_gap is None) and (group_gap is not None)):\n        bar_gap = max(0, (group_gap / 2))\n        if (not chart.style.use_fractional_gap_spacing):\n            bar_gap = int(bar_gap)\n    if ((group_gap is None) and (bar_gap is not None)):\n        group_gap = max(0, (bar_gap * 2))\n    if (bar_thickness is None):\n        if chart.style.use_fractional_gap_spacing:\n            bar_thickness = 'r'\n        else:\n            bar_thickness = 'a'\n    elif chart.style.use_fractional_gap_spacing:\n        if bar_gap:\n            bar_gap = int((bar_thickness * bar_gap))\n        if group_gap:\n            group_gap = int((bar_thickness * group_gap))\n    spec = [bar_thickness]\n    if (bar_gap is not None):\n        spec.append(bar_gap)\n        if ((group_gap is not None) and (not chart.stacked)):\n            spec.append(group_gap)\n    return util.JoinLists(bar_size=spec)\n", "label": 1}
{"function": "\n\ndef __new__(cls, latitude=None, longitude=None, altitude=None):\n    single_arg = ((longitude is None) and (altitude is None))\n    if (single_arg and (not isinstance(latitude, util.NUMBER_TYPES))):\n        arg = latitude\n        if (arg is None):\n            pass\n        elif isinstance(arg, Point):\n            return cls.from_point(arg)\n        elif isinstance(arg, basestring):\n            return cls.from_string(arg)\n        else:\n            try:\n                seq = iter(arg)\n            except TypeError:\n                raise TypeError(('Failed to create Point instance from %r.' % (arg,)))\n            else:\n                return cls.from_sequence(seq)\n    latitude = float((latitude or 0))\n    if (abs(latitude) > 90):\n        raise ValueError(('Latitude out of range [-90, 90]: %r' % latitude))\n    longitude = float((longitude or 0))\n    if (abs(longitude) > 180):\n        raise ValueError(('Longitude out of range [-180, 180]: %r' % longitude))\n    altitude = float((altitude or 0))\n    self = super(Point, cls).__new__(cls)\n    self.latitude = latitude\n    self.longitude = longitude\n    self.altitude = altitude\n    return self\n", "label": 1}
{"function": "\n\ndef GetArtifacts(self, os_name=None, name_list=None, source_type=None, exclude_dependents=False, provides=None, reload_datastore_artifacts=False):\n    'Retrieve artifact classes with optional filtering.\\n\\n    All filters must match for the artifact to be returned.\\n\\n    Args:\\n      os_name: string to match against supported_os\\n      name_list: list of strings to match against artifact names\\n      source_type: rdf_artifacts.ArtifactSource.SourceType to match against\\n                      source_type\\n      exclude_dependents: if true only artifacts with no dependencies will be\\n                          returned\\n      provides: return the artifacts that provide these dependencies\\n      reload_datastore_artifacts: If true, the data store sources are queried\\n                                  for new artifacts.\\n    Returns:\\n      set of artifacts matching filter criteria\\n    '\n    self._CheckDirty(reload_datastore_artifacts=reload_datastore_artifacts)\n    results = set()\n    for artifact in self._artifacts.itervalues():\n        if (os_name and artifact.supported_os and (os_name not in artifact.supported_os)):\n            continue\n        if (name_list and (artifact.name not in name_list)):\n            continue\n        if source_type:\n            source_types = [c.type for c in artifact.sources]\n            if (source_type not in source_types):\n                continue\n        if (exclude_dependents and artifact.GetArtifactPathDependencies()):\n            continue\n        if provides:\n            for provide_string in artifact.provides:\n                if (provide_string in provides):\n                    results.add(artifact)\n                    continue\n            continue\n        results.add(artifact)\n    return results\n", "label": 1}
{"function": "\n\ndef identifyConceptsUsed(self):\n    self.relationshipSets = [(arcrole, ELR, linkqname, arcqname) for (arcrole, ELR, linkqname, arcqname) in self.modelXbrl.baseSets.keys() if (ELR and (arcrole.startswith('XBRL-') or (linkqname and arcqname)))]\n    conceptsUsed = set((f.qname for f in self.modelXbrl.factsInInstance))\n    for cntx in self.modelXbrl.contexts.values():\n        for dim in cntx.qnameDims.values():\n            conceptsUsed.add(dim.dimensionQname)\n            if dim.isExplicit:\n                conceptsUsed.add(dim.memberQname)\n            else:\n                conceptsUsed.add(dim.typedMember.qname)\n    for (defaultDim, defaultDimMember) in self.modelXbrl.qnameDimensionDefaults.items():\n        conceptsUsed.add(defaultDim)\n        conceptsUsed.add(defaultDimMember)\n    for relationshipSetKey in self.relationshipSets:\n        relationshipSet = self.modelXbrl.relationshipSet(*relationshipSetKey)\n        for rel in relationshipSet.modelRelationships:\n            if isinstance(rel.fromModelObject, ModelConcept):\n                conceptsUsed.add(rel.fromModelObject)\n            if isinstance(rel.toModelObject, ModelConcept):\n                conceptsUsed.add(rel.toModelObject)\n    for qn in (XbrlConst.qnXbrliIdentifier, XbrlConst.qnXbrliPeriod, XbrlConst.qnXbrliUnit):\n        conceptsUsed.add(self.modelXbrl.qnameConcepts[qn])\n    conceptsUsed -= {None}\n    self.conceptsUsed = conceptsUsed\n", "label": 1}
{"function": "\n\ndef _sync_author_detail(self, key='author'):\n    context = self._getContext()\n    detail = context.get(('%ss' % key), [FeedParserDict()])[(- 1)]\n    if detail:\n        name = detail.get('name')\n        email = detail.get('email')\n        if (name and email):\n            context[key] = ('%s (%s)' % (name, email))\n        elif name:\n            context[key] = name\n        elif email:\n            context[key] = email\n    else:\n        (author, email) = (context.get(key), None)\n        if (not author):\n            return\n        emailmatch = re.search('(([a-zA-Z0-9\\\\_\\\\-\\\\.\\\\+]+)@((\\\\[[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.)|(([a-zA-Z0-9\\\\-]+\\\\.)+))([a-zA-Z]{2,4}|[0-9]{1,3})(\\\\]?))(\\\\?subject=\\\\S+)?', author)\n        if emailmatch:\n            email = emailmatch.group(0)\n            author = author.replace(email, '')\n            author = author.replace('()', '')\n            author = author.replace('<>', '')\n            author = author.replace('&lt;&gt;', '')\n            author = author.strip()\n            if (author and (author[0] == '(')):\n                author = author[1:]\n            if (author and (author[(- 1)] == ')')):\n                author = author[:(- 1)]\n            author = author.strip()\n        if (author or email):\n            context.setdefault(('%s_detail' % key), detail)\n        if author:\n            detail['name'] = author\n        if email:\n            detail['email'] = email\n", "label": 1}
{"function": "\n\ndef solve_TLE(self, board):\n    '\\n\\n        :param board: a 9x9 2D array\\n        :return: Boolean\\n        '\n    n = len(board)\n    if all([(board[(i / n)][(i % n)] != '.') for i in xrange((n * n))]):\n        return True\n    for i in xrange(n):\n        for j in xrange(n):\n            if (board[i][j] == '.'):\n                for num in range(1, 10):\n                    num_str = str(num)\n                    condition_row = all([(board[i][col] != num_str) for col in xrange(n)])\n                    condition_col = all([(board[row][j] != num_str) for row in xrange(n)])\n                    condition_square = all([(board[(((i / 3) * 3) + (count / 3))][(((j / 3) * 3) + (count % 3))] != num_str) for count in xrange(n)])\n                    if (condition_col and condition_row and condition_square):\n                        board[i][j] = num_str\n                        if (not self.solve(board)):\n                            board[i][j] = '.'\n                        else:\n                            return True\n    return False\n", "label": 1}
{"function": "\n\ndef _make_cfg_defaults(self, module_name=NotGiven, default_distribution=NotGiven, guess_maintainer=NotGiven):\n    defaults = {\n        \n    }\n    default_re = re.compile('^.* \\\\(Default: (.*)\\\\)$')\n    for (longopt, shortopt, description) in stdeb_cfg_options:\n        assert longopt.endswith('=')\n        assert (longopt.lower() == longopt)\n        key = longopt[:(- 1)]\n        matchobj = default_re.search(description)\n        if (matchobj is not None):\n            groups = matchobj.groups()\n            assert (len(groups) == 1)\n            value = groups[0]\n            if (value == '<source-debianized-setup-name>'):\n                assert (key == 'source')\n                value = source_debianize_name(module_name)\n            elif (value == 'python-<debianized-setup-name>'):\n                assert (key == 'package')\n                value = ('python-' + debianize_name(module_name))\n            elif (value == 'python3-<debianized-setup-name>'):\n                assert (key == 'package3')\n                value = ('python3-' + debianize_name(module_name))\n            elif (value == '<setup-maintainer-or-author>'):\n                assert (key == 'maintainer')\n                value = guess_maintainer\n            if (key == 'suite'):\n                if (default_distribution is not None):\n                    value = default_distribution\n                    log.warn('Deprecation warning: you are using the --default-distribution option. Switch to the --suite option.')\n        else:\n            value = ''\n        defaults[key] = value\n    return defaults\n", "label": 1}
{"function": "\n\ndef check_internet_scheme(self, elb_item):\n    '\\n        alert when an ELB has an \"internet-facing\" scheme.\\n        '\n    scheme = elb_item.config.get('scheme', None)\n    vpc = elb_item.config.get('vpc_id', None)\n    if (scheme and (scheme == 'internet-facing') and (not vpc)):\n        self.add_issue(1, 'ELB is Internet accessible.', elb_item)\n    elif (scheme and (scheme == 'internet-facing') and vpc):\n        security_groups = elb_item.config.get('security_groups', [])\n        for sgid in security_groups:\n            sg = Item.query.filter(Item.name.ilike((('%' + sgid) + '%'))).first()\n            if (not sg):\n                continue\n            sg_cidrs = []\n            config = sg.revisions[0].config\n            for rule in config.get('rules', []):\n                cidr = rule.get('cidr_ip', '')\n                if ((rule.get('rule_type', None) == 'ingress') and cidr):\n                    if ((not _check_rfc_1918(cidr)) and (not self._check_inclusion_in_network_whitelist(cidr))):\n                        sg_cidrs.append(cidr)\n            if sg_cidrs:\n                notes = 'SG [{sgname}] via [{cidr}]'.format(sgname=sg.name, cidr=', '.join(sg_cidrs))\n                self.add_issue(1, 'VPC ELB is Internet accessible.', elb_item, notes=notes)\n", "label": 1}
{"function": "\n\ndef resolve(self, context, ignore_failures=False):\n    if isinstance(self.var, Variable):\n        try:\n            obj = self.var.resolve(context)\n        except VariableDoesNotExist:\n            if ignore_failures:\n                obj = None\n            else:\n                string_if_invalid = context.template.engine.string_if_invalid\n                if string_if_invalid:\n                    if ('%s' in string_if_invalid):\n                        return (string_if_invalid % self.var)\n                    else:\n                        return string_if_invalid\n                else:\n                    obj = string_if_invalid\n    else:\n        obj = self.var\n    for (func, args) in self.filters:\n        arg_vals = []\n        for (lookup, arg) in args:\n            if (not lookup):\n                arg_vals.append(mark_safe(arg))\n            else:\n                arg_vals.append(arg.resolve(context))\n        if getattr(func, 'expects_localtime', False):\n            obj = template_localtime(obj, context.use_tz)\n        if getattr(func, 'needs_autoescape', False):\n            new_obj = func(obj, *arg_vals, autoescape=context.autoescape)\n        else:\n            new_obj = func(obj, *arg_vals)\n        if (getattr(func, 'is_safe', False) and isinstance(obj, SafeData)):\n            obj = mark_safe(new_obj)\n        elif isinstance(obj, EscapeData):\n            obj = mark_for_escaping(new_obj)\n        else:\n            obj = new_obj\n    return obj\n", "label": 1}
{"function": "\n\ndef updateIndels(self, snp, is_negative_strand):\n    contig = snp.chromosome\n    lcontig = self.mFasta.getLength(contig)\n    code = self.mAnnotations.getSequence(contig, '+', snp.pos, (snp.pos + 2))\n    self.mCode = code\n    variants = snp.genotype.split('/')\n    for variant in variants:\n        if (variant[0] == '*'):\n            self.mVariantType.append('W')\n        elif (variant[0] == '+'):\n            toinsert = variant[1:]\n            self.mVariantType.append('I')\n        elif (variant[0] == '-'):\n            todelete = variant[1:]\n            self.mVariantType.append('D')\n        else:\n            raise ValueError((\"unknown variant sign '%s'\" % variant[0]))\n    if (code[0] and (code[1] not in 'abcABC')):\n        return\n    if is_negative_strand:\n        variants = [Genomics.complement(x) for x in variants]\n    for reference_codon in self.mReferenceCodons:\n        variants = snp.genotype.split('/')\n        variants = [x[1:] for x in variants]\n        for variant in variants:\n            if ((len(variant) % 3) != 0):\n                self.mVariantCodons.append('!')\n            else:\n                self.mVariantCodons.append(variant)\n        self.mVariantAAs.extend([Genomics.translate(x) for x in self.mVariantCodons])\n", "label": 1}
{"function": "\n\ndef poll(self, event, timeout):\n    if self.state.SHUTDOWN:\n        raise Error(errno.EBADF)\n    if (not (event in ('recv', 'send', 'acks'))):\n        raise Error(errno.EINVAL)\n    if (event == 'recv'):\n        if (self.state.ESTABLISHED or self.state.CLOSE_WAIT):\n            ptype = super(DataLinkConnection, self).poll(event, timeout)\n            if (self.state.ESTABLISHED or self.state.CLOSE_WAIT):\n                return (ptype == ProtocolDataUnit.Information)\n            else:\n                return False\n    if (event == 'send'):\n        if self.state.ESTABLISHED:\n            if super(DataLinkConnection, self).poll(event, timeout):\n                return self.state.ESTABLISHED\n    if (event == 'acks'):\n        with self.acks_ready:\n            while (not (self.acks_recvd > 0)):\n                self.acks_ready.wait(timeout)\n            if (self.acks_recvd > 0):\n                self.acks_recvd = (self.acks_recvd - 1)\n                return True\n    return False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.end_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.endpoints = []\n                (_etype31, _size28) = iprot.readListBegin()\n                for _i32 in xrange(_size28):\n                    _elem33 = iprot.readString()\n                    self.endpoints.append(_elem33)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef lock(self):\n    '\\n        Acquire this lock.\\n\\n        @rtype: C{bool}\\n        @return: True if the lock is acquired, false otherwise.\\n\\n        @raise: Any exception os.symlink() may raise, other than\\n        EEXIST.\\n        '\n    clean = True\n    while True:\n        try:\n            symlink(str(os.getpid()), self.name)\n        except OSError as e:\n            if (_windows and (e.errno in (errno.EACCES, errno.EIO))):\n                return False\n            if (e.errno == errno.EEXIST):\n                try:\n                    pid = readlink(self.name)\n                except (IOError, OSError) as e:\n                    if (e.errno == errno.ENOENT):\n                        continue\n                    elif (_windows and (e.errno == errno.EACCES)):\n                        return False\n                    raise\n                try:\n                    if (kill is not None):\n                        kill(int(pid), 0)\n                except OSError as e:\n                    if (e.errno == errno.ESRCH):\n                        try:\n                            rmlink(self.name)\n                        except OSError as e:\n                            if (e.errno == errno.ENOENT):\n                                continue\n                            raise\n                        clean = False\n                        continue\n                    raise\n                return False\n            raise\n        self.locked = True\n        self.clean = clean\n        return True\n", "label": 1}
{"function": "\n\n@count_calls\ndef check_referenced_versions(self):\n    'Deeply checks all the references in the scene and returns a\\n        dictionary which uses the ids of the Versions as key and the action as\\n        value.\\n\\n        Uses the top level references to get a Stalker Version instance and\\n        then tracks all the changes from these Version instances.\\n\\n        :return: list\\n        '\n    dfs_version_references = []\n    version = self.get_current_version()\n    resolution_dictionary = empty_reference_resolution(root=self.get_referenced_versions())\n    for v in version.walk_hierarchy():\n        dfs_version_references.append(v)\n    dfs_version_references.pop(0)\n    for v in reversed(dfs_version_references):\n        to_be_updated_list = []\n        for ref_v in v.inputs:\n            if (not ref_v.is_latest_published_version()):\n                to_be_updated_list.append(ref_v)\n        if to_be_updated_list:\n            action = 'create'\n            latest_published_version = v.latest_published_version\n            if (latest_published_version and (not v.is_latest_published_version())):\n                if all([(ref_v.latest_published_version in latest_published_version.inputs) for ref_v in to_be_updated_list]):\n                    action = 'update'\n                else:\n                    action = 'create'\n        else:\n            if v.is_latest_published_version():\n                action = 'leave'\n            else:\n                action = 'update'\n            if any((((rev_v in resolution_dictionary['update']) or (rev_v in resolution_dictionary['create'])) for rev_v in v.inputs)):\n                action = 'create'\n        resolution_dictionary[action].append(v)\n    return resolution_dictionary\n", "label": 1}
{"function": "\n\ndef init_colors(self):\n    'Configure the coloring of the widget'\n    if self.pure:\n        return\n    try:\n        colors = self.config.ZMQInteractiveShell.colors\n    except AttributeError:\n        colors = None\n    try:\n        style = self.config.IPythonWidget.colors\n    except AttributeError:\n        style = None\n    if colors:\n        colors = colors.lower()\n        if (colors in ('lightbg', 'light')):\n            colors = 'lightbg'\n        elif (colors in ('dark', 'linux')):\n            colors = 'linux'\n        else:\n            colors = 'nocolor'\n    elif style:\n        if (style == 'bw'):\n            colors = 'nocolor'\n        elif styles.dark_style(style):\n            colors = 'linux'\n        else:\n            colors = 'lightbg'\n    else:\n        colors = None\n    widget = self.widget\n    if style:\n        widget.style_sheet = styles.sheet_from_template(style, colors)\n        widget.syntax_style = style\n        widget._syntax_style_changed()\n        widget._style_sheet_changed()\n    elif colors:\n        widget.set_default_style(colors=colors)\n    else:\n        widget.set_default_style()\n    if self.stylesheet:\n        if os.path.isfile(self.stylesheet):\n            with open(self.stylesheet) as f:\n                sheet = f.read()\n            widget.style_sheet = sheet\n            widget._style_sheet_changed()\n        else:\n            raise IOError(('Stylesheet %r not found.' % self.stylesheet))\n", "label": 1}
{"function": "\n\ndef List(self, device_path):\n    'Prints a directory listing.\\n\\n  Args:\\n    device_path: Directory to list.\\n  '\n    files = adb_commands.AdbCommands.List(self, device_path)\n    files.sort(key=(lambda x: x.filename))\n    maxname = max((len(f.filename) for f in files))\n    maxsize = max((len(str(f.size)) for f in files))\n    for f in files:\n        mode = (((((((((('d' if stat.S_ISDIR(f.mode) else '-') + ('r' if (f.mode & stat.S_IRUSR) else '-')) + ('w' if (f.mode & stat.S_IWUSR) else '-')) + ('x' if (f.mode & stat.S_IXUSR) else '-')) + ('r' if (f.mode & stat.S_IRGRP) else '-')) + ('w' if (f.mode & stat.S_IWGRP) else '-')) + ('x' if (f.mode & stat.S_IXGRP) else '-')) + ('r' if (f.mode & stat.S_IROTH) else '-')) + ('w' if (f.mode & stat.S_IWOTH) else '-')) + ('x' if (f.mode & stat.S_IXOTH) else '-'))\n        t = time.gmtime(f.mtime)\n        (yield ('%s %*d %04d-%02d-%02d %02d:%02d:%02d %-*s\\n' % (mode, maxsize, f.size, t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec, maxname, f.filename)))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.value = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.timestamp = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.ttl = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef fuzzy(event, base=None, date_format=None):\n    if (not base):\n        base = datetime.now()\n    if date_format:\n        event = datetime.strptime(event, date_format)\n    elif (type(event) == str):\n        event = datetime.fromtimestamp(int(event))\n    elif (type(event) == int):\n        event = datetime.fromtimestamp(event)\n    elif (type(event) != datetime):\n        raise Exception('Cannot convert object `{}` to fuzzy date string'.format(event))\n    delta = (base - event)\n    if (delta.days == 0):\n        if (delta.seconds < 60):\n            return '{} seconds ago'.format(delta.seconds)\n        elif (delta.seconds < 120):\n            return '1 min and {} secs ago'.format((delta.seconds - 60))\n        elif (delta.seconds < TEN_MINS):\n            return '{} mins and {} secs ago'.format((delta.seconds // 60), (delta.seconds % 60))\n        elif (delta.seconds < ONE_HOUR):\n            return '{} minutes ago'.format((delta.seconds // 60))\n        elif (delta.seconds < TWO_HOURS):\n            return '1 hour and {} mins ago'.format(((delta.seconds % ONE_HOUR) // 60))\n        return 'over {} hours ago'.format((delta.seconds // ONE_HOUR))\n    elif (delta.days < 2):\n        return 'over a day ago'\n    elif (delta.days < 7):\n        return 'over {} days ago'.format(delta.days)\n    return '{date:%b} {date.day}, {date.year}'.format(date=event)\n", "label": 1}
{"function": "\n\ndef _convert_relation(self, prop, kwargs):\n    form_columns = getattr(self.view, 'form_columns', None)\n    if (form_columns and (prop.key not in form_columns)):\n        return None\n    remote_model = prop.mapper.class_\n    column = prop.local_remote_pairs[0][0]\n    if (not column.foreign_keys):\n        column = prop.local_remote_pairs[0][1]\n    kwargs['label'] = self._get_label(prop.key, kwargs)\n    kwargs['description'] = self._get_description(prop.key, kwargs)\n    requirement_options = (validators.Optional, validators.InputRequired)\n    if (not any((isinstance(v, requirement_options) for v in kwargs['validators']))):\n        if (column.nullable or (prop.direction.name != 'MANYTOONE')):\n            kwargs['validators'].append(validators.Optional())\n        else:\n            kwargs['validators'].append(validators.InputRequired())\n    if ('allow_blank' not in kwargs):\n        kwargs['allow_blank'] = column.nullable\n    override = self._get_field_override(prop.key)\n    if override:\n        return override(**kwargs)\n    if ((prop.direction.name == 'MANYTOONE') or (not prop.uselist)):\n        return self._model_select_field(prop, False, remote_model, **kwargs)\n    elif (prop.direction.name == 'ONETOMANY'):\n        return self._model_select_field(prop, True, remote_model, **kwargs)\n    elif (prop.direction.name == 'MANYTOMANY'):\n        return self._model_select_field(prop, True, remote_model, **kwargs)\n", "label": 1}
{"function": "\n\ndef split_command_line(command_line):\n    \"This splits a command line into a list of arguments. It splits arguments\\n    on spaces, but handles embedded quotes, doublequotes, and escaped\\n    characters. It's impossible to do this with a regular expression, so I\\n    wrote a little state machine to parse the command line. \"\n    arg_list = []\n    arg = ''\n    state_basic = 0\n    state_esc = 1\n    state_singlequote = 2\n    state_doublequote = 3\n    state_whitespace = 4\n    state = state_basic\n    for c in command_line:\n        if ((state == state_basic) or (state == state_whitespace)):\n            if (c == '\\\\'):\n                state = state_esc\n            elif (c == \"'\"):\n                state = state_singlequote\n            elif (c == '\"'):\n                state = state_doublequote\n            elif c.isspace():\n                if (state == state_whitespace):\n                    None\n                else:\n                    arg_list.append(arg)\n                    arg = ''\n                    state = state_whitespace\n            else:\n                arg = (arg + c)\n                state = state_basic\n        elif (state == state_esc):\n            arg = (arg + c)\n            state = state_basic\n        elif (state == state_singlequote):\n            if (c == \"'\"):\n                state = state_basic\n            else:\n                arg = (arg + c)\n        elif (state == state_doublequote):\n            if (c == '\"'):\n                state = state_basic\n            else:\n                arg = (arg + c)\n    if (arg != ''):\n        arg_list.append(arg)\n    return arg_list\n", "label": 1}
{"function": "\n\ndef binomial_pdf(x, a, b):\n    'binomial PDF by H. Gene Shin\\n    \\n    '\n    if (a < 1):\n        return 0\n    elif ((x < 0) or (a < x)):\n        return 0\n    elif (b == 0):\n        if (x == 0):\n            return 1\n        else:\n            return 0\n    elif (b == 1):\n        if (x == a):\n            return 1\n        else:\n            return 0\n    else:\n        if (x > (a - x)):\n            p = (1 - b)\n            mn = (a - x)\n            mx = x\n        else:\n            p = b\n            mn = x\n            mx = (a - x)\n        pdf = 1\n        t = 0\n        for q in xrange(1, (mn + 1)):\n            pdf *= ((((a - q) + 1) * p) / ((mn - q) + 1))\n            if (pdf < 1e-100):\n                while (pdf < 0.001):\n                    pdf /= (1 - p)\n                    t -= 1\n            if (pdf > 1e+100):\n                while ((pdf > 1000.0) and (t < mx)):\n                    pdf *= (1 - p)\n                    t += 1\n        for i in xrange((mx - t)):\n            pdf *= (1 - p)\n        pdf = float(('%.10e' % pdf))\n        return pdf\n", "label": 1}
{"function": "\n\ndef parse_time(val):\n    if (not val):\n        return None\n    hr = mi = 0\n    val = val.lower()\n    amflag = ((- 1) != val.find('a'))\n    pmflag = ((- 1) != val.find('p'))\n    for noise in ':amp.':\n        val = val.replace(noise, ' ')\n    val = val.split()\n    if (len(val) > 1):\n        hr = int(val[0])\n        mi = int(val[1])\n    else:\n        val = val[0]\n        if (len(val) < 1):\n            pass\n        elif ('now' == val):\n            tm = localtime()\n            hr = tm[3]\n            mi = tm[4]\n        elif ('noon' == val):\n            hr = 12\n        elif (len(val) < 3):\n            hr = int(val)\n            if ((not amflag) and (not pmflag) and (hr < 7)):\n                hr += 12\n        elif (len(val) < 5):\n            hr = int(val[:(- 2)])\n            mi = int(val[(- 2):])\n        else:\n            hr = int(val[:1])\n    if (amflag and (hr >= 12)):\n        hr = (hr - 12)\n    if (pmflag and (hr < 12)):\n        hr = (hr + 12)\n    return time(hr, mi)\n", "label": 1}
{"function": "\n\ndef resolve_job_references(io_hash, job_outputs, should_resolve=True):\n    '\\n    :param io_hash: an input or output hash in which to resolve any job-based object references possible\\n    :type io_hash: dict\\n    :param job_outputs: a mapping of finished local jobs to their output hashes\\n    :type job_outputs: dict\\n    :param should_resolve: whether it is an error if a job-based object reference in *io_hash* cannot be resolved yet\\n    :type should_resolve: boolean\\n\\n    Modifies *io_hash* in-place.\\n    '\n    q = []\n    for field in io_hash:\n        if is_job_ref(io_hash[field]):\n            io_hash[field] = resolve_job_ref(io_hash[field], job_outputs, should_resolve)\n        elif (isinstance(io_hash[field], list) or isinstance(io_hash[field], dict)):\n            q.append(io_hash[field])\n    while (len(q) > 0):\n        thing = q.pop()\n        if isinstance(thing, list):\n            for i in range(len(thing)):\n                if is_job_ref(thing[i]):\n                    thing[i] = resolve_job_ref(thing[i], job_outputs, should_resolve)\n                elif (isinstance(thing[i], list) or isinstance(thing[i], dict)):\n                    q.append(thing[i])\n        else:\n            for field in thing:\n                if is_job_ref(thing[field]):\n                    thing[field] = resolve_job_ref(thing[field], job_outputs, should_resolve)\n                elif (isinstance(thing[field], list) or isinstance(thing[field], dict)):\n                    q.append(thing[field])\n", "label": 1}
{"function": "\n\ndef keypress(self, size, key):\n    command = self._command_map[key]\n    if (key == ']'):\n        self.shift_order((+ 1))\n        return True\n    elif (key == '['):\n        self.shift_order((- 1))\n        return True\n    elif (key == '>'):\n        self.focus_hotspot(size)\n        return True\n    elif (key == '\\\\'):\n        layout = {\n            FLAT: NESTED,\n            NESTED: FLAT,\n        }[self.layout]\n        self.set_layout(layout)\n        return True\n    command = self._command_map[key]\n    if (command == 'menu'):\n        self.defocus()\n        return True\n    elif (command == urwid.CURSOR_RIGHT):\n        if (self.layout == FLAT):\n            return True\n        (widget, node) = self.tbody.get_focus()\n        if widget.expanded:\n            heavy_widget = widget.first_child()\n            if (heavy_widget is not None):\n                heavy_node = heavy_widget.get_node()\n                self.tbody.change_focus(size, heavy_node)\n            return True\n    elif (command == urwid.CURSOR_LEFT):\n        if (self.layout == FLAT):\n            return True\n        (widget, node) = self.tbody.get_focus()\n        if (not widget.expanded):\n            parent_node = node.get_parent()\n            if (not parent_node.is_root()):\n                self.tbody.change_focus(size, parent_node)\n            return True\n    elif (command == urwid.ACTIVATE):\n        if self.viewer.paused:\n            self.viewer.resume()\n        else:\n            self.viewer.pause()\n        return True\n    return super(StatisticsTable, self).keypress(size, key)\n", "label": 1}
{"function": "\n\ndef _analyze(self):\n    ' works out the updates to be performed '\n    if ((self.value is None) or (self.value == self.previous)):\n        pass\n    elif (self._operation == 'append'):\n        self._append = self.value\n    elif (self._operation == 'prepend'):\n        self._prepend = self.value\n    elif (self.previous is None):\n        self._assignments = self.value\n    elif (len(self.value) < len(self.previous)):\n        self._assignments = self.value\n    elif (len(self.previous) == 0):\n        self._assignments = self.value\n    else:\n        search_space = (len(self.value) - max(0, (len(self.previous) - 1)))\n        search_size = len(self.previous)\n        for i in range(search_space):\n            j = (i + search_size)\n            sub = self.value[i:j]\n            idx_cmp = (lambda idx: (self.previous[idx] == sub[idx]))\n            if (idx_cmp(0) and idx_cmp((- 1)) and (self.previous == sub)):\n                self._prepend = (self.value[:i] or None)\n                self._append = (self.value[j:] or None)\n                break\n        if (self._prepend is self._append is None):\n            self._assignments = self.value\n    self._analyzed = True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef obtainSystemConstants():\n    lines = filter(None, map(str.strip, subprocess.check_output(['qhost']).split('\\n')))\n    line = lines[0]\n    items = line.strip().split()\n    num_columns = len(items)\n    cpu_index = None\n    mem_index = None\n    for i in range(num_columns):\n        if (items[i] == 'NCPU'):\n            cpu_index = i\n        elif (items[i] == 'MEMTOT'):\n            mem_index = i\n    if ((cpu_index is None) or (mem_index is None)):\n        RuntimeError('qhost command does not return NCPU or MEMTOT columns')\n    maxCPU = 0\n    maxMEM = MemoryString('0')\n    for line in lines[2:]:\n        items = line.strip().split()\n        if (len(items) < num_columns):\n            RuntimeError('qhost output has a varying number of columns')\n        if ((items[cpu_index] != '-') and (items[cpu_index] > maxCPU)):\n            maxCPU = items[cpu_index]\n        if ((items[mem_index] != '-') and (MemoryString(items[mem_index]) > maxMEM)):\n            maxMEM = MemoryString(items[mem_index])\n    if ((maxCPU is 0) or (maxMEM is 0)):\n        RuntimeError('qhost returned null NCPU or MEMTOT info')\n    return (maxCPU, maxMEM)\n", "label": 1}
{"function": "\n\ndef test_reset_late(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, _time_function=time)\n    time.time = 13\n    assert (timer.sleep_time() == 0)\n    still_alive = timer.run()\n    assert (not still_alive)\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == Decimal('inf'))\n    timer.reset()\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 10)\n    time.time = 21\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 2)\n    time.time = 30\n    assert (timer.sleep_time() == 0)\n    still_alive = timer.run()\n    assert (not still_alive)\n    assert (callback.nb_calls == 2)\n    assert (timer.sleep_time() == Decimal('inf'))\n", "label": 1}
{"function": "\n\ndef _fetch_objects(self, doc_type=None):\n    'Fetch all references and convert to their document objects\\n        '\n    object_map = {\n        \n    }\n    for (collection, dbrefs) in self.reference_map.iteritems():\n        if hasattr(collection, 'objects'):\n            col_name = collection._get_collection_name()\n            refs = [dbref for dbref in dbrefs if ((col_name, dbref) not in object_map)]\n            references = collection.objects.in_bulk(refs)\n            for (key, doc) in references.iteritems():\n                object_map[(col_name, key)] = doc\n        else:\n            if isinstance(doc_type, (ListField, DictField, MapField)):\n                continue\n            refs = [dbref for dbref in dbrefs if ((collection, dbref) not in object_map)]\n            if doc_type:\n                references = doc_type._get_db()[collection].find({\n                    '_id': {\n                        '$in': refs,\n                    },\n                })\n                for ref in references:\n                    doc = doc_type._from_son(ref)\n                    object_map[(collection, doc.id)] = doc\n            else:\n                references = get_db()[collection].find({\n                    '_id': {\n                        '$in': refs,\n                    },\n                })\n                for ref in references:\n                    if ('_cls' in ref):\n                        doc = get_document(ref['_cls'])._from_son(ref)\n                    elif (doc_type is None):\n                        doc = get_document(''.join((x.capitalize() for x in collection.split('_'))))._from_son(ref)\n                    else:\n                        doc = doc_type._from_son(ref)\n                    object_map[(collection, doc.id)] = doc\n    return object_map\n", "label": 1}
{"function": "\n\ndef _generate_events(self, event):\n    try:\n        if (not any([self._read, self._write])):\n            return\n        timeout = event.time_left\n        if (timeout < 0):\n            (r, w, _) = select.select(self._read, self._write, [])\n        else:\n            (r, w, _) = select.select(self._read, self._write, [], timeout)\n    except ValueError as e:\n        return self._preenDescriptors()\n    except TypeError as e:\n        return self._preenDescriptors()\n    except (SelectError, SocketError, IOError) as e:\n        if (e.args[0] in (0, 2)):\n            if ((not self._read) and (not self._write)):\n                return\n            else:\n                raise\n        elif (e.args[0] == EINTR):\n            return\n        elif (e.args[0] == EBADF):\n            return self._preenDescriptors()\n        else:\n            raise\n    for sock in w:\n        if self.isWriting(sock):\n            self.fire(_write(sock), self.getTarget(sock))\n    for sock in r:\n        if (sock == self._ctrl_recv):\n            self._read_ctrl()\n            continue\n        if self.isReading(sock):\n            self.fire(_read(sock), self.getTarget(sock))\n", "label": 1}
{"function": "\n\ndef checkpins(contents, designator, errs):\n    pins = re_pins.findall(contents)\n    nums = []\n    for (name, num, x, y, length, numsize, namesize) in pins:\n        if (((int(x) % 100) != 0) or ((int(y) % 100) != 0)):\n            errs.append(\"Pin '{}' not on 100mil grid\".format(name))\n        if ((designator in ('IC', 'U')) and (int(length) not in (100, 150))):\n            errs.append(\"Pin '{}' not 100 or 150mil long, but part is IC or U\".format(name))\n        if ((int(namesize) != 50) or ((int(numsize) != 50) and num.isdigit())):\n            errs.append(\"Pin '{}' font size not 50mil\".format(name))\n        if num.isdigit():\n            nums.append(int(num))\n    if nums:\n        expected = set(range(min(nums), (max(nums) + 1)))\n        if (set(nums) != expected):\n            missing = [str(x) for x in (set(expected) - set(nums))]\n            errs.append('Missing pins {}'.format(', '.join(missing)))\n        duplicates = set([str(x) for x in nums if (nums.count(x) > 1)])\n        if duplicates:\n            errs.append('Duplicated pins {}'.format(', '.join(duplicates)))\n", "label": 1}
{"function": "\n\ndef __init__(self, kernel, it_space, *args, **kwargs):\n    read_args = [a.data for a in args if (a.access in [READ, RW])]\n    written_args = [a.data for a in args if (a.access in [RW, WRITE, MIN, MAX, INC])]\n    inc_args = [a.data for a in args if (a.access in [INC])]\n    LazyComputation.__init__(self, (set(read_args) | Const._defs), set(written_args), set(inc_args))\n    self._kernel = kernel\n    self._actual_args = args\n    self._it_space = it_space\n    for (i, arg) in enumerate(self._actual_args):\n        arg.position = i\n        arg.indirect_position = i\n    for (i, arg1) in enumerate(self._actual_args):\n        if (arg1._is_dat and arg1._is_indirect):\n            for arg2 in self._actual_args[i:]:\n                if ((arg2.data is arg1.data) and (arg2.map is arg1.map)):\n                    arg2.indirect_position = arg1.indirect_position\n    self._all_args = kwargs.get('all_args', [args])\n    self._inspection = kwargs.get('inspection')\n    self._executor = kwargs.get('executor')\n", "label": 1}
{"function": "\n\ndef _click_autocomplete(root, text):\n    'Completer generator for click applications.'\n    try:\n        parts = shlex.split(text)\n    except ValueError:\n        raise StopIteration\n    (location, incomplete) = _click_resolve_command(root, parts)\n    if ((not text.endswith(' ')) and (not incomplete) and text):\n        raise StopIteration\n    if (incomplete and (not incomplete[0:2].isalnum())):\n        for param in location.params:\n            if (not isinstance(param, click.Option)):\n                continue\n            for opt in itertools.chain(param.opts, param.secondary_opts):\n                if opt.startswith(incomplete):\n                    (yield completion.Completion(opt, (- len(incomplete)), display_meta=param.help))\n    elif isinstance(location, (click.MultiCommand, click.core.Group)):\n        ctx = click.Context(location)\n        commands = location.list_commands(ctx)\n        for command in commands:\n            if command.startswith(incomplete):\n                cmd = location.get_command(ctx, command)\n                (yield completion.Completion(command, (- len(incomplete)), display_meta=cmd.short_help))\n", "label": 1}
{"function": "\n\ndef print_tweet(tweet, settings):\n    'Format and print the tweet dict.\\n\\n    Returns:\\n        boolean status of if the tweet was printed\\n    '\n    tweet_text = tweet.get('text')\n    if ((tweet_text is None) or ((not settings['spam']) and AntiSpam.is_spam(tweet_text))):\n        return False\n    if (sys.version_info[0] == 2):\n        for encoding in ['utf-8', 'latin-1']:\n            try:\n                tweet_text.decode(encoding)\n            except UnicodeEncodeError:\n                pass\n            else:\n                break\n        else:\n            return False\n    if settings.get('json'):\n        print(json.dumps(tweet, indent=4, sort_keys=True))\n    else:\n        prepend = []\n        if (settings.get('date') or settings.get('time')):\n            date = parse_date(tweet['created_at'])\n            if settings.get('date'):\n                prepend.append('{0:%b} {1}'.format(date, int(datetime.strftime(date, '%d'))))\n            if settings.get('time'):\n                prepend.append('{0:%H}:{0:%M}:{0:%S}'.format(date))\n        tweet = '{0}{1}@{2}: {3}'.format(' '.join(prepend), (' ' * int((prepend != []))), tweet.get('user', {\n            \n        }).get('screen_name', ''), unescape(tweet_text))\n        print(highlight_tweet(tweet))\n    return True\n", "label": 1}
{"function": "\n\ndef visit_statement(self, tokens):\n    tables = set()\n    if self._tables:\n        table_tokens = self._GetDescendants(tokens, 'table', do_not_cross=('exclude',))\n        table_aliases = self._GetTableAliases(tokens)\n        for table in table_tokens:\n            table_name = str(table[0])\n            if (table_name in table_aliases):\n                table_name = str(table_aliases[table_name]['table'][0])\n            tables.add(table_name)\n        if (not (self._tables & tables)):\n            return\n    columns = set()\n    if self._columns:\n        columns = set((str(x[0]).lower() for x in self._GetDescendants(tokens, 'column', do_not_cross=('exclude',))))\n        if (not (self._columns & columns)):\n            return\n    values = set()\n    if self._values:\n        values = set((str(x[0]) for x in self._GetDescendants(tokens, 'val', do_not_cross=('exclude',))))\n        if (not (self._values & values)):\n            return\n    operations = set()\n    if self._operations:\n        for operation in self._operations:\n            if self._GetDescendants(tokens, operation):\n                operations.add(operation)\n        if (not operations):\n            return\n    msg = (self._msg % {\n        'tables': tables,\n        'columns': columns,\n        'values': values,\n        'operations': operations,\n    })\n    self.AddWarning(tokens, msg)\n", "label": 1}
{"function": "\n\ndef assert_tables_equal(self, table, reflected_table, strict_types=False):\n    assert (len(table.c) == len(reflected_table.c))\n    for (c, reflected_c) in zip(table.c, reflected_table.c):\n        eq_(c.name, reflected_c.name)\n        assert (reflected_c is reflected_table.c[c.name])\n        eq_(c.primary_key, reflected_c.primary_key)\n        eq_(c.nullable, reflected_c.nullable)\n        if strict_types:\n            msg = \"Type '%s' doesn't correspond to type '%s'\"\n            assert isinstance(reflected_c.type, type(c.type)), (msg % (reflected_c.type, c.type))\n        else:\n            self.assert_types_base(reflected_c, c)\n        if isinstance(c.type, sqltypes.String):\n            eq_(c.type.length, reflected_c.type.length)\n        eq_(set([f.column.name for f in c.foreign_keys]), set([f.column.name for f in reflected_c.foreign_keys]))\n        if c.server_default:\n            assert isinstance(reflected_c.server_default, schema.FetchedValue)\n    assert (len(table.primary_key) == len(reflected_table.primary_key))\n    for c in table.primary_key:\n        assert (reflected_table.primary_key.columns[c.name] is not None)\n", "label": 1}
{"function": "\n\ndef main():\n    'Entry point for this script.'\n    (parser, args) = parse_command_line_arguments()\n    logger = initialize_logging(args.debug, args.less_verbose)\n    result = 0\n    if args.download:\n        try:\n            download_listing(args.listing)\n        except DownloadRetfListingFailed as ex:\n            logger.error('Downloading latest RETF listing failed: %s.', ex)\n            result = 1\n    if ((not args.path) and (not args.file) and (not args.download)):\n        parser.print_help()\n        result = 2\n    if ((not result) and (not os.path.isfile(args.listing))):\n        logger.error('RETF listing not found at %s.', args.listing)\n        logger.info('Please download the RETF listing first by using the parameter --download.')\n        result = 1\n    if (not result):\n        files = get_file_listing(args.path, args.file, args.extension)\n        rules = generate_listing(args.listing)\n        disabled = load_disabled_rules(args.disabled)\n        all_findings = 0\n        for check in files:\n            if (not os.path.isfile(check)):\n                continue\n            (findings, content) = check_file(check, rules, disabled)\n            if (findings > 0):\n                all_findings += findings\n                logger.warning('%s finding(s) in file %s.', findings, check)\n            if ((findings > 0) and args.write_changes):\n                write_text_to_file(check, content, args.no_backup, args.in_place)\n        if (all_findings > 0):\n            logger.warning('%s finding(s) in all checked files.', all_findings)\n            result = 1\n    return result\n", "label": 1}
{"function": "\n\ndef get_fields(self):\n    to_update = False\n    doc = inspect.getdoc(self.method)\n    if ((not doc) and issubclass(self.method.im_class.model, models.Model)):\n        fields = None\n        if (self.method.__name__ == 'read'):\n            fields = (self.method.im_class.fields if self.method.im_class.fields else tuple((attr.name for attr in self.method.im_class.model._meta.local_fields)))\n        elif (self.method.__name__ in ('create', 'update')):\n            to_update = True\n            if (hasattr(self.method.im_class, 'form') and hasattr(self.method.im_class.form, '_meta')):\n                fields = self.method.im_class.form._meta.fields\n            else:\n                fields = self.method.im_class.fields\n        if fields:\n            for field in fields:\n                for mfield in self.method.im_class.model._meta.fields:\n                    if (mfield.name == field):\n                        (yield {\n                            'name': mfield.name,\n                            'required': ((not mfield.blank) if to_update else False),\n                            'type': get_field_data_type(mfield),\n                            'verbose': mfield.verbose_name,\n                            'help_text': mfield.help_text,\n                        })\n                        break\n", "label": 1}
{"function": "\n\ndef _parse(stream, ptr=0):\n    i = ptr\n    laststr = None\n    lasttok = None\n    deserialized = {\n        \n    }\n    while (i < len(stream)):\n        c = stream[i]\n        if (c == STRING):\n            (string, i) = _symtostr(stream, i)\n            if (lasttok == STRING):\n                deserialized[laststr] = string\n            laststr = string\n        elif (c == NODE_OPEN):\n            (deserialized[laststr], i) = _parse(stream, (i + 1))\n        elif (c == NODE_CLOSE):\n            return (deserialized, i)\n        elif (c == COMMENT):\n            if (((i + 1) < len(stream)) and (stream[(i + 1)] == '/')):\n                i = stream.find('\\n', i)\n        elif ((c == CR) or (c == LF)):\n            ni = (i + 1)\n            if ((ni < len(stream)) and (stream[ni] == LF)):\n                i = ni\n            if (lasttok != LF):\n                c = LF\n        else:\n            c = lasttok\n        lasttok = c\n        i += 1\n    return (deserialized, i)\n", "label": 1}
{"function": "\n\ndef do_show(self, *args):\n    if 'running-config'.startswith(args[0]):\n        if 'vlan'.startswith(args[1]):\n            self.show_run_vlan()\n        if 'interface'.startswith(args[1]):\n            self.show_run_int(args)\n    elif 'interfaces'.startswith(args[0]):\n        self.show_int(args)\n    elif 'vlan'.startswith(args[0]):\n        if args[1].isdigit():\n            self._show_vlan(int(args[1]))\n        elif 'brief'.startswith(args[1]):\n            self.show_vlan_brief()\n        elif 'ethernet'.startswith(args[1]):\n            self.show_vlan_int(args)\n        else:\n            self.write_line(('Invalid input -> %s' % args[1]))\n            self.write_line('Type ? for a list')\n    elif ('ip'.startswith(args[0]) and 'route'.startswith(args[1]) and 'static'.startswith(args[2])):\n        routes = self.switch_configuration.static_routes\n        if routes:\n            self.write_line('        Destination        Gateway        Port          Cost          Type Uptime src-vrf')\n        for (n, route) in enumerate(routes):\n            self.write_line('{index:<8}{destination:<18} {next_hop:}'.format(index=(n + 1), destination=route.dest, next_hop=route.next_hop))\n        self.write_line('')\n    elif 'version'.startswith(args[0]):\n        self.show_version()\n", "label": 1}
{"function": "\n\ndef check(self, app, sha, config):\n    token = current_app.config['GITHUB_TOKEN']\n    if (not token):\n        raise CheckFailed('GITHUB_TOKEN is not set')\n    api_root = (config.get('api_root') or current_app.config['GITHUB_API_ROOT']).rstrip('/')\n    contexts = set((config.get('contexts') or []))\n    repo = config['repo']\n    url = '{api_root}/repos/{repo}/commits/{ref}/statuses'.format(api_root=api_root, repo=repo, ref=sha)\n    headers = {\n        'Accepts': 'application/json',\n        'Authorization': 'token {}'.format(token),\n    }\n    resp = http.get(url, headers=headers)\n    context_list = resp.json()\n    if (not context_list):\n        raise CheckFailed('No contexts were present in GitHub')\n    valid_contexts = set()\n    for data in context_list:\n        if (data['state'] == 'success'):\n            valid_contexts.add(data['context'])\n            try:\n                contexts.remove(data['context'])\n            except KeyError:\n                pass\n        if (data['context'] in valid_contexts):\n            continue\n        if (contexts and (data['context'] not in contexts)):\n            continue\n        if (data['state'] == 'pending'):\n            raise CheckPending(ERR_CHECK.format(data['context'], data['state']))\n        elif (data['state'] != 'success'):\n            raise CheckFailed(ERR_CHECK.format(data['context'], data['state']))\n        contexts.remove(data['context'])\n    if contexts:\n        raise CheckFailed(ERR_MISSING_CONTEXT.format(iter(contexts).next()))\n", "label": 1}
{"function": "\n\ndef resumeProducing(self):\n    self.paused = False\n    if self._buffer:\n        data = ''.join(self._buffer)\n        bytesSent = self._writeSomeData(data)\n        if (bytesSent < len(data)):\n            unsent = data[bytesSent:]\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer[:] = [unsent]\n        else:\n            self._buffer[:] = []\n    else:\n        bytesSent = 0\n    if (self.unregistered and bytesSent and (not self._buffer) and (self.consumer is not None)):\n        self.consumer.unregisterProducer()\n    if (not self.iAmStreaming):\n        self.outstandingPull = (not bytesSent)\n    if (self.producer is not None):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (self.producerPaused and (bytesBuffered < self.bufferSize)):\n            self.producerPaused = False\n            self.producer.resumeProducing()\n        elif self.outstandingPull:\n            self.producer.resumeProducing()\n", "label": 1}
{"function": "\n\ndef post(self, *args, **kwargs):\n    widget = self.object\n    ordering = self.kwargs.get('ordering')\n    if (int(ordering) == 0):\n        widget.ordering = 0\n        widget.save()\n        widgets = getattr(widget.parent.content, widget.region)\n        widgets = [w for w in widgets if (w.id != widget.id)]\n        for (i, _widget) in enumerate(widgets):\n            _widget.ordering = (i + 1)\n            _widget.save()\n    elif (int(ordering) == (- 1)):\n        next_ordering = (widget.ordering - 1)\n        widgets = getattr(widget.parent.content, widget.region)\n        for w in widgets:\n            if (w.ordering == next_ordering):\n                w.ordering = widget.ordering\n                w.save()\n                widget.ordering = next_ordering\n                widget.save()\n    elif (int(ordering) == 1):\n        next_ordering = (widget.ordering + 1)\n        widgets = getattr(widget.parent.content, widget.region)\n        for w in widgets:\n            if (w.ordering == next_ordering):\n                w.ordering = widget.ordering\n                w.save()\n                widget.ordering = next_ordering\n                widget.save()\n    else:\n        widget.ordering = widget.next_ordering\n        widget.save()\n        widgets = getattr(widget.parent.content, widget.region)\n        widgets = [w for w in widgets if (w.id != widget.id)]\n        widgets.sort(key=(lambda w: w.ordering))\n        for (i, _widget) in enumerate(widgets):\n            _widget.ordering = i\n            _widget.save()\n    messages.success(self.request, _('Widget was successfully moved.'))\n    success_url = self.get_success_url()\n    response = HttpResponseRedirect(success_url)\n    response['X-Horizon-Location'] = success_url\n    return response\n", "label": 1}
{"function": "\n\ndef parse(self):\n    '\\n    Parse the vmstat file\\n    :return: status of the metric parse\\n    '\n    file_status = True\n    for input_file in self.infile_list:\n        file_status = (file_status and naarad.utils.is_valid_file(input_file))\n    if (not file_status):\n        return False\n    status = True\n    data = {\n        \n    }\n    for input_file in self.infile_list:\n        logger.info('Processing : %s', input_file)\n        timestamp_format = None\n        with open(input_file) as fh:\n            for line in fh:\n                words = line.split()\n                if (len(words) < 3):\n                    continue\n                ts = ((words[0] + ' ') + words[1])\n                if ((not timestamp_format) or (timestamp_format == 'unknown')):\n                    timestamp_format = naarad.utils.detect_timestamp_format(ts)\n                if (timestamp_format == 'unknown'):\n                    continue\n                ts = naarad.utils.get_standardized_timestamp(ts, timestamp_format)\n                if self.ts_out_of_range(ts):\n                    continue\n                col = words[2]\n                if (self.sub_metrics and (col not in self.sub_metrics)):\n                    continue\n                self.sub_metric_unit[col] = 'pages'\n                if (col in self.column_csv_map):\n                    out_csv = self.column_csv_map[col]\n                else:\n                    out_csv = self.get_csv(col)\n                    data[out_csv] = []\n                data[out_csv].append(((ts + ',') + words[3]))\n    for csv in data.keys():\n        self.csv_files.append(csv)\n        with open(csv, 'w') as fh:\n            fh.write('\\n'.join(data[csv]))\n    return status\n", "label": 1}
{"function": "\n\ndef onColor(self, event=None, item=None):\n    color = hexcolor(event.GetValue())\n    setattr(self.parent.conf, item, color)\n    if (item == 'spectra_color'):\n        self.parent.panel.conf.set_trace_color(color, trace=0)\n    elif (item == 'roi_color'):\n        self.parent.panel.conf.set_trace_color(color, trace=1)\n    elif (item == 'marker_color'):\n        for lmark in self.parent.cursor_markers:\n            if (lmark is not None):\n                lmark.set_color(color)\n    elif ((item == 'roi_fillcolor') and (self.parent.roi_patch is not None)):\n        self.parent.roi_patch.set_color(color)\n    elif (item == 'major_elinecolor'):\n        for l in self.parent.major_markers:\n            l.set_color(color)\n    elif (item == 'minor_elinecolor'):\n        for l in self.parent.minor_markers:\n            l.set_color(color)\n    elif (item == 'hold_elinecolor'):\n        for l in self.parent.hold_markers:\n            l.set_color(color)\n    self.parent.panel.canvas.draw()\n    self.parent.panel.Refresh()\n", "label": 1}
{"function": "\n\ndef get_filters(self):\n    is_public = public_filter()\n    is_active = active_filter()\n    is_datetime = datetime_filter(self.filter_yaml.get_image_date())\n    is_tenant = tenant_filter(self.filter_yaml.get_tenant())\n    images_list = self.filter_yaml.get_image_ids()\n    excluded_images_list = self.filter_yaml.get_excluded_image_ids()\n    if (images_list and excluded_images_list):\n        raise exception.AbortMigrationError(\"In the filter config file specified 'images_list' and 'exclude_images_list'. Must be only one list with images - 'images_list' or 'exclude_images_list'.\")\n    if excluded_images_list:\n        is_image_id = image_id_exclude_filter(excluded_images_list)\n    else:\n        is_image_id = image_id_filter(images_list)\n    is_member = member_filter(self.glance_client, self.filter_yaml.get_tenant())\n    if self.filter_yaml.is_public_and_member_images_filtered():\n        return [(lambda i: (is_active(i) and is_tenant(i) and is_image_id(i) and is_datetime(i)))]\n    else:\n        return [(lambda i: ((is_active(i) and is_public(i)) or (is_active(i) and is_member(i)) or (is_active(i) and is_tenant(i) and is_image_id(i) and is_datetime(i))))]\n", "label": 1}
{"function": "\n\ndef delete_file(self, path, prefixed_path, source_storage, **options):\n    symlink = options['link']\n    if self.storage.exists(prefixed_path):\n        try:\n            target_last_modified = self.storage.modified_time(prefixed_path)\n        except (OSError, NotImplementedError):\n            pass\n        else:\n            try:\n                source_last_modified = source_storage.modified_time(path)\n            except (OSError, NotImplementedError):\n                pass\n            else:\n                if self.local:\n                    full_path = self.storage.path(prefixed_path)\n                else:\n                    full_path = None\n                if (target_last_modified >= source_last_modified):\n                    if (not ((symlink and full_path and (not os.path.islink(full_path))) or ((not symlink) and full_path and os.path.islink(full_path)))):\n                        if (prefixed_path not in self.unmodified_files):\n                            self.unmodified_files.append(prefixed_path)\n                        self.log((\"Skipping '%s' (not modified)\" % path))\n                        return False\n        if options['dry_run']:\n            self.log((\"Pretending to delete '%s'\" % path))\n        else:\n            self.log((\"Deleting '%s'\" % path))\n            self.storage.delete(prefixed_path)\n    return True\n", "label": 1}
{"function": "\n\ndef image_vacuum(name):\n    '\\n    Delete images not in use or installed via image_present\\n    '\n    name = name.lower()\n    ret = {\n        'name': name,\n        'changes': {\n            \n        },\n        'result': None,\n        'comment': '',\n    }\n    images = []\n    for state in __salt__['state.show_lowstate']():\n        if ('state' not in state):\n            continue\n        if (state['state'] != __virtualname__):\n            continue\n        if (state['fun'] not in ['image_present']):\n            continue\n        if ('name' in state):\n            images.append(state['name'])\n    for image_uuid in __salt__['vmadm.list'](order='image_uuid'):\n        if (image_uuid in images):\n            continue\n        images.append(image_uuid)\n    ret['result'] = True\n    for image_uuid in __salt__['imgadm.list']():\n        if (image_uuid in images):\n            continue\n        if (image_uuid in __salt__['imgadm.delete'](image_uuid)):\n            ret['changes'][image_uuid] = None\n        else:\n            ret['result'] = False\n            ret['comment'] = 'failed to delete images'\n    if (ret['result'] and (len(ret['changes']) == 0)):\n        ret['comment'] = 'no images deleted'\n    elif (ret['result'] and (len(ret['changes']) > 0)):\n        ret['comment'] = 'images deleted'\n    return ret\n", "label": 1}
{"function": "\n\n@register_specialize\n@gof.local_optimizer([T.mul, T.true_div])\ndef local_abs_merge(node):\n    \"\\n    Merge abs generated by local_abs_lift when the canonizer don't\\n    need it anymore\\n\\n    \"\n    if ((node.op == T.mul) and (sum([(i.owner.op == T.abs_) for i in node.inputs if i.owner]) > 1)):\n        inputs = []\n        for i in node.inputs:\n            if (i.owner and (i.owner.op == T.abs_)):\n                inputs.append(i.owner.inputs[0])\n            elif isinstance(i, Constant):\n                try:\n                    const = get_scalar_constant_value(i)\n                except NotScalarConstantError:\n                    return False\n                if (not (const >= 0).all()):\n                    return False\n                inputs.append(i)\n            else:\n                return False\n        return [T.abs_(T.mul(*inputs))]\n    if ((node.op == T.true_div) and (sum([(i.owner.op == T.abs_) for i in node.inputs if i.owner]) == 2)):\n        return [T.abs_(T.true_div(node.inputs[0].owner.inputs[0], node.inputs[1].owner.inputs[0]))]\n", "label": 1}
{"function": "\n\ndef visit_Task(self, node):\n    self.push_module(None)\n    name = node.name\n    _task = task.Task(name)\n    statement = [self.visit(s) for s in node.statement]\n    body = []\n    for s in statement:\n        if isinstance(s, (tuple, list)):\n            for d in s:\n                if isinstance(d, vtypes.Input):\n                    t = _task.Input(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Reg):\n                    t = _task.Reg(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Integer):\n                    t = _task.Integer(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                else:\n                    body.append(s)\n        else:\n            body.append(s)\n    _task.Body(*body)\n    self.pop_module()\n    self.add_object(_task)\n    return _task\n", "label": 1}
{"function": "\n\ndef _update(self, frame_no):\n    if self._signal:\n        start_x = int(((self._screen.width - self._signal.max_width) // 2))\n        start_y = int(((self._screen.height - self._signal.max_height) // 2))\n        (text, colours) = self._signal.rendered_text\n    else:\n        start_x = start_y = 0\n        (text, colours) = ('', [])\n    for y in range(self._screen.height):\n        if (self._strength < 1.0):\n            offset = randint(0, int((6 - (6 * self._strength))))\n        else:\n            offset = 0\n        for x in range(self._screen.width):\n            ix = (x - start_x)\n            iy = (y - start_y)\n            if (self._signal and (random() <= self._strength) and (x >= start_x) and (y >= start_y) and (iy < len(text)) and ((ix + offset) < len(text[iy]))):\n                self._screen.paint(text[iy][(ix + offset)], x, y, colour_map=[colours[iy][ix]])\n            elif (random() < 0.2):\n                self._screen.print_at(chr(randint(33, 126)), x, y)\n    self._strength += self._step\n    if ((self._strength >= 1.25) or (self._strength <= (- 0.5))):\n        self._step = (- self._step)\n", "label": 1}
{"function": "\n\ndef test_complex_inverse_functions():\n    mp.dps = 15\n    iv.dps = 15\n    for (z1, z2) in random_complexes(30):\n        assert sinh(asinh(z1)).ae(z1)\n        assert acosh(z1).ae(cmath.acosh(z1))\n        assert atanh(z1).ae(cmath.atanh(z1))\n        assert atan(z1).ae(cmath.atan(z1))\n        assert asin(z1).ae(cmath.asin(z1), rel_eps=1e-12)\n        assert acos(z1).ae(cmath.acos(z1), rel_eps=1e-12)\n        one = mpf(1)\n    for i in range((- 9), 10, 3):\n        for k in range((- 9), 10, 3):\n            a = (((0.9 * j) * (10 ** k)) + ((0.8 * one) * (10 ** i)))\n            b = cos(acos(a))\n            assert b.ae(a)\n            b = sin(asin(a))\n            assert b.ae(a)\n    one = mpf(1)\n    err = (2 * (10 ** (- 15)))\n    for i in range((- 9), 9, 3):\n        for k in range((- 9), 9, 3):\n            a = (((- 0.9) * (10 ** k)) + (((j * 0.8) * one) * (10 ** i)))\n            b = cosh(acosh(a))\n            assert b.ae(a, err)\n            b = sinh(asinh(a))\n            assert b.ae(a, err)\n", "label": 1}
{"function": "\n\ndef test_get_memory_maps(self):\n    p = psutil.Process(os.getpid())\n    maps = p.get_memory_maps()\n    paths = [x for x in maps]\n    self.assertEqual(len(paths), len(set(paths)))\n    ext_maps = p.get_memory_maps(grouped=False)\n    for nt in maps:\n        if (not nt.path.startswith('[')):\n            assert os.path.isabs(nt.path), nt.path\n            if POSIX:\n                assert os.path.exists(nt.path), nt.path\n            elif ('64' not in os.path.basename(nt.path)):\n                assert os.path.exists(nt.path), nt.path\n    for nt in ext_maps:\n        for fname in nt._fields:\n            value = getattr(nt, fname)\n            if (fname == 'path'):\n                continue\n            elif (fname in ('addr', 'perms')):\n                assert value, value\n            else:\n                assert isinstance(value, (int, long))\n                assert (value >= 0), value\n", "label": 1}
{"function": "\n\ndef update(self):\n    '\\n        The function to draw a new frame for the particle system.\\n        '\n    if (self.time_left > 0):\n        self.time_left -= 1\n        for _ in range(self._count):\n            new_particle = self._new_particle()\n            if (new_particle is not None):\n                self.particles.append(new_particle)\n    for particle in self.particles:\n        last = particle.last()\n        if (last is not None):\n            (char, x, y, fg, attr, bg) = last\n            screen_data = self._screen.get_from(x, y)\n            if (self._blend and screen_data):\n                (char2, fg2, attr2, bg2) = screen_data\n                index = 0\n                for (i, colours) in enumerate(particle.colours):\n                    if ((fg2, attr2, bg2) == colours):\n                        index = i\n                        break\n                index -= 1\n                (fg, attr, bg) = particle.colours[max(index, 0)]\n            self._screen.print_at(' ', x, y, fg, attr, bg)\n        if (particle.time < particle.life_time):\n            (char, x, y, fg, attr, bg) = particle.next()\n            screen_data = self._screen.get_from(x, y)\n            if (self._blend and screen_data):\n                (char2, fg2, attr2, bg2) = screen_data\n                index = (- 1)\n                for (i, colours) in enumerate(particle.colours):\n                    if ((fg2, attr2, bg2) == colours):\n                        index = i\n                        break\n                index += 1\n                (fg, attr, bg) = particle.colours[min(index, (len(particle.colours) - 1))]\n            self._screen.print_at(char, x, y, fg, attr, bg)\n        else:\n            self.particles.remove(particle)\n", "label": 1}
{"function": "\n\ndef _skip_instance(self):\n    skip = self._buffer.read_bits(8)\n    if (skip == 0):\n        length = self._vint()\n        for i in xrange(length):\n            self._skip_instance()\n    elif (skip == 1):\n        length = self._vint()\n        self._buffer.read_aligned_bytes(((length + 7) / 8))\n    elif (skip == 2):\n        length = self._vint()\n        self._buffer.read_aligned_bytes(length)\n    elif (skip == 3):\n        tag = self._vint()\n        self._skip_instance()\n    elif (skip == 4):\n        exists = (self._buffer.read_bits(8) != 0)\n        if exists:\n            self._skip_instance()\n    elif (skip == 5):\n        length = self._vint()\n        for i in xrange(length):\n            tag = self._vint()\n            self._skip_instance()\n    elif (skip == 6):\n        self._buffer.read_aligned_bytes(1)\n    elif (skip == 7):\n        self._buffer.read_aligned_bytes(4)\n    elif (skip == 8):\n        self._buffer.read_aligned_bytes(8)\n    elif (skip == 9):\n        self._vint()\n", "label": 1}
{"function": "\n\ndef _non_dominated_front_fast(iterable, key=(lambda x: x), allowequality=True):\n    'Return a subset of items from iterable which are not dominated by any\\n    other item in iterable.\\n\\n    Faster version.\\n    '\n    items = list(iterable)\n    keys = dict(((i, key(i)) for i in items))\n    dim = len(list(keys.values())[0])\n    dominations = {\n        \n    }\n    for i in items:\n        for j in items:\n            good = True\n            if allowequality:\n                for k in range(dim):\n                    if (keys[i][k] >= keys[j][k]):\n                        good = False\n                        break\n            else:\n                for k in range(dim):\n                    if (keys[i][k] > keys[j][k]):\n                        good = False\n                        break\n            if good:\n                dominations[(i, j)] = None\n    res = set()\n    items = set(items)\n    for i in items:\n        res.add(i)\n        for j in list(res):\n            if (i is j):\n                continue\n            if ((j, i) in dominations):\n                res.remove(i)\n                break\n            elif ((i, j) in dominations):\n                res.remove(j)\n    return res\n", "label": 1}
{"function": "\n\ndef _validate_python(self, value, state):\n    if (not value):\n        raise Invalid(self.message('empty', state), value, state)\n    value = value.strip()\n    splitted = value.split('@', 1)\n    try:\n        (username, domain) = splitted\n    except ValueError:\n        raise Invalid(self.message('noAt', state), value, state)\n    if (not self.usernameRE.search(username)):\n        raise Invalid(self.message('badUsername', state, username=username), value, state)\n    try:\n        idna_domain = [idna.ToASCII(p) for p in domain.split('.')]\n        if (six.text_type is str):\n            idna_domain = [p.decode('ascii') for p in idna_domain]\n        idna_domain = '.'.join(idna_domain)\n    except UnicodeError:\n        raise Invalid(self.message('badDomain', state, domain=domain), value, state)\n    if (not self.domainRE.search(idna_domain)):\n        raise Invalid(self.message('badDomain', state, domain=domain), value, state)\n    if self.resolve_domain:\n        assert have_dns, 'dnspython should be available'\n        global socket\n        if (socket is None):\n            import socket\n        try:\n            try:\n                dns.resolver.query(domain, 'MX')\n            except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                try:\n                    dns.resolver.query(domain, 'A')\n                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                    raise Invalid(self.message('domainDoesNotExist', state, domain=domain), value, state)\n        except (socket.error, dns.exception.DNSException) as e:\n            raise Invalid(self.message('socketError', state, error=e), value, state)\n", "label": 1}
{"function": "\n\ndef transpile_md_to_python(markdown):\n    'A very naive markdown to python converter.'\n    for line in markdown.split('\\n'):\n        line = line.lstrip()\n        if line.startswith('# '):\n            (yield Heading(1, line, 'h1'))\n        elif line.startswith('## '):\n            (yield Heading(2, line, 'h2'))\n        elif line.startswith('### '):\n            (yield Heading(3, line, 'h3'))\n        elif line.startswith('#### '):\n            (yield Heading(4, line, 'h4'))\n        elif line.startswith('##### '):\n            (yield Heading(5, line, 'h5'))\n        elif line.startswith('###### '):\n            (yield Heading(6, line, 'h6'))\n        elif (line.startswith('*') and (not line.endswith('**'))):\n            (yield Tag('emphasis', line, 'em'))\n        elif (line.startswith('**') and line.endswith('**')):\n            (yield Tag('strong', line, 'strong'))\n        elif (line.startswith('[') and line.endswith(')')):\n            (yield Tag('href', line, 'a'))\n", "label": 1}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a datetime. Returns a\\n        Python datetime.datetime object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value\n    if isinstance(value, datetime.date):\n        return datetime.datetime(value.year, value.month, value.day)\n    if isinstance(value, list):\n        if (len(value) != 4):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES) and (value[2] in validators.EMPTY_VALUES) and (value[3] in validators.EMPTY_VALUES)):\n            return None\n        start_value = ('%s %s' % tuple(value[:2]))\n        end_value = ('%s %s' % tuple(value[2:]))\n    start_datetime = None\n    end_datetime = None\n    for format in (self.input_formats or formats.get_format('DATETIME_INPUT_FORMATS')):\n        try:\n            start_datetime = datetime.datetime(*time.strptime(start_value, format)[:6])\n        except ValueError:\n            continue\n    for format in (self.input_formats or formats.get_format('DATETIME_INPUT_FORMATS')):\n        try:\n            end_datetime = datetime.datetime(*time.strptime(end_value, format)[:6])\n        except ValueError:\n            continue\n    return (start_datetime, end_datetime)\n", "label": 1}
{"function": "\n\ndef test_lrucache(self):\n    c = LRUCache(2, dispose=(lambda _: None))\n    assert (len(c) == 0)\n    assert (c.items() == set())\n    for (i, x) in enumerate('abc'):\n        c[x] = i\n    assert (len(c) == 2)\n    assert (c.items() == set([('b', 1), ('c', 2)]))\n    assert ('a' not in c)\n    assert ('b' in c)\n    with pytest.raises(KeyError):\n        c['a']\n    assert (c['b'] == 1)\n    assert (c['c'] == 2)\n    c['d'] = 3\n    assert (len(c) == 2)\n    assert (c['c'] == 2)\n    assert (c['d'] == 3)\n    del c['c']\n    assert (len(c) == 1)\n    with pytest.raises(KeyError):\n        c['c']\n    assert (c['d'] == 3)\n    c.clear()\n    assert (c.items() == set())\n", "label": 1}
{"function": "\n\ndef test_disk_partitions(self):\n    for disk in psutil.disk_partitions(all=False):\n        assert os.path.exists(disk.device), disk\n        assert os.path.isdir(disk.mountpoint), disk\n        assert disk.fstype, disk\n        assert isinstance(disk.opts, str)\n    for disk in psutil.disk_partitions(all=True):\n        if (not WINDOWS):\n            try:\n                os.stat(disk.mountpoint)\n            except OSError:\n                err = sys.exc_info()[1]\n                if (err.errno not in (errno.EPERM, errno.EACCES)):\n                    raise\n            else:\n                assert os.path.isdir(disk.mountpoint), disk.mountpoint\n        assert isinstance(disk.fstype, str)\n        assert isinstance(disk.opts, str)\n\n    def find_mount_point(path):\n        path = os.path.abspath(path)\n        while (not os.path.ismount(path)):\n            path = os.path.dirname(path)\n        return path\n    mount = find_mount_point(__file__)\n    mounts = [x.mountpoint for x in psutil.disk_partitions(all=True)]\n    self.assertIn(mount, mounts)\n    psutil.disk_usage(mount)\n", "label": 1}
{"function": "\n\ndef visit_Function(self, node):\n    self.push_module(None)\n    name = node.name\n    width = (self.visit(node.retwidth) if (node.retwidth is not None) else None)\n    func = function.Function(name, width)\n    if (node.retwidth is not None):\n        func._set_raw_width(self.visit(node.retwidth.msb), self.visit(node.retwidth.lsb))\n    statement = [self.visit(s) for s in node.statement]\n    body = []\n    for s in statement:\n        if isinstance(s, (tuple, list)):\n            for d in s:\n                if isinstance(d, vtypes.Input):\n                    t = func.Input(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Reg):\n                    t = func.Reg(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Integer):\n                    t = func.Integer(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                else:\n                    body.append(s)\n        else:\n            body.append(s)\n    func.Body(*body)\n    self.pop_module()\n    self.add_object(func)\n    return func\n", "label": 1}
{"function": "\n\n@coroutine\ndef _wrap_awaitable(x):\n    if hasattr(x, '__await__'):\n        _i = x.__await__()\n    else:\n        _i = iter(x)\n    try:\n        _y = next(_i)\n    except StopIteration as _e:\n        _r = _value_from_stopiteration(_e)\n    else:\n        while 1:\n            try:\n                _s = (yield _y)\n            except GeneratorExit as _e:\n                try:\n                    _m = _i.close\n                except AttributeError:\n                    pass\n                else:\n                    _m()\n                raise _e\n            except BaseException as _e:\n                _x = sys.exc_info()\n                try:\n                    _m = _i.throw\n                except AttributeError:\n                    raise _e\n                else:\n                    try:\n                        _y = _m(*_x)\n                    except StopIteration as _e:\n                        _r = _value_from_stopiteration(_e)\n                        break\n            else:\n                try:\n                    if (_s is None):\n                        _y = next(_i)\n                    else:\n                        _y = _i.send(_s)\n                except StopIteration as _e:\n                    _r = _value_from_stopiteration(_e)\n                    break\n    raise Return(_r)\n", "label": 1}
{"function": "\n\ndef _caas_2_2_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_network_vlan_ALLFILTERS(self, method, url, body, headers):\n    (_, params) = url.split('?')\n    parameters = params.split('&')\n    for parameter in parameters:\n        (key, value) = parameter.split('=')\n        if (key == 'datacenterId'):\n            assert (value == 'fake_location')\n        elif (key == 'networkDomainId'):\n            assert (value == 'fake_network_domain')\n        elif (key == 'ipv6Address'):\n            assert (value == 'fake_ipv6')\n        elif (key == 'privateIpv4Address'):\n            assert (value == 'fake_ipv4')\n        elif (key == 'name'):\n            assert (value == 'fake_name')\n        elif (key == 'state'):\n            assert (value == 'fake_state')\n        else:\n            raise ValueError('Could not find in url parameters {0}:{1}'.format(key, value))\n    body = self.fixtures.load('network_vlan.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 1}
{"function": "\n\ndef _complete_authz(self, user, areq, sid, **kwargs):\n    _log_debug = logger.debug\n    _log_debug('- in authenticated() -')\n    try:\n        permission = self.authz(user, client_id=areq['client_id'])\n        self.sdb.update(sid, 'permission', permission)\n    except Exception:\n        raise\n    _log_debug(('response type: %s' % areq['response_type']))\n    if self.sdb.is_revoked(sid):\n        return self._error(error='access_denied', descr='Token is revoked')\n    info = self.create_authn_response(areq, sid)\n    if isinstance(info, Response):\n        return info\n    else:\n        (aresp, fragment_enc) = info\n    try:\n        redirect_uri = self.get_redirect_uri(areq)\n    except (RedirectURIError, ParameterError) as err:\n        return BadRequest(('%s' % err))\n    info = self.aresp_check(aresp, areq)\n    if isinstance(info, Response):\n        return info\n    headers = []\n    try:\n        _kaka = kwargs['cookie']\n    except KeyError:\n        pass\n    else:\n        if (_kaka and (self.cookie_name not in _kaka)):\n            headers.append(self.cookie_func(user, typ='sso', ttl=self.sso_ttl))\n    if ('response_mode' in areq):\n        try:\n            resp = self.response_mode(areq, fragment_enc, aresp=aresp, redirect_uri=redirect_uri, headers=headers)\n        except InvalidRequest as err:\n            return self._error('invalid_request', err)\n        else:\n            if (resp is not None):\n                return resp\n    return (aresp, headers, redirect_uri, fragment_enc)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _unbroadcast(grad_op_tree, x, be):\n    '\\n        Reverse broadcast from shape(grad_op_tree) to shape(x)\\n\\n        Arguments:\\n            grad_op_tree (OpTreeNode or Tensor): The OpTreeNode to broadcast.\\n            x (OpTreeNode or Tensor): Provides the dimension to be broadcasted to.\\n            be: (Backend): The backend to be used.\\n\\n        Returns:\\n            OpTreeNode or Tensor: The broadcasted result.\\n        '\n    if ((not grad_op_tree) or (not x)):\n        return grad_op_tree\n    if (type(x) in _scalar_types):\n        return 0.0\n    in_shape = x.shape\n    out_shape = grad_op_tree.shape\n    if (in_shape == out_shape):\n        return grad_op_tree\n    elif ((len(in_shape) == 2) and (len(out_shape) == 2)):\n        if (in_shape == (1, 1)):\n            return be.sum(grad_op_tree)\n        elif ((in_shape[0] == out_shape[0]) and (in_shape[1] == 1)):\n            return be.sum(grad_op_tree, axis=1)\n        elif ((in_shape[0] == 1) and (in_shape[1] == out_shape[1])):\n            return be.sum(grad_op_tree, axis=0)\n        elif (((out_shape[0] == in_shape[0]) and (out_shape[1] == 1)) or ((out_shape[0] == 1) and (out_shape[1] == in_shape[1]))):\n            return ((0 * x) + grad_op_tree)\n        else:\n            return NotImplemented\n    else:\n        return NotImplemented\n", "label": 1}
{"function": "\n\ndef _match_url(self, request):\n    if (self._url is ANY):\n        return True\n    if hasattr(self._url, 'search'):\n        return (self._url.search(request.url) is not None)\n    if (self._url_parts.scheme and (request.scheme != self._url_parts.scheme)):\n        return False\n    if (self._url_parts.netloc and (request.netloc != self._url_parts.netloc)):\n        return False\n    if ((request.path or '/') != (self._url_parts.path or '/')):\n        return False\n    request_qs = urlparse.parse_qs(request.query)\n    matcher_qs = urlparse.parse_qs(self._url_parts.query)\n    for (k, vals) in six.iteritems(matcher_qs):\n        for v in vals:\n            try:\n                request_qs.get(k, []).remove(v)\n            except ValueError:\n                return False\n    if self._complete_qs:\n        for v in six.itervalues(request_qs):\n            if v:\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef get(self, getme=None, fromEnd=False):\n    if (not getme):\n        return self\n    try:\n        getme = int(getme)\n        if (getme < 0):\n            return self[:((- 1) * getme)]\n        else:\n            return [self[(getme - 1)]]\n    except IndexError:\n        return []\n    except ValueError:\n        rangeResult = self.rangePattern.search(getme)\n        if rangeResult:\n            start = (rangeResult.group('start') or None)\n            end = (rangeResult.group('start') or None)\n            if start:\n                start = (int(start) - 1)\n            if end:\n                end = int(end)\n            return self[start:end]\n        getme = getme.strip()\n        if (getme.startswith('/') and getme.endswith('/')):\n            finder = re.compile(getme[1:(- 1)], ((re.DOTALL | re.MULTILINE) | re.IGNORECASE))\n\n            def isin(hi):\n                return finder.search(hi)\n        else:\n\n            def isin(hi):\n                return (getme.lower() in hi.lowercase)\n        return [itm for itm in self if isin(itm)]\n", "label": 1}
{"function": "\n\ndef _set_field_names(self, val):\n    val = [self._unicode(x) for x in val]\n    self._validate_option('field_names', val)\n    if self._field_names:\n        old_names = self._field_names[:]\n    self._field_names = val\n    if (self._align and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._align[new_name] = self._align[old_name]\n        for old_name in old_names:\n            if (old_name not in self._align):\n                self._align.pop(old_name)\n    else:\n        for field in self._field_names:\n            self._align[field] = 'c'\n    if (self._valign and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._valign[new_name] = self._valign[old_name]\n        for old_name in old_names:\n            if (old_name not in self._valign):\n                self._valign.pop(old_name)\n    else:\n        for field in self._field_names:\n            self._valign[field] = 't'\n", "label": 1}
{"function": "\n\n@plumbing.route('/repos/<repo_key>/git/commits/')\n@corsify\n@jsonify\ndef get_commit_list(repo_key):\n    ref_name = (request.args.get('ref_name') or None)\n    start_sha = (request.args.get('start_sha') or None)\n    limit = (request.args.get('limit') or current_app.config['RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT'])\n    try:\n        limit = int(limit)\n    except ValueError:\n        raise BadRequest('invalid limit')\n    if (limit < 0):\n        raise BadRequest('invalid limit')\n    repo = get_repo(repo_key)\n    start_commit_id = None\n    if (start_sha is not None):\n        start_commit_id = start_sha\n    else:\n        if (ref_name is None):\n            ref_name = 'HEAD'\n        ref = lookup_ref(repo, ref_name)\n        if (ref is None):\n            raise NotFound('reference not found')\n        start_ref = lookup_ref(repo, ref_name)\n        try:\n            start_commit_id = start_ref.resolve().target\n        except KeyError:\n            if (ref_name == 'HEAD'):\n                return []\n            else:\n                raise NotFound('reference not found')\n    try:\n        walker = repo.walk(start_commit_id, GIT_SORT_TIME)\n    except ValueError:\n        raise BadRequest('invalid start_sha')\n    except KeyError:\n        raise NotFound('commit not found')\n    commits = [convert_commit(repo_key, commit) for commit in islice(walker, limit)]\n    return commits\n", "label": 1}
{"function": "\n\ndef _download_url(resp, link, temp_location):\n    fp = open(temp_location, 'wb')\n    download_hash = None\n    if (link.hash and link.hash_name):\n        try:\n            download_hash = hashlib.new(link.hash_name)\n        except ValueError:\n            logger.warn(('Unsupported hash name %s for package %s' % (link.hash_name, link)))\n    try:\n        total_length = int(resp.info()['content-length'])\n    except (ValueError, KeyError, TypeError):\n        total_length = 0\n    downloaded = 0\n    show_progress = ((total_length > (40 * 1000)) or (not total_length))\n    show_url = link.show_url\n    try:\n        if show_progress:\n            if total_length:\n                logger.start_progress(('Downloading %s (%s): ' % (show_url, format_size(total_length))))\n            else:\n                logger.start_progress(('Downloading %s (unknown size): ' % show_url))\n        else:\n            logger.notify(('Downloading %s' % show_url))\n        logger.info(('Downloading from URL %s' % link))\n        while True:\n            chunk = resp.read(4096)\n            if (not chunk):\n                break\n            downloaded += len(chunk)\n            if show_progress:\n                if (not total_length):\n                    logger.show_progress(('%s' % format_size(downloaded)))\n                else:\n                    logger.show_progress(('%3i%%  %s' % (((100 * downloaded) / total_length), format_size(downloaded))))\n            if (download_hash is not None):\n                download_hash.update(chunk)\n            fp.write(chunk)\n        fp.close()\n    finally:\n        if show_progress:\n            logger.end_progress(('%s downloaded' % format_size(downloaded)))\n    return download_hash\n", "label": 1}
{"function": "\n\ndef __setitem__(self, key, value):\n    if self._set_slice(key, value, self):\n        return\n    if isinstance(value, tuple):\n        if (not (0 < len(value) <= 2)):\n            raise ValueError('A Header item may be set with either a scalar value, a 1-tuple containing a scalar value, or a 2-tuple containing a scalar value and comment string.')\n        if (len(value) == 1):\n            (value, comment) = (value[0], None)\n            if (value is None):\n                value = ''\n        elif (len(value) == 2):\n            (value, comment) = value\n            if (value is None):\n                value = ''\n            if (comment is None):\n                comment = ''\n    else:\n        comment = None\n    card = None\n    if isinstance(key, int):\n        card = self._cards[key]\n    elif isinstance(key, tuple):\n        card = self._cards[self._cardindex(key)]\n    if card:\n        card.value = value\n        if (comment is not None):\n            card.comment = comment\n        if card._modified:\n            self._modified = True\n    else:\n        self._update((key, value, comment))\n", "label": 1}
{"function": "\n\ndef writeBuffer(buff, col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall):\n    for x in buff:\n        printPreceedingCharacter = False\n        altOptions = x[col['alt']].split(',')\n        for y in altOptions:\n            if (((len(x[col['ref']]) != len(y)) or (len(x[col['ref']]) == 0) or (len(y) == 0)) and (not re.search('[^ATGCNatgcn\\\\.-]', y))):\n                printPreceedingCharacter = True\n                break\n        if printPreceedingCharacter:\n            writeRowCheck(x[:], col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall)\n    for x in buff:\n        printPreceedingCharacter = False\n        altOptions = x[col['alt']].split(',')\n        for y in altOptions:\n            if (((len(x[col['ref']]) != len(y)) or (len(x[col['ref']]) == 0) or (len(y) == 0)) and (not re.search('[^ATGCNatgcn\\\\.-]', y))):\n                printPreceedingCharacter = True\n                break\n        if (printPreceedingCharacter == False):\n            writeRowCheck(x[:], col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall)\n", "label": 1}
{"function": "\n\ndef reset_settings(self):\n    for sgroup in self.settings['setting_groups']:\n        for setting in sgroup.values():\n            widget = self.find_child_by_name(setting.name)\n            if (widget is None):\n                continue\n            if ((setting.type == 'string') or (setting.type == 'file') or (setting.type == 'folder')):\n                old_val = ''\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val.replace('\\\\', '\\\\\\\\')\n                widget.setText(old_val)\n            elif (setting.type == 'strings'):\n                old_val = []\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = [v.replace('\\\\', '\\\\\\\\') for v in old_val]\n                widget.setText(','.join(setting.value))\n            elif (setting.type == 'check'):\n                old_val = False\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val\n                widget.setChecked(old_val)\n            elif (setting.type == 'range'):\n                old_val = 0\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val\n                widget.setValue(old_val)\n", "label": 1}
{"function": "\n\ndef test_islice():\n    sl = SortedList(load=7)\n    assert ([] == list(sl.islice()))\n    values = list(range(53))\n    sl.update(values)\n    for start in range(53):\n        for stop in range(53):\n            assert (list(sl.islice(start, stop)) == values[start:stop])\n    for start in range(53):\n        for stop in range(53):\n            assert (list(sl.islice(start, stop, reverse=True)) == values[start:stop][::(- 1)])\n    for start in range(53):\n        assert (list(sl.islice(start=start)) == values[start:])\n        assert (list(sl.islice(start=start, reverse=True)) == values[start:][::(- 1)])\n    for stop in range(53):\n        assert (list(sl.islice(stop=stop)) == values[:stop])\n        assert (list(sl.islice(stop=stop, reverse=True)) == values[:stop][::(- 1)])\n", "label": 1}
{"function": "\n\ndef __doStemming(self, word, intact_word):\n    'Perform the actual word stemming\\n        '\n    valid_rule = re.compile('^([a-z]+)(\\\\*?)(\\\\d)([a-z]*)([>\\\\.]?)$')\n    proceed = True\n    while proceed:\n        last_letter_position = self.__getLastLetter(word)\n        if ((last_letter_position < 0) or (word[last_letter_position] not in self.rule_dictionary)):\n            proceed = False\n        else:\n            rule_was_applied = False\n            for rule in self.rule_dictionary[word[last_letter_position]]:\n                rule_match = valid_rule.match(rule)\n                if rule_match:\n                    (ending_string, intact_flag, remove_total, append_string, cont_flag) = rule_match.groups()\n                    remove_total = int(remove_total)\n                    if word.endswith(ending_string[::(- 1)]):\n                        if intact_flag:\n                            if ((word == intact_word) and self.__isAcceptable(word, remove_total)):\n                                word = self.__applyRule(word, remove_total, append_string)\n                                rule_was_applied = True\n                                if (cont_flag == '.'):\n                                    proceed = False\n                                break\n                        elif self.__isAcceptable(word, remove_total):\n                            word = self.__applyRule(word, remove_total, append_string)\n                            rule_was_applied = True\n                            if (cont_flag == '.'):\n                                proceed = False\n                            break\n            if (rule_was_applied == False):\n                proceed = False\n    return word\n", "label": 1}
{"function": "\n\ndef _result__repr__(self):\n    '\\n    This is used as the `__repr__` function for the :class:`Result`\\n    '\n    details = []\n    flags = self.__class__._fldprops\n    rcstr = 'rc=0x{0:X}'.format(self.rc)\n    if (self.rc != 0):\n        rcstr += '[{0}]'.format(self.errstr)\n    details.append(rcstr)\n    if ((flags & C.PYCBC_RESFLD_KEY) and hasattr(self, 'key')):\n        details.append('key={0}'.format(repr(self.key)))\n    if ((flags & C.PYCBC_RESFLD_VALUE) and hasattr(self, 'value')):\n        details.append('value={0}'.format(repr(self.value)))\n    if ((flags & C.PYCBC_RESFLD_CAS) and hasattr(self, 'cas')):\n        details.append('cas=0x{cas:x}'.format(cas=self.cas))\n    if ((flags & C.PYCBC_RESFLD_CAS) and hasattr(self, 'flags')):\n        details.append('flags=0x{flags:x}'.format(flags=self.flags))\n    if ((flags & C.PYCBC_RESFLD_HTCODE) and hasattr(self, 'http_status')):\n        details.append('http_status={0}'.format(self.http_status))\n    if ((flags & C.PYCBC_RESFLD_URL) and hasattr(self, 'url')):\n        details.append('url={0}'.format(self.url))\n    if hasattr(self, '_pycbc_repr_extra'):\n        details += self._pycbc_repr_extra()\n    ret = '{0}<{1}>'.format(self.__class__.__name__, ', '.join(details))\n    return ret\n", "label": 1}
{"function": "\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    'Setup the ISY994 platform.'\n    logger = logging.getLogger(__name__)\n    devs = []\n    if ((ISY is None) or (not ISY.connected)):\n        logger.error('A connection has not been made to the ISY controller.')\n        return False\n    for (path, node) in ISY.nodes:\n        if ((not node.dimmable) and (SENSOR_STRING not in node.name)):\n            if (HIDDEN_STRING in path):\n                node.name += HIDDEN_STRING\n            devs.append(ISYSwitchDevice(node))\n    for (folder_name, states) in (('HA.doors', [STATE_ON, STATE_OFF]), ('HA.switches', [STATE_ON, STATE_OFF])):\n        try:\n            folder = ISY.programs['My Programs'][folder_name]\n        except KeyError:\n            pass\n        else:\n            for (dtype, name, node_id) in folder.children:\n                if (dtype is 'folder'):\n                    custom_switch = folder[node_id]\n                    try:\n                        actions = custom_switch['actions'].leaf\n                        assert (actions.dtype == 'program'), 'Not a program'\n                        node = custom_switch['status'].leaf\n                    except (KeyError, AssertionError):\n                        pass\n                    else:\n                        devs.append(ISYProgramDevice(name, node, actions, states))\n    add_devices(devs)\n", "label": 1}
{"function": "\n\ndef validate_kwargs(func, kwargs):\n    'Validate arguments to be supplied to func.'\n    func_name = func.__name__\n    argspec = inspect.getargspec(func)\n    all_args = argspec.args[:]\n    defaults = list((argspec.defaults or []))\n    if (inspect.ismethod(func) and (all_args[:1] == ['self'])):\n        all_args[:1] = []\n    if defaults:\n        required = all_args[:(- len(defaults))]\n    else:\n        required = all_args[:]\n    trans = {arg: ((arg.endswith('_') and arg[:(- 1)]) or arg) for arg in all_args}\n    for key in list(kwargs):\n        key_adj = ('%s_' % key)\n        if (key_adj in all_args):\n            kwargs[key_adj] = kwargs.pop(key)\n    supplied = sorted(kwargs)\n    missing = [trans.get(arg, arg) for arg in required if (arg not in supplied)]\n    if missing:\n        raise MeteorError(400, func.err, ('Missing required arguments to %s: %s' % (func_name, ' '.join(missing))))\n    extra = [arg for arg in supplied if (arg not in all_args)]\n    if extra:\n        raise MeteorError(400, func.err, ('Unknown arguments to %s: %s' % (func_name, ' '.join(extra))))\n", "label": 1}
{"function": "\n\ndef testSubstitute1(self):\n    config = Configuration()\n    config.readfp(StringIO(CONFIG1))\n    assert config.has_section('section1')\n    assert (not config.has_section('section2'))\n    assert (not config.has_section('foo'))\n    assert (not config.has_section('bar'))\n    assert (not config.has_section('bar2'))\n    assert config.has_option('section1', 'foo')\n    assert config.has_option('section1', 'name')\n    assert (config.get('section1', 'name') == os.path.basename(sys.argv[0]))\n    assert (config.get('section1', 'cwd') == os.getcwd())\n    assert config.has_option('section1', 'bar')\n    assert config.has_option('section1', 'bar2')\n    assert (config.get('section1', 'foo') == 'bar')\n    assert (config.get('section1', 'bar') == 'bar')\n    assert (config.get('section1', 'bar2') == 'bar')\n", "label": 1}
{"function": "\n\ndef timeago(time=False):\n    \"\\n    Get a datetime object or a int() Epoch timestamp and return a\\n    pretty string like 'an hour ago', 'Yesterday', '3 months ago',\\n    'just now', etc\\n    \"\n    from datetime import datetime\n    now = datetime.now()\n    if (type(time) is int):\n        diff = (now - datetime.fromtimestamp(time))\n    elif isinstance(time, datetime):\n        diff = (now - time)\n    elif (not time):\n        diff = (now - now)\n    second_diff = diff.seconds\n    day_diff = diff.days\n    if (day_diff < 0):\n        return ''\n    if (day_diff == 0):\n        if (second_diff < 10):\n            return 'just now'\n        if (second_diff < 60):\n            return (str(second_diff) + ' seconds ago')\n        if (second_diff < 120):\n            return 'a minute ago'\n        if (second_diff < 3600):\n            return (str((second_diff / 60)) + ' minutes ago')\n        if (second_diff < 7200):\n            return 'an hour ago'\n        if (second_diff < 86400):\n            return (str((second_diff / 3600)) + ' hours ago')\n    if (day_diff == 1):\n        return 'Yesterday'\n    if (day_diff < 7):\n        return (str(day_diff) + ' days ago')\n    if (day_diff < 31):\n        return (str((day_diff / 7)) + ' weeks ago')\n    if (day_diff < 365):\n        return (str((day_diff / 30)) + ' months ago')\n    return (str((day_diff / 365)) + ' years ago')\n", "label": 1}
{"function": "\n\ndef test_insertComps_K1_D3(self, K=1, D=3):\n    A = ParamBag(K=K, D=D)\n    s = 123.456\n    A.setField('scalar', s, dims=None)\n    A.setField('N', [1.0], dims='K')\n    A.setField('x', np.random.rand(K, D), dims=('K', 'D'))\n    A.setField('xxT', np.random.rand(K, D, D), dims=('K', 'D', 'D'))\n    Abig = A.copy()\n    Abig.insertComps(A)\n    assert (Abig.K == 2)\n    assert np.allclose(Abig.N, np.hstack([A.N, A.N]))\n    assert (Abig.scalar == (2 * s))\n    assert (Abig.xxT.shape == (2, 3, 3))\n    assert np.allclose(Abig.xxT[0], A.xxT)\n    assert np.allclose(Abig.xxT[1], A.xxT)\n    Abig.insertComps(A)\n    assert (Abig.K == 3)\n    assert np.allclose(Abig.N, np.hstack([A.N, A.N, A.N]))\n    assert (Abig.scalar == (3 * s))\n    assert (Abig.xxT.shape == (3, 3, 3))\n    assert np.allclose(Abig.xxT[0], A.xxT)\n    assert np.allclose(Abig.xxT[1], A.xxT)\n    A.insertComps(Abig)\n    assert (A.K == 4)\n    assert (A.scalar == (4 * s))\n    assert np.allclose(A.N, np.hstack([1, 1, 1, 1]))\n", "label": 1}
{"function": "\n\ndef test():\n    pid = get_player('Tim', 'Duncan')\n    vs_pid = get_player('Stephen', 'Curry')\n    assert player.PlayerList()\n    assert player.PlayerSummary(pid)\n    assert player.PlayerLastNGamesSplits(pid)\n    assert player.PlayerInGameSplits(pid)\n    assert player.PlayerClutchSplits(pid)\n    assert player.PlayerPerformanceSplits(pid)\n    assert player.PlayerYearOverYearSplits(pid)\n    assert player.PlayerCareer(pid)\n    assert player.PlayerProfile(pid)\n    assert player.PlayerGameLogs(pid)\n    assert player.PlayerShotTracking(pid)\n    assert player.PlayerReboundTracking(pid)\n    assert player.PlayerPassTracking(pid)\n    assert player.PlayerDefenseTracking(pid)\n    assert player.PlayerVsPlayer(pid, vs_pid)\n", "label": 1}
{"function": "\n\ndef add_total_row(result, columns):\n    total_row = ([''] * len(columns))\n    has_percent = []\n    for row in result:\n        for (i, col) in enumerate(columns):\n            fieldtype = None\n            if isinstance(col, basestring):\n                col = col.split(':')\n                if (len(col) > 1):\n                    fieldtype = col[1]\n                    if ('/' in fieldtype):\n                        fieldtype = fieldtype.split('/')[0]\n            else:\n                fieldtype = col.get('fieldtype')\n            if ((fieldtype in ['Currency', 'Int', 'Float', 'Percent']) and flt(row[i])):\n                total_row[i] = (flt(total_row[i]) + flt(row[i]))\n            if ((fieldtype == 'Percent') and (i not in has_percent)):\n                has_percent.append(i)\n    for i in has_percent:\n        total_row[i] = (total_row[i] / len(result))\n    first_col_fieldtype = None\n    if isinstance(columns[0], basestring):\n        first_col = columns[0].split(':')\n        if (len(first_col) > 1):\n            first_col_fieldtype = first_col[1].split('/')[0]\n    else:\n        first_col_fieldtype = columns[0].get('fieldtype')\n    if (first_col_fieldtype not in ['Currency', 'Int', 'Float', 'Percent']):\n        if (first_col_fieldtype == 'Link'):\n            total_row[0] = ((\"'\" + _('Total')) + \"'\")\n        else:\n            total_row[0] = _('Total')\n    result.append(total_row)\n    return result\n", "label": 1}
{"function": "\n\ndef _check_fusion(self, root):\n    roots = root.table._root_tables()\n    validator = ExprValidator([root.table])\n    fused_exprs = []\n    can_fuse = False\n    resolved = _maybe_resolve_exprs(root.table, self.input_exprs)\n    if (not resolved):\n        return None\n    for val in resolved:\n        lifted_val = substitute_parents(val)\n        if (isinstance(val, ir.TableExpr) and (self.parent.op().is_ancestor(val) or ((len(roots) == 1) and (val._root_tables()[0] is roots[0])))):\n            can_fuse = True\n            have_root = False\n            for y in root.selections:\n                if y.equals(root.table):\n                    fused_exprs.append(root.table)\n                    have_root = True\n                    continue\n                fused_exprs.append(y)\n            if ((not have_root) and (len(root.selections) == 0)):\n                fused_exprs = ([root.table] + fused_exprs)\n        elif validator.validate(lifted_val):\n            can_fuse = True\n            fused_exprs.append(lifted_val)\n        elif (not validator.validate(val)):\n            can_fuse = False\n            break\n        else:\n            fused_exprs.append(val)\n    if can_fuse:\n        return ops.Selection(root.table, fused_exprs, predicates=root.predicates, sort_keys=root.sort_keys)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef main(script):\n    'Tests the functions in this module.\\n\\n    script: string script name\\n    '\n    preg = ReadFemPreg()\n    print(preg.shape)\n    assert (len(preg) == 13593)\n    assert (preg.caseid[13592] == 12571)\n    assert (preg.pregordr.value_counts()[1] == 5033)\n    assert (preg.nbrnaliv.value_counts()[1] == 8981)\n    assert (preg.babysex.value_counts()[1] == 4641)\n    assert (preg.birthwgt_lb.value_counts()[7] == 3049)\n    assert (preg.birthwgt_oz.value_counts()[0] == 1037)\n    assert (preg.prglngth.value_counts()[39] == 4744)\n    assert (preg.outcome.value_counts()[1] == 9148)\n    assert (preg.birthord.value_counts()[1] == 4413)\n    assert (preg.agepreg.value_counts()[22.75] == 100)\n    assert (preg.totalwgt_lb.value_counts()[7.5] == 302)\n    weights = preg.finalwgt.value_counts()\n    key = max(weights.keys())\n    assert (preg.finalwgt.value_counts()[key] == 6)\n    print(('%s: All tests passed.' % script))\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    outcols = []\n    if self.rgb:\n        outcols.append(self.rgb)\n    if (self.thickEnd or outcols):\n        outcols.append((self.thickEnd if self.thickEnd else self.end))\n    if (self.thickStart or outcols):\n        outcols.append((self.thickStart if self.thickStart else self.start))\n    if (self.strand or outcols):\n        outcols.append(self.strand)\n    if ((self.score_int != '') or outcols):\n        outcols.append(self.score_int)\n    if (self.name or outcols):\n        outcols.append(self.name)\n    outcols.append(self.end)\n    outcols.append(self.start)\n    outcols.append(self.chrom)\n    return '\\t'.join([str(x) for x in outcols[::(- 1)]])\n", "label": 1}
{"function": "\n\ndef assertMessage(self, response, content, level=None, tags=None, limit=None):\n    '\\n        Asserts that the response has a particular message in its context.\\n        \\n        If limit is provided, checks that there are at most the specified\\n        number of messages.\\n        \\n        If level or tags are specified, checks for existence of message having\\n        matching content, level and/or tags. Otherwise, it simply checks for a\\n        message with the content.\\n        \\n        '\n    self.assertTrue((hasattr(response, 'context') and response.context), 'The response must have a non-empty context attribute.')\n    messages = list((response.context['messages'] if ('messages' in response.context) else []))\n    self.assertTrue(bool(messages), \"The response's context must contain at least one message.\")\n    if limit:\n        self.assertGreaterEqual(limit, len(messages), \"The response's context must have at most {limit:d} messages, but it has {actual:d} messages.\".format(limit=limit, actual=len(messages)))\n    self.assertTrue(any((((message.message == content) and ((not level) or (message.level == level)) and ((not tags) or (set((tag.strip() for tag in (message.tags or '').split(' ') if tag)) == set((tag.strip() for tag in (tags or '').split(' ') if tag))))) for message in messages)), \"A message matching the content, level and tags was not found in the response's context.\")\n", "label": 1}
{"function": "\n\ndef _compose(self, public=None, private=None, no_cache=None, no_store=False, max_age=None, s_maxage=None, no_transform=False, **extensions):\n    assert isinstance(max_age, (type(None), int))\n    assert isinstance(s_maxage, (type(None), int))\n    expires = 0\n    result = []\n    if (private is True):\n        assert ((not public) and (not no_cache) and (not s_maxage))\n        result.append('private')\n    elif (no_cache is True):\n        assert ((not public) and (not private) and (not max_age))\n        result.append('no-cache')\n    else:\n        assert ((public is None) or (public is True))\n        assert ((not private) and (not no_cache))\n        expires = max_age\n        result.append('public')\n    if no_store:\n        result.append('no-store')\n    if no_transform:\n        result.append('no-transform')\n    if (max_age is not None):\n        result.append(('max-age=%d' % max_age))\n    if (s_maxage is not None):\n        result.append(('s-maxage=%d' % s_maxage))\n    for (k, v) in six.iteritems(extensions):\n        if (k not in self.extensions):\n            raise AssertionError((\"unexpected extension used: '%s'\" % k))\n        result.append(('%s=\"%s\"' % (k.replace('_', '-'), v)))\n    return (result, expires)\n", "label": 1}
{"function": "\n\ndef format(self, value):\n    '\\n        Formats a value.\\n\\n        @type value\\n          `float`\\n        '\n    sign = ('' if (self.__sign is None) else ('-' if (value < 0) else ('+' if (self.__sign == '+') else ' ')))\n    if math.isnan(value):\n        result = text.pad(self.__nan_str, self.__width, pad=' ', left=True)\n    elif ((value < 0) and (self.__sign is None)):\n        result = ('#' * self.__width)\n    elif math.isinf(value):\n        result = text.pad((sign + self.__inf_str), self.__width, pad=' ', left=True)\n    else:\n        precision = (0 if (self.__precision is None) else self.__precision)\n        rnd_value = round(value, precision)\n        abs_value = abs(rnd_value)\n        int_value = int(abs_value)\n        result = str(int_value)\n        if (len(result) > self.__size):\n            return ('#' * self.__width)\n        if (self.__pad == ' '):\n            result = text.pad((sign + result), (self.__size + len(sign)), pad=self.__pad, left=True)\n        else:\n            result = (sign + text.pad(result, self.__size, pad=self.__pad, left=True))\n        if (self.__precision is None):\n            pass\n        elif (self.__precision == 0):\n            result += self.__point\n        else:\n            frac = int(round(((abs_value - int_value) * self.__multiplier)))\n            frac = str(frac)\n            assert (len(frac) <= precision)\n            frac = text.pad(frac, self.__precision, pad='0', left=True)\n            result += (self.__point + frac)\n    return result\n", "label": 1}
{"function": "\n\ndef find(pattern, path='.', exclude=None, recursive=True):\n    'Find files that match *pattern* in *path*'\n    import fnmatch\n    import os\n    if recursive:\n        for (root, dirnames, filenames) in os.walk(path):\n            for pat in _to_list(pattern):\n                for filename in fnmatch.filter(filenames, pat):\n                    filepath = join(abspath(root), filename)\n                    for excl in _to_list(exclude):\n                        if (excl and fnmatch.fnmatch(filepath, excl)):\n                            break\n                    else:\n                        (yield filepath)\n    else:\n        for pat in _to_list(pattern):\n            for filename in fnmatch.filter(list(path), pat):\n                filepath = join(abspath(path), filename)\n                for excl in _to_list(exclude):\n                    if (excl and fnmatch.fnmatch(filepath, excl)):\n                        break\n                    else:\n                        (yield filepath)\n", "label": 1}
{"function": "\n\n@classmethod\ndef deserialize(cls, buf):\n    'Returns a Match object deserialized from a sequence of bytes.\\n\\n        Args:\\n            buf: A ReceiveBuffer object that contains the bytes that\\n                are the serialized form of the Match object.\\n\\n        Returns:\\n            A new Match object deserialized from the buffer.\\n\\n        Raises:\\n            ValueError: The buffer has an invalid number of available\\n                bytes, or some elements cannot be deserialized.\\n        '\n    (wildcards_ser, in_port, dl_src, dl_dst, dl_vlan, dl_vlan_pcp, dl_type, nw_tos, nw_proto, nw_src, nw_dst, tp_src, tp_dst) = buf.unpack(cls.FORMAT)\n    wildcards = Wildcards.deserialize(wildcards_ser)\n    if (nw_tos & 3):\n        nw_tos &= 252\n    nw_src_prefix_length = (32 - wildcards.nw_src)\n    nw_dst_prefix_length = (32 - wildcards.nw_dst)\n    return Match((None if wildcards.in_port else in_port), (None if wildcards.dl_src else dl_src), (None if wildcards.dl_dst else dl_dst), (None if wildcards.dl_vlan else dl_vlan), (None if wildcards.dl_vlan_pcp else dl_vlan_pcp), (None if wildcards.dl_type else dl_type), (None if wildcards.nw_tos else nw_tos), (None if wildcards.nw_proto else nw_proto), ((nw_src, nw_src_prefix_length) if (nw_src_prefix_length > 0) else None), ((nw_dst, nw_dst_prefix_length) if (nw_dst_prefix_length > 0) else None), (None if wildcards.tp_src else tp_src), (None if wildcards.tp_dst else tp_dst))\n", "label": 1}
{"function": "\n\ndef get_substrings(self, min_freq=2, check_positive=True, sort_by_length=False):\n    movetos = set()\n    for (idx, tok) in enumerate(self.rev_keymap):\n        if (isinstance(tok, basestring) and (tok[(- 6):] == 'moveto')):\n            movetos.add(idx)\n    try:\n        hmask = self.rev_keymap.index('hintmask')\n    except ValueError:\n        hmask = None\n    matches = {\n        \n    }\n    for (glyph_idx, program) in enumerate(self.data):\n        cur_start = 0\n        last_op = (- 1)\n        for (pos, tok) in enumerate(program):\n            if (tok in movetos):\n                stop = (last_op + 1)\n                if ((stop - cur_start) > 0):\n                    if (program[cur_start:stop] in matches):\n                        matches[program[cur_start:stop]].freq += 1\n                    else:\n                        span = pyCompressor.CandidateSubr((stop - cur_start), (glyph_idx, cur_start), 1, self.data, self.cost_map)\n                        matches[program[cur_start:stop]] = span\n                cur_start = (pos + 1)\n            elif (tok == hmask):\n                last_op = (pos + 1)\n            elif (type(self.rev_keymap[tok]) == str):\n                last_op = pos\n    constraints = (lambda s: ((s.freq >= min_freq) and ((s.subr_saving() > 0) or (not check_positive))))\n    self.substrings = filter(constraints, matches.values())\n    if sort_by_length:\n        self.substrings.sort(key=(lambda s: len(s)))\n    else:\n        self.substrings.sort(key=(lambda s: s.subr_saving()), reverse=True)\n    return self.substrings\n", "label": 1}
{"function": "\n\ndef _print_Mul(self, expr):\n    prec = precedence(expr)\n    (c, e) = expr.as_coeff_Mul()\n    if (c < 0):\n        expr = _keep_coeff((- c), e)\n        sign = '-'\n    else:\n        sign = ''\n    a = []\n    b = []\n    if (self.order not in ('old', 'none')):\n        args = expr.as_ordered_factors()\n    else:\n        args = Mul.make_args(expr)\n    for item in args:\n        if (item.is_commutative and item.is_Pow and item.exp.is_Rational and item.exp.is_negative):\n            if (item.exp != (- 1)):\n                b.append(Pow(item.base, (- item.exp), evaluate=False))\n            else:\n                b.append(Pow(item.base, (- item.exp)))\n        else:\n            a.append(item)\n    a = (a or [S.One])\n    a_str = [self.parenthesize(x, prec) for x in a]\n    b_str = [self.parenthesize(x, prec) for x in b]\n    if (len(b) == 0):\n        return (sign + '*'.join(a_str))\n    elif (len(b) == 1):\n        return (((sign + '*'.join(a_str)) + '/') + b_str[0])\n    else:\n        return ((sign + '*'.join(a_str)) + ('/(%s)' % '*'.join(b_str)))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef merge(left, right, func):\n    if (left is right):\n        return left\n    if (left is None):\n        (left, right) = (right, left)\n    default = left.default\n    merge = _TreeListSub.merge\n    if (right is None):\n        direct = [func(x, default) for x in left.direct]\n        children = [merge(child, None, func) for child in left.children]\n        if ((direct == left.direct) and (children == left.children)):\n            return left\n        return _TreeListSub(default, direct, children)\n    direct = [func(x, y) for (x, y) in zip(left.direct, right.direct)]\n    children = [merge(c1, c2, func) for (c1, c2) in zip(left.children, right.children)]\n    if ((direct == left.direct) and (children == left.children)):\n        return left\n    if ((direct == right.direct) and (children == right.children)):\n        return right\n    return _TreeListSub(default, direct, children)\n", "label": 1}
{"function": "\n\ndef same_shape(self, x, y, dim_x=None, dim_y=None):\n    'Return True if we are able to assert that x and y have the\\n        same shape.\\n\\n        dim_x and dim_y are optional. If used, they should be an index\\n        to compare only 1 dimension of x and y.\\n\\n        '\n    sx = self.shape_of[x]\n    sy = self.shape_of[y]\n    if ((sx is None) or (sy is None)):\n        return False\n    if (dim_x is not None):\n        sx = [sx[dim_x]]\n    if (dim_y is not None):\n        sy = [sy[dim_y]]\n    assert (len(sx) == len(sy))\n    for (dx, dy) in zip(sx, sy):\n        if (dx is dy):\n            continue\n        if ((not dx.owner) or (not dy.owner)):\n            return False\n        if ((not isinstance(dx.owner.op, Shape_i)) or (not isinstance(dy.owner.op, Shape_i))):\n            return False\n        opx = dx.owner.op\n        opy = dy.owner.op\n        if (not (opx.i == opy.i)):\n            return False\n        if (dx.owner.inputs[0] == dy.owner.inputs[0]):\n            continue\n        from theano.scan_module.scan_utils import equal_computations\n        if (not equal_computations([dx], [dy])):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_02_atom_init_with_statement(self):\n    s = Atom_Sword_Statement(ATOM_TEST_STATEMENT)\n    assert (len(s.states) == 1)\n    assert (len(s.original_deposits) == 1)\n    assert (len(s.resources) == 1)\n    assert (s.xml_document != None)\n    assert (s.dom != None)\n    assert s.parsed\n    assert s.valid\n    (uri, description) = s.states[0]\n    assert (uri == 'http://purl.org/net/sword/terms/state/Testing')\n    assert (description == 'The work has passed through review and is now in the archive')\n    t = datetime.strptime('2011-03-02T20:50:06Z', '%Y-%m-%dT%H:%M:%SZ')\n    entry = s.resources[0]\n    assert (len(entry.packaging) == 1)\n    assert (entry.deposited_by == 'sword')\n    assert (entry.deposited_on_behalf_of == 'jbloggs')\n    assert (entry.deposited_on == t)\n    assert (entry.uri == 'http://localhost:8080/part-IRI/43/my_deposit/example.zip')\n    assert (entry.packaging[0] == 'http://purl.org/net/sword/package/SimpleZip')\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    if (not [b for b in bases if isinstance(b, HideMetaOpts)]):\n        return super(HideMetaOpts, cls).__new__(cls, name, bases, attrs)\n    else:\n        meta_opts = deepcopy(cls.default_meta_opts)\n        if (('Meta' in attrs) and (attrs['Meta'].__module__ != 'django.db.models.query_utils')):\n            meta = attrs.get('Meta')\n        else:\n            for base in bases:\n                meta = getattr(base, '_meta', None)\n                if meta:\n                    break\n        if meta:\n            for (opt, value) in vars(meta).items():\n                if ((opt not in models.options.DEFAULT_NAMES) and (cls.hide_unknown_opts or (opt in meta_opts))):\n                    meta_opts[opt] = value\n                    delattr(meta, opt)\n        new_class = super(HideMetaOpts, cls).__new__(cls, name, bases, attrs)\n        if meta:\n            for opt in meta_opts:\n                setattr(meta, opt, meta_opts[opt])\n        for opt in meta_opts:\n            setattr(new_class._meta, opt, meta_opts[opt])\n        return new_class\n", "label": 1}
{"function": "\n\ndef cache_response(self, request, response, body=None):\n    '\\n        Algorithm for caching requests.\\n\\n        This assumes a requests Response object.\\n        '\n    if (response.status not in [200, 203, 300, 301]):\n        return\n    response_headers = CaseInsensitiveDict(response.headers)\n    cc_req = self.parse_cache_control(request.headers)\n    cc = self.parse_cache_control(response_headers)\n    cache_url = self.cache_url(request.url)\n    no_store = (cc.get('no-store') or cc_req.get('no-store'))\n    if (no_store and self.cache.get(cache_url)):\n        self.cache.delete(cache_url)\n    if (self.cache_etags and ('etag' in response_headers)):\n        self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n    elif (response.status == 301):\n        self.cache.set(cache_url, self.serializer.dumps(request, response))\n    elif ('date' in response_headers):\n        if (cc and cc.get('max-age')):\n            if (int(cc['max-age']) > 0):\n                self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n        elif ('expires' in response_headers):\n            if response_headers['expires']:\n                self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n", "label": 1}
{"function": "\n\ndef replicate_attributes(source, target, cache=None):\n    'Replicates common SQLAlchemy attributes from the `source` object to the\\n    `target` object.'\n    target_manager = manager_of_class(type(target))\n    column_attrs = set()\n    relationship_attrs = set()\n    relationship_columns = set()\n    for attr in manager_of_class(type(source)).attributes:\n        if (attr.key not in target_manager):\n            continue\n        target_attr = target_manager[attr.key]\n        if isinstance(attr.property, ColumnProperty):\n            assert isinstance(target_attr.property, ColumnProperty)\n            column_attrs.add(attr)\n        elif isinstance(attr.property, RelationshipProperty):\n            assert isinstance(target_attr.property, RelationshipProperty)\n            relationship_attrs.add(attr)\n            if (attr.property.direction is MANYTOONE):\n                relationship_columns.update(attr.property.local_columns)\n    for attr in column_attrs:\n        if _column_property_in_registry(attr.property, _excluded):\n            continue\n        elif ((not _column_property_in_registry(attr.property, _included)) and all(((column in relationship_columns) for column in attr.property.columns))):\n            continue\n        setattr(target, attr.key, getattr(source, attr.key))\n    for attr in relationship_attrs:\n        target_attr_model = target_manager[attr.key].property.argument\n        if (not is_relation_replicatable(attr)):\n            continue\n        replicate_relation(source, target, attr, target_manager[attr.key], cache=cache)\n", "label": 1}
{"function": "\n\ndef __getattribute__(self, name):\n    try:\n        result = super(GsxElement, self).__getattribute__(name)\n    except AttributeError:\n        \"\\n            The XML returned by GSX can be pretty inconsistent, especially\\n            between the different environments. It's therefore more\\n            practical to return None than to expect AttributeErrors all\\n            over your application.\\n            \"\n        return\n    if (name in STRING_TYPES):\n        return unicode((result.text or ''))\n    if isinstance(result, objectify.NumberElement):\n        return result.pyval\n    if isinstance(result, objectify.StringElement):\n        name = result.tag\n        result = (result.text or '')\n        result = unicode(result)\n        if (not result):\n            return\n        if (name in DATETIME_TYPES):\n            return gsx_datetime(result)\n        if (name in DIAGS_TIMESTAMP_TYPES):\n            return gsx_diags_timestamp(result)\n        if (name in BASE64_TYPES):\n            return gsx_attachment(result)\n        if (name in FLOAT_TYPES):\n            return gsx_price(result)\n        if name.endswith('Date'):\n            return gsx_date(result)\n        if name.endswith('Timestamp'):\n            return gsx_timestamp(result)\n        if re.search('^[YN]$', result):\n            return gsx_boolean(result)\n    return result\n", "label": 1}
{"function": "\n\ndef reverse(viewname, urlconf=None, args=None, kwargs=None, prefix=None, current_app=None):\n    if (urlconf is None):\n        urlconf = get_urlconf()\n    resolver = get_resolver(urlconf)\n    args = (args or [])\n    kwargs = (kwargs or {\n        \n    })\n    if (prefix is None):\n        prefix = get_script_prefix()\n    if (not isinstance(viewname, six.string_types)):\n        view = viewname\n    else:\n        parts = viewname.split(':')\n        parts.reverse()\n        view = parts[0]\n        path = parts[1:]\n        resolved_path = []\n        ns_pattern = ''\n        while path:\n            ns = path.pop()\n            try:\n                app_list = resolver.app_dict[ns]\n                if (current_app and (current_app in app_list)):\n                    ns = current_app\n                elif (ns not in app_list):\n                    ns = app_list[0]\n            except KeyError:\n                pass\n            try:\n                (extra, resolver) = resolver.namespace_dict[ns]\n                resolved_path.append(ns)\n                ns_pattern = (ns_pattern + extra)\n            except KeyError as key:\n                if resolved_path:\n                    raise NoReverseMatch((\"%s is not a registered namespace inside '%s'\" % (key, ':'.join(resolved_path))))\n                else:\n                    raise NoReverseMatch(('%s is not a registered namespace' % key))\n        if ns_pattern:\n            resolver = get_ns_resolver(ns_pattern, resolver)\n    return iri_to_uri(resolver._reverse_with_prefix(view, prefix, *args, **kwargs))\n", "label": 1}
{"function": "\n\ndef match(self, value=None, name=None):\n    nv = vv = False\n    if value:\n        if re.search(value, self.value):\n            vv = True\n        else:\n            vv = False\n    if name:\n        if re.search(name, self.name):\n            nv = True\n        else:\n            nv = False\n    if (name and value):\n        return (nv and vv)\n    if (name and (not value)):\n        return nv\n    if ((not name) and value):\n        return vv\n    if ((not name) and (not value)):\n        return False\n", "label": 1}
{"function": "\n\n@classmethod\ndef size(cls, val, str_cache=None, object_cache=None, traits_cache=None):\n    if (str_cache is None):\n        str_cache = []\n    if (object_cache is None):\n        object_cache = []\n    if (traits_cache is None):\n        traits_cache = []\n    size = U8.size\n    if (isinstance(val, bool) and (val in (False, True))):\n        pass\n    elif (val is None):\n        pass\n    elif isinstance(val, integer_types):\n        if ((val < AMF3_MIN_INTEGER) or (val > AMF3_MAX_INTEGER)):\n            size += AMF3Double.size\n        else:\n            size += AMF3Integer.size(val)\n    elif isinstance(val, float):\n        size += AMF3Double.size\n    elif isinstance(val, (AMF3Array, list)):\n        size += AMF3ArrayPacker.size(val, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif isinstance(val, string_types):\n        size += AMF3String.size(val, cache=str_cache)\n    elif isinstance(val, AMF3ObjectBase):\n        size += AMF3ObjectPacker.size(val, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif isinstance(val, AMF3Date):\n        size += AMF3DatePacker.size(val, cache=object_cache)\n    else:\n        raise ValueError('Unable to pack value of type {0}'.format(type(val)))\n    return size\n", "label": 1}
{"function": "\n\ndef check_if_doc_is_dynamically_linked(doc, method='Delete'):\n    'Raise `frappe.LinkExistsError` if the document is dynamically linked'\n    for df in get_dynamic_link_map().get(doc.doctype, []):\n        if (df.parent in ('Communication', 'ToDo', 'DocShare', 'Email Unsubscribe')):\n            continue\n        meta = frappe.get_meta(df.parent)\n        if meta.issingle:\n            refdoc = frappe.db.get_singles_dict(df.parent)\n            if ((refdoc.get(df.options) == doc.doctype) and (refdoc.get(df.fieldname) == doc.name) and (((method == 'Delete') and (refdoc.docstatus < 2)) or ((method == 'Cancel') and (refdoc.docstatus == 1)))):\n                frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, df.parent, ''), frappe.LinkExistsError)\n        else:\n            for refdoc in frappe.db.sql('select name, docstatus from `tab{parent}` where\\n\\t\\t\\t\\t{options}=%s and {fieldname}=%s'.format(**df), (doc.doctype, doc.name), as_dict=True):\n                if (((method == 'Delete') and (refdoc.docstatus < 2)) or ((method == 'Cancel') and (refdoc.docstatus == 1))):\n                    frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, df.parent, refdoc.name), frappe.LinkExistsError)\n", "label": 1}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_start_timestamp_milliseconds():\n        self.set_start_timestamp_milliseconds(x.start_timestamp_milliseconds())\n    if x.has_http_method():\n        self.set_http_method(x.http_method())\n    if x.has_http_path():\n        self.set_http_path(x.http_path())\n    if x.has_http_query():\n        self.set_http_query(x.http_query())\n    if x.has_http_status():\n        self.set_http_status(x.http_status())\n    if x.has_duration_milliseconds():\n        self.set_duration_milliseconds(x.duration_milliseconds())\n    if x.has_api_mcycles():\n        self.set_api_mcycles(x.api_mcycles())\n    if x.has_processor_mcycles():\n        self.set_processor_mcycles(x.processor_mcycles())\n    for i in xrange(x.rpc_stats_size()):\n        self.add_rpc_stats().CopyFrom(x.rpc_stats(i))\n    for i in xrange(x.cgi_env_size()):\n        self.add_cgi_env().CopyFrom(x.cgi_env(i))\n    if x.has_overhead_walltime_milliseconds():\n        self.set_overhead_walltime_milliseconds(x.overhead_walltime_milliseconds())\n    if x.has_user_email():\n        self.set_user_email(x.user_email())\n    if x.has_is_admin():\n        self.set_is_admin(x.is_admin())\n    for i in xrange(x.individual_stats_size()):\n        self.add_individual_stats().CopyFrom(x.individual_stats(i))\n", "label": 1}
{"function": "\n\ndef _rule_to_post_data(rule):\n    post_data = {\n        \n    }\n    post_data['ruleType'] = rule['ruleType']\n    if ('filter' in rule):\n        filter = rule['filter']\n        if ('httpProtocol' in filter):\n            post_data['filterProtocol'] = filter['httpProtocol']\n        if ('method' in filter):\n            post_data['filterMethod'] = filter['method']\n        if ('url' in filter):\n            post_data['filterUrl'] = filter['url']\n        if ('statusCode' in filter):\n            post_data['filterstatusCode'] = filter['statusCode']\n    if ('action' in rule):\n        action = rule['action']\n        if ('type' in action):\n            post_data['actionType'] = action['type']\n        if ('httpProtocol' in action):\n            post_data['actionProtocol'] = action['httpProtocol']\n        if ('method' in action):\n            post_data['actionMethod'] = action['method']\n        if ('url' in action):\n            post_data['actionUrl'] = action['url']\n        if ('statusCode' in action):\n            post_data['actionStatusCode'] = action['statusCode']\n        if ('statusDescription' in action):\n            post_data['actionStatusDescription'] = action['statusDescription']\n        if ('payload' in action):\n            post_data['actionPayload'] = action['payload']\n        if ('setHeaders' in action):\n            action['headers'] = action.pop('setHeaders')\n    return post_data\n", "label": 1}
{"function": "\n\ndef publish(self):\n    super(PyFS, self).publish()\n    deploy_fs = OSFS(self.site.config.deploy_root_path.path)\n    for (dirnm, local_filenms) in deploy_fs.walk():\n        logger.info('Making directory: %s', dirnm)\n        self.fs.makedir(dirnm, allow_recreate=True)\n        remote_fileinfos = self.fs.listdirinfo(dirnm, files_only=True)\n        for filenm in local_filenms:\n            filepath = pathjoin(dirnm, filenm)\n            for (nm, info) in remote_fileinfos:\n                if (nm == filenm):\n                    break\n            else:\n                info = {\n                    \n                }\n            if (self.check_etag and ('etag' in info)):\n                with deploy_fs.open(filepath, 'rb') as f:\n                    local_etag = self._calculate_etag(f)\n                if (info['etag'] == local_etag):\n                    logger.info('Skipping file [etag]: %s', filepath)\n                    continue\n            if (self.check_mtime and ('modified_time' in info)):\n                local_mtime = deploy_fs.getinfo(filepath)['modified_time']\n                if (info['modified_time'] > local_mtime):\n                    logger.info('Skipping file [mtime]: %s', filepath)\n                    continue\n            logger.info('Uploading file: %s', filepath)\n            with deploy_fs.open(filepath, 'rb') as f:\n                self.fs.setcontents(filepath, f)\n        for (filenm, info) in remote_fileinfos:\n            filepath = pathjoin(dirnm, filenm)\n            if (filenm not in local_filenms):\n                logger.info('Removing file: %s', filepath)\n                self.fs.remove(filepath)\n", "label": 1}
{"function": "\n\ndef _ask(self, stage, args, tag):\n    self._report_driver.report_sync(tag, report_type=stage)\n    while True:\n        answer = raw_input('Continue? ([d]etailed/[C]oncise report,[y]es,[n]o,[r]etry): ')\n        if ((not answer) or (answer == 'c') or (answer == 'C')):\n            self._report_driver.report_sync(tag, report_type=stage)\n        elif ((answer == 'd') or (answer == 'D')):\n            self._report_driver.report_sync(tag, report_type=stage, detailed=True)\n        elif ((answer == 'Y') or (answer == 'y')):\n            return True\n        elif ((answer == 'N') or (answer == 'n')):\n            return False\n        elif ((answer == 'R') or (answer == 'r')):\n            if (stage == 'fetch'):\n                self._fetch(args)\n            if (stage == 'checkout'):\n                self._checkout(args)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.privilege = TSentryPrivilege()\n                self.privilege.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef data(self, index, role=Qt.DisplayRole):\n    if ((not index.isValid()) or (not (0 <= index.row() < len(self.packages)))):\n        return to_qvariant()\n    package = self.packages[index.row()]\n    column = index.column()\n    if ((role == Qt.CheckStateRole) and (column == CHECK)):\n        return to_qvariant((package in self.checked))\n    elif (role == Qt.DisplayRole):\n        if (column == NAME):\n            return to_qvariant(package.name)\n        elif (column == VERSION):\n            return to_qvariant(package.version)\n        elif (column == ACTION):\n            action = self.actions.get(package)\n            if (action is not None):\n                return to_qvariant(action)\n        elif (column == DESCRIPTION):\n            return to_qvariant(package.description)\n    elif (role == Qt.TextAlignmentRole):\n        if (column == ACTION):\n            return to_qvariant(int((Qt.AlignRight | Qt.AlignVCenter)))\n        else:\n            return to_qvariant(int((Qt.AlignLeft | Qt.AlignVCenter)))\n    elif (role == Qt.BackgroundColorRole):\n        if (package in self.checked):\n            color = QColor(Qt.darkGreen)\n            color.setAlphaF(0.1)\n            return to_qvariant(color)\n        else:\n            color = QColor(Qt.lightGray)\n            color.setAlphaF(0.3)\n            return to_qvariant(color)\n    return to_qvariant()\n", "label": 1}
{"function": "\n\ndef _get_focus_next(self, focus_dir):\n    current = self\n    walk_tree = ('walk' if (focus_dir is 'focus_next') else 'walk_reverse')\n    while 1:\n        while (getattr(current, focus_dir) is not None):\n            current = getattr(current, focus_dir)\n            if ((current is self) or (current is StopIteration)):\n                return None\n            if (current.is_focusable and (not current.disabled)):\n                return current\n        itr = getattr(current, walk_tree)(loopback=True)\n        if (focus_dir is 'focus_next'):\n            next(itr)\n        for current in itr:\n            if isinstance(current, FocusBehavior):\n                break\n        if isinstance(current, FocusBehavior):\n            if (current is self):\n                return None\n            if (current.is_focusable and (not current.disabled)):\n                return current\n        else:\n            return None\n", "label": 1}
{"function": "\n\ndef _create_result(cmd, params):\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ)\n    result = Result()\n    for line in p.stdout.readlines():\n        if (sys.version_info[0] == 3):\n            line = line.decode(sys.stdout.encoding)\n        result._add_stdout_line(line)\n    for line in p.stderr.readlines():\n        if (sys.version_info[0] == 3):\n            line = line.decode(sys.stderr.encoding)\n        result._add_stderr_line(line)\n    p.wait()\n    if ((_is_param_set(params, _PARAM_PRINT_STDOUT) or config.PRINT_STDOUT_ALWAYS) and (len(result.stdout) > 0)):\n        _print_stdout(result.stdout)\n    if ((_is_param_set(params, _PARAM_PRINT_STDERR) or config.PRINT_STDERR_ALWAYS) and (len(result.stderr) > 0)):\n        if _is_colorama_enabled():\n            _print_stderr(((Fore.RED + result.stderr) + Style.RESET_ALL))\n        else:\n            _print_stderr(result.stderr)\n    result.returncode = p.returncode\n    if ((p.returncode != 0) and (not _is_param_set(params, _PARAM_NO_THROW))):\n        raise NonZeroReturnCodeError(cmd, result)\n    return result\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    foo = 'aaa\\naaa\\naaa\\n'\n    result = list(chunked(foo, 5))\n    assert (len(result) == 3)\n    assert (result[0] == 'aaa\\n')\n    assert (result[1] == 'aaa\\n')\n    assert (result[2] == 'aaa\\n')\n    result = list(chunked(foo, 8))\n    assert (len(result) == 2)\n    assert (result[0] == 'aaa\\naaa\\n')\n    assert (result[1] == 'aaa\\n')\n    result = list(chunked(foo, 4))\n    assert (len(result) == 3)\n    assert (result[0] == 'aaa\\n')\n    assert (result[1] == 'aaa\\n')\n    assert (result[2] == 'aaa\\n')\n    foo = ('a' * 10)\n    result = list(chunked(foo, 2))\n    assert (len(result) == 5)\n    assert all(((r == 'aa') for r in result))\n    foo = 'aaaa\\naaaa'\n    result = list(chunked(foo, 3))\n    assert (len(result) == 4)\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kw):\n    super(DateConverter, self).__init__(*args, **kw)\n    month_style = (self.month_style or DateConverter.month_style).lower()\n    accept_day = bool(self.accept_day)\n    self.accept_day = self.accept_day\n    if (month_style in ('mdy', 'md', 'mm/dd/yyyy', 'mm/dd', 'us', 'american')):\n        month_style = 'mdy'\n    elif (month_style in ('dmy', 'dm', 'dd/mm/yyyy', 'dd/mm', 'euro', 'european')):\n        month_style = 'dmy'\n    elif (month_style in ('ymd', 'ym', 'yyyy/mm/dd', 'yyyy/mm', 'iso', 'china', 'chinese')):\n        month_style = 'ymd'\n    else:\n        raise TypeError(('Bad month_style: %r' % month_style))\n    self.month_style = month_style\n    separator = self.separator\n    if ((not separator) or (separator == 'auto')):\n        separator = dict(mdy='/', dmy='.', ymd='-')[month_style]\n    elif (separator not in ('-', '.', '/', '\\\\')):\n        raise TypeError(('Bad separator: %r' % separator))\n    self.separator = separator\n    self.format = separator.join((self._formats[part] for part in month_style if ((part != 'd') or accept_day)))\n    self.human_format = separator.join((self._human_formats[part] for part in month_style if ((part != 'd') or accept_day)))\n", "label": 1}
{"function": "\n\ndef _parse_see_also(self, content):\n    '\\n        func_name : Descriptive text\\n            continued text\\n        another_func_name : Descriptive text\\n        func_name1, func_name2, func_name3\\n\\n        '\n    functions = []\n    current_func = None\n    rest = []\n    for line in content:\n        if (not line.strip()):\n            continue\n        if (':' in line):\n            if current_func:\n                functions.append((current_func, rest))\n            r = line.split(':', 1)\n            current_func = r[0].strip()\n            r[1] = r[1].strip()\n            if r[1]:\n                rest = [r[1]]\n            else:\n                rest = []\n        elif (not line.startswith(' ')):\n            if current_func:\n                functions.append((current_func, rest))\n                current_func = None\n                rest = []\n            if (',' in line):\n                for func in line.split(','):\n                    func = func.strip()\n                    if func:\n                        functions.append((func, []))\n            elif line.strip():\n                current_func = line.strip()\n        elif (current_func is not None):\n            rest.append(line.strip())\n    if current_func:\n        functions.append((current_func, rest))\n    return functions\n", "label": 1}
{"function": "\n\ndef _subx(self, template, string, count=0, subn=False):\n    filter = template\n    if ((not callable(template)) and ('\\\\' in template)):\n        import re as sre\n        filter = sre._subx(self, template)\n    state = _State(string, 0, sys.maxsize, self.flags)\n    sublist = []\n    n = last_pos = 0\n    while ((not count) or (n < count)):\n        state.reset()\n        state.string_position = state.start\n        if (not state.search(self._code)):\n            break\n        if (last_pos < state.start):\n            sublist.append(string[last_pos:state.start])\n        if (not ((last_pos == state.start) and (last_pos == state.string_position) and (n > 0))):\n            if callable(filter):\n                sublist.append(filter(SRE_Match(self, state)))\n            else:\n                sublist.append(filter)\n            last_pos = state.string_position\n            n += 1\n        if (state.string_position == state.start):\n            state.start += 1\n        else:\n            state.start = state.string_position\n    if (last_pos < state.end):\n        sublist.append(string[last_pos:state.end])\n    item = ''.join(sublist)\n    if subn:\n        return (item, n)\n    else:\n        return item\n", "label": 1}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-d', '--delay', default=0, type=int)\n    args = parser.parse_args()\n    if sys.stdin.isatty():\n        parser.error('no input, pipe another btc command output into this command')\n    torrents = sys.stdin.read()\n    if (len(torrents.strip()) == 0):\n        exit(1)\n    try:\n        torrents = decoder.decode(torrents)\n    except ValueError:\n        error(('unexpected input: %s' % torrents))\n    time.sleep(args.delay)\n    hashes = [t['hash'] for t in torrents]\n    for h in hashes:\n        client.start_torrent(h)\n    while True:\n        d = list_to_dict(client.list_torrents(), 'hash')\n        all_started = True\n        for h in d:\n            if (h not in hashes):\n                continue\n            if (d[h]['state'] not in ('DOWNLOADING', 'DOWNLOADING_FORCED', 'SEEDING', 'SEEDING_FORCED', 'QUEUED_SEED')):\n                all_started = False\n                break\n        if all_started:\n            break\n        time.sleep(1)\n    if (not sys.stdout.isatty()):\n        d = list_to_dict(client.list_torrents(), 'hash')\n        d = dict(((h, d[h]) for h in hashes if (h in d)))\n        print(encoder.encode(dict_to_list(d, 'hash')))\n", "label": 1}
{"function": "\n\ndef normpath(path):\n    'Normalize path, eliminating double slashes, etc.'\n    (slash, dot) = (('/', '.') if isinstance(path, unicode) else ('/', '.'))\n    if (path == ''):\n        return dot\n    initial_slashes = path.startswith('/')\n    if (initial_slashes and path.startswith('//') and (not path.startswith('///'))):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if (comp in ('', '.')):\n            continue\n        if ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(- 1)] == '..'))):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = ((slash * initial_slashes) + path)\n    return (path or dot)\n", "label": 1}
{"function": "\n\ndef _validate_policies(self, policies):\n    '\\n        :param policies: list of policies\\n        '\n    for policy in policies:\n        if (int(policy) in self.by_index):\n            raise PolicyError(('Duplicate index %s conflicts with %s' % (policy, self.get_by_index(int(policy)))))\n        for name in policy.alias_list:\n            if (name.upper() in self.by_name):\n                raise PolicyError(('Duplicate name %s conflicts with %s' % (policy, self.get_by_name(name))))\n        if policy.is_default:\n            if (not self.default):\n                self.default = policy\n            else:\n                raise PolicyError(('Duplicate default %s conflicts with %s' % (policy, self.default)))\n        self._add_policy(policy)\n    if (0 not in self.by_index):\n        if (len(self) != 0):\n            raise PolicyError('You must specify a storage policy section for policy index 0 in order to define multiple policies')\n        self._add_policy(StoragePolicy(0, name=LEGACY_POLICY_NAME))\n    enabled_policies = [p for p in self if (not p.is_deprecated)]\n    if (not enabled_policies):\n        raise PolicyError(\"Unable to find policy that's not deprecated!\")\n    if (not self.default):\n        if (len(self) > 1):\n            raise PolicyError('Unable to find default policy')\n        self.default = self[0]\n        self.default.is_default = True\n", "label": 1}
{"function": "\n\ndef vote(self):\n    if self.lock:\n        return\n    self.log('in vote', proposal=self.proposal, pid=id(self.proposal))\n    last_lock = self.hm.last_lock\n    if self.proposal:\n        if isinstance(self.proposal, VotingInstruction):\n            assert self.proposal.lockset.has_quorum_possible\n            self.log('voting on instruction')\n            v = VoteBlock(self.height, self.round, self.proposal.blockhash)\n        elif (not isinstance(last_lock, VoteBlock)):\n            assert isinstance(self.proposal, BlockProposal)\n            assert isinstance(self.proposal.block, Block)\n            assert (self.proposal.lockset.has_noquorum or (self.round == 0))\n            assert (self.proposal.block.prevhash == self.cm.head.hash)\n            self.log('voting proposed block')\n            v = VoteBlock(self.height, self.round, self.proposal.blockhash)\n        else:\n            self.log('voting on last vote')\n            v = VoteBlock(self.height, self.round, last_lock.blockhash)\n    elif ((self.timeout_time is not None) and (self.cm.chainservice.now >= self.timeout_time)):\n        if isinstance(last_lock, VoteBlock):\n            self.log('timeout voting on last vote')\n            v = VoteBlock(self.height, self.round, last_lock.blockhash)\n        else:\n            self.log('timeout voting not locked')\n            v = VoteNil(self.height, self.round)\n    else:\n        return\n    self.cm.sign(v)\n    self.log('voted', vote=v)\n    self.lock = v\n    assert (self.hm.last_lock == self.lock)\n    self.lockset.add(v)\n    return v\n", "label": 1}
{"function": "\n\ndef _prepare_imports(self, dicts):\n    ' an override for prepare imports that sorts the imports by parent_id dependencies '\n    pseudo_ids = set()\n    pseudo_matches = {\n        \n    }\n    prepared = dict(super(OrganizationImporter, self)._prepare_imports(dicts))\n    for (_, data) in prepared.items():\n        parent_id = (data.get('parent_id', None) or '')\n        if parent_id.startswith('~'):\n            pseudo_ids.add(parent_id)\n    pseudo_ids = [(ppid, get_pseudo_id(ppid)) for ppid in pseudo_ids]\n    for (json_id, data) in prepared.items():\n        for (ppid, spec) in pseudo_ids:\n            match = True\n            for (k, v) in spec.items():\n                if (data[k] != v):\n                    match = False\n                    break\n            if match:\n                if (ppid in pseudo_matches):\n                    raise UnresolvedIdError(('multiple matches for pseudo id: ' + ppid))\n                pseudo_matches[ppid] = json_id\n    network = Network()\n    in_network = set()\n    import_order = []\n    for (json_id, data) in prepared.items():\n        parent_id = data.get('parent_id', None)\n        if (parent_id in pseudo_matches):\n            parent_id = pseudo_matches[parent_id]\n        network.add_node(json_id)\n        if parent_id:\n            network.add_edge(parent_id, json_id)\n    for jid in network.sort():\n        import_order.append((jid, prepared[jid]))\n        in_network.add(jid)\n    if (in_network != set(prepared.keys())):\n        raise PupaInternalError('import is missing nodes in network set')\n    return import_order\n", "label": 1}
{"function": "\n\ndef traverse(obj, *path, **kwargs):\n    '\\n    Traverse the object we receive with the given path. Path\\n    items can be either strings or lists of strings (or any\\n    nested combination thereof). Behavior in given cases is\\n    laid out line by line below.\\n    '\n    if path:\n        if (isinstance(obj, list) or isinstance(obj, tuple)):\n            return [traverse(x, *path) for x in obj]\n        elif isinstance(obj, dict):\n            if (isinstance(path[0], list) or isinstance(path[0], tuple)):\n                for branch in path[0]:\n                    if (not isinstance(branch, basestring)):\n                        raise TraversalError(obj, path[0])\n                return {name: traverse(obj[name], *path[1:], split=True) for name in path[0]}\n            elif (not isinstance(path[0], basestring)):\n                raise TraversalError(obj, path[0])\n            elif (path[0] == '\\\\*'):\n                return {name: traverse(item, *path[1:], split=True) for (name, item) in obj.items()}\n            elif (path[0] in obj):\n                return traverse(obj[path[0]], *path[1:])\n            else:\n                raise TraversalError(obj, path[0])\n        elif kwargs.get('split', False):\n            return obj\n        else:\n            raise TraversalError(obj, path[0])\n    else:\n        return obj\n", "label": 1}
{"function": "\n\ndef _check_command_response(response, reset, msg=None, allowable_errors=None):\n    'Check the response to a command for errors.\\n    '\n    if ('ok' not in response):\n        raise OperationFailure(response.get('$err'), response.get('code'), response)\n    if response.get('wtimeout', False):\n        raise WTimeoutError(response.get('errmsg', response.get('err')), response.get('code'), response)\n    if (not response['ok']):\n        details = response\n        if ('raw' in response):\n            for shard in response['raw'].itervalues():\n                if (not shard.get('ok')):\n                    details = shard\n                    break\n        errmsg = details['errmsg']\n        if ((allowable_errors is None) or (errmsg not in allowable_errors)):\n            if (errmsg.startswith('not master') or errmsg.startswith('node is recovering')):\n                if (reset is not None):\n                    reset()\n                raise AutoReconnect(errmsg)\n            if (errmsg == 'db assertion failure'):\n                errmsg = (\"db assertion failure, assertion: '%s'\" % details.get('assertion', ''))\n                raise OperationFailure(errmsg, details.get('assertionCode'), response)\n            code = details.get('code')\n            if (code in (11000, 11001, 12582)):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif (code == 50):\n                raise ExecutionTimeout(errmsg, code, response)\n            msg = (msg or '%s')\n            raise OperationFailure((msg % errmsg), code, response)\n", "label": 1}
{"function": "\n\ndef cancelUpload(self, upload):\n    '\\n        Delete the temporary files associated with a given upload.\\n        '\n    if ('s3' not in upload):\n        return\n    if ('key' not in upload['s3']):\n        return\n    bucket = self._getBucket()\n    if bucket:\n        key = bucket.get_key(upload['s3']['key'], validate=True)\n        if key:\n            bucket.delete_key(key)\n        if (('s3' in upload) and ('uploadId' in upload['s3']) and ('key' in upload['s3'])):\n            getParams = {\n                \n            }\n            while True:\n                try:\n                    multipartUploads = bucket.get_all_multipart_uploads(**getParams)\n                except boto.exception.S3ResponseError:\n                    break\n                if (not len(multipartUploads)):\n                    break\n                for multipartUpload in multipartUploads:\n                    if ((multipartUpload.id == upload['s3']['uploadId']) and (multipartUpload.key_name == upload['s3']['key'])):\n                        multipartUpload.cancel_upload()\n                if (not multipartUploads.is_truncated):\n                    break\n                getParams['key_marker'] = multipartUploads.next_key_marker\n                getParams['upload_id_marker'] = multipartUploads.next_upload_id_marker\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.schema = hive_metastore.ttypes.Schema()\n                self.schema.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.table_dir = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.in_tablename = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.delim = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    self.use_greenlets = False\n    self.auto_start_request = False\n    kwargs.pop('auto_start_request', None)\n    if (('read_preference' not in kwargs) and (kwargs.get('tag_sets') not in (None, [], [{\n        \n    }]))):\n        raise ConfigurationError()\n    if (('read_preference' not in kwargs) and (('slaveok' in kwargs) or ('slave_okay' in kwargs))):\n        secondary = kwargs.pop('slave_okay', kwargs.pop('slaveok', False))\n        kwargs['read_preference'] = (ReadPreference.SECONDARY_PREFERRED if secondary else ReadPreference.PRIMARY)\n    gle_opts = dict([(k, v) for (k, v) in kwargs.items() if (k in SAFE_OPTIONS)])\n    if (gle_opts and ('w' not in gle_opts)):\n        kwargs['w'] = 1\n    if ('safe' in kwargs):\n        safe = kwargs.pop('safe')\n        if (not safe):\n            kwargs.setdefault('w', 0)\n    self.delegate = kwargs.pop('delegate', None)\n    if (not self.delegate):\n        self.delegate = self.__delegate_class__(*args, **kwargs)\n        if kwargs.get('_connect', True):\n            self.synchro_connect()\n", "label": 1}
{"function": "\n\ndef poll(self, timeout):\n    self._lock.acquire()\n    if (timeout == 0):\n        self.poll_timeout = 0\n    elif self._timeouts:\n        self.poll_timeout = (self._timeouts[0][0] - _time())\n        if (self.poll_timeout < 0.0001):\n            self.poll_timeout = 0\n        elif (timeout is not None):\n            self.poll_timeout = min(timeout, self.poll_timeout)\n    elif (timeout is None):\n        self.poll_timeout = _AsyncNotifier._Block\n    else:\n        self.poll_timeout = timeout\n    timeout = self.poll_timeout\n    self._lock.release()\n    if (timeout and (timeout != _AsyncNotifier._Block)):\n        timeout = int((timeout * 1000))\n    (err, n, key, overlap) = win32file.GetQueuedCompletionStatus(self.iocp, timeout)\n    while (err != winerror.WAIT_TIMEOUT):\n        if (overlap and overlap.object):\n            overlap.object(err, n)\n        else:\n            logger.warning('invalid overlap!')\n        (err, n, key, overlap) = win32file.GetQueuedCompletionStatus(self.iocp, 0)\n    self.poll_timeout = 0\n    if (timeout == 0):\n        now = _time()\n        self._lock.acquire()\n        while (self._timeouts and (self._timeouts[0][0] <= now)):\n            (fd_timeout, fd) = self._timeouts.pop(0)\n            if (fd._timeout_id == fd_timeout):\n                fd._timeout_id = None\n                fd._timed_out()\n        self._lock.release()\n", "label": 1}
{"function": "\n\n@classmethod\ndef put(cls, kvs, entity=None):\n    for k in kvs.keys():\n        if (not (k in cls.properties())):\n            del kvs[k]\n            continue\n        v = cls.__dict__[k]\n        if isinstance(v, db.IntegerProperty):\n            kvs[k] = int(kvs[k])\n        elif isinstance(v, db.FloatProperty):\n            kvs[k] = float(kvs[k])\n        elif isinstance(v, db.BooleanProperty):\n            kvs[k] = (True if (kvs[k] == 'True') else False)\n        elif (isinstance(v, db.StringProperty) or isinstance(v, db.TextProperty)):\n            kvs[k] = kvs[k]\n        elif isinstance(v, db.DateProperty):\n            kvs[k] = datetime.strptime(kvs[k], settings.DATE_FORMAT).date()\n        elif isinstance(v, db.LinkProperty):\n            kvs[k] = kvs[k]\n        elif isinstance(v, db.EmailProperty):\n            kvs[k] = kvs[k]\n        else:\n            raise UnsupportedFieldTypeError(v)\n    cls.validate(kvs)\n    try:\n        if (not entity):\n            entity = cls(**kvs)\n        else:\n            for k in kvs.keys():\n                setattr(entity, k, kvs[k])\n        db.put(entity)\n    except Exception as e:\n        logging.error((\"Couldn't put entity: %s\" % kvs))\n        logging.error(e)\n        return None\n    return entity\n", "label": 1}
{"function": "\n\ndef MoveWindow(windowID, xpos=None, ypos=None, width=None, height=None, center=None):\n    (left, top, right, bottom) = win32gui.GetWindowRect(windowID)\n    if ((xpos is None) and (ypos is None)):\n        xpos = left\n        ypos = top\n    if ((width is None) and (height is None)):\n        width = (right - left)\n        height = (bottom - top)\n    if ((xpos is None) and (ypos is not None)):\n        xpos = left\n    if ((ypos is None) and (xpos is not None)):\n        ypos = top\n    if (not width):\n        width = (right - left)\n    if (not height):\n        height = (bottom - top)\n    if center:\n        screenx = win32api.GetSystemMetrics(win32con.SM_CXSCREEN)\n        screeny = win32api.GetSystemMetrics(win32con.SM_CYSCREEN)\n        xpos = int(math.floor(((screenx - width) / 2)))\n        ypos = int(math.floor(((screeny - height) / 2)))\n        if (xpos < 0):\n            xpos = 0\n        if (ypos < 0):\n            ypos = 0\n    win32gui.MoveWindow(windowID, xpos, ypos, width, height, 1)\n", "label": 1}
{"function": "\n\ndef node_clique_number(G, nodes=None, cliques=None):\n    ' Returns the size of the largest maximal clique containing\\n    each given node.\\n\\n    Returns a single or list depending on input nodes.\\n    Optional list of cliques can be input if already computed.\\n    '\n    if (cliques is None):\n        if (nodes is not None):\n            if isinstance(nodes, list):\n                d = {\n                    \n                }\n                for n in nodes:\n                    H = networkx.ego_graph(G, n)\n                    d[n] = max((len(c) for c in find_cliques(H)))\n            else:\n                H = networkx.ego_graph(G, nodes)\n                d = max((len(c) for c in find_cliques(H)))\n            return d\n        cliques = list(find_cliques(G))\n    if (nodes is None):\n        nodes = list(G.nodes())\n    if (not isinstance(nodes, list)):\n        v = nodes\n        d = max([len(c) for c in cliques if (v in c)])\n    else:\n        d = {\n            \n        }\n        for v in nodes:\n            d[v] = max([len(c) for c in cliques if (v in c)])\n    return d\n", "label": 1}
{"function": "\n\ndef __gt__(self, other):\n    if (self.date == 'infinity'):\n        if isinstance(other, Date):\n            return (other.date != 'infinity')\n        else:\n            from .Range import Range\n            if isinstance(other, Range):\n                return (other.end != 'infinity')\n            return (other != 'infinity')\n    elif isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return False\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) > other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date > other.date.replace(tzinfo=self.tz))\n        return (self.date > other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            if (other.end.date == 'infinity'):\n                return False\n            if (other.end.tz and (self.tz is None)):\n                return (self.date.replace(tzinfo=other.end.tz) > other.end.date)\n            elif (self.tz and (other.end.tz is None)):\n                return (self.date > other.end.date.replace(tzinfo=self.tz))\n            return (self.date > other.end.date)\n        else:\n            return self.__gt__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args):\n    ' Construct a Trace object.\\n\\n        Parameters\\n        ==========\\n        args = sympy expression\\n        indices = tuple/list if indices, optional\\n\\n        '\n    if (len(args) == 2):\n        if (not isinstance(args[1], (list, Tuple, tuple))):\n            indices = Tuple(args[1])\n        else:\n            indices = Tuple(*args[1])\n        expr = args[0]\n    elif (len(args) == 1):\n        indices = Tuple()\n        expr = args[0]\n    else:\n        raise ValueError('Arguments to Tr should be of form (expr[, [indices]])')\n    if isinstance(expr, Matrix):\n        return expr.trace()\n    elif (hasattr(expr, 'trace') and callable(expr.trace)):\n        return expr.trace()\n    elif isinstance(expr, Add):\n        return Add(*[Tr(arg, indices) for arg in expr.args])\n    elif isinstance(expr, Mul):\n        (c_part, nc_part) = expr.args_cnc()\n        if (len(nc_part) == 0):\n            return Mul(*c_part)\n        else:\n            obj = Expr.__new__(cls, Mul(*nc_part), indices)\n            return ((Mul(*c_part) * obj) if (len(c_part) > 0) else obj)\n    elif isinstance(expr, Pow):\n        if (_is_scalar(expr.args[0]) and _is_scalar(expr.args[1])):\n            return expr\n        else:\n            return Expr.__new__(cls, expr, indices)\n    else:\n        if _is_scalar(expr):\n            return expr\n        return Expr.__new__(cls, expr, indices)\n", "label": 1}
{"function": "\n\ndef process_pdu(self, pdu):\n    ' Process a PDU by sending a copy to each node as dictated by the\\n            addressing and if a node is promiscuous.\\n        '\n    if _debug:\n        Network._debug('process_pdu %r', pdu)\n    if (self.dropPercent != 0.0):\n        if ((random.random() * 100.0) < self.dropPercent):\n            if _debug:\n                Network._debug('    - packet dropped')\n            return\n    if ((not pdu.pduDestination) or (not isinstance(pdu.pduDestination, Address))):\n        raise RuntimeError('invalid destination address')\n    elif (pdu.pduDestination.addrType == Address.localBroadcastAddr):\n        for n in self.nodes:\n            if (pdu.pduSource != n.address):\n                n.response(deepcopy(pdu))\n    elif (pdu.pduDestination.addrType == Address.localStationAddr):\n        for n in self.nodes:\n            if (n.promiscuous or (pdu.pduDestination == n.address)):\n                n.response(deepcopy(pdu))\n    else:\n        raise RuntimeError('invalid destination address type')\n", "label": 1}
{"function": "\n\ndef dispatch(self, request, *args, **kwargs):\n    '\\n        Override dispatch to call appropriate methods:\\n        * $query - ng_query\\n        * $get - ng_get\\n        * $save - ng_save\\n        * $delete and $remove - ng_delete\\n        '\n    allowed_methods = self.get_allowed_methods()\n    try:\n        if ((request.method == 'GET') and ('GET' in allowed_methods)):\n            if (('pk' in request.GET) or (self.slug_field in request.GET)):\n                return self.ng_get(request, *args, **kwargs)\n            return self.ng_query(request, *args, **kwargs)\n        elif ((request.method == 'POST') and ('POST' in allowed_methods)):\n            return self.ng_save(request, *args, **kwargs)\n        elif ((request.method == 'DELETE') and ('DELETE' in allowed_methods)):\n            return self.ng_delete(request, *args, **kwargs)\n    except self.model.DoesNotExist as e:\n        return self.error_json_response(e.args[0], 404)\n    except NgMissingParameterError as e:\n        return self.error_json_response(e.args[0])\n    except JSONResponseException as e:\n        return self.error_json_response(e.args[0], e.status_code)\n    except ValidationError as e:\n        if hasattr(e, 'error_dict'):\n            return self.error_json_response('Form not valid', detail=e.message_dict)\n        else:\n            return self.error_json_response(e.message)\n    return self.error_json_response('This view can not handle method {0}'.format(request.method), 405)\n", "label": 1}
{"function": "\n\ndef __init__(self, options, default):\n    self.default = default\n    (args, kwargs) = ([], {\n        \n    })\n    if options:\n        for (key, val) in OPTIONS_PARSER.findall(options):\n            if (val.lower() == 'none'):\n                val = None\n            elif (val.lower() == 'true'):\n                val = True\n            elif (val.lower() == 'false'):\n                val = False\n            else:\n                try:\n                    val = int(val)\n                except ValueError:\n                    try:\n                        val = float(val)\n                    except ValueError:\n                        pass\n            if isinstance(val, basestring):\n                val = val\n                if (((val[0] == '\"') and (val[(- 1)] == '\"')) or ((val[0] == \"'\") and (val[(- 1)] == \"'\"))):\n                    val = val[1:(- 1)]\n            if key:\n                kwargs[key] = val\n            else:\n                args.append(val)\n    self.configure(*args, **kwargs)\n    self._regex = re.compile(('^%s$' % self.regex))\n    self.capture_groups = self._regex.groups\n", "label": 1}
{"function": "\n\ndef all_media(self, from_apps=None):\n    from corehq.apps.hqmedia.models import CommCareMultimedia\n    dom_with_media = (self if (not self.is_snapshot) else self.copied_from)\n    if self.is_snapshot:\n        app_ids = [app.copied_from.get_id for app in self.full_applications()]\n        if from_apps:\n            from_apps = set([a_id for a_id in app_ids if (a_id in from_apps)])\n        else:\n            from_apps = app_ids\n    if from_apps:\n        media = []\n        media_ids = set()\n        apps = [app for app in dom_with_media.full_applications() if (app.get_id in from_apps)]\n        for app in apps:\n            if (app.doc_type != 'Application'):\n                continue\n            for (_, m) in app.get_media_objects():\n                if (m.get_id not in media_ids):\n                    media.append(m)\n                    media_ids.add(m.get_id)\n        return media\n    return CommCareMultimedia.view('hqmedia/by_domain', key=dom_with_media.name, include_docs=True).all()\n", "label": 1}
{"function": "\n\ndef telescopic(L, R, limits):\n    'Tries to perform the summation using the telescopic property\\n\\n    return None if not possible\\n    '\n    (i, a, b) = limits\n    if (L.is_Add or R.is_Add):\n        return None\n    k = Wild('k')\n    sol = (- R).match(L.subs(i, (i + k)))\n    s = None\n    if (sol and (k in sol)):\n        s = sol[k]\n        if (not (s.is_Integer and (L.subs(i, (i + s)) == (- R)))):\n            s = None\n    if (s is None):\n        m = Dummy('m')\n        try:\n            sol = (solve((L.subs(i, (i + m)) + R), m) or [])\n        except NotImplementedError:\n            return None\n        sol = [si for si in sol if (si.is_Integer and (L.subs(i, (i + si)) + R).expand().is_zero)]\n        if (len(sol) != 1):\n            return None\n        s = sol[0]\n    if (s < 0):\n        return telescopic_direct(R, L, abs(s), (i, a, b))\n    elif (s > 0):\n        return telescopic_direct(L, R, s, (i, a, b))\n", "label": 1}
{"function": "\n\ndef get_linked_doctypes(columns, data):\n    linked_doctypes = {\n        \n    }\n    columns_dict = get_columns_dict(columns)\n    for (idx, col) in enumerate(columns):\n        df = columns_dict[idx]\n        if (df.get('fieldtype') == 'Link'):\n            if isinstance(col, basestring):\n                linked_doctypes[df['options']] = idx\n            else:\n                linked_doctypes[df['options']] = df['fieldname']\n    columns_with_value = []\n    for row in data:\n        if row:\n            if (len(row) != len(columns_with_value)):\n                if isinstance(row, (list, tuple)):\n                    row = enumerate(row)\n                elif isinstance(row, dict):\n                    row = row.items()\n                for (col, val) in row:\n                    if (val and (col not in columns_with_value)):\n                        columns_with_value.append(col)\n    for (doctype, key) in linked_doctypes.items():\n        if (key not in columns_with_value):\n            del linked_doctypes[doctype]\n    return linked_doctypes\n", "label": 1}
{"function": "\n\ndef doWaitForMultipleEvents(self, timeout):\n    log.msg(channel='system', event='iteration', reactor=self)\n    if (timeout is None):\n        timeout = 100\n    ranUserCode = False\n    for reader in self._closedAndReading.keys():\n        ranUserCode = True\n        self._runAction('doRead', reader)\n    for fd in self._writes.keys():\n        ranUserCode = True\n        log.callWithLogger(fd, self._runWrite, fd)\n    if ranUserCode:\n        timeout = 0\n    if (not (self._events or self._writes)):\n        time.sleep(timeout)\n        return\n    handles = (self._events.keys() or [self.dummyEvent])\n    timeout = int((timeout * 1000))\n    val = MsgWaitForMultipleObjects(handles, 0, timeout, QS_ALLINPUT)\n    if (val == WAIT_TIMEOUT):\n        return\n    elif (val == (WAIT_OBJECT_0 + len(handles))):\n        exit = win32gui.PumpWaitingMessages()\n        if exit:\n            self.callLater(0, self.stop)\n            return\n    elif ((val >= WAIT_OBJECT_0) and (val < (WAIT_OBJECT_0 + len(handles)))):\n        event = handles[(val - WAIT_OBJECT_0)]\n        (fd, action) = self._events[event]\n        if (fd in self._reads):\n            fileno = fd.fileno()\n            if (fileno == (- 1)):\n                self._disconnectSelectable(fd, posixbase._NO_FILEDESC, False)\n                return\n            events = WSAEnumNetworkEvents(fileno, event)\n            if (FD_CLOSE in events):\n                self._closedAndReading[fd] = True\n        log.callWithLogger(fd, self._runAction, action, fd)\n", "label": 1}
{"function": "\n\ndef mix_codes(self, agents, actors):\n    '\\n        Combine the actor codes and agent codes addressing duplicates\\n        and removing the general \"~PPL\" if there\\'s a better option.\\n        \\n        Parameters\\n        -----------\\n        agents, actors : Lists of their respective codes\\n        \\n        \\n        Returns\\n        -------\\n        codes: list\\n               [Agent codes] x [Actor codes]\\n        \\n        '\n    codes = set()\n    mix = (lambda a, b: ((a + b) if (not (b in a)) else a))\n    actors = (actors if actors else ['~'])\n    for ag in agents:\n        if ((ag == '~PPL') and (len(agents) > 1)):\n            continue\n        actors = map((lambda a: mix(a, ag[1:])), actors)\n    return filter((lambda a: (a not in ['', '~', '~~', None])), actors)\n    codes = set()\n    print('WTF-1')\n    for act in (actors if actors else ['~']):\n        for ag in (agents if agents else ['~']):\n            if ((ag == '~PPL') and (len(agents) > 1)):\n                continue\n            code = act\n            if (not (ag[1:] in act)):\n                code += ag[1:]\n            if (not (code in ['~', '~~', ''])):\n                codes.add(code)\n    return list(codes)\n", "label": 1}
{"function": "\n\ndef _read_object(self, relpath, max_symlinks):\n    path_so_far = ''\n    components = list(relpath.split(os.path.sep))\n    symlinks = 0\n    while components:\n        component = components.pop(0)\n        if ((component == '') or (component == '.')):\n            continue\n        parent_tree = self._read_tree(path_so_far)\n        parent_path = path_so_far\n        if (path_so_far != ''):\n            path_so_far += '/'\n        path_so_far += component\n        try:\n            obj = parent_tree[component]\n        except KeyError:\n            raise self.MissingFileException(self.rev, relpath)\n        if isinstance(obj, self.File):\n            if components:\n                raise self.NotADirException(self.rev, relpath)\n            else:\n                return (obj, path_so_far)\n        elif isinstance(obj, self.Dir):\n            if (not components):\n                return (obj, (path_so_far + '/'))\n        elif isinstance(obj, self.Symlink):\n            symlinks += 1\n            if (symlinks > max_symlinks):\n                return (obj, path_so_far)\n            (object_type, path_data) = self._read_object_from_repo(sha=obj.sha)\n            assert (object_type == 'blob')\n            if (path_data[0] == '/'):\n                raise self.ExternalSymlinkException(self.rev, relpath)\n            link_to = os.path.normpath(os.path.join(parent_path, path_data))\n            if (link_to.startswith('../') or (link_to[0] == '/')):\n                raise self.ExternalSymlinkException(self.rev, relpath)\n            components = (link_to.split(SLASH) + components)\n            path_so_far = ''\n        else:\n            raise self.UnexpectedGitObjectTypeException()\n    return (self.Dir('./', None), './')\n", "label": 1}
{"function": "\n\ndef update(self):\n    'Get the latest data from Transmission and updates the state.'\n    self.refresh_transmission_data()\n    if (self.type == 'current_status'):\n        if self.transmission_client.session:\n            upload = self.transmission_client.session.uploadSpeed\n            download = self.transmission_client.session.downloadSpeed\n            if ((upload > 0) and (download > 0)):\n                self._state = 'Up/Down'\n            elif ((upload > 0) and (download == 0)):\n                self._state = 'Seeding'\n            elif ((upload == 0) and (download > 0)):\n                self._state = 'Downloading'\n            else:\n                self._state = 'Idle'\n        else:\n            self._state = 'Unknown'\n    if self.transmission_client.session:\n        if (self.type == 'download_speed'):\n            mb_spd = float(self.transmission_client.session.downloadSpeed)\n            mb_spd = ((mb_spd / 1024) / 1024)\n            self._state = round(mb_spd, (2 if (mb_spd < 0.1) else 1))\n        elif (self.type == 'upload_speed'):\n            mb_spd = float(self.transmission_client.session.uploadSpeed)\n            mb_spd = ((mb_spd / 1024) / 1024)\n            self._state = round(mb_spd, (2 if (mb_spd < 0.1) else 1))\n", "label": 1}
{"function": "\n\ndef test_field_assumptions():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.real_elements(X), Q.real_elements(X))\n    assert (not ask(Q.integer_elements(X), Q.real_elements(X)))\n    assert ask(Q.complex_elements(X), Q.real_elements(X))\n    assert (ask(Q.real_elements((X + Y)), Q.real_elements(X)) is None)\n    assert ask(Q.real_elements((X + Y)), (Q.real_elements(X) & Q.real_elements(Y)))\n    from sympy.matrices.expressions.hadamard import HadamardProduct\n    assert ask(Q.real_elements(HadamardProduct(X, Y)), (Q.real_elements(X) & Q.real_elements(Y)))\n    assert ask(Q.complex_elements((X + Y)), (Q.real_elements(X) & Q.complex_elements(Y)))\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X.I), (Q.real_elements(X) & Q.invertible(X)))\n    assert ask(Q.real_elements(Trace(X)), Q.real_elements(X))\n    assert ask(Q.integer_elements(Determinant(X)), Q.integer_elements(X))\n    assert (not ask(Q.integer_elements(X.I), Q.integer_elements(X)))\n    alpha = Symbol('alpha')\n    assert ask(Q.real_elements((alpha * X)), (Q.real_elements(X) & Q.real(alpha)))\n    assert ask(Q.real_elements(LofLU(X)), Q.real_elements(X))\n", "label": 1}
{"function": "\n\ndef _node_for(self, object):\n    ' Returns the TreeNode associated with a specified object.\\n        '\n    if ((type(object) is tuple) and (len(object) == 2) and isinstance(object[1], TreeNode)):\n        return object\n    factory = self.factory\n    nodes = [node for node in factory.nodes if node.is_node_for(object)]\n    if (len(nodes) == 1):\n        return (object, nodes[0])\n    if (len(nodes) == 0):\n        return (object, ITreeNodeAdapterBridge(adapter=object))\n    base = nodes[0].node_for\n    nodes = [node for node in nodes if (base == node.node_for)]\n    if (len(nodes) == 1):\n        return (object, nodes[0])\n    root_node = None\n    for (i, node) in enumerate(nodes):\n        if (node.children == ''):\n            root_node = node\n            del nodes[i]\n            break\n    else:\n        root_node = nodes[0]\n    key = ((root_node,) + tuple(nodes))\n    if (key in factory.multi_nodes):\n        return (object, factory.multi_nodes[key])\n    factory.multi_nodes[key] = multi_node = MultiTreeNode(root_node=root_node, nodes=nodes)\n    return (object, multi_node)\n", "label": 1}
{"function": "\n\ndef ping_received(self, on_sock, sender_nid, version, t):\n    if ((on_sock == OUT) and (sender_nid not in self.channels_out)):\n        return\n    self.last_seen[sender_nid] = t\n    if ((on_sock == IN) and (sender_nid not in self.channels_in)):\n        self.channels_in.add(sender_nid)\n        if self.is_relay:\n            (yield (RelaySigNew, IN, sender_nid))\n        elif (sender_nid in self.cl_relayees):\n            relay_nid = self.cl_relayees.pop(sender_nid)\n            self.cl_avail_relays[relay_nid].remove(sender_nid)\n            (yield (RelayNvm, (IN if (relay_nid in self.channels_in) else OUT), relay_nid, sender_nid))\n    inout = (IN if (sender_nid in self.channels_in) else OUT)\n    if self._needs_ping(sender_nid, t):\n        (yield (Ping, inout, sender_nid, self._next_version()))\n    if (sender_nid in self.queues):\n        for msg_h in self.queues.pop(sender_nid):\n            (yield (Send, inout, sender_nid, self._next_version(), msg_h))\n    elif (not (version > self.versions.get(sender_nid, (- 1)))):\n        self.versions[sender_nid] = version\n        assert (sender_nid not in self.queues)\n        if (sender_nid in self.cl_avail_relays):\n            self._handle_relay_down(sender_nid)\n        (yield (NodeDown, sender_nid))\n    self.versions[sender_nid] = version\n", "label": 1}
{"function": "\n\ndef ensure_completely_loaded(force=False):\n    '\\n    This method ensures all models are completely loaded\\n\\n    FeinCMS requires Django to be completely initialized before proceeding,\\n    because of the extension mechanism and the dynamically created content\\n    types.\\n\\n    For more informations, have a look at issue #23 on github:\\n    http://github.com/feincms/feincms/issues#issue/23\\n    '\n    global COMPLETELY_LOADED\n    if (COMPLETELY_LOADED and (not force)):\n        return True\n    try:\n        from django.apps import apps\n    except ImportError:\n        from django.db.models import loading as apps\n    else:\n        if (not apps.ready):\n            return\n    import django\n    if (django.get_version() < '1.8'):\n        from feincms._internal import get_models\n        for model in get_models():\n            for cache_name in ('_field_cache', '_field_name_cache', '_m2m_cache', '_related_objects_cache', '_related_many_to_many_cache', '_name_map'):\n                try:\n                    delattr(model._meta, cache_name)\n                except AttributeError:\n                    pass\n            model._meta._fill_fields_cache()\n        if hasattr(apps, 'cache'):\n            try:\n                apps.cache.get_models.cache_clear()\n            except AttributeError:\n                apps.cache._get_models_cache.clear()\n    if hasattr(apps, 'ready'):\n        if apps.ready:\n            COMPLETELY_LOADED = True\n    elif apps.app_cache_ready():\n        COMPLETELY_LOADED = True\n    return True\n", "label": 1}
{"function": "\n\ndef test_file_magic():\n    kernel = get_kernel()\n    kernel.do_execute('%%file TEST.txt\\nLINE1\\nLINE2\\nLINE3', False)\n    assert os.path.exists('TEST.txt')\n    with open('TEST.txt') as fp:\n        lines = fp.readlines()\n        assert (len(lines) == 3)\n        assert (lines[0] == 'LINE1\\n')\n        assert (lines[1] == 'LINE2\\n')\n        assert (lines[2] == 'LINE3')\n    kernel.do_execute('%%file -a TEST.txt\\n\\nLINE4\\nLINE5\\nLINE6', False)\n    assert os.path.exists('TEST.txt')\n    with open('TEST.txt') as fp:\n        lines = fp.readlines()\n        assert (len(lines) == 6)\n        assert (lines[3] == 'LINE4\\n')\n        assert (lines[4] == 'LINE5\\n')\n        assert (lines[5] == 'LINE6')\n    kernel.do_execute('%%file /tmp/tmp/TEST.txt\\nTEST1\\nTEST2\\nTEST3')\n    with open('/tmp/tmp/TEST.txt') as fp:\n        lines = fp.readlines()\n        assert (len(lines) == 3)\n        assert (lines[0] == 'TEST1\\n')\n        assert (lines[1] == 'TEST2\\n')\n        assert (lines[2] == 'TEST3')\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.ip = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.start_time = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.file = PacketCaptureFile()\n                self.file.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.pid = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef resolve_pattern(self, dirname, pattern, globstar_with_root):\n    \"Apply ``pattern`` (contains no path elements) to the\\n        literal directory`` in dirname``.\\n\\n        If pattern=='', this will filter for directories. This is\\n        a special case that happens when the user's glob expression ends\\n        with a slash (in which case we only want directories). It simpler\\n        and faster to filter here than in :meth:`_iglob`.\\n        \"\n    if (isinstance(pattern, six.text_type) and (not isinstance(dirname, six.text_type))):\n        dirname = six.u(dirname, (sys.getfilesystemencoding() or sys.getdefaultencoding()))\n    if (not has_magic(pattern)):\n        if (pattern == ''):\n            if self.isdir(dirname):\n                return [(pattern, ())]\n        elif self.exists(os.path.join(dirname, pattern)):\n            return [(pattern, ())]\n        return []\n    if (not dirname):\n        dirname = os.curdir\n    try:\n        if (pattern == '**'):\n            names = ([''] if globstar_with_root else [])\n            for (top, entries) in self.walk(dirname):\n                _mkabs = (lambda s: os.path.join(top[(len(dirname) + 1):], s))\n                names.extend(map(_mkabs, entries))\n            pattern = '*'\n        else:\n            names = self.listdir(dirname)\n    except os.error:\n        return []\n    if (pattern[0] != '.'):\n        names = filter((lambda x: ((not x) or (x[0] != '.'))), names)\n    return fnmatch_filter(names, pattern)\n", "label": 1}
{"function": "\n\ndef extract_concerns(module_text):\n    concerns = []\n    with contextlib.closing(cStringIO.StringIO(module_text)) as text:\n        tokens = tokenize.generate_tokens(text.readline)\n        function_id = None\n        for token in tokens:\n            if ((token[0] == tokenize.NAME) and token[4].lstrip().startswith('def') and token[1].startswith('test_')):\n                function_id = get_function_id(token[1])\n            elif (function_id and (token[0] == tokenize.COMMENT)):\n                comment = token[1][1:].lstrip()\n                comment_str = None\n                if comment.startswith('CONCERN:'):\n                    comment_str = comment.replace('CONCERN:', '', 1).lstrip()\n                if comment.startswith('['):\n                    comment_str = re.sub('\\\\[.*\\\\]', '', comment, 1).lstrip()\n                if comment_str:\n                    while True:\n                        next_token = next(tokens, None)\n                        if (next_token and (next_token[0] == tokenize.COMMENT)):\n                            next_comment = next_token[1][1:].lstrip()\n                            comment_str += (' ' + next_comment)\n                        elif (next_token and (next_token[0] == tokenize.NL)):\n                            continue\n                        else:\n                            break\n                    concern = '[ {func_id}] {comment}'.format(func_id=function_id, comment=comment_str)\n                    concerns.append(concern)\n    return concerns\n", "label": 1}
{"function": "\n\ndef run(self):\n    figwidth = self.options.pop('figwidth', None)\n    figclasses = self.options.pop('figclass', None)\n    align = self.options.pop('align', None)\n    (image_node,) = Image.run(self)\n    if isinstance(image_node, nodes.system_message):\n        return [image_node]\n    figure_node = nodes.figure('', image_node)\n    if (figwidth == 'image'):\n        if (PIL and self.state.document.settings.file_insertion_enabled):\n            try:\n                i = PIL.open(str(image_node['uri']))\n            except (IOError, UnicodeError):\n                pass\n            else:\n                self.state.document.settings.record_dependencies.add(image_node['uri'])\n                figure_node['width'] = i.size[0]\n    elif (figwidth is not None):\n        figure_node['width'] = figwidth\n    if figclasses:\n        figure_node['classes'] += figclasses\n    if align:\n        figure_node['align'] = align\n    if self.content:\n        node = nodes.Element()\n        self.state.nested_parse(self.content, self.content_offset, node)\n        first_node = node[0]\n        if isinstance(first_node, nodes.paragraph):\n            caption = nodes.caption(first_node.rawsource, '', *first_node.children)\n            figure_node += caption\n        elif (not (isinstance(first_node, nodes.comment) and (len(first_node) == 0))):\n            error = self.state_machine.reporter.error('Figure caption must be a paragraph or empty comment.', nodes.literal_block(self.block_text, self.block_text), line=self.lineno)\n            return [figure_node, error]\n        if (len(node) > 1):\n            figure_node += nodes.legend('', *node[1:])\n    return [figure_node]\n", "label": 1}
{"function": "\n\ndef __read(self):\n    '\\n        Read the next frame(s) from the socket.\\n        '\n    fastbuf = BytesIO()\n    while self.running:\n        try:\n            try:\n                c = self.receive()\n            except exception.InterruptedException:\n                log.debug('socket read interrupted, restarting')\n                continue\n        except Exception:\n            c = b''\n        if (len(c) == 0):\n            raise exception.ConnectionClosedException()\n        fastbuf.write(c)\n        if (b'\\x00' in c):\n            break\n        elif (c == b'\\n'):\n            return [c]\n    self.__recvbuf += fastbuf.getvalue()\n    fastbuf.close()\n    result = []\n    if (self.__recvbuf and self.running):\n        while True:\n            pos = self.__recvbuf.find(b'\\x00')\n            if (pos >= 0):\n                frame = self.__recvbuf[0:pos]\n                preamble_end_match = utils.PREAMBLE_END_RE.search(frame)\n                if preamble_end_match:\n                    preamble_end = preamble_end_match.start()\n                    content_length_match = BaseTransport.__content_length_re.search(frame[0:preamble_end])\n                    if content_length_match:\n                        content_length = int(content_length_match.group('value'))\n                        content_offset = preamble_end_match.end()\n                        frame_size = (content_offset + content_length)\n                        if (frame_size > len(frame)):\n                            if (frame_size < len(self.__recvbuf)):\n                                pos = frame_size\n                                frame = self.__recvbuf[0:pos]\n                            else:\n                                break\n                result.append(frame)\n                self.__recvbuf = self.__recvbuf[(pos + 1):]\n            else:\n                break\n    return result\n", "label": 1}
{"function": "\n\ndef assert_both_values(actual, expected_plain, expected_color, kind=None):\n    'Handle asserts for color and non-color strings in color and non-color tests.\\n\\n    :param ColorStr actual: Return value of ColorStr class method.\\n    :param expected_plain: Expected non-color value.\\n    :param expected_color: Expected color value.\\n    :param str kind: Type of string to test.\\n    '\n    if kind.endswith('plain'):\n        assert (actual.value_colors == expected_plain)\n        assert (actual.value_no_colors == expected_plain)\n        assert (actual.has_colors is False)\n    elif kind.endswith('color'):\n        assert (actual.value_colors == expected_color)\n        assert (actual.value_no_colors == expected_plain)\n        if ('\\x1b' in actual.value_colors):\n            assert (actual.has_colors is True)\n        else:\n            assert (actual.has_colors is False)\n    else:\n        assert (actual == expected_plain)\n    if kind.startswith('ColorStr'):\n        assert (actual.__class__ == ColorStr)\n    elif kind.startswith('Color'):\n        assert (actual.__class__ == Color)\n", "label": 1}
{"function": "\n\ndef on_modified_async(self, view):\n    if (not is_php_file(view)):\n        return\n    cursor = view.sel()[0].b\n    if (cursor < 1):\n        return\n    while (cursor > 0):\n        curChar = view.substr(sublime.Region((cursor - 1), cursor))\n        if (curChar == '\\\\'):\n            return self.run_completion(view)\n        if (curChar == '$'):\n            return self.run_completion(view)\n        if (curChar == '('):\n            return self.run_completion(view)\n        if (cursor > 1):\n            curChar = view.substr(sublime.Region((cursor - 2), cursor))\n            if (curChar == '->'):\n                return self.run_completion(view)\n            if (curChar == '::'):\n                return self.run_completion(view)\n        if (cursor > 3):\n            curChar = view.substr(sublime.Region((cursor - 4), cursor))\n            if (curChar == 'use '):\n                return self.run_completion(view)\n            if (curChar == 'new '):\n                return self.run_completion(view)\n            if (cursor > 9):\n                curChar = view.substr(sublime.Region((cursor - 10), cursor))\n                if (curChar == 'namespace '):\n                    return self.run_completion(view)\n        cursor -= 1\n", "label": 1}
{"function": "\n\ndef test_single_repeating_timer(self):\n    time = TestingTimeFunction()\n    manager = TimerManager(_time_function=time)\n    callback = MockCallback()\n    manager.add_timer(30, callback, repeat=True)\n    assert (manager.sleep_time() == 30)\n    assert (callback.nb_calls == 0)\n    manager.run()\n    assert (callback.nb_calls == 0)\n    time.time = 22\n    assert (manager.sleep_time() == 8)\n    manager.run()\n    assert (callback.nb_calls == 0)\n    time.time = 30\n    assert (manager.sleep_time() == 0)\n    manager.run()\n    assert (callback.nb_calls == 1)\n    assert (manager.sleep_time() == 30)\n    time.time = 71\n    assert (manager.sleep_time() == 0)\n    manager.run()\n    assert (callback.nb_calls == 2)\n    assert (manager.sleep_time() == 19)\n    time.time = 200\n    assert (manager.sleep_time() == 0)\n    manager.run()\n    assert (callback.nb_calls == 3)\n    assert (manager.sleep_time() == 10)\n", "label": 1}
{"function": "\n\ndef is_acronym(token, exclude=None):\n    '\\n    Pass single token as a string, return True/False if is/is not valid acronym.\\n\\n    Args:\\n        token (str): single word to check for acronym-ness\\n        exclude (set[str]): if technically valid but not actually good acronyms\\n        are known in advance, pass them in as a set of strings; matching tokens\\n        will return False\\n\\n    Returns:\\n        bool\\n    '\n    if (exclude and (token in exclude)):\n        return False\n    if (not token):\n        return False\n    if (' ' in token):\n        return False\n    if ((len(token) == 2) and (not token.isupper())):\n        return False\n    if token.isdigit():\n        return False\n    if ((not any((char.isupper() for char in token))) and (not (token[0].isdigit() or token[(- 1)].isdigit()))):\n        return False\n    if (not (2 <= sum((1 for char in token if char.isalnum())) <= 10)):\n        return False\n    if (not ACRONYM_REGEX.match(token)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef _add_group_id_fields(self, out_collection, group_func_keys):\n    date_operators = ['dayOfYear', 'dayOfMonth', 'dayOfWeek', 'year', 'month', 'week', 'hour', 'minute', 'second', 'millisecond']\n    clear_group_func_keys = []\n    for key in group_func_keys:\n        out_field = key.split('__')[0]\n        for doc in out_collection:\n            if (key not in doc.keys()):\n                func_field = key.split('__')[1]\n                (func, in_field) = func_field.split('_')\n                out_value = doc.get(in_field)\n                if (func in date_operators):\n                    if (func == 'dayOfYear'):\n                        out_value = out_value.timetuple().tm_yday\n                    elif (func == 'dayOfMonth'):\n                        out_value = out_value.day\n                    elif (func == 'dayOfWeek'):\n                        out_value = out_value.isoweekday()\n                    elif (func == 'year'):\n                        out_value = out_value.year\n                    elif (func == 'month'):\n                        out_value = out_value.month\n                    elif (func == 'week'):\n                        out_value = out_value.isocalendar()[1]\n                    elif (func == 'hour'):\n                        out_value = out_value.hour\n                    elif (func == 'minute'):\n                        out_value = out_value.minute\n                    elif (func == 'second'):\n                        out_value = out_value.second\n                    elif (func == 'millisecond'):\n                        out_value = int((out_value.microsecond / 1000))\n                doc[out_field] = out_value\n        clear_group_func_keys.append(out_field)\n    return (out_collection, clear_group_func_keys)\n", "label": 1}
{"function": "\n\ndef _lookup(self, name=None, create=False):\n    'looks up symbol in search path\\n        returns symbol given symbol name,\\n        creating symbol if needed (and create=True)'\n    debug = False\n    if debug:\n        print('====\\nLOOKUP ', name)\n    searchGroups = self._fix_searchGroups()\n    self.__parents = []\n    if (self not in searchGroups):\n        searchGroups.append(self)\n\n    def public_attr(grp, name):\n        return (hasattr(grp, name) and (not ((grp is self) and (name in self._private))))\n    parts = name.split('.')\n    if (len(parts) == 1):\n        for grp in searchGroups:\n            if public_attr(grp, name):\n                self.__parents.append(grp)\n                return getattr(grp, name)\n    parts.reverse()\n    top = parts.pop()\n    out = self.__invalid_name\n    if (top == self.top_group):\n        out = self\n    else:\n        for grp in searchGroups:\n            if public_attr(grp, top):\n                self.__parents.append(grp)\n                out = getattr(grp, top)\n    if (out is self.__invalid_name):\n        raise NameError((\"'%s' is not defined\" % name))\n    if (len(parts) == 0):\n        return out\n    while parts:\n        prt = parts.pop()\n        if hasattr(out, prt):\n            out = getattr(out, prt)\n        elif create:\n            val = None\n            if (len(parts) > 0):\n                val = Group(name=prt)\n            setattr(out, prt, val)\n            out = getattr(out, prt)\n        else:\n            raise LookupError((\"cannot locate member '%s' of '%s'\" % (prt, out)))\n    return out\n", "label": 1}
{"function": "\n\ndef _handle_message(self, conn, message):\n    self.total_rdy = max((self.total_rdy - 1), 0)\n    rdy_conn = conn\n    if ((len(self.conns) > self.max_in_flight) and ((time.time() - self.random_rdy_ts) > 30)):\n        self.random_rdy_ts = time.time()\n        conns_with_no_rdy = [c for c in self.conns.itervalues() if (not c.rdy)]\n        if conns_with_no_rdy:\n            rdy_conn = random.choice(conns_with_no_rdy)\n            if (rdy_conn is not conn):\n                logger.info('[%s:%s] redistributing RDY to %s', conn.id, self.name, rdy_conn.id)\n    self._maybe_update_rdy(rdy_conn)\n    success = False\n    try:\n        if (0 < self.max_tries < message.attempts):\n            self.giving_up(message)\n            return message.finish()\n        pre_processed_message = self.preprocess_message(message)\n        if (not self.validate_message(pre_processed_message)):\n            return message.finish()\n        success = self.process_message(message)\n    except Exception:\n        logger.exception('[%s:%s] uncaught exception while handling message %s body:%r', conn.id, self.name, message.id, message.body)\n        if (not message.has_responded()):\n            return message.requeue()\n    if ((not message.is_async()) and (not message.has_responded())):\n        assert (success is not None), 'ambiguous return value for synchronous mode'\n        if success:\n            return message.finish()\n        return message.requeue()\n", "label": 1}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_time_lowmem():\n    ' low memory reading/writing of 3D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(DATA_DIR, 'rnmrtk_3d', 'time_3d.sec'))\n    assert (data.shape == (128, 88, 1250))\n    assert (np.abs((data[(0, 1, 2)].real - 7.98)) <= 0.01)\n    assert (np.abs((data[(0, 1, 2)].imag - 33.82)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].real - (- 9.36))) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].imag - (- 7.75))) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    lowmem_write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef parse_diff(self):\n    sections = []\n    state = None\n    prev_file = None\n    current_file = {\n        \n    }\n    current_hunks = []\n    prev_hunk = None\n    current_hunk = None\n    for line in self.view.lines(sublime.Region(0, self.view.size())):\n        linetext = self.view.substr(line)\n        if linetext.startswith('diff --git'):\n            state = 'header'\n            if (prev_file != line):\n                if (prev_file is not None):\n                    if current_hunk:\n                        current_hunks.append(current_hunk)\n                    sections.append((current_file, current_hunks))\n                prev_file = line\n                prev_hunk = None\n            current_file = line\n            current_hunks = []\n        elif ((state == 'header') and RE_DIFF_HEAD.match(linetext)):\n            current_file = current_file.cover(line)\n        elif linetext.startswith('@@'):\n            state = 'hunk'\n            if (prev_hunk != line):\n                if (prev_hunk is not None):\n                    current_hunks.append(current_hunk)\n                prev_hunk = line\n            current_hunk = line\n        elif ((state == 'hunk') and (linetext[0] in (' ', '-', '+'))):\n            current_hunk = current_hunk.cover(line)\n        elif (state == 'header'):\n            current_file = current_file.cover(line)\n    if (current_file and current_hunk):\n        current_hunks.append(current_hunk)\n        sections.append((current_file, current_hunks))\n    return sections\n", "label": 1}
{"function": "\n\ndef test_can_create_path(self):\n    path = Path({\n        'name': 'Alice',\n    }, 'KNOWS', {\n        'name': 'Bob',\n    })\n    nodes = path.nodes()\n    assert (len(path) == 1)\n    assert (nodes[0]['name'] == 'Alice')\n    assert (path[0].type() == 'KNOWS')\n    assert (nodes[(- 1)]['name'] == 'Bob')\n    path = Path(path, 'KNOWS', {\n        'name': 'Carol',\n    })\n    nodes = path.nodes()\n    assert (len(path) == 2)\n    assert (nodes[0]['name'] == 'Alice')\n    assert (path[0].type() == 'KNOWS')\n    assert (nodes[1]['name'] == 'Bob')\n    path = Path({\n        'name': 'Zach',\n    }, 'KNOWS', path)\n    nodes = path.nodes()\n    assert (len(path) == 3)\n    assert (nodes[0]['name'] == 'Zach')\n    assert (path[0].type() == 'KNOWS')\n    assert (nodes[1]['name'] == 'Alice')\n    assert (path[1].type() == 'KNOWS')\n    assert (nodes[2]['name'] == 'Bob')\n", "label": 1}
{"function": "\n\ndef selector(self, output):\n    if isinstance(output, compute_base.Node):\n        return self.parse(output, FieldLists.NODE)\n    elif isinstance(output, compute_base.NodeSize):\n        return self.parse(output, FieldLists.NODE_SIZE)\n    elif isinstance(output, compute_base.NodeImage):\n        return self.parse(output, FieldLists.NODE_IMAGE)\n    elif isinstance(output, compute_base.NodeLocation):\n        return self.parse(output, FieldLists.LOCATION)\n    elif isinstance(output, compute_base.NodeAuthSSHKey):\n        return self.parse(output, FieldLists.NODE_KEY)\n    elif isinstance(output, compute_base.NodeAuthPassword):\n        return self.parse(output, FieldLists.NODE_PASSWORD)\n    elif isinstance(output, compute_base.StorageVolume):\n        return self.parse(output, FieldLists.STORAGE_VOLUME)\n    elif isinstance(output, compute_base.VolumeSnapshot):\n        return self.parse(output, FieldLists.VOLUME_SNAPSHOT)\n    elif isinstance(output, dns_base.Zone):\n        return self.parse(output, FieldLists.ZONE)\n    elif isinstance(output, dns_base.Record):\n        return self.parse(output, FieldLists.RECORD)\n    elif isinstance(output, lb_base.Member):\n        return self.parse(output, FieldLists.MEMBER)\n    elif isinstance(output, lb_base.LoadBalancer):\n        return self.parse(output, FieldLists.BALANCER)\n    elif isinstance(output, container_base.Container):\n        return self.parse(output, FieldLists.CONTAINER)\n    elif isinstance(output, container_base.ContainerImage):\n        return self.parse(output, FieldLists.CONTAINER_IMAGE)\n    elif isinstance(output, container_base.ContainerCluster):\n        return self.parse(output, FieldLists.CONTAINER_CLUSTER)\n    else:\n        return output\n", "label": 1}
{"function": "\n\ndef line_ends_with_close_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)\\\\*/(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(/\\\\*)(.*)\\\\*/(\\\\s*)$', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\"\"\")(.*)\"\"\"(\\\\s*)$', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\->(\\\\s*)$)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)(\\\\s*)$', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=end)(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)(.*)(\\\\=end)(\\\\s*)$', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\})(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)(.*)(%\\\\})(\\\\s*)$', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef parse_challenge(stuff):\n    '\\n    '\n    ret = {\n        \n    }\n    var = b''\n    val = b''\n    in_var = True\n    in_quotes = False\n    new = False\n    escaped = False\n    for c in stuff:\n        if (sys.version_info >= (3, 0)):\n            c = bytes([c])\n        if in_var:\n            if c.isspace():\n                continue\n            if (c == b'='):\n                in_var = False\n                new = True\n            else:\n                var += c\n        elif new:\n            if (c == b'\"'):\n                in_quotes = True\n            else:\n                val += c\n            new = False\n        elif in_quotes:\n            if escaped:\n                escaped = False\n                val += c\n            elif (c == b'\\\\'):\n                escaped = True\n            elif (c == b'\"'):\n                in_quotes = False\n            else:\n                val += c\n        elif (c == b','):\n            if var:\n                ret[var] = val\n            var = b''\n            val = b''\n            in_var = True\n        else:\n            val += c\n    if var:\n        ret[var] = val\n    return ret\n", "label": 1}
{"function": "\n\ndef localStr(self, local_addr=None, local_port=None):\n    l = []\n    w = l.append\n    w('sip:')\n    if (self.username != None):\n        w(self.username)\n        for v in self.userparams:\n            w((';%s' % v))\n        if (self.password != None):\n            w((':%s' % self.password))\n        w('@')\n    if ((local_addr != None) and ('my' in dir(self.host))):\n        w(local_addr)\n    else:\n        w(str(self.host))\n    if (self.port != None):\n        if ((local_port != None) and ('my' in dir(self.port))):\n            w((':%d' % local_port))\n        else:\n            w((':%d' % self.port))\n    if (self.usertype != None):\n        w((';user=%s' % self.usertype))\n    for n in ('transport', 'ttl', 'maddr', 'method', 'tag'):\n        v = getattr(self, n)\n        if (v != None):\n            w((';%s=%s' % (n, v)))\n    if self.lr:\n        w(';lr')\n    for v in self.other:\n        w((';%s' % v))\n    if self.headers:\n        w('?')\n        w('&'.join([('%s=%s' % (h.capitalize(), quote(v))) for (h, v) in self.headers.items()]))\n    return ''.join(l)\n", "label": 1}
{"function": "\n\ndef open_url(url, wait=False, locate=False):\n    import subprocess\n\n    def _unquote_file(url):\n        try:\n            import urllib\n        except ImportError:\n            import urllib\n        if url.startswith('file://'):\n            url = urllib.unquote(url[7:])\n        return url\n    if (sys.platform == 'darwin'):\n        args = ['open']\n        if wait:\n            args.append('-W')\n        if locate:\n            args.append('-R')\n        args.append(_unquote_file(url))\n        null = open('/dev/null', 'w')\n        try:\n            return subprocess.Popen(args, stderr=null).wait()\n        finally:\n            null.close()\n    elif WIN:\n        if locate:\n            url = _unquote_file(url)\n            args = ('explorer /select,\"%s\"' % _unquote_file(url.replace('\"', '')))\n        else:\n            args = ('start %s \"\" \"%s\"' % (((wait and '/WAIT') or ''), url.replace('\"', '')))\n        return os.system(args)\n    try:\n        if locate:\n            url = (os.path.dirname(_unquote_file(url)) or '.')\n        else:\n            url = _unquote_file(url)\n        c = subprocess.Popen(['xdg-open', url])\n        if wait:\n            return c.wait()\n        return 0\n    except OSError:\n        if (url.startswith(('http://', 'https://')) and (not locate) and (not wait)):\n            import webbrowser\n            webbrowser.open(url)\n            return 0\n        return 1\n", "label": 1}
{"function": "\n\ndef determine_result(self, returncode, returnsignal, output, isTimeout):\n    if ((returnsignal == 0) and (returncode == 0)):\n        status = None\n        for line in output:\n            line = line.strip()\n            if (line == 'unsat'):\n                status = result.RESULT_UNSAT\n            elif (line == 'sat'):\n                status = result.RESULT_SAT\n            elif ((not status) and line.startswith('(error ')):\n                status = 'ERROR'\n        if (not status):\n            status = result.RESULT_UNKNOWN\n    elif (((returnsignal == 9) or (returnsignal == 15)) and isTimeout):\n        status = 'TIMEOUT'\n    elif (returnsignal == 9):\n        status = 'KILLED BY SIGNAL 9'\n    elif (returnsignal == 6):\n        status = 'ABORTED'\n    elif (returnsignal == 15):\n        status = 'KILLED'\n    else:\n        status = 'ERROR ({0})'.format(returncode)\n    return status\n", "label": 1}
{"function": "\n\ndef transform_source_batch(self, source, source_name):\n    self.verify_axis_labels(('batch', 'channel', 'height', 'width'), self.data_stream.axis_labels[source_name], source_name)\n    rotation_angles = self.rng.uniform((- self.maximum_rotation), self.maximum_rotation, len(source))\n    if (isinstance(source, list) and all(((isinstance(b, numpy.ndarray) and (b.ndim == 3)) for b in source))):\n        return [self._example_transform(im, angle) for (im, angle) in zip(source, rotation_angles)]\n    elif (isinstance(source, numpy.ndarray) and (source.dtype == object) and all(((isinstance(b, numpy.ndarray) and (b.ndim == 3)) for b in source))):\n        out = numpy.empty(len(source), dtype=object)\n        for (im_idx, (im, angle)) in enumerate(zip(source, rotation_angles)):\n            out[im_idx] = self._example_transform(im, angle)\n        return out\n    elif (isinstance(source, numpy.ndarray) and (source.ndim == 4)):\n        return numpy.array([self._example_transform(im, angle) for (im, angle) in zip(source, rotation_angles)], dtype=source.dtype)\n    else:\n        raise ValueError('uninterpretable batch format; expected a list of arrays with ndim = 3, or an array with ndim = 4')\n", "label": 1}
{"function": "\n\ndef get_features2(self):\n    '\\n        Return all features with its names.\\n\\n        Returns\\n        -------\\n        names : list\\n            Feature names.\\n        values : list\\n            Feature values\\n        '\n    feature_names = []\n    feature_values = []\n    all_vars = vars(self)\n    for name in all_vars.keys():\n        if (not ((name == 'date') or (name == 'mag') or (name == 'err') or (name == 'n_threads') or (name == 'min_period'))):\n            if (not ((name == 'f') or (name == 'f_phase') or (name == 'period_log10FAP') or (name == 'weight') or (name == 'weighted_sum') or (name == 'median') or (name == 'mean') or (name == 'std'))):\n                feature_names.append(name)\n    feature_names.sort()\n    for name in feature_names:\n        feature_values.append(all_vars[name])\n    return (feature_names, feature_values)\n", "label": 1}
{"function": "\n\ndef delete_top_level(self, context=None):\n    \"\\n        Delete the top level calls which are not part of the user's code.\\n\\n        TODO: If CELL_MAGIC then also merge the entries for the cell (which are\\n        seperated by line into individual code objects...)\\n        \"\n    if (context == 'LINE_MAGIC'):\n        new_roots = []\n        for function in self.cprofile_tree:\n            try:\n                if (function.co_name == '<module>'):\n                    new_roots += self.cprofile_tree[function]['calls']\n            except AttributeError:\n                pass\n        new_roots = [r for r in new_roots if ((type(r) == str) or (r.co_name != '<module>'))]\n        for i in range(len(new_roots)):\n            function = new_roots[i]\n            try:\n                if (function.co_name == '<module>'):\n                    del new_roots[i]\n            except AttributeError:\n                pass\n    if (context == 'CELL_MAGIC'):\n        new_roots = []\n        for function in self.cprofile_tree:\n            try:\n                if ('<ipython-input' in function.co_filename):\n                    new_roots += self.cprofile_tree[function]['calls']\n            except AttributeError:\n                pass\n    new_cprofile_tree = {\n        \n    }\n\n    def populate_new_tree(roots):\n        for root in roots:\n            if (root not in new_cprofile_tree):\n                new_cprofile_tree[root] = self.cprofile_tree[root]\n                populate_new_tree(self.cprofile_tree[root]['calls'])\n    populate_new_tree(new_roots)\n    self.cprofile_tree = new_cprofile_tree\n    self.roots = new_roots\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    _get_chronos_service_config(args)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.chronos_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                stop_job(args, hosts[host_id].ip, job_name, instance_id)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.chronos_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                deploy_utils.wait_for_job_stopping('chronos', args.chronos_config.cluster.name, job_name, hosts[host_id].ip, instance_id)\n                start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)\n", "label": 1}
{"function": "\n\ndef _setup_document_fields(self):\n    for f in self.document._fields.values():\n        if (not hasattr(f, 'rel')):\n            if isinstance(f, ReferenceField):\n                f.rel = Relation(f.document_type)\n                f.is_relation = True\n            elif (isinstance(f, ListField) and isinstance(f.field, ReferenceField)):\n                f.field.rel = Relation(f.field.document_type)\n                f.field.is_relation = True\n            else:\n                f.many_to_many = None\n                f.many_to_one = None\n                f.one_to_many = None\n                f.one_to_one = None\n                f.related_model = None\n                f.rel = None\n                f.is_relation = False\n        if ((not hasattr(f, 'verbose_name')) or (f.verbose_name is None)):\n            f.verbose_name = capfirst(create_verbose_name(f.name))\n        if (not hasattr(f, 'flatchoices')):\n            flat = []\n            if (f.choices is not None):\n                for (choice, value) in f.choices:\n                    if isinstance(value, (list, tuple)):\n                        flat.extend(value)\n                    else:\n                        flat.append((choice, value))\n            f.flatchoices = flat\n        if (isinstance(f, ReferenceField) and (not isinstance(f.document_type._meta, (DocumentMetaWrapper, LazyDocumentMetaWrapper))) and (self.document != f.document_type)):\n            f.document_type._meta = LazyDocumentMetaWrapper(f.document_type)\n        if (not hasattr(f, 'auto_created')):\n            f.auto_created = False\n", "label": 1}
{"function": "\n\ndef _read_one_coil_point(fid):\n    'Read coil coordinate information from the hc file'\n    one = '#'\n    while ((len(one) > 0) and (one[0] == '#')):\n        one = fid.readline()\n    if (len(one) == 0):\n        return None\n    one = one.strip().decode('utf-8')\n    if ('Unable' in one):\n        raise RuntimeError('HPI information not available')\n    p = dict()\n    p['valid'] = ('measured' in one)\n    for (key, val) in _coord_dict.items():\n        if (key in one):\n            p['coord_frame'] = val\n            break\n    else:\n        p['coord_frame'] = (- 1)\n    for (key, val) in _kind_dict.items():\n        if (key in one):\n            p['kind'] = val\n            break\n    else:\n        p['kind'] = (- 1)\n    p['r'] = np.empty(3)\n    for (ii, coord) in enumerate('xyz'):\n        sp = fid.readline().decode('utf-8').strip()\n        if (len(sp) == 0):\n            continue\n        sp = sp.split(' ')\n        if ((len(sp) != 3) or (sp[0] != coord) or (sp[1] != '=')):\n            raise RuntimeError(('Bad line: %s' % one))\n        p['r'][ii] = (float(sp[2]) / 100.0)\n    return p\n", "label": 1}
{"function": "\n\ndef asXML(self, doctag=None, namedItemsOnly=False, indent='', formatted=True):\n    'Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.'\n    nl = '\\n'\n    out = []\n    namedItems = dict(((v[1], k) for (k, vlist) in self.__tokdict.items() for v in vlist))\n    nextLevelIndent = (indent + '  ')\n    if (not formatted):\n        indent = ''\n        nextLevelIndent = ''\n        nl = ''\n    selfTag = None\n    if (doctag is not None):\n        selfTag = doctag\n    elif self.__name:\n        selfTag = self.__name\n    if (not selfTag):\n        if namedItemsOnly:\n            return ''\n        else:\n            selfTag = 'ITEM'\n    out += [nl, indent, '<', selfTag, '>']\n    worklist = self.__toklist\n    for (i, res) in enumerate(worklist):\n        if isinstance(res, ParseResults):\n            if (i in namedItems):\n                out += [res.asXML(namedItems[i], (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n            else:\n                out += [res.asXML(None, (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n        else:\n            resTag = None\n            if (i in namedItems):\n                resTag = namedItems[i]\n            if (not resTag):\n                if namedItemsOnly:\n                    continue\n                else:\n                    resTag = 'ITEM'\n            xmlBodyText = _xml_escape(_ustr(res))\n            out += [nl, nextLevelIndent, '<', resTag, '>', xmlBodyText, '</', resTag, '>']\n    out += [nl, indent, '</', selfTag, '>']\n    return ''.join(out)\n", "label": 1}
{"function": "\n\ndef on_text_command(self, view, cmd, args):\n    if (isearch_info_for(view) is not None):\n        if (cmd not in ('sbp_inc_search', 'sbp_inc_search_escape')):\n            return ('sbp_inc_search_escape', {\n                'next_cmd': cmd,\n                'next_args': args,\n            })\n        return\n    vs = ViewState.get(view)\n    self.on_anything(view)\n    if (args is None):\n        args = {\n            \n        }\n    if (not cmd.startswith('sbp_')):\n        vs.this_cmd = cmd\n    if (cmd == 'drag_select'):\n        info = isearch_info_for(view)\n        if info:\n            info.done()\n        vs.drag_count = (2 if ('by' in args) else 0)\n    if ((cmd in ('move', 'move_to')) and vs.active_mark and (not args.get('extend', False))):\n        args['extend'] = True\n        return (cmd, args)\n    if (not vs.argument_supplied):\n        return None\n    if (cmd in repeatable_cmds):\n        count = vs.get_count()\n        args.update({\n            'cmd': cmd,\n            '_times': abs(count),\n        })\n        if ((count < 0) and ('forward' in args)):\n            args['forward'] = (not args['forward'])\n        return ('sbp_do_times', args)\n    elif (cmd == 'scroll_lines'):\n        args['amount'] *= vs.get_count()\n        return (cmd, args)\n", "label": 1}
{"function": "\n\ndef _dynamic_init(self, only_fields, include_fields, exclude_fields):\n    \"\\n        Modifies `request_fields` via higher-level dynamic field interfaces.\\n\\n        Arguments:\\n            only_fields: List of field names to render.\\n                All other fields will be deferred (respects sideloads).\\n            include_fields: List of field names to include.\\n                Adds to default field set, (respects sideloads).\\n                `*` means include all fields.\\n            exclude_fields: List of field names to exclude.\\n                Removes from default field set. If set to '*', all fields are\\n                removed, except for ones that are explicitly included.\\n        \"\n    if (not self.dynamic):\n        return\n    if (isinstance(self.request_fields, dict) and (self.request_fields.pop('*', None) is False)):\n        exclude_fields = '*'\n    only_fields = set((only_fields or []))\n    include_fields = (include_fields or [])\n    exclude_fields = (exclude_fields or [])\n    all_fields = set(self.get_all_fields().keys())\n    if only_fields:\n        exclude_fields = '*'\n        include_fields = only_fields\n    if (exclude_fields == '*'):\n        include_fields = set((list(include_fields) + [field for (field, val) in six.iteritems(self.request_fields) if (val or (val == {\n            \n        }))]))\n        exclude_fields = (all_fields - include_fields)\n    elif (include_fields == '*'):\n        include_fields = all_fields\n    for name in exclude_fields:\n        self.request_fields[name] = False\n    for name in include_fields:\n        if (not isinstance(self.request_fields.get(name), dict)):\n            self.request_fields[name] = True\n", "label": 1}
{"function": "\n\ndef line_contains_close_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)\\\\*/', line) and (not re.match('^(\\\\+|\\\\-)(.*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(.*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\->)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=end)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\})', line) and (not re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef main(argv):\n    if (len(argv) != 2):\n        print('Usage: ./countLoc.py <file_to_count.[py | c]')\n        return\n    file = argv[1]\n    mode = file.split('.')[1]\n    if (mode == 'py'):\n        cKey = '#'\n    elif (mode == 'c'):\n        cKey = '//'\n        sKey = '/*'\n        eKey = '*/'\n    else:\n        print('Invalid extension, please input a Python or C code file')\n        return\n    comment = loc = blank = 0\n    with open(file, 'r') as f:\n        for line in f.readlines():\n            line = line.strip()\n            if (not line):\n                blank += 1\n            elif line.startswith(cKey):\n                comment += 1\n            elif ((mode == 'c') and line.startswith(sKey) and line.endswith(eKey)):\n                comment += 1\n            elif ((mode == 'c') and line.startswith(sKey)):\n                cFlag = True\n                comment += 1\n            elif ((mode == 'c') and line.endswith(eKey)):\n                cFlag = False\n                comment += 1\n            elif ((mode == 'c') and cFlag):\n                comment += 1\n            else:\n                loc += 1\n    print()\n    print('File: ', file)\n    print('LoC: ', loc)\n    print('Comments: ', comment)\n    print('Blank: ', blank)\n    print()\n", "label": 1}
{"function": "\n\ndef test_default_updates_chained(self):\n    x = shared(2)\n    y = shared(1)\n    z = shared((- 1))\n    x.default_update = (x - y)\n    y.default_update = z\n    z.default_update = (z - 1)\n    f1 = pfunc([], [x])\n    f1()\n    assert (x.get_value() == 1)\n    assert (y.get_value() == (- 1))\n    assert (z.get_value() == (- 2))\n    f2 = pfunc([], [x, y])\n    f2()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 2))\n    assert (z.get_value() == (- 3))\n    f3 = pfunc([], [y])\n    f3()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 3))\n    assert (z.get_value() == (- 4))\n    f4 = pfunc([], [x, y], no_default_updates=[x])\n    f4()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 4))\n    assert (z.get_value() == (- 5))\n    f5 = pfunc([], [x, y, z], no_default_updates=[z])\n    f5()\n    assert (x.get_value() == 6)\n    assert (y.get_value() == (- 5))\n    assert (z.get_value() == (- 5))\n", "label": 1}
{"function": "\n\ndef is_active(self, key, *instances, **kwargs):\n    \"\\n        Returns ``True`` if any of ``instances`` match an active switch. Otherwise\\n        returns ``False``.\\n\\n        >>> gargoyle.is_active('my_feature', request) #doctest: +SKIP\\n        \"\n    default = kwargs.pop('default', False)\n    parts = key.split(':')\n    if (len(parts) > 1):\n        child_kwargs = kwargs.copy()\n        child_kwargs['default'] = None\n        result = self.is_active(':'.join(parts[:(- 1)]), *instances, **child_kwargs)\n        if (result is False):\n            return result\n        elif (result is True):\n            default = result\n    try:\n        switch = self[key]\n    except KeyError:\n        return default\n    if (switch.status == GLOBAL):\n        return True\n    elif (switch.status == DISABLED):\n        return False\n    elif (switch.status == INHERIT):\n        return default\n    conditions = switch.value\n    if (not conditions):\n        return default\n    if instances:\n        instances = list(instances)\n        for v in instances:\n            if (isinstance(v, HttpRequest) and hasattr(v, 'user')):\n                instances.append(v.user)\n    return_value = False\n    for switch in self._registry.itervalues():\n        result = switch.has_active_condition(conditions, instances)\n        if (result is False):\n            return False\n        elif (result is True):\n            return_value = True\n    return return_value\n", "label": 1}
{"function": "\n\ndef _quick_drag_menu(self, object):\n    ' Displays the quick drag menu for a specified drag object.\\n        '\n    feature_lists = []\n    if isinstance(object, IFeatureTool):\n        msg = 'Apply to'\n        for dc in self.dock_control.dock_controls:\n            if (dc.visible and (object.feature_can_drop_on(dc.object) or object.feature_can_drop_on_dock_control(dc))):\n                from feature_tool import FeatureTool\n                feature_lists.append([FeatureTool(dock_control=dc)])\n    else:\n        msg = 'Send to'\n        for dc in self.dock_control.dock_controls:\n            if dc.visible:\n                allowed = [f for f in dc.features if ((f.feature_name != '') and f.can_drop(object))]\n                if (len(allowed) > 0):\n                    feature_lists.append(allowed)\n    if (len(feature_lists) > 0):\n        features = []\n        actions = []\n        for list in feature_lists:\n            if (len(list) > 1):\n                sub_actions = []\n                for feature in list:\n                    sub_actions.append(Action(name=('%s Feature' % feature.feature_name), action=('self._drop_on(%d)' % len(features))))\n                    features.append(feature)\n                actions.append(Menu(*sub_actions, name=('%s the %s' % (msg, feature.dock_control.name))))\n            else:\n                actions.append(Action(name=('%s %s' % (msg, list[0].dock_control.name)), action=('self._drop_on(%d)' % len(features))))\n                features.append(list[0])\n        self._object = object\n        self._features = features\n        self.popup_menu(Menu(*actions, name='popup'))\n        self._object = self._features = None\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef run(report_name, filters=()):\n    report = get_report_doc(report_name)\n    if (filters and isinstance(filters, basestring)):\n        filters = json.loads(filters)\n    if (not frappe.has_permission(report.ref_doctype, 'report')):\n        frappe.msgprint(_('Must have report permission to access this report.'), raise_exception=True)\n    (columns, result, message) = ([], [], None)\n    if (report.report_type == 'Query Report'):\n        if (not report.query):\n            frappe.msgprint(_('Must specify a Query to run'), raise_exception=True)\n        if (not report.query.lower().startswith('select')):\n            frappe.msgprint(_('Query must be a SELECT'), raise_exception=True)\n        result = [list(t) for t in frappe.db.sql(report.query, filters)]\n        columns = [cstr(c[0]) for c in frappe.db.get_description()]\n    else:\n        module = (report.module or frappe.db.get_value('DocType', report.ref_doctype, 'module'))\n        if (report.is_standard == 'Yes'):\n            method_name = (get_report_module_dotted_path(module, report.name) + '.execute')\n            res = frappe.get_attr(method_name)(frappe._dict(filters))\n            (columns, result) = (res[0], res[1])\n            if (len(res) > 2):\n                message = res[2]\n    if (report.apply_user_permissions and result):\n        result = get_filtered_data(report.ref_doctype, columns, result)\n    if (cint(report.add_total_row) and result):\n        result = add_total_row(result, columns)\n    return {\n        'result': result,\n        'columns': columns,\n        'message': message,\n    }\n", "label": 1}
{"function": "\n\ndef Prepend(self, **kw):\n    'Prepend values to existing construction variables\\n        in an Environment.\\n        '\n    kw = copy_non_reserved_keywords(kw)\n    for (key, val) in kw.items():\n        try:\n            orig = self._dict[key]\n        except KeyError:\n            self._dict[key] = val\n        else:\n            try:\n                update_dict = orig.update\n            except AttributeError:\n                try:\n                    self._dict[key] = (val + orig)\n                except (KeyError, TypeError):\n                    try:\n                        add_to_val = val.append\n                    except AttributeError:\n                        if val:\n                            orig.insert(0, val)\n                    else:\n                        if orig:\n                            add_to_val(orig)\n                        self._dict[key] = val\n            else:\n                if SCons.Util.is_List(val):\n                    for v in val:\n                        orig[v] = None\n                else:\n                    try:\n                        update_dict(val)\n                    except (AttributeError, TypeError, ValueError):\n                        if SCons.Util.is_Dict(val):\n                            for (k, v) in val.items():\n                                orig[k] = v\n                        else:\n                            orig[val] = None\n    self.scanner_map_delete(kw)\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    for base in bases:\n        for base_name in dir(base):\n            base_value = getattr(base, base_name)\n            if ((not callable(base_value)) or (not base_name.startswith('assert'))):\n                continue\n            if (not (base_name in attrs)):\n                attrs.update({\n                    base_name: base_value,\n                })\n    for (attr_name, attr_value) in attrs.items():\n        if ((not attr_name.startswith('assert')) or (attr_name == 'assert_')):\n            continue\n        if (attr_name[6] == '_'):\n            new_name = underscore_to_camelcase(attr_name)\n        else:\n            new_name = camelcase_to_underscore(attr_name)\n        if (not (new_name in attrs)):\n            attrs[new_name] = attr_value\n    for attr in tools.__all__:\n        attrs.update({\n            attr: getattr(tools, attr),\n        })\n        attrs[attr] = staticmethod(attrs[attr])\n    if datadiff_assert_equal:\n        key = ((attrs.get('use_datadiff', False) and 'assert_equal') or 'datadiff_assert_equal')\n        attrs.update({\n            key: datadiff_assert_equal,\n        })\n        attrs[key] = staticmethod(attrs[key])\n    elif attrs.get('use_datadiff', False):\n        warnings.warn('You enabled ``datadiff.tools.assert_equal``, but looks like you have not ``datadiff`` library installed in your system.')\n    return type.__new__(cls, name, bases, attrs)\n", "label": 1}
{"function": "\n\ndef _parse_signature(self, args, kw):\n    values = {\n        \n    }\n    (sig_args, var_args, var_kw, defaults) = self._func_signature\n    extra_kw = {\n        \n    }\n    for (name, value) in kw.iteritems():\n        if ((not var_kw) and (name not in sig_args)):\n            raise TypeError(('Unexpected argument %s' % name))\n        if (name in sig_args):\n            values[sig_args] = value\n        else:\n            extra_kw[name] = value\n    args = list(args)\n    sig_args = list(sig_args)\n    while args:\n        while (sig_args and (sig_args[0] in values)):\n            sig_args.pop(0)\n        if sig_args:\n            name = sig_args.pop(0)\n            values[name] = args.pop(0)\n        elif var_args:\n            values[var_args] = tuple(args)\n            break\n        else:\n            raise TypeError(('Extra position arguments: %s' % ', '.join((repr(v) for v in args))))\n    for (name, value_expr) in defaults.iteritems():\n        if (name not in values):\n            values[name] = self._template._eval(value_expr, self._ns, self._pos)\n    for name in sig_args:\n        if (name not in values):\n            raise TypeError(('Missing argument: %s' % name))\n    if var_kw:\n        values[var_kw] = extra_kw\n    return values\n", "label": 1}
{"function": "\n\ndef convert_func(self, lib, opts, args):\n    if (not opts.dest):\n        opts.dest = self.config['dest'].get()\n    if (not opts.dest):\n        raise ui.UserError('no convert destination set')\n    opts.dest = util.bytestring_path(opts.dest)\n    if (not opts.threads):\n        opts.threads = self.config['threads'].get(int)\n    if self.config['paths']:\n        path_formats = ui.get_path_formats(self.config['paths'])\n    else:\n        path_formats = ui.get_path_formats()\n    if (not opts.format):\n        opts.format = self.config['format'].get(unicode).lower()\n    pretend = (opts.pretend if (opts.pretend is not None) else self.config['pretend'].get(bool))\n    if (not pretend):\n        ui.commands.list_items(lib, ui.decargs(args), opts.album)\n        if (not (opts.yes or ui.input_yn('Convert? (Y/n)'))):\n            return\n    if opts.album:\n        albums = lib.albums(ui.decargs(args))\n        items = (i for a in albums for i in a.items())\n        if self.config['copy_album_art']:\n            for album in albums:\n                self.copy_album_art(album, opts.dest, path_formats, pretend)\n    else:\n        items = iter(lib.items(ui.decargs(args)))\n    convert = [self.convert_item(opts.dest, opts.keep_new, path_formats, opts.format, pretend) for _ in range(opts.threads)]\n    pipe = util.pipeline.Pipeline([items, convert])\n    pipe.run_parallel()\n", "label": 1}
{"function": "\n\ndef _scheduleTransmitActual(self, intent):\n    if (intent.targetAddr == self.myAddress):\n        self._processReceivedEnvelope(ReceiveEnvelope(intent.targetAddr, intent.message))\n        return self._finishIntent(intent)\n    if isinstance(intent.targetAddr.addressDetails, RoutedTCPv4ActorAddress):\n        if (not isinstance(intent.message, ForwardMessage)):\n            routing = [(A or self._adminAddr) for A in intent.targetAddr.addressDetails.routing]\n            while (routing and (routing[0] == self.myAddress)):\n                routing = routing[1:]\n            if (self.txOnly and routing and (routing[0] != self._adminAddr)):\n                routing.insert(0, self._adminAddr)\n            if routing:\n                if ((len(routing) != 1) or (routing[0] != intent.targetAddr)):\n                    intent.changeMessage(ForwardMessage(intent.message, intent.targetAddr, self.myAddress, routing))\n                    intent.addCallback((lambda r, i, ta=intent.targetAddr: i.changeTargetAddr(ta)))\n                    intent.changeTargetAddr(intent.message.fwdTargets[0])\n                    self.scheduleTransmit(getattr(self, '_addressMgr', None), intent)\n                    return\n    intent.stage = self._XMITStepSendConnect\n    if self._nextTransmitStep(intent):\n        if hasattr(intent, 'socket'):\n            self._transmitIntents[intent.socket.fileno()] = intent\n        else:\n            self._waitingTransmits.append(intent)\n", "label": 1}
{"function": "\n\ndef test_update_crossing_duration1_not_duration_at_1st_step(self, duration1, duration2):\n    global rec\n    if (duration2 == 0.0):\n        return\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    composite = ac.sequence(a1, a2)\n    rec = []\n    node.do(composite)\n    elapsed = 0.0\n    next_elapsed = ((duration1 + duration2) / 2.0)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    recx = [e for e in rec if (e[1] != 'step')]\n    rec = [e for e in recx if (e[0] == name1)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert ((rec[1][1] == 'update') and (rec[1][2] == 1.0))\n    assert (rec[2][1] == 'stop')\n    assert (len(rec) == 3)\n    rec = [e for e in recx if (e[0] == name2)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert (rec[1][1] == 'update')\n    assert (abs((rec[1][2] - ((next_elapsed - duration1) / duration2))) < fe)\n    assert (len(rec) == 2)\n", "label": 1}
{"function": "\n\ndef norm(x, ord):\n    x = as_tensor_variable(x)\n    ndim = x.ndim\n    if (ndim == 0):\n        raise ValueError(\"'axis' entry is out of bounds.\")\n    elif (ndim == 1):\n        if (ord is None):\n            return (tensor.sum((x ** 2)) ** 0.5)\n        elif (ord == 'inf'):\n            return tensor.max(abs(x))\n        elif (ord == '-inf'):\n            return tensor.min(abs(x))\n        elif (ord == 0):\n            return x[x.nonzero()].shape[0]\n        else:\n            try:\n                z = (tensor.sum(abs((x ** ord))) ** (1.0 / ord))\n            except TypeError:\n                raise ValueError('Invalid norm order for vectors.')\n            return z\n    elif (ndim == 2):\n        if ((ord is None) or (ord == 'fro')):\n            return (tensor.sum(abs((x ** 2))) ** 0.5)\n        elif (ord == 'inf'):\n            return tensor.max(tensor.sum(abs(x), 1))\n        elif (ord == '-inf'):\n            return tensor.min(tensor.sum(abs(x), 1))\n        elif (ord == 1):\n            return tensor.max(tensor.sum(abs(x), 0))\n        elif (ord == (- 1)):\n            return tensor.min(tensor.sum(abs(x), 0))\n        else:\n            raise ValueError(0)\n    elif (ndim > 2):\n        raise NotImplementedError(\"We don't support norm witn ndim > 2\")\n", "label": 1}
{"function": "\n\ndef pytest_generate_tests(metafunc):\n    'Parametrize tests to run on all backends.'\n    if ('backend' in metafunc.fixturenames):\n        skip_backends = set()\n        if hasattr(metafunc.module, 'skip_backends'):\n            skip_backends = skip_backends.union(set(metafunc.module.skip_backends))\n        if hasattr(metafunc.cls, 'skip_backends'):\n            skip_backends = skip_backends.union(set(metafunc.cls.skip_backends))\n        if metafunc.config.option.backend:\n            backend = set([x.lower() for x in metafunc.config.option.backend])\n        else:\n            backend = set(backends.keys())\n        if hasattr(metafunc.module, 'backends'):\n            backend = backend.intersection(set(metafunc.module.backends))\n        if hasattr(metafunc.cls, 'backends'):\n            backend = backend.intersection(set(metafunc.cls.backends))\n        lazy = []\n        if (not (('skip_greedy' in metafunc.fixturenames) or metafunc.config.option.lazy)):\n            lazy.append('greedy')\n        if (not (('skip_lazy' in metafunc.fixturenames) or metafunc.config.option.greedy)):\n            lazy.append('lazy')\n        backend = [b for b in backend.difference(skip_backends) if (not (('skip_' + b) in metafunc.fixturenames))]\n        params = list(product(backend, lazy))\n        metafunc.parametrize('backend', (params or [(None, None)]), indirect=True, ids=['-'.join(p) for p in params])\n", "label": 1}
{"function": "\n\n@specialize.ll()\ndef wrap(*_pyval):\n    if (len(_pyval) == 1):\n        pyval = _pyval[0]\n        if isinstance(pyval, bool):\n            return (w_true if pyval else w_false)\n        if isinstance(pyval, int):\n            return W_Fixnum(pyval)\n        if isinstance(pyval, float):\n            return W_Flonum(pyval)\n        if isinstance(pyval, W_Object):\n            return pyval\n    elif (len(_pyval) == 2):\n        car = _pyval[0]\n        cdr = wrap(_pyval[1])\n        if isinstance(car, bool):\n            if cdr.is_proper_list():\n                return W_WrappedConsProper(wrap(car), cdr)\n            return W_WrappedCons(wrap(car), cdr)\n        if isinstance(car, int):\n            if cdr.is_proper_list():\n                return W_UnwrappedFixnumConsProper(car, cdr)\n            return W_UnwrappedFixnumCons(car, cdr)\n        if isinstance(car, float):\n            if cdr.is_proper_list():\n                return W_UnwrappedFlonumConsProper(car, cdr)\n            return W_UnwrappedFlonumCons(car, cdr)\n        if isinstance(car, W_Object):\n            return W_Cons.make(car, cdr)\n    assert False\n", "label": 1}
{"function": "\n\ndef body(self, dirname=None, part=None):\n    '\\n        Recursively traverses a directory and generates the multipart encoded\\n        body.\\n        '\n    if (part is None):\n        outer = True\n        part = self.envelope\n        dirname = self.directory\n    else:\n        outer = False\n    if (dirname is None):\n        dirname = part.name\n    for chunk in self.gen_chunks(part.open()):\n        (yield chunk)\n    subpart = BodyGenerator(dirname)\n    for chunk in self.gen_chunks(subpart.write_headers()):\n        (yield chunk)\n    (files, subdirs) = utils.ls_dir(dirname)\n    for fn in files:\n        if (not fnmatch.fnmatch(fn, self.fnpattern)):\n            continue\n        fullpath = os.path.join(dirname, fn)\n        for chunk in self.gen_chunks(subpart.file_open(fullpath)):\n            (yield chunk)\n        with open(fullpath, 'rb') as fp:\n            for chunk in self.file_chunks(fp):\n                (yield chunk)\n        for chunk in self.gen_chunks(subpart.file_close()):\n            (yield chunk)\n    if self.recursive:\n        for subdir in subdirs:\n            fullpath = os.path.join(dirname, subdir)\n            for chunk in self.body(fullpath, subpart):\n                (yield chunk)\n    for chunk in self.gen_chunks(subpart.close()):\n        (yield chunk)\n    if outer:\n        for chunk in self.close():\n            (yield chunk)\n", "label": 1}
{"function": "\n\ndef merge_tops(self, tops):\n    '\\n        Cleanly merge the top files\\n        '\n    top = collections.defaultdict(OrderedDict)\n    orders = collections.defaultdict(OrderedDict)\n    for ctops in six.itervalues(tops):\n        for ctop in ctops:\n            for (saltenv, targets) in six.iteritems(ctop):\n                if (saltenv == 'include'):\n                    continue\n                for tgt in targets:\n                    matches = []\n                    states = OrderedDict()\n                    orders[saltenv][tgt] = 0\n                    ignore_missing = False\n                    for comp in ctop[saltenv][tgt]:\n                        if isinstance(comp, dict):\n                            if ('match' in comp):\n                                matches.append(comp)\n                            if ('order' in comp):\n                                order = comp['order']\n                                if (not isinstance(order, int)):\n                                    try:\n                                        order = int(order)\n                                    except ValueError:\n                                        order = 0\n                                orders[saltenv][tgt] = order\n                            if comp.get('ignore_missing', False):\n                                ignore_missing = True\n                        if isinstance(comp, six.string_types):\n                            states[comp] = True\n                    if ignore_missing:\n                        if (saltenv not in self.ignored_pillars):\n                            self.ignored_pillars[saltenv] = []\n                        self.ignored_pillars[saltenv].extend(states.keys())\n                    top[saltenv][tgt] = matches\n                    top[saltenv][tgt].extend(states)\n    return self.sort_top_targets(top, orders)\n", "label": 1}
{"function": "\n\ndef checkReplaceSubstring(self, arguments):\n    allowedFields = {\n        \n    }\n    allowedFields['replace'] = (str, True)\n    allowedFields['with'] = (str, True)\n    for category in arguments.keys():\n        for argumentAttributes in arguments[category]:\n            if ('replace substring' in argumentAttributes):\n                argument = argumentAttributes['long form argument']\n                replace = argumentAttributes['replace substring']\n                for data in replace:\n                    if (not methods.checkIsDictionary(data, self.allowTermination)):\n                        if self.allowTermination:\n                            self.errors.invalidReplaceSubstring(self.name, category, argument)\n                        else:\n                            return False\n                    observedFields = []\n                    toReplace = None\n                    replaceWith = None\n                    for key in data:\n                        value = data[key]\n                        if (key not in allowedFields):\n                            if self.allowTermination:\n                                self.errors.invalidReplaceSubstring(self.name, category, argument)\n                            else:\n                                return False\n                        if (key == 'replace'):\n                            toReplace = value\n                        elif (key == 'with'):\n                            replaceWith = value\n                        observedFields.append(key)\n                    self.arguments[argument].isReplaceSubstring = True\n                    self.arguments[argument].replaceSubstring.append((str(toReplace), str(replaceWith)))\n                    for field in allowedFields.keys():\n                        if (allowedFields[field][1] and (field not in observedFields)):\n                            if self.allowTermination:\n                                self.errors.invalidReplaceSubstring(self.name, category, argument)\n                            else:\n                                return False\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, bits, length=None):\n    if isinstance(bits, int):\n        if (length is None):\n            length = bits.bit_length()\n        else:\n            assert (length >= bits.bit_length())\n        if (bits < 0):\n            bits &= ((1 << length) - 1)\n        hash_value = None\n    elif isinstance(bits, BitString):\n        (bits, length, hash_value) = (bits._bits, bits._length, bits._hash)\n    elif isinstance(bits, str):\n        bit_str = bits\n        bits = 0\n        for char in bit_str:\n            bits <<= 1\n            if (char == '1'):\n                bits += 1\n            else:\n                assert (char == '0')\n        if (length is None):\n            length = len(bit_str)\n        else:\n            assert (length >= len(bit_str))\n        hash_value = None\n    else:\n        bit_sequence = bits\n        bits = 0\n        count = 0\n        for bit in bit_sequence:\n            count += 1\n            bits <<= 1\n            if bit:\n                bits += 1\n        if (length is None):\n            length = count\n        else:\n            assert (length >= count)\n        hash_value = None\n    super().__init__(bits, hash_value)\n    self._length = length\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.re:\n        result = self.re.match(instring, loc)\n        if (not result):\n            raise ParseException(instring, loc, self.errmsg, self)\n        loc = result.end()\n        return (loc, result.group())\n    if (not (instring[loc] in self.initChars)):\n        raise ParseException(instring, loc, self.errmsg, self)\n    start = loc\n    loc += 1\n    instrlen = len(instring)\n    bodychars = self.bodyChars\n    maxloc = (start + self.maxLen)\n    maxloc = min(maxloc, instrlen)\n    while ((loc < maxloc) and (instring[loc] in bodychars)):\n        loc += 1\n    throwException = False\n    if ((loc - start) < self.minLen):\n        throwException = True\n    if (self.maxSpecified and (loc < instrlen) and (instring[loc] in bodychars)):\n        throwException = True\n    if self.asKeyword:\n        if (((start > 0) and (instring[(start - 1)] in bodychars)) or ((loc < instrlen) and (instring[loc] in bodychars))):\n            throwException = True\n    if throwException:\n        raise ParseException(instring, loc, self.errmsg, self)\n    return (loc, instring[start:loc])\n", "label": 1}
{"function": "\n\ndef test_has_iterative():\n    (A, B, C) = symbols('A,B,C', commutative=False)\n    f = (((((((x * gamma(x)) * sin(x)) * exp((x * y))) * A) * B) * C) * cos(((x * A) * B)))\n    assert f.has(x)\n    assert f.has((x * y))\n    assert f.has((x * sin(x)))\n    assert (not f.has((x * sin(y))))\n    assert f.has((x * A))\n    assert f.has(((x * A) * B))\n    assert (not f.has(((x * A) * C)))\n    assert f.has((((x * A) * B) * C))\n    assert (not f.has((((x * A) * C) * B)))\n    assert f.has(((((x * sin(x)) * A) * B) * C))\n    assert (not f.has(((((x * sin(x)) * A) * C) * B)))\n    assert (not f.has(((((x * sin(y)) * A) * B) * C)))\n    assert f.has((x * gamma(x)))\n    assert (not f.has((x + sin(x))))\n    assert ((x & y) & z).has((x & z))\n", "label": 1}
{"function": "\n\n@classmethod\ndef set_field_value(cls, obj, field, value):\n    if isinstance(value, basestring):\n        value = value.strip()\n    try:\n        (field, subfield) = field.split(':')\n    except Exception:\n        pass\n    else:\n        field = field.strip()\n        if (field not in obj):\n            obj[field] = {\n                \n            }\n        cls.set_field_value(obj[field], subfield, value)\n        return\n    try:\n        (field, _) = field.split()\n    except Exception:\n        pass\n    else:\n        dud = {\n            \n        }\n        cls.set_field_value(dud, field, value)\n        ((field, value),) = dud.items()\n        if (field not in obj):\n            obj[field] = []\n        elif (not isinstance(obj[field], list)):\n            obj[field] = [obj[field]]\n        if (value not in (None, '')):\n            obj[field].append(value)\n        return\n    try:\n        (field, nothing) = field.split('?')\n        assert (nothing.strip() == '')\n    except Exception:\n        pass\n    else:\n        try:\n            value = {\n                'yes': True,\n                'true': True,\n                'no': False,\n                'false': False,\n                '': False,\n                None: False,\n            }[(value.lower() if hasattr(value, 'lower') else value)]\n        except KeyError:\n            raise JSONReaderError(('Values for field %s must be \"yes\" or \"no\", not \"%s\"' % (field, value)))\n    field = field.strip()\n    if (field in obj):\n        raise JSONReaderError(('You have a repeat field: %s' % field))\n    obj[field] = value\n", "label": 1}
{"function": "\n\ndef GetItem(self, user, route, has_perm=False, need_perm=False):\n    self.CheckUpdate()\n    if (self.mtitle.find('(BM:') != (- 1)):\n        if (Board.Board.IsBM(user, self.mtitle[4:]) or user.IsSysop()):\n            has_perm = True\n        elif (need_perm and (not has_perm)):\n            return None\n    if ((self.mtitle.find('(BM: BMS)') != (- 1)) or (self.mtitle.find('(BM: SECRET)') != (- 1)) or (self.mtitle.find('(BM: SYSOPS)') != (- 1))):\n        need_perm = True\n    if (len(route) == 0):\n        return self\n    target = (route[0] - 1)\n    _id = target\n    if (_id >= len(self.items)):\n        return None\n    while (self.items[_id].EffectiveId(user) < target):\n        _id += 1\n        if (_id >= len(self.items)):\n            return None\n    item = self.items[_id]\n    item.mtitle = item.title\n    if (len(route) == 1):\n        return item\n    elif item.IsDir():\n        if (not item.CheckUpdate()):\n            return None\n        return item.GetItem(user, route[1:], has_perm, need_perm)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef check_record(d):\n    'check for mandatory, select options, dates. these should ideally be in doclist'\n    from frappe.utils.dateutils import parse_date\n    doc = frappe.get_doc(d)\n    for key in d:\n        docfield = doc.meta.get_field(key)\n        val = d[key]\n        if docfield:\n            if (docfield.reqd and ((val == '') or (val == None))):\n                frappe.msgprint(_('{0} is required').format(docfield.label), raise_exception=1)\n            if ((docfield.fieldtype == 'Select') and val and docfield.options):\n                if (val not in docfield.options.split('\\n')):\n                    frappe.throw(_('{0} must be one of {1}').format(_(docfield.label), comma_or(docfield.options.split('\\n'))))\n            if (val and (docfield.fieldtype == 'Date')):\n                d[key] = parse_date(val)\n            elif (val and (docfield.fieldtype in ['Int', 'Check'])):\n                d[key] = cint(val)\n            elif (val and (docfield.fieldtype in ['Currency', 'Float', 'Percent'])):\n                d[key] = flt(val)\n", "label": 1}
{"function": "\n\ndef get_changes(self):\n    model_defs = freeze_apps([self.migrations.app_label()])\n    for model in models.get_models(models.get_app(self.migrations.app_label())):\n        if (model._meta.abstract or getattr(model._meta, 'proxy', False) or (not getattr(model._meta, 'managed', True))):\n            continue\n        (real_fields, meta, m2m_fields) = self.split_model_def(model, model_defs[model_key(model)])\n        (yield ('AddModel', {\n            'model': model,\n            'model_def': real_fields,\n        }))\n        if meta:\n            for (attr, operation) in (('unique_together', 'AddUnique'), ('index_together', 'AddIndex')):\n                together = eval(meta.get(attr, '[]'))\n                if together:\n                    if isinstance(together[0], string_types):\n                        together = [together]\n                    for fields in together:\n                        (yield (operation, {\n                            'model': model,\n                            'fields': [model._meta.get_field_by_name(x)[0] for x in fields],\n                        }))\n        for (name, triple) in m2m_fields.items():\n            field = model._meta.get_field_by_name(name)[0]\n            if field.rel.through:\n                try:\n                    through_model = field.rel.through_model\n                except AttributeError:\n                    through_model = field.rel.through\n            if ((not field.rel.through) or getattr(through_model._meta, 'auto_created', False)):\n                (yield ('AddM2M', {\n                    'model': model,\n                    'field': field,\n                }))\n", "label": 1}
{"function": "\n\ndef line_starts_with_open_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)/\\\\*', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef _set_knspace(self, value):\n    if (value is self._knspace):\n        return\n    knspace = (self._knspace or self.__last_knspace)\n    name = self.knsname\n    if (name and knspace and (getattr(knspace, name) == self)):\n        setattr(knspace, name, None)\n    if (value == 'fork'):\n        if (not knspace):\n            knspace = self.knspace\n        if knspace:\n            value = knspace.fork()\n        else:\n            raise ValueError('Cannot fork with no namesapce')\n    for (obj, prop_name, uid) in (self.__callbacks or []):\n        obj.unbind_uid(prop_name, uid)\n    self.__last_knspace = self.__callbacks = None\n    if name:\n        if (value is None):\n            knspace = self.__set_parent_knspace()\n            if knspace:\n                setattr(knspace, name, self)\n            self._knspace = None\n        else:\n            setattr(value, name, self)\n            knspace = self._knspace = value\n        if (not knspace):\n            raise ValueError('Object has name \"{}\", but no namespace'.format(name))\n    else:\n        if (value is None):\n            self.__set_parent_knspace()\n        self._knspace = value\n", "label": 1}
{"function": "\n\ndef __init__(self, template):\n    self.template = template\n    self.parts = []\n    parts = re.split('(\\\\{[^\\\\}]*\\\\})', self.template)\n    for part in parts:\n        if part:\n            if (('{' == part[0]) and ('}' == part[(- 1)])):\n                expression = part[1:(- 1)]\n                if re.match('^([a-zA-Z0-9_]|%[0-9a-fA-F][0-9a-fA-F]).*$', expression):\n                    self.parts.append(SimpleExpansion(expression))\n                elif ('+' == part[1]):\n                    self.parts.append(ReservedExpansion(expression))\n                elif ('#' == part[1]):\n                    self.parts.append(FragmentExpansion(expression))\n                elif ('.' == part[1]):\n                    self.parts.append(LabelExpansion(expression))\n                elif ('/' == part[1]):\n                    self.parts.append(PathExpansion(expression))\n                elif (';' == part[1]):\n                    self.parts.append(PathStyleExpansion(expression))\n                elif ('?' == part[1]):\n                    self.parts.append(FormStyleQueryExpansion(expression))\n                elif ('&' == part[1]):\n                    self.parts.append(FormStyleQueryContinuation(expression))\n                elif (part[1] in '=,!@|'):\n                    raise UnsupportedExpression(part)\n                else:\n                    raise BadExpression(part)\n            elif (('{' not in part) and ('}' not in part)):\n                self.parts.append(Literal(part))\n            else:\n                raise BadExpression(part)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.messageBox = MessageBox()\n                self.messageBox.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.displayName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.contacts = []\n                (_etype49, _size46) = iprot.readListBegin()\n                for _i50 in xrange(_size46):\n                    _elem51 = Contact()\n                    _elem51.read(iprot)\n                    self.contacts.append(_elem51)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.mystery = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\n@register.inclusion_tag('zinnia/tags/dummy.html', takes_context=True)\ndef get_calendar_entries(context, year=None, month=None, template='zinnia/tags/entries_calendar.html'):\n    '\\n    Return an HTML calendar of entries.\\n    '\n    if (not (year and month)):\n        day_week_month = (context.get('day') or context.get('week') or context.get('month'))\n        publication_date = getattr(context.get('object'), 'publication_date', None)\n        if day_week_month:\n            current_month = day_week_month\n        elif publication_date:\n            if settings.USE_TZ:\n                publication_date = timezone.localtime(publication_date)\n            current_month = publication_date.date()\n        else:\n            today = timezone.now()\n            if settings.USE_TZ:\n                today = timezone.localtime(today)\n            current_month = today.date()\n        current_month = current_month.replace(day=1)\n    else:\n        current_month = date(year, month, 1)\n    dates = list(map((lambda x: ((settings.USE_TZ and timezone.localtime(x).date()) or x.date())), Entry.published.datetimes('publication_date', 'month')))\n    if (current_month not in dates):\n        dates.append(current_month)\n        dates.sort()\n    index = dates.index(current_month)\n    previous_month = (((index > 0) and dates[(index - 1)]) or None)\n    next_month = (((index != (len(dates) - 1)) and dates[(index + 1)]) or None)\n    calendar = Calendar()\n    return {\n        'template': template,\n        'next_month': next_month,\n        'previous_month': previous_month,\n        'calendar': calendar.formatmonth(current_month.year, current_month.month, previous_month=previous_month, next_month=next_month),\n    }\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _handle_results(outqueue, get, cache):\n    thread = threading.current_thread()\n    while 1:\n        try:\n            task = get()\n        except (IOError, EOFError):\n            debug('result handler got EOFError/IOError -- exiting')\n            return\n        if thread._state:\n            assert (thread._state == TERMINATE)\n            debug('result handler found thread._state=TERMINATE')\n            break\n        if (task is None):\n            debug('result handler got sentinel')\n            break\n        (job, i, obj) = task\n        try:\n            cache[job]._set(i, obj)\n        except KeyError:\n            pass\n    while (cache and (thread._state != TERMINATE)):\n        try:\n            task = get()\n        except (IOError, EOFError):\n            debug('result handler got EOFError/IOError -- exiting')\n            return\n        if (task is None):\n            debug('result handler ignoring extra sentinel')\n            continue\n        (job, i, obj) = task\n        try:\n            cache[job]._set(i, obj)\n        except KeyError:\n            pass\n    if hasattr(outqueue, '_reader'):\n        debug('ensuring that outqueue is not full')\n        try:\n            for i in range(10):\n                if (not outqueue._reader.poll()):\n                    break\n                get()\n        except (IOError, EOFError):\n            pass\n    debug('result handler exiting: len(cache)=%s, thread._state=%s', len(cache), thread._state)\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    a = self.msg\n    b = ''\n    if self.http_scheme:\n        b += ('%s://' % self.http_scheme)\n    if self.http_host:\n        b += self.http_host\n    if self.http_port:\n        b += (':%s' % self.http_port)\n    if self.http_path:\n        b += self.http_path\n    if self.http_query:\n        b += ('?%s' % self.http_query)\n    if self.http_status:\n        if b:\n            b = ('%s %s' % (b, self.http_status))\n        else:\n            b = str(self.http_status)\n    if self.http_reason:\n        if b:\n            b = ('%s %s' % (b, self.http_reason))\n        else:\n            b = ('- %s' % self.http_reason)\n    if self.http_device:\n        if b:\n            b = ('%s: device %s' % (b, self.http_device))\n        else:\n            b = ('device %s' % self.http_device)\n    if self.http_response_content:\n        if (len(self.http_response_content) <= 60):\n            b += ('   %s' % self.http_response_content)\n        else:\n            b += ('  [first 60 chars of response] %s' % self.http_response_content[:60])\n    return ((b and ('%s: %s' % (a, b))) or a)\n", "label": 1}
{"function": "\n\ndef m44is_identity(m):\n    (m0, m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13, m14, m15) = m\n    return ((m0 == 1) and (m1 == 0) and (m2 == 0) and (m3 == 0) and (m4 == 0) and (m5 == 1) and (m6 == 0) and (m7 == 0) and (m8 == 0) and (m9 == 0) and (m10 == 1) and (m11 == 0) and (m12 == 0) and (m13 == 0) and (m14 == 0) and (m15 == 1))\n", "label": 1}
{"function": "\n\ndef get_sizes(self, total_width, total_height, xoffset=0, yoffset=0):\n    width = 0\n    height = 0\n    results = []\n    (rows, cols, orientation) = self.calc(self.num_windows, total_width, total_height)\n    if (orientation == ROWCOL):\n        y = 0\n        for (i, row) in enumerate(range(rows)):\n            x = 0\n            width = (total_width // cols)\n            for (j, col) in enumerate(range(cols)):\n                height = (total_height // rows)\n                if ((i == (rows - 1)) and (j == 0)):\n                    remaining = (self.num_windows - len(results))\n                    width = (total_width // remaining)\n                elif ((j == (cols - 1)) or ((len(results) + 1) == self.num_windows)):\n                    width = (total_width - x)\n                results.append(((x + xoffset), (y + yoffset), width, height))\n                if (len(results) == self.num_windows):\n                    return results\n                x += width\n            y += height\n    else:\n        x = 0\n        for (i, col) in enumerate(range(cols)):\n            y = 0\n            height = (total_height // rows)\n            for (j, row) in enumerate(range(rows)):\n                width = (total_width // cols)\n                if ((i == (cols - 1)) and (j == 0)):\n                    remaining = (self.num_windows - len(results))\n                    height = (total_height // remaining)\n                elif ((j == (rows - 1)) or ((len(results) + 1) == self.num_windows)):\n                    height = (total_height - y)\n                results.append(((x + xoffset), (y + yoffset), width, height))\n                if (len(results) == self.num_windows):\n                    return results\n                y += height\n            x += width\n    return results\n", "label": 1}
{"function": "\n\ndef byte_count(byte_count):\n    if isinstance(byte_count, int):\n        return positive_nonzero_integer(byte_count)\n    byte_count = unicode_str(byte_count)\n\n    def _get_byte_count(postfix, base, exponant):\n        char_num = len(postfix)\n        if (byte_count[(- char_num):] == postfix):\n            count = decimal.Decimal(byte_count[:(- char_num)])\n            return positive_nonzero_integer((count * (base ** exponant)))\n        return None\n    if (len(byte_count) > 1):\n        n = None\n        n = (n if (n is not None) else _get_byte_count('K', 1024, 1))\n        n = (n if (n is not None) else _get_byte_count('M', 1024, 2))\n        n = (n if (n is not None) else _get_byte_count('G', 1024, 3))\n        n = (n if (n is not None) else _get_byte_count('T', 1024, 4))\n        n = (n if (n is not None) else _get_byte_count('P', 1024, 5))\n        if (n is not None):\n            return n\n    if (len(byte_count) > 2):\n        n = None\n        n = (n if (n is not None) else _get_byte_count('KB', 1000, 1))\n        n = (n if (n is not None) else _get_byte_count('MB', 1000, 2))\n        n = (n if (n is not None) else _get_byte_count('GB', 1000, 3))\n        n = (n if (n is not None) else _get_byte_count('TB', 1000, 4))\n        n = (n if (n is not None) else _get_byte_count('PB', 1000, 5))\n        if (n is not None):\n            return n\n    return positive_nonzero_integer(byte_count)\n", "label": 1}
{"function": "\n\ndef tokens(self, event, next):\n    (kind, data, pos) = event\n    if (kind == START):\n        (tag, attribs) = data\n        name = tag.localname\n        namespace = tag.namespace\n        converted_attribs = {\n            \n        }\n        for (k, v) in attribs:\n            if isinstance(k, QName):\n                converted_attribs[(k.namespace, k.localname)] = v\n            else:\n                converted_attribs[(None, k)] = v\n        if ((namespace == namespaces['html']) and (name in voidElements)):\n            for token in self.emptyTag(namespace, name, converted_attribs, ((not next) or (next[0] != END) or (next[1] != tag))):\n                (yield token)\n        else:\n            (yield self.startTag(namespace, name, converted_attribs))\n    elif (kind == END):\n        name = data.localname\n        namespace = data.namespace\n        if (name not in voidElements):\n            (yield self.endTag(namespace, name))\n    elif (kind == COMMENT):\n        (yield self.comment(data))\n    elif (kind == TEXT):\n        for token in self.text(data):\n            (yield token)\n    elif (kind == DOCTYPE):\n        (yield self.doctype(*data))\n    elif (kind in (XML_NAMESPACE, DOCTYPE, START_NS, END_NS, START_CDATA, END_CDATA, PI)):\n        pass\n    else:\n        (yield self.unknown(kind))\n", "label": 1}
{"function": "\n\ndef reverse(viewname, urlconf=None, args=None, kwargs=None, current_app=None):\n    if (urlconf is None):\n        urlconf = get_urlconf()\n    resolver = get_resolver(urlconf)\n    args = (args or [])\n    kwargs = (kwargs or {\n        \n    })\n    prefix = get_script_prefix()\n    if (not isinstance(viewname, six.string_types)):\n        view = viewname\n    else:\n        parts = viewname.split(':')\n        parts.reverse()\n        view = parts[0]\n        path = parts[1:]\n        if current_app:\n            current_path = current_app.split(':')\n            current_path.reverse()\n        else:\n            current_path = None\n        resolved_path = []\n        ns_pattern = ''\n        while path:\n            ns = path.pop()\n            current_ns = (current_path.pop() if current_path else None)\n            try:\n                app_list = resolver.app_dict[ns]\n                if (current_ns and (current_ns in app_list)):\n                    ns = current_ns\n                elif (ns not in app_list):\n                    ns = app_list[0]\n            except KeyError:\n                pass\n            if (ns != current_ns):\n                current_path = None\n            try:\n                (extra, resolver) = resolver.namespace_dict[ns]\n                resolved_path.append(ns)\n                ns_pattern = (ns_pattern + extra)\n            except KeyError as key:\n                if resolved_path:\n                    raise NoReverseMatch((\"%s is not a registered namespace inside '%s'\" % (key, ':'.join(resolved_path))))\n                else:\n                    raise NoReverseMatch(('%s is not a registered namespace' % key))\n        if ns_pattern:\n            resolver = get_ns_resolver(ns_pattern, resolver)\n    return force_text(iri_to_uri(resolver._reverse_with_prefix(view, prefix, *args, **kwargs)))\n", "label": 1}
{"function": "\n\ndef tokensMatch(expectedTokens, receivedTokens, ignoreErrorOrder, ignoreErrors=False):\n    \"Test whether the test has passed or failed\\n\\n    If the ignoreErrorOrder flag is set to true we don't test the relative\\n    positions of parse errors and non parse errors\\n    \"\n    checkSelfClosing = False\n    for token in expectedTokens:\n        if (((token[0] == 'StartTag') and (len(token) == 4)) or ((token[0] == 'EndTag') and (len(token) == 3))):\n            checkSelfClosing = True\n            break\n    if (not checkSelfClosing):\n        for token in receivedTokens:\n            if ((token[0] == 'StartTag') or (token[0] == 'EndTag')):\n                token.pop()\n    if ((not ignoreErrorOrder) and (not ignoreErrors)):\n        return (expectedTokens == receivedTokens)\n    else:\n        tokens = {\n            'expected': [[], []],\n            'received': [[], []],\n        }\n        for (tokenType, tokenList) in zip(tokens.keys(), (expectedTokens, receivedTokens)):\n            for token in tokenList:\n                if (token != 'ParseError'):\n                    tokens[tokenType][0].append(token)\n                elif (not ignoreErrors):\n                    tokens[tokenType][1].append(token)\n        return (tokens['expected'] == tokens['received'])\n", "label": 1}
{"function": "\n\ndef queue_events(self, timeout):\n    with self._lock:\n        inotify_events = self._inotify.read_events()\n        if (not any([(event.is_moved_from or event.is_moved_to) for event in inotify_events])):\n            self._inotify.clear_move_records()\n        for event in inotify_events:\n            if event.is_moved_to:\n                try:\n                    src_path = self._inotify.source_for_move(event)\n                    to_event = event\n                    dest_path = to_event.src_path\n                    klass = ACTION_EVENT_MAP[(to_event.is_directory, EVENT_TYPE_MOVED)]\n                    event = klass(src_path, dest_path)\n                    self.queue_event(event)\n                    if (event.is_directory and self.watch.is_recursive):\n                        for sub_event in event.sub_moved_events():\n                            self.queue_event(sub_event)\n                except KeyError:\n                    pass\n            elif event.is_attrib:\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_MODIFIED)]\n                self.queue_event(klass(event.src_path))\n            elif event.is_close_write:\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_MODIFIED)]\n                self.queue_event(klass(event.src_path))\n            elif event.is_modify:\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_MODIFIED)]\n                self.queue_event(klass(event.src_path))\n            elif (event.is_delete or event.is_delete_self):\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_DELETED)]\n                self.queue_event(klass(event.src_path))\n            elif event.is_create:\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_CREATED)]\n                self.queue_event(klass(event.src_path))\n", "label": 1}
{"function": "\n\ndef legalize_return_type(return_type, interp, targetctx):\n    '\\n    Only accept array return type iff it is passed into the function.\\n    Reject function object return types if in nopython mode.\\n    '\n    if ((not targetctx.enable_nrt) and isinstance(return_type, types.Array)):\n        retstmts = []\n        caststmts = {\n            \n        }\n        argvars = set()\n        for (bid, blk) in interp.blocks.items():\n            for inst in blk.body:\n                if isinstance(inst, ir.Return):\n                    retstmts.append(inst.value.name)\n                elif isinstance(inst, ir.Assign):\n                    if (isinstance(inst.value, ir.Expr) and (inst.value.op == 'cast')):\n                        caststmts[inst.target.name] = inst.value\n                    elif isinstance(inst.value, ir.Arg):\n                        argvars.add(inst.target.name)\n        assert retstmts, 'No return statements?'\n        for var in retstmts:\n            cast = caststmts.get(var)\n            if ((cast is None) or (cast.value.name not in argvars)):\n                raise TypeError('Only accept returning of array passed into the function as argument')\n    elif (isinstance(return_type, types.Function) or isinstance(return_type, types.Phantom)):\n        raise TypeError(\"Can't return function object in nopython mode\")\n", "label": 1}
{"function": "\n\ndef test_entity_version():\n    entity = File(parent=project['id'])\n    entity['path'] = utils.make_bogus_data_file()\n    schedule_for_cleanup(entity['path'])\n    entity = syn.createEntity(entity)\n    syn.setAnnotations(entity, {\n        'fizzbuzz': 111222,\n    })\n    entity = syn.getEntity(entity)\n    assert (entity.versionNumber == 1)\n    entity.foo = 998877\n    entity['name'] = 'foobarbat'\n    entity['description'] = 'This is a test entity...'\n    entity = syn.updateEntity(entity, incrementVersion=True, versionLabel='Prada remix')\n    assert (entity.versionNumber == 2)\n    annotations = syn.getAnnotations(entity, version=1)\n    assert (annotations['fizzbuzz'][0] == 111222)\n    returnEntity = syn.getEntity(entity, version=1)\n    assert (returnEntity.versionNumber == 1)\n    assert (returnEntity['fizzbuzz'][0] == 111222)\n    assert ('foo' not in returnEntity)\n    returnEntity = syn.getEntity(entity)\n    assert (returnEntity.versionNumber == 2)\n    assert (returnEntity['foo'][0] == 998877)\n    assert (returnEntity['name'] == 'foobarbat')\n    assert (returnEntity['description'] == 'This is a test entity...')\n    assert (returnEntity['versionLabel'] == 'Prada remix')\n    returnEntity = syn.downloadEntity(entity, version=1)\n    assert (returnEntity.versionNumber == 1)\n    assert (returnEntity['fizzbuzz'][0] == 111222)\n    assert ('foo' not in returnEntity)\n    syn.delete(entity, version=2)\n    returnEntity = syn.getEntity(entity)\n    assert (returnEntity.versionNumber == 1)\n", "label": 1}
{"function": "\n\ndef _possibly_convert_objects(values, convert_dates=True, convert_numeric=True, convert_timedeltas=True, copy=True):\n    ' if we have an object dtype, try to coerce dates and/or numbers '\n    if isinstance(values, (list, tuple)):\n        values = np.array(values, dtype=np.object_)\n    if (not hasattr(values, 'dtype')):\n        values = np.array([values], dtype=np.object_)\n    if (convert_dates and (values.dtype == np.object_)):\n        if (convert_dates == 'coerce'):\n            new_values = _possibly_cast_to_datetime(values, 'M8[ns]', errors='coerce')\n            if (not isnull(new_values).all()):\n                values = new_values\n        else:\n            values = lib.maybe_convert_objects(values, convert_datetime=convert_dates)\n    if (convert_timedeltas and (values.dtype == np.object_)):\n        if (convert_timedeltas == 'coerce'):\n            from pandas.tseries.timedeltas import to_timedelta\n            new_values = to_timedelta(values, coerce=True)\n            if (not isnull(new_values).all()):\n                values = new_values\n        else:\n            values = lib.maybe_convert_objects(values, convert_timedelta=convert_timedeltas)\n    if (values.dtype == np.object_):\n        if convert_numeric:\n            try:\n                new_values = lib.maybe_convert_numeric(values, set(), coerce_numeric=True)\n                if (not isnull(new_values).all()):\n                    values = new_values\n            except:\n                pass\n        else:\n            values = lib.maybe_convert_objects(values)\n    values = (values.copy() if copy else values)\n    return values\n", "label": 1}
{"function": "\n\ndef CalculateReducedDataFittingTarget(self, inCoeffs):\n    if (not self.AreCoefficientsWithinBounds(inCoeffs)):\n        try:\n            if (self.upperCoefficientBounds != []):\n                for i in range(len(inCoeffs)):\n                    if (self.upperCoefficientBounds[i] != None):\n                        if (inCoeffs[i] > self.upperCoefficientBounds[i]):\n                            inCoeffs[i] = self.upperCoefficientBounds[i]\n            if (self.lowerCoefficientBounds != []):\n                for i in range(len(inCoeffs)):\n                    if (self.lowerCoefficientBounds[i] != None):\n                        if (inCoeffs[i] < self.lowerCoefficientBounds[i]):\n                            inCoeffs[i] = self.lowerCoefficientBounds[i]\n        except:\n            pass\n    try:\n        if (self.fixedCoefficients != []):\n            for i in range(len(inCoeffs)):\n                if (self.fixedCoefficients[i] != None):\n                    inCoeffs[i] = self.fixedCoefficients[i]\n        error = (self.CalculateModelPredictions(inCoeffs, self.dataCache.reducedDataCacheDictionary) - self.dataCache.reducedDataCacheDictionary['DependentData'])\n        ssq = numpy.sum(numpy.square(error))\n    except:\n        return 1e+300\n    if numpy.isfinite(ssq):\n        return ssq\n    else:\n        return 1e+300\n", "label": 1}
{"function": "\n\n@staticmethod\ndef layout(item):\n    ' Custom Layout Method '\n    if (not item.authorized):\n        enabled = False\n        visible = False\n    elif ((item.enabled is None) or item.enabled):\n        enabled = True\n        visible = True\n    if (enabled and visible):\n        if (item.parent is not None):\n            if (item.enabled and item.authorized):\n                if item.components:\n                    _class = ''\n                    if ((item.parent.parent is None) and item.selected):\n                        _class = 'highlight'\n                    items = item.render_components()\n                    if items:\n                        items = LI(UL(items, _class='menu-extention'))\n                    return [LI(A(item.label, _href=item.url(), _id=item.attr._id, _class=_class)), items]\n                else:\n                    if (item.parent.parent is None):\n                        _class = ((item.selected and 'highlight') or '')\n                    else:\n                        _class = ' '\n                    return LI(A(item.label, _href=item.url(), _id=item.attr._id, _class=_class))\n        else:\n            items = item.render_components()\n            return UL(items, _id='main-sub-menu', _class='sub-menu')\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef delete_file(self, path, prefixed_path, source_storage):\n    '\\n        Checks if the target file should be deleted if it already exists\\n        '\n    if self.storage.exists(prefixed_path):\n        try:\n            target_last_modified = self.storage.modified_time(prefixed_path)\n        except (OSError, NotImplementedError, AttributeError):\n            pass\n        else:\n            try:\n                source_last_modified = source_storage.modified_time(path)\n            except (OSError, NotImplementedError, AttributeError):\n                pass\n            else:\n                if self.local:\n                    full_path = self.storage.path(prefixed_path)\n                else:\n                    full_path = None\n                if (target_last_modified >= source_last_modified):\n                    if (not ((self.symlink and full_path and (not os.path.islink(full_path))) or ((not self.symlink) and full_path and os.path.islink(full_path)))):\n                        if (prefixed_path not in self.unmodified_files):\n                            self.unmodified_files.append(prefixed_path)\n                        self.log((\"Skipping '%s' (not modified)\" % path))\n                        return False\n        if self.dry_run:\n            self.log((\"Pretending to delete '%s'\" % path))\n        else:\n            self.log((\"Deleting '%s'\" % path))\n            self.storage.delete(prefixed_path)\n    return True\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    _get_fds_service_config(args)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.fds_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                stop_job(args, hosts[host_id].ip, job_name, instance_id)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.fds_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                deploy_utils.wait_for_job_stopping('fds', args.fds_config.cluster.name, job_name, hosts[host_id].ip, instance_id)\n                start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)\n", "label": 1}
{"function": "\n\ndef should_be_compact_paragraph(self, node):\n    '\\n        Determine if the <p> tags around paragraph ``node`` can be omitted.\\n        '\n    if (isinstance(node.parent, nodes.document) or isinstance(node.parent, nodes.compound)):\n        return False\n    for (key, value) in node.attlist():\n        if (node.is_not_default(key) and (not ((key == 'classes') and (value in ([], ['first'], ['last'], ['first', 'last']))))):\n            return False\n    first = isinstance(node.parent[0], nodes.label)\n    for child in node.parent.children[first:]:\n        if isinstance(child, nodes.Invisible):\n            continue\n        if (child is node):\n            break\n        return False\n    parent_length = len([n for n in node.parent if (not isinstance(n, (nodes.Invisible, nodes.label)))])\n    if (self.compact_simple or self.compact_field_list or (self.compact_p and (parent_length == 1))):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef _get_with(self, key):\n    if isinstance(key, slice):\n        indexer = self.index._convert_slice_indexer(key, kind='getitem')\n        return self._get_values(indexer)\n    elif isinstance(key, ABCDataFrame):\n        raise TypeError('Indexing a Series with DataFrame is not supported, use the appropriate DataFrame column')\n    else:\n        if isinstance(key, tuple):\n            try:\n                return self._get_values_tuple(key)\n            except:\n                if (len(key) == 1):\n                    key = key[0]\n                    if isinstance(key, slice):\n                        return self._get_values(key)\n                raise\n        if (not isinstance(key, (list, np.ndarray, Series, Index))):\n            key = list(key)\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key)\n        if (key_type == 'integer'):\n            if (self.index.is_integer() or self.index.is_floating()):\n                return self.reindex(key)\n            else:\n                return self._get_values(key)\n        elif (key_type == 'boolean'):\n            return self._get_values(key)\n        else:\n            try:\n                if isinstance(key, (list, tuple)):\n                    return self.ix[key]\n                return self.reindex(key)\n            except Exception:\n                if isinstance(key[0], slice):\n                    return self._get_values(key)\n                raise\n", "label": 1}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if ((self.email_box_type == 'imap') and (not self.email_box_imap_folder)):\n        self.email_box_imap_folder = 'INBOX'\n    if self.socks_proxy_type:\n        if (not self.socks_proxy_host):\n            self.socks_proxy_host = '127.0.0.1'\n        if (not self.socks_proxy_port):\n            self.socks_proxy_port = 9150\n    else:\n        self.socks_proxy_host = None\n        self.socks_proxy_port = None\n    if (not self.email_box_port):\n        if ((self.email_box_type == 'imap') and self.email_box_ssl):\n            self.email_box_port = 993\n        elif ((self.email_box_type == 'imap') and (not self.email_box_ssl)):\n            self.email_box_port = 143\n        elif ((self.email_box_type == 'pop3') and self.email_box_ssl):\n            self.email_box_port = 995\n        elif ((self.email_box_type == 'pop3') and (not self.email_box_ssl)):\n            self.email_box_port = 110\n    if (not self.id):\n        basename = self.prepare_permission_name()\n        Permission.objects.create(name=(_('Permission for queue: ') + self.title), content_type=ContentType.objects.get(model='queue'), codename=basename)\n    super(Queue, self).save(*args, **kwargs)\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, x, y):\n    I = S.Infinity\n    N = S.NegativeInfinity\n    O = S.Zero\n    if ((x is S.NaN) or (y is S.NaN)):\n        return S.NaN\n    elif (x == y):\n        return S.Zero\n    elif (((x is I) or (x is N) or (x is O)) or ((y is I) or (y is N) or (y is O))):\n        return (erf(y) - erf(x))\n    if ((y.func is erf2inv) and (y.args[0] == x)):\n        return y.args[1]\n    sign_x = x.could_extract_minus_sign()\n    sign_y = y.could_extract_minus_sign()\n    if (sign_x and sign_y):\n        return (- cls((- x), (- y)))\n    elif (sign_x or sign_y):\n        return (erf(y) - erf(x))\n", "label": 1}
{"function": "\n\ndef start():\n    'Start updating from a command and arguments.'\n    try:\n        data = getopt.getopt(sys.argv[1:], 'hms:e:', ['help', 'hide', 'more', 'start=', 'end='])\n    except getopt.GetoptError:\n        usage()\n        sys.exit(2)\n    hide = False\n    more = False\n    start = '01-01-2012'\n    today = date.today()\n    end = ('%i-%i-%i' % (today.month, today.day, today.year))\n    for x in data[0]:\n        if ((x[0] == '-h') or (x[0] == '--help')):\n            usage()\n            sys.exit()\n        elif (x[0] == '--hide'):\n            hide = True\n        elif ((x[0] == '-m') or (x[0] == '--more')):\n            more = True\n        elif ((x[0] == '-s') or (x[0] == '--start')):\n            start = x[1]\n        elif ((x[0] == '-e') or (x[0] == '--end')):\n            end = x[1]\n    try:\n        split = start.split('-')\n        split2 = end.split('-')\n        date1 = date(int(split[2]), int(split[0]), int(split[1]))\n        date2 = date(int(split2[2]), int(split2[0]), int(split2[1]))\n        if ((date1 > date2) or (date1 >= today) or (date2 > today)):\n            date_usage()\n            sys.exit(2)\n    except:\n        date_usage()\n        sys.exit(2)\n    run(hide, more, start, end)\n", "label": 1}
{"function": "\n\ndef get_tokens_unprocessed(self, data):\n    sql = PsqlRegexLexer(**self.options)\n    lines = lookahead(line_re.findall(data))\n    while 1:\n        curcode = ''\n        insertions = []\n        while 1:\n            try:\n                line = lines.next()\n            except StopIteration:\n                break\n            if (line.startswith('$') and (not curcode)):\n                lexer = get_lexer_by_name('console', **self.options)\n                for x in lexer.get_tokens_unprocessed(line):\n                    (yield x)\n                break\n            mprompt = re_prompt.match(line)\n            if (mprompt is not None):\n                insertions.append((len(curcode), [(0, Generic.Prompt, mprompt.group())]))\n                curcode += line[len(mprompt.group()):]\n            else:\n                curcode += line\n            if (re_psql_command.match(curcode) or re_end_command.search(curcode)):\n                break\n        for item in do_insertions(insertions, sql.get_tokens_unprocessed(curcode)):\n            (yield item)\n        out_token = Generic.Output\n        while 1:\n            line = lines.next()\n            mprompt = re_prompt.match(line)\n            if (mprompt is not None):\n                lines.send(line)\n                break\n            mmsg = re_message.match(line)\n            if (mmsg is not None):\n                if (mmsg.group(1).startswith('ERROR') or mmsg.group(1).startswith('FATAL')):\n                    out_token = Generic.Error\n                (yield (mmsg.start(1), Generic.Strong, mmsg.group(1)))\n                (yield (mmsg.start(2), out_token, mmsg.group(2)))\n            else:\n                (yield (0, out_token, line))\n", "label": 1}

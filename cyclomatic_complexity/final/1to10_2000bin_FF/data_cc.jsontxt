{"function": "\n\ndef test_s3_domain_with_default_root_object(self):\n    cmdline = ((self.prefix + '--origin-domain-name foo.s3.amazonaws.com ') + '--default-root-object index.html')\n    result = {\n        'DistributionConfig': {\n            'Origins': {\n                'Quantity': 1,\n                'Items': [{\n                    'S3OriginConfig': mock.ANY,\n                    'DomainName': 'foo.s3.amazonaws.com',\n                    'Id': mock.ANY,\n                    'OriginPath': '',\n                }],\n            },\n            'CallerReference': mock.ANY,\n            'Comment': '',\n            'Enabled': True,\n            'DefaultCacheBehavior': mock.ANY,\n            'DefaultRootObject': 'index.html',\n        },\n    }\n    self.run_cmd(cmdline)\n    self.assertEqual(self.last_kwargs, result)\n", "label": 0}
{"function": "\n\ndef pages_dynamic_tree_menu(context, page, url='/'):\n    '\\n    Render a \"dynamic\" tree menu, with all nodes expanded which are either\\n    ancestors or the current page itself.\\n\\n    Override ``pages/dynamic_tree_menu.html`` if you want to change the\\n    design.\\n\\n    :param page: the current page\\n    :param url: not used anymore\\n    '\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    children = None\n    if (page and ('current_page' in context)):\n        current_page = context['current_page']\n        if ((page.tree_id == current_page.tree_id) and (page.lft <= current_page.lft) and (page.rght >= current_page.rght)):\n            children = page.get_children_for_frontend()\n    context.update({\n        'children': children,\n        'page': page,\n    })\n    return context\n", "label": 1}
{"function": "\n\ndef remove(path, recursive=False, use_sudo=False):\n    '\\n    Remove a file or directory\\n    '\n    func = ((use_sudo and run_as_root) or run)\n    options = ('-r ' if recursive else '')\n    func('/bin/rm {0}{1}'.format(options, quote(path)))\n", "label": 0}
{"function": "\n\ndef marshal_dump(code, f):\n    if isinstance(f, file):\n        marshal.dump(code, f)\n    else:\n        f.write(marshal.dumps(code))\n", "label": 0}
{"function": "\n\ndef test_show_body(self):\n    client = Mock()\n    client.indices.get_settings.return_value = testvars.settings_one\n    client.cluster.state.return_value = testvars.clu_state_one\n    client.indices.stats.return_value = testvars.stats_one\n    ilo = curator.IndexList(client)\n    ao = curator.Alias(name='alias')\n    ao.remove(ilo)\n    ao.add(ilo)\n    body = ao.body()\n    self.assertEqual(testvars.alias_one_body['actions'][0], body['actions'][0])\n    self.assertEqual(testvars.alias_one_body['actions'][1], body['actions'][1])\n", "label": 0}
{"function": "\n\n@mock.patch('SoftLayer.API.BaseClient.iter_call')\ndef test_iterate(self, _iter_call):\n    self.client['SERVICE'].METHOD(iter=True)\n    _iter_call.assert_called_with('SERVICE', 'METHOD')\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    return self.build('add', self, other)\n", "label": 0}
{"function": "\n\n@classmethod\ndef read(cls, handle):\n    self = cls()\n    self._read_luminosity(handle)\n    self.name = handle.attrs['name'].decode('utf-8')\n    self.peeloff = str2bool(handle.attrs['peeloff'])\n    if (handle.attrs['spectrum'] == b'spectrum'):\n        self.spectrum = Table(np.array(handle['spectrum']))\n    elif (handle.attrs['spectrum'] == b'temperature'):\n        self.temperature = handle.attrs['temperature']\n    elif (handle.attrs['spectrum'] == b'lte'):\n        pass\n    else:\n        raise ValueError(('Unexpected value for `spectrum`: %s' % handle.attrs['spectrum']))\n    return self\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    category = resolve(self.category, context)\n    if isinstance(category, CategoryBase):\n        cat = category\n    else:\n        cat = get_category(category, self.model)\n    try:\n        if (cat is not None):\n            context[self.varname] = drilldown_tree_for_node(cat)\n        else:\n            context[self.varname] = []\n    except:\n        context[self.varname] = []\n    return ''\n", "label": 0}
{"function": "\n\ndef __init__(self, manufacturer, data):\n    self.manufacturer = manufacturer\n    self.data = list(data)\n", "label": 0}
{"function": "\n\ndef createLineSet(self, indices, inputlist, materialid):\n    'Create a set of lines for use in this geometry instance.\\n\\n        :param numpy.array indices:\\n          unshaped numpy array that contains the indices for\\n          the inputs referenced in inputlist\\n        :param collada.source.InputList inputlist:\\n          The inputs for this primitive\\n        :param str materialid:\\n          A string containing a symbol that will get used to bind this lineset\\n          to a material when instantiating into a scene\\n\\n        :rtype: :class:`collada.lineset.LineSet`\\n        '\n    inputdict = primitive.Primitive._getInputsFromList(self.collada, self.sourceById, inputlist.getList())\n    return lineset.LineSet(inputdict, materialid, indices)\n", "label": 0}
{"function": "\n\ndef stepSlice(self, offset):\n    ' Move the selected structure one slice up or down\\n        :param offset: +1 or -1\\n        :return:\\n        '\n    volumeId = self.volumeSelector.currentNodeId\n    if (volumeId == ''):\n        self.showUnselectedVolumeWarningMessage()\n        return\n    selectedStructure = self.getCurrentSelectedStructure()\n    if (selectedStructure == self.logic.NONE):\n        self.showUnselectedStructureWarningMessage()\n        return\n    if (selectedStructure == self.logic.BOTH):\n        self.logic.stepSlice(volumeId, self.logic.AORTA, offset)\n        newSlice = self.logic.stepSlice(volumeId, self.logic.PA, offset)\n    else:\n        newSlice = self.logic.stepSlice(volumeId, selectedStructure, offset)\n    self.moveRedWindowToSlice(newSlice)\n", "label": 0}
{"function": "\n\ndef normalize_diff_filename(self, filename):\n    \"Normalize filenames in diffs.\\n\\n        The default behavior of stripping off leading slashes doesn't work for\\n        Perforce (because depot paths start with //), so this overrides it to\\n        just return the filename un-molested.\\n        \"\n    return filename\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/item/shared_item_chair_metal_s1.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef post_undelete(self, *args, **kwargs):\n    self.post_undelete_called = True\n", "label": 0}
{"function": "\n\ndef timecolon(data):\n    match = re.search('(\\\\d+:\\\\d+:\\\\d+):(\\\\d+)', data)\n    return ('%s,%s' % (match.group(1), match.group(2)))\n", "label": 0}
{"function": "\n\ndef package_private_devel_path(self, package):\n    'The path to the linked devel space for a given package.'\n    return os.path.join(self.private_devel_path, package.name)\n", "label": 0}
{"function": "\n\ndef test_getset_owner(self):\n    m = meta.Metadata()\n    o = m.get_owner('files/one')\n    m.set_owner('files/one', *o)\n", "label": 0}
{"function": "\n\ndef indent(string, prefix='    '):\n    '\\n    Indent every line of this string.\\n    '\n    return ''.join((('%s%s\\n' % (prefix, s)) for s in string.split('\\n')))\n", "label": 0}
{"function": "\n\ndef stories(self, task, params={\n    \n}, **options):\n    'Returns a compact representation of all of the stories on the task.\\n\\n        Parameters\\n        ----------\\n        task : {Id} The task containing the stories to get.\\n        [params] : {Object} Parameters for the request\\n        '\n    path = ('/tasks/%s/stories' % task)\n    return self.client.get_collection(path, params, **options)\n", "label": 0}
{"function": "\n\ndef get_context_types_value(context_id, source, filter_func, codebase):\n    query = CodeElementLink.objects.filter(code_element__kind__is_type=True).filter(index=0).filter(code_element__codebase=codebase).filter(code_reference__source=source)\n    query = filter_func(query, context_id)\n    context_types = []\n    pk_set = set()\n    for link in query.all():\n        code_element = link.code_element\n        if (code_element.pk not in pk_set):\n            context_types.append(code_element)\n            pk_set.add(code_element.pk)\n    return context_types\n", "label": 0}
{"function": "\n\ndef test_list(self):\n    lists = {('l%s' % i): list(string.ascii_lowercase[:i]) for i in range(1, 10)}\n    self.collections_common_tests(lists, 'l')\n", "label": 0}
{"function": "\n\ndef fit_transform(self, X, y):\n    'Fit and transform.'\n    self.fit(X, y)\n    return self.transform(X)\n", "label": 0}
{"function": "\n\ndef update_app(self, app_id, app, force=False, minimal=True):\n    'Update an app.\\n\\n        Applies writable settings in `app` to `app_id`\\n        Note: this method can not be used to rename apps.\\n\\n        :param str app_id: target application ID\\n        :param app: application settings\\n        :type app: :class:`marathon.models.app.MarathonApp`\\n        :param bool force: apply even if a deployment is in progress\\n        :param bool minimal: ignore nulls and empty collections\\n\\n        :returns: a dict containing the deployment id and version\\n        :rtype: dict\\n        '\n    app.version = None\n    params = {\n        'force': force,\n    }\n    data = app.to_json(minimal=minimal)\n    response = self._do_request('PUT', '/v2/apps/{app_id}'.format(app_id=app_id), params=params, data=data)\n    return response.json()\n", "label": 0}
{"function": "\n\ndef create_toplevel_ws(self, wsname, width, height, group=2, x=None, y=None):\n    root = self.app.make_window()\n    ws = self.make_ws(wsname, wstype='tabs')\n    vbox = Widgets.VBox()\n    vbox.set_border_width(0)\n    self._add_toolbar(vbox, ws)\n    vbox.add_widget(bnch.widget)\n    root.set_widget(vbox)\n    root.resize(width, height)\n    root.show()\n    self.toplevels.append(root)\n    if (x is not None):\n        root.move(x, y)\n    return bnch\n", "label": 0}
{"function": "\n\ndef get_queryset(self):\n    queryset = self.queryset\n    if isinstance(queryset, (QuerySet, Manager)):\n        queryset = queryset.all()\n    return queryset\n", "label": 0}
{"function": "\n\ndef get_current_editor(self):\n    page = self.notebook.get_current_page()\n    if (page is None):\n        return None\n    return page.get_text_widget()\n", "label": 0}
{"function": "\n\ndef start(self, fileStore):\n    subprocess.check_call((self.cmd + ' 1'), shell=True)\n", "label": 0}
{"function": "\n\ndef check_write_package(self, username, package_reference):\n    '\\n        username: User that request to write the package\\n        package_reference: PackageReference\\n        '\n    self.check_write_conan(username, package_reference.conan)\n", "label": 0}
{"function": "\n\ndef test_it_knows_how_many_total_errors_it_contains(self):\n    errors = [mock.MagicMock() for _ in range(8)]\n    tree = exceptions.ErrorTree(errors)\n    self.assertEqual(tree.total_errors, 8)\n", "label": 0}
{"function": "\n\ndef test_get_lock_multiple_coords(self):\n    member_id2 = self._get_random_uuid()\n    client2 = tooz.coordination.get_coordinator(self.url, member_id2)\n    client2.start()\n    lock_name = self._get_random_uuid()\n    lock = self._coord.get_lock(lock_name)\n    self.assertTrue(lock.acquire())\n    lock2 = client2.get_lock(lock_name)\n    self.assertFalse(lock2.acquire(blocking=False))\n    self.assertTrue(lock.release())\n    self.assertTrue(lock2.acquire(blocking=True))\n    self.assertTrue(lock2.release())\n", "label": 0}
{"function": "\n\ndef test_keys(self):\n    getkeys = self.ts.keys\n    self.assertIs(getkeys(), self.ts.index)\n", "label": 0}
{"function": "\n\ndef user_add_stage(request):\n    if (not request.user.has_perm('auth.change_user')):\n        raise PermissionDenied\n    manipulator = UserCreationForm()\n    if (request.method == 'POST'):\n        new_data = request.POST.copy()\n        errors = manipulator.get_validation_errors(new_data)\n        if (not errors):\n            new_user = manipulator.save(new_data)\n            msg = (_('The %(name)s \"%(obj)s\" was added successfully.') % {\n                'name': 'user',\n                'obj': new_user,\n            })\n            if request.POST.has_key('_addanother'):\n                request.user.message_set.create(message=msg)\n                return HttpResponseRedirect(request.path)\n            else:\n                request.user.message_set.create(message=((msg + ' ') + _('You may edit it again below.')))\n                return HttpResponseRedirect(('../%s/' % new_user.id))\n    else:\n        errors = new_data = {\n            \n        }\n    form = oldforms.FormWrapper(manipulator, new_data, errors)\n    return render_to_response('admin/auth/user/add_form.html', {\n        'title': _('Add user'),\n        'form': form,\n        'is_popup': request.REQUEST.has_key('_popup'),\n        'add': True,\n        'change': False,\n        'has_delete_permission': False,\n        'has_change_permission': True,\n        'has_file_field': False,\n        'has_absolute_url': False,\n        'auto_populated_fields': (),\n        'bound_field_sets': (),\n        'first_form_field_id': 'id_username',\n        'opts': User._meta,\n        'username_help_text': User._meta.get_field('username').help_text,\n    }, context_instance=template.RequestContext(request))\n", "label": 0}
{"function": "\n\ndef generateDictOperationInCode(to_name, expression, emit, context):\n    inverted = expression.isExpressionDictOperationNOTIn()\n    (dict_name, key_name) = generateChildExpressionsCode(expression=expression, emit=emit, context=context)\n    res_name = context.getIntResName()\n    emit(('%s = PyDict_Contains( %s, %s );' % (res_name, key_name, dict_name)))\n    getReleaseCodes(release_names=(dict_name, key_name), emit=emit, context=context)\n    getErrorExitBoolCode(condition=('%s == -1' % res_name), needs_check=expression.mayRaiseException(BaseException), emit=emit, context=context)\n    emit(('%s = BOOL_FROM( %s == %s );' % (to_name, res_name, ('1' if (not inverted) else '0'))))\n", "label": 0}
{"function": "\n\ndef Add(self, node):\n    self.binary('Add', node)\n", "label": 0}
{"function": "\n\ndef random_orthogonal(dim, special=True):\n    if (dim == 1):\n        if (np.random.uniform() < 0.5):\n            return np.ones((1, 1))\n        return (- np.ones((1, 1)))\n    P = np.random.randn(dim, dim)\n    while (np.linalg.matrix_rank(P) != dim):\n        P = np.random.randn(dim, dim)\n    (U, S, V) = np.linalg.svd(P)\n    P = np.dot(U, V)\n    if special:\n        if (np.linalg.det(P) < 0):\n            P[:, [0, 1]] = P[:, [1, 0]]\n    return P\n", "label": 1}
{"function": "\n\ndef register(self, observer):\n    ' Called when an observer wants to be notified\\n        about project changes\\n\\n        '\n    self._observers.append(observer)\n", "label": 0}
{"function": "\n\n@feature('cxx')\n@after('apply_lib_vars')\ndef apply_defines_cxx(self):\n    'after uselib is set for CXXDEFINES'\n    self.defines = getattr(self, 'defines', [])\n    lst = (self.to_list(self.defines) + self.to_list(self.env['CXXDEFINES']))\n    milst = []\n    for defi in lst:\n        if (not (defi in milst)):\n            milst.append(defi)\n    libs = self.to_list(self.uselib)\n    for l in libs:\n        val = self.env[('CXXDEFINES_' + l)]\n        if val:\n            milst += self.to_list(val)\n    self.env['DEFLINES'] = [('%s %s' % (x[0], Utils.trimquotes('='.join(x[1:])))) for x in [y.split('=') for y in milst]]\n    y = self.env['CXXDEFINES_ST']\n    self.env['_CXXDEFFLAGS'] = [(y % x) for x in milst]\n", "label": 1}
{"function": "\n\ndef __iadd__(self, other):\n    self.extend(other)\n    return self\n", "label": 0}
{"function": "\n\n@classmethod\ndef __subclasshook__(cls, other_cls):\n    if (cls is Tombola):\n        interface_names = function_names(cls)\n        found_names = set()\n        for a_cls in other_cls.__mro__:\n            found_names |= function_names(a_cls)\n        if (found_names >= interface_names):\n            return True\n    return NotImplemented\n", "label": 0}
{"function": "\n\ndef destroy(self):\n    ' Destroy the dock manager.\\n\\n        This method will free all of the resources held by the dock\\n        manager. The primary dock area and dock items will not be\\n        destroyed. After the method is called, the dock manager is\\n        invalid and should no longer be used.\\n\\n        '\n    for frame in self._dock_frames:\n        if isinstance(frame, QDockContainer):\n            frame.setDockItem(None)\n            frame.setParent(None, Qt.Widget)\n            frame.hide()\n    for frame in self._dock_frames:\n        if isinstance(frame, QDockWindow):\n            frame.setParent(None, Qt.Widget)\n            frame.hide()\n    for item in self._dock_items:\n        item._manager = None\n    self._dock_area.setCentralWidget(None)\n    self._dock_area.setMaximizedWidget(None)\n    del self._dock_area\n    del self._dock_frames\n    del self._dock_items\n    del self._proximity_handler\n    del self._container_monitor\n    del self._overlay\n", "label": 1}
{"function": "\n\ndef test_incr_sample_rate(self):\n    client = statsd.StatsdClient('localhost', 8125, prefix='', sample_rate=0.999)\n    client.incr('buck.counter', 5)\n    self.assertEqual(client._socket.data, b'buck.counter:5|c|@0.999')\n    if (client._socket.data != 'buck.counter:5|c'):\n        self.assertTrue(client._socket.data.endswith(b'|@0.999'))\n", "label": 0}
{"function": "\n\ndef test_delete_group_inuse_process(self):\n    url = ('/v1/groups/' + GID)\n    req = get_request(url, 'DELETE')\n    self.stubs.Set(db, 'keypair_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'securitygroup_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'network_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'process_get_all', fake_not_group_data_exists)\n    res = req.get_response(self.app)\n    self.assertEqual(res.status_code, 409)\n", "label": 0}
{"function": "\n\ndef test_validate_configuration_invalid_disk_type(self):\n    raid_config = json.loads(raid_constants.RAID_CONFIG_INVALID_DISK_TYPE)\n    self.assertRaises(exception.InvalidParameterValue, raid.validate_configuration, raid_config, raid_config_schema=self.schema)\n", "label": 0}
{"function": "\n\ndef test_notification_no_pause(self):\n    self.displayer.notification('message', 10)\n    string = self.mock_stdout.write.call_args[0][0]\n    self.assertTrue(('message' in string))\n", "label": 0}
{"function": "\n\ndef _augmented_orthonormal_cols(x, k):\n    (n, m) = x.shape\n    y = np.empty((n, (m + k)), dtype=x.dtype)\n    y[:, :m] = x\n    for i in range(k):\n        v = np.random.randn(n)\n        if np.iscomplexobj(x):\n            v = (v + (1j * np.random.randn(n)))\n        for j in range((m + i)):\n            u = y[:, j]\n            v -= ((np.dot(v, u.conj()) / np.dot(u, u.conj())) * u)\n        v /= np.sqrt(np.dot(v, v.conj()))\n        y[:, (m + i)] = v\n    return y\n", "label": 0}
{"function": "\n\ndef test_call_and_missing_check_with_obj_list(self):\n\n    def yield_hashes(device, partition, policy, suffixes=None, **kwargs):\n        if ((device == 'dev') and (partition == '9') and (suffixes == ['abc']) and (policy == POLICIES.legacy)):\n            (yield ('/srv/node/dev/objects/9/abc/9d41d8cd98f00b204e9800998ecf0abc', '9d41d8cd98f00b204e9800998ecf0abc', {\n                'ts_data': Timestamp(1380144470.0),\n            }))\n        else:\n            raise Exception(('No match for %r %r %r' % (device, partition, suffixes)))\n    job = {\n        'device': 'dev',\n        'partition': '9',\n        'policy': POLICIES.legacy,\n        'frag_index': 0,\n    }\n    self.sender = ssync_sender.Sender(self.daemon, None, job, ['abc'], ['9d41d8cd98f00b204e9800998ecf0abc'])\n    self.sender.connection = FakeConnection()\n    self.sender.response = FakeResponse(chunk_body=':MISSING_CHECK: START\\r\\n:MISSING_CHECK: END\\r\\n')\n    self.sender.daemon._diskfile_mgr.yield_hashes = yield_hashes\n    self.sender.connect = mock.MagicMock()\n    self.sender.updates = mock.MagicMock()\n    self.sender.disconnect = mock.MagicMock()\n    (success, candidates) = self.sender()\n    self.assertTrue(success)\n    self.assertEqual(candidates, dict([('9d41d8cd98f00b204e9800998ecf0abc', {\n        'ts_data': Timestamp(1380144470.0),\n    })]))\n    self.assertEqual(self.sender.failures, 0)\n", "label": 0}
{"function": "\n\ndef test_queryset_deleted_on(self):\n    'qs delete() sets deleted_on to same time as parent on cascade.'\n    p = self.F.ProductFactory.create()\n    s = self.F.SuiteFactory.create(product=p)\n    self.model.Product.objects.all().delete()\n    p = self.refresh(p)\n    s = self.refresh(s)\n    self.assertIsNot(p.deleted_on, None)\n    self.assertEqual(s.deleted_on, p.deleted_on)\n", "label": 0}
{"function": "\n\ndef __init__(self, status):\n    super(RCException, self).__init__(('RAMCloud error ' + str(status)))\n    self.status = status\n", "label": 0}
{"function": "\n\ndef from_text(cls, rdclass, rdtype, tok, origin=None, relativize=True):\n    address = tok.get_string()\n    tok.get_eol()\n    return cls(rdclass, rdtype, address)\n", "label": 0}
{"function": "\n\ndef update_parser_common(self, parser):\n    parser.add_argument('network', metavar='<network>', nargs='+', help='Network(s) to delete (name or ID)')\n    return parser\n", "label": 0}
{"function": "\n\n@process_multiple\ndef to_html(self, values, fields, context):\n    toks = []\n    for value in values:\n        if (value in self.html_map):\n            tok = self.html_map[value]\n        elif (value is None):\n            continue\n        elif (type(value) is float):\n            tok = filters.floatformat(value)\n        else:\n            tok = unicode(value)\n        toks.append(tok)\n    return self.delimiter.join(toks)\n", "label": 0}
{"function": "\n\ndef test_dependency_sorting_4(self):\n    sorted_deps = sort_dependencies([('fixtures_regress', [Store, Person, Book])])\n    self.assertEqual(sorted_deps, [Store, Person, Book])\n", "label": 0}
{"function": "\n\ndef _eval_rewrite_as_gamma(self, x, k):\n    from sympy import gamma\n    return ((((- 1) ** k) * gamma((k - x))) / gamma((- x)))\n", "label": 0}
{"function": "\n\ndef _get_x(self):\n    if (len(self.names) > 1):\n        return ([self.__getattribute__(name) for name in self.names] + list(self.args))\n    return ([self.__getattribute__(self.names[0])] + list(self.args))\n", "label": 0}
{"function": "\n\ndef turn_off(self, **kwargs):\n    'Turn the device off/open the device.'\n    self.action_node.runElse()\n", "label": 0}
{"function": "\n\ndef test_set_rewrite(self):\n    '`LocalMemStorage` set method of existing key'\n    s = LocalMemStorage()\n    s.set('key', 'value')\n    s.set('key', 'value1')\n    self.assertEqual(s.storage['key'], 'value1')\n", "label": 0}
{"function": "\n\ndef do_create(self, max_size=0, dir=None, pre='', suf=''):\n    if (dir is None):\n        dir = tempfile.gettempdir()\n    file = tempfile.SpooledTemporaryFile(max_size=max_size, dir=dir, prefix=pre, suffix=suf)\n    return file\n", "label": 0}
{"function": "\n\ndef mapToJson(self, objects, writer):\n    writer.write(self.header)\n    writer.write('\\n')\n    for (ind, obj) in enumerate(objects):\n        if (ind > 0):\n            writer.write(',\\n')\n        else:\n            writer.write('\\n')\n        writer.write(self.jsonDumpser(self.objConverter(obj)))\n    writer.write(self.footer)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (self is other):\n        return True\n    if (not isinstance(other, collections.Sequence)):\n        raise TypeError('Can only compare repeated scalar fields against sequences.')\n    return (other == self[slice(None, None, None)])\n", "label": 0}
{"function": "\n\ndef RemoveMenu(self, menu):\n    ' Remove a wx menu from the Menu.\\n\\n        If the menu does not exist in the menu, this is a no-op.\\n\\n        Parameters\\n        ----------\\n        menu : wxMenu\\n            The wxMenu instance to remove from this menu.\\n\\n        '\n    all_items = self._all_items\n    if (menu in all_items):\n        all_items.remove(menu)\n        menu.Unbind(EVT_MENU_CHANGED, handler=self.OnMenuChanged)\n        menu_item = self._menus_map.pop(menu, None)\n        if (menu_item is not None):\n            self.RemoveItem(menu_item)\n            menu_item.SetSubMenu(None)\n", "label": 0}
{"function": "\n\ndef get_latest_dist():\n    lib = file(os.path.join('petlib', '__init__.py')).read()\n    v = re.findall('VERSION.*=.*[\\'\"](.*)[\\'\"]', lib)[0]\n    return os.path.join('dist', ('petlib-%s.tar.gz' % v))\n", "label": 0}
{"function": "\n\ndef dump(self):\n    out = []\n    for key in self._keys:\n        att_key = self._att_key(key)\n        value = self[att_key]\n        if hasattr(self, ('dump_%s' % att_key)):\n            value = getattr(self, ('dump_%s' % att_key))(value)\n        out.append(('%s: %s' % (key, value)))\n    return '\\n'.join(out)\n", "label": 0}
{"function": "\n\ndef get(self, request):\n    form = bforms.PasswordResetForm()\n    self.payload['form'] = form\n    return render(request, self.payload, 'registration/reset_password.html')\n", "label": 0}
{"function": "\n\ndef test_default_theme_is_empty(self):\n    doc = Document()\n    for (class_name, props) in doc.theme._json['attrs'].items():\n        self._compare_dict_to_model_defaults(props, class_name)\n    self.assertEqual(0, len(doc.theme._json['attrs']))\n    self._compare_dict_to_model_class_defaults(doc.theme._fill_defaults, FillProps)\n    self.assertEqual(0, len(doc.theme._fill_defaults))\n    self._compare_dict_to_model_class_defaults(doc.theme._text_defaults, TextProps)\n    self.assertEqual(0, len(doc.theme._text_defaults))\n    self._compare_dict_to_model_class_defaults(doc.theme._line_defaults, LineProps)\n    self.assertEqual(0, len(doc.theme._line_defaults))\n", "label": 0}
{"function": "\n\ndef default(self, obj):\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    return super(_JSONEncoder, self).default(obj)\n", "label": 0}
{"function": "\n\ndef __init__(self, r, color4):\n    super(ProbeQuad, self).__init__()\n    self.color4 = color4\n    self.vertexes = [(r, 0, 0), (0, r, 0), ((- r), 0, 0), (0, (- r), 0)]\n", "label": 0}
{"function": "\n\ndef format(self, value):\n    if isinstance(value, types.StringTypes):\n        return value\n    else:\n        return str(value)\n", "label": 0}
{"function": "\n\ndef find_item_before(menu, index=0):\n    _items = menu['menu'][:index][:]\n    _items.reverse()\n    for item in _items:\n        if item['enabled']:\n            return menu['menu'].index(item)\n    return find_item_before(menu, index=len(menu['menu']))\n", "label": 0}
{"function": "\n\ndef onWindowResized(self, width, height):\n    shortcutHeight = ((height - self.shortcuts.getAbsoluteTop()) - 8)\n    if (shortcutHeight < 1):\n        shortcutHeight = 1\n    self.shortcuts.setHeight(('%dpx' % shortcutHeight))\n    self.mailDetail.adjustSize(width, height)\n", "label": 0}
{"function": "\n\ndef test_ex_get_node_security_groups(self):\n    node = Node(id='1c01300f-ef97-4937-8f03-ac676d6234be', name=None, state=None, public_ips=None, private_ips=None, driver=self.driver)\n    security_groups = self.driver.ex_get_node_security_groups(node)\n    self.assertEqual(len(security_groups), 2, 'Wrong security groups count')\n    security_group = security_groups[1]\n    self.assertEqual(security_group.id, 4)\n    self.assertEqual(security_group.tenant_id, '68')\n    self.assertEqual(security_group.name, 'ftp')\n    self.assertEqual(security_group.description, 'FTP Client-Server - Open 20-21 ports')\n    self.assertEqual(security_group.rules[0].id, 1)\n    self.assertEqual(security_group.rules[0].parent_group_id, 4)\n    self.assertEqual(security_group.rules[0].ip_protocol, 'tcp')\n    self.assertEqual(security_group.rules[0].from_port, 20)\n    self.assertEqual(security_group.rules[0].to_port, 21)\n    self.assertEqual(security_group.rules[0].ip_range, '0.0.0.0/0')\n", "label": 0}
{"function": "\n\ndef test_call_chooses_correct_handler(self):\n    (sentinel1, sentinel2, sentinel3) = (object(), object(), object())\n    self.commands.add('foo')((lambda context: sentinel1))\n    self.commands.add('bar')((lambda context: sentinel2))\n    self.commands.add('baz')((lambda context: sentinel3))\n    self.assertEqual(sentinel1, self.commands.call(['foo']))\n    self.assertEqual(sentinel2, self.commands.call(['bar']))\n    self.assertEqual(sentinel3, self.commands.call(['baz']))\n", "label": 0}
{"function": "\n\ndef apply_linear(self, params, unknowns, dparams, dunknowns, dresids, mode):\n    \"\\n        Multiplies incoming vector by the Jacobian (fwd mode) or the\\n        transpose Jacobian (rev mode). If the user doesn't provide this\\n        method, then we just multiply by the cached jacobian.\\n\\n        Args\\n        ----\\n        params : `VecWrapper`\\n            `VecWrapper` containing parameters. (p)\\n\\n        unknowns : `VecWrapper`\\n            `VecWrapper` containing outputs and states. (u)\\n\\n        dparams : `VecWrapper`\\n            `VecWrapper` containing either the incoming vector in forward mode\\n            or the outgoing result in reverse mode. (dp)\\n\\n        dunknowns : `VecWrapper`\\n            In forward mode, this `VecWrapper` contains the incoming vector for\\n            the states. In reverse mode, it contains the outgoing vector for\\n            the states. (du)\\n\\n        dresids : `VecWrapper`\\n            `VecWrapper` containing either the outgoing result in forward mode\\n            or the incoming vector in reverse mode. (dr)\\n\\n        mode : string\\n            Derivative mode, can be 'fwd' or 'rev'.\\n        \"\n    self._apply_linear_jac(params, unknowns, dparams, dunknowns, dresids, mode)\n", "label": 0}
{"function": "\n\ndef test_context(self):\n    order = Order(name='Dummy Order')\n    order.save()\n    for i in range(10):\n        item = Item(name=('Item %i' % i), sku=(str(i) * 13), price=D('9.99'), order=order, status=0)\n        item.save()\n    res = self.client.get('/modelformset/simple/')\n    self.assertTrue(('object_list' in res.context))\n    self.assertEqual(len(res.context['object_list']), 10)\n", "label": 0}
{"function": "\n\ndef read_channel(model, channel_name, monitor_name='monitor'):\n    '\\n    Returns the last value recorded in a channel.\\n\\n    Parameters\\n    ----------\\n    model : Model\\n        The model to read the channel from\\n    channel_name : str\\n        The name of the channel to read from\\n    monitor_name : str, optional\\n        The name of the Monitor to read from\\n        (In case you want to read from an old Monitor moved by\\n        `push_monitor`)\\n\\n    Returns\\n    -------\\n    value : float\\n        The last value recorded in this monitoring channel\\n    '\n    return getattr(model, monitor_name).channels[channel_name].val_record[(- 1)]\n", "label": 0}
{"function": "\n\ndef set_h_ffactor(self, *args, **kwargs):\n    return apply(self._cobj.set_h_ffactor, args, kwargs)\n", "label": 0}
{"function": "\n\ndef init_stroke(self, g, touch):\n    l = [touch.x, touch.y]\n    col = g.color\n    new_line = Line(points=l, width=self.line_width, group=g.id)\n    g._strokes[str(touch.uid)] = new_line\n    if self.line_width:\n        canvas_add = self.canvas.add\n        canvas_add(Color(col[0], col[1], col[2], mode='rgb', group=g.id))\n        canvas_add(new_line)\n    g.update_bbox(touch)\n    if self.draw_bbox:\n        self._update_canvas_bbox(g)\n    g.add_stroke(touch, new_line)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/item/shared_item_music_microphone_s2.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef autodiscover():\n    'Auto-discover INSTALLED_APPS mails.py modules.'\n    for app in settings.INSTALLED_APPS:\n        module = ('%s.mails' % app)\n        try:\n            import_module(module)\n        except:\n            app_module = import_module(app)\n            if module_has_submodule(app_module, 'mails'):\n                raise\n", "label": 0}
{"function": "\n\ndef setup_basic_delete_test(self, user, with_local_site, local_site_name):\n    review_request = self.create_review_request(with_local_site=with_local_site, publish=True)\n    profile = user.get_profile()\n    profile.starred_review_requests.add(review_request)\n    return (get_watched_review_request_item_url(user.username, review_request.display_id, local_site_name), [profile, review_request])\n", "label": 0}
{"function": "\n\ndef unregister_module(self, module):\n    if (module not in self.modules):\n        raise NotRegistered(('The module %s is not registered' % module.__name__))\n    del self.modules[module]\n", "label": 0}
{"function": "\n\ndef sh(cmdline, stdout=subprocess.PIPE, stderr=subprocess.PIPE):\n    'run cmd in a subprocess and return its output.\\n    raises RuntimeError on error.\\n    '\n    p = subprocess.Popen(cmdline, shell=True, stdout=stdout, stderr=stderr)\n    (stdout, stderr) = p.communicate()\n    if (p.returncode != 0):\n        raise RuntimeError(stderr)\n    if stderr:\n        warn(stderr)\n    if PY3:\n        stdout = str(stdout, sys.stdout.encoding)\n    return stdout.strip()\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.content_type == other.content_type) and (_join_b(self.iter_bytes()) == _join_b(other.iter_bytes())))\n", "label": 0}
{"function": "\n\ndef _libname(self, libpath):\n    \"Converts a full library filepath to the library's name.\\n    Ex: /path/to/libhello.a --> hello\\n    \"\n    return os.path.basename(libpath)[3:(- 2)]\n", "label": 0}
{"function": "\n\ndef description(self, around=False):\n    if around:\n        return 'Expand Selection to Quotes'\n    else:\n        return 'Expand Selection to Quoted'\n", "label": 0}
{"function": "\n\ndef get_form(self, request, obj=None, **kwargs):\n    _thread_locals.request = request\n    _thread_locals.obj = obj\n    return super(XOSAdminMixin, self).get_form(request, obj, **kwargs)\n", "label": 0}
{"function": "\n\ndef input(self, data):\n    if ('type' in data):\n        function_name = ('process_' + data['type'])\n        self._dbg('got {}'.format(function_name))\n        for plugin in self.bot_plugins:\n            plugin.register_jobs()\n            plugin.do(function_name, data)\n", "label": 0}
{"function": "\n\ndef db_exists(database_name, **kwargs):\n    \"\\n    Find if a specific database exists on the MS SQL server.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt minion mssql.db_exists database_name='DBNAME'\\n    \"\n    return (len(tsql_query(\"SELECT database_id FROM sys.databases WHERE NAME='{0}'\".format(database_name), **kwargs)) == 1)\n", "label": 0}
{"function": "\n\ndef test_sort_mapping_reverse(self):\n    stream = DataStream(IterableDataset(self.data))\n    transformer = Mapping(stream, SortMapping(operator.itemgetter(0), reverse=True))\n    assert_equal(list(transformer.get_epoch_iterator()), list(zip(([[3, 2, 1]] * 3))))\n", "label": 0}
{"function": "\n\ndef load_train_data(self, input_data_file=''):\n    '\\n        Load train data\\n        Please check dataset/logistic_regression_train.dat to understand the data format\\n        Each feature of data x separated with spaces\\n        And the ground truth y put in the end of line separated by a space\\n        '\n    self.status = 'load_train_data'\n    if (input_data_file == ''):\n        input_data_file = os.path.normpath(os.path.join(os.path.join(os.getcwd(), os.path.dirname(__file__)), 'dataset/logistic_regression_train.dat'))\n    elif (os.path.isfile(input_data_file) is not True):\n        print('Please make sure input_data_file path is correct.')\n        return (self.train_X, self.train_Y)\n    (self.train_X, self.train_Y) = utility.DatasetLoader.load(input_data_file)\n    return (self.train_X, self.train_Y)\n", "label": 0}
{"function": "\n\ndef register(self, parent, key, create_only=False, **kwargs):\n    '\\n        Add/replace an entry within directory, below a parent node or \"/\".\\n        Note: Replaces (not merges) the attribute values of the entry if existing\\n        @param create_only  If True, does not change an existing entry\\n        @retval  DirEntry if previously existing\\n        '\n    if (not (parent and key)):\n        raise BadRequest('Illegal arguments')\n    if ((not (type(parent) is str)) or (not parent.startswith('/'))):\n        raise BadRequest('Illegal arguments: parent')\n    dn = self._get_path(parent, key)\n    log.debug('Directory.register(%s): %s', dn, kwargs)\n    entry_old = None\n    cur_time = get_ion_ts()\n    direntry = self._read_by_path(dn)\n    if (direntry and create_only):\n        return direntry\n    elif direntry:\n        entry_old = direntry.attributes\n        direntry.attributes = kwargs\n        direntry.ts_updated = cur_time\n        self.dir_store.update(direntry)\n    else:\n        direntry = self._create_dir_entry(parent, key, attributes=kwargs, ts=cur_time)\n        self._ensure_parents_exist([direntry])\n        self.dir_store.create(direntry, create_unique_directory_id())\n    return entry_old\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    s = ('%s' % self._name)\n    if self._location:\n        s = ('%s@%s' % (s, self._location))\n    return s\n", "label": 0}
{"function": "\n\ndef test_add(self):\n    'Test that we can add an image via the s3 backend'\n    expected_image_id = utils.generate_uuid()\n    expected_s3_size = FIVE_KB\n    expected_s3_contents = ('*' * expected_s3_size)\n    expected_checksum = hashlib.md5(expected_s3_contents).hexdigest()\n    expected_location = format_s3_location(S3_CONF['s3_store_access_key'], S3_CONF['s3_store_secret_key'], S3_CONF['s3_store_host'], S3_CONF['s3_store_bucket'], expected_image_id)\n    image_s3 = StringIO.StringIO(expected_s3_contents)\n    (location, size, checksum) = self.store.add(expected_image_id, image_s3, expected_s3_size)\n    self.assertEquals(expected_location, location)\n    self.assertEquals(expected_s3_size, size)\n    self.assertEquals(expected_checksum, checksum)\n    loc = get_location_from_uri(expected_location)\n    (new_image_s3, new_image_size) = self.store.get(loc)\n    new_image_contents = StringIO.StringIO()\n    for chunk in new_image_s3:\n        new_image_contents.write(chunk)\n    new_image_s3_size = new_image_contents.len\n    self.assertEquals(expected_s3_contents, new_image_contents.getvalue())\n    self.assertEquals(expected_s3_size, new_image_s3_size)\n", "label": 0}
{"function": "\n\ndef move_cat(self):\n    speed = random.randint(20, 200)\n    self.cat_body.angle -= random.randint((- 1), 1)\n    direction = Vec2d(1, 0).rotated(self.cat_body.angle)\n    self.cat_body.velocity = (speed * direction)\n", "label": 0}
{"function": "\n\ndef job_status(self, job_id=None):\n    job_id = (job_id or self.lookup_job_id(batch_id))\n    uri = urlparse.urljoin((self.endpoint + '/'), 'job/{0}'.format(job_id))\n    response = requests.get(uri, headers=self.headers())\n    if (response.status_code != 200):\n        self.raise_error(response.content, response.status_code)\n    tree = ET.fromstring(response.content)\n    result = {\n        \n    }\n    for child in tree:\n        result[re.sub('{.*?}', '', child.tag)] = child.text\n    return result\n", "label": 0}
{"function": "\n\ndef __init__(self, view=False):\n    self.view = view\n    if self.view:\n        self.view_map = {\n            0: [0],\n        }\n", "label": 0}
{"function": "\n\ndef replace_body(self, body, payload):\n    if (body is None):\n        return body\n    if (self.fsig in body):\n        return body.replace(self.fsig, urllib.quote_plus(payload))\n    template_sig = self.template_signature(body)\n    if template_sig:\n        tp = TemplateParser()\n        tp.set_payload(payload)\n        new_payload = repr(tp.transform(self.template_signature(body), self.sig))[1:(- 1)]\n        return body.replace(template_sig, new_payload)\n    return body\n", "label": 0}
{"function": "\n\ndef delete(self, image):\n    \"\\n        Delete an image.\\n        \\n        It should go without saying that you can't delete an image \\n        that you didn't create.\\n        \\n        :param image: The :class:`Image` (or its ID) to delete.\\n        \"\n    self._delete(('/images/%s' % base.getid(image)))\n", "label": 0}
{"function": "\n\ndef load_parent(parent_id):\n    parent = Node.load(parent_id)\n    if (parent is None):\n        return None\n    parent_info = {\n        \n    }\n    if ((parent is not None) and parent.is_public):\n        parent_info['title'] = parent.title\n        parent_info['url'] = parent.url\n        parent_info['is_registration'] = parent.is_registration\n        parent_info['id'] = parent._id\n    else:\n        parent_info['title'] = '-- private project --'\n        parent_info['url'] = ''\n        parent_info['is_registration'] = None\n        parent_info['id'] = None\n    return parent_info\n", "label": 0}
{"function": "\n\ndef write(self, filename, content):\n    create_dirs(self.webd, dirname(filename))\n    buff = BytesIO(content)\n    self.webd.upload_from(buff, b(filename))\n", "label": 0}
{"function": "\n\ndef test_gtkApplicationActivate(self):\n    '\\n        L{Gtk.Application} instances can be registered with a gtk3reactor.\\n        '\n    reactor = gtk3reactor.Gtk3Reactor()\n    self.addCleanup(self.unbuildReactor, reactor)\n    app = Gtk.Application(application_id='com.twistedmatrix.trial.gtk3reactor', flags=Gio.ApplicationFlags.FLAGS_NONE)\n    self.runReactor(app, reactor)\n", "label": 0}
{"function": "\n\ndef test_patch_mocksignature_callable(self):\n    original_something = something\n    something_name = ('%s.something' % __name__)\n\n    @patch(something_name, mocksignature=True)\n    def test(MockSomething):\n        something(3, 4)\n        MockSomething.assert_called_with(3, 4)\n        something(6)\n        MockSomething.assert_called_with(6, 5)\n        self.assertRaises(TypeError, something)\n    test()\n    self.assertIs(something, original_something)\n", "label": 0}
{"function": "\n\ndef __init__(self, allure_helper, title):\n    self.allure_helper = allure_helper\n    self.title = title\n    self.step = None\n", "label": 0}
{"function": "\n\ndef load_xml_config(self, path=None):\n    if (path is not None):\n        self.path = path\n    if (not os.path.isfile(self.path)):\n        raise KaresasnuiServiceConfigParamException(('service.xml not found. path=%s' % str(self.path)))\n    document = XMLParse(self.path)\n    self.services = []\n    service_num = XMLXpathNum(document, '/services/service')\n    for n in xrange(1, (service_num + 1)):\n        system_name = XMLXpath(document, ('/services/service[%i]/system/name/text()' % n))\n        system_command = XMLXpath(document, ('/services/service[%i]/system/command/text()' % n))\n        system_readonly = XMLXpath(document, ('/services/service[%i]/system/readonly/text()' % n))\n        display_name = XMLXpath(document, ('/services/service[%i]/display/name/text()' % n))\n        display_description = XMLXpath(document, ('/services/service[%i]/display/description/text()' % n))\n        self.add_service(str(system_name), str(system_command), str(system_readonly), str(display_name), str(display_description))\n", "label": 0}
{"function": "\n\ndef _should_create_constraint(self, compiler):\n    return (not compiler.dialect.supports_native_boolean)\n", "label": 0}
{"function": "\n\ndef put_object(self, name, fp, metadata):\n    '\\n        Store object into memory\\n\\n        :param name: standard object name\\n        :param fp: `StringIO` in-memory representation object\\n        :param metadata: dictionary of metadata to be written\\n        '\n    self._filesystem[name] = (fp, metadata)\n", "label": 0}
{"function": "\n\ndef _raise_test_exc(self, exc_msg):\n    raise TestException(exc_msg)\n", "label": 0}
{"function": "\n\ndef install_inplace(pkg):\n    'Install scripts of pkg in the current directory.'\n    for (basename, executable) in pkg.executables.items():\n        version_str = '.'.join([str(i) for i in sys.version_info[:2]])\n        scripts_node = root._ctx.srcnode\n        for name in [basename, ('%s-%s' % (basename, version_str))]:\n            nodes = _create_executable(name, executable, scripts_node)\n            installed = ','.join([n.path_from(scripts_node) for n in nodes])\n            pprint('GREEN', ('installing %s in current directory' % installed))\n", "label": 0}
{"function": "\n\ndef testSuccess(self):\n    vor = rapi.testutils.VerifyOpResult\n    vor(opcodes.OpClusterVerify.OP_ID, {\n        constants.JOB_IDS_KEY: [(False, 'error message')],\n    })\n", "label": 0}
{"function": "\n\ndef kilobyte(self, value=None):\n    return self.convertb(value, self.byte)\n", "label": 0}
{"function": "\n\ndef _irfft_out_chunks(a, n, axis):\n    if (n is None):\n        n = (2 * (a.chunks[axis][0] - 1))\n    chunks = list(a.chunks)\n    chunks[axis] = (n,)\n    return chunks\n", "label": 0}
{"function": "\n\n@skipUnless(cache_file_exists('ia', '20100608__ia__primary__adair__precinct.xls'), CACHED_FILE_MISSING_MSG)\ndef test_results(self):\n    filename = '20100608__ia__primary__adair__precinct.xls'\n    mapping = self._get_mapping(filename)\n    self._prep_loader_attrs(mapping)\n    results = self.loader._results(mapping)\n    us_rep_dist_5_rep_results = [r for r in results if ((r.office == 'U.S. REPRESENTATIVE') and (r.district == '5') and (r.primary_party == 'REPUBLICAN'))]\n    self.assertEqual(len(us_rep_dist_5_rep_results), 35)\n    result = us_rep_dist_5_rep_results[0]\n    self.assertEqual(result.source, mapping['generated_filename'])\n    self.assertEqual(result.election_id, mapping['election'])\n    self.assertEqual(result.state, 'IA')\n    self.assertEqual(result.election_type, 'primary')\n    self.assertEqual(result.district, '5')\n    self.assertEqual(result.party, 'REPUBLICAN')\n    self.assertEqual(result.jurisdiction, '1 NW')\n    self.assertEqual(result.reporting_level, 'precinct')\n    self.assertEqual(result.full_name, 'STEVE KING')\n    self.assertEqual(result.votes, 123)\n", "label": 0}
{"function": "\n\ndef get_context_data(self, **kwargs):\n    ctx = super(AddressCreateView, self).get_context_data(**kwargs)\n    ctx['title'] = _('Add a new address')\n    return ctx\n", "label": 0}
{"function": "\n\ndef find_by_selector(self, selector, search_regions=None):\n    search_regions = (search_regions or self.regions)\n    return GrammarParser.filter_by_selector(selector, search_regions)\n", "label": 0}
{"function": "\n\ndef nova(context):\n    global _nova_api_version\n    if (not _nova_api_version):\n        _nova_api_version = _get_nova_api_version(context)\n    clnt = novaclient.Client(_nova_api_version, session=context.session, service_type=CONF.nova_service_type)\n    if (not hasattr(clnt.client, 'last_request_id')):\n        setattr(clnt.client, 'last_request_id', None)\n    return clnt\n", "label": 0}
{"function": "\n\ndef iteritems(self):\n    for tag in self.tags:\n        (yield (tag.name, tag))\n", "label": 0}
{"function": "\n\ndef split_multiline(value):\n    value = [element for element in (line.strip() for line in value.split('\\n')) if element]\n    return value\n", "label": 0}
{"function": "\n\ndef get_tags_count(journal):\n    'Returns a set of tuples (count, tag) for all tags present in the journal.'\n    tags = [tag for entry in journal.entries for tag in set(entry.tags)]\n    tag_counts = set([(tags.count(tag), tag) for tag in tags])\n    return tag_counts\n", "label": 0}
{"function": "\n\ndef vmstats():\n    \"\\n    Return the virtual memory stats for this minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' status.vmstats\\n    \"\n\n    def linux_vmstats():\n        '\\n        linux specific implementation of vmstats\\n        '\n        procf = '/proc/vmstat'\n        if (not os.path.isfile(procf)):\n            return {\n                \n            }\n        stats = salt.utils.fopen(procf, 'r').read().splitlines()\n        ret = {\n            \n        }\n        for line in stats:\n            if (not line):\n                continue\n            comps = line.split()\n            ret[comps[0]] = _number(comps[1])\n        return ret\n\n    def freebsd_vmstats():\n        '\\n        freebsd specific implementation of vmstats\\n        '\n        ret = {\n            \n        }\n        for line in __salt__['cmd.run']('vmstat -s').splitlines():\n            comps = line.split()\n            if comps[0].isdigit():\n                ret[' '.join(comps[1:])] = _number(comps[0])\n        return ret\n    get_version = {\n        'Linux': linux_vmstats,\n        'FreeBSD': freebsd_vmstats,\n    }\n    errmsg = 'This method is unsupported on the current operating system!'\n    return get_version.get(__grains__['kernel'], (lambda : errmsg))()\n", "label": 0}
{"function": "\n\ndef render(self, name, value, attrs=None):\n    value = util.serialize_references(value)\n    return super(ReferencesFieldWidget, self).render(name, value, attrs)\n", "label": 0}
{"function": "\n\ndef inline_assets(self, base_path, content):\n    for type in self.asset_types:\n        for (statement, path) in self.get_matches(type['pattern'], base_path, content):\n            asset_content = self.get_binary_file_contents(path)\n            encoded_content = urllib.quote(base64.encodestring(asset_content))\n            new_statement = ('url(data:%s;base64,%s)' % (type['mime'], encoded_content))\n            content = content.replace(statement, new_statement)\n    return content\n", "label": 0}
{"function": "\n\ndef test_choice_update(self):\n    self.choice.choice_text = 'third text'\n    self.choice.save()\n    p = Choice.objects.get()\n    self.assertEqual(p.choice_text, 'third text')\n", "label": 0}
{"function": "\n\ndef abort_run(self, drain=False):\n    self._aborting_run = drain\n", "label": 0}
{"function": "\n\ndef get_oauth_request(request):\n    ' Converts a Django request object into an `oauth2.Request` object. '\n    headers = {\n        \n    }\n    if ('HTTP_AUTHORIZATION' in request.META):\n        headers['Authorization'] = request.META['HTTP_AUTHORIZATION']\n    return oauth.Request.from_request(request.method, request.build_absolute_uri(request.path), headers, dict(request.REQUEST))\n", "label": 0}
{"function": "\n\ndef close_review_request(server_url, username, password, review_request_id, description):\n    'Closes the specified review request as submitted.'\n    (api_client, api_root) = get_api(server_url, username, password)\n    review_request = get_review_request(review_request_id, api_root)\n    if (review_request.status == SUBMITTED):\n        logging.warning('Review request #%s is already %s.', review_request_id, SUBMITTED)\n        return\n    if description:\n        review_request = review_request.update(status=SUBMITTED, description=description)\n    else:\n        review_request = review_request.update(status=SUBMITTED)\n    print(('Review request #%s is set to %s.' % (review_request_id, review_request.status)))\n", "label": 0}
{"function": "\n\ndef GetLastRequestTimedelta(api_query, from_time=None):\n    'Returns how long since the API Query response was last requested.\\n\\n  Args:\\n    api_query: The API Query from which to retrieve the last request timedelta.\\n    from_time: A DateTime object representing the start time to calculate the\\n               timedelta from.\\n\\n  Returns:\\n    A string that describes how long since the API Query response was last\\n    requested in the form of \"HH hours, MM minutes, ss seconds ago\" or None\\n    if the API Query response has never been requested.\\n  '\n    if (not from_time):\n        from_time = datetime.utcnow()\n    if api_query.last_request:\n        time_delta = (from_time - api_query.last_request)\n        return FormatTimedelta(time_delta)\n    return None\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/particle/shared_particle_test_16.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef __init__(self, name):\n    Exception.__init__(self, (\"Method not found: '%s'\" % name))\n", "label": 0}
{"function": "\n\ndef testUnshareSecondLevelRemoved(self):\n    'Re-share photos, remove the reshared viewpoint, then unshare the source viewpoint.'\n    (child_vp_id, child_ep_ids) = self._tester.ShareNew(self._cookie2, [(self._new_ep_id, self._photo_ids)], [self._user3.user_id], **self._CreateViewpointDict(self._cookie2))\n    self._tester.RemoveViewpoint(self._cookie3, child_vp_id)\n    self._tester.Unshare(self._cookie, self._new_vp_id, [(self._new_ep_id, self._photo_ids[:1])])\n", "label": 0}
{"function": "\n\ndef test_search_comment(self):\n    result = self.search(comment=['fantastic'])\n    self.assertEqual(list(result[0].tracks), self.tracks[3:4])\n    result = self.search(comment=['antasti'])\n    self.assertEqual(list(result[0].tracks), self.tracks[3:4])\n", "label": 0}
{"function": "\n\n@converts('ImageField')\ndef conv_Image(self, model, field, kwargs):\n    return f.FileField(**kwargs)\n", "label": 0}
{"function": "\n\ndef parse(self, text):\n    return [ErrorLine(m) for m in self.regex.finditer(text)]\n", "label": 0}
{"function": "\n\ndef clamp_vect(self, v):\n    'Returns a copy of the vector v clamped to the bounding box'\n    return cpffi.cpBBClampVect(self._bb, v)\n", "label": 0}
{"function": "\n\ndef __exit__(self, *args):\n    self.delegate.disconnect()\n", "label": 0}
{"function": "\n\n@view_config(context='velruse.AuthenticationComplete', renderer='{}:templates/result.mako'.format(__name__))\ndef login_complete_view(request):\n    context = request.context\n    result = {\n        'profile': context.profile,\n        'credentials': context.credentials,\n    }\n    return {\n        'result': json.dumps(result, indent=4),\n    }\n", "label": 0}
{"function": "\n\ndef emit(self, *args, **kwargs):\n    try:\n        self.__emitting = True\n        for handler in self.__handlers:\n            handler(*args, **kwargs)\n    finally:\n        self.__emitting = False\n        self.__applyChanges()\n", "label": 0}
{"function": "\n\ndef child_removed(self, child):\n    ' Handle the child removed event for a QtWindow.\\n\\n        '\n    if isinstance(child, WxContainer):\n        self.widget.SetCentralWidget(self.central_widget())\n", "label": 0}
{"function": "\n\ndef test_stubs(self):\n    df = pd.DataFrame([[0, 1, 2, 3, 8], [4, 5, 6, 7, 9]])\n    df.columns = ['id', 'inc1', 'inc2', 'edu1', 'edu2']\n    stubs = ['inc', 'edu']\n    df_long = pd.wide_to_long(df, stubs, i='id', j='age')\n    self.assertEqual(stubs, ['inc', 'edu'])\n", "label": 0}
{"function": "\n\ndef collectstreamuuid(self, streamname):\n    if (not streamname):\n        return\n    shouter.shout(('Get UUID of configured stream ' + streamname))\n    showuuidcommand = ('%s --show-alias n --show-uuid y show attributes -r %s -w %s' % (self.scmcommand, self.repo, streamname))\n    output = shell.getoutput(showuuidcommand)\n    splittedfirstline = output[0].split(' ')\n    streamuuid = splittedfirstline[0].strip()[1:(- 1)]\n    return streamuuid\n", "label": 0}
{"function": "\n\n@classmethod\ndef resource_uri(cls, obj=None):\n    object_id = 'id'\n    if (obj is not None):\n        object_id = obj.id\n    return ('api_events', [object_id])\n", "label": 0}
{"function": "\n\ndef __init__(self, text):\n    if (len(text) >= 10000):\n        text = (text[:9995] + '\\n...')\n    self.text = text\n    self.recipient = None\n", "label": 0}
{"function": "\n\ndef info(self, msg_format, *values):\n    'For progress and other informative messages.'\n    if (len(values) > 0):\n        msg_format = (msg_format % values)\n    print(msg_format)\n", "label": 0}
{"function": "\n\ndef get_result(self, vlan_range_len):\n    self.intersect()\n    if (vlan_range_len > 1):\n        return self.get_final_available_vlan_range(vlan_range_len)\n    else:\n        return self.get_final_available_vlan()\n", "label": 0}
{"function": "\n\ndef __init__(self, collector, callback=None, *args, **kw):\n    '\\n        Create a pager with a Reference to a remote collector and\\n        an optional callable to invoke upon completion.\\n        '\n    if callable(callback):\n        self.callback = callback\n        self.callbackArgs = args\n        self.callbackKeyword = kw\n    else:\n        self.callback = None\n    self._stillPaging = 1\n    self.collector = collector\n    collector.broker.registerPageProducer(self)\n", "label": 0}
{"function": "\n\ndef __init__(self, name, consumer_key, consumer_secret):\n    self.name = name\n    self.type = 'twitter'\n    self.consumer_key = consumer_key\n    self.consumer_secret = consumer_secret\n    self.login_route = ('velruse.%s-login' % name)\n    self.callback_route = ('velruse.%s-callback' % name)\n", "label": 0}
{"function": "\n\n@mock.patch(('%s.flavors.osclients.Clients' % CTX))\ndef test_cleanup(self, mock_clients):\n    real_context = {\n        'flavors': {\n            'flavor_name': {\n                'flavor_name': 'flavor_name',\n                'id': 'flavor_name',\n            },\n        },\n        'admin': {\n            'credential': mock.MagicMock(),\n        },\n        'task': mock.MagicMock(),\n    }\n    flavors_ctx = flavors.FlavorsGenerator(real_context)\n    flavors_ctx.cleanup()\n    mock_clients.assert_called_with(real_context['admin']['credential'])\n    mock_flavors_delete = mock_clients().nova().flavors.delete\n    mock_flavors_delete.assert_called_with('flavor_name')\n", "label": 0}
{"function": "\n\ndef get_all_vms(self):\n    '\\n        Returns a generator over all VMs known to this vCenter host.\\n        '\n    for folder in self.get_first_level_of_vm_folders():\n        for vm in get_all_vms_in_folder(folder):\n            (yield vm)\n", "label": 0}
{"function": "\n\ndef wait_for_responses(self, client):\n    'Waits for all responses to come back and resolves the\\n        eventual results.\\n        '\n    assert_open(self)\n    if self.has_pending_requests:\n        raise RuntimeError('Cannot wait for responses if there are pending requests outstanding.  You need to wait for pending requests to be sent first.')\n    pending = self.pending_responses\n    self.pending_responses = []\n    for (command_name, promise) in pending:\n        value = client.parse_response(self.connection, command_name)\n        promise.resolve(value)\n", "label": 0}
{"function": "\n\ndef __update_copyright(self):\n    'Finds the copyright text and replaces it.'\n    region = self.__find_copyright()\n    self.__replace_copyright(region)\n", "label": 0}
{"function": "\n\ndef __init__(self, idx):\n    self.idx = _uidx()\n    self.isBatch = False\n    self.isSeq = True\n    if isinstance(idx, BaseArray):\n        arr = ct.c_void_p(0)\n        if (idx.type() == Dtype.b8.value):\n            safe_call(backend.get().af_where(ct.pointer(arr), idx.arr))\n        else:\n            safe_call(backend.get().af_retain_array(ct.pointer(arr), idx.arr))\n        self.idx.arr = arr\n        self.isSeq = False\n    elif isinstance(idx, ParallelRange):\n        self.idx.seq = idx\n        self.isBatch = True\n    else:\n        self.idx.seq = Seq(idx)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/armor/component/shared_deflector_shield_generator_energy_ray.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef leq(levels, int_time=1.0):\n    '\\n    Equivalent level :math:`L_{eq}`.\\n    \\n    :param levels: Levels as function of time.\\n    :param int_time: Integration time. Default value is 1.0 second.\\n    :returns: Equivalent level L_{eq}.\\n    \\n    Sum of levels in dB.\\n    '\n    levels = np.asarray(levels)\n    time = (levels.size * int_time)\n    return _leq(levels, time)\n", "label": 0}
{"function": "\n\ndef get_next_instruction(self):\n    dis = self.disassemble(address=self.program_counter()[1], count=1)\n    return dis.partition('\\n')[0].strip()\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/munition/shared_detonator_thermal_imperial_issue.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef test_tx_out_bitcoin_address(self):\n    coinbase_bytes = h2b('04ed66471b02c301')\n    tx = Tx.coinbase_tx(COINBASE_PUB_KEY_FROM_80971, int((50 * 100000000.0)), COINBASE_BYTES_FROM_80971)\n    self.assertEqual(tx.txs_out[0].bitcoin_address(), '1DmapcnrJNGeJB13fv9ngRFX1iRvR4zamn')\n", "label": 0}
{"function": "\n\ndef alert_smtp(alert, metric):\n    if ('@' in alert[1]):\n        sender = settings.ALERT_SENDER\n        recipient = alert[1]\n    else:\n        sender = settings.SMTP_OPTS['sender']\n        recipients = settings.SMTP_OPTS['recipients'][alert[0]]\n    if (type(recipients) is str):\n        recipients = [recipients]\n    for recipient in recipients:\n        msg = MIMEMultipart('alternative')\n        msg['Subject'] = ('[skyline alert] ' + metric[1])\n        msg['From'] = sender\n        msg['To'] = recipient\n        link = (settings.GRAPH_URL % metric[1])\n        body = ('Anomalous value: %s <br> Next alert in: %s seconds <a href=\"%s\"><img src=\"%s\"/></a>' % (metric[0], alert[2], link, link))\n        msg.attach(MIMEText(body, 'html'))\n        s = SMTP('127.0.0.1')\n        s.sendmail(sender, recipient, msg.as_string())\n        s.quit()\n", "label": 0}
{"function": "\n\ndef delete_key(self, key):\n    self.server.request('delete', ('/keys/%s' % key))\n", "label": 0}
{"function": "\n\ndef _create_placeholders(self, n_features, n_classes):\n    ' Create the TensorFlow placeholders for the model.\\n        :param n_features: number of features of the first layer\\n        :param n_classes: number of classes\\n        :return: self\\n        '\n    self.keep_prob = tf.placeholder('float')\n    self.hrand = [tf.placeholder('float', [None, self.layers[(l + 1)]]) for l in range((self.n_layers - 1))]\n    self.vrand = [tf.placeholder('float', [None, self.layers[l]]) for l in range((self.n_layers - 1))]\n    self.x = tf.placeholder('float', [None, n_features])\n    self.y_ = tf.placeholder('float', [None, n_classes])\n", "label": 0}
{"function": "\n\ndef __init__(self, name_suggestion):\n    self.name_suggestion = name_suggestion\n", "label": 0}
{"function": "\n\ndef render_datalist(self, list_id):\n    return ''.join([('<datalist id=\"%s\">' % list_id), ''.join([('<option>%s</option>' % color) for color in self.colors]), '</datalist>'])\n", "label": 0}
{"function": "\n\ndef _do_remove(self, section, option):\n    if (not self.config.has_option(section, option)):\n        raise AdminCommandError(_(\"Option '%(option)s' doesn't exist in section '%(section)s'\", option=option, section=section))\n    self.config.remove(section, option)\n    self.config.save()\n    if ((section == 'inherit') and (option == 'file')):\n        self.config.parse_if_needed(force=True)\n", "label": 0}
{"function": "\n\ndef _create_flavor(self, description=None):\n    flavor = {\n        'flavor': {\n            'name': 'GOLD',\n            'service_type': constants.DUMMY,\n            'description': (description or 'the best flavor'),\n            'enabled': True,\n        },\n    }\n    return (self.plugin.create_flavor(self.ctx, flavor), flavor)\n", "label": 0}
{"function": "\n\ndef test_logout(self):\n    'Tests when logging out with and without continue URL.'\n    host = 'foo.com:1234'\n    path_info = '/_ah/login'\n    cookie_dict = {\n        'dev_appserver_login': ('%s:False:%s' % (EMAIL, USER_ID)),\n    }\n    action = 'Logout'\n    set_email = ''\n    set_admin = False\n    continue_url = ''\n    expected_set = login._clear_user_info_cookie().strip()\n    (status, location, set_cookie, _) = self._run_test(host, path_info, cookie_dict, action, set_email, set_admin, continue_url)\n    self.assertEqual(302, status)\n    self.assertEqual(('http://%s%s' % (host, path_info)), location)\n    self.assertEqual(expected_set, set_cookie)\n    self.assertIsInstance(location, str)\n    self.assertIsInstance(set_cookie, str)\n    continue_url = 'http://foo.com/blah'\n    (status, location, set_cookie, _) = self._run_test(host, path_info, cookie_dict, action, set_email, set_admin, continue_url)\n    self.assertEqual(302, status)\n    self.assertEqual(continue_url, location)\n    self.assertEqual(expected_set, set_cookie)\n    self.assertIsInstance(location, str)\n    self.assertIsInstance(set_cookie, str)\n", "label": 0}
{"function": "\n\ndef test04c__getitem__(self):\n    'Checking cols.__getitem__() with subgroups with a range index with\\n        step.'\n    tbl = self.h5file.create_table('/', 'test', self._TestTDescr, title=self._getMethodName())\n    tbl.append(self._testAData)\n    if self.reopen:\n        self._reopen()\n        tbl = self.h5file.root.test\n    nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)\n    tblcols = tbl.cols._f_col('Info')[0::2]\n    nrarrcols = nrarr['Info'][0::2]\n    if common.verbose:\n        print('Read cols:', tblcols)\n        print('Should look like:', nrarrcols)\n    self.assertTrue(common.areArraysEqual(nrarrcols, tblcols), \"Original array are retrieved doesn't match.\")\n", "label": 0}
{"function": "\n\ndef calcScale(self, testCount):\n    import math\n    scale = int((self.size / (math.sqrt(testCount) + 1)))\n    return scale\n", "label": 0}
{"function": "\n\ndef get_repository_info(self):\n    '\\n        Find out information about the current Bazaar branch (if any) and\\n        return it.\\n        '\n    if (not check_install(['bzr', 'help'])):\n        logging.debug('Unable to execute \"bzr help\": skipping Bazaar')\n        return None\n    bzr_info = execute(['bzr', 'info'], ignore_errors=True)\n    if ('ERROR: Not a branch:' in bzr_info):\n        repository_info = None\n    else:\n        branch_match = re.search(self.BRANCH_REGEX, bzr_info, re.MULTILINE)\n        path = branch_match.group('branch_path')\n        if (path == '.'):\n            path = os.getcwd()\n        repository_info = RepositoryInfo(path=path, base_path='/', supports_parent_diffs=True)\n    return repository_info\n", "label": 0}
{"function": "\n\ndef test_set_messages_success(self):\n    author = {\n        'name': 'John Doe',\n        'slug': 'success-msg',\n    }\n    add_url = reverse('add_success_msg')\n    req = self.client.post(add_url, author)\n    self.assertIn((ContactFormViewWithMsg.success_message % author), req.cookies['messages'].value)\n", "label": 0}
{"function": "\n\ndef __init__(self, question, docs):\n    super(Extractor, self).__init__(question, docs, tag=TAG)\n", "label": 0}
{"function": "\n\ndef expect(self, method=None, uri=None, params={\n    \n}):\n    if method:\n        self.assertEqual(method, self.executor.request.method)\n    if uri:\n        self.assertEqual(self.executor.request.uri, ('https://api-ssl.bitly.com/v3' + uri))\n    if params:\n        params.update({\n            'access_token': 'my-access-token',\n        })\n        self.assertEqual(self.executor.request.params, params)\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    if isinstance(other, BaseNull):\n        return other\n    return self.map((Q + _unwrap(other)))\n", "label": 0}
{"function": "\n\ndef next_hop(tokeniser):\n    value = tokeniser()\n    if (value.lower() == 'self'):\n        return (IPSelf(tokeniser.afi), NextHopSelf(tokeniser.afi))\n    else:\n        ip = IP.create(value)\n        if (ip.afi == AFI.ipv4):\n            return (ip, NextHop(ip.top()))\n        return (ip, None)\n", "label": 0}
{"function": "\n\ndef test_is_variant(self):\n    expander = GvcfExpander()\n    self.assertTrue(expander.is_variant(json.loads(self.snp_1)))\n    self.assertTrue(expander.is_variant(json.loads(self.snp_2)))\n    self.assertTrue(expander.is_variant(json.loads(self.insertion_1)))\n    self.assertTrue(expander.is_variant(json.loads(self.deletion_1)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_a)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_b)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_c)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_d)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_ambiguous)))\n    self.assertFalse(expander.is_variant(json.loads(self.no_call_1)))\n", "label": 0}
{"function": "\n\ndef modify_updates(self, updates):\n    '\"\\n        Modifies the parameters before a learning update is applied. Behavior\\n        is defined by subclass\\'s implementation of _modify_updates and any\\n        ModelExtension\\'s implementation of post_modify_updates.\\n\\n        Parameters\\n        ----------\\n        updates : dict\\n            A dictionary mapping shared variables to symbolic values they\\n            will be updated to\\n\\n        Notes\\n        -----\\n        For example, if a given parameter is not meant to be learned, a\\n        subclass or extension\\n        should remove it from the dictionary. If a parameter has a restricted\\n        range, e.g.. if it is the precision of a normal distribution,\\n        a subclass or extension should clip its update to that range. If a\\n        parameter\\n        has any other special properties, its updates should be modified\\n        to respect that here, e.g. a matrix that must be orthogonal should\\n        have its update value modified to be orthogonal here.\\n\\n        This is the main mechanism used to make sure that generic training\\n        algorithms such as those found in pylearn2.training_algorithms\\n        respect the specific properties of the models passed to them.\\n        '\n    self._modify_updates(updates)\n    self._ensure_extensions()\n    for extension in self.extensions:\n        extension.post_modify_updates(updates, self)\n", "label": 0}
{"function": "\n\ndef set_flipped(self, x, y):\n    ' Sets the specified piece as flipped.\\n        '\n    self.pieces[(x + (y * self.width))].set_flipped()\n", "label": 0}
{"function": "\n\ndef on_leave(self, details):\n    self.disconnect()\n", "label": 0}
{"function": "\n\n@needs_mail\n@needs_link\ndef proxy(request, mail, link):\n    return link.get_target(mail)(request, mail.person, mail.job.group_object)\n", "label": 0}
{"function": "\n\ndef TestParallelModify(instances):\n    'PERFORMANCE: Parallel instance modify.\\n\\n  @type instances: list of L{qa_config._QaInstance}\\n  @param instances: list of instances to issue modify commands against\\n\\n  '\n    job_driver = _JobQueueDriver()\n    new_min_mem = qa_config.get(constants.BE_MAXMEM)\n    for instance in instances:\n        cmd = ['gnt-instance', 'modify', '--submit', '-B', ('%s=%s' % (constants.BE_MINMEM, new_min_mem))]\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n        cmd = ['gnt-instance', 'modify', '--submit', '-O', 'fake_os_param=fake_value']\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n        cmd = ['gnt-instance', 'modify', '--submit', '-O', 'fake_os_param=fake_value', '-B', ('%s=%s' % (constants.BE_MINMEM, new_min_mem))]\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n    job_driver.WaitForCompletion()\n", "label": 0}
{"function": "\n\ndef _dependencies(self):\n    projects = []\n    for attr in ('install_requires', 'tests_require'):\n        requirements = (getattr(self.distribution, attr, None) or [])\n        for project in requirements:\n            if (not project):\n                continue\n            projects.append(pypi.just_name(project))\n    extras = (getattr(self.distribution, 'extras_require', None) or {\n        \n    })\n    for value in extras.values():\n        projects.extend(map(pypi.just_name, value))\n    return projects\n", "label": 1}
{"function": "\n\ndef save_graph_db_refs(self, sourceDB=None, targetDB=None, edgeDB=None, simpleKeys=False, unpack_edge=None, edgeDictClass=None, graph=None, **kwargs):\n    'apply kwargs to reference DB objects for this graph'\n    if (sourceDB is not None):\n        self.sourceDB = sourceDB\n    else:\n        simpleKeys = True\n    if (targetDB is not None):\n        self.targetDB = targetDB\n    if (edgeDB is not None):\n        self.edgeDB = edgeDB\n    else:\n        self.pack_edge = self.unpack_edge = (lambda edge: edge)\n    if simpleKeys:\n        self.__class__ = self._IDGraphClass\n    if (unpack_edge is not None):\n        self.unpack_edge = unpack_edge\n    if (graph is not None):\n        self.graph = graph\n    if (edgeDictClass is not None):\n        self.edgeDictClass = edgeDictClass\n", "label": 1}
{"function": "\n\ndef __init__(self, errmsg='You need override this method'):\n    super(NeedOverrideError, self).__init__(self, errmsg)\n", "label": 0}
{"function": "\n\ndef play_rtmpdump_stream(player, url, params={\n    \n}):\n    cmdline = (\"rtmpdump -r '%s' \" % url)\n    for key in params.keys():\n        cmdline += (((key + ' ') + params[key]) if (params[key] != None) else ('' + ' '))\n    cmdline += (' -o - | %s -' % player)\n    print(cmdline)\n    os.system(cmdline)\n    return\n", "label": 0}
{"function": "\n\ndef test_disable_logging(self):\n    NAME = 'name'\n    before = {\n        'logging': {\n            'logBucket': 'logs',\n            'logObjectPrefix': 'pfx',\n        },\n    }\n    bucket = self._makeOne(name=NAME, properties=before)\n    self.assertTrue((bucket.get_logging() is not None))\n    bucket.disable_logging()\n    self.assertTrue((bucket.get_logging() is None))\n", "label": 0}
{"function": "\n\ndef test_benchmark_variance_06(self):\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['Monthly'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.three_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['3-Month'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.six_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['6-month'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.year_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['year'])\n", "label": 0}
{"function": "\n\ndef all_job_data(jobs, job_type):\n    ' Return an iterator over all job data. Exclude config template dups. '\n    conf_tmpl_ids = []\n    for job in jobs:\n        for (jt, data) in job.iteritems():\n            if (jt == job_type):\n                if (data['config_template_id'] not in conf_tmpl_ids):\n                    conf_tmpl_ids.append(data['config_template_id'])\n                    (yield data)\n", "label": 0}
{"function": "\n\ndef take_action(self, parsed_args):\n    identity_client = self.app.client_manager.identity\n    consumer = utils.find_resource(identity_client.oauth1.consumers, parsed_args.consumer)\n    identity_client.oauth1.consumers.delete(consumer.id)\n", "label": 0}
{"function": "\n\ndef test_validate_type_negative(self):\n    sla1 = TestCriterion(0)\n\n    class AnotherTestCriterion(TestCriterion):\n        pass\n    sla2 = AnotherTestCriterion(0)\n    self.assertRaises(TypeError, sla1.validate_type, sla2)\n", "label": 0}
{"function": "\n\ndef test_profile_topics_bookmarks(self):\n    \"\\n        profile user's topics with bookmarks\\n        \"\n    bookmark = CommentBookmark.objects.create(topic=self.topic, user=self.user)\n    utils.login(self)\n    response = self.client.get(reverse('spirit:user:topics', kwargs={\n        'pk': self.user2.pk,\n        'slug': self.user2.st.slug,\n    }))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(list(response.context['topics']), [self.topic])\n    self.assertEqual(response.context['topics'][0].bookmark, bookmark)\n", "label": 0}
{"function": "\n\ndef test_volume_create_properties(self):\n    arglist = ['--property', 'Alpha=a', '--property', 'Beta=b', '--size', str(self.new_volume.size), self.new_volume.name]\n    verifylist = [('property', {\n        'Alpha': 'a',\n        'Beta': 'b',\n    }), ('size', self.new_volume.size), ('name', self.new_volume.name)]\n    parsed_args = self.check_parser(self.cmd, arglist, verifylist)\n    (columns, data) = self.cmd.take_action(parsed_args)\n    self.volumes_mock.create.assert_called_with(size=self.new_volume.size, snapshot_id=None, name=self.new_volume.name, description=None, volume_type=None, user_id=None, project_id=None, availability_zone=None, metadata={\n        'Alpha': 'a',\n        'Beta': 'b',\n    }, imageRef=None, source_volid=None)\n    self.assertEqual(self.columns, columns)\n    self.assertEqual(self.datalist, data)\n", "label": 0}
{"function": "\n\ndef init_relation(self, models, relation):\n    '\\n        Initialize the relation on a set of models.\\n\\n        :type models: list\\n        :type relation: str\\n        '\n    for model in models:\n        model.set_relation(relation, Result(None, self, model))\n    return models\n", "label": 0}
{"function": "\n\ndef test_with_some_synonyms(self):\n    SynonymFactory(from_words='foo', to_words='bar')\n    SynonymFactory(from_words='baz', to_words='qux')\n    (_, body) = es_utils.es_get_synonym_filter('en-US')\n    expected = {\n        'type': 'synonym',\n        'synonyms': ['foo => bar', 'baz => qux'],\n    }\n    eq_(body, expected)\n", "label": 0}
{"function": "\n\ndef run(self):\n    f = self.output().open('w')\n    print('hello, world', file=f)\n    f.close()\n", "label": 0}
{"function": "\n\ndef __init__(self, area, width=4, char=' '):\n    self.width = width\n    self.char = char\n    area.install(('NORMAL', '<Key-greater>', (lambda event: event.widget.shift_sel_right(self.width, self.char))), ('NORMAL', '<Key-less>', (lambda event: event.widget.shift_sel_left(self.width))))\n", "label": 0}
{"function": "\n\n@classmethod\ndef validate(cls, level):\n    level = int(level)\n    if (level in (cls.NONE, cls.READ, cls.WRITE, cls.ADMIN, cls.SITE_ADMIN)):\n        return level\n    else:\n        raise ValueError(('Invalid AccessType: %d.' % level))\n", "label": 0}
{"function": "\n\ndef __init__(self, lib, dtype, N, C, K, H, W, P, Q, pad_h, pad_w, relu, bsum):\n    (R, S) = (3, 3)\n    GC32 = _ceil_div(C, 32)\n    GC16 = _ceil_div((GC32 * 32), 16)\n    GK16 = _ceil_div(K, 16)\n    self.filter_func = _get_bprop_filter_trans_4x4_kernel\n    self.filter_size = (((dtype.itemsize * 1152) * K) * GC32)\n    self.filter_args = [(GK16, GC16, 1), (256, 1, 1), None, None, None, ((R * S) * K), (S * K), ((S * K) * 2), K, C, (K * 1152)]\n    super(BpropWinograd_4x4_3x3, self).__init__(lib, dtype, N, K, C, P, Q, H, W, (2 - pad_h), (2 - pad_w), relu, bsum)\n", "label": 0}
{"function": "\n\n@records.post(validators=record_validator, permission='post_record')\ndef post_record(request):\n    'Saves a single model record.\\n\\n    Posted record attributes will be matched against the related model\\n    definition.\\n\\n    '\n    if (request.headers.get('Validate-Only', 'false') == 'true'):\n        return\n    model_id = request.matchdict['model_id']\n    if request.credentials_id:\n        credentials_id = request.credentials_id\n    else:\n        credentials_id = Everyone\n    record_id = request.db.put_record(model_id, request.data_clean, [credentials_id])\n    request.notify('RecordCreated', model_id, record_id)\n    created = ('%s/models/%s/records/%s' % (request.application_url, model_id, record_id))\n    request.response.status = '201 Created'\n    request.response.headers['location'] = str(created)\n    return {\n        'id': record_id,\n    }\n", "label": 0}
{"function": "\n\ndef on_files_selected(self, paths):\n    \" Handle the 'filesSelected' signal from the dialog.\\n\\n        \"\n    d = self.declaration\n    if (d is not None):\n        d.selected_paths = paths\n", "label": 0}
{"function": "\n\ndef set_context(self, serializer):\n    '\\n        This hook is called by the serializer instance,\\n        prior to the validation call being made.\\n        '\n    self.instance = getattr(serializer, 'instance', None)\n", "label": 0}
{"function": "\n\ndef db_add_ts_start(self, ts_start):\n    self._db_ts_start = ts_start\n", "label": 0}
{"function": "\n\n@expose('/<string:locale>')\ndef index(self, locale):\n    session['locale'] = locale\n    refresh()\n    self.update_redirect()\n    return redirect(self.get_redirect())\n", "label": 0}
{"function": "\n\ndef reselect(self, pos):\n\n    def select(view, edit):\n        region = pos\n        if hasattr(pos, '__call__'):\n            region = run_callback(pos, view)\n        if isinstance(region, int):\n            region = sublime.Region(region, region)\n        elif isinstance(region, (tuple, list)):\n            region = sublime.Region(*region)\n        view.sel().clear()\n        view.sel().add(region)\n        view.show(region, False)\n    self.callback(select)\n", "label": 0}
{"function": "\n\ndef test_mutual_info_regression():\n    (X, y) = make_regression(n_samples=100, n_features=10, n_informative=2, shuffle=False, random_state=0, noise=10)\n    univariate_filter = SelectKBest(mutual_info_regression, k=2)\n    X_r = univariate_filter.fit(X, y).transform(X)\n    assert_best_scores_kept(univariate_filter)\n    X_r2 = GenericUnivariateSelect(mutual_info_regression, mode='k_best', param=2).fit(X, y).transform(X)\n    assert_array_equal(X_r, X_r2)\n    support = univariate_filter.get_support()\n    gtruth = np.zeros(10)\n    gtruth[:2] = 1\n    assert_array_equal(support, gtruth)\n    univariate_filter = SelectPercentile(mutual_info_regression, percentile=20)\n    X_r = univariate_filter.fit(X, y).transform(X)\n    X_r2 = GenericUnivariateSelect(mutual_info_regression, mode='percentile', param=20).fit(X, y).transform(X)\n    assert_array_equal(X_r, X_r2)\n    support = univariate_filter.get_support()\n    gtruth = np.zeros(10)\n    gtruth[:2] = 1\n    assert_array_equal(support, gtruth)\n", "label": 0}
{"function": "\n\ndef test_derived(self):\n    import time\n\n    class Local(threading.local):\n\n        def __init__(self):\n            time.sleep(0.01)\n    local = Local()\n\n    def f(i):\n        local.x = i\n        self.assertEqual(local.x, i)\n    threads = []\n    for i in range(10):\n        t = threading.Thread(target=f, args=(i,))\n        t.start()\n        threads.append(t)\n    for t in threads:\n        t.join()\n", "label": 0}
{"function": "\n\ndef test_retry_in_graph_flow_requires_and_provides(self):\n    flow = gf.Flow('gf', retry.AlwaysRevert('rt', requires=['x', 'y'], provides=['a', 'b']))\n    self.assertEqual(set(['x', 'y']), flow.requires)\n    self.assertEqual(set(['a', 'b']), flow.provides)\n", "label": 0}
{"function": "\n\ndef update(self, t, dt):\n    if (random.random() < 0.02):\n        self.a += ((random.randint((- 1), 1) * pi) / 8)\n    dx = cos(self.a)\n    dz = sin(self.a)\n    self.x += (dx * dt)\n    self.z += (dz * dt)\n", "label": 0}
{"function": "\n\ndef encryption_oracle(rawInput):\n    key = generateAESKey()\n    iv = generateAESKey()\n    prependAmount = (5 + (getOneRandomByte() % 6))\n    appendAmount = (5 + (getOneRandomByte() % 6))\n    plaintext = (((b'x' * prependAmount) + rawInput) + (b'y' * appendAmount))\n    if (getOneRandomByte() & 1):\n        return aes_ecb_enc(addPKCS7Padding(plaintext, 16), key)\n    else:\n        return aes_cbc_enc(addPKCS7Padding(plaintext, 16), key, iv)\n", "label": 0}
{"function": "\n\ndef parse_status(self, lines):\n    activity = []\n    seen_times = set()\n    for line in lines:\n        (time, fields) = line.split('|')\n        if (time not in seen_times):\n            seen_times.add(time)\n            status_obj = status.Status(int(float(time)), fields)\n            activity.append(status_obj)\n    return activity\n", "label": 0}
{"function": "\n\n@property\ndef directions(self):\n    hashes = []\n    t_query = 'select trip_headsign, trip_short_name, bikes_allowed,\\n        trip_id from trips where route_id=:route_id and _feed=:_feed'\n    t_filter = {\n        'route_id': self.id,\n        '_feed': self.provider.feed_id,\n    }\n    cur = self.provider.conn.cursor()\n    for trip in cur.execute(t_query, t_filter):\n        direction = {\n            \n        }\n        innercur = self.provider.conn.cursor()\n        result = innercur.execute('select s.* from stop_times as st join stops as s\\n                on st.stop_id=s.stop_id and st._feed=s._feed\\n                where st.trip_id=:t_id and st._feed=:_feed', {\n            't_id': trip['trip_id'],\n            '_feed': self.provider.feed_id,\n        })\n        direction['stops'] = [GTFSStop(self.provider, **dict(row)) for row in result]\n        if (trip['trip_headsign'] is not None):\n            direction['headsign'] = trip['trip_headsign']\n        if (trip['trip_short_name'] is not None):\n            direction['short_name'] = trip['trip_short_name']\n        if (trip['bikes_allowed'] is not None):\n            direction['bikes_ok'] = trip['bikes_allowed']\n        h = util.freezehash(direction)\n        if (h not in hashes):\n            hashes.append(h)\n            (yield direction)\n", "label": 1}
{"function": "\n\ndef _partitions_to_src(partitions):\n    return ''.join((part.src for part in partitions))\n", "label": 0}
{"function": "\n\ndef get_member_addresses(self):\n    addresses = []\n    for member in self.config_members:\n        addresses.append(member.get_server().get_address())\n    return addresses\n", "label": 0}
{"function": "\n\ndef test_dispatch_on_heartbeat_frame(self):\n    frame = mock()\n    expect(frame.type).returns(HeartbeatFrame.type())\n    expect(self.ch.send_heartbeat)\n    self.ch.dispatch(frame)\n", "label": 0}
{"function": "\n\ndef __init__(self, output_type, inplace=False):\n    Op.__init__(self)\n    self.output_type = output_type\n    self.inplace = inplace\n    if inplace:\n        self.destroy_map = {\n            0: [0],\n        }\n    self.warned_numpy_version = False\n", "label": 0}
{"function": "\n\ndef CheckLoggingWorks(self):\n    logger = StringIO.StringIO()\n    expected_output = (';\\n'.join([sqlite.main._BEGIN, 'CREATE TABLE TEST(FOO INTEGER)', 'INSERT INTO TEST(FOO) VALUES (?)', 'ROLLBACK']) + ';\\n')\n    self.cnx = sqlite.connect(self.getfilename(), command_logfile=logger)\n    cu = self.cnx.cursor()\n    cu.execute('CREATE TABLE TEST(FOO INTEGER)')\n    cu.execute('INSERT INTO TEST(FOO) VALUES (?)', (5,))\n    self.cnx.rollback()\n    logger.seek(0)\n    real_output = logger.read()\n    if (expected_output != real_output):\n        self.fail(\"Logging didn't produce expected output.\")\n", "label": 0}
{"function": "\n\ndef bitand(self, other):\n    return self._combine(other, self.BITAND, False)\n", "label": 0}
{"function": "\n\ndef get_environment(self, app):\n    return app.extensions['gears']['environment']\n", "label": 0}
{"function": "\n\ndef load(self, odffile):\n    ' Loads a document into the parser and parses it.\\n            The argument can either be a filename or a document in memory.\\n        '\n    self.lines = []\n    self._wfunc = self._wlines\n    if isinstance(odffile, str):\n        self.document = load(odffile)\n    else:\n        self.document = odffile\n    self._walknode(self.document.topnode)\n", "label": 0}
{"function": "\n\ndef db_children(self, parent=(None, None), orphan=False):\n    children = []\n    children.append((self, parent[0], parent[1]))\n    return children\n", "label": 0}
{"function": "\n\ndef blacklist_delete_user_agents(self, request, queryset):\n    self.blacklist_user_agents(request, queryset)\n    self.delete_queryset(request, queryset)\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n\n    def format_row(row):\n        return ('(%s)' % ', '.join((format_number(value) for value in row)))\n    return ('Matrix44(%s)' % ', '.join((format_row(row) for row in self.rows())))\n", "label": 0}
{"function": "\n\ndef _createFile(self):\n    self.h5file.create_array('/', 'arr1', [1, 2])\n    group1 = self.h5file.create_group('/', 'group1')\n    arr2 = self.h5file.create_array(group1, 'arr2', [1, 2, 3])\n    lgroup1 = self.h5file.create_hard_link('/', 'lgroup1', '/group1')\n    self.assertTrue((lgroup1 is not None))\n    larr1 = self.h5file.create_hard_link(group1, 'larr1', '/arr1')\n    self.assertTrue((larr1 is not None))\n    larr2 = self.h5file.create_hard_link('/', 'larr2', arr2)\n    self.assertTrue((larr2 is not None))\n", "label": 0}
{"function": "\n\ndef saveXML(self, snode):\n    if (snode is None):\n        snode = self\n    elif (snode.ownerDocument is not self):\n        raise xml.dom.WrongDocumentErr()\n    return snode.toxml()\n", "label": 0}
{"function": "\n\ndef install(self, instance):\n    cfg = self.config\n    if (self.database == self.master_database):\n        template = 'template1'\n    else:\n        template = self.master_database\n    config_opt_map = dict(host='host', user='username', password='password', encoding='encoding', lc_collate='lc-collate', lc_ctype='lc-ctype', tablespace='tablespace')\n    options = [((('--' + opt) + '=') + str(cfg[cfg_name])) for (cfg_name, opt) in config_opt_map.iteritems() if (cfg_name in cfg)]\n    fail = instance.do(((['createdb', '--template', template] + options) + [self.database]))\n    if fail:\n        instance.do(['pg_dump', '-h', str(cfg['host']), '-U', str(cfg['user']), '-E', str(cfg['encoding']), '-f', (('/tmp/' + template) + '.sql'), template])\n        instance.do(((['createdb', '--template', 'template1'] + options) + [self.database]))\n        instance.do(['psql', '-h', str(cfg['host']), '-U', str(cfg['user']), '-f', (('/tmp/' + template) + '.sql'), self.database])\n    info = self.connection_info\n    info['database'] = self.database\n    return info\n", "label": 0}
{"function": "\n\n@contextmanager\ndef trace_ms(statName):\n    if (_instatrace is None):\n        (yield)\n        return\n    now = _instatrace.now_ms()\n    (yield)\n    _instatrace.trace(statName, (_instatrace.now_ms() - now))\n", "label": 0}
{"function": "\n\ndef test_get_bad_column_qualifier(self):\n    families = {\n        cf2: 'oberyn',\n    }\n    res = self.c.get(table, self.row_prefix, families=families)\n    self.assertEqual(result_to_dict(res), {\n        \n    })\n", "label": 0}
{"function": "\n\ndef db_add_portSpec(self, portSpec):\n    self.__db_portSpec = portSpec\n", "label": 0}
{"function": "\n\ndef do(args):\n    ' Main method '\n    if args.name:\n        tc_names = [args.name]\n    else:\n        tc_names = qitoolchain.get_tc_names()\n    for tc_name in tc_names:\n        toolchain = qitoolchain.get_toolchain(tc_name)\n        ui.info(str(toolchain))\n", "label": 0}
{"function": "\n\ndef test_IRParamStmt_dump():\n    stmt = {\n        'param': 'IR',\n        'angle': '45',\n    }\n    ir = IRParamStmt.from_dict(stmt)\n    assert_equal(ir.to_gerber(), '%IR45*%')\n", "label": 0}
{"function": "\n\ndef get_resources(self):\n    resources = []\n    resource = extensions.ResourceExtension('os-agents', AgentController())\n    resources.append(resource)\n    return resources\n", "label": 0}
{"function": "\n\ndef _save(self, key, attributes):\n    s_uuid = self.repo.save(key, attributes)\n    self.logger.debug(('creating object with uuid = %s' % s_uuid))\n    attribute_type = AT.UNIQUE_IDENTIFIER\n    attribute = self.attribute_factory.create_attribute(attribute_type, s_uuid)\n    attributes.append(attribute)\n    self.repo.update(s_uuid, key, attributes)\n    return (s_uuid, attribute)\n", "label": 0}
{"function": "\n\ndef to_array(self):\n    '\\n        Convert the RiakLinkPhase to a format that can be output into\\n        JSON. Used internally.\\n        '\n    stepdef = {\n        'bucket': self._bucket,\n        'tag': self._tag,\n        'keep': self._keep,\n    }\n    return {\n        'link': stepdef,\n    }\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Tangible()\n    result.template = 'object/tangible/ship/crafted/capacitor/shared_quick_recharge_battery_mk5.iff'\n    result.attribute_template_id = 8\n    result.stfName('space_crafting_n', 'quick_recharge_battery_mk5')\n    return result\n", "label": 0}
{"function": "\n\ndef abort_request(self, stream, ident, parent):\n    'abort a specifig msg by id'\n    msg_ids = parent['content'].get('msg_ids', None)\n    if isinstance(msg_ids, str):\n        msg_ids = [msg_ids]\n    if (not msg_ids):\n        self.abort_queues()\n    for mid in msg_ids:\n        self.aborted.add(str(mid))\n    content = dict(status='ok')\n    reply_msg = self.session.send(stream, 'abort_reply', content=content, parent=parent, ident=ident)\n    self.log.debug(str(reply_msg))\n", "label": 0}
{"function": "\n\ndef p_basic_statement(self, p):\n    'basic_statement : if_statement\\n        | case_statement\\n        | casex_statement\\n        | for_statement\\n        | while_statement\\n        | event_statement\\n        | wait_statement\\n        | forever_statement\\n        | block\\n        | namedblock\\n        | parallelblock\\n        | blocking_substitution\\n        | nonblocking_substitution\\n        | single_statement\\n        '\n    p[0] = p[1]\n    p.set_lineno(0, p.lineno(1))\n", "label": 0}
{"function": "\n\ndef write(self, handle, name):\n    self._check_all_set()\n    g = handle.create_group(name)\n    g.attrs['type'] = np.string_('extern_box'.encode('utf-8'))\n    g.attrs['xmin'] = self.bounds[0][0]\n    g.attrs['xmax'] = self.bounds[0][1]\n    g.attrs['ymin'] = self.bounds[1][0]\n    g.attrs['ymax'] = self.bounds[1][1]\n    g.attrs['zmin'] = self.bounds[2][0]\n    g.attrs['zmax'] = self.bounds[2][1]\n    Source.write(self, g)\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    self.one_to_one = True\n    return super(OneToOneFieldDefinition, self).save(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef testComplexLabels(self):\n    logging.debug('Running testComplexLabels method.')\n    expression = 'a_123'\n    self._RunMathQuery(expression, ['a_123'])\n    expression = 'a_1.b2'\n    self._RunMathQuery(expression, ['a_1.b2'])\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    ' Returns the materialized path '\n    return ('/'.join([x.value for x in self.parts]) + ('/' if self.is_dir else ''))\n", "label": 0}
{"function": "\n\ndef retry_subflow(self, retry):\n    'Prepares a retrys + its subgraph for execution.\\n\\n        This sets the retrys intention to ``EXECUTE`` and resets all of its\\n        subgraph (its successors) to the ``PENDING`` state with an ``EXECUTE``\\n        intention.\\n        '\n    tweaked = self.reset_atoms([retry], state=None, intention=st.EXECUTE)\n    tweaked.extend(self.reset_subgraph(retry))\n    return tweaked\n", "label": 0}
{"function": "\n\ndef draw_random(G, **kwargs):\n    'Draw the graph G with a random layout.'\n    draw(G, random_layout(G), **kwargs)\n", "label": 0}
{"function": "\n\ndef post(self):\n    ' pass additionalMetadata and file to global\\n        variables.\\n        '\n    global received_file\n    global received_meta\n    received_file = self.request.files['file'][0].body\n    received_meta = self.get_argument('additionalMetadata')\n", "label": 0}
{"function": "\n\ndef __init__(self, gates, system_desc, wh_codes):\n    self.gates = gates\n    self.system_desc = system_desc\n    self.wh_codes = wh_codes\n", "label": 0}
{"function": "\n\ndef __init__(self, args):\n    super(RemoveVariantSetRunner, self).__init__(args)\n    self.variantSetName = args.variantSetName\n", "label": 0}
{"function": "\n\n@override_djconfig(comments_per_page=1)\ndef test_profile_comments_paginate(self):\n    \"\\n        profile user's comments paginated\\n        \"\n    utils.create_comment(user=self.user2, topic=self.topic)\n    comment = utils.create_comment(user=self.user2, topic=self.topic)\n    utils.login(self)\n    response = self.client.get(reverse('spirit:user:detail', kwargs={\n        'pk': self.user2.pk,\n        'slug': self.user2.st.slug,\n    }))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(list(response.context['comments']), [comment])\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    self.maxDiff = None\n    filename = 'chart_data_labels24.xlsx'\n    test_dir = 'xlsxwriter/test/comparison/'\n    self.got_filename = ((test_dir + '_test_') + filename)\n    self.exp_filename = ((test_dir + 'xlsx_files/') + filename)\n    self.ignore_files = []\n    self.ignore_elements = {\n        \n    }\n", "label": 0}
{"function": "\n\ndef get_available_user_FIELD_transitions(instance, user, field):\n    '\\n    List of transitions available in current model state\\n    with all conditions met and user have rights on it\\n    '\n    for transition in get_available_FIELD_transitions(instance, field):\n        if transition.has_perm(instance, user):\n            (yield transition)\n", "label": 0}
{"function": "\n\ndef pixelCollision(rect1, rect2, hitmask1, hitmask2):\n    'Checks if two objects collide and not just their rects'\n    rect = rect1.clip(rect2)\n    if ((rect.width == 0) or (rect.height == 0)):\n        return False\n    (x1, y1) = ((rect.x - rect1.x), (rect.y - rect1.y))\n    (x2, y2) = ((rect.x - rect2.x), (rect.y - rect2.y))\n    for x in xrange(rect.width):\n        for y in xrange(rect.height):\n            if (hitmask1[(x1 + x)][(y1 + y)] and hitmask2[(x2 + x)][(y2 + y)]):\n                return True\n    return False\n", "label": 1}
{"function": "\n\ndef _item_position(self, item):\n    return self.items.index(item)\n", "label": 0}
{"function": "\n\ndef GetModifiedShellCommand(self, Command, PluginOutputDir):\n    self.RefreshReplacements()\n    NewCommand = ((('cd ' + self.ShellPathEscape(PluginOutputDir)) + '; ') + MultipleReplace(Command, self.DynamicReplacements))\n    self.OldCommands[NewCommand] = Command\n    return NewCommand\n", "label": 0}
{"function": "\n\ndef report_for_conf(self, conf):\n    'Returns the path to the ivy report for the provided conf.\\n\\n     Returns None if there is no path.\\n    '\n    return self._reports_by_conf.get(conf)\n", "label": 0}
{"function": "\n\ndef our_x2_iterates(n_iters=100):\n    history = []\n    searchspace = SearchSpace()\n    searchspace.add_float('x', (- 10), 10)\n    random = np.random.RandomState(0)\n\n    def fn(params):\n        return (- (params['x'] ** 2))\n    for i in range(n_iters):\n        params = HyperoptTPE(seed=random).suggest(history, searchspace)\n        history.append((params, fn(params), 'SUCCEEDED'))\n    return np.array([h[0]['x'] for h in history])\n", "label": 0}
{"function": "\n\ndef insert(self, window, first_line, *lines):\n    (row, column) = cursor = self.cursors[window]\n    (left, right) = (self[row][:column], self[row][column:])\n    added = len(lines)\n    if lines:\n        last_line = lines[(- 1)]\n        column = len(last_line)\n    else:\n        last_line = first_line\n        column += len(first_line)\n    self[row] = (left + first_line)\n    self[(row + 1):(row + 1)] = lines\n    self[(row + added)] += right\n    for other in self.cursors.itervalues():\n        if (other.row > row):\n            other._row += added\n    cursor.coords = ((row + added), column)\n", "label": 0}
{"function": "\n\n@keep_alive('server')\ndef address_is_mine(self, address):\n    result = self.server.validateaddress(address)\n    return result['ismine']\n", "label": 0}
{"function": "\n\ndef npm_command(self, args):\n    'Creates a command that can run `npm`, passing the given args to it.\\n\\n    :param list args: A list of arguments to pass to `npm`.\\n    :returns: An `npm` command that can be run later.\\n    :rtype: :class:`NodeDistribution.Command`\\n    '\n    return self._create_command('npm', args)\n", "label": 0}
{"function": "\n\ndef test_disenroll_with_no_enrollment(self):\n    courses = Course.objects.all()\n    for course in courses:\n        course.delete()\n    client = Client()\n    client.login(username=TEST_USER_USERNAME, password=TEST_USER_PASSWORD)\n    kwargs = {\n        'HTTP_X_REQUESTED_WITH': 'XMLHttpRequest',\n    }\n    response = client.post('/disenroll', {\n        'course_id': 1,\n    }, **kwargs)\n    self.assertEqual(response.status_code, 200)\n    json_string = response.content.decode(encoding='UTF-8')\n    array = json.loads(json_string)\n    self.assertEqual(array['message'], 'record does not exist')\n    self.assertEqual(array['status'], 'failed')\n", "label": 0}
{"function": "\n\n@parameterized.expand([('split', 2, 3, 3, 1.5), ('merger', 2, 3, 3, 1.8), ('dividend', 2, 3, 3, 2.88)])\ndef test_spot_price_adjustments(self, adjustment_type, liquid_day_0_price, liquid_day_1_price, illiquid_day_0_price, illiquid_day_1_price_adjusted):\n    'Test the behaviour of spot prices during adjustments.'\n    table_name = (adjustment_type + 's')\n    liquid_asset = getattr(self, (adjustment_type.upper() + '_ASSET'))\n    illiquid_asset = getattr(self, (('ILLIQUID_' + adjustment_type.upper()) + '_ASSET'))\n    adjustments = self.adjustment_reader.get_adjustments_for_sid(table_name, liquid_asset.sid)\n    self.assertEqual(1, len(adjustments))\n    adjustment = adjustments[0]\n    self.assertEqual(adjustment[0], pd.Timestamp('2016-01-06', tz='UTC'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[0]), 'daily')\n    self.assertEqual(liquid_day_0_price, bar_data.current(liquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[1]), 'daily')\n    self.assertEqual(liquid_day_1_price, bar_data.current(liquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[1]), 'daily')\n    self.assertEqual(illiquid_day_0_price, bar_data.current(illiquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[2]), 'daily')\n    self.assertAlmostEqual(illiquid_day_1_price_adjusted, bar_data.current(illiquid_asset, 'price'))\n", "label": 0}
{"function": "\n\ndef test__merge(self):\n    seg1a = fake_neo(Block, seed=self.seed1, n=self.nchildren).segments[0]\n    assert_same_sub_schema(self.seg1, seg1a)\n    seg1a.spikes.append(self.spikes2[0])\n    seg1a.epocharrays.append(self.epcas2[0])\n    seg1a.annotate(seed=self.seed2)\n    seg1a.merge(self.seg2)\n    self.check_creation(self.seg2)\n    assert_same_sub_schema((self.sigs1a + self.sigs2), seg1a.analogsignals)\n    assert_same_sub_schema((self.sigarrs1a + self.sigarrs2), seg1a.analogsignalarrays)\n    assert_same_sub_schema((self.irsigs1a + self.irsigs2), seg1a.irregularlysampledsignals)\n    assert_same_sub_schema((self.epcs1 + self.epcs2), seg1a.epochs)\n    assert_same_sub_schema((self.epcas1 + self.epcas2), seg1a.epocharrays)\n    assert_same_sub_schema((self.evts1 + self.evts2), seg1a.events)\n    assert_same_sub_schema((self.evtas1 + self.evtas2), seg1a.eventarrays)\n    assert_same_sub_schema((self.spikes1 + self.spikes2), seg1a.spikes)\n    assert_same_sub_schema((self.trains1 + self.trains2), seg1a.spiketrains)\n", "label": 0}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    (ishape, fshape) = shapes\n    (igroups, icolors_per_group, irows, icols, icount) = ishape\n    (fmodulesR, fmodulesC, fcolors, frows, fcols) = fshape[:(- 2)]\n    (fgroups, filters_per_group) = fshape[(- 2):]\n    if ((not any_symbolic(irows, icols)) and (irows != icols)):\n        raise ValueError('non-square image argument', (irows, icols))\n    if ((not any_symbolic(frows, fcols)) and (frows != fcols)):\n        raise ValueError('non-square filter shape', (frows, fcols))\n    if ((not any_symbolic(fmodulesR, fmodulesC)) and (fmodulesR != fmodulesC)):\n        raise ValueError('non-square filter grouping', (fmodulesR, fmodulesC))\n    if ((not any_symbolic(icolors_per_group, fcolors)) and (icolors_per_group != fcolors)):\n        raise ValueError(\"color counts don't match\", (icolors_per_group, fcolors))\n    hshape = (fgroups, filters_per_group, fmodulesR, fmodulesC, icount)\n    return [hshape]\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    return ('Tuple(%s)' % ', '.join((str(elt) for elt in self.elts)))\n", "label": 0}
{"function": "\n\ndef test_create_ticket_ticket(self):\n    '\\n        A ticket ought to be created with a provided ticket string,\\n        if present.\\n        '\n    ticket = 'ST-0000000000-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'\n    st = ServiceTicket.objects.create_ticket(ticket=ticket, user=self.user)\n    self.assertEqual(st.ticket, ticket)\n", "label": 0}
{"function": "\n\ndef test_binary_infix_operators(self):\n    (a, b, h) = self.table.get_columns(['a', 'b', 'h'])\n    bool_col = (a > 0)\n    cases = [((a + b), '`a` + `b`'), ((a - b), '`a` - `b`'), ((a * b), '`a` * `b`'), ((a / b), '`a` / `b`'), ((a ** b), 'pow(`a`, `b`)'), ((a < b), '`a` < `b`'), ((a <= b), '`a` <= `b`'), ((a > b), '`a` > `b`'), ((a >= b), '`a` >= `b`'), ((a == b), '`a` = `b`'), ((a != b), '`a` != `b`'), ((h & bool_col), '`h` AND (`a` > 0)'), ((h | bool_col), '`h` OR (`a` > 0)'), ((h ^ bool_col), '(`h` OR (`a` > 0)) AND NOT (`h` AND (`a` > 0))')]\n    self._check_expr_cases(cases)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (other is None):\n        return (self._value is None)\n    other = str(other)\n    if (other not in self.values_range):\n        raise ConanException(bad_value_msg(self._name, other, self.values_range))\n    return (other == self.__str__())\n", "label": 0}
{"function": "\n\ndef _set_play_state(self, state):\n    'Helper method for play/pause/toggle.'\n    players = self._get_players()\n    if (len(players) != 0):\n        self._server.Player.PlayPause(players[0]['playerid'], state)\n    self.update_ha_state()\n", "label": 0}
{"function": "\n\ndef test_multiple_sequences(self):\n    msa = TabularMSA([DNA('ACGT'), DNA('AG-.'), DNA('AC-.')])\n    cons = msa.consensus()\n    self.assertEqual(cons, DNA('AC--'))\n", "label": 0}
{"function": "\n\ndef appletGetDetails(*args, **kwargs):\n    '\\n\\n    .. deprecated:: 0.42.0\\n       Use :func:`applet_get_details()` instead.\\n\\n    '\n    print('dxpy.appletGetDetails is deprecated; please use applet_get_details instead.', file=sys.stderr)\n    return applet_get_details(*args, **kwargs)\n", "label": 0}
{"function": "\n\n@continuation\ndef imp_struct_set_cont(orig_struct, setter, field, app, env, cont, _vals):\n    from pycket.interpreter import check_one_val\n    val = check_one_val(_vals)\n    if (setter is values.w_false):\n        return orig_struct.set_with_extra_info(field, val, app, env, cont)\n    return setter.call_with_extra_info([orig_struct, val], env, cont, app)\n", "label": 0}
{"function": "\n\ndef _do_if_else_condition(self, condition):\n    '\\n        Common logic for evaluating the conditions on #if, #ifdef and\\n        #ifndef lines.\\n        '\n    self.save()\n    d = self.dispatch_table\n    if condition:\n        self.start_handling_includes()\n        d['elif'] = self.stop_handling_includes\n        d['else'] = self.stop_handling_includes\n    else:\n        self.stop_handling_includes()\n        d['elif'] = self.do_elif\n        d['else'] = self.start_handling_includes\n", "label": 0}
{"function": "\n\ndef test_level(self):\n    key = EncryptionKey(data='', level='SL3')\n    self.assertEquals('SL3', key.level)\n", "label": 0}
{"function": "\n\ndef __setitem__(self, key, value):\n    'Dictionary style assignment.'\n    (rval, cval) = self.value_encode(value)\n    self.__set(key, rval, cval)\n", "label": 0}
{"function": "\n\n@sig((((H / ((H / 'a') >> bool)) >> ['a']) >> [int]))\ndef findIndicies(f, xs):\n    '\\n    findIndices :: (a -> Bool) -> [a] -> [Int]\\n\\n    The findIndices function extends findIndex, by returning the indices of all\\n    elements satisfying the predicate, in ascending order.\\n    '\n    return L[(i for (i, x) in enumerate(xs) if f(x))]\n", "label": 0}
{"function": "\n\ndef __init__(self, mediator=None):\n    'Initializes the scanner object.\\n\\n    Args:\\n      mediator: a volume scanner mediator (instance of\\n                VolumeScannerMediator) or None.\\n    '\n    super(VolumeScanner, self).__init__()\n    self._mediator = mediator\n    self._source_path = None\n    self._source_scanner = source_scanner.SourceScanner()\n    self._source_type = None\n    self._vss_stores = None\n", "label": 0}
{"function": "\n\ndef get_all_active_nodes(self, is_running=None):\n    if self.active_gen_id:\n        return self.get_all_nodes(self.active_gen_id, is_running=is_running)\n    return []\n", "label": 0}
{"function": "\n\ndef test_basic_start(self):\n    configjson = self.experiment.do_start_experiment()\n    self.assertIsNotNone(configjson)\n", "label": 0}
{"function": "\n\ndef test_options_disallowed(self):\n    request = factory.options('/', HTTP_AUTHORIZATION=self.disallowed_credentials)\n    response = root_view(request, pk='1')\n    self.assertEqual(response.status_code, status.HTTP_200_OK)\n    self.assertNotIn('actions', response.data)\n    request = factory.options('/1', HTTP_AUTHORIZATION=self.disallowed_credentials)\n    response = instance_view(request, pk='1')\n    self.assertEqual(response.status_code, status.HTTP_200_OK)\n    self.assertNotIn('actions', response.data)\n", "label": 0}
{"function": "\n\ndef test__build_key_none(self):\n    result = filecache._build_key(None, None)\n    self.assertEqual('None:None', result)\n", "label": 0}
{"function": "\n\ndef _option_required(self, key):\n    conf = S3_CONF.copy()\n    del conf[key]\n    try:\n        self.store = Store(test_utils.TestConfigOpts(conf))\n        return (self.store.add == self.store.add_disabled)\n    except:\n        return False\n    return False\n", "label": 0}
{"function": "\n\ndef show_analyzer_list_panel(self, callback):\n    list_panel = AnalyzerListPanel(self.window, self.client, self.settings.index)\n    list_panel.show(callback)\n", "label": 0}
{"function": "\n\ndef clone_settings(self, original):\n    self.replace_tabs_by_spaces = original.replace_tabs_by_spaces\n    self.safe_save = original.replace_tabs_by_spaces\n    self.clean_trailing_whitespaces = original.clean_trailing_whitespaces\n    self.restore_cursor = original.restore_cursor\n", "label": 0}
{"function": "\n\ndef _is_verified_address(self, address):\n    if (address in self.addresses):\n        return True\n    (user, host) = address.split('@', 1)\n    return (host in self.domains)\n", "label": 0}
{"function": "\n\ndef deserialize(self, raw_value):\n    if (raw_value.upper() in self.TRUE_RAW_VALUES):\n        return True\n    elif (raw_value.upper() in self.FALSE_RAW_VALUES):\n        return False\n    else:\n        raise DeserializationError('Value \"{}\" must be one of {} for \"{}\"!'.format(raw_value, self.ALLOWED_RAW_VALUES, self.name), raw_value, self.name)\n", "label": 0}
{"function": "\n\ndef _create_service_command(self, name, command):\n    command_table = self.create_command_table(command.get('subcommands', {\n        \n    }), self._create_operation_command)\n    service_command = ServiceCommand(name, None)\n    service_command._service_model = {\n        \n    }\n    service_command._command_table = command_table\n    return service_command\n", "label": 0}
{"function": "\n\n@override_settings(DEFAULT_FROM_EMAIL='foo@bar.com')\ndef test_sender_from_email(self):\n    '\\n        Should use DEFAULT_FROM_EMAIL instead of the default\\n        '\n\n    class SiteMock():\n        name = 'foo'\n        domain = 'bar.com'\n\n    def monkey_get_current_site(*args, **kw):\n        return SiteMock\n\n    def monkey_render_to_string(*args, **kw):\n        return 'email body'\n    req = RequestFactory().get('/')\n    token = 'token'\n    subject = SiteMock.name\n    template_name = 'template.html'\n    context = {\n        'user_id': self.user.pk,\n        'token': token,\n    }\n    (org_site, email.get_current_site) = (email.get_current_site, monkey_get_current_site)\n    (org_render_to_string, email.render_to_string) = (email.render_to_string, monkey_render_to_string)\n    try:\n        sender(req, subject, template_name, context, [self.user.email])\n    finally:\n        email.get_current_site = org_site\n        email.render_to_string = org_render_to_string\n    self.assertEquals(len(mail.outbox), 1)\n    self.assertEquals(mail.outbox[0].from_email, 'foo@bar.com')\n", "label": 0}
{"function": "\n\ndef extract_info(self, body):\n    '\\n        Extract metadata url\\n        '\n    xhr_url_match = re.search(self._XHR_REQUEST_PATH, body)\n    if (xhr_url_match is not None):\n        xhr_url = xhr_url_match.group(1)\n    else:\n        xhr_url = None\n    if ((xhr_url is not None) and xhr_url.endswith('xml')):\n        default_filename = xhr_url.split('/')[1]\n    else:\n        self.error(ExtractionError, \"ERROR: can't get default_filename.\")\n    return {\n        'default_filename': default_filename,\n        'xhr_url': xhr_url,\n    }\n", "label": 0}
{"function": "\n\ndef test_content_length_0(self):\n\n    class ContentLengthChecker(list):\n\n        def __init__(self):\n            list.__init__(self)\n            self.content_length = None\n\n        def append(self, item):\n            kv = item.split(b':', 1)\n            if ((len(kv) > 1) and (kv[0].lower() == b'content-length')):\n                self.content_length = kv[1].strip()\n            list.append(self, item)\n    conn = client.HTTPConnection('example.com')\n    conn.sock = FakeSocket(None)\n    conn._buffer = ContentLengthChecker()\n    conn.request('POST', '/', '')\n    self.assertEqual(conn._buffer.content_length, b'0', 'Header Content-Length not set')\n    conn = client.HTTPConnection('example.com')\n    conn.sock = FakeSocket(None)\n    conn._buffer = ContentLengthChecker()\n    conn.request('PUT', '/', '')\n    self.assertEqual(conn._buffer.content_length, b'0', 'Header Content-Length not set')\n", "label": 0}
{"function": "\n\ndef WriteScanNode(self, scan_node, indentation=''):\n    'Writes the source scanner node to stdout.\\n\\n    Args:\\n      scan_node: the scan node (instance of SourceScanNode).\\n      indentation: optional indentation string.\\n      scan_step: optional integer indicating the scan step.\\n    '\n    if (not scan_node):\n        return\n    values = []\n    part_index = getattr(scan_node.path_spec, 'part_index', None)\n    if (part_index is not None):\n        values.append('{0:d}'.format(part_index))\n    store_index = getattr(scan_node.path_spec, 'store_index', None)\n    if (store_index is not None):\n        values.append('{0:d}'.format(store_index))\n    start_offset = getattr(scan_node.path_spec, 'start_offset', None)\n    if (start_offset is not None):\n        values.append('start offset: {0:d} (0x{0:08x})'.format(start_offset))\n    location = getattr(scan_node.path_spec, 'location', None)\n    if (location is not None):\n        values.append('location: {0:s}'.format(location))\n    print('{0:s}{1:s}: {2:s}'.format(indentation, scan_node.path_spec.type_indicator, ', '.join(values)))\n    indentation = '  {0:s}'.format(indentation)\n    for sub_scan_node in scan_node.sub_nodes:\n        self.WriteScanNode(sub_scan_node, indentation=indentation)\n", "label": 1}
{"function": "\n\ndef deserialize(self, obj):\n    return datetime.datetime.strptime(obj, self.format).time()\n", "label": 0}
{"function": "\n\ndef seek(self, offset, whence=os.SEEK_SET):\n    'Seek to the provided location in the file.\\n\\n        :param offset: location to seek to\\n        :type offset: int\\n        :param whence: determines whether `offset` represents a\\n                       location that is absolute, relative to the\\n                       beginning of the file, or relative to the end\\n                       of the file\\n        :type whence: os.SEEK_SET | os.SEEK_CUR | os.SEEK_END\\n        :returns: None\\n        :rtype: None\\n        '\n    if (whence == os.SEEK_SET):\n        self._cursor = (0 + offset)\n    elif (whence == os.SEEK_CUR):\n        self._cursor += offset\n    elif (whence == os.SEEK_END):\n        self._cursor = (self.size() + offset)\n    else:\n        raise ValueError('Unexpected value for `whence`: {}'.format(whence))\n", "label": 0}
{"function": "\n\ndef _eval_is_infinite(self):\n    if any((a.is_infinite for a in self.args)):\n        if any((a.is_zero for a in self.args)):\n            return S.NaN.is_infinite\n        if any(((a.is_zero is None) for a in self.args)):\n            return None\n        return True\n", "label": 1}
{"function": "\n\ndef process(self, value):\n    return format_date_time_sqlite(value)\n", "label": 0}
{"function": "\n\ndef populate_link(self, finder, upgrade):\n    'Ensure that if a link can be found for this, that it is found.\\n\\n        Note that self.link may still be None - if Upgrade is False and the\\n        requirement is already installed.\\n        '\n    if (self.link is None):\n        self.link = finder.find_requirement(self, upgrade)\n", "label": 0}
{"function": "\n\ndef get_list_display_links(self, request, list_display):\n    '\\n        Return a sequence containing the fields to be displayed as links\\n        on the changelist. The list_display parameter is the list of fields\\n        returned by get_list_display().\\n        '\n    if (self.list_display_links or (self.list_display_links is None) or (not list_display)):\n        return self.list_display_links\n    else:\n        return list(list_display)[:1]\n", "label": 0}
{"function": "\n\ndef __init__(self, canvas, pf, config):\n    super(Win32CanvasConfig, self).__init__(canvas, config)\n    self._pf = pf\n    self._pfd = PIXELFORMATDESCRIPTOR()\n    _gdi32.DescribePixelFormat(canvas.hdc, self._pf, sizeof(PIXELFORMATDESCRIPTOR), byref(self._pfd))\n    self.double_buffer = bool((self._pfd.dwFlags & PFD_DOUBLEBUFFER))\n    self.sample_buffers = 0\n    self.samples = 0\n    self.stereo = bool((self._pfd.dwFlags & PFD_STEREO))\n    self.buffer_size = self._pfd.cColorBits\n    self.red_size = self._pfd.cRedBits\n    self.green_size = self._pfd.cGreenBits\n    self.blue_size = self._pfd.cBlueBits\n    self.alpha_size = self._pfd.cAlphaBits\n    self.accum_red_size = self._pfd.cAccumRedBits\n    self.accum_green_size = self._pfd.cAccumGreenBits\n    self.accum_blue_size = self._pfd.cAccumBlueBits\n    self.accum_alpha_size = self._pfd.cAccumAlphaBits\n    self.depth_size = self._pfd.cDepthBits\n    self.stencil_size = self._pfd.cStencilBits\n    self.aux_buffers = self._pfd.cAuxBuffers\n", "label": 0}
{"function": "\n\ndef merge_bins(distribution, limit):\n    'Merges the bins of a regression distribution to the given limit number\\n\\n    '\n    length = len(distribution)\n    if ((limit < 1) or (length <= limit) or (length < 2)):\n        return distribution\n    index_to_merge = 2\n    shortest = float('inf')\n    for index in range(1, length):\n        distance = (distribution[index][0] - distribution[(index - 1)][0])\n        if (distance < shortest):\n            shortest = distance\n            index_to_merge = index\n    new_distribution = distribution[:(index_to_merge - 1)]\n    left = distribution[(index_to_merge - 1)]\n    right = distribution[index_to_merge]\n    new_bin = [(((left[0] * left[1]) + (right[0] * right[1])) / (left[1] + right[1])), (left[1] + right[1])]\n    new_distribution.append(new_bin)\n    if (index_to_merge < (length - 1)):\n        new_distribution.extend(distribution[(index_to_merge + 1):])\n    return merge_bins(new_distribution, limit)\n", "label": 1}
{"function": "\n\ndef __set_db_what(self, what):\n    self._db_what = what\n    self.is_dirty = True\n", "label": 0}
{"function": "\n\ndef extendMarkdown(self, md, md_globals):\n    ' Replace subscript with SubscriptPattern '\n    md.inlinePatterns.add('subscript', SimpleTagPattern(SUBSCRIPT_RE, 'sub'), '<not_strong')\n", "label": 0}
{"function": "\n\ndef _services_dns_createRecord_php_WITH_EXTRA_PARAMS(self, method, url, body, headers):\n    body = self.fixtures.load('create_record_WITH_EXTRA_PARAMS.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 0}
{"function": "\n\n@defer.inlineCallbacks\ndef test_GetMoreCount(self):\n    counter = _CallCounter(MongoClientProtocol.send_GETMORE)\n    self.patch(MongoClientProtocol, 'send_GETMORE', counter)\n    (yield self.coll.insert([{\n        'x': 42,\n    } for _ in range(20)]))\n    result = (yield self.coll.find({\n        \n    }, limit=10))\n    self.assertEqual(len(result), 10)\n    self.assertEqual(counter.call_count, 0)\n", "label": 0}
{"function": "\n\ndef print_selection(self, *e):\n    if (self._root is None):\n        return\n    if (self._selection is None):\n        tkinter.messagebox.showerror('Print Error', 'No tree selected')\n    else:\n        c = self._cframe.canvas()\n        for widget in self._treewidgets:\n            if (widget is not self._selection):\n                self._cframe.destroy_widget(widget)\n        c.delete(self._selectbox)\n        (x1, y1, x2, y2) = self._selection.bbox()\n        self._selection.move((10 - x1), (10 - y1))\n        c['scrollregion'] = ('0 0 %s %s' % (((x2 - x1) + 20), ((y2 - y1) + 20)))\n        self._cframe.print_to_file()\n        self._treewidgets = [self._selection]\n        self.clear()\n        self.update()\n", "label": 0}
{"function": "\n\ndef smart_split(text):\n    '\\n    Generator that splits a string by spaces, leaving quoted phrases together.\\n    Supports both single and double quotes, and supports escaping quotes with\\n    backslashes. In the output, strings will keep their initial and trailing\\n    quote marks and escaped quotes will remain escaped (the results can then\\n    be further processed with unescape_string_literal()).\\n\\n    >>> list(smart_split(r\\'This is \"a person\\\\\\'s\" test.\\'))\\n    [\\'This\\', \\'is\\', \\'\"a person\\\\\\\\\\\\\\'s\"\\', \\'test.\\']\\n    >>> list(smart_split(r\"Another \\'person\\\\\\'s\\' test.\"))\\n    [\\'Another\\', \"\\'person\\\\\\\\\\'s\\'\", \\'test.\\']\\n    >>> list(smart_split(r\\'A \"\\\\\"funky\\\\\" style\" test.\\'))\\n    [\\'A\\', \\'\"\\\\\\\\\"funky\\\\\\\\\" style\"\\', \\'test.\\']\\n    '\n    text = force_text(text)\n    for bit in smart_split_re.finditer(text):\n        (yield bit.group(0))\n", "label": 0}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    return shapes\n", "label": 0}
{"function": "\n\ndef put(self, key, value):\n    ' Updates or inserts data for a specified key '\n    url = ((self.base_url + '/') + str(key))\n    headers = {\n        'content-type': 'application/json',\n    }\n    jvalue = jsonpickle.encode(value)\n    data = self.session.put(url, data=jvalue, headers=headers)\n    logging.debug(('Sending request to ' + url))\n    if (data.status_code == 200):\n        logging.debug(((('The value ' + str(value)) + ' was put in the region for the key ') + str(key)))\n        return True\n    else:\n        self.error_response(data)\n", "label": 0}
{"function": "\n\ndef utcoffset(self, dt):\n    'Returns minutesEast from the constructor, as a datetime.timedelta.'\n    return self.offset\n", "label": 0}
{"function": "\n\ndef tearDown(self):\n    for fname in os.listdir(self.tempdir):\n        os.remove(os.path.join(self.tempdir, fname))\n    os.rmdir(self.tempdir)\n", "label": 0}
{"function": "\n\n@property\ndef responses(self):\n    return [response for (request, response) in self.data]\n", "label": 0}
{"function": "\n\ndef _delegate_getter(self, object, name):\n    return getattr(self.delegate, self.name)\n", "label": 0}
{"function": "\n\ndef _process_element(self, element):\n    'Process first level element of the stream.\\n\\n        The element may be stream error or features, StartTLS\\n        request/response, SASL request/response or a stanza.\\n\\n        :Parameters:\\n            - `element`: XML element\\n        :Types:\\n            - `element`: :etree:`ElementTree.Element`\\n        '\n    tag = element.tag\n    if (tag in self._element_handlers):\n        handler = self._element_handlers[tag]\n        logger.debug('Passing element {0!r} to method {1!r}'.format(element, handler))\n        handled = handler(self, element)\n        if handled:\n            return\n    if tag.startswith(self._stanza_namespace_p):\n        stanza = stanza_factory(element, self, self.language)\n        self.uplink_receive(stanza)\n    elif (tag == ERROR_TAG):\n        error = StreamErrorElement(element)\n        self.process_stream_error(error)\n    elif (tag == FEATURES_TAG):\n        logger.debug('Got features element: {0}'.format(serialize(element)))\n        self._got_features(element)\n    else:\n        logger.debug('Unhandled element: {0}'.format(serialize(element)))\n        logger.debug(' known handlers: {0!r}'.format(self._element_handlers))\n", "label": 1}
{"function": "\n\ndef relax():\n    selection = pm.ls(sl=1)\n    if (not selection):\n        return\n    verts = pm.ls(pm.polyListComponentConversion(tv=1))\n    if (not verts):\n        return\n    shape = verts[0].node()\n    dup = shape.duplicate()[0]\n    dup_shape = dup.getShape()\n    pm.polyAverageVertex(verts, i=1, ch=0)\n    ta_node = pm.transferAttributes(dup, verts, transferPositions=True, transferNormals=False, transferUVs=False, transferColors=False, sampleSpace=0, searchMethod=0, flipUVs=False, colorBorders=1)\n    pm.delete(shape, ch=1)\n    pm.delete(dup)\n    pm.select(selection)\n", "label": 0}
{"function": "\n\ndef archive(self):\n    'Archives an experiment'\n    pipe = self.redis.pipeline(transaction=True)\n    pipe.srem(ACTIVE_EXPERIMENTS_REDIS_KEY, self.name)\n    pipe.sadd(ARCHIVED_EXPERIMENTS_REDIS_KEY, self.name)\n    pipe.execute()\n", "label": 0}
{"function": "\n\ndef mutable_total_billed_ops(self, i):\n    return self.total_billed_ops_[i]\n", "label": 0}
{"function": "\n\ndef get_diffs(self, commit):\n    return commit.parents[0].diff(commit, create_patch=True)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/vehicle/military/shared_military_c.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef test_tee_del_backward(self):\n    (forward, backward) = tee(repeat(None, 20000000))\n    any(forward)\n    del backward\n", "label": 0}
{"function": "\n\ndef test_stats(self):\n    (key, stats) = self.memcache.get_stats()[0]\n    self.assertEqual('127.0.0.1:21122 (1)', key)\n    keys = ['bytes', 'pid', 'time', 'limit_maxbytes', 'cmd_get', 'version', 'bytes_written', 'cmd_set', 'get_misses', 'total_connections', 'curr_connections', 'curr_items', 'uptime', 'get_hits', 'total_items', 'rusage_system', 'rusage_user', 'bytes_read']\n    for key in keys:\n        self.assert_(stats.has_key(key), (\"key '%s' is not in stats\" % key))\n", "label": 0}
{"function": "\n\ndef onMouseUp(self, sender, x, y):\n    (x, y) = self.adjustMousePos(x, y)\n    if self.mousehandler:\n        self.mousetarget.onBrowserEvent(DOM.eventGetCurrentEvent())\n    else:\n        self.mousetarget.onMouseUp(sender, x, y)\n", "label": 0}
{"function": "\n\ndef buildIndex(self, l):\n    index = self.mIndex()\n    for (start, end, value) in self.l:\n        index.add(start, end)\n    return index\n", "label": 0}
{"function": "\n\ndef __init__(self, uri):\n    self.client = pymongo.MongoClient(uri)\n    self.cache = {\n        \n    }\n    self.uri = uri\n", "label": 0}
{"function": "\n\ndef test_escaping(self):\n    text = '<p>Hello World!'\n    app = flask.Flask(__name__)\n\n    @app.route('/')\n    def index():\n        return flask.render_template('escaping_template.html', text=text, html=flask.Markup(text))\n    lines = app.test_client().get('/').data.splitlines()\n    self.assert_equal(lines, ['&lt;p&gt;Hello World!', '<p>Hello World!', '<p>Hello World!', '<p>Hello World!', '&lt;p&gt;Hello World!', '<p>Hello World!'])\n", "label": 0}
{"function": "\n\ndef paid_totals_for(self, year, month):\n    return self.during(year, month).filter(paid=True).aggregate(total_amount=models.Sum('amount'), total_fee=models.Sum('fee'), total_refunded=models.Sum('amount_refunded'))\n", "label": 0}
{"function": "\n\ndef visit_Assign(self, node):\n    left = self.visit(node.left)\n    right = self.visit(node.right)\n    ldelay = (self.visit(node.ldelay.value) if (node.ldelay is not None) else None)\n    rdelay = (self.visit(node.rdelay.value) if (node.rdelay is not None) else None)\n    subst = vtypes.Subst(left, right, ldelay=ldelay, rdelay=rdelay)\n    assign = vtypes.Assign(subst)\n    self.add_object(assign)\n    return assign\n", "label": 0}
{"function": "\n\ndef copy(self):\n    res = LoopType()\n    for (key, value) in self.__dict__.iteritems():\n        setattr(res, key, value)\n    return res\n", "label": 0}
{"function": "\n\ndef test_object_list_delimiter(self):\n    self.requests_mock.register_uri('GET', (FAKE_URL + '/qaz?delimiter=%7C'), json=LIST_OBJECT_RESP, status_code=200)\n    ret = self.api.object_list(container='qaz', delimiter='|')\n    self.assertEqual(LIST_OBJECT_RESP, ret)\n", "label": 0}
{"function": "\n\ndef test_list_switch_machines(self):\n    url = '/switches/2/machines'\n    return_value = self.get(url)\n    resp = json.loads(return_value.get_data())\n    count = len(resp)\n    self.assertEqual(count, 2)\n    self.assertEqual(return_value.status_code, 200)\n    url = '/switches/99/machines'\n    return_value = self.get(url)\n    self.assertEqual(return_value.status_code, 404)\n", "label": 0}
{"function": "\n\n@retry()\ndef node_list(self):\n    return [item.name() for item in self.conn.listAllDomains()]\n", "label": 0}
{"function": "\n\n@classmethod\ndef convert_json(cls, d, convert):\n    new_d = {\n        \n    }\n    for (k, v) in d.iteritems():\n        new_d[convert(k)] = (cls.convert_json(v, convert) if isinstance(v, dict) else v)\n    return new_d\n", "label": 0}
{"function": "\n\ndef add_dependency_links(self, links):\n    if self.process_dependency_links:\n        warnings.warn('Dependency Links processing has been deprecated and will be removed in a future release.', RemovedInPip8Warning)\n        self.dependency_links.extend(links)\n", "label": 0}
{"function": "\n\ndef run(self, suite):\n    filtered_test = FilterSuite(suite, self.ShouldTestRun)\n    return super(_RunnerImpl, self).run(filtered_test)\n", "label": 0}
{"function": "\n\ndef _untagged_response(self, typ, dat, name):\n    if (typ == 'NO'):\n        return (typ, dat)\n    data = self._get_untagged_response(name)\n    if (not data):\n        return (typ, [None])\n    while True:\n        dat = self._get_untagged_response(name)\n        if (not dat):\n            break\n        data += dat\n    if __debug__:\n        self._log(4, ('_untagged_response(%s, ?, %s) => %.80r' % (typ, name, data)))\n    return (typ, data)\n", "label": 1}
{"function": "\n\n@mock.patch.object(shade.OpenStackCloud, 'search_subnets')\n@mock.patch.object(shade.OpenStackCloud, 'neutron_client')\ndef test_delete_subnet_not_found(self, mock_client, mock_search):\n    mock_search.return_value = []\n    r = self.cloud.delete_subnet('goofy')\n    self.assertFalse(r)\n    self.assertFalse(mock_client.delete_subnet.called)\n", "label": 0}
{"function": "\n\ndef get_conf_from_module(mod):\n    'return configuration from module with defaults no worry about None type\\n\\n    '\n    conf = ModuleConfig(CONF_SPEC)\n    mod = _get_correct_module(mod)\n    conf.set_module(mod)\n    if hasattr(mod, 'default'):\n        default = mod.default\n        conf = extract_conf_from(default, conf)\n    else:\n        conf = extract_conf_from(mod, conf)\n    return conf\n", "label": 0}
{"function": "\n\ndef onBeforeTabSelected(self, sender, tabIndex):\n    if (self.fTabs.getWidgetCount() == 6):\n        self.fTabs.add(HTML('2nd Test.<br />Tab should be on right'), '2nd Test', name='test2')\n        return True\n    self.fTabs.remove('test2')\n    return (tabIndex != 6)\n", "label": 0}
{"function": "\n\ndef process(args):\n    conduit = phlsys_makeconduit.make_conduit(args.uri, args.user, args.cert, args.act_as_user)\n    if args.diff_id:\n        diff_id = args.diff_id\n    else:\n        d = {\n            'diff': args.raw_diff_file.read(),\n        }\n        diff_id = conduit('differential.createrawdiff', d)['id']\n    fields = {\n        \n    }\n    d = {\n        'id': args.revision_id,\n        'diffid': diff_id,\n        'fields': fields,\n        'message': args.message,\n    }\n    MessageFields = phlcon_differential.MessageFields\n    args.ccs = _get_set_or_none(args.ccs)\n    args.reviewers = _get_set_or_none(args.reviewers)\n    if args.reviewers:\n        fields[MessageFields.reviewer_phids] = args.reviewers\n    if args.ccs:\n        fields[MessageFields.cc_phids] = args.ccs\n    user_phids = phlcon_user.UserPhidCache(conduit)\n    for users in fields.itervalues():\n        user_phids.add_hint_list(users)\n    for key in fields.iterkeys():\n        fields[key] = [user_phids.get_phid(u) for u in fields[key]]\n    result = conduit('differential.updaterevision', d)\n    if args.format_id:\n        print(result['revisionid'])\n    elif args.format_url:\n        print(result['uri'])\n    else:\n        print(\"Updated revision '{rev_id}', you can view it at this URL:\\n  {url}\".format(rev_id=result['revisionid'], url=result['uri']))\n", "label": 1}
{"function": "\n\ndef test_create_handler_with_str_method_name(self):\n\n    @endpoint('/api', 'GET')\n    def fake_handler(request, *args, **kwargs):\n        pass\n    (path, handler, methods, name) = fake_handler()\n    self.assertEqual(methods, 'GET')\n", "label": 0}
{"function": "\n\ndef _prep_loader_attrs(self, mapping):\n    self.loader.source = mapping['generated_filename']\n    self.loader.election_id = mapping['election']\n    self.loader.timestamp = datetime.datetime.now()\n", "label": 0}
{"function": "\n\ndef test_has_error_type(self):\n    error = spotify.LibError(0)\n    self.assertEqual(error.error_type, 0)\n    error = spotify.LibError(1)\n    self.assertEqual(error.error_type, 1)\n", "label": 0}
{"function": "\n\n@classmethod\ndef make(cls, value, cache=None, timeout=None):\n    self = CacheKey(value)\n    self.cache = cache\n    self.timeout = timeout\n    return self\n", "label": 0}
{"function": "\n\ndef test_stdinCache_trailing_backslash_3(self):\n    stdinCache = StdinCache.StdinCache()\n    stdinCache.refreshFromText(dedentAndStrip('\\n                x+                z+                y\\n                f+                g+                h\\n                '))\n    self.assertEqual(len(stdinCache.blocks), 2)\n", "label": 0}
{"function": "\n\n@property\ndef parents(self):\n    if (self._parents is None):\n        self._parents = [self._odb.get_commit(hash) for hash in self._obj.parents]\n    return list(self._parents)\n", "label": 0}
{"function": "\n\ndef process_request(self, request):\n    if (not getattr(settings, 'MOBILE_DOMAIN', False)):\n        return\n    if ((request.COOKIES.get('ismobile', '0') == '1') or (('HTTP_USER_AGENT' in request.META) and (request.COOKIES.get('isbrowser', '0') != '1') and is_mobile(request.META['HTTP_USER_AGENT']))):\n        redirect = settings.MOBILE_DOMAIN\n        if getattr(settings, 'MOBILE_REDIRECT_PRESERVE_URL', False):\n            redirect = (redirect.rstrip('/') + request.path_info)\n        response = HttpResponseRedirect(redirect)\n        max_age = getattr(settings, 'MOBILE_COOKIE_MAX_AGE', DEFAULT_COOKIE_MAX_AGE)\n        expires_time = (time.time() + max_age)\n        expires = cookie_date(expires_time)\n        response.set_cookie('ismobile', '1', domain=settings.SESSION_COOKIE_DOMAIN, max_age=max_age, expires=expires)\n        return response\n", "label": 1}
{"function": "\n\ndef setHighlighted(self, highlighted):\n    GafferUI.PlugValueWidget.setHighlighted(self, highlighted)\n    self.__boolWidget.setHighlighted(highlighted)\n", "label": 0}
{"function": "\n\ndef gen_key(self, prefix=None):\n    if (not prefix):\n        prefix = 'python-couchbase-key_'\n    ret = '{0}{1}'.format(prefix, self._key_counter)\n    self._key_counter += 1\n    return ret\n", "label": 0}
{"function": "\n\ndef test_functions_unchanged(self):\n    s = 'def foo(): pass'\n    self.unchanged(s, from3=True)\n    s = '\\n        def foo():\\n            pass\\n            pass\\n        '\n    self.unchanged(s, from3=True)\n    s = \"\\n        def foo(bar='baz'):\\n            pass\\n            pass\\n        \"\n    self.unchanged(s, from3=True)\n", "label": 0}
{"function": "\n\ndef test_project_add_child(self):\n    project = Project()\n    child = Task()\n    project.add_child(child)\n    self.assertEquals(project, child.parent)\n    self.assertEquals(project, child.project)\n    project = Project()\n    child = Task()\n    grandchild = Task()\n    child.add_child(grandchild)\n    project.add_child(child)\n    self.assertEquals(project, child.parent)\n    self.assertEquals(project, child.project)\n    self.assertEquals(project, grandchild.project)\n", "label": 0}
{"function": "\n\ndef RegisterSubException(self, hunt_urn, plugin_name, exception):\n    self.exceptions_by_hunt.setdefault(hunt_urn, {\n        \n    }).setdefault(plugin_name, []).append(exception)\n", "label": 0}
{"function": "\n\ndef test_issue_1264(self):\n    n = 100\n    x = np.random.uniform(size=(n * 3)).reshape((n, 3))\n    expected = distance_matrix(x)\n    actual = njit(distance_matrix)(x)\n    np.testing.assert_array_almost_equal(expected, actual)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef info(msg, *args):\n    ((print >> sys.stderr), (msg % args))\n", "label": 0}
{"function": "\n\ndef Layout(self, request, response):\n    self.default = str(self.descriptor.type().Generate())\n    response = super(AES128KeyFormRenderer, self).Layout(request, response)\n    return self.CallJavascript(response, 'AES128KeyFormRenderer.Layout', prefix=self.prefix)\n", "label": 0}
{"function": "\n\ndef new(self, user, repo, title, body=''):\n    'Create a new issue.'\n    return self._posted('/'.join(['issues', 'open', user, repo]), title=title, body=body)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Building()\n    result.template = 'object/building/general/shared_rori_kobola_cave.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('building_name', 'cave')\n    return result\n", "label": 0}
{"function": "\n\ndef _str_allocation_pools(allocation_pools):\n    if isinstance(allocation_pools, str):\n        return allocation_pools\n    return '\\n'.join([('%s,%s' % (pool['start'], pool['end'])) for pool in allocation_pools])\n", "label": 0}
{"function": "\n\ndef row(self, i):\n    'Returns column i from self as a row vector.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.matrices import SparseMatrix\\n        >>> a = SparseMatrix(((1, 2), (3, 4)))\\n        >>> a.row(0)\\n        Matrix([[1, 2]])\\n\\n        See Also\\n        ========\\n        col\\n        row_list\\n        '\n    return self[i, :]\n", "label": 0}
{"function": "\n\n@patch('hc.payments.views.braintree')\ndef test_it_works(self, mock_braintree):\n    self.client.login(username='alice@example.org', password='password')\n    r = self.client.post('/pricing/cancel_plan/')\n    self.assertRedirects(r, '/pricing/')\n    self.sub.refresh_from_db()\n    self.assertEqual(self.sub.subscription_id, '')\n    self.assertEqual(self.sub.plan_id, '')\n", "label": 0}
{"function": "\n\ndef _put_n_deployments(self, id_prefix, number_of_deployments, skip_creation=None, add_modification=None):\n    for i in range(0, number_of_deployments):\n        deployment_id = '{0}{1}_{2}'.format(id_prefix, str(i), 'deployment')\n        blueprint_id = '{0}{1}_{2}'.format(id_prefix, str(i), 'blueprint')\n        if (not skip_creation):\n            self.put_deployment(deployment_id=deployment_id, blueprint_id=blueprint_id)\n        if add_modification:\n            response = self._put_deployment_modification(deployment_id=deployment_id)\n            self._mark_deployment_modification_finished(modification_id=response['id'])\n", "label": 0}
{"function": "\n\ndef _eval_rewrite_as_FallingFactorial(self, n, k):\n    if k.is_integer:\n        return (ff(n, k) / factorial(k))\n", "label": 0}
{"function": "\n\n@classmethod\ndef _parse_repo(cls, repo, name=None):\n    regexp = '(?P<type>deb(-src)?) (?P<uri>[^\\\\s]+) (?P<suite>[^\\\\s]+)( (?P<section>[\\\\w\\\\s]*))?(,(?P<priority>[\\\\d]+))?'\n    match = re.match(regexp, repo)\n    if (not match):\n        raise errors.IncorrectRepository(\"Couldn't parse repository '{0}'\".format(repo))\n    repo_type = match.group('type')\n    repo_suite = match.group('suite')\n    repo_section = match.group('section')\n    repo_uri = match.group('uri')\n    repo_priority = match.group('priority')\n    return {\n        'name': name,\n        'type': repo_type,\n        'uri': repo_uri,\n        'priority': repo_priority,\n        'suite': repo_suite,\n        'section': (repo_section or ''),\n    }\n", "label": 0}
{"function": "\n\ndef register_category(self, category, label, index=None):\n    if index:\n        self._categories.insert(index, category, label)\n    else:\n        self._categories[category] = label\n", "label": 0}
{"function": "\n\ndef get_formsets_with_inlines(self, request, obj=None):\n    if (request.is_add_view and (obj is not None)):\n        raise Exception(\"'obj' passed to get_formsets_with_inlines wasn't None during add_view\")\n    if ((not request.is_add_view) and (obj is None)):\n        raise Exception(\"'obj' passed to get_formsets_with_inlines was None during change_view\")\n    return super(GetFormsetsArgumentCheckingAdmin, self).get_formsets_with_inlines(request, obj)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    super(RequestPageTypeFormSet, self).clean()\n    cnt_rpts = 0\n    cnt_rpts_mp = 0\n    cnt_rpts_dp = 0\n    for form in self.forms:\n        if (not hasattr(form, 'cleaned_data')):\n            continue\n        data = form.cleaned_data\n        if (('DELETE' in data) and data['DELETE']):\n            continue\n        if (not ('page_type' in data)):\n            continue\n        cnt_rpts += 1\n        pt = data['page_type']\n        if (pt == 'MP'):\n            cnt_rpts_mp += 1\n        else:\n            cnt_rpts_dp += 1\n    if (cnt_rpts_mp == 0):\n        raise ValidationError('For every request page type used for scraper elems definition a RequestPageType object with a corresponding page type has to be added!')\n    if (cnt_rpts_mp > 1):\n        raise ValidationError('Only one RequestPageType object for main page requests allowed!')\n", "label": 1}
{"function": "\n\n@wrap_exception()\n@wrap_instance_event\n@wrap_instance_fault\ndef check_can_live_migrate_destination(self, ctxt, instance, block_migration, disk_over_commit):\n    'Check if it is possible to execute live migration.\\n\\n        This runs checks on the destination host, and then calls\\n        back to the source host to check the results.\\n\\n        :param context: security context\\n        :param instance: dict of instance data\\n        :param block_migration: if true, prepare for block migration\\n                                if None, calculate it in driver\\n        :param disk_over_commit: if true, allow disk over commit\\n                                 if None, ignore disk usage checking\\n        :returns: a dict containing migration info\\n        '\n    return self._do_check_can_live_migrate_destination(ctxt, instance, block_migration, disk_over_commit)\n", "label": 0}
{"function": "\n\ndef _install_associating_flows(self, security_group_id, lport):\n    self._install_associating_flow_by_direction(security_group_id, lport, 'ingress')\n    self._install_associating_flow_by_direction(security_group_id, lport, 'egress')\n", "label": 0}
{"function": "\n\ndef _argcheck(self, c):\n    self.b = (1.0 / c)\n    return (c > 0)\n", "label": 0}
{"function": "\n\ndef __init__(self, build_env, file_pairs, *args, **kwargs):\n    super(_StripCommentsRuleTask, self).__init__(build_env, *args, **kwargs)\n    self.file_pairs = file_pairs\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Tangible()\n    result.template = 'object/tangible/lair/base/shared_poi_all_lair_rock_shelter_large_fog_red.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('lair_n', 'rock_shelter')\n    return result\n", "label": 0}
{"function": "\n\n@property\ndef primary_key_names(self):\n    'Primary keys of the table\\n        '\n    return [c.name for c in self.columns() if c.primary]\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    super(OrderPayment, self).save(*args, **kwargs)\n    self._recalculate_paid()\n    if (self.currency != self.order.currency):\n        self.order.notes += ('\\n' + (_('Currency of payment %s does not match.') % self))\n        self.order.save()\n", "label": 0}
{"function": "\n\ndef __init__(self, activation, dims=None, **kwargs):\n    super(SpeechBottom, self).__init__(**kwargs)\n    self.num_features = self.input_dims['recordings']\n    if (activation is None):\n        activation = Tanh()\n    if dims:\n        child = MLP(([activation] * len(dims)), ([self.num_features] + dims), name='bottom')\n        self.output_dim = child.output_dim\n    else:\n        child = Identity(name='bottom')\n        self.output_dim = self.num_features\n    self.children.append(child)\n    self.mask = tensor.matrix('recordings_mask')\n    self.batch_inputs = {\n        'recordings': tensor.tensor3('recordings'),\n    }\n    self.single_inputs = {\n        'recordings': tensor.matrix('recordings'),\n    }\n", "label": 0}
{"function": "\n\ndef __init__(self, obfuscation_algorithm_ref=None, refanging_transform_type=None, has_changed=None, delimiter='##comma##', pattern_type=None, datatype='string', refanging_transform=None, is_case_sensitive=True, bit_mask=None, appears_random=None, observed_encoding=None, defanging_algorithm_ref=None, is_obfuscated=None, regex_syntax=None, apply_condition='ANY', trend=None, idref=None, is_defanged=None, id=None, condition=None, valueOf_=None):\n    super(RouteType, self).__init__(obfuscation_algorithm_ref, refanging_transform_type, has_changed, delimiter, pattern_type, datatype, refanging_transform, is_case_sensitive, bit_mask, appears_random, observed_encoding, defanging_algorithm_ref, is_obfuscated, regex_syntax, apply_condition, trend, idref, is_defanged, id, condition, valueOf_)\n    self.datatype = _cast(None, datatype)\n    self.valueOf_ = valueOf_\n", "label": 0}
{"function": "\n\ndef load(self):\n    session_data = self._cache.get(self.session_key)\n    if (session_data is not None):\n        return session_data\n    self.create()\n    return {\n        \n    }\n", "label": 0}
{"function": "\n\ndef test_name_and_description(self):\n    '\\n        Tests that the benefit proxy classes all return a name and\\n        description. Unfortunately, the current implementations means\\n        a valid range is required.\\n        This test became necessary because the complex name/description logic\\n        broke with the python_2_unicode_compatible decorator.\\n        '\n    range = factories.RangeFactory()\n    for (type, __) in Benefit.TYPE_CHOICES:\n        benefit = Benefit(type=type, range=range)\n        self.assertTrue(all([benefit.name, benefit.description, six.text_type(benefit)]))\n", "label": 0}
{"function": "\n\n@defer.deferredGenerator\ndef allapps_action(self, argstr):\n    \"Usage allapps: <method> [args]\\n\\n  dispatch the same command to all application managers.\\n\\n    <method>\\tmethod to invoke on all appmanagers.\\n    [args]\\toptional arguments to pass along.\\n\\n  examples:\\n\\n    ''            #shows help documentation for all applications\\n    'status'      #invoke status assumes there is only one instance\\n    'status all'  #invoke status on all application instances\\n    'status 0'    #invoke status on application instance label '0'\\n\\n  full cli usage:\\n\\n    $ droneblaster allapps\\n    $ droneblaster allapps status\\n    $ droneblaster allapps status all\\n    $ droneblaster allapps status 0\\n\"\n    result = {\n        \n    }\n    descriptions = []\n    code = 0\n    for obj in AppManager.objects:\n        try:\n            action = obj.action\n            if (not action):\n                continue\n            d = action(argstr)\n            wfd = defer.waitForDeferred(d)\n            (yield wfd)\n            foo = wfd.getResult()\n            descriptions.append(foo.get('description', 'None'))\n            code += int(foo.get('code', 0))\n        except:\n            pass\n    result['description'] = '\\n'.join(descriptions)\n    if (not result['description']):\n        result['description'] = 'None'\n    result['code'] = code\n    (yield result)\n", "label": 0}
{"function": "\n\n@pytest.fixture\ndef PhysPkg_(self, request, phys_pkg_):\n    PhysPkg_ = class_mock('opcdiag.model.PhysPkg', request)\n    PhysPkg_.read.return_value = phys_pkg_\n    return PhysPkg_\n", "label": 0}
{"function": "\n\ndef __init__(self, deployment, config):\n    super(ExistingServers, self).__init__(deployment, config)\n    self.credentials = config['credentials']\n", "label": 0}
{"function": "\n\n@test.attr(type='benchmark')\ndef test_002_fill_volume(self):\n    'Fill volume with data'\n    if (self.ctx.ssh is None):\n        raise self.skipException('Booting failed')\n    if (not self.ctx.volume_ready):\n        raise self.skipException('Volume preparation failed')\n    self._start_test()\n    self.ctx.ssh.exec_command('sudo mkdir -m 777 /vol/data')\n    file_lines = (102 * int(self.volume_size))\n    for i in xrange(int(self.volume_fill)):\n        self.ctx.ssh.exec_command((((\"cat /dev/urandom | tr -d -c 'a-zA-Z0-9' | fold -w 1020 | head -n \" + str(file_lines)) + ' > /vol/data/file') + str(i)))\n    self._end_test('Volume filling')\n    self.ctx.volume_filled = True\n    self._check_test()\n", "label": 0}
{"function": "\n\ndef _url_coerce_fn(r):\n    '\\n    :rtype: str\\n    '\n    p = urllib.parse.urlparse(r)\n    if (not p.scheme):\n        raise InvalidInput('Specify an URL scheme (e.g. http://)')\n    if (not p.netloc):\n        raise InvalidInput('Specify a domain (e.g. example.com)')\n    if (p.path and (p.path != '/')):\n        raise InvalidInput('Do not specify a path')\n    if (p.params or p.query or p.fragment):\n        raise InvalidInput('Do not leave trailing elements')\n    if (not p.path):\n        r += '/'\n    r = r.lower()\n    return r\n", "label": 1}
{"function": "\n\ndef __init__(self, device, zone_id, master, away_temp):\n    'Initialize the thermostat.'\n    self.device = device\n    self._current_temperature = None\n    self._target_temperature = None\n    self._name = 'round connected'\n    self._id = zone_id\n    self._master = master\n    self._is_dhw = False\n    self._away_temp = away_temp\n    self._away = False\n    self.update()\n", "label": 0}
{"function": "\n\ndef test_find_by_external_id():\n    app_id = 13\n    external_id = 37\n    (client, check_assertions) = check_client_method()\n    result = client.Item.find_all_by_external_id(app_id, external_id)\n    check_assertions(result, 'GET', ('/item/app/%s/v2/?external_id=%s' % (app_id, external_id)))\n", "label": 0}
{"function": "\n\ndef _reindent_stats(tokens):\n    \"Return list of (lineno, indentlevel) pairs.\\n\\n    One for each stmt and comment line. indentlevel is -1 for comment lines, as\\n    a signal that tokenize doesn't know what to do about them; indeed, they're\\n    our headache!\\n\\n    \"\n    find_stmt = 1\n    level = 0\n    stats = []\n    for t in tokens:\n        token_type = t[0]\n        sline = t[2][0]\n        line = t[4]\n        if (token_type == tokenize.NEWLINE):\n            find_stmt = 1\n        elif (token_type == tokenize.INDENT):\n            find_stmt = 1\n            level += 1\n        elif (token_type == tokenize.DEDENT):\n            find_stmt = 1\n            level -= 1\n        elif (token_type == tokenize.COMMENT):\n            if find_stmt:\n                stats.append((sline, (- 1)))\n        elif (token_type == tokenize.NL):\n            pass\n        elif find_stmt:\n            find_stmt = 0\n            if line:\n                stats.append((sline, level))\n    return stats\n", "label": 1}
{"function": "\n\n@classmethod\ndef _get_data_source_properties_from_case(cls, case_properties):\n    property_map = {\n        'closed': _('Case Closed'),\n        'user_id': _('User ID Last Updating Case'),\n        'owner_name': _('Case Owner'),\n        'mobile worker': _('Mobile Worker Last Updating Case'),\n    }\n    properties = OrderedDict()\n    for property in case_properties:\n        properties[property] = DataSourceProperty(type='case_property', id=property, column_id=get_column_name(property), text=property_map.get(property, property.replace('_', ' ')), source=property)\n    properties['computed/owner_name'] = cls._get_owner_name_pseudo_property()\n    properties['computed/user_name'] = cls._get_user_name_pseudo_property()\n    return properties\n", "label": 0}
{"function": "\n\ndef __init__(self, mu, var, **kwargs):\n    (self.mu, self.var) = (None, None)\n    if (not isinstance(mu, Layer)):\n        (self.mu, mu) = (mu, None)\n    if (not isinstance(var, Layer)):\n        (self.var, var) = (var, None)\n    input_lst = [i for i in [mu, var] if (not (i is None))]\n    super(GaussianMarginalLogDensityLayer, self).__init__(input_lst, **kwargs)\n", "label": 0}
{"function": "\n\ndef confidence(self):\n    \"\\n        Returns a tuple (chi squared, confident) of the experiment. Confident\\n        is simply a boolean specifying whether we're > 95%% sure that the\\n        results are statistically significant.\\n        \"\n    choices = self.choices\n    if (len(choices) >= 2):\n        csq = chi_squared(*choices)\n        confident = (is_confident(csq, len(choices)) if (len(choices) <= 10) else None)\n    else:\n        csq = None\n        confident = False\n    return (csq, confident)\n", "label": 0}
{"function": "\n\ndef compile_function(code, arg_names, local_dict, global_dict, module_dir, compiler='', verbose=1, support_code=None, headers=[], customize=None, type_converters=None, auto_downcast=1, **kw):\n    code = ((ndarray_api_version + '\\n') + code)\n    module_path = function_catalog.unique_module_name(code, module_dir)\n    (storage_dir, module_name) = os.path.split(module_path)\n    mod = inline_ext_module(module_name, compiler)\n    ext_func = inline_ext_function('compiled_func', code, arg_names, local_dict, global_dict, auto_downcast, type_converters=type_converters)\n    mod.add_function(ext_func)\n    if customize:\n        mod.customize = customize\n    if support_code:\n        mod.customize.add_support_code(support_code)\n    for header in headers:\n        mod.customize.add_header(header)\n    if (verbose > 0):\n        print('<weave: compiling>')\n    mod.compile(location=storage_dir, compiler=compiler, verbose=verbose, **kw)\n    try:\n        sys.path.insert(0, storage_dir)\n        exec(('import ' + module_name))\n        func = eval((module_name + '.compiled_func'))\n    finally:\n        del sys.path[0]\n    return func\n", "label": 0}
{"function": "\n\ndef _load_allowed_remote_addresses(self, app):\n    key = 'PSDASH_ALLOWED_REMOTE_ADDRESSES'\n    addrs = app.config.get(key)\n    if (not addrs):\n        return\n    if isinstance(addrs, (str, unicode)):\n        app.config[key] = [a.strip() for a in addrs.split(',')]\n", "label": 0}
{"function": "\n\ndef validate_config(self):\n    self.config.set('boss', 'data_dir', fs.abspath(self.config.get('boss', 'data_dir')))\n    if (not os.path.exists(self.config.get('boss', 'data_dir'))):\n        os.makedirs(self.config.get('boss', 'data_dir'))\n    pth = os.path.join(self.config.get('boss', 'data_dir'), 'cache')\n    if (not os.path.exists(fs.abspath(pth))):\n        os.makedirs(fs.abspath(pth))\n    self.config.set('boss', 'cache_dir', pth)\n    pth = os.path.join(self.config.get('boss', 'data_dir'), 'boss.db')\n    self.config.set('boss', 'db_path', pth)\n", "label": 0}
{"function": "\n\ndef __init__(self, notifier=None):\n    if (self.__class__.__instance is None):\n        self.__class__.__instance = self\n        if (notifier is None):\n            self._notifier = _AsyncNotifier()\n        else:\n            self._notifier = notifier\n        self._location = None\n        self._name = None\n        self._coros = {\n            \n        }\n        self._scheduled = set()\n        self._suspended = set()\n        self._timeouts = []\n        self._lock = threading.RLock()\n        self._quit = False\n        self._complete = threading.Event()\n        self._daemons = 0\n        self._polling = False\n        self._channels = {\n            \n        }\n        self._atexit = []\n        Coro._asyncoro = Channel._asyncoro = self\n        self._scheduler = threading.Thread(target=self._schedule)\n        self._scheduler.daemon = True\n        self._scheduler.start()\n        atexit.register(self.finish)\n        logger.info('version %s with %s I/O notifier', __version__, self._notifier._poller_name)\n", "label": 0}
{"function": "\n\ndef from_jsobj(jsobj, cls=None):\n    'Create an instance of the given class from a JSON object.\\n\\n    Arguments:\\n      cls: a class that serves as a \"type hint.\"\\n    '\n    if isinstance(jsobj, LIST_TYPES):\n        return [from_jsobj(o, cls=cls) for o in jsobj]\n    if (cls is not None):\n        return cls.from_jsobj(jsobj)\n    if (jsobj is None):\n        return JS_NULL\n    return jsobj\n", "label": 0}
{"function": "\n\ndef shouldDestroyCircuit(self, circuit):\n    'Return **True** iff CircuitManager thinks the calling circuit\\n        should be destroyed.\\n\\n        Circuits call shouldDestroyCircuit() when their number of open\\n        streams drops to zero. Since CircuitManager knows about all open\\n        and pending circuits, it can make an informed judgement about whether\\n        the calling circuit should be destroyed or remain open.\\n\\n        Currently, CircuitManager maintains at least 4 open or pending IPv4\\n        circuits and one open or pending IPv6 circuit. If the number of\\n        streams on any circuit drops to zero and it can be closed while still\\n        satisfying these basic constraints, then CircuitManager tells it\\n        to begin destroying itself (returns True).\\n\\n        :param oppy.circuit.circuit.Circuit circuit: circuit to\\n            consider destroying.\\n        :returns: **bool** **True** if CircuitManager decides this circuit\\n            should be destroyed, **False** otherwise.\\n        '\n    if (circuit.circuit_type == CircuitType.IPv4):\n        return ((self._totalIPv4Count() - 1) > self._min_IPv4_count)\n    else:\n        return ((self._totalIPv6Count() - 1) > self._min_IPv6_count)\n", "label": 0}
{"function": "\n\ndef contains_subsequence(seq, subseq):\n    for i in range((len(seq) - len(subseq))):\n        if (seq[i:(i + len(subseq))] == subseq):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef tables(self, db=None):\n    '\\n        Enumerates all tables fro a given database. If not specified, use the\\n        current database.\\n        '\n    if self.has_cap(TABLES_ENUM):\n        if (db is None):\n            if (self.current_db is None):\n                self.database()\n            db = self.current_db\n        n = self.get_nb_tables(db)\n        for i in range(n):\n            (yield TableWrapper(self, self.get_table_name(i, db), db))\n    else:\n        raise Unavailable()\n", "label": 0}
{"function": "\n\ndef _is_us_state(abbr, result):\n    for sep in ('/', '-'):\n        if (result.source_base == 'us{sep}{abbr}'.format(**locals())):\n            return True\n        if result.source_base.startswith('us{sep}{abbr}.'.format(**locals())):\n            return True\n        if result.source_base.startswith('us{sep}{abbr}{sep}'.format(**locals())):\n            return True\n    return False\n", "label": 0}
{"function": "\n\n@util.positional(2)\ndef error(status_code, status_message=None, content_type='text/plain; charset=utf-8', headers=None, content=None):\n    'Create WSGI application that statically serves an error page.\\n\\n  Creates a static error page specifically for non-200 HTTP responses.\\n\\n  Browsers such as Internet Explorer will display their own error pages for\\n  error content responses smaller than 512 bytes.  For this reason all responses\\n  are right-padded up to 512 bytes.\\n\\n  Error pages that are not provided will content will contain the standard HTTP\\n  status message as their content.\\n\\n  Args:\\n    status_code: Integer status code of error.\\n    status_message: Status message.\\n\\n  Returns:\\n    Static WSGI application that sends static error response.\\n  '\n    if (status_message is None):\n        status_message = httplib.responses.get(status_code, 'Unknown Error')\n    if (content is None):\n        content = status_message\n    content = util.pad_string(content)\n    return static_page(content, status=(status_code, status_message), content_type=content_type, headers=headers)\n", "label": 0}
{"function": "\n\ndef test_params(self):\n    params = np.array([r.params for r in self.results])\n    params_1 = np.array(([self.results[0].params] * len(self.results)))\n    assert_allclose(params, params_1)\n", "label": 0}
{"function": "\n\ndef usesTime(self):\n    fmt = self._fmt\n    return ((fmt.find('$asctime') >= 0) or (fmt.find(self.asctime_format) >= 0))\n", "label": 0}
{"function": "\n\ndef get_list(self, *args, **kwargs):\n    (count, data) = super(TweetView, self).get_list(*args, **kwargs)\n    query = {\n        '_id': {\n            '$in': [x['user_id'] for x in data],\n        },\n    }\n    users = db.user.find(query, fields=('name',))\n    users_map = dict(((x['_id'], x['name']) for x in users))\n    for item in data:\n        item['user_name'] = users_map.get(item['user_id'])\n    return (count, data)\n", "label": 0}
{"function": "\n\n@conf.commands.register\ndef srbt1(peer, pkts, *args, **kargs):\n    'send and receive 1 packet using a bluetooth socket'\n    (a, b) = srbt(peer, pkts, *args, **kargs)\n    if (len(a) > 0):\n        return a[0][1]\n", "label": 0}
{"function": "\n\ndef find_payload_class(payload_type):\n    'Iterate through inherited classes to find a matching class name'\n    subclasses = set()\n    work = [Payload]\n    while work:\n        parent_subclass = work.pop()\n        for child_subclass in parent_subclass.__subclasses__():\n            if (child_subclass not in subclasses):\n                if (hasattr(child_subclass, 'payload_type') and (child_subclass.payload_type == payload_type)):\n                    return child_subclass\n                subclasses.add(child_subclass)\n                work.append(child_subclass)\n    return None\n", "label": 1}
{"function": "\n\ndef parse_inline(text):\n    '\\n    Takes a string of text from a text inline and returns a 3 tuple of\\n    (name, value, **kwargs).\\n    '\n    m = INLINE_SPLITTER.match(text)\n    if (not m):\n        raise InlineUnparsableError\n    args = m.group('args')\n    name = m.group('name')\n    value = ''\n    kwtxt = ''\n    kwargs = {\n        \n    }\n    if args:\n        kwtxt = INLINE_KWARG_PARSER.search(args).group('kwargs')\n        value = re.sub(('%s\\\\Z' % kwtxt), '', args)\n        value = value.strip()\n    if m.group('variant'):\n        kwargs['variant'] = m.group('variant')\n    if kwtxt:\n        for kws in kwtxt.split():\n            (k, v) = kws.split('=')\n            kwargs[str(k)] = v\n    return (name, value, kwargs)\n", "label": 1}
{"function": "\n\ndef write(self, *args, **kwargs):\n    if (not self.file):\n        self.file = tempfile.TemporaryFile()\n    self.file.write(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _Rotate(self, image, transform):\n    'Use PIL to rotate the given image with the given transform.\\n\\n    Args:\\n      image: PIL.Image.Image object to rotate.\\n      transform: images_service_pb.Transform to use when rotating.\\n\\n    Returns:\\n      PIL.Image.Image with transforms performed on it.\\n\\n    Raises:\\n      BadRequestError if the rotate data given is bad.\\n    '\n    degrees = transform.rotate()\n    if ((degrees < 0) or ((degrees % 90) != 0)):\n        raise apiproxy_errors.ApplicationError(images_service_pb.ImagesServiceError.BAD_TRANSFORM_DATA)\n    degrees %= 360\n    degrees = (360 - degrees)\n    return image.rotate(degrees)\n", "label": 0}
{"function": "\n\ndef publish_state(self, payload, state):\n    if (not state):\n        raise Exception('Unable to publish unassigned state.')\n    self._state_publisher.publish(payload, self._state_exchange, state)\n", "label": 0}
{"function": "\n\ndef no_translate_debug_logs(logical_line, filename):\n    \"Check for 'LOG.debug(_('\\n\\n    As per our translation policy,\\n    https://wiki.openstack.org/wiki/LoggingStandards#Log_Translation\\n    we shouldn't translate debug level logs.\\n\\n    * This check assumes that 'LOG' is a logger.\\n    * Use filename so we can start enforcing this in specific folders instead\\n      of needing to do so all at once.\\n    S373\\n    \"\n    msg = \"S373 Don't translate debug level logs\"\n    if logical_line.startswith('LOG.debug(_('):\n        (yield (0, msg))\n", "label": 0}
{"function": "\n\ndef all_terms(f):\n    '\\n        Returns all terms from a univariate polynomial ``f``.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy import Poly\\n        >>> from sympy.abc import x\\n\\n        >>> Poly(x**3 + 2*x - 1, x).all_terms()\\n        [((3,), 1), ((2,), 0), ((1,), 2), ((0,), -1)]\\n\\n        '\n    return [(m, f.rep.dom.to_sympy(c)) for (m, c) in f.rep.all_terms()]\n", "label": 0}
{"function": "\n\ndef ensure_role(self, role, dry_run=False):\n    '\\n        Adds the role if it does not already exist, otherwise skips it.\\n        '\n    existing_roles = Role.objects.filter(slug=role.slug)\n    if existing_roles:\n        logger.info('Role already exists: %s', role.name)\n        return existing_roles[0]\n    elif dry_run:\n        logger.info('[DRY RUN] Creating role: %s', role.name)\n    else:\n        if self.verbose:\n            logger.info('Creating role: %s', role.name)\n        role.save()\n", "label": 0}
{"function": "\n\ndef _initPopulation(self, seeds):\n    if (self.parentChildAverage < 1):\n        for s in seeds:\n            s.parent = None\n    self.pop = self._extendPopulation(seeds, self.populationSize)\n", "label": 0}
{"function": "\n\ndef _get_task_with_policy(queue_name, task_id, owner):\n    'Fetches the specified task and enforces ownership policy.\\n\\n    Args:\\n        queue_name: Name of the queue the work item is on.\\n        task_id: ID of the task that is finished.\\n        owner: Who or what has the current lease on the task.\\n\\n    Returns:\\n        The valid WorkQueue task that is currently owned.\\n\\n    Raises:\\n        TaskDoesNotExistError if the task does not exist.\\n        LeaseExpiredError if the lease is no longer active.\\n        NotOwnerError if the specified owner no longer owns the task.\\n    '\n    now = datetime.datetime.utcnow()\n    task = WorkQueue.query.filter_by(queue_name=queue_name, task_id=task_id).with_lockmode('update').first()\n    if (not task):\n        raise TaskDoesNotExistError(('task_id=%r' % task_id))\n    lease_delta = (now - task.eta)\n    if (lease_delta > datetime.timedelta(0)):\n        db.session.rollback()\n        raise LeaseExpiredError(('queue=%r, task_id=%r expired %s' % (task.queue_name, task_id, lease_delta)))\n    if (task.last_owner != owner):\n        db.session.rollback()\n        raise NotOwnerError(('queue=%r, task_id=%r, owner=%r' % (task.queue_name, task_id, task.last_owner)))\n    return task\n", "label": 0}
{"function": "\n\ndef showSublimeContext(self, filename, line):\n    debug(((('showSublimeContext: ' + str(filename)) + ' : ') + str(line)))\n    console_output((((('@@@ Stopped at ' + str(filename.replace((self.projectDir + '/'), ''))) + ':') + str(line)) + ' @@@'))\n    window = sublime.active_window()\n    if window:\n        window.focus_group(0)\n        view = window.active_view()\n        if ((view is not None) and (view.size() >= 0)):\n            filename = os.path.join(self.projectDir, filename)\n            if (view.file_name() != filename):\n                self.activateViewWithFile(filename, line)\n            window.run_command('goto_line', {\n                'line': line,\n            })\n            view = window.active_view()\n            mark = [view.line(view.text_point((line - 1), 0))]\n            view.erase_regions('current_line')\n            view.add_regions('current_line', mark, 'current_line', 'dot', sublime.DRAW_OUTLINED)\n        else:\n            debug('No current view')\n", "label": 0}
{"function": "\n\ndef do_command(self, verb, args):\n    conn = http_client.HTTPConnection(self.host, self.port, timeout=self.http_timeout)\n    try:\n        body = ('cmd=' + urllib_parse.quote_plus(unicode(verb).encode('utf-8')))\n        for i in range(len(args)):\n            body += ((('&' + unicode((i + 1))) + '=') + urllib_parse.quote_plus(unicode(args[i]).encode('utf-8')))\n        if (None != self.sessionId):\n            body += ('&sessionId=' + unicode(self.sessionId))\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8',\n        }\n        conn.request('POST', '/selenium-server/driver/', body, headers)\n        response = conn.getresponse()\n        data = unicode(response.read(), 'UTF-8')\n        if (not data.startswith('OK')):\n            raise Exception(data)\n        return data\n    finally:\n        conn.close()\n", "label": 0}
{"function": "\n\ndef fdiff(self, argindex=1):\n    (z, m) = self.args\n    fm = sqrt((1 - (m * (sin(z) ** 2))))\n    if (argindex == 1):\n        return (1 / fm)\n    elif (argindex == 2):\n        return (((elliptic_e(z, m) / ((2 * m) * (1 - m))) - (elliptic_f(z, m) / (2 * m))) - (sin((2 * z)) / ((4 * (1 - m)) * fm)))\n    raise ArgumentIndexError(self, argindex)\n", "label": 0}
{"function": "\n\ndef get_default_machine(self):\n    ' Reads the default machine from the package configuration\\n        \\n        '\n    server = username = port = password = ''\n    if configuration.check('server'):\n        server = configuration.server\n    if (not server):\n        return None\n    if configuration.check('username'):\n        username = configuration.username\n    if (not username):\n        username = current_user()\n    if (configuration.check('port') is not None):\n        port = configuration.port\n    if configuration.check('password'):\n        password = configuration.password\n    self.annotate({\n        'RemoteQ-server': server,\n        'RemoteQ-username': username,\n        'RemoteQ-port': port,\n    })\n    return (server, port, username, password)\n", "label": 1}
{"function": "\n\ndef get_all_hosts(self):\n    '\\n            Get list of all hosts in cluster\\n            Args:\\n                None\\n            Return:\\n                list of hostnames\\n            Raise:\\n                None\\n       '\n    zook = self.zk_client\n    broker_id_path = self.zk_paths[BROKER_IDS]\n    if zook.exists(broker_id_path):\n        broker_ids = zook.get_children(broker_id_path)\n        brokers = []\n        for broker_id in broker_ids:\n            brokers.append(self.get_host(broker_id))\n        return brokers\n", "label": 0}
{"function": "\n\ndef assemble(self):\n    assembled = {\n        self.type: self.body,\n    }\n    if self.aggregations:\n        assembled['aggs'] = {\n            \n        }\n        for agg in self.aggregations:\n            assembled['aggs'][agg.name] = agg.assemble()\n    return assembled\n", "label": 0}
{"function": "\n\ndef _ml_train_iterative(self, database_matrix, params=[], sliding_window=168, k=1):\n    '\\n            Training method used by Fred 09 paper.\\n        '\n    p = (database_matrix.shape[1] - 1)\n    number_iterations = ((((database_matrix.shape[0] + p) - k) - sliding_window) + 1)\n    print('Number of iterations: ', number_iterations)\n    tr_size = ((sliding_window - p) - 1)\n    z = (database_matrix[0:(- k), 1].reshape((- 1), 1) * np.ones((1, p)))\n    database_matrix[k:, 1:] = (database_matrix[k:, 1:] - z)\n    pr_target = []\n    ex_target = []\n    for i in range(number_iterations):\n        self._ml_train(database_matrix[(k + i):(((k + i) + tr_size) - 1), :], params)\n        pr_t = self._ml_predict(horizon=1)\n        pr_t = (pr_t[0][0] + z[(i, 0)])\n        pr_target.append(pr_t)\n        ex_target.append(database_matrix[(((k + i) + tr_size), 0)])\n    pr_result = Error(expected=ex_target, predicted=pr_target)\n    return pr_result\n", "label": 0}
{"function": "\n\ndef get(self, filepath, version=None, mode='r'):\n    'Returns a bytestring with the file content, but no metadata.'\n    file_stream = self.open(filepath, version=version, mode=mode)\n    if (file_stream is None):\n        raise IOError(('File %s (version %s) not found.' % (filepath, (version if version else 'latest'))))\n    return file_stream.read()\n", "label": 0}
{"function": "\n\ndef read_bytesmap(f):\n    numpairs = read_short(f)\n    bytesmap = {\n        \n    }\n    for _ in range(numpairs):\n        k = read_string(f)\n        bytesmap[k] = read_value(f)\n    return bytesmap\n", "label": 0}
{"function": "\n\ndef clearAlert(self):\n    ' Clear the current alert level, if any.\\n\\n        '\n    if (self._alert_data is not None):\n        self._alert_data.timer.stop()\n        self._alert_data = None\n        app = QApplication.instance()\n        app.focusChanged.disconnect(self._onAppFocusChanged)\n        self.alerted.emit('')\n", "label": 0}
{"function": "\n\ndef upload_form(self):\n    '\\n            Instantiate file upload form and return it.\\n\\n            Override to implement custom behavior.\\n        '\n    upload_form_class = self.get_upload_form()\n    if request.form:\n        formdata = request.form.copy()\n        formdata.update(request.files)\n        return upload_form_class(formdata, admin=self)\n    elif request.files:\n        return upload_form_class(request.files, admin=self)\n    else:\n        return upload_form_class(admin=self)\n", "label": 0}
{"function": "\n\ndef downcaseTokens(s, l, t):\n    'Helper parse action to convert tokens to lower case.'\n    return [tt.lower() for tt in map(_ustr, t)]\n", "label": 0}
{"function": "\n\n@skipUnless(cache_file_exists('ia', '20101102__ia__general__poweshiek__precinct.xls'), CACHED_FILE_MISSING_MSG)\ndef test_results(self):\n    filename = '20101102__ia__general__poweshiek__precinct.xls'\n    mapping = self._get_mapping(filename)\n    self._prep_loader_attrs(mapping)\n    results = self.loader._results(mapping)\n    montezuma_abs_results = [r for r in results if ((r.jurisdiction == 'Montezuma') and (r.votes_type == 'absentee'))]\n    self.assertEqual(len(montezuma_abs_results), 34)\n    result = montezuma_abs_results[0]\n    self.assertEqual(result.office, 'United States Senator')\n    self.assertEqual(result.district, None)\n    self.assertEqual(result.full_name, 'Roxanne Conlin')\n    self.assertEqual(result.party, 'DEM')\n    self.assertEqual(result.write_in, None)\n    self.assertEqual(result.votes, 59)\n    result = montezuma_abs_results[(- 1)]\n    self.assertEqual(result.office, 'State Rep')\n    self.assertEqual(result.district, '75')\n    self.assertEqual(result.full_name, 'Write-In')\n    self.assertEqual(result.party, None)\n    self.assertEqual(result.write_in, 'Write-In')\n    self.assertEqual(result.votes, 0)\n", "label": 0}
{"function": "\n\ndef _as_vector(self, keep_channels=False):\n    '\\n        The vectorized form of this image.\\n\\n        Parameters\\n        ----------\\n        keep_channels : `bool`, optional\\n\\n            ========== =============================\\n            Value      Return shape\\n            ========== =============================\\n            `False`    ``(n_channels * n_pixels,)``\\n            `True`     ``(n_channels, n_pixels)``\\n            ========== =============================\\n\\n        Returns\\n        -------\\n        vec : (See ``keep_channels`` above) `ndarray`\\n            Flattened representation of this image, containing all pixel\\n            and channel information.\\n        '\n    if keep_channels:\n        return self.pixels.reshape([self.n_channels, (- 1)])\n    else:\n        return self.pixels.ravel()\n", "label": 0}
{"function": "\n\ndef show(mousetarget, **kwargs):\n    global mousecapturer\n    mc = getMouseCapturer(**kwargs)\n    mc.mousetarget = mousetarget\n    if isinstance(mousetarget, MouseHandler):\n        mc.mousehandler = True\n    mc.show()\n", "label": 0}
{"function": "\n\ndef write(self, bytes):\n    '\\n        Write C{bytes} to the underlying consumer unless\\n        C{_noMoreWritesExpected} has been called or there are/have been too\\n        many bytes.\\n        '\n    if (self._finished is None):\n        self._producer.stopProducing()\n        raise ExcessWrite()\n    if (len(bytes) <= self._length):\n        self._length -= len(bytes)\n        self._consumer.write(bytes)\n    else:\n        _callAppFunction(self._producer.stopProducing)\n        self._finished.errback(WrongBodyLength('too many bytes written'))\n        self._allowNoMoreWrites()\n", "label": 0}
{"function": "\n\ndef update_fpointer(self, nid, mode=ADD):\n    'set _fpointer recursively'\n    if (nid is None):\n        return\n    if (mode is self.ADD):\n        self._fpointer.append(nid)\n    elif (mode is self.DELETE):\n        if (nid in self._fpointer):\n            self._fpointer.remove(nid)\n    elif (mode is self.INSERT):\n        print('WARNNING: INSERT is deprecated to ADD mode')\n        self.update_fpointer(nid)\n", "label": 1}
{"function": "\n\ndef __init__(self):\n    super().__init__()\n    db_host = os.environ.get('MONGO_HOST')\n    db_host = (db_host if db_host else 'localhost')\n    db_port = int(os.environ.get('MONGO_PORT'))\n    db_port = (db_port if db_port else 27017)\n    db_name = os.environ.get('MONGO_DB')\n    db_name = (db_name if db_name else 'default')\n    db_bucket = os.environ.get('MONGO_BUCKET')\n    db_bucket = (db_bucket if db_bucket else 'rxnorm')\n    import pymongo\n    conn = pymongo.MongoClient(host=db_host, port=db_port)\n    db = conn[db_name]\n    db_user = os.environ.get('MONGO_USER')\n    db_pass = os.environ.get('MONGO_PASS')\n    if (db_user and db_pass):\n        db.authenticate(db_user, db_pass)\n    self.mng = db[db_bucket]\n    self.mng.ensure_index('ndc')\n    self.mng.ensure_index('label', text=pymongo.TEXT)\n", "label": 1}
{"function": "\n\ndef __init__(self, **kwargs):\n    super(discreteBarChart, self).__init__(**kwargs)\n    self.model = 'discreteBarChart'\n    height = kwargs.get('height', 450)\n    width = kwargs.get('width', None)\n    if kwargs.get('x_is_date', False):\n        self.set_date_flag(True)\n        self.create_x_axis('xAxis', format=kwargs.get('x_axis_format', '%d %b %Y %H %S'), date=True)\n    else:\n        self.create_x_axis('xAxis', format=None)\n    self.create_y_axis('yAxis', format=kwargs.get('y_axis_format', '.0f'))\n    self.set_custom_tooltip_flag(True)\n    self.set_graph_height(height)\n    if width:\n        self.set_graph_width(width)\n    tooltips = kwargs.get('tooltips', True)\n    if (not tooltips):\n        self.chart_attr = {\n            'tooltips': 'false',\n        }\n", "label": 0}
{"function": "\n\n@access.public\ndef describeResource(self, resource, params):\n    if (resource not in docs.routes):\n        raise RestException(('Invalid resource: %s' % resource))\n    return {\n        'apiVersion': API_VERSION,\n        'swaggerVersion': SWAGGER_VERSION,\n        'basePath': getApiUrl(),\n        'models': dict(docs.models[resource], **docs.models[None]),\n        'apis': [{\n            'path': route,\n            'operations': sorted(op, key=functools.cmp_to_key(self._compareOperations)),\n        } for (route, op) in sorted(six.viewitems(docs.routes[resource]), key=functools.cmp_to_key(self._compareRoutes))],\n    }\n", "label": 0}
{"function": "\n\ndef _es_down_template(request, *args, **kwargs):\n    'Returns the appropriate \"Elasticsearch is down!\" template'\n    return ('search/mobile/down.html' if request.MOBILE else 'search/down.html')\n", "label": 0}
{"function": "\n\ndef __isub__(self, val):\n    if (type(val) in (int, float)):\n        self.x -= val\n        self.y -= val\n    else:\n        self.x -= val.x\n        self.y -= val.y\n    return self\n", "label": 0}
{"function": "\n\ndef reserve_provider_segment(self, session, segment):\n    filters = {\n        \n    }\n    physical_network = segment.get(api.PHYSICAL_NETWORK)\n    if (physical_network is not None):\n        filters['physical_network'] = physical_network\n        vlan_id = segment.get(api.SEGMENTATION_ID)\n        if (vlan_id is not None):\n            filters['vlan_id'] = vlan_id\n    if self.is_partial_segment(segment):\n        alloc = self.allocate_partially_specified_segment(session, **filters)\n        if (not alloc):\n            raise exc.NoNetworkAvailable()\n    else:\n        alloc = self.allocate_fully_specified_segment(session, **filters)\n        if (not alloc):\n            raise exc.VlanIdInUse(**filters)\n    return {\n        api.NETWORK_TYPE: p_const.TYPE_VLAN,\n        api.PHYSICAL_NETWORK: alloc.physical_network,\n        api.SEGMENTATION_ID: alloc.vlan_id,\n        api.MTU: self.get_mtu(alloc.physical_network),\n    }\n", "label": 1}
{"function": "\n\ndef _match_rhs(self, rhs, rightmost_stack):\n    \"\\n        :rtype: bool\\n        :return: true if the right hand side of a CFG production\\n            matches the rightmost elements of the stack.  ``rhs``\\n            matches ``rightmost_stack`` if they are the same length,\\n            and each element of ``rhs`` matches the corresponding\\n            element of ``rightmost_stack``.  A nonterminal element of\\n            ``rhs`` matches any Tree whose node value is equal\\n            to the nonterminal's symbol.  A terminal element of ``rhs``\\n            matches any string whose type is equal to the terminal.\\n        :type rhs: list(terminal and Nonterminal)\\n        :param rhs: The right hand side of a CFG production.\\n        :type rightmost_stack: list(string and Tree)\\n        :param rightmost_stack: The rightmost elements of the parser's\\n            stack.\\n        \"\n    if (len(rightmost_stack) != len(rhs)):\n        return False\n    for i in range(len(rightmost_stack)):\n        if isinstance(rightmost_stack[i], Tree):\n            if (not isinstance(rhs[i], Nonterminal)):\n                return False\n            if (rightmost_stack[i].label() != rhs[i].symbol()):\n                return False\n        else:\n            if isinstance(rhs[i], Nonterminal):\n                return False\n            if (rightmost_stack[i] != rhs[i]):\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef put(self):\n    pet = self.json_args\n    if (not isinstance(pet['id'], int)):\n        self.set_status(400)\n    if (not self.db.update_(**pet)):\n        self.set_status(404)\n    else:\n        self.set_status(200)\n    self.finish()\n", "label": 0}
{"function": "\n\ndef get_response(self, cmd, fid, *args):\n    for source in self.select_best_source(fid.decode()):\n        dealer = None\n        try:\n            dealer = self.context.socket(zmq.DEALER)\n            dealer.connect(get_events_uri(self.session, source, 'router'))\n            dealer.send_multipart(((cmd, fid) + args))\n            response = dealer.recv_multipart()\n            if ((not response) or (response[0] == ERROR)):\n                self.logger.debug('Error with source {}', source)\n                continue\n            return response\n        finally:\n            if dealer:\n                dealer.close()\n    self.logger.debug('No more source available.')\n    return [ERROR]\n", "label": 0}
{"function": "\n\ndef httpapi(self, arg, opts):\n    sc = HttpAPIStatsCollector()\n    headers = ['#Item', 'Value']\n    table = []\n    for (k, v) in sc.get().getStats().iteritems():\n        if isinstance(v, dict):\n            v = json.dumps(v)\n        row = []\n        row.append(('#%s' % k))\n        if (k[(- 3):] == '_at'):\n            row.append(formatDateTime(v))\n        else:\n            row.append(v)\n        table.append(row)\n    self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))\n", "label": 0}
{"function": "\n\ndef test_cmovpe(self):\n    asm = ['cmovpe eax, ebx']\n    ctx_init = self.__init_context()\n    (x86_ctx_out, reil_ctx_out) = self.__run_code(asm, 3735928559, ctx_init)\n    cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)\n    if (not cmp_result):\n        self.__save_failing_context(ctx_init)\n    self.assertTrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))\n", "label": 0}
{"function": "\n\ndef _find_tab(self, widget):\n    for (key, bnch) in self.tab.items():\n        if (widget == bnch.widget):\n            return bnch\n    return None\n", "label": 0}
{"function": "\n\ndef redraw(self):\n    self.data.sort(self.sortfn)\n    rows = len(self.data)\n    cols = 0\n    if (rows > 0):\n        cols = len(self.data[0])\n    self.grid.resize(rows, cols)\n    self.header.resize(1, cols)\n    cf = self.grid.getCellFormatter()\n    for (nrow, row) in enumerate(self.data):\n        for (ncol, item) in enumerate(row):\n            self.grid.setHTML(nrow, ncol, str(item))\n            cf.setWidth(nrow, ncol, '200px')\n    cf = self.header.getCellFormatter()\n    self.sortbuttons = []\n    for ncol in range(cols):\n        sb = Button(('sort col %d' % ncol))\n        sb.addClickListener(self)\n        self.header.setWidget(0, ncol, sb)\n        cf.setWidth(0, ncol, '200px')\n        self.sortbuttons.append(sb)\n", "label": 0}
{"function": "\n\ndef is_user_notified(user_id, target_id):\n    res = dbsession.query(Notify).filter((Notify.target_id == target_id))\n    res = res.filter((Notify.user_id == user_id))\n    try:\n        r = res.all()[0]\n        return r\n    except:\n        return False\n", "label": 0}
{"function": "\n\n@group_only\ndef seen(self, msg, matches):\n    chat_id = msg.dest.id\n    if (matches.group(2) is not None):\n        return self.seen_by_id(chat_id, matches.group(2))\n    elif (matches.group(3) is not None):\n        return self.seen_by_username(chat_id, matches.group(3))\n    else:\n        return self.seen_by_fullname(chat_id, matches.group(4))\n", "label": 0}
{"function": "\n\ndef updateProperties(self):\n    if (self.modelXbrl is not None):\n        modelXbrl = self.modelXbrl\n        if (modelXbrl.modelDocument.type == ModelDocument.Type.INSTANCE):\n            tbl = self.table\n            coordinates = tbl.getCurrentCellCoordinates()\n            if (coordinates is not None):\n                objId = tbl.getObjectId(coordinates)\n                if ((objId is not None) and (len(objId) > 0)):\n                    if (objId and (objId[0] == 'f')):\n                        viewableObject = self.factPrototypes[int(objId[1:])]\n                    elif (objId[0] != 'a'):\n                        viewableObject = self.modelXbrl.modelObject(objId)\n                    else:\n                        return\n                    modelXbrl.viewModelObject(viewableObject)\n", "label": 1}
{"function": "\n\ndef _destroy_kernel_ramdisk(self, instance, vm_ref):\n    'Three situations can occur:\\n\\n            1. We have neither a ramdisk nor a kernel, in which case we are a\\n               RAW image and can omit this step\\n\\n            2. We have one or the other, in which case, we should flag as an\\n               error\\n\\n            3. We have both, in which case we safely remove both the kernel\\n               and the ramdisk.\\n\\n        '\n    instance_uuid = instance['uuid']\n    if ((not instance['kernel_id']) and (not instance['ramdisk_id'])):\n        LOG.debug('Using RAW or VHD, skipping kernel and ramdisk deletion', instance=instance)\n        return\n    if (not (instance['kernel_id'] and instance['ramdisk_id'])):\n        raise exception.InstanceUnacceptable(instance_id=instance_uuid, reason=_('instance has a kernel or ramdisk but not both'))\n    (kernel, ramdisk) = vm_utils.lookup_kernel_ramdisk(self._session, vm_ref)\n    if (kernel or ramdisk):\n        vm_utils.destroy_kernel_ramdisk(self._session, instance, kernel, ramdisk)\n        LOG.debug('kernel/ramdisk files removed', instance=instance)\n", "label": 1}
{"function": "\n\ndef filter_log_files_for_zipping(log_files):\n    \"Identify unzipped log files that are approporate for zipping.\\n\\n    Each unique log type found should have the most recent log file unzipped\\n    as it's probably still in use.\\n    \"\n    out_files = []\n    for lf in filter_log_files_for_active(log_files):\n        if lf.bzip:\n            continue\n        out_files.append(lf)\n    return out_files\n", "label": 0}
{"function": "\n\ndef http_method_not_allowed(self, request, *args, **kwargs):\n    allowed_methods = [m for m in self.http_method_names if hasattr(self, m)]\n    logger.warning(('Method Not Allowed (%s): %s' % (request.method, request.path)), extra={\n        'status_code': 405,\n        'request': self.request,\n    })\n    return http.HttpResponseNotAllowed(allowed_methods)\n", "label": 0}
{"function": "\n\ndef register(self, name, c):\n    if ((name in self.registered) and (c is not self.registered[name])):\n        raise NameError('{} has been registered by {}'.format(name, self.registered[name]))\n    self.registered[name] = c\n", "label": 0}
{"function": "\n\ndef stackhut_api_call(endpoint, msg, secure=True, return_json=True):\n    url = urllib.parse.urljoin(utils.SERVER_URL, endpoint)\n    log.debug('Calling Stackhut Server at {} with \\n\\t{}'.format(url, json.dumps(msg)))\n    r = requests.post(url, data=json.dumps(msg), headers=json_header)\n    if (r.status_code == requests.codes.ok):\n        return (r.json() if return_json else r.text)\n    else:\n        log.error('Error {} talking to Stackhut Server'.format(r.status_code))\n        log.error(r.text)\n        r.raise_for_status()\n", "label": 0}
{"function": "\n\ndef IsAtEnd(self, string):\n    'Returns whether this position is at the end of the given string.\\n\\n    Args:\\n      string: The string to test for the end of.\\n\\n    Returns:\\n      Whether this position is at the end of the given string.\\n    '\n    return ((self.start == len(string)) and (self.length == 0))\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    'We want it to print as a Cycle, not as a dict.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.combinatorics import Cycle\\n        >>> Cycle(1, 2)\\n        (1 2)\\n        >>> print(_)\\n        (1 2)\\n        >>> list(Cycle(1, 2).items())\\n        [(1, 2), (2, 1)]\\n        '\n    if (not self):\n        return 'Cycle()'\n    cycles = Permutation(self).cyclic_form\n    s = ''.join((str(tuple(c)) for c in cycles))\n    big = (self.size - 1)\n    if (not any(((i == big) for c in cycles for i in c))):\n        s += ('(%s)' % big)\n    return ('Cycle%s' % s)\n", "label": 1}
{"function": "\n\ndef testConnectionLeaks(self):\n    for i in range(3):\n        self.assertEquals(self._countConnections(11211), 0)\n        new_conf = {\n            'init_config': {\n                \n            },\n            'instances': [{\n                'url': 'localhost',\n            }],\n        }\n        self.run_check(new_conf)\n        self.assertEquals(self._countConnections(11211), 0)\n", "label": 0}
{"function": "\n\ndef info2iob(sentence, chunks, informations):\n    info_list = ([], [], [])\n    for information in informations:\n        temp_list = positions(information, sentence)\n        for i in range(3):\n            if (temp_list[i] not in info_list[i]):\n                info_list[i].append(temp_list[i])\n    return tag_sent(chunks, info_list)\n", "label": 0}
{"function": "\n\ndef find_duplicates(conn, limit=50, index=None):\n    query = 'SELECT f.id, fingerprint, length FROM fingerprint f LEFT JOIN fingerprint_deduplicate d ON f.id=d.id WHERE d.id IS NULL ORDER BY f.id LIMIT 1000'\n    for fingerprint in conn.execute(query):\n        find_track_duplicates(conn, fingerprint, index=index)\n", "label": 0}
{"function": "\n\ndef _delete(self, filter, multi=False):\n    if (filter is None):\n        filter = {\n            \n        }\n    if (not isinstance(filter, collections.Mapping)):\n        filter = {\n            '_id': filter,\n        }\n    to_delete = list(self.find(filter))\n    deleted_count = 0\n    for doc in to_delete:\n        doc_id = doc['_id']\n        if isinstance(doc_id, dict):\n            doc_id = helpers.hashdict(doc_id)\n        del self._documents[doc_id]\n        deleted_count += 1\n        if (not multi):\n            break\n    return {\n        'connectionId': self._database.client._id,\n        'n': deleted_count,\n        'ok': 1.0,\n        'err': None,\n    }\n", "label": 1}
{"function": "\n\ndef items(self):\n    'Dict-like items() that returns a list of name-value tuples from the jar.\\n        See keys() and values(). Allows client-code to call \"dict(RequestsCookieJar)\\n        and get a vanilla python dict of key value pairs.'\n    items = []\n    for cookie in iter(self):\n        items.append((cookie.name, cookie.value))\n    return items\n", "label": 0}
{"function": "\n\ndef intersects(self, other):\n    return ((other.start < self.end) or (self.start < other.end))\n", "label": 0}
{"function": "\n\ndef select_coins(self, colorvalue, use_fee_estimator=None):\n    self._validate_select_coins_parameters(colorvalue, use_fee_estimator)\n    colordef = colorvalue.get_colordef()\n    if (colordef == UNCOLORED_MARKER):\n        return self.select_uncolored_coins(colorvalue, use_fee_estimator)\n    color_id = colordef.get_color_id()\n    if (color_id in self.inputs):\n        total = SimpleColorValue.sum([cv_u[0] for cv_u in self.inputs[color_id]])\n        if (total < colorvalue):\n            msg = 'Not enough coins: %s requested, %s found!'\n            raise InsufficientFundsError((msg % (colorvalue, total)))\n        return ([cv_u[1] for cv_u in self.inputs[color_id]], total)\n    if (colorvalue > self.our_value_limit):\n        raise InsufficientFundsError(('%s requested, %s found!' % (colorvalue, self.our_value_limit)))\n    return super(OperationalETxSpec, self).select_coins(colorvalue)\n", "label": 1}
{"function": "\n\ndef match(self, left, collected=None):\n    collected = ([] if (collected is None) else collected)\n    outcomes = []\n    for p in self.children:\n        (matched, _, _) = outcome = p.match(left, collected)\n        if matched:\n            outcomes.append(outcome)\n    if outcomes:\n        return min(outcomes, key=(lambda outcome: len(outcome[1])))\n    return (False, left, collected)\n", "label": 0}
{"function": "\n\ndef _get_input(self):\n    (title, msg) = ('Get Input Dialog', 'Enter your name:')\n    (text, resp) = QtGui.QInputDialog.getText(self, title, msg)\n    if resp:\n        self.label.setText(web.safeunicode(text))\n", "label": 0}
{"function": "\n\ndef _replace_cdata_list_attribute_values(self, tag_name, attrs):\n    'Replaces class=\"foo bar\" with class=[\"foo\", \"bar\"]\\n\\n        Modifies its input in place.\\n        '\n    if (not attrs):\n        return attrs\n    if self.cdata_list_attributes:\n        universal = self.cdata_list_attributes.get('*', [])\n        tag_specific = self.cdata_list_attributes.get(tag_name.lower(), None)\n        for attr in list(attrs.keys()):\n            if ((attr in universal) or (tag_specific and (attr in tag_specific))):\n                value = attrs[attr]\n                if isinstance(value, str):\n                    values = whitespace_re.split(value)\n                else:\n                    values = value\n                attrs[attr] = values\n    return attrs\n", "label": 1}
{"function": "\n\ndef _sanitize_mod_params(self, other):\n    \"Sanitize the object being modded with this Message.\\n\\n        - Add support for modding 'None' so translation supports it\\n        - Trim the modded object, which can be a large dictionary, to only\\n        those keys that would actually be used in a translation\\n        - Snapshot the object being modded, in case the message is\\n        translated, it will be used as it was when the Message was created\\n        \"\n    if (other is None):\n        params = (other,)\n    elif isinstance(other, dict):\n        params = self._trim_dictionary_parameters(other)\n    else:\n        params = self._copy_param(other)\n    return params\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (not (type(other) is STP)):\n        return False\n    return ((self.network == other.network) and (self.port == other.port) and (self.label == other.label))\n", "label": 0}
{"function": "\n\ndef validate(self, doc):\n    if (not doc.get('_id', '')):\n        raise Exception('Attempting to store empty password.')\n    return doc\n", "label": 0}
{"function": "\n\ndef __init__(self, **kwargs):\n    dict_type = (kwargs.pop('dict_type', None) or OrderedDict)\n    ConfigParser.__init__(self, dict_type=dict_type, **kwargs)\n", "label": 0}
{"function": "\n\ndef setup():\n    '\\n    Create necessary directories for testing.\\n    '\n    train_dir = join(_my_dir, 'train')\n    if (not exists(train_dir)):\n        os.makedirs(train_dir)\n    output_dir = join(_my_dir, 'output')\n    if (not exists(output_dir)):\n        os.makedirs(output_dir)\n", "label": 0}
{"function": "\n\ndef render_result(result_data, test_bundle):\n    result_data = lcase_keys(result_data)\n    result_string = (sublime.expand_variables(RESULTS_TEMPLATES['results'], filter_stats_dict(result_data)) + '\\n')\n    for bundle in result_data['bundlestats']:\n        if (len(test_bundle) and (bundle['path'] != test_bundle)):\n            continue\n        result_string += (('\\n' + sublime.expand_variables(RESULTS_TEMPLATES['bundle'], filter_stats_dict(bundle))) + '\\n')\n        if isinstance(bundle['globalexception'], dict):\n            result_string += (('\\n' + sublime.expand_variables(RESULTS_TEMPLATES['global_exception'], filter_exception_dict(bundle['globalexception']))) + '\\n')\n        for suite in bundle['suitestats']:\n            result_string += ('\\n' + gen_suite_report(suite))\n    result_string += ('\\n' + RESULTS_TEMPLATES['legend'])\n    return result_string\n", "label": 1}
{"function": "\n\ndef _dictify(data, name='input', key_mod=(lambda x: x), value_mod=(lambda x: x)):\n    if data:\n        if isinstance(data, collections.Sequence):\n            return dict(((key_mod(str(v)), value_mod(str(v))) for v in data))\n        elif isinstance(data, collections.Mapping):\n            return dict(((key_mod(str(k)), value_mod(str((v or k)))) for (k, v) in list(data.items())))\n        else:\n            raise BlockadeConfigError(('invalid %s: need list or map' % (name,)))\n    else:\n        return {\n            \n        }\n", "label": 1}
{"function": "\n\ndef login_action(request):\n    username = request.REQUEST['username']\n    password = request.REQUEST['password']\n    user = authenticate(username=username, password=password)\n    if (user is None):\n        return json_failure(_('Invalid username or password'))\n    if (not user.is_active):\n        return json_failure(_('Account disabled.'))\n    login(request, user)\n    return json_response()\n", "label": 0}
{"function": "\n\ndef add(self, grid):\n    '\\n        Used to add quantities from another grid\\n\\n        Parameters\\n        ----------\\n        grid : 3D Numpy array or SphericalPolarGridView instance\\n            The grid to copy the quantity from\\n        '\n    if (type(self.quantities[self.viewed_quantity]) is list):\n        raise Exception('need to first specify the item to add to')\n    if isinstance(grid, SphericalPolarGridView):\n        if (type(grid.quantities[grid.viewed_quantity]) is list):\n            raise Exception('need to first specify the item to add')\n        self._check_array_dimensions(grid.quantities[grid.viewed_quantity])\n        self.quantities[self.viewed_quantity] += grid.quantities[grid.viewed_quantity]\n    elif isinstance(grid, np.ndarray):\n        self._check_array_dimensions(grid)\n        self.quantities[self.viewed_quantity] += grid\n    else:\n        raise ValueError('grid should be a Numpy array or a SphericalPolarGridView instance')\n", "label": 0}
{"function": "\n\ndef create_junk():\n    fileh = open_file(filename, mode='w')\n    group = fileh.create_group(fileh.root, 'newgroup')\n    for i in range(NLEAVES):\n        table = fileh.create_table(group, ('table' + str(i)), Particle, 'A table', Filters(1))\n        particle = table.row\n        print('Creating table-->', table._v_name)\n        for i in range(NROWS):\n            particle.append()\n        table.flush()\n    fileh.close()\n", "label": 0}
{"function": "\n\ndef _send_500(req, extra=None):\n    req.start_response('500 Error', [('Content-Type', 'text/html')])\n    req.write('<h1>500 Internal Server Error</h1>\\n')\n    req.write('The server encountered an internal error or misconfiguration and was unable to complete your request.\\n')\n    if (extra is not None):\n        req.write(extra)\n    req.close()\n", "label": 0}
{"function": "\n\ndef __iter__(self):\n    for d in reversed(self.dicts):\n        (yield d)\n", "label": 0}
{"function": "\n\ndef getopt(args, shortopts):\n    'getopt(args, options) -> opts, long_opts, args \\nReturns options as list of tuples, long options as entries in a dictionary, and\\nthe remaining arguments.'\n    opts = []\n    longopts = {\n        \n    }\n    while (args and args[0].startswith('-') and (args[0] != '-')):\n        if (args[0] == '--'):\n            args = args[1:]\n            break\n        if args[0].startswith('--'):\n            arg = args.pop(0)\n            _do_longs(longopts, arg)\n        else:\n            (opts, args) = _do_shorts(opts, args[0][1:], shortopts, args[1:])\n    return (opts, longopts, args)\n", "label": 1}
{"function": "\n\n@app.route('/api')\ndef api():\n    \"WebSocket endpoint; Takes a 'topic' GET param.\"\n    ws = request.environ.get('wsgi.websocket')\n    topic = request.args.get('topic')\n    if (None in (ws, topic)):\n        return\n    topic = topic.encode('ascii')\n    for (message, message_topic) in CircusConsumer(topic, endpoint=ZMQ_ENDPOINT):\n        response = json.dumps(dict(message=message, topic=message_topic))\n        ws.send(response)\n", "label": 0}
{"function": "\n\ndef authenticate(self):\n    (client_address, _) = self.client_address\n    NAMESPACE.machine = NAMESPACE.session.query(Builder).filter_by(ip=client_address).first()\n    NAMESPACE.user = NAMESPACE.session.query(Person).filter_by(ip=client_address).first()\n    return (NAMESPACE.machine or NAMESPACE.user)\n", "label": 0}
{"function": "\n\ndef cleanup(self, tc_name=''):\n    if (tc_name != ''):\n        self._log.info(('%s: FAILED' % tc_name))\n    for obj in ['ruleset', 'rule', 'classifier', 'action']:\n        self.gbpcfg.gbp_del_all_anyobj(obj)\n", "label": 0}
{"function": "\n\ndef _geo_field(self, field_name=None):\n    \"\\n        Returns the first Geometry field encountered; or specified via the\\n        `field_name` keyword.  The `field_name` may be a string specifying\\n        the geometry field on this GeoQuery's model, or a lookup string\\n        to a geometry field via a ForeignKey relation.\\n        \"\n    if (field_name is None):\n        for fld in self.model._meta.fields:\n            if isinstance(fld, GeometryField):\n                return fld\n        return False\n    else:\n        return GeoWhereNode._check_geo_field(self.model._meta, field_name)\n", "label": 0}
{"function": "\n\ndef compute(self):\n    if self.has_input('foo'):\n        v1 = self.get_input('foo')\n    else:\n        v1 = 0\n    if (v1 != 12):\n        self.change_parameter('foo', (v1 + 1))\n", "label": 0}
{"function": "\n\ndef __init__(self, parentContainer, localId, randomSeed=1, numCalls=1, variantDensity=1):\n    super(SimulatedVariantSet, self).__init__(parentContainer, localId)\n    self._randomSeed = randomSeed\n    self._numCalls = numCalls\n    for j in range(numCalls):\n        self.addCallSet('simCallSet_{}'.format(j))\n    self._variantDensity = variantDensity\n    now = protocol.convertDatetime(datetime.datetime.now())\n    self._creationTime = now\n    self._updatedTime = now\n", "label": 0}
{"function": "\n\n@staticmethod\ndef overwrite_attribute(entity_id, attrs, vals):\n    'Overwrite any attribute of an entity.\\n\\n        This function should receive a list of attributes and a\\n        list of values. Set attribute to None to remove any overwritten\\n        value in place.\\n        '\n    for (attr, val) in zip(attrs, vals):\n        if (val is None):\n            _OVERWRITE[entity_id.lower()].pop(attr, None)\n        else:\n            _OVERWRITE[entity_id.lower()][attr] = val\n", "label": 0}
{"function": "\n\ndef _check_params(length, size):\n    _check_size(size)\n    if ((length % size) != 0):\n        raise error('not a whole number of frames')\n", "label": 0}
{"function": "\n\ndef ex_update_node_affinity_group(self, node, affinity_group_list):\n    '\\n        Updates the affinity/anti-affinity group associations of a virtual\\n        machine. The VM has to be stopped and restarted for the new properties\\n        to take effect.\\n\\n        :param node: Node to update.\\n        :type node: :class:`CloudStackNode`\\n\\n        :param affinity_group_list: List of CloudStackAffinityGroup to\\n                                    associate\\n        :type affinity_group_list: ``list`` of :class:`CloudStackAffinityGroup`\\n\\n        :rtype :class:`CloudStackNode`\\n        '\n    affinity_groups = ','.join((ag.id for ag in affinity_group_list))\n    result = self._async_request(command='updateVMAffinityGroup', params={\n        'id': node.id,\n        'affinitygroupids': affinity_groups,\n    }, method='GET')\n    return self._to_node(data=result['virtualmachine'])\n", "label": 0}
{"function": "\n\ndef html_body(input_string, source_path=None, destination_path=None, input_encoding='unicode', output_encoding='unicode', doctitle=1, initial_header_level=1):\n    '\\n    Given an input string, returns an HTML fragment as a string.\\n\\n    The return value is the contents of the <body> element.\\n\\n    Parameters (see `html_parts()` for the remainder):\\n\\n    - `output_encoding`: The desired encoding of the output.  If a Unicode\\n      string is desired, use the default value of \"unicode\" .\\n    '\n    parts = html_parts(input_string=input_string, source_path=source_path, destination_path=destination_path, input_encoding=input_encoding, doctitle=doctitle, initial_header_level=initial_header_level)\n    fragment = parts['html_body']\n    if (output_encoding != 'unicode'):\n        fragment = fragment.encode(output_encoding)\n    return fragment\n", "label": 0}
{"function": "\n\ndef rgb_to_hsv(r, g, b):\n    maxc = max(r, g, b)\n    minc = min(r, g, b)\n    v = maxc\n    if (minc == maxc):\n        return (0.0, 0.0, v)\n    s = ((maxc - minc) / maxc)\n    rc = ((maxc - r) / (maxc - minc))\n    gc = ((maxc - g) / (maxc - minc))\n    bc = ((maxc - b) / (maxc - minc))\n    if (r == maxc):\n        h = (bc - gc)\n    elif (g == maxc):\n        h = ((2.0 + rc) - bc)\n    else:\n        h = ((4.0 + gc) - rc)\n    h = ((h / 6.0) % 1.0)\n    return (h, s, v)\n", "label": 0}
{"function": "\n\ndef build_xform(self):\n    xform = XFormBuilder(self.name)\n    for ig in self.iter_item_groups():\n        data_type = ('repeatGroup' if self.is_repeating else 'group')\n        group = xform.new_group(ig.question_name, ig.question_label, data_type)\n        for item in ig.iter_items():\n            group.new_question(item.question_name, item.question_label, ODK_DATA_TYPES[item.data_type], choices=item.choices)\n    return xform.tostring(pretty_print=True, encoding='utf-8', xml_declaration=True)\n", "label": 0}
{"function": "\n\ndef permutations(xs):\n    if (not xs):\n        (yield [])\n    else:\n        for (y, ys) in selections(xs):\n            for pys in permutations(ys):\n                (yield ([y] + pys))\n", "label": 0}
{"function": "\n\ndef on_response(self, response):\n    self.stop()\n    self.got_response = True\n    if (not (response.status_code == 418)):\n        self.response_valid = False\n", "label": 0}
{"function": "\n\ndef _annotate_local(self):\n    \"Annotate the primaryjoin and secondaryjoin\\n        structures with 'local' annotations.\\n\\n        This annotates all column elements found\\n        simultaneously in the parent table\\n        and the join condition that don't have a\\n        'remote' annotation set up from\\n        _annotate_remote() or user-defined.\\n\\n        \"\n    if self._has_annotation(self.primaryjoin, 'local'):\n        return\n    if self._local_remote_pairs:\n        local_side = util.column_set([l for (l, r) in self._local_remote_pairs])\n    else:\n        local_side = util.column_set(self.parent_selectable.c)\n\n    def locals_(elem):\n        if (('remote' not in elem._annotations) and (elem in local_side)):\n            return elem._annotate({\n                'local': True,\n            })\n    self.primaryjoin = visitors.replacement_traverse(self.primaryjoin, {\n        \n    }, locals_)\n", "label": 0}
{"function": "\n\ndef __cmp__(self, other):\n    'ensure that same seq intervals match in cmp()'\n    if (not isinstance(other, SeqPath)):\n        return (- 1)\n    if (self.path is other.path):\n        return cmp((self.start, self.stop), (other.start, other.stop))\n    else:\n        return NOT_ON_SAME_PATH\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    if ((not self.frozen_by_south) and (name not in [f.name for f in cls._meta.fields])):\n        super(CurrencyField, self).contribute_to_class(cls, name)\n", "label": 0}
{"function": "\n\ndef dapInfo(self, id_):\n    cmd = []\n    cmd.append(COMMAND_ID['DAP_INFO'])\n    cmd.append(ID_INFO[id_])\n    self.interface.write(cmd)\n    resp = self.interface.read()\n    if (resp[0] != COMMAND_ID['DAP_INFO']):\n        raise ValueError('DAP_INFO response error')\n    if (resp[1] == 0):\n        return\n    if (id_ in ('CAPABILITIES', 'PACKET_COUNT', 'PACKET_SIZE')):\n        if (resp[1] == 1):\n            return resp[2]\n        if (resp[1] == 2):\n            return ((resp[3] << 8) | resp[2])\n    x = array.array('B', [i for i in resp[2:(2 + resp[1])]])\n    return x.tostring()\n", "label": 1}
{"function": "\n\ndef fingerprint(self):\n    try:\n        pubkey = sshpubkeys.SSHKey(self.key)\n        return pubkey.hash()\n    except:\n        'There are a small parcel of exceptions that can be throw to indicate invalid keys'\n        return ''\n", "label": 0}
{"function": "\n\n@property\ndef servicenames(self):\n    'Give the list of services available in this folder.'\n    return set([service['name'].rstrip('/').split('/')[(- 1)] for service in self._json_struct.get('services', [])])\n", "label": 0}
{"function": "\n\ndef is_lazy_user(user):\n    ' Return True if the passed user is a lazy user. '\n    if user.is_anonymous():\n        return False\n    backend = getattr(user, 'backend', None)\n    if (backend == 'lazysignup.backends.LazySignupBackend'):\n        return True\n    from lazysignup.models import LazyUser\n    return bool((LazyUser.objects.filter(user=user).count() > 0))\n", "label": 0}
{"function": "\n\ndef check_variable(self, name):\n    ' check_variable(name: str) -> Boolean\\n        Returns True if the vistrail already has the variable name\\n\\n        '\n    variableBox = self.parent().parent().parent()\n    if variableBox.controller:\n        return variableBox.controller.check_vistrail_variable(name)\n    return False\n", "label": 0}
{"function": "\n\n@click.command()\n@click.argument('identifier')\n@click.option('--postinstall', '-i', help='Post-install script to download')\n@click.option('--image', help=\"Image ID. The default is to use the current operating system.\\nSee: 'slcli image list' for reference\")\n@helpers.multi_option('--key', '-k', help='SSH keys to add to the root user')\n@environment.pass_env\ndef cli(env, identifier, postinstall, key, image):\n    'Reload operating system on a virtual server.'\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    keys = []\n    if key:\n        for single_key in key:\n            resolver = SoftLayer.SshKeyManager(env.client).resolve_ids\n            key_id = helpers.resolve_id(resolver, single_key, 'SshKey')\n            keys.append(key_id)\n    if (not (env.skip_confirmations or formatting.no_going_back(vs_id))):\n        raise exceptions.CLIAbort('Aborted')\n    vsi.reload_instance(vs_id, post_uri=postinstall, ssh_keys=keys, image_id=image)\n", "label": 0}
{"function": "\n\ndef findPeak(self, A):\n    '\\n        Binary search\\n        Microsoft Interview, Oct 2014\\n\\n        To reduce the complexity of dealing the edge cases:\\n        * add two anti-peak dummies on the both ends\\n\\n        :param A: An integers list. A[0] and A[-1] are dummies.\\n        :return: return any of peek positions.\\n        '\n    n = len(A)\n    l = 0\n    h = n\n    while (l < h):\n        m = ((l + h) / 2)\n        if (A[(m - 1)] < A[m] > A[(m + 1)]):\n            return m\n        elif (A[(m + 1)] > A[m]):\n            l = (m + 1)\n        else:\n            h = m\n    raise Exception\n", "label": 0}
{"function": "\n\ndef season_by_id(season_id):\n    url = endpoints.season_by_id.format(season_id)\n    q = _query_endpoint(url)\n    if q:\n        return Season(q)\n    else:\n        raise SeasonNotFound(\"Couldn't find Season with ID: {0}\".format(season_id))\n", "label": 0}
{"function": "\n\ndef check_migrations(self):\n    \"\\n        Checks to see if the set of migrations on disk matches the\\n        migrations in the database. Prints a warning if they don't match.\\n        \"\n    executor = MigrationExecutor(connections[DEFAULT_DB_ALIAS])\n    plan = executor.migration_plan(executor.loader.graph.leaf_nodes())\n    if (plan and self.show_startup_messages):\n        self.stdout.write(self.style.NOTICE('\\nYou have unapplied migrations; your app may not work properly until they are applied.'))\n        self.stdout.write(self.style.NOTICE(\"Run 'python manage.py migrate' to apply them.\\n\"))\n", "label": 0}
{"function": "\n\ndef friend_list(self, player_id):\n    return ' '.join([user_manager.id_to_name(friend_id) for friend_id in fetch_set_keys(friend_key(player_id))])\n", "label": 0}
{"function": "\n\ndef onModelChanged(self, model):\n    newTrackPosition = np.array(self.jointController.q[:3])\n    delta = (newTrackPosition - self.lastTrackPosition)\n    for i in xrange(3):\n        if (not self.followAxes[i]):\n            delta[i] = 0.0\n    self.lastTrackPosition = newTrackPosition\n    c = self.view.camera()\n    oldFocalPoint = np.array(c.GetFocalPoint())\n    oldPosition = np.array(c.GetPosition())\n    c.SetFocalPoint((oldFocalPoint + delta))\n    c.SetPosition((oldPosition + delta))\n    self.view.render()\n", "label": 0}
{"function": "\n\ndef log_notifications(self, notifications):\n    main_logger = logging.getLogger(config.main_logger_name)\n    notification_logger = logging.getLogger(config.notifications_logger_name)\n    for notification in notifications:\n        try:\n            notification['content'] = notification['content'].encode('utf-8').replace(',', '\\\\,')\n            keys = ['status', 'login_id', 'content', 'message_id', 'campaign_id', 'sending_id', 'game', 'world_id', 'screen', 'time', 'time_to_live_ts_bigint', 'platform', 'receiver_id']\n            notification_logger.info(','.join([str(notification[key]) for key in keys]))\n        except:\n            main_logger.exception('Error while logging notification to csv log!')\n", "label": 0}
{"function": "\n\n@replace_call(BaseDatabaseWrapper.cursor)\ndef cursor(func, self):\n    djdt = DebugToolbarMiddleware.get_current()\n    if djdt:\n        djdt._panels[SQLDebugPanel] = djdt.get_panel(SQLLoggingPanel)\n    return func(self)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    if isinstance(key, int):\n        return list(self.values()).__getitem__(key)\n    elif isinstance(key, slice):\n        items = list(self.items()).__getitem__(key)\n        return Layers(items)\n    else:\n        return super(Layers, self).__getitem__(key)\n", "label": 0}
{"function": "\n\ndef load(self, config):\n    self.items = collections.OrderedDict()\n    values = config.get('axes', self.name).split(',')\n    if config.has_section(('axis:%s' % self.name)):\n        self.defaults = collections.OrderedDict(config.items(('axis:%s' % self.name)))\n    else:\n        self.defaults = {\n            \n        }\n    for value in values:\n        self.items[value.strip('*')] = AxisItem(self, value, config)\n", "label": 0}
{"function": "\n\ndef write_output(args, powerline, segment_info, write):\n    if args.renderer_arg:\n        segment_info.update(args.renderer_arg)\n    if args.side.startswith('above'):\n        for line in powerline.render_above_lines(width=args.width, segment_info=segment_info, mode=segment_info.get('mode', None)):\n            if line:\n                write((line + '\\n'))\n        args.side = args.side[len('above'):]\n    if args.side:\n        rendered = powerline.render(width=args.width, side=args.side, segment_info=segment_info, mode=segment_info.get('mode', None))\n        write(rendered)\n", "label": 1}
{"function": "\n\ndef lines(self, text):\n    for line in text.split('\\n'):\n        (yield ('         \"%s\\\\n\"' % escape_quote(line)))\n", "label": 0}
{"function": "\n\n@pg.production('binop_expr : binop_expr PIPELINE_FIRST_BIND binop_expr')\ndef binop_expr(p):\n    (left, _, right) = p\n    input_sym = get_temp_name()\n    return [Symbol('|>'), p[0], [Symbol('bind'), [Symbol('fn'), [input_sym], ([p[2][0], input_sym] + p[2][(1 if (len(p[2]) > 1) else len(p[2])):])]]]\n", "label": 0}
{"function": "\n\ndef index(self, keypair_list):\n    return dict(keypairs=[self._base_response(keypair) for keypair in keypair_list])\n", "label": 0}
{"function": "\n\ndef validate(self, value):\n    super(Interval, self).validate(value)\n    if (not ((value is None) or (self.interval_type.is_valid(value) and (value >= self.start) and (value <= self.end)))):\n        raise ValueError(('expected a value of type %s in range [%s, %s], got %r' % (self.interval_type, self.start, self.end, value)))\n", "label": 0}
{"function": "\n\ndef _on_nick(self, c, e):\n    '[Internal]'\n    before = nm_to_n(e.source())\n    after = e.target()\n    for ch in self.channels.values():\n        if ch.has_user(before):\n            ch.change_nick(before, after)\n", "label": 0}
{"function": "\n\ndef test_K4_normalized(self):\n    'Betweenness centrality: K4'\n    G = networkx.complete_graph(4)\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True)\n    b_answer = {\n        0: 0.25,\n        1: 0.25,\n        2: 0.25,\n        3: 0.25,\n    }\n    for n in sorted(G):\n        assert_almost_equal(b[n], b_answer[n])\n    G.add_edge(0, 1, {\n        'weight': 0.5,\n        'other': 0.3,\n    })\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True, weight=None)\n    for n in sorted(G):\n        assert_almost_equal(b[n], b_answer[n])\n    wb_answer = {\n        0: 0.2222222,\n        1: 0.2222222,\n        2: 0.30555555,\n        3: 0.30555555,\n    }\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert_almost_equal(b[n], wb_answer[n])\n    wb_answer = {\n        0: 0.2051282,\n        1: 0.2051282,\n        2: 0.33974358,\n        3: 0.33974358,\n    }\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True, weight='other')\n    for n in sorted(G):\n        assert_almost_equal(b[n], wb_answer[n])\n", "label": 0}
{"function": "\n\ndef get_auth_params(self, request, action):\n    settings = self.get_settings()\n    ret = settings.get('AUTH_PARAMS', {\n        \n    })\n    dynamic_auth_params = request.GET.get('auth_params', None)\n    if dynamic_auth_params:\n        ret.update(dict(parse_qsl(dynamic_auth_params)))\n    return ret\n", "label": 0}
{"function": "\n\ndef _emit(self, record, stream):\n    self.stream = stream\n    try:\n        return logging.StreamHandler.emit(self, record)\n    except:\n        raise\n    else:\n        self.stream = None\n", "label": 0}
{"function": "\n\ndef data_path(path, createdir=False):\n    'If path is relative, return the given path inside the project data dir,\\n    otherwise return the path unmodified\\n    '\n    if (not isabs(path)):\n        path = join(project_data_dir(), path)\n    if (createdir and (not exists(path))):\n        os.makedirs(path)\n    return path\n", "label": 0}
{"function": "\n\ndef __init__(self, groupDateTime, bars, frequency):\n    resamplebase.Grouper.__init__(self, groupDateTime)\n    self.__barGroupers = {\n        \n    }\n    self.__frequency = frequency\n    for (instrument, bar_) in bars.items():\n        barGrouper = resampled.BarGrouper(groupDateTime, bar_, frequency)\n        self.__barGroupers[instrument] = barGrouper\n", "label": 0}
{"function": "\n\ndef ex_attach_nic_to_node(self, node, network, ip_address=None):\n    \"\\n        Add an extra Nic to a VM\\n\\n        :param  network: NetworkOffering object\\n        :type   network: :class:'CloudStackNetwork`\\n\\n        :param  node: Node Object\\n        :type   node: :class:'CloudStackNode`\\n\\n        :param  ip_address: Optional, specific IP for this Nic\\n        :type   ip_address: ``str``\\n\\n\\n        :rtype: ``bool``\\n        \"\n    args = {\n        'virtualmachineid': node.id,\n        'networkid': network.id,\n    }\n    if (ip_address is not None):\n        args['ipaddress'] = ip_address\n    self._async_request(command='addNicToVirtualMachine', params=args)\n    return True\n", "label": 0}
{"function": "\n\ndef bind(self, lan):\n    'bind to a LAN.'\n    if _debug:\n        Node._debug('bind %r', lan)\n    lan.add_node(self)\n", "label": 0}
{"function": "\n\ndef columns_used(self):\n    '\\n        Returns all the columns used across all models in the group\\n        for filtering and in the model expression.\\n\\n        '\n    return list(tz.unique(tz.concat((m.columns_used() for m in self.models.values()))))\n", "label": 0}
{"function": "\n\ndef S_e(self, prob):\n    '\\n        Electric source term\\n\\n        :param Problem prob: FDEM Problem\\n        :rtype: numpy.ndarray\\n        :return: electric source term on mesh\\n        '\n    if ((prob._formulation is 'EB') and (self.integrate is True)):\n        return (prob.Me * self._S_e)\n    return self._S_e\n", "label": 0}
{"function": "\n\ndef __call__(self, fn):\n\n    def wrapper(*args, **kwargs):\n        that = args[0]\n        that.logger.debug(self.start)\n        ret = fn(*args, **kwargs)\n        that.logger.debug(self.finish)\n        if self.getter:\n            that.logger.debug(pformat(self.getter(ret)))\n        else:\n            that.logger.debug(pformat(ret))\n        return ret\n    wrapper.func_name = fn.func_name\n    if hasattr(fn, '__name__'):\n        wrapper.__name__ = self.name = fn.__name__\n    if hasattr(fn, '__doc__'):\n        wrapper.__doc__ = fn.__doc__\n    if hasattr(fn, '__module__'):\n        wrapper.__module__ = fn.__module__\n    return wrapper\n", "label": 0}
{"function": "\n\ndef write_packed(self, outfile, rows):\n    '\\n        Write PNG file to `outfile`.  The pixel data comes from `rows`\\n        which should be in boxed row packed format.  Each row should be\\n        a sequence of packed bytes.\\n\\n        Technically, this method does work for interlaced images but it\\n        is best avoided.  For interlaced images, the rows should be\\n        presented in the order that they appear in the file.\\n\\n        This method should not be used when the source image bit depth\\n        is not one naturally supported by PNG; the bit depth should be\\n        1, 2, 4, 8, or 16.\\n        '\n    if self.rescale:\n        raise Error(('write_packed method not suitable for bit depth %d' % self.rescale[0]))\n    return self.write_passes(outfile, rows, packed=True)\n", "label": 0}
{"function": "\n\ndef _print_slots(self):\n    slots = ', '.join((((\"'\" + snake(name)) + \"'\") for (type, name, nullable, plural) in self._fields))\n    print(\"    __slots__ = ('loc', {slots},)\".format(slots=slots))\n", "label": 0}
{"function": "\n\ndef register_scheme(scheme):\n    for method in dir(urlparse):\n        if method.startswith('uses_'):\n            getattr(urlparse, method).append(scheme)\n", "label": 0}
{"function": "\n\ndef configure_uploads(app, upload_sets):\n    \"\\n    Call this after the app has been configured. It will go through all the\\n    upload sets, get their configuration, and store the configuration on the\\n    app. It will also register the uploads module if it hasn't been set. This\\n    can be called multiple times with different upload sets.\\n    \\n    .. versionchanged:: 0.1.3\\n       The uploads module/blueprint will only be registered if it is needed\\n       to serve the upload sets.\\n    \\n    :param app: The `~flask.Flask` instance to get the configuration from.\\n    :param upload_sets: The `UploadSet` instances to configure.\\n    \"\n    if isinstance(upload_sets, UploadSet):\n        upload_sets = (upload_sets,)\n    if (not hasattr(app, 'upload_set_config')):\n        app.upload_set_config = {\n            \n        }\n    set_config = app.upload_set_config\n    defaults = dict(dest=app.config.get('UPLOADS_DEFAULT_DEST'), url=app.config.get('UPLOADS_DEFAULT_URL'))\n    for uset in upload_sets:\n        config = config_for_set(uset, app, defaults)\n        set_config[uset.name] = config\n    should_serve = any(((s.base_url is None) for s in set_config.itervalues()))\n    if using_blueprints:\n        if (('_uploads' not in app.blueprints) and should_serve):\n            app.register_blueprint(uploads_mod)\n    elif (('_uploads' not in app.modules) and should_serve):\n        app.register_module(uploads_mod)\n", "label": 1}
{"function": "\n\ndef change_primary_name(self, name):\n    '\\n        Changes the primary/default name of the policy to a specified name.\\n\\n        :param name: a string name to replace the current primary name.\\n        '\n    if (name == self.name):\n        return\n    elif (name in self.alias_list):\n        self.remove_name(name)\n    else:\n        self._validate_policy_name(name)\n    self.alias_list.insert(0, name)\n", "label": 0}
{"function": "\n\ndef configure_host(self):\n    if self.mail.use_ssl:\n        host = smtplib.SMTP_SSL(self.mail.server, self.mail.port)\n    else:\n        host = smtplib.SMTP(self.mail.server, self.mail.port)\n    host.set_debuglevel(int(self.mail.debug))\n    if self.mail.use_tls:\n        host.starttls()\n    if (self.mail.username and self.mail.password):\n        host.login(self.mail.username, self.mail.password)\n    return host\n", "label": 0}
{"function": "\n\ndef get(self, key):\n    key = key.lower()\n    if self.has_key(key):\n        return self._config.get(key.lower())\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef add_all_wordstarts_matching(self, lower_hits, query, max_hits_hint):\n    lower_query = query.lower()\n    if (lower_query in self.basenames_by_wordstarts):\n        for basename in self.basenames_by_wordstarts[lower_query]:\n            lower_hits.add(basename)\n            if (len(lower_hits) >= max_hits_hint):\n                return\n", "label": 0}
{"function": "\n\ndef _setup_nodes(self):\n    self.add_node(LocalNode())\n    nodes = self.app.config.get('PSDASH_NODES', [])\n    logger.info('Registering %d nodes', len(nodes))\n    for n in nodes:\n        self.register_node(n['name'], n['host'], int(n['port']))\n", "label": 0}
{"function": "\n\ndef __init__(self, parent_model, admin_site):\n    self.admin_site = admin_site\n    self.parent_model = parent_model\n    self.opts = self.model._meta\n    self.has_registered_model = admin_site.is_registered(self.model)\n    super(InlineModelAdmin, self).__init__()\n    if (self.verbose_name is None):\n        self.verbose_name = self.model._meta.verbose_name\n    if (self.verbose_name_plural is None):\n        self.verbose_name_plural = self.model._meta.verbose_name_plural\n", "label": 0}
{"function": "\n\ndef __init__(self, name, table=None, foreign_key=None, other_key=None, relation=None):\n    if isinstance(foreign_key, (types.FunctionType, types.MethodType)):\n        raise RuntimeError('morphed_by_many relation requires a name')\n    self._name = name\n    self._table = table\n    self._foreign_key = foreign_key\n    self._other_key = other_key\n    super(morphed_by_many, self).__init__(relation=relation)\n", "label": 0}
{"function": "\n\ndef get_object(self, bits):\n    if (len(bits) != 0):\n        raise models.Topic.DoesNotExist\n    return 'LatestFeed'\n", "label": 0}
{"function": "\n\n@ComputedGraph.Function\ndef extractVectorDataAsPythonArray(self):\n    if (self.computedValueVector.vectorImplVal is None):\n        return None\n    if ((len(self.vectorDataIds) > 0) and (not self.isLoaded)):\n        return None\n    result = ComputedValueGateway.getGateway().extractVectorDataAsPythonArray(self.computedValueVector, self.lowIndex, self.highIndex)\n    if ((result is None) and (not self.vdmThinksIsLoaded())):\n        logging.info('CumulusClient: %s was marked loaded but returned None. reloading', self)\n        self.isLoaded = False\n        ComputedValueGateway.getGateway().reloadVector(self)\n    return result\n", "label": 1}
{"function": "\n\ndef parse_policy(policy):\n    ret = {\n        \n    }\n    ret['name'] = policy['name']\n    ret['type'] = policy['type']\n    attrs = policy['Attributes']\n    if (policy['type'] != 'SSLNegotiationPolicyType'):\n        return ret\n    ret['sslv2'] = bool(attrs.get('Protocol-SSLv2'))\n    ret['sslv3'] = bool(attrs.get('Protocol-SSLv3'))\n    ret['tlsv1'] = bool(attrs.get('Protocol-TLSv1'))\n    ret['tlsv1_1'] = bool(attrs.get('Protocol-TLSv1.1'))\n    ret['tlsv1_2'] = bool(attrs.get('Protocol-TLSv1.2'))\n    ret['server_defined_cipher_order'] = bool(attrs.get('Server-Defined-Cipher-Order'))\n    ret['reference_security_policy'] = attrs.get('Reference-Security-Policy', None)\n    non_ciphers = ['Server-Defined-Cipher-Order', 'Protocol-SSLv2', 'Protocol-SSLv3', 'Protocol-TLSv1', 'Protocol-TLSv1.1', 'Protocol-TLSv1.2', 'Reference-Security-Policy']\n    ciphers = []\n    for cipher in attrs:\n        if (attrs[cipher] and (cipher not in non_ciphers)):\n            ciphers.append(cipher)\n    ciphers.sort()\n    ret['supported_ciphers'] = ciphers\n    return ret\n", "label": 0}
{"function": "\n\ndef _update(self, context):\n    'Update partial stats locally and populate them to Scheduler.'\n    if (not self._resource_change()):\n        return\n    self.scheduler_client.update_resource_stats(self.compute_node)\n    if self.pci_tracker:\n        self.pci_tracker.save(context)\n", "label": 0}
{"function": "\n\ndef get_command_aliases(self):\n    if (not self.config.has_option('commands', 'aliases')):\n        return []\n    value = self.config.get('commands', 'aliases')\n    return list(map((lambda x: x.strip()), value.split(',')))\n", "label": 0}
{"function": "\n\ndef make_url(base, filename, rev):\n    'Helper to construct the URL to fetch.\\n\\n  Args:\\n    base: The base property of the Issue to which the Patch belongs.\\n    filename: The filename property of the Patch instance.\\n    rev: Revision number, or None for head revision.\\n\\n  Returns:\\n    A URL referring to the given revision of the file.\\n  '\n    (scheme, netloc, path, _, _, _) = urlparse.urlparse(base)\n    if netloc.endswith('.googlecode.com'):\n        if (rev is None):\n            raise FetchError(\"Can't access googlecode.com without a revision\")\n        if (not path.startswith('/svn/')):\n            raise FetchError(('Malformed googlecode.com URL (%s)' % base))\n        path = path[5:]\n        url = ('%s://%s/svn-history/r%d/%s/%s' % (scheme, netloc, rev, path, filename))\n        return url\n    elif (netloc.endswith('sourceforge.net') and (rev is not None)):\n        if path.strip().endswith('/'):\n            path = path.strip()[:(- 1)]\n        else:\n            path = path.strip()\n        splitted_path = path.split('/')\n        url = ('%s://%s/%s/!svn/bc/%d/%s/%s' % (scheme, netloc, '/'.join(splitted_path[1:3]), rev, '/'.join(splitted_path[3:]), filename))\n        return url\n    url = base\n    if (not url.endswith('/')):\n        url += '/'\n    url += filename\n    if (rev is not None):\n        url += ('?rev=%s' % rev)\n    return url\n", "label": 1}
{"function": "\n\n@mock.patch('sys.platform', 'linux2')\n@mock.patch('bento.commands.configure.virtualenv_prefix', (lambda : None))\n@mock.patch('bento.core.platforms.sysconfig.bento.utils.path.find_root', (lambda ignored: '/'))\n@mock.patch('distutils.command.install.INSTALL_SCHEMES', {\n    'unix_local': MOCK_DEBIAN_SCHEME,\n}, create=True)\ndef test_scheme_debian(self):\n    bento_info = 'Name: foo\\n'\n    scheme = self._compute_scheme(bento_info, self.options)\n    prefix = scheme.pop('prefix')\n    eprefix = scheme.pop('eprefix')\n    sitedir = scheme.pop('sitedir')\n    includedir = scheme.pop('includedir')\n    self.assertEqual(prefix, '/usr/local')\n    self.assertEqual(eprefix, '/usr/local')\n    self.assertEqual(sitedir, ('/usr/local/lib/python%s/dist-packages' % PY_VERSION_SHORT))\n    self.assertEqual(includedir, ('/usr/local/include/python%s/foo' % PY_VERSION_SHORT))\n    scheme.pop('py_version_short')\n    scheme.pop('pkgname')\n    for (k, v) in scheme.items():\n        self.assertEqual(UNIX_REFERENCE[k], v)\n", "label": 0}
{"function": "\n\ndef visit_binop(self, obj):\n    lhs = obj.lhs.accept(self)\n    op = obj.op\n    rhs = obj.rhs.accept(self)\n    if (op == '+'):\n        return (lhs + rhs)\n    elif (op == '-'):\n        return (lhs - rhs)\n    elif (op == '*'):\n        return (lhs * rhs)\n    elif (op == '/'):\n        return (lhs / rhs)\n    else:\n        raise ValueError('invalid op', op)\n", "label": 0}
{"function": "\n\n@object_base.remotable\ndef update_test(self, context=None):\n    if (context and (context.tenant == 'alternate')):\n        self.bar = 'alternate-context'\n    else:\n        self.bar = 'updated'\n", "label": 0}
{"function": "\n\ndef StartTransform(self):\n    'Starts CSV transformation on Hadoop cluster.'\n    self._LoadMapper()\n    gcs_dir = self.config['hadoopTmpDir']\n    hadoop_input_filename = ('%s/inputs/input.csv' % gcs_dir)\n    logging.info('Starting Hadoop transform from %s to %s', self.config['sources'][0], self.config['sinks'][0])\n    logging.debug('Hadoop input file: %s', hadoop_input_filename)\n    output_file = self.cloud_storage_client.OpenObject(self.config['sinks'][0], mode='w')\n    input_file = self.cloud_storage_client.OpenObject(self.config['sources'][0])\n    hadoop_input = self.cloud_storage_client.OpenObject(hadoop_input_filename, mode='w')\n    line_count = 0\n    for line in input_file:\n        if (line_count < self.config['skipLeadingRows']):\n            output_file.write(line)\n        else:\n            hadoop_input.write(line)\n        line_count += 1\n    hadoop_input.close()\n    input_file.close()\n    mapreduce_id = self._StartHadoopMapReduce(gcs_dir)\n    self._WaitForMapReduce(mapreduce_id)\n    (bucket, hadoop_dir) = gcs.Gcs.UrlToBucketAndName(gcs_dir)\n    tab_strip_pattern = re.compile('\\t\\r?\\n')\n    for hadoop_result in self.cloud_storage_client.ListBucket(('/%s' % bucket), prefix=('%s/outputs/part-' % hadoop_dir)):\n        logging.debug('Hadoop result file: %s', hadoop_result)\n        hadoop_output = self.cloud_storage_client.OpenObject(hadoop_result)\n        for line in hadoop_output:\n            output_file.write(tab_strip_pattern.sub('\\n', line))\n    output_file.close()\n", "label": 0}
{"function": "\n\ndef file_upload_view_verify(request):\n    '\\n    Use the sha digest hash to verify the uploaded contents.\\n    '\n    form_data = request.POST.copy()\n    form_data.update(request.FILES)\n    for (key, value) in form_data.items():\n        if key.endswith('_hash'):\n            continue\n        if ((key + '_hash') not in form_data):\n            continue\n        submitted_hash = form_data[(key + '_hash')]\n        if isinstance(value, UploadedFile):\n            new_hash = hashlib.sha1(value.read()).hexdigest()\n        else:\n            new_hash = hashlib.sha1(force_bytes(value)).hexdigest()\n        if (new_hash != submitted_hash):\n            return HttpResponseServerError()\n    largefile = request.FILES['file_field2']\n    obj = FileModel()\n    obj.testfile.save(largefile.name, largefile)\n    return HttpResponse('')\n", "label": 1}
{"function": "\n\ndef __init__(self, name, loop_chain, tile_size):\n    if self._initialized:\n        return\n    if (not hasattr(self, '_inspected')):\n        self._inspected = 0\n    self._name = name\n    self._tile_size = tile_size\n    self._loop_chain = loop_chain\n", "label": 0}
{"function": "\n\ndef _limit(self, uri, comment):\n    rv = self.db.execute(['SELECT id FROM comments WHERE remote_addr = ? AND ? - created < 60;'], (comment['remote_addr'], time.time())).fetchall()\n    if (len(rv) >= self.conf.getint('ratelimit')):\n        return (False, '{0}: ratelimit exceeded ({1})'.format(comment['remote_addr'], ', '.join(Guard.ids(rv))))\n    if (comment['parent'] is None):\n        rv = self.db.execute(['SELECT id FROM comments WHERE', '    tid = (SELECT id FROM threads WHERE uri = ?)', 'AND remote_addr = ?', 'AND parent IS NULL;'], (uri, comment['remote_addr'])).fetchall()\n        if (len(rv) >= self.conf.getint('direct-reply')):\n            return (False, ('%i direct responses to %s' % (len(rv), uri)))\n    elif (self.conf.getboolean('reply-to-self') == False):\n        rv = self.db.execute(['SELECT id FROM comments WHERE    remote_addr = ?', 'AND id = ?', 'AND ? - created < ?'], (comment['remote_addr'], comment['parent'], time.time(), self.max_age)).fetchall()\n        if (len(rv) > 0):\n            return (False, 'edit time frame is still open')\n    if (self.conf.getboolean('require-email') and (not comment.get('email'))):\n        return (False, 'email address required but not provided')\n    return (True, '')\n", "label": 1}
{"function": "\n\ndef search(self, query, **kwargs):\n    qstring = query['query']['query_string']['query']\n    if (qstring in self._queries):\n        return load_by_bug(self._queries[qstring])\n    return load_empty()\n", "label": 0}
{"function": "\n\ndef execute(self, content):\n    command = self.get_link_command()\n    if (not command):\n        sublime.error_message('Could not get link opener command.\\nPlatform not yet supported.')\n        return None\n    if (sys.version_info[0] < 3):\n        content = content.encode(sys.getfilesystemencoding())\n    cmd = (command + [content])\n    arg_list_wrapper = self.settings.get('orgmode.open_link.resolver.abstract.arg_list_wrapper', [])\n    if arg_list_wrapper:\n        cmd = (arg_list_wrapper + [' '.join(cmd)])\n        source_filename = (('\"' + self.view.file_name()) + '\"')\n        cmd += [source_filename]\n        if (sys.platform != 'win32'):\n            cmd += ['--origin', source_filename, '--quiet']\n    print('*****')\n    print(repr(content), content)\n    print(cmd)\n    sublime.status_message(('Executing: %s' % cmd))\n    if (sys.platform != 'win32'):\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    else:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    (stdout, stderr) = process.communicate()\n    if stdout:\n        stdout = str(stdout, sys.getfilesystemencoding())\n        sublime.status_message(stdout)\n    if stderr:\n        stderr = str(stderr, sys.getfilesystemencoding())\n        sublime.error_message(stderr)\n", "label": 1}
{"function": "\n\ndef test_handle_router_snat_rules_add_back_jump(self):\n    ri = l3router.RouterInfo(_uuid(), {\n        \n    }, **self.ri_kwargs)\n    ri.iptables_manager = mock.MagicMock()\n    port = {\n        'fixed_ips': [{\n            'ip_address': '192.168.1.4',\n        }],\n    }\n    ri._handle_router_snat_rules(port, 'iface')\n    nat = ri.iptables_manager.ipv4['nat']\n    nat.empty_chain.assert_any_call('snat')\n    nat.add_rule.assert_any_call('snat', '-j $float-snat')\n    for call in nat.mock_calls:\n        (name, args, kwargs) = call\n        if (name == 'add_rule'):\n            self.assertEqual(('snat', '-j $float-snat'), args)\n            self.assertEqual({\n                \n            }, kwargs)\n            break\n", "label": 0}
{"function": "\n\ndef test_should_exclude_with__returns_false_with_disabled_tag_and_more(self):\n    traits = self.traits\n    test_patterns = [([traits.category1_enabled_tag, traits.category1_disabled_tag], 'case: first'), ([traits.category1_disabled_tag, traits.category1_enabled_tag], 'case: last'), (['foo', traits.category1_enabled_tag, traits.category1_disabled_tag, 'bar'], 'case: middle')]\n    enabled = True\n    for (tags, case) in test_patterns:\n        self.assertEqual((not enabled), self.tag_matcher.should_exclude_with(tags), ('%s: tags=%s' % (case, tags)))\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TSentryPrivilegeMap')\n    if (self.privilegeMap is not None):\n        oprot.writeFieldBegin('privilegeMap', TType.MAP, 1)\n        oprot.writeMapBegin(TType.STRING, TType.SET, len(self.privilegeMap))\n        for (kiter104, viter105) in self.privilegeMap.items():\n            oprot.writeString(kiter104)\n            oprot.writeSetBegin(TType.STRUCT, len(viter105))\n            for iter106 in viter105:\n                iter106.write(oprot)\n            oprot.writeSetEnd()\n        oprot.writeMapEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\n@classmethod\ndef handle_token(cls, parser, token):\n    bits = token.split_contents()\n    if ((len(bits) == 3) and (bits[1] == 'as')):\n        return cls(bits[2])\n    elif ((len(bits) == 4) and (bits[2] == 'as')):\n        return cls(bits[3], bits[1])\n    else:\n        raise template.TemplateSyntaxError((\"%r takes 'as var' or 'level as var'\" % bits[0]))\n", "label": 0}
{"function": "\n\ndef occurrence_view(request, event_pk, pk, template='swingtime/occurrence_detail.html', form_class=forms.SingleOccurrenceForm):\n    '\\n    View a specific occurrence and optionally handle any updates.\\n    \\n    Context parameters:\\n    \\n    ``occurrence``\\n        the occurrence object keyed by ``pk``\\n\\n    ``form``\\n        a form object for updating the occurrence\\n    '\n    occurrence = get_object_or_404(Occurrence, pk=pk, event__pk=event_pk)\n    if (request.method == 'POST'):\n        form = form_class(request.POST, instance=occurrence)\n        if form.is_valid():\n            form.save()\n            return http.HttpResponseRedirect(request.path)\n    else:\n        form = form_class(instance=occurrence)\n    return render(request, template, {\n        'occurrence': occurrence,\n        'form': form,\n    })\n", "label": 0}
{"function": "\n\ndef testWhitelisted(self):\n    mvp = vcluster.MakeVirtualPath\n    for path in vcluster._VPATH_WHITELIST:\n        self.assertEqual(mvp(path), path)\n        self.assertEqual(mvp(path, _noderoot=None), path)\n        self.assertEqual(mvp(path, _noderoot='/tmp'), path)\n", "label": 0}
{"function": "\n\ndef depack(self, args):\n    self.is_touch = True\n    self.sx = args['x']\n    self.sy = args['y']\n    self.profile = ['pos']\n    if (('size_w' in args) and ('size_h' in args)):\n        self.shape = ShapeRect()\n        self.shape.width = args['size_w']\n        self.shape.height = args['size_h']\n        self.profile.append('shape')\n    if ('pressure' in args):\n        self.pressure = args['pressure']\n        self.profile.append('pressure')\n    super(MTDMotionEvent, self).depack(args)\n", "label": 0}
{"function": "\n\ndef send(self, message, flags=0, copy=False, track=False):\n    zmq_msg = ffi.new('zmq_msg_t*')\n    c_message = ffi.new('char[]', message)\n    C.zmq_msg_init_size(zmq_msg, len(message))\n    C.memcpy(C.zmq_msg_data(zmq_msg), c_message, len(message))\n    if (zmq_version == 2):\n        ret = C.zmq_send(self.zmq_socket, zmq_msg, flags)\n    else:\n        ret = C.zmq_sendmsg(self.zmq_socket, zmq_msg, flags)\n    C.zmq_msg_close(zmq_msg)\n    if (ret < 0):\n        self.last_errno = C.zmq_errno()\n    return ret\n", "label": 0}
{"function": "\n\ndef GetBlendMethod(self):\n    'Get the blend method'\n    currentMethod = self.component.PropertyList.Find('SourceBlendMode').Data\n    for (method, idx) in self.kBlendMethods.iteritems():\n        if (currentMethod == idx):\n            return method\n", "label": 0}
{"function": "\n\ndef anno(self, node):\n    if ((node.type is None) and (not getattr(node, 'escapes', False))):\n        return\n    if isinstance(node.type, (types.function, types.Type)):\n        return\n    self.write(' [')\n    if (node.type is not None):\n        self.visit(node.type)\n    if ((node.type is not None) and getattr(node, 'escapes', False)):\n        self.write(':')\n    if getattr(node, 'escapes', False):\n        self.write('E')\n    self.write(']')\n", "label": 1}
{"function": "\n\ndef _read_track_origin(self, group):\n    self.track_origin = group.attrs['track_origin'].decode('ascii')\n    if ('track_n_scat' in group.attrs):\n        self.track_n_scat = group.attrs['track_n_scat']\n    else:\n        self.track_n_scat = 0\n", "label": 0}
{"function": "\n\ndef visit_NVARCHAR(self, type_, **kw):\n    if type_.length:\n        return self._extend_string(type_, {\n            'national': True,\n        }, ('VARCHAR(%(length)s)' % {\n            'length': type_.length,\n        }))\n    else:\n        raise exc.CompileError(('NVARCHAR requires a length on dialect %s' % self.dialect.name))\n", "label": 0}
{"function": "\n\n@webob.dec.wsgify\ndef process_request(self, req):\n    if (req.path != self._path):\n        return None\n    results = [ext.obj.healthcheck(req.server_port) for ext in self._backends]\n    healthy = self._are_results_healthy(results)\n    if (req.method == 'HEAD'):\n        functor = self._make_head_response\n        status = self.HEAD_HEALTHY_TO_STATUS_CODES[healthy]\n    else:\n        status = self.HEALTHY_TO_STATUS_CODES[healthy]\n        accept_type = req.accept.best_match(self._accept_order)\n        if (not accept_type):\n            accept_type = self._default_accept\n        functor = self._accept_to_functor[accept_type]\n    (body, content_type) = functor(results, healthy)\n    return webob.response.Response(status=status, body=body, content_type=content_type)\n", "label": 0}
{"function": "\n\ndef strip_units(self):\n    '\\n        Strips units from an xypoint structure.\\n\\n        Returns:\\n          A copy of the xypoint with no units\\n          The x-units\\n          the y-units\\n        '\n    xunits = (self.x.unit if isinstance(self.x, u.Quantity) else 1.0)\n    yunits = (self.y.unit if isinstance(self.y, u.Quantity) else 1.0)\n    x = (self.x.value if isinstance(self.x, u.Quantity) else self.x)\n    y = (self.y.value if isinstance(self.y, u.Quantity) else self.y)\n    err = (self.err.value if isinstance(self.err, u.Quantity) else self.err)\n    cont = (self.cont.value if isinstance(self.cont, u.Quantity) else self.cont)\n    return (xypoint(x=x, y=y, cont=cont, err=err), xunits, yunits)\n", "label": 1}
{"function": "\n\ndef get_shipping_method(self, basket, shipping_address=None, **kwargs):\n    '\\n        Return the selected shipping method instance from this checkout session\\n\\n        The shipping address is passed as we need to check that the method\\n        stored in the session is still valid for the shipping address.\\n        '\n    code = self.checkout_session.shipping_method_code(basket)\n    methods = Repository().get_shipping_methods(basket=basket, user=self.request.user, shipping_addr=shipping_address, request=self.request)\n    for method in methods:\n        if (method.code == code):\n            return method\n", "label": 0}
{"function": "\n\n@classmethod\ndef get_template(cls, message, messenger):\n    'Get a template path to compile a message.\\n\\n        1. `tpl` field of message context;\\n        2. `template` field of message class;\\n        3. deduced from message, messenger data and `template_ext` message type field\\n           (e.g. `sitemessage/messages/plain__smtp.txt` for `plain` message type).\\n\\n        :param Message message: Message model\\n        :param MessengerBase messenger: a MessengerBase heir\\n        :return: str\\n        :rtype: str\\n        '\n    template = message.context.get('tpl', None)\n    if template:\n        return template\n    if (cls.template is None):\n        cls.template = ('sitemessage/messages/%s__%s.%s' % (cls.get_alias(), messenger.get_alias(), cls.template_ext))\n    return cls.template\n", "label": 0}
{"function": "\n\ndef teardown(self):\n    for key in self._event_fns:\n        event.remove(*key)\n    super_ = super(RemovesEvents, self)\n    if hasattr(super_, 'teardown'):\n        super_.teardown()\n", "label": 0}
{"function": "\n\ndef process_data(self):\n    polar_data = build_wedge_source(self._data.df, cat_cols=self.attributes['label'].columns, agg_col=self.values.selection, agg=self.agg, level_width=self.level_width, level_spacing=self.level_spacing)\n    polar_data['color'] = ''\n    for group in self._data.groupby(**self.attributes):\n        polar_data.loc[(group['stack'], 'color')] = group['color']\n    self.chart_data = ColumnDataSource(polar_data)\n    self.text_data = build_wedge_text_source(polar_data)\n", "label": 0}
{"function": "\n\n@db_access\ndef _getContainerField(container, field, default):\n    ' Returns the metadata field for the given container or the default value. '\n    container_id = _getContainerId(container)\n    found = _getContainerFieldRecord(container_id, field)\n    return (found.value if found else default)\n", "label": 0}
{"function": "\n\ndef map(self, path):\n    'Map `path` through the aliases.\\n\\n        `path` is checked against all of the patterns.  The first pattern to\\n        match is used to replace the root of the path with the result root.\\n        Only one pattern is ever used.  If no patterns match, `path` is\\n        returned unchanged.\\n\\n        The separator style in the result is made to match that of the result\\n        in the alias.\\n\\n        '\n    for (regex, result, pattern_sep, result_sep) in self.aliases:\n        m = regex.match(path)\n        if m:\n            new = path.replace(m.group(0), result)\n            if (pattern_sep != result_sep):\n                new = new.replace(pattern_sep, result_sep)\n            if self.locator:\n                new = self.locator.canonical_filename(new)\n            return new\n    return path\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _filter_pools_for_numa_cells(pools, numa_cells):\n    numa_cells = ([None] + [cell.id for cell in numa_cells])\n    return [pool for pool in pools if any((utils.pci_device_prop_match(pool, [{\n        'numa_node': cell,\n    }]) for cell in numa_cells))]\n", "label": 0}
{"function": "\n\ndef _setup(self):\n    'Initiate lists for objects contained within this object'\n    for field in self._contains:\n        setattr(self, field, [])\n", "label": 0}
{"function": "\n\n@classmethod\ndef validate(cls, value):\n    if (not value):\n        return value\n    value = os.path.abspath(value)\n    dirname = os.path.dirname(value)\n    if os.path.isfile(value):\n        if (not os.access(value, os.W_OK)):\n            raise config_option.BadValue('You do not have write permissions')\n        if (not os.access(dirname, os.W_OK)):\n            raise config_option.BadValue(('You do not have write permissions for \"%s\"' % dirname))\n        return value\n    elif os.path.isdir(value):\n        raise config_option.BadValue(('\"%s\" is a directory' % value))\n    else:\n        if os.path.isdir(dirname):\n            if (not os.access(dirname, os.W_OK)):\n                raise config_option.BadValue(('You do not have write permissions for \"%s\"' % dirname))\n            return value\n        previous_dir = os.path.dirname(dirname)\n        if (not os.path.isdir(previous_dir)):\n            raise config_option.BadValue(('\"%s\" not found' % value))\n        if (not os.access(previous_dir, os.W_OK)):\n            raise config_option.BadValue(('You do not have write permissions for \"%s\"' % previous_dir))\n        return value\n", "label": 1}
{"function": "\n\ndef update(self, x1, x2, y):\n    self.phase = 'train'\n    for layer in self.layers:\n        x1 = layer.fprop(x1)\n    for layer in self.layers2:\n        x2 = layer.fprop(x2)\n    (grad1, grad2) = self.loss.grad(y, x1, x2)\n    layers = self.layers[self.bprop_until:]\n    for layer in reversed(layers[1:]):\n        grad1 = layer.bprop(grad1)\n    layers[0].bprop(grad1)\n    layers2 = self.layers2[self.bprop_until:]\n    for layer in reversed(layers2[1:]):\n        grad2 = layer.bprop(grad2)\n    layers2[0].bprop(grad2)\n    return self.loss.loss(y, x1, x2)\n", "label": 0}
{"function": "\n\ndef type_continue(self, node):\n    return self.__addSemicolon(('continue' if (not hasattr(node, 'label')) else ('continue %s' % node.label)))\n", "label": 0}
{"function": "\n\ndef __onNewValues(self, dateTime, value):\n    if (self.__range is None):\n        self.__range = resamplebase.build_range(dateTime, self.getFrequency())\n        self.__grouper = BarsGrouper(self.__range.getBeginning(), value, self.getFrequency())\n    elif self.__range.belongs(dateTime):\n        self.__grouper.addValue(value)\n    else:\n        self.__values.append(self.__grouper.getGrouped())\n        self.__range = resamplebase.build_range(dateTime, self.getFrequency())\n        self.__grouper = BarsGrouper(self.__range.getBeginning(), value, self.getFrequency())\n", "label": 0}
{"function": "\n\ndef __init__(self, client, data=None):\n    super(Zone, self).__init__()\n    self.client = client\n    self.zone_type = 'temperatureZone'\n    if (data is not None):\n        self.__dict__.update(data)\n", "label": 0}
{"function": "\n\ndef register(self, field_type, impl=None):\n    '\\n        Register form field data function.\\n        \\n        Could be used as decorator\\n        '\n\n    def _wrapper(func):\n        self.registry[field_type] = func\n        return func\n    if impl:\n        return _wrapper(impl)\n    return _wrapper\n", "label": 0}
{"function": "\n\ndef scaffold_auto_joins(self):\n    '\\n            Return a list of joined tables by going through the\\n            displayed columns.\\n        '\n    if (not self.column_auto_select_related):\n        return []\n    relations = set()\n    for p in self._get_model_iterator():\n        if hasattr(p, 'direction'):\n            if (p.mapper.class_ == self.model):\n                continue\n            if (p.direction.name in ['MANYTOONE', 'MANYTOMANY']):\n                relations.add(p.key)\n    joined = []\n    for (prop, name) in self._list_columns:\n        if (prop in relations):\n            joined.append(getattr(self.model, prop))\n    return joined\n", "label": 1}
{"function": "\n\n@log_debug\ndef generate_room(self, section):\n    '\\n        Generate room\\n\\n        :param section: section for generator to draw to\\n        :type section: Section\\n        '\n    self.square_generator.generate_room(section)\n    offset = [(1, 1), ((- 1), 1), ((- 1), (- 1)), (1, (- 1))]\n    for (index, corner) in enumerate(self.square_generator.room_corners):\n        self.add_pillar(section, corner, offset[index])\n", "label": 0}
{"function": "\n\ndef do_register_opts(opts, group=None, ignore_errors=False):\n    try:\n        cfg.CONF.register_opts(opts, group=group)\n    except:\n        if (not ignore_errors):\n            raise\n", "label": 0}
{"function": "\n\ndef get_select_precolumns(self, select):\n    'Called when building a ``SELECT`` statement, position is just\\n        before column list.\\n\\n        '\n    return ((select._distinct and 'DISTINCT ') or '')\n", "label": 0}
{"function": "\n\ndef visit_WaitStatement(self, node):\n    filename = getfilename(node)\n    template = self.get_template(filename)\n    template_dict = {\n        'cond': del_paren(self.visit(node.cond)),\n        'statement': (self.visit(node.statement) if node.statement else ''),\n    }\n    rslt = template.render(template_dict)\n    return rslt\n", "label": 0}
{"function": "\n\ndef on_text_changed(self):\n    \" Handle the 'textChanged' signal on the widget.\\n\\n        \"\n    d = self.declaration\n    if (d is not None):\n        d.text_changed()\n", "label": 0}
{"function": "\n\ndef onBrowserEvent(self, event):\n    etype = DOM.eventGetType(event)\n    if (etype == 'mousewheel'):\n        if self._mouseWheelPreventDefault:\n            DOM.eventPreventDefault(event)\n        velocity = DOM.eventGetMouseWheelVelocityY(event)\n        for listener in self._mouseWheelListeners:\n            listener.onMouseWheel(self, velocity)\n        return True\n", "label": 0}
{"function": "\n\ndef raw_field_definition_proxy_post_save(sender, instance, raw, **kwargs):\n    \"\\n    When proxy field definitions are loaded from a fixture they're not\\n    passing through the `field_definition_post_save` signal. Make sure they\\n    are.\\n    \"\n    if raw:\n        model_class = instance.content_type.model_class()\n        opts = model_class._meta\n        if (opts.proxy and (opts.concrete_model is sender)):\n            field_definition_post_save(sender=model_class, instance=instance.type_cast(), raw=raw, **kwargs)\n", "label": 0}
{"function": "\n\ndef test_moves_a_block_up_within_a_container(self):\n    for (idx, pos) in [(0, 0), (1, 1), (2, 2)]:\n        block = TextBlock.objects.get(id=self.left_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n    self.app.put(reverse('fp-api:block-move', kwargs={\n        'uuid': self.main_blocks[1].uuid,\n    }), params={\n        'container': self.left_container.uuid,\n        'index': 1,\n    }, user=self.user)\n    moved_block = TextBlock.objects.get(id=self.main_blocks[1].id)\n    self.assertEquals(moved_block.container, self.page.get_container_from_name('left-container'))\n    self.assertEquals(moved_block.display_order, 1)\n    for (idx, pos) in [(0, 0), (1, 2), (2, 3)]:\n        block = TextBlock.objects.get(id=self.left_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n    for (idx, pos) in [(0, 0), (2, 1)]:\n        block = TextBlock.objects.get(id=self.main_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n", "label": 0}
{"function": "\n\ndef run_cmd(self, util, toggle_active_mark_mode=False):\n    if (util.state.argument_supplied or toggle_active_mark_mode):\n        util.toggle_active_mark_mode()\n    else:\n        util.swap_point_and_mark()\n", "label": 0}
{"function": "\n\ndef test_asizer_limit(self):\n    'Test limit setting for Asizer.\\n        '\n    objs = [Foo(42), ThinFoo('spam'), OldFoo(67)]\n    sizer = [asizeof.Asizer() for _ in range(4)]\n    for (limit, asizer) in enumerate(sizer):\n        asizer.asizeof(objs, limit=limit)\n    limit_sizes = [asizer.total for asizer in sizer]\n    self.assertTrue((limit_sizes[0] < limit_sizes[1]), limit_sizes)\n    self.assertTrue((limit_sizes[1] < limit_sizes[2]), limit_sizes)\n    self.assertTrue((limit_sizes[2] < limit_sizes[3]), limit_sizes)\n", "label": 0}
{"function": "\n\ndef _op(self, method, other):\n    if isinstance(other, Counter):\n        other = other.value()\n    if (not isinstance(other, int)):\n        raise TypeError(('Cannot add %s, not an integer.' % other))\n    method(other)\n    return self\n", "label": 0}
{"function": "\n\ndef test_neighbors(self):\n    graph = nx.complete_graph(100)\n    pop = random.sample(list(graph), 1)\n    nbors = list(nx.neighbors(graph, pop[0]))\n    assert_equal(len(nbors), (len(graph) - 1))\n    graph = nx.path_graph(100)\n    node = random.sample(list(graph), 1)[0]\n    nbors = list(nx.neighbors(graph, node))\n    if ((node != 0) and (node != 99)):\n        assert_equal(len(nbors), 2)\n    else:\n        assert_equal(len(nbors), 1)\n    graph = nx.star_graph(99)\n    nbors = list(nx.neighbors(graph, 0))\n    assert_equal(len(nbors), 99)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super().clean()\n    actions = Action.objects.in_bulk(cleaned_data['actions'])\n    attachment_counter = 0\n    one_action = False\n    one_action_name = ''\n    actions_items = actions.items()\n    for (k, v) in actions_items:\n        action = getattr(self.model, v.name)\n        if getattr(action, 'return_attachment', False):\n            attachment_counter += 1\n        if getattr(action, 'only_one_action', False):\n            one_action = True\n            one_action_name = getattr(action, 'verbose_name', '')\n    if (attachment_counter > 1):\n        msg = _('Please select at most one action which return attachment.')\n        self.add_error('actions', msg)\n    if (one_action and (len(actions_items) > 1)):\n        msg = (_('You have chosen action: %(name)s can only be selected for transition') % {\n            'name': one_action_name,\n        })\n        self.add_error('actions', msg)\n    return cleaned_data\n", "label": 1}
{"function": "\n\ndef list_hosts(self, filters):\n    host_data_list = (self._hosts_collection().find(filters) or [])\n    return [host.Host.from_dict(h_data, conf=self.config) for h_data in host_data_list]\n", "label": 0}
{"function": "\n\ndef input_pins(self, pins):\n    'Read multiple pins specified in the given list and return list of pin values\\n        GPIO.HIGH/True if the pin is pulled high, or GPIO.LOW/False if pulled low.\\n        '\n    return [self.input(pin) for pin in pins]\n", "label": 0}
{"function": "\n\ndef _collapse(intervals):\n    '\\n    Collapse an iterable of intervals sorted by start coord.\\n    \\n    '\n    span = None\n    for (start, stop) in intervals:\n        if (span is None):\n            span = _Interval(start, stop)\n        elif (start <= span.stop < stop):\n            span = _Interval(span.start, stop)\n        elif (start > span.stop):\n            (yield span)\n            span = _Interval(start, stop)\n    if (span is not None):\n        (yield span)\n", "label": 1}
{"function": "\n\ndef claim_invitations(user):\n    \"Claims any pending invitations for the given user's email address.\"\n    invitation_user_id = ('%s:%s' % (models.User.EMAIL_INVITATION, user.email_address))\n    invitation_user = models.User.query.get(invitation_user_id)\n    if invitation_user:\n        invited_build_list = list(invitation_user.builds)\n        if (not invited_build_list):\n            return\n        db.session.add(user)\n        logging.debug('Found %d build admin invitations for id=%r, user=%r', len(invited_build_list), invitation_user_id, user)\n        for build in invited_build_list:\n            build.owners.remove(invitation_user)\n            if (not build.is_owned_by(user.id)):\n                build.owners.append(user)\n                logging.debug('Claiming invitation for build_id=%r', build.id)\n                save_admin_log(build, invite_accepted=True)\n            else:\n                logging.debug('User already owner of build. id=%r, build_id=%r', user.id, build.id)\n            db.session.add(build)\n        db.session.delete(invitation_user)\n        db.session.commit()\n        db.session.add(current_user)\n", "label": 0}
{"function": "\n\ndef process_exception(self, request, exception):\n    if (settings.DEBUG or isinstance(exception, Http404)):\n        return None\n    if isinstance(exception, apiproxy_errors.CapabilityDisabledError):\n        msg = 'Rietveld: App Engine is undergoing maintenance. Please try again in a while.'\n        status = 503\n    elif isinstance(exception, (DeadlineExceededError, MemoryError)):\n        msg = 'Rietveld is too hungry at the moment.Please try again in a while.'\n        status = 503\n    else:\n        msg = 'Unhandled exception.'\n        status = 500\n    logging.exception(('%s: ' % exception.__class__.__name__))\n    technical = ('%s [%s]' % (exception, exception.__class__.__name__))\n    if self._text_requested(request):\n        content = ('%s\\n\\n%s\\n' % (msg, technical))\n        content_type = 'text/plain'\n    else:\n        tpl = loader.get_template('exception.html')\n        ctx = Context({\n            'msg': msg,\n            'technical': technical,\n        })\n        content = tpl.render(ctx)\n        content_type = 'text/html'\n    return HttpResponse(content, status=status, content_type=content_type)\n", "label": 1}
{"function": "\n\n@property\ndef autofit(self):\n    \"\\n        Return |False| if there is a ``<w:tblLayout>`` child with ``w:type``\\n        attribute set to ``'fixed'``. Otherwise return |True|.\\n        \"\n    tblLayout = self.tblLayout\n    if (tblLayout is None):\n        return True\n    return (False if (tblLayout.type == 'fixed') else True)\n", "label": 0}
{"function": "\n\ndef get_episode(self, url, imdb, tvdb, title, date, season, episode):\n    try:\n        if (url == None):\n            return\n        url = urlparse.urljoin(self.base_link, url)\n        (season, episode) = (('%01d' % int(season)), ('%01d' % int(episode)))\n        result = client.source(url)\n        if (not (season == '1')):\n            url = client.parseDOM(result, 'a', ret='href', attrs={\n                'class': 'season-.+?',\n            })\n            url = [i for i in url if (('/%s-sezon-' % season) in i)][0]\n            result = client.source(url)\n        result = client.parseDOM(result, 'a', ret='href')\n        result = [i for i in result if (('%s-sezon-%s-bolum-' % (season, episode)) in i)][0]\n        try:\n            url = re.compile('//.+?(/.+)').findall(result)[0]\n        except:\n            url = result\n        url = client.replaceHTMLCodes(url)\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef update_dimension_fields(self, instance, force=False, *args, **kwargs):\n    if ((getattr(instance, 'mimetype', None) is not None) and ('image' in instance.mimetype)):\n        super(FileField, self).update_dimension_fields(instance, force, *args, **kwargs)\n    else:\n        pass\n", "label": 0}
{"function": "\n\ndef HKDF_extract(salt, IKM, hashmod=hashlib.sha256):\n    'HKDF-Extract; see RFC-5869 for the details.'\n    if (salt is None):\n        salt = (b'\\x00' * hashmod().digest_size)\n    if isinstance(salt, text_type):\n        salt = salt.encode('utf-8')\n    return python_hmac.new(salt, IKM, hashmod).digest()\n", "label": 0}
{"function": "\n\ndef _handle_object_info_reply(self, rep):\n    ' Handle replies for call tips.\\n        '\n    cursor = self._get_cursor()\n    info = self._request_info.get('call_tip')\n    if (info and (info.id == rep['parent_header']['msg_id']) and (info.pos == cursor.position())):\n        content = rep['content']\n        if content.get('ismagic', False):\n            (call_info, doc) = (None, None)\n        else:\n            (call_info, doc) = call_tip(content, format_call=True)\n        if (call_info or doc):\n            self._call_tip_widget.show_call_info(call_info, doc)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _options(fieldname):\n    \"\\n            Lookup the full set of options for a Filter Widget\\n            - for Subscriptions we don't want to see just the options available in current data\\n        \"\n    db = current.db\n    if (fieldname == 'location_id'):\n        table = current.s3db.gis_location\n        query = ((table.deleted == False) & (table.level == 'L1'))\n        rows = db(query).select(table.id)\n        options = [row.id for row in rows]\n    return options\n", "label": 0}
{"function": "\n\ndef create_user_profile(sender, instance, created, **kwargs):\n    if created:\n        UserProfile.objects.create(user=instance)\n", "label": 0}
{"function": "\n\ndef test_radius_neighbors_classifier_when_no_neighbors():\n    X = np.array([[1.0, 1.0], [2.0, 2.0]])\n    y = np.array([1, 2])\n    radius = 0.1\n    z1 = np.array([[1.01, 1.01], [2.01, 2.01]])\n    z2 = np.array([[1.01, 1.01], [1.4, 1.4]])\n    weight_func = _weight_func\n    for outlier_label in [0, (- 1), None]:\n        for algorithm in ALGORITHMS:\n            for weights in ['uniform', 'distance', weight_func]:\n                rnc = neighbors.RadiusNeighborsClassifier\n                clf = rnc(radius=radius, weights=weights, algorithm=algorithm, outlier_label=outlier_label)\n                clf.fit(X, y)\n                assert_array_equal(np.array([1, 2]), clf.predict(z1))\n                if (outlier_label is None):\n                    assert_raises(ValueError, clf.predict, z2)\n                elif False:\n                    assert_array_equal(np.array([1, outlier_label]), clf.predict(z2))\n", "label": 1}
{"function": "\n\ndef get(self, attributes, key, value=None):\n    key = (key, self.path)\n    if (key in attributes):\n        return attributes[key]\n    else:\n        return value\n", "label": 0}
{"function": "\n\n@fluff.date_emitter\ndef reviewed_due_to_registration(self, form):\n    if (self._get_total(form, BOOKING_FORMS, 'reviewed') == True):\n        (yield [form.received_on, 1])\n", "label": 0}
{"function": "\n\ndef addNode(self, value):\n    if (not self.head):\n        self._addFirst(value)\n    else:\n        self._add(value)\n", "label": 0}
{"function": "\n\ndef OutputPartial(self, out):\n    if self.has_package_:\n        out.putVarInt32(10)\n        out.putPrefixedString(self.package_)\n    for i in xrange(len(self.capability_)):\n        out.putVarInt32(18)\n        out.putPrefixedString(self.capability_[i])\n    for i in xrange(len(self.call_)):\n        out.putVarInt32(26)\n        out.putPrefixedString(self.call_[i])\n", "label": 0}
{"function": "\n\ndef check_records(self, template, stream):\n    expected = [line.format(self.filename) for line in template]\n    records = ''.join(stream.buflist).splitlines()\n    self.assertEqual(records, expected)\n", "label": 0}
{"function": "\n\ndef fetchall(self):\n    return [self.format_results(row) for row in self.cursor.fetchall()]\n", "label": 0}
{"function": "\n\ndef startbody(self, ctype, plist=[], prefix=1):\n    'Returns a file-like object for writing the body of the message.\\n\\n        The content-type is set to the provided ctype, and the optional\\n        parameter, plist, provides additional parameters for the\\n        content-type declaration.  The optional argument prefix determines\\n        where the header is inserted; 0 means append at the end, 1 means\\n        insert at the start. The default is to insert at the start.\\n\\n        '\n    for (name, value) in plist:\n        ctype = (ctype + (';\\n %s=\"%s\"' % (name, value)))\n    self.addheader('Content-Type', ctype, prefix=prefix)\n    self.flushheaders()\n    self._fp.write('\\n')\n    return self._fp\n", "label": 0}
{"function": "\n\ndef get_cert_store(self):\n    '\\n        Get the certificate store for the context.\\n\\n        :return: A X509Store object or None if it does not have one.\\n        '\n    store = _lib.SSL_CTX_get_cert_store(self._context)\n    if (store == _ffi.NULL):\n        return None\n    pystore = X509Store.__new__(X509Store)\n    pystore._store = store\n    return pystore\n", "label": 0}
{"function": "\n\ndef has_valid_checksum(self, number):\n    (given_number, given_checksum) = (number[:(- 1)], number[(- 1)])\n    calculated_checksum = 0\n    fragment = ''\n    parameter = 7\n    for i in range(len(given_number)):\n        fragment = str((int(given_number[i]) * parameter))\n        if fragment.isalnum():\n            calculated_checksum += int(fragment[(- 1)])\n        if (parameter == 1):\n            parameter = 7\n        elif (parameter == 3):\n            parameter = 1\n        elif (parameter == 7):\n            parameter = 3\n    return (str(calculated_checksum)[(- 1)] == given_checksum)\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Collects CProfile stats for specified Python program.'\n    if self._run_args:\n        sys.argv[:] = [self._run_object, self._run_args]\n    else:\n        sys.argv[:] = [self._run_object]\n    prof = cProfile.Profile()\n    run_dispatcher = self.get_run_dispatcher()\n    run_dispatcher(prof)\n    prof.create_stats()\n    cprofile_stats = pstats.Stats(prof)\n    return {\n        'programName': self._object_name,\n        'runTime': cprofile_stats.total_tt,\n        'primitiveCalls': cprofile_stats.prim_calls,\n        'totalCalls': cprofile_stats.total_calls,\n        'callStats': self._transform_stats(cprofile_stats),\n    }\n", "label": 0}
{"function": "\n\ndef _wait_async_done(self, reservation_id, reqids):\n    \"\\n        _wait_async_done(session_id, reqids)\\n        Helper methods that waits for the specified asynchronous requests to be finished,\\n        and which asserts that they were successful. Note that it doesn't actually return\\n        their responses.\\n        @param reqids Tuple containing the request ids for the commands to check.\\n        @return Nothing\\n        \"\n    reqsl = list(reqids)\n    max_count = 15\n    while (len(reqsl) > 0):\n        time.sleep(0.1)\n        max_count -= 1\n        if (max_count == 0):\n            raise Exception('Maximum time spent waiting async done')\n        requests = self.client.check_async_command_status(reservation_id, tuple(reqsl))\n        self.assertEquals(len(reqsl), len(requests))\n        for (rid, req) in six.iteritems(requests):\n            status = req[0]\n            self.assertTrue((status in ('running', 'ok', 'error')))\n            if (status != 'running'):\n                self.assertEquals('ok', status, ('Contents: ' + req[1]))\n                reqsl.remove(rid)\n", "label": 0}
{"function": "\n\ndef _importer(target):\n    components = target.split('.')\n    import_path = components.pop(0)\n    thing = __import__(import_path)\n    for comp in components:\n        import_path += ('.%s' % comp)\n        thing = _dot_lookup(thing, comp, import_path)\n    return thing\n", "label": 0}
{"function": "\n\ndef rotate(image, angle, center=None, scale=1.0):\n    (h, w) = image.shape[:2]\n    if (center is None):\n        center = ((w / 2), (h / 2))\n    M = cv2.getRotationMatrix2D(center, angle, scale)\n    rotated = cv2.warpAffine(image, M, (w, h))\n    return rotated\n", "label": 0}
{"function": "\n\ndef get_or_create_menu(self, key, verbose_name, side=LEFT, position=None):\n    if (key in self.menus):\n        return self.menus[key]\n    menu = SubMenu(verbose_name, self.csrf_token, side=side)\n    self.menus[key] = menu\n    self.add_item(menu, position=position)\n    return menu\n", "label": 0}
{"function": "\n\ndef l2cap_send(self, data):\n    dbg(('sending: [%s]' % data))\n    (status, written) = self._wc.write(data)\n    if status:\n        dbg('sent okay')\n        return written\n    else:\n        raise BluetoothError(_port_return_code_to_str(status))\n", "label": 0}
{"function": "\n\ndef list_names(self, **kwargs):\n    'Get a list of metric names.'\n    url_str = (self.base_url + '/names')\n    newheaders = self.get_headers()\n    if ('dimensions' in kwargs):\n        dimstr = self.get_dimensions_url_string(kwargs['dimensions'])\n        kwargs['dimensions'] = dimstr\n    if kwargs:\n        url_str = (url_str + ('?%s' % urlutils.urlencode(kwargs, True)))\n    (resp, body) = self.client.json_request('GET', url_str, headers=newheaders)\n    return (body['elements'] if (type(body) is dict) else body)\n", "label": 0}
{"function": "\n\ndef mayRaiseException(self, exception_type):\n    if self.tolerant:\n        return False\n    else:\n        if (self.variable_trace is not None):\n            variable = self.getTargetVariableRef().getVariable()\n            if variable.isTempVariable():\n                return False\n            if ((self.previous_trace is not None) and self.previous_trace.mustHaveValue()):\n                return False\n        return True\n", "label": 1}
{"function": "\n\ndef __exit__(self, exc_type, exc_value, traceback):\n    ':func:`burpui.misc.auth.ldap.LdapLoader.__exit__` closes the\\n        connection to the LDAP server.\\n        '\n    if (self.ldap and self.ldap.bound):\n        self.ldap.unbind()\n", "label": 0}
{"function": "\n\ndef str_slice_replace(arr, start=None, stop=None, repl=None):\n    '\\n    Replace a slice of each string in the Series/Index with another\\n    string.\\n\\n    Parameters\\n    ----------\\n    start : int or None\\n    stop : int or None\\n    repl : str or None\\n        String for replacement\\n\\n    Returns\\n    -------\\n    replaced : Series/Index of objects\\n    '\n    if (repl is None):\n        repl = ''\n\n    def f(x):\n        if (x[start:stop] == ''):\n            local_stop = start\n        else:\n            local_stop = stop\n        y = ''\n        if (start is not None):\n            y += x[:start]\n        y += repl\n        if (stop is not None):\n            y += x[local_stop:]\n        return y\n    return _na_map(f, arr)\n", "label": 0}
{"function": "\n\n@ConnectorExist(cid_key='smppc')\ndef smppc(self, arg, opts):\n    sc = SMPPClientStatsCollector()\n    headers = ['#Item', 'Value']\n    table = []\n    for (k, v) in sc.get(opts.smppc).getStats().iteritems():\n        if isinstance(v, dict):\n            v = json.dumps(v)\n        row = []\n        row.append(('#%s' % k))\n        if (k[(- 3):] == '_at'):\n            row.append(formatDateTime(v))\n        else:\n            row.append(v)\n        table.append(row)\n    self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))\n", "label": 0}
{"function": "\n\n@property\ndef event_description(self):\n    'complete description of this event in text form\\n\\n        :rtype: str\\n        :returns: event description\\n        '\n    location = (('\\nLocation: ' + self.location) if (self.location != '') else '')\n    description = (('\\nDescription: ' + self.description) if (self.description != '') else '')\n    repitition = (('\\nRepeat: ' + self.recurpattern) if (self.recurpattern != '') else '')\n    return '{}: {}{}{}{}'.format(self._rangestr, self.summary, location, repitition, description)\n", "label": 0}
{"function": "\n\ndef create(self):\n    ' Insert Action '\n    token = request.POST.pop('__token', '')\n    if (not self._is_token_match(token)):\n        success = False\n        error_message = 'Invalid Token'\n        data = None\n    else:\n        data = self.__model__()\n        data.set_state_insert()\n        data.assign_from_dict(request.POST)\n        data.save()\n        success = data.success\n        error_message = data.error_message\n    self._setup_view_parameter()\n    self._set_view_parameter(self.__model_name__, data)\n    self._set_view_parameter('success', success)\n    self._set_view_parameter('error_message', error_message)\n    if (request.is_xhr or request.POST.pop('__as_json', False)):\n        if success:\n            token = self._set_token()\n        self._set_view_parameter('__token', token)\n        return self._get_view_parameter_as_json()\n    return self._load_view('create')\n", "label": 0}
{"function": "\n\ndef is_editable(proposal, user):\n    return ((not proposal.scheduled) and (((proposal.proposer == user) and (proposal.status != 'A')) or topiclead(user, proposal.topic)))\n", "label": 0}
{"function": "\n\ndef visit_BIT(self, type_):\n    if type_.varying:\n        compiled = 'BIT VARYING'\n        if (type_.length is not None):\n            compiled += ('(%d)' % type_.length)\n    else:\n        compiled = ('BIT(%d)' % type_.length)\n    return compiled\n", "label": 0}
{"function": "\n\ndef get(self, **kwargs):\n    redirect_url = self.redirect_path()\n    if self.auth.user:\n        return self.redirect(redirect_url)\n    opts = {\n        'continue': self.redirect_path(),\n    }\n    context = {\n        'form': self.form,\n        'facebook_login_url': self.url_for('auth/facebook', **opts),\n        'friendfeed_login_url': self.url_for('auth/friendfeed', **opts),\n        'google_login_url': self.url_for('auth/google', **opts),\n        'twitter_login_url': self.url_for('auth/twitter', **opts),\n        'yahoo_login_url': self.url_for('auth/yahoo', **opts),\n    }\n    return self.render_response('login.html', **context)\n", "label": 0}
{"function": "\n\ndef text(self, value=no_default):\n    \"Get or set the text representation of sub nodes.\\n\\n        Get the text value::\\n\\n            >>> doc = PyQuery('<div><span>toto</span><span>tata</span></div>')\\n            >>> print(doc.text())\\n            toto tata\\n\\n        Set the text value::\\n\\n            >>> doc.text('Youhou !')\\n            [<div>]\\n            >>> print(doc)\\n            <div>Youhou !</div>\\n\\n        \"\n    if (value is no_default):\n        if (not self):\n            return None\n        text = []\n\n        def add_text(tag, no_tail=False):\n            if tag.text:\n                text.append(tag.text)\n            for child in tag.getchildren():\n                add_text(child)\n            if ((not no_tail) and tag.tail):\n                text.append(tag.tail)\n        for tag in self:\n            add_text(tag, no_tail=True)\n        return ' '.join([t.strip() for t in text if t.strip()])\n    for tag in self:\n        for child in tag.getchildren():\n            tag.remove(child)\n        tag.text = value\n    return self\n", "label": 1}
{"function": "\n\ndef encode(self, node):\n    if (not node):\n        return 'null'\n    else:\n        return str(node.val)\n", "label": 0}
{"function": "\n\ndef col_references_table(col, table):\n    for fk in col.foreign_keys:\n        if fk.references(table):\n            return True\n    return False\n", "label": 0}
{"function": "\n\n@auth\ndef _GET(self, *param, **params):\n    (host_id, guest_id) = self.chk_guestby1(param)\n    if (guest_id is None):\n        return web.notfound()\n    model = findbyguest1(self.orm, guest_id)\n    kvc = KaresansuiVirtConnection()\n    try:\n        domname = kvc.uuid_to_domname(model.uniq_key)\n        if (not domname):\n            return web.notfound()\n        virt = kvc.search_kvg_guests(domname)[0]\n        vcpus_info = virt.get_vcpus_info()\n        self.view.max_vcpus_limit = kvc.get_max_vcpus()\n        self.view.max_vcpus = vcpus_info['bootup_vcpus']\n        self.view.vcpus_limit = vcpus_info['max_vcpus']\n        self.view.vcpus = vcpus_info['vcpus']\n        self.view.cpuTime = virt.get_info()['cpuTime']\n        self.view.hypervisor = virt.get_info()['hypervisor']\n        self.view.guest = model\n    finally:\n        kvc.close()\n    return True\n", "label": 0}
{"function": "\n\ndef Execute(self, action, *args, **kw):\n    'Directly execute an action through an Environment\\n        '\n    action = self.Action(action, *args, **kw)\n    result = action([], [], self)\n    if isinstance(result, SCons.Errors.BuildError):\n        errstr = result.errstr\n        if result.filename:\n            errstr = ((result.filename + ': ') + errstr)\n        sys.stderr.write(('scons: *** %s\\n' % errstr))\n        return result.status\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef modify_cluster(self, **cluster_kwargs):\n    cluster_identifier = cluster_kwargs.pop('cluster_identifier')\n    new_cluster_identifier = cluster_kwargs.pop('new_cluster_identifier', None)\n    cluster = self.describe_clusters(cluster_identifier)[0]\n    for (key, value) in cluster_kwargs.items():\n        setattr(cluster, key, value)\n    if new_cluster_identifier:\n        self.delete_cluster(cluster_identifier)\n        cluster.cluster_identifier = new_cluster_identifier\n        self.clusters[new_cluster_identifier] = cluster\n    return cluster\n", "label": 0}
{"function": "\n\ndef categories(self, fileids=None, patterns=None):\n    meta = self._get_meta()\n    fileids = make_iterable(fileids, meta.keys())\n    result = sorted(set((cat for cat in itertools.chain(*(meta[str(doc_id)].categories for doc_id in fileids)))))\n    if patterns:\n        patterns = make_iterable(patterns)\n        result = [cat for cat in result if some_items_match([cat], patterns)]\n    return result\n", "label": 1}
{"function": "\n\ndef update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES):\n    'Update a wrapper function to look like the wrapped function\\n\\n       wrapper is the function to be updated\\n       wrapped is the original function\\n       assigned is a tuple naming the attributes assigned directly\\n       from the wrapped function to the wrapper function (defaults to\\n       functools.WRAPPER_ASSIGNMENTS)\\n       updated is a tuple naming the attributes of the wrapper that\\n       are updated with the corresponding attribute from the wrapped\\n       function (defaults to functools.WRAPPER_UPDATES)\\n    '\n    for attr in assigned:\n        setattr(wrapper, attr, getattr(wrapped, attr))\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {\n            \n        }))\n    return wrapper\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    from ..classtypes.base import FieldsClassType\n    from ..classtypes.inputobjecttype import InputObjectType\n    if issubclass(cls, InputObjectType):\n        inputfield = self.as_inputfield()\n        return inputfield.contribute_to_class(cls, name)\n    elif issubclass(cls, FieldsClassType):\n        field = self.as_field()\n        return field.contribute_to_class(cls, name)\n", "label": 0}
{"function": "\n\ndef test_analytic_functions(self):\n    t = self.alltypes.limit(1000)\n    g = t.group_by('string_col').order_by('double_col')\n    f = t.float_col\n    exprs = [f.lag(), f.lead(), f.rank(), f.dense_rank(), f.first(), f.last(), f.first().over(ibis.window(preceding=10)), f.first().over(ibis.window(following=10)), ibis.row_number(), f.cumsum(), f.cummean(), f.cummin(), f.cummax(), (f == 0).cumany(), (f == 0).cumall(), f.sum(), f.mean(), f.min(), f.max()]\n    proj_exprs = [expr.name(('e%d' % i)) for (i, expr) in enumerate(exprs)]\n    proj_table = g.mutate(proj_exprs)\n    proj_table.execute()\n", "label": 0}
{"function": "\n\ndef go_back(self):\n    isdir = is_dir(self.input)\n    input_stripped = self.input.rstrip(os.sep)\n    if (not input_stripped):\n        return\n    input_splitted = input_stripped.split(os.sep)\n    entry_name = input_splitted[(- 1)]\n    if isdir:\n        entry_name += os.sep\n    new_input = os.sep.join(input_splitted[0:(- 1)])\n    if new_input:\n        new_input += os.sep\n    self.set_input(new_input)\n    self.set_selected_entry(entry_name)\n", "label": 0}
{"function": "\n\ndef _adapt_expression(self, op, other_comparator):\n    if hasattr(self.type, 'adapt_operator'):\n        util.warn_deprecated('UserDefinedType.adapt_operator is deprecated.  Create a UserDefinedType.Comparator subclass instead which generates the desired expression constructs, given a particular operator.')\n        return (self.type.adapt_operator(op), self.type)\n    else:\n        return (op, self.type)\n", "label": 0}
{"function": "\n\ndef build_port_bindings(ports):\n    port_bindings = {\n        \n    }\n    for port in ports:\n        (internal_port_range, external_range) = split_port(port)\n        add_port(port_bindings, internal_port_range, external_range)\n    return port_bindings\n", "label": 0}
{"function": "\n\n@property\ndef full_cyclic_form(self):\n    'Return permutation in cyclic form including singletons.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.combinatorics.permutations import Permutation\\n        >>> Permutation([0, 2, 1]).full_cyclic_form\\n        [[0], [1, 2]]\\n        '\n    need = (set(range(self.size)) - set(flatten(self.cyclic_form)))\n    rv = self.cyclic_form\n    rv.extend([[i] for i in need])\n    rv.sort()\n    return rv\n", "label": 0}
{"function": "\n\ndef _iter_cursor_results(self):\n    col_names = [c[0] for c in self._cursor.description]\n    while 1:\n        row = self._cursor.fetchone()\n        if (row is None):\n            break\n        (yield self._make_row(row, col_names))\n", "label": 0}
{"function": "\n\ndef input_field(self, name, value, sample_values, back_uri):\n    string_value = (self.format(value) if value else '')\n    html = ('<input class=\"%s\" name=\"%s\" type=\"text\" size=\"%d\" value=\"%s\"/>' % (cgi.escape(self.name()), cgi.escape(name), self.input_field_size(), cgi.escape(string_value, True)))\n    if value:\n        html += ('<br><a href=\"/datastore/edit/%s?next=%s\">%s</a>' % (cgi.escape(string_value, True), urllib.quote_plus(back_uri), cgi.escape(_format_datastore_key(value), True)))\n    return html\n", "label": 0}
{"function": "\n\ndef _format_row(self, row):\n    return [six.text_type(r) for r in row]\n", "label": 0}
{"function": "\n\ndef assertRedirects(self, response, expected_url, **kwargs):\n    '\\n        Wrapper for assertRedirects to handle Django pre-1.9.\\n        '\n    if ((VERSION >= (1, 9)) and expected_url.startswith('http://testserver')):\n        expected_url = expected_url[len('http://testserver'):]\n    return super(CommentTestCase, self).assertRedirects(response, expected_url, **kwargs)\n", "label": 0}
{"function": "\n\ndef html_tag(self, template, logical_path, debug=False):\n    environment = self.get_environment(current_app)\n    if (debug or self.debug(current_app)):\n        asset = build_asset(environment, logical_path)\n        urls = []\n        for requirement in asset.requirements:\n            logical_path = requirement.attributes.logical_path\n            url = url_for('static', filename=logical_path, body=1)\n            urls.append(url)\n    else:\n        if (logical_path in environment.manifest.files):\n            logical_path = environment.manifest.files[logical_path]\n        urls = (url_for('static', filename=logical_path),)\n    return Markup('\\n'.join((template.format(url=url) for url in urls)))\n", "label": 1}
{"function": "\n\ndef _format_rule(rule):\n    'Break up rule string so it fits on screen.'\n    rule_split = jsonutils.dumps(rule).split(':-')\n    formatted_string = (rule_split[0] + ':-\\n')\n    for rule in rule_split[1].split('), '):\n        formatted_string += (rule + '\\n')\n    return formatted_string\n", "label": 0}
{"function": "\n\ndef reject(self, delivery_tag, requeue=False):\n    if requeue:\n        self.restore_by_tag(delivery_tag, leftmost=True)\n    self.ack(delivery_tag)\n", "label": 0}
{"function": "\n\ndef write(self, cell):\n    if (cell['cell_type'] == 'markdown'):\n        self.contents.append(cell['source'])\n", "label": 0}
{"function": "\n\ndef testConversion(self):\n    for (expected_value, binary) in self.tests:\n        binary_sid = ''.join([chr(x) for x in binary])\n        if (expected_value is None):\n            self.assertRaises(ValueError, wmi_parser.BinarySIDtoStringSID, binary_sid)\n        else:\n            self.assertEqual(wmi_parser.BinarySIDtoStringSID(binary_sid), expected_value)\n", "label": 0}
{"function": "\n\ndef test_node_cutset_exception():\n    G = nx.Graph()\n    G.add_edges_from([(1, 2), (3, 4)])\n    for flow_func in flow_funcs:\n        assert_raises(nx.NetworkXError, nx.minimum_node_cut, G, flow_func=flow_func)\n", "label": 0}
{"function": "\n\ndef astar_path_length(G, source, target, heuristic=None, weight='weight'):\n    'Return the length of the shortest path between source and target using\\n    the A* (\"A-star\") algorithm.\\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n\\n    source : node\\n       Starting node for path\\n\\n    target : node\\n       Ending node for path\\n\\n    heuristic : function\\n       A function to evaluate the estimate of the distance\\n       from the a node to the target.  The function takes\\n       two nodes arguments and must return a number.\\n\\n    Raises\\n    ------\\n    NetworkXNoPath\\n        If no path exists between source and target.\\n\\n    See Also\\n    --------\\n    astar_path\\n\\n    '\n    if ((source not in G) or (target not in G)):\n        msg = 'Either source {} or target {} is not in G'\n        raise nx.NodeNotFound(msg.format(source, target))\n    path = astar_path(G, source, target, heuristic, weight)\n    return sum((G[u][v].get(weight, 1) for (u, v) in zip(path[:(- 1)], path[1:])))\n", "label": 0}
{"function": "\n\ndef serialize(self):\n    buf = bytearray(struct.pack(self._PACK_STR, self.type_, self.aux_len, self.num, addrconv.ipv6.text_to_bin(self.address)))\n    for src in self.srcs:\n        buf.extend(struct.pack('16s', addrconv.ipv6.text_to_bin(src)))\n    if (0 == self.num):\n        self.num = len(self.srcs)\n        struct.pack_into('!H', buf, 2, self.num)\n    if (self.aux is not None):\n        mod = (len(self.aux) % 4)\n        if mod:\n            self.aux += bytearray((4 - mod))\n            self.aux = six.binary_type(self.aux)\n        buf.extend(self.aux)\n        if (0 == self.aux_len):\n            self.aux_len = (len(self.aux) // 4)\n            struct.pack_into('!B', buf, 1, self.aux_len)\n    return six.binary_type(buf)\n", "label": 1}
{"function": "\n\ndef execute(self, cluster, commands):\n    pool = self.get_pool(commands)\n    for (db_num, command_list) in commands.iteritems():\n        for command in command_list:\n            pool.add(command, command.clone().resolve, [cluster[db_num]])\n    return dict(pool.join())\n", "label": 0}
{"function": "\n\ndef __init__(self, p2):\n    VerticalPanel.__init__(self)\n    self.setSpacing(10)\n    if p2:\n        self.b = VerticalDemoSlider2(0, 100)\n    else:\n        self.b = VerticalDemoSlider(0, 100)\n    self.add(self.b)\n    self.b.setWidth('20px')\n    self.b.setHeight('100px')\n    self.b.addControlValueListener(self)\n    self.label = InputControl(0, 100)\n    self.add(self.label)\n    self.label.addControlValueListener(self)\n", "label": 0}
{"function": "\n\ndef backup_reports(items):\n    if (not items):\n        return\n    KEEP_MAX_REPORTS = 100\n    tm = app.get_state_item('telemetry', {\n        \n    })\n    if ('backup' not in tm):\n        tm['backup'] = []\n    for params in items:\n        for key in params.keys():\n            if (key in ('v', 'tid', 'cid', 'cd1', 'cd2', 'sr', 'an')):\n                del params[key]\n        if ('qt' not in params):\n            params['qt'] = time()\n        elif (not isinstance(params['qt'], float)):\n            params['qt'] = (time() - (params['qt'] / 1000))\n        tm['backup'].append(params)\n    tm['backup'] = tm['backup'][(KEEP_MAX_REPORTS * (- 1)):]\n    app.set_state_item('telemetry', tm)\n", "label": 1}
{"function": "\n\ndef on_load(self, event=None):\n    '\\n        Load a ``*.grad`` lookuptable file.\\n        '\n    wildcard = 'Gradient Files (*.grad);;All Files (*.*)'\n    (filename, filter) = QtGui.QFileDialog.getOpenFileName(self, 'Open gradient file...', '', wildcard)\n    if filename:\n        self.load(filename)\n", "label": 0}
{"function": "\n\ndef test_setslice2(self):\n    pyfunc = list_setslice2\n    cfunc = jit(nopython=True)(pyfunc)\n    sizes = [5, 40]\n    for (n, n_src) in itertools.product(sizes, sizes):\n        indices = [0, 1, (n - 2), (- 1), (- 2), ((- n) + 3), ((- n) - 1), (- n)]\n        for (start, stop) in itertools.product(indices, indices):\n            expected = pyfunc(n, n_src, start, stop)\n            self.assertPreciseEqual(cfunc(n, n_src, start, stop), expected)\n", "label": 0}
{"function": "\n\ndef build(self):\n    from math import cos, sin, radians\n    x = y = 150\n    l = 100\n    points = [x, y]\n    for i in range(45, 360, 45):\n        i = radians(i)\n        points.extend([(x + (cos(i) * l)), (y + (sin(i) * l))])\n    return BezierTest(points=points, loop=True)\n", "label": 0}
{"function": "\n\ndef __enter__(self):\n    if self._entered:\n        raise RuntimeError(('Cannot enter %r twice' % self))\n    self._entered = True\n    self._filters = self._module.filters\n    self._module.filters = self._filters[:]\n    self._showwarning = self._module.showwarning\n    if self._record:\n        log = []\n\n        def showwarning(*args, **kwargs):\n            log.append(WarningMessage(*args, **kwargs))\n        self._module.showwarning = showwarning\n        return log\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef pass_test(self, message):\n    self.passed += 1\n    status = '[PASS]'\n    if self.test['outcomes']['colour_output']:\n        status = COLOURS.to_green(status)\n    return (message, status)\n", "label": 0}
{"function": "\n\ndef _bytecode_filenames(self, py_filenames):\n    bytecode_files = []\n    for py_file in py_filenames:\n        if (not py_file.endswith('.py')):\n            continue\n        if self.compile:\n            bytecode_files.append((py_file + 'c'))\n        if (self.optimize > 0):\n            bytecode_files.append((py_file + 'o'))\n    return bytecode_files\n", "label": 0}
{"function": "\n\ndef test_non_rebin(self):\n    'This will take a monthly dataset and ask for a monthly rebin of 28 days.  The resulting\\n        dataset should have the same time values'\n    monthly_dataset = dp.temporal_rebin(self.ten_year_monthly_dataset, 'monthly')\n    bins = list(set([datetime.datetime(time_reading.year, time_reading.month, 15) for time_reading in self.ten_year_monthly_dataset.times]))\n    bins = np.array(bins)\n    bins.sort()\n    np.testing.assert_array_equal(monthly_dataset.times, bins)\n", "label": 0}
{"function": "\n\ndef _ocd_id_for_url_path(self, url_path):\n    ocd_id = 'ocd-division/country:us/state:wa'\n    if url_path['jurisdiction']:\n        ocd_id = '{}/county:{}'.format(ocd_id, ocd_type_id(url_path['jurisdiction']))\n    return ocd_id\n", "label": 0}
{"function": "\n\ndef _transport_closed(self):\n    for session in self._sessions.values():\n        session._state = 'CLOSED'\n", "label": 0}
{"function": "\n\ndef check_func(conf, func, libs=None):\n    if (libs is None):\n        libs = []\n    code = ('\\nchar %(func)s (void);\\n\\n#ifdef _MSC_VER\\n#pragma function(%(func)s)\\n#endif\\n\\nint main (void)\\n{\\n    return %(func)s();\\n}\\n' % {\n        'func': func,\n    })\n    if libs:\n        msg = ('Checking for function %s in %s' % (func, ' '.join([(conf.env['LIB_FMT'] % lib) for lib in libs])))\n    else:\n        msg = ('Checking for function %s' % func)\n    conf.start_message(msg)\n    old_lib = copy.deepcopy(conf.env['LIBS'])\n    try:\n        for lib in libs[::(- 1)]:\n            conf.env['LIBS'].insert(0, lib)\n        ret = conf.builders['ctasks'].try_program('check_func', code, None)\n        if ret:\n            conf.end_message('yes')\n        else:\n            conf.end_message('no !')\n    finally:\n        conf.env['LIBS'] = old_lib\n    conf.conf_results.append({\n        'type': 'func',\n        'value': func,\n        'result': ret,\n    })\n    return ret\n", "label": 1}
{"function": "\n\ndef test_user_cannot_access_cloud_allowed_for_project_he_has_no_role_in(self):\n    self.client.force_authenticate(user=self.users['no_role'])\n    for cloud_type in ('admined', 'managed'):\n        response = self.client.get(factories.CloudFactory.get_url(self.clouds[cloud_type]))\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND, (('User (role=none) should not see cloud (type=' + cloud_type) + ')'))\n", "label": 0}
{"function": "\n\ndef get_memory_amount(builder, installProfile, is_mandatory):\n    if (('hardwareSettings' in builder) and ('memory' in builder['hardwareSettings'])):\n        installProfile.memorySize = builder['hardwareSettings']['memory']\n        return installProfile\n    elif is_mandatory:\n        printer.out((('Error: no hardwareSettings part for builder [' + builder['type']) + ']'), printer.ERROR)\n        return 2\n    else:\n        return installProfile\n", "label": 0}
{"function": "\n\n@rest_utils.ajax(data_required=True)\ndef patch(self, request):\n    'Associate or disassociate a floating IP address.\\n\\n        :param address_id: The ID of the floating IP address to associate\\n                           or disassociate.\\n        :param port_id: The ID of the port to associate.\\n        '\n    address = request.DATA['address_id']\n    port = request.DATA.get('port_id')\n    if (port is None):\n        api.network.floating_ip_disassociate(request, address)\n    else:\n        api.network.floating_ip_associate(request, address, port)\n", "label": 0}
{"function": "\n\ndef __init__(self, raw, buffer_size=DEFAULT_BUFFER_SIZE, max_buffer_size=None):\n    if (not raw.writable()):\n        raise IOError('\"raw\" argument must be writable.')\n    _BufferedIOMixin.__init__(self, raw)\n    if (buffer_size <= 0):\n        raise ValueError('invalid buffer size')\n    if (max_buffer_size is not None):\n        warnings.warn('max_buffer_size is deprecated', DeprecationWarning, self._warning_stack_offset)\n    self.buffer_size = buffer_size\n    self._write_buf = bytearray()\n    self._write_lock = Lock()\n    self._ok = True\n", "label": 0}
{"function": "\n\ndef __init__(self, max_length=None, *args, **kwargs):\n    if max_length:\n        max_length += len(self.prefix)\n    super(EncryptedCharField, self).__init__(*args, max_length=max_length, **kwargs)\n", "label": 0}
{"function": "\n\ndef AnnotateMethod(self, unused_the_api, method, unused_resource):\n    'Annotate a Method with Java Proto specific elements.\\n\\n    Args:\\n      unused_the_api: (Api) The API tree which owns this method.\\n      method: (Method) The method to annotate.\\n      unused_resource: (Resource) The resource which owns this method.\\n\\n    Raises:\\n      ValueError: if missing externalTypeName\\n    '\n    for attr in ('requestType', 'responseType'):\n        schema = method.get(attr)\n        if (schema and (not isinstance(schema, data_types.Void))):\n            name = schema.get('externalTypeName')\n            if (not name):\n                raise ValueError(('missing externalTypeName for %s (%s of method %s)' % (schema['id'], attr, method['rpcMethod'])))\n            java_name = schema.get('javaTypeName')\n            proto_name = (java_name or ('TO_BE_COMPUTED.' + name[(name.rfind('.') + 1):]))\n            schema.SetTemplateValue('protoFullClassName', proto_name)\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    for tag in self.tags:\n        (yield tag)\n", "label": 0}
{"function": "\n\ndef _notification(self, context, method, routers, operation, shuffle_agents):\n    'Notify all or individual Cisco cfg agents.'\n    if utils.is_extension_supported(self._l3plugin, L3AGENT_SCHED):\n        adm_context = ((context.is_admin and context) or context.elevated())\n        self._l3plugin.schedule_routers(adm_context, routers)\n        self._agent_notification(context, method, routers, operation, shuffle_agents)\n    else:\n        cctxt = self.client.prepare(topics=topics.L3_AGENT, fanout=True)\n        cctxt.cast(context, method, routers=[r['id'] for r in routers])\n", "label": 0}
{"function": "\n\ndef fireDNDEvent(self, name, target, widget):\n    if (name == 'dragstart'):\n        self.dragDataStore.setMode(READ_WRITE)\n    elif (name == 'drop'):\n        self.dragDataStore.setMode(READ_ONLY)\n    event = self.makeDragEvent(self.mouseEvent, name, target)\n    widget.onBrowserEvent(event)\n    self.finalize(event)\n    return event\n", "label": 0}
{"function": "\n\ndef _canonicalize(self, field):\n    field = field.lower().strip()\n    for c in (' ', '-'):\n        field = field.replace(c, '_')\n    return field\n", "label": 0}
{"function": "\n\n@base.remotable_classmethod\ndef get_by_instance_and_network(cls, context, instance_uuid, network_id):\n    db_vif = db.virtual_interface_get_by_instance_and_network(context, instance_uuid, network_id)\n    if db_vif:\n        return cls._from_db_object(context, cls(), db_vif)\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if (not self.key):\n        self.key = self.generate_key()\n    return super(Token, self).save(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef directory_files(fpath):\n    for (dir, _, files) in walk(fpath):\n        for file_name in files:\n            (yield path.join(dir, file_name))\n", "label": 0}
{"function": "\n\ndef __init__(self, size, maximumSize=None, parenting=None):\n    GafferUI.Widget.__init__(self, QtGui.QWidget(), parenting=parenting)\n    self._qtWidget().setMinimumWidth(size.x)\n    self._qtWidget().setMinimumHeight(size.y)\n    if (maximumSize is not None):\n        self._qtWidget().setMaximumWidth(maximumSize.x)\n        self._qtWidget().setMaximumHeight(maximumSize.y)\n", "label": 0}
{"function": "\n\ndef _get_longname(self, obj):\n    names = [obj.name]\n    parent = obj.parent\n    while (parent is not None):\n        if (isinstance(parent, TestCaseFile) or isinstance(parent, TestDataDirectory) or isinstance(parent, TestCase) or isinstance(parent, UserKeyword)):\n            names.insert(0, parent.name)\n        parent = parent.parent\n    return '.'.join(names)\n", "label": 1}
{"function": "\n\ndef bootstrap_unicel_gateway(apps):\n    currency = (apps.get_model('accounting.Currency') if apps else Currency).objects.get(code='INR')\n    sms_gateway_fee_class = (apps.get_model('smsbillables.SmsGatewayFee') if apps else SmsGatewayFee)\n    sms_gateway_fee_criteria_class = (apps.get_model('smsbillables.SmsGatewayFeeCriteria') if apps else SmsGatewayFeeCriteria)\n    SmsGatewayFee.create_new(SQLUnicelBackend.get_api_id(), INCOMING, 0.5, currency=currency, fee_class=sms_gateway_fee_class, criteria_class=sms_gateway_fee_criteria_class)\n    SmsGatewayFee.create_new(SQLUnicelBackend.get_api_id(), OUTGOING, 0.5, currency=currency, fee_class=sms_gateway_fee_class, criteria_class=sms_gateway_fee_criteria_class)\n    log_smsbillables_info('Updated Unicel gateway fees.')\n", "label": 0}
{"function": "\n\ndef diff_files(filename1, filename2):\n    pipeline = [('diff -U 3 %(filename1)s %(filename2)s' % {\n        'filename1': filename1,\n        'filename2': filename2,\n    })]\n    if (subprocess.call(['which', 'colordiff']) == 0):\n        pipeline.append('colordiff')\n    pipeline.append('less -R')\n    cmd = ' | '.join(pipeline)\n    subprocess.check_call(cmd, shell=True)\n", "label": 0}
{"function": "\n\ndef format_delta(delta):\n    days = (delta / ((60 * 60) * 24))\n    hours = (delta / (60 * 60))\n    mins = (delta / 60)\n    if (days == 1):\n        return ('%d day' % days)\n    elif (days > 1):\n        return ('%d days' % days)\n    elif (hours == 1):\n        return ('%d hour' % hours)\n    elif (hours > 1):\n        return ('%d hours' % hours)\n    elif (mins == 1):\n        return ('%d min' % mins)\n    elif (mins > 1):\n        return ('%d mins' % mins)\n    else:\n        return 'just now'\n", "label": 1}
{"function": "\n\ndef lt(x, y):\n    if ((x is None) and (y is not None)):\n        return True\n    elif ((x is not None) and (y is None)):\n        return False\n    else:\n        return (x < y)\n", "label": 0}
{"function": "\n\ndef test_connection_is_skipped_when_dead(self):\n    pool = ConnectionPool([(x, {\n        \n    }) for x in range(2)])\n    pool.mark_dead(0)\n    self.assertEquals([1, 1, 1], [pool.get_connection(), pool.get_connection(), pool.get_connection()])\n", "label": 0}
{"function": "\n\ndef sql_execution_asserter(self, db=None):\n    if (db is None):\n        from . import db as db\n    return assertsql.assert_engine(db)\n", "label": 0}
{"function": "\n\ndef choice_SA(parser, node, children):\n    if (len(children) > 1):\n        return OrderedChoice(nodes=children[:])\n    else:\n        return children[0]\n", "label": 0}
{"function": "\n\ndef yixia_miaopai_download_by_scid(scid, output_dir='.', merge=True, info_only=False):\n    ''\n    api_endpoint = 'http://api.miaopai.com/m/v2_channel.json?fillType=259&scid={scid}&vend=miaopai'.format(scid=scid)\n    html = get_content(api_endpoint)\n    api_content = loads(html)\n    video_url = match1(api_content['result']['stream']['base'], '(.+)\\\\?vend')\n    title = api_content['result']['ext']['t']\n    (type, ext, size) = url_info(video_url)\n    print_info(site_info, title, type, size)\n    if (not info_only):\n        download_urls([video_url], title, ext, size, output_dir, merge=merge)\n", "label": 0}
{"function": "\n\ndef make_hadoop_path(base_dirs, suffix):\n    return [(base_dir + suffix) for base_dir in base_dirs]\n", "label": 0}
{"function": "\n\ndef print_row(data):\n    'print a single db row in chr and str\\n    '\n    index_line = ''\n    pri_line1 = ''\n    chr_line2 = ''\n    asci = re.compile('[a-zA-Z0-9 ]')\n    for (i, xi) in enumerate(data):\n        if (not (i % 5)):\n            diff = (len(pri_line1) - len(index_line))\n            i = str(i)\n            index_line += (diff * ' ')\n            index_line += i\n        str_v = str(xi)\n        pri_line1 += (str(xi) + ',')\n        c = chr(xi)\n        c = (c if asci.match(c) else ' ')\n        w = len(str_v)\n        c = ((c + ((w - 1) * ' ')) + ',')\n        chr_line2 += c\n    print(index_line)\n    print(pri_line1)\n    print(chr_line2)\n", "label": 0}
{"function": "\n\ndef collect_ensure_common(data, cgroup):\n    '\\n    Some cgroup exists in only one controller. Attempt to collect common metrics\\n    (tasks clount, owner, ...) from the first controller we find the task in.\\n    '\n    if ('tasks' in data):\n        return\n    data['tasks'] = cgroup['tasks']\n    data['owner'] = cgroup.owner\n    data['type'] = cgroup.type\n", "label": 0}
{"function": "\n\ndef tick(self):\n    elapsed = (time.time() - self.startTime)\n    t = ((elapsed / float(self.flyTime)) if (self.flyTime > 0) else 1.0)\n    self.interp.InterpolateCamera(t, self.view.camera())\n    self.view.render()\n    if (t >= 1.0):\n        return False\n", "label": 0}
{"function": "\n\ndef battery_status(self):\n    'Attempts to get the battery charge percent.'\n    value = None\n    methods = [self.battery_status_read, self.battery_status_acpi, self.battery_status_upower]\n    for m in methods:\n        value = m()\n        if (value is not None):\n            break\n    return value\n", "label": 0}
{"function": "\n\ndef _percent(statements, missing):\n    s = len(statements)\n    e = (s - len(missing))\n    if (s > 0):\n        return int(round(((100.0 * e) / s)))\n    return 0\n", "label": 0}
{"function": "\n\ndef additional_tests():\n    import simplejson\n    import simplejson.encoder\n    import simplejson.decoder\n    suite = unittest.TestSuite()\n    for mod in (simplejson, simplejson.encoder, simplejson.decoder):\n        suite.addTest(doctest.DocTestSuite(mod))\n    suite.addTest(doctest.DocFileSuite('../../index.rst'))\n    return suite\n", "label": 0}
{"function": "\n\ndef _update_section_contents(self, contents, section_name, new_values):\n    new_values = new_values.copy()\n    section_start_line_num = self._find_section_start(contents, section_name)\n    last_matching_line = section_start_line_num\n    j = (last_matching_line + 1)\n    while (j < len(contents)):\n        line = contents[j]\n        if (self.SECTION_REGEX.search(line) is not None):\n            self._insert_new_values(line_number=last_matching_line, contents=contents, new_values=new_values)\n            return\n        match = self.OPTION_REGEX.search(line)\n        if (match is not None):\n            last_matching_line = j\n            key_name = match.group(1).strip()\n            if (key_name in new_values):\n                if (not isinstance(new_values[key_name], dict)):\n                    option_value = new_values[key_name]\n                    new_line = ('%s = %s\\n' % (key_name, option_value))\n                    contents[j] = new_line\n                    del new_values[key_name]\n                else:\n                    j = self._update_subattributes(j, contents, new_values[key_name], (len(match.group(1)) - len(match.group(1).lstrip())))\n                    return\n        j += 1\n    if new_values:\n        if (not contents[(- 1)].endswith('\\n')):\n            contents.append('\\n')\n        self._insert_new_values(line_number=(last_matching_line + 1), contents=contents, new_values=new_values)\n", "label": 1}
{"function": "\n\ndef _connect_to_upstream(self):\n    if self.proxy_server:\n        upstream_sock = socks.socksocket()\n        upstream_sock.set_proxy(**self.proxy_server)\n    else:\n        upstream_sock = socket.socket()\n    try:\n        upstream_sock.settimeout(self.proxy_timeout)\n        upstream_sock.connect(self.upstream)\n        if self.use_ssl:\n            upstream_sock = wrap_ssl(upstream_sock)\n    except:\n        drop_socket(upstream_sock)\n        raise\n    self.logger.info(('Connected to upstream %s:%d' % self.upstream))\n    return upstream_sock\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(CreateForm, self).clean()\n    source_type = self.cleaned_data.get('volume_source_type')\n    if ((source_type == 'image_source') and (not cleaned_data.get('image_source'))):\n        msg = _('Image source must be specified')\n        self._errors['image_source'] = self.error_class([msg])\n    elif ((source_type == 'snapshot_source') and (not cleaned_data.get('snapshot_source'))):\n        msg = _('Snapshot source must be specified')\n        self._errors['snapshot_source'] = self.error_class([msg])\n    elif ((source_type == 'volume_source') and (not cleaned_data.get('volume_source'))):\n        msg = _('Volume source must be specified')\n        self._errors['volume_source'] = self.error_class([msg])\n    return cleaned_data\n", "label": 1}
{"function": "\n\ndef __init__(self, content, url, headers=None, trusted=None):\n    encoding = None\n    if (headers and ('Content-Type' in headers)):\n        (content_type, params) = cgi.parse_header(headers['Content-Type'])\n        if ('charset' in params):\n            encoding = params['charset']\n    self.content = content\n    self.parsed = html5lib.parse(self.content, encoding=encoding, namespaceHTMLElements=False)\n    self.url = url\n    self.headers = headers\n    self.trusted = trusted\n", "label": 0}
{"function": "\n\ndef __init__(self, alias_format=None, param_stream=None):\n    self._format = (alias_format or '')\n    self._param_stream = (param_stream or '')\n", "label": 0}
{"function": "\n\ndef GetEditMediaLink(self):\n    'The Picasa API mistakenly returns media-edit rather than edit-media, but\\n    this may change soon.\\n    '\n    for a_link in self.link:\n        if (a_link.rel == 'edit-media'):\n            return a_link\n        if (a_link.rel == 'media-edit'):\n            return a_link\n    return None\n", "label": 0}
{"function": "\n\ndef _pd_assert_dibbler_calls(self, expected, actual):\n    \"Check the external process calls for dibbler are expected\\n\\n        in the case of multiple pd-enabled router ports, the exact sequence\\n        of these calls are not deterministic. It's known, though, that each\\n        external_process call is followed with either an enable() or disable()\\n        \"\n    num_ext_calls = (len(expected) // 2)\n    expected_ext_calls = []\n    actual_ext_calls = []\n    expected_action_calls = []\n    actual_action_calls = []\n    for c in range(num_ext_calls):\n        expected_ext_calls.append(expected[(c * 2)])\n        actual_ext_calls.append(actual[(c * 2)])\n        expected_action_calls.append(expected[((c * 2) + 1)])\n        actual_action_calls.append(actual[((c * 2) + 1)])\n    self.assertEqual(expected_action_calls, actual_action_calls)\n    for exp in expected_ext_calls:\n        for act in actual_ext_calls:\n            if (exp == act):\n                break\n        else:\n            msg = 'Unexpected dibbler external process call.'\n            self.fail(msg)\n", "label": 1}
{"function": "\n\ndef do_complete(self, code, cursor_pos):\n    no_complete = {\n        'status': 'ok',\n        'matches': [],\n        'cursor_start': 0,\n        'cursor_end': cursor_pos,\n        'metadata': dict(),\n    }\n    if ((not code) or (code[(- 1)] == ' ')):\n        return no_complete\n    tokens = code.split()\n    if (not tokens):\n        return no_complete\n    token = tokens[(- 1)]\n    start = (cursor_pos - len(token))\n    matches = self.qsh._complete(code, token)\n    return {\n        'status': 'ok',\n        'matches': sorted(matches),\n        'cursor_start': start,\n        'cursor_end': cursor_pos,\n        'metadata': dict(),\n    }\n", "label": 0}
{"function": "\n\ndef _check_expression(self, symbol, myself):\n    for (cre, state, action, next_state) in self._expressions:\n        mo = cre.match(symbol)\n        if ((state is self.current_state) and mo):\n            if (action is not None):\n                action(mo, self)\n            self.current_state = next_state\n", "label": 0}
{"function": "\n\ndef Tool(self, tool, toolpath=None, **kw):\n    if SCons.Util.is_String(tool):\n        tool = self.subst(tool)\n        if (toolpath is None):\n            toolpath = self.get('toolpath', [])\n        toolpath = list(map(self._find_toolpath_dir, toolpath))\n        tool = SCons.Tool.Tool(tool, toolpath, **kw)\n    tool(self)\n", "label": 0}
{"function": "\n\ndef get_connection_mgr():\n    connection_mgr = context.get_context().connection_mgr\n    (server_models, sockpools) = ({\n        \n    }, {\n        \n    })\n    for k in connection_mgr.server_models:\n        server_model = connection_mgr.server_models[k]\n        server_models[repr(k)] = {\n            'info': repr(server_model),\n            'fds': [s.fileno() for s in server_model.active_connections.keys()],\n        }\n    for prot in connection_mgr.sockpools:\n        for sock_type in connection_mgr.sockpools[prot]:\n            sockpool = connection_mgr.sockpools[prot][sock_type]\n            sockpools[repr((prot, sock_type))] = {\n                'info': repr(sockpool),\n                'addresses': map(repr, sockpool.free_socks_by_addr.keys()),\n            }\n    return {\n        'server_models': server_models,\n        'sockpools': sockpools,\n    }\n", "label": 0}
{"function": "\n\ndef handler(self, zh, rc, data, stat):\n    'Handle zookeeper.aget() responses.\\n\\n    This code handles the zookeeper.aget callback. It does not handle watches.\\n\\n    Numeric arguments map to constants. See ``DATA`` in ``help(zookeeper)``\\n    for more information.\\n\\n    Args:\\n      zh Zookeeper handle that made this request.\\n      rc Return code.\\n      data Data stored in the znode.\\n\\n    Does not provide a return value.\\n    '\n    if (zookeeper.OK == rc):\n        logger.debug('This is where your application does work.')\n    else:\n        if (zookeeper.NONODE == rc):\n            logger.info('Node not found. Trying again to set the watch.')\n            time.sleep(1)\n        if (zookeeper.OK != self.aget()):\n            logger.critical('Unable to get znode! Exiting.')\n            sys.exit(1)\n", "label": 0}
{"function": "\n\ndef get_user_by_uri(self, uri):\n    for group in self.group_list:\n        user = group.get_user_by_uri(uri)\n        if user:\n            return user\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    super(BaseMigrationTestCase, self).setUp()\n    for model_class in MODELS:\n        model_class._meta.database = self.database\n    self.database.drop_tables(MODELS, True)\n    self.database.create_tables(MODELS)\n    self.migrator = self.migrator_class(self.database)\n    if ('newpages' in User._meta.reverse_rel):\n        del User._meta.reverse_rel['newpages']\n        delattr(User, 'newpages')\n", "label": 0}
{"function": "\n\ndef plot_connectivity_topos(self, fig=None):\n    ' Plot scalp projections of the sources.\\n\\n        This function only plots the topos. Use in combination with connectivity plotting.\\n\\n        Parameters\\n        ----------\\n        fig : {None, Figure object}, optional\\n            Where to plot the topos. f set to **None**, a new figure is created. Otherwise plot into the provided\\n            figure object.\\n\\n        Returns\\n        -------\\n        fig : Figure object\\n            Instance of the figure in which was plotted.\\n        '\n    self._prepare_plots(True, False)\n    if self.plot_outside_topo:\n        fig = self.plotting.plot_connectivity_topos('outside', self.topo_, self.mixmaps_, fig)\n    elif (self.plot_diagonal == 'topo'):\n        fig = self.plotting.plot_connectivity_topos('diagonal', self.topo_, self.mixmaps_, fig)\n    return fig\n", "label": 0}
{"function": "\n\ndef __getattr__(self, name):\n    if (name == 'x'):\n        return self._x\n    elif (name == 'y'):\n        return self._y\n    else:\n        return name\n", "label": 0}
{"function": "\n\ndef matchesPredicates(self, elem):\n    if ((self.elementName != None) and (self.elementName != elem.name)):\n        return 0\n    for p in self.predicates:\n        if (not p.value(elem)):\n            return 0\n    return 1\n", "label": 0}
{"function": "\n\ndef start_selection(self, event):\n    if (event.inaxes != self._axes):\n        return False\n    if (event.key == SCRUBBING_KEY):\n        if (not self._roi.defined()):\n            return False\n        elif (not self._roi.contains(event.xdata, event.ydata)):\n            return False\n    self._roi_store()\n    if (event.key == SCRUBBING_KEY):\n        self._scrubbing = True\n        self._dx = (event.xdata - self._roi.center())\n    else:\n        self.reset()\n        self._roi.set_range(event.xdata, event.xdata)\n        self._xi = event.xdata\n    self._mid_selection = True\n    self._sync_patch()\n", "label": 1}
{"function": "\n\ndef getUncontacted(self):\n    return [n for n in self if (n.id not in self.contacted)]\n", "label": 0}
{"function": "\n\ndef write_lofl(lofl, filename):\n    a_file = open(filename, 'w')\n    for seq in lofl:\n        for it in seq:\n            a_file.write(('%s\\n' % str(it)))\n        a_file.write('\\n')\n    a_file.close()\n", "label": 0}
{"function": "\n\n@account_group.command(context_settings=CONTEXT_SETTINGS)\n@click.option('--token', '-t', type=str, help='digital ocean authentication token', metavar='<token>')\n@click.option('--tablefmt', '-f', type=click.Choice(['fancy_grid', 'simple', 'plain', 'grid', 'pipe', 'orgtbl', 'psql', 'rst', 'mediawiki', 'html', 'latex', 'latex_booktabs', 'tsv']), help='output table format', default='fancy_grid', metavar='<format>')\n@click.option('--proxy', '-p', help='proxy url to be used for this call', metavar='<http://ip:port>')\ndef account(token, tablefmt, proxy):\n    '\\n\\tget digital ocean account info\\n\\t'\n    method = 'GET'\n    url = ACCOUNT_INFO\n    result = DigitalOcean.do_request(method, url, token=token, proxy=proxy)\n    if result['has_error']:\n        click.echo()\n        click.echo(('Error: %s' % result['error_message']))\n    else:\n        record = 'account'\n        headers = ['Fields', 'Values']\n        table = []\n        for key in result['account'].keys():\n            table.append([key, result['account'][key]])\n        data = {\n            'headers': headers,\n            'table_data': table,\n        }\n        print_table(tablefmt, data, record)\n", "label": 0}
{"function": "\n\ndef write_content(self, content, content_type=None):\n    'Helper method to write content bytes to output stream.'\n    if (content_type is not None):\n        self.send_header(HTTP_HEADER_CONTENT_TYPE, content_type)\n    if ('gzip' in self.headers.get(HTTP_HEADER_ACCEPT_ENCODING, '')):\n        content = gzip.compress(content)\n        self.send_header(HTTP_HEADER_CONTENT_ENCODING, 'gzip')\n        self.send_header(HTTP_HEADER_VARY, HTTP_HEADER_ACCEPT_ENCODING)\n    self.send_header(HTTP_HEADER_CONTENT_LENGTH, str(len(content)))\n    self.end_headers()\n    if (self.command == 'HEAD'):\n        return\n    self.wfile.write(content)\n", "label": 0}
{"function": "\n\ndef test_get_vifs_by_ids(self):\n    for i in range(2):\n        self.create_ovs_port()\n    vif_ports = [self.create_ovs_vif_port() for i in range(3)]\n    by_id = self.br.get_vifs_by_ids([v.vif_id for v in vif_ports])\n    by_id = {vid: str(vport) for (vid, vport) in by_id.items()}\n    self.assertEqual({v.vif_id: str(v) for v in vif_ports}, by_id)\n", "label": 1}
{"function": "\n\ndef all_consumed_offsets(self):\n    'Returns consumed offsets as {TopicPartition: OffsetAndMetadata}'\n    all_consumed = {\n        \n    }\n    for (partition, state) in six.iteritems(self.assignment):\n        if state.has_valid_position:\n            all_consumed[partition] = OffsetAndMetadata(state.position, '')\n    return all_consumed\n", "label": 0}
{"function": "\n\ndef _format_child_instances(self, children, parent_id):\n    '\\n        The goal of this method is to add an indent at every level. This way the\\n        WF is represented as a tree structure while in a list. For the right visuals\\n        representation the list must be a DF traversal else the idents will end up\\n        looking strange.\\n        '\n    children = format_wf_instances(children)\n    depth = {\n        parent_id: 0,\n    }\n    result = []\n    for child in children:\n        if (child.parent not in depth):\n            parent = None\n            for instance in children:\n                if (WF_PREFIX in instance.id):\n                    instance_id = instance.id[(instance.id.index(WF_PREFIX) + len(WF_PREFIX)):]\n                else:\n                    instance_id = instance.id\n                if (instance_id == child.parent):\n                    parent = instance\n            if (parent and parent.parent and (parent.parent in depth)):\n                depth[child.parent] = (depth[parent.parent] + 1)\n            else:\n                depth[child.parent] = 0\n        child.id = ((INDENT_CHAR * depth[child.parent]) + child.id)\n        result.append(self._format_for_common_representation(child))\n    return result\n", "label": 1}
{"function": "\n\ndef send(self):\n    '\\n        Sends the batch request to the server and returns a list of RpcResponse\\n        objects.  The list will be in the order that the requests were made to\\n        the batch.  Note that the RpcResponse objects may contain an error or a\\n        successful result.  When you iterate through the list, you must test for\\n        response.error.\\n\\n        send() may not be called more than once.\\n        '\n    if self.sent:\n        raise Exception('Batch already sent. Cannot send() again.')\n    else:\n        self.sent = True\n        results = self.client.transport.request(self.req_list)\n        id_to_method = {\n            \n        }\n        by_id = {\n            \n        }\n        for res in results:\n            reqid = res['id']\n            by_id[reqid] = res\n        in_req_order = []\n        for req in self.req_list:\n            reqid = req['id']\n            result = None\n            error = None\n            resp = safe_get(by_id, reqid)\n            if (resp is None):\n                msg = ('Batch response missing result for request id: %s' % reqid)\n                error = RpcException(ERR_INVALID_RESP, msg)\n            else:\n                r_err = safe_get(resp, 'error')\n                if (r_err is None):\n                    result = resp['result']\n                else:\n                    error = RpcException(r_err['code'], r_err['message'], safe_get(r_err, 'data'))\n            in_req_order.append(RpcResponse(req, result, error))\n        return in_req_order\n", "label": 1}
{"function": "\n\ndef visit_Input(self, node):\n    name = node.name\n    width = (self.visit(node.width) if (node.width is not None) else None)\n    signed = node.signed\n    obj = vtypes.Input(width, signed=signed, name=name)\n    if (node.width is not None):\n        obj._set_raw_width(self.visit(node.width.msb), self.visit(node.width.lsb))\n    self.add_object(obj)\n    return obj\n", "label": 0}
{"function": "\n\ndef find(l, predicate):\n    results = [x for x in l if predicate(x)]\n    return (results[0] if (len(results) > 0) else None)\n", "label": 0}
{"function": "\n\ndef ack(self, delivery_tag=None, multiple=False):\n    'Acknowledge Message.\\n\\n        :param int/long delivery_tag: Server-assigned delivery tag\\n        :param bool multiple: Acknowledge multiple messages\\n\\n        :raises AMQPInvalidArgument: Invalid Parameters\\n        :raises AMQPChannelError: Raises if the channel encountered an error.\\n        :raises AMQPConnectionError: Raises if the connection\\n                                     encountered an error.\\n\\n        :return:\\n        '\n    if ((delivery_tag is not None) and (not compatibility.is_integer(delivery_tag))):\n        raise AMQPInvalidArgument('delivery_tag should be an integer or None')\n    elif (not isinstance(multiple, bool)):\n        raise AMQPInvalidArgument('multiple should be a boolean')\n    ack_frame = pamqp_spec.Basic.Ack(delivery_tag=delivery_tag, multiple=multiple)\n    self._channel.write_frame(ack_frame)\n", "label": 0}
{"function": "\n\ndef tostring(self, sep='', endcard=True, padding=True):\n    \"\\n        Returns a string representation of the header.\\n\\n        By default this uses no separator between cards, adds the END card, and\\n        pads the string with spaces to the next multiple of 2880 bytes.  That\\n        is, it returns the header exactly as it would appear in a FITS file.\\n\\n        Parameters\\n        ----------\\n        sep : str, optional\\n            The character or string with which to separate cards.  By default\\n            there is no separator, but one could use ``'\\\\\\\\n'``, for example, to\\n            separate each card with a new line\\n\\n        endcard : bool, optional\\n            If True (default) adds the END card to the end of the header\\n            string\\n\\n        padding : bool, optional\\n            If True (default) pads the string with spaces out to the next\\n            multiple of 2880 characters\\n\\n        Returns\\n        -------\\n        s : string\\n            A string representing a FITS header.\\n        \"\n    lines = []\n    for card in self._cards:\n        s = str(card)\n        while s:\n            lines.append(s[:Card.length])\n            s = s[Card.length:]\n    s = sep.join(lines)\n    if endcard:\n        s += (sep + _pad('END'))\n    if padding:\n        s += (' ' * _pad_length(len(s)))\n    return s\n", "label": 0}
{"function": "\n\ndef run_conv_nnet2_classif(use_gpu, seed, isize, ksize, bsize, n_train=10, check_isfinite=True, pickle=False, verbose=0, version=(- 1)):\n    'Run the train function returned by build_conv_nnet2_classif on one device.\\n    '\n    utt.seed_rng(seed)\n    (train, params, x_shape, y_shape, mode) = build_conv_nnet2_classif(use_gpu=use_gpu, isize=isize, ksize=ksize, n_batch=bsize, verbose=verbose, version=version, check_isfinite=check_isfinite)\n    if use_gpu:\n        device = 'GPU'\n    else:\n        device = 'CPU'\n    xval = my_rand(*x_shape)\n    yval = my_rand(*y_shape)\n    lr = theano._asarray(0.01, dtype='float32')\n    rvals = my_zeros(n_train)\n    t0 = time.time()\n    for i in xrange(n_train):\n        rvals[i] = train(xval, yval, lr)[0]\n    t1 = time.time()\n    print_mode(mode)\n    if (pickle and isinstance(mode, theano.compile.ProfileMode)):\n        import pickle\n        print(('BEGIN %s profile mode dump' % device))\n        print(pickle.dumps(mode))\n        print(('END %s profile mode dump' % device))\n", "label": 0}
{"function": "\n\ndef iqilu_download(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    ''\n    if re.match('http://v.iqilu.com/\\\\w+', url):\n        html = get_content(url)\n        url = match1(html, \"<input type='hidden' id='playerId' url='(.+)'\")\n        title = match1(html, '<meta name=\"description\" content=\"(.*?)\\\\\"\\\\W')\n        (type_, ext, size) = url_info(url)\n        print_info(site_info, title, type_, size)\n        if (not info_only):\n            download_urls([url], title, ext, total_size=None, output_dir=output_dir, merge=merge)\n", "label": 0}
{"function": "\n\ndef bPrimary(self, prob):\n    '\\n        The primary magnetic flux density from a magnetic vector potential\\n\\n        :param Problem prob: FDEM problem\\n        :rtype: numpy.ndarray\\n        :return: primary magnetic field\\n        '\n    formulation = prob._formulation\n    if (formulation is 'EB'):\n        gridX = prob.mesh.gridEx\n        gridY = prob.mesh.gridEy\n        gridZ = prob.mesh.gridEz\n        C = prob.mesh.edgeCurl\n    elif (formulation is 'HJ'):\n        gridX = prob.mesh.gridFx\n        gridY = prob.mesh.gridFy\n        gridZ = prob.mesh.gridFz\n        C = prob.mesh.edgeCurl.T\n    if (prob.mesh._meshType is 'CYL'):\n        if (not prob.mesh.isSymmetric):\n            raise NotImplementedError('Non-symmetric cyl mesh not implemented yet!')\n        a = MagneticDipoleVectorPotential(self.loc, gridY, 'y', mu=self.mu, moment=self.moment)\n    else:\n        srcfct = MagneticDipoleVectorPotential\n        ax = srcfct(self.loc, gridX, 'x', mu=self.mu, moment=self.moment)\n        ay = srcfct(self.loc, gridY, 'y', mu=self.mu, moment=self.moment)\n        az = srcfct(self.loc, gridZ, 'z', mu=self.mu, moment=self.moment)\n        a = np.concatenate((ax, ay, az))\n    return (C * a)\n", "label": 0}
{"function": "\n\ndef __init__(self, getter, attribute, new, spec, create, spec_set, autospec, new_callable, kwargs):\n    if (new_callable is not None):\n        if (new is not DEFAULT):\n            raise ValueError(\"Cannot use 'new' and 'new_callable' together\")\n        if (autospec is not None):\n            raise ValueError(\"Cannot use 'autospec' and 'new_callable' together\")\n    self.getter = getter\n    self.attribute = attribute\n    self.new = new\n    self.new_callable = new_callable\n    self.spec = spec\n    self.create = create\n    self.has_local = False\n    self.spec_set = spec_set\n    self.autospec = autospec\n    self.kwargs = kwargs\n    self.additional_patchers = []\n", "label": 0}
{"function": "\n\ndef validate_configurator_version():\n    '\\n    Arch is a rolling release distro, therefore it is important to ensure\\n    the configurator version is current.\\n    '\n    if (settings.CONFIGURATOR_MODULE == 'bootmachine.contrib.configurators.salt'):\n        pkgver = settings.SALT_AUR_PKGVER\n        pkgrel = settings.SALT_AUR_PKGREL\n        response = urllib2.urlopen('https://aur.archlinux.org/packages/sa/salt/PKGBUILD')\n        for line in response:\n            if (line.startswith('pkgver=') and (not (pkgver in line))):\n                abort(\"The requested Salt 'pkgrel={0}' in the AUR was updated to '{1}'.\".format(pkgver, line.strip()))\n            if (line.startswith('pkgrel=') and (not (pkgrel in line))):\n                abort(\"The requested Salt 'pkgrel={0}' in the AUR was updated to '{1}'.\".format(pkgrel, line.strip()))\n", "label": 1}
{"function": "\n\ndef slide(*args, **kwargs):\n\n    def wrap(f):\n\n        @wraps(f)\n        def wrapped_f():\n            print_func(f)\n            sys.stdin.readline()\n            if kwargs.get('executable', False):\n                print_func_result(f)\n                sys.stdin.readline()\n        return wrapped_f\n    if ((len(args) == 1) and callable(args[0])):\n        return wrap(args[0])\n    else:\n        return wrap\n", "label": 0}
{"function": "\n\ndef decistmt(s):\n    'Substitute Decimals for floats in a string of statements.\\n\\n    >>> from decimal import Decimal\\n    >>> s = \\'print +21.3e-5*-.1234/81.7\\'\\n    >>> decistmt(s)\\n    \"print +Decimal (\\'21.3e-5\\')*-Decimal (\\'.1234\\')/Decimal (\\'81.7\\')\"\\n\\n    The format of the exponent is inherited from the platform C library.\\n    Known cases are \"e-007\" (Windows) and \"e-07\" (not Windows).  Since\\n    we\\'re only showing 12 digits, and the 13th isn\\'t close to 5, the\\n    rest of the output should be platform-independent.\\n\\n    >>> exec(s) #doctest: +ELLIPSIS\\n    -3.21716034272e-0...7\\n\\n    Output from calculations with Decimal should be identical across all\\n    platforms.\\n\\n    >>> exec(decistmt(s))\\n    -3.217160342717258261933904529E-7\\n    '\n    result = []\n    g = generate_tokens(StringIO(s).readline)\n    for (toknum, tokval, _, _, _) in g:\n        if ((toknum == NUMBER) and ('.' in tokval)):\n            result.extend([(NAME, 'Decimal'), (OP, '('), (STRING, repr(tokval)), (OP, ')')])\n        else:\n            result.append((toknum, tokval))\n    return untokenize(result)\n", "label": 0}
{"function": "\n\ndef onRadioSelect(self, sender, keyCode=None, modifiers=None):\n    if (sender == self.radLevel5):\n        self.level = 5\n    elif (sender == self.radLevel10):\n        self.level = 10\n    elif (sender == self.radLevel15):\n        self.level = 15\n    elif (sender == self.radLevel20):\n        self.level = 20\n", "label": 0}
{"function": "\n\ndef alphanumeric_sort_key(key):\n    '\\n    Sort the given iterable in the way that humans expect.\\n    Thanks to http://stackoverflow.com/a/2669120/240553\\n    '\n    import re\n    convert = (lambda text: (int(text) if text.isdigit() else text))\n    return [convert(c) for c in re.split('([0-9]+)', key)]\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _fill_scope_refs(name, scope):\n    \"Put referenced name in 'ref' dictionary of a scope.\\n\\n        Walks up the scope tree and adds the name to 'ref' of every scope\\n        up in the tree until a scope that defines referenced name is reached.\\n        \"\n    symbol = scope.resolve(name)\n    if (symbol is None):\n        return\n    orig_scope = symbol.scope\n    scope.refs[name] = orig_scope\n    while (scope is not orig_scope):\n        scope = scope.get_enclosing_scope()\n        scope.refs[name] = orig_scope\n", "label": 0}
{"function": "\n\ndef _read_word_block(self, stream):\n    words = []\n    for i in range(20):\n        line = stream.readline()\n        if (not line):\n            continue\n        words.append(line.strip())\n    return words\n", "label": 0}
{"function": "\n\ndef deallocate_for_instance(self, context, instance, **kwargs):\n    'Deallocate all network resources related to the instance.'\n    LOG.debug('deallocate_for_instance()', instance=instance)\n    search_opts = {\n        'device_id': instance.uuid,\n    }\n    neutron = get_client(context)\n    data = neutron.list_ports(**search_opts)\n    ports = [port['id'] for port in data.get('ports', [])]\n    requested_networks = (kwargs.get('requested_networks') or [])\n    if isinstance(requested_networks, objects.NetworkRequestList):\n        requested_networks = requested_networks.as_tuples()\n    ports_to_skip = set([port_id for (nets, fips, port_id, pci_request_id) in requested_networks])\n    ports_to_skip |= set(self._get_preexisting_port_ids(instance))\n    ports = (set(ports) - ports_to_skip)\n    self._unbind_ports(context, ports_to_skip, neutron)\n    self._delete_ports(neutron, instance, ports, raise_if_fail=True)\n    base_api.update_instance_cache_with_nw_info(self, context, instance, network_model.NetworkInfo([]))\n", "label": 0}
{"function": "\n\ndef process_failure(self, partial_eval, error, feature, dpoint, d_index):\n    logger.warning(('Fail evaluating %s: %s %s' % (feature, type(error), error)))\n    self._fit_failure_stats['discarded_samples'].append(dpoint.get('pk', 'PK-NOT-FOUND'))\n    feature_errors = self._fit_failure_stats['features'][feature]\n    feature_errors.append(dpoint)\n    if (d_index < self.FEATURE_STRICT_UNTIL):\n        self.exclude_feature(feature, partial_eval)\n    elif (len(feature_errors) > self.FEATURE_MAX_ERRORS_ALLOWED):\n        self.exclude_feature(feature, partial_eval)\n", "label": 0}
{"function": "\n\ndef compile_dir(env, src_path, dst_path, pattern='^.*\\\\.html$', encoding='utf-8', base_dir=None):\n    'Compiles a directory of Jinja2 templates to python code.\\n  \\n  :param env: a Jinja2 Environment instance.\\n  :param src_path: path to the source directory.\\n  :param dst_path: path to the destination directory.\\n  :param encoding: template encoding.\\n  :param base_dir: the base path to be removed from the compiled template filename.\\n  '\n    from os import path, listdir, mkdir\n    file_re = re.compile(pattern)\n    if (base_dir is None):\n        base_dir = src_path\n    for filename in listdir(src_path):\n        src_name = path.join(src_path, filename)\n        dst_name = path.join(dst_path, filename)\n        if path.isdir(src_name):\n            mkdir(dst_name)\n            compile_dir(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n        elif (path.isfile(src_name) and file_re.match(filename)):\n            compile_file(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n", "label": 1}
{"function": "\n\ndef get_project_users_roles(request, project):\n    users_roles = collections.defaultdict(list)\n    if (VERSIONS.active < 3):\n        project_users = user_list(request, project=project)\n        for user in project_users:\n            roles = roles_for_user(request, user.id, project)\n            roles_ids = [role.id for role in roles]\n            users_roles[user.id].extend(roles_ids)\n    else:\n        project_role_assignments = role_assignments_list(request, project=project)\n        for role_assignment in project_role_assignments:\n            if (not hasattr(role_assignment, 'user')):\n                continue\n            user_id = role_assignment.user['id']\n            role_id = role_assignment.role['id']\n            if (('project' in role_assignment.scope) and (role_assignment.scope['project']['id'] == project)):\n                users_roles[user_id].append(role_id)\n    return users_roles\n", "label": 1}
{"function": "\n\n@attr('slow')\ndef test_basic(self):\n    '\\n        Tests that basic correlations work for odd and even\\n        dimensions of image and filter shapes, as well as rectangular\\n        images and filters.\\n        '\n    border_modes = ['valid', 'full', 'half', (1, 1), (2, 1), (1, 2), (3, 3), 1]\n    img_shapes = [(2, 2, 3, 3), (3, 2, 8, 8), (3, 2, 7, 5), (3, 2, 7, 5), (3, 2, 8, 8), (3, 2, 7, 5)]\n    fil_shapes = [(2, 2, 2, 2), (4, 2, 5, 5), (5, 2, 2, 3), (5, 2, 3, 2), (4, 2, 5, 5), (5, 2, 2, 3)]\n    for border_mode in border_modes:\n        for (img, fil) in zip(img_shapes, fil_shapes):\n            self.validate(img, fil, border_mode, verify_grad=False)\n    self.validate((1, 10, 213, 129), (46, 10, 212, 1), 'valid', verify_grad=False)\n", "label": 0}
{"function": "\n\ndef clean_content(form):\n    status = form.cleaned_data.get('status')\n    content = form.cleaned_data.get('content')\n    if ((status == CONTENT_STATUS_PUBLISHED) and (not content)):\n        raise ValidationError(_('This field is required if status is set to published.'))\n    return content\n", "label": 0}
{"function": "\n\ndef create_list_setting(self, name):\n    hlayout = QtGui.QHBoxLayout()\n    setting = self.get_setting(name)\n    button = None\n    if setting.button:\n        button = QtGui.QPushButton(setting.button)\n        button.clicked.connect((lambda : setting.button_callback(button)))\n    combo = QtGui.QComboBox()\n    combo.setObjectName(setting.name)\n    combo.currentIndexChanged.connect(self.call_with_object('setting_changed', combo, setting))\n    combo.editTextChanged.connect(self.call_with_object('setting_changed', combo, setting))\n    combo.setStatusTip(setting.description)\n    combo.setToolTip(setting.description)\n    for val in setting.values:\n        combo.addItem(val)\n    default_index = combo.findText(setting.default_value)\n    if (default_index != (- 1)):\n        combo.setCurrentIndex(default_index)\n    hlayout.addWidget(QtGui.QLabel())\n    hlayout.addWidget(combo)\n    if button:\n        hlayout.addWidget(button)\n    return hlayout\n", "label": 0}
{"function": "\n\ndef _fill_dict(self, xmldoc, element_name):\n    xmlelements = self._get_child_nodes(xmldoc, element_name)\n    if xmlelements:\n        return_obj = {\n            \n        }\n        for child in xmlelements[0].childNodes:\n            if child.firstChild:\n                return_obj[child.nodeName] = child.firstChild.nodeValue\n        return return_obj\n", "label": 0}
{"function": "\n\ndef onMouseUp(self, sender, x, y):\n    self.dragging = NOT_DRAGGING\n    if self.draggingImage:\n        GlassWidget.hide()\n        if ((self.currentDragOperation == 'none') or (not self.currentTargetElement)):\n            if self.currentTargetElement:\n                self.fireDNDEvent('dragleave', self.currentTargetElement, self.currentDropWidget)\n            else:\n                self.currentDragOperation = 'none'\n            self.returnDrag()\n        else:\n            drop_event = self.fireDNDEvent('drop', self.currentTargetElement, self.currentDropWidget)\n            if isCanceled(drop_event):\n                self.currentDragOperation = drop_event.dataTransfer.dropEffect\n            else:\n                self.currentDragOperation = 'none'\n            self.zapDragImage()\n        self.fireDNDEvent('dragend', None, self.dragWidget)\n", "label": 1}
{"function": "\n\n@blueprint.route('/remove', methods=['POST'])\n@api_wrapper\n@require_login\n@require_team\ndef team_remove_hook():\n    if (api.auth.is_logged_in() and api.user.in_team()):\n        uid = request.form.get('uid')\n        user = api.user.get_user()\n        if (uid == user['uid']):\n            confirm = request.form.get('confirm')\n            team = api.team.get_team()\n            if (confirm != team['teamname']):\n                raise WebException('Please confirm your name.')\n        message = api.team.remove(uid)\n        return {\n            'success': 1,\n            'message': message,\n        }\n    else:\n        raise WebException('Stop. Just stop.')\n", "label": 0}
{"function": "\n\ndef init_layout(self):\n    ' Initialize the layout for the menu bar.\\n\\n        '\n    super(QtMenuBar, self).init_layout()\n    widget = self.widget\n    for child in self.children():\n        if isinstance(child, QtMenu):\n            widget.addMenu(child.widget)\n", "label": 0}
{"function": "\n\ndef _plot_mean(self, canvas, helper_data, helper_prediction, levels=20, projection='2d', label=None, **kwargs):\n    (_, free_dims, Xgrid, x, y, _, _, resolution) = helper_data\n    if (len(free_dims) <= 2):\n        (mu, _, _) = helper_prediction\n        if (len(free_dims) == 1):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_1d)\n            plots = dict(gpmean=[pl().plot(canvas, Xgrid[:, free_dims], mu, label=label, **kwargs)])\n        elif (projection == '2d'):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_2d)\n            plots = dict(gpmean=[pl().contour(canvas, x[:, 0], y[0, :], mu.reshape(resolution, resolution).T, levels=levels, label=label, **kwargs)])\n        elif (projection == '3d'):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_3d)\n            plots = dict(gpmean=[pl().surface(canvas, x, y, mu.reshape(resolution, resolution), label=label, **kwargs)])\n    elif (len(free_dims) == 0):\n        pass\n    else:\n        raise RuntimeError('Cannot plot mean in more then 2 input dimensions')\n    return plots\n", "label": 1}
{"function": "\n\n@classmethod\ndef update_pd(cls, pd):\n    if (len(pd.data_units) > 0):\n        du_urls = [i.url for i in pd.data_units.values()]\n", "label": 0}
{"function": "\n\ndef missing_fetch_positions(self):\n    missing = set()\n    for (partition, state) in six.iteritems(self.assignment):\n        if (not state.has_valid_position):\n            missing.add(partition)\n    return missing\n", "label": 0}
{"function": "\n\ndef append(self, value):\n    'adds value to ring buffer'\n    idx = self.index = ((self.index + 1) % self.data.size)\n    self.data[idx] = max(0, (value - self.lastval))\n    self.lastval = value\n    dt = (time.time() - self.toffset)\n    if ((idx == 0) and (max(self.times) < 0)):\n        dt = 0\n    self.times[idx] = dt\n", "label": 0}
{"function": "\n\ndef user_shipping_address_view(request, checkout):\n    data = (request.POST or None)\n    additional_addresses = request.user.addresses.all()\n    checkout.email = request.user.email\n    shipping_address = checkout.shipping_address\n    if ((shipping_address is not None) and shipping_address.id):\n        address_form = AddressForm(data, autocomplete_type='shipping', initial={\n            'country': request.country,\n        })\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses, initial={\n            'address': shipping_address.id,\n        })\n    elif shipping_address:\n        address_form = AddressForm(data, instance=shipping_address)\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses)\n    else:\n        address_form = AddressForm(data, initial={\n            'country': request.country,\n        })\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses)\n    if addresses_form.is_valid():\n        if (addresses_form.cleaned_data['address'] != ShippingAddressesForm.NEW_ADDRESS):\n            address_id = addresses_form.cleaned_data['address']\n            checkout.shipping_address = Address.objects.get(id=address_id)\n            return redirect('checkout:shipping-method')\n        elif address_form.is_valid():\n            checkout.shipping_address = address_form.instance\n            return redirect('checkout:shipping-method')\n    return TemplateResponse(request, 'checkout/shipping_address.html', context={\n        'address_form': address_form,\n        'user_form': addresses_form,\n        'checkout': checkout,\n        'additional_addresses': additional_addresses,\n    })\n", "label": 1}
{"function": "\n\ndef __init__(self, socket_name=None, socket_path=None, config_file=None, colors=None, **kwargs):\n    EnvironmentMixin.__init__(self, '-g')\n    self._windows = []\n    self._panes = []\n    if socket_name:\n        self.socket_name = socket_name\n    if socket_path:\n        self.socket_path = socket_path\n    if config_file:\n        self.config_file = config_file\n    if colors:\n        self.colors = colors\n", "label": 0}
{"function": "\n\ndef length_of_longest_string(list_of_strings):\n    if (len(list_of_strings) == 0):\n        return 0\n    result = 0\n    for string in list_of_strings:\n        length_of_string = len(string)\n        if (length_of_string > result):\n            result = length_of_string\n    return result\n", "label": 0}
{"function": "\n\ndef check_inline(self, cls, parent_model):\n    \" Validate inline class's fk field is not excluded. \"\n    fk = _get_foreign_key(parent_model, cls.model, fk_name=cls.fk_name, can_fail=True)\n    if (hasattr(cls, 'exclude') and cls.exclude):\n        if (fk and (fk.name in cls.exclude)):\n            raise ImproperlyConfigured((\"%s cannot exclude the field '%s' - this is the foreign key to the parent model %s.%s.\" % (cls.__name__, fk.name, parent_model._meta.app_label, parent_model.__name__)))\n", "label": 0}
{"function": "\n\ndef _get_kind(self, path):\n    obj = self.repository._repo[self._get_id_for_path(path)]\n    if isinstance(obj, objects.Blob):\n        return NodeKind.FILE\n    elif isinstance(obj, objects.Tree):\n        return NodeKind.DIR\n", "label": 0}
{"function": "\n\ndef reexecutable_tasks(self, task_filter):\n    'Keep only reexecutable tasks which match the filter.\\n\\n        Filter is the list of values. If task has reexecute_on key and its\\n        value matches the value from filter then task is not skipped.\\n        :param task_filter: filter (list)\\n        '\n    if (not task_filter):\n        return\n    task_filter = set(task_filter)\n    for task in six.itervalues(self.node):\n        reexecute_on = task.get('reexecute_on')\n        if ((reexecute_on is not None) and task_filter.issubset(reexecute_on)):\n            task['skipped'] = False\n        else:\n            self.make_skipped_task(task)\n", "label": 0}
{"function": "\n\ndef post(self, request):\n    email = request.POST.get('email')\n    api_key = request.POST.get('api_key')\n    if ((not email) or (not api_key)):\n        message = {\n            'success': False,\n            'errors': [],\n        }\n        if (not email):\n            message['errors'].append('Email is mandatory.')\n        if (not api_key):\n            message['errors'].append('API key is mandatory.')\n        return HttpResponseBadRequest(json.dumps(message))\n    member = self.get_member(email, api_key)\n    if (member is None):\n        return HttpResponseForbidden(json.dumps({\n            'success': False,\n            'errors': ['Bad credentials.'],\n        }))\n    return HttpResponse(json.dumps({\n        'success': True,\n        'output': member.get_storage_limit(),\n    }))\n", "label": 1}
{"function": "\n\n@property\ndef active_gen_id(self):\n    if self._active_gen_id:\n        return self._active_gen_id\n    active_nodes = InfrastructureNode.objects.filter(deployment_name=self.deployment_name, is_active_generation=1)\n    if (len(active_nodes) == 0):\n        return None\n    first_active_node = active_nodes[0]\n    gen_id = first_active_node.generation_id\n    for active_node in active_nodes:\n        if (active_node.generation_id != gen_id):\n            err_str = 'Inconsistent generation ids in simpledb. %s:%s and %s:%s both marked active'\n            context = (first_active_node.aws_id, first_active_node.generation_id, active_node.aws_id, active_node.generation_id)\n            raise Exception((err_str % context))\n    self._active_gen_id = gen_id\n    return self._active_gen_id\n", "label": 0}
{"function": "\n\ndef _extend_network_dict_address_scope(self, network_res, network_db):\n    network_res[ext_address_scope.IPV4_ADDRESS_SCOPE] = None\n    network_res[ext_address_scope.IPV6_ADDRESS_SCOPE] = None\n    subnetpools = {subnet.subnetpool for subnet in network_db.subnets if subnet.subnetpool}\n    for subnetpool in subnetpools:\n        as_id = subnetpool[ext_address_scope.ADDRESS_SCOPE_ID]\n        if (subnetpool['ip_version'] == constants.IP_VERSION_4):\n            network_res[ext_address_scope.IPV4_ADDRESS_SCOPE] = as_id\n        if (subnetpool['ip_version'] == constants.IP_VERSION_6):\n            network_res[ext_address_scope.IPV6_ADDRESS_SCOPE] = as_id\n    return network_res\n", "label": 1}
{"function": "\n\ndef fixup_indent(suite):\n    ' If an INDENT is followed by a thing with a prefix then nuke the prefix\\n        Otherwise we get in trouble when removing __metaclass__ at suite start\\n    '\n    kids = suite.children[::(- 1)]\n    while kids:\n        node = kids.pop()\n        if (node.type == token.INDENT):\n            break\n    while kids:\n        node = kids.pop()\n        if (isinstance(node, Leaf) and (node.type != token.DEDENT)):\n            if node.prefix:\n                node.prefix = ''\n            return\n        else:\n            kids.extend(node.children[::(- 1)])\n", "label": 1}
{"function": "\n\n@property\ndef message(self):\n    esc = self.enhanced_status_code\n    msg = self._message\n    if (esc and msg):\n        return ' '.join((esc, msg))\n    else:\n        return msg\n", "label": 0}
{"function": "\n\n@dispatch(Expr, MongoQuery)\ndef post_compute(e, q, scope=None):\n    \"\\n    Execute a query using MongoDB's aggregation pipeline\\n\\n    The compute_up functions operate on Mongo Collection / list-of-dict\\n    queries.  Once they're done we need to actually execute the query on\\n    MongoDB.  We do this using the aggregation pipeline framework.\\n\\n    http://docs.mongodb.org/manual/core/aggregation-pipeline/\\n    \"\n    scope = {\n        '$project': toolz.merge({\n            '_id': 0,\n        }, dict(((col, 1) for col in e.fields))),\n    }\n    q = q.append(scope)\n    if (not e.dshape.shape):\n        result = get_result(q.coll.aggregate(list(q.query)))[0]\n        if isscalar(e.dshape.measure):\n            return result[e._name]\n        else:\n            return get(e.fields, result)\n    dicts = get_result(q.coll.aggregate(list(q.query)))\n    if isscalar(e.dshape.measure):\n        return list(pluck(e.fields[0], dicts, default=None))\n    else:\n        return list(pluck(e.fields, dicts, default=None))\n", "label": 0}
{"function": "\n\ndef test_deploy(self):\n    fake_tenants_list = [self.fake_tenant_0, self.fake_tenant_1]\n    fake_users_list = [self.fake_user_0, self.fake_user_1]\n    fake_roles_list = [self.fake_role_0, self.fake_role_1]\n    fake_info = self._get_fake_info(fake_tenants_list, fake_users_list, fake_roles_list)\n    self.mock_client().tenants.list.return_value = [fake_tenants_list[0]]\n    self.mock_client().users.list.return_value = [fake_users_list[0]]\n    self.mock_client().roles.list.return_value = [fake_roles_list[0]]\n    self.mock_client().roles.roles_for_user.return_value = [self.fake_role_1]\n\n    def tenant_create(**kwargs):\n        self.mock_client().tenants.list.return_value.append(fake_tenants_list[1])\n        return fake_tenants_list[1]\n\n    def user_create(**kwars):\n        self.mock_client().users.list.return_value.append(fake_users_list[1])\n        return fake_users_list[1]\n\n    def roles_create(role_name):\n        self.mock_client().roles.list.return_value.append(fake_roles_list[1])\n        return fake_roles_list[1]\n    self.mock_client().tenants.create = tenant_create\n    self.mock_client().users.create = user_create\n    self.mock_client().roles.create = roles_create\n    self.keystone_client.deploy(fake_info)\n    mock_calls = []\n    for user in fake_users_list:\n        for tenant in fake_tenants_list:\n            mock_calls.append(mock.call(user.id, fake_roles_list[0].id, tenant.id))\n    self.assertEquals(mock_calls, self.mock_client().roles.add_user_role.mock_calls)\n", "label": 0}
{"function": "\n\ndef update(self, other=None, **kwargs):\n    if (other is None):\n        pass\n    elif hasattr(other, 'iteritems'):\n        for (k, v) in other.iteritems():\n            self[k] = v\n    elif hasattr(other, 'keys'):\n        for k in other.keys():\n            self[k] = other[k]\n    else:\n        for (k, v) in other:\n            self[k] = v\n    if kwargs:\n        self.update(kwargs)\n", "label": 1}
{"function": "\n\ndef showStack(self, index):\n    if ((index >= self.getWidgetCount()) or (index == self.visibleStack)):\n        return\n    if (self.visibleStack >= 0):\n        self.setStackVisible(self.visibleStack, False)\n    self.visibleStack = index\n    self.setStackVisible(self.visibleStack, True)\n    for listener in self.stackListeners:\n        listener.onStackChanged(self, index)\n", "label": 0}
{"function": "\n\ndef list_users(order_by='id'):\n    '\\n    Show all users for this company.\\n\\n    CLI Example:\\n\\n        salt myminion bamboohr.list_users\\n\\n    By default, the return data will be keyed by ID. However, it can be ordered\\n    by any other field. Keep in mind that if the field that is chosen contains\\n    duplicate values (i.e., location is used, for a company which only has one\\n    location), then each duplicate value will be overwritten by the previous.\\n    Therefore, it is advisable to only sort by fields that are guaranteed to be\\n    unique.\\n\\n    CLI Examples:\\n\\n        salt myminion bamboohr.list_users order_by=id\\n        salt myminion bamboohr.list_users order_by=email\\n    '\n    ret = {\n        \n    }\n    (status, result) = _query(action='meta', command='users')\n    root = ET.fromstring(result)\n    users = root.getchildren()\n    for user in users:\n        user_id = None\n        user_ret = {\n            \n        }\n        for item in user.items():\n            user_ret[item[0]] = item[1]\n            if (item[0] == 'id'):\n                user_id = item[1]\n        for item in user.getchildren():\n            user_ret[item.tag] = item.text\n        ret[user_ret[order_by]] = user_ret\n    return ret\n", "label": 0}
{"function": "\n\ndef set_display(self, locale_id=None, media_image=None, media_audio=None):\n    text = (Text(locale_id=locale_id) if locale_id else None)\n    if (media_image or media_audio):\n        self.display = Display(text=text, media_image=media_image, media_audio=media_audio)\n    elif text:\n        self.text = text\n", "label": 0}
{"function": "\n\ndef downloadAnimea(self, manga, chapter_start, chapter_end, download_path, download_format):\n    for current_chapter in range(chapter_start, (chapter_end + 1)):\n        manga_chapter_prefix = ((manga.lower().replace('-', '_') + '_') + str(current_chapter).zfill(3))\n        if ((os.path.exists(((download_path + manga_chapter_prefix) + '.cbz')) or os.path.exists(((download_path + manga_chapter_prefix) + '.zip'))) and (overwrite_FLAG == False)):\n            print((('Chapter ' + str(current_chapter)) + ' already downloaded, skipping to next chapter...'))\n            continue\n        url = (((('http://manga.animea.net/' + manga) + '-chapter-') + str(current_chapter)) + '-page-1.html')\n        source = getSourceCode(url)\n        max_pages = int(re.compile('of (.*?)</title>').search(source).group(1))\n        for page in range(1, (max_pages + 1)):\n            url = (((((('http://manga.animea.net/' + manga) + '-chapter-') + str(current_chapter)) + '-page-') + str(page)) + '.html')\n            source = getSourceCode(url)\n            img_url = re.compile('img src=\"(http.*?.[jp][pn]g)\"').search(source).group(1)\n            print((((('Chapter ' + str(current_chapter)) + ' / ') + 'Page ') + str(page)))\n            print(img_url)\n            downloadImage(img_url, os.path.join('mangadl_tmp', ((manga_chapter_prefix + '_') + str(page).zfill(3))))\n        compress(manga_chapter_prefix, download_path, max_pages, download_format)\n", "label": 1}
{"function": "\n\ndef __init__(self, noop=False):\n    if noop:\n        self.func_name = 'posix_fallocate'\n        self.fallocate = noop_libc_function\n        return\n    for func in ('fallocate', 'posix_fallocate'):\n        self.func_name = func\n        self.fallocate = load_libc_function(func, log_error=False)\n        if (self.fallocate is not noop_libc_function):\n            break\n    if (self.fallocate is noop_libc_function):\n        logging.warning(_('Unable to locate fallocate, posix_fallocate in libc.  Leaving as a no-op.'))\n", "label": 0}
{"function": "\n\ndef setup_logger(logger, stream, filename=None, fmt=None):\n    \"Sets up a logger (if no handlers exist) for console output,\\n    and file 'tee' output if desired.\"\n    if (len(logger.handlers) < 1):\n        console = logging.StreamHandler(stream)\n        console.setLevel(logging.DEBUG)\n        console.setFormatter(logging.Formatter(fmt))\n        logger.addHandler(console)\n        logger.setLevel(logging.DEBUG)\n        if filename:\n            outfile = logging.FileHandler(filename)\n            outfile.setLevel(logging.INFO)\n            outfile.setFormatter(logging.Formatter(('%(asctime)s ' + (fmt if fmt else '%(message)s'))))\n            logger.addHandler(outfile)\n", "label": 0}
{"function": "\n\ndef get_value(self, select, table_name, where=None, extra=None):\n    '\\n        Get a value from the table.\\n\\n        :param str select: Attribute for SELECT query\\n        :param str table_name: Table name of executing the query.\\n        :return: Result of execution of the query.\\n        :raises simplesqlite.NullDatabaseConnectionError:\\n            |raises_check_connection|\\n        :raises sqlite3.OperationalError: |raises_operational_error|\\n\\n        .. seealso::\\n\\n            :py:meth:`.sqlquery.SqlQuery.make_select`\\n        '\n    self.verify_table_existence(table_name)\n    query = SqlQuery.make_select(select, table_name, where, extra)\n    result = self.execute_query(query, logging.getLogger().findCaller())\n    if (result is None):\n        return None\n    fetch = result.fetchone()\n    if (fetch is None):\n        return None\n    return fetch[0]\n", "label": 0}
{"function": "\n\n@property\ndef photos(self):\n    if (not hasattr(self, '_photos')):\n        available_photos = glob.glob(os.path.join(self.directory, '*.jpg'))\n        self._photos = []\n        for photo_path in available_photos:\n            (_pdir, photo) = os.path.split(photo_path)\n            if (self.xml.find(photo) > 0):\n                self._photos.append(photo_path)\n    return self._photos\n", "label": 0}
{"function": "\n\ndef glob_staticfiles(self, item):\n    for finder in finders.get_finders():\n        if hasattr(finder, 'storages'):\n            storages = finder.storages.values()\n        elif hasattr(finder, 'storage'):\n            storages = [finder.storage]\n        else:\n            continue\n        for storage in storages:\n            globber = StorageGlobber(storage)\n            for file in globber.glob(item):\n                (yield storage.path(file))\n", "label": 1}
{"function": "\n\n@test.create_stubs({\n    api.nova: ('aggregate_get', 'add_host_to_aggregate', 'remove_host_from_aggregate', 'host_list'),\n})\ndef _test_manage_hosts_update(self, host, aggregate, form_data, addAggregate=False, cleanAggregates=False):\n    if cleanAggregates:\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host3').InAnyOrder()\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host2').InAnyOrder()\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host1').InAnyOrder()\n    api.nova.aggregate_get(IsA(http.HttpRequest), str(aggregate.id)).AndReturn(aggregate)\n    api.nova.host_list(IsA(http.HttpRequest)).AndReturn(self.hosts.list())\n    api.nova.aggregate_get(IsA(http.HttpRequest), str(aggregate.id)).AndReturn(aggregate)\n    if addAggregate:\n        api.nova.add_host_to_aggregate(IsA(http.HttpRequest), str(aggregate.id), host.host_name)\n    self.mox.ReplayAll()\n    res = self.client.post(reverse(constants.AGGREGATES_MANAGE_HOSTS_URL, args=[aggregate.id]), form_data)\n    self.assertNoFormErrors(res)\n    self.assertRedirectsNoFollow(res, reverse(constants.AGGREGATES_INDEX_URL))\n", "label": 0}
{"function": "\n\ndef _uncached_match(self, text, pos, cache, error):\n    new_pos = pos\n    children = []\n    while True:\n        node = self.members[0].match_core(text, new_pos, cache, error)\n        if ((node is None) or (not (node.end - node.start))):\n            return Node(self.name, text, pos, new_pos, children)\n        children.append(node)\n        new_pos += (node.end - node.start)\n", "label": 0}
{"function": "\n\ndef autoassign(self, locals):\n    '\\n    Automatically assigns local variables to `self`.\\n    Generally used in `__init__` methods, as in::\\n\\n        def __init__(self, foo, bar, baz=1): \\n            autoassign(self, locals())\\n\\n    '\n    for (key, value) in locals.iteritems():\n        if (key == 'self'):\n            continue\n        setattr(self, key, value)\n", "label": 0}
{"function": "\n\ndef parse_args(self, sys_argv):\n    'Parsing, public for unit testing.  sys_argv should be sys.argv.\\nThe first entry is skipped, to account for sys.argv putting the\\nprogram name as the first element.\\n        '\n    if (len(sys_argv) == 0):\n        raise Driver.ParsingException('sys_argv must contain at least one element (name of the executing program, per sys.argv[0])')\n    parser = argparse.ArgumentParser(description='Migrate one or more databases (or default database, if one is assigned).')\n    parser.add_argument('-n', '--new', help='Create new (empty) database(s)', action='store_true')\n    parser.add_argument('-s', '--schema', help='Run baseline schema(e)', action='store_true')\n    parser.add_argument('-m', '--migrations', help='Run migrations', action='store_true')\n    parser.add_argument('-c', '--code', help='Run code (for views, stored procs, etc)', action='store_true')\n    parser.add_argument('-d', '--data', help='Load reference (bootstrap) data', action='store_true')\n    parser.add_argument('-u', '--update', help='Updates database (runs migrations, code, and data)', action='store_true')\n    parser.add_argument('databases', metavar='db', nargs='*', help='nickname of database to manipulate')\n    args = parser.parse_args(sys_argv[1:])\n    if ((len(args.databases) == 0) and (self.default_database != '')):\n        args.databases.append(self.default_database)\n    return args\n", "label": 0}
{"function": "\n\ndef computeExpression(self, constraint_collection):\n    function = self.getFunction()\n    values = self.getArgumentValues()\n    cost = function.getCallCost(values)\n    if function.getFunctionRef().getFunctionBody().mayRaiseException(BaseException):\n        constraint_collection.onExceptionRaiseExit(BaseException)\n    if ((cost is not None) and (cost < 50)):\n        result = function.createOutlineFromCall(provider=self.getParentVariableProvider(), values=values)\n        return (result, 'new_statements', 'Function call in-lined.')\n    return (self, None, None)\n", "label": 0}
{"function": "\n\ndef __l2cap_advertise_service(self, name, service_id, service_classes, profiles, provider, description, protocols):\n    if (self._sdpservice is not None):\n        raise BluetoothError('Service already advertised')\n    if (not self.listening):\n        raise BluetoothError('Socket must be listening before advertised')\n    if protocols:\n        raise NotImplementedError('extra protocols not yet supported in Widcomm stack')\n    self._sdpservice = _widcomm._WCSdpService()\n    if service_classes:\n        service_classes = [to_full_uuid(s) for s in service_classes]\n        _sdp_checkraise(self._sdpservice.add_service_class_id_list(service_classes))\n    _sdp_checkraise(self._sdpservice.add_l2cap_protocol_descriptor(self.port))\n    if profiles:\n        for (uuid, version) in profiles:\n            uuid = to_full_uuid(uuid)\n            _sdp_checkraise(self._sdpservice.add_profile_descriptor_list(uuid, version))\n    _sdp_checkraise(self._sdpservice.add_service_name(name))\n    _sdp_checkraise(self._sdpservice.make_public_browseable())\n", "label": 1}
{"function": "\n\ndef listPropsAndMethods(self):\n    res = []\n    if (sublime.platform() == 'windows'):\n        app = 'node'\n        pathToJS = (sublime.packages_path() + '\\\\NPMInfo\\\\npm-info.js')\n        cmd = [app, pathToJS, self.pkgPath]\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, startupinfo=startupinfo)\n    else:\n        pathToJS = (sublime.packages_path() + '/NPMInfo/npm-info.js')\n        cmd = ['/usr/local/bin/node', pathToJS, self.pkgPath]\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    for line in iter(p.stdout.readline, b''):\n        lineStrip = line.rstrip()\n        if ((len(lineStrip) > 0) and (' ' not in lineStrip)):\n            res.append(lineStrip)\n    self.view.window().show_quick_panel(res, self.onSelectPropAndMethod)\n", "label": 0}
{"function": "\n\ndef _visible_changed(self, value):\n    if ((self.scene is not None) and self.marker.interactor):\n        super(OrientationAxes, self)._visible_changed(value)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(StripeFormMixin, self).__init__(*args, hidden_inputs=False, **kwargs)\n    widget = StripeCheckoutWidget(provider=self.provider, payment=self.payment)\n    self.fields['stripeToken'] = forms.CharField(widget=widget)\n    if (self.is_bound and (not self.data.get('stripeToken'))):\n        self.payment.change_status('rejected')\n        raise RedirectNeeded(self.payment.get_failure_url())\n", "label": 0}
{"function": "\n\ndef skip_past(self, endtag):\n    while self.tokens:\n        token = self.next_token()\n        if ((token.token_type == TOKEN_BLOCK) and (token.contents == endtag)):\n            return\n    self.unclosed_block_tag([endtag])\n", "label": 0}
{"function": "\n\n@AccountRequired\n@PostOnly\ndef follow(request, user_id):\n    followed = get_object_or_404(User, pk=user_id)\n    (follow_instance, created) = UserToUserFollow.objects.get_or_create(follower=request.user, followed=followed)\n    if (not follow_instance.is_following):\n        follow_instance.is_following = True\n        follow_instance.save()\n    if created:\n        send_notification(type=EmailTypes.FOLLOW, user=followed, entity=request.user)\n    cache.bust(followed)\n    if request.is_ajax():\n        button = render_inclusiontag(request, 'follow_button followed', 'users_tags', {\n            'followed': followed,\n        })\n        return json_response({\n            'button': button,\n        })\n    else:\n        return redirect(followed)\n", "label": 0}
{"function": "\n\ndef __unicode__(self):\n    qs = []\n    ctx_id = self.context_id\n    if self.is_assignment:\n        qs += ['\"{0}\" = %({1})s'.format(self.field, ctx_id)]\n    else:\n        for _ in (self._updates or []):\n            qs += ['\"{0}\"[%({1})s] = %({2})s'.format(self.field, ctx_id, (ctx_id + 1))]\n            ctx_id += 2\n    return ', '.join(qs)\n", "label": 0}
{"function": "\n\ndef visit_Name(self, node):\n    if (node.id in {'True', 'False', 'None'}):\n        return node\n    elif (node.id in self.args):\n\n        def match_capture(n, context):\n            if (not isinstance(n, self.args[node.id])):\n                return False\n            context['captures'][node.id] = n\n            if (self.args[node.id] == ast.Name):\n                if ((node.id in context['names']) != (n.id in context['rev'])):\n                    return False\n                if (node.id in context['names']):\n                    return (n.id == context['names'][node.id])\n                else:\n                    context['names'][node.id] = n.id\n                    context['rev'][n.id] = node.id\n                    return True\n            return True\n        return match_capture\n    else:\n        return node\n", "label": 0}
{"function": "\n\ndef getRecord(self, metadataPrefix, identifier):\n    self._checkMetadataPrefix(metadataPrefix)\n    header = None\n    metadata = None\n    for record in self._listQuery(identifier=identifier):\n        (header, metadata) = self._createHeaderAndMetadata(record)\n    if (header is None):\n        raise oaipmh.error.IdDoesNotExistError(identifier)\n    return (header, metadata, None)\n", "label": 0}
{"function": "\n\ndef visit_Assign(self, node):\n    is_name = (lambda n: isinstance(n, ast.Name))\n    for name in filter(is_name, node.targets):\n        if ((name.id == '__all__') and isinstance(node.value, ast.List)):\n            for subnode in node.value.elts:\n                if isinstance(subnode, ast.Str):\n                    self._tree.add_explicit_export(subnode.s, 1.2)\n        elif (not name.id.startswith('_')):\n            self._tree.add(name.id, 1.1)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    'Compares the current instance with another one.'\n    if (self is other):\n        return True\n    if isinstance(other, self.__class__):\n        return (other._values == self._values)\n    return (other == self._values)\n", "label": 0}
{"function": "\n\n@property\ndef cols(self):\n    '\\n        Return all cells in range by row\\n        '\n    for col in range(self.min_col, (self.max_col + 1)):\n        (yield tuple((('%s%d' % (get_column_letter(col), row)) for row in range(self.min_row, (self.max_row + 1)))))\n", "label": 0}
{"function": "\n\ndef www_authenticate(realm, key, algorithm='MD5', nonce=None, qop=qop_auth, stale=False):\n    'Constructs a WWW-Authenticate header for Digest authentication.'\n    if (qop not in valid_qops):\n        raise ValueError((\"Unsupported value for qop: '%s'\" % qop))\n    if (algorithm not in valid_algorithms):\n        raise ValueError((\"Unsupported value for algorithm: '%s'\" % algorithm))\n    if (nonce is None):\n        nonce = synthesize_nonce(realm, key)\n    s = ('Digest realm=\"%s\", nonce=\"%s\", algorithm=\"%s\", qop=\"%s\"' % (realm, nonce, algorithm, qop))\n    if stale:\n        s += ', stale=\"true\"'\n    return s\n", "label": 0}
{"function": "\n\ndef delete_color(self, color, palette_type, palette_name):\n    'Delete color.'\n    if (palette_type == '__special__'):\n        if (palette_name == 'Favorites'):\n            favs = util.get_favs()['colors']\n            if (color in favs):\n                favs.remove(color)\n                util.save_palettes(favs, favs=True)\n                self.show_colors(palette_type, palette_name, delete=True, update=False)\n    elif (palette_type in ('__global__', '__project__')):\n        if (palette_type == '__global__'):\n            color_palettes = util.get_palettes()\n        else:\n            color_palettes = util.get_project_palettes(self.view.window())\n        for palette in color_palettes:\n            if (palette_name == palette['name']):\n                if (color in palette['colors']):\n                    palette['colors'].remove(color)\n                    if (palette_type == '__global__'):\n                        util.save_palettes(color_palettes)\n                    else:\n                        util.save_project_palettes(self.view.window(), color_palettes)\n                    self.show_colors(palette_type, palette_name, delete=True, update=False)\n                    break\n", "label": 1}
{"function": "\n\ndef _get_ports(self, remote_address, network_id=None, router_id=None):\n    'Search for all ports that contain passed ip address and belongs to\\n        given network.\\n\\n        If no network is passed ports are searched on all networks connected to\\n        given router. Either one of network_id or router_id must be passed.\\n\\n        '\n    if network_id:\n        networks = (network_id,)\n    elif router_id:\n        networks = self._get_router_networks(router_id)\n    else:\n        raise TypeError(_('Either one of parameter network_id or router_id must be passed to _get_ports method.'))\n    return self._get_ports_for_remote_address(remote_address, networks)\n", "label": 0}
{"function": "\n\ndef decode_response(client_message, to_object=None):\n    ' Decode response from client message'\n    parameters = dict(partitions=None)\n    partitions_size = client_message.read_int()\n    partitions = {\n        \n    }\n    for partitions_index in xrange(0, partitions_size):\n        partitions_key = AddressCodec.decode(client_message, to_object)\n        partitions_val_size = client_message.read_int()\n        partitions_val = []\n        for partitions_val_index in xrange(0, partitions_val_size):\n            partitions_val_item = client_message.read_int()\n            partitions_val.append(partitions_val_item)\n        partitions[partitions_key] = partitions_val\n    parameters['partitions'] = partitions\n    return parameters\n", "label": 0}
{"function": "\n\ndef downloadMovie(self, url, path, title, extension):\n    if (not os.path.exists(path)):\n        common.log('Path does not exist')\n        return None\n    if (title == ''):\n        common.log('No title given')\n        return None\n    file_path = xbmc.makeLegalFilename(os.path.join(path, (title + extension)))\n    file_path = urllib.unquote_plus(file_path)\n    if os.path.isfile(file_path):\n        self.pDialog = xbmcgui.Dialog()\n        if (not common.ask(('File already exists. Overwrite?\\n' + os.path.basename(file_path)))):\n            title = common.showOSK(urllib.unquote_plus(title), common.translate(30102))\n            if (not title):\n                return None\n            file_path = xbmc.makeLegalFilename(os.path.join(path, (title + extension)))\n            file_path = urllib.unquote_plus(file_path)\n    success = self.__download(url, file_path)\n    if success:\n        return file_path\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef _checkMetadata(self, variantFile):\n    '\\n        Checks that metadata is consistent\\n        '\n    metadata = self._getMetadataFromVcf(variantFile)\n    if ((self._metadata is not None) and (self._metadata != metadata)):\n        raise exceptions.InconsistentMetaDataException(variantFile.filename)\n", "label": 0}
{"function": "\n\ndef get_user(self, identifier):\n    if self._is_numeric(identifier):\n        return self.user_model.query.get(identifier)\n    for attr in get_identity_attributes():\n        query = getattr(self.user_model, attr).ilike(identifier)\n        rv = self.user_model.query.filter(query).first()\n        if (rv is not None):\n            return rv\n", "label": 0}
{"function": "\n\ndef _get_external_player(self):\n    ' Determines external sound player to available.\\n\\n            Returns string or ``None``.\\n            '\n    if common.IS_MACOSX:\n        return 'afplay'\n    else:\n        for name in ('mpg123', 'play', 'aplay'):\n            if common.which(name):\n                return name\n    return None\n", "label": 0}
{"function": "\n\ndef _get_parent(self, path, ref, parent_type):\n    ' Return the path of the parent of type \"parent_type\" for the object\\n        in \"path\" with id \"ref\". Returns an empty string if no parent extists.\\n        '\n    parts = path.split('/')\n    if ((parent_type.lower() == 'block') or (parts[(- 4)] == (parent_type.lower() + 's'))):\n        return '/'.join(parts[:(- 2)])\n    object_folder = parts[(- 2)]\n    parent_folder = parts[(- 4)]\n    if (parent_folder in ('recordingchannels', 'units')):\n        block_path = '/'.join(parts[:(- 6)])\n    else:\n        block_path = '/'.join(parts[:(- 4)])\n    if (parent_type.lower() in ('recordingchannel', 'unit')):\n        path = (block_path + '/recordingchannelgroups')\n        for n in self._data.iterNodes(path):\n            if (not ('_type' in n._v_attrs)):\n                continue\n            p = self._search_parent(('%s/%ss' % (n._v_pathname, parent_type.lower())), object_folder, ref)\n            if (p != ''):\n                return p\n        return ''\n    if (parent_type.lower() == 'segment'):\n        path = (block_path + '/segments')\n    elif (parent_type.lower() in ('recordingchannelgroup', 'recordingchannelgroups')):\n        path = (block_path + '/recordingchannelgroups')\n    else:\n        return ''\n    return self._search_parent(path, object_folder, ref)\n", "label": 1}
{"function": "\n\ndef __init__(self, model, columns, values, where):\n    self.model = model\n    self.columns = columns\n    self.where = where\n    self.values = []\n    for value in values:\n        if hasattr(value, 'sql'):\n            self.values.append(value.sql())\n        else:\n            self.values.append(value)\n", "label": 0}
{"function": "\n\n@app.route('/signin', methods=['GET', 'POST'])\ndef signin():\n    'Sign in'\n    form = SignInForm()\n    if form.validate_on_submit():\n        username = form.username.data.strip()\n        password = form.password.data.strip()\n        if is_valid_login(username, password):\n            session['theme'] = _get_users_theme(username)\n            session['logged_in'] = True\n            session['username'] = username\n            if form.remember_me.data:\n                session.permanent = True\n            else:\n                session.permanent = False\n            if (request.args.get('next') == 'signin'):\n                return redirect('/')\n            else:\n                return redirect((request.args.get('next') or request.referrer or '/'))\n        else:\n            return render_template('signin.html', form=form, error='Failed')\n    return render_template('signin.html', form=form, error=None)\n", "label": 1}
{"function": "\n\ndef checkRemainingArguments(self, arguments):\n    allowedAttributes = {\n        \n    }\n    allowedAttributes['allow multiple values'] = (bool, False, True, 'allowMultipleValues')\n    allowedAttributes['command line argument'] = (str, True, True, 'commandLineArgument')\n    allowedAttributes['construct filename'] = (dict, False, True, 'constructionInstructions')\n    allowedAttributes['data type'] = (str, True, True, 'dataType')\n    allowedAttributes['description'] = (str, True, True, 'description')\n    allowedAttributes['extensions'] = (list, False, True, 'extensions')\n    allowedAttributes['hide argument in help'] = (bool, False, True, 'hideInHelp')\n    allowedAttributes['include in reduced plot'] = (bool, False, True, 'includeInReducedPlot')\n    allowedAttributes['include value in quotations'] = (bool, False, True, 'includeInQuotations')\n    allowedAttributes['long form argument'] = (str, True, True, 'longFormArgument')\n    allowedAttributes['modify argument'] = (str, False, True, 'modifyArgument')\n    allowedAttributes['modify value'] = (str, False, True, 'modifyValue')\n    allowedAttributes['replace substring'] = (list, False, False, None)\n    allowedAttributes['required'] = (bool, False, True, 'isRequired')\n    allowedAttributes['short form argument'] = (str, False, True, 'shortFormArgument')\n    allowedAttributes['value command'] = (dict, False, True, 'valueCommand')\n    for category in arguments:\n        if ((category != 'Inputs') and (category != 'Outputs')):\n            self.checkArguments(category, arguments[category], allowedAttributes, isInput=False, isOutput=False)\n", "label": 0}
{"function": "\n\ndef format_text_table(table):\n    col_width = [max((len(str(x)) for x in col)) for col in zip(*table)]\n    output = []\n    for row in table:\n        inner = ' | '.join(('{0:{1}}'.format(x, col_width[i]) for (i, x) in enumerate(row)))\n        output.append('| {0} |'.format(inner))\n    return output\n", "label": 0}
{"function": "\n\n@csrf_protect_m\n@filter_hook\ndef get(self, request, *args, **kwargs):\n    \"\\n        The 'change list' admin view for this model.\\n        \"\n    response = self.get_result_list()\n    if response:\n        return response\n    context = self.get_context()\n    context.update((kwargs or {\n        \n    }))\n    response = self.get_response(context, *args, **kwargs)\n    return (response or TemplateResponse(request, (self.object_list_template or self.get_template_list('views/model_list.html')), context))\n", "label": 0}
{"function": "\n\ndef getattr(self, path, fh=None):\n    '\\n        Returns a dictionary with keys identical to the stat C structure of\\n        stat(2).\\n\\n        st_atime, st_mtime and st_ctime should be floats.\\n\\n        NOTE: There is an incombatibility between Linux and Mac OS X\\n        concerning st_nlink of directories. Mac OS X counts all files inside\\n        the directory, while Linux counts only the subdirectories.\\n        '\n    if ((path not in self.repo.get_commit_dates()) and (path != '/')):\n        raise FuseOSError(ENOENT)\n    attrs = super(HistoryView, self).getattr(path, fh)\n    attrs.update({\n        'st_mode': (S_IFDIR | 365),\n        'st_nlink': 2,\n        'st_ctime': self._get_first_commit_time(),\n        'st_mtime': self._get_last_commit_time(),\n    })\n    return attrs\n", "label": 0}
{"function": "\n\ndef __init__(self, database, name, is_admin=None, read_from=None, write_to=None):\n    self.__database = database\n    self.__client = database.client\n    self.__name = name\n    self.__is_admin = (is_admin or self.IS_ADMIN)\n    self.__read_from = (read_from or self.READ_FROM)\n    self.__write_to = (write_to or self.WRITE_TO)\n", "label": 0}
{"function": "\n\ndef _on_gstplayer_message(mtype, message):\n    if (mtype == 'error'):\n        Logger.error('VideoGstplayer: {}'.format(message))\n    elif (mtype == 'warning'):\n        Logger.warning('VideoGstplayer: {}'.format(message))\n    elif (mtype == 'info'):\n        Logger.info('VideoGstplayer: {}'.format(message))\n", "label": 0}
{"function": "\n\ndef get_mapping(self, cls, no_mapping_ok=False):\n    db = None\n    for candidate_cls in getmro(cls):\n        db = self.mapping.get(candidate_cls, None)\n        if (db is not None):\n            break\n    if (db is None):\n        db = self.default_database\n    if (db is None):\n        if no_mapping_ok:\n            return None\n        raise ValueError(('There is no database mapping for %s' % repr(cls)))\n    return db\n", "label": 1}
{"function": "\n\ndef __init__(self, game_list, game_path=None, slug=None, games_root=None, deploy_enable=False, manifest_name=None):\n    self.game_list = game_list\n    self.slug = None\n    self.title = None\n    self.path = None\n    self.cover_art = ImageDetail(self, 'cover_art.jpg')\n    self.title_logo = ImageDetail(self, 'title_logo.jpg')\n    self.modified = None\n    self.deployed = None\n    self.is_temporary = True\n    self.plugin_main = None\n    self.canvas_main = None\n    self.flash_main = None\n    self.mapping_table = None\n    self.deploy_files = None\n    self.has_mapping_table = None\n    self.engine_version = EngineDetail('')\n    self.is_multiplayer = False\n    self.aspect_ratio = AspectRatioDetail('')\n    if (manifest_name is None):\n        self.manifest_name = 'manifest.yaml'\n    else:\n        self.manifest_name = manifest_name\n    if (game_path is not None):\n        self.load(game_path, self.manifest_name)\n    elif (slug is not None):\n        self.update({\n            'slug': slug,\n        })\n    self.games_root = games_root\n    self.deploy_enable = deploy_enable\n", "label": 0}
{"function": "\n\ndef get_localhost_ip():\n    cmd = ['/sbin/ifconfig']\n    eth_ip = {\n        \n    }\n    try:\n        r = exec_command(cmd)\n    except:\n        current_app.logger.error('[Dial Helpers]: exec_command error: %s:%s', cmd, sys.exc_info()[1])\n        return False\n    if (r['return_code'] == 0):\n        r_data = r['stdout'].split('\\n')\n        for (index, line) in enumerate(r_data):\n            if line.startswith('inet addr:'):\n                eth_ip[r_data[(index - 1)].split()[0]] = line.split().split(':')[1]\n    else:\n        current_app.logger.error('[Dial Helpers]: exec_command return: %s:%s:%s', cmd, r['return_code'], r['stderr'])\n        return False\n    return eth_ip\n", "label": 0}
{"function": "\n\ndef execute(self, grades, moduleDict, solutionDict):\n    failureOutputFileString = ''\n    failureOutputStdString = ''\n    for n in self.numsExperiencesForDisplay:\n        (testPass, stdOutString, fileOutString) = self.executeNExperiences(grades, moduleDict, solutionDict, n)\n        failureOutputStdString += stdOutString\n        failureOutputFileString += fileOutString\n        if (not testPass):\n            self.addMessage(failureOutputStdString)\n            self.addMessage(('For more details to help you debug, see test output file %s\\n\\n' % self.testOutFile))\n            self.writeFailureFile(failureOutputFileString)\n            return self.testFail(grades)\n    self.removeFailureFileIfExists()\n    return self.testPass(grades)\n", "label": 0}
{"function": "\n\ndef test_push_selects_groupby(self):\n    'Test pushing selections through groupby.'\n    lp = StoreTemp('OUTPUT', Select(expression.LTEQ(AttRef('c'), AttRef('a')), Select(expression.LTEQ(AttRef('b'), AttRef('c')), GroupBy([AttIndex(1), AttIndex(2), AttIndex(0)], [expression.COUNTALL()], Scan(self.x_key, self.x_scheme)))))\n    expected = collections.Counter([(b, c, a) for (a, b, c) in self.x_data if ((c <= a) and (b <= c))])\n    expected = collections.Counter(((k + (v,)) for (k, v) in expected.items()))\n    self.assertEquals(self.get_count(lp, Select), 2)\n    self.assertEquals(self.get_count(lp, Scan), 1)\n    self.assertIsInstance(lp.input, Select)\n    pp = self.logical_to_physical(lp)\n    self.assertIsInstance(pp.input, MyriaSplitConsumer)\n    self.assertIsInstance(pp.input.input.input, GroupBy)\n    self.assertEquals(self.get_count(pp, Select), 1)\n    self.db.evaluate(pp)\n    result = self.db.get_temp_table('OUTPUT')\n    self.assertEquals(result, expected)\n", "label": 0}
{"function": "\n\ndef render_to_response(self, context, **response_kwargs):\n    if (hasattr(self, 'permission') and (not self.request.user.has_perm(self.permission))):\n        return HttpResponseForbidden('You do not have permissions to access this content.')\n    return self.response_class(request=self.request, template=self.get_template_names(), context=context, **response_kwargs)\n", "label": 0}
{"function": "\n\ndef _get_project_attribute_data(row, project_attribute_key_data):\n    name = project_attribute_key_data['name']\n    data_type = project_attribute_key_data['data_type']\n    project_attribute_data = {\n        'key': project_attribute_key_data['id'],\n    }\n    if (data_type == 'BOOLEAN'):\n        project_attribute_data['boolean_value'] = (row[name] == 'True')\n    elif (data_type == 'CHAR'):\n        project_attribute_data['char_value'] = row[name]\n    elif (data_type == 'DATE'):\n        project_attribute_data['date_value'] = row[name]\n    elif (data_type == 'DATETIME'):\n        dt = datetime.strptime(row[name], '%Y-%m-%dT%H:%M:%S%z')\n        project_attribute_data['datetime_value'] = row[name]\n    elif (data_type == 'FLOAT'):\n        project_attribute_data['float_value'] = float(row[name])\n    elif (data_type == 'INTEGER'):\n        project_attribute_data['integer_value'] = int(row[name])\n    else:\n        raise NotImplementedError\n    return project_attribute_data\n", "label": 1}
{"function": "\n\ndef test_getitem(self):\n    u = self.type2test([0, 1, 2, 3, 4])\n    for i in range(len(u)):\n        self.assertEqual(u[i], i)\n        self.assertEqual(u[int(i)], i)\n    for i in range((- len(u)), (- 1)):\n        self.assertEqual(u[i], (len(u) + i))\n        self.assertEqual(u[int(i)], (len(u) + i))\n    self.assertRaises(IndexError, u.__getitem__, ((- len(u)) - 1))\n    self.assertRaises(IndexError, u.__getitem__, len(u))\n    self.assertRaises(ValueError, u.__getitem__, slice(0, 10, 0))\n    u = self.type2test()\n    self.assertRaises(IndexError, u.__getitem__, 0)\n    self.assertRaises(IndexError, u.__getitem__, (- 1))\n    self.assertRaises(TypeError, u.__getitem__)\n    a = self.type2test([10, 11])\n    self.assertEqual(a[0], 10)\n    self.assertEqual(a[1], 11)\n    self.assertEqual(a[(- 2)], 10)\n    self.assertEqual(a[(- 1)], 11)\n    self.assertRaises(IndexError, a.__getitem__, (- 3))\n    self.assertRaises(IndexError, a.__getitem__, 3)\n", "label": 0}
{"function": "\n\ndef get_tokens_unprocessed(self, text):\n    bashlexer = BashLexer(**self.options)\n    pos = 0\n    curcode = ''\n    insertions = []\n    for match in line_re.finditer(text):\n        line = match.group()\n        m = re.match('^((?:\\\\(\\\\S+\\\\))?(?:|sh\\\\S*?|\\\\w+\\\\S+[@:]\\\\S+(?:\\\\s+\\\\S+)?|\\\\[\\\\S+[@:][^\\\\n]+\\\\].+)[$#%])(.*\\\\n?)', line)\n        if m:\n            if (not insertions):\n                pos = match.start()\n            insertions.append((len(curcode), [(0, Generic.Prompt, m.group(1))]))\n            curcode += m.group(2)\n        elif line.startswith('>'):\n            insertions.append((len(curcode), [(0, Generic.Prompt, line[:1])]))\n            curcode += line[1:]\n        else:\n            if insertions:\n                toks = bashlexer.get_tokens_unprocessed(curcode)\n                for (i, t, v) in do_insertions(insertions, toks):\n                    (yield ((pos + i), t, v))\n            (yield (match.start(), Generic.Output, line))\n            insertions = []\n            curcode = ''\n    if insertions:\n        for (i, t, v) in do_insertions(insertions, bashlexer.get_tokens_unprocessed(curcode)):\n            (yield ((pos + i), t, v))\n", "label": 1}
{"function": "\n\ndef create_sample_table(cursor, metadata, args):\n    NUM_BUILT_IN = 6\n    cols = [sql.Column('sample_id', sql.Integer, primary_key=True)]\n    fields = get_ped_fields(args.ped_file)\n    optional_fields = ['family_id', 'name', 'paternal_id', 'maternal_id', 'sex', 'phenotype']\n    optional_fields += fields[NUM_BUILT_IN:]\n    for field in optional_fields:\n        if (field == 'name'):\n            cols.append(sql.Column(field, sql.String(50)))\n        else:\n            cols.append(sql.Column(field, sql.TEXT))\n    t = sql.Table('samples', metadata, *cols)\n    t.drop(checkfirst=True)\n    metadata.create_all(tables=[t])\n", "label": 0}
{"function": "\n\ndef makeRadials(self, numRadials):\n    ' make geodesic radials from number of radials '\n    segmentAngle = (360.0 / float(numRadials))\n    segmentAngleList = []\n    a = 0.0\n    while (a < 360.0):\n        segmentAngleList.append(a)\n        a += segmentAngle\n    fields = {\n        'x': 'DOUBLE',\n        'y': 'DOUBLE',\n        'Bearing': 'DOUBLE',\n        'Range': 'DOUBLE',\n    }\n    tab = self._makeTempTable('radTable', fields)\n    cursor = arcpy.da.InsertCursor(tab, ['x', 'y', 'Bearing', 'Range'])\n    for i in self.center:\n        pt = i.firstPoint\n        for r in segmentAngleList:\n            cursor.insertRow([pt.X, pt.Y, r, self.ringMax])\n    del cursor\n    self.deleteme.append(tab)\n    outRadialFeatures = os.path.join('in_memory', 'outRadials')\n    arcpy.BearingDistanceToLine_management(tab, outRadialFeatures, 'x', 'y', 'Range', self.distanceUnits, 'Bearing', 'DEGREES', 'GEODESIC', '#', self.sr)\n    self.deleteme.append(outRadialFeatures)\n    self.radialFeatures = outRadialFeatures\n    return outRadialFeatures\n", "label": 0}
{"function": "\n\ndef indication(self, server, pdu):\n    if _debug:\n        UDPMultiplexer._debug('indication %r %r', server, pdu)\n    if (pdu.pduDestination.addrType == Address.localBroadcastAddr):\n        dest = self.addrBroadcastTuple\n        if _debug:\n            UDPMultiplexer._debug('    - requesting local broadcast: %r', dest)\n    elif (pdu.pduDestination.addrType == Address.localStationAddr):\n        dest = unpack_ip_addr(pdu.pduDestination.addrAddr)\n        if _debug:\n            UDPMultiplexer._debug('    - requesting local station: %r', dest)\n    else:\n        raise RuntimeError('invalid destination address type')\n    self.directPort.indication(PDU(pdu, destination=dest))\n", "label": 1}
{"function": "\n\ndef visit_Assign(self, node):\n    ' Visits an assignment node. '\n    assign = self._assign_factory.from_ast(self.klass, node)\n    if assign.source:\n        assign.is_trait = self.klass.is_trait(assign.source)\n    else:\n        assign.is_trait = False\n    for target in assign.targets:\n        self.klass.locals[target] = assign\n        self.klass._is_trait[target] = assign.is_trait\n        if assign.is_trait:\n            self.klass.traits[target] = assign\n        else:\n            self.klass.attributes[target] = assign\n    return\n", "label": 0}
{"function": "\n\ndef _choose(old_style, new_style):\n    family = distrib_family()\n    if (family == 'debian'):\n        distrib = distrib_id()\n        at_least_trusty = ((distrib == 'Ubuntu') and (V(distrib_release()) >= V('14.04')))\n        at_least_jessie = ((distrib == 'Debian') and (V(distrib_release()) >= V('8.0')))\n        if (at_least_trusty or at_least_jessie):\n            return new_style\n        else:\n            return old_style\n    else:\n        raise UnsupportedFamily(supported=['debian'])\n", "label": 1}
{"function": "\n\ndef _1_0_cloud_ips_cip_jsjc5_map(self, method, url, body, headers):\n    if (method == 'POST'):\n        body = json.loads(body)\n        if ('destination' in body):\n            return self.response(httplib.ACCEPTED, '')\n        else:\n            data = '{\"error_name\":\"bad destination\", \"errors\": [\"Bad destination\"]}'\n            return self.response(httplib.BAD_REQUEST, data)\n", "label": 0}
{"function": "\n\ndef write(self, cell):\n    if (cell['cell_type'] == 'markdown'):\n        self.append_comments(cell['source'])\n    elif (cell['cell_type'] == 'code'):\n        self.append_code(cell['input'])\n", "label": 0}
{"function": "\n\ndef title_matches(self, regex, name=None):\n    \"\\n        Generates a :class:`revscoring.Feature` that returns True the page's\\n        title (namespace excluded) matches `regex`.\\n\\n        :Parameters:\\n            regex : `str` | `re.compile`\\n                The regex to match.  Case-insensitive by default.\\n            name : `str`\\n                A name for the new feature.\\n        \"\n    if (not hasattr(regex, 'pattern')):\n        regex = re.compile(regex, re.I)\n    if (name is None):\n        name = '{0}({1})'.format((self._name + '.title_matches'), repr(regex.pattern))\n    return bools.regex_match(regex, self.datasources.title, name=name)\n", "label": 0}
{"function": "\n\n@lower_cast(types.BaseTuple, types.BaseTuple)\ndef tuple_to_tuple(context, builder, fromty, toty, val):\n    if (isinstance(fromty, types.BaseNamedTuple) or isinstance(toty, types.BaseNamedTuple)):\n        raise NotImplementedError\n    if (len(fromty) != len(toty)):\n        raise NotImplementedError\n    olditems = cgutils.unpack_tuple(builder, val, len(fromty))\n    items = [context.cast(builder, v, f, t) for (v, f, t) in zip(olditems, fromty, toty)]\n    return context.make_tuple(builder, toty, items)\n", "label": 0}
{"function": "\n\ndef clear_cache_fields(self):\n    'Set cache fields to ``None``. Check :attr:`Field.as_cache`\\nfor information regarding fields which are considered cache.'\n    for field in self._meta.scalarfields:\n        if field.as_cache:\n            setattr(self, field.name, None)\n", "label": 0}
{"function": "\n\ndef get(self, name, default=None):\n    '\\n            Get a Python-attribute of this item instance, falls back\\n            to the same attribute in the parent item if not set in\\n            this instance, used to inherit attributes to components\\n\\n            @param name: the attribute name\\n        '\n    if (name in self.__dict__):\n        value = self.__dict__[name]\n    else:\n        value = None\n    if (value is not None):\n        return value\n    if (name[:2] == '__'):\n        raise AttributeError\n    parent = self.parent\n    if (parent is not None):\n        return parent.get(name)\n    return default\n", "label": 0}
{"function": "\n\ndef request(self, action, params=None, data='', headers=None, method='GET'):\n    ' Add the X-NFSN-Authentication header to an HTTP request. '\n    if (not headers):\n        headers = {\n            \n        }\n    if (not params):\n        params = {\n            \n        }\n    header = self._header(action, data)\n    headers['X-NFSN-Authentication'] = header\n    if (method == 'POST'):\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    return ConnectionUserAndKey.request(self, action, params, data, headers, method)\n", "label": 0}
{"function": "\n\ndef _got_payload_instance_info(self, response):\n    if ('error' in response):\n        self.activated = False\n        self.ipmi_session.unregister_keepalive(self.keepaliveid)\n        self._print_error(response['error'])\n        return\n    currowner = struct.unpack('<I', struct.pack('4B', *response['data'][:4]))\n    if (currowner[0] != self.ipmi_session.sessionid):\n        self.activated = False\n        self.ipmi_session.unregister_keepalive(self.keepaliveid)\n        self._print_error('SOL deactivated')\n        return\n", "label": 0}
{"function": "\n\ndef next3Fixtures(self, type_return='string'):\n    now = datetime.datetime.now()\n    url = (('http://www.premierleague.com/en-gb/matchday/league-table.html?season=2015-2016&month=' + months[now.month]) + '&timelineView=date&toDate=1451433599999&tableView=NEXT_3_FIXTURES')\n    team_names = soup(template='.next3FixturesTable')\n    for i in range(len(team_names)):\n        team_names[i] = str(team_names[i].text)\n    next_3_fixtures = soup.select('.club-row .col-fixture')\n    for i in range(len(next_3_fixtures)):\n        next_3_fixtures[i] = str(next_3_fixtures[i].text)\n    return_dict = {\n        \n    }\n    for i in range(len(team_names)):\n        return_dict[team_names[i]] = next_3_fixtures[i]\n    if (type_return == 'dict'):\n        return return_dict\n    return str(return_dict)\n", "label": 0}
{"function": "\n\ndef method_repr(method):\n    methname = method.im_func.func_name\n    varnames = list(method.im_func.func_code.co_varnames)[:method.im_func.func_code.co_argcount]\n    if method.im_func.func_defaults:\n        ld = len(method.im_func.func_defaults)\n        varlist = [', '.join(varnames[:(- ld)]), ', '.join([('%s=%r' % (n, v)) for (n, v) in zip(varnames[(- ld):], method.im_func.func_defaults)])]\n        return ('%s(%s)' % (methname, ', '.join(varlist)))\n    else:\n        return ('%s(%s)' % (methname, ', '.join(varnames)))\n", "label": 0}
{"function": "\n\ndef decrypt(self, id, data):\n    if (data[0] != self.TYPE):\n        raise IntegrityError('Invalid encryption envelope')\n    data = zlib.decompress(memoryview(data)[1:])\n    if (id and (sha256(data).digest() != id)):\n        raise IntegrityError('Chunk id verification failed')\n    return data\n", "label": 0}
{"function": "\n\n@defer.inlineCallbacks\ndef test_SpecifiedFields(self):\n    (yield self.coll.insert([dict(((k, v) for k in 'abcdefg')) for v in range(5)], safe=True))\n    res = (yield self.coll.find(fields={\n        'a': 1,\n        'c': 1,\n    }))\n    (yield self.coll.count(fields={\n        'a': 1,\n        'c': 1,\n    }))\n    self.assertTrue(all(((x in ['a', 'c', '_id']) for x in res[0].keys())))\n    res = (yield self.coll.find(fields=['a', 'c']))\n    (yield self.coll.count(fields=['a', 'c']))\n    self.assertTrue(all(((x in ['a', 'c', '_id']) for x in res[0].keys())))\n    res = (yield self.coll.find(fields=[]))\n    (yield self.coll.count(fields=[]))\n    self.assertTrue(all(((x in ['_id']) for x in res[0].keys())))\n    (yield self.assertFailure(self.coll.find({\n        \n    }, fields=[1]), TypeError))\n", "label": 1}
{"function": "\n\ndef returner(ret):\n    '\\n    Return data to a mongodb server\\n    '\n    (conn, mdb) = _get_conn(ret)\n    col = mdb[ret['id']]\n    if isinstance(ret['return'], dict):\n        back = _remove_dots(ret['return'])\n    else:\n        back = ret['return']\n    if isinstance(ret, dict):\n        full_ret = _remove_dots(ret)\n    else:\n        full_ret = ret\n    log.debug(back)\n    sdata = {\n        'minion': ret['id'],\n        'jid': ret['jid'],\n        'return': back,\n        'fun': ret['fun'],\n        'full_ret': full_ret,\n    }\n    if ('out' in ret):\n        sdata['out'] = ret['out']\n    if (float(version) > 2.3):\n        mdb.saltReturns.insert_one(sdata.copy())\n    else:\n        mdb.saltReturns.insert(sdata.copy())\n", "label": 0}
{"function": "\n\n@classmethod\ndef _update(cls, args):\n    api._timeout = args.timeout\n    format = args.format\n    options = None\n    if (args.options is not None):\n        try:\n            options = json.loads(args.options)\n        except:\n            raise Exception('bad json parameter')\n    res = api.Monitor.update(args.monitor_id, type=args.type, query=args.query, name=args.name, message=args.message, options=options)\n    report_warnings(res)\n    report_errors(res)\n    if (format == 'pretty'):\n        print(pretty_json(res))\n    else:\n        print(json.dumps(res))\n", "label": 0}
{"function": "\n\ndef start(self):\n    self.queue.start_print()\n    if (not self.is_alive):\n        baudrate = self.arduino_settings.get('baudrate', 9600)\n        self.serial.baudrate = baudrate\n        if is_serial_available(self.port):\n            self.serial.open()\n            self.is_alive = True\n            monitor_thread = threading.Thread(target=self.receive)\n            monitor_thread.start()\n        else:\n            msg = 'Serial port {0} already in use. '\n            msg += 'Try quitting any programs that may be using it.\\\\n'\n            self.queue.put(msg, self.port)\n            self.stop()\n", "label": 0}
{"function": "\n\ndef _get_position_ref_node(self, instance):\n    if self.is_sorted:\n        position = 'sorted-child'\n        node_parent = instance.get_parent()\n        if node_parent:\n            ref_node_id = node_parent.pk\n        else:\n            ref_node_id = ''\n    else:\n        prev_sibling = instance.get_prev_sibling()\n        if prev_sibling:\n            position = 'right'\n            ref_node_id = prev_sibling.pk\n        else:\n            position = 'first-child'\n            if instance.is_root():\n                ref_node_id = ''\n            else:\n                ref_node_id = instance.get_parent().pk\n    return {\n        '_ref_node_id': ref_node_id,\n        '_position': position,\n    }\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _parse_rule_sections(parser, config):\n    sections = [section for section in parser.sections() if (section != 'general')]\n    for rule_name in sections:\n        for (option_name, option_value) in parser.items(rule_name):\n            config.set_rule_option(rule_name, option_name, option_value)\n", "label": 0}
{"function": "\n\ndef on_input_timeout(self, cli):\n    if (not self.show_help):\n        return\n    document = cli.current_buffer.document\n    text = document.text\n    LOG.debug('document.text = %s', text)\n    LOG.debug('current_command = %s', self.completer.current_command)\n    if text.strip():\n        command = self.completer.current_command\n        key_name = '.'.join(command.split()).encode('utf-8')\n        last_option = self.completer.last_option\n        if last_option:\n            self.current_docs = self._docs.extract_param(key_name, last_option)\n        else:\n            self.current_docs = self._docs.extract_description(key_name)\n    else:\n        self.current_docs = ''\n    cli.buffers['clidocs'].reset(initial_document=Document(self.current_docs, cursor_position=0))\n    cli.request_redraw()\n", "label": 0}
{"function": "\n\ndef find_by_key(self, key=None, parent='/', **kwargs):\n    \"\\n        Returns a list of DirEntry for each directory entry that matches the given key name.\\n        If a parent is provided, only checks in this parent and all subtree.\\n        These entries are in the same org's directory but have different parents.\\n        \"\n    if (key is None):\n        raise BadRequest('Illegal arguments')\n    if (parent is None):\n        raise BadRequest('Illegal arguments')\n    start_key = [self.orgname, key, parent]\n    end_key = [self.orgname, key, (parent + 'ZZZZZZ')]\n    res = self.dir_store.find_by_view('directory', 'by_key', start_key=start_key, end_key=end_key, id_only=True, convert_doc=True, **kwargs)\n    match = [value for (docid, indexkey, value) in res]\n    return match\n", "label": 0}
{"function": "\n\ndef _get_subwin(self, widget):\n    for subwin in list(self.widget.subWindowList()):\n        if (subwin.widget() == widget):\n            return subwin\n    return None\n", "label": 0}
{"function": "\n\ndef __init__(self, port=None, path=None, timeout=60, **kwargs):\n    super(HttpLocalEapiConnection, self).__init__()\n    port = (port or DEFAULT_HTTP_LOCAL_PORT)\n    path = (path or DEFAULT_HTTP_PATH)\n    self.transport = HttpConnection(path, 'localhost', port, timeout=timeout)\n", "label": 0}
{"function": "\n\ndef begin(self):\n    'Called before any tests are collected or run\\n\\n        Loads the application, and in turn its configuration.\\n\\n        '\n    global pylonsapp\n    path = os.getcwd()\n    sys.path.insert(0, path)\n    pkg_resources.working_set.add_entry(path)\n    self.app = pylonsapp = loadapp(('config:' + self.config_file), relative_to=path)\n    conf = getattr(pylonsapp, 'config')\n    if conf:\n        pylons.config._push_object(conf)\n        if ('pylons.app_globals' in conf):\n            pylons.app_globals._push_object(conf['pylons.app_globals'])\n    translator = _get_translator(pylons.config.get('lang'))\n    pylons.translator._push_object(translator)\n", "label": 0}
{"function": "\n\ndef _parse_request_line(self, line):\n    request_line = _read_request_line_dict(line)\n    if (not request_line):\n        return\n    uri = urlparse(request_line['uri'])\n    if uri.scheme:\n        self.request.protocol = uri.scheme\n    if uri.netloc:\n        if (':' in uri.netloc):\n            (self.request.host, self.request.port) = uri.netloc.split(':')\n        else:\n            self.request.host = uri.netloc\n    if uri.port:\n        self.request.port = uri.port\n    if uri.path:\n        self.request.path = uri.path\n    if uri.query:\n        query = parse_qs(uri.query)\n        for key in query:\n            self.request.query[key] = query[key]\n    if ('method' in request_line):\n        self.request.method = request_line['method']\n", "label": 1}
{"function": "\n\ndef send_remote(self, url, data, headers=None, callback=None):\n    if (headers is None):\n        headers = {\n            \n        }\n    if (not self.state.should_try()):\n        data = self.decode(data)\n        self._log_failed_submission(data)\n        return\n    future = self._send_remote(url=url, data=data, headers=headers, callback=callback)\n    ioloop.IOLoop.current().add_future(future, partial(self._handle_result, url, data))\n    return future\n", "label": 0}
{"function": "\n\ndef __call__(self, *args):\n    if (len(args) == 0):\n        return self\n    name = ('%s/%s' % (self._name, '/'.join([str(arg) for arg in args])))\n    return _Callable(self._gh, name)\n", "label": 0}
{"function": "\n\ndef snipt_page(request, user, snipt_id):\n    try:\n        snipt = Snippet.objects.get(slug=snipt_id)\n        if ('c' in request.GET):\n            return HttpResponseRedirect(((snipt.get_absolute_url() + '#comment-') + request.GET['c']))\n    except:\n        return HttpResponseRedirect(((('/' + user) + '/tag/') + snipt_id))\n    context_user = User.objects.get(id=snipt.user.id)\n    if (request.user.id == context_user.id):\n        mine = True\n    else:\n        mine = False\n    if ((not snipt.public) and (not mine)):\n        try:\n            if (request.GET['key'] != snipt.key):\n                raise Http404()\n            else:\n                key = True\n        except:\n            raise Http404()\n    disable_wrap = request.session.get('disable_wrap')\n    disable_message = request.session.get('disable_message')\n    return render_to_response('snipt.html', locals(), context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef apply(self):\n    '\\n        Rewrite all matching setitems as static_setitems.\\n        '\n    new_block = self.block.copy()\n    new_block.clear()\n    for inst in self.block.body:\n        if (inst in self.raises):\n            (exc_type, exc_args) = self.raises[inst]\n            new_inst = ir.StaticRaise(exc_type, exc_args, inst.loc)\n            new_block.append(new_inst)\n        else:\n            new_block.append(inst)\n    return new_block\n", "label": 0}
{"function": "\n\ndef __and__(self, other):\n    'Implementation of & operator - returns C{L{Each}}'\n    if isinstance(other, basestring):\n        other = ParserElement.literalStringClass(other)\n    if (not isinstance(other, ParserElement)):\n        warnings.warn(('Cannot combine element of type %s with ParserElement' % type(other)), SyntaxWarning, stacklevel=2)\n        return None\n    return Each([self, other])\n", "label": 0}
{"function": "\n\ndef load_certificate_request(type, buffer):\n    '\\n    Load a certificate request from a buffer\\n\\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\\n    :param buffer: The buffer the certificate request is stored in\\n    :return: The X509Req object\\n    '\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if (type == FILETYPE_PEM):\n        req = _lib.PEM_read_bio_X509_REQ(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif (type == FILETYPE_ASN1):\n        req = _lib.d2i_X509_REQ_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if (req == _ffi.NULL):\n        _raise_current_error()\n    x509req = X509Req.__new__(X509Req)\n    x509req._req = _ffi.gc(req, _lib.X509_REQ_free)\n    return x509req\n", "label": 0}
{"function": "\n\ndef test_export_xls():\n    headers = ['x', 'y']\n    data = [['1', '2'], ['3', '4'], ['5,6', '7'], [None, None]]\n    sheet = ([headers] + data)\n    generator = create_generator(content_generator(headers, data), 'xls')\n    response = make_response(generator, 'xls', 'foo')\n    assert_equal('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', response['content-type'])\n    expected_data = [[(((cell is not None) and cell) or 'NULL') for cell in row] for row in sheet]\n    sheet_data = _read_xls_sheet_data(response)\n    assert_equal(expected_data, sheet_data)\n    assert_equal('attachment; filename=foo.xlsx', response['content-disposition'])\n", "label": 0}
{"function": "\n\ndef parse_novarc(filename):\n    opts = {\n        \n    }\n    f = open(filename, 'r')\n    for line in f:\n        try:\n            line = line.replace('export', '').strip()\n            parts = line.split('=')\n            if (len(parts) > 1):\n                value = parts[1].replace(\"'\", '')\n                value = value.replace('\"', '')\n                opts[parts[0]] = value\n        except:\n            pass\n    f.close()\n    return opts\n", "label": 0}
{"function": "\n\ndef __init__(self, hits=None, truncated=False):\n    if hits:\n        self._filenames = [x[0] for x in hits]\n        self._ranks = [x[1] for x in hits]\n    else:\n        self._filenames = []\n        self._ranks = []\n    self.truncated = truncated\n    self.debug_info = []\n", "label": 0}
{"function": "\n\ndef run(self, image, command=None, create_kwargs=None, start_kwargs=None):\n    '\\n        create container from provided image and start it\\n\\n        for more info, see documentation of REST API calls:\\n         * containers/{}/start\\n         * container/create\\n\\n        :param image: ImageName or string, name or id of the image\\n        :param command: str\\n        :param create_kwargs: dict, kwargs for docker.create_container\\n        :param start_kwargs: dict, kwargs for docker.start\\n        :return: str, container id\\n        '\n    logger.info(\"creating container from image '%s' and running it\", image)\n    create_kwargs = (create_kwargs or {\n        \n    })\n    start_kwargs = (start_kwargs or {\n        \n    })\n    logger.debug(\"image = '%s', command = '%s', create_kwargs = '%s', start_kwargs = '%s'\", image, command, create_kwargs, start_kwargs)\n    if isinstance(image, ImageName):\n        image = image.to_str()\n    container_dict = self.d.create_container(image, command=command, **create_kwargs)\n    container_id = container_dict['Id']\n    logger.debug(\"container_id = '%s'\", container_id)\n    self.d.start(container_id, **start_kwargs)\n    return container_id\n", "label": 0}
{"function": "\n\ndef one_time_from_two_time(two_time_corr):\n    \"\\n    This will provide the one-time correlation data from two-time\\n    correlation data.\\n\\n    Parameters\\n    ----------\\n    two_time_corr : array\\n        matrix of two time correlation\\n        shape (number of labels(ROI's), number of frames, number of frames)\\n\\n    Returns\\n    -------\\n    one_time_corr : array\\n        matrix of one time correlation\\n        shape (number of labels(ROI's), number of frames)\\n    \"\n    one_time_corr = np.zeros((two_time_corr.shape[0], two_time_corr.shape[2]))\n    for g in two_time_corr:\n        for j in range(two_time_corr.shape[2]):\n            one_time_corr[:, j] = (np.trace(g, offset=j) / two_time_corr.shape[2])\n    return one_time_corr\n", "label": 0}
{"function": "\n\ndef _walknode(self, node):\n    if (node.nodeType == Node.ELEMENT_NODE):\n        self.startElementNS(node.qname, node.tagName, node.attributes)\n        for c in node.childNodes:\n            self._walknode(c)\n        self.endElementNS(node.qname, node.tagName)\n    if ((node.nodeType == Node.TEXT_NODE) or (node.nodeType == Node.CDATA_SECTION_NODE)):\n        self.characters(str(node))\n", "label": 0}
{"function": "\n\ndef _extract_error_code(self, exception):\n    match = re.compile('^(\\\\d+)L?:|^\\\\((\\\\d+)L?,').match(str(exception))\n    code = ((match.group(1) or match.group(2)) if match else None)\n    if code:\n        return int(code)\n", "label": 0}
{"function": "\n\ndef host_passes(self, host_state, spec_obj):\n    instance_type = spec_obj.flavor\n    extra = (instance_type.extra_specs if ('extra_specs' in instance_type) else {\n        \n    })\n    trust = extra.get('trust:trusted_host')\n    host = host_state.nodename\n    if trust:\n        return self.compute_attestation.is_trusted(host, trust)\n    return True\n", "label": 0}
{"function": "\n\ndef post(self, *args, **kwargs):\n    if (self.request.POST.get('action') == 'invite'):\n        if (not self.domain_object.sms_mobile_worker_registration_enabled):\n            return self.get(*args, **kwargs)\n        if self.invitations_form.is_valid():\n            phone_numbers = self.invitations_form.cleaned_data.get('phone_numbers')\n            app_id = self.invitations_form.cleaned_data.get('app_id')\n            result = SelfRegistrationInvitation.initiate_workflow(self.domain, phone_numbers, app_id=app_id)\n            (success_numbers, invalid_format_numbers, numbers_in_use) = result\n            if success_numbers:\n                messages.success(self.request, (_('Invitations sent to: %(phone_numbers)s') % {\n                    'phone_numbers': ','.join(success_numbers),\n                }))\n            if invalid_format_numbers:\n                messages.error(self.request, (_('Invitations could not be sent to: %(phone_numbers)s. These number(s) are in an invalid format.') % {\n                    'phone_numbers': ','.join(invalid_format_numbers),\n                }))\n            if numbers_in_use:\n                messages.error(self.request, (_('Invitations could not be sent to: %(phone_numbers)s. These number(s) are already in use.') % {\n                    'phone_numbers': ','.join(numbers_in_use),\n                }))\n        return self.get(*args, **kwargs)\n    else:\n        if (not self.domain_object.sms_mobile_worker_registration_enabled):\n            raise Http404()\n        return self.paginate_crud_response\n", "label": 1}
{"function": "\n\ndef discover_affected_files(include_test_sources, include_scripts, project):\n    source_dir = project.get_property('dir_source_main_python')\n    files = discover_python_files(source_dir)\n    if include_test_sources:\n        if project.get_property('dir_source_unittest_python'):\n            unittest_dir = project.get_property('dir_source_unittest_python')\n            files = itertools.chain(files, discover_python_files(unittest_dir))\n        if project.get_property('dir_source_integrationtest_python'):\n            integrationtest_dir = project.get_property('dir_source_integrationtest_python')\n            files = itertools.chain(files, discover_python_files(integrationtest_dir))\n    if (include_scripts and project.get_property('dir_source_main_scripts')):\n        scripts_dir = project.get_property('dir_source_main_scripts')\n        files = itertools.chain(files, discover_files_matching(scripts_dir, '*'))\n    return files\n", "label": 1}
{"function": "\n\ndef _setMod(self, modname, mod):\n    haschanged = self._modules[modname].changed()\n    if haschanged:\n        if (type(mod) == File):\n            mod.update()\n        self._modules[modname].setValue(mod.getValue())\n        self._modules[modname].updateFingerprint()\n        self.untrust(modname)\n        dependents = self._getOutNodesRecursive(modname)\n        if (dependents != []):\n            dbgstr(('These nodes are dependent: ' + str(dependents)), 0)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, index):\n    record = self.resultset.__getitem__(index)\n    array = []\n    for field in self.field_names:\n        value = record\n        for key in field.split('.'):\n            if (key in value):\n                value = value[key]\n            else:\n                break\n        array.append(value)\n    return tuple(array)\n", "label": 0}
{"function": "\n\ndef set_editor_memento(self, memento):\n    (structure, editor_references) = memento\n    if (len(structure.contents) > 0):\n        handler = EditorSetStructureHandler(self, editor_references)\n        self._wx_editor_dock_window.set_structure(structure, handler)\n        for editor in self.window.editors:\n            control = self._wx_editor_dock_window.get_control(editor.id)\n            if (control is not None):\n                self._wx_initialize_editor_dock_control(editor, control)\n    return\n", "label": 0}
{"function": "\n\ndef _remove_client_present(self, client):\n    id_ = client.id_\n    if ((id_ is None) or (id_ not in self.present)):\n        return\n    clients = self.present[id_]\n    if (client not in clients):\n        return\n    clients.remove(client)\n    if (len(clients) == 0):\n        del self.present[id_]\n        if self.presence_events:\n            data = {\n                'new': [],\n                'lost': [id_],\n            }\n            self.event.fire_event(data, salt.utils.event.tagify('change', 'presence'))\n            data = {\n                'present': list(self.present.keys()),\n            }\n            self.event.fire_event(data, salt.utils.event.tagify('present', 'presence'))\n", "label": 1}
{"function": "\n\ndef mean_quadratic_weighted_kappa(kappas, weights=None):\n    \"\\n    Calculates the mean of the quadratic\\n    weighted kappas after applying Fisher's r-to-z transform, which is\\n    approximately a variance-stabilizing transformation.  This\\n    transformation is undefined if one of the kappas is 1.0, so all kappa\\n    values are capped in the range (-0.999, 0.999).  The reverse\\n    transformation is then applied before returning the result.\\n\\n    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\\n    kappa values\\n\\n    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\\n    of weights that is the same size as kappas.  Weights are applied in the\\n    z-space\\n    \"\n    kappas = np.array(kappas, dtype=float)\n    if (weights is None):\n        weights = np.ones(np.shape(kappas))\n    else:\n        weights = (weights / np.mean(weights))\n    kappas = np.array([min(x, 0.999) for x in kappas])\n    kappas = np.array([max(x, (- 0.999)) for x in kappas])\n    z = ((0.5 * np.log(((1 + kappas) / (1 - kappas)))) * weights)\n    z = np.mean(z)\n    return ((np.exp((2 * z)) - 1) / (np.exp((2 * z)) + 1))\n", "label": 0}
{"function": "\n\ndef get_latest_package(artifact, package_name):\n    if package_name:\n        package_list = Package.objects.filter(artifact=artifact, name=package_name).order_by('id').reverse()\n    else:\n        package_list = Package.objects.filter(artifact=artifact).order_by('id').reverse()\n    if (len(package_list) > 0):\n        return package_list[0]\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef test_simulate_shape():\n    P = [[0.4, 0.6], [0.2, 0.8]]\n    mcs = [MarkovChain(P), MarkovChain(sparse.csr_matrix(P))]\n    for mc in mcs:\n        (ts_length, init, num_reps) = (10, None, None)\n        assert_array_equal(mc.simulate(ts_length, init, num_reps).shape, (ts_length,))\n        (ts_length, init, num_reps) = (10, [0, 1], None)\n        assert_array_equal(mc.simulate(ts_length, init, num_reps).shape, (len(init), ts_length))\n        (ts_length, init, num_reps) = (10, [0, 1], 3)\n        assert_array_equal(mc.simulate(ts_length, init, num_reps).shape, ((len(init) * num_reps), ts_length))\n        for (ts_length, init, num_reps) in [(10, None, 3), (10, None, 1)]:\n            assert_array_equal(mc.simulate(ts_length, init, num_reps).shape, (num_reps, ts_length))\n", "label": 0}
{"function": "\n\ndef init(allocator=drv.mem_alloc):\n    '\\n    Initialize libraries used by scikit-cuda.\\n\\n    Initialize the CUBLAS, CUSOLVER, and CULA libraries used by \\n    high-level functions provided by scikit-cuda.\\n\\n    Parameters\\n    ----------\\n    allocator : an allocator used internally by some of the high-level\\n        functions.\\n\\n    Notes\\n    -----\\n    This function does not initialize PyCUDA; it uses whatever device\\n    and context were initialized in the current host thread.\\n    '\n    global _global_cublas_handle, _global_cublas_allocator\n    if (not _global_cublas_handle):\n        from . import cublas\n        _global_cublas_handle = cublas.cublasCreate()\n    if (_global_cublas_allocator is None):\n        _global_cublas_allocator = allocator\n    global _global_cusolver_handle\n    if (not _global_cusolver_handle):\n        from . import cusolver\n        _global_cusolver_handle = cusolver.cusolverDnCreate()\n    if _has_cula:\n        cula.culaInitialize()\n    if _has_magma:\n        magma.magma_init()\n", "label": 1}
{"function": "\n\ndef init_ipython_session(argv=[], auto_symbols=False, auto_int_to_Integer=False):\n    'Construct new IPython session. '\n    import IPython\n    if (V(IPython.__version__) >= '0.11'):\n        if (V(IPython.__version__) >= '1.0'):\n            from IPython.terminal import ipapp\n        else:\n            from IPython.frontend.terminal import ipapp\n        app = ipapp.TerminalIPythonApp()\n        app.display_banner = False\n        app.initialize(argv)\n        if auto_symbols:\n            readline = import_module('readline')\n            if readline:\n                enable_automatic_symbols(app)\n        if auto_int_to_Integer:\n            enable_automatic_int_sympification(app)\n        return app.shell\n    else:\n        from IPython.Shell import make_IPython\n        return make_IPython(argv)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef dict_to_str(dictionary):\n    resstr = ''\n    new_dict = {\n        \n    }\n    for (key, val) in dictionary.items():\n        if (key == 0):\n            key = 'annotation'\n        new_dict[key] = val\n    for key in sorted(new_dict.keys()):\n        resstr += ('%s=%s; ' % (key, str(new_dict[key])))\n    return resstr[:(- 2)]\n", "label": 0}
{"function": "\n\ndef filter_tests(self, status):\n    'Filter results by given status.'\n    filtered_tests = {\n        \n    }\n    for test in self.tests:\n        if (self.tests[test]['status'] == status):\n            filtered_tests[test] = self.tests[test]\n    return filtered_tests\n", "label": 0}
{"function": "\n\ndef resurrect(self, auto=True):\n    unregister(self._bind_pulse)\n    del self._bind_pulse\n    res_room = None\n    if self.touchstone:\n        res_room = load_object(self.touchstone, Room)\n    if (not res_room):\n        res_room = load_object(default_start_room, Room)\n    self.change_env(res_room)\n    self.display_line('With a sick feeling, you return to consciousness')\n    self.status = 'ok'\n    self.heath = 1\n    self.start_refresh()\n", "label": 0}
{"function": "\n\ndef _CheckDirty(self, reload_datastore_artifacts=False):\n    if self._dirty:\n        self._dirty = False\n        self._ReloadArtifacts()\n    elif reload_datastore_artifacts:\n        self.ReloadDatastoreArtifacts()\n", "label": 0}
{"function": "\n\ndef test_enabled(self):\n    SilkyConfig().SILKY_META = True\n    r = self._execute_request()\n    self.assertTrue(((r.meta_time is not None) or (r.meta_num_queries is not None) or (r.meta_time_spent_queries is not None)))\n", "label": 0}
{"function": "\n\ndef _media(self):\n    if ('collapse' in self.classes):\n        extra = ('' if settings.DEBUG else '.min')\n        js = [('jquery%s.js' % extra), 'jquery.init.js', ('collapse%s.js' % extra)]\n        return forms.Media(js=[static(('admin/js/%s' % url)) for url in js])\n    return forms.Media()\n", "label": 0}
{"function": "\n\ndef _add_xtra_pore_data(self):\n    xpdata = self._xtra_pore_data\n    if (xpdata is not None):\n        if isinstance(xpdata, type([])):\n            for pdata in xpdata:\n                try:\n                    self[('pore.' + pdata)] = self._dictionary[('p' + pdata)][self._pore_map]\n                except:\n                    logger.warning((('Could not add pore data: ' + pdata) + ' to network'))\n                    pass\n        else:\n            try:\n                self[('pore.' + xpdata)] = self._dictionary[('p' + xpdata)][self._pore_map]\n            except:\n                logger.warning((('Could not add pore data: ' + xpdata) + ' to network'))\n                pass\n", "label": 1}
{"function": "\n\n@property\ndef size(self):\n    size = ((2 + 2) + 2)\n    for header in self.headers:\n        size += header.size\n    for message in self.messages:\n        size += message.size\n    return size\n", "label": 0}
{"function": "\n\ndef _get(self, instance):\n    r = instance.belongs_to_many(self._related, self._table, self._foreign_key, self._other_key, self._relation, _wrapped=False)\n    if self._timestamps:\n        r = r.with_timestamps()\n    if self._pivot:\n        r = r.with_pivot(*self._pivot)\n    return r\n", "label": 0}
{"function": "\n\ndef addWMOStationDataTests(inputTestList):\n    if (Configuration.DEBUG == True):\n        print('      MilitaryAspectsOfWeatherTestSuite.addWMOStationDataTests')\n    from . import ImportWMOStationDataTestCase\n    for test in inputTestList:\n        print(('adding test: ' + str(test)))\n        Configuration.Logger.info(test)\n        TestSuite.addTest(ImportWMOStationDataTestCase.ImportWMOStationDataTestCase(test))\n", "label": 0}
{"function": "\n\ndef on_done(self, index):\n    if (index == (- 1)):\n        return\n    for item in self.items_property[self.items[index]]:\n        self.window.run_command(self.callback_command, {\n            'options': item,\n        })\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.version == other.version) and (self.vrid == other.vrid) and (self.priority == other.priority) and (self.ip_addresses == other.ip_addresses) and (self.advertisement_interval == other.advertisement_interval) and (self.preempt_mode == other.preempt_mode) and (self.preempt_delay == other.preempt_delay) and (self.accept_mode == other.accept_mode) and (self.is_ipv6 == other.is_ipv6))\n", "label": 1}
{"function": "\n\n@no_oracle\ndef test05_extent(self):\n    'Testing the `extent` GeoQuerySet method.'\n    if DISABLE:\n        return\n    expected = ((- 96.8016128540039), 29.7633724212646, (- 95.3631439208984), 32.78205871582)\n    qs = City.objects.filter(name__in=('Houston', 'Dallas'))\n    extent = qs.extent()\n    for (val, exp) in zip(extent, expected):\n        self.assertAlmostEqual(exp, val, 8)\n", "label": 0}
{"function": "\n\ndef mark_invalid_input_sequences(self):\n    \"Fill in domain knowledge about valid input\\n    sequences (e.g. don't prune failure without pruning recovery.)\\n    Only do so if this isn't a view of a previously computed DAG\"\n    fingerprint2previousfailure = {\n        \n    }\n    for event in self._events_list:\n        if hasattr(event, 'fingerprint'):\n            fingerprint = event.fingerprint[1:]\n            if (type(event) in self._failure_types):\n                fingerprint2previousfailure[fingerprint] = event\n            elif (type(event) in self._recovery_types):\n                if (fingerprint in fingerprint2previousfailure):\n                    failure = fingerprint2previousfailure[fingerprint]\n                    failure.dependent_labels.append(event.label)\n", "label": 1}
{"function": "\n\ndef pillar_format(ret, keys, value):\n    '\\n    Perform data formatting to be used as pillar data and\\n    merge it with the current pillar data\\n    '\n    if (value is None):\n        return ret\n    pillar_value = yaml.load(value)\n    keyvalue = keys.pop()\n    pil = {\n        keyvalue: pillar_value,\n    }\n    keys.reverse()\n    for k in keys:\n        pil = {\n            k: pil,\n        }\n    return dict_merge(ret, pil)\n", "label": 0}
{"function": "\n\ndef uncommented_lines(filename, use_sudo=False):\n    '\\n    Get the lines of a remote file, ignoring empty or commented ones\\n    '\n    func = (run_as_root if use_sudo else run)\n    res = func(('cat %s' % quote(filename)), quiet=True)\n    if res.succeeded:\n        return [line for line in res.splitlines() if (line and (not line.startswith('#')))]\n    else:\n        return []\n", "label": 1}
{"function": "\n\ndef __init__(self, amr_grid, quantity):\n    self.viewed_quantity = quantity\n    AMRGrid.__init__(self)\n    for level_ref in amr_grid.levels:\n        level = self.add_level()\n        for grid_ref in level_ref.grids:\n            grid = level.add_grid()\n            grid.nx = grid_ref.nx\n            grid.ny = grid_ref.ny\n            grid.nz = grid_ref.nz\n            (grid.xmin, grid.xmax) = (grid_ref.xmin, grid_ref.xmax)\n            (grid.ymin, grid.ymax) = (grid_ref.ymin, grid_ref.ymax)\n            (grid.zmin, grid.zmax) = (grid_ref.zmin, grid_ref.zmax)\n            grid.quantities = {\n                \n            }\n            grid.quantities[quantity] = grid_ref.quantities[quantity]\n", "label": 0}
{"function": "\n\ndef test_get(self):\n    entries = self.ts.entries()\n    self.assertIsNotNone(entries)\n    for entry in entries:\n        self.assertIsNotNone(entry.id)\n        self.assertIsNotNone(entry.spent_at)\n        self.assertIsNotNone(entry.user_id)\n        self.assertIsNotNone(entry.client_name)\n        self.assertIsNotNone(entry.project_id)\n        self.assertIsNotNone(entry.project_name)\n        self.assertIsNotNone(entry.task_id)\n        self.assertIsNotNone(entry.task_name)\n        self.assertIsNotNone(entry.hours)\n        self.assertIsNotNone(entry.notes)\n        if entry.started:\n            self.assertIsNotNone(entry.timer_started)\n            self.assertIsNotNone(entry.timer_created)\n            self.assertIsNotNone(entry.timer_updated)\n", "label": 0}
{"function": "\n\ndef _walk(self, expr):\n    node = expr.op()\n    if isinstance(node, ops.TableColumn):\n        is_valid = self._validate_column(expr)\n        self.valid = (self.valid and is_valid)\n    for arg in node.flat_args():\n        if isinstance(arg, ir.ValueExpr):\n            self._walk(arg)\n", "label": 0}
{"function": "\n\n@property\ndef rows(self):\n    '\\n        Return all cells in range by column\\n        '\n    for row in range(self.min_row, (self.max_row + 1)):\n        (yield tuple((('%s%d' % (get_column_letter(col), row)) for col in range(self.min_col, (self.max_col + 1)))))\n", "label": 0}
{"function": "\n\n@property\ndef line_print(self):\n    inv_types = {v: k for (k, v) in defines.Types.iteritems()}\n    if (self._code is None):\n        self._code = defines.Codes.EMPTY.number\n    msg = 'From {source}, To {destination}, {type}-{mid}, {code}-{token}, ['.format(source=self._source, destination=self._destination, type=inv_types[self._type], mid=self._mid, code=defines.Codes.LIST[self._code].name, token=self._token)\n    for opt in self._options:\n        msg += '{name}: {value}, '.format(name=opt.name, value=opt.value)\n    msg += ']'\n    if (self.payload is not None):\n        msg += ' {payload}...{length} bytes'.format(payload=self.payload[0:20], length=len(self.payload))\n    else:\n        msg += ' No payload'\n    return msg\n", "label": 0}
{"function": "\n\ndef isPowerOfFour(self, num):\n    '\\n        :type num: int\\n        :rtype: bool\\n        '\n    return ((num > 0) and ((num & (num - 1)) == 0) and ((num & 1431655765) == num))\n", "label": 0}
{"function": "\n\ndef test_delete_cards(self):\n    self._add_card('card to be deleted')\n    cards = self._board.open_cards()\n    nb_open_cards = len(cards)\n    for card in cards:\n        card.delete()\n    self._board.fetch_actions(action_filter='all', action_limit=nb_open_cards)\n    self.assertEquals(len(self._board.actions), nb_open_cards)\n    for action in self._board.actions:\n        self.assertEqual(action['type'], 'deleteCard')\n", "label": 0}
{"function": "\n\ndef _fire_listeners(self, node, lint_context):\n    node_type = NodeType(node['type'])\n    if (node_type not in self._listeners_map):\n        return\n    listening_policies = self._listeners_map[node_type]\n    for listening_policy in listening_policies:\n        violation = listening_policy.get_violation_if_found(node, lint_context)\n        if (violation is not None):\n            self._violations.append(violation)\n", "label": 0}
{"function": "\n\ndef register(self, fid, event):\n    if event:\n        if (event & _AsyncPoller._Read):\n            self.rset.add(fid)\n        if (event & _AsyncPoller._Write):\n            self.wset.add(fid)\n        if (event & _AsyncPoller._Error):\n            self.xset.add(fid)\n", "label": 0}
{"function": "\n\ndef _insert_index(self, data):\n    data = data.copy()\n    idx_nlevels = data.index.nlevels\n    if (idx_nlevels == 1):\n        data.insert(0, 'Index', data.index)\n    else:\n        for i in range(idx_nlevels):\n            data.insert(i, 'Index{0}'.format(i), data.index.get_level_values(i))\n    col_nlevels = data.columns.nlevels\n    if (col_nlevels > 1):\n        col = data.columns.get_level_values(0)\n        values = [data.columns.get_level_values(i).values for i in range(1, col_nlevels)]\n        col_df = pd.DataFrame(values)\n        data.columns = col_df.columns\n        data = pd.concat([col_df, data])\n        data.columns = col\n    return data\n", "label": 0}
{"function": "\n\ndef login_user(self, email, password, response):\n    \" Checks to see if the user has entered in a valid email and password,\\n    logging the user in if they have.\\n\\n    Args:\\n      email: A str containing the e-mail address of the user to login.\\n      password: A str containing the cleartext password of the user to login.\\n      response: A webapp2 response that the new user's logged in cookie\\n        should be set in.\\n    Return:\\n      True if the user logged in successfully, and False otherwise.\\n    \"\n    user_data = self.query_user_data(email)\n    server_re = re.search(self.USER_DATA_PASSWORD_REGEX, user_data)\n    if (not server_re):\n        logging.error('Failed Login: {0} regex failed'.format(email))\n        return False\n    server_pwd = server_re.group(1)\n    encrypted_pass = LocalState.encrypt_password(email, password)\n    if (server_pwd != encrypted_pass):\n        logging.info('Failed Login: {0} password mismatch'.format(email))\n        return False\n    self.create_token(email, email)\n    self.set_appserver_cookie(email, self.get_user_app_list(email), response)\n    return True\n", "label": 0}
{"function": "\n\ndef handle(self, request, data):\n    project = self._update_project(request, data)\n    if (not project):\n        return False\n    project_id = data['project_id']\n    domain_id = getattr(project, 'domain_id', '')\n    ret = self._update_project_members(request, data, project_id)\n    if (not ret):\n        return False\n    if PROJECT_GROUP_ENABLED:\n        ret = self._update_project_groups(request, data, project_id, domain_id)\n        if (not ret):\n            return False\n    if api.keystone.is_cloud_admin(request):\n        ret = self._update_project_quota(request, data, project_id)\n        if (not ret):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef run_command(jboss_config, command, fail_on_error=True):\n    '\\n    Execute a command against jboss instance through the CLI interface.\\n\\n    jboss_config\\n           Configuration dictionary with properties specified above.\\n    command\\n           Command to execute against jboss instance\\n    fail_on_error (default=True)\\n           Is true, raise CommandExecutionException exception if execution fails.\\n           If false, \\'success\\' property of the returned dictionary is set to False\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt \\'*\\' jboss7_cli.run_command \\'{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}\\' my_command\\n    '\n    cli_command_result = __call_cli(jboss_config, command)\n    if (cli_command_result['retcode'] == 0):\n        cli_command_result['success'] = True\n    elif fail_on_error:\n        raise CommandExecutionError(\"Command execution failed, return code={retcode}, stdout='{stdout}', stderr='{stderr}' \".format(**cli_command_result))\n    else:\n        cli_command_result['success'] = False\n    return cli_command_result\n", "label": 0}
{"function": "\n\ndef Dictionary(self, *args):\n    if (not args):\n        return self._dict\n    dlist = [self._dict[x] for x in args]\n    if (len(dlist) == 1):\n        dlist = dlist[0]\n    return dlist\n", "label": 0}
{"function": "\n\ndef disambiguate(records):\n    choices = [itemgetter(0), itemgetter(0, 1), itemgetter(0, 1, 3)]\n    for choice in choices:\n        result = list(map(choice, records))\n        if is_unique_list(result):\n            return result\n    return records\n", "label": 0}
{"function": "\n\ndef getFieldExtractorForReadMessage(self, jsonMessage, objectToRead):\n    if ('field' not in jsonMessage):\n        raise MalformedMessageException(\"missing 'field'\")\n    field = jsonMessage['field']\n    if (not isinstance(field, str)):\n        raise MalformedMessageException('fieldname not a string')\n    field = intern(field)\n    try:\n        fieldDef = getattr(getObjectClass(objectToRead), field)\n    except:\n        raise InvalidFieldException()\n    if (not Decorators.isPropertyToExpose(fieldDef)):\n        raise InvalidFieldException()\n\n    def getter(x):\n        return getattr(x, field)\n    return getter\n", "label": 0}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.add_application_key(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            self.set_tag(d.getPrefixedString())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 0}
{"function": "\n\ndef match_alphabet(self, pattern):\n    'Initialise the alphabet for the Bitap algorithm.\\n\\n    Args:\\n      pattern: The text to encode.\\n\\n    Returns:\\n      Hash of character locations.\\n    '\n    s = {\n        \n    }\n    for char in pattern:\n        s[char] = 0\n    for i in xrange(len(pattern)):\n        s[pattern[i]] |= (1 << ((len(pattern) - i) - 1))\n    return s\n", "label": 0}
{"function": "\n\ndef on_request_success(request_type, name, response_time, response_length):\n    if ((global_stats.max_requests is not None) and ((global_stats.num_requests + global_stats.num_failures) >= global_stats.max_requests)):\n        raise StopLocust('Maximum number of requests reached')\n    global_stats.get(name, request_type).log(response_time, response_length)\n", "label": 0}
{"function": "\n\ndef field_count(self):\n    num_of_fields = 0\n    if self.has_auto_field():\n        num_of_fields += 1\n    num_of_fields += len(self.fieldsets[0][1]['fields'])\n    if self.formset.can_order:\n        num_of_fields += 1\n    if self.formset.can_delete:\n        num_of_fields += 1\n    return num_of_fields\n", "label": 0}
{"function": "\n\ndef _layout_state(self, state):\n    ' Layout the dock panes in the specified TaskState using its\\n            TaskLayout.\\n        '\n    for (name, corner) in CORNER_MAP.iteritems():\n        area = getattr(state.layout, (name + '_corner'))\n        self.control.setCorner(corner, AREA_MAP[area])\n    self._main_window_layout.state = state\n    self._main_window_layout.set_layout(state.layout)\n    for dock_pane in state.dock_panes:\n        if (dock_pane.control not in self._main_window_layout.consumed):\n            self.control.addDockWidget(AREA_MAP[dock_pane.dock_area], dock_pane.control)\n            if dock_pane.visible:\n                dock_pane.control.show()\n", "label": 0}
{"function": "\n\ndef _parse_metadata(context, repos, record):\n    'parse metadata formats'\n    if isinstance(record, str):\n        exml = etree.fromstring(record, context.parser)\n    elif hasattr(record, 'getroot'):\n        exml = record.getroot()\n    else:\n        exml = record\n    root = exml.tag\n    LOGGER.debug('Serialized metadata, parsing content model')\n    if (root == ('{%s}MD_Metadata' % context.namespaces['gmd'])):\n        return [_parse_iso(context, repos, exml)]\n    elif (root == '{http://www.isotc211.org/2005/gmi}MI_Metadata'):\n        return [_parse_iso(context, repos, exml)]\n    elif (root == 'metadata'):\n        return [_parse_fgdc(context, repos, exml)]\n    elif (root == ('{%s}TRANSFER' % context.namespaces['gm03'])):\n        return [_parse_gm03(context, repos, exml)]\n    elif (root == ('{%s}Record' % context.namespaces['csw'])):\n        return [_parse_dc(context, repos, exml)]\n    elif (root == ('{%s}RDF' % context.namespaces['rdf'])):\n        return [_parse_dc(context, repos, exml)]\n    elif (root == ('{%s}DIF' % context.namespaces['dif'])):\n        pass\n    else:\n        raise RuntimeError('Unsupported metadata format')\n", "label": 1}
{"function": "\n\ndef watcher(self, zh, event, state, path):\n    'Handle zookeeper.aget() watches.\\n\\n    This code is called when a znode changes and triggers a data watch.\\n    It is not called to handle the zookeeper.aget call itself.\\n\\n    Numeric arguments map to constants. See ``DATA`` in ``help(zookeeper)``\\n    for more information.\\n\\n    Args:\\n      zh Zookeeper handle that set this watch.\\n      event Event that caused the watch (often called ``type`` elsewhere).\\n      state Connection state.\\n      path Znode that triggered this watch.\\n\\n    Does not provide a return value.\\n    '\n    out = ['Running watcher:', ('zh=%d' % zh), ('event=%d' % event), ('state=%d' % state), ('path=%s' % path)]\n    logger.debug(' '.join(out))\n    if ((event == zookeeper.CHANGED_EVENT) and (state == zookeeper.CONNECTED_STATE) and (self.znode == path)):\n        if (zookeeper.OK != self.aget()):\n            logger.critical('Unable to get znode! Exiting.')\n            sys.exit(1)\n", "label": 0}
{"function": "\n\ndef request_is_managed_by_flask_classy(self):\n    if (request.endpoint is None):\n        return False\n    if (':' not in request.endpoint):\n        return False\n    (class_name, action) = request.endpoint.split(':')\n    return (any(((class_name == classy_class.__name__) for classy_class in self.flask_classy_classes)) and (action in self.special_methods))\n", "label": 0}
{"function": "\n\ndef get_rule_match(self, gram_handler):\n    for grammar in gram_handler.get_matching_grammars():\n        for rule in grammar._rules:\n            rule_match = matching.get_rule_match(rule, self.remaining_words, grammar.settings['filtered words'])\n            if (rule_match is not None):\n                return rule_match\n", "label": 0}
{"function": "\n\ndef __str__(self, prefix='', printElemNumber=0):\n    res = ''\n    if self.has_is_available_:\n        res += (prefix + ('is_available: %s\\n' % self.DebugFormatBool(self.is_available_)))\n    if self.has_presence_:\n        res += (prefix + ('presence: %s\\n' % self.DebugFormatInt32(self.presence_)))\n    if self.has_valid_:\n        res += (prefix + ('valid: %s\\n' % self.DebugFormatBool(self.valid_)))\n    return res\n", "label": 0}
{"function": "\n\ndef remove(self, entity):\n    '\\n        Remove a subecriber from the channel.\\n\\n        Args:\\n            entity (Player, Object or list): The entity or\\n                entities to un-subscribe from the channel.\\n\\n        '\n    for subscriber in make_iter(entity):\n        if subscriber:\n            clsname = subscriber.__dbclass__.__name__\n            if (clsname == 'PlayerDB'):\n                self.obj.db_subscriptions.remove(entity)\n            elif (clsname == 'ObjectDB'):\n                self.obj.db_object_subscriptions.remove(entity)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (not other):\n        return False\n    return ((self.x_coord == other.x_coord) and (self.y_coord == other.y_coord) and (self.z_coord == other.z_coord))\n", "label": 0}
{"function": "\n\ndef beacon(config):\n    '\\n    Emit the status of a connected display to the minion\\n\\n    Mainly this is used to detect when the display fails to connect for whatever reason.\\n\\n    .. code-block:: yaml\\n\\n        beacons:\\n          glxinfo:\\n            user: frank\\n            screen_event: True\\n\\n    '\n    log.trace('glxinfo beacon starting')\n    ret = []\n    _validate = validate(config)\n    if (not _validate[0]):\n        return ret\n    retcode = __salt__['cmd.retcode']('DISPLAY=:0 glxinfo', runas=config['user'], python_shell=True)\n    if (('screen_event' in config) and config['screen_event']):\n        last_value = last_state.get('screen_available', False)\n        screen_available = (retcode == 0)\n        if ((last_value != screen_available) or ('screen_available' not in last_state)):\n            ret.append({\n                'tag': 'screen_event',\n                'screen_available': screen_available,\n            })\n        last_state['screen_available'] = screen_available\n    return ret\n", "label": 1}
{"function": "\n\ndef rescale_parent_proportion(self, *args):\n    if (not self.parent):\n        return\n    if self.rescale_with_parent:\n        parent_proportion = self._parent_proportion\n        if (self.sizable_from in ('top', 'bottom')):\n            new_height = (parent_proportion * self.parent.height)\n            self.height = max(self.min_size, min(new_height, self.max_size))\n        else:\n            new_width = (parent_proportion * self.parent.width)\n            self.width = max(self.min_size, min(new_width, self.max_size))\n", "label": 0}
{"function": "\n\ndef make_command(command, env=None, sudo=False, sudo_user=None):\n    '\\n    Builds a shell command with various kwargs.\\n    '\n    if env:\n        env_string = ' '.join(['{0}={1}'.format(key, value) for (key, value) in six.iteritems(env)])\n        command = '{0} {1}'.format(env_string, command)\n    command = command.replace(\"'\", \"\\\\'\")\n    if (not sudo):\n        command = \"sh -c '{0}'\".format(command)\n    elif sudo_user:\n        command = \"sudo -H -u {0} -S sh -c '{1}'\".format(sudo_user, command)\n    else:\n        command = \"sudo -H -S sh -c '{0}'\".format(command)\n    return command\n", "label": 0}
{"function": "\n\ndef __init__(self, qtile):\n    self.groups = []\n    self.screens = {\n        \n    }\n    self.current_screen = 0\n    for group in qtile.groups:\n        self.groups.append((group.name, group.layout.name))\n    for (index, screen) in enumerate(qtile.screens):\n        self.screens[index] = screen.group.name\n        if (screen == qtile.currentScreen):\n            self.current_screen = index\n", "label": 0}
{"function": "\n\ndef __init__(self, parent=None, win=None, xrefs=None, headers=None):\n    super(XrefValueWindow, self).__init__(parent)\n    self.parent = parent\n    self.mainwin = win\n    self.xrefs = xrefs\n    self.headers = headers\n    self.reverse_strings = {\n        \n    }\n    self.proxyModel = QtGui.QSortFilterProxyModel()\n    self.proxyModel.setDynamicSortFilter(True)\n    self.model = QtGui.QStandardItemModel(len(self.xrefs), len(self.headers), self)\n    column = 0\n    for header in headers:\n        self.model.setHeaderData(column, QtCore.Qt.Horizontal, header)\n        column += 1\n    row = 0\n    for ref in xrefs:\n        for column in range(len(self.headers)):\n            self.model.setData(self.model.index(row, column, QtCore.QModelIndex()), ('%s' % ref[column]))\n        row += 1\n    self.proxyModel.setSourceModel(self.model)\n    self.setRootIsDecorated(False)\n    self.setAlternatingRowColors(True)\n    self.setModel(self.proxyModel)\n    self.setSortingEnabled(True)\n    self.setEditTriggers(QtGui.QAbstractItemView.NoEditTriggers)\n    self.doubleClicked.connect(self.slotDoubleClicked)\n", "label": 0}
{"function": "\n\ndef _convert_to_python(self, value_dict, state):\n    is_empty = self.field_is_empty\n    if ((self.field in value_dict) and (value_dict.get(self.field) == self.expected_value)):\n        for required_field in self.required_fields:\n            if ((required_field not in value_dict) or is_empty(value_dict.get(required_field))):\n                raise Invalid((_('You must give a value for %s') % required_field), value_dict, state, error_dict={\n                    required_field: Invalid(self.message('empty', state), value_dict.get(required_field), state),\n                })\n    return value_dict\n", "label": 1}
{"function": "\n\ndef wpc_channel(url):\n    query = urlparse(url)\n    path_elements = query.path.strip('/').split('/')\n    if ((query.hostname in ('www.watchpeoplecode.com', 'watchpeoplecode.com')) and (len(path_elements) == 2) and (path_elements[0] == 'streamer')):\n        return path_elements[1]\n", "label": 0}
{"function": "\n\ndef _get_functions(self):\n    'Create a mapping from commands to command functions.'\n    functions = {\n        \n    }\n    for each in dir(self):\n        if (not each.startswith('cmd_')):\n            continue\n        func = getattr(self, each)\n        for cmd in getattr(self, ('commands_' + each[4:]), '').split():\n            functions[cmd] = func\n    return functions\n", "label": 0}
{"function": "\n\ndef __lt__(self, other):\n    if (self._version != other._version):\n        raise TypeError(('%s and %s are not of the same version' % (self, other)))\n    if (not isinstance(other, _BaseAddress)):\n        raise TypeError(('%s and %s are not of the same type' % (self, other)))\n    if (self._ip != other._ip):\n        return (self._ip < other._ip)\n    return False\n", "label": 0}
{"function": "\n\ndef build_5(self):\n    'level 5'\n    pig = Pig(900, 70, self.space)\n    self.pigs.append(pig)\n    pig = Pig(1000, 152, self.space)\n    self.pigs.append(pig)\n    for i in range(9):\n        p = (800, (70 + (i * 21)))\n        self.beams.append(Polygon(p, 85, 20, self.space))\n    for i in range(4):\n        p = (1000, (70 + (i * 21)))\n        self.beams.append(Polygon(p, 85, 20, self.space))\n    p = (970, 176)\n    self.columns.append(Polygon(p, 20, 85, self.space))\n    p = (1026, 176)\n    self.columns.append(Polygon(p, 20, 85, self.space))\n    p = (1000, 230)\n    self.beams.append(Polygon(p, 85, 20, self.space))\n    self.number_of_birds = 4\n    if self.bool_space:\n        self.number_of_birds = 8\n", "label": 0}
{"function": "\n\ndef blockCombine(l):\n    ' Produce a matrix from a list of lists of its components. '\n    l = [list(map(mat, row)) for row in l]\n    hdims = [m.shape[1] for m in l[0]]\n    hs = sum(hdims)\n    vdims = [row[0].shape[0] for row in l]\n    vs = sum(vdims)\n    res = zeros((hs, vs))\n    vindex = 0\n    for (i, row) in enumerate(l):\n        hindex = 0\n        for (j, m) in enumerate(row):\n            res[vindex:(vindex + vdims[i]), hindex:(hindex + hdims[j])] = m\n            hindex += hdims[j]\n        vindex += vdims[i]\n    return res\n", "label": 1}
{"function": "\n\n@classmethod\ndef handle_token(cls, parser, token):\n    tokens = token.split_contents()\n    if (len(tokens) == 1):\n        return cls(obj='user-notification', target='page')\n    if (len(tokens) > 3):\n        raise template.TemplateSyntaxError(_('Max arguments are two'))\n    elif (tokens[1] != 'for'):\n        raise template.TemplateSyntaxError(_(\"First argument must be 'for'\"))\n    elif (not tokens[2]):\n        raise template.TemplateSyntaxError(_(\"Second argument must either 'box' or 'page'\"))\n    else:\n        return cls(obj='user-notification', target=tokens[2])\n", "label": 0}
{"function": "\n\ndef filter_song_md(song, md_list=['id'], no_singletons=True):\n    'Returns a list of desired metadata from a song.\\n    Does not modify the given song.\\n\\n    :param song: Dictionary representing a GM song.\\n    :param md_list: (optional) the ordered list of metadata to select.\\n    :param no_singletons: (optional) if md_list is of length 1, return the data,\\n      not a singleton list.\\n    '\n    filtered = [song[md_type] for md_type in md_list]\n    if ((len(md_list) == 1) and no_singletons):\n        return filtered[0]\n    else:\n        return filtered\n", "label": 0}
{"function": "\n\n@classmethod\ndef from_obj(cls, obj, return_obj=None, contained_type=None, binding_var=None):\n    if (not obj):\n        return None\n    if (return_obj is None):\n        return_obj = cls()\n    if (not contained_type):\n        contained_type = cls._contained_type\n    if (not binding_var):\n        binding_var = cls._binding_var\n    for item in getattr(obj, binding_var):\n        return_obj.append(contained_type.from_obj(item))\n    return return_obj\n", "label": 1}
{"function": "\n\n@view_config(context=APIPackagingResource, request_method='GET', subpath=(), renderer='json')\n@addslash\n@argify\ndef all_packages(request, verbose=False):\n    ' List all packages '\n    if verbose:\n        packages = request.db.summary()\n    else:\n        packages = request.db.distinct()\n    i = 0\n    while (i < len(packages)):\n        package = packages[i]\n        name = (package if isinstance(package, basestring) else package['name'])\n        if (not request.access.has_permission(name, 'read')):\n            del packages[i]\n            continue\n        i += 1\n    return {\n        'packages': packages,\n    }\n", "label": 0}
{"function": "\n\ndef get_artist(self, artists):\n    'Returns an artist string (all artists) and an artist_id (the main\\n        artist) for a list of discogs album or track artists.\\n        '\n    artist_id = None\n    bits = []\n    for (i, artist) in enumerate(artists):\n        if (not artist_id):\n            artist_id = artist['id']\n        name = artist['name']\n        name = re.sub(' \\\\(\\\\d+\\\\)$', '', name)\n        name = re.sub('(?i)^(.*?), (a|an|the)$', '\\\\2 \\\\1', name)\n        bits.append(name)\n        if (artist['join'] and (i < (len(artists) - 1))):\n            bits.append(artist['join'])\n    artist = (' '.join(bits).replace(' ,', ',') or None)\n    return (artist, artist_id)\n", "label": 1}
{"function": "\n\ndef fromhg(src, dst):\n    if (src.startswith('-') or dst.startswith('-')):\n        raise ValueError('Bad src or dst')\n    proc = subprocess.Popen(['hg', 'clone', src, dst], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    output = proc.communicate()[0]\n    ret = proc.wait()\n    if ret:\n        raise CopyError('Could not copy. Mercurial returned {0}'.format(output))\n", "label": 0}
{"function": "\n\ndef spj(path, max_cpu_time, max_memory, in_path, user_out_path):\n    if (file_exists(in_path) and file_exists(user_out_path)):\n        result = judger.run(path=path, in_file=in_path, out_file='/tmp/spj.out', max_cpu_time=max_cpu_time, max_memory=max_memory, args=[in_path, user_out_path], env=[('PATH=' + os.environ.get('PATH', ''))], use_sandbox=True, use_nobody=True)\n        if ((result['signal'] == 0) and (result['exit_status'] in [AC, WA, SPJ_ERROR])):\n            result['spj_result'] = result['exit_status']\n        else:\n            result['spj_result'] = SPJ_ERROR\n        return result\n    else:\n        raise ValueError('in_path or user_out_path does not exist')\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _translate_msgid(msgid, domain, desired_locale=None):\n    if (not desired_locale):\n        system_locale = locale.getdefaultlocale()\n        if (not system_locale[0]):\n            desired_locale = 'en_US'\n        else:\n            desired_locale = system_locale[0]\n    locale_dir = os.environ.get((domain.upper() + '_LOCALEDIR'))\n    lang = gettext.translation(domain, localedir=locale_dir, languages=[desired_locale], fallback=True)\n    if six.PY3:\n        translator = lang.gettext\n    else:\n        translator = lang.ugettext\n    translated_message = translator(msgid)\n    return translated_message\n", "label": 0}
{"function": "\n\ndef calculate_debounced_passing(recent_results, debounce=0):\n    '\\n    `debounce` is the number of previous failures we need (not including this)\\n    to mark a search as passing or failing\\n    Returns:\\n      True if passing given debounce factor\\n      False if failing\\n    '\n    if (not recent_results):\n        return True\n    debounce_window = recent_results[:(debounce + 1)]\n    for r in debounce_window:\n        if r.succeeded:\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef make_revision_with_plugins(obj, user=None, message=None):\n    '\\n    Only add to revision if it is a draft.\\n    '\n    from cms.models.pluginmodel import CMSPlugin\n    from cms.utils.reversion_hacks import revision_context, revision_manager\n    cls = obj.__class__\n    if hasattr(revision_manager, '_registration_key_for_model'):\n        model_key = revision_manager._registration_key_for_model(cls)\n    else:\n        model_key = cls\n    if (model_key in revision_manager._registered_models):\n        placeholder_relation = find_placeholder_relation(obj)\n        if revision_context.is_active():\n            if user:\n                revision_context.set_user(user)\n            if message:\n                revision_context.set_comment(message)\n            adapter = revision_manager.get_adapter(obj.__class__)\n            revision_context.add_to_context(revision_manager, obj, adapter.get_version_data(obj))\n            for ph in obj.get_placeholders():\n                phadapter = revision_manager.get_adapter(ph.__class__)\n                revision_context.add_to_context(revision_manager, ph, phadapter.get_version_data(ph))\n            filters = {\n                ('placeholder__%s' % placeholder_relation): obj,\n            }\n            for plugin in CMSPlugin.objects.filter(**filters):\n                (plugin_instance, admin) = plugin.get_plugin_instance()\n                if plugin_instance:\n                    padapter = revision_manager.get_adapter(plugin_instance.__class__)\n                    revision_context.add_to_context(revision_manager, plugin_instance, padapter.get_version_data(plugin_instance))\n                bpadapter = revision_manager.get_adapter(plugin.__class__)\n                revision_context.add_to_context(revision_manager, plugin, bpadapter.get_version_data(plugin))\n", "label": 1}
{"function": "\n\ndef _assert_get_rules_by_name(self, name, expected, immediate=None):\n    if (immediate is None):\n        rules = get_rules_by_name(name, self.rules_dir)\n    else:\n        rules = get_rules_by_name(name, self.rules_dir, immediate)\n    actual = [[rule.name, rule.params.keysets.keys()] for rule in rules]\n    for rule in actual:\n        rule[1].sort()\n    eq_(actual, expected)\n", "label": 0}
{"function": "\n\ndef getPluginNameAndModuleFromStream(self, infoFileObject, candidate_infofile=None):\n    for analyzer in self._analyzers:\n        if (analyzer.name == 'info_ext'):\n            return analyzer.getPluginNameAndModuleFromStream(infoFileObject)\n    else:\n        raise RuntimeError('No current file analyzer is able to provide plugin information from stream')\n", "label": 0}
{"function": "\n\ndef bot_process_action(self, action):\n    if (action.action_type == 'message'):\n        body = action.meta.get('body')\n        if body:\n            for dest in action.destination_rooms:\n                self.sendLine('>>> {0}: {1}'.format(dest, body))\n", "label": 0}
{"function": "\n\n@classmethod\ndef create(cls, folder, revision=None, path=None):\n    if (revision and path):\n        raise TypeError('You may specify a git revision or a local path; not both.')\n    if revision:\n        return GitRevisionJiraLinkManager(folder, revision)\n    else:\n        return WorkingCopyJiraLinkManager(folder, path)\n", "label": 0}
{"function": "\n\ndef decode_if_str(arg):\n    if isinstance(arg, list):\n        return [decode_if_str(elem) for elem in arg]\n    else:\n        return (arg.decode('utf-8') if isinstance(arg, str) else arg)\n", "label": 0}
{"function": "\n\ndef convert_to_color(object, name, value):\n    ' Converts a tuple or an integer to an RGB color value, or raises a\\n    TraitError if that is not possible.\\n    '\n    if ((type(value) in SequenceTypes) and (len(value) == 3)):\n        return (range_check(value[0]), range_check(value[1]), range_check(value[2]))\n    if (type(value) is int):\n        num = int(value)\n        return ((((num / 65536) / 255.0(((num / 256) & 255))) / 255.0), ((num & 255) / 255.0))\n    raise TraitError\n", "label": 0}
{"function": "\n\n@overrides.setter\ndef overrides(self, newch):\n    self._overrides._active = False\n    self._overrides.clear()\n    for (k, v) in newch.iteritems():\n        if v:\n            self._overrides[str(k)] = v\n        else:\n            try:\n                del self._overrides[str(k)]\n            except:\n                pass\n    self._overrides._active = True\n    self._overrides._send()\n", "label": 0}
{"function": "\n\ndef parse(text, pos=0, endpos=None):\n    pos = 0\n    if (endpos is None):\n        endpos = len(text)\n    d = {\n        \n    }\n    while 1:\n        m = entityRE.search(text, pos, endpos)\n        if (not m):\n            break\n        (name, charcode, comment) = m.groups()\n        d[name] = (charcode, comment)\n        pos = m.end()\n    return d\n", "label": 0}
{"function": "\n\ndef create_hc(G):\n    'Creates hierarchical cluster of graph G from distance matrix'\n    path_length = nx.all_pairs_shortest_path_length(G)\n    distances = numpy.zeros((len(G), len(G)))\n    for (u, p) in path_length.items():\n        for (v, d) in p.items():\n            distances[u][v] = d\n    Y = distance.squareform(distances)\n    Z = hierarchy.complete(Y)\n    membership = list(hierarchy.fcluster(Z, t=1.15))\n    partition = defaultdict(list)\n    for (n, p) in zip(list(range(len(G))), membership):\n        partition[p].append(n)\n    return list(partition.values())\n", "label": 0}
{"function": "\n\ndef test_algorithm_returns_06(self):\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['Monthly'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.three_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['3-Month'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.six_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['6-month'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.year_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['year'])\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    from sympy.combinatorics.permutations import Permutation, Cycle\n    if Permutation.print_cyclic:\n        if (not self.size):\n            return 'Permutation()'\n        s = Cycle(self)((self.size - 1)).__repr__()[len('Cycle'):]\n        last = s.rfind('(')\n        if ((not (last == 0)) and (',' not in s[last:])):\n            s = (s[last:] + s[:last])\n        return ('Permutation%s' % s)\n    else:\n        s = self.support()\n        if (not s):\n            if (self.size < 5):\n                return ('Permutation(%s)' % str(self.array_form))\n            return ('Permutation([], size=%s)' % self.size)\n        trim = (str(self.array_form[:(s[(- 1)] + 1)]) + (', size=%s' % self.size))\n        use = full = str(self.array_form)\n        if (len(trim) < len(full)):\n            use = trim\n        return ('Permutation%s' % use)\n", "label": 1}
{"function": "\n\ndef ui_dialog(ui, parent, is_modal):\n    ' Creates a wxPython dialog box for a specified UI object.\\n\\n    Changes are not immediately applied to the underlying object. The user must\\n    click **Apply** or **OK** to apply changes. The user can revert changes by\\n    clicking **Revert** or **Cancel**.\\n    '\n    if (ui.owner is None):\n        ui.owner = ModalDialog()\n    ui.owner.init(ui, parent, is_modal)\n    ui.control = ui.owner.control\n    ui.control._parent = parent\n    try:\n        ui.prepare_ui()\n    except:\n        ui.control.Destroy()\n        ui.control.ui = None\n        ui.control = None\n        ui.owner = None\n        ui.result = False\n        raise\n    ui.handler.position(ui.info)\n    restore_window(ui)\n    if is_modal:\n        ui.control.ShowModal()\n    else:\n        ui.control.Show()\n", "label": 0}
{"function": "\n\ndef _fit_cg(f, score, start_params, fargs, kwargs, disp=True, maxiter=100, callback=None, retall=False, full_output=True, hess=None):\n    gtol = kwargs.setdefault('gtol', 1e-05)\n    norm = kwargs.setdefault('norm', np.Inf)\n    epsilon = kwargs.setdefault('epsilon', 1.4901161193847656e-08)\n    retvals = optimize.fmin_cg(f, start_params, score, gtol=gtol, norm=norm, epsilon=epsilon, maxiter=maxiter, full_output=full_output, disp=disp, retall=retall, callback=callback)\n    if full_output:\n        if (not retall):\n            (xopt, fopt, fcalls, gcalls, warnflag) = retvals\n        else:\n            (xopt, fopt, fcalls, gcalls, warnflag, allvecs) = retvals\n        converged = (not warnflag)\n        retvals = {\n            'fopt': fopt,\n            'fcalls': fcalls,\n            'gcalls': gcalls,\n            'warnflag': warnflag,\n            'converged': converged,\n        }\n        if retall:\n            retvals.update({\n                'allvecs': allvecs,\n            })\n    else:\n        xopt = retvals\n        retvals = None\n    return (xopt, retvals)\n", "label": 0}
{"function": "\n\ndef get_oauth_signature(self, request):\n    'Get an OAuth signature to be used in signing a request\\n        '\n    if (self.signature_method == SIGNATURE_PLAINTEXT):\n        return signature.sign_plaintext(self.client_secret, self.resource_owner_secret)\n    (uri, headers, body) = self._render(request)\n    collected_params = signature.collect_parameters(uri_query=urlparse.urlparse(uri).query, body=body, headers=headers)\n    logger.debug('Collected params: {0}'.format(collected_params))\n    normalized_params = signature.normalize_parameters(collected_params)\n    normalized_uri = signature.normalize_base_string_uri(request.uri)\n    logger.debug('Normalized params: {0}'.format(normalized_params))\n    logger.debug('Normalized URI: {0}'.format(normalized_uri))\n    base_string = signature.construct_base_string(request.http_method, normalized_uri, normalized_params)\n    logger.debug('Base signing string: {0}'.format(base_string))\n    if (self.signature_method == SIGNATURE_HMAC):\n        sig = signature.sign_hmac_sha1(base_string, self.client_secret, self.resource_owner_secret)\n    elif (self.signature_method == SIGNATURE_RSA):\n        sig = signature.sign_rsa_sha1(base_string, self.rsa_key)\n    else:\n        sig = signature.sign_plaintext(self.client_secret, self.resource_owner_secret)\n    logger.debug('Signature: {0}'.format(sig))\n    return sig\n", "label": 0}
{"function": "\n\ndef set_serial_number(self, serial):\n    '\\n        Set the serial number of the certificate.\\n\\n        :param serial: The new serial number.\\n        :type serial: :py:class:`int`\\n\\n        :return: :py:data`None`\\n        '\n    if (not isinstance(serial, _integer_types)):\n        raise TypeError('serial must be an integer')\n    hex_serial = hex(serial)[2:]\n    if (not isinstance(hex_serial, bytes)):\n        hex_serial = hex_serial.encode('ascii')\n    bignum_serial = _ffi.new('BIGNUM**')\n    small_serial = _lib.BN_hex2bn(bignum_serial, hex_serial)\n    if (bignum_serial[0] == _ffi.NULL):\n        set_result = _lib.ASN1_INTEGER_set(_lib.X509_get_serialNumber(self._x509), small_serial)\n        if set_result:\n            _raise_current_error()\n    else:\n        asn1_serial = _lib.BN_to_ASN1_INTEGER(bignum_serial[0], _ffi.NULL)\n        _lib.BN_free(bignum_serial[0])\n        if (asn1_serial == _ffi.NULL):\n            _raise_current_error()\n        asn1_serial = _ffi.gc(asn1_serial, _lib.ASN1_INTEGER_free)\n        set_result = _lib.X509_set_serialNumber(self._x509, asn1_serial)\n        if (not set_result):\n            _raise_current_error()\n", "label": 1}
{"function": "\n\ndef fileUpdated(self, file):\n    '\\n        On file update, if the name or the MIME type changed, we must update\\n        them accordingly on the S3 key so that the file downloads with the\\n        correct name and content type.\\n        '\n    if file.get('imported'):\n        return\n    bucket = self._getBucket()\n    key = bucket.get_key(file['s3Key'], validate=True)\n    if (not key):\n        return\n    disp = ('attachment; filename=\"%s\"' % file['name'])\n    mime = (file.get('mimeType') or '')\n    if ((key.content_type != mime) or (key.content_disposition != disp)):\n        key.set_remote_metadata(metadata_plus={\n            'Content-Type': mime,\n            'Content-Disposition': disp.encode('utf8'),\n        }, metadata_minus=[], preserve_acl=True)\n", "label": 1}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    argvals = list(args)\n    ops = []\n    tapes = set()\n    for (i, arg) in enumerate(args):\n        if isinstance(arg, Node):\n            argvals[i] = arg.value\n            if (i in self.zero_grads):\n                continue\n            for (tape, parent_rnode) in iteritems(arg.tapes):\n                if (not tape.complete):\n                    ops.append((tape, i, parent_rnode))\n                    tapes.add(tape)\n    (result, aux) = self.fun(*argvals, **kwargs)\n    if (result is NotImplemented):\n        return result\n    if ops:\n        result = new_node(result, tapes)\n        for (tape, argnum, parent) in ops:\n            gradfun = self.gradmaker(argnum, aux, result, args, kwargs)\n            rnode = result.tapes[tape]\n            rnode.parent_grad_ops.append((gradfun, parent))\n    return result\n", "label": 1}
{"function": "\n\ndef register_model(self, app_label, model):\n    model_name = model._meta.model_name\n    app_models = self.all_models[app_label]\n    if (model_name in app_models):\n        if ((model.__name__ == app_models[model_name].__name__) and (model.__module__ == app_models[model_name].__module__)):\n            warnings.warn((\"Model '%s.%s' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\" % (app_label, model_name)), RuntimeWarning, stacklevel=2)\n        else:\n            raise RuntimeError((\"Conflicting '%s' models in application '%s': %s and %s.\" % (model_name, app_label, app_models[model_name], model)))\n    app_models[model_name] = model\n    self.do_pending_operations(model)\n    self.clear_cache()\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    self.admin = credentials.UsernamePassword('admin', 'asdf')\n    self.alice = credentials.UsernamePassword('alice', 'foo')\n    self.badPass = credentials.UsernamePassword('alice', 'foobar')\n    self.badUser = credentials.UsernamePassword('x', 'yz')\n    self.checker = strcred.makeChecker('unix')\n    if pwd:\n        database = UserDatabase()\n        for (username, password) in self.users.items():\n            database.addUser(username, crypt.crypt(password, 'F/'), 1000, 1000, username, ('/home/' + username), '/bin/sh')\n        self.patch(pwd, 'getpwnam', database.getpwnam)\n    if spwd:\n        self._spwd_getspnam = spwd.getspnam\n        spwd.getspnam = self._spwd\n", "label": 0}
{"function": "\n\ndef generate(env):\n    'Add Builders and construction variables for gcc to an Environment.'\n    if ('CC' not in env):\n        env['CC'] = (env.Detect(compilers) or compilers[0])\n    cc.generate(env)\n    if (env['PLATFORM'] in ['cygwin', 'win32']):\n        env['SHCCFLAGS'] = SCons.Util.CLVar('$CCFLAGS')\n    else:\n        env['SHCCFLAGS'] = SCons.Util.CLVar('$CCFLAGS -fPIC')\n    version = detect_version(env, env['CC'])\n    if version:\n        env['CCVERSION'] = version\n", "label": 0}
{"function": "\n\ndef filterOnStatusExt(statusExt, commits):\n    return [commit for commit in commits if (('_' in commit.status) and (commit.status[(commit.status.rfind('_') + 1):] == statusExt))]\n", "label": 0}
{"function": "\n\ndef crossdomain(origin=None, methods=None, headers=None, max_age=21600, attach_to_all=True, automatic_options=True):\n    if (methods is not None):\n        methods = ', '.join(sorted((x.upper() for x in methods)))\n    if ((headers is not None) and (not isinstance(headers, str))):\n        headers = ', '.join((x.upper() for x in headers))\n    if (not isinstance(origin, str)):\n        origin = ', '.join(origin)\n    if isinstance(max_age, timedelta):\n        max_age = max_age.total_seconds()\n\n    def get_methods():\n        if (methods is not None):\n            return methods\n        options_resp = current_app.make_default_options_response()\n        return options_resp.headers['allow']\n\n    def decorator(f):\n\n        def wrapped_function(*args, **kwargs):\n            if (automatic_options and (request.method == 'OPTIONS')):\n                resp = current_app.make_default_options_response()\n            else:\n                resp = make_response(f(*args, **kwargs))\n            if ((not attach_to_all) and (request.method != 'OPTIONS')):\n                return resp\n            h = resp.headers\n            h['Access-Control-Allow-Origin'] = origin\n            h['Access-Control-Allow-Methods'] = get_methods()\n            h['Access-Control-Max-Age'] = str(max_age)\n            if (headers is not None):\n                h['Access-Control-Allow-Headers'] = headers\n            return resp\n        f.provide_automatic_options = False\n        return update_wrapper(wrapped_function, f)\n    return decorator\n", "label": 1}
{"function": "\n\ndef GatheringUserDatasGet(self, socketId):\n    command = 'GatheringUserDatasGet(double *,double *,double *,double *,double *,double *,double *,double *)'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(8):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef invert(self):\n    for polygon in self.polygons:\n        polygon.flip()\n    self.plane.flip()\n    if self.front:\n        self.front.invert()\n    if self.back:\n        self.back.invert()\n    (self.front, self.back) = (self.back, self.front)\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.stdout.write('Collecting duplicates...')\n    duplicates = sum([self.get_quota_duplicate_versions(quota) for quota in Quota.objects.all()], [])\n    self.stdout.write('...Done')\n    if (not duplicates):\n        self.stdout.write('No duplicates were found. Congratulations!')\n    else:\n        self.stdout.write(('There are %s duplicates for quotas versions.' % len(duplicates)))\n        while True:\n            delete = (raw_input('  Do you want to delete them? [Y/n]:') or 'y')\n            if (delete.lower() not in ('y', 'n')):\n                self.stdout.write('  Please enter letter \"y\" or \"n\"')\n            else:\n                delete = (delete.lower() == 'y')\n                break\n        if delete:\n            for duplicate in duplicates:\n                duplicate.delete()\n            self.stdout.write('All duplicates were deleted.')\n        else:\n            self.stdout.write('Duplicates were not deleted.')\n", "label": 1}
{"function": "\n\ndef start_upload_workers(self, files, uploaded_queue, boundary, local_deploy):\n    num_files = len(files)\n    num_workers = 4\n    if (num_workers > num_files):\n        num_workers = num_files\n    start = 0\n    step = int(((num_files + (num_workers - 1)) / num_workers))\n    for _ in range(num_workers):\n        end = (start + step)\n        if (end > num_files):\n            end = num_files\n        Thread(target=self.post_files, args=[files, start, end, uploaded_queue.put, boundary, local_deploy]).start()\n        start = end\n", "label": 0}
{"function": "\n\ndef next(self):\n    if ((self.next_cursor == 0) or (self.limit and (self.count == self.limit))):\n        raise StopIteration\n    (data, cursors) = self.method(*self.args, cursor=self.next_cursor, **self.kargs)\n    (self.prev_cursor, self.next_cursor) = cursors\n    if (len(data) == 0):\n        raise StopIteration\n    self.count += 1\n    return data\n", "label": 0}
{"function": "\n\ndef parse_output(output):\n    'Parse fabric output and return the output per host'\n    line_pattern = re.compile('\\\\[(?P<host>.+)\\\\] out: (?P<line>.+)')\n    results = {\n        \n    }\n    for line in output:\n        m = line_pattern.match(line)\n        if m:\n            host = m.group('host')\n            if (host not in results):\n                results[host] = []\n            line = m.group('line').strip()\n            if line:\n                results[host].append(line)\n    return results\n", "label": 0}
{"function": "\n\ndef save_template_dict(self, templ_dict):\n    if templ_dict:\n        try:\n            self.es.index(index=(self.project_name + '-crawler-template_dict'), doc_type='template_dict', id=self.plugin_name, body=json.dumps({repr(k): v for (k, v) in templ_dict.items()}))\n        except:\n            self.es.update(index=(self.project_name + '-crawler-template_dict'), doc_type='template_dict', id=self.plugin_name, body=json.dumps({\n                'doc': {repr(k): v for (k, v) in templ_dict.items()},\n            }))\n", "label": 0}
{"function": "\n\ndef group_connections(connections):\n    \"\\n    Return a list of (language code, respective connections) pairs, while\\n    using Django's translation.override() to set each language.\\n    \"\n    grouped_conns = defaultdict(list)\n    if isinstance(connections, QuerySet):\n        languages = connections.values_list('contact__language', flat=True)\n        for language in languages.distinct():\n            lang_conns = connections.filter(contact__language=language)\n            grouped_conns[language].extend(lang_conns)\n    else:\n        for connection in connections:\n            language = connection.contact.language\n            grouped_conns[language].append(connection)\n    for (lang, conns) in grouped_conns.items():\n        (yield (lang, conns))\n", "label": 0}
{"function": "\n\ndef get_user(self, check_permissions=True):\n    key = self.kwargs[self.user_lookup_url_kwarg]\n    current_user = self.request.user\n    if (key == 'me'):\n        if isinstance(current_user, AnonymousUser):\n            raise NotAuthenticated\n        else:\n            return self.request.user\n    obj = get_object_or_error(User, key, 'user')\n    if check_permissions:\n        self.check_object_permissions(self.request, obj)\n    return obj\n", "label": 0}
{"function": "\n\n@log\ndef getattr(self, path, fh=None):\n    (uid, gid, pid) = fuse_get_context()\n    working_path = path\n    if is_special_file(path):\n        (working_path, special) = explode_special_file(working_path)\n    record = self._get_record(working_path)\n    if record.is_directory():\n        mode = (stat.S_IFDIR | PERMISSION_ALL_READ)\n        nlink = 2\n    else:\n        mode = (stat.S_IFREG | PERMISSION_ALL_READ)\n        nlink = 1\n    if is_special_file(path):\n        size = 0\n        (working_path, special) = explode_special_file(path)\n        if (special == 'meta'):\n            node = self._get_node(working_path)\n            record_buf = self._enumerator.get_record_buf(node.get_record_number())\n            size = len(get_meta_for_file(record, working_path))\n    else:\n        data_attribute = record.data_attribute()\n        if (data_attribute is not None):\n            if (data_attribute.non_resident() == 0):\n                size = len(data_attribute.value())\n            else:\n                size = data_attribute.data_size()\n        else:\n            size = record.filename_information().logical_size()\n    return {\n        'st_atime': unixtimestamp(record.standard_information().accessed_time()),\n        'st_ctime': unixtimestamp(record.standard_information().changed_time()),\n        'st_mtime': unixtimestamp(record.standard_information().modified_time()),\n        'st_size': size,\n        'st_uid': uid,\n        'st_gid': gid,\n        'st_mode': mode,\n        'st_nlink': nlink,\n    }\n", "label": 1}
{"function": "\n\ndef _update(self, url, data, kwargs, coerce):\n    params = {\n        \n    }\n    for k in data.keys():\n        if (data[k] is None):\n            del data[k]\n    if (('who' in kwargs) and (kwargs['who'] is not None)):\n        params['_who'] = kwargs['who']\n    if (('why' in kwargs) and (kwargs['why'] is not None)):\n        params['_why'] = kwargs['why']\n    headers = kwargs.get('headers', {\n        \n    })\n    resp = self.connection.request(url, method='PUT', params=params, data=data, headers=headers)\n    if (resp.status == httplib.NO_CONTENT):\n        location = resp.headers.get('location')\n        if (not location):\n            raise LibcloudError('Missing location header')\n        obj_ids = self._url_to_obj_ids(location)\n        return coerce(**obj_ids)\n    else:\n        raise LibcloudError(('Unexpected status code: %s' % resp.status))\n", "label": 1}
{"function": "\n\ndef to_json(s):\n    'Return a valid json string, given a jsarray string.\\n\\n    :param s: string of jsarray data\\n    '\n    out = []\n    for t in generate_tokens(StringIO(s).readline):\n        if (out and any(((',' == t[1] == out[(- 1)]), ((out[(- 1)] == '[') and (t[1] == ','))))):\n            out.append('null')\n        out.append(t[1])\n    return ''.join(out)\n", "label": 0}
{"function": "\n\ndef viewRssFeed(self, modelDocument, parentNode):\n    self.id = 1\n    for rssItem in modelDocument.rssItems:\n        node = self.treeView.insert(parentNode, 'end', rssItem.objectId(), text=(rssItem.companyName or ''), tags=(('odd' if (self.id & 1) else 'even'),))\n        self.treeView.set(node, 'form', rssItem.formType)\n        self.treeView.set(node, 'filingDate', rssItem.filingDate)\n        self.treeView.set(node, 'cik', rssItem.cikNumber)\n        self.treeView.set(node, 'status', rssItem.status)\n        self.treeView.set(node, 'period', rssItem.period)\n        self.treeView.set(node, 'fiscalYrEnd', rssItem.fiscalYearEnd)\n        self.treeView.set(node, 'results', (' '.join((str(result) for result in (rssItem.results or []))) + ((' ' + str(rssItem.assertions)) if rssItem.assertions else '')))\n        self.id += 1\n    else:\n        pass\n", "label": 1}
{"function": "\n\ndef convert_from_timestamp(timestamp, to_format, to_type=str):\n    if (to_type == str):\n        return converter.timestamp_to_string(timestamp, to_format)\n    elif (to_type == datetime):\n        return converter.timestamp_to_datetime(timestamp, to_format)\n    elif (to_type == date):\n        return converter.timestamp_to_date(timestamp)\n", "label": 0}
{"function": "\n\ndef __init__(self, path=None):\n    self.path = os.path.abspath((path or os.path.curdir))\n    try:\n        self.master = 'origin/master'\n        master_sha = self.shell('git', 'log', '-1', '--pretty=format:%H', self.master, exceptions=True).split\n    except:\n        self.master = 'master'\n        master_sha = self.shell('git', 'log', '-1', '--pretty=format:%H', self.master).split\n    self.master_sha = ((master_sha and master_sha[0].strip()) or '')\n", "label": 0}
{"function": "\n\ndef _retry_reboot(self, context, instance):\n    current_power_state = self._get_power_state(context, instance)\n    current_task_state = instance.task_state\n    retry_reboot = False\n    reboot_type = compute_utils.get_reboot_type(current_task_state, current_power_state)\n    pending_soft = ((current_task_state == task_states.REBOOT_PENDING) and (instance.vm_state in vm_states.ALLOW_SOFT_REBOOT))\n    pending_hard = ((current_task_state == task_states.REBOOT_PENDING_HARD) and (instance.vm_state in vm_states.ALLOW_HARD_REBOOT))\n    started_not_running = ((current_task_state in [task_states.REBOOT_STARTED, task_states.REBOOT_STARTED_HARD]) and (current_power_state != power_state.RUNNING))\n    if (pending_soft or pending_hard or started_not_running):\n        retry_reboot = True\n    return (retry_reboot, reboot_type)\n", "label": 1}
{"function": "\n\ndef mouseMoveEvent(self, event):\n    channel = None\n    point = None\n    if self.cur_drag:\n        channel = self.cur_drag[0]\n        point = self.cur_drag[1]\n        if (not point.fixed):\n            point.set_pos(channel.get_index_pos(event.x()))\n            point.activate_channels(self.active_channels_string)\n            self.table.sort_control_points()\n        channel.set_value_index(point.color, event.y())\n        self.table_config_changed(final_update=False)\n    screenX = event.x()\n    screenY = event.y()\n    (width, height) = (self.size().width(), self.size().height())\n    master = self.master\n    (s1, s2) = master.get_table_range()\n    if (channel is not None):\n        name = self.text_map[channel.name]\n        pos = (s1 + ((s2 - s1) * point.pos))\n        val = channel.get_value(point.color)\n        txt = ('%s: (%.3f, %.3f)' % (name, pos, val))\n    else:\n        x = (s1 + (((s2 - s1) * float(screenX)) / (width - 1)))\n        y = (1.0 - (float(screenY) / (height - 1)))\n        txt = ('position: (%.3f, %.3f)' % (x, y))\n    self.master.set_status_text(txt)\n", "label": 0}
{"function": "\n\ndef solve(self, rhs_mat, system, mode):\n    \" Solves the linear system for the problem in self.system. The\\n        full solution vector is returned.\\n\\n        Args\\n        ----\\n        rhs_mat : dict of ndarray\\n            Dictionary containing one ndarry per top level quantity of\\n            interest. Each array contains the right-hand side for the linear\\n            solve.\\n\\n        system : `System`\\n            Parent `System` object.\\n\\n        mode : string\\n            Derivative mode, can be 'fwd' or 'rev'.\\n\\n        Returns\\n        -------\\n        dict of ndarray : Solution vectors\\n        \"\n    self.system = system\n    if (self.mode is None):\n        self.mode = mode\n    sol_buf = OrderedDict()\n    for (voi, rhs) in rhs_mat.items():\n        self.voi = None\n        if system._jacobian_changed:\n            self.jacobian = self._assemble_jacobian(rhs, mode)\n            system._jacobian_changed = False\n            if (self.options['solve_method'] == 'LU'):\n                self.lup = lu_factor(self.jacobian)\n        if (self.options['solve_method'] == 'LU'):\n            deriv = lu_solve(self.lup, rhs)\n        else:\n            deriv = np.linalg.solve(self.jacobian, rhs)\n        self.system = None\n        sol_buf[voi] = deriv\n    return sol_buf\n", "label": 1}
{"function": "\n\ndef cluster_separation(self, cluster, clustering, clustering_cohesion, condensed_distance_matrix):\n    '\\n        Returns the cohesion plus separation value of a cluster. The condensed matrix\\n        given as parameter stores the distances of the elements of the dataset used to\\n        extract the cluster.\\n        '\n    if (clustering_cohesion > 0):\n        weight = (1.0 / clustering_cohesion)\n        sep_and_cohe = 0.0\n        where_am_i = clustering.cluster_index(cluster)\n        for i in range(len(clustering.clusters)):\n            if (i != where_am_i):\n                c_j = clustering.clusters[i]\n                sep_and_cohe = (sep_and_cohe + self.__between_cluster_distance(cluster, c_j, condensed_distance_matrix))\n        return (weight * sep_and_cohe)\n    else:\n        return 0.0\n", "label": 0}
{"function": "\n\ndef get_dots_case_json(casedoc, anchor_date=None):\n    '\\n    Return JSON-ready array of the DOTS block for given patient.\\n    Pulling properties from PATIENT document.\\n    Patient document trumps casedoc in this use case.\\n    '\n    if (anchor_date is None):\n        anchor_date = datetime.now(tz=timezone(settings.TIME_ZONE))\n    enddate = anchor_date\n    ret = {\n        'regimens': [int((getattr(casedoc, CASE_NONART_REGIMEN_PROP, None) or 0)), int((getattr(casedoc, CASE_ART_REGIMEN_PROP, None) or 0))],\n        'regimen_labels': [list(casedoc.nonart_labels), list(casedoc.art_labels)],\n        'days': [],\n        'anchor': anchor_date.strftime('%d %b %Y'),\n    }\n    observations = query_observations(casedoc._id, (enddate - timedelta(days=DOT_DAYS_INTERVAL)), enddate)\n    for delta in range(DOT_DAYS_INTERVAL):\n        obs_date = (enddate - timedelta(days=delta))\n        day_arr = filter_obs_for_day(obs_date.date(), observations)\n        day_data = DOTDay.merge_from_observations(day_arr)\n        ret['days'].append(day_data.to_case_json(casedoc, ret['regimen_labels']))\n    ret['days'].reverse()\n    return ret\n", "label": 0}
{"function": "\n\ndef value__get(self):\n    if (self.selectedIndex is not None):\n        return self.options[self.selectedIndex][0]\n    else:\n        for (option, checked) in self.options:\n            if checked:\n                return option\n        else:\n            if self.options:\n                return self.options[0][0]\n            else:\n                return None\n", "label": 1}
{"function": "\n\ndef options(self):\n    result = []\n    if self.title:\n        result.append(('title: \"%s\"' % self.title))\n    if self.icon:\n        result.append(('icon: %s' % self.icon.varname))\n    if self.draggable:\n        result.append('draggable: true')\n    return ('{%s}' % ','.join(result))\n", "label": 0}
{"function": "\n\ndef get_available_screens():\n    '\\n    Gets the available screens in this package for dynamic instantiation.\\n    '\n    ignore_list = ['__init__.py']\n    screens = []\n    for module in os.listdir(os.path.join(os.path.dirname(__file__))):\n        if ((module in ignore_list) or (module[(- 3):] != '.py')):\n            continue\n        module_name = module[:(- 3)]\n        m = __import__(module_name, globals(), locals())\n        for (name, obj) in inspect.getmembers(m):\n            if (inspect.isclass(obj) and issubclass(obj, base.ScreenBase) and (not name.endswith('Base'))):\n                screens.append(obj)\n    screens.extend(get_available_plugin_screens())\n    return screens\n", "label": 1}
{"function": "\n\ndef finalize(self):\n    'finalizing this Report sends off the email.'\n    self.write(self._formatter.finalize())\n    report = ezmail.MIMEText.MIMEText(self._fo.getvalue(), self._formatter.MIMETYPE.split('/')[1])\n    report['Content-Disposition'] = 'inline'\n    self._message.attach(report)\n    if (self._attach_logfile and self._logfile):\n        try:\n            lfd = open(self._logfile, 'rb').read()\n        except:\n            pass\n        else:\n            logmsg = ezmail.MIMEText.MIMEText(lfd, charset=chardet.detect(lfd))\n            logmsg['Content-Disposition'] = ('attachment; filename=%s' % (os.path.basename(self._logfile),))\n            self._message.attach(logmsg)\n    ezmail.mail(self._message)\n", "label": 0}
{"function": "\n\ndef list(self, json=False, limit=500, offset=0, search=None):\n    if search:\n        search = {\n            'match': [{\n                '_all': search,\n            }],\n        }\n    sessions = self.client.sessions.list_all(limit=limit, offset=offset, search=search)\n    if json:\n        return sessions\n    return [utils.SessionObject(session.get('session_id'), session.get('description'), session.get('status'), session.get('jobs'), session.get('schedule', {\n        \n    }).get('schedule_start_date'), session.get('schedule', {\n        \n    }).get('schedule_interval'), session.get('schedule', {\n        \n    }).get('schedule_end_date')) for session in sessions]\n", "label": 0}
{"function": "\n\ndef m_values(j):\n    j = sympify(j)\n    size = ((2 * j) + 1)\n    if ((not size.is_Integer) or (not (size > 0))):\n        raise ValueError(('Only integer or half-integer values allowed for j, got: : %r' % j))\n    return (size, [(j - i) for i in range(int(((2 * j) + 1)))])\n", "label": 0}
{"function": "\n\ndef test_alphabet(self):\n    h = Hashids(alphabet='!\"#%&\\',-/0123456789:;<=>ABCDEFGHIJKLMNOPQRSTUVWXYZ_`abcdefghijklmnopqrstuvwxyz~')\n    assert (h.encode(2839, 12, 32, 5) == '_nJUNTVU3')\n    assert (h.encode(1, 2, 3) == '7xfYh2')\n    assert (h.encode(23832) == 'Z6R>')\n    assert (h.encode(99, 25) == 'AYyIB')\n", "label": 0}
{"function": "\n\ndef store_current():\n    '\\n    Stores the current logs in the database\\n    '\n    for (key, getwork_c) in getworks.items():\n        accepted_c = accepted.get(key, 0)\n        rejected_c = rejected.get(key, 0)\n        server = key[0]\n        username = key[1]\n        password = key[2]\n        difficulty = key[3]\n        timestamp = time.asctime(time.gmtime())\n        try:\n            if (get_diff(server) != difficulty):\n                continue\n        except:\n            logging.error(traceback.format_exc())\n            continue\n        sql = (\"UPDATE Statistics SET Getworks = %s, Accepted = %s, Rejected = %s, Timestamp = '%s' WHERE Server = '%s' AND Username = '%s' AND Password = '%s' AND Difficulty = %s\" % (getwork_c, accepted_c, rejected_c, timestamp, server, username, password, difficulty))\n        bitHopper.Database.execute(sql)\n        result = bitHopper.Database.execute(('SELECT Getworks from Statistics WHERE Server = \"%s\" AND Username = \"%s\" AND Password = \"%s\" AND Difficulty = %s' % (server, username, password, difficulty)))\n        result = list(result)\n        if (len(result) == 0):\n            sql = (\"INSERT INTO Statistics (Server, Username, Password, Difficulty, Timestamp, Getworks, Accepted, Rejected) VALUES ('%s', '%s', '%s', %s, '%s', %s, %s, %s)\" % (server, username, password, difficulty, timestamp, getwork_c, accepted_c, rejected_c))\n            bitHopper.Database.execute(sql)\n", "label": 0}
{"function": "\n\ndef fix_file(filename, options=None, output=None):\n    if (not options):\n        options = parse_args([filename])\n    original_source = readlines_from_file(filename)\n    fixed_source = original_source\n    if (options.in_place or output):\n        encoding = detect_encoding(filename)\n    if output:\n        output = codecs.getwriter(encoding)((output.buffer if hasattr(output, 'buffer') else output))\n        output = LineEndingWrapper(output)\n    fixed_source = fix_lines(fixed_source, options, filename=filename)\n    if options.diff:\n        new = io.StringIO(fixed_source)\n        new = new.readlines()\n        diff = get_diff_text(original_source, new, filename)\n        if output:\n            output.write(diff)\n            output.flush()\n        else:\n            return diff\n    elif options.in_place:\n        fp = open_with_encoding(filename, encoding=encoding, mode='w')\n        fp.write(fixed_source)\n        fp.close()\n    elif output:\n        output.write(fixed_source)\n        output.flush()\n    else:\n        return fixed_source\n", "label": 1}
{"function": "\n\n@handle_response_format\n@treeio_login_required\ndef settings_view(request, response_format='html'):\n    'Settings admin view'\n    try:\n        conf = ModuleSetting.get_for_module('treeio.messaging', 'default_contact_type', user=request.user.profile)[0]\n        default_contact_type = ContactType.objects.get(pk=long(conf.value))\n    except:\n        default_contact_type = None\n    try:\n        conf = ModuleSetting.get_for_module('treeio.messaging', 'default_imap_folder')[0]\n        default_imap_folder = conf.value\n    except:\n        default_imap_folder = getattr(settings, 'HARDTREE_MESSAGING_IMAP_DEFAULT_FOLDER_NAME', 'UNSEEN')\n    try:\n        conf = ModuleSetting.get_for_module('treeio.messaging', 'signature', user=request.user.profile, strict=True)[0]\n        signature = conf.value\n    except:\n        signature = ''\n    types = Object.filter_by_request(request, ContactType.objects.order_by('name'))\n    context = _get_default_context(request)\n    context.update({\n        'types': types,\n        'signature': signature,\n        'default_contact_type': default_contact_type,\n        'default_imap_folder': default_imap_folder,\n    })\n    return render_to_response('messaging/settings_view', context, context_instance=RequestContext(request), response_format=response_format)\n", "label": 0}
{"function": "\n\ndef test_native_type(self):\n    b = BPF(text='BPF_TABLE(\"array\", int, u64, table1, 128);')\n    t1 = b['table1']\n    t1[0] = ct.c_ulonglong(100)\n    t1[(- 2)] = ct.c_ulonglong(37)\n    t1[127] = ct.c_ulonglong(1000)\n    for (i, v) in t1.items():\n        if (i.value == 0):\n            self.assertEqual(v.value, 100)\n        if (i.value == 127):\n            self.assertEqual(v.value, 1000)\n    self.assertEqual(len(t1), 128)\n    self.assertEqual(t1[(- 2)].value, 37)\n    self.assertEqual(t1[(- 1)].value, t1[127].value)\n", "label": 0}
{"function": "\n\ndef _check_var(self, doc):\n    '\\n        Run checks on the variable whose documentation is C{var} and\\n        whose name is C{name}.\\n        \\n        @param doc: The documentation for the variable to check.\\n        @type doc: L{APIDoc}\\n        @rtype: C{None}\\n        '\n    if (self._checks & DocChecker.VAR):\n        if ((self._checks & (DocChecker.DESCR | DocChecker.TYPE)) and (doc.descr in (None, UNKNOWN)) and (doc.type_descr in (None, UNKNOWN)) and (doc.docstring in (None, UNKNOWN))):\n            self.warning('Undocumented', doc)\n        else:\n            if ((self._checks & DocChecker.DESCR) and (doc.descr in (None, UNKNOWN))):\n                self.warning('No description', doc)\n            if ((self._checks & DocChecker.TYPE) and (doc.type_descr in (None, UNKNOWN))):\n                self.warning('No type information', doc)\n", "label": 1}
{"function": "\n\ndef validate_prepopulated_fields(self, cls, model):\n    ' Validate that prepopulated_fields if a dictionary  containing allowed field types. '\n    if hasattr(cls, 'prepopulated_fields'):\n        check_isdict(cls, 'prepopulated_fields', cls.prepopulated_fields)\n        for (field, val) in cls.prepopulated_fields.items():\n            f = get_field(cls, model, 'prepopulated_fields', field)\n            if isinstance(f, (models.DateTimeField, models.ForeignKey, models.ManyToManyField)):\n                raise ImproperlyConfigured((\"'%s.prepopulated_fields['%s']' is either a DateTimeField, ForeignKey or ManyToManyField. This isn't allowed.\" % (cls.__name__, field)))\n            check_isseq(cls, (\"prepopulated_fields['%s']\" % field), val)\n            for (idx, f) in enumerate(val):\n                get_field(cls, model, (\"prepopulated_fields['%s'][%d]\" % (field, idx)), f)\n", "label": 0}
{"function": "\n\n@dec.skip_win32\ndef test_signal_kernel_subprocesses(self):\n    self._install_test_kernel()\n    (km, kc) = start_new_kernel(kernel_name='signaltest')\n\n    def execute(cmd):\n        kc.execute(cmd)\n        reply = kc.get_shell_msg(TIMEOUT)\n        content = reply['content']\n        self.assertEqual(content['status'], 'ok')\n        return content\n    self.addCleanup(kc.stop_channels)\n    self.addCleanup(km.shutdown_kernel)\n    N = 5\n    for i in range(N):\n        execute('start')\n    time.sleep(1)\n    reply = execute('check')\n    self.assertEqual(reply['user_expressions']['poll'], ([None] * N))\n    kc.execute('sleep')\n    time.sleep(1)\n    km.interrupt_kernel()\n    reply = kc.get_shell_msg(TIMEOUT)\n    content = reply['content']\n    self.assertEqual(content['status'], 'ok')\n    self.assertEqual(content['user_expressions']['interrupted'], True)\n    for i in range(50):\n        reply = execute('check')\n        if (reply['user_expressions']['poll'] != ([(- signal.SIGINT)] * N)):\n            time.sleep(0.1)\n        else:\n            break\n    self.assertEqual(reply['user_expressions']['poll'], ([(- signal.SIGINT)] * N))\n", "label": 0}
{"function": "\n\ndef main(argv=None):\n    argv = sys.argv\n    path = os.path.abspath(os.path.dirname(__file__))\n    if ((len(argv) == 1) or (argv[1] == '--help') or (argv[1] == '-h')):\n        print(globals()['__doc__'])\n        map_keyword2script = mapKeyword2Script(path)\n        if (len(argv) <= 2):\n            print('CGAT tools are grouped by keywords. The following keywords')\n            print('are defined:\\n')\n            print(('%s\\n' % printListInColumns(map_keyword2script.keys(), 3)))\n        if ('all' in argv[2:]):\n            print('The list of all available commands is:\\n')\n            print(('%s\\n' % printListInColumns(sorted([os.path.basename(x)[:(- 3)] for x in glob.glob(os.path.join(path, '*.py'))]), 3)))\n        else:\n            for arg in argv[2:]:\n                if (arg in map_keyword2script):\n                    print((\"Tools matching the keyword '%s':\\n\" % arg))\n                    print(('%s\\n' % printListInColumns(sorted(map_keyword2script[arg]), 3)))\n        return\n    command = argv[1]\n    (file, pathname, description) = imp.find_module(command, [path])\n    module = imp.load_module(command, file, pathname, description)\n    del sys.argv[0]\n    module.main(sys.argv)\n", "label": 1}
{"function": "\n\ndef _LineContainsI18n(uwline):\n    'Return true if there are i18n comments or function calls in the line.\\n\\n  I18n comments and pseudo-function calls are closely related. They cannot\\n  be moved apart without breaking i18n.\\n\\n  Arguments:\\n    uwline: (unwrapped_line.UnwrappedLine) The line currently being formatted.\\n\\n  Returns:\\n    True if the line contains i18n comments or function calls. False otherwise.\\n  '\n    if style.Get('I18N_COMMENT'):\n        for tok in uwline.tokens:\n            if (tok.is_comment and re.match(style.Get('I18N_COMMENT'), tok.value)):\n                return True\n    if style.Get('I18N_FUNCTION_CALL'):\n        length = len(uwline.tokens)\n        index = 0\n        while (index < (length - 1)):\n            if ((uwline.tokens[(index + 1)].value == '(') and (uwline.tokens[index].value in style.Get('I18N_FUNCTION_CALL'))):\n                return True\n            index += 1\n    return False\n", "label": 1}
{"function": "\n\n@classmethod\ndef invalidate(cls, region):\n    'Invalidate an entire region\\n\\n        .. note::\\n\\n            This does not actually *clear* the region of data, but\\n            just sets the value to expire on next access.\\n\\n        :param region: Region name\\n        :type region: string\\n\\n        '\n    redis = global_connection.redis\n    namespaces = redis.smembers(('retools:%s:namespaces' % region))\n    if (not namespaces):\n        return None\n    longest_expire = max([x['expires'] for x in CacheRegion.regions.values()])\n    new_created = ((time.time() - longest_expire) - 3600)\n    for ns in namespaces:\n        cache_keyset_key = ('retools:%s:%s:keys' % (region, ns))\n        keys = (set(['']) | redis.smembers(cache_keyset_key))\n        for key in keys:\n            cache_key = ('retools:%s:%s:%s' % (region, ns, key))\n            if (not redis.exists(cache_key)):\n                redis.srem(cache_keyset_key, key)\n            else:\n                redis.hset(cache_key, 'created', new_created)\n", "label": 1}
{"function": "\n\ndef Local(self, *targets):\n    ret = []\n    for targ in targets:\n        if isinstance(targ, SCons.Node.Node):\n            targ.set_local()\n            ret.append(targ)\n        else:\n            for t in self.arg2nodes(targ, self.fs.Entry):\n                t.set_local()\n                ret.append(t)\n    return ret\n", "label": 0}
{"function": "\n\ndef check_vendor_specific_is_not_set(self, use_net_template=False):\n    node = self.env.create_node(cluster_id=self.cluster.id, roles=['controller'], primary_roles=['controller'])\n    objects.Cluster.set_network_template(self.cluster, (self.net_template if use_net_template else None))\n    objects.Cluster.prepare_for_deployment(self.cluster)\n    serializer = deployment_serializers.get_serializer_for_cluster(self.cluster)\n    net_serializer = serializer.get_net_provider_serializer(self.cluster)\n    nm = objects.Cluster.get_network_manager(self.cluster)\n    networks = nm.get_node_networks(node)\n    endpoints = net_serializer.generate_network_scheme(node, networks)['endpoints']\n    for name in endpoints:\n        if endpoints[name].get('vendor_specific'):\n            self.assertItemsEqual(['provider_gateway'], endpoints[name]['vendor_specific'])\n", "label": 0}
{"function": "\n\ndef get_duplicate_fullnames():\n    'Returns a list of fullnames that are opened in multiple instances'\n    open_xl_workbooks = []\n    for xl_app in get_xl_apps():\n        for xl_workbook in get_all_open_xl_workbooks(xl_app):\n            open_xl_workbooks.append(xl_workbook)\n    return get_duplicates([i.FullName.lower() for i in open_xl_workbooks])\n", "label": 0}
{"function": "\n\ndef request(url, post=None, headers=None, mobile=False, safe=False, timeout='30'):\n    try:\n        control.log(('[cloudflare] request %s' % url))\n        try:\n            headers.update(headers)\n        except:\n            headers = {\n                \n            }\n        agent = cache.get(cloudflareAgent, 168)\n        if (not ('User-Agent' in headers)):\n            headers['User-Agent'] = agent\n        u = ('%s://%s' % (urlparse.urlparse(url).scheme, urlparse.urlparse(url).netloc))\n        cookie = cache.get(cloudflareCookie, 168, u, post, headers, mobile, safe, timeout)\n        result = client.request(url, cookie=cookie, post=post, headers=headers, mobile=mobile, safe=safe, timeout=timeout, output='response', error=True)\n        if (result[0] == '503'):\n            agent = cache.get(cloudflareAgent, 0)\n            headers['User-Agent'] = agent\n            cookie = cache.get(cloudflareCookie, 0, u, post, headers, mobile, safe, timeout)\n            result = client.request(url, cookie=cookie, post=post, headers=headers, mobile=mobile, safe=safe, timeout=timeout)\n        else:\n            result = result[1]\n        return result\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef _out_of_memory_get_data(self, state=None, request=None):\n    if (not isinstance(request, (numbers.Integral, slice, list))):\n        raise ValueError()\n    data = []\n    shapes = []\n    handle = self._file_handle\n    for (source_name, subset) in zip(self.sources, self.subsets):\n        data.append(subset.index_within_subset(handle[source_name], request, sort_indices=self.sort_indices))\n        if (source_name in self.vlen_sources):\n            shapes.append(subset.index_within_subset(handle[source_name].dims[0]['shapes'], request, sort_indices=self.sort_indices))\n        else:\n            shapes.append(None)\n    return (data, shapes)\n", "label": 0}
{"function": "\n\n@expose_api(name='create', url='/api/create', methods=['POST'])\n@has_access_api\n@permission_name('add')\ndef api_create(self):\n    is_valid_form = True\n    get_filter_args(self._filters)\n    exclude_cols = self._filters.get_relation_cols()\n    form = self.add_form.refresh()\n    self._fill_form_exclude_cols(exclude_cols, form)\n    if form.validate():\n        item = self.datamodel.obj()\n        form.populate_obj(item)\n        self.pre_add(item)\n        if self.datamodel.add(item):\n            self.post_add(item)\n            http_return_code = 200\n        else:\n            http_return_code = 500\n    else:\n        is_valid_form = False\n    if is_valid_form:\n        response = make_response(jsonify({\n            'message': self.datamodel.message[0],\n            'severity': self.datamodel.message[1],\n        }), http_return_code)\n    else:\n        response = make_response(jsonify({\n            'message': 'Invalid form',\n            'severity': 'warning',\n        }), 500)\n    return response\n", "label": 0}
{"function": "\n\ndef __protect__(self, key, value=sentinel):\n    'Protected keys add its parents, not sure if useful'\n    if (not isinstance(key, list)):\n        key = (key.split('.') if isinstance(key, basestring) else [key])\n    (key, path) = (key.pop(0), key)\n    if (len(path) > 0):\n        self.get(key).protect(path, value)\n    elif (value is not sentinel):\n        self[key] = value\n    if (key not in self):\n        raise KeyError(('key %s has no value to protect' % key))\n    self.__PROTECTED__.add(key)\n", "label": 1}
{"function": "\n\ndef decode(self, request, value):\n    value = int(value)\n    if (((self.min is not None) and (value < self.min)) or ((self.max is not None) and (value > self.max))):\n        raise ValueError(('Value %d is out of range.' % value))\n    return value\n", "label": 0}
{"function": "\n\ndef compute(self):\n    im = self.get_input('Input Image')\n    if self.has_input('Input PixelType'):\n        inPixelType = self.get_input('Input PixelType')\n    else:\n        inPixelType = im.getPixelType()\n    if self.has_input('Dimension'):\n        dim = self.get_input('Dimension')\n    else:\n        dim = im.getDim()\n    inImgType = itk.Image[(inPixelType._type, dim)]\n    try:\n        self.filter_ = itk.CurvatureAnisotropicDiffusionImageFilter[(inImgType, inImgType)].New(im.getImg())\n    except:\n        raise ModuleError(self, 'Filter requires a decimal PixelType')\n    if self.has_input('Iterations'):\n        iterations = self.get_input('Iterations')\n    else:\n        iterations = 5\n    if self.has_input('TimeStep'):\n        timestep = self.get_input('TimeStep')\n    elif (dim == 2):\n        timestep = 0.125\n    else:\n        timestep = 0.0625\n    if self.has_input('Conductance'):\n        conductance = self.get_input('Conductance')\n    else:\n        conductance = 3.0\n    self.filter_.SetNumberOfIterations(iterations)\n    self.filter_.SetTimeStep(timestep)\n    self.filter_.SetConductanceParameter(conductance)\n    self.filter_.Update()\n    outIm = Image()\n    outIm.setImg(self.filter_.GetOutput())\n    outIm.setPixelType(inPixelType)\n    outIm.setDim(dim)\n    self.set_output('Output Image', outIm)\n    self.set_output('Filter', self)\n", "label": 1}
{"function": "\n\ndef __init__(self, dictionary):\n    self.championId = dictionary.get('championId', 0)\n    self.createDate = dictionary.get('createDate', 0)\n    self.fellowPlayers = [(Player(player) if (not isinstance(player, Player)) else player) for player in dictionary.get('fellowPlayers', []) if player]\n    self.gameId = dictionary.get('gameId', 0)\n    self.gameMode = dictionary.get('gameMode', '')\n    self.gameType = dictionary.get('gameType', '')\n    self.invalid = dictionary.get('invalid', False)\n    self.ipEarned = dictionary.get('ipEarned', 0)\n    self.level = dictionary.get('level', 0)\n    self.mapId = dictionary.get('mapId', 0)\n    self.spell1 = dictionary.get('spell1', 0)\n    self.spell2 = dictionary.get('spell2', 0)\n    val = dictionary.get('stats', None)\n    self.stats = (RawStats(val) if (val and (not isinstance(val, RawStats))) else val)\n    self.subType = dictionary.get('subType', '')\n    self.teamId = dictionary.get('teamId', 0)\n", "label": 1}
{"function": "\n\ndef recv_Logout(self, iprot, mtype, rseqid):\n    d = self._reqs.pop(rseqid)\n    if (mtype == TMessageType.EXCEPTION):\n        x = TApplicationException()\n        x.read(iprot)\n        iprot.readMessageEnd()\n        return d.errback(x)\n    result = Logout_result()\n    result.read(iprot)\n    iprot.readMessageEnd()\n    if (result.success is not None):\n        return d.callback(result.success)\n    if (result.error is not None):\n        return d.errback(result.error)\n    return d.errback(TApplicationException(TApplicationException.MISSING_RESULT, 'Logout failed: unknown result'))\n", "label": 0}
{"function": "\n\ndef updateDragOperation(self, event):\n    '\\n        http://dev.w3.org/html5/spec/dnd.html\\n        '\n    dataTransfer = event.dataTransfer\n    ea = dataTransfer.effectAllowed\n    de = dataTransfer.dropEffect\n    if ((de == 'copy') and (ea in ['uninitialized', 'copy', 'copyLink', 'copyMove', 'all'])):\n        self.currentDragOperation = 'copy'\n    elif ((de == 'link') and (ea in ['uninitialized', 'link', 'copyLink', 'linkMove', 'all'])):\n        self.currentDragOperation = 'link'\n    elif ((de == 'move') and (ea in ['uninitialized', 'move', 'copyMove', 'linkMove', 'all'])):\n        self.currentDragOperation = 'move'\n    else:\n        self.currentDragOperation = 'none'\n", "label": 1}
{"function": "\n\ndef _sage_(self):\n    import sage.all as sage\n    (f, limits) = (self.function._sage_(), list(self.limits))\n    for limit in limits:\n        if (len(limit) == 1):\n            x = limit[0]\n            f = sage.integral(f, x._sage_(), hold=True)\n        elif (len(limit) == 2):\n            (x, b) = limit\n            f = sage.integral(f, x._sage_(), b._sage_(), hold=True)\n        else:\n            (x, a, b) = limit\n            f = sage.integral(f, (x._sage_(), a._sage_(), b._sage_()), hold=True)\n    return f\n", "label": 0}
{"function": "\n\ndef ByteSizePartial(self):\n    n = 0\n    if self.has_application_key_:\n        n += 1\n        n += self.lengthString(len(self.application_key_))\n    if self.has_is_available_:\n        n += 2\n    if self.has_error_code_:\n        n += 1\n        n += self.lengthVarInt64(self.error_code_)\n    return n\n", "label": 0}
{"function": "\n\n@classmethod\ndef all(cls, session, page_size=1000, order_by=None):\n    offset = 0\n    order_by = (order_by or cls.id)\n    while True:\n        page = cls.find(session, order_by=order_by, limit=page_size, offset=offset)\n        for m in page:\n            (yield m)\n        session.flush()\n        if (len(page) != page_size):\n            raise StopIteration()\n        offset += page_size\n", "label": 0}
{"function": "\n\ndef dump_publickey(type, pkey):\n    '\\n    Dump a public key to a buffer.\\n\\n    :param type: The file type (one of :data:`FILETYPE_PEM` or\\n        :data:`FILETYPE_ASN1`).\\n    :param PKey pkey: The public key to dump\\n    :return: The buffer with the dumped key in it.\\n    :rtype: bytes\\n    '\n    bio = _new_mem_buf()\n    if (type == FILETYPE_PEM):\n        write_bio = _lib.PEM_write_bio_PUBKEY\n    elif (type == FILETYPE_ASN1):\n        write_bio = _lib.i2d_PUBKEY_bio\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    result_code = write_bio(bio, pkey._pkey)\n    if (result_code != 1):\n        _raise_current_error()\n    return _bio_to_string(bio)\n", "label": 0}
{"function": "\n\ndef testDirtyPropogation(self):\n    c = GafferImage.Constant()\n    r = GafferImage.ImageReader()\n    r['fileName'].setValue(self.checkerFile)\n    inMetadata = r['out']['metadata'].getValue()\n    m = GafferImage.CopyImageMetadata()\n    m['in'].setInput(c['out'])\n    m['copyFrom'].setInput(r['out'])\n    cs = GafferTest.CapturingSlot(m.plugDirtiedSignal())\n    m['copyFrom'].setInput(c['out'])\n    self.assertTrue((m['out']['metadata'] in set((e[0] for e in cs))))\n    del cs[:]\n    m['names'].setValue('test')\n    self.assertTrue((m['out']['metadata'] in set((e[0] for e in cs))))\n    del cs[:]\n    m['invertNames'].setValue(True)\n    self.assertTrue((m['out']['metadata'] in set((e[0] for e in cs))))\n", "label": 0}
{"function": "\n\ndef clean_Commit(self):\n    commit = getattr(self, 'Commit', None)\n    if (commit is not None):\n        if ((commit is not True) and (commit is not False)):\n            raise AvalaraValidationException(AvalaraException.CODE_BAD_BOOL, 'Commit should either be True, or False')\n", "label": 0}
{"function": "\n\ndef getOrCreateUser(self, name, token=''):\n    self.lock.acquire()\n    userIndex = (- 1)\n    for i in self.getActiveUserIndexes():\n        if (self.arrayOfUsers[i]._name == name):\n            userIndex = i\n    if (userIndex < 0):\n        userIndex = self.arrayOfUsers.size()\n        self.arrayOfUsers.add(UserEntry(userIndex, (userIndex - self.deletedUserCount), name, token))\n        for roleIndex in self.getActiveRoleIndexes():\n            self.arrayOfUsers[userIndex].addRoleByIndex(roleIndex)\n    self.lock.release()\n    return userIndex\n", "label": 0}
{"function": "\n\ndef apply(self, obj, defaults={\n    \n}, **kwargs):\n    ': Apply this style to the given object using the supplied defaults.\\n\\n      = NOTE\\n      - This can apply to any matplotlib Text.\\n\\n      = INPUT VARIABLES\\n      - obj       The object to apply the style to.\\n      - defaults  Keyword-value dictionary with defaults values to use if a\\n                  property value is not specified.\\n      - kwargs    Keyword-value dictionary whose values will supercede\\n                  any values set by the properties of this sub-style.\\n      '\n    if (not isinstance(obj, mpltext.Text)):\n        msg = (\"Unable to apply this sub-style to the given element.Expected a matplotlib 'Text' and instead received the following:\\n%s\" % (obj,))\n        raise Exception(msg)\n    properties = {\n        'bgColor': 'backgroundcolor',\n        'fgColor': 'color',\n        'vertAlign': 'verticalalignment',\n        'horizAlign': 'horizontalalignment',\n        'multiAlign': 'multialignment',\n        'lineSpacing': 'linespacing',\n        'rotation': 'rotation',\n    }\n    subKwargs = kwargs.get('font', {\n        \n    })\n    subDefaults = S.lib.resolveDefaults(defaults, ['font'])\n    self.font.apply(obj.get_font_properties(), subDefaults, **subKwargs)\n    MplArtistStyle.apply(self, obj, defaults, **kwargs)\n    kw = {\n        \n    }\n    for p in properties:\n        mplProp = properties[p]\n        value = self.getValue(p, defaults, **kwargs)\n        if (value is not None):\n            kw[mplProp] = value\n    if kw:\n        obj.update(kw)\n", "label": 0}
{"function": "\n\ndef status(msg, e=None):\n    msg = ('[InsertDate] ' + msg)\n    sublime.status_message(msg)\n    if (e is not None):\n        msg += ('\\n%s: %s' % (type(e).__name__, e))\n    print(msg)\n    if (e and DEBUG):\n        import traceback\n        traceback.print_exc()\n", "label": 0}
{"function": "\n\ndef onBrowserEvent(self, event):\n    Widget.onBrowserEvent(self, event)\n    type = DOM.eventGetType(event)\n    if (type == 'load'):\n        for listener in self.loadListeners:\n            listener.onLoad(self)\n    elif (type == 'error'):\n        for listener in self.loadListeners:\n            listener.onError(self)\n", "label": 0}
{"function": "\n\ndef _parse_flags(self, flags):\n    if isinstance(flags, basestring):\n        flags = [flags]\n    computed = 0\n    for flag in flags:\n        if isinstance(flag, basestring):\n            flag = getattr(zookeeper, flag)\n        computed |= flag\n    return computed\n", "label": 0}
{"function": "\n\ndef _extract_video_info(self, video_data):\n    video_id = compat_str(video_data['id'])\n    FORMAT_KEYS = (('sd', 'progressive_url'), ('hd', 'progressive_url_hd'))\n    formats = [{\n        'format_id': format_id,\n        'url': video_data[key],\n        'quality': (i + 1),\n    } for (i, (format_id, key)) in enumerate(FORMAT_KEYS) if video_data.get(key)]\n    smil_url = video_data.get('smil_url')\n    if smil_url:\n        formats.extend(self._parse_smil(video_id, smil_url))\n    self._sort_formats(formats)\n    return {\n        'id': video_id,\n        'formats': formats,\n        'title': video_data['caption'],\n        'thumbnail': video_data.get('thumbnail_url'),\n        'upload_date': video_data['updated_at'].replace('-', '')[:8],\n        'like_count': video_data.get('likes', {\n            \n        }).get('total'),\n        'view_count': video_data.get('views'),\n    }\n", "label": 0}
{"function": "\n\ndef _index_document(index_list):\n    'Helper to generate an index specifying document.\\n\\n    Takes a list of (key, direction) pairs.\\n    '\n    if isinstance(index_list, collections.Mapping):\n        raise TypeError(('passing a dict to sort/create_index/hint is not allowed - use a list of tuples instead. did you mean %r?' % list(iteritems(index_list))))\n    elif (not isinstance(index_list, (list, tuple))):\n        raise TypeError(('must use a list of (key, direction) pairs, not: ' + repr(index_list)))\n    if (not len(index_list)):\n        raise ValueError('key_or_list must not be the empty list')\n    index = SON()\n    for (key, value) in index_list:\n        if (not isinstance(key, string_type)):\n            raise TypeError('first item in each key pair must be a string')\n        if (not isinstance(value, (string_type, int, collections.Mapping))):\n            raise TypeError(\"second item in each key pair must be 1, -1, '2d', 'geoHaystack', or another valid MongoDB index specifier.\")\n        index[key] = value\n    return index\n", "label": 1}
{"function": "\n\ndef _build_spanning_datetimes(self, d1, d2):\n    businessdays = list(self.iterbusinessdays(d1, d2))\n    if (len(businessdays) == 0):\n        return businessdays\n    businessdays = [datetime.datetime.combine(d, self.business_hours[0]) for d in businessdays]\n    if (d1 > businessdays[0]):\n        businessdays[0] = d1\n    if (self.isbusinessday(d2) and (d2 >= datetime.datetime.combine(d2, self.business_hours[0]))):\n        businessdays.append(datetime.datetime.combine(d2, self.business_hours[1]))\n        if (d2 < businessdays[(- 1)]):\n            businessdays[(- 1)] = datetime.datetime.combine(businessdays[(- 1)], d2.time())\n    elif (len(businessdays) == 1):\n        businessdays.append(datetime.datetime.combine(businessdays[0], self.business_hours[1]))\n    else:\n        businessdays[(- 1)] = datetime.datetime.combine(businessdays[(- 1)], self.business_hours[1])\n    return businessdays\n", "label": 1}
{"function": "\n\ndef validate_email(self, email):\n    if (not email):\n        return (email, 'The e-mail is empty.')\n    parts = email.split('@')\n    if (len(parts) != 2):\n        return (email, 'An email address must contain a single @')\n    (local, domain) = parts\n    (domain, err) = self.validate_domain(domain)\n    if err:\n        return (email, ('The e-mail has a problem to the right of the @: %s' % err))\n    (local, err) = self.validate_local_part(local)\n    if err:\n        return (email, ('The email has a problem to the left of the @: %s' % err))\n    return (((local + '@') + domain), '')\n", "label": 0}
{"function": "\n\ndef write(self, data, escape=True):\n    if (not escape):\n        self.body.write(str(data))\n    elif (hasattr(data, 'xml') and callable(data.xml)):\n        self.body.write(data.xml())\n    else:\n        if (not isinstance(data, (str, unicode))):\n            data = str(data)\n        elif isinstance(data, unicode):\n            data = data.encode('utf8', 'xmlcharrefreplace')\n        data = cgi.escape(data, True).replace(\"'\", '&#x27;')\n        self.body.write(data)\n", "label": 1}
{"function": "\n\ndef _apply_operator_JzKet(self, ket, **options):\n    j = ket.j\n    m = ket.m\n    if (m.is_Number and j.is_Number):\n        if (m >= j):\n            return S.Zero\n    return ((hbar * sqrt(((j * (j + S.One)) - (m * (m + S.One))))) * JzKet(j, (m + S.One)))\n", "label": 0}
{"function": "\n\ndef __call__(self, event, sep=os.path.sep, join=os.path.join):\n    '\\n        Handle event and print filename, line number and source code. If event.kind is a `return` or `exception` also\\n        prints values.\\n        '\n    lines = self._safe_source(event)\n    thread_name = (threading.current_thread().name if event.tracer.threading_support else '')\n    thread_align = (self.thread_alignment if event.tracer.threading_support else '')\n    self.stream.write('{thread:{thread_align}}{filename}{:>{align}}{colon}:{lineno}{:<5} {kind}{:9} {code}{}{reset}\\n'.format(self._format_filename(event), event.lineno, event.kind, lines[0], thread=thread_name, thread_align=thread_align, align=self.filename_alignment, code=self.code_colors[event.kind], **self.event_colors))\n    for line in lines[1:]:\n        self.stream.write('{thread:{thread_align}}{:>{align}}       {kind}{:9} {code}{}{reset}\\n'.format('', '   |', line, thread=thread_name, thread_align=thread_align, align=self.filename_alignment, code=self.code_colors[event.kind], **self.event_colors))\n    if (event.kind in ('return', 'exception')):\n        self.stream.write('{thread:{thread_align}}{:>{align}}       {continuation}{:9} {color}{} value: {detail}{}{reset}\\n'.format('', '...', event.kind, self._safe_repr(event.arg), thread=thread_name, thread_align=thread_align, align=self.filename_alignment, color=self.event_colors[event.kind], **self.event_colors))\n", "label": 0}
{"function": "\n\ndef has(self, name):\n    '\\n        Returns whether there is a value for the given field name.\\n        '\n    if (not ('.' in name)):\n        return (name in self.__data)\n    splits = name.split('.')\n    current = self.__data\n    for split in splits:\n        if (split in current):\n            current = current[split]\n        else:\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef _fake_getheader_blank_origin(self, header):\n    if (header == 'cookie'):\n        return 'token=\"123-456-789\"'\n    elif (header == 'Origin'):\n        return ''\n    elif (header == 'Host'):\n        return 'example.net:6080'\n    else:\n        return\n", "label": 0}
{"function": "\n\ndef set_profile_properties(server, options, username, props):\n    ' curl -u admin:admin -Fprofile/age=29 http://localhost:4502/home/users/t/testuser1.rw.html '\n    user_path = get_user_path(username)\n    path = '{}.rw.html'.format(user_path)\n    url = server.url(path)\n    props = {('profile/' + k): v for (k, v) in props.items()}\n    resp = requests.post(url, auth=server.auth, data=props)\n    if (resp.status_code != 200):\n        sys.stderr.write('error: Failed to set profile property on path {}, request returned {}\\n'.format(path, resp.status_code))\n        return SERVER_ERROR\n    if options.raw:\n        sys.stdout.write('{}\\n'.format(resp.content))\n    else:\n        sys.stdout.write('{}\\n'.format(user_path))\n    return OK\n", "label": 0}
{"function": "\n\ndef _extract_metrics(self, page, status_code=200, id=None):\n    if (status_code != 200):\n        if (status_code == 404):\n            return {\n                \n            }\n        else:\n            raise self._get_error(status_code)\n    if (not ('user_id' in page)):\n        raise ProviderContentMalformedError\n    json_response = provider._load_json(page)\n    this_video_json = json_response[0]\n    dict_of_keylists = {\n        'vimeo:plays': ['stats_number_of_plays'],\n        'vimeo:likes': ['stats_number_of_likes'],\n        'vimeo:comments': ['stats_number_of_comments'],\n    }\n    metrics_dict = provider._extract_from_data_dict(this_video_json, dict_of_keylists)\n    return metrics_dict\n", "label": 0}
{"function": "\n\ndef merge(self, follower):\n    if ((follower not in self.followers) and (follower != self.creator)):\n        logger.error('user {0} is not a follower of {1}@{2}'.format(follower, self.creator, self.name))\n        return False\n    self.mergeQueue.add(follower)\n    if (len(self.mergeQueue) >= min(len(self.followers), self.pPartialBarrier)):\n        for follower in self.mergeQueue:\n            [panda.merge(follower) for panda in self.pandas]\n        self.mergeQueue.clear()\n        [panda.update_fields({\n            panda.FCONSENSUS: panda.z.generic(),\n        }) for panda in self.pandas]\n        self.pMergeClock += 1\n        self.update_fields({\n            self.FMERGECLOCK: self.pMergeClock,\n        })\n        logger.debug('merge clock {0}'.format(self.pMergeClock))\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef __init__(self, idl_parsed):\n    '\\n        Creates a new Contract from the parsed IDL JSON\\n\\n        :Parameters:\\n          idl_parsed\\n            Barrister parsed IDL as a list of dicts\\n        '\n    self.idl_parsed = idl_parsed\n    self.interfaces = {\n        \n    }\n    self.structs = {\n        \n    }\n    self.enums = {\n        \n    }\n    self.meta = {\n        \n    }\n    for e in idl_parsed:\n        if (e['type'] == 'struct'):\n            self.structs[e['name']] = Struct(e, self)\n        elif (e['type'] == 'enum'):\n            self.enums[e['name']] = Enum(e)\n        elif (e['type'] == 'interface'):\n            self.interfaces[e['name']] = Interface(e, self)\n        elif (e['type'] == 'meta'):\n            for (k, v) in list(e.items()):\n                if (k != 'type'):\n                    self.meta[k] = v\n", "label": 1}
{"function": "\n\ndef load_plugins(self):\n    for plugin in glob.glob((self.directory + '/plugins/*')):\n        sys.path.insert(0, plugin)\n        sys.path.insert(0, (self.directory + '/plugins/'))\n    for plugin in (glob.glob((self.directory + '/plugins/*.py')) + glob.glob((self.directory + '/plugins/*/*.py'))):\n        logging.info(plugin)\n        name = plugin.split('/')[(- 1)][:(- 3)]\n        if (name in self.config):\n            logging.info(('config found for: ' + name))\n        plugin_config = self.config.get(name, {\n            \n        })\n        plugin_config['DEBUG'] = self.debug\n        self.bot_plugins.append(Plugin(name, plugin_config))\n", "label": 0}
{"function": "\n\ndef _live_receivers(self, senderkey):\n    '\\n        Filter sequence of receivers to get resolved, live receivers.\\n\\n        This checks for weak references and resolves them, then returning only\\n        live receivers.\\n        '\n    none_senderkey = _make_id(None)\n    receivers = []\n    for ((receiverkey, r_senderkey), receiver) in self.receivers:\n        if ((r_senderkey == none_senderkey) or (r_senderkey == senderkey)):\n            if isinstance(receiver, WEAKREF_TYPES):\n                receiver = receiver()\n                if (receiver is not None):\n                    receivers.append(receiver)\n            else:\n                receivers.append(receiver)\n    return receivers\n", "label": 1}
{"function": "\n\ndef indent(payload, by=4, character=' '):\n    \"\\n    Indents a sequence of strings with whitespace.\\n\\n    By default it will indent by 4 spaces. Change the amount of indent with\\n    ``by`` and the character that is used with ``character``.\\n\\n    Example:\\n\\n        >>> print(indent(u'Jig', by=6, character=u'-'))\\n        ------Jig\\n\\n    \"\n    return_first = False\n    if isinstance(payload, basestring):\n        payload = [payload]\n        return_first = True\n    indented = []\n    for line in payload:\n        indented.append((''.join(([unicode(character)] * by)) + unicode(line)))\n    if return_first:\n        return indented[0]\n    return indented\n", "label": 0}
{"function": "\n\ndef enableSpell(self, lang=None):\n    if SpellTextEditor.canSpell():\n        if lang:\n            self._dict = enchant.Dict(lang)\n        else:\n            self._dict = enchant.Dict()\n    else:\n        self._dict = None\n    self._highlighter = SpellHighlighter(self.document())\n    if self._dict:\n        self._highlighter.setDict(self._dict)\n        self._highlighter.rehighlight()\n", "label": 0}
{"function": "\n\ndef parse_date(string, locale=LC_TIME):\n    \"Parse a date from a string.\\n\\n    This function uses the date format for the locale as a hint to determine\\n    the order in which the date fields appear in the string.\\n\\n    >>> parse_date('4/1/04', locale='en_US')\\n    datetime.date(2004, 4, 1)\\n    >>> parse_date('01.04.2004', locale='de_DE')\\n    datetime.date(2004, 4, 1)\\n\\n    :param string: the string containing the date\\n    :param locale: a `Locale` object or a locale identifier\\n    \"\n    format = get_date_format(locale=locale).pattern.lower()\n    year_idx = format.index('y')\n    month_idx = format.index('m')\n    if (month_idx < 0):\n        month_idx = format.index('l')\n    day_idx = format.index('d')\n    indexes = [(year_idx, 'Y'), (month_idx, 'M'), (day_idx, 'D')]\n    indexes.sort()\n    indexes = dict([(item[1], idx) for (idx, item) in enumerate(indexes)])\n    numbers = re.findall('(\\\\d+)', string)\n    year = numbers[indexes['Y']]\n    if (len(year) == 2):\n        year = (2000 + int(year))\n    else:\n        year = int(year)\n    month = int(numbers[indexes['M']])\n    day = int(numbers[indexes['D']])\n    if (month > 12):\n        (month, day) = (day, month)\n    return date(year, month, day)\n", "label": 0}
{"function": "\n\ndef testConvertActualData(self):\n    feed = webmastertools.SitesFeedFromString(test_data.SITES_FEED)\n    self.assert_((len(feed.entry[0].verification_method) == 2))\n    check = 0\n    for method in feed.entry[0].verification_method:\n        self.assert_(isinstance(method, webmastertools.VerificationMethod))\n        if (method.type == 'metatag'):\n            self.assert_((method.in_use == 'false'))\n            self.assert_((method.text is None))\n            self.assert_((method.meta.name == 'verify-v1'))\n            self.assert_((method.meta.content == 'a2Ai'))\n            check = (check | 1)\n        elif (method.type == 'htmlpage'):\n            self.assert_((method.in_use == 'false'))\n            self.assert_((method.text == '456456-google.html'))\n            check = (check | 2)\n        else:\n            self.fail(('Wrong Verification Method: %s' % method.type))\n    self.assert_((check == ((2 ** 2) - 1)), 'Should only have two Verification Methods, metatag and htmlpage')\n", "label": 0}
{"function": "\n\ndef msvc_exists():\n    ' Determine whether MSVC is available on the machine.\\n    '\n    result = 0\n    try:\n        p = subprocess.Popen(['cl'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        str_result = p.stdout.read()\n        if ('Microsoft' in str_result):\n            result = 1\n    except:\n        import distutils.msvccompiler\n        try:\n            version = distutils.msvccompiler.get_devstudio_versions()\n        except:\n            version = distutils.msvccompiler.get_build_version()\n        if version:\n            result = 1\n    return result\n", "label": 0}
{"function": "\n\ndef level_matches(self, level, consumer_level):\n    '\\n        >>> l = Logger([])\\n        >>> l.level_matches(3, 4)\\n        False\\n        >>> l.level_matches(3, 2)\\n        True\\n        >>> l.level_matches(slice(None, 3), 3)\\n        False\\n        >>> l.level_matches(slice(None, 3), 2)\\n        True\\n        >>> l.level_matches(slice(1, 3), 1)\\n        True\\n        >>> l.level_matches(slice(2, 3), 1)\\n        False\\n        '\n    if isinstance(level, slice):\n        (start, stop) = (level.start, level.stop)\n        if ((start is not None) and (start > consumer_level)):\n            return False\n        if ((stop is not None) and (stop <= consumer_level)):\n            return False\n        return True\n    else:\n        return (level >= consumer_level)\n", "label": 1}
{"function": "\n\ndef __acquire_locks(self, lock_list):\n    'Acquire all the locks on the lock_list.\\n\\n        Either sucessfully acquires *all* the locks or raises a\\n        LockError and releases all the locks obtained so far.\\n        '\n    if lock_list:\n        if self.datacube.lock_object(lock_list[0]):\n            try:\n                self.__acquire_locks(lock_list[1:])\n            except:\n                self.datacube.unlock_object(lock_list[0])\n                raise\n        else:\n            raise LockError()\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('PacketCaptureException')\n    if (self.message is not None):\n        oprot.writeFieldBegin('message', TType.STRING, 1)\n        oprot.writeString(self.message)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.history == other.history) and (self.channels == other.channels) and (self.supported == other.supported) and (self.nicksToHostmasks == other.nicksToHostmasks) and (self.batches == other.batches))\n", "label": 0}
{"function": "\n\n@content\ndef PUT(self):\n    ':returns: node id.\\n\\n        :http: * 200 (node are successfully updated)\\n               * 304 (node data not changed since last request)\\n               * 400 (data validation failed)\\n               * 404 (node not found)\\n        '\n    nd = self.checked_data(self.validator.validate_update, data=web.data())\n    node = self.collection.single.get_by_meta(nd)\n    if (not node):\n        raise self.http(404, \"Can't find node: {0}\".format(nd))\n    node.timestamp = datetime.now()\n    if (not node.online):\n        node.online = True\n        msg = \"Node '{0}' is back online\".format(node.human_readable_name)\n        logger.info(msg)\n        notifier.notify('discover', msg, node_id=node.id)\n    db().flush()\n    if (('agent_checksum' in nd) and (node.agent_checksum == nd['agent_checksum'])):\n        return {\n            'id': node.id,\n            'cached': True,\n        }\n    self.collection.single.update_by_agent(node, nd)\n    return {\n        'id': node.id,\n    }\n", "label": 0}
{"function": "\n\ndef boundary_edges(polys):\n    'Returns the edges that are on the boundary of a mesh, as defined by belonging to only 1 face'\n    edges = dict()\n    for (i, poly) in enumerate(np.sort(polys)):\n        for (a, b) in [(0, 1), (1, 2), (0, 2)]:\n            key = (poly[a], poly[b])\n            if (key not in edges):\n                edges[key] = []\n            edges[key].append(i)\n    epts = []\n    for (edge, faces) in edges.items():\n        if (len(faces) == 1):\n            epts.append(edge)\n    return np.array(epts)\n", "label": 1}
{"function": "\n\ndef make_trace_rows(trace):\n    if (not trace.events):\n        return []\n    rows = [[trace.request_type, str(datetime_from_utc_to_local(trace.started_at)), trace.coordinator, 0]]\n    for event in trace.events:\n        rows.append([('%s [%s]' % (event.description, event.thread_name)), str(datetime_from_utc_to_local(event.datetime)), event.source, (event.source_elapsed.microseconds if event.source_elapsed else '--')])\n    if trace.duration:\n        finished_at = (datetime_from_utc_to_local(trace.started_at) + trace.duration)\n    else:\n        finished_at = trace.duration = '--'\n    rows.append(['Request complete', str(finished_at), trace.coordinator, trace.duration.microseconds])\n    return rows\n", "label": 0}
{"function": "\n\ndef has_changes(self):\n    if self.is_dirty:\n        return True\n    for child in self._db_actions:\n        if child.has_changes():\n            return True\n    for child in self._db_tags:\n        if child.has_changes():\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef prep(self, resource, records):\n    logged_in_person = current.auth.s3_logged_in_person()\n    if (not logged_in_person):\n        self.logged_in = False\n        return\n    self.logged_in = True\n    db = current.db\n    atable = current.s3db.work_assignment\n    record_ids = [record['_row']['work_job.id'] for record in records]\n    query = ((atable.job_id.belongs(record_ids) & (atable.person_id == logged_in_person)) & (atable.deleted != True))\n    rows = db(query).select(atable.job_id)\n    self.current_assignments = set((row.job_id for row in rows))\n", "label": 0}
{"function": "\n\ndef key_factory(repository, manifest_data):\n    if (manifest_data[0] == KeyfileKey.TYPE):\n        return KeyfileKey.detect(repository, manifest_data)\n    elif (manifest_data[0] == PassphraseKey.TYPE):\n        return PassphraseKey.detect(repository, manifest_data)\n    elif (manifest_data[0] == PlaintextKey.TYPE):\n        return PlaintextKey.detect(repository, manifest_data)\n    else:\n        raise UnsupportedPayloadError(manifest_data[0])\n", "label": 0}
{"function": "\n\ndef test_replacements_in_network_assignments(self):\n    node_roles_vs_net_names = [(['controller'], ['public', 'management', 'fuelweb_admin']), (['compute', 'cinder'], ['storage', 'management', 'fuelweb_admin'])]\n    template_meta = self.net_template['adv_net_template']['default']\n    iface_var = template_meta['nic_mapping']['default'].keys()[0]\n    ep_with_var = '<% {0} %>.123'.format(iface_var)\n    template_meta['network_assignments']['storage']['ep'] = ep_with_var\n    template_meta['network_scheme']['storage']['endpoints'] = [ep_with_var]\n    objects.Cluster.set_network_template(self.cluster, self.net_template)\n    cluster_db = objects.Cluster.get_by_uid(self.cluster['id'])\n    objects.Cluster.prepare_for_deployment(cluster_db)\n    serializer = get_serializer_for_cluster(self.cluster)\n    serialized_for_astute = serializer(AstuteGraph(cluster_db)).serialize(self.cluster, cluster_db.nodes)\n    for node_data in serialized_for_astute:\n        node = objects.Node.get_by_uid(node_data['uid'])\n        for (node_roles, net_names) in node_roles_vs_net_names:\n            if (node.all_roles == set(node_roles)):\n                self.check_node_ips_on_certain_networks(node, net_names)\n                break\n        else:\n            self.fail('Unexpected combination of node roles: {0}'.format(node.all_roles))\n", "label": 0}
{"function": "\n\ndef ntime(*args, **kwargs):\n    if args:\n        if (args[0] is None):\n            return None\n    elif kwargs:\n        if (None in [v for (k, v) in kwargs.iteritems() if (k != 'tz')]):\n            return None\n    return SaneTime(*args, **kwargs)\n", "label": 1}
{"function": "\n\ndef str_replace(arr, pat, repl, n=(- 1), case=True, flags=0):\n    '\\n    Replace occurrences of pattern/regex in the Series/Index with\\n    some other string. Equivalent to :meth:`str.replace` or\\n    :func:`re.sub`.\\n\\n    Parameters\\n    ----------\\n    pat : string\\n        Character sequence or regular expression\\n    repl : string\\n        Replacement sequence\\n    n : int, default -1 (all)\\n        Number of replacements to make from start\\n    case : boolean, default True\\n        If True, case sensitive\\n    flags : int, default 0 (no flags)\\n        re module flags, e.g. re.IGNORECASE\\n\\n    Returns\\n    -------\\n    replaced : Series/Index of objects\\n    '\n    use_re = ((not case) or (len(pat) > 1) or flags)\n    if use_re:\n        if (not case):\n            flags |= re.IGNORECASE\n        regex = re.compile(pat, flags=flags)\n        n = (n if (n >= 0) else 0)\n\n        def f(x):\n            return regex.sub(repl, x, count=n)\n    else:\n        f = (lambda x: x.replace(pat, repl, n))\n    return _na_map(f, arr)\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    return (obj for obj in (ref() for ref in self._storage) if (obj is not None))\n", "label": 0}
{"function": "\n\ndef _expand_node(self, nid):\n    ' Expands the contents of a specified node (if required).\\n        '\n    (expanded, node, object) = self._get_node_data(nid)\n    if (not expanded):\n        dummy = getattr(nid, '_dummy', None)\n        if (dummy is not None):\n            nid.removeChild(dummy)\n            del nid._dummy\n        for child in node.get_children(object):\n            (child, child_node) = self._node_for(child)\n            if (child_node is not None):\n                self._append_node(nid, child_node, child)\n        self._set_node_data(nid, (True, node, object))\n", "label": 0}
{"function": "\n\ndef _AddToHostData(self, host_data, artifact, data, parser):\n    'Parse raw data collected for an artifact into the host_data table.'\n    if (type(data) != dict):\n        raise test_lib.Error(('Data for %s is not of type dictionary.' % artifact))\n    rdfs = []\n    stats = []\n    files = []\n    for (path, lines) in data.items():\n        stat = self.CreateStat(path)\n        stats.append(stat)\n        file_obj = StringIO.StringIO(lines)\n        files.append(file_obj)\n        if (not parser.process_together):\n            rdfs.extend(list(parser.Parse(stat, file_obj, None)))\n    if parser.process_together:\n        rdfs = list(parser.ParseMultiple(stats, files, None))\n    host_data[artifact] = self.SetArtifactData(anomaly=[a for a in rdfs if isinstance(a, rdf_anomaly.Anomaly)], parsed=[r for r in rdfs if (not isinstance(r, rdf_anomaly.Anomaly))], raw=stats, results=host_data.get(artifact))\n    return host_data\n", "label": 1}
{"function": "\n\ndef get_file(self, filename):\n    if self.base_path:\n        filename = path.join(self.base_path, filename)\n    pieces = split_template_path(filename)\n    for searchpath in self.searchpath:\n        filename = path.join(searchpath, *pieces)\n        f = open_if_exists(filename)\n        if (f is None):\n            continue\n        try:\n            contents = f.read()\n        finally:\n            f.close()\n        return (contents, filename)\n    raise FileNotFound(filename)\n", "label": 0}
{"function": "\n\ndef validate_password(self, field):\n    account = self.account.data\n    if ('@' in account):\n        user = Account.query.filter_by(email=account).first()\n    else:\n        user = Account.query.filter_by(username=account).first()\n    if (not user):\n        raise ValueError(_('Wrong account or password'))\n    if user.check_password(field.data):\n        self.user = user\n        return user\n    raise ValueError(_('Wrong account or password'))\n", "label": 0}
{"function": "\n\ndef login(username, password):\n    verify_to_schema(UserLoginSchema, {\n        'username': username,\n        'password': password,\n    })\n    user = api.user.get_user(username_lower=username.lower())\n    if (user is None):\n        raise WebException('No user with that username exists!')\n    if user.get('disabled', False):\n        raise WebException('This account is disabled.')\n    if confirm_password(password, user['password']):\n        if (user['uid'] is not None):\n            session['uid'] = user['uid']\n            if (user['type'] == 0):\n                session['admin'] = True\n            session.permanent = True\n        else:\n            raise WebException('Login error. Error code: 1.')\n    else:\n        raise WebException('Wrong password.')\n", "label": 1}
{"function": "\n\ndef forms_get_form_add_entry(request, form_slug, **kwargs):\n    form = forms_get_form(form_slug)\n    if form:\n        kwargs['rget'] = request.GET\n        args = (form, (request.POST or None), (request.FILES or None))\n        form_for_form = FormForForm(*args, **kwargs)\n        return form_for_form\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef apply_theme(self, property_values):\n    ' Apply a set of theme values which will be used rather than\\n        defaults, but will not override application-set values.\\n\\n        The passed-in dictionary may be kept around as-is and shared with\\n        other instances to save memory (so neither the caller nor the\\n        |HasProps| instance should modify it).\\n\\n        .. |HasProps| replace:: :class:`~bokeh.properties.HasProps`\\n\\n        '\n    old_dict = None\n    if hasattr(self, '__themed_values__'):\n        old_dict = getattr(self, '__themed_values__')\n    if (old_dict is property_values):\n        return\n    removed = set()\n    if (old_dict is not None):\n        removed.update(set(old_dict.keys()))\n    added = set(property_values.keys())\n    old_values = dict()\n    for k in added.union(removed):\n        old_values[k] = getattr(self, k)\n    if (len(property_values) > 0):\n        setattr(self, '__themed_values__', property_values)\n    elif hasattr(self, '__themed_values__'):\n        delattr(self, '__themed_values__')\n    for (k, v) in old_values.items():\n        prop = self.lookup(k)\n        prop.trigger_if_changed(self, v)\n", "label": 1}
{"function": "\n\ndef get_columns(self, with_aliases=False):\n    '\\n        Remove table names and strip quotes from column names.\\n        '\n    soql_trans = self.query_topology()\n    (cols, col_params) = compiler.SQLCompiler.get_columns(self, with_aliases)\n    out = []\n    for col in cols:\n        if (soql_trans and re.match('^\\\\w+\\\\.\\\\w+$', col)):\n            (tab_name, col_name) = col.split('.')\n            out.append(('%s.%s' % (soql_trans[tab_name], col_name)))\n        else:\n            out.append(col)\n    cols = out\n    result = [x.replace(' AS ', ' ') for x in cols]\n    return (result, col_params)\n", "label": 0}
{"function": "\n\ndef __contains__(self, taskid):\n    if ((taskid in self.priority_queue) or (taskid in self.time_queue)):\n        return True\n    if ((taskid in self.processing) and self.processing[taskid].taskid):\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef _register_callbacks(self):\n    zk = self._zk_util._zk\n    path_for_brokers = self._zk_util.path_for_brokers()\n    path_for_topic = self._zk_util.path_for_topic(self.topic)\n    if ((self._brokers_watch is None) and zk.exists(path_for_brokers)):\n        self._brokers_watch = zk.children(path_for_brokers)(self._unbalance)\n    if ((self._topic_watch is None) and zk.exists(path_for_topic)):\n        self._topic_watch = zk.children(path_for_topic)(self._unbalance)\n    log.debug('Producer {0} has watches: {1}'.format(self._id, sorted(zk.watches.data.keys())))\n", "label": 0}
{"function": "\n\ndef random_interface(dut, exclude=None):\n    exclude = ([] if (exclude is None) else exclude)\n    interfaces = dut.api('interfaces')\n    names = [name for name in list(interfaces.keys()) if name.startswith('Et')]\n    exclude_interfaces = dut.settings.get('exclude_interfaces', [])\n    if exclude_interfaces:\n        exclude_interfaces = exclude_interfaces.split(',')\n    exclude_interfaces.extend(exclude)\n    if (sorted(exclude_interfaces) == sorted(names)):\n        raise TypeError('unable to allocate interface from dut')\n    choices = set(names).difference(exclude)\n    return random.choice(list(choices))\n", "label": 1}
{"function": "\n\ndef test_independent_generators(self):\n    N = 10\n    random_seed(1)\n    py_numbers = [random_random() for i in range(N)]\n    numpy_seed(2)\n    np_numbers = [numpy_random() for i in range(N)]\n    random_seed(1)\n    numpy_seed(2)\n    pairs = [(random_random(), numpy_random()) for i in range(N)]\n    self.assertPreciseEqual([p[0] for p in pairs], py_numbers)\n    self.assertPreciseEqual([p[1] for p in pairs], np_numbers)\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(CloneDatabaseForm, self).__init__(*args, **kwargs)\n    if ('initial' in kwargs):\n        instance = Database.objects.get(id=kwargs['initial']['origin_database_id'])\n    elif ('origin_database_id' in self.data):\n        instance = Database.objects.get(id=self.data['origin_database_id'])\n    if instance:\n        LOG.debug(('instance database form found! %s' % instance))\n        self.define_engine_field(database=instance)\n        self.define_available_plans(database=instance)\n    self.initial['old_plan'] = instance.plan.id\n", "label": 0}
{"function": "\n\ndef _register_port(self, name, porttype, is_terminal, is_physical, flags):\n    'Create a new port.'\n    if is_terminal:\n        flags |= _lib.JackPortIsTerminal\n    if is_physical:\n        flags |= _lib.JackPortIsPhysical\n    port_ptr = _lib.jack_port_register(self._ptr, name.encode(), porttype, flags, 0)\n    if (not port_ptr):\n        raise JackError('{0!r}: port registration failed'.format(name))\n    return self._wrap_port_ptr(port_ptr)\n", "label": 0}
{"function": "\n\ndef to_cudandarray(x):\n    ' take a pycuda.gpuarray.GPUArray and make a CudaNdarray that point to its memory\\n\\n    :note: CudaNdarray support only float32, so only float32 GPUArray are accepted\\n    '\n    if (not isinstance(x, pycuda.gpuarray.GPUArray)):\n        raise ValueError('We can transfer only pycuda.gpuarray.GPUArray to CudaNdarray')\n    elif (x.dtype != 'float32'):\n        raise ValueError('CudaNdarray support only float32')\n    else:\n        strides = [1]\n        for i in x.shape[::(- 1)][:(- 1)]:\n            strides.append((strides[(- 1)] * i))\n        strides = tuple(strides[::(- 1)])\n        ptr = int(x.gpudata)\n        z = cuda.from_gpu_pointer(ptr, x.shape, strides, x)\n        return z\n", "label": 0}
{"function": "\n\ndef execute(self):\n    i = ('You have %s %s.\\n' % (str(self.pc.currency), CURRENCY))\n    if (not self.pc.inventory):\n        i += 'Your inventory is empty.'\n    else:\n        i += 'Your inventory consists of:\\n'\n        for item in self.pc.inventory:\n            if (item not in self.pc.isequipped):\n                i += (item.name + '\\n')\n    self.pc.update_output(i)\n", "label": 0}
{"function": "\n\ndef __deleteData__(self, uri, query, auth=True):\n    if self.ssl:\n        url = ('%s://%s:%s%s' % ('https', self.hostname, self.port, uri))\n    else:\n        url = ('%s://%s:%s%s' % ('http', self.hostname, self.port, uri))\n    full_uri = ('%s' % uri)\n    if (query != None):\n        full_uri = ('%s?%s' % (full_uri, urllib.urlencode(query)))\n    if self.token:\n        if auth:\n            headers = {\n                'Content-Type': 'application/json',\n                'X-Auth-Token': self.token,\n            }\n        else:\n            headers = {\n                'Content-Type': 'application/json',\n            }\n    else:\n        date = utils.formatdate()\n        if auth:\n            sig = self.__signRequest__('DELETE', full_uri, date, 'application/json')\n            headers = {\n                'Content-Type': 'application/json',\n                'Date': date,\n                'Authorization': sig,\n            }\n        else:\n            headers = {\n                'Content-Type': 'application/json',\n                'Date': date,\n            }\n    r = requests.delete(url, params=query, headers=headers)\n    return self.__processResponse__({\n        'status': r.status_code,\n        'data': r.text,\n    })\n", "label": 1}
{"function": "\n\ndef check_sizes(size, width, height):\n    'Check that these arguments, in supplied, are consistent.\\n    Return a (width, height) pair.\\n    '\n    if (not size):\n        return (width, height)\n    if (len(size) != 2):\n        raise ValueError('size argument should be a pair (width, height)')\n    if ((width is not None) and (width != size[0])):\n        raise ValueError(('size[0] (%r) and width (%r) should match when both are used.' % (size[0], width)))\n    if ((height is not None) and (height != size[1])):\n        raise ValueError(('size[1] (%r) and height (%r) should match when both are used.' % (size[1], height)))\n    return size\n", "label": 1}
{"function": "\n\ndef check_access_key_rotation(self, iamuser_item):\n    '\\n        alert when an IAM User has an active access key created more than 90 days go.\\n        '\n    akeys = iamuser_item.config.get('accesskeys', {\n        \n    })\n    for akey in akeys.keys():\n        if ('status' in akeys[akey]):\n            if (akeys[akey]['status'] == 'Active'):\n                create_date = akeys[akey]['create_date']\n                create_date = parser.parse(create_date)\n                if (create_date < self.ninety_days_ago):\n                    notes = '> 90 days ago'\n                    self.add_issue(1, 'Active accesskey has not been rotated.', iamuser_item, notes=notes)\n", "label": 0}
{"function": "\n\ndef check_or_raise_extra(value, message='WAMP message invalid'):\n    '\\n    Check a value for being a valid WAMP extra dictionary.\\n\\n    If the value is not a valid WAMP extra dictionary, raises :class:`autobahn.wamp.exception.ProtocolError`.\\n    Otherwise return the value.\\n\\n    :param value: The value to check.\\n    :type value: dict\\n    :param message: Prefix for message in exception raised when value is invalid.\\n    :type message: unicode\\n\\n    :returns: The extra dictionary (if valid).\\n    :rtype: dict\\n    :raises: instance of :class:`autobahn.wamp.exception.ProtocolError`\\n    '\n    if (type(value) != dict):\n        raise ProtocolError('{0}: invalid type {1}'.format(message, type(value)))\n    for k in value.keys():\n        if (type(k) != six.text_type):\n            raise ProtocolError(\"{0}: invalid type {1} for key '{2}'\".format(message, type(k), k))\n    return value\n", "label": 0}
{"function": "\n\ndef iter_source_code(paths, config, skipped):\n    'Iterate over all Python source files defined in paths.'\n    for path in paths:\n        if os.path.isdir(path):\n            if should_skip(path, config, os.getcwd()):\n                skipped.append(path)\n                continue\n            for (dirpath, dirnames, filenames) in os.walk(path, topdown=True):\n                for dirname in list(dirnames):\n                    if should_skip(dirname, config, dirpath):\n                        skipped.append(dirname)\n                        dirnames.remove(dirname)\n                for filename in filenames:\n                    if filename.endswith('.py'):\n                        if should_skip(filename, config, dirpath):\n                            skipped.append(filename)\n                        else:\n                            (yield os.path.join(dirpath, filename))\n        else:\n            (yield path)\n", "label": 1}
{"function": "\n\ndef _parse(self, opt, fg, bg, attr):\n    if (not opt):\n        return _Color(fg, bg, attr)\n    v = self._config.GetString(('%s.%s' % (self._section, opt)))\n    if (v is None):\n        return _Color(fg, bg, attr)\n    v = v.strip().lower()\n    if (v == 'reset'):\n        return RESET\n    elif (v == ''):\n        return _Color(fg, bg, attr)\n    have_fg = False\n    for a in v.split(' '):\n        if is_color(a):\n            if have_fg:\n                bg = a\n            else:\n                fg = a\n        elif is_attr(a):\n            attr = a\n    return _Color(fg, bg, attr)\n", "label": 1}
{"function": "\n\ndef __init__(self, row=None):\n    if row:\n        for (field, value) in self._fields.iteritems():\n            self._tester.assertEqual(row[field], value)\n    for (field, value) in self._fields.iteritems():\n        setattr(self, field, value)\n", "label": 0}
{"function": "\n\ndef isSane(totalLen, payloadLen, flags):\n    \"\\n    Verifies whether the given header fields are sane.\\n\\n    The values of the fields `totalLen', `payloadLen' and `flags' are checked\\n    for their sanity.  If they are in the expected range, `True' is returned.\\n    If any of these fields has an invalid value, `False' is returned.\\n    \"\n\n    def isFine(length):\n        '\\n        Check if the given length is fine.\\n        '\n        return (True if (0 <= length <= const.MPU) else False)\n    log.debug(('Message header: totalLen=%d, payloadLen=%d, flags=%s' % (totalLen, payloadLen, getFlagNames(flags))))\n    validFlags = [const.FLAG_PAYLOAD, const.FLAG_NEW_TICKET, const.FLAG_PRNG_SEED]\n    return (isFine(totalLen) and isFine(payloadLen) and (totalLen >= payloadLen) and (flags in validFlags))\n", "label": 0}
{"function": "\n\ndef isnpint(ctx, x):\n    '\\n        Determine if *x* is a nonpositive integer.\\n        '\n    if (not x):\n        return True\n    if hasattr(x, '_mpf_'):\n        (sign, man, exp, bc) = x._mpf_\n        return (sign and (exp >= 0))\n    if hasattr(x, '_mpc_'):\n        return ((not x.imag) and ctx.isnpint(x.real))\n    if (type(x) in int_types):\n        return (x <= 0)\n    if isinstance(x, ctx.mpq):\n        (p, q) = x._mpq_\n        if (not p):\n            return True\n        return ((q == 1) and (p <= 0))\n    return ctx.isnpint(ctx.convert(x))\n", "label": 1}
{"function": "\n\ndef hasSpliceMotif(self, seq5, seq3):\n    'find a splice motif in seq.\\n\\n        returns the name of the splice motif or None\\n        '\n    for (name, prime5, prime3) in self.mIntronTypes:\n        if (seq5.startswith(prime5) and seq3.endswith(prime3)):\n            return (name, prime5, prime3)\n    return (None, None, None)\n", "label": 0}
{"function": "\n\n@memoize\ndef non_proxy(model):\n    while model._meta.proxy:\n        model = next((b for b in model.__bases__ if (issubclass(b, models.Model) and (not b._meta.abstract))))\n    return model\n", "label": 0}
{"function": "\n\ndef PositionerDACOffsetDualGet(self, socketId, PositionerName):\n    command = (('PositionerDACOffsetDualGet(' + PositionerName) + ',short *,short *,short *,short *)')\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(4):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef cleanup(self, value, elements, scraped_page=None):\n    url = urlparse.urljoin(scraped_page.scrape_url, value)\n    factory = (_SCRAPER_CLASSES[self.page_factory] if isinstance(self.page_factory, six.string_types) else self.page_factory)\n    if (self.referer is True):\n        referer = scraped_page.scrape_url\n    elif (not self.referer):\n        referer = None\n    else:\n        referer = self.referer\n    return factory(scrape_url=url, scrape_referer=referer)\n", "label": 0}
{"function": "\n\ndef check_signature(script, signature_hash, public_key_blob, sig_blob, hash_type):\n    'Ensure the given transaction has the correct signature. Invoked by the VM.\\n    Adapted from official Bitcoin-QT client.\\n\\n    script: the script that is claimed to unlock the coins used in this transaction\\n    signature_hash: the signature hash of the transaction being verified\\n    public_key_blob: the blob representing the SEC-encoded public pair\\n    sig_blob: the blob representing the DER-encoded signature\\n    hash_type: expected signature_type (or 0 for wild card)\\n    '\n    signature_type = ord(sig_blob[(- 1):])\n    if (signature_type != 1):\n        raise ScriptError(('unknown signature type %d' % signature_type))\n    sig_pair = der.sigdecode_der(sig_blob[:(- 1)])\n    if (hash_type == 0):\n        hash_type = signature_type\n    elif (hash_type != signature_type):\n        raise ScriptError('wrong hash type')\n    public_pair = sec_to_public_pair(public_key_blob)\n    v = ecdsa.verify(ecdsa.generator_secp256k1, public_pair, signature_hash, sig_pair)\n    return make_bool(v)\n", "label": 0}
{"function": "\n\ndef assertErrorPage(self, status, message=None, pattern=''):\n    'Compare the response body with a built in error page.\\n        \\n        The function will optionally look for the regexp pattern,\\n        within the exception embedded in the error page.'\n    page = cherrypy._cperror.get_error_page(status, message=message)\n    esc = re.escape\n    epage = esc(page)\n    epage = epage.replace(esc('<pre id=\"traceback\"></pre>'), ((esc('<pre id=\"traceback\">') + '(.*)') + esc('</pre>')))\n    m = re.match(ntob(epage, self.encoding), self.body, re.DOTALL)\n    if (not m):\n        self._handlewebError(('Error page does not match; expected:\\n' + page))\n        return\n    if (pattern is None):\n        if (m and m.group(1)):\n            self._handlewebError('Error page contains traceback')\n    elif ((m is None) or (not re.search(ntob(re.escape(pattern), self.encoding), m.group(1)))):\n        msg = 'Error page does not contain %s in traceback'\n        self._handlewebError((msg % repr(pattern)))\n", "label": 1}
{"function": "\n\ndef pre_validate(self, form):\n    if self._invalid_formdata:\n        raise ValidationError(self.gettext('Not a valid choice'))\n    elif self.data:\n        obj_list = list((x[1] for x in self._get_object_list()))\n        for v in self.data:\n            if (v not in obj_list):\n                raise ValidationError(self.gettext('Not a valid choice'))\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('SlicePredicate')\n    if (self.column_names != None):\n        oprot.writeFieldBegin('column_names', TType.LIST, 1)\n        oprot.writeListBegin(TType.STRING, len(self.column_names))\n        for iter13 in self.column_names:\n            oprot.writeString(iter13)\n        oprot.writeListEnd()\n        oprot.writeFieldEnd()\n    if (self.slice_range != None):\n        oprot.writeFieldBegin('slice_range', TType.STRUCT, 2)\n        self.slice_range.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n\n    def validate(self):\n        return\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('IndexExpression')\n    if (self.column_name != None):\n        oprot.writeFieldBegin('column_name', TType.STRING, 1)\n        oprot.writeString(self.column_name)\n        oprot.writeFieldEnd()\n    if (self.op != None):\n        oprot.writeFieldBegin('op', TType.I32, 2)\n        oprot.writeI32(self.op)\n        oprot.writeFieldEnd()\n    if (self.value != None):\n        oprot.writeFieldBegin('value', TType.STRING, 3)\n        oprot.writeString(self.value)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n\n    def validate(self):\n        if (self.column_name is None):\n            raise TProtocol.TProtocolException(message='Required field column_name is unset!')\n        if (self.op is None):\n            raise TProtocol.TProtocolException(message='Required field op is unset!')\n        if (self.value is None):\n            raise TProtocol.TProtocolException(message='Required field value is unset!')\n        return\n", "label": 1}
{"function": "\n\ndef get_search_results(self, req, terms, filters):\n    if (not ('milestone' in filters)):\n        return\n    term_regexps = search_to_regexps(terms)\n    milestone_realm = Resource(self.realm)\n    for (name, due, completed, description) in MilestoneCache(self.env).milestones.itervalues():\n        if all(((r.search(description) or r.search(name)) for r in term_regexps)):\n            milestone = milestone_realm(id=name)\n            if ('MILESTONE_VIEW' in req.perm(milestone)):\n                dt = (completed if completed else (due if due else datetime_now(utc)))\n                (yield (get_resource_url(self.env, milestone, req.href), get_resource_name(self.env, milestone), dt, '', shorten_result(description, terms)))\n    for result in AttachmentModule(self.env).get_search_results(req, milestone_realm, terms):\n        (yield result)\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('query_result')\n    if (self.success is not None):\n        oprot.writeFieldBegin('success', TType.STRUCT, 0)\n        self.success.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.error is not None):\n        oprot.writeFieldBegin('error', TType.STRUCT, 1)\n        self.error.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef __exit__(self, ex_type, ex_value, ex_traceback):\n    if (not ex_value):\n        return True\n    if isinstance(ex_value, exception.Forbidden):\n        raise Fault(webob.exc.HTTPForbidden(explanation=ex_value.format_message()))\n    elif isinstance(ex_value, exception.VersionNotFoundForAPIMethod):\n        raise\n    elif isinstance(ex_value, exception.Invalid):\n        raise Fault(exception.ConvertedException(code=ex_value.code, explanation=ex_value.format_message()))\n    elif isinstance(ex_value, TypeError):\n        exc_info = (ex_type, ex_value, ex_traceback)\n        LOG.error(_LE('Exception handling resource: %s'), ex_value, exc_info=exc_info)\n        raise Fault(webob.exc.HTTPBadRequest())\n    elif isinstance(ex_value, Fault):\n        LOG.info(_LI('Fault thrown: %s'), ex_value)\n        raise ex_value\n    elif isinstance(ex_value, webob.exc.HTTPException):\n        LOG.info(_LI('HTTP exception thrown: %s'), ex_value)\n        raise Fault(ex_value)\n    return False\n", "label": 1}
{"function": "\n\ndef count_types(gen):\n    counts = {\n        'TOTAL': 0,\n        'DEBUG': 0,\n        'INFO': 0,\n        'WARNING': 0,\n        'ERROR': 0,\n        'TRACE': 0,\n        'AUDIT': 0,\n    }\n    laststatus = None\n    for line in gen:\n        counts['TOTAL'] = (counts['TOTAL'] + 1)\n        for key in counts:\n            if ((' %s ' % key) in line):\n                laststatus = key\n                continue\n        if laststatus:\n            counts[laststatus] = (counts[laststatus] + 1)\n    return counts\n", "label": 0}
{"function": "\n\ndef __iter__(self):\n    for token in _base.Filter.__iter__(self):\n        if (token['type'] in ('StartTag', 'EmptyTag')):\n            attrs = OrderedDict()\n            for (name, value) in sorted(token['data'].items(), key=(lambda x: x[0])):\n                attrs[name] = value\n            token['data'] = attrs\n        (yield token)\n", "label": 0}
{"function": "\n\ndef check_args(self, f, methods_args):\n    to_check = []\n    args = inspect.getargspec(f)\n    for arg in self.check:\n        if (arg in args[0]):\n            to_check.append(args[0].index(arg))\n    for index in to_check:\n        arg = methods_args[(index - 1)]\n        if self.look_at.cache.get(arg, False):\n            raise FuseOSError(errno.EACCES)\n        if self.look_at.check_key(arg):\n            self.look_at.cache[arg] = True\n            raise FuseOSError(errno.ENOENT)\n        self.look_at.cache[arg] = False\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if isinstance(other, string_types):\n        other = SpecifierSet(other)\n    elif isinstance(other, _IndividualSpecifier):\n        other = SpecifierSet(str(other))\n    elif (not isinstance(other, SpecifierSet)):\n        return NotImplemented\n    return (self._specs == other._specs)\n", "label": 0}
{"function": "\n\ndef _release_savepoint_impl(self, name, context):\n    if (self._has_events or self.engine._has_events):\n        self.dispatch.release_savepoint(self, name, context)\n    if self._still_open_and_connection_is_valid:\n        self.engine.dialect.do_release_savepoint(self, name)\n    self.__transaction = context\n", "label": 0}
{"function": "\n\ndef _eval_is_irrational(self):\n    for t in self.args:\n        a = t.is_irrational\n        if a:\n            others = list(self.args)\n            others.remove(t)\n            if all((((x.is_rational and fuzzy_not(x.is_zero)) is True) for x in others)):\n                return True\n            return\n        if (a is None):\n            return\n    return False\n", "label": 1}
{"function": "\n\n@contextfunction\ndef core_logo_content(context, gif=False):\n    'Return current logo encoded as base64'\n    staticpath = getattr(settings, 'STATIC_DOC_ROOT', './static')\n    logopath = (staticpath + '/logo')\n    if gif:\n        logopath += '.gif'\n        mimetype = 'image/gif'\n    else:\n        logopath += '.png'\n        mimetype = 'image/png'\n    customlogo = ''\n    try:\n        conf = ModuleSetting.get_for_module('treeio.core', 'logopath')[0]\n        customlogo = (getattr(settings, 'MEDIA_ROOT', './static/media') + conf.value)\n    except:\n        pass\n    logofile = ''\n    if customlogo:\n        try:\n            logofile = open(customlogo, 'r')\n        except:\n            pass\n    if (not logofile):\n        try:\n            logofile = open(logopath, 'r')\n        except:\n            pass\n    result = ((('data:' + mimetype) + ';base64,') + base64.b64encode(logofile.read()))\n    return Markup(result)\n", "label": 1}
{"function": "\n\ndef _Exists(self):\n    'Returns true if the cluster exists.'\n    cmd = util.GcloudCommand(self, 'alpha', 'bigtable', 'clusters', 'list')\n    cmd.flags['format'] = 'json'\n    cmd.flags['project'] = self.project\n    cmd.flags['zone'] = []\n    (stdout, stderr, retcode) = cmd.Issue(suppress_warning=True)\n    if (retcode != 0):\n        logging.error('Unable to list GCP Bigtable clusters. Return code %s STDOUT: %s\\nSTDERR: %s', retcode, stdout, stderr)\n        return False\n    result = json.loads(stdout)\n    if ('clusters' not in result):\n        return False\n    clusters = {cluster['name']: cluster for cluster in result['clusters']}\n    expected_cluster_name = 'projects/{0}/zones/{1}/clusters/{2}'.format(self.project, self.zone, self.name)\n    return (expected_cluster_name in clusters)\n", "label": 0}
{"function": "\n\ndef add_endpoint(self, listener):\n    \"\\n        Adds a listening endpoint to be managed by this ION process.\\n\\n        Spawns the listen loop and sets the routing call to synchronize incoming messages\\n        here. If this process hasn't been started yet, adds it to the list of listeners\\n        to start on startup.\\n        \"\n    if self.proc:\n        listener.routing_call = self._routing_call\n        if self.name:\n            svc_name = 'unnamed-service'\n            if ((self.service is not None) and hasattr(self.service, 'name')):\n                svc_name = self.service.name\n            listen_thread_name = ('%s-%s-listen-%s' % (svc_name, self.name, (len(self.listeners) + 1)))\n        else:\n            listen_thread_name = ('unknown-listener-%s' % (len(self.listeners) + 1))\n        gl = self.thread_manager.spawn(listener.listen, thread_name=listen_thread_name)\n        gl.proc._glname = ('ION Proc listener %s' % listen_thread_name)\n        self._listener_map[listener] = gl\n        self.listeners.append(listener)\n    else:\n        self._startup_listeners.append(listener)\n", "label": 0}
{"function": "\n\ndef _tune_workers(self):\n    for (i, w) in enumerate(self._workers):\n        if (not w.is_alive()):\n            del self._workers[i]\n    need_nums = min(self._queue.qsize(), self.MAX_WORKERS)\n    active_nums = len(self._workers)\n    if (need_nums <= active_nums):\n        return\n    for i in range((need_nums - active_nums)):\n        t = threading.Thread(target=self._worker)\n        t.daemon = True\n        t.start()\n        self._workers.append(t)\n", "label": 0}
{"function": "\n\ndef writelines(self, lines):\n    for line in lines:\n        if ((not _PY3) and isinstance(line, unicode)):\n            line = line.encode(self.encoding)\n        msg(line, printed=1, isError=self.isError)\n", "label": 0}
{"function": "\n\ndef plugins(self, *plugins):\n    import json\n    ret = []\n    available_plugins = self.get('system/plugins')\n    self.message['debug']['available_plugins'] = available_plugins\n    plugins = set(plugins)\n    enabled_plugins = set(available_plugins['enabled'])\n    if ('*' in plugins):\n        plugins = set(available_plugins['all'].keys())\n    if (not (plugins <= set(available_plugins['all'].keys()))):\n        self.fail('{}, not available!'.format(','.join(list((plugins - set(available_plugins['all'].keys()))))))\n    if (self.module.params['state'] == 'present'):\n        if (not (plugins <= enabled_plugins)):\n            ret = self.put('system/plugins', {\n                'plugins': json.dumps(list((plugins | enabled_plugins))),\n            })\n            self.changed = True\n    elif (self.module.params['state'] == 'absent'):\n        if len((enabled_plugins & plugins)):\n            ret = self.put('system/plugins', {\n                'plugins': json.dumps(list((enabled_plugins - plugins))),\n            })\n            self.changed = True\n    return ret\n", "label": 1}
{"function": "\n\ndef _compress_hextets(self, hextets):\n    'Compresses a list of hextets.\\n\\n        Compresses a list of strings, replacing the longest continuous\\n        sequence of \"0\" in the list with \"\" and adding empty strings at\\n        the beginning or at the end of the string such that subsequently\\n        calling \":\".join(hextets) will produce the compressed version of\\n        the IPv6 address.\\n\\n        Args:\\n            hextets: A list of strings, the hextets to compress.\\n\\n        Returns:\\n            A list of strings.\\n\\n        '\n    best_doublecolon_start = (- 1)\n    best_doublecolon_len = 0\n    doublecolon_start = (- 1)\n    doublecolon_len = 0\n    for (index, hextet) in enumerate(hextets):\n        if (hextet == '0'):\n            doublecolon_len += 1\n            if (doublecolon_start == (- 1)):\n                doublecolon_start = index\n            if (doublecolon_len > best_doublecolon_len):\n                best_doublecolon_len = doublecolon_len\n                best_doublecolon_start = doublecolon_start\n        else:\n            doublecolon_len = 0\n            doublecolon_start = (- 1)\n    if (best_doublecolon_len > 1):\n        best_doublecolon_end = (best_doublecolon_start + best_doublecolon_len)\n        if (best_doublecolon_end == len(hextets)):\n            hextets += ['']\n        hextets[best_doublecolon_start:best_doublecolon_end] = ['']\n        if (best_doublecolon_start == 0):\n            hextets = ([''] + hextets)\n    return hextets\n", "label": 1}
{"function": "\n\ndef post(self, request, *args, **kwargs):\n    if (self.request.POST['form_type'] == 'location-settings'):\n        return self.settings_form_post(request, *args, **kwargs)\n    elif (self.request.POST['form_type'] == 'location-users'):\n        return self.users_form_post(request, *args, **kwargs)\n    elif ((self.request.POST['form_type'] == 'location-products') and toggles.PRODUCTS_PER_LOCATION.enabled(request.domain)):\n        return self.products_form_post(request, *args, **kwargs)\n    else:\n        raise Http404()\n", "label": 0}
{"function": "\n\ndef define(self, interface):\n    '\\n        Define a handler based on the provided interface.  Defines a handler\\n        type based on ``<interface>.IMeta.label``.\\n\\n        :param interface: The interface class that defines the interface to be\\n            implemented by handlers.\\n        :raises: :class:`cement.core.exc.InterfaceError`\\n        :raises: :class:`cement.core.exc.FrameworkError`\\n\\n        Usage:\\n\\n        .. code-block:: python\\n\\n            app.handler.define(IDatabaseHandler)\\n\\n        '\n    if (not hasattr(interface, 'IMeta')):\n        raise exc.InterfaceError((('Invalid %s, ' % interface) + \"missing 'IMeta' class.\"))\n    if (not hasattr(interface.IMeta, 'label')):\n        raise exc.InterfaceError((('Invalid %s, ' % interface) + \"missing 'IMeta.label' class.\"))\n    LOG.debug((\"defining handler type '%s' (%s)\" % (interface.IMeta.label, interface.__name__)))\n    if (interface.IMeta.label in self.__handlers__):\n        raise exc.FrameworkError((\"Handler type '%s' already defined!\" % interface.IMeta.label))\n    self.__handlers__[interface.IMeta.label] = {\n        '__interface__': interface,\n    }\n", "label": 0}
{"function": "\n\ndef _write_wavelength_range(self, group):\n    if (self.n_wav is None):\n        raise Exception('Wavelength range has not been set')\n    group.attrs['n_wav'] = self.n_wav\n    if (self.wav_min is None):\n        raise Exception('Wavelength minimum has not been set')\n    group.attrs['wav_min'] = self.wav_min\n    if (self.wav_max is None):\n        raise Exception('Wavelength maximum has not been set')\n    group.attrs['wav_max'] = self.wav_max\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.sync_tenant = options.get('tenant')\n    self.sync_public = options.get('shared')\n    self.schema_name = options.get('schema_name')\n    self.executor = options.get('executor')\n    self.installed_apps = settings.INSTALLED_APPS\n    self.args = args\n    self.options = options\n    if self.schema_name:\n        if self.sync_public:\n            raise CommandError('schema should only be used with the --tenant switch.')\n        elif (self.schema_name == get_public_schema_name()):\n            self.sync_public = True\n        else:\n            self.sync_tenant = True\n    elif ((not self.sync_public) and (not self.sync_tenant)):\n        self.sync_tenant = True\n        self.sync_public = True\n    if hasattr(settings, 'TENANT_APPS'):\n        self.tenant_apps = settings.TENANT_APPS\n    if hasattr(settings, 'SHARED_APPS'):\n        self.shared_apps = settings.SHARED_APPS\n", "label": 1}
{"function": "\n\ndef delete_access_list_items(self, loadbalancer, item_ids):\n    \"\\n        Removes the item(s) from the load balancer's access list\\n        that match the provided IDs. 'item_ids' should be one or\\n        more access list item IDs.\\n        \"\n    if (not isinstance(item_ids, (list, tuple))):\n        item_ids = [item_ids]\n    valid_ids = [itm['id'] for itm in self.get_access_list(loadbalancer)]\n    bad_ids = [str(itm) for itm in item_ids if (itm not in valid_ids)]\n    if bad_ids:\n        raise exc.AccessListIDNotFound(('The following ID(s) are not valid Access List items: %s' % ', '.join(bad_ids)))\n    items = '&'.join([('id=%s' % item_id) for item_id in item_ids])\n    uri = ('/loadbalancers/%s/accesslist?%s' % (utils.get_id(loadbalancer), items))\n    (resp, body) = self.api.method_delete(uri)\n    return body\n", "label": 1}
{"function": "\n\ndef gcd(self, other):\n    'Return Factors of ``gcd(self, other)``. The keys are\\n        the intersection of factors with the minimum exponent for\\n        each factor.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.core.exprtools import Factors\\n        >>> from sympy.abc import x, y, z\\n        >>> a = Factors((x*y**2).as_powers_dict())\\n        >>> b = Factors((x*y/z).as_powers_dict())\\n        >>> a.gcd(b)\\n        Factors({x: 1, y: 1})\\n        '\n    if (not isinstance(other, Factors)):\n        other = Factors(other)\n        if other.is_zero:\n            return Factors(self.factors)\n    factors = {\n        \n    }\n    for (factor, exp) in self.factors.items():\n        (factor, exp) = (sympify(factor), sympify(exp))\n        if (factor in other.factors):\n            lt = (exp - other.factors[factor]).is_negative\n            if (lt == True):\n                factors[factor] = exp\n            elif (lt == False):\n                factors[factor] = other.factors[factor]\n    return Factors(factors)\n", "label": 1}
{"function": "\n\ndef GetDisplayHTML(self):\n    if (self.xPolynomialOrder == None):\n        self._HTML = 'z = user-selectable polynomial'\n    else:\n        self._HTML = 'z = '\n        cd = self.GetCoefficientDesignators()\n        indexmax = ((self.xPolynomialOrder + 1) * (self.yPolynomialOrder + 1))\n        for i in range((self.xPolynomialOrder + 1)):\n            for j in range((self.yPolynomialOrder + 1)):\n                index = ((i * (self.yPolynomialOrder + 1)) + j)\n                if (index == 0):\n                    self._HTML += cd[index]\n                else:\n                    self._HTML += (((((cd[index] + 'x<SUP>') + str(i)) + '</SUP>y<SUP>') + str(j)) + '</SUP>')\n                if (((i + 1) * (j + 1)) != indexmax):\n                    self._HTML += ' + '\n    return self.extendedVersionHandler.AssembleDisplayHTML(self)\n", "label": 1}
{"function": "\n\ndef env(target_key, entity, *_):\n    if (not target_key):\n        (yield entity.env)\n    for extra in entity.env.extras:\n        if (target_key in getattr(extra, 'target_keys', ())):\n            (yield extra)\n        for target in recursive_targets(getattr(extra, 'target_providers', ()), target_key):\n            (yield target)\n", "label": 0}
{"function": "\n\ndef ControllerMotionKernelPeriodMinMaxGet(self, socketId):\n    command = 'ControllerMotionKernelPeriodMinMaxGet(double *,double *,double *,double *,double *,double *)'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(6):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef estimate_scale(self):\n    '\\n        Returns an estimate of the scale parameter at the current\\n        parameter value.\\n        '\n    if isinstance(self.family, (families.Binomial, families.Poisson, _Multinomial)):\n        return 1.0\n    endog = self.endog_li\n    exog = self.exog_li\n    cached_means = self.cached_means\n    nobs = self.nobs\n    varfunc = self.family.variance\n    scale = 0.0\n    fsum = 0.0\n    for i in range(self.num_group):\n        if (len(endog[i]) == 0):\n            continue\n        (expval, _) = cached_means[i]\n        f = (self.weights_li[i] if (self.weights is not None) else 1.0)\n        sdev = np.sqrt(varfunc(expval))\n        resid = ((endog[i] - expval) / sdev)\n        scale += (f * np.sum((resid ** 2)))\n        fsum += (f * len(endog[i]))\n    scale /= ((fsum * (nobs - self.ddof_scale)) / float(nobs))\n    return scale\n", "label": 0}
{"function": "\n\ndef get_args_without_osxims():\n    new_argv = []\n    i = 1\n    while (i < len(sys.argv)):\n        arg = sys.argv[i]\n        next_arg = (sys.argv[(i + 1)] if ((i + 1) < len(sys.argv)) else None)\n        if ((arg == '-isysroot') and ('SDKs/MacOSX' in next_arg)):\n            i = (i + 2)\n        elif ('-mmacosx-version-min' in arg):\n            i = (i + 1)\n        elif (arg == '-F/Applications/Xcode.app/Contents/Developer/Library/Frameworks'):\n            i = (i + 1)\n        else:\n            i = (i + 1)\n            new_argv.append(arg)\n    return new_argv\n", "label": 1}
{"function": "\n\ndef deliver(self, message, timeout=None):\n    \"Must be used with 'yield' as 'yield coro.deliver(message)'.\\n\\n        Can also be used on remotely running coroutines.\\n\\n        Return value indicates status of delivering the message: If it\\n        is 1, then message has been delivered, if it is 0, it couldn't\\n        be delivered before timeout, and if it is < 0, then the\\n        (remote) coroutine is not valid.\\n        \"\n    if (self._location == Coro._asyncoro._location):\n        reply = Coro._asyncoro._resume(self, message, AsynCoro._AwaitMsg_)\n        if (reply == 0):\n            reply = 1\n    else:\n        request = _NetRequest('deliver', kwargs={\n            'coro': self._id,\n            'message': message,\n        }, dst=self._location, timeout=timeout)\n        request.reply = (- 1)\n        reply = (yield Coro._asyncoro._sync_reply(request, alarm_value=0))\n        if (reply is None):\n            reply = (- 1)\n    raise StopIteration(reply)\n", "label": 0}
{"function": "\n\ndef GroupPositionSetpointGet(self, socketId, GroupName, nbElement):\n    command = (('GroupPositionSetpointGet(' + GroupName) + ',')\n    for i in range(nbElement):\n        if (i > 0):\n            command += ','\n        command += 'double *'\n    command += ')'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(nbElement):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 1}
{"function": "\n\ndef matches(self, elem):\n    if (not self.matchesPredicates(elem)):\n        return 0\n    if (self.childLocation != None):\n        for c in elem.elements():\n            if self.childLocation.matches(c):\n                return 1\n    else:\n        return 1\n    return 0\n", "label": 0}
{"function": "\n\ndef __new__(cls, j, m):\n    j = sympify(j)\n    m = sympify(m)\n    if j.is_number:\n        if ((2 * j) != int((2 * j))):\n            raise ValueError(('j must be integer or half-integer, got: %s' % j))\n        if (j < 0):\n            raise ValueError(('j must be >= 0, got: %s' % j))\n    if m.is_number:\n        if ((2 * m) != int((2 * m))):\n            raise ValueError(('m must be integer or half-integer, got: %s' % m))\n    if (j.is_number and m.is_number):\n        if (abs(m) > j):\n            raise ValueError(('Allowed values for m are -j <= m <= j, got j, m: %s, %s' % (j, m)))\n        if (int((j - m)) != (j - m)):\n            raise ValueError(('Both j and m must be integer or half-integer, got j, m: %s, %s' % (j, m)))\n    return State.__new__(cls, j, m)\n", "label": 1}
{"function": "\n\n@policy.setter\ndef policy(self, args):\n    '\\n        setter for the policy descriptor\\n        '\n    word = args[0]\n    if (word == 'reject'):\n        self.accepted_ports = None\n        self.rejected_ports = []\n        target = self.rejected_ports\n    elif (word == 'accept'):\n        self.accepted_ports = []\n        self.rejected_ports = None\n        target = self.accepted_ports\n    else:\n        raise RuntimeError(('Don\\'t understand policy word \"%s\"' % word))\n    for port in args[1].split(','):\n        if ('-' in port):\n            (a, b) = port.split('-')\n            target.append(PortRange(int(a), int(b)))\n        else:\n            target.append(int(port))\n", "label": 0}
{"function": "\n\ndef unique_everseen(iterable, key=None):\n    'List unique elements, preserving order. Remember all elements ever seen.'\n    seen = set()\n    seen_add = seen.add\n    if (key is None):\n        for element in ifilterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            (yield element)\n    else:\n        for element in iterable:\n            k = key(element)\n            if (k not in seen):\n                seen_add(k)\n                (yield element)\n", "label": 0}
{"function": "\n\ndef __updateWidth(self):\n    charWidth = None\n    if (self.getPlug() is not None):\n        charWidth = Gaffer.Metadata.plugValue(self.getPlug(), 'numericPlugValueWidget:fixedCharacterWidth')\n    if ((charWidth is None) and isinstance(self.getPlug(), Gaffer.IntPlug) and self.getPlug().hasMaxValue()):\n        charWidth = len(str(self.getPlug().maxValue()))\n    self.__numericWidget.setFixedCharacterWidth(charWidth)\n", "label": 0}
{"function": "\n\ndef get_previous_link(self):\n    if (not self.has_previous):\n        return None\n    if (self.cursor and (not self.cursor.reverse) and (self.cursor.offset != 0)):\n        compare = self._get_position_from_instance(self.page[0], self.ordering)\n    else:\n        compare = self.previous_position\n    offset = 0\n    for item in self.page:\n        position = self._get_position_from_instance(item, self.ordering)\n        if (position != compare):\n            break\n        compare = position\n        offset += 1\n    else:\n        if (not self.has_next):\n            offset = self.page_size\n            position = None\n        elif self.cursor.reverse:\n            offset = (self.cursor.offset + self.page_size)\n            position = self.next_position\n        else:\n            offset = 0\n            position = self.next_position\n    cursor = Cursor(offset=offset, reverse=True, position=position)\n    return self.encode_cursor(cursor)\n", "label": 1}
{"function": "\n\ndef summary_cdf(self, idx, frac, crit, varnames=None, title=None):\n    'summary table for cumulative density function\\n\\n\\n        Parameters\\n        ----------\\n        idx : None or list of integers\\n            List of indices into the Monte Carlo results (columns) that should\\n            be used in the calculation\\n        frac : array_like, float\\n            probabilities for which\\n        crit : array_like\\n            values for which cdf is calculated\\n        varnames : None, or list of strings\\n            optional list of variable names, same length as idx\\n\\n        Returns\\n        -------\\n        table : instance of SimpleTable\\n            use `print(table` to see results\\n\\n\\n        '\n    idx = np.atleast_1d(idx)\n    mml = []\n    for i in range(len(idx)):\n        mml.append(self.cdf(crit[:, i], [idx[i]])[1].ravel())\n    mmlar = np.column_stack(([frac] + mml))\n    if title:\n        title = (title + ' Probabilites')\n    else:\n        title = 'Probabilities'\n    if (varnames is None):\n        varnames = [('var%d' % i) for i in range((mmlar.shape[1] - 1))]\n    headers = (['prob'] + varnames)\n    return SimpleTable(mmlar, txt_fmt={\n        'data_fmts': (['%#6.3f'] + (['%#10.4f'] * (np.array(mml).shape[1] - 1))),\n    }, title=title, headers=headers)\n", "label": 0}
{"function": "\n\ndef switch_to_correct_strategy(self, w_dict, w_key):\n    if (type(w_key) is values.W_Fixnum):\n        strategy = FixnumHashmapStrategy.singleton\n    elif (type(w_key) is values.W_Symbol):\n        strategy = SymbolHashmapStrategy.singleton\n    elif isinstance(w_key, values_string.W_String):\n        strategy = StringHashmapStrategy.singleton\n    elif isinstance(w_key, values.W_Bytes):\n        strategy = ByteHashmapStrategy.singleton\n    else:\n        strategy = ObjectHashmapStrategy.singleton\n    storage = strategy.create_storage([], [])\n    w_dict.strategy = strategy\n    w_dict.hstorage = storage\n", "label": 0}
{"function": "\n\ndef visit_table(self, table, asfrom=False, iscrud=False, ashint=False, fromhints=None, **kwargs):\n    if (asfrom or ashint):\n        if getattr(table, 'schema', None):\n            ret = ((self.preparer.quote_schema(table.schema) + '.') + self.preparer.quote(table.name))\n        else:\n            ret = self.preparer.quote(table.name)\n        if (fromhints and (table in fromhints)):\n            ret = self.format_from_hint_text(ret, table, fromhints[table], iscrud)\n        return ret\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef __init__(self, typename=None, critical=None, value=None, subject=None, issuer=None, _ext=None):\n    if (_ext is not None):\n        ext = _ext\n    elif ((subject is None) and (issuer is None)):\n        ext = crypto.X509Extension(typename, critical, value)\n    elif ((subject is not None) and (issuer is None)):\n        subject = subject._cert\n        ext = crypto.X509Extension(typename, critical, value, subject=subject)\n    elif ((subject is None) and (issuer is not None)):\n        issuer = issuer._cert\n        ext = crypto.X509Extension(typename, critical, value, issuer=issuer)\n    elif ((subject is not None) and (issuer is not None)):\n        issuer = issuer._cert\n        ext = crypto.X509Extension(typename, critical, value, subject=subject, issuer=issuer)\n    self._ext = ext\n", "label": 1}
{"function": "\n\n@count_calls\ndef update_versions(self, reference_resolution):\n    'A mock update_versions implementation, does the update indeed but\\n        partially.\\n\\n        :param reference_resolution: The reference_resolution dictionary\\n        :return: a list of new versions\\n        '\n    new_versions = []\n    from stalker import Version\n    current_version = self.get_current_version()\n    for version in reference_resolution['create']:\n        local_reference_resolution = self.open(version, force=True)\n        new_version = Version(task=version.task, take_name=version.take_name, parent=version, description='Automatically created with Deep Reference Update')\n        new_version.is_published = True\n        for v in self._version.inputs:\n            new_version.inputs.append(v.latest_published_version)\n        new_versions.append(new_version)\n    current_version_after_create = self.get_current_version()\n    if current_version:\n        if (current_version != current_version_after_create):\n            self.open(current_version)\n        reference_resolution['update'].extend(reference_resolution['create'])\n        self.update_first_level_versions(reference_resolution)\n    return new_versions\n", "label": 0}
{"function": "\n\ndef scan(self, result):\n    if result.valid:\n        if result.description.startswith('ASCII cpio archive'):\n            self.consecutive_hits += 1\n            if ((not self.found_archive) or (self.found_archive_in_file != result.file.name)):\n                self.found_archive_in_file = result.file.name\n                self.found_archive = True\n                result.extract = True\n            elif ('TRAILER!!!' in result.description):\n                self.found_archive = False\n                result.extract = False\n                self.consecutive_hits = 0\n            else:\n                result.extract = False\n        elif (self.consecutive_hits < 4):\n            self.found_archive = False\n            self.found_archive_in_file = None\n            self.consecutive_hits = 0\n        elif (self.consecutive_hits >= 4):\n            result.valid = False\n", "label": 1}
{"function": "\n\ndef _visit_call_helper_list(self, node):\n    name = self.visit(node.func)\n    if node.args:\n        args = [self.visit(e) for e in node.args]\n        args = ', '.join([e for e in args if e])\n    else:\n        args = '[]'\n    return ('%s(%s)' % (name, args))\n", "label": 0}
{"function": "\n\ndef _get_lock_path(name, lock_file_prefix, lock_path=None):\n    name = name.replace(os.sep, '_')\n    if lock_file_prefix:\n        sep = ('' if lock_file_prefix.endswith('-') else '-')\n        name = ('%s%s%s' % (lock_file_prefix, sep, name))\n    local_lock_path = (lock_path or CONF.oslo_concurrency.lock_path)\n    if (not local_lock_path):\n        raise cfg.RequiredOptError('lock_path')\n    return os.path.join(local_lock_path, name)\n", "label": 0}
{"function": "\n\ndef perform_search(self, dir, s=None, start=None, update_search_start=False):\n    self.cancel_highlight()\n    if (s is None):\n        s = self.last_search_string\n        if (s is None):\n            self.ui.message('No previous search term.')\n            return False\n    else:\n        self.last_search_string = s\n    if (start is None):\n        start = self.search_start\n    case_insensitive = (s.lower() == s)\n    if (start > len(self.ui.source)):\n        start = 0\n    i = ((start + dir) % len(self.ui.source))\n    if (i >= len(self.ui.source)):\n        i = 0\n    while (i != start):\n        sline = self.ui.source[i].text\n        if case_insensitive:\n            sline = sline.lower()\n        if (s in sline):\n            sl = self.ui.source[i]\n            sl.set_highlight(True)\n            self.highlight_line = sl\n            self.ui.source.set_focus(i)\n            if update_search_start:\n                self.search_start = i\n            return True\n        i = ((i + dir) % len(self.ui.source))\n    return False\n", "label": 1}
{"function": "\n\n@float_format.setter\ndef float_format(self, val):\n    if ((val is None) or (isinstance(val, dict) and (len(val) is 0))):\n        self._float_format = {\n            \n        }\n    else:\n        self._validate_option('float_format', val)\n        for field in self._field_names:\n            self._float_format[field] = val\n", "label": 0}
{"function": "\n\ndef __resolve_options(ctx, value):\n    'Resolve the given value to JipUndefined or, if its not\\n    an option, try to find one if a tool is associated with the context\\n\\n    :param ctx: the context\\n    :param value: the source value\\n    :returns: resolved or as is\\n    '\n    if isinstance(value, JipUndefined):\n        value = value._undefined_name\n    if (not isinstance(value, Option)):\n        script = ctx.get('tool', None)\n        if script:\n            v = script.options[value]\n            if (v is not None):\n                value = v\n    return value\n", "label": 0}
{"function": "\n\ndef answers(self, other):\n    if (other.id == self.id):\n        if (self.code == 1):\n            return 1\n        if ((other.code in [2, 4, 6, 8, 10]) and (self.code == (other.code + 1))):\n            if (other.code == 8):\n                return 1\n            return self.payload.answers(other.payload)\n    return 0\n", "label": 1}
{"function": "\n\ndef handle_children_state(children, kids):\n    'Given a list of children (as `children`) of a particular object\\n    and their states in the `kids` argument, this function sets up the\\n    children by removing unnecessary ones, fixing existing ones and\\n    adding new children if necessary (depending on the state).\\n    '\n    m_children = list(children)\n    (n_child, n_kid) = (len(m_children), len(kids))\n    for i in range((n_child - n_kid)):\n        m_children.pop()\n    for i in range(n_child):\n        (child, kid) = (m_children[i], kids[i])\n        md = kid.__metadata__\n        if ((child.__module__ != md['module']) or (child.__class__.__name__ != md['class_name'])):\n            m_children[i] = create_instance(kid)\n    for i in range((n_kid - n_child)):\n        child = create_instance(kids[(n_child + i)])\n        m_children.append(child)\n    children[:] = m_children\n", "label": 1}
{"function": "\n\ndef test_random_udir_noweights(self):\n    if (not use_networkx):\n        sys.stderr.write('Skipping TestBetweenessCentrality.test_random_udir_noweights due to missing networkx library\\n')\n        return\n    Gnx = networkx.Graph()\n    G = Graph()\n    nodes = range(500)\n    random.shuffle(nodes)\n    while (len(nodes) > 0):\n        n1 = nodes.pop()\n        n2 = nodes.pop()\n        w = random.randint(1, 5)\n        eidx = G.add_edge(n1, n2)\n        G.set_weight_(eidx, w)\n        Gnx.add_edge(n1, n2, weight=w)\n    nodes = range(500)\n    random.shuffle(nodes)\n    while (len(nodes) > 0):\n        n1 = nodes.pop()\n        n2 = nodes.pop()\n        if (not G.has_edge(n1, n2)):\n            w = random.randint(1, 5)\n            eidx = G.add_edge(n1, n2)\n            G.set_weight_(eidx, w)\n            Gnx.add_edge(n1, n2, weight=w)\n    R = betweenness_centrality_(G, True, False)\n    Rnx = networkx.betweenness_centrality(Gnx, normalized=True, weight=None)\n    for n in G.nodes_iter():\n        self.assertAlmostEqual(Rnx[n], R[G.node_idx(n)])\n", "label": 1}
{"function": "\n\ndef next(self):\n    if ((self.length is not None) and (self.length <= 0)):\n        raise StopIteration\n    chunk = self.fileobj.read(self.chunk_size)\n    if (not chunk):\n        raise StopIteration\n    if (self.length is not None):\n        self.length -= len(chunk)\n        if (self.length < 0):\n            chunk = chunk[:self.length]\n    return chunk\n", "label": 1}
{"function": "\n\ndef find_node(self, node, path):\n    'Finds a node by the given path from the given node.'\n    for hash_value in path:\n        if isinstance(node, LeafStatisticsNode):\n            break\n        for stats in node.get_child_keys():\n            if (hash(stats) == hash_value):\n                node = node.get_child_node(stats)\n                break\n        else:\n            break\n    return node\n", "label": 1}
{"function": "\n\ndef create_with_kwargs(self, context, **kwargs):\n    volume_id = kwargs.get('volume_id', None)\n    v = fake_volume(kwargs['size'], kwargs['name'], kwargs['description'], str(volume_id), None, None, None, None)\n    if (kwargs.get('status', None) is not None):\n        v.vol['status'] = kwargs['status']\n    if (kwargs['host'] is not None):\n        v.vol['host'] = kwargs['host']\n    if (kwargs['attach_status'] is not None):\n        v.vol['attach_status'] = kwargs['attach_status']\n    if (kwargs.get('snapshot_id', None) is not None):\n        v.vol['snapshot_id'] = kwargs['snapshot_id']\n    self.volume_list.append(v.vol)\n    return v.vol\n", "label": 0}
{"function": "\n\ndef createAES(key, IV, implList=None):\n    'Create a new AES object.\\n\\n    @type key: str\\n    @param key: A 16, 24, or 32 byte string.\\n\\n    @type IV: str\\n    @param IV: A 16 byte string\\n\\n    @rtype: L{tlslite.utils.AES}\\n    @return: An AES object.\\n    '\n    if (implList == None):\n        implList = ['openssl', 'pycrypto', 'python']\n    for impl in implList:\n        if ((impl == 'openssl') and cryptomath.m2cryptoLoaded):\n            return openssl_aes.new(key, 2, IV)\n        elif ((impl == 'pycrypto') and cryptomath.pycryptoLoaded):\n            return pycrypto_aes.new(key, 2, IV)\n        elif (impl == 'python'):\n            return python_aes.new(key, 2, IV)\n    raise NotImplementedError()\n", "label": 1}
{"function": "\n\ndef on_modified(self, view):\n    if view.settings().get('is_widget'):\n        return\n    if (not view.settings().get('auto_wrap', False)):\n        return\n    if (not self.check_selection(view)):\n        return\n    insertpt = self.get_insert_pt(view)\n    if (not insertpt):\n        return\n    self.set_status()\n    join = (self.status >= 2)\n    left_delete = (' ' not in view.substr(sublime.Region((insertpt - 1), (insertpt + 1))))\n    view.settings().set('auto_wrap', False)\n    view.run_command('auto_wrap_insert', {\n        'insertpt': insertpt,\n        'join': join,\n        'left_delete': self.left_delete,\n    })\n    self.left_delete = left_delete\n    view.settings().set('auto_wrap', True)\n", "label": 0}
{"function": "\n\ndef block_average_above(x, y, new_x):\n    '\\n    Linearly interpolates values in new_x based on the values in x and y.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        Independent values.\\n    y : array_like\\n        Dependent values.\\n    new_x : array_like\\n        The x values to interpolate y values.\\n\\n    '\n    bad_index = None\n    x = atleast_1d_and_contiguous(x, np.float64)\n    y = atleast_1d_and_contiguous(y, np.float64)\n    new_x = atleast_1d_and_contiguous(new_x, np.float64)\n    if (y.ndim > 2):\n        raise ValueError('`linear` only works with 1-D or 2-D arrays.')\n    if (len(y.shape) == 2):\n        new_y = np.zeros((y.shape[0], len(new_x)), np.float64)\n        for i in range(len(new_y)):\n            bad_index = _interpolate.block_averave_above_dddd(x, y[i], new_x, new_y[i])\n            if (bad_index is not None):\n                break\n    else:\n        new_y = np.zeros(len(new_x), np.float64)\n        bad_index = _interpolate.block_average_above_dddd(x, y, new_x, new_y)\n    if (bad_index is not None):\n        msg = ('block_average_above cannot extrapolate and new_x[%d]=%f is out of the x range (%f, %f)' % (bad_index, new_x[bad_index], x[0], x[(- 1)]))\n        raise ValueError(msg)\n    return new_y\n", "label": 1}
{"function": "\n\ndef get_flinalg_funcs(names, arrays=(), debug=0):\n    'Return optimal available _flinalg function objects with\\n    names. arrays are used to determine optimal prefix.'\n    ordering = []\n    for i in range(len(arrays)):\n        t = arrays[i].dtype.char\n        if (t not in _type_conv):\n            t = 'd'\n        ordering.append((t, i))\n    if ordering:\n        ordering.sort()\n        required_prefix = _type_conv[ordering[0][0]]\n    else:\n        required_prefix = 'd'\n    if (ordering and has_column_major_storage(arrays[ordering[0][1]])):\n        (suffix1, suffix2) = ('_c', '_r')\n    else:\n        (suffix1, suffix2) = ('_r', '_c')\n    funcs = []\n    for name in names:\n        func_name = (required_prefix + name)\n        func = getattr(_flinalg, (func_name + suffix1), getattr(_flinalg, (func_name + suffix2), None))\n        funcs.append(func)\n    return tuple(funcs)\n", "label": 1}
{"function": "\n\ndef maximalSquare(self, matrix):\n    if (not matrix):\n        return 0\n    (m, n) = (len(matrix), len(matrix[0]))\n    size = [[0 for j in xrange(n)] for i in xrange(2)]\n    max_size = 0\n    for j in xrange(n):\n        if (matrix[0][j] == '1'):\n            size[0][j] = 1\n        max_size = max(max_size, size[0][j])\n    for i in xrange(1, m):\n        if (matrix[i][0] == '1'):\n            size[(i % 2)][0] = 1\n        else:\n            size[(i % 2)][0] = 0\n        for j in xrange(1, n):\n            if (matrix[i][j] == '1'):\n                size[(i % 2)][j] = (min(size[(i % 2)][(j - 1)], size[((i - 1) % 2)][j], size[((i - 1) % 2)][(j - 1)]) + 1)\n                max_size = max(max_size, size[(i % 2)][j])\n            else:\n                size[(i % 2)][j] = 0\n    return (max_size * max_size)\n", "label": 1}
{"function": "\n\n@utils.enforce_id_param\ndef edit_playlist(self, playlist_id, new_name=None, new_description=None, public=None):\n    'Changes the name of a playlist and returns its id.\\n\\n        :param playlist_id: the id of the playlist\\n        :param new_name: (optional) desired title\\n        :param new_description: (optional) desired description\\n        :param public: (optional) if True and the user has a subscription, share playlist.\\n        '\n    if all(((value is None) for value in (new_name, new_description, public))):\n        raise ValueError('new_name, new_description, or public must be provided')\n    if (public is None):\n        share_state = public\n    else:\n        share_state = ('PUBLIC' if public else 'PRIVATE')\n    mutate_call = mobileclient.BatchMutatePlaylists\n    update_mutations = mutate_call.build_playlist_updates([{\n        'id': playlist_id,\n        'name': new_name,\n        'description': new_description,\n        'public': share_state,\n    }])\n    res = self._make_call(mutate_call, update_mutations)\n    return res['mutate_response'][0]['id']\n", "label": 0}
{"function": "\n\ndef sql_flush(self, style, tables, sequences):\n    if tables:\n        if ((self.postgres_version[0] >= 8) and (self.postgres_version[1] >= 1)):\n            sql = [('%s %s;' % (style.SQL_KEYWORD('TRUNCATE'), style.SQL_FIELD(', '.join([self.quote_name(table) for table in tables]))))]\n        else:\n            sql = [('%s %s %s;' % (style.SQL_KEYWORD('DELETE'), style.SQL_KEYWORD('FROM'), style.SQL_FIELD(self.quote_name(table)))) for table in tables]\n        for sequence_info in sequences:\n            table_name = sequence_info['table']\n            column_name = sequence_info['column']\n            if (column_name and (len(column_name) > 0)):\n                sequence_name = ('%s_%s_seq' % (table_name, column_name))\n            else:\n                sequence_name = ('%s_id_seq' % table_name)\n            sql.append((\"%s setval('%s', 1, false);\" % (style.SQL_KEYWORD('SELECT'), style.SQL_FIELD(self.quote_name(sequence_name)))))\n        return sql\n    else:\n        return []\n", "label": 1}
{"function": "\n\ndef expandtargets(self, source_glob, target):\n    if ((not isdirname(target)) and (('*' in source_glob) or ('?' in source_glob))):\n        raise SymlinkError(\"Invalid symlink: {0} => {1} (did you mean to add a trailing '/'?)\".format(source_glob, target))\n    mkdir_p(os.path.dirname(os.path.expanduser(target)))\n    sources = iglob(os.path.join(self.symlink_dir, source_glob))\n    expanded = []\n    for source in sources:\n        source = os.path.join(self.cider_dir, source)\n        source_target = self.expandtarget(source, target)\n        expanded.append((source, source_target))\n    return expanded\n", "label": 0}
{"function": "\n\ndef _topological_sort(self, goal_info_by_goal):\n    dependees_by_goal = OrderedDict()\n\n    def add_dependee(goal, dependee=None):\n        dependees = dependees_by_goal.get(goal)\n        if (dependees is None):\n            dependees = set()\n            dependees_by_goal[goal] = dependees\n        if dependee:\n            dependees.add(dependee)\n    for (goal, goal_info) in goal_info_by_goal.items():\n        add_dependee(goal)\n        for dependency in goal_info.goal_dependencies:\n            add_dependee(dependency, goal)\n    satisfied = set()\n    while dependees_by_goal:\n        count = len(dependees_by_goal)\n        for (goal, dependees) in dependees_by_goal.items():\n            unsatisfied = len((dependees - satisfied))\n            if (unsatisfied == 0):\n                satisfied.add(goal)\n                dependees_by_goal.pop(goal)\n                (yield goal_info_by_goal[goal])\n                break\n        if (len(dependees_by_goal) == count):\n            for dependees in dependees_by_goal.values():\n                dependees.difference_update(satisfied)\n            raise self.GoalCycleError('Cycle detected in goal dependencies:\\n\\t{0}'.format('\\n\\t'.join(('{0} <- {1}'.format(goal, list(dependees)) for (goal, dependees) in dependees_by_goal.items()))))\n", "label": 1}
{"function": "\n\ndef overrules(self, other):\n    '\\n        Detects if the other index is a non-unique, non primary index\\n        that can be overwritten by this one.\\n\\n        :param other: The other index\\n        :type other: Index\\n\\n        :rtype: bool\\n        '\n    if other.is_primary():\n        return False\n    elif (self.is_simple_index() and other.is_unique()):\n        return False\n    same_columns = self.spans_columns(other.get_columns())\n    if (same_columns and (self.is_primary() or self.is_unique()) and self.same_partial_index(other)):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    if isinstance(key, six.string_types):\n        if (self._index_key is not None):\n            for element in self:\n                if (getattr(element, self._index_key) == key):\n                    return element\n        raise KeyError(key)\n    else:\n        return self._elements[key]\n", "label": 0}
{"function": "\n\ndef resolve_columns(self, row, fields=()):\n    '\\n        This routine is necessary so that distances and geometries returned\\n        from extra selection SQL get resolved appropriately into Python\\n        objects.\\n        '\n    values = []\n    aliases = self.extra_select.keys()\n    if self.aggregates:\n        aliases.extend([None for i in xrange(len(self.aggregates))])\n    rn_offset = 0\n    if SpatialBackend.oracle:\n        if ((self.high_mark is not None) or self.low_mark):\n            rn_offset = 1\n    index_start = (rn_offset + len(aliases))\n    values = [self.convert_values(v, self.extra_select_fields.get(a, None)) for (v, a) in izip(row[rn_offset:index_start], aliases)]\n    if (SpatialBackend.oracle or getattr(self, 'geo_values', False)):\n        for (value, field) in izip(row[index_start:], fields):\n            values.append(self.convert_values(value, field))\n    else:\n        values.extend(row[index_start:])\n    return tuple(values)\n", "label": 1}
{"function": "\n\n@utils.synchronized('dhcp-agent')\ndef port_update_end(self, context, payload):\n    'Handle the port.update.end notification event.'\n    updated_port = dhcp.DictModel(payload['port'])\n    network = self.cache.get_network_by_id(updated_port.network_id)\n    if network:\n        LOG.info(_LI('Trigger reload_allocations for port %s'), updated_port)\n        driver_action = 'reload_allocations'\n        if self._is_port_on_this_agent(updated_port):\n            orig = self.cache.get_port_by_id(updated_port['id'])\n            old_ips = {i['ip_address'] for i in (orig['fixed_ips'] or [])}\n            new_ips = {i['ip_address'] for i in updated_port['fixed_ips']}\n            if (old_ips != new_ips):\n                driver_action = 'restart'\n        self.cache.put_port(updated_port)\n        self.call_driver(driver_action, network)\n", "label": 1}
{"function": "\n\ndef change_subscription(self, topics):\n    'Change the topic subscription.\\n\\n        Arguments:\\n            topics (list of str): topics for subscription\\n\\n        Raises:\\n            IllegalStateErrror: if assign_from_user has been used already\\n        '\n    if self._user_assignment:\n        raise IllegalStateError(self._SUBSCRIPTION_EXCEPTION_MESSAGE)\n    if (self.subscription == set(topics)):\n        log.warning('subscription unchanged by change_subscription(%s)', topics)\n        return\n    log.info('Updating subscribed topics to: %s', topics)\n    self.subscription = set(topics)\n    self._group_subscription.update(topics)\n    self.needs_partition_assignment = True\n    for tp in set(self.assignment.keys()):\n        if (tp.topic not in self.subscription):\n            del self.assignment[tp]\n", "label": 0}
{"function": "\n\ndef get_next_url(request):\n    'Given a request object, looks for the best possible next URL.\\n\\n    Useful for e.g. redirects back to original page after a POST request.\\n\\n    '\n    if ('next' in request.POST):\n        url = request.POST.get('next')\n    elif ('next' in request.GET):\n        url = request.GET.get('next')\n    else:\n        url = request.META.get('HTTP_REFERER')\n    if ((not settings.DEBUG) and (not is_safe_url(url, Site.objects.get_current().domain))):\n        return None\n    return url\n", "label": 0}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    get_hbase_service_config(args)\n    for job_name in (args.job or reversed(ALL_JOBS)):\n        hosts = args.hbase_config.jobs[job_name].hosts\n        task_list = deploy_utils.schedule_task_for_threads(args, hosts, job_name, 'stop')\n        parallel_deploy.start_deploy_threads(stop_job, task_list)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.hbase_config.jobs[job_name].hosts\n        task_list = deploy_utils.schedule_task_for_threads(args, hosts, job_name, 'start', is_wait=True)\n        parallel_deploy.start_deploy_threads(start_job, task_list)\n", "label": 1}
{"function": "\n\ndef split_shard(self, stream_name, shard_to_split, new_starting_hash_key):\n    stream = self.describe_stream(stream_name)\n    if (shard_to_split not in stream.shards):\n        raise ResourceNotFoundError(shard_to_split)\n    if (not re.match('0|([1-9]\\\\d{0,38})', new_starting_hash_key)):\n        raise InvalidArgumentError(new_starting_hash_key)\n    new_starting_hash_key = int(new_starting_hash_key)\n    shard = stream.shards[shard_to_split]\n    last_id = sorted(stream.shards.values(), key=attrgetter('_shard_id'))[(- 1)]._shard_id\n    if (shard.starting_hash < new_starting_hash_key < shard.ending_hash):\n        new_shard = Shard((last_id + 1), new_starting_hash_key, shard.ending_hash)\n        shard.ending_hash = new_starting_hash_key\n        stream.shards[new_shard.shard_id] = new_shard\n    else:\n        raise InvalidArgumentError(new_starting_hash_key)\n    records = shard.records\n    shard.records = OrderedDict()\n    for index in records:\n        record = records[index]\n        stream.put_record(record.partition_key, record.explicit_hash_key, None, record.data)\n", "label": 0}
{"function": "\n\ndef onModuleSourceCode(self, module_name, source_code):\n    annotations = {\n        \n    }\n    for (count, line) in enumerate(source_code.split('\\n')):\n        match = re.search('#.*pylint:\\\\s*disable=\\\\s*([\\\\w,]+)', line)\n        if match:\n            comment_only = (line[:(line.find('#') - 1)].strip() == '')\n            if comment_only:\n                pass\n            else:\n                annotations[(count + 1)] = set((match.strip() for match in match.group(1).split(',')))\n    if annotations:\n        self.line_annotations[module_name] = annotations\n    return source_code\n", "label": 1}
{"function": "\n\ndef get_mtv(self):\n    pos = (self.pos + self.vec)\n    pos = collision.uncenter_position(pos, self.col.bbox)\n    q = collections.deque((Vector3(),))\n    while q:\n        current_vector = q.popleft()\n        transform_vectors = self.col.check_collision(pos, current_vector)\n        if (not all(transform_vectors)):\n            break\n        for vector in transform_vectors:\n            test_vec = ((self.vec + current_vector) + vector)\n            if (test_vec.dist_sq() <= (self.vec.dist_sq() + FP_MAGIC)):\n                q.append((current_vector + vector))\n    else:\n        logger.debug('Physics failed to generate an MTV, bailing out')\n        self.vec.zero()\n        return Vector3()\n    possible_mtv = [current_vector]\n    while q:\n        current_vector = q.popleft()\n        transform_vectors = self.col.check_collision(pos, current_vector)\n        if (not all(transform_vectors)):\n            possible_mtv.append(current_vector)\n    return min(possible_mtv)\n", "label": 1}
{"function": "\n\ndef streamed_search(twit=None, settings=None):\n    'Stream a search query to the console.'\n    kwargs = {\n        \n    }\n    if settings.get('search'):\n        kwargs.update({\n            'track': ','.join(settings['search']),\n        })\n    if settings.get('user'):\n        kwargs.update({\n            'follow': settings['uid'],\n        })\n    for tweet in twit.statuses.filter(**kwargs):\n        if tweet:\n            print_tweet(tweet, settings)\n", "label": 0}
{"function": "\n\ndef setResults(self, modelXbrl):\n    self.results = []\n    self.assertionUnsuccessful = False\n    self.status = 'pass'\n    for error in modelXbrl.errors:\n        if isinstance(error, dict):\n            self.assertions = error\n            for (countSuccessful, countNotsuccessful) in error.items():\n                if (countNotsuccessful > 0):\n                    self.assertionUnsuccessful = True\n                    self.status = 'unsuccessful'\n        else:\n            self.results.append(error)\n            self.status = 'fail'\n    self.results.sort()\n", "label": 0}
{"function": "\n\ndef wannabe_omnispec_to_rspec(omnispec, filter_allocated):\n    root = ET.Element('rspec')\n    for (id, r) in omnispec.get_resources().items():\n        if (filter_allocated and (not r.get_allocate())):\n            continue\n        res_type = r.get_type()\n        if (res_type == 'node'):\n            add_node(root, r)\n        elif (res_type == 'link'):\n            add_link(root, r)\n        else:\n            raise Exception(('Unknown resource type ' + res_type))\n    return ET.tostring(root)\n", "label": 1}
{"function": "\n\ndef _get_network(self, kind, router=True, vlans=True, vlan_ids=True):\n    \"Wrapper for getting details about networks.\\n\\n            :param string kind: network kind. Typically 'public' or 'private'\\n            :param boolean router: flag to include router information\\n            :param boolean vlans: flag to include vlan information\\n            :param boolean vlan_ids: flag to include vlan_ids\\n\\n        \"\n    network = {\n        \n    }\n    macs = self.get(('%s_mac' % kind))\n    network['mac_addresses'] = macs\n    if (len(macs) == 0):\n        return network\n    if router:\n        network['router'] = self.get('router', macs[0])\n    if vlans:\n        network['vlans'] = self.get('vlans', macs[0])\n    if vlan_ids:\n        network['vlan_ids'] = self.get('vlan_ids', macs[0])\n    return network\n", "label": 0}
{"function": "\n\ndef visit_FLOAT(self, type_, **kw):\n    if (self._mysql_type(type_) and (type_.scale is not None) and (type_.precision is not None)):\n        return self._extend_numeric(type_, ('FLOAT(%s, %s)' % (type_.precision, type_.scale)))\n    elif (type_.precision is not None):\n        return self._extend_numeric(type_, ('FLOAT(%s)' % (type_.precision,)))\n    else:\n        return self._extend_numeric(type_, 'FLOAT')\n", "label": 0}
{"function": "\n\ndef get_turns_since(state, maximum=8):\n    \"A feature encoding the age of the stone at each location up to 'maximum'\\n\\n\\tNote:\\n\\t- the [maximum-1] plane is used for any stone with age greater than or equal to maximum\\n\\t- EMPTY locations are all-zero features\\n\\t\"\n    planes = np.zeros((maximum, state.size, state.size))\n    depth = 0\n    for move in state.history[::(- 1)]:\n        if (move is not go.PASS_MOVE):\n            if (state.board[move] != go.EMPTY):\n                (x, y) = move\n                if (np.sum(planes[:, x, y]) == 0):\n                    planes[(depth, x, y)] = 1\n        if (depth < (maximum - 1)):\n            depth += 1\n    return planes\n", "label": 1}
{"function": "\n\ndef send(self, email_address, subject, message, request_url=None):\n    if ((not email_address) or (not subject) or (not message)):\n        return False\n    if request_url:\n        request_url = urlparse(request_url)\n        message = (((((((message + '\\n\\n') + 'Click the following link below at any time to unsubscribe.\\n') + request_url.scheme) + '://') + request_url.netloc) + '/subscriptions/email/') + base64.urlsafe_b64encode(email_address))\n    return mail.send_mail(self.sender, email_address, subject, message)\n", "label": 0}
{"function": "\n\ndef get_subjects(args, skip_filter=False):\n    '\\n    return a dictionary of subjects, optionally using the\\n    subjects_query argument to filter them.\\n    '\n    gq = GeminiQuery.GeminiQuery(args.db)\n    query = ''\n    if (not skip_filter):\n        if (hasattr(args, 'sample_filter') and args.sample_filter):\n            query += args.sample_filter\n    res = gq.metadata.tables['samples'].select().where(sql.text(query)).execute()\n    samples_dict = {\n        \n    }\n    for row in res:\n        subject = Subject(row)\n        samples_dict[subject.name] = subject\n    return samples_dict\n", "label": 0}
{"function": "\n\ndef installed(self, prefix=None):\n    bootstrap = self.read_bootstrap()\n    key = ('casks' if self.cask else 'formulas')\n    formulas = bootstrap.get(key, [])\n    if prefix:\n        return [x for x in formulas if x.startswith(prefix)]\n    return formulas\n", "label": 0}
{"function": "\n\ndef bitcount(self, key, start=None, end=None):\n    '\\n        Returns the count of set bits in the value of ``key``.  Optional\\n        ``start`` and ``end`` paramaters indicate which bytes to consider\\n        '\n    params = [key]\n    if ((start is not None) and (end is not None)):\n        params.append(start)\n        params.append(end)\n    elif (((start is not None) and (end is None)) or ((end is not None) and (start is None))):\n        raise RedisError('Both start and end must be specified')\n    return self.execute_command('BITCOUNT', *params)\n", "label": 1}
{"function": "\n\ndef _format_action(self, action):\n    help_position = min((self._action_max_length + 2), self._max_help_position)\n    help_width = (self._width - help_position)\n    action_width = ((help_position - self._current_indent) - 2)\n    action_header = self._format_action_invocation(action)\n    if (not action.help):\n        tup = (self._current_indent, '', action_header)\n        action_header = ('%*s%s\\n' % tup)\n    elif (len(action_header) <= action_width):\n        tup = (self._current_indent, '', action_width, action_header)\n        action_header = ('%*s%-*s  ' % tup)\n        indent_first = 0\n    else:\n        tup = (self._current_indent, '', action_header)\n        action_header = ('%*s%s\\n' % tup)\n        indent_first = help_position\n    parts = [action_header]\n    if action.help:\n        help_text = self._expand_help(action)\n        help_lines = self._split_lines(help_text, help_width)\n        parts.append(('%*s%s\\n' % (indent_first, '', help_lines[0])))\n        for line in help_lines[1:]:\n            parts.append(('%*s%s\\n' % (help_position, '', line)))\n    elif (not action_header.endswith('\\n')):\n        parts.append('\\n')\n    for subaction in self._iter_indented_subactions(action):\n        parts.append(self._format_action(subaction))\n    return self._join_parts(parts)\n", "label": 1}
{"function": "\n\ndef __wrap(self, value):\n    if isinstance(value, (tuple, set, frozenset)):\n        return type(value)([self.__wrap(v) for v in value])\n    elif (isinstance(value, list) and (not isinstance(value, Collection))):\n        return Collection(value, self.__class__)\n    elif isinstance(value, Object):\n        return value\n    elif isinstance(value, Raw):\n        return value.value\n    elif isinstance(value, dict):\n        if isinstance(self, CaseInsensitiveObject):\n            return CaseInsensitiveObject(value)\n        else:\n            return Object(value)\n    else:\n        return value\n", "label": 1}
{"function": "\n\ndef save_new_objects(self, commit=True):\n    self.new_objects = []\n    for form in self.extra_forms:\n        if (not form.has_changed()):\n            continue\n        if (self.can_delete and self._should_delete_form(form)):\n            continue\n        self.new_objects.append(self.save_new(form, commit=commit))\n        if (not commit):\n            self.saved_forms.append(form)\n    return self.new_objects\n", "label": 1}
{"function": "\n\ndef get_form(formcls):\n    '\\n    get form class according form class path or form class object\\n    '\n    from uliweb.form import Form\n    import inspect\n    if (inspect.isclass(formcls) and issubclass(formcls, Form)):\n        return formcls\n    elif isinstance(formcls, (str, unicode)):\n        path = settings.FORMS.get(formcls)\n        if path:\n            _cls = import_attr(path)\n            return _cls\n        else:\n            raise UliwebError((\"Can't find formcls name %s in settings.FORMS\" % formcls))\n    else:\n        raise UliwebError(('formcls should be Form class object or string path format, but %r found!' % formcls))\n", "label": 0}
{"function": "\n\ndef raise_tab(self, tabname):\n    l = []\n    name = tabname.lower()\n    while self.tab.has_key(name):\n        bnch = self.tab[name]\n        l.insert(0, name)\n        name = bnch.wsname.lower()\n        if (name in l):\n            break\n    for name in l:\n        (nb, index) = self._find_nb(name)\n        if ((nb is not None) and (index >= 0)):\n            nb.set_index(index)\n", "label": 1}
{"function": "\n\ndef _toggleCheckedButtons(self, allowed):\n    ' Toggle off the checked buttons.\\n\\n        Parameters\\n        ----------\\n        allowed : QDockBarButton\\n            The dock button which should be allowed to remain checked.\\n\\n        '\n    for key in self._widgets:\n        if isinstance(key, QDockBarButton):\n            if ((key is not allowed) and key.isChecked()):\n                key.setChecked(False)\n", "label": 0}
{"function": "\n\ndef submit(self, job):\n    if (self.master_process is None):\n        raise jip.cluster.SubmissionError('No Grid master found!')\n    local_job = (_Job.from_job(job) if (not isinstance(job, _Job)) else job)\n    if (not self._remote_ids):\n        local_job.job_id = self._next_id()\n    self.master_requests.put(['SUBMIT', local_job])\n    if self._remote_ids:\n        job.job_id = self.master_response.get()\n    else:\n        job.job_id = local_job.job_id\n    self.log.info('Submitted new job with id %s', job.job_id)\n    return job\n", "label": 0}
{"function": "\n\ndef __init__(self):\n    'initialize our translation dictionary by applying N,Y,R codes'\n    geneticCode = self.geneticCode.copy()\n    for (codon, aa) in self.geneticCode.items():\n        if (codon[2] == 'N'):\n            geneticCode[(codon[:2] + 'A')] = aa\n            geneticCode[(codon[:2] + 'T')] = aa\n            geneticCode[(codon[:2] + 'G')] = aa\n            geneticCode[(codon[:2] + 'C')] = aa\n        elif (codon[2] == 'Y'):\n            geneticCode[(codon[:2] + 'T')] = aa\n            geneticCode[(codon[:2] + 'C')] = aa\n        elif (codon[2] == 'R'):\n            geneticCode[(codon[:2] + 'A')] = aa\n            geneticCode[(codon[:2] + 'G')] = aa\n    self.geneticCode = geneticCode\n", "label": 0}
{"function": "\n\ndef generate_ipv4list(num_items=100, include_hosts=False):\n    '\\n    Generate a list of unique IPv4 addresses. This is a total hack.\\n\\n    :param num_items:\\n        Number of items to generate\\n\\n    :param include_hosts:\\n        Whether to include /32 addresses\\n    '\n    ipset = set()\n    while (len(ipset) < num_items):\n        ip = generate_ipv4()\n        if ip.startswith('0'):\n            continue\n        if ip.endswith('.0.0.0'):\n            prefix = '/8'\n        elif ip.endswith('.0.0'):\n            prefix = '/16'\n        elif ip.endswith('.0'):\n            prefix = '/24'\n        elif include_hosts:\n            prefix = '/32'\n        else:\n            continue\n        ip += prefix\n        ipset.add(ip)\n    return sorted(ipset)\n", "label": 1}
{"function": "\n\ndef __init__(self, request, **kwargs):\n    \"\\n            Matching the following example jsonpath:\\n            \\n                /path/[?(@.field1='searchterm*'&@.field2='*search*')][/@['field1'],/@['field2']][0:24]\\n                \\n            The last part of the URL will contain a JSONPath-query:\\n            \\n                [filter][sort][start:end:step]\\n        \"\n    path = request.path\n    if (not path.endswith('/')):\n        path = (path + '/')\n    match = re.match('^/.*/(\\\\[.*\\\\])/$', path)\n    if match:\n        self.jsonpath = match.groups()[0]\n    if self.jsonpath:\n        parts = self.jsonpath[1:(- 1)].split('][')\n        for part in parts:\n            if part.startswith('?'):\n                self.jsonpath_filters = part\n            elif re.match('^[/\\\\\\\\].*$', part):\n                self.jsonpath_sorting = part\n            elif re.match('^\\\\d*:\\\\d*:{0,1}\\\\d*$', part):\n                self.jsonpath_paging = part\n    super(JsonQueryRestStoreInfo, self).__init__(request, **kwargs)\n", "label": 1}
{"function": "\n\ndef cwd_for_window(window):\n    \"\\n    Return the working directory in which the window's commands should run.\\n\\n    In the common case when the user has one folder open, return that.\\n    Otherwise, return one of the following (in order of preference):\\n        1) One of the open folders, preferring a folder containing the active\\n           file.\\n        2) The directory containing the active file.\\n        3) The user's home directory.\\n    \"\n    folders = window.folders()\n    if (len(folders) == 1):\n        return folders[0]\n    else:\n        active_view = window.active_view()\n        active_file_name = (active_view.file_name() if active_view else None)\n        if (not active_file_name):\n            return (folders[0] if len(folders) else os.path.expanduser('~'))\n        for folder in folders:\n            if active_file_name.startswith(folder):\n                return folder\n        return os.path.dirname(active_file_name)\n", "label": 1}
{"function": "\n\ndef _complete_batch(self, batch, error, base_offset):\n    'Complete or retry the given batch of records.\\n\\n        Arguments:\\n            batch (RecordBatch): The record batch\\n            error (Exception): The error (or None if none)\\n            base_offset (int): The base offset assigned to the records if successful\\n        '\n    if (error is Errors.NoError):\n        error = None\n    if ((error is not None) and self._can_retry(batch, error)):\n        log.warning('Got error produce response on topic-partition %s, retrying (%d attempts left). Error: %s', batch.topic_partition, ((self.config['retries'] - batch.attempts) - 1), error)\n        self._accumulator.reenqueue(batch)\n    else:\n        if (error is Errors.TopicAuthorizationFailedError):\n            error = error(batch.topic_partition.topic)\n        batch.done(base_offset, error)\n        self._accumulator.deallocate(batch)\n    if getattr(error, 'invalid_metadata', False):\n        self._metadata.request_update()\n", "label": 1}
{"function": "\n\ndef get_record(params, record_uid):\n    'Return the referenced record'\n    record_uid = record_uid.strip()\n    if (not record_uid):\n        print('No record UID provided')\n        return\n    if (not params.record_cache):\n        print('No record cache.  Sync down first.')\n        return\n    if (not (record_uid in params.record_cache)):\n        print('Record UID not found.')\n        return\n    cached_rec = params.record_cache[record_uid]\n    if params.debug:\n        print(('Cached Rec: ' + str(cached_rec)))\n    data = json.loads(cached_rec['data'].decode('utf-8'))\n    rec = Record(record_uid)\n    rec.load(data, cached_rec['revision'])\n    return rec\n", "label": 0}
{"function": "\n\ndef GetRealPath(file=None, encodeURL=False):\n    if (file is None):\n        file = ''\n    if ((file.find('/') != 0) and (file.find('\\\\') != 0) and (not re.search('^[a-zA-Z]+:[/\\\\\\\\]?', file))):\n        if hasattr(sys, 'frozen'):\n            path = os.path.dirname(sys.executable)\n        elif ('__file__' in globals()):\n            path = os.path.dirname(os.path.realpath(__file__))\n        else:\n            path = os.getcwd()\n        path = ((path + os.sep) + file)\n        path = re.sub('[/\\\\\\\\]+', re.escape(os.sep), path)\n        path = re.sub('[/\\\\\\\\]+$', '', path)\n        if encodeURL:\n            return urllib_pathname2url(path)\n        else:\n            return path\n    return file\n", "label": 1}
{"function": "\n\ndef on_request(self, request):\n    http = util.http.parse_request(request)\n    if (http and (not http.error_code)):\n        host = http.headers.get('host', self.connection.server_addr)\n        if (not self.connection.hostname):\n            self.connection.hostname = host\n        if self.filter(http):\n            return self.replace(http)\n    return request\n", "label": 0}
{"function": "\n\ndef _check_boolean(self, row, col):\n    from .base import isspmatrix\n    if (isspmatrix(row) or isspmatrix(col)):\n        raise IndexError('Indexing with sparse matrices is not supported except boolean indexing where matrix and index are equal shapes.')\n    if (isinstance(row, np.ndarray) and (row.dtype.kind == 'b')):\n        row = self._boolean_index_to_array(row)\n    if (isinstance(col, np.ndarray) and (col.dtype.kind == 'b')):\n        col = self._boolean_index_to_array(col)\n    return (row, col)\n", "label": 1}
{"function": "\n\ndef can_fetch(self, useragent, url):\n    'using the parsed robots.txt decide if useragent can fetch url'\n    if self.disallow_all:\n        return False\n    if self.allow_all:\n        return True\n    parsed_url = urlparse.urlparse(urllib.unquote(url))\n    url = urlparse.urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment))\n    url = urllib.quote(url)\n    if (not url):\n        url = '/'\n    for entry in self.entries:\n        if entry.applies_to(useragent):\n            return entry.allowance(url)\n    if self.default_entry:\n        return self.default_entry.allowance(url)\n    return True\n", "label": 1}
{"function": "\n\ndef generate_query_string(self, otp, nonce, timestamp=False, sl=None, timeout=None):\n    '\\n        Returns a query string which is sent to the validation servers.\\n        '\n    data = [('id', self.client_id), ('otp', otp), ('nonce', nonce)]\n    if timestamp:\n        data.append(('timestamp', '1'))\n    if (sl is not None):\n        if ((sl not in range(0, 101)) and (sl not in ['fast', 'secure'])):\n            raise Exception('sl parameter value must be between 0 and 100 or string \"fast\" or \"secure\"')\n        data.append(('sl', sl))\n    if timeout:\n        data.append(('timeout', timeout))\n    query_string = urlencode(data)\n    if self.key:\n        hmac_signature = self.generate_message_signature(query_string)\n        hmac_signature = hmac_signature\n        query_string += ('&h=%s' % hmac_signature.replace('+', '%2B'))\n    return query_string\n", "label": 1}
{"function": "\n\ndef _eval_is_prime(self):\n    '\\n        If product is a positive integer, multiplication\\n        will never result in a prime number.\\n        '\n    if self.is_number:\n        '\\n            If input is a number that is not completely simplified.\\n            e.g. Mul(sqrt(3), sqrt(3), evaluate=False)\\n            So we manually evaluate it and return whether that is prime or not.\\n            '\n        r = S.One\n        for arg in self.args:\n            r *= arg\n        return r.is_prime\n    if (self.is_integer and self.is_positive):\n        '\\n            Here we count the number of arguments that have a minimum value\\n            greater than two.\\n            If there are more than one of such a symbol then the result is not prime.\\n            Else, the result cannot be determined.\\n            '\n        number_of_args = 0\n        for arg in self.args:\n            if (arg - 1).is_positive:\n                number_of_args += 1\n        if (number_of_args > 1):\n            return False\n", "label": 1}
{"function": "\n\n@handle_response_format\n@treeio_login_required\ndef mlist_delete(request, mlist_id, response_format='html'):\n    'Delete mlist page'\n    mlist = get_object_or_404(MailingList, pk=mlist_id)\n    if (not request.user.profile.has_permission(mlist, mode='w')):\n        return user_denied(request, message=\"You don't have access to this Mailing List\", response_format=response_format)\n    if request.POST:\n        if ('delete' in request.POST):\n            if ('trash' in request.POST):\n                mlist.trash = True\n                mlist.save()\n            else:\n                mlist.delete()\n            return HttpResponseRedirect('/messaging/')\n        elif ('cancel' in request.POST):\n            return HttpResponseRedirect(reverse('messaging_mlist_view', args=[mlist.id]))\n    context = _get_default_context(request)\n    context.update({\n        'mlist': mlist,\n    })\n    return render_to_response('messaging/mlist_delete', context, context_instance=RequestContext(request), response_format=response_format)\n", "label": 1}
{"function": "\n\ndef check_scoring(estimator, scoring=None, allow_none=False):\n    \"Determine scorer from user options.\\n\\n    A TypeError will be thrown if the estimator cannot be scored.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator object implementing 'fit'\\n        The object to use to fit the data.\\n\\n    scoring : string, callable or None, optional, default: None\\n        A string (see model evaluation documentation) or\\n        a scorer callable object / function with signature\\n        ``scorer(estimator, X, y)``.\\n\\n    allow_none : boolean, optional, default: False\\n        If no scoring is specified and the estimator has no score function, we\\n        can either return None or raise an exception.\\n\\n    Returns\\n    -------\\n    scoring : callable\\n        A scorer callable object / function with signature\\n        ``scorer(estimator, X, y)``.\\n    \"\n    has_scoring = (scoring is not None)\n    if (not hasattr(estimator, 'fit')):\n        raise TypeError((\"estimator should be an estimator implementing 'fit' method, %r was passed\" % estimator))\n    elif has_scoring:\n        return get_scorer(scoring)\n    elif hasattr(estimator, 'score'):\n        return _passthrough_scorer\n    elif allow_none:\n        return None\n    else:\n        raise TypeError((\"If no scoring is specified, the estimator passed should have a 'score' method. The estimator %r does not.\" % estimator))\n", "label": 0}
{"function": "\n\ndef pptable(table):\n    head = table['head']\n    body = table['body']\n    pretty_head = [(('+' + '+'.join(['{:->24}'.format('') for n in head])) + '+'), (('|' + '|'.join(['{:^24}'.format(str(n)[(- 24):]) for n in head])) + '|'), (('+' + '+'.join(['{:=>24}'.format('') for n in head])) + '+')]\n    pretty_body = []\n    for row in body:\n        pretty_body.append((('|' + '|'.join(['{:>24}'.format(str(n)[(- 24):]) for n in row])) + '|'))\n        pretty_body.append((('+' + '+'.join(['{:->24}'.format('') for n in head])) + '+'))\n    prettytable = '\\n'.join((pretty_head + pretty_body))\n    print(prettytable)\n", "label": 1}
{"function": "\n\ndef check_command(command, startswith, endswith):\n    output = subprocess.check_output(command)\n    lines = output.split('\\n')\n    matches = False\n    for line in lines:\n        line = line.strip()\n        if line.startswith(startswith):\n            matches = True\n            if (not line.endswith(endswith)):\n                status = 0\n                break\n    else:\n        if matches:\n            status = 1\n        else:\n            raise BadOutputError(('The output was not in the expected format:\\n%s' % output))\n    return status\n", "label": 1}
{"function": "\n\ndef get_survey_element(self, name_or_xpath):\n    element = self.get_element(name_or_xpath)\n    name = ((element and element['name']) or name_or_xpath)\n    for field in self.get_survey_elements():\n        if (field.name == name):\n            return field\n    return None\n", "label": 0}
{"function": "\n\ndef unique_list(seq, hashfunc=None):\n    seen = {\n        \n    }\n    if (not hashfunc):\n        return [x for x in seq if ((x not in seen) and (not seen.__setitem__(x, True)))]\n    else:\n        return [x for x in seq if ((hashfunc(x) not in seen) and (not seen.__setitem__(hashfunc(x), True)))]\n", "label": 1}
{"function": "\n\ndef __get_resolution_milestone_dict(self, ticket, action):\n    transitions = self.config.get('ticket-workflow', (action + '.milestone')).strip()\n    transition = [x.strip() for x in transitions.split('->')]\n    res_milestone = {\n        \n    }\n    if (len(transition) == 2):\n        resolutions = [y.strip() for y in transition[0].split(',')]\n        for res in resolutions:\n            res_milestone[res] = transition[1]\n    return res_milestone\n", "label": 0}
{"function": "\n\ndef function_line(self, point):\n    line = self.view.line(point)\n    if self.is_end_bracket(line):\n        return line\n    above = line\n    while ((not self.is_end_bracket(above)) and (not self.function_name(above)) and (above.begin() > 0)):\n        above = self.view.line((above.begin() - 1))\n    return above\n", "label": 0}
{"function": "\n\ndef GroupSpinParametersGet(self, socketId, GroupName):\n    command = (('GroupSpinParametersGet(' + GroupName) + ',double *,double *)')\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(2):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef paintEvent(self, e):\n    if (not self.addr):\n        return\n    black = QColor(0, 0, 0, 255)\n    white = QColor(255, 255, 255, 255)\n    if (not self.qr):\n        qp = QtGui.QPainter()\n        qp.begin(self)\n        qp.setBrush(white)\n        qp.setPen(white)\n        qp.drawRect(0, 0, 198, 198)\n        qp.end()\n        return\n    k = self.qr.getModuleCount()\n    qp = QtGui.QPainter()\n    qp.begin(self)\n    r = qp.viewport()\n    boxsize = ((min(r.width(), r.height()) * 0.8) / k)\n    size = (k * boxsize)\n    left = ((r.width() - size) / 2)\n    top = ((r.height() - size) / 2)\n    margin = 10\n    qp.setBrush(white)\n    qp.drawRect((left - margin), (top - margin), (size + (margin * 2)), (size + (margin * 2)))\n    for r in range(k):\n        for c in range(k):\n            if self.qr.isDark(r, c):\n                qp.setBrush(black)\n                qp.setPen(black)\n            else:\n                qp.setBrush(white)\n                qp.setPen(white)\n            qp.drawRect((left + (c * boxsize)), (top + (r * boxsize)), boxsize, boxsize)\n    qp.end()\n", "label": 1}
{"function": "\n\ndef GetFont(self):\n    '\\n        Return Font that should be used to draw text in this block\\n        '\n    font = None\n    if (self.engine.reportFormat.UseListCtrlTextFormat and self.GetListCtrl()):\n        listCtrl = self.GetListCtrl()\n        if listCtrl.IsVirtual():\n            attr = listCtrl.OnGetItemAttr(self.rowIndex)\n            if (attr and attr.HasFont()):\n                font = attr.GetFont()\n        else:\n            font = listCtrl.GetItemFont(self.rowIndex)\n    if (font and font.IsOk()):\n        return font\n    else:\n        return self.GetFormat().GetFont()\n", "label": 1}
{"function": "\n\ndef _unpack_index(self, index):\n    ' Parse index. Always return a tuple of the form (row, col).\\n        Where row/col is a integer, slice, or array of integers.\\n        '\n    from .base import spmatrix\n    if (isinstance(index, (spmatrix, np.ndarray)) and (index.ndim == 2) and (index.dtype.kind == 'b')):\n        return index.nonzero()\n    index = self._check_ellipsis(index)\n    if isinstance(index, tuple):\n        if (len(index) == 2):\n            (row, col) = index\n        elif (len(index) == 1):\n            (row, col) = (index[0], slice(None))\n        else:\n            raise IndexError('invalid number of indices')\n    else:\n        (row, col) = (index, slice(None))\n    (row, col) = self._check_boolean(row, col)\n    return (row, col)\n", "label": 1}
{"function": "\n\ndef __init__(self, value=None, feature=None, last_supported_version=None, useinstead=None, issue=None, deprecated_since_version=None):\n    self.fullMessage = ''\n    if feature:\n        if deprecated_since_version:\n            self.fullMessage = ('%s has been deprecated since SymPy %s. ' % (feature, deprecated_since_version))\n        else:\n            self.fullMessage = ('%s has been deprecated. ' % feature)\n    if last_supported_version:\n        self.fullMessage += ('It will be last supported in SymPy version %s. ' % last_supported_version)\n    if useinstead:\n        self.fullMessage += ('Use %s instead. ' % useinstead)\n    if issue:\n        self.fullMessage += ('See https://github.com/sympy/sympy/issues/%d for more info. ' % issue)\n    if value:\n        if (not isinstance(value, str)):\n            value = ('(%s)' % repr(value))\n        value = (' ' + value)\n    else:\n        value = ''\n    self.fullMessage += value\n", "label": 1}
{"function": "\n\ndef _exc_info_to_string(self, err, test):\n    '\\n        Converts a sys.exc_info()-style tuple of values into a string.\\n        '\n    (exctype, value, tb) = err\n    while (tb and self._is_relevant_tb_level(tb)):\n        tb = tb.tb_next\n    if (exctype is test.failureException):\n        length = self._count_relevant_tb_levels(tb)\n        msgLines = traceback.format_exception(exctype, value, tb, length)\n    else:\n        msgLines = traceback.format_exception(exctype, value, tb)\n    if self.buffer:\n        output = self._stdout_buffer.getvalue()\n        error = self._stderr_buffer.getvalue()\n        if output:\n            if (not output.endswith('\\n')):\n                output += '\\n'\n            msgLines.append((STDOUT_LINE % output))\n        if error:\n            if (not error.endswith('\\n')):\n                error += '\\n'\n            msgLines.append((STDERR_LINE % error))\n    return ''.join(msgLines)\n", "label": 1}
{"function": "\n\ndef visit_Subscript(self, node):\n    if ((not isinstance(node.ctx, ast.Load)) or (not isinstance(node.value, ast.Name))):\n        return\n    if (node.value.id != 'context'):\n        return\n    if ((not isinstance(node.slice, ast.Index)) or (not isinstance(node.slice.value, ast.Str))):\n        return\n    self.contextReads.add(node.slice.value.s)\n", "label": 1}
{"function": "\n\ndef _unsugar_count_from(**kw):\n    'Builds counting functions from keyword arguments.\\n\\n    Keyword argument filter, prepares a simple ``ordering_func`` from a\\n    ``count_from`` argument, otherwise passes ``ordering_func`` on unchanged.\\n    '\n    count_from = kw.pop('count_from', None)\n    if ((kw.get('ordering_func', None) is None) and (count_from is not None)):\n        if (count_from == 0):\n            kw['ordering_func'] = count_from_0\n        elif (count_from == 1):\n            kw['ordering_func'] = count_from_1\n        else:\n            kw['ordering_func'] = count_from_n_factory(count_from)\n    return kw\n", "label": 0}
{"function": "\n\n@reflection.cache\ndef get_view_names(self, connection, schema=None, **kw):\n    if (schema is not None):\n        current_schema = schema\n    else:\n        current_schema = self.default_schema_name\n    s = (\"\\n        SELECT relname\\n        FROM pg_class c\\n        WHERE relkind = 'v'\\n          AND '%(schema)s' = (select nspname from pg_namespace n\\n          where n.oid = c.relnamespace)\\n        \" % dict(schema=current_schema))\n    if util.py2k:\n        view_names = [row[0].decode(self.encoding) for row in connection.execute(s)]\n    else:\n        view_names = [row[0] for row in connection.execute(s)]\n    return view_names\n", "label": 0}
{"function": "\n\ndef __init__(self, api, name=None, public_key=None, private_key=None, private_key_passphrase=None, consumers=[], container_ref=None, created=None, updated=None, status=None, public_key_ref=None, private_key_ref=None, private_key_passphrase_ref=None):\n    secret_refs = {\n        \n    }\n    if public_key_ref:\n        secret_refs['public_key'] = public_key_ref\n    if private_key_ref:\n        secret_refs['private_key'] = private_key_ref\n    if private_key_passphrase_ref:\n        secret_refs['private_key_passphrase'] = private_key_passphrase_ref\n    super(RSAContainer, self).__init__(api=api, name=name, consumers=consumers, container_ref=container_ref, created=created, updated=updated, status=status, secret_refs=secret_refs)\n    if public_key:\n        self.public_key = public_key\n    if private_key:\n        self.private_key = private_key\n    if private_key_passphrase:\n        self.private_key_passphrase = private_key_passphrase\n", "label": 1}
{"function": "\n\ndef teardown_method(self, method):\n    '\\n        Default teardown method for all scripts. If run through Sauce Labs OnDemand, the job name, status and tags\\n        are updated. Also the video and server log are downloaded if so configured.\\n        '\n    if hasattr(self, 'config'):\n        if (('sauce labs' in self.cf['browsers'][self.cf['saunter']['default_browser']]) and (not self.cf['browsers'][self.cf['saunter']['default_browser']]['sauce labs']['ondemand']) and self.cf['saunter']['screenshots']['on_finish']):\n            self.take_named_screenshot('final')\n    if hasattr(self, 'driver'):\n        self.driver.quit()\n    if hasattr(self.browser, 'proxy'):\n        self.config['saunter']['proxies'].append(self.proxy)\n", "label": 1}
{"function": "\n\n@render_to('race/awards.html')\ndef awards(request):\n    if request.user.is_authenticated():\n        user_badges = set(((slug, level) for (slug, level) in BadgeAward.objects.filter(user=request.user).values_list('slug', 'level')))\n    else:\n        user_badges = []\n    badges_awarded = BadgeAward.objects.values('slug', 'level').annotate(num=Count('pk'))\n    badges_counts = {'{0}_{1}'.format(k['slug'], k['level']): k['num'] for k in badges_awarded}\n    user_count = UserProfile.objects.count()\n    badges_list = list()\n    for badge_cls in badges._registry.values():\n        for (level, badge) in enumerate(badge_cls.levels):\n            badge_count = badges_counts.get('{0}_{1}'.format(badge_cls.slug, level), 0)\n            badges_list.append({\n                'slug': badge_cls.slug,\n                'level': level,\n                'name': badge.name,\n                'description': badge.description,\n                'count': badge_count,\n                'percentage': (((badge_count / float(user_count)) * 100) if user_count else 0),\n                'user_has': ((badge_cls.slug, level) in user_badges),\n            })\n    return {\n        'badges_list': badges_list,\n        'user_count': user_count,\n    }\n", "label": 1}
{"function": "\n\ndef OnKeyPressed(self, event):\n    if self.CallTipActive():\n        self.CallTipCancel()\n    key = event.GetKeyCode()\n    if ((key == 32) and event.ControlDown()):\n        pos = self.GetCurrentPos()\n        if event.ShiftDown():\n            self.CallTipSetBackground('yellow')\n            self.CallTipShow(pos, 'lots of of text: blah, blah, blah\\n\\nshow some suff, maybe parameters..\\n\\nfubar(param1, param2)')\n        else:\n            kw = keyword.kwlist[:]\n            kw.sort()\n            self.AutoCompSetIgnoreCase(False)\n            for i in range(len(kw)):\n                if (kw[i] in keyword.kwlist):\n                    kw[i] = kw[i]\n            self.AutoCompShow(0, ' '.join(kw))\n    else:\n        event.Skip()\n", "label": 1}
{"function": "\n\ndef product(*args, **kwds):\n    pools = (list(map(tuple, args)) * kwds.get('repeat', 1))\n    result = [[]]\n    for pool in pools:\n        result = [(x + [y]) for x in result for y in pool]\n    for prod in result:\n        (yield tuple(prod))\n", "label": 0}
{"function": "\n\ndef export(self, lwrite, level, namespace_='VolumeObj:', name_='FileSystemFlagListType', namespacedef_='', pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    showIndent(lwrite, level, pretty_print)\n    lwrite(('<%s%s%s' % (namespace_, name_, ((namespacedef_ and (' ' + namespacedef_)) or ''))))\n    already_processed = set()\n    self.exportAttributes(lwrite, level, already_processed, namespace_, name_='FileSystemFlagListType')\n    if self.hasContent_():\n        lwrite(('>%s' % (eol_,)))\n        self.exportChildren(lwrite, (level + 1), namespace_, name_, pretty_print=pretty_print)\n        showIndent(lwrite, level, pretty_print)\n        lwrite(('</%s%s>%s' % (namespace_, name_, eol_)))\n    else:\n        lwrite(('/>%s' % (eol_,)))\n", "label": 0}
{"function": "\n\ndef generate_server(raml_root, config):\n    ' Handle server generation process.\\n\\n    :param raml_root: Instance of ramlfications.raml.RootNode.\\n    :param config: Pyramid Configurator instance.\\n    '\n    log.info('Server generation started')\n    if (not raml_root.resources):\n        return\n    root_resource = config.get_root_resource()\n    generated_resources = {\n        \n    }\n    for raml_resource in raml_root.resources:\n        if (raml_resource.path in generated_resources):\n            continue\n        parent_resource = _get_nefertari_parent_resource(raml_resource, generated_resources, root_resource)\n        new_resource = generate_resource(config, raml_resource, parent_resource)\n        if (new_resource is not None):\n            generated_resources[raml_resource.path] = new_resource\n", "label": 0}
{"function": "\n\ndef to_terminal(self):\n    'Yield lines to be printed in a terminal.'\n    average_cc = 0.0\n    analyzed = 0\n    for (name, blocks) in self.results:\n        if ('error' in blocks):\n            (yield (name, (blocks['error'],), {\n                'error': True,\n            }))\n            continue\n        (res, cc, n) = cc_to_terminal(blocks, self.config.show_complexity, self.config.min, self.config.max, self.config.total_average)\n        average_cc += cc\n        analyzed += n\n        if res:\n            (yield (name, (), {\n                \n            }))\n            (yield (res, (), {\n                'indent': 1,\n            }))\n    if ((self.config.average or self.config.total_average) and analyzed):\n        cc = (average_cc / analyzed)\n        ranked_cc = cc_rank(cc)\n        (yield ('\\n{0} blocks (classes, functions, methods) analyzed.', (analyzed,), {\n            \n        }))\n        (yield ('Average complexity: {0}{1} ({2}){3}', (RANKS_COLORS[ranked_cc], ranked_cc, cc, RESET), {\n            \n        }))\n", "label": 1}
{"function": "\n\n@contextlib.contextmanager\ndef enable_nova_compute_services(compute_api, *hosts):\n    '\\n    Disable nova-compute services on hosts listed in *hosts before decorated\\n    function will execute and disable previously enabled services after.\\n    :param compute_api:\\n    :param hosts:\\n    :return:\\n    '\n    LOG.debug('Enabling %s nova-compute services.', ', '.join(hosts))\n    services = compute_api.services.list(binary=NOVA_SERVICE)\n    temporary_enabled_services = []\n    for service in services:\n        if (service.host not in hosts):\n            continue\n        if (service.status == SERVICE_ENABLED):\n            continue\n        temporary_enabled_services.append(service.host)\n        compute_api.services.enable(service.host, NOVA_SERVICE)\n    try:\n        (yield)\n    finally:\n        LOG.debug('Disabling %s nova-compute services.', ', '.join(temporary_enabled_services))\n        for service_host in temporary_enabled_services:\n            compute_api.services.disable(service_host, NOVA_SERVICE)\n", "label": 0}
{"function": "\n\ndef test_uncollectable(self):\n    'Test uncollectable object tracking.\\n\\n        This is fixed in Python 3.4 (PEP 442).\\n        '\n    foo = Foo()\n    foo.parent = foo\n    enemy = Enemy()\n    enemy.parent = enemy\n    idfoo = id(foo)\n    idenemy = id(enemy)\n    del foo\n    del enemy\n    gb = GarbageGraph(collectable=0)\n    gfoo = [x for x in gb.metadata if (x.id == idfoo)]\n    self.assertEqual(len(gfoo), 0)\n    genemy = [x for x in gb.metadata if (x.id == idenemy)]\n    if (sys.version_info < (3, 4)):\n        self.assertEqual(len(genemy), 1)\n    self.assertEqual(gb.reduce_to_cycles(), None)\n", "label": 1}
{"function": "\n\ndef check_implicit_rules(self):\n    ' if you are using flask classy are using the standard index,new,put,post, etc ... type routes, we will\\n            automatically check the permissions for you\\n        '\n    if (not self.request_is_managed_by_flask_classy()):\n        return\n    if self.method_is_explictly_overwritten():\n        return\n    (class_name, action) = request.endpoint.split(':')\n    clazz = [classy_class for classy_class in self.flask_classy_classes if (classy_class.__name__ == class_name)][0]\n    Condition(action, clazz.__target_model__).test()\n", "label": 0}
{"function": "\n\ndef _swap_generator(self, coro):\n    'Internal use only.\\n        '\n    self._lock.acquire()\n    cid = coro._id\n    coro = self._coros.get(cid, None)\n    if (coro is None):\n        logger.warning('invalid coroutine %s to swap', cid)\n        self._lock.release()\n        return (- 1)\n    if (coro._callers or (not coro._hot_swappable)):\n        logger.debug('postponing hot swapping of %s', str(coro))\n        self._lock.release()\n        return 0\n    else:\n        coro._timeout = None\n        if (coro._state is None):\n            coro._generator = coro._swap_generator\n            coro._value = None\n            if (coro._complete == 0):\n                coro._complete = None\n            elif isinstance(coro._complete, Event):\n                coro._complete.clear()\n            self._scheduled.add(cid)\n            coro._state = AsynCoro._Scheduled\n            coro._hot_swappable = False\n        else:\n            coro._exceptions.append((HotSwapException, HotSwapException(coro._swap_generator)))\n            if (coro._state in (AsynCoro._Suspended, AsynCoro._AwaitMsg_)):\n                self._suspended.discard(cid)\n                self._scheduled.add(cid)\n                coro._state = AsynCoro._Scheduled\n        coro._swap_generator = None\n        if (self._polling and (len(self._scheduled) == 1)):\n            self._notifier.interrupt()\n    self._lock.release()\n    return 0\n", "label": 1}
{"function": "\n\ndef __init__(self, parameters=None, return_annotation=_empty, __validate_parameters__=True):\n    \"Constructs Signature from the given list of Parameter\\n        objects and 'return_annotation'.  All arguments are optional.\\n        \"\n    if (parameters is None):\n        params = OrderedDict()\n    elif __validate_parameters__:\n        params = OrderedDict()\n        top_kind = _POSITIONAL_ONLY\n        for (idx, param) in enumerate(parameters):\n            kind = param.kind\n            if (kind < top_kind):\n                msg = 'wrong parameter order: {0} before {1}'\n                msg = msg.format(top_kind, param.kind)\n                raise ValueError(msg)\n            else:\n                top_kind = kind\n            name = param.name\n            if (name is None):\n                name = str(idx)\n                param = param.replace(name=name)\n            if (name in params):\n                msg = 'duplicate parameter name: {0!r}'.format(name)\n                raise ValueError(msg)\n            params[name] = param\n    else:\n        params = OrderedDict(((param.name, param) for param in parameters))\n    self._parameters = params\n    self._return_annotation = return_annotation\n", "label": 1}
{"function": "\n\ndef format(self, record):\n    'Uses contextstring if request_id is set, otherwise default.'\n    record.project = self.project\n    record.version = self.version\n    context = getattr(local.store, 'context', None)\n    if context:\n        d = _dictify_context(context)\n        for (k, v) in d.items():\n            setattr(record, k, v)\n    for key in ('instance', 'color'):\n        if (key not in record.__dict__):\n            record.__dict__[key] = ''\n    if record.__dict__.get('request_id', None):\n        self._fmt = CONF.logging_context_format_string\n    else:\n        self._fmt = CONF.logging_default_format_string\n    if ((record.levelno == logging.DEBUG) and CONF.logging_debug_format_suffix):\n        self._fmt += (' ' + CONF.logging_debug_format_suffix)\n    if record.exc_info:\n        record.exc_text = self.formatException(record.exc_info, record)\n    return logging.Formatter.format(self, record)\n", "label": 1}
{"function": "\n\ndef calc_frozen_apps(self, migrations, freeze_list):\n    '\\n        Works out, from the current app, settings, and the command line options,\\n        which apps should be frozen.\\n        '\n    apps_to_freeze = []\n    for to_freeze in freeze_list:\n        if ('.' in to_freeze):\n            self.error((\"You cannot freeze %r; you must provide an app label, like 'auth' or 'books'.\" % to_freeze))\n        if (not models.get_app(to_freeze)):\n            self.error((\"You cannot freeze %r; it's not an installed app.\" % to_freeze))\n        apps_to_freeze.append(to_freeze)\n    if getattr(settings, 'SOUTH_AUTO_FREEZE_APP', True):\n        apps_to_freeze.append(migrations.app_label())\n    return apps_to_freeze\n", "label": 0}
{"function": "\n\ndef handle_class(val, class_name):\n    cls_errors = []\n    docstring = inspect.getdoc(val)\n    if (docstring is None):\n        cls_errors.append((class_name, '**missing** class-level docstring'))\n    else:\n        cls_errors = [(e,) for e in NumpyClassDocString(docstring, class_name, val).get_errors()]\n        methods = dict(((name, func) for (name, func) in inspect.getmembers(val) if ((not name.startswith('_')) and callable(func) and (type(func) is not type))))\n        for (m_name, method) in six.iteritems(methods):\n            if (inspect.getmodule(method) is not None):\n                continue\n            cls_errors.extend(handle_method(method, m_name, class_name))\n    return cls_errors\n", "label": 1}
{"function": "\n\ndef get_features(self, context_obj):\n    if (('source_token' not in context_obj) or (len(context_obj['source_token']) == 0)):\n        return [0.0 for i in range((len(self.thresholds) * 2))]\n    (translations, translations_weighted) = ([], [])\n    for thr in self.thresholds:\n        (all_words, all_words_weighted) = ([], [])\n        for word in context_obj['source_token']:\n            trans = [fl for fl in self.lex_prob[word] if (fl >= thr)]\n            all_words.append(len(trans))\n            all_words_weighted.append((len(trans) * self.corpus_freq.freq(word)))\n        translations.append(np.average(all_words))\n        translations_weighted.append(np.average(all_words_weighted))\n    return (translations + translations_weighted)\n", "label": 1}
{"function": "\n\ndef save_font(self, name, family, generic):\n    \" It is possible that the HTML browser doesn't know how to\\n            show a particular font. Fortunately ODF provides generic fallbacks.\\n            Unfortunately they are not the same as CSS2.\\n            CSS2: serif, sans-serif, cursive, fantasy, monospace\\n            ODF: roman, swiss, modern, decorative, script, system\\n            This method put the font and fallback into a dictionary\\n        \"\n    htmlgeneric = 'sans-serif'\n    if (generic == 'roman'):\n        htmlgeneric = 'serif'\n    elif (generic == 'swiss'):\n        htmlgeneric = 'sans-serif'\n    elif (generic == 'modern'):\n        htmlgeneric = 'monospace'\n    elif (generic == 'decorative'):\n        htmlgeneric = 'sans-serif'\n    elif (generic == 'script'):\n        htmlgeneric = 'monospace'\n    elif (generic == 'system'):\n        htmlgeneric = 'serif'\n    self.fontdict[name] = (family, htmlgeneric)\n", "label": 1}
{"function": "\n\ndef getNextValues(self):\n    dateTime = None\n    bars = self.getNextBars()\n    if (bars is not None):\n        dateTime = bars.getDateTime()\n        if ((self.__currentBars is not None) and (self.__currentBars.getDateTime() >= dateTime)):\n            raise Exception(('Bar date times are not in order. Previous datetime was %s and current datetime is %s' % (self.__currentBars.getDateTime(), dateTime)))\n        self.__currentBars = bars\n        for instrument in bars.getInstruments():\n            self.__lastBars[instrument] = bars[instrument]\n    return (dateTime, bars)\n", "label": 0}
{"function": "\n\ndef to_console_title(self, message):\n    if (not self.params.get('consoletitle', False)):\n        return\n    if ((os.name == 'nt') and ctypes.windll.kernel32.GetConsoleWindow()):\n        ctypes.windll.kernel32.SetConsoleTitleW(ctypes.c_wchar_p(message))\n    elif ('TERM' in os.environ):\n        self._write_string(('\\x1b]0;%s\\x07' % message), self._screen_file)\n", "label": 0}
{"function": "\n\ndef get_json(self, path, expect_errors=False, headers=None, extra_environ=None, q=[], path_prefix=PATH_PREFIX, **params):\n    'Sends simulated HTTP GET request to Pecan test app.\\n\\n        :param path: url path of target service\\n        :param expect_errors: Boolean value;whether an error is expected based\\n                              on request\\n        :param headers: a dictionary of headers to send along with the request\\n        :param extra_environ: a dictionary of environ variables to send along\\n                              with the request\\n        :param q: list of queries consisting of: field, value, op, and type\\n                  keys\\n        :param path_prefix: prefix of the url path\\n        :param params: content for wsgi.input of request\\n        '\n    full_path = (path_prefix + path)\n    query_params = {\n        'q.field': [],\n        'q.value': [],\n        'q.op': [],\n    }\n    for query in q:\n        for name in ['field', 'op', 'value']:\n            query_params[('q.%s' % name)].append(query.get(name, ''))\n    all_params = {\n        \n    }\n    all_params.update(params)\n    if q:\n        all_params.update(query_params)\n    print(('GET: %s %r' % (full_path, all_params)))\n    response = self.app.get(full_path, params=all_params, headers=headers, extra_environ=extra_environ, expect_errors=expect_errors)\n    if (not expect_errors):\n        response = response.json\n    print(('GOT:%s' % response))\n    return response\n", "label": 0}
{"function": "\n\ndef LoadAppInclude(app_include):\n    'Load a single AppInclude object where one and only one is expected.\\n\\n  Args:\\n    app_include: A file-like object or string.  If it is a string, parse it as\\n    a configuration file.  If it is a file-like object, read in data and\\n    parse.\\n\\n  Returns:\\n    An instance of AppInclude as loaded from a YAML file.\\n\\n  Raises:\\n    EmptyConfigurationFile: when there are no documents in YAML file.\\n    MultipleConfigurationFile: when there is more than one document in YAML\\n    file.\\n  '\n    builder = yaml_object.ObjectBuilder(AppInclude)\n    handler = yaml_builder.BuilderHandler(builder)\n    listener = yaml_listener.EventListener(handler)\n    listener.Parse(app_include)\n    includes = handler.GetResults()\n    if (len(includes) < 1):\n        raise appinfo_errors.EmptyConfigurationFile()\n    if (len(includes) > 1):\n        raise appinfo_errors.MultipleConfigurationFile()\n    includeyaml = includes[0]\n    if includeyaml.handlers:\n        for handler in includeyaml.handlers:\n            handler.FixSecureDefaults()\n            handler.WarnReservedURLs()\n    if includeyaml.builtins:\n        BuiltinHandler.Validate(includeyaml.builtins)\n    return includeyaml\n", "label": 1}
{"function": "\n\ndef _loop(self):\n    import jedi\n    while True:\n        data = stream_read(sys.stdin)\n        if (not isinstance(data, tuple)):\n            break\n        (source, line, col, filename) = data\n        log.debug('Line: %r, Col: %r, Filename: %r', line, col, filename)\n        completions = jedi.Script(source, line, col, filename).completions()\n        out = []\n        tmp_filecache = {\n            \n        }\n        for c in completions:\n            (name, type_, desc, abbr) = self.parse_completion(c, tmp_filecache)\n            kind = (type_ if (not self.use_short_types) else (_types.get(type_) or type_))\n            out.append((c.module_path, name, type_, desc, abbr, kind))\n        stream_write(sys.stdout, out)\n", "label": 1}
{"function": "\n\ndef walk(self, node_type=None):\n    'Walk through the query tree, returning nodes of a specific type.'\n    pending = [self._root]\n    while pending:\n        node = pending.pop()\n        if ((not node_type) or isinstance(node, node_type)):\n            (yield node)\n        if isinstance(node, nodes.UnaryOp):\n            pending.append(node.node)\n        elif isinstance(node, nodes.BinaryOp):\n            pending.extend([node.left, node.right])\n", "label": 1}
{"function": "\n\ndef should_download(self, url_split):\n    'Returns True if the url does not start with an ignored prefix and if\\n        it is local or outside links are allowed.'\n    local = self.is_local(url_split)\n    if ((not self.options.test_outside) and (not local)):\n        return False\n    url = url_split.geturl()\n    for ignored_prefix in self.ignored_prefixes:\n        if url.startswith(ignored_prefix):\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef PositionerCorrectorPIPositionGet(self, socketId, PositionerName):\n    command = (('PositionerCorrectorPIPositionGet(' + PositionerName) + ',bool *,double *,double *,double *)')\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(4):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef __call__(self, *values):\n    '\\n        Evaluates the expansion of a #define macro function called\\n        with the specified values.\\n        '\n    if (len(self.args) != len(values)):\n        raise ValueError((\"Incorrect number of arguments to `%s'\" % self.name))\n    locals = {\n        \n    }\n    for (k, v) in zip(self.args, values):\n        locals[k] = v\n    parts = []\n    for s in self.expansion:\n        if (not (s in self.args)):\n            s = repr(s)\n        parts.append(s)\n    statement = ' + '.join(parts)\n    return eval(statement, globals(), locals)\n", "label": 0}
{"function": "\n\ndef validate(self, obj, value):\n    if (value is None):\n        if self._allow_none:\n            return value\n    if (not isinstance(value, basestring)):\n        self.error(obj, value)\n    for v in self.values:\n        if (v.lower() == value.lower()):\n            return v\n    self.error(obj, value)\n", "label": 1}
{"function": "\n\ndef url_for_static(filename=None, **kwargs):\n    from uliweb import settings, application\n    from uliweb.core.SimpleFrame import get_url_adapter\n    from urlparse import urlparse, urlunparse, urljoin\n    import urllib\n    domain = application.domains.get('static', {\n        \n    })\n    ver = settings.GLOBAL.STATIC_VER\n    if ver:\n        kwargs['ver'] = ver\n    external = kwargs.pop('_external', False)\n    if (not external):\n        external = domain.get('display', False)\n    if filename.startswith('/'):\n        if filename.endswith('/'):\n            filename = filename[:(- 1)]\n        if kwargs:\n            filename += ('?' + urllib.urlencode(kwargs))\n        if external:\n            return urljoin(domain.get('domain', ''), filename)\n        return filename\n    r = urlparse(filename)\n    if (r.scheme or r.netloc):\n        x = list(r)\n        if kwargs:\n            x[4] = urllib.urlencode(kwargs)\n            return urlunparse(x)\n        else:\n            return filename\n    kwargs['filename'] = filename\n    url_adapter = get_url_adapter('static')\n    return url_adapter.build('uliweb.contrib.staticfiles.static', kwargs, force_external=external)\n", "label": 1}
{"function": "\n\ndef filter_items(value, startswith=None, strip_prefix=False):\n    'Jinja2 filter used to filter a dictionary\\'s keys by specifying a\\n    required prefix.\\n\\n    Returns a list of key/value tuples.\\n\\n    Usage:\\n        {{ my_dict|filter_items }}\\n        {{ my_dict|filter_items(\"MY_PREFIX_\") }}\\n        {{ my_dict|filter_items(\"MY_PREFIX_\", True) }}\\n    '\n    if (startswith is not None):\n        value = [x for x in value.items() if x[0].startswith(startswith)]\n    else:\n        value = value.items()\n    if ((startswith is not None) and strip_prefix):\n        value = [(x[0].replace(startswith, '', 1), x[1]) for x in value]\n    return value\n", "label": 1}
{"function": "\n\ndef _build_resource(self):\n    'Generate a resource for :meth:`begin`.'\n    resource = {\n        'query': self.query,\n    }\n    if (self.default_dataset is not None):\n        resource['defaultDataset'] = {\n            'projectId': self.project,\n            'datasetId': self.default_dataset.name,\n        }\n    if (self.max_results is not None):\n        resource['maxResults'] = self.max_results\n    if (self.preserve_nulls is not None):\n        resource['preserveNulls'] = self.preserve_nulls\n    if (self.timeout_ms is not None):\n        resource['timeoutMs'] = self.timeout_ms\n    if (self.use_query_cache is not None):\n        resource['useQueryCache'] = self.use_query_cache\n    return resource\n", "label": 1}
{"function": "\n\ndef _get_my_tid(self):\n    \"determines this (self's) thread id\"\n    if (not self.isAlive()):\n        raise threading.ThreadError('the thread is not active')\n    if hasattr(self, '_thread_id'):\n        return self._thread_id\n    for (tid, tobj) in list(threading._active.items()):\n        if (tobj is self):\n            self._thread_id = tid\n            return tid\n    raise AssertionError(\"could not determine the thread's id\")\n", "label": 0}
{"function": "\n\ndef parse(self, string, name='<string>'):\n    '\\n        Divide the given string into examples and intervening text,\\n        and return them as a list of alternating Examples and strings.\\n        Line numbers for the Examples are 0-based.  The optional\\n        argument `name` is a name identifying this string, and is only\\n        used for error messages.\\n        '\n    string = string.expandtabs()\n    min_indent = self._min_indent(string)\n    if (min_indent > 0):\n        string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n    output = []\n    (charno, lineno) = (0, 0)\n    for m in self._EXAMPLE_RE.finditer(string):\n        output.append(string[charno:m.start()])\n        lineno += string.count('\\n', charno, m.start())\n        (source, options, want, exc_msg) = self._parse_example(m, name, lineno)\n        if (not self._IS_BLANK_OR_COMMENT(source)):\n            output.append(doctest.Example(source, want, exc_msg, lineno=lineno, indent=(min_indent + len((m.group('indent') or m.group('runindent')))), options=options))\n        lineno += string.count('\\n', m.start(), m.end())\n        charno = m.end()\n    output.append(string[charno:])\n    return output\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TListSentryRolesRequest')\n    if (self.protocol_version is not None):\n        oprot.writeFieldBegin('protocol_version', TType.I32, 1)\n        oprot.writeI32(self.protocol_version)\n        oprot.writeFieldEnd()\n    if (self.requestorUserName is not None):\n        oprot.writeFieldBegin('requestorUserName', TType.STRING, 2)\n        oprot.writeString(self.requestorUserName)\n        oprot.writeFieldEnd()\n    if (self.groupName is not None):\n        oprot.writeFieldBegin('groupName', TType.STRING, 3)\n        oprot.writeString(self.groupName)\n        oprot.writeFieldEnd()\n    if (self.component is not None):\n        oprot.writeFieldBegin('component', TType.STRING, 4)\n        oprot.writeString(self.component)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef find(self, path):\n    'Return the node for a path, or None.'\n    path = path.split('.')\n    node = self\n    while node._parent:\n        node = node._parent\n    for name in path:\n        node = node._tree.get(name, None)\n        if ((node is None) or (type(node) is float)):\n            return None\n    return node\n", "label": 0}
{"function": "\n\ndef _set_item(self, item_id, value):\n    'Set the current value of an item in the dialog\\n        '\n    item_hwnd = wrapped(win32gui.GetDlgItem, self.hwnd, item_id)\n    class_name = wrapped(win32gui.GetClassName, item_hwnd)\n    styles = wrapped(win32gui.GetWindowLong, self.hwnd, win32con.GWL_STYLE)\n    if (class_name == 'Edit'):\n        if isinstance(value, datetime.date):\n            value = value.strftime('%d %b %Y')\n        value = unicode(value).replace('\\r\\n', '\\n').replace('\\n', '\\r\\n')\n        wrapped(win32gui.SetDlgItemText, self.hwnd, item_id, value)\n    elif (class_name == 'Button'):\n        SendMessage(item_hwnd, win32con.BM_SETCHECK, int(value), 0)\n    elif (class_name == 'ComboBox'):\n        for item in value:\n            if isinstance(item, tuple):\n                item = item[0]\n            SendMessage(item_hwnd, win32con.CB_ADDSTRING, 0, utils.string_as_pointer(str(item)))\n        SendMessage(item_hwnd, win32con.CB_SETCURSEL, 0, 0)\n    elif (class_name == 'Static'):\n        wrapped(win32gui.SetDlgItemText, self.hwnd, item_id, unicode(value))\n    else:\n        raise RuntimeError(('Unknown class: %s' % class_name))\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    print(('Querying database at %s' % CURRENCY_API_URL))\n    currencies = []\n    if len(args):\n        currencies = list(args)\n    api = urlopen(CURRENCY_API_URL)\n    d = json.loads(api.read())\n    i = 0\n    for currency in sorted(d.keys()):\n        if ((not currencies) or (currency in currencies)):\n            if (not Currency.objects.filter(code=currency)):\n                print(('Creating %r (%s)' % (d[currency], currency)))\n                Currency(code=currency, name=d[currency], factor=1.0, is_active=False).save()\n                i += 1\n    if (i == 1):\n        print(('%i new currency' % i))\n    else:\n        print(('%i new currencies' % i))\n", "label": 1}
{"function": "\n\ndef ToPrimitiveDict(self):\n    'Handle dict generation specifically for Artifacts.'\n    artifact_dict = super(Artifact, self).ToPrimitiveDict()\n    artifact_dict['name'] = utils.SmartStr(self.name)\n    for source in artifact_dict['sources']:\n        if ('type' in source):\n            source['type'] = str(source['type'])\n        if ('key_value_pairs' in source['attributes']):\n            outarray = []\n            for indict in source['attributes']['key_value_pairs']:\n                outarray.append(dict(indict.items()))\n            source['attributes']['key_value_pairs'] = outarray\n    for field in self.required_repeated_fields:\n        if (field not in artifact_dict):\n            artifact_dict[field] = []\n    return artifact_dict\n", "label": 1}
{"function": "\n\ndef get_version(accessors):\n    if isinstance(accessors, tuple):\n        version = resolve_accessor(accessors[0])\n        details = resolve_accessor(accessors[1])\n        if version:\n            return ('%s (%s)' % (version, (details or '?')))\n    else:\n        version = resolve_accessor(accessors)\n    return (version or 'not installed')\n", "label": 0}
{"function": "\n\ndef filter_users(self):\n    users = []\n    for user in config.users:\n        if user.get('deleted'):\n            continue\n        if (self.src_cloud.tenant_exists(user.get('tenant')) or self.src_cloud.user_has_not_primary_tenants(user['name'])):\n            users.append(user['name'])\n    return self._get_keystone_resources('users', users)\n", "label": 0}
{"function": "\n\n@login_required\ndef new(request, form_class=BlogForm, template_name='blog/new.html'):\n    if (request.method == 'POST'):\n        if (request.POST['action'] == 'create'):\n            blog_form = form_class(request.user, request.POST)\n            if blog_form.is_valid():\n                blog = blog_form.save(commit=False)\n                blog.author = request.user\n                if getattr(settings, 'BEHIND_PROXY', False):\n                    blog.creator_ip = request.META['HTTP_X_FORWARDED_FOR']\n                else:\n                    blog.creator_ip = request.META['REMOTE_ADDR']\n                blog.save()\n                messages.add_message(request, messages.SUCCESS, (ugettext(\"Successfully saved post '%s'\") % blog.title))\n                if notification:\n                    if (blog.status == 2):\n                        if friends:\n                            notification.send((x['friend'] for x in Friendship.objects.friends_for_user(blog.author)), 'blog_friend_post', {\n                                'post': blog,\n                            })\n                return HttpResponseRedirect(reverse('blog_list_yours'))\n        else:\n            blog_form = form_class()\n    else:\n        blog_form = form_class()\n    return render_to_response(template_name, {\n        'blog_form': blog_form,\n    }, context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef register(self, documents):\n    \"Register one or more :class:`mongokit.Document` instances to the\\n        connection.\\n\\n        Can be also used as a decorator on documents:\\n\\n        .. code-block:: python\\n\\n            db = MongoKit(app)\\n\\n            @db.register\\n            class Task(Document):\\n                structure = {\\n                   'title': unicode,\\n                   'text': unicode,\\n                   'creation': datetime,\\n                }\\n\\n        :param documents: A :class:`list` of :class:`mongokit.Document`.\\n        \"\n    decorator = None\n    if (not isinstance(documents, (list, tuple, set, frozenset))):\n        decorator = documents\n        documents = [documents]\n    for document in documents:\n        if (document not in self.registered_documents):\n            self.registered_documents.append(document)\n    if (decorator is None):\n        return self.registered_documents\n    else:\n        return decorator\n", "label": 0}
{"function": "\n\ndef _checkServices(self, expectedServices):\n    'Check to make sure that the expected services are found in\\n        that order in the parsed document.'\n    it = iter(self._getServices())\n    for (type_uri, uri) in expectedServices:\n        for service in it:\n            if (type_uri in service.type_uris):\n                self.failUnlessEqual(service.uri, uri)\n                break\n        else:\n            self.fail(('Did not find %r service' % (type_uri,)))\n", "label": 0}
{"function": "\n\ndef dispatch_loop(self):\n    while self.running:\n        backoff_seconds = 0.001\n        while self.running:\n            work_response = self.get_work()\n            if work_response.work:\n                break\n            time.sleep(backoff_seconds)\n            backoff_seconds = min(1.0, (backoff_seconds * 2))\n        for computation_work in work_response.work:\n            self.process_computation(computation_work)\n", "label": 0}
{"function": "\n\n@property\ndef initial_selection(self):\n    'Initial selection matrix'\n    if (not (self.state_regression and self.time_varying_regression)):\n        if (self.k_posdef > 0):\n            selection = np.r_[(([0] * self._k_states_diff), ([1] * (self._k_order > 0)), ([0] * (self._k_order - 1)), ([0] * ((1 - self.mle_regression) * self.k_exog)))][:, None]\n        else:\n            selection = np.zeros((self.k_states, 0))\n    else:\n        selection = np.zeros((self.k_states, self.k_posdef))\n        if (self._k_order > 0):\n            selection[(0, 0)] = 1\n        for i in range(self.k_exog, 0, (- 1)):\n            selection[((- i), (- i))] = 1\n    return selection\n", "label": 1}
{"function": "\n\ndef update(self, **kw):\n    \"\\n        Shortcut for doing an UPDATE on this object.\\n\\n        If _signal=False is in ``kw`` the post_save signal won't be sent.\\n        \"\n    signal = kw.pop('_signal', True)\n    cls = self.__class__\n    for (k, v) in kw.items():\n        setattr(self, k, v)\n    if signal:\n        attrs = dict(self.__dict__)\n        models.signals.pre_save.send(sender=cls, instance=self)\n        for (k, v) in self.__dict__.items():\n            if (attrs[k] != v):\n                kw[k] = v\n                setattr(self, k, v)\n    cls.objects.filter(pk=self.pk).update(**kw)\n    if signal:\n        models.signals.post_save.send(sender=cls, instance=self, created=False)\n", "label": 1}
{"function": "\n\ndef dedup_views(window):\n    group = window.active_group()\n    for g in range(window.num_groups()):\n        found = dict()\n        views = window.views_in_group(g)\n        active = window.active_view_in_group(g)\n        for v in views:\n            if v.is_dirty():\n                continue\n            id = v.buffer_id()\n            if (id in found):\n                if (v == active):\n                    before = found[id]\n                    found[id] = v\n                    v = before\n                window.focus_view(v)\n                window.run_command('close')\n            else:\n                found[id] = v\n        window.focus_view(active)\n    window.focus_group(group)\n", "label": 1}
{"function": "\n\ndef resolve_promise(o):\n    if isinstance(o, dict):\n        for (k, v) in o.items():\n            o[k] = resolve_promise(v)\n    elif isinstance(o, (list, tuple)):\n        o = [resolve_promise(x) for x in o]\n    elif isinstance(o, Promise):\n        try:\n            o = force_unicode(o)\n        except:\n            try:\n                o = [resolve_promise(x) for x in o]\n            except:\n                raise Exception(('Unable to resolve lazy object %s' % o))\n    elif callable(o):\n        o = o()\n    return o\n", "label": 1}
{"function": "\n\ndef parse_args():\n    'Parses arguments and returns a dictionary'\n    (opt_list, _) = getopt.getopt(sys.argv[1:], 'k', ['folder=', 'suite='])\n    opt_dict = {\n        \n    }\n    for (opt, arg) in opt_list:\n        if (opt == '-k'):\n            opt_dict['remove'] = False\n            errwrite('I will keep all files.')\n        if (opt == '--folder'):\n            opt_dict['folder'] = arg\n            errwrite(('I will put all data into folder `%s`.' % arg))\n        if (opt == '--suite'):\n            opt_dict['suite_no'] = arg\n            errwrite(('I will run suite `%s`.' % arg))\n    sys.argv = [sys.argv[0]]\n    return opt_dict\n", "label": 0}
{"function": "\n\ndef levenshtein(x, y):\n    'Levenshtein edit distance\\n\\n    :param x:\\n    :param y: strings\\n    :returns: distance\\n    :complexity: `O(|x|*|y|)`\\n    '\n    n = len(x)\n    m = len(y)\n    A = [[(i + j) for j in range((m + 1))] for i in range((n + 1))]\n    for i in range(n):\n        for j in range(m):\n            A[(i + 1)][(j + 1)] = min((A[i][(j + 1)] + 1), (A[(i + 1)][j] + 1), (A[i][j] + int((x[i] != y[j]))))\n    return A[n][m]\n", "label": 0}
{"function": "\n\n@verbose\ndef save(self, fname, ftype='stc', verbose=None):\n    'Save the source estimates to a file\\n\\n        Parameters\\n        ----------\\n        fname : string\\n            The stem of the file name. The stem is extended with \"-vl.stc\"\\n            or \"-vl.w\".\\n        ftype : string\\n            File format to use. Allowed values are \"stc\" (default) and \"w\".\\n            The \"w\" format only supports a single time point.\\n        verbose : bool, str, int, or None\\n            If not None, override default verbose level (see mne.verbose).\\n            Defaults to self.verbose.\\n        '\n    if (ftype not in ['stc', 'w']):\n        raise ValueError(('ftype must be \"stc\" or \"w\", not \"%s\"' % ftype))\n    if (ftype == 'stc'):\n        logger.info('Writing STC to disk...')\n        if (not (fname.endswith('-vl.stc') or fname.endswith('-vol.stc'))):\n            fname += '-vl.stc'\n        _write_stc(fname, tmin=self.tmin, tstep=self.tstep, vertices=self.vertices, data=self.data)\n    elif (ftype == 'w'):\n        logger.info('Writing STC to disk (w format)...')\n        if (not (fname.endswith('-vl.w') or fname.endswith('-vol.w'))):\n            fname += '-vl.w'\n        _write_w(fname, vertices=self.vertices, data=self.data)\n    logger.info('[done]')\n", "label": 1}
{"function": "\n\ndef decrypt(self, encBytes):\n    'Decrypt the passed-in bytes.\\n\\n        This requires the key to have a private component.  It performs\\n        PKCS1 decryption of the passed-in data.\\n\\n        @type encBytes: L{bytearray} of unsigned bytes\\n        @param encBytes: The value which will be decrypted.\\n\\n        @rtype: L{bytearray} of unsigned bytes or None.\\n        @return: A PKCS1 decryption of the passed-in data or None if\\n        the data is not properly formatted.\\n        '\n    if (not self.hasPrivateKey()):\n        raise AssertionError()\n    if (len(encBytes) != numBytes(self.n)):\n        return None\n    c = bytesToNumber(encBytes)\n    if (c >= self.n):\n        return None\n    m = self._rawPrivateKeyOp(c)\n    decBytes = numberToBytes(m, numBytes(self.n))\n    if ((decBytes[0] != 0) or (decBytes[1] != 2)):\n        return None\n    for x in range(1, (len(decBytes) - 1)):\n        if (decBytes[x] == 0):\n            break\n    else:\n        return None\n    return decBytes[(x + 1):]\n", "label": 1}
{"function": "\n\ndef run(self):\n    isIdle = False\n    while (not self._finish):\n        (state, nextCheck, idle) = self._tracker.check_idle()\n        if ((state == 'idle') and (not isIdle) and (idle >= (self._idleSeconds * 1000))):\n            self.emit(QtCore.SIGNAL('idle()'))\n        elif ((state is not None) and isIdle):\n            self.emit(QtCore.SIGNAL('active()'))\n        if (state is not None):\n            isIdle = (state == 'idle')\n        self.usleep((nextCheck * 1000))\n", "label": 1}
{"function": "\n\ndef prepareQsub(self, cpu, mem, jobID):\n    qsubline = ['qsub', '-b', 'y', '-terse', '-j', 'y', '-cwd', '-o', '/dev/null', '-e', '/dev/null', '-N', ('toil_job_' + str(jobID))]\n    if self.boss.environment:\n        qsubline.append('-v')\n        qsubline.append(','.join((((k + '=') + quote((os.environ[k] if (v is None) else v))) for (k, v) in self.boss.environment.iteritems())))\n    reqline = list()\n    if (mem is not None):\n        memStr = (str((mem / 1024)) + 'K')\n        reqline += [('vf=' + memStr), ('h_vmem=' + memStr)]\n    if (len(reqline) > 0):\n        qsubline.extend(['-hard', '-l', ','.join(reqline)])\n    if ((cpu is not None) and (math.ceil(cpu) > 1)):\n        peConfig = (os.getenv('TOIL_GRIDENGINE_PE') or 'shm')\n        qsubline.extend(['-pe', peConfig, str(int(math.ceil(cpu)))])\n    return qsubline\n", "label": 1}
{"function": "\n\ndef get_groups(qids):\n    'Makes an iterator of query groups on the provided list of query ids.\\n\\n    Parameters\\n    ----------\\n    qids : array_like of shape = [n_samples]\\n        List of query ids.\\n\\n    Yields\\n    ------\\n    row : (qid, int, int)\\n        Tuple of query id, from, to.\\n        ``[i for i, q in enumerate(qids) if q == qid] == range(from, to)``\\n\\n    '\n    prev_qid = None\n    prev_limit = 0\n    total = 0\n    for (i, qid) in enumerate(qids):\n        total += 1\n        if (qid != prev_qid):\n            if (i != prev_limit):\n                (yield (prev_qid, prev_limit, i))\n            prev_qid = qid\n            prev_limit = i\n    if (prev_limit != total):\n        (yield (prev_qid, prev_limit, total))\n", "label": 0}
{"function": "\n\ndef extract_images(self):\n    from frappe.utils.file_manager import extract_images_from_html\n    if self.format_data:\n        data = json.loads(self.format_data)\n        for df in data:\n            if (df.get('fieldtype') and (df['fieldtype'] in ('HTML', 'Custom HTML')) and df.get('options')):\n                df['options'] = extract_images_from_html(self, df['options'])\n        self.format_data = json.dumps(data)\n", "label": 1}
{"function": "\n\n@classmethod\ndef email_url_config(cls, url, backend=None):\n    'Parses an email URL.'\n    config = {\n        \n    }\n    url = (urlparse.urlparse(url) if (not isinstance(url, cls.URL_CLASS)) else url)\n    path = url.path[1:]\n    path = path.split('?', 2)[0]\n    config.update({\n        'EMAIL_FILE_PATH': path,\n        'EMAIL_HOST_USER': url.username,\n        'EMAIL_HOST_PASSWORD': url.password,\n        'EMAIL_HOST': url.hostname,\n        'EMAIL_PORT': _cast_int(url.port),\n    })\n    if backend:\n        config['EMAIL_BACKEND'] = backend\n    elif (url.scheme not in cls.EMAIL_SCHEMES):\n        raise ImproperlyConfigured(('Invalid email schema %s' % url.scheme))\n    elif (url.scheme in cls.EMAIL_SCHEMES):\n        config['EMAIL_BACKEND'] = cls.EMAIL_SCHEMES[url.scheme]\n    if (url.scheme in ('smtps', 'smtp+tls')):\n        config['EMAIL_USE_TLS'] = True\n    elif (url.scheme == 'smtp+ssl'):\n        config['EMAIL_USE_SSL'] = True\n    if url.query:\n        config_options = {\n            \n        }\n        for (k, v) in urlparse.parse_qs(url.query).items():\n            opt = {\n                k.upper(): _cast_int(v[0]),\n            }\n            if (k.upper() in cls._EMAIL_BASE_OPTIONS):\n                config.update(opt)\n            else:\n                config_options.update(opt)\n        config['OPTIONS'] = config_options\n    return config\n", "label": 1}
{"function": "\n\n@register(_dump_registry, bytes)\ndef _dump_bytes(obj, stream):\n    l = len(obj)\n    if (l == 0):\n        stream.append(TAG_EMPTY_STR)\n    elif (l == 1):\n        stream.append((TAG_STR1 + obj))\n    elif (l == 2):\n        stream.append((TAG_STR2 + obj))\n    elif (l == 3):\n        stream.append((TAG_STR3 + obj))\n    elif (l == 4):\n        stream.append((TAG_STR4 + obj))\n    elif (l < 256):\n        stream.append(((TAG_STR_L1 + I1.pack(l)) + obj))\n    else:\n        stream.append(((TAG_STR_L4 + I4.pack(l)) + obj))\n", "label": 1}
{"function": "\n\ndef on_widget(self, instance, value):\n    stack = self.ids.stack\n    prefs = [btn.widget_ref() for btn in self.parents]\n    if (value in prefs):\n        index = prefs.index(value)\n        for btn in self.parents:\n            btn.state = 'normal'\n        self.parents[index].state = 'down'\n        return\n    stack.clear_widgets()\n    if (not value):\n        return\n    widget = value\n    parents = []\n    while True:\n        btn = ConsoleButton(text=widget.__class__.__name__)\n        btn.widget_ref = weakref.ref(widget)\n        btn.bind(on_release=self.highlight_widget)\n        parents.append(btn)\n        if (widget == widget.parent):\n            break\n        widget = widget.parent\n    for btn in reversed(parents):\n        stack.add_widget(btn)\n    self.ids.sv.scroll_x = 1\n    self.parents = parents\n    btn.state = 'down'\n", "label": 1}
{"function": "\n\ndef mesh(N, edges=True):\n    if (not edges):\n        N = (N - 1)\n    tups = []\n    for (count, x1) in enumerate(np.linspace(0.0, 1.0, num=N, endpoint=edges)):\n        for x2 in np.linspace(0.0, (1.0 - x1), num=(N - count), endpoint=edges):\n            x3 = ((1 - x1) - x2)\n            tups.append((x1, x2, x3))\n    tups = np.array(tups)\n    if (not edges):\n        tups = tups[np.logical_not((tups == 0).any(axis=1))]\n    return tups\n", "label": 0}
{"function": "\n\ndef log_int_fixed(n, prec, ln2=None):\n    '\\n    Fast computation of log(n), caching the value for small n,\\n    intended for zeta sums.\\n    '\n    if (n in log_int_cache):\n        (value, vprec) = log_int_cache[n]\n        if (vprec >= prec):\n            return (value >> (vprec - prec))\n    wp = (prec + 10)\n    if (wp <= LOG_TAYLOR_SHIFT):\n        if (ln2 is None):\n            ln2 = ln2_fixed(wp)\n        r = bitcount(n)\n        x = (n << (wp - r))\n        v = (log_taylor_cached(x, wp) + (r * ln2))\n    else:\n        v = to_fixed(mpf_log(from_int(n), (wp + 5)), wp)\n    if (n < MAX_LOG_INT_CACHE):\n        log_int_cache[n] = (v, wp)\n    return (v >> (wp - prec))\n", "label": 1}
{"function": "\n\ndef parse_host(host, port, resource):\n    if (not host.startswith('http')):\n        host = ('http://' + host)\n    url_pack = parse_url(host)\n    is_secure = (url_pack.scheme == 'https')\n    port = (port or url_pack.port or (443 if is_secure else 80))\n    url = ('%s:%d%s/%s' % (url_pack.hostname, port, url_pack.path, resource))\n    return (is_secure, url)\n", "label": 0}
{"function": "\n\ndef print_scoreboard(self):\n    'Print object as a scoreboard.'\n    output = ''\n    innings = []\n    away = []\n    home = []\n    for x in self:\n        innings.append(x['inning'])\n        away.append(x['away'])\n        home.append(x['home'])\n    output += 'Inning\\t'\n    for x in innings:\n        output += (str(x) + ' ')\n    output += '\\n'\n    for x in innings:\n        output += '---'\n    output += '\\nAway\\t'\n    for (y, x) in enumerate(away, start=1):\n        if (y >= 10):\n            output += (str(x) + '  ')\n        else:\n            output += (str(x) + ' ')\n    output += '\\nHome\\t'\n    for (y, x) in enumerate(home, start=1):\n        if (y >= 10):\n            output += (str(x) + '  ')\n        else:\n            output += (str(x) + ' ')\n    return output\n", "label": 1}
{"function": "\n\ndef include(self, other):\n    for (name, (tp, quals)) in other._declarations.items():\n        if name.startswith('anonymous $enum_$'):\n            continue\n        kind = name.split(' ', 1)[0]\n        if (kind in ('struct', 'union', 'enum', 'anonymous', 'typedef')):\n            self._declare(name, tp, included=True, quals=quals)\n    for (k, v) in other._int_constants.items():\n        self._add_constants(k, v)\n", "label": 0}
{"function": "\n\ndef body(self):\n    for chunk in self.gen_chunks(self.envelope.file_open(self.name)):\n        (yield chunk)\n    for chunk in self.gen_chunks(self.text):\n        (yield chunk)\n    for chunk in self.gen_chunks(self.envelope.file_close()):\n        (yield chunk)\n    for chunk in self.close():\n        (yield chunk)\n", "label": 0}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    (images, hidacts, frows, fcols) = node.inputs\n    (ishape, hshape, frowshp, fcolshp) = shapes\n    (igroups, icolors_per_group, irows, icols, icount) = ishape\n    (hgroups, hcolors_per_group, hrows, hcols, hcount) = hshape\n    fmodulesR = hrows\n    fmodulesC = hcols\n    fcolors = icolors_per_group\n    fgroups = hgroups\n    filters_per_group = hcolors_per_group\n    fshape = (fmodulesR, fmodulesC, fcolors, frows, fcols, fgroups, filters_per_group)\n    if (not_symbolic(irows, icols) and (irows != icols)):\n        raise NotImplementedError('non-square image argument', (irows, icols))\n    if (not_symbolic(hrows, hcols) and (hrows != hcols)):\n        raise NotImplementedError('non-square filter shape', (hrows, hcols))\n    if (not_symbolic(icount, hcount) and (icount != hcount)):\n        raise NotImplementedError('different number of images', (icount, hcount))\n    if (not_symbolic(igroups, hgroups) and (igroups != hgroups)):\n        raise ValueError('hgroups must match igroups', igroups, hgroups)\n    return [fshape]\n", "label": 1}
{"function": "\n\ndef visit_IfStatement(self, node):\n    if isinstance(self.m, (module.GenerateFor, module.GenerateIf)):\n        return self._visit_GenerateIf(node)\n    condition = self.visit(node.cond)\n    true_statement = self.visit(node.true_statement)\n    false_statement = (self.visit(node.false_statement) if (node.false_statement is not None) else None)\n    true_statement = to_tuple(true_statement)\n    false_statement = (to_tuple(false_statement) if (false_statement is not None) else None)\n    _if = vtypes.If(condition)\n    _if = _if(*true_statement)\n    if (false_statement is not None):\n        _if = _if(*false_statement)\n    return _if\n", "label": 0}
{"function": "\n\ndef _init_request_urls(self, api_urls):\n    '\\n        Returns a list of the API URLs.\\n        '\n    if (not isinstance(api_urls, (str, list, tuple))):\n        raise TypeError('api_urls needs to be string or iterable!')\n    if isinstance(api_urls, str):\n        api_urls = (api_urls,)\n    api_urls = list(api_urls)\n    for url in api_urls:\n        if ((not url.startswith('http://')) and (not url.startswith('https://'))):\n            raise ValueError(('URL \"%s\" contains an invalid or missing scheme' % url))\n    return list(api_urls)\n", "label": 1}
{"function": "\n\ndef _HasHeaders(self, args):\n    'Look at the args and decided if we expect headers or not.'\n    headers = True\n    for arg in args:\n        if (arg in ['--no-headers', 'h', '--no-heading']):\n            headers = False\n        elif (arg in ['--headers']):\n            headers = True\n        elif (('h' in arg) and (not arg.startswith('-')) and (',' not in arg)):\n            headers = False\n    return headers\n", "label": 1}
{"function": "\n\ndef load_app(app_path, wsgi_app_full_name):\n    if app_path:\n        absolute_path = os.path.abspath(os.path.expanduser(app_path))\n        log.debug(('Adding %s to sys.path' % absolute_path))\n        sys.path.append(absolute_path)\n    if wsgi_app_full_name:\n        log.info(('Loading WSGI application object: %s' % wsgi_app_full_name))\n        return import_object(wsgi_app_full_name)\n    app_loaders = IAppLoader.implementors()\n    log.debug('Found {0} loaders: {1}'.format(len(app_loaders), app_loaders))\n    for loader in app_loaders:\n        if loader.can_load(app_path):\n            log.info(('Using AppLoader: %s' % loader.__class__.__name__))\n            return loader.load_app(absolute_path, wsgi_app_full_name)\n    raise Exception(('No Loader found for app %s and no --wsgi-app option found\\n' % app_path))\n", "label": 0}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.set_package(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            self.set_capability(d.getPrefixedString())\n            continue\n        if (tt == 24):\n            self.set_status(d.getVarInt32())\n            continue\n        if (tt == 34):\n            self.set_internal_message(d.getPrefixedString())\n            continue\n        if (tt == 42):\n            self.set_admin_message(d.getPrefixedString())\n            continue\n        if (tt == 50):\n            self.set_error_message(d.getPrefixedString())\n            continue\n        if (tt == 58):\n            self.set_scheduled_time(d.getPrefixedString())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef __init__(self, max_size=DEFAULT_MAX_SIZE, values=None, on_miss=None):\n    if (max_size <= 0):\n        raise ValueError(('expected max_size > 0, not %r' % max_size))\n    self.hit_count = self.miss_count = self.soft_miss_count = 0\n    self.max_size = max_size\n    self._lock = RLock()\n    self._init_ll()\n    if ((on_miss is not None) and (not callable(on_miss))):\n        raise TypeError(('expected on_miss to be a callable (or None), not %r' % on_miss))\n    self.on_miss = on_miss\n    if values:\n        self.update(values)\n", "label": 0}
{"function": "\n\ndef combine_trailing_code_chunks(partitions):\n    chunks = list(partitions)\n    NON_COMBINABLE = (CodeType.IMPORT, CodeType.PRE_IMPORT_CODE)\n    if (chunks and (chunks[(- 1)].code_type not in NON_COMBINABLE)):\n        src = chunks.pop().src\n        while (chunks and (chunks[(- 1)].code_type not in NON_COMBINABLE)):\n            src = (chunks.pop().src + src)\n        chunks.append(CodePartition(CodeType.CODE, src))\n    return chunks\n", "label": 0}
{"function": "\n\ndef save(self, user, account):\n    'Save or update account mailbox.'\n    if (self.cleaned_data['email'] == ''):\n        return None\n    if self.cleaned_data['quota_act']:\n        self.cleaned_data['quota'] = None\n    if ((not hasattr(self, 'mb')) or (self.mb is None)):\n        self.create_mailbox(user, account)\n    else:\n        self.cleaned_data['use_domain_quota'] = self.cleaned_data['quota_act']\n        self.mb.update_from_dict(user, self.cleaned_data)\n    events.raiseEvent('SaveExtraFormFields', 'mailform', self.mb, self.cleaned_data)\n    account.email = self.cleaned_data['email']\n    account.save()\n    self._update_aliases(user, account)\n    return self.mb\n", "label": 0}
{"function": "\n\ndef delete_buf(self, path):\n    if (not utils.is_shared(path)):\n        msg.error(('Skipping deleting %s because it is not in shared path %s.' % (path, G.PROJECT_PATH)))\n        return\n    if os.path.isdir(path):\n        for (dirpath, dirnames, filenames) in os.walk(path):\n            dirnames[:] = [d for d in dirnames if (d[0] != '.')]\n            for f in filenames:\n                f_path = os.path.join(dirpath, f)\n                if (f[0] == '.'):\n                    msg.log(('Not deleting buf for hidden file %s' % f_path))\n                else:\n                    self.delete_buf(f_path)\n        return\n    buf_to_delete = self.get_buf_by_path(path)\n    if (buf_to_delete is None):\n        msg.error(('%s is not in this workspace' % path))\n        return\n    msg.log('deleting buffer ', utils.to_rel_path(path))\n    event = {\n        'name': 'delete_buf',\n        'id': buf_to_delete['id'],\n    }\n    G.AGENT.send(event)\n", "label": 1}
{"function": "\n\ndef finalize(self):\n    if (self.ct == 0):\n        return\n    elif (self.ct == 1):\n        return 0\n    prev = min_diff = None\n    while self.heap:\n        if (min_diff is None):\n            if (prev is None):\n                prev = heapq.heappop(self.heap)\n                continue\n        curr = heapq.heappop(self.heap)\n        diff = (curr - prev)\n        if ((min_diff is None) or (min_diff > diff)):\n            min_diff = diff\n        prev = curr\n    return min_diff\n", "label": 1}
{"function": "\n\ndef decrypt(self, id, data):\n    if (data[0] != self.TYPE):\n        raise IntegrityError('Invalid encryption envelope')\n    hmac = memoryview(data)[1:33]\n    if (memoryview(HMAC(self.enc_hmac_key, memoryview(data)[33:], sha256).digest()) != hmac):\n        raise IntegrityError('Encryption envelope checksum mismatch')\n    self.dec_cipher.reset(iv=(PREFIX + data[33:41]))\n    data = zlib.decompress(self.dec_cipher.decrypt(data[41:]))\n    if (id and (HMAC(self.id_key, data, sha256).digest() != id)):\n        raise IntegrityError('Chunk id verification failed')\n    return data\n", "label": 0}
{"function": "\n\ndef subset_byname(self, variables=None, samples=None):\n    'Returns a subset of the dataset (and metadata).\\n\\n        Same as Dataset.subset() except that variables and samples can be\\n        specified by their names.  \\n        \\n        Some examples:\\n\\n            - d.subset(variables=[\\'shh\\', \\'genex\\'])\\n            - s.subset(samples=[\"control%d\" % i for i in xrange(10)])\\n\\n        '\n    vardict = dict(((v.name, i) for (i, v) in enumerate(self.variables)))\n    sampledict = dict(((s.name, i) for (i, s) in enumerate(self.samples)))\n    variables = ([vardict[v] for v in variables] if variables else variables)\n    samples = ([sampledict[s] for s in samples] if samples else samples)\n    return self.subset(variables, samples)\n", "label": 1}
{"function": "\n\ndef run(self, ctx):\n    argv = ctx.command_argv\n    p = ctx.options_context.parser\n    (o, a) = p.parse_args(argv)\n    if o.help:\n        p.print_help()\n        return\n    n = ctx.build_node.make_node(BUILD_MANIFEST_PATH)\n    build_manifest = BuildManifest.from_file(n.abspath())\n    scheme = ctx.retrieve_configured_scheme()\n    build_manifest.update_paths(scheme)\n    node_sections = build_manifest.resolve_paths_with_destdir(ctx.build_node)\n    if o.list_files:\n        for (kind, source, target) in iter_files(node_sections):\n            print(target.abspath())\n        return\n    if o.transaction:\n        trans = TransactionLog('transaction.log')\n        try:\n            for (kind, source, target) in iter_files(node_sections):\n                trans.copy(source.abspath(), target.abspath(), kind)\n        finally:\n            trans.close()\n    else:\n        for (kind, source, target) in iter_files(node_sections):\n            copy_installer(source.abspath(), target.abspath(), kind)\n", "label": 1}
{"function": "\n\ndef init_asd_db():\n    global asd_db\n    if (not asd_db):\n        try:\n            response = requests.get('https://raw.github.com/stefanschmidt/warranty/master/asdcheck')\n            for (model, val) in [model_str.strip().split(':') for model_str in response.content.split('\\n') if model_str.strip()]:\n                asd_db[model] = val\n        except:\n            asd_db = {\n                \n            }\n", "label": 1}
{"function": "\n\ndef initialize(self, taskParent, override=None):\n    \"This method initializes a task for (re)use.  taskParent is the\\n        object instance of the parent task, or a 'task environment' (something\\n        that runs tasks).\\n        If subclass overrides this method, it should call the superclass\\n        method at some point.\\n\\n        - Copy shared data from taskParent, overriding items from _override_\\n          if they are present there ('contagion' of task values).\\n        - Generate a unique tag, to be used with the Gen2 Monitor.\\n        - Clear done event, initialize times and result.\\n        \"\n    if (taskParent and hasattr(taskParent, 'shares')):\n        for var in taskParent.shares:\n            if (override and (var in override)):\n                self.__dict__[var] = override[var]\n            else:\n                self.__dict__[var] = taskParent.__dict__[var]\n    else:\n        pass\n    if (not self.tag):\n        try:\n            self.tag = ((str(taskParent) + '.') + self.tagger.get_tag(self))\n        except:\n            self.tag = get_tag(taskParent)\n    self.ev_done.clear()\n    self.starttime = time.time()\n    self.endtime = 0\n    self.totaltime = 0\n    self.result = None\n    return self.tag\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    if ((self._mock_name is None) and (self._spec_class is None)):\n        return object.__repr__(self)\n    name_string = ''\n    spec_string = ''\n    if (self._mock_name is not None):\n\n        def get_name(name):\n            if (name is None):\n                return 'mock'\n            return name\n        parent = self._mock_parent\n        name = self._mock_name\n        while (parent is not None):\n            name = ((get_name(parent._mock_name) + '.') + name)\n            parent = parent._mock_parent\n        name_string = (' name=%r' % name)\n    if (self._spec_class is not None):\n        spec_string = ' spec=%r'\n        if self._spec_set:\n            spec_string = ' spec_set=%r'\n        spec_string = (spec_string % self._spec_class.__name__)\n    return (\"<%s%s%s id='%s'>\" % (type(self).__name__, name_string, spec_string, id(self)))\n", "label": 1}
{"function": "\n\ndef __str__(self, prefix='', printElemNumber=0):\n    res = ''\n    if self.has_class_or_file_name_:\n        res += (prefix + ('class_or_file_name: %s\\n' % self.DebugFormatString(self.class_or_file_name_)))\n    if self.has_line_number_:\n        res += (prefix + ('line_number: %s\\n' % self.DebugFormatInt32(self.line_number_)))\n    if self.has_function_name_:\n        res += (prefix + ('function_name: %s\\n' % self.DebugFormatString(self.function_name_)))\n    cnt = 0\n    for e in self.variables_:\n        elm = ''\n        if printElemNumber:\n            elm = ('(%d)' % cnt)\n        res += (prefix + ('variables%s <\\n' % elm))\n        res += e.__str__((prefix + '  '), printElemNumber)\n        res += (prefix + '>\\n')\n        cnt += 1\n    return res\n", "label": 1}
{"function": "\n\ndef tograph(self, filename='fsm.png', nolabel=False):\n    import pygraphviz as pgv\n    graph = pgv.AGraph(directed=True)\n    for (src, dstdict) in self.fsm.items():\n        graph.add_node(str(src), label=str(src))\n        for (cond, dst) in dstdict.items():\n            graph.add_node(str(dst), label=str(dst))\n            if nolabel:\n                graph.add_edge(str(src), str(dst), label='')\n            else:\n                graph.add_edge(str(src), str(dst), label=str(cond))\n    srcs = self.fsm.keys()\n    for src in srcs:\n        for (cond, dst) in self.any.items():\n            graph.add_node(str(dst), label=str(dst))\n            if nolabel:\n                graph.add_edge(str(src), str(dst), label='')\n            else:\n                graph.add_edge(str(src), str(dst), label=str(cond))\n    graph.write('file.dot')\n    graph.layout(prog='dot')\n    graph.draw(filename)\n", "label": 1}
{"function": "\n\ndef arg_opts(self, name, default, taken_names):\n    opts = {\n        \n    }\n    opts['positional'] = (name in self.positional)\n    opts['optional'] = (name in self.optional)\n    if ('_' in name):\n        opts['attr_name'] = name\n        name = translate_underscores(name)\n    names = [name]\n    if self.auto_shortflags:\n        for char in name:\n            if (not ((char == name) or (char in taken_names))):\n                names.append(char)\n                break\n    opts['names'] = names\n    if (default not in (None, NO_DEFAULT)):\n        opts['kind'] = type(default)\n        opts['default'] = default\n    if (name in self.help):\n        opts['help'] = self.help[name]\n    return opts\n", "label": 1}
{"function": "\n\ndef add_website_theme(context):\n    bootstrap = frappe.get_hooks('bootstrap')[0]\n    web_include_css = context.web_include_css\n    context.theme = frappe._dict()\n    if (not context.disable_website_theme):\n        website_theme = get_active_theme()\n        context.theme = ((website_theme and website_theme.as_dict()) or frappe._dict())\n        if website_theme:\n            if website_theme.bootstrap:\n                bootstrap = website_theme.bootstrap\n            context.no_sidebar = website_theme.no_sidebar\n            context.web_include_css = (['website_theme.css'] + context.web_include_css)\n    context.web_include_css = ([bootstrap] + context.web_include_css)\n", "label": 1}
{"function": "\n\ndef format(self, record):\n    'Uses contextstring if request_id is set, otherwise default.'\n    for key in ('instance', 'color'):\n        if (key not in record.__dict__):\n            record.__dict__[key] = ''\n    if record.__dict__.get('request_id', None):\n        self._fmt = CONF.logging_context_format_string\n    else:\n        self._fmt = CONF.logging_default_format_string\n    if ((record.levelno == logging.DEBUG) and CONF.logging_debug_format_suffix):\n        self._fmt += (' ' + CONF.logging_debug_format_suffix)\n    if record.exc_info:\n        record.exc_text = self.formatException(record.exc_info, record)\n    return logging.Formatter.format(self, record)\n", "label": 1}
{"function": "\n\ndef __init__(self, uri, **config):\n    self.uri = uri\n    prefix = config.pop('prefix', '')\n    log.info(\"Connecting MongoEngine to '%s'.\", _safe_uri_replace.sub('\\\\1://\\\\2@', uri))\n    connection = self.connection = dict(tz_aware=True)\n    (scheme, parts) = uri.split('://', 1)\n    (parts, self.db) = parts.split('/', 1)\n    (auth, host) = (parts.split('@', 1) if ('@' in parts) else (None, parts))\n    if (scheme != 'mongodb'):\n        raise Exception(\"The URL must begin with 'mongodb://'!\")\n    (connection['host'], connection['port']) = (host.split(':') if (':' in host) else (host, '27017'))\n    connection['port'] = int(connection['port'])\n    if auth:\n        (connection['username'], _, connection['password']) = auth.partition(':')\n    for (k, v) in items(config):\n        (pfx, _, k) = k.rpartition('.')\n        if ((pfx != prefix) or (k in ('engine', 'model', 'ready'))):\n            continue\n        connection[k] = (int(v) if v.isdigit() else v)\n    self.cb = config.get('ready', None)\n", "label": 1}
{"function": "\n\ndef _assert_contains(needle, haystack, invert):\n    matched = re.search(needle, haystack, re.M)\n    if ((invert and matched) or ((not invert) and (not matched))):\n        raise AssertionError((\"r'%s' %sfound in '%s'\" % (needle, ('' if invert else 'not '), haystack)))\n", "label": 1}
{"function": "\n\ndef checkArguments(self, category, arguments, allowedAttributes, isInput, isOutput):\n    for argumentInformation in arguments:\n        helpInfo = (self.name, 'arguments', category)\n        attributes = argumentAttributes()\n        (self.success, attributes) = methods.checkAttributes(argumentInformation, allowedAttributes, attributes, self.allowTermination, helpInfo)\n        if (attributes.longFormArgument in self.arguments):\n            if self.allowTermination:\n                self.errors.repeatedLongFormArgument(helpInfo, attributes.longFormArgument)\n            else:\n                self.success = False\n                return\n        if (attributes.shortFormArgument in self.shortFormArguments):\n            if self.allowTermination:\n                self.errors.repeatedShortFormArgument(helpInfo, attributes.longFormArgument, attributes.shortFormArgument)\n            else:\n                self.success = False\n                return\n        attributes.category = category\n        if isInput:\n            attributes.isInput = True\n        elif isOutput:\n            attributes.isOutput = True\n        self.arguments[attributes.longFormArgument] = attributes\n        self.shortFormArguments.append(attributes.shortFormArgument)\n", "label": 1}
{"function": "\n\ndef val(self, env):\n    'Returns the value of this variable'\n    if self.name:\n        return env[self.name]\n    if (self.constuction[0] == 'Gen+'):\n        gather = [v.val(env) for v in self.constuction[1:]]\n        Sum = None\n        for v in gather:\n            if (Sum is None):\n                Sum = v\n            else:\n                Sum = (v + Sum)\n        return Sum\n    if (self.constuction[0] == 'Gen*'):\n        base = self.constuction[1].val(env)\n        exps = [v.val(env) for v in self.constuction[2:]]\n        Prod = 1\n        for v in exps:\n            Prod = (v * Prod)\n        return (Prod * base)\n    raise Exception('Unknown case')\n", "label": 1}
{"function": "\n\ndef add_data(self, group, name, data):\n    if self.isgroup(data):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'group',\n            'class': data.__class__.__name__,\n        })\n        for comp in dir(data):\n            self.add_data(g, comp, getattr(data, comp))\n    elif isinstance(data, (list, tuple)):\n        dtype = 'list'\n        if isinstance(data, tuple):\n            dtype = 'tuple'\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': dtype,\n        })\n        for (ix, comp) in enumerate(data):\n            iname = ('item%i' % ix)\n            self.add_data(g, iname, comp)\n    elif isinstance(data, dict):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'dict',\n        })\n        for (key, val) in data.items():\n            self.add_data(g, key, val)\n    elif isParameter(data):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'parameter',\n        })\n        self.add_h5dataset(g, 'json', data.asjson())\n    else:\n        d = self.add_h5dataset(group, name, data)\n", "label": 1}
{"function": "\n\ndef get_definition(self, value):\n    '\\n        Get the definition site for the given variable name or instance.\\n        A Expr instance is returned.\\n        '\n    while True:\n        if isinstance(value, ir.Var):\n            name = value.name\n        elif isinstance(value, str):\n            name = value\n        else:\n            return value\n        defs = self.definitions[name]\n        if (len(defs) == 0):\n            raise KeyError(('no definition for %r' % (name,)))\n        if (len(defs) > 1):\n            raise KeyError(('more than one definition for %r' % (name,)))\n        value = defs[0]\n", "label": 1}
{"function": "\n\ndef __init__(self, *fields, **attributes):\n    if self.abstract:\n        raise TypeError('abstract nodes are not instanciable')\n    if fields:\n        if (len(fields) != len(self.fields)):\n            if (not self.fields):\n                raise TypeError(('%r takes 0 arguments' % self.__class__.__name__))\n            raise TypeError(('%r takes 0 or %d argument%s' % (self.__class__.__name__, len(self.fields), (((len(self.fields) != 1) and 's') or ''))))\n        for (name, arg) in izip(self.fields, fields):\n            setattr(self, name, arg)\n    for attr in self.attributes:\n        setattr(self, attr, attributes.pop(attr, None))\n    if attributes:\n        raise TypeError(('unknown attribute %r' % iter(attributes).next()))\n", "label": 1}
{"function": "\n\ndef load(self, fname=None, name=None):\n    if (name is None):\n        self._mName = fname\n    else:\n        self._mName = name\n    if (fname is not None):\n        if os.path.exists(fname):\n            self._fhandle = os.path.abspath(fname)\n        else:\n            self._fhandle = os.path.join(LAUNCH_PATH, 'Features', 'HaarCascades', fname)\n            if (not os.path.exists(self._fhandle)):\n                logger.warning(('Could not find Haar Cascade file ' + fname))\n                logger.warning('Try running the function img.listHaarFeatures() to see what is available')\n                return None\n        self._mCascade = cv.Load(self._fhandle)\n        if HaarCascade._cache.has_key(self._fhandle):\n            self._mCascade = HaarCascade._cache[fname]\n            return\n        HaarCascade._cache[self._fhandle] = self._mCascade\n    else:\n        logger.warning('No file path mentioned.')\n", "label": 1}
{"function": "\n\ndef rType(value):\n    if (not isinstance(value, list)):\n        value = [value]\n    t = commonType(value)\n    if (t == list):\n        v = rinterface.SexpVector(map(rType, value), rinterface.LISTSXP)\n    elif (t == float):\n        v = rinterface.FloatSexpVector(value)\n    elif (t == int):\n        v = rinterface.IntSexpVector(value)\n    elif (t == bool):\n        v = rinterface.BoolSexpVector(value)\n    elif (t == str):\n        v = rinterface.StrSexpVector(value)\n    else:\n        v = None\n    return v\n", "label": 1}
{"function": "\n\ndef __init__(self, *items):\n    DiagramItem.__init__(self, 'g')\n    self.items = [wrapString(item) for item in items]\n    self.width = sum(((item.width + (20 if item.needsSpace else 0)) for item in self.items))\n    self.up = 0\n    self.down = 0\n    for item in self.items:\n        self.up = max(self.up, (item.up - self.yAdvance))\n        self.down = max(self.down, (item.down + self.yAdvance))\n        self.yAdvance += item.yAdvance\n    if DEBUG:\n        self.attrs['data-updown'] = '{0} {1}'.format(self.up, self.down)\n        self.attrs['data-type'] = 'sequence'\n", "label": 1}
{"function": "\n\ndef Parse(self, cmd, args, stdout, stderr, return_val, time_taken, knowledge_base):\n    'Parse the dpkg output.'\n    _ = (stderr, time_taken, args, knowledge_base)\n    self.CheckReturn(cmd, return_val)\n    column_lengths = []\n    i = 0\n    for (i, line) in enumerate(stdout.splitlines()):\n        if line.startswith('+++-'):\n            for col in line.split('-')[1:]:\n                if (not re.match('=*', col)):\n                    raise parsers.ParseError(('Invalid header parsing for %s at line %s' % (cmd, i)))\n                column_lengths.append(len(col))\n            break\n    if column_lengths:\n        remaining_lines = stdout.splitlines()[(i + 1):]\n        for (i, line) in enumerate(remaining_lines):\n            cols = line.split(None, len(column_lengths))\n            (status, name, version, arch, desc) = cols\n            if (status[1] == 'i'):\n                status = rdf_client.SoftwarePackage.InstallState.INSTALLED\n            else:\n                status = rdf_client.SoftwarePackage.InstallState.UNKNOWN\n            (yield rdf_client.SoftwarePackage(name=name, description=desc, version=version, architecture=arch, install_state=status))\n", "label": 1}
{"function": "\n\ndef compose_doc(self, current_line, edit):\n    params_match = re.search('def +[^ (]+[ (]*([^)]*)\\\\)?', current_line)\n    if params_match:\n        if re.search('def initialize*', current_line):\n            return self.initialize_doc(params_match, current_line)\n        else:\n            return self.method_doc(params_match, current_line)\n    params_match = re.search('class | module', current_line)\n    if params_match:\n        return self.class_doc(params_match, current_line)\n    params_match = re.search('[A-Z]+[ ]+=', current_line)\n    if params_match:\n        return self.const_doc(params_match, current_line)\n    params_match = re.search('attr_reader | attr_writer | attr_accessor ', current_line)\n    if params_match:\n        return self.attributes_doc(params_match, current_line)\n", "label": 1}
{"function": "\n\ndef get_function_signature(function, method=True):\n    signature = inspect.getargspec(function)\n    defaults = signature.defaults\n    if method:\n        args = signature.args[1:]\n    else:\n        args = signature.args\n    if defaults:\n        kwargs = zip(args[(- len(defaults)):], defaults)\n        args = args[:(- len(defaults))]\n    else:\n        kwargs = []\n    st = ('%s.%s(' % (function.__module__, function.__name__))\n    for a in args:\n        st += (str(a) + ', ')\n    for (a, v) in kwargs:\n        if (type(v) == str):\n            v = ((\"'\" + v) + \"'\")\n        st += (((str(a) + '=') + str(v)) + ', ')\n    if (kwargs or args):\n        return (st[:(- 2)] + ')')\n    else:\n        return (st + ')')\n", "label": 1}
{"function": "\n\ndef user_has_any_permission_for_instance(self, user, actions, instance):\n    if (('change' in actions) or ('delete' in actions)):\n        if self._check_perm(user, ['change'], collection=instance.collection):\n            return True\n        elif (self._check_perm(user, ['add'], collection=instance.collection) and (getattr(instance, self.owner_field_name) == user)):\n            return True\n        else:\n            return False\n    else:\n        return (user.is_active and user.is_superuser)\n", "label": 1}
{"function": "\n\ndef run(self, name, board_id, list_id, api_key=None, token=None):\n    if api_key:\n        self._set_creds(api_key=api_key, token=token)\n    cards = []\n    board = self._client().get_board(board_id)\n    lst = board.get_list(list_id)\n    for card in lst.list_cards():\n        if ((card.name == name) and (not card.closed)):\n            cards.append(card.id)\n    if (len(cards) == 0):\n        return False\n    else:\n        return cards\n", "label": 1}
{"function": "\n\ndef cmd(self, *args, **kwargs):\n    data = self.main(*args, **kwargs)\n    result = CommandResult()\n    for (key, value) in data.items():\n        result = result.add_line('Ran build plugin {plugin}', plugin=key)\n        if (not value):\n            continue\n        if (not isinstance(value, basestring)):\n            value = json.dumps(value, indent=4, sort_keys=True)\n        for line in value.split('\\n'):\n            if (not line.strip()):\n                continue\n            result = result.add_line('\\t{line}', line=line)\n    return result\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('RemoteControlInstance')\n    if (self.device is not None):\n        oprot.writeFieldBegin('device', TType.STRUCT, 1)\n        self.device.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.timeout is not None):\n        oprot.writeFieldBegin('timeout', TType.I32, 2)\n        oprot.writeI32(self.timeout)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef update_record(self, record, name=None, type=None, data=None, extra=None):\n    extra = (extra or {\n        \n    })\n    params = {\n        'z': record.zone.domain,\n        'id': record.id,\n    }\n    params['name'] = (name or record.name)\n    params['type'] = (type or record.type)\n    params['content'] = (data or record.data)\n    params['ttl'] = (extra.get('ttl', None) or record.extra['ttl'])\n    self.connection.set_context({\n        'zone_domain': record.zone.domain,\n    })\n    self.connection.set_context({\n        'record_id': record.id,\n    })\n    resp = self.connection.request(action='rec_edit', params=params)\n    item = resp.object['response']['rec']['obj']\n    record = self._to_record(zone=record.zone, item=item)\n    return record\n", "label": 1}
{"function": "\n\ndef check_hbase_cluster_status(self, cluster):\n    job = cluster.jobs['master']\n    if (job.running_tasks_count < 1):\n        job.last_status = Status.ERROR\n        job.last_message = 'No running masters!'\n    else:\n        active = 0\n        for task in job.running_tasks.itervalues():\n            if self.is_master_active(task):\n                cluster.entry = ('%s:%d' % (task.host, task.port))\n                version = self.get_latest_metric(task, 'hadoop:service=HBase,name=Info', 'version')\n                revision = self.get_latest_metric(task, 'hadoop:service=HBase,name=Info', 'revision')\n                cluster.version = ('%s, r%s' % (version, revision))\n                active += 1\n        if (active > 1):\n            job.last_status = Status.ERROR\n            job.last_message = 'Too many active masters!'\n        elif (active < 1):\n            job.last_status = Status.ERROR\n            job.last_message = 'No active masters!'\n        elif (job.running_tasks_count < 2):\n            pass\n    job = cluster.jobs['regionserver']\n    if (job.running_tasks_count < 3):\n        job.last_status = Status.ERROR\n        job.last_message = 'Too few running regionservers!'\n    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])\n", "label": 1}
{"function": "\n\ndef mkdir(self, path, parents=True, raise_if_exists=False):\n    if (raise_if_exists and self.isdir(path)):\n        raise FileAlreadyExists()\n    (_, key) = self._path_to_bucket_and_key(path)\n    if self._is_root(key):\n        return\n    key = self._add_path_delimiter(key)\n    if ((not parents) and (not self.isdir(os.path.dirname(key)))):\n        raise MissingParentDirectory()\n    return self.put_string('', self._add_path_delimiter(path))\n", "label": 1}
{"function": "\n\ndef status_done(self, result):\n    if (result is False):\n        self.view.set_status('git-status-index', '')\n        self.view.set_status('git-status-working', '')\n    else:\n        lines = [line for line in result.splitlines() if re.match('^[ MADRCU?!]{1,2}\\\\s+.*', line)]\n        index = [line[0] for line in lines if (not line[0].isspace())]\n        working = [line[1] for line in lines if (not line[1].isspace())]\n        self.view.set_status('git-status-index', ('index: ' + self.status_string(index)))\n        self.view.set_status('git-status-working', ('working: ' + self.status_string(working)))\n", "label": 1}
{"function": "\n\ndef _absolute_path(path, relative_to=None):\n    '\\n    Return an absolute path. In case ``relative_to`` is passed and ``path`` is\\n    not an absolute path, we try to prepend ``relative_to`` to ``path``and if\\n    that path exists, return that one\\n    '\n    if (path and os.path.isabs(path)):\n        return path\n    if (path and (relative_to is not None)):\n        _abspath = os.path.join(relative_to, path)\n        if os.path.isfile(_abspath):\n            log.debug(\"Relative path '{0}' converted to existing absolute path '{1}'\".format(path, _abspath))\n            return _abspath\n    return path\n", "label": 1}
{"function": "\n\ndef run(self, count=None, region=None, metadata=None):\n    if region:\n        cs = self.pyrax.connect_to_cloudservers(region=region)\n    else:\n        cs = self.pyrax.cloudservers\n    servers = cs.list()\n    result = []\n    for server in servers:\n        item = to_server_dict(server=server)\n        if metadata:\n            include = self._metadata_intersection(server=item, metadata=metadata)\n            if (not include):\n                continue\n        result.append(item['name'])\n    if count:\n        return result[0:count]\n    else:\n        return result\n", "label": 1}
{"function": "\n\n@throttle(0.1)\ndef update_panel(self):\n    window = sublime.active_window()\n    try:\n        result_panel = window.create_output_panel('test_runner')\n    except:\n        result_panel = window.get_output_panel('test_runner')\n    result_panel.set_syntax_file('Packages/Test Runner/TestRunnerOutput.tmLanguage')\n    result_panel.run_command('update_panel', {\n        'message': self.result['message'],\n    })\n    if ((self.result['failed'] > 0) or settings.get('show_panel_default', False)):\n        window.run_command('show_panel', {\n            'panel': 'output.test_runner',\n        })\n    elif (self.result['status'] == 'executed'):\n        window.run_command('hide_panel', {\n            'panel': 'output.test_runner',\n        })\n    if self.is_alive():\n        self.update_panel()\n", "label": 1}
{"function": "\n\ndef _process_dependent_arguments(self):\n    'Convert incoming configuration arguments to their\\n        proper form.\\n\\n        Callables are resolved, ORM annotations removed.\\n\\n        '\n    for attr in ('order_by', 'primaryjoin', 'secondaryjoin', 'secondary', '_user_defined_foreign_keys', 'remote_side'):\n        attr_value = getattr(self, attr)\n        if util.callable(attr_value):\n            setattr(self, attr, attr_value())\n    for attr in ('primaryjoin', 'secondaryjoin'):\n        val = getattr(self, attr)\n        if (val is not None):\n            setattr(self, attr, _orm_deannotate(expression._only_column_elements(val, attr)))\n    if ((self.order_by is not False) and (self.order_by is not None)):\n        self.order_by = [expression._only_column_elements(x, 'order_by') for x in util.to_list(self.order_by)]\n    self._user_defined_foreign_keys = util.column_set((expression._only_column_elements(x, 'foreign_keys') for x in util.to_column_set(self._user_defined_foreign_keys)))\n    self.remote_side = util.column_set((expression._only_column_elements(x, 'remote_side') for x in util.to_column_set(self.remote_side)))\n    self.target = self.mapper.mapped_table\n", "label": 1}
{"function": "\n\ndef test_assert_has_calls(self):\n    kalls1 = [call(1, 2), ({\n        'a': 3,\n    },), ((3, 4),), call(b=6), ('', (1,), {\n        'b': 6,\n    })]\n    kalls2 = [call.foo(), call.bar(1)]\n    kalls2.extend(call.spam().baz(a=3).call_list())\n    kalls2.extend(call.bam(set(), foo={\n        \n    }).fish([1]).call_list())\n    mocks = []\n    for mock in (Mock(), MagicMock()):\n        mock(1, 2)\n        mock(a=3)\n        mock(3, 4)\n        mock(b=6)\n        mock(1, b=6)\n        mocks.append((mock, kalls1))\n    mock = Mock()\n    mock.foo()\n    mock.bar(1)\n    mock.spam().baz(a=3)\n    mock.bam(set(), foo={\n        \n    }).fish([1])\n    mocks.append((mock, kalls2))\n    for (mock, kalls) in mocks:\n        for i in range(len(kalls)):\n            for step in (1, 2, 3):\n                these = kalls[i:(i + step)]\n                mock.assert_has_calls(these)\n                if (len(these) > 1):\n                    self.assertRaises(AssertionError, mock.assert_has_calls, list(reversed(these)))\n", "label": 1}
{"function": "\n\ndef fragment_count(self):\n    table = self.fragmentruntable.payload.fragment_run_entry_table\n    (first_fragment, end_fragment) = (None, None)\n    for (i, fragmentrun) in enumerate(table):\n        if (fragmentrun.discontinuity_indicator is not None):\n            if (fragmentrun.discontinuity_indicator == 0):\n                break\n            elif (fragmentrun.discontinuity_indicator > 0):\n                continue\n        if (first_fragment is None):\n            first_fragment = fragmentrun.first_fragment\n        end_fragment = fragmentrun.first_fragment\n        fragment_duration = (fragmentrun.first_fragment_timestamp + fragmentrun.fragment_duration)\n        if (self.timestamp > fragment_duration):\n            offset = ((self.timestamp - fragment_duration) / fragmentrun.fragment_duration)\n            end_fragment += int(offset)\n    if (first_fragment is None):\n        first_fragment = 1\n    if (end_fragment is None):\n        end_fragment = 1\n    return (first_fragment, end_fragment)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if isinstance(other, datetime):\n        other = Date(other)\n    if isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return (self.date == 'infinity')\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) == other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date == other.date.replace(tzinfo=self.tz))\n        return (self.date == other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            return False\n        else:\n            return self.__eq__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef performAction(action, workflow):\n    if (action.actionType == 'add'):\n        for operation in action.db_operations:\n            workflow.db_add_object(operation.db_data, operation.db_parentObjType, operation.db_parentObjId)\n    elif (action.actionType == 'change'):\n        for operation in action.db_operations:\n            workflow.db_change_object(operation.db_data, operation.db_parentObjType, operation.db_parentObjId)\n    elif (action.actionType == 'delete'):\n        for operation in action.operations:\n            workflow.db_delete_object(operation.db_objectId, operation.db_what, operation.db_parentObjType, operation.db_parentObjId)\n    else:\n        msg = (\"Unrecognized action type '%s'\" % action.db_actionType)\n        raise TypeError(msg)\n", "label": 1}
{"function": "\n\ndef onSelectDet(self, event=None, index=0, init=False, **kws):\n    if (index > 0):\n        self.det_fore = index\n    self.det_back = self.wids['bkg_det'].GetSelection()\n    if (self.det_fore == self.det_back):\n        self.det_back = 0\n    for i in range(1, (self.nmca + 1)):\n        dname = ('det%i' % i)\n        bcol = (220, 220, 220)\n        fcol = (0, 0, 0)\n        if (i == self.det_fore):\n            bcol = (60, 50, 245)\n            fcol = (240, 230, 100)\n        if (i == self.det_back):\n            bcol = (80, 200, 20)\n        self.wids[dname].SetBackgroundColour(bcol)\n        self.wids[dname].SetForegroundColour(fcol)\n    self.clear_mcas()\n    self.show_mca(init=init)\n    self.Refresh()\n", "label": 1}
{"function": "\n\ndef _ensure_holidays_span_datetime(self, dt):\n    if callable(self.holidays):\n        if ((self._holidaysGeneratorStart is None) or (dt < self._holidaysGeneratorStart)):\n            self._holidaysGeneratorStart = dt\n            self._holidaysGenerator = self.holidays(dt)\n        while ((len(self._holidays) == 0) or (dt > self._holidays[(- 1)])):\n            self._holidays.append(next(self._holidaysGenerator))\n", "label": 1}
{"function": "\n\ndef proxy_request(service_name, instance_name, method, path, body=None, headers=None):\n    target = get_env('TSURU_TARGET').rstrip('/')\n    token = get_env('TSURU_TOKEN')\n    if ((not target.startswith('http://')) and (not target.startswith('https://'))):\n        target = 'http://{}'.format(target)\n    url = '{}/services/{}/proxy/{}?callback={}'.format(target, service_name, instance_name, path)\n    if body:\n        body = json.dumps(body)\n    request = Request(method, url, data=body)\n    request.add_header('Authorization', ('bearer ' + token))\n    if headers:\n        for (header, value) in headers.items():\n            request.add_header(header, value)\n    return urllib2.urlopen(request, timeout=30)\n", "label": 1}
{"function": "\n\ndef gf_add(f, g, p, K):\n    '\\n    Add polynomials in ``GF(p)[x]``.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import ZZ\\n    >>> from sympy.polys.galoistools import gf_add\\n\\n    >>> gf_add([3, 2, 4], [2, 2, 2], 5, ZZ)\\n    [4, 1]\\n\\n    '\n    if (not f):\n        return g\n    if (not g):\n        return f\n    df = gf_degree(f)\n    dg = gf_degree(g)\n    if (df == dg):\n        return gf_strip([((a + b) % p) for (a, b) in zip(f, g)])\n    else:\n        k = abs((df - dg))\n        if (df > dg):\n            (h, f) = (f[:k], f[k:])\n        else:\n            (h, g) = (g[:k], g[k:])\n        return (h + [((a + b) % p) for (a, b) in zip(f, g)])\n", "label": 1}
{"function": "\n\ndef to_obj(self, return_obj=None, ns_info=None):\n    self._collect_ns_info(ns_info)\n    artifact_obj = artifact_binding.ArtifactObjectType()\n    super(Artifact, self).to_obj(return_obj=artifact_obj, ns_info=ns_info)\n    if self.packaging:\n        packaging = artifact_binding.PackagingType()\n        for p in self.packaging:\n            p_obj = p.to_obj(ns_info=ns_info)\n            if isinstance(p, Compression):\n                packaging.add_Compression(p_obj)\n            elif isinstance(p, Encryption):\n                packaging.add_Encryption(p_obj)\n            elif isinstance(p, Encoding):\n                packaging.add_Encoding(p_obj)\n            else:\n                raise ValueError(('Unsupported Packaging Type: %s' % type(p)))\n        artifact_obj.Packaging = packaging\n    if self.packed_data:\n        artifact_obj.Raw_Artifact = RawArtifact(self.packed_data).to_obj(ns_info=ns_info)\n    artifact_obj.type_ = self.type_\n    return artifact_obj\n", "label": 1}
{"function": "\n\ndef _verify_source_estimate_compat(a, b):\n    'Make sure two SourceEstimates are compatible for arith. operations'\n    compat = False\n    if (len(a.vertices) == len(b.vertices)):\n        if all((np.array_equal(av, vv) for (av, vv) in zip(a.vertices, b.vertices))):\n            compat = True\n    if (not compat):\n        raise ValueError('Cannot combine SourceEstimates that do not have the same vertices. Consider using stc.expand().')\n    if (a.subject != b.subject):\n        raise ValueError(('source estimates do not have the same subject names, %r and %r' % (a.subject, b.subject)))\n", "label": 1}
{"function": "\n\ndef processLine(result, line):\n    initLine(result, line)\n    if (result['mode'] == INDENT_MODE):\n        result['trackingIndent'] = ((len(result['parenStack']) != 0) and (not result['isInStr']))\n    elif (result['mode'] == PAREN_MODE):\n        result['trackingIndent'] = (not result['isInStr'])\n    chars = (line + NEWLINE)\n    for c in chars:\n        processChar(result, c)\n    if (result['lineNo'] == result['parenTrail']['lineNo']):\n        finishNewParenTrail(result)\n", "label": 1}
{"function": "\n\ndef do_copy(self, new_ids=False, id_scope=None, id_remap=None):\n    cp = DBWorkflowExec()\n    cp._db_id = self._db_id\n    cp._db_user = self._db_user\n    cp._db_ip = self._db_ip\n    cp._db_vt_version = self._db_vt_version\n    cp._db_ts_start = self._db_ts_start\n    cp._db_ts_end = self._db_ts_end\n    cp._db_parent_id = self._db_parent_id\n    cp._db_parent_type = self._db_parent_type\n    cp._db_parent_version = self._db_parent_version\n    cp._db_name = self._db_name\n    if (self._db_module_execs is None):\n        cp._db_module_execs = []\n    else:\n        cp._db_module_execs = [v.do_copy(new_ids, id_scope, id_remap) for v in self._db_module_execs]\n    if new_ids:\n        new_id = id_scope.getNewId(self.vtType)\n        if (self.vtType in id_scope.remap):\n            id_remap[(id_scope.remap[self.vtType], self._db_id)] = new_id\n        else:\n            id_remap[(self.vtType, self._db_id)] = new_id\n        cp._db_id = new_id\n    for v in cp._db_module_execs:\n        cp.db_module_execs_id_index[v._db_id] = v\n    cp.is_dirty = self.is_dirty\n    cp.is_new = self.is_new\n    return cp\n", "label": 1}
{"function": "\n\ndef bind_port(self, context):\n    LOG.debug('Attempting to bind port %(port)s on network %(network)s', {\n        'port': context.current['id'],\n        'network': context.network.current['id'],\n    })\n    vnic_type = context.current.get(portbindings.VNIC_TYPE, portbindings.VNIC_NORMAL)\n    if (vnic_type not in self.supported_vnic_types):\n        LOG.debug('Refusing to bind due to unsupported vnic_type: %s', vnic_type)\n        return\n    vif_type = self.vnic_type_for_vif_type.get(vnic_type, VIF_TYPE_HW_VEB)\n    if (not self._check_supported_pci_vendor_device(context)):\n        LOG.debug('Refusing to bind due to unsupported pci_vendor device')\n        return\n    if (vnic_type == portbindings.VNIC_DIRECT_PHYSICAL):\n        self.try_to_bind(context, None, vif_type)\n        return\n    for agent in context.host_agents(self.agent_type):\n        LOG.debug('Checking agent: %s', agent)\n        if agent['alive']:\n            if self.try_to_bind(context, agent, vif_type):\n                return\n        else:\n            LOG.warning(_LW('Attempting to bind with dead agent: %s'), agent)\n", "label": 1}
{"function": "\n\ndef on_query_context(self, view, key, operator, operand, match_all):\n\n    def test(a):\n        if (operator == sublime.OP_EQUAL):\n            return (a == operand)\n        if (operator == sublime.OP_NOT_EQUAL):\n            return (a != operand)\n        return False\n    if (key == 'i_search_active'):\n        return test((isearch_info_for(view) is not None))\n    if (key == 'sbp_has_visible_mark'):\n        if (not SettingsManager.get('sbp_cancel_mark_enabled', False)):\n            return False\n        return (CmdUtil(view).state.mark_ring.has_visible_mark() == operand)\n    if (key == 'sbp_use_alt_bindings'):\n        return test(SettingsManager.get('sbp_use_alt_bindings'))\n    if (key == 'sbp_use_super_bindings'):\n        return test(SettingsManager.get('sbp_use_super_bindings'))\n    if (key == 'sbp_alt+digit_inserts'):\n        return test((SettingsManager.get('sbp_alt+digit_inserts') or (not SettingsManager.get('sbp_use_alt_bindings'))))\n    if (key == 'sbp_has_prefix_argument'):\n        return test(CmdUtil(view).has_prefix_arg())\n", "label": 1}
{"function": "\n\ndef respond(self, message, user=None):\n    if ('brb' in message.lower()):\n        matches = re.findall(self.regex, message.lower())\n        if matches:\n            now = datetime.now()\n            due = (now + timedelta(minutes=int(matches[0])))\n            self.memory[user] = due\n    elif ('all: back' in message.lower()):\n        if (user in self.memory.keys()):\n            due = self.memory[user]\n            now = datetime.now()\n            if (now > due):\n                message = ((user + ': ') + 'You are late. :)')\n                self.speak(message)\n            self.memory.pop(user)\n    else:\n        pass\n", "label": 1}
{"function": "\n\ndef get_recent(thing1=None, thing2=None):\n    ctx = context.get_context()\n    if (thing1 is None):\n        return [k for k in ctx.recent.keys()]\n    if (thing2 is None):\n        if isinstance(ctx.recent[thing1], (dict, cache.Cache)):\n            return dict([(repr(k), list(v)) for (k, v) in ctx.recent[thing1].items()])\n        else:\n            return list(ctx.recent[thing1])\n    elif isinstance(ctx.recent[thing1], (dict, cache.Cache)):\n        return dict([(repr(k), list(v)) for (k, v) in ctx.recent[thing1].items() if (thing2 in str(k))])\n    else:\n        return (('{\"error\":\"' + thing1) + ' recent data is not dict.\"}')\n", "label": 1}
{"function": "\n\ndef read(self, expressions):\n    data = {\n        \n    }\n    for i in expressions.split('|'):\n        (name, type) = i.split(':')\n        if (type == 'byte'):\n            data[name] = self.read_byte()\n        if (type == 'ubyte'):\n            data[name] = self.read_ubyte()\n        if (type == 'short'):\n            data[name] = self.read_short()\n        if (type == 'ushort'):\n            data[name] = self.read_ushort()\n        if (type == 'double'):\n            data[name] = self.read_double()\n        if (type == 'position'):\n            data[name] = self.read_position()\n    return data\n", "label": 1}
{"function": "\n\ndef _var_beta_panel(y, x, beta, xx, rmse, cluster_axis, nw_lags, nobs, df, nw_overlap):\n    xx_inv = math.inv(xx)\n    yv = y.values\n    if (cluster_axis is None):\n        if (nw_lags is None):\n            return (xx_inv * (rmse ** 2))\n        else:\n            resid = (yv - np.dot(x.values, beta))\n            m = (x.values.T * resid).T\n            xeps = math.newey_west(m, nw_lags, nobs, df, nw_overlap)\n            return np.dot(xx_inv, np.dot(xeps, xx_inv))\n    else:\n        Xb = np.dot(x.values, beta).reshape((len(x.values), 1))\n        resid = DataFrame((yv[:, None] - Xb), index=y.index, columns=['resid'])\n        if (cluster_axis == 1):\n            x = x.swaplevel(0, 1).sortlevel(0)\n            resid = resid.swaplevel(0, 1).sortlevel(0)\n        m = _group_agg((x.values * resid.values), x.index._bounds, (lambda x: np.sum(x, axis=0)))\n        if (nw_lags is None):\n            nw_lags = 0\n        xox = 0\n        for i in range(len(x.index.levels[0])):\n            xox += math.newey_west(m[i:(i + 1)], nw_lags, nobs, df, nw_overlap)\n        return np.dot(xx_inv, np.dot(xox, xx_inv))\n", "label": 1}
{"function": "\n\ndef parse(self, data):\n    new = []\n    for part in data.split(self.sep):\n        if (part == '*'):\n            new.append((self.MIN, self.MAX))\n            break\n        tmplist = []\n        for subp in part.split(self.rng):\n            s = subp.strip()\n            tmplist.append(int(s))\n        if (len(tmplist) == 1):\n            new.append((tmplist[0], tmplist[0]))\n        elif ((len(tmplist) == 2) and (tmplist[0] <= tmplist[1])):\n            new.append((tmplist[0], tmplist[1]))\n        else:\n            raise ValueError(('Unable to parse: %r' % (data,)))\n    self.pairs.extend(new)\n    self.normalize()\n", "label": 1}
{"function": "\n\ndef __init__(self, module, module_filename=None, template=None, template_filename=None, module_source=None, template_source=None, output_encoding=None, encoding_errors='strict', disable_unicode=False, bytestring_passthrough=False, format_exceptions=False, error_handler=None, lookup=None, cache_args=None, cache_impl='beaker', cache_enabled=True, cache_type=None, cache_dir=None, cache_url=None):\n    self.module_id = re.sub('\\\\W', '_', module._template_uri)\n    self.uri = module._template_uri\n    self.input_encoding = module._source_encoding\n    self.output_encoding = output_encoding\n    self.encoding_errors = encoding_errors\n    self.disable_unicode = disable_unicode\n    self.bytestring_passthrough = (bytestring_passthrough or disable_unicode)\n    self.enable_loop = module._enable_loop\n    if (compat.py3k and disable_unicode):\n        raise exceptions.UnsupportedError('Mako for Python 3 does not support disabling Unicode')\n    elif (output_encoding and disable_unicode):\n        raise exceptions.UnsupportedError('output_encoding must be set to None when disable_unicode is used.')\n    self.module = module\n    self.filename = template_filename\n    ModuleInfo(module, module_filename, self, template_filename, module_source, template_source)\n    self.callable_ = self.module.render_body\n    self.format_exceptions = format_exceptions\n    self.error_handler = error_handler\n    self.lookup = lookup\n    self._setup_cache_args(cache_impl, cache_enabled, cache_args, cache_type, cache_dir, cache_url)\n", "label": 1}
{"function": "\n\ndef update(self, keys):\n    '\\n        Update arrow position.\\n        '\n    if self.allow_input:\n        if (keys[pg.K_DOWN] and (not keys[pg.K_UP]) and (self.index == 0)):\n            self.index = 1\n            self.allow_input = False\n            self.notify(c.CLICK)\n        elif (keys[pg.K_UP] and (not keys[pg.K_DOWN]) and (self.index == 1)):\n            self.index = 0\n            self.allow_input = False\n            self.notify(c.CLICK)\n        self.rect.y = self.pos_list[self.index]\n    if ((not keys[pg.K_DOWN]) and (not keys[pg.K_UP])):\n        self.allow_input = True\n", "label": 1}
{"function": "\n\ndef post(self, message, card=None, color=None, notify=False):\n    params = {\n        'notify': notify,\n    }\n    if self.author:\n        params['from'] = self.author[:25]\n    if (notify and self.targets):\n        message = ((self.targets + ': ') + message)\n    if card:\n        message += '\\n[card attached]'\n    params['message'] = message\n    params['message_format'] = 'text'\n    if card:\n        card['id'] = uuid.uuid4().urn\n        params['card'] = card\n    if color:\n        params['color'] = color\n    headers = {\n        'Authorization': ('Bearer ' + self.token),\n    }\n    r = requests.post('https://api.hipchat.com/v2/room/{0}/notification'.format(self.room_id), json=params, headers=headers)\n    r.raise_for_status()\n", "label": 1}
{"function": "\n\ndef convert_mode(in_text, str_len=0):\n    arduino_settings = settings.get_arduino_settings()\n    text = ''\n    display_mode = arduino_settings.get('display_mode', 'Text')\n    if (display_mode == 'Ascii'):\n        for character in in_text:\n            text += chr(character)\n    elif (display_mode == 'Hex'):\n        for (index, character) in enumerate(in_text):\n            text += ('%02X ' % character)\n            if ((((index + str_len) + 1) % 8) == 0):\n                text += '\\t'\n            if ((((index + str_len) + 1) % 16) == 0):\n                text += '\\n'\n    else:\n        text = in_text.decode('utf-8', 'replace')\n    return text\n", "label": 1}
{"function": "\n\ndef find_consecutive_slots(self, num_consecutive):\n    if (num_consecutive == 1):\n        return self.find_slot()\n    for i in range(len(self._data)):\n        any_taken = False\n        for k in range(num_consecutive):\n            if self._data[(i + k)]:\n                any_taken = True\n                break\n        if (not any_taken):\n            return i\n    return (- 1)\n", "label": 1}
{"function": "\n\ndef heuristic_search(graph, start, goal, heuristic):\n    '\\n    A* search algorithm.\\n    \\n    A set of heuristics is available under C{graph.algorithms.heuristics}. User-created heuristics\\n    are allowed too.\\n    \\n    @type graph: graph, digraph\\n    @param graph: Graph\\n    \\n    @type start: node\\n    @param start: Start node\\n    \\n    @type goal: node\\n    @param goal: Goal node\\n    \\n    @type heuristic: function\\n    @param heuristic: Heuristic function\\n    \\n    @rtype: list\\n    @return: Optimized path from start to goal node \\n    '\n    queue = [(0, start, 0, None)]\n    g = {\n        \n    }\n    explored = {\n        \n    }\n    while queue:\n        (_, current, dist, parent) = heappop(queue)\n        if (current == goal):\n            path = ([current] + [n for n in _reconstruct_path(parent, explored)])\n            path.reverse()\n            return path\n        if (current in explored):\n            continue\n        explored[current] = parent\n        for neighbor in graph[current]:\n            if (neighbor in explored):\n                continue\n            ncost = (dist + graph.edge_weight((current, neighbor)))\n            if (neighbor in g):\n                (qcost, h) = g[neighbor]\n                if (qcost <= ncost):\n                    continue\n            else:\n                h = heuristic(neighbor, goal)\n            g[neighbor] = (ncost, h)\n            heappush(queue, ((ncost + h), neighbor, ncost, current))\n    raise NodeUnreachable(start, goal)\n", "label": 1}
{"function": "\n\ndef parse_test(self, node):\n    token = next(self.stream)\n    if self.stream.current.test('name:not'):\n        next(self.stream)\n        negated = True\n    else:\n        negated = False\n    name = self.stream.expect('name').value\n    while (self.stream.current.type == 'dot'):\n        next(self.stream)\n        name += ('.' + self.stream.expect('name').value)\n    dyn_args = dyn_kwargs = None\n    kwargs = []\n    if (self.stream.current.type == 'lparen'):\n        (args, kwargs, dyn_args, dyn_kwargs) = self.parse_call(None)\n    elif ((self.stream.current.type in ('name', 'string', 'integer', 'float', 'lparen', 'lbracket', 'lbrace')) and (not self.stream.current.test_any('name:else', 'name:or', 'name:and'))):\n        if self.stream.current.test('name:is'):\n            self.fail('You cannot chain multiple tests with is')\n        args = [self.parse_expression()]\n    else:\n        args = []\n    node = nodes.Test(node, name, args, kwargs, dyn_args, dyn_kwargs, lineno=token.lineno)\n    if negated:\n        node = nodes.Not(node, lineno=token.lineno)\n    return node\n", "label": 1}
{"function": "\n\ndef delete_peer(self, addr):\n    if _debug:\n        BIPBBMD._debug('delete_peer %r', addr)\n    if isinstance(addr, Address):\n        pass\n    elif isinstance(addr, str):\n        addr = LocalStation(addr)\n    else:\n        raise TypeError('addr must be a string or an Address')\n    for i in range((len(self.bbmdBDT) - 1), (- 1), (- 1)):\n        if (addr == self.bbmdBDT[i]):\n            del self.bbmdBDT[i]\n            break\n    else:\n        pass\n", "label": 1}
{"function": "\n\ndef rectangle(self, rect=None, duration=1.0, block=True, color=(1, 1, 1, 1), parent=None, depth=0):\n    'Draw a single-colored rectangle.'\n    if (duration == 0):\n        block = False\n    l = rect[0]\n    r = rect[1]\n    t = rect[2]\n    b = rect[3]\n    obj = self._engine.direct.gui.OnscreenImage.OnscreenImage(image='blank.tga', pos=(((l + r) / 2), depth, ((b + t) / 2)), scale=(((r - l) / 2), 1, ((b - t) / 2)), color=color, parent=parent)\n    self._to_destroy.append(obj)\n    obj.setTransparency(self._engine.pandac.TransparencyAttrib.MAlpha)\n    if self.implicit_markers:\n        self.marker(250)\n    if block:\n        if ((type(duration) == list) or (type(duration) == tuple)):\n            self.sleep(duration[0])\n            self.waitfor(duration[1])\n        elif (type(duration) == str):\n            self.waitfor(duration)\n        else:\n            self.sleep(duration)\n        self._destroy_object(obj, 251)\n    else:\n        if (duration > 0):\n            self._engine.base.taskMgr.doMethodLater(duration, self._destroy_object, 'ConvenienceFunctions, remove_rect', extraArgs=[obj, 251])\n        return obj\n", "label": 1}
{"function": "\n\ndef add_type_view(self, request, form_url=''):\n    '\\n        Display a choice form to select which page type to add.\\n        '\n    if (not self.has_add_permission(request)):\n        raise PermissionDenied\n    extra_qs = ''\n    if request.META['QUERY_STRING']:\n        extra_qs = ('&' + request.META['QUERY_STRING'])\n    choices = self.get_child_type_choices(request, 'add')\n    if (len(choices) == 1):\n        return HttpResponseRedirect('?ct_id={0}{1}'.format(choices[0][0], extra_qs))\n    form = self.add_type_form(data=(request.POST if (request.method == 'POST') else None), initial={\n        'ct_id': choices[0][0],\n    })\n    form.fields['ct_id'].choices = choices\n    if form.is_valid():\n        return HttpResponseRedirect('?ct_id={0}{1}'.format(form.cleaned_data['ct_id'], extra_qs))\n    fieldsets = ((None, {\n        'fields': ('ct_id',),\n    }),)\n    adminForm = AdminForm(form, fieldsets, {\n        \n    }, model_admin=self)\n    media = (self.media + adminForm.media)\n    opts = self.model._meta\n    context = {\n        'title': (_('Add %s') % force_text(opts.verbose_name)),\n        'adminform': adminForm,\n        'is_popup': (('_popup' in request.POST) or ('_popup' in request.GET)),\n        'media': mark_safe(media),\n        'errors': AdminErrorList(form, ()),\n        'app_label': opts.app_label,\n    }\n    return self.render_add_type_form(request, context, form_url)\n", "label": 1}
{"function": "\n\ndef mod_pi2(man, exp, mag, wp):\n    if (mag > 0):\n        i = 0\n        while 1:\n            cancellation_prec = (20 << i)\n            wpmod = ((wp + mag) + cancellation_prec)\n            pi2 = pi_fixed((wpmod - 1))\n            pi4 = (pi2 >> 1)\n            offset = (wpmod + exp)\n            if (offset >= 0):\n                t = (man << offset)\n            else:\n                t = (man >> (- offset))\n            (n, y) = divmod(t, pi2)\n            if (y > pi4):\n                small = (pi2 - y)\n            else:\n                small = y\n            if (small >> ((wp + mag) - 10)):\n                n = int(n)\n                t = (y >> mag)\n                wp = (wpmod - mag)\n                break\n            i += 1\n    else:\n        wp += (- mag)\n        offset = (exp + wp)\n        if (offset >= 0):\n            t = (man << offset)\n        else:\n            t = (man >> (- offset))\n        n = 0\n    return (t, n, wp)\n", "label": 1}
{"function": "\n\ndef get_field_value(self, field_name, field_type, value):\n    if (value is None):\n        return None\n    values_map = self._import_config.get_value_mapping(field_name)\n    if isinstance(value, list):\n        return [self.get_field_value(field_name, field_type, v) for v in value]\n    if field_type.startswith('user'):\n        return self._to_yt_user(value)\n    if (field_type.lower() == 'date'):\n        return self.to_unix_date(value)\n    if isinstance(value, basestring):\n        return values_map.get(value, value)\n    if isinstance(value, int):\n        return values_map.get(value, str(value))\n", "label": 1}
{"function": "\n\n@fails('aminator.provisioner.provision_scripts.error')\n@lapse('aminator.provisioner.provision_scripts.duration')\ndef _run_provision_scripts(self, scripts_dir):\n    '\\n        execute every python or shell script found in scripts_dir\\n            1. run python or shell scripts in lexical order\\n\\n        :param scripts_dir: path in chroot to look for python and shell scripts\\n        :return: None\\n        '\n    script_files = sorted((glob((scripts_dir + '/*.py')) + glob((scripts_dir + '/*.sh'))))\n    if (not script_files):\n        log.debug('no python or shell scripts found in {0}'.format(scripts_dir))\n    else:\n        log.debug('found scripts {0} in {1}'.format(script_files, scripts_dir))\n        for script in script_files:\n            log.debug('executing script {0}'.format(script))\n            if os.access(script, os.X_OK):\n                result = run_script(script)\n            elif script.endswith('.py'):\n                result = run_script(['python', script])\n            else:\n                result = run_script(['sh', script])\n            if (not result.success):\n                log.critical('script failed: {0}: {1.std_err}'.format(script, result.result))\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef _get_test_labels(test_modules):\n    test_labels = []\n    for test_module in test_modules:\n        for module in [name for (_, name, _) in pkgutil.iter_modules([os.path.join(test_module, 'tests')])]:\n            clsmembers = pyclbr.readmodule(('%s.tests.%s' % (test_module, module)))\n            for (clsname, cls) in clsmembers.items():\n                for (method, _) in cls.methods.items():\n                    if method.startswith('test_'):\n                        test_labels.append(('%s.%s.%s' % (test_module, clsname, method)))\n    return test_labels\n", "label": 1}
{"function": "\n\ndef _get_rows(self, options):\n    'Return only those data rows that should be printed, based on slicing and sorting.\\n\\n        Arguments:\\n\\n        options - dictionary of option settings.'\n    if options['oldsortslice']:\n        rows = copy.deepcopy(self._rows[options['start']:options['end']])\n    else:\n        rows = copy.deepcopy(self._rows)\n    if options['sortby']:\n        sortindex = self._field_names.index(options['sortby'])\n        rows = [([row[sortindex]] + row) for row in rows]\n        rows.sort(reverse=options['reversesort'], key=options['sort_key'])\n        rows = [row[1:] for row in rows]\n    if (not options['oldsortslice']):\n        rows = rows[options['start']:options['end']]\n    return rows\n", "label": 1}
{"function": "\n\ndef AddAll(self, rdf_values, callback=None):\n    'Adds a list of rdfvalues to the collection.'\n    for rdf_value in rdf_values:\n        if (rdf_value is None):\n            raise ValueError(\"Can't add None to the collection via AddAll.\")\n        if (self._rdf_type and (not isinstance(rdf_value, self._rdf_type))):\n            raise ValueError(('This collection only accepts values of type %s' % self._rdf_type.__name__))\n        if (not rdf_value.age):\n            rdf_value.age.Now()\n    buf = cStringIO.StringIO()\n    for (index, rdf_value) in enumerate(rdf_values):\n        data = rdf_protodict.EmbeddedRDFValue(payload=rdf_value).SerializeToString()\n        buf.write(struct.pack('<i', len(data)))\n        buf.write(data)\n        self.size += 1\n        if callback:\n            callback(index, rdf_value)\n    self.fd.Seek(0, 2)\n    self.fd.Write(buf.getvalue())\n    self.stream_dirty = True\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef get_permissions(doctype=None, role=None):\n    frappe.only_for('System Manager')\n    out = frappe.db.sql(('select * from tabDocPerm\\n\\t\\twhere %s%s order by parent, permlevel, role' % (((doctype and (\" parent='%s'\" % frappe.db.escape(doctype))) or ''), ((role and (((doctype and ' and ') or '') + (\" role='%s'\" % frappe.db.escape(role)))) or ''))), as_dict=True)\n    linked_doctypes = {\n        \n    }\n    for d in out:\n        d.linked_doctypes = linked_doctypes.setdefault(d.parent, get_linked_doctypes(d.parent))\n    return out\n", "label": 1}
{"function": "\n\ndef _fix_slicing_order(self, outer_fields, inner_select, order, inner_table_name):\n    '\\n        Apply any necessary fixes to the outer_fields, inner_select, and order \\n        strings due to slicing.\\n        '\n    if (order is None):\n        meta = self.query.get_meta()\n        column = (meta.pk.db_column or meta.pk.get_attname())\n        order = '{0}.{1} ASC'.format(inner_table_name, self.connection.ops.quote_name(column))\n    else:\n        alias_id = 0\n        new_order = []\n        for x in order.split(','):\n            m = _re_find_order_direction.search(x)\n            if m:\n                direction = m.groups()[0]\n            else:\n                direction = 'ASC'\n            x = _re_find_order_direction.sub('', x)\n            col = x.rsplit('.', 1)[(- 1)]\n            if (x not in inner_select):\n                alias_id += 1\n                col = '{left_sql_quote}{0}___o{1}{right_sql_quote}'.format(col.strip((self.connection.ops.left_sql_quote + self.connection.ops.right_sql_quote)), alias_id, left_sql_quote=self.connection.ops.left_sql_quote, right_sql_quote=self.connection.ops.right_sql_quote)\n                inner_select = '({0}) AS {1}, {2}'.format(x, col, inner_select)\n            new_order.append('{0}.{1} {2}'.format(inner_table_name, col, direction))\n        order = ', '.join(new_order)\n    return (outer_fields, inner_select, order)\n", "label": 1}
{"function": "\n\ndef strategy(self, opponent):\n    '\\n        Check whether the number of cooperations in the first and second halves\\n        of the history are close. The variance of the uniform distribution (1/4)\\n        is a reasonable delta but use something lower for certainty and avoiding\\n        false positives. This approach will also detect a lot of random players.\\n        '\n    n = len(self.history)\n    if ((n >= 8) and opponent.cooperations and opponent.defections):\n        (start1, end1) = (0, (n // 2))\n        (start2, end2) = ((n // 4), ((3 * n) // 4))\n        (start3, end3) = ((n // 2), n)\n        count1 = (opponent.history[start1:end1].count(C) + self.history[start1:end1].count(C))\n        count2 = (opponent.history[start2:end2].count(C) + self.history[start2:end2].count(C))\n        count3 = (opponent.history[start3:end3].count(C) + self.history[start3:end3].count(C))\n        ratio1 = ((0.5 * count1) / (end1 - start1))\n        ratio2 = ((0.5 * count2) / (end2 - start2))\n        ratio3 = ((0.5 * count3) / (end3 - start3))\n        if ((abs((ratio1 - ratio2)) < 0.2) and (abs((ratio1 - ratio3)) < 0.2)):\n            return D\n    return C\n", "label": 1}
{"function": "\n\ndef inet_ntop(af, packed_ip):\n    'Convert an packed IP address of the given family to string format.'\n    if (af == AF_INET):\n        return inet_ntoa(packed_ip)\n    elif (af == AF_INET6):\n        if ((len(packed_ip) != 16) or (not _is_str(packed_ip))):\n            raise ValueError('invalid length of packed IP address string')\n        tokens = [('%x' % i) for i in _unpack('>8H', packed_ip)]\n        words = list(_unpack('>8H', packed_ip))\n        int_val = 0\n        for (i, num) in enumerate(reversed(words)):\n            word = num\n            word = (word << (16 * i))\n            int_val = (int_val | word)\n        if ((65535 < int_val <= 4294967295) or ((int_val >> 32) == 65535)):\n            packed_ipv4 = _pack('>2H', *[int(i, 16) for i in tokens[(- 2):]])\n            ipv4_str = inet_ntoa(packed_ipv4)\n            tokens = (tokens[0:(- 2)] + [ipv4_str])\n        return ':'.join(_compact_ipv6_tokens(tokens))\n    else:\n        raise ValueError(('unknown address family %d' % af))\n", "label": 1}
{"function": "\n\ndef _setupTypeInfo(self, test, maskPattern):\n    data = ((self.errorCorrectLevel << 3) | maskPattern)\n    bits = QRUtil.getBCHTypeInfo(data)\n    for i in range(15):\n        mod = ((not test) and (((bits >> i) & 1) == 1))\n        if (i < 6):\n            self.modules[i][8] = mod\n        elif (i < 8):\n            self.modules[(i + 1)][8] = mod\n        else:\n            self.modules[((self.moduleCount - 15) + i)][8] = mod\n    for i in range(15):\n        mod = ((not test) and (((bits >> i) & 1) == 1))\n        if (i < 8):\n            self.modules[8][((self.moduleCount - i) - 1)] = mod\n        elif (i < 9):\n            self.modules[8][(((15 - i) - 1) + 1)] = mod\n        else:\n            self.modules[8][((15 - i) - 1)] = mod\n    self.modules[(self.moduleCount - 8)][8] = (not test)\n", "label": 1}
{"function": "\n\ndef submit(obj, username=None, email=None, password=None, formid=None, tos=False):\n    '\\n    Submit form for register new user. Note - if you have captcha\\n    in the form, you need to mock it.\\n    '\n    formid = (formid or 1)\n    obj.formvalue(formid, 'username', (username or USERNAME))\n    obj.formvalue(formid, 'email', (email or EMAIL))\n    obj.formvalue(formid, 'password1', (password or PASSWORD))\n    obj.formvalue(formid, 'password2', (password or PASSWORD))\n    if tos:\n        obj.formvalue(formid, 'tos', 'on')\n    obj.submit200()\n", "label": 1}
{"function": "\n\ndef rob(self, nums):\n    '\\n        Two cases: cannot touch 1st element vs. cannot touch 2nd element.\\n        There are two cases here 1) 1st element is included and last is not included 2) 1st is not included and last is\\n        included.\\n        :type nums: list\\n        :rtype: int\\n        '\n    n = len(nums)\n    if (n < 2):\n        return sum(nums)\n    dp = [0 for _ in xrange(((n - 1) + 2))]\n    for i in xrange(2, (n + 1)):\n        dp[i] = max(dp[(i - 1)], (dp[(i - 2)] + nums[(i - 2)]))\n    ret = dp[(- 1)]\n    dp = [0 for _ in xrange(((n - 1) + 2))]\n    for i in xrange(2, (n + 1)):\n        dp[i] = max(dp[(i - 1)], (dp[(i - 2)] + nums[(i - 1)]))\n    ret = max(ret, dp[(- 1)])\n    return ret\n", "label": 1}
{"function": "\n\ndef get_module(os_mapping, dirpath):\n    mapping_config = ConfigParser.RawConfigParser()\n    mapping_config.readfp(StringIO.StringIO(os_mapping))\n    opts = config.controller\n    dist = opts['distribution_name']\n    ver = opts['distribution_version']\n    combinations = ((dist, ver), (dist, 'default'), ('default', 'default'))\n    mod_name = None\n    for comb in combinations:\n        mod_name = get_module_name(mapping_config, comb[0], comb[1])\n        if mod_name:\n            break\n    if ((mod_name != 'default') and (mod_name is not None)):\n        (fp, path, desc) = imp.find_module(mod_name, [dirpath])\n        return imp.load_module(mod_name, fp, path, desc)\n    elif (mod_name == 'default'):\n        return None\n", "label": 1}
{"function": "\n\ndef test_apply(self):\n    '\\n        Tests marking migrations as applied/unapplied.\\n        '\n    recorder = MigrationRecorder(connection)\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), set())\n    recorder.record_applied('myapp', '0432_ponies')\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), {('myapp', '0432_ponies')})\n    recorder_other = MigrationRecorder(connections['other'])\n    self.assertEqual(set(((x, y) for (x, y) in recorder_other.applied_migrations() if (x == 'myapp'))), set())\n    recorder.record_unapplied('myapp', '0432_ponies')\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), set())\n", "label": 1}
{"function": "\n\ndef get_table_list(self, cursor):\n    'Returns a list of table names in the current database and schema.'\n    cursor.execute((\"\\n            SELECT c.relname, c.relkind\\n            FROM pg_catalog.pg_class c\\n            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\\n            WHERE c.relkind IN ('r', 'v', '')\\n                AND n.nspname = '%s'\\n                AND pg_catalog.pg_table_is_visible(c.oid)\" % self.connection.schema_name))\n    if (django.VERSION >= (1, 8, 0)):\n        return [TableInfo(row[0], {\n            'r': 't',\n            'v': 'v',\n        }.get(row[1])) for row in cursor.fetchall() if (row[0] not in self.ignored_tables)]\n    else:\n        return [row[0] for row in cursor.fetchall() if (row[0] not in self.ignored_tables)]\n", "label": 1}
{"function": "\n\ndef _camel_case_to_underscores(self, text):\n    result = []\n    pos = 0\n    while (pos < len(text)):\n        if text[pos].isupper():\n            if ((((pos - 1) > 0) and text[(pos - 1)].islower()) or (((pos - 1) > 0) and ((pos + 1) < len(text)) and text[(pos + 1)].islower())):\n                result.append(('_%s' % text[pos].lower()))\n            else:\n                result.append(text[pos].lower())\n        else:\n            result.append(text[pos])\n        pos += 1\n    return ''.join(result)\n", "label": 1}
{"function": "\n\ndef _update_no_improvement_count(self, early_stopping, X_val, y_val):\n    if early_stopping:\n        self.validation_scores_.append(self.score(X_val, y_val))\n        if self.verbose:\n            print(('Validation score: %f' % self.validation_scores_[(- 1)]))\n        last_valid_score = self.validation_scores_[(- 1)]\n        if (last_valid_score < (self.best_validation_score_ + self.tol)):\n            self._no_improvement_count += 1\n        else:\n            self._no_improvement_count = 0\n        if (last_valid_score > self.best_validation_score_):\n            self.best_validation_score_ = last_valid_score\n            self._best_coefs = [c.copy() for c in self.coefs_]\n            self._best_intercepts = [i.copy() for i in self.intercepts_]\n    else:\n        if (self.loss_curve_[(- 1)] > (self.best_loss_ - self.tol)):\n            self._no_improvement_count += 1\n        else:\n            self._no_improvement_count = 0\n        if (self.loss_curve_[(- 1)] < self.best_loss_):\n            self.best_loss_ = self.loss_curve_[(- 1)]\n", "label": 1}
{"function": "\n\ndef information(self, b, ties='breslow'):\n    info = 0\n    score = 0\n    for t in iterkeys(self.failures):\n        fail = self.failures[t]\n        d = len(fail)\n        risk = self.risk[t]\n        Z = self.design[t]\n        if (ties == 'breslow'):\n            w = np.exp(np.dot(Z, b))\n            rv = Discrete(Z[risk], w=w[risk])\n            info += rv.cov()\n        elif (ties == 'efron'):\n            w = np.exp(np.dot(Z, b))\n            score += Z[fail].sum()\n            for j in range(d):\n                efron_w = w\n                efron_w[fail] -= ((i * w[fail]) / d)\n                rv = Discrete(Z[risk], w=efron_w[risk])\n                info += rv.cov()\n        elif (ties == 'cox'):\n            raise NotImplementedError('Cox tie breaking method not implemented')\n        else:\n            raise NotImplementedError('tie breaking method not recognized')\n    return score\n", "label": 1}
{"function": "\n\ndef on_status(self, status):\n    if (status.in_reply_to_status_id == tweetID):\n        parsedNumbers = ''.join(status.text.split(' ')[1:]).replace(' ', '').replace('[', '').replace(']', '').replace('(', '').replace(')', '').split(',')\n        givenNumbers = list(map(int, parsedNumbers))\n        areSorted = True\n        if (len(givenNumbers) != len(numbers)):\n            areSorted = False\n        for n in givenNumbers:\n            if (givenNumbers.count(n) != numbers.count(n)):\n                areSorted = False\n                break\n        for i in range(len(givenNumbers)):\n            if (i > 0):\n                if (not (givenNumbers[i] >= givenNumbers[(i - 1)])):\n                    areSorted = False\n                    break\n        if areSorted:\n            print(givenNumbers)\n            api.update_status((('@' + status.author.screen_name) + ' Awesome! Thanks!'), in_reply_to_status_id=status.id)\n            return False\n        else:\n            api.update_status((('@' + status.author.screen_name) + \" Those numbers aren't sorted!\"), in_reply_to_status_id=status.id)\n            return True\n    else:\n        return True\n", "label": 1}
{"function": "\n\ndef getSliceAttr(self, sliceInfo, attr):\n    if (attr == 'id'):\n        return sliceInfo[0]\n    elif (attr == 'start'):\n        return sliceInfo[1]\n    elif (attr == 'stop'):\n        return sliceInfo[2]\n    elif (attr == 'orientation'):\n        raise AttributeError('ori not saved')\n    else:\n        v = self.server.get_annotation_attr(sliceInfo[0], attr)\n        if (v == ''):\n            raise AttributeError(('this annotation has no attr: ' + attr))\n        return v\n", "label": 1}
{"function": "\n\ndef update(self, key):\n    if ((self.plot.dynamic == 'bounded') and (not isinstance(key, int))):\n        key = tuple(((dim.values[k] if dim.values else k) for (dim, k) in zip(self.mock_obj.kdims, tuple(key))))\n    if (self.renderer.mode == 'nbagg'):\n        if (not self.manager._shown):\n            self.comm.start()\n            self.manager.add_web_socket(self.comm)\n            self.manager._shown = True\n        fig = self.plot[key]\n        fig.canvas.draw_idle()\n        return ''\n    frame = self._plot_figure(key)\n    if (self.renderer.mode == 'mpld3'):\n        frame = self.encode_frames({\n            0: frame,\n        })\n    return frame\n", "label": 1}
{"function": "\n\ndef _update_an_article(self, postid):\n    (afile, aname) = self.conf.get_article(postid, self.args.type)\n    if self.args.output:\n        self._write_html_file(afile)\n        return\n    (html, meta, txt, medias) = self._get_and_update_article_content(afile)\n    if (not html):\n        return\n    resultclass = WordPressPost\n    if (self.args.type == 'page'):\n        postid = meta.postid\n        resultclass = WordPressPage\n    post = self.wpcall(GetPost(postid, result_class=resultclass))\n    if (not post):\n        slog.warning(('No post \"%s\"!' % postid))\n        return\n    slog.info('Old article:')\n    self.print_results(post)\n    post.title = meta.title\n    post.user = meta.author\n    post.slug = meta.nicename\n    post.date = meta.date\n    post.content = html\n    post.post_status = meta.poststatus\n    if meta.modified:\n        post.date_modified = meta.modified\n    terms = self.cache.get_terms_from_meta(meta.category, meta.tags)\n    if terms:\n        post.terms = terms\n    elif (self.args.type == 'post'):\n        slog.warning('Please provide some terms.')\n        return\n    succ = self.wpcall(EditPost(postid, post))\n    if (succ == None):\n        return\n    if succ:\n        slog.info(('Update %s successfully!' % postid))\n    else:\n        slog.info(('Update %s fail!' % postid))\n", "label": 1}
{"function": "\n\ndef _filter_requirements(lines_iter, filter_names=None, filter_sys_version=False):\n    for line in lines_iter:\n        line = line.strip()\n        if ((not line) or line.startswith('#')):\n            continue\n        match = REQ_PATTERN.match(line)\n        if (match is None):\n            raise AssertionError((\"Could not parse requirement: '%s'\" % line))\n        name = match.group('name')\n        if ((filter_names is not None) and (name not in filter_names)):\n            continue\n        if (filter_sys_version and match.group('pyspec')):\n            (pycomp, pyspec) = match.group('pycomp', 'pyspec')\n            comp = STR_TO_CMP[pycomp]\n            pyver_spec = StrictVersion(pyspec)\n            if comp(SYS_VERSION, pyver_spec):\n                (yield line.split(';')[0])\n            continue\n        (yield line)\n", "label": 1}
{"function": "\n\ndef set(self, name, value, retries=3, wait_ready=False):\n    if wait_ready:\n        self.wait_ready()\n    name = name.upper()\n    value = float(value)\n    success = False\n    remaining = retries\n    while True:\n        self._vehicle._master.param_set_send(name, value)\n        tstart = monotonic.monotonic()\n        if (remaining == 0):\n            break\n        remaining -= 1\n        while ((monotonic.monotonic() - tstart) < 1):\n            if ((name in self._vehicle._params_map) and (self._vehicle._params_map[name] == value)):\n                return True\n            time.sleep(0.1)\n    if (retries > 0):\n        errprinter(('timeout setting parameter %s to %f' % (name, value)))\n    return False\n", "label": 1}
{"function": "\n\ndef get(self):\n    'Handle GET.'\n    (pkgs, unused_dt) = models.ReportsCache.GetInstallCounts()\n    for p in models.PackageInfo.all():\n        if (not p.plist):\n            continue\n        if (p.munki_name not in pkgs):\n            continue\n        elif (not pkgs[p.munki_name].get('duration_seconds_avg', None)):\n            continue\n        lock = ('pkgsinfo_%s' % p.filename)\n        if (not gae_util.ObtainLock(lock, timeout=5.0)):\n            continue\n        old_desc = p.plist['description']\n        avg_duration_text = (models.PackageInfo.AVG_DURATION_TEXT % (pkgs[p.munki_name]['duration_count'], pkgs[p.munki_name]['duration_seconds_avg']))\n        p.description = ('%s\\n\\n%s' % (p.description, avg_duration_text))\n        if (p.plist['description'] != old_desc):\n            p.put()\n        gae_util.ReleaseLock(lock)\n    delay = 0\n    for track in common.TRACKS:\n        delay += 5\n        models.Catalog.Generate(track, delay=delay)\n", "label": 1}
{"function": "\n\ndef parse_candidates(self, result):\n    completion = {\n        'dup': 1,\n    }\n    _type = ''\n    word = ''\n    placeholder = ''\n    for chunk in [x for x in result.string if x.spelling]:\n        chunk_spelling = chunk.spelling\n        if chunk.isKindTypedText():\n            word += chunk_spelling\n            placeholder += chunk_spelling\n            continue\n        elif chunk.isKindResultType():\n            _type += chunk_spelling\n        else:\n            placeholder += chunk_spelling\n    completion['word'] = word\n    completion['abbr'] = completion['info'] = placeholder\n    completion['kind'] = ' '.join([(index_h.kinds[result.cursorKind] if (result.cursorKind in index_h.kinds) else str(result.cursorKind)), _type])\n    return completion\n", "label": 1}
{"function": "\n\ndef __init__(self, params=None, responses=None):\n    super(MetaModel, self).__init__()\n    if ((params is None) or (not isinstance(params, tuple))):\n        msg = ('Metamodel params argument needs to be a tuple of ' + 'variable names.')\n        self.raise_exception(msg, ValueError)\n    if ((responses is None) or (not isinstance(responses, tuple))):\n        msg = ('Metamodel responses argument needs to be a tuple of ' + 'variable names.')\n        self.raise_exception(msg, ValueError)\n    input_tree = self.get('params')\n    self._param_data = []\n    for name in params:\n        self.add(name, Float(0.0, iotype='in', desc='metamodel param'))\n        input_tree.add(name, List([], desc='training param'))\n    output_tree = self.get('responses')\n    self._response_data = {\n        \n    }\n    for name in responses:\n        self.add(name, Float(0.0, iotype='out', desc='metamodel response'))\n        output_tree.add(name, List([], desc='training response'))\n        self._response_data[name] = []\n        self.surrogates[name] = None\n    self._surrogate_input_names = params\n    self._surrogate_output_names = responses\n    self._train = True\n    self._surrogate_overrides = set()\n    self._default_surrogate_copies = {\n        \n    }\n    self.on_trait_change(self._surrogate_updated, 'surrogates_items')\n", "label": 1}
{"function": "\n\ndef get_projects(building, organization):\n    \"return an JSON friendly list of the building's projects\\n\\n    :param building: the BuildingSnapshot inst.\\n    :param organization: the Organization inst.\\n    :returns: list of projects\\n    \"\n    projects = []\n    for p in building.project_set.filter(super_organization=organization).distinct():\n        project_dict = p.__dict__.copy()\n        project_dict['is_compliance'] = p.has_compliance\n        if project_dict['is_compliance']:\n            c = p.get_compliance()\n            project_dict['compliance_type'] = c.compliance_type\n            project_dict['end_date'] = c.end_date.strftime('%m/%d/%Y')\n            project_dict['deadline_date'] = c.deadline_date.strftime('%m/%d/%Y')\n        del project_dict['_state']\n        del project_dict['modified']\n        del project_dict['created']\n        pb = ProjectBuilding.objects.get(project=p, building_snapshot=building)\n        if pb.approved_date:\n            approved_date = pb.approved_date.strftime('%m/%d/%Y')\n        else:\n            approved_date = None\n        project_dict['building'] = {\n            'compliant': pb.compliant,\n            'approver': (pb.approver.email if pb.approver else None),\n            'approved_date': approved_date,\n        }\n        if pb.status_label:\n            label = {\n                'name': pb.status_label.name,\n                'color': pb.status_label.color,\n                'id': pb.status_label.pk,\n            }\n            project_dict['building']['label'] = label\n        projects.append(project_dict)\n    return projects\n", "label": 1}
{"function": "\n\ndef readlines(self, sizehint=(- 1)):\n    '\\n        readlines([size]) -> list of strings, each a line from the file.\\n\\n        Call readline() repeatedly and return a list of the lines so read.\\n        The optional size argument, if given, is an approximate bound on the\\n        total number of bytes in the lines returned.\\n        '\n    if self.closed:\n        raise ValueError('I/O operation on closed file')\n    lines = []\n    while True:\n        line = self.readline(sizehint)\n        if (not line):\n            break\n        lines.append(line)\n        if (sizehint >= 0):\n            sizehint -= len(line)\n            if (sizehint <= 0):\n                break\n    return lines\n", "label": 1}
{"function": "\n\ndef _addPKCS1Padding(self, bytes, blockType):\n    padLength = (numBytes(self.n) - (len(bytes) + 3))\n    if (blockType == 1):\n        pad = ([255] * padLength)\n    elif (blockType == 2):\n        pad = createByteArraySequence([])\n        while (len(pad) < padLength):\n            padBytes = getRandomBytes((padLength * 2))\n            pad = [b for b in padBytes if (b != 0)]\n            pad = pad[:padLength]\n    else:\n        raise AssertionError()\n    padding = createByteArraySequence((([0, blockType] + pad) + [0]))\n    paddedBytes = (padding + bytes)\n    return paddedBytes\n", "label": 1}
{"function": "\n\ndef calculate_pages(self):\n    has_max_record_limit = (self.get_max_record_limit() is not False)\n    max_record_limit = self.get_max_record_limit()\n    if has_max_record_limit:\n        self.set_nb_results(min(self.results_counter, max_record_limit))\n    else:\n        self.set_nb_results(self.results_counter)\n    page = self.get_page()\n    if ((0 == self.get_page()) or (0 == self.get_max_per_page())):\n        self.set_last_page(0)\n    else:\n        self.set_last_page(int(ceil((self.get_nb_results() / float(self.get_max_per_page())))))\n        self.offset = ((page - 1) * self.get_max_per_page())\n        if has_max_record_limit:\n            max_record_limit -= self.offset\n            if (max_record_limit > self.get_max_per_page()):\n                self.limit = self.get_max_per_page()\n            else:\n                self.limit = max_record_limit\n        else:\n            self.limit = self.get_max_per_page()\n", "label": 1}
{"function": "\n\ndef pp_options_list(keys, width=80, _print=False):\n    ' Builds a concise listing of available options, grouped by prefix '\n    from textwrap import wrap\n    from itertools import groupby\n\n    def pp(name, ks):\n        pfx = ((('- ' + name) + '.[') if name else '')\n        ls = wrap(', '.join(ks), width, initial_indent=pfx, subsequent_indent='  ', break_long_words=False)\n        if (ls and ls[(- 1)] and name):\n            ls[(- 1)] = (ls[(- 1)] + ']')\n        return ls\n    ls = []\n    singles = [x for x in sorted(keys) if (x.find('.') < 0)]\n    if singles:\n        ls += pp('', singles)\n    keys = [x for x in keys if (x.find('.') >= 0)]\n    for (k, g) in groupby(sorted(keys), (lambda x: x[:x.rfind('.')])):\n        ks = [x[(len(k) + 1):] for x in list(g)]\n        ls += pp(k, ks)\n    s = '\\n'.join(ls)\n    if _print:\n        print(s)\n    else:\n        return s\n", "label": 1}
{"function": "\n\n@decorators.which_bin(['lsblk', 'df'])\ndef fstype(device):\n    \"\\n    Return the filesystem name of a block device\\n\\n    .. versionadded:: 2015.8.2\\n\\n    device\\n        The name of the block device\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' blockdev.fstype /dev/sdX1\\n    \"\n    if salt.utils.which('lsblk'):\n        lsblk_out = __salt__['cmd.run']('lsblk -o fstype {0}'.format(device)).splitlines()\n        if (len(lsblk_out) > 1):\n            fs_type = lsblk_out[1].strip()\n            if fs_type:\n                return fs_type\n    if salt.utils.which('df'):\n        df_out = __salt__['cmd.run']('df -T {0}'.format(device)).splitlines()\n        if (len(df_out) > 1):\n            fs_type = df_out[1]\n            if fs_type:\n                return fs_type\n    return ''\n", "label": 1}
{"function": "\n\ndef _add_container_actions(self, container):\n    title_group_map = {\n        \n    }\n    for group in self._action_groups:\n        if (group.title in title_group_map):\n            msg = _('cannot merge actions - two groups are named %r')\n            raise ValueError((msg % group.title))\n        title_group_map[group.title] = group\n    group_map = {\n        \n    }\n    for group in container._action_groups:\n        if (group.title not in title_group_map):\n            title_group_map[group.title] = self.add_argument_group(title=group.title, description=group.description, conflict_handler=group.conflict_handler)\n        for action in group._group_actions:\n            group_map[action] = title_group_map[group.title]\n    for group in container._mutually_exclusive_groups:\n        mutex_group = self.add_mutually_exclusive_group(required=group.required)\n        for action in group._group_actions:\n            group_map[action] = mutex_group\n    for action in container._actions:\n        group_map.get(action, self)._add_action(action)\n", "label": 1}
{"function": "\n\ndef _check_frames(self, frames, fill_value):\n    'Reduce frames to no more than are available in the file.'\n    if self.seekable():\n        remaining_frames = (len(self) - self.tell())\n        if ((frames < 0) or ((frames > remaining_frames) and (fill_value is None))):\n            frames = remaining_frames\n    elif (frames < 0):\n        raise ValueError('frames must be specified for non-seekable files')\n    return frames\n", "label": 1}
{"function": "\n\ndef remove_results_below_q_threshold(search_results):\n    \"removes buildings if total count of buildings grouped by org is less\\n    than their org's public query threshold\\n\\n    :param list/queryset search_results: search results\\n    :returns: list or queryset\\n    \"\n    manual_group_by = {\n        \n    }\n    thresholds = {\n        \n    }\n    for b in search_results:\n        parent_org = b.super_organization.get_parent()\n        if (parent_org.id not in manual_group_by):\n            manual_group_by[parent_org.id] = 1\n            thresholds[parent_org.id] = parent_org.query_threshold\n        else:\n            manual_group_by[parent_org.id] += 1\n    orgs_below_threshold = []\n    for (org_id, count) in manual_group_by.items():\n        if (count < thresholds[org_id]):\n            orgs_below_threshold.append(org_id)\n    results = []\n    for sr in search_results:\n        if (sr.super_organization.get_parent() not in orgs_below_threshold):\n            results.append(sr)\n    return results\n", "label": 1}
{"function": "\n\ndef take_action(self, parsed_args):\n    volume_client = self.app.client_manager.volume\n    snapshot = utils.find_resource(volume_client.volume_snapshots, parsed_args.snapshot)\n    kwargs = {\n        \n    }\n    if parsed_args.name:\n        kwargs['name'] = parsed_args.name\n    if parsed_args.description:\n        kwargs['description'] = parsed_args.description\n    if ((not kwargs) and (not parsed_args.property)):\n        self.app.log.error('No changes requested\\n')\n        return\n    if parsed_args.property:\n        volume_client.volume_snapshots.set_metadata(snapshot.id, parsed_args.property)\n    volume_client.volume_snapshots.update(snapshot.id, **kwargs)\n", "label": 1}
{"function": "\n\ndef flush_user_profile(sender, **kwargs):\n    user_profile = kwargs['instance']\n    update_user_profile_caches([user_profile])\n    if ((kwargs.get('update_fields') is None) or (len((set(['full_name', 'short_name', 'email', 'is_active']) & set(kwargs['update_fields']))) > 0)):\n        cache_delete(active_user_dicts_in_realm_cache_key(user_profile.realm))\n    bot_fields = {'full_name', 'api_key', 'avatar_source', 'default_all_public_streams', 'is_active', 'default_sending_stream', 'default_events_register_stream'}\n    if (user_profile.is_bot and ((kwargs['update_fields'] is None) or (bot_fields & set(kwargs['update_fields'])))):\n        cache_delete(active_bot_dicts_in_realm_cache_key(user_profile.realm))\n    if ((kwargs.get('update_fields') is None) or ('alert_words' in kwargs['update_fields'])):\n        cache_delete(realm_alert_words_cache_key(user_profile.realm))\n", "label": 1}
{"function": "\n\ndef render(self, name, value, attrs=None):\n    if (attrs is None):\n        attrs = {\n            \n        }\n    related_url = ('../../../%s/%s/' % (self.rel.to._meta.app_label, self.rel.to._meta.object_name.lower()))\n    params = self.url_parameters()\n    if params:\n        url = ('?' + '&amp;'.join([('%s=%s' % (k, v)) for (k, v) in params.items()]))\n    else:\n        url = ''\n    if ('class' not in attrs):\n        attrs['class'] = 'vForeignKeyRawIdAdminField'\n    output = [super(ForeignKeyRawIdWidget, self).render(name, value, attrs)]\n    output.append(('<a href=\"%s%s\" class=\"related-lookup\" id=\"lookup_id_%s\" onclick=\"return showRelatedObjectLookupPopup(this);\"> ' % (related_url, url, name)))\n    output.append(('<img src=\"%simg/admin/selector-search.gif\" width=\"16\" height=\"16\" alt=\"%s\" /></a>' % (settings.ADMIN_MEDIA_PREFIX, _('Lookup'))))\n    if value:\n        output.append(self.label_for_value(value))\n    return mark_safe(''.join(output))\n", "label": 1}
{"function": "\n\n@register(_dump_registry, tuple)\ndef _dump_tuple(obj, stream):\n    l = len(obj)\n    if (l == 0):\n        stream.append(TAG_EMPTY_TUPLE)\n    elif (l == 1):\n        stream.append(TAG_TUP1)\n    elif (l == 2):\n        stream.append(TAG_TUP2)\n    elif (l == 3):\n        stream.append(TAG_TUP3)\n    elif (l == 4):\n        stream.append(TAG_TUP4)\n    elif (l < 256):\n        stream.append((TAG_TUP_L1 + I1.pack(l)))\n    else:\n        stream.append((TAG_TUP_L4 + I4.pack(l)))\n    for item in obj:\n        _dump(item, stream)\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    data = DefaultDict(None)\n    key = None\n    for line in self.f:\n        heading = self.isSectionHeading(line)\n        if heading:\n            if (data and (heading == self.newTestHeading)):\n                data[key] = data[key][:(- 1)]\n                (yield self.normaliseOutput(data))\n                data = DefaultDict(None)\n            key = heading\n            data[key] = ''\n        elif (key is not None):\n            data[key] += line\n    if data:\n        (yield self.normaliseOutput(data))\n", "label": 1}
{"function": "\n\n@requires_admin\ndef post(self, project_id):\n    project = Project.get(project_id)\n    if (project is None):\n        return ('', 404)\n    args = self.post_parser.parse_args()\n    if args.name:\n        project.name = args.name\n    if args.slug:\n        match = Project.query.filter((Project.slug == args.slug), (Project.id != project.id)).first()\n        if match:\n            return (('{\"error\": \"Project with slug %r already exists\"}' % (args.slug,)), 400)\n        project.slug = args.slug\n    if args.repository:\n        repository = Repository.get(args.repository)\n        if (repository is None):\n            return (('{\"error\": \"Repository with url %r does not exist\"}' % (args.repository,)), 400)\n        project.repository = repository\n    if (args.status == 'inactive'):\n        project.status = ProjectStatus.inactive\n    elif (args.status == 'active'):\n        project.status = ProjectStatus.active\n    db.session.add(project)\n    data = self.serialize(project)\n    data['repository'] = self.serialize(project.repository)\n    return self.respond(data, serialize=False)\n", "label": 1}
{"function": "\n\ndef accept(self, socket):\n    if (not isinstance(socket, TransmissionControlObject)):\n        raise Error(errno.ENOTSOCK)\n    if (not isinstance(socket, DataLinkConnection)):\n        raise Error(errno.EOPNOTSUPP)\n    while True:\n        client = socket.accept()\n        if (not client.is_bound):\n            self.bind(client)\n        if self.sap[client.addr].insert_socket(client):\n            log.debug('new data link connection ({0} <=== {1})'.format(client.addr, client.peer))\n            return client\n        else:\n            pdu = DisconnectedMode(client.peer, socket.addr, reason=32)\n            super(DataLinkConnection, socket).send(pdu)\n", "label": 1}
{"function": "\n\ndef _calc_correction(self):\n    '\\n        if self.tau > 0 this will be used in the correction factor calculation\\n        if self.tau = 0 then we assume ocr = icr in the correction factor calculation,\\n                      ie only lt correction\\n                     (note deadtime.calc_icr handles above two conditions)\\n        if self.tau < 0 (or None):\\n           if input_counts > 0  this will be used for icr in the factor calculation\\n           if input_counts <= 0 we assume ocr = icr in the correction factor calculation,\\n                                ie only lt correction\\n        '\n    if ((self.live_time <= 0) or (self.real_time <= 0)):\n        self.dt_factor = 1.0\n        return\n    if (self.total_counts > 0):\n        ocr = (self.total_counts / self.live_time)\n    else:\n        ocr = None\n    if (self.tau >= 0):\n        icr = calc_icr(ocr, self.tau)\n        if (icr is None):\n            icr = 0\n        self.icr_calc = icr\n    elif (self.input_counts > 0):\n        icr = (self.input_counts / self.live_time)\n    else:\n        icr = ocr = None\n    self.dt_factor = correction_factor(self.real_time, self.live_time, icr=icr, ocr=ocr)\n    if (self.dt_factor <= 0):\n        print('Error computing counts correction factor --> setting to 1')\n        self.dt_factor = 1.0\n", "label": 1}
{"function": "\n\ndef _calculateVectors(self):\n    log.msg('* Calculating shortest-path vectors', debug=True, system=LOG_SYSTEM)\n    paths = {\n        \n    }\n    for (port, vectors) in self.vectors.items():\n        for (network, cost) in vectors.items():\n            if (network in self.local_networks):\n                continue\n            if (network in self.blacklist_networks):\n                log.msg(('Skipping network %s in vector calculation, is blacklisted' % network), system=LOG_SYSTEM)\n                continue\n            if (cost > self.max_cost):\n                log.msg(('Skipping network %s in vector calculation, cost %i exceeds max cost %i' % (network, cost, self.max_cost)), system=LOG_SYSTEM)\n                continue\n            if (not (network in paths)):\n                paths[network] = (port, cost)\n                log.msg(('Added path to %s via %s. Cost %i' % (network, port, cost)), debug=True, system=LOG_SYSTEM)\n            elif (cost < paths[network][1]):\n                paths[network] = (port, cost)\n                log.msg(('Updated path to %s via %s. Cost %i' % (network, port, cost)), debug=True, system=LOG_SYSTEM)\n    self._shortest_paths = paths\n", "label": 1}
{"function": "\n\n@app.route('/division', methods=['GET', 'POST', 'OPTIONS'])\n@crossdomain(origin='*', headers=['Content-Type', 'X-Requested-With'])\ndef division_lookup():\n    try:\n        if ((request.json is None) and (request.method == 'POST')):\n            abort(400, 'Must provide JSON (did you set Content-type?)')\n        elif (request.method == 'POST'):\n            args = request.json\n        else:\n            args = request.args\n        if ('latitude' not in args):\n            abort(400, 'Most provide latitude and longitude')\n        if ('longitude' not in args):\n            abort(400, 'Most provide latitude and longitude')\n        conn = psycopg2.connect(host=dbcreds.HOSTNAME, database=dbcreds.DATABASE, user=dbcreds.USERNAME, password=dbcreds.PASSWORD)\n        cursor = conn.cursor()\n        cursor.execute(QUERY_FORMAT.format(latitude=float(args['latitude']), longitude=float(args['longitude'])))\n        result = cursor.fetchone()\n        if (result is None):\n            name = None\n        else:\n            name = result[0].lower().translate(None, \" -'\")\n        return jsonify({\n            'division': name,\n        })\n    except:\n        rollbar.report_exc_info()\n        raise\n", "label": 1}
{"function": "\n\ndef visit_label(self, label, add_to_result_map=None, within_label_clause=False, within_columns_clause=False, render_label_as_label=None, **kw):\n    render_label_with_as = (within_columns_clause and (not within_label_clause))\n    render_label_only = (render_label_as_label is label)\n    if (render_label_only or render_label_with_as):\n        if isinstance(label.name, elements._truncated_label):\n            labelname = self._truncated_identifier('colident', label.name)\n        else:\n            labelname = label.name\n    if render_label_with_as:\n        if (add_to_result_map is not None):\n            add_to_result_map(labelname, label.name, ((label, labelname) + label._alt_names), label.type)\n        return ((label.element._compiler_dispatch(self, within_columns_clause=True, within_label_clause=True, **kw) + OPERATORS[operators.as_]) + self.preparer.format_label(label, labelname))\n    elif render_label_only:\n        return self.preparer.format_label(label, labelname)\n    else:\n        return label.element._compiler_dispatch(self, within_columns_clause=False, **kw)\n", "label": 1}
{"function": "\n\ndef encode_features(X, enc_map=None):\n    'Converts categorical values in each column of X to integers in the range\\n    [0, n_unique_values_in_column - 1], if X is not already of integer type.\\n    Unknown values get a value of -1.\\n\\n    If mapping is not provided, it is calculated based on the valus in X.\\n    '\n    if np.issubdtype(X.dtype, np.integer):\n        return (X, enc_map)\n    if (enc_map is None):\n        fit = True\n        enc_map = []\n    else:\n        fit = False\n    Xenc = np.zeros(X.shape).astype('int')\n    for ii in range(X.shape[1]):\n        if fit:\n            enc_map.append({val: jj for (jj, val) in enumerate(np.unique(X[:, ii]))})\n        Xenc[:, ii] = np.array([enc_map[ii].get(x, (- 1)) for x in X[:, ii]])\n    return (Xenc, enc_map)\n", "label": 1}
{"function": "\n\ndef add_progress(kwargs, git, progress):\n    'Add the --progress flag to the given kwargs dict if supported by the\\n    git command. If the actual progress in the given progress instance is not\\n    given, we do not request any progress\\n    :return: possibly altered kwargs'\n    if (progress is not None):\n        v = git.version_info\n        if ((v[0] > 1) or (v[1] > 7) or (v[2] > 0) or (v[3] > 3)):\n            kwargs['progress'] = True\n    return kwargs\n", "label": 1}
{"function": "\n\ndef _getFileSizeQuota(self, model, resource):\n    '\\n        Get the current fileSizeQuota for a resource.  This takes the default\\n        quota into account if necessary.\\n\\n        :param model: the type of resource (e.g., user or collection)\\n        :param resource: the resource document.\\n        :returns: the fileSizeQuota.  None for no quota (unlimited), otherwise\\n                 a positive integer.\\n        '\n    useDefault = resource[QUOTA_FIELD].get('useQuotaDefault', True)\n    quota = resource[QUOTA_FIELD].get('fileSizeQuota', None)\n    if useDefault:\n        if (model == 'user'):\n            key = constants.PluginSettings.QUOTA_DEFAULT_USER_QUOTA\n        elif (model == 'collection'):\n            key = constants.PluginSettings.QUOTA_DEFAULT_COLLECTION_QUOTA\n        else:\n            key = None\n        if key:\n            quota = self.model('setting').get(key, None)\n    if ((not quota) or (quota < 0) or (not isinstance(quota, six.integer_types))):\n        return None\n    return quota\n", "label": 1}
{"function": "\n\ndef summary(self, alpha=0.05):\n    '\\n        Provide a summary of a mediation analysis.\\n        '\n    columns = ['Estimate', 'Lower CI bound', 'Upper CI bound', 'P-value']\n    index = ['ACME (control)', 'ACME (treated)', 'ADE (control)', 'ADE (treated)', 'Total effect', 'Prop. mediated (control)', 'Prop. mediated (treated)', 'ACME (average)', 'ADE (average)', 'Prop. mediated (average)']\n    smry = pd.DataFrame(columns=columns, index=index)\n    for (i, vec) in enumerate([self.ACME_ctrl, self.ACME_tx, self.ADE_ctrl, self.ADE_tx, self.total_effect, self.prop_med_ctrl, self.prop_med_tx, self.ACME_avg, self.ADE_avg, self.prop_med_avg]):\n        if ((vec is self.prop_med_ctrl) or (vec is self.prop_med_tx) or (vec is self.prop_med_avg)):\n            smry.iloc[(i, 0)] = np.median(vec)\n        else:\n            smry.iloc[(i, 0)] = vec.mean()\n        smry.iloc[(i, 1)] = np.percentile(vec, ((100 * alpha) / 2))\n        smry.iloc[(i, 2)] = np.percentile(vec, (100 * (1 - (alpha / 2))))\n        smry.iloc[(i, 3)] = _pvalue(vec)\n    if (pdc.version < '0.17.0'):\n        smry = smry.convert_objects(convert_numeric=True)\n    else:\n        smry = smry.apply(pd.to_numeric, errors='coerce')\n    return smry\n", "label": 1}
{"function": "\n\ndef _get_constraint_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances', 1)\n    solver_cache = filter_properties['solver_cache']\n    constraint_matrix = [[True for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n    solver_cache['constraint_matrix'] = constraint_matrix\n    constraint_objects = [cons() for cons in self.constraint_classes]\n    constraint_objects.sort(key=(lambda cons: cons.precedence))\n    precedence_level = 0\n    for constraint_object in constraint_objects:\n        if (constraint_object.precedence > precedence_level):\n            solver_cache['constraint_matrix'] = constraint_matrix\n            precedence_level = constraint_object.precedence\n        this_cons_mat = constraint_object.get_constraint_matrix(hosts, filter_properties)\n        if (not this_cons_mat):\n            continue\n        for i in xrange(num_hosts):\n            constraint_matrix[i][1:] = [(constraint_matrix[i][(j + 1)] & this_cons_mat[i][j]) for j in xrange(num_instances)]\n    solver_cache['constraint_matrix'] = constraint_matrix\n    return constraint_matrix\n", "label": 1}
{"function": "\n\ndef local_add_mul_fusion(node):\n    'Fuse consecutive add or mul in one such node with more inputs.\\n\\n    It is better to fuse add/mul that way then in a Composite node as\\n    this make the inner graph of the Compiste smaller. This allow to\\n    put more computation in a Composite before hitting the max\\n    recusion limit when pickling Composite.\\n\\n    '\n    if ((not isinstance(node.op, Elemwise)) or (not isinstance(node.op.scalar_op, (scalar.Add, scalar.Mul)))):\n        return False\n    s_op = node.op.scalar_op.__class__\n    for inp in node.inputs:\n        if (inp.owner and isinstance(inp.owner.op, Elemwise) and isinstance(inp.owner.op.scalar_op, s_op)):\n            l = list(node.inputs)\n            l.remove(inp)\n            output_node = node.op(*(l + inp.owner.inputs))\n            copy_stack_trace(node.outputs[0], output_node)\n            return [output_node]\n", "label": 1}
{"function": "\n\n@tornado.web.asynchronous\ndef _on_auth(self, user):\n    if (not user):\n        raise tornado.web.HTTPError(500, 'OAuth authentication failed')\n    access_token = user['access_token']\n    req = httpclient.HTTPRequest('https://api.github.com/user/emails', headers={\n        'Authorization': ('token ' + access_token),\n        'User-agent': 'Tornado auth',\n    })\n    response = httpclient.HTTPClient().fetch(req)\n    emails = [email['email'].lower() for email in json.loads(response.body.decode('utf-8')) if (email['verified'] and re.match(self.application.options.auth, email['email']))]\n    if (not emails):\n        message = 'Access denied. Please use another account or ask your admin to add your email to flower --auth.'\n        raise tornado.web.HTTPError(403, message)\n    self.set_secure_cookie('user', str(emails.pop()))\n    next_ = self.get_argument('next', '/')\n    self.redirect(next_)\n", "label": 1}
{"function": "\n\ndef findStyleName(element, style):\n    oldStyle = DOM.getAttribute(element, 'className')\n    if (oldStyle is None):\n        return (- 1)\n    idx = oldStyle.find(style)\n    lastPos = len(oldStyle)\n    while (idx != (- 1)):\n        if ((idx == 0) or (oldStyle[(idx - 1)] == ' ')):\n            last = (idx + len(style))\n            if ((last == lastPos) or ((last < lastPos) and (oldStyle[last] == ' '))):\n                break\n        idx = oldStyle.find(style, (idx + 1))\n    return idx\n", "label": 1}
{"function": "\n\ndef is_valid(self, response=None):\n    'Check if the current session ID is valid.\\n\\n    :param response: If passed, this reponse will be used to determine the\\n      validity of the session. Otherwise a simple test request will be emitted.\\n\\n    '\n    self._logger.debug('Checking if current session is valid.')\n    if (not self.id):\n        self._logger.debug('No previous ID found.')\n        return False\n    if (response is None):\n        self._logger.debug('Checking if ID %s is valid.', self.id)\n        response = _azkaban_request('POST', ('%s/manager' % (self.url,)), data={\n            'session.id': self.id,\n        }, verify=self.verify)\n    if (('<!-- /.login -->' in response.text) or ('Login error' in response.text) or ('\"error\" : \"session\"' in response.text)):\n        self._logger.debug('ID %s is invalid:\\n%s', self.id, response.text)\n        return False\n    else:\n        self._logger.debug('ID %s is valid.', self.id)\n        return True\n", "label": 1}
{"function": "\n\ndef bfs_deque(self, rooms, x, y):\n    from collections import deque\n    m = len(rooms)\n    n = len(rooms[0])\n    q = deque()\n    q.append((x, y, 0))\n    while q:\n        (i, j, level) = q.popleft()\n        rooms[i][j] = min(rooms[i][j], level)\n        for d in self.dirs:\n            (i_t, j_t) = ((i + d[0]), (j + d[1]))\n            if ((0 <= i_t < m) and (0 <= j_t < n) and (rooms[i_t][j_t] != (- 1)) and (rooms[i_t][j_t] >= (level + 1))):\n                q.append((i_t, j_t, (level + 1)))\n", "label": 1}
{"function": "\n\ndef _structured_bootstrap(args, n_boot, units, func, func_kwargs, rs):\n    'Resample units instead of datapoints.'\n    unique_units = np.unique(units)\n    n_units = len(unique_units)\n    args = [[a[(units == unit)] for unit in unique_units] for a in args]\n    boot_dist = []\n    for i in range(int(n_boot)):\n        resampler = rs.randint(0, n_units, n_units)\n        sample = [np.take(a, resampler, axis=0) for a in args]\n        lengths = map(len, sample[0])\n        resampler = [rs.randint(0, n, n) for n in lengths]\n        sample = [[c.take(r, axis=0) for (c, r) in zip(a, resampler)] for a in sample]\n        sample = list(map(np.concatenate, sample))\n        boot_dist.append(func(*sample, **func_kwargs))\n    return np.array(boot_dist)\n", "label": 1}
{"function": "\n\ndef test_benchmark():\n    TRIALS = range(1000000)\n    integer = 1\n    float = 3.0\n    long = 293203948032948023984023948023957245\n    stime = time.clock()\n    for i in TRIALS:\n        answer = isinstance(integer, (int, float, long, complex))\n        ok_(answer)\n    print(('isinstance, %s' % (time.clock() - stime)))\n    stime = time.clock()\n    for i in TRIALS:\n        answer = (integer.__class__ in set((int, float, long, complex)))\n        ok_(answer)\n    print(('__class__, set, %s' % (time.clock() - stime)))\n    stime = time.clock()\n    for i in TRIALS:\n        answer = ((integer.__class__ == int) or (integer.__class__ == float) or (integer.__class__ == long) or (integer.__class__ == complex))\n        ok_(answer)\n    print(('__class__, or, %s' % (time.clock() - stime)))\n", "label": 1}
{"function": "\n\ndef addFile(self, relPath, fullPath, distname, override=False):\n    fileName = os.path.basename(relPath)\n    fileExtension = os.path.splitext(fileName)[1]\n    if self.__package:\n        fileId = ('%s/' % self.__package)\n    else:\n        fileId = ''\n    if ((fileExtension in classExtensions) and (distname == 'classes')):\n        fileId += os.path.splitext(relPath)[0]\n        construct = jasy.item.Class.ClassItem\n        dist = self.classes\n    elif ((fileExtension in translationExtensions) and (distname == 'translations')):\n        fileId += os.path.splitext(relPath)[0]\n        construct = jasy.item.Translation.TranslationItem\n        dist = self.translations\n    elif (fileName in docFiles):\n        fileId += os.path.dirname(relPath)\n        fileId = fileId.strip('/')\n        construct = jasy.item.Doc.DocItem\n        dist = self.docs\n    else:\n        fileId += relPath\n        construct = jasy.item.Asset.AssetItem\n        dist = self.assets\n    if (construct != jasy.item.Asset.AssetItem):\n        fileId = fileId.replace('/', '.')\n    if ((fileId in dist) and (not override)):\n        raise UserError(('Item ID was registered before: %s' % fileId))\n    item = construct(self, fileId).attach(fullPath)\n    Console.debug(('Registering %s %s' % (item.kind, fileId)))\n    dist[fileId] = item\n", "label": 1}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    cls._meta = self\n    self.parent = cls\n    self.object_name = cls.__name__\n    self.type_name = self.object_name\n    self.description = cls.__doc__\n    self.original_attrs = {\n        \n    }\n    if self.meta:\n        meta_attrs = self.meta.__dict__.copy()\n        for name in self.meta.__dict__:\n            if name.startswith('_'):\n                del meta_attrs[name]\n        for attr_name in self.valid_attrs:\n            if (attr_name in meta_attrs):\n                setattr(self, attr_name, meta_attrs.pop(attr_name))\n                self.original_attrs[attr_name] = getattr(self, attr_name)\n            elif hasattr(self.meta, attr_name):\n                setattr(self, attr_name, getattr(self.meta, attr_name))\n                self.original_attrs[attr_name] = getattr(self, attr_name)\n        del self.valid_attrs\n        if (meta_attrs != {\n            \n        }):\n            raise TypeError((\"'class Meta' got invalid attribute(s): %s\" % ','.join(meta_attrs.keys())))\n    del self.meta\n", "label": 1}
{"function": "\n\ndef GetValidHostsForCert(cert):\n    'Returns a list of valid host globs for an SSL certificate.\\n\\n  Args:\\n    cert: A dictionary representing an SSL certificate.\\n  Returns:\\n    list: A list of valid host globs.\\n  '\n    if ('subjectAltName' in cert):\n        return [x[1] for x in cert['subjectAltName'] if (x[0].lower() == 'dns')]\n    else:\n        return [x[0][1] for x in cert['subject'] if (x[0][0].lower() == 'commonname')]\n", "label": 1}
{"function": "\n\n@wraps(urlparse_buggy)\ndef urlparse(urlstring, scheme='', allow_fragments=True, *args, **kwargs):\n    'A wrapper for :py:func:`urlparse.urlparse` with the following\\n    differences:\\n\\n    * Handles buckets in S3 URIs correctly. (:py:func:`~urlparse.urlparse`\\n      does this correctly sometime after 2.6.1; this is just a patch for older\\n      Python versions.)\\n    * Splits the fragment correctly in all URIs, not just Web-related ones.\\n      This behavior was fixed in the Python 2.7.4 standard library but we have\\n      to back-port it for previous versions.\\n    '\n    (scheme, netloc, path, params, query, fragment) = urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)\n    if ((netloc == '') and path.startswith('//')):\n        m = _NETLOC_RE.match(path)\n        netloc = m.group(1)\n        path = m.group(2)\n    if (allow_fragments and ('#' in path) and (not fragment)):\n        (path, fragment) = path.split('#', 1)\n    return ParseResult(scheme, netloc, path, params, query, fragment)\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    tools = ('jpegoptim', 'jpegtran', 'gifsicle', 'optipng', 'advpng', 'pngcrush')\n    not_found = []\n    for tool in tools:\n        setting_name = ('DIET_' + tool.upper())\n        if getattr(settings, setting_name):\n            retcode = call(('which %s' % tool), shell=True, stdout=PIPE)\n            if (retcode == 0):\n                self.stdout.write(('Found: %s\\n' % tool))\n            else:\n                self.stdout.write(('MISSING: %s\\n' % tool))\n                not_found.append(tool)\n        else:\n            self.stdout.write(('Disabled: %s\\n' % tool))\n    if len(not_found):\n        off_settings = [('DIET_%s = False' % tool.upper()) for tool in not_found]\n        self.stdout.write('\\n')\n        self.stdout.write('You can disable missing tools by adding following lines to your settings.py: \\n')\n        self.stdout.write('\\n'.join(off_settings))\n        self.stdout.write('\\n')\n", "label": 1}
{"function": "\n\ndef _fill_remote(cur, keep_files):\n    'Add references to remote Keep files if present and not local.\\n    '\n    if isinstance(cur, (list, tuple)):\n        return [_fill_remote(x, keep_files) for x in cur]\n    elif isinstance(cur, dict):\n        out = {\n            \n        }\n        for (k, v) in cur.items():\n            out[k] = _fill_remote(v, keep_files)\n        return out\n    elif (isinstance(cur, basestring) and os.path.splitext(cur)[(- 1)] and (not os.path.exists(cur))):\n        for test_keep in keep_files:\n            if test_keep.endswith(cur):\n                return test_keep\n        return cur\n    else:\n        return cur\n", "label": 1}
{"function": "\n\ndef _check_array_dimensions(self, array=None):\n    \"\\n        Check that a grid's array dimensions agree with this grid's metadata\\n\\n        Parameters\\n        ----------\\n        array : np.ndarray or list of np.ndarray, optional\\n            The array for which to test the dimensions. If this is not\\n            specified, this method performs a self-consistency check of array\\n            dimensions and meta-data.\\n        \"\n    n_pop_ref = None\n    if isinstance(array, OctreeGridView):\n        array = array.quantities[array.viewed_quantity]\n    for quantity in self.quantities:\n        if (array is None):\n            (n_pop, shape) = single_grid_dims(self.quantities[quantity], ndim=1)\n        else:\n            (n_pop, shape) = single_grid_dims(array, ndim=1)\n        if (shape != self.shape):\n            raise ValueError(('Quantity arrays do not have the right dimensions: %s instead of %s' % (shape, self.shape)))\n        if (n_pop is not None):\n            if (n_pop_ref is None):\n                n_pop_ref = n_pop\n            elif (n_pop != n_pop_ref):\n                raise ValueError('Not all dust lists in the grid have the same size')\n", "label": 1}
{"function": "\n\ndef grouping_prefix(self, path):\n    letters = [x for x in path.split('/') if x]\n    for prefix in self.ignore:\n        if (letters[:len(prefix)] == prefix):\n            return ('/' + '/'.join(letters[:(len(prefix) + 1)]))\n    return ('/%s' % ((letters[0] if letters else ''),))\n", "label": 1}
{"function": "\n\ndef __init__(self, path, upload_to=None, preview_w=None, preview_h=None):\n    self.upload_to = upload_to\n    self.preview_width = preview_w\n    self.preview_height = preview_h\n    self.metadata = {\n        \n    }\n    if (not path):\n        self.name = None\n        return\n    if ('%' in path):\n        path = urlunquote_plus(path)\n    if path.startswith(settings.MEDIA_URL):\n        self._path = get_relative_media_url(path, clean_slashes=False)\n    elif re.search('^(?:http(?:s)?:)?//', path):\n        self._path = self.download_image_url(path)\n    else:\n        abs_path = get_media_path(path)\n        if os.path.exists(abs_path):\n            self._path = get_relative_media_url(abs_path)\n    if ((not self._path) or (not os.path.exists(os.path.join(settings.MEDIA_ROOT, self._path)))):\n        self.name = None\n        return\n    super(ImageFile, self).__init__(self._path)\n    if self:\n        self.preview_image = self.get_for_size('preview')\n", "label": 1}
{"function": "\n\ndef get_columns_dict(columns):\n    \"Returns a dict with column docfield values as dict\\n\\t\\tThe keys for the dict are both idx and fieldname,\\n\\t\\tso either index or fieldname can be used to search for a column's docfield properties\\n\\t\"\n    columns_dict = {\n        \n    }\n    for (idx, col) in enumerate(columns):\n        col_dict = {\n            \n        }\n        if isinstance(col, basestring):\n            col = col.split(':')\n            if (len(col) > 1):\n                if ('/' in col[1]):\n                    (col_dict['fieldtype'], col_dict['options']) = col[1].split('/')\n                else:\n                    col_dict['fieldtype'] = col[1]\n            col_dict['fieldname'] = frappe.scrub(col[0])\n        else:\n            col_dict.update(col)\n            if ('fieldname' not in col_dict):\n                col_dict['fieldname'] = frappe.scrub(col_dict['label'])\n        columns_dict[idx] = col_dict\n        columns_dict[col_dict['fieldname']] = col_dict\n    return columns_dict\n", "label": 1}
{"function": "\n\ndef _dirichlet_check_input(alpha, x):\n    x = np.asarray(x)\n    if (((x.shape[0] + 1) != alpha.shape[0]) and (x.shape[0] != alpha.shape[0])):\n        raise ValueError((\"Vector 'x' must have either the same number of entries as, or one entry fewer than, parameter vector 'a', but alpha.shape = %s and x.shape = %s.\" % (alpha.shape, x.shape)))\n    if (x.shape[0] != alpha.shape[0]):\n        xk = np.array([(1 - np.sum(x, 0))])\n        if (xk.ndim == 1):\n            x = np.append(x, xk)\n        elif (xk.ndim == 2):\n            x = np.vstack((x, xk))\n        else:\n            raise ValueError('The input must be one dimensional or a two dimensional matrix containing the entries.')\n    if (np.min(x) <= 0):\n        raise ValueError(\"Each entry in 'x' must be greater than zero.\")\n    if (np.max(x) > 1):\n        raise ValueError(\"Each entry in 'x' must be smaller or equal one.\")\n    if (np.abs((np.sum(x, 0) - 1.0)) > 1e-09).any():\n        raise ValueError((\"The input vector 'x' must lie within the normal simplex. but np.sum(x, 0) = %s.\" % np.sum(x, 0)))\n    return x\n", "label": 1}
{"function": "\n\ndef serialize_options(opts):\n    '\\n    A helper method to serialize and processes the options dictionary.\\n    '\n    options = (opts or {\n        \n    }).copy()\n    for key in opts.keys():\n        if (key not in DEFAULT_OPTIONS):\n            LOG.warn('Unknown option passed to Flask-CORS: %s', key)\n    options['origins'] = sanitize_regex_param(options.get('origins'))\n    options['allow_headers'] = sanitize_regex_param(options.get('allow_headers'))\n    if (('.*' in options['origins']) and options['supports_credentials'] and options['send_wildcard']):\n        raise ValueError(\"Cannot use supports_credentials in conjunction withan origin string of '*'. See: http://www.w3.org/TR/cors/#resource-requests\")\n    serialize_option(options, 'expose_headers')\n    serialize_option(options, 'methods', upper=True)\n    if isinstance(options.get('max_age'), timedelta):\n        options['max_age'] = str(int(options['max_age'].total_seconds()))\n    return options\n", "label": 1}
{"function": "\n\ndef guess_seqtype(s):\n    dna_letters = 'AaTtUuGgCcNn'\n    ndna = 0\n    nU = 0\n    nT = 0\n    for l in s:\n        if (l in dna_letters):\n            ndna += 1\n        if ((l == 'U') or (l == 'u')):\n            nU += 1\n        elif ((l == 'T') or (l == 't')):\n            nT += 1\n    ratio = (ndna / float(len(s)))\n    if (ratio > 0.85):\n        if (nT > nU):\n            return DNA_SEQTYPE\n        else:\n            return RNA_SEQTYPE\n    else:\n        return PROTEIN_SEQTYPE\n", "label": 1}
{"function": "\n\ndef attach(self, canvas):\n    super(Win32ARBContext, self).attach(canvas)\n    share = self.context_share\n    if share:\n        if (not share.canvas):\n            raise RuntimeError('Share context has no canvas.')\n        share = share._context\n    attribs = []\n    if (self.config.major_version is not None):\n        attribs.extend([wglext_arb.WGL_CONTEXT_MAJOR_VERSION_ARB, self.config.major_version])\n    if (self.config.minor_version is not None):\n        attribs.extend([wglext_arb.WGL_CONTEXT_MINOR_VERSION_ARB, self.config.minor_version])\n    flags = 0\n    if self.config.forward_compatible:\n        flags |= wglext_arb.WGL_CONTEXT_FORWARD_COMPATIBLE_BIT_ARB\n    if self.config.debug:\n        flags |= wglext_arb.WGL_DEBUG_BIT_ARB\n    if flags:\n        attribs.extend([wglext_arb.WGL_CONTEXT_FLAGS_ARB, flags])\n    attribs.append(0)\n    attribs = (c_int * len(attribs))(*attribs)\n    self.config._set_pixel_format(canvas)\n    self._context = wglext_arb.wglCreateContextAttribsARB(canvas.hdc, share, attribs)\n", "label": 1}
{"function": "\n\ndef visit_clauselist(self, clauselist, order_by_select=None, **kw):\n    if (order_by_select is not None):\n        return self._order_by_clauselist(clauselist, order_by_select, **kw)\n    sep = clauselist.operator\n    if (sep is None):\n        sep = ' '\n    else:\n        sep = OPERATORS[clauselist.operator]\n    return sep.join((s for s in (c._compiler_dispatch(self, **kw) for c in clauselist.clauses) if s))\n", "label": 1}
{"function": "\n\ndef GetCanonicalName(disk):\n    target = QueryDosDevice(disk)\n    if (target is None):\n        print(('QueryDosDevice(%r) is %r' % (letter, target)))\n        return None\n    elif target.startswith('\\\\??\\\\'):\n        if target.startswith('\\\\??\\\\'):\n            target = target[len('\\\\??\\\\'):]\n        return (target, False)\n    elif target.startswith('\\\\Device\\\\LanmanRedirector\\\\'):\n        return (Net.NetUseGetInfo(None, (disk[0] + ':'))['remote'], True)\n    elif target.startswith('UNC\\\\'):\n        return (target[4:], True)\n    else:\n        return (GetMountVolume(letter), False)\n", "label": 1}
{"function": "\n\ndef iter_version_links(html, name):\n    '\\n    Iterate through version links (in order) within HTML.\\n\\n    Filtering out links that don\\'t \"look\" like versions.\\n\\n    Either yields hrefs to be recursively searches or tuples of (name, href)\\n    that match the given name.\\n    '\n    soup = BeautifulSoup(html)\n    for node in soup.findAll('a'):\n        if (node.get('href') is None):\n            continue\n        try:\n            (guessed_name, _) = guess_name_and_version(node.text)\n        except ValueError:\n            href = node['href']\n            for extension in ['.tar.gz', '.zip']:\n                if href.endswith(extension):\n                    (yield (basename(href), href))\n                    break\n            else:\n                if (node.get('rel') == 'download'):\n                    (yield href)\n        else:\n            if (guessed_name.replace('_', '-').lower() != name.replace('_', '-').lower()):\n                continue\n            (yield (node.text, node['href']))\n", "label": 1}
{"function": "\n\ndef scoreDuplicates(records, data_model, classifier, num_cores=1, threshold=0):\n    if (num_cores < 2):\n        from multiprocessing.dummy import Process, Pool, Queue\n        SimpleQueue = Queue\n    else:\n        from .backport import Process, Pool, SimpleQueue\n    record_pairs_queue = SimpleQueue()\n    score_queue = SimpleQueue()\n    result_queue = SimpleQueue()\n    n_map_processes = max((num_cores - 1), 1)\n    score_records = ScoreRecords(data_model, classifier, threshold)\n    map_processes = [Process(target=score_records, args=(record_pairs_queue, score_queue)) for _ in range(n_map_processes)]\n    [process.start() for process in map_processes]\n    reduce_process = Process(target=mergeScores, args=(score_queue, result_queue, n_map_processes))\n    reduce_process.start()\n    fillQueue(record_pairs_queue, records, n_map_processes)\n    result = result_queue.get()\n    if isinstance(result, Exception):\n        raise ChildProcessError\n    if result:\n        (scored_pairs_file, dtype) = result\n        scored_pairs = numpy.memmap(scored_pairs_file, dtype=dtype)\n    else:\n        scored_pairs = result\n    return scored_pairs\n", "label": 1}
{"function": "\n\ndef consolidate_listing(self, con, current_artifacts):\n    (server_artifacts, duplicates) = self.read_existing_artifacts(con)\n    known_folders = set()\n    for artifact_name in iterkeys(current_artifacts):\n        known_folders.add(posixpath.dirname(artifact_name))\n    for (artifact_name, checksum) in iteritems(server_artifacts):\n        if (artifact_name not in current_artifacts):\n            con.log_buffer.append(('000 Deleting %s' % artifact_name))\n            con.delete_file(artifact_name)\n            folder = posixpath.dirname(artifact_name)\n            if (folder not in known_folders):\n                con.log_buffer.append(('000 Deleting %s' % folder))\n                con.delete_folder(folder)\n    if (duplicates or (server_artifacts != current_artifacts)):\n        listing = []\n        for (artifact_name, checksum) in iteritems(current_artifacts):\n            listing.append(('%s|%s\\n' % (artifact_name, checksum)))\n        listing.sort()\n        con.upload_file('.lektor/.listing.tmp', ''.join(listing))\n        con.rename_file('.lektor/.listing.tmp', '.lektor/listing')\n", "label": 1}
{"function": "\n\ndef _compare_files(self, filename1, filename2):\n    '\\n        Compares two files, giving precedence to header files over source\\n        files. This allows the resulting list of files to be more\\n        intelligently sorted.\\n        '\n    if ((filename1.find('.') != (- 1)) and (filename2.find('.') != (- 1))):\n        (basename1, ext1) = filename1.rsplit('.', 1)\n        (basename2, ext2) = filename2.rsplit('.', 1)\n        if (basename1 == basename2):\n            if ((ext1 in self.HEADER_EXTENSIONS) and (ext2 in self.IMPL_EXTENSIONS)):\n                return (- 1)\n            elif ((ext1 in self.IMPL_EXTENSIONS) and (ext2 in self.HEADER_EXTENSIONS)):\n                return 1\n    return cmp(filename1, filename2)\n", "label": 1}
{"function": "\n\n@click.command()\n@click.argument('pod_path', default='.')\n@click.option('--source', type=click.Path(), required=True, help='Path to source (either zip file, directory, or file).')\n@click.option('--locale', type=str, help='Locale of the message catalog to import. This option is only applicable when --source is a .po file.')\ndef import_translations(pod_path, source, locale):\n    'Imports translations from an external source.'\n    if (source.endswith('.po') and (locale is None)):\n        text = 'Must specify --locale when --source is a .po file.'\n        raise click.ClickException(text)\n    if ((not source.endswith('.po')) and (locale is not None)):\n        text = 'Cannot specify --locale when --source is not a .po file.'\n        raise click.ClickException(text)\n    source = os.path.expanduser(source)\n    root = os.path.abspath(os.path.join(os.getcwd(), pod_path))\n    pod = pods.Pod(root, storage=storage.FileStorage)\n    if (not pod.exists):\n        raise click.ClickException('Pod does not exist: {}'.format(pod.root))\n    pod.catalogs.import_translations(source, locale=locale)\n", "label": 1}
{"function": "\n\ndef debug(s):\n    global debugFP\n    global corona_sdk_debug\n    try:\n        if ((not debugFP) and corona_sdk_debug):\n            if (not os.path.isdir(_corona_utils.PACKAGE_USER_DIR)):\n                os.makedirs(_corona_utils.PACKAGE_USER_DIR)\n            debugFP = open(os.path.normpath(os.path.join(_corona_utils.PACKAGE_USER_DIR, 'debug.log')), 'w', 1)\n    except:\n        pass\n    thread_id = re.sub('.*\\\\(([^,]*),.*', '\\\\1', str(threading.current_thread()))\n    log_line = ((((str(datetime.datetime.now()) + ' (') + str(thread_id)) + '): ') + str(s))\n    if debugFP:\n        debugFP.write((log_line + '\\n'))\n    _corona_utils.debug(log_line)\n", "label": 1}
{"function": "\n\ndef rescale_ctfs(volume_property, new_range):\n    \" Given the volume_property with a new_range for the data while\\n    using the same transfer functions, this function rescales the\\n    CTF's so that everything works OK.\\n\\n    It returns the CTF and OTF.\\n    \"\n    ctf = volume_property.rgb_transfer_function\n    otf = volume_property.get_scalar_opacity()\n    old_range = ctf.range\n\n    def _rescale_value(x, old, new):\n        nx = ((x - old[0]) / (old[1] - old[0]))\n        return (new[0] + (nx * (new[1] - new[0])))\n    if ((new_range[0] != old_range[0]) and (new_range[1] != old_range[1])):\n        s_d = save_ctfs(volume_property)\n        (s1, s2) = new_range\n        if (s1 > s2):\n            s_d['range'] = (s2, s1)\n        else:\n            s_d['range'] = (s1, s2)\n        rgb = s_d['rgb']\n        for v in rgb:\n            v[0] = _rescale_value(v[0], old_range, new_range)\n        alpha = s_d['alpha']\n        for v in alpha:\n            v[0] = _rescale_value(v[0], old_range, new_range)\n        (ctf, otf) = load_ctfs(s_d, volume_property)\n    return (ctf, otf)\n", "label": 1}
{"function": "\n\ndef _WaitForCondition(condition_callback, timeout=None):\n    'Waits until the specified callback returns a value that evaluates True.\\n\\n  Similar to the threading.Condition.wait method that is the basis of most\\n  threading class wait routines. Polls the condition, starting with frequent\\n  checks but extending the delay between checks upon each failure.\\n\\n  Args:\\n    condition_callback: Callable that returns a value that evaluates True to end\\n        the wait or evaluates False to continue the wait.\\n    timeout: Optional float. Number of seconds to wait before giving up. If\\n        provided, the condition is still checked at least once before giving up.\\n        If not provided, the wait does not time out.\\n\\n  Returns:\\n    True if condition_callback returned a value that evaluated True. False if\\n    condition_callback did not return a value that evaluated True before the\\n    timeout.\\n  '\n    deadline = (None if (timeout is None) else (time.time() + timeout))\n    delay = _WAIT_MIN_RECHECK_DELAY\n    while True:\n        if condition_callback():\n            return True\n        remaining_time = (_WAIT_MAX_RECHECK_DELAY if (deadline is None) else (deadline - time.time()))\n        if (remaining_time <= 0):\n            return False\n        time.sleep(delay)\n        delay = min((delay * 2), remaining_time, _WAIT_MAX_RECHECK_DELAY)\n", "label": 1}
{"function": "\n\ndef _app_hook_handler(self, alert_body, hook_headers):\n    if (not alert_body['application_name']):\n        self._log.info('No application found for alert %s. Will Ignore.', alert_body)\n        return\n    long_description = alert_body['long_description']\n    if (self._is_alert_opened(long_description) or self._is_escalated_downtime(long_description)):\n        payload = {\n            'alert': alert_body,\n            'header': hook_headers,\n        }\n        self._dispatch_trigger(WEB_APP_ALERT_TRIGGER_REF, payload)\n    elif (self._is_alert_closed(long_description) or self._is_downtime_recovered(long_description)):\n        payload = {\n            'alert': alert_body,\n            'header': hook_headers,\n        }\n        self._log.info('App alert closed. Delay.')\n        eventlet.spawn_after(self._normal_report_delay, self._dispatch_application_normal, payload)\n    elif (self._is_alert_canceled(long_description) or self._is_alert_acknowledged(long_description)):\n        self._log.info('Ignored alert : %s.', alert_body)\n", "label": 1}
{"function": "\n\ndef stop(self, signum=None, _unused=None):\n    'Stop the consumer from consuming by calling BasicCancel and setting\\n        our state.\\n\\n        :param int signum: The signal received\\n        :param frame _unused: The stack frame from when the signal was called\\n\\n        '\n    LOGGER.debug('Stop called in state: %s', self.state_description)\n    if self.is_stopped:\n        LOGGER.warning('Stop requested but consumer is already stopped')\n        return\n    elif self.is_shutting_down:\n        LOGGER.warning('Stop requested, consumer is already shutting down')\n        return\n    elif self.is_waiting_to_shutdown:\n        LOGGER.warning('Stop requested but already waiting to shut down')\n        return\n    self.cancel_consumer_with_rabbitmq()\n    if self.is_processing:\n        LOGGER.info('Waiting for consumer to finish processing')\n        self.set_state(self.STATE_STOP_REQUESTED)\n        if (signum == signal.SIGTERM):\n            signal.siginterrupt(signal.SIGTERM, False)\n        return\n    if self.stats.statsd:\n        self.stats.statsd.stop()\n    self.on_ready_to_stop()\n", "label": 1}
{"function": "\n\ndef translatable_modelformset_factory(language, model, form=TranslatableModelForm, formfield_callback=None, formset=BaseModelFormSet, extra=1, can_delete=False, can_order=False, max_num=None, fields=None, exclude=None, **kwargs):\n    (form_kwargs, formset_kwargs) = ({\n        \n    }, {\n        \n    })\n    for key in ('widgets', 'localized_fields', 'labels', 'help_texts', 'error_messages'):\n        if (key in kwargs):\n            form_kwargs[key] = kwargs.pop(key)\n    for key in ('validate_max',):\n        if (key in kwargs):\n            formset_kwargs[key] = kwargs.pop(key)\n    if kwargs:\n        raise TypeError(('Unknown arguments %r for translatable_modelformset_factory. If it is legit, it is probably new in Django. Please open a ticket so we can add it.' % tuple(kwargs.keys())))\n    form = translatable_modelform_factory(language, model, form=form, fields=fields, exclude=exclude, formfield_callback=formfield_callback, **form_kwargs)\n    FormSet = formset_factory(form, formset, extra=extra, max_num=max_num, can_order=can_order, can_delete=can_delete, **formset_kwargs)\n    FormSet.model = model\n    return FormSet\n", "label": 1}
{"function": "\n\ndef format_params(self, params):\n    fp = []\n    for p in params:\n        if isinstance(p, text_type):\n            if (not self.driver_supports_utf8):\n                fp.append(p.encode('utf-8'))\n            else:\n                fp.append(p)\n        elif isinstance(p, binary_type):\n            if (not self.driver_supports_utf8):\n                fp.append(p.decode(self.encoding).encode('utf-8'))\n            else:\n                fp.append(p)\n        elif isinstance(p, type(True)):\n            if p:\n                fp.append(1)\n            else:\n                fp.append(0)\n        else:\n            fp.append(p)\n    return tuple(fp)\n", "label": 1}
{"function": "\n\ndef CallExpression(traverser, node):\n    args = [traverser.traverse_node(a) for a in node['arguments']]\n    member = traverser.traverse_node(node['callee'])\n    if ((node['callee']['type'] == 'MemberExpression') and (node['callee']['property']['type'] == 'Identifier')):\n        identifier_name = node['callee']['property']['name']\n        if (identifier_name in instanceactions.INSTANCE_DEFINITIONS):\n            traverser._debug('Calling instance action...')\n            result = instanceactions.INSTANCE_DEFINITIONS[identifier_name](args, traverser, traverser.traverse_node(node['callee']['object']))\n            if (result is not None):\n                return result\n    if (isinstance(member, JSGlobal) and ('return' in member.global_data)):\n        traverser._debug('EVALUATING RETURN...')\n        output = member.global_data['return'](wrapper=member, arguments=args, traverser=traverser)\n        if (output is not None):\n            return output\n    return JSObject(traverser=traverser)\n", "label": 1}
{"function": "\n\ndef _prepare_plots(self, mixing=False, unmixing=False):\n    if (self.locations_ is None):\n        raise RuntimeError('Need sensor locations for plotting')\n    if (self.topo_ is None):\n        from scot.eegtopo.topoplot import Topoplot\n        self.topo_ = Topoplot(clipping=self.topo_clipping)\n        self.topo_.set_locations(self.locations_)\n    if (mixing and (not self.mixmaps_)):\n        premix = (self.premixing_ if (self.premixing_ is not None) else np.eye(self.mixing_.shape[1]))\n        self.mixmaps_ = self.plotting.prepare_topoplots(self.topo_, np.dot(self.mixing_, premix))\n    if (unmixing and (not self.unmixmaps_)):\n        preinv = (np.linalg.pinv(self.premixing_) if (self.premixing_ is not None) else np.eye(self.unmixing_.shape[0]))\n        self.unmixmaps_ = self.plotting.prepare_topoplots(self.topo_, np.dot(preinv, self.unmixing_).T)\n", "label": 1}
{"function": "\n\ndef read_fasta_lengths(ifile):\n    'Generate sequence ID,length from stream ifile'\n    id = None\n    seqLength = 0\n    isEmpty = True\n    for line in ifile:\n        if ('>' == line[0]):\n            if ((id is not None) and (seqLength > 0)):\n                (yield (id, seqLength))\n                isEmpty = False\n            id = line[1:].split()[0]\n            seqLength = 0\n        elif (id is not None):\n            for word in line.split():\n                seqLength += len(word)\n    if ((id is not None) and (seqLength > 0)):\n        (yield (id, seqLength))\n    elif isEmpty:\n        raise IOError('no readable sequence in FASTA file!')\n", "label": 1}
{"function": "\n\ndef _match_task_syslog_path(path, application_id=None, job_id=None):\n    'Is this the path/URI of a task syslog?\\n\\n    If so, return a dictionary containing application_id and container_id\\n    (on YARN) or attempt_id (on pre-YARN Hadoop). Otherwise, return None\\n\\n    Optionally, filter by application_id (YARN) or job_id (pre-YARN).\\n    '\n    m = _PRE_YARN_TASK_SYSLOG_PATH_RE.match(path)\n    if m:\n        if (job_id and (job_id != _to_job_id(m.group('attempt_id')))):\n            return None\n        return dict(attempt_id=m.group('attempt_id'))\n    m = _YARN_TASK_SYSLOG_PATH_RE.match(path)\n    if m:\n        if (application_id and (application_id != m.group('application_id'))):\n            return None\n        return dict(application_id=m.group('application_id'), container_id=m.group('container_id'))\n    return None\n", "label": 1}
{"function": "\n\ndef mayRaiseException(self, exception_type):\n    for default in self.getDefaults():\n        if default.mayRaiseException(exception_type):\n            return True\n    kw_defaults = self.getKwDefaults()\n    if ((kw_defaults is not None) and kw_defaults.mayRaiseException(exception_type)):\n        return True\n    annotations = self.getAnnotations()\n    if ((annotations is not None) and annotations.mayRaiseException(exception_type)):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef key_press(self, event):\n    if (self.box.figure.canvas.toolbar.mode != ''):\n        return\n    if ((event.key == 'enter') and (self.mode == 0)):\n        self.mode += 1\n        self.box.x = self.box.x[:(- 1)]\n        self.box.y = self.box.y[:(- 1)]\n    if ((event.key == 'y') and (self.mode == 2)):\n        self.show_poly = (not self.show_poly)\n        self.draw_slicer(event)\n        self.box.figure.canvas.draw()\n", "label": 1}
{"function": "\n\ndef mouseMoveEvent(self, event):\n    self.oldPosition = self.pos()\n    if (self.isSideClicked and self.isCusorRightSide):\n        w = max(self.minimumWidth(), ((self.currentWidth + event.x()) - self.rdragx))\n        h = self.currentHeight\n        self.resize(w, h)\n    elif (self.isSideClicked and self.isCusorDownSide):\n        w = self.currentWidth\n        h = max(self.minimumHeight(), ((self.currentHeight + event.y()) - self.rdragy))\n        self.resize(w, h)\n    elif self.isMaximized():\n        event.ignore()\n    elif (not self.isLocked()):\n        if hasattr(self, 'dragPosition'):\n            if (event.buttons() == Qt.LeftButton):\n                self.move((event.globalPos() - self.dragPosition))\n                event.accept()\n", "label": 1}
{"function": "\n\ndef fetch_art(self, session, task):\n    'Find art for the album being imported.'\n    if task.is_album:\n        if (task.album.artpath and os.path.isfile(task.album.artpath)):\n            return\n        if (task.choice_flag == importer.action.ASIS):\n            local = True\n        elif (task.choice_flag == importer.action.APPLY):\n            local = False\n        else:\n            return\n        candidate = self.art_for_album(task.album, task.paths, local)\n        if candidate:\n            self.art_candidates[task] = candidate\n", "label": 1}
{"function": "\n\ndef _is_in_collision(self, pos):\n    (x, y) = pos\n    structure = self.__class__.MAZE_STRUCTURE\n    size_scaling = self.__class__.MAZE_SIZE_SCALING\n    for i in xrange(len(structure)):\n        for j in xrange(len(structure[0])):\n            if (structure[i][j] == 1):\n                minx = (((j * size_scaling) - (size_scaling * 0.5)) - self._init_torso_x)\n                maxx = (((j * size_scaling) + (size_scaling * 0.5)) - self._init_torso_x)\n                miny = (((i * size_scaling) - (size_scaling * 0.5)) - self._init_torso_y)\n                maxy = (((i * size_scaling) + (size_scaling * 0.5)) - self._init_torso_y)\n                if ((minx <= x <= maxx) and (miny <= y <= maxy)):\n                    return True\n    return False\n", "label": 1}
{"function": "\n\ndef __init__(self, items=None, identifier=None, parent=None, **kwargs):\n    self.__dict__['_display'] = 'auto'\n    self.__dict__['_max_cols'] = 4\n    if (items and all((isinstance(item, Dimensioned) for item in items))):\n        items = self.from_values(items).data\n    params = {p: kwargs.pop(p) for p in (list(self.params().keys()) + ['id']) if (p in kwargs)}\n    AttrTree.__init__(self, items, identifier, parent, **kwargs)\n    Dimensioned.__init__(self, self.data, **params)\n", "label": 1}
{"function": "\n\ndef get_literal_value(self, traverser=None):\n    'Arrays return a comma-delimited version of themselves.'\n    if self.recursing:\n        return '(recursion)'\n    self.recursing = True\n    output = ','.join((unicode((w.get_literal_value(traverser=(traverser or self.traverser)) if ((w is not None) and (w is not self)) else '')) for w in self.elements))\n    self.recursing = False\n    return output\n", "label": 1}
{"function": "\n\ndef __gen_opts(self, opts_in, grains, saltenv=None, ext=None, pillarenv=None):\n    '\\n        The options need to be altered to conform to the file client\\n        '\n    opts = copy.deepcopy(opts_in)\n    opts['file_roots'] = opts['pillar_roots']\n    opts['file_client'] = 'local'\n    if (not grains):\n        opts['grains'] = {\n            \n        }\n    else:\n        opts['grains'] = grains\n    if ('environment' not in opts):\n        opts['environment'] = saltenv\n    opts['id'] = self.minion_id\n    if ('pillarenv' not in opts):\n        opts['pillarenv'] = pillarenv\n    if opts['state_top'].startswith('salt://'):\n        opts['state_top'] = opts['state_top']\n    elif opts['state_top'].startswith('/'):\n        opts['state_top'] = salt.utils.url.create(opts['state_top'][1:])\n    else:\n        opts['state_top'] = salt.utils.url.create(opts['state_top'])\n    if self.__valid_ext(ext):\n        if ('ext_pillar' in opts):\n            opts['ext_pillar'].append(ext)\n        else:\n            opts['ext_pillar'] = [ext]\n    return opts\n", "label": 1}
{"function": "\n\ndef regroup_commands(commands):\n    '\\n    Returns a list of tuples:\\n\\n        [(command_to_run, [list, of, commands])]\\n\\n    If the list of commands has a single item, the command was not grouped.\\n    '\n    grouped = []\n    pending = []\n\n    def group_pending():\n        if (not pending):\n            return\n        new_command = grouped_command(pending)\n        result = []\n        while pending:\n            result.append(pending.pop(0))\n        grouped.append((new_command, result))\n    for (command, next_command) in peek(commands):\n        if can_group_commands(command, next_command):\n            if (pending and (not can_group_commands(pending[0], command))):\n                group_pending()\n            pending.append(command)\n        else:\n            if (pending and can_group_commands(pending[0], command)):\n                pending.append(command)\n            else:\n                grouped.append((command.clone(), [command]))\n            group_pending()\n    group_pending()\n    return grouped\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_stream(body):\n    if (len(body) < 3):\n        raise ValueError('Not enough data')\n    if ((ord(body[0]) & 128) == 0):\n        raise ValueError('Long header not supported')\n    (length, msg_type) = struct.unpack_from('!HB', body, 0)\n    length &= 32767\n    msg_data = body[3:(2 + length)]\n    if (len(msg_data) != (length - 1)):\n        raise ValueError('Not enough data')\n    type = typemap.get(msg_type)\n    if (not type):\n        raise ValueError(('Unsupported message type: %d' % msg_type))\n    (obj, size) = type.from_stream(msg_data)\n    if (len(msg_data) != size):\n        raise ValueError('Read mismatch')\n    return (HandshakeMessage(msg_type, obj), (2 + length))\n", "label": 1}
{"function": "\n\ndef get_form(self, form_class=None):\n    '\\n        Returns an instance of the form to be used in this view.\\n        '\n    self.form = super(SmartFormMixin, self).get_form(form_class)\n    fields = list(self.derive_fields())\n    exclude = self.derive_exclude()\n    exclude += self.derive_readonly()\n    for field in exclude:\n        if (field in self.form.fields):\n            del self.form.fields[field]\n    if (fields is not None):\n        for (name, field) in self.form.fields.items():\n            if (not (name in fields)):\n                del self.form.fields[name]\n    location = forms.CharField(widget=forms.widgets.HiddenInput(), required=False)\n    if ('HTTP_REFERER' in self.request.META):\n        location.initial = self.request.META['HTTP_REFERER']\n    self.form.fields['loc'] = location\n    if fields:\n        fields.append('loc')\n    for (name, field) in self.form.fields.items():\n        field = self.customize_form_field(name, field)\n        self.form.fields[name] = field\n    return self.form\n", "label": 1}
{"function": "\n\ndef _restore_orig_vm_and_cleanup_orphan(self, instance, block_device_info=None, power_on=True):\n    name_label = self._get_orig_vm_name_label(instance)\n    vm_ref = vm_utils.lookup(self._session, name_label)\n    new_ref = vm_utils.lookup(self._session, instance['name'])\n    if (vm_ref is not None):\n        if (new_ref is not None):\n            self._destroy(instance, new_ref)\n        name_label = instance['name']\n        vm_utils.set_vm_name_label(self._session, vm_ref, name_label)\n        self._attach_mapped_block_devices(instance, block_device_info)\n    elif (new_ref is not None):\n        vm_ref = new_ref\n    if (power_on and vm_utils.is_vm_shutdown(self._session, vm_ref)):\n        self._start(instance, vm_ref)\n", "label": 1}
{"function": "\n\ndef render_change_form(self, request, context, add=False, change=False, form_url='', obj=None):\n    deployment_nodes = []\n    for node in Node.objects.order_by('name'):\n        deployment_nodes.append((node.site_deployment.deployment.id, node.id, node.name))\n    deployment_flavors = []\n    for flavor in Flavor.objects.all():\n        for deployment in flavor.deployments.all():\n            deployment_flavors.append((deployment.id, flavor.id, flavor.name))\n    deployment_images = []\n    for image in Image.objects.all():\n        for deployment_image in image.imagedeployments.all():\n            deployment_images.append((deployment_image.deployment.id, image.id, image.name))\n    site_login_bases = []\n    for site in Site.objects.all():\n        site_login_bases.append((site.id, site.login_base))\n    context['deployment_nodes'] = deployment_nodes\n    context['deployment_flavors'] = deployment_flavors\n    context['deployment_images'] = deployment_images\n    context['site_login_bases'] = site_login_bases\n    return super(InstanceAdmin, self).render_change_form(request, context, add, change, form_url, obj)\n", "label": 1}
{"function": "\n\ndef parse_socket_string(socket_string, default_port):\n    '\\n    Given a string representing a socket, returns a tuple of (host, port).\\n    Valid strings are DNS names, IPv4 addresses, or IPv6 addresses, with an\\n    optional port. If an IPv6 address is specified it **must** be enclosed in\\n    [], like *[::1]* or *[::1]:11211*. This follows the accepted prescription\\n    for `IPv6 host literals`_.\\n\\n    Examples::\\n\\n        server.org\\n        server.org:1337\\n        127.0.0.1:1337\\n        [::1]:1337\\n        [::1]\\n\\n    .. _IPv6 host literals: https://tools.ietf.org/html/rfc3986#section-3.2.2\\n    '\n    port = default_port\n    if socket_string.startswith('['):\n        match = IPV6_RE.match(socket_string)\n        if (not match):\n            raise ValueError(('Invalid IPv6 address: %s' % socket_string))\n        host = match.group('address')\n        port = (match.group('port') or port)\n    elif (':' in socket_string):\n        tokens = socket_string.split(':')\n        if (len(tokens) > 2):\n            raise ValueError(\"IPv6 addresses must be between '[]'\")\n        (host, port) = tokens\n    else:\n        host = socket_string\n    return (host, port)\n", "label": 1}
{"function": "\n\ndef main(argv=None):\n    args = docopt.docopt(__doc__, argv=argv)\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(name)s -- %(message)s')\n    revert_radius = int(args['--revert-radius'])\n    revert_window = (int(args['--revert-window']) * (60 * 60))\n    if args['--host']:\n        session = mwapi.Session(args['--host'], user_agent='ORES revert labeling utility')\n    else:\n        session = None\n    dumps = args['<dump-file>']\n    verbose = args['--verbose']\n    start = args['--start']\n    if start:\n        start = Timestamp(start)\n    end = args['--end']\n    if end:\n        end = Timestamp(end)\n    reverted_only = args['--reverted-only']\n    trusted_groups = args['--trusted-groups']\n    if trusted_groups:\n        trusted_groups = trusted_groups.split(',')\n        trusted_users = load_user_group_members(trusted_groups, session)\n    else:\n        trusted_users = None\n    trusted_edits = args['--trusted-edits']\n    if trusted_edits:\n        trusted_edits = int(trusted_edits)\n    if (args['--rev-reverteds'] == '<stdout>'):\n        rev_reverteds = mysqltsv.Writer(sys.stdout)\n    else:\n        rev_reverteds = mysqltsv.Writer(open(args['--rev-reverteds'], 'w'))\n    check_blocked = args['--check-blocked']\n    run(dumps, session, start, end, revert_radius, revert_window, reverted_only, trusted_users, trusted_edits, rev_reverteds, check_blocked, verbose=verbose)\n", "label": 1}
{"function": "\n\ndef set_regex(self, file_regex=None, line_regex=None):\n    \"Update the view's result_(file|line)_regex patterns.\\n        Only overrides the previous settings if parameters are not None.\\n        \"\n    if (file_regex is not None):\n        self.file_regex = file_regex\n    if hasattr(self, 'file_regex'):\n        self.settings.result_file_regex = self.file_regex\n    if (line_regex is not None):\n        self.line_regex = line_regex\n    if hasattr(self, 'line_regex'):\n        self.settings.result_line_regex = self.line_regex\n    contents = get_text(self.view)\n    sel = self.view.sel()\n    selections = list(sel)\n    self.view = self.window.get_output_panel(self.panel_name)\n    sel.clear()\n    for reg in selections:\n        sel.add(reg)\n    self.write(contents)\n", "label": 1}
{"function": "\n\ndef get(self, identifier, default=None):\n    split_label = (tuple(identifier.split('.')) if isinstance(identifier, str) else tuple(identifier))\n    if (len(split_label) == 1):\n        identifier = split_label[0]\n        return self.__dict__.get(identifier, default)\n    path_item = self\n    for identifier in split_label:\n        if ((path_item == default) or (path_item is None)):\n            return default\n        path_item = path_item.get(identifier, default)\n    return path_item\n", "label": 1}
{"function": "\n\ndef RGS_unrank(rank, m):\n    '\\n    Gives the unranked restricted growth string for a given\\n    superset size.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.combinatorics.partitions import RGS_unrank\\n    >>> RGS_unrank(14, 4)\\n    [0, 1, 2, 3]\\n    >>> RGS_unrank(0, 4)\\n    [0, 0, 0, 0]\\n    '\n    if (m < 1):\n        raise ValueError('The superset size must be >= 1')\n    if ((rank < 0) or (RGS_enum(m) <= rank)):\n        raise ValueError('Invalid arguments')\n    L = ([1] * (m + 1))\n    j = 1\n    D = RGS_generalized(m)\n    for i in range(2, (m + 1)):\n        v = D[((m - i), j)]\n        cr = (j * v)\n        if (cr <= rank):\n            L[i] = (j + 1)\n            rank -= cr\n            j += 1\n        else:\n            L[i] = int(((rank / v) + 1))\n            rank %= v\n    return [(x - 1) for x in L[1:]]\n", "label": 1}
{"function": "\n\ndef get_ambient_temperature(self, n=5):\n    '\\n        Populates the self.ambient_temp property\\n\\n        Calculation is taken from Rs232_Comms_v100.pdf section \"Converting values\\n        sent by the device to meaningful units\" item 5.\\n        '\n    if self.logger:\n        self.logger.info('Getting ambient temperature')\n    values = []\n    for i in range(0, n):\n        try:\n            value = (float(self.query('!T')[0]) / 100.0)\n        except:\n            pass\n        else:\n            if self.logger:\n                self.logger.debug('  Ambient Temperature Query = {:.1f}'.format(value))\n            values.append(value)\n    if (len(values) >= (n - 1)):\n        self.ambient_temp = (np.median(values) * u.Celsius)\n        if self.logger:\n            self.logger.info('  Ambient Temperature = {:.1f}'.format(self.ambient_temp))\n    else:\n        self.ambient_temp = None\n        if self.logger:\n            self.logger.info('  Failed to Read Ambient Temperature')\n    return self.ambient_temp\n", "label": 1}
{"function": "\n\ndef _get_show_by_search(show_name, show_year, show_network, show_language, show_country, show_web_channel, embed):\n    if show_year:\n        show_year = str(show_year)\n    qualifiers = list(filter(None, [show_year, show_network, show_language, show_country, show_web_channel]))\n    if qualifiers:\n        qualifiers = [q.lower() for q in qualifiers if q]\n        show = _get_show_with_qualifiers(show_name, qualifiers)\n    else:\n        return show_single_search(show=show_name, embed=embed)\n    if embed:\n        return show_main_info(maze_id=show.id, embed=embed)\n    else:\n        return show\n", "label": 1}
{"function": "\n\ndef g(self, x):\n    g1 = ((x[0] + x[1]) - 2.0)\n    g2 = ((6.0 - x[0]) - x[1])\n    g3 = ((2.0 - x[1]) + x[0])\n    g4 = ((1.0 - x[0]) + (3.0 * x[1]))\n    g5 = ((4.0 - ((x[2] - 3) ** 2)) - x[3])\n    g6 = ((((x[4] - 3) ** 2) + x[5]) - 4.0)\n    if ((g1 >= 0) and (g2 >= 0) and (g3 >= 0) and (g4 >= 0) and (g5 >= 0) and (g6 >= 0)):\n        return (True, array([0.0, 0.0]))\n    return (False, array([g1, g2, g3, g4, g5, g6]))\n", "label": 1}
{"function": "\n\n@derived_from(pd.Series)\ndef append(self, other):\n    if isinstance(other, (list, dict)):\n        msg = \"append doesn't support list or dict input\"\n        raise NotImplementedError(msg)\n    if (not isinstance(other, _Frame)):\n        from .io import from_pandas\n        other = from_pandas(other, 1)\n    from .multi import _append\n    if (self.known_divisions and other.known_divisions):\n        if (self.divisions[(- 1)] < other.divisions[0]):\n            divisions = (self.divisions[:(- 1)] + other.divisions)\n            return _append(self, other, divisions)\n        else:\n            msg = 'Unable to append two dataframes to each other with known divisions if those divisions are not ordered. The divisions/index of the second dataframe must be greater than the divisions/index of the first dataframe.'\n            raise ValueError(msg)\n    else:\n        divisions = ([None] * ((self.npartitions + other.npartitions) + 1))\n        return _append(self, other, divisions)\n", "label": 1}
{"function": "\n\ndef job_read_inputs(self):\n    d = {\n        \n    }\n    if (not self.has_input('command')):\n        raise ModuleError(self, 'No command specified')\n    d['command'] = self.get_input('command').strip()\n    d['working_directory'] = (self.get_input('working_directory') if self.has_input('working_directory') else '.')\n    if (not self.has_input('input_directory')):\n        raise ModuleError(self, 'No input directory specified')\n    d['input_directory'] = self.get_input('input_directory').strip()\n    d['additional_arguments'] = {\n        'processes': 1,\n        'time': (- 1),\n        'mpi': False,\n        'threads': 1,\n        'memory': (- 1),\n        'diskspace': (- 1),\n    }\n    for k in d['additional_arguments']:\n        if self.has_input(k):\n            d['additional_arguments'][k] = self.get_input(k)\n    return d\n", "label": 1}
{"function": "\n\n@classmethod\ndef MergeAppYamlAppInclude(cls, appyaml, appinclude):\n    'This function merges an app.yaml file with referenced builtins/includes.\\n    '\n    if (not appinclude):\n        return appyaml\n    if appinclude.handlers:\n        tail = (appyaml.handlers or [])\n        appyaml.handlers = []\n        for h in appinclude.handlers:\n            if ((not h.position) or (h.position == 'head')):\n                appyaml.handlers.append(h)\n            else:\n                tail.append(h)\n            h.position = None\n        appyaml.handlers.extend(tail)\n    AppInclude.MergeManualScaling(appyaml, appinclude)\n    appyaml.admin_console = AdminConsole.Merge(appyaml.admin_console, appinclude.admin_console)\n    appyaml.vm_settings = VmSettings.Merge(appyaml.vm_settings, appinclude.vm_settings)\n    return appyaml\n", "label": 1}
{"function": "\n\ndef config_file(kind='local'):\n    'Get the filename of the distutils, local, global, or per-user config\\n\\n    `kind` must be one of \"local\", \"global\", or \"user\"\\n    '\n    if (kind == 'local'):\n        return 'setup.cfg'\n    if (kind == 'global'):\n        return os.path.join(os.path.dirname(distutils.__file__), 'distutils.cfg')\n    if (kind == 'user'):\n        dot = (((os.name == 'posix') and '.') or '')\n        return os.path.expanduser(convert_path(('~/%spydistutils.cfg' % dot)))\n    raise ValueError(\"config_file() type must be 'local', 'global', or 'user'\", kind)\n", "label": 1}
{"function": "\n\ndef validate_dict(template, to_be_checked, parent, missing_log, extras_log):\n    '\\n    Given a dictionary as a template will check that the to_be_checked\\n    dictionary.\\n    '\n    for k in to_be_checked:\n        k = k.strip()\n        if (not (k in template)):\n            extras_log.append(('Field %r in record %r' % (k, parent)))\n    for k in template:\n        k = k.strip()\n        if (not (k in to_be_checked)):\n            missing_log.append(('Field %r in record %r' % (k, parent)))\n    for (key, val) in template.iteritems():\n        if isinstance(val, dict):\n            validate_dict(val, to_be_checked[key], parent, missing_log, extras_log)\n", "label": 1}
{"function": "\n\ndef uninstall(self):\n    platform = self.get_type()\n    installed_platforms = PlatformFactory.get_platforms(installed=True).keys()\n    if (platform not in installed_platforms):\n        raise exception.PlatformNotInstalledYet(platform)\n    deppkgs = set()\n    for item in installed_platforms:\n        if (item == platform):\n            continue\n        p = PlatformFactory.newPlatform(item)\n        deppkgs = deppkgs.union(set(p.get_packages().keys()))\n    pm = PackageManager()\n    for name in self.get_packages().keys():\n        if ((not pm.is_installed(name)) or (name in deppkgs)):\n            continue\n        pm.uninstall(name)\n    installed_platforms.remove(platform)\n    set_state_item('installed_platforms', installed_platforms)\n    return True\n", "label": 1}
{"function": "\n\ndef redirect_permissions_request(request, perm_name=None, target_ct_id=None, target_id=None, user_ct_id=None, user_id=None):\n    \"\\n    Gets the target and user objects and passes them along with the \\n    L{ExpedientPermission} object named by C{perm_name} to the view that's\\n    used by the permission.\\n    \"\n    permission = get_object_or_404(ExpedientPermission, name=perm_name)\n    target_obj_or_class = get_object_from_ids(target_ct_id, target_id)\n    if (type(target_obj_or_class) == ContentType):\n        target_obj_or_class = target_obj_or_class.model_class()\n    user = get_object_from_ids(user_ct_id, user_id)\n    if (not permission.view):\n        raise PermissionDenied(perm_name, target_obj_or_class, user, False)\n    view = get_callable(permission.view)\n    redirect_to = request.GET.get('next', '')\n    if ((not redirect_to) or (' ' in redirect_to) or ('//' in redirect_to)):\n        redirect_to = None\n    return view(request, permission, user, target_obj_or_class, redirect_to=redirect_to)\n", "label": 1}
{"function": "\n\ndef get_response(self):\n    '\\n        gets the message type and message from rexster\\n\\n        :returns: RexProMessage\\n        '\n    msg_version = self.recv(1)\n    if (not msg_version):\n        raise exceptions.RexProConnectionException('socket connection has been closed')\n    if (bytearray([msg_version])[0] != 1):\n        raise exceptions.RexProConnectionException('unsupported protocol version: {}'.format())\n    serializer_type = self.recv(1)\n    if (bytearray(serializer_type)[0] != 0):\n        raise exceptions.RexProConnectionException('unsupported serializer version: {}'.format())\n    self.recv(4)\n    msg_type = self.recv(1)\n    msg_type = bytearray(msg_type)[0]\n    msg_len = struct.unpack('!I', self.recv(4))[0]\n    response = ''\n    while (len(response) < msg_len):\n        response += self.recv(msg_len)\n    MessageTypes = messages.MessageTypes\n    type_map = {\n        MessageTypes.ERROR: messages.ErrorResponse,\n        MessageTypes.SESSION_RESPONSE: messages.SessionResponse,\n        MessageTypes.SCRIPT_RESPONSE: messages.MsgPackScriptResponse,\n    }\n    if (msg_type not in type_map):\n        raise exceptions.RexProConnectionException(\"can't deserialize message type {}\".format(msg_type))\n    return type_map[msg_type].deserialize(response)\n", "label": 1}
{"function": "\n\ndef get_language_from_extension(file_name):\n    'Returns a matching language for the given file extension.\\n    '\n    (filepart, extension) = os.path.splitext(file_name)\n    if (os.path.exists(u('{0}{1}').format(u(filepart), u('.c'))) or os.path.exists(u('{0}{1}').format(u(filepart), u('.C')))):\n        return 'C'\n    extension = extension.lower()\n    if (extension == '.h'):\n        directory = os.path.dirname(file_name)\n        available_files = os.listdir(directory)\n        available_extensions = list(zip(*map(os.path.splitext, available_files)))[1]\n        available_extensions = [ext.lower() for ext in available_extensions]\n        if ('.cpp' in available_extensions):\n            return 'C++'\n        if ('.c' in available_extensions):\n            return 'C'\n    return None\n", "label": 1}
{"function": "\n\ndef info(self, query):\n    response = self.command('remote-info', query)[0].decode(*enc)\n    if (response and ('Unknown package' not in response)):\n        from re import match\n        from string import strip\n        res = [map(strip, match('(\\\\w*)(.*$)', line.replace('\\n', '')).groups()) for line in response.splitlines() if (line and ('PACKAGE' not in line) and ('====' not in line) and match('(\\\\w*)(.*$)', line))]\n        return self.munge_lines(res)\n    return ['Aborted: No info available']\n", "label": 1}
{"function": "\n\ndef __init__(self, parent, obj, parent_obj, name, temp_obj, temp_parent_obj):\n    lst = [name]\n    t = type(obj)\n    if (t == bool):\n        self._obj_type = bool_conv\n    else:\n        self._obj_type = t\n    self._parent_obj = parent_obj\n    self._temp_parent_obj = temp_parent_obj\n    self._name = name\n    if (t == ConfigurationObject):\n        lst.extend(['', ''])\n        QtGui.QTreeWidgetItem.__init__(self, parent, lst)\n        self.setFlags((self.flags() & (~ (QtCore.Qt.ItemIsDragEnabled | QtCore.Qt.ItemIsSelectable))))\n    elif ((t == tuple) and (obj[0] is None) and isinstance(obj[1], type)):\n        self._obj_type = obj[1]\n        lst.extend(['', obj[1].__name__])\n        QtGui.QTreeWidgetItem.__init__(self, parent, lst)\n        self.setFlags(((self.flags() & (~ QtCore.Qt.ItemIsDragEnabled)) | QtCore.Qt.ItemIsEditable))\n    else:\n        lst.extend([str(obj), type(obj).__name__])\n        QtGui.QTreeWidgetItem.__init__(self, parent, lst)\n        self.setFlags(((self.flags() & (~ QtCore.Qt.ItemIsDragEnabled)) | QtCore.Qt.ItemIsEditable))\n", "label": 1}
{"function": "\n\ndef check_protected_from_update(object, data):\n    if (object.is_protected and data.get('is_protected', True)):\n        if ('is_public' in data):\n            obj = object.to_dict()\n            if all((((k == 'is_public') or ((k in obj) and (obj[k] == v))) for (k, v) in six.iteritems(data))):\n                return\n        raise ex.UpdateFailedException(object.id, _(\"{object} with id '%s' could not be updated because it's marked as protected\").format(object=type(object).__name__))\n", "label": 1}
{"function": "\n\ndef _check_result(args, result, expected, cmd_err, ex_class):\n    if (expected == ANY):\n        return\n    if (expected == None):\n        raise ex_class(args, result, expected, cmd_err)\n    if isinstance(expected, (list, tuple)):\n        if (not (result in expected)):\n            raise ex_class(args, result, expected, cmd_err)\n    elif (result != expected):\n        raise ex_class(args, result, expected, cmd_err)\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('LoginUser_args')\n    if (self.username is not None):\n        oprot.writeFieldBegin('username', TType.STRING, 1)\n        oprot.writeString(self.username)\n        oprot.writeFieldEnd()\n    if (self.Pass is not None):\n        oprot.writeFieldBegin('Pass', TType.STRING, 2)\n        oprot.writeString(self.Pass)\n        oprot.writeFieldEnd()\n    if (self.Remember is not None):\n        oprot.writeFieldBegin('Remember', TType.BOOL, 3)\n        oprot.writeBool(self.Remember)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('explain_result')\n    if (self.success is not None):\n        oprot.writeFieldBegin('success', TType.STRUCT, 0)\n        self.success.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.error is not None):\n        oprot.writeFieldBegin('error', TType.STRUCT, 1)\n        self.error.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef get_reasons(reasons):\n    'return string with description of reason task is not up-to-date'\n    lines = []\n    if reasons['has_no_dependencies']:\n        lines.append(' * The task has no dependencies.')\n    if reasons['uptodate_false']:\n        lines.append(' * The following uptodate objects evaluate to false:')\n        for (utd, utd_args, utd_kwargs) in reasons['uptodate_false']:\n            msg = '    - {} (args={}, kwargs={})'\n            lines.append(msg.format(utd, utd_args, utd_kwargs))\n    if reasons['checker_changed']:\n        msg = ' * The file_dep checker changed from {0} to {1}.'\n        lines.append(msg.format(*reasons['checker_changed']))\n    sentences = {\n        'missing_target': 'The following targets do not exist:',\n        'changed_file_dep': 'The following file dependencies have changed:',\n        'missing_file_dep': 'The following file dependencies are missing:',\n        'removed_file_dep': 'The following file dependencies were removed:',\n        'added_file_dep': 'The following file dependencies were added:',\n    }\n    for (reason, sentence) in sentences.items():\n        entries = reasons.get(reason)\n        if entries:\n            lines.append(' * {}'.format(sentence))\n            for item in entries:\n                lines.append('    - {}'.format(item))\n    return '\\n'.join(lines)\n", "label": 1}
{"function": "\n\ndef cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n    'Returns a CookieJar from a key/value dictionary.\\n\\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\\n    :param cookiejar: (optional) A cookiejar to add the cookies to.\\n    :param overwrite: (optional) If False, will not replace cookies\\n        already in the jar with new ones.\\n    '\n    if (cookiejar is None):\n        cookiejar = RequestsCookieJar()\n    if (cookie_dict is not None):\n        names_from_jar = [cookie.name for cookie in cookiejar]\n        for name in cookie_dict:\n            if (overwrite or (name not in names_from_jar)):\n                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n    return cookiejar\n", "label": 1}
{"function": "\n\ndef levenshtein_distance(first, second):\n    'Find the Levenshtein distance between two strings.'\n    if (len(first) > len(second)):\n        (first, second) = (second, first)\n    if (len(second) == 0):\n        return len(first)\n    first_length = (len(first) + 1)\n    second_length = (len(second) + 1)\n    distance_matrix = [([0] * second_length) for x in range(first_length)]\n    for i in range(first_length):\n        distance_matrix[i][0] = i\n    for j in range(second_length):\n        distance_matrix[0][j] = j\n    for i in xrange(1, first_length):\n        for j in range(1, second_length):\n            deletion = (distance_matrix[(i - 1)][j] + 1)\n            insertion = (distance_matrix[i][(j - 1)] + 1)\n            substitution = distance_matrix[(i - 1)][(j - 1)]\n            if (first[(i - 1)] != second[(j - 1)]):\n                substitution += 1\n            distance_matrix[i][j] = min(insertion, deletion, substitution)\n    return distance_matrix[(first_length - 1)][(second_length - 1)]\n", "label": 1}
{"function": "\n\ndef generic(self, args, kws):\n    if (len(args) == 1):\n        return\n    (left, right) = args\n    if isinstance(right, types.NPTimedelta):\n        dt = left\n        td = right\n    elif isinstance(left, types.NPTimedelta):\n        dt = right\n        td = left\n    else:\n        return\n    if isinstance(dt, types.NPDatetime):\n        unit = npdatetime.combine_datetime_timedelta_units(dt.unit, td.unit)\n        if (unit is not None):\n            return signature(types.NPDatetime(unit), left, right)\n", "label": 1}
{"function": "\n\ndef doctree_read(app, doctree):\n    secnums = app.builder.env.toc_secnumbers\n    for node in doctree.traverse(subfigstart):\n        parentloc = node.parent.children.index(node)\n        subfigendloc = parentloc\n        while (subfigendloc < len(node.parent.children)):\n            n = node.parent.children[subfigendloc]\n            if isinstance(n, subfigend):\n                break\n            subfigendloc += 1\n        if (subfigendloc == len(node.parent.children)):\n            return\n        between_nodes = node.parent.children[parentloc:subfigendloc]\n        subfigend_node = node.parent.children[subfigendloc]\n        node['subfigend'] = subfigend_node\n        for (i, n) in enumerate(between_nodes):\n            if isinstance(n, nodes.figure):\n                children = [n]\n                prevnode = between_nodes[(i - 1)]\n                if isinstance(prevnode, nodes.target):\n                    node.parent.children.remove(prevnode)\n                    children.insert(0, prevnode)\n                nodeloc = node.parent.children.index(n)\n                node.parent.children[nodeloc] = subfig('', *children)\n                node.parent.children[nodeloc]['width'] = subfigend_node['width']\n                node.parent.children[nodeloc]['mainfigid'] = subfigend_node['ids'][0]\n", "label": 1}
{"function": "\n\ndef check(self):\n    if (not self.graph.is_acyclic()):\n        err = 'Graph cannot be processed because it contains cycles in it:'\n        err += ', '.join(six.moves.map(str, nx.simple_cycles(nx.DiGraph(self.graph))))\n        err += '\\n'\n        raise errors.InvalidData(err)\n    non_existing_tasks = []\n    invalid_tasks = []\n    for (node_key, node_value) in six.iteritems(self.graph.node):\n        if (not node_value.get('id')):\n            successors = self.graph.successors(node_key)\n            predecessors = self.graph.predecessors(node_key)\n            neighbors = (successors + predecessors)\n            non_existing_tasks.append(node_key)\n            invalid_tasks.extend(neighbors)\n    if non_existing_tasks:\n        raise errors.InvalidData(\"Tasks '{non_existing_tasks}' can't be in requires|required_for|groups|tasks for [{invalid_tasks}] because they don't exist in the graph\".format(non_existing_tasks=', '.join((str(x) for x in sorted(non_existing_tasks))), invalid_tasks=', '.join((str(x) for x in sorted(set(invalid_tasks))))))\n", "label": 1}
{"function": "\n\ndef get_extensions_to_close(case, domain):\n    outgoing_extension_indices = [index.relationship for index in case.indices if (index.relationship == CASE_INDEX_EXTENSION)]\n    if ((not outgoing_extension_indices) and case.closed and EXTENSION_CASES_SYNC_ENABLED.enabled(domain)):\n        return get_extension_chain([case.case_id], domain)\n    else:\n        return set()\n", "label": 1}
{"function": "\n\ndef clone_scrapers(self, request, queryset):\n    for scraper in queryset:\n        scraper_elems = scraper.scraperelem_set.all()\n        rpts = scraper.requestpagetype_set.all()\n        checkers = scraper.checker_set.all()\n        scraper.pk = None\n        scraper.name = (scraper.name + ' (COPY)')\n        scraper.status = 'P'\n        scraper.save()\n        for se in scraper_elems:\n            se.pk = None\n            se.scraper = scraper\n            se.save()\n        for rpt in rpts:\n            rpt.pk = None\n            rpt.scraper = scraper\n            rpt.save()\n        for checker in checkers:\n            checker.pk = None\n            checker.scraper = scraper\n            checker.save()\n    rows_updated = queryset.count()\n    if (rows_updated == 1):\n        message_bit = '1 scraper was'\n    else:\n        message_bit = '{num} scrapers were'.format(num=rows_updated)\n    self.message_user(request, '{mb} successfully cloned.'.format(mb=message_bit))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef find_parent_scope(block):\n    '\\n        Find parent scope, if the block is not a fold trigger.\\n\\n        '\n    original = block\n    if (not TextBlockHelper.is_fold_trigger(block)):\n        while ((block.text().strip() == '') and block.isValid()):\n            block = block.next()\n        ref_lvl = (TextBlockHelper.get_fold_lvl(block) - 1)\n        block = original\n        while (block.blockNumber() and ((not TextBlockHelper.is_fold_trigger(block)) or (TextBlockHelper.get_fold_lvl(block) > ref_lvl))):\n            block = block.previous()\n    return block\n", "label": 1}
{"function": "\n\ndef __getattribute__(self, name):\n    \"\\n\\t\\tChecks the returned value from fields to see if it's known to be\\n\\t\\timmutable. If it isn't, adds it to :attr:`._mutable_fields` so we know\\n\\t\\tto push it back to the db. This allows us to cover the case wherein a\\n\\t\\tmutable value is accessed and then some part of that value is altered.\\n\\t\\t\\n\\t\\t\"\n    value = super(BaseChangeTracker, self).__getattribute__(name)\n    if ((not hasattr(value, '__call__')) and ('_mutable_fields' in super(BaseChangeTracker, self).__getattribute__('__dict__')) and (name in (field.attname for field in super(BaseChangeTracker, self).__getattribute__('_meta').concrete_fields))):\n        if is_mutable(value):\n            super(BaseChangeTracker, self).__getattribute__('_mutable_fields')[name] = deepcopy(value)\n    return value\n", "label": 1}
{"function": "\n\ndef enqueue_event(self, event):\n    'Enqueue the given event.\\n\\n        The event contains host data (ip, mac, port) which will be used to\\n        update the spoofing rule for the host in the iptables.\\n        '\n    LOG.debug('Enqueue iptable event %s.', event)\n    if (event.get('status') == 'up'):\n        for rule in self.rule_info:\n            if ((rule.mac == event.get('mac').lower()) and (rule.port == event.get('port'))):\n                if (rule.ip != event.get('ip')):\n                    LOG.debug(('enqueue_event: Only updating IP from %s to %s.' % (rule.ip, event.get('ip'))))\n                    rule.ip = event.get('ip')\n                    return\n    self._iptq.put(event)\n", "label": 1}
{"function": "\n\ndef _create_initial_state(self, ip, jumpkind):\n    '\\n        Obtain a SimState object for a specific address\\n\\n        Fastpath means the CFG generation will work in an IDA-like way, in which it will not try to execute every\\n        single statement in the emulator, but will just do the decoding job. This is much faster than the old way.\\n\\n        :param int ip: The instruction pointer\\n        :param str jumpkind: The jumpkind upon executing the block\\n        :return: The newly-generated state\\n        :rtype: simuvex.SimState\\n        '\n    jumpkind = ('Ijk_Boring' if (jumpkind is None) else jumpkind)\n    if (self._initial_state is None):\n        state = self.project.factory.entry_state(addr=ip, mode='fastpath')\n    else:\n        state = self._initial_state\n        state.scratch.jumpkind = jumpkind\n        state.set_mode('fastpath')\n        state.ip = state.se.BVV(ip, self.project.arch.bits)\n    if (jumpkind is not None):\n        state.scratch.jumpkind = jumpkind\n    state_info = None\n    if ((ip is not None) and (self.project.arch.name in ('MIPS32', 'MIPS64'))):\n        state_info = {\n            't9': state.se.BVV(ip, self.project.arch.bits),\n        }\n    elif ((ip is not None) and (self.project.arch.name == 'PPC64')):\n        state_info = {\n            'r2': state.registers.load('r2'),\n        }\n    state = self.project.arch.prepare_state(state, state_info)\n    return state\n", "label": 1}
{"function": "\n\n@luminosity.setter\ndef luminosity(self, value):\n    if (value is not None):\n        if is_numpy_array(value):\n            if (value.ndim != 1):\n                raise ValueError('luminosity should be a 1-D array')\n            if (not np.all((value > 0.0))):\n                raise ValueError('luminosity should be positive')\n            if ((self.position is not None) and (value.shape[0] != self.position.shape[0])):\n                raise ValueError('luminosity should be a 1-D array with the same number of rows as position')\n        else:\n            raise ValueError('luminosity should be a Numpy array')\n    self._luminosity = value\n", "label": 1}
{"function": "\n\n@action(route='/radioepg/servicefollowing/turn/<mode>/<id>')\n@json_only()\n@only_orga_admin_user()\ndef radioepg_servicefollowing_trun(request, mode, id):\n    'Turn on or off a servicefollowing entry.'\n    object = GenericServiceFollowingEntry.query.filter_by(id=int(id)).first()\n    station_id = request.args.get('station_id')\n    if (not station_id):\n        return None\n    if (object.type == 'ip'):\n        station_to_test = object.station\n    else:\n        station_to_test = object.channel.station\n    if (station_to_test.id == int(station_id)):\n        if (station_to_test.orga == int((request.args.get('ebuio_orgapk') or request.form.get('ebuio_orgapk')))):\n            object.active = (mode == 'on')\n            db.session.commit()\n    return PlugItRedirect(((('radioepg/servicefollowing/?turned=' + mode) + '&station_id=') + station_id))\n", "label": 1}
{"function": "\n\ndef add_spatial_info(features, add_x=True, add_y=True, inplace=False, dtype=None):\n    \"\\n    Adds spatial information to image features (which should contain a frames\\n    attribute in the format created by extract_image_features).\\n\\n    Adds a feature for x (if add_x) and y (if add_y), which are relative (x, y)\\n    locations within the image of the feature between 0 and 1 (inclusive).\\n\\n    Returns a new Features object with these additional features, or modifies\\n    features and returns None if inplace is True.\\n\\n    If dtype is not None, the resulting array will have that dtype. Otherwise,\\n    it will maintain features.dtype if it's a float type, or float32 if not.\\n    \"\n    if ((not add_x) and (not add_y)):\n        return (None if inplace else features)\n    indices = []\n    if add_x:\n        indices.append(0)\n    if add_y:\n        indices.append(1)\n    if (dtype is None):\n        dtype = features.dtype\n        if (dtype.kind != 'f'):\n            dtype = np.float32\n    spatial = np.asarray(np.vstack(features.frames)[:, indices], dtype=dtype)\n    spatial /= spatial.max(axis=0)\n    new_feats = np.hstack((features._features, spatial))\n    if inplace:\n        features._features = new_feats\n        features._refresh_features()\n    else:\n        return Features(new_feats, n_pts=features._n_pts, categories=features.categories, names=features.names, **dict(((k, features.data[k]) for k in features._extra_names)))\n", "label": 1}
{"function": "\n\n@factory.post_generation\ndef groups(self, create, extracted, **kwargs):\n    if (not create):\n        return\n    if (not extracted):\n        extracted = [GroupFactory() for i in range(2)]\n    if extracted:\n        for group in extracted:\n            self.groups.add(group)\n", "label": 1}
{"function": "\n\ndef output_dim(self, X, S, padding, strides, pooling=False):\n    '\\n        compute along 1 dimension, with these sizes, what will be the output dimension\\n\\n        Arguments:\\n            X (int): input data dimension\\n            S (int): filter dimension\\n            padding (int): padding on each side\\n            strides (int): striding\\n            pooling (bool): flag for setting pooling layer size\\n        '\n    if (self.check_caffe_compat() and pooling):\n        size = (int(ceil((float(((X - S) + (2 * padding))) / strides))) + 1)\n        if ((padding > 0) and (((size - 1) * strides) >= (X + padding))):\n            size -= 1\n    else:\n        size = ((((X - S) + (2 * padding)) / strides) + 1)\n    if (pooling and (padding >= S)):\n        raise ValueError(('Padding dim %d incompatible with filter size %d' % (padding, S)))\n    return size\n", "label": 1}
{"function": "\n\ndef lookup(self, *features):\n    if (len(self.builders) == 0):\n        return None\n    if (len(features) == 0):\n        return self.builders[0]\n    features = list(features)\n    features.reverse()\n    candidates = None\n    candidate_set = None\n    while (len(features) > 0):\n        feature = features.pop()\n        we_have_the_feature = self.builders_for_feature.get(feature, [])\n        if (len(we_have_the_feature) > 0):\n            if (candidates is None):\n                candidates = we_have_the_feature\n                candidate_set = set(candidates)\n            else:\n                candidate_set = candidate_set.intersection(set(we_have_the_feature))\n    if (candidate_set is None):\n        return None\n    for candidate in candidates:\n        if (candidate in candidate_set):\n            return candidate\n    return None\n", "label": 1}
{"function": "\n\ndef __combineVarStatements(node):\n    'Top level method called to optimize a script node'\n    if (len(node.scope.declared) == 0):\n        return\n    firstVar = __findFirstVarStatement(node)\n    if (not firstVar):\n        firstVar = Node.Node(None, 'var')\n        node.insert(0, firstVar)\n    __patchVarStatements(node, firstVar)\n    __cleanFirst(firstVar)\n    if (len(firstVar) == 0):\n        firstVar.parent.remove(firstVar)\n    else:\n        firstVarParent = firstVar.parent\n        firstVarPos = firstVarParent.index(firstVar)\n        if (len(firstVarParent) > (firstVarPos + 1)):\n            possibleForStatement = firstVarParent[(firstVarPos + 1)]\n            if ((possibleForStatement.type == 'for') and (not hasattr(possibleForStatement, 'setup'))):\n                possibleForStatement.append(firstVar, 'setup')\n", "label": 1}
{"function": "\n\ndef get_args(self, client):\n    \"\\n        Builds final set of XML-RPC method arguments based on\\n        the method's arguments, any default arguments, and their\\n        defined respective ordering.\\n        \"\n    default_args = self.default_args(client)\n    if (self.method_args or self.optional_args):\n        optional_args = getattr(self, 'optional_args', tuple())\n        args = []\n        for arg in (self.method_args + optional_args):\n            if hasattr(self, arg):\n                obj = getattr(self, arg)\n                if hasattr(obj, 'struct'):\n                    args.append(obj.struct)\n                else:\n                    args.append(obj)\n        args = (list(default_args) + args)\n    else:\n        args = default_args\n    return args\n", "label": 1}
{"function": "\n\n@login_required\n@transaction.atomic\ndef delete_posts(request, topic_id):\n    topic = Topic.objects.select_related().get(pk=topic_id)\n    if forum_moderated_by(topic, request.user):\n        deleted = False\n        post_list = request.POST.getlist('post')\n        for post_id in post_list:\n            if (not deleted):\n                deleted = True\n            delete_post(request, post_id)\n        if deleted:\n            messages.success(request, _('Post deleted.'))\n            return HttpResponseRedirect(topic.get_absolute_url())\n    last_post = topic.posts.latest()\n    if request.user.is_authenticated():\n        topic.update_read(request.user)\n    posts = topic.posts.all().select_related()\n    initial = {\n        \n    }\n    if request.user.is_authenticated():\n        initial = {\n            'markup': request.user.forum_profile.markup,\n        }\n    form = AddPostForm(topic=topic, initial=initial)\n    moderator = (request.user.is_superuser or (request.user in topic.forum.moderators.all()))\n    if (request.user.is_authenticated() and (request.user in topic.subscribers.all())):\n        subscribed = True\n    else:\n        subscribed = False\n    return render(request, 'djangobb_forum/delete_posts.html', {\n        'topic': topic,\n        'last_post': last_post,\n        'form': form,\n        'moderator': moderator,\n        'subscribed': subscribed,\n        'posts_page': get_page(posts, request, forum_settings.TOPIC_PAGE_SIZE),\n    })\n", "label": 1}
{"function": "\n\ndef write_csv(outfile, data):\n    'generate csv output'\n    output = []\n    headers = []\n    keys = ['uu', 'us', 'upl', 'ueid', 'peid', 'uipl', 'pid', 'pmrp', 'ptrp', 'upa', 'pda', 'ut']\n    for k in keys:\n        try:\n            headers.append(FIELDS[k]['human'])\n        except:\n            headers.append(k)\n    output.append(headers)\n    for record in data:\n        line = []\n        try:\n            for k in keys:\n                if (k in record):\n                    line.append(record[k])\n                else:\n                    line.append('')\n            output.append(line)\n        except:\n            output.append(['Error parsing line.'])\n    writer = csv.writer(outfile, delimiter=',', quotechar='\"', dialect='excel', quoting=csv.QUOTE_ALL)\n    writer.writerows(output)\n", "label": 1}
{"function": "\n\n@data.setter\ndef data(self, data):\n    numpy = import_module('numpy')\n    data = _TensorDataLazyEvaluator.parse_data(data)\n    if (data.ndim > 2):\n        raise ValueError('data have to be of rank 1 (diagonal metric) or 2.')\n    if (data.ndim == 1):\n        if (self.dim is not None):\n            nda_dim = data.shape[0]\n            if (nda_dim != self.dim):\n                raise ValueError('Dimension mismatch')\n        dim = data.shape[0]\n        newndarray = numpy.zeros((dim, dim), dtype=object)\n        for (i, val) in enumerate(data):\n            newndarray[(i, i)] = val\n        data = newndarray\n    (dim1, dim2) = data.shape\n    if (dim1 != dim2):\n        raise ValueError('Non-square matrix tensor.')\n    if (self.dim is not None):\n        if (self.dim != dim1):\n            raise ValueError('Dimension mismatch')\n    _tensor_data_substitution_dict[self] = data\n    _tensor_data_substitution_dict.add_metric_data(self.metric, data)\n    delta = self.get_kronecker_delta()\n    i1 = TensorIndex('i1', self)\n    i2 = TensorIndex('i2', self)\n    delta(i1, (- i2)).data = _TensorDataLazyEvaluator.parse_data(eye(dim1))\n", "label": 1}
{"function": "\n\ndef log_request(self, extra=''):\n    '\\n        Send request details to logging system.\\n        '\n    thread_name = threading.currentThread().getName().lower()\n    if (thread_name == 'mainthread'):\n        thread_name = ''\n    else:\n        thread_name = ('-%s' % thread_name)\n    if self.config['proxy']:\n        if self.config['proxy_userpwd']:\n            auth = ' with authorization'\n        else:\n            auth = ''\n        proxy_info = (' via %s proxy of type %s%s' % (self.config['proxy'], self.config['proxy_type'], auth))\n    else:\n        proxy_info = ''\n    if extra:\n        extra = ('[%s] ' % extra)\n    logger_network.debug('[%02d%s] %s%s %s%s', self.request_counter, thread_name, extra, (self.request_method or 'GET'), self.config['url'], proxy_info)\n", "label": 1}
{"function": "\n\ndef render(self, context):\n    generator_id = (self._generator_id.resolve(context) if self._generator_id else DEFAULT_THUMBNAIL_GENERATOR)\n    dimensions = parse_dimensions(self._dimensions.resolve(context))\n    kwargs = dict(((k, v.resolve(context)) for (k, v) in self._generator_kwargs.items()))\n    kwargs['source'] = self._source.resolve(context)\n    kwargs.update(dimensions)\n    generator = generator_registry.get(generator_id, **kwargs)\n    file = ImageCacheFile(generator)\n    attrs = dict(((k, v.resolve(context)) for (k, v) in self._html_attrs.items()))\n    if ((not ('width' in attrs)) and (not ('height' in attrs))):\n        attrs.update(width=file.width, height=file.height)\n    attrs['src'] = file.url\n    attr_str = ' '.join((('%s=\"%s\"' % (escape(k), escape(v))) for (k, v) in attrs.items()))\n    return mark_safe(('<img %s />' % attr_str))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef budget_monitoring_percentage(row):\n    '\\n            Virtual Field to show the percentage used of the Budget\\n        '\n    if hasattr(row, 'budget_monitoring'):\n        row = row.budget_monitoring\n    if hasattr(row, 'planned'):\n        planned = row.planned\n        if (planned == 0.0):\n            return current.messages['NONE']\n    else:\n        planned = None\n    if hasattr(row, 'value'):\n        actual = row.value\n    else:\n        actual = None\n    if ((planned is not None) and (actual is not None)):\n        percentage = ((actual / planned) * 100)\n        return ('%s %%' % '{0:.2f}'.format(percentage))\n    if hasattr(row, 'id'):\n        table = current.s3db.budget_monitoring\n        r = current.db((table.id == row.id)).select(table.planned, table.value, limitby=(0, 1)).first()\n        if r:\n            planned = r.planned\n            if (planned == 0.0):\n                return current.messages['NONE']\n            percentage = ((r.value / planned) * 100)\n            return ('%s %%' % percentage)\n    return current.messages['NONE']\n", "label": 1}
{"function": "\n\ndef __setupConnections(self, reuseUntil=None):\n    if (self.__graphComponent is None):\n        self.__connections = []\n        return\n    updatedConnections = []\n    n = 0\n    g = self.__graphComponent\n    reuse = (reuseUntil is not None)\n    while ((g is not None) and (n < self.__numComponents)):\n        if reuse:\n            updatedConnections.extend(self.__connections[(n * 2):((n * 2) + 2)])\n        else:\n            updatedConnections.append(g.nameChangedSignal().connect(Gaffer.WeakMethod(self.__setText)))\n            if (n < (self.__numComponents - 1)):\n                updatedConnections.append(g.parentChangedSignal().connect(Gaffer.WeakMethod(self.__parentChanged)))\n        if g.isSame(reuseUntil):\n            reuse = False\n        g = g.parent()\n        n += 1\n    self.__connections = updatedConnections\n", "label": 1}
{"function": "\n\ndef nome(ctx, m):\n    m = ctx.convert(m)\n    if (not m):\n        return m\n    if (m == ctx.one):\n        return m\n    if ctx.isnan(m):\n        return m\n    if ctx.isinf(m):\n        if (m == ctx.ninf):\n            return type(m)((- 1))\n        else:\n            return ctx.mpc((- 1))\n    a = ctx.ellipk((ctx.one - m))\n    b = ctx.ellipk(m)\n    v = ctx.exp((((- ctx.pi) * a) / b))\n    if ((not ctx._im(m)) and (ctx._re(m) < 1)):\n        if ctx._is_real_type(m):\n            return v.real\n        else:\n            return (v.real + 0j)\n    elif (m == 2):\n        v = ctx.mpc(0, v.imag)\n    return v\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _tensAdd_check_automatrix(args):\n    if (not args):\n        return args\n    auto_left_types = set([])\n    auto_right_types = set([])\n    args_auto_left_types = []\n    args_auto_right_types = []\n    for (i, arg) in enumerate(args):\n        arg_auto_left_types = set([])\n        arg_auto_right_types = set([])\n        for index in get_indices(arg):\n            if (index in (index._tensortype.auto_left, (- index._tensortype.auto_left))):\n                auto_left_types.add(index._tensortype)\n                arg_auto_left_types.add(index._tensortype)\n            if (index in (index._tensortype.auto_right, (- index._tensortype.auto_right))):\n                auto_right_types.add(index._tensortype)\n                arg_auto_right_types.add(index._tensortype)\n        args_auto_left_types.append(arg_auto_left_types)\n        args_auto_right_types.append(arg_auto_right_types)\n    for (arg, aas_left, aas_right) in zip(args, args_auto_left_types, args_auto_right_types):\n        missing_left = (auto_left_types - aas_left)\n        missing_right = (auto_right_types - aas_right)\n        missing_intersection = (missing_left & missing_right)\n        for j in missing_intersection:\n            args[i] *= j.delta(j.auto_left, (- j.auto_right))\n        if (missing_left != missing_right):\n            raise ValueError('cannot determine how to add auto-matrix indices on some args')\n    return args\n", "label": 1}
{"function": "\n\ndef getValueAt(self, rowIndex, columnIndex):\n    messageEntry = self._db.getMessageByRow(rowIndex)\n    if messageEntry:\n        if (columnIndex == 0):\n            return str(messageEntry.getTableRow())\n        elif (columnIndex == 1):\n            return messageEntry._name\n        elif (columnIndex == 2):\n            return messageEntry._successRegex\n        else:\n            roleEntry = self._db.getRoleByMessageTableColumn(columnIndex)\n            if roleEntry:\n                roleIndex = roleEntry._index\n                return ((roleIndex in messageEntry._roles) and messageEntry._roles[roleIndex])\n    return ''\n", "label": 1}
{"function": "\n\ndef __init__(self, *args):\n    if (len(args) == 0):\n        self._empty = True\n        self._head = None\n        self._tail = None\n    elif ((len(args) == 1) and isinstance(args[0], (list, tuple))):\n        if (len(args[0]) == 0):\n            self._empty = True\n            self._head = None\n            self._tail = None\n        else:\n            self._head = args[0][0]\n            if (len(args[0]) > 1):\n                self._tail = ImmutableList(args[0][1:])\n            else:\n                self._tail = ImmutableList()\n            self._empty = False\n    elif ((len(args) == 2) and isinstance(args[1], ImmutableList)):\n        self._head = args[0]\n        self._tail = args[1]\n        self._empty = False\n", "label": 1}
{"function": "\n\ndef rectangularize_featureset(featureset):\n    'Convert xarray.Dataset into (2d) Pandas.DataFrame for use with sklearn.'\n    featureset = featureset.drop([coord for coord in ['target', 'class'] if (coord in featureset)])\n    feature_df = featureset.to_dataframe()\n    if ('channel' in featureset):\n        feature_df = feature_df.unstack(level='channel')\n        if (len(featureset.channel) == 1):\n            feature_df.columns = [pair[0] for pair in feature_df.columns]\n        else:\n            feature_df.columns = ['_'.join([str(el) for el in pair]) for pair in feature_df.columns]\n    return feature_df.loc[featureset.name]\n", "label": 1}
{"function": "\n\ndef recv_frame(self):\n    '\\n        recieve data as frame from server.\\n\\n        return value: ABNF frame object.\\n        '\n    header_bytes = self._recv_strict(2)\n    if (not header_bytes):\n        return None\n    b1 = header_bytes[0]\n    fin = ((b1 >> 7) & 1)\n    rsv1 = ((b1 >> 6) & 1)\n    rsv2 = ((b1 >> 5) & 1)\n    rsv3 = ((b1 >> 4) & 1)\n    opcode = (b1 & 15)\n    b2 = header_bytes[1]\n    mask = ((b2 >> 7) & 1)\n    length = (b2 & 127)\n    length_data = b''\n    if (length == 126):\n        length_data = self._recv_strict(2)\n        length = struct.unpack('!H', length_data)[0]\n    elif (length == 127):\n        length_data = self._recv_strict(8)\n        length = struct.unpack('!Q', length_data)[0]\n    mask_key = b''\n    if mask:\n        mask_key = self._recv_strict(4)\n    data = self._recv_strict(length)\n    if traceEnabled:\n        recieved = (((header_bytes + length_data) + mask_key) + data)\n        logger.debug(('recv: ' + repr(recieved)))\n    if mask:\n        data = ABNF.mask(mask_key, data)\n    frame = ABNF(fin, rsv1, rsv2, rsv3, opcode, mask, data)\n    return frame\n", "label": 1}
{"function": "\n\ndef _refresh_branding(self):\n    ' Refresh the branding object for the window model.\\n\\n        '\n    workbench = self.workbench\n    point = workbench.get_extension_point(BRANDING_POINT)\n    extensions = point.extensions\n    if (not extensions):\n        self._branding_extension = None\n        self._model.branding = Branding()\n        return\n    extension = extensions[(- 1)]\n    if (extension is self._branding_extension):\n        return\n    if extension.factory:\n        branding = extension.factory(workbench)\n        if (not isinstance(branding, Branding)):\n            msg = \"extension '%s' created non-Branding type '%s'\"\n            args = (extension.qualified_id, type(branding).__name__)\n            raise TypeError((msg % args))\n    else:\n        branding = extension.get_child(Branding, reverse=True)\n        branding = (branding or Branding())\n    self._branding_extension = extension\n    self._model.branding = branding\n", "label": 1}
{"function": "\n\ndef check(self, instance):\n    btrfs_devices = {\n        \n    }\n    excluded_devices = instance.get('excluded_devices', [])\n    for p in psutil.disk_partitions():\n        if ((p.fstype == 'btrfs') and (p.device not in btrfs_devices) and (p.device not in excluded_devices)):\n            btrfs_devices[p.device] = p.mountpoint\n    if (len(btrfs_devices) == 0):\n        raise Exception('No btrfs device found')\n    for (device, mountpoint) in btrfs_devices.iteritems():\n        for (flags, total_bytes, used_bytes) in self.get_usage(mountpoint):\n            (replication_type, usage_type) = FLAGS_MAPPER[flags]\n            tags = ['usage_type:{0}'.format(usage_type), 'replication_type:{0}'.format(replication_type)]\n            free = (total_bytes - used_bytes)\n            usage = (float(free) / float(total_bytes))\n            self.gauge('system.disk.btrfs.total', total_bytes, tags=tags, device_name=device)\n            self.gauge('system.disk.btrfs.used', used_bytes, tags=tags, device_name=device)\n            self.gauge('system.disk.btrfs.free', free, tags=tags, device_name=device)\n            self.gauge('system.disk.btrfs.usage', usage, tags=tags, device_name=device)\n", "label": 1}
{"function": "\n\ndef __init__(self, port=None, layout=BOARDS['arduino'], baudrate=57600, name=None, timeout=None):\n    \"\\n        default_leds = [\\n            'arduino',\\n            'arduino_leonardo',\\n            'arduino_mega',\\n            'arduino_due',\\n            'arduino_yun',\\n            'arduino_zero',\\n            'arduino_micro',\\n            'arduino_lilypad_usb',\\n        ]\\n        \"\n    self.DEFAULT_LEDS = ['Uno', 'LilypadUSB']\n\n    def led_hook(pin_number):\n        led = Led(self, pin_number)\n        self.led = led\n        return led\n\n    def pin_hook(pin_number):\n        pin = Pin(self, pin_number)\n        self.pin = pin\n        return pin\n    self.name = name\n    self.util = ArduinoUtil()\n    self.pin = (lambda pin_number: pin_hook(pin_number))\n    self.led = (Led(self) if (self.__class__.__name__ in self.DEFAULT_LEDS) else (lambda pin_number: led_hook(pin_number)))\n    auto_port = PortUtil.scan()\n    self.sp = serial.Serial(auto_port, baudrate, timeout=timeout)\n    self.pass_time(BOARD_SETUP_WAIT_TIME)\n    self._layout = layout\n    self._iterator = False\n    self.it = util.Iterator(self)\n    if (not self.name):\n        self.name = port\n    else:\n        pass\n    if layout:\n        self.setup_layout(layout)\n    else:\n        self.auto_setup()\n    if self._iterator:\n        self.it.start()\n    while self.bytes_available():\n        self.iterate()\n", "label": 1}
{"function": "\n\ndef gpg_exception_factory(returncode, message):\n    if (returncode == 2):\n        if (b'decryption failed: bad key' in message):\n            return IncorrectPasswordException(message)\n        if (b'CRC error;' in message):\n            return FileCorruptionException(message)\n        if (b'fatal: zlib inflate problem: invalid distance' in message):\n            return FileCorruptionException(message)\n        if (b'decryption failed: invalid packet' in message):\n            return FileCorruptionException(message)\n        if b'no valid OpenPGP data found':\n            return InvalidEncryptedFileException(message)\n    return Exception('unkown error', returncode, message)\n", "label": 1}
{"function": "\n\ndef setPosition(self, value):\n    if ((value is None) or (value == '')):\n        return\n    posisize = value.split(' ')\n    textpos = posisize[0]\n    if (textpos.find('%') == (- 1)):\n        if (textpos == 'sub'):\n            self.superscript = False\n            self.subscript = True\n        elif (textpos == 'super'):\n            self.superscript = True\n            self.subscript = False\n    else:\n        itextpos = int(textpos[:textpos.find('%')])\n        if (itextpos > 10):\n            self.superscript = False\n            self.subscript = True\n        elif (itextpos < (- 10)):\n            self.superscript = True\n            self.subscript = False\n", "label": 1}
{"function": "\n\ndef _add_value_to_fields_in_project(self, project_id):\n    for field in self._get_fields_with_values(project_id):\n        field_name = self._get_field_name(field[NAME], project_id)\n        pcf = self._target.getProjectCustomField(project_id, field_name)\n        if hasattr(pcf, 'bundle'):\n            field_type = pcf.type[0:(- 3)]\n            bundle = self._target.getBundle(field_type, pcf.bundle)\n            yt_values = [v for v in [field['converter'](value, bundle, (lambda name, value_name: self._import_config.get_field_value(name, field_type, value_name))) for value in field['values']] if len(v)]\n            for value in yt_values:\n                self._target.addValueToBundle(bundle, value)\n", "label": 1}
{"function": "\n\ndef extract_test_sentences(string, comment_chars='#%;', encoding=None):\n    '\\n    Parses a string with one test sentence per line.\\n    Lines can optionally begin with:\\n      - a bool, saying if the sentence is grammatical or not, or\\n      - an int, giving the number of parse trees is should have,\\n    The result information is followed by a colon, and then the sentence.\\n    Empty lines and lines beginning with a comment char are ignored.\\n\\n    :return: a list of tuple of sentences and expected results,\\n        where a sentence is a list of str,\\n        and a result is None, or bool, or int\\n\\n    :param comment_chars: ``str`` of possible comment characters.\\n    :param encoding: the encoding of the string, if it is binary\\n    '\n    if (encoding is not None):\n        string = string.decode(encoding)\n    sentences = []\n    for sentence in string.split('\\n'):\n        if ((sentence == '') or (sentence[0] in comment_chars)):\n            continue\n        split_info = sentence.split(':', 1)\n        result = None\n        if (len(split_info) == 2):\n            if (split_info[0] in ['True', 'true', 'False', 'false']):\n                result = (split_info[0] in ['True', 'true'])\n                sentence = split_info[1]\n            else:\n                result = int(split_info[0])\n                sentence = split_info[1]\n        tokens = sentence.split()\n        if (tokens == []):\n            continue\n        sentences += [(tokens, result)]\n    return sentences\n", "label": 1}
{"function": "\n\ndef keyPressEvent(self, ev):\n    '\\n        Re-implemented to handle the user input a key at a time.\\n        \\n        @param ev key event (QKeyEvent)\\n        '\n    txt = ev.text()\n    key = ev.key()\n    ctrl = (ev.modifiers() & Qt.ControlModifier)\n    shift = (ev.modifiers() & Qt.ShiftModifier)\n    if (self.keymap.has_key(key) and (not shift) and (not ctrl)):\n        self.keymap[key]()\n    elif (ev == QtGui.QKeySequence.Paste):\n        self.paste()\n    elif (self.__isCursorOnLastLine() and txt.length()):\n        QsciScintilla.keyPressEvent(self, ev)\n        self.incrementalSearchActive = True\n        if (txt == '.'):\n            self.__showDynCompletion()\n    elif (ctrl or shift):\n        QsciScintilla.keyPressEvent(self, ev)\n    else:\n        ev.ignore()\n", "label": 1}
{"function": "\n\ndef getAssociation(self, server_url, handle=None):\n    assocs = []\n    if (handle is not None):\n        assocs = Association.objects.filter(server_url=server_url, handle=handle)\n    else:\n        assocs = Association.objects.filter(server_url=server_url)\n    if (not assocs):\n        return None\n    associations = []\n    for assoc in assocs:\n        association = OIDAssociation(assoc.handle, base64.decodestring(assoc.secret), assoc.issued, assoc.lifetime, assoc.assoc_type)\n        if (association.getExpiresIn() == 0):\n            self.removeAssociation(server_url, assoc.handle)\n        else:\n            associations.append((association.issued, association))\n    if (not associations):\n        return None\n    return associations[(- 1)][1]\n", "label": 1}
{"function": "\n\ndef validate_top_bar_items(self):\n    'validate url in top bar items'\n    for top_bar_item in self.get('top_bar_items'):\n        if top_bar_item.parent_label:\n            parent_label_item = self.get('top_bar_items', {\n                'label': top_bar_item.parent_label,\n            })\n            if (not parent_label_item):\n                frappe.throw(_('{0} does not exist in row {1}').format(top_bar_item.parent_label, top_bar_item.idx))\n            elif ((not parent_label_item[0]) or parent_label_item[0].url):\n                frappe.throw(_('{0} in row {1} cannot have both URL and child items').format(top_bar_item.parent_label, top_bar_item.idx))\n", "label": 1}
{"function": "\n\ndef network(ip, netmask, gateway):\n    '\\n    Ensure the DRAC network settings are consistent\\n    '\n    ret = {\n        'name': ip,\n        'result': True,\n        'changes': {\n            \n        },\n        'comment': '',\n    }\n    current_network = __salt__['drac.network_info']()\n    new_network = {\n        \n    }\n    if (ip != current_network['IPv4 settings']['IP Address']):\n        ret['changes'].update({\n            'IP Address': {\n                'Old': current_network['IPv4 settings']['IP Address'],\n                'New': ip,\n            },\n        })\n    if (netmask != current_network['IPv4 settings']['Subnet Mask']):\n        ret['changes'].update({\n            'Netmask': {\n                'Old': current_network['IPv4 settings']['Subnet Mask'],\n                'New': netmask,\n            },\n        })\n    if (gateway != current_network['IPv4 settings']['Gateway']):\n        ret['changes'].update({\n            'Gateway': {\n                'Old': current_network['IPv4 settings']['Gateway'],\n                'New': gateway,\n            },\n        })\n    if __opts__['test']:\n        ret['result'] = None\n        return ret\n    if __salt__['drac.set_network'](ip, netmask, gateway):\n        if (not ret['changes']):\n            ret['comment'] = 'Network is in the desired state'\n        return ret\n    ret['result'] = False\n    ret['comment'] = 'unable to configure network'\n    return ret\n", "label": 1}
{"function": "\n\ndef _redirect(self, request, secure):\n    protocol = ((secure and 'https') or 'http')\n    if secure:\n        host = getattr(settings, 'SSL_HOST', request.get_host())\n    else:\n        host = getattr(settings, 'HTTP_HOST', request.get_host())\n    newurl = ('%s://%s%s' % (protocol, host, request.get_full_path()))\n    if (settings.DEBUG and (request.method == 'POST')):\n        raise Exception(\"Django can't perform a SSL redirect while maintaining POST data. Please structure your views so that redirects only occur during GETs.\")\n    return HttpResponseRedirect(newurl)\n", "label": 1}
{"function": "\n\ndef contains(self, val):\n    '\\n        Check if given value or range is present.\\n\\n        Parameters\\n        ----------\\n        val : int or tuple or list or range\\n            Range or integer being checked.\\n\\n        Returns\\n        -------\\n        retlen : int\\n            Length of overlapping with `val` subranges.\\n        '\n    (start, end) = self.__val_convert(val)\n    retlen = 0\n    for r in self.__has:\n        if ((start < r[1]) and (end > r[0])):\n            retlen += ((((end < r[1]) and end) or r[1]) - (((start > r[0]) and start) or r[0]))\n    return retlen\n", "label": 1}
{"function": "\n\ndef find_in_app(self, app, path):\n    matched_path = super(AppDirectoriesFinder, self).find_in_app(app, path)\n    if (matched_path and self.serve_unminimized):\n        (base, ext) = os.path.splitext(matched_path)\n        (base, minext) = os.path.splitext(base)\n        if (minext == '.min'):\n            storage = self.storages.get(app, None)\n            path = '{}{}'.format(base, ext)\n            if storage.exists(path):\n                path = storage.path(path)\n                if path:\n                    return path\n    return matched_path\n", "label": 1}
{"function": "\n\ndef get_db_prep_value(self, value, connection=None, prepared=False):\n    if self.null:\n        value = (value or None)\n    if (value is None):\n        return None\n    value = force_bytes(value)\n    if (not self._is_encrypted(value)):\n        padding = self._get_padding(value)\n        if (padding > 0):\n            value = ((value + b'\\x00') + (b'*' * (padding - 1)))\n        value = (self.prefix + binascii.b2a_hex(self.cipher.encrypt(value)))\n    return force_text(value)\n", "label": 1}
{"function": "\n\ndef Decider(self, function):\n    copy_function = self._copy2_from_cache\n    if (function in ('MD5', 'content')):\n        if (not SCons.Util.md5):\n            raise UserError('MD5 signatures are not available in this version of Python.')\n        function = self._changed_content\n    elif (function == 'MD5-timestamp'):\n        function = self._changed_timestamp_then_content\n    elif (function in ('timestamp-newer', 'make')):\n        function = self._changed_timestamp_newer\n        copy_function = self._copy_from_cache\n    elif (function == 'timestamp-match'):\n        function = self._changed_timestamp_match\n    elif (not callable(function)):\n        raise UserError(('Unknown Decider value %s' % repr(function)))\n    self.decide_target = function\n    self.decide_source = function\n    self.copy_from_cache = copy_function\n", "label": 1}
{"function": "\n\ndef _learnStep(self):\n    if self.evaluatorIsNoisy:\n        self.bestEvaluation = self._oneEvaluation(self.bestEvaluable)\n    challenger = self.bestEvaluable.copy()\n    challenger.mutate()\n    newEval = self._oneEvaluation(challenger)\n    if (((not self.minimize) and (newEval < self.bestEvaluation)) or (self.minimize and (newEval > self.bestEvaluation))):\n        acceptProbability = exp(((- abs((newEval - self.bestEvaluation))) / self.temperature))\n        if (random() < acceptProbability):\n            (self.bestEvaluable, self.bestEvaluation) = (challenger, newEval)\n", "label": 1}
{"function": "\n\ndef _update(self, frame_no):\n    self._frame_no = frame_no\n    if ((self._clear and (frame_no == (self._stop_frame - 1))) or (self._delete_count == 1)):\n        for i in range(0, self._renderer.max_height):\n            self._screen.print_at((' ' * self._renderer.max_width), self._x, (self._y + i), bg=self._bg)\n    elif ((frame_no % self._speed) == 0):\n        (image, colours) = self._renderer.rendered_text\n        for (i, line) in enumerate(image):\n            self._screen.paint(line, self._x, (self._y + i), self._colour, attr=self._attr, bg=self._bg, transparent=self._transparent, colour_map=colours[i])\n", "label": 1}
{"function": "\n\ndef filter(self, data):\n    resp = util.http.parse_response(data)\n    content_type = (resp.getheader('content-type', '').strip() if resp else '')\n    return (resp and (((resp.status == 200) and any((re.match(type, content_type) for type in SSLStrip.content_types))) or (((resp.status / 100) == 3) and resp.getheader('location', '').startswith('https://'))))\n", "label": 1}
{"function": "\n\ndef match(self, other):\n    'Return true if this MediaType satisfies the given MediaType.'\n    for key in self.params.keys():\n        if ((key != 'q') and (other.params.get(key, None) != self.params.get(key, None))):\n            return False\n    if ((self.sub_type != '*') and (other.sub_type != '*') and (other.sub_type != self.sub_type)):\n        return False\n    if ((self.main_type != '*') and (other.main_type != '*') and (other.main_type != self.main_type)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\n@classmethod\ndef on_change(cls, seconds, now):\n    'Change the theme and get the next time point to change themes.'\n    cls.busy = True\n    if ((cls.next_change is not None) and ((cls.next_change.theme != cls.current_theme) or (cls.next_change.msg != cls.current_msg) or (cls.next_change.filters != cls.current_filters) or (cls.next_change.ui_theme != cls.current_ui_theme) or (cls.next_change.command is not None))):\n        debug_log('Change needed!')\n        update = True\n    else:\n        debug_log('Change not needed!')\n        update = False\n    cls.update_theme(seconds, now, update)\n    cls.busy = False\n", "label": 1}
{"function": "\n\ndef get_prep_lookup(self, lookup_type, value, prepared=False):\n    ' Cleanup value for the jsonb lookup types\\n\\n        contains requires json encoded string\\n        has_any and has_all require array of string_types\\n        has requires string, but we can easily convert int to string\\n\\n        '\n    if (lookup_type in ['jcontains']):\n        if (not isinstance(value, six.string_types)):\n            value = json.dumps(value, cls=get_encoder_class(), **self._options)\n    if (lookup_type in ['jhas_any', 'jhas_all']):\n        if isinstance(value, six.string_types):\n            value = [value]\n        value = [('%s' % v) for v in value]\n    elif ((lookup_type in ['jhas']) and (not isinstance(value, six.string_types))):\n        if isinstance(value, six.integer_types):\n            value = str(value)\n        else:\n            raise TypeError('jhas lookup requires str or int')\n    return value\n", "label": 1}
{"function": "\n\ndef client_update_all_switches(self):\n    ' Updates all the switch states on the OSC client.'\n    if (self.client_mode == 'name'):\n        for switch in self.machine.switches:\n            if self.machine.switch_controller.is_active(switch.name):\n                data = 1\n            else:\n                data = 0\n            self.client_send_osc_message('sw', switch.name, data)\n    elif (self.client_mode == 'wpc'):\n        for switch in self.machine.switches:\n            if self.machine.switch_controller.is_active(switch.name):\n                data = 1\n            else:\n                data = 0\n            self.client_send_osc_message('sw', switch.config['number_str'].lower(), data)\n", "label": 1}
{"function": "\n\n@classmethod\ndef cli_resp_formatter(cls, resp):\n    'Override this method to provide custom formatting of cli response.\\n        '\n    if (not resp.value):\n        return ''\n    if (resp.status == STATUS_OK):\n        if (type(resp.value) in (str, bool, int, float, six.text_type)):\n            return str(resp.value)\n        ret = ''\n        val = resp.value\n        if (not isinstance(val, list)):\n            val = [val]\n        for line in val:\n            for (k, v) in line.items():\n                if isinstance(v, dict):\n                    ret += cls.cli_resp_line_template.format(k, ('\\n' + pprint.pformat(v)))\n                else:\n                    ret += cls.cli_resp_line_template.format(k, v)\n        return ret\n    else:\n        return 'Error: {0}'.format(resp.value)\n", "label": 1}
{"function": "\n\ndef _read_structure(f, array_desc, struct_desc):\n    '\\n    Read a structure, with the array and structure descriptors given as\\n    `array_desc` and `structure_desc` respectively.\\n    '\n    nrows = array_desc['nelements']\n    columns = struct_desc['tagtable']\n    dtype = []\n    for col in columns:\n        if (col['structure'] or col['array']):\n            dtype.append(((col['name'].lower(), col['name']), np.object_))\n        elif (col['typecode'] in DTYPE_DICT):\n            dtype.append(((col['name'].lower(), col['name']), DTYPE_DICT[col['typecode']]))\n        else:\n            raise Exception(('Variable type %i not implemented' % col['typecode']))\n    structure = np.recarray((nrows,), dtype=dtype)\n    for i in range(nrows):\n        for col in columns:\n            dtype = col['typecode']\n            if col['structure']:\n                structure[col['name']][i] = _read_structure(f, struct_desc['arrtable'][col['name']], struct_desc['structtable'][col['name']])\n            elif col['array']:\n                structure[col['name']][i] = _read_array(f, dtype, struct_desc['arrtable'][col['name']])\n            else:\n                structure[col['name']][i] = _read_data(f, dtype)\n    if (array_desc['ndims'] > 1):\n        dims = array_desc['dims'][:int(array_desc['ndims'])]\n        dims.reverse()\n        structure = structure.reshape(dims)\n    return structure\n", "label": 1}
{"function": "\n\ndef create_order_model(self, user, basket, shipping_address, shipping_method, shipping_charge, billing_address, total, order_number, status, **extra_order_fields):\n    '\\n        Create an order model.\\n        '\n    order_data = {\n        'basket': basket,\n        'number': order_number,\n        'site': Site._default_manager.get_current(),\n        'currency': total.currency,\n        'total_incl_tax': total.incl_tax,\n        'total_excl_tax': total.excl_tax,\n        'shipping_incl_tax': shipping_charge.incl_tax,\n        'shipping_excl_tax': shipping_charge.excl_tax,\n        'shipping_method': shipping_method.name,\n        'shipping_code': shipping_method.code,\n    }\n    if shipping_address:\n        order_data['shipping_address'] = shipping_address\n    if billing_address:\n        order_data['billing_address'] = billing_address\n    if (user and user.is_authenticated()):\n        order_data['user_id'] = user.id\n    if status:\n        order_data['status'] = status\n    if extra_order_fields:\n        order_data.update(extra_order_fields)\n    order = Order(**order_data)\n    order.save()\n    return order\n", "label": 1}
{"function": "\n\n@property\ndef template_width(self):\n    '\\n        Set column width to accommodate widest image.\\n        '\n    width = 0\n    if self.app.enable_case_list_icon_dynamic_width:\n        for (i, item) in enumerate(self.column.enum):\n            for path in item.value.values():\n                map_item = self.app.multimedia_map[path]\n                if (map_item is not None):\n                    image = CommCareMultimedia.get(map_item.multimedia_id)\n                    if (image is not None):\n                        for media in image.aux_media:\n                            width = max(width, media.media_meta['size']['width'])\n    if (width == 0):\n        return '13%'\n    return str(width)\n", "label": 1}
{"function": "\n\ndef _apply_path_joins(self, query, joins, path, inner_join=True):\n    '\\n            Apply join path to the query.\\n\\n            :param query:\\n                Query to add joins to\\n            :param joins:\\n                List of current joins. Used to avoid joining on same relationship more than once\\n            :param path:\\n                Path to be joined\\n            :param fn:\\n                Join function\\n        '\n    last = None\n    if path:\n        for item in path:\n            key = (inner_join, item)\n            alias = joins.get(key)\n            if (key not in joins):\n                if (not isinstance(item, Table)):\n                    alias = aliased(item.property.mapper.class_)\n                fn = (query.join if inner_join else query.outerjoin)\n                if (last is None):\n                    query = (fn(item) if (alias is None) else fn(alias, item))\n                else:\n                    prop = getattr(last, item.key)\n                    query = (fn(prop) if (alias is None) else fn(alias, prop))\n                joins[key] = alias\n            last = alias\n    return (query, joins, last)\n", "label": 1}
{"function": "\n\ndef response(self):\n    '\\n        '\n    vitals = ['username']\n    if (not self.has_values(['key_hash'])):\n        vitals.append('password')\n    self.check_values(vitals)\n    resp = {\n        \n    }\n    if ('auth-int' in self._qops):\n        self._qop = b'auth-int'\n    resp['qop'] = self._qop\n    if ('realm' in self.values):\n        resp['realm'] = quote(self.values['realm'])\n    resp['username'] = quote(bytes(self.values['username']))\n    resp['nonce'] = quote(self.values['nonce'])\n    if self.values['nc']:\n        self._cnonce = self.values['cnonce']\n    else:\n        self._cnonce = bytes(('%s' % random.random()))[2:]\n    resp['cnonce'] = quote(self._cnonce)\n    self.values['nc'] += 1\n    resp['nc'] = bytes(('%08x' % self.values['nc']))\n    service = bytes(self.sasl.service)\n    host = bytes(self.sasl.host)\n    self._digest_uri = ((service + b'/') + host)\n    resp['digest-uri'] = quote(self._digest_uri)\n    a2 = (b'AUTHENTICATE:' + self._digest_uri)\n    if (self._qop != b'auth'):\n        a2 += b':00000000000000000000000000000000'\n        resp['maxbuf'] = b'16777215'\n    resp['response'] = self.gen_hash(a2)\n    return b','.join([((bytes(k) + b'=') + bytes(v)) for (k, v) in resp.items()])\n", "label": 1}
{"function": "\n\ndef _check_send(self):\n    if (self._active < self._max):\n        return\n    while (self._active >= self._max):\n        first_q = min(self._queues.itervalues(), key=self._key_getter)\n        key = first_q[2]\n        second_q = min((q for (k, q) in self._queues.iteritems() if (k != key)), key=self._key_getter)\n        switch_grpos = second_q[0]\n        frame_queue = first_q[1]\n        while frame_queue:\n            if (frame_queue[0][0] > switch_grpos):\n                break\n            (grpos, frame) = frame_queue.popleft()\n            self._callback(grpos, key, frame)\n        if (not frame_queue):\n            self._active -= 1\n        else:\n            first_q[0] = frame_queue[0][0]\n", "label": 1}
{"function": "\n\ndef is_nash(self, action_profile):\n    '\\n        Return True if `action_profile` is a Nash equilibrium.\\n\\n        Parameters\\n        ----------\\n        action_profile : array_like(int or array_like(float))\\n            An array of N objects, where each object must be an integer\\n            (pure action) or an array of floats (mixed action).\\n\\n        Returns\\n        -------\\n        bool\\n            True if `action_profile` is a Nash equilibrium; False\\n            otherwise.\\n\\n        '\n    if (self.N == 2):\n        for (i, player) in enumerate(self.players):\n            (own_action, opponent_action) = (action_profile[i], action_profile[(1 - i)])\n            if (not player.is_best_response(own_action, opponent_action)):\n                return False\n    elif (self.N >= 3):\n        for (i, player) in enumerate(self.players):\n            own_action = action_profile[i]\n            opponents_actions = (tuple(action_profile[(i + 1):]) + tuple(action_profile[:i]))\n            if (not player.is_best_response(own_action, opponents_actions)):\n                return False\n    elif (not self.players[0].is_best_response(action_profile[0], None)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef diagonalize(S, n, m):\n    for k in range(min(n, m)):\n        (val, i, j) = max(((abs(S[i][j]), i, j) for i in range(k, m) for j in range(k, n)))\n        if is_zero(val):\n            return k\n        (S[i], S[k]) = (S[k], S[i])\n        for r in range((m + 1)):\n            (S[r][j], S[r][k]) = (S[r][k], S[r][j])\n        pivot = float(S[k][k])\n        for j in range(k, (n + 1)):\n            S[k][j] /= pivot\n        for i in range(m):\n            if (i != k):\n                fact = S[i][k]\n                for j in range(k, (n + 1)):\n                    S[i][j] -= (fact * S[k][j])\n    return min(n, m)\n", "label": 1}
{"function": "\n\ndef rule_params_SA(parser, node, children):\n    params = {\n        \n    }\n    for (name, value) in children[0].items():\n        if (name not in ['skipws', 'ws']):\n            raise TextXSyntaxError('Invalid rule param \"{}\" at {}.'.format(name, parser.pos_to_linecol(node.position)))\n        if ((name == 'ws') and ('\\\\' in value)):\n            new_value = ''\n            if ('\\\\n' in value):\n                new_value += '\\n'\n            if ('\\\\r' in value):\n                new_value += '\\r'\n            if ('\\\\t' in value):\n                new_value += '\\t'\n            if (' ' in value):\n                new_value += ' '\n            value = new_value\n        params[name] = value\n    return params\n", "label": 1}
{"function": "\n\ndef sudoku(G):\n    'Solving Sudoku\\n\\n    :param G: integer matrix with 0 at empty cells\\n    :returns bool: True if grid could be solved\\n    :modifies: G will contain the solution\\n    :complexity: huge, but linear for usual published 9x9 grids\\n    '\n    global N, N2, N4\n    if (len(G) == 16):\n        (N, N2, N4) = (4, 16, 256)\n    e = (4 * N4)\n    univers = (e + 1)\n    S = [[rc(a), rv(a), cv(a), bv(a)] for a in range((N4 * N2))]\n    A = [e]\n    for r in range(N2):\n        for c in range(N2):\n            if (G[r][c] != 0):\n                a = assignation(r, c, (G[r][c] - 1))\n                A += S[a]\n    sol = dancing_links(univers, (S + [A]))\n    if sol:\n        for a in sol:\n            if (a < len(S)):\n                G[row(a)][col(a)] = (val(a) + 1)\n        return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef make_relative_path(source, dest, dest_is_directory=True):\n    \"\\n    Make a filename relative, where the filename is dest, and it is\\n    being referred to from the filename source.\\n\\n        >>> make_relative_path('/usr/share/something/a-file.pth',\\n        ...                    '/usr/share/another-place/src/Directory')\\n        '../another-place/src/Directory'\\n        >>> make_relative_path('/usr/share/something/a-file.pth',\\n        ...                    '/home/user/src/Directory')\\n        '../../../home/user/src/Directory'\\n        >>> make_relative_path('/usr/share/a-file.pth', '/usr/share/')\\n        './'\\n    \"\n    source = os.path.dirname(source)\n    if (not dest_is_directory):\n        dest_filename = os.path.basename(dest)\n        dest = os.path.dirname(dest)\n    dest = os.path.normpath(os.path.abspath(dest))\n    source = os.path.normpath(os.path.abspath(source))\n    dest_parts = dest.strip(os.path.sep).split(os.path.sep)\n    source_parts = source.strip(os.path.sep).split(os.path.sep)\n    while (dest_parts and source_parts and (dest_parts[0] == source_parts[0])):\n        dest_parts.pop(0)\n        source_parts.pop(0)\n    full_parts = ((['..'] * len(source_parts)) + dest_parts)\n    if (not dest_is_directory):\n        full_parts.append(dest_filename)\n    if (not full_parts):\n        return './'\n    return os.path.sep.join(full_parts)\n", "label": 1}
{"function": "\n\ndef _is_removing_self_admin_role(self, request, project_id, user_id, available_roles, current_role_ids):\n    is_current_user = (user_id == request.user.id)\n    is_current_project = (project_id == request.user.tenant_id)\n    _admin_roles = self.get_admin_roles()\n    available_admin_role_ids = [role.id for role in available_roles if (role.name.lower() in _admin_roles)]\n    admin_roles = [role for role in current_role_ids if (role in available_admin_role_ids)]\n    if len(admin_roles):\n        removing_admin = any([(role in current_role_ids) for role in admin_roles])\n    else:\n        removing_admin = False\n    if (is_current_user and is_current_project and removing_admin):\n        msg = _('You cannot revoke your administrative privileges from the project you are currently logged into. Please switch to another project with administrative privileges or remove the administrative role manually via the CLI.')\n        messages.warning(request, msg)\n        return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef GetCombinedLists(self):\n    '\\n        Return a collection of Buckets that hold all the values of the\\n        subclass-overridable collections above\\n        '\n    buckets = [Bucket(cellWidth=x, text='', align=None, image=None) for x in self.GetCellWidths()]\n    for (i, x) in enumerate(self.GetSubstitutedTexts()):\n        buckets[i].text = x\n    for (i, x) in enumerate(self.GetAlignments()):\n        buckets[i].align = x\n    if self.IncludeImages():\n        for (i, x) in enumerate(self.GetImages()):\n            buckets[i].image = x\n    cellPadding = self.GetFormat().CalculateCellPadding()\n    for x in buckets:\n        x.innerCellWidth = max(0, (x.cellWidth - (cellPadding[0] + cellPadding[2])))\n    return buckets\n", "label": 1}
{"function": "\n\ndef __init__(self, data=None):\n    if ((data is None) or isinstance(data, dict)):\n        data = (data or [])\n        super(SortedDict, self).__init__(data)\n        self.keyOrder = (list(data) if data else [])\n    else:\n        super(SortedDict, self).__init__()\n        super_set = super(SortedDict, self).__setitem__\n        for (key, value) in data:\n            if (key not in self):\n                self.keyOrder.append(key)\n            super_set(key, value)\n", "label": 1}
{"function": "\n\ndef _list_xml_members(cls):\n    \"Generator listing all members which are XML elements or attributes.\\n    \\n    The following members would be considered XML members:\\n    foo = 'abc' - indicates an XML attribute with the qname abc\\n    foo = SomeElement - indicates an XML child element\\n    foo = [AnElement] - indicates a repeating XML child element, each instance\\n        will be stored in a list in this member\\n    foo = ('att1', '{http://example.com/namespace}att2') - indicates an XML\\n        attribute which has different parsing rules in different versions of \\n        the protocol. Version 1 of the XML parsing rules will look for an\\n        attribute with the qname 'att1' but verion 2 of the parsing rules will\\n        look for a namespaced attribute with the local name of 'att2' and an\\n        XML namespace of 'http://example.com/namespace'.\\n    \"\n    members = []\n    for pair in inspect.getmembers(cls):\n        if ((not pair[0].startswith('_')) and (pair[0] != 'text')):\n            member_type = pair[1]\n            if (isinstance(member_type, tuple) or isinstance(member_type, list) or isinstance(member_type, (str, unicode)) or (inspect.isclass(member_type) and issubclass(member_type, XmlElement))):\n                members.append(pair)\n    return members\n", "label": 1}
{"function": "\n\ndef get_queryset_from_db(self):\n    tags = (Tag.objects.filter(slug=self.tag).values_list('name') or [])\n    tags_names = []\n    if tags:\n        tags_names = [i[0] for i in tags]\n    ids = []\n    for tag in tags_names:\n        result = self.containers = self.model.objects.filter(site_domain=self.site, tags__contains=tag, date_available__lte=timezone.now(), published=True)\n        if result.exists():\n            ids.extend([i.id for i in result])\n    ids = list(set(ids))\n    self.containers = self.model.objects.filter(id__in=ids)\n    return self.containers\n", "label": 1}
{"function": "\n\ndef to_html(self):\n    from itertools import groupby\n\n    def _make(v, caption):\n        v = safe_str(v)\n        args = {\n            'value': v,\n        }\n        if (isinstance(self.value, (tuple, list)) and (v in [safe_str(x) for x in self.value])):\n            args['selected'] = None\n        elif (v == safe_str(self.value)):\n            args['selected'] = None\n        return str(Tag('option', safe_str(caption), **args))\n    s = []\n    group = False\n    if self.choices:\n        group = (len(self.choices[0]) > 2)\n    if group:\n        for (k, g) in groupby(self.choices, (lambda x: x[0])):\n            s.append(begin_tag('optgroup', label=k))\n            for x in g:\n                s.append(_make(x[1], x[2]))\n            s.append(end_tag('optgroup'))\n    else:\n        for (v, caption) in self.choices:\n            s.append(_make(v, caption))\n    args = self.kwargs.copy()\n    if self.multiple:\n        args['multiple'] = None\n        args['size'] = self.size\n    return str(Tag('select', '\\n'.join(s), newline=True, **args))\n", "label": 1}
{"function": "\n\ndef values_from_response(self, response, sreg_names=None, ax_names=None):\n    'Return values from SimpleRegistration response or\\n        AttributeExchange response if present.\\n\\n        @sreg_names and @ax_names must be a list of name and aliases\\n        for such name. The alias will be used as mapping key.\\n        '\n    values = {\n        \n    }\n    if sreg_names:\n        resp = sreg.SRegResponse.fromSuccessResponse(response)\n        if resp:\n            values.update(((alias, (resp.get(name) or '')) for (name, alias) in sreg_names))\n    if ax_names:\n        resp = ax.FetchResponse.fromSuccessResponse(response)\n        if resp:\n            for (src, alias) in ax_names:\n                name = alias.replace('old_', '')\n                values[name] = (resp.getSingle(src, '') or values.get(name))\n    return values\n", "label": 1}
{"function": "\n\ndef check_permissions(self, request):\n    '\\n        Retrieve the controlled object and perform the permissions check.\\n        '\n    obj = ((hasattr(self, 'get_controlled_object') and self.get_controlled_object()) or (hasattr(self, 'get_object') and self.get_object()) or getattr(self, 'object', None))\n    user = request.user\n    perms = self.get_required_permissions(self)\n    has_permissions = self.perform_permissions_check(user, obj, perms)\n    if ((not has_permissions) and (not user.is_authenticated())):\n        return HttpResponseRedirect('{}?{}={}'.format(self.login_url, self.redirect_field_name, urlquote(request.get_full_path())))\n    elif (not has_permissions):\n        raise PermissionDenied\n", "label": 1}
{"function": "\n\ndef set_content(self, *values):\n    ' Sets the content of a view.\\n        '\n    content = []\n    accum = []\n    for value in values:\n        if isinstance(value, ViewSubElement):\n            content.append(value)\n        elif (type(value) in SequenceTypes):\n            content.append(Group(*value))\n        elif (isinstance(value, basestring) and (value[:1] == '<') and (value[(- 1):] == '>')):\n            content.append(Include(value[1:(- 1)].strip()))\n        else:\n            content.append(Item(value))\n    for item in content:\n        if isinstance(item, Item):\n            content = [Group(*content)]\n            break\n    self.content = Group(*content, container=self)\n", "label": 1}
{"function": "\n\ndef _link(self, target, output_file, classpaths):\n    args = ['--output', output_file]\n    if (self.get_options().level == 'debug'):\n        args.append('--debug')\n    if self.get_options().full_opt:\n        args.append('--fullOpt')\n    if self.get_options().check_ir:\n        args.append('--checkIR')\n    args.extend((jar for (_, jar) in classpaths.get_for_targets(target.closure(bfs=True))))\n    result = self.runjava(classpath=self.tool_classpath('scala-js-cli'), main=self._SCALA_JS_CLI_MAIN, jvm_options=self.get_options().jvm_options, args=args, workunit_name='scala-js-link')\n    if (result != 0):\n        raise TaskError('java {main} ... exited non-zero ({result}) for {target}'.format(main=self._SCALA_JS_CLI_MAIN, result=result, target=target.address.spec), failed_targets=[target])\n    if (not os.path.exists(output_file)):\n        raise TaskError('java {main} ... failed to produce an output for {target}'.format(main=self._SCALA_JS_CLI_MAIN, target=target.address.spec), failed_targets=[target])\n", "label": 1}
{"function": "\n\ndef save(self, commit=True):\n    (position_type, reference_node_id) = self._clean_cleaned_data()\n    if (self.instance.pk is None):\n        cl_data = {\n            \n        }\n        for field in self.cleaned_data:\n            if (not isinstance(self.cleaned_data[field], (list, QuerySet))):\n                cl_data[field] = self.cleaned_data[field]\n        if reference_node_id:\n            reference_node = self._meta.model.objects.get(pk=reference_node_id)\n            self.instance = reference_node.add_child(**cl_data)\n            self.instance.move(reference_node, pos=position_type)\n        else:\n            self.instance = self._meta.model.add_root(**cl_data)\n    else:\n        self.instance.save()\n        if reference_node_id:\n            reference_node = self._meta.model.objects.get(pk=reference_node_id)\n            self.instance.move(reference_node, pos=position_type)\n        else:\n            if self.is_sorted:\n                pos = 'sorted-sibling'\n            else:\n                pos = 'first-sibling'\n            self.instance.move(self._meta.model.get_first_root_node(), pos)\n    self.instance = self._meta.model.objects.get(pk=self.instance.pk)\n    super(MoveNodeForm, self).save(commit=commit)\n    return self.instance\n", "label": 1}
{"function": "\n\ndef import_(module, objects=None, via=None):\n    '\\n    :param module: py3 compatiable module path\\n    :param objects: objects want to imported, it should be a list\\n    :param via: for some py2 module, you should give the import path according the\\n        objects which you want to imported\\n    :return: object or module\\n    '\n    if PY3:\n        mod = __import__(module, fromlist=['*'])\n    else:\n        path = modules_mapping.get(module)\n        if (not path):\n            raise Exception((\"Can't find the module %s in mappings.\" % module))\n        if isinstance(path, list):\n            if (not via):\n                raise Exception('You should give a via parameter to enable import from py2.')\n            path = via\n        mod = __import__(path, fromlist=['*'])\n    if objects:\n        if (not isinstance(objects, (list, tuple))):\n            raise Exception('objects parameter should be a list or tuple.')\n        r = []\n        for x in objects:\n            r.append(getattr(mod, x))\n        if (len(r) > 1):\n            return tuple(r)\n        else:\n            return r[0]\n    else:\n        return mod\n", "label": 1}
{"function": "\n\ndef _process_axis_and_grid(plot, axis_type, axis_location, minor_ticks, axis_label, rng, dim):\n    axiscls = _get_axis_class(axis_type, rng)\n    if axiscls:\n        if (axiscls is LogAxis):\n            if (dim == 0):\n                plot.x_mapper_type = 'log'\n            elif (dim == 1):\n                plot.y_mapper_type = 'log'\n            else:\n                raise ValueError(('received invalid dimension value: %r' % dim))\n        axis = axiscls(plot=(plot if axis_location else None))\n        if isinstance(axis.ticker, ContinuousTicker):\n            axis.ticker.num_minor_ticks = _get_num_minor_ticks(axiscls, minor_ticks)\n        axis_label = axis_label\n        if axis_label:\n            axis.axis_label = axis_label\n        grid = Grid(plot=plot, dimension=dim, ticker=axis.ticker)\n        grid\n        if (axis_location is not None):\n            getattr(plot, axis_location).append(axis)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _item_style_sheet_changed(cls, item):\n    item_styles = cls._item_styles\n    item_sheets = cls._item_style_sheets\n    items = [i for i in item.traverse() if (i in cls._queried_items)]\n    for item in items:\n        sheets = item_sheets.pop(item, None)\n        if (sheets is not None):\n            sheet_items = cls._style_sheet_items\n            for sheet in sheets:\n                if (sheet in sheet_items):\n                    sheet_items[sheet].discard(item)\n        styles = item_styles.pop(item, None)\n        if (styles is not None):\n            style_items = cls._style_items\n            for style in styles:\n                if (style in style_items):\n                    style_items[style].discard(item)\n    cls._request_restyle(items)\n", "label": 1}
{"function": "\n\ndef require_instances(self, instances=(), instance_ids=()):\n    used = {(instance.id, instance.src) for instance in self.instances}\n    for instance in instances:\n        if ((instance.id, instance.src) not in used):\n            self.instances.append(Instance(id=instance.id, src=instance.src))\n            if (len(self.instances) == 1):\n                instance_node = self.node.find('instance')\n                command_node = self.node.find('command')\n                self.node.remove(instance_node)\n                self.node.insert((self.node.index(command_node) + 1), instance_node)\n    covered_ids = {instance_id for (instance_id, _) in used}\n    for instance_id in instance_ids:\n        if (instance_id not in covered_ids):\n            raise UnknownInstanceError('Instance reference not recognized: {} in xpath \"{}\"'.format(instance_id, getattr(instance_id, 'xpath', '(Xpath Unknown)')))\n    sorted_instances = sorted(self.instances, key=(lambda instance: instance.id))\n    if (sorted_instances != self.instances):\n        self.instances = sorted_instances\n", "label": 1}
{"function": "\n\ndef testCrossBelowMany(self):\n    count = 100\n    values1 = [0 for i in range(count)]\n    values2 = [((- 1) if ((i % 2) == 0) else 1) for i in range(count)]\n    self.assertEqual(cross.cross_below(values1, values2, 0, 0), 0)\n    period = 2\n    for i in range(1, count):\n        if ((i % 2) == 0):\n            self.assertEqual(cross.cross_below(values1, values2, ((i - period) + 1), (i + 1)), 0)\n        else:\n            self.assertEqual(cross.cross_below(values1, values2, ((i - period) + 1), (i + 1)), 1)\n    period = 4\n    for i in range(3, count):\n        if ((i % 2) == 0):\n            self.assertEqual(cross.cross_below(values1, values2, ((i - period) + 1), (i + 1)), 1)\n        else:\n            self.assertEqual(cross.cross_below(values1, values2, ((i - period) + 1), (i + 1)), 2)\n    self.assertEqual(cross.cross_below(values1, values2, 0, count), (count / 2))\n", "label": 1}
{"function": "\n\ndef minion_config(path, env_var='SALT_MINION_CONFIG', defaults=None, cache_minion_id=False):\n    \"\\n    Reads in the minion configuration file and sets up special options\\n\\n    This is useful for Minion-side operations, such as the\\n    :py:class:`~salt.client.Caller` class, and manually running the loader\\n    interface.\\n\\n    .. code-block:: python\\n\\n        import salt.client\\n        minion_opts = salt.config.minion_config('/etc/salt/minion')\\n    \"\n    if (defaults is None):\n        defaults = DEFAULT_MINION_OPTS\n    if ((path is not None) and path.endswith('proxy')):\n        defaults.update(DEFAULT_PROXY_MINION_OPTS)\n    if (not os.environ.get(env_var, None)):\n        salt_config_dir = os.environ.get('SALT_CONFIG_DIR', None)\n        if salt_config_dir:\n            env_config_file_path = os.path.join(salt_config_dir, 'minion')\n            if (salt_config_dir and os.path.isfile(env_config_file_path)):\n                os.environ[env_var] = env_config_file_path\n    overrides = load_config(path, env_var, DEFAULT_MINION_OPTS['conf_file'])\n    default_include = overrides.get('default_include', defaults['default_include'])\n    include = overrides.get('include', [])\n    overrides.update(include_config(default_include, path, verbose=False))\n    overrides.update(include_config(include, path, verbose=True))\n    opts = apply_minion_config(overrides, defaults, cache_minion_id=cache_minion_id)\n    _validate_opts(opts)\n    return opts\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, n):\n    n = sympify(n)\n    if n.is_Number:\n        if (n is S.Zero):\n            return S.One\n        elif (n is S.Infinity):\n            return S.Infinity\n        elif n.is_Integer:\n            if n.is_negative:\n                return S.ComplexInfinity\n            else:\n                n = n.p\n                if (n < 20):\n                    if (not cls._small_factorials):\n                        result = 1\n                        for i in range(1, 20):\n                            result *= i\n                            cls._small_factorials.append(result)\n                    result = cls._small_factorials[(n - 1)]\n                else:\n                    bits = bin(n).count('1')\n                    result = (cls._recursive(n) * (2 ** (n - bits)))\n                return Integer(result)\n", "label": 1}
{"function": "\n\ndef __exit__(self, type, value, tb):\n    if (value == None):\n        return True\n    if isinstance(value, NoPerm):\n        self.svc.return_error(403, value.args[0])\n    elif isinstance(value, WrongArgs):\n        self.svc.return_error(400, value.args[0])\n    elif isinstance(value, Unauthorized):\n        self.svc.return_error(401, value.args[0])\n    elif isinstance(value, NotFound):\n        self.svc.return_error(404, value.args[0])\n    elif isinstance(value, OutOfRange):\n        self.svc.return_error(416, value.args[0])\n    elif isinstance(value, ServerError):\n        self.svc.return_error(500, value.args[0])\n    else:\n        info = ''.join(traceback.format_exception(type, value, tb))\n        Log.error(('Unexpected exception! %s' % info))\n        self.svc.return_error(500, 'Unexpected server exception', info)\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef _CorrectOrientation(self, image, orientation):\n    'Use PIL to correct the image orientation based on its EXIF.\\n\\n    See JEITA CP-3451 at http://www.exif.org/specifications.html,\\n    Exif 2.2, page 18.\\n\\n    Args:\\n      image: source PIL.Image.Image object.\\n      orientation: integer in range (1,8) inclusive, corresponding the image\\n        orientation from EXIF.\\n\\n    Returns:\\n      PIL.Image.Image with transforms performed on it. If no correction was\\n        done, it returns the input image.\\n    '\n    if (orientation == 2):\n        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n    elif (orientation == 3):\n        image = image.rotate(180)\n    elif (orientation == 4):\n        image = image.transpose(Image.FLIP_TOP_BOTTOM)\n    elif (orientation == 5):\n        image = image.transpose(Image.FLIP_TOP_BOTTOM)\n        image = image.rotate(270)\n    elif (orientation == 6):\n        image = image.rotate(270)\n    elif (orientation == 7):\n        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n        image = image.rotate(270)\n    elif (orientation == 8):\n        image = image.rotate(90)\n    return image\n", "label": 1}
{"function": "\n\ndef NormalizeVmSettings(self):\n    'Normalize Vm settings.\\n    '\n    if self.IsVm():\n        if (not self.vm_settings):\n            self.vm_settings = VmSettings()\n        if ('vm_runtime' not in self.vm_settings):\n            self.SetEffectiveRuntime(self.runtime)\n        if (hasattr(self, 'beta_settings') and self.beta_settings):\n            for field in ['vm_runtime', 'has_docker_image', 'image', 'module_yaml_path']:\n                if ((field not in self.beta_settings) and (field in self.vm_settings)):\n                    self.beta_settings[field] = self.vm_settings[field]\n", "label": 1}
{"function": "\n\ndef match(self, left, collected=None):\n    collected = ([] if (collected is None) else collected)\n    args = [l for l in left if (type(l) is Argument)]\n    if (not len(args)):\n        return (False, left, collected)\n    left.remove(args[0])\n    if (type(self.value) is not list):\n        return (True, left, (collected + [Argument(self.name, args[0].value)]))\n    same_name = [a for a in collected if ((type(a) is Argument) and (a.name == self.name))]\n    if len(same_name):\n        same_name[0].value += [args[0].value]\n        return (True, left, collected)\n    else:\n        return (True, left, (collected + [Argument(self.name, [args[0].value])]))\n", "label": 1}
{"function": "\n\ndef single_step(s, x86_64=False):\n    i = fetch_instruction(s, x86_64)\n    if (i is None):\n        return []\n    if (i.address in _hit_count):\n        hc = _hit_count[i.address] = (_hit_count[i.address] + 1)\n    else:\n        hc = _hit_count[i.address] = 1\n    if (i.address in s.symbols):\n        symbol = s.symbols[i.address]\n        if (s.symbols[i.address] in s.function_hooks):\n            ss = s.function_hooks[symbol](s)\n            for s in ss:\n                if (s.solver.solve_time() > 30):\n                    s.log.warning('concretising (last solve took: {}s)', s.solver.solve_time)\n                    s.solver.concretise()\n                s.clear_il_state()\n            return ss\n        else:\n            s.log.function_call(None, symbol)\n    s.log.native_instruction(hc, i, x86_64)\n    max_il_index = len(i.il_instructions)\n    states = [s]\n    exit_states = []\n    while (len(states) > 0):\n        s = states.pop()\n        if (s.il_index >= max_il_index):\n            if (s.solver.solve_time() > 30):\n                s.log.warning('concretising (last solve took: {}s)'.format(s.solver.solve_time()))\n                s.solver.concretise()\n            s.clear_il_state()\n            exit_states.append(s)\n            continue\n        ri = i.il_instructions[s.il_index]\n        s.il_index += 1\n        s.log.reil_instruction(ri)\n        states += reil_single_step(ri, s)\n    return exit_states\n", "label": 1}
{"function": "\n\ndef __new__(cls, inputs=inputs, outputs=outputs, window_length=window_length, mask=mask, *args, **kwargs):\n    if (inputs is NotSpecified):\n        inputs = cls.inputs\n    if (inputs is not NotSpecified):\n        inputs = tuple(inputs)\n    if (outputs is NotSpecified):\n        outputs = cls.outputs\n    if (outputs is not NotSpecified):\n        outputs = tuple(outputs)\n    if (mask is NotSpecified):\n        mask = cls.mask\n    if (mask is NotSpecified):\n        mask = AssetExists()\n    if (window_length is NotSpecified):\n        window_length = cls.window_length\n    return super(ComputableTerm, cls).__new__(cls, *args, inputs=inputs, outputs=outputs, mask=mask, window_length=window_length, **kwargs)\n", "label": 1}
{"function": "\n\ndef update_site(env, debug):\n    'Run through commands to update this site.'\n    error_updating = False\n    here = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n    project_branch = {\n        'branch': ENV_BRANCH[env][PROJECT],\n    }\n    commands = [(CHDIR, here), (EXEC, (GIT_PULL % project_branch)), (EXEC, GIT_SYNC), (EXEC, GIT_SUBMODULE)]\n    commands += [(EXEC, 'python2.6 manage.py collectstatic --noinput'), (EXEC, 'python2.6 manage.py syncdb'), (EXEC, 'python2.6 manage.py migrate'), (EXEC, '/etc/init.d/httpd restart')]\n    for (cmd, cmd_args) in commands:\n        if (CHDIR == cmd):\n            if debug:\n                sys.stdout.write(('cd %s\\n' % cmd_args))\n            os.chdir(cmd_args)\n        elif (EXEC == cmd):\n            if debug:\n                sys.stdout.write(('%s\\n' % cmd_args))\n            if (not (0 == os.system(cmd_args))):\n                error_updating = True\n                break\n        else:\n            raise Exception(('Unknown type of command %s' % cmd))\n    if error_updating:\n        sys.stderr.write('There was an error while updating. Please try again later. Aborting.\\n')\n", "label": 1}
{"function": "\n\ndef validate_params(valid_options, params):\n    '\\n    Helps us validate the parameters for the request\\n\\n    :param valid_options: a list of strings of valid options for the\\n                          api request\\n    :param params: a dict, the key-value store which we really only care about\\n                   the key which has tells us what the user is using for the\\n                   API request\\n\\n    :returns: None or throws an exception if the validation fails\\n    '\n    if (not params):\n        return\n    data_filter = ['data', 'source', 'external_url', 'embed']\n    multiple_data = [key for key in params.keys() if (key in data_filter)]\n    if (len(multiple_data) > 1):\n        raise Exception(\"You can't mix and match data parameters\")\n    disallowed_fields = [key for key in params.keys() if (key not in valid_options)]\n    if disallowed_fields:\n        field_strings = ','.join(disallowed_fields)\n        raise Exception('{0} are not allowed fields'.format(field_strings))\n", "label": 1}
{"function": "\n\ndef write_worksheet_cols(doc, worksheet):\n    'Write worksheet columns to xml.'\n    if worksheet.column_dimensions:\n        start_tag(doc, 'cols')\n        for (column_string, columndimension) in worksheet.column_dimensions.items():\n            col_index = column_index_from_string(column_string)\n            col_def = {\n                \n            }\n            col_def['collapsed'] = str(columndimension.style_index)\n            col_def['min'] = str(col_index)\n            col_def['max'] = str(col_index)\n            if (columndimension.width != worksheet.default_column_dimension.width):\n                col_def['customWidth'] = 'true'\n            if (not columndimension.visible):\n                col_def['hidden'] = 'true'\n            if (columndimension.outline_level > 0):\n                col_def['outlineLevel'] = str(columndimension.outline_level)\n            if columndimension.collapsed:\n                col_def['collapsed'] = 'true'\n            if columndimension.auto_size:\n                col_def['bestFit'] = 'true'\n            if (columndimension.width > 0):\n                col_def['width'] = str(columndimension.width)\n            else:\n                col_def['width'] = '9.10'\n            tag(doc, 'col', col_def)\n        end_tag(doc, 'cols')\n", "label": 1}
{"function": "\n\ndef clone(self):\n    ' Clone this item and its components '\n    item = self.__class__()\n    item.label = self.label\n    item.tags = self.tags\n    item.r = self.r\n    item.application = self.application\n    item.controller = self.controller\n    item.function = self.function\n    item.match_controller = [c for c in self.match_controller]\n    item.match_function = [f for f in self.match_function]\n    item.args = [a for a in self.args]\n    item.vars = Storage(**self.vars)\n    item.extension = self.extension\n    item.tablename = self.tablename\n    item.method = self.method\n    item.p = self.p\n    item.override_url = self.override_url\n    item.attr = Storage(**self.attr)\n    item.opts = Storage(**self.opts)\n    item.parent = self.parent\n    item.components = [i.clone() for i in self.components]\n    item.enabled = self.enabled\n    item.selected = self.selected\n    item.visible = self.visible\n    item.link = self.link\n    item.mandatory = self.mandatory\n    if (self.restrict is not None):\n        item.restrict = [r for r in self.restrict]\n    else:\n        item.restrict = None\n    item.check = self.check\n    item.renderer = self.renderer\n    return item\n", "label": 1}
{"function": "\n\ndef render(self, context):\n    site_domain = (getattr(settings, 'SITE_DOMAIN', None) or Site.objects.get_current().domain)\n    list_keys = []\n    result_list = []\n    for (key, value) in self.properties.items():\n        if (type(value) == list):\n            list_keys.append([(key, item) for item in value])\n        else:\n            result_list.append(self.render_tag(key, value, context))\n    if (len(list_keys) > 0):\n        for item in roundrobin(*list_keys):\n            result_list.append(self.render_tag(item[0], item[1], context))\n    return '\\n'.join(result_list)\n", "label": 1}
{"function": "\n\ndef execute(self, ns, package, _type=None):\n    properties = ['Name', ('Type', (lambda i: (software.FILE_TYPES[i.FileType] if i.FileExists else 'Missing'))), ('FileSize', (lambda i: i.FileSize)), ('Passed', (lambda i: (len(i.FailedFlags) < 1)))]\n    if (_type is not None):\n        del properties[1]\n    pkgs = list((p.to_instance() for p in software.find_package(ns, pkg_spec=package)))\n    pkgs = [p for p in pkgs if software.is_package_installed(p)]\n    if (len(pkgs) < 1):\n        raise errors.LmiFailed(('No package matching \"%s\" found.' % package))\n    if (len(pkgs) > 1):\n        LOG().warn('More than one package found for \"%s\": %s', package, ', '.join((p.ElementName for p in pkgs)))\n    return (properties, software.list_package_files(ns, pkgs[(- 1)], file_type=_type))\n", "label": 1}
{"function": "\n\ndef edit_record(self, record):\n    new_record = list(record)\n    new_record[0] = (self.prompt(('Name [%s]: ' % record[0]), required=False) or record[0])\n    new_record[1] = (self.prompt(('Username [%s]: ' % record[1]), required=False) or record[1])\n    pw = (self.prompt('Password []/g: ', required=False, password=True) or record[2])\n    if (pw == 'g'):\n        new_record[2] = gen_password_entropy(128)\n    elif pw:\n        new_record[2] = pw\n    self.output.write(('Notes: %s\\n' % record[3]))\n    edit = (self.prompt('Edit? [n]: ', required=False) or 'n')\n    if (edit[0] == 'y'):\n        new_record[3] = edit_in_editor(record[3])\n    return tuple(new_record)\n", "label": 1}
{"function": "\n\ndef find_square(frame_in):\n    frame_out = frame_in.copy()\n    frame_gray = cv2.cvtColor(frame_in, cv2.COLOR_BGR2GRAY)\n    thresh = adap_threshold(frame_gray)\n    frame_blur = cv2.blur(thresh, (3, 3))\n    (contours, hry) = cv2.findContours(frame_blur, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    squares = []\n    for cnt in contours:\n        cnt_len = cv2.arcLength(cnt, True)\n        cnt = cv2.approxPolyDP(cnt, (0.01 * cnt_len), True)\n        if ((cv2.contourArea(cnt) > 10) and (len(cnt) == 4) and cv2.isContourConvex(cnt)):\n            cnt = cnt.reshape((- 1), 2)\n            max_cos = np.max([angle_cos(cnt[i], cnt[((i + 1) % 4)], cnt[((i + 2) % 4)]) for i in xrange(4)])\n            if ((max_cos < 0.1) and equal(cnt[0], cnt[1], cnt[2], 0.2)):\n                squares.append(cnt)\n    cv2.drawContours(frame_out, squares, (- 1), (0, 255, 0), 2)\n    return frame_out\n", "label": 1}
{"function": "\n\ndef gather_sample(self):\n    'Invoked once per sample interval to gather a statistic.\\n        '\n\n    def get_value_as_str(value):\n        if (type(value) is int):\n            return ('%d' % value)\n        elif (type(value) is float):\n            return ('%f' % value)\n        elif (type(value) is str):\n            return ('%r' % value)\n        else:\n            return ('%r' % value)\n    status = self._get_status(self._monitor_url)\n    if (status != None):\n        stats = self._parse_general_status(status)\n        heap = self._parse_heap_status(status)\n        if (stats != None):\n            for key in stats.keys():\n                extra = None\n                if (len(stats[key]) == 4):\n                    extra = {\n                        stats[key][2]: stats[key][3],\n                    }\n                self._logger.emit_value(('tomcat.runtime.%s' % stats[key][0]), stats[key][1], extra)\n        if (heap != None):\n            for key in heap.keys():\n                extra = {\n                    heap[key][2]: heap[key][3],\n                }\n                self._logger.emit_value(('tomcat.memory_pools.%s' % heap[key][0]), heap[key][1], extra)\n", "label": 1}
{"function": "\n\ndef plugins(self, *plugins):\n    import json\n    ret = []\n    available_plugins = self.get('system/plugins')\n    self.message['debug']['available_plugins'] = available_plugins\n    plugins = set(plugins)\n    enabled_plugins = set(available_plugins['enabled'])\n    if ('*' in plugins):\n        plugins = set(available_plugins['all'].keys())\n    if (not (plugins <= set(available_plugins['all'].keys()))):\n        self.fail(('%s, not available!' % ','.join(list((plugins - set(available_plugins['all'].keys()))))))\n    if (self.module.params['state'] == 'present'):\n        if (not (plugins <= enabled_plugins)):\n            ret = self.put('system/plugins', {\n                'plugins': json.dumps(list((plugins | enabled_plugins))),\n            })\n            self.changed = True\n    elif (self.module.params['state'] == 'absent'):\n        if len((enabled_plugins & plugins)):\n            ret = self.put('system/plugins', {\n                'plugins': json.dumps(list((enabled_plugins - plugins))),\n            })\n            self.changed = True\n    return ret\n", "label": 1}
{"function": "\n\ndef _eval_expand_func(self, **hints):\n    '\\n        Function to expand binomial(n,k) when m is positive integer\\n        Also,\\n        n is self.args[0] and k is self.args[1] while using binomial(n, k)\\n        '\n    n = self.args[0]\n    if n.is_Number:\n        return binomial(*self.args)\n    k = self.args[1]\n    if (k.is_Add and (n in k.args)):\n        k = (n - k)\n    if k.is_Integer:\n        if (k == S.Zero):\n            return S.One\n        elif (k < 0):\n            return S.Zero\n        else:\n            n = self.args[0]\n            result = ((n - k) + 1)\n            for i in range(2, (k + 1)):\n                result *= ((n - k) + i)\n                result /= i\n            return result\n    else:\n        return binomial(*self.args)\n", "label": 1}
{"function": "\n\ndef remove_tech(self):\n    if (self.selected_tech is None):\n        return\n    item = self.selected_tech\n    visible = [x['name'] for x in self.player.get_visible_techs()]\n    enabled = [x['name'] for x in self.player.get_enabled_techs()]\n    self.player.set_visible_techs(make_tech_list([x for x in visible if (x != item)]))\n    self.player.set_enabled_techs(make_tech_list([x for x in enabled if (x != item)]))\n    self.update_lists()\n    self.update_selection()\n", "label": 1}
{"function": "\n\n@property\ndef rows(self):\n    if self.config['location_id']:\n        selected_location = SQLLocation.objects.get(location_id=self.config['location_id'])\n        key = (selected_location.location_type.name.lower() + '_id')\n        data = self.custom_data(domain=self.config['domain'], filters={\n            key: [Choice(value=selected_location.location_id, display=selected_location.name)],\n        })\n        rows = []\n        for row in self.row_config:\n            row_data = []\n            for (idx, cell) in enumerate(row):\n                if isinstance(cell, tuple):\n                    x = data.get(cell[0], 0)\n                    y = data.get(cell[1], 0)\n                    row_data.append((x + y))\n                else:\n                    row_data.append(data.get(cell, (cell if ((cell == '--') or (idx in [0, 1])) else 0)))\n            rows.append(row_data)\n        return rows\n", "label": 1}
{"function": "\n\ndef collect(self):\n    '\\n        Collect memory stats of LXCs.\\n        '\n    lxc_metrics = ['memory.usage_in_bytes', 'memory.limit_in_bytes']\n    if (os.path.isdir(self.config['sys_path']) is False):\n        self.log.debug(\"sys_path '%s' isn't directory.\", self.config['sys_path'])\n        return {\n            \n        }\n    collected = {\n        \n    }\n    for item in os.listdir(self.config['sys_path']):\n        fpath = ('%s/%s' % (self.config['sys_path'], item))\n        if (os.path.isdir(fpath) is False):\n            continue\n        for lxc_metric in lxc_metrics:\n            filename = ('%s/%s' % (fpath, lxc_metric))\n            metric_name = ('%s.%s' % (item.replace('.', '_'), lxc_metric.replace('_in_bytes', '')))\n            self.log.debug('Trying to collect from %s', filename)\n            collected[metric_name] = self._read_file(filename)\n    for key in collected.keys():\n        if (collected[key] is None):\n            continue\n        for unit in self.config['byte_unit']:\n            value = diamond.convertor.binary.convert(collected[key], oldUnit='B', newUnit=unit)\n            new_key = ('%s_in_%ss' % (key, unit))\n            self.log.debug(\"Publishing '%s %s'\", new_key, value)\n            self.publish(new_key, value, metric_type='GAUGE')\n", "label": 1}
{"function": "\n\ndef execute():\n    for table in frappe.db.get_tables():\n        doctype = table[3:]\n        if frappe.db.exists('DocType', doctype):\n            fieldnames = [df['fieldname'] for df in frappe.get_all('DocField', fields=['fieldname'], filters={\n                'parent': doctype,\n            })]\n            custom_fieldnames = [df['fieldname'] for df in frappe.get_all('Custom Field', fields=['fieldname'], filters={\n                'dt': doctype,\n            })]\n        else:\n            fieldnames = custom_fieldnames = []\n        for column in frappe.db.sql('desc `{0}`'.format(table), as_dict=True):\n            if (column['Type'] == 'int(1)'):\n                fieldname = column['Field']\n                if (not ((fieldname in default_fields) or (fieldname in fieldnames) or (fieldname in custom_fieldnames))):\n                    continue\n                frappe.db.sql('update `{table}` set `{column}`=0 where `{column}` is null'.format(table=table, column=fieldname))\n                frappe.db.commit()\n                frappe.db.sql_ddl('alter table `{table}`\\n\\t\\t\\t\\t\\tmodify `{column}` int(1) not null default {default}'.format(table=table, column=fieldname, default=cint(column['Default'])))\n", "label": 1}
{"function": "\n\ndef get_errors(self, check_order=True):\n    errors = []\n    self._doc.reset()\n    for (j, line) in enumerate(self._doc):\n        if (len(line) > 75):\n            if hasattr(self, 'name'):\n                errors.append(('%s: Line %d exceeds 75 chars: \"%s\"...' % (self.name, (j + 1), line[:30])))\n            else:\n                errors.append(('Line %d exceeds 75 chars: \"%s\"...' % ((j + 1), line[:30])))\n    if check_order:\n        canonical_order = ['Signature', 'Summary', 'Extended Summary', 'Attributes', 'Methods', 'Parameters', 'Other Parameters', 'Returns', 'Raises', 'Warns', 'See Also', 'Notes', 'References', 'Examples', 'index']\n        canonical_order_copy = list(canonical_order)\n        for s in self.section_order:\n            while (canonical_order_copy and (s != canonical_order_copy[0])):\n                canonical_order_copy.pop(0)\n                if (not canonical_order_copy):\n                    errors.append(('Sections in wrong order (starting at %s). The right order is %s' % (s, canonical_order)))\n    return errors\n", "label": 1}
{"function": "\n\ndef colorize(style, msg, resp):\n    \"Taken and modified from `django.utils.log.ServerFormatter.format`\\n    to mimic runserver's styling.\\n    \"\n    code = resp.status.split(maxsplit=1)[0]\n    if (code[0] == '2'):\n        msg = style.HTTP_SUCCESS(msg)\n    elif (code[0] == '1'):\n        msg = style.HTTP_INFO(msg)\n    elif (code == '304'):\n        msg = style.HTTP_NOT_MODIFIED(msg)\n    elif (code[0] == '3'):\n        msg = style.HTTP_REDIRECT(msg)\n    elif (code == '404'):\n        msg = style.HTTP_NOT_FOUND(msg)\n    elif (code[0] == '4'):\n        msg = style.HTTP_BAD_REQUEST(msg)\n    else:\n        msg = style.HTTP_SERVER_ERROR(msg)\n    return msg\n", "label": 1}
{"function": "\n\ndef riemann_cyclic(t2):\n    \"\\n    replace each Riemann tensor with an equivalent expression\\n    satisfying the cyclic identity.\\n\\n    This trick is discussed in the reference guide to Cadabra.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.tensor.tensor import TensorIndexType, tensor_indices, tensorhead, riemann_cyclic\\n    >>> Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\\n    >>> i, j, k, l = tensor_indices('i,j,k,l', Lorentz)\\n    >>> R = tensorhead('R', [Lorentz]*4, [[2, 2]])\\n    >>> t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l))\\n    >>> riemann_cyclic(t)\\n    0\\n    \"\n    if isinstance(t2, (TensMul, Tensor)):\n        args = [t2]\n    else:\n        args = t2.args\n    a1 = [x.split() for x in args]\n    a2 = [[riemann_cyclic_replace(tx) for tx in y] for y in a1]\n    a3 = [tensor_mul(*v) for v in a2]\n    t3 = TensAdd(*a3)\n    if (not t3):\n        return t3\n    else:\n        return canon_bp(t3)\n", "label": 1}
{"function": "\n\ndef initial(self, request, *args, **kwargs):\n    if (self.action in ('update', 'partial_update', 'destroy')):\n        instance = self.get_object()\n        if (instance and (instance.state not in instance.States.STABLE_STATES)):\n            raise core_exceptions.IncorrectStateException('Modification allowed in stable states only')\n    elif (self.action in ('stop', 'start', 'resize')):\n        instance = self.get_object()\n        if (instance and (instance.state == instance.States.PROVISIONING_SCHEDULED)):\n            raise core_exceptions.IncorrectStateException('Provisioning scheduled. Disabled modifications.')\n    return super(InstanceViewSet, self).initial(request, *args, **kwargs)\n", "label": 1}
{"function": "\n\ndef kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB_ALIAS):\n    \"\\n    This view generates KML for the given app label, model, and field name.\\n\\n    The model's default manager must be GeoManager, and the field name\\n    must be that of a geographic field.\\n    \"\n    placemarks = []\n    klass = get_model(label, model)\n    if (not klass):\n        raise Http404(('You must supply a valid app label and module name.  Got \"%s.%s\"' % (label, model)))\n    if field_name:\n        try:\n            info = klass._meta.get_field_by_name(field_name)\n            if (not isinstance(info[0], GeometryField)):\n                raise Exception\n        except:\n            raise Http404('Invalid geometry field.')\n    connection = connections[using]\n    if connection.ops.postgis:\n        placemarks = klass._default_manager.using(using).kml(field_name=field_name)\n    else:\n        placemarks = []\n        if connection.ops.oracle:\n            qs = klass._default_manager.using(using).transform(4326, field_name=field_name)\n        else:\n            qs = klass._default_manager.using(using).all()\n        for mod in qs:\n            mod.kml = getattr(mod, field_name).kml\n            placemarks.append(mod)\n    if compress:\n        render = render_to_kmz\n    else:\n        render = render_to_kml\n    return render('gis/kml/placemarks.kml', {\n        'places': placemarks,\n    })\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    initial = kwargs.get('initial', {\n        \n    })\n    instance = kwargs.get('instance', None)\n    if ((not instance) and (not ('publish_date' in initial))):\n        initial['publish_date'] = datetime.datetime.now().date()\n    if ((not instance) and (not ('publish_time' in initial))):\n        initial['publish_time'] = datetime.datetime.now().time().strftime('%H:%M:%S')\n    if ((not instance) and (not ('site' in initial))):\n        initial['site'] = Site.objects.get_current().id\n    kwargs.update({\n        'initial': initial,\n    })\n    super(StoryForm, self).__init__(*args, **kwargs)\n", "label": 1}
{"function": "\n\ndef parse_parameters_from_response(self, response):\n    \"\\n        Returns a response signature and query string generated from the\\n        server response. 'h' aka signature argument is stripped from the\\n        returned query string.\\n        \"\n    lines = response.splitlines()\n    pairs = [line.strip().split('=', 1) for line in lines if ('=' in line)]\n    pairs = sorted(pairs)\n    signature = ([unquote(v) for (k, v) in pairs if (k == 'h')] or [None])[0]\n    query_string = '&'.join([((k + '=') + v) for (k, v) in pairs if (k != 'h')])\n    return (signature, query_string)\n", "label": 1}
{"function": "\n\ndef parse_csv(self, data):\n    if ((data == None) or (data == '')):\n        return\n    if isinstance(data, str):\n        str_data = data.split('\\n')\n    else:\n        str_data = data.decode(encoding='utf-8').split('\\n')\n    for f in str_data:\n        if (';' in str(f)):\n            s = f.split(';')[0]\n            ns = f.split(';')[1]\n            if ((ns is not None) and (ns != '')):\n                ns = int(re.sub('\\\\D', '', ns))\n                s = int(re.sub('\\\\D', '', s))\n                self.data.append(((s * 1000000000.0) + ns))\n", "label": 1}
{"function": "\n\ndef server_list(request, search_opts=None, all_tenants=False):\n    page_size = utils.get_page_size(request)\n    c = novaclient(request)\n    paginate = False\n    if (search_opts is None):\n        search_opts = {\n            \n        }\n    elif ('paginate' in search_opts):\n        paginate = search_opts.pop('paginate')\n        if paginate:\n            search_opts['limit'] = (page_size + 1)\n    if all_tenants:\n        search_opts['all_tenants'] = True\n    else:\n        search_opts['project_id'] = request.user.tenant_id\n    servers = [Server(s, request) for s in c.servers.list(True, search_opts)]\n    has_more_data = False\n    if (paginate and (len(servers) > page_size)):\n        servers.pop((- 1))\n        has_more_data = True\n    elif (paginate and (len(servers) == getattr(settings, 'API_RESULT_LIMIT', 1000))):\n        has_more_data = True\n    return (servers, has_more_data)\n", "label": 1}
{"function": "\n\n@view_config(renderer='ban.mak', route_name='ban')\ndef ban(request):\n    r = request\n    s = request.session\n    p = s['safe_post']\n    if (('logged_in_admin' not in s) or (s['logged_in_admin'] == False)):\n        return HTTPNotFound()\n    if ('ip' in p):\n        if (p['ip'].strip() == ''):\n            ip = None\n        else:\n            ip = p['ip']\n        if (p['username'].strip() == ''):\n            username = None\n            user_id = None\n        else:\n            username = p['username']\n        if (p['duration'].strip() == 'infinite'):\n            duration = None\n        else:\n            duration = 'timedelta({0})'.format(p['duration'])\n            duration = eval(duration)\n        if username:\n            user_id = users.get_user_by_name(username).id\n        b = Ban(ip=ip, username=username, duration=duration, user_id=user_id, added_by=s['users.id'])\n        dbsession = DBSession()\n        dbsession.add(b)\n    bans = general.list_bans()\n    return {\n        'bans': bans,\n    }\n", "label": 1}
{"function": "\n\ndef _check_available_product_reminder(self, card, last_state):\n    ' Check if card has new products '\n    current_prod = (card.products if card.products else [])\n    old_prod = (last_state.products if last_state.products else [])\n    for product in current_prod:\n        if (product not in old_prod):\n            return True\n    current_pending = (card.pending if card.pending else [])\n    old_pending = (last_state.pending if last_state.pending else [])\n    for pending in current_pending:\n        if (pending in old_pending):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef start_history_lines_completion(self):\n    '\\n        Start a completion based on all the other lines in the document and the\\n        history.\\n        '\n    found_completions = set()\n    completions = []\n    current_line = self.document.current_line_before_cursor.lstrip()\n    for (i, string) in enumerate(self._working_lines):\n        for (j, l) in enumerate(string.split('\\n')):\n            l = l.strip()\n            if (l and l.startswith(current_line)):\n                if (l not in found_completions):\n                    found_completions.add(l)\n                    if (i == self.working_index):\n                        display_meta = ('Current, line %s' % (j + 1))\n                    else:\n                        display_meta = ('History %s, line %s' % ((i + 1), (j + 1)))\n                    completions.append(Completion(l, start_position=(- len(current_line)), display_meta=display_meta))\n    self.set_completions(completions=completions[::(- 1)])\n", "label": 1}
{"function": "\n\ndef preprocessOptions(self):\n    'Processes \"action\" oriented options.'\n    if self.getOption('runit_install'):\n        self._install()\n    if HAS_DAEMONIZE:\n        if self.status:\n            if daemon.status(pidfile=self.pidfile, logger=self.logger):\n                sys.exit(0)\n            else:\n                sys.exit(1)\n        if self.kill:\n            if daemon.kill(pidfile=self.pidfile, logger=self.logger):\n                sys.exit(0)\n            else:\n                sys.exit(1)\n    if (self.options.tasks == []):\n        print('Available Tasks:')\n        for t in self.tasks:\n            print((' - %s' % t.__name__))\n        sys.exit(1)\n", "label": 1}
{"function": "\n\ndef test_r(self):\n    cur_dir = os.path.dirname(os.path.abspath(__file__))\n    rdir = os.path.join(cur_dir, 'results')\n    fnames = os.listdir(rdir)\n    fnames = [x for x in fnames if (x.startswith('lme') and x.endswith('.csv'))]\n    for fname in fnames:\n        for reml in (False, True):\n            for irf in (False, True):\n                ds_ix = int(fname[3:5])\n                (yield (self.do1, reml, irf, ds_ix))\n", "label": 1}
{"function": "\n\ndef compute(self):\n    machine = self.get_machine()\n    jm = self.job_monitor()\n    cache = jm.getCache(self.signature)\n    if cache:\n        result = cache.parameters['result']\n    else:\n        if (not self.has_input('local_file')):\n            raise ModuleError(self, 'No local file specified')\n        local_file = self.get_input('local_file').strip()\n        if (not self.has_input('remote_file')):\n            raise ModuleError(self, 'No remote file specified')\n        remote_file = self.get_input('remote_file').strip()\n        whereto = 'remote'\n        if (self.has_input('to_local') and self.get_input('to_local')):\n            whereto = 'local'\n        command = (machine.getfile if (whereto == 'local') else machine.sendfile)\n        result = command(local_file, remote_file)\n        d = {\n            'result': result,\n        }\n        self.set_job_machine(d, machine)\n        jm.setCache(self.signature, d, self.job_name())\n    self.set_output('machine', machine)\n    self.set_output('output', result)\n", "label": 1}
{"function": "\n\ndef calc_max_bits(self, signed, values):\n    ' Calculates the maximim needed bits to represent a value '\n    b = 0\n    vmax = (- 10000000)\n    for val in values:\n        if signed:\n            b = ((b | val) if (val >= 0) else (b | ((~ val) << 1)))\n            vmax = (val if (vmax < val) else vmax)\n        else:\n            b |= val\n    bits = 0\n    if (b > 0):\n        bits = (len(self.bin(b)) - 2)\n        if (signed and (vmax > 0) and ((len(self.bin(vmax)) - 2) >= bits)):\n            bits += 1\n    return bits\n", "label": 1}
{"function": "\n\ndef get_distance(self, dist_val, lookup_type):\n    '\\n        Returns a distance number in units of the field.  For example, if\\n        `D(km=1)` was passed in and the units of the field were in meters,\\n        then 1000 would be returned.\\n        '\n    if (len(dist_val) == 1):\n        (dist, option) = (dist_val[0], None)\n    else:\n        (dist, option) = dist_val\n    if isinstance(dist, Distance):\n        if self.geodetic:\n            if (SpatialBackend.postgis and (lookup_type == 'dwithin')):\n                raise TypeError('Only numeric values of degree units are allowed on geographic DWithin queries.')\n            dist_param = dist.m\n        else:\n            dist_param = getattr(dist, Distance.unit_attname(self.units_name))\n    else:\n        dist_param = dist\n    if (SpatialBackend.postgis and self.geodetic and (lookup_type != 'dwithin') and (option == 'spheroid')):\n        return [gqn(self._spheroid), dist_param]\n    else:\n        return [dist_param]\n", "label": 1}
{"function": "\n\ndef _county_meta(self, year, elections):\n    payload = []\n    (general, primary, special) = self._races_by_type(elections)\n    for jurisdiction in self._jurisdictions():\n        meta = {\n            'ocd_id': jurisdiction['ocd_id'],\n            'name': jurisdiction['name'],\n        }\n        county = jurisdiction['url_name']\n        for precinct_val in (True, False):\n            general_url = self._build_county_url(year, county, precinct=precinct_val)\n            general_filename = self._generate_county_filename(general_url, general['start_date'], jurisdiction)\n            gen_meta = meta.copy()\n            gen_meta.update({\n                'raw_url': general_url,\n                'generated_filename': general_filename,\n                'election': general['slug'],\n            })\n            payload.append(gen_meta)\n        if (primary and (int(year) > 2000)):\n            for party in ['Democratic', 'Republican']:\n                for precinct_val in (True, False):\n                    pri_meta = meta.copy()\n                    primary_url = self._build_county_url(year, county, party, precinct_val)\n                    primary_filename = self._generate_county_filename(primary_url, primary['start_date'], jurisdiction)\n                    pri_meta.update({\n                        'raw_url': primary_url,\n                        'generated_filename': primary_filename,\n                        'election': primary['slug'],\n                    })\n                    payload.append(pri_meta)\n    return payload\n", "label": 1}
{"function": "\n\ndef set_PWM(self, percent, ntries=15):\n    '\\n        '\n    count = 0\n    success = False\n    if (percent < 0.0):\n        percent = 0.0\n    if (percent > 100.0):\n        percent = 100.0\n    while ((not success) and (count <= ntries)):\n        if self.logger:\n            self.logger.info('Setting PWM value to {:.1f} %'.format(percent))\n        send_digital = int(((1023.0 * float(percent)) / 100.0))\n        send_string = 'P{:04d}!'.format(send_digital)\n        result = self.query(send_string)\n        count += 1\n        if result:\n            self.PWM = ((float(result[0]) * 100.0) / 1023.0)\n            if (abs((self.PWM - percent)) > 5.0):\n                if self.logger:\n                    self.logger.warning('  Failed to set PWM value!')\n                time.sleep(2)\n            else:\n                success = True\n            if self.logger:\n                self.logger.info('  PWM Value = {:.1f}'.format(self.PWM))\n", "label": 1}
{"function": "\n\ndef _create_task_add_task_resources(task, ns):\n    task_resources = dict(ns.mesos_task_resources)\n    seen = set()\n    for key in set(SCALAR_KEYS).intersection(task_resources):\n        seen.add(key)\n        resource = task.resources.add()\n        resource.name = key\n        resource.type = mesos_pb2.Value.SCALAR\n        typecast = SCALAR_KEYS[key]\n        resource.scalar.value = typecast(task_resources[key])\n    for key in set(RANGE_KEYS).intersection(task_resources):\n        seen.add(key)\n        resource = task.resources.add()\n        resource.name = key\n        resource.type = mesos_pb2.Value.RANGES\n        for range_data in task_resources[key]:\n            inst = resource.ranges.range.add()\n            typecast = RANGE_KEYS[key]\n            inst.begin = typecast(range_data[0])\n            inst.end = typecast(range_data[1])\n    for key in set(SET_KEYS).intersection(task_resources):\n        typecast = SET_KEYS[key]\n        seen.add(key)\n        resource = task.resources.add()\n        resource = task.resources.add()\n        resource.name = key\n        resource.type = mesos_pb2.Value.SET\n        for elem in task_resources[key]:\n            resource.set.item.append(typecast(elem))\n    unrecognized_keys = set(task_resources).difference(seen)\n    if unrecognized_keys:\n        msg = 'Unrecognized keys in task_resources!'\n        log.error(msg, extra=dict(unrecognized_keys=unrecognized_keys, mesos_framework_name=ns.mesos_framework_name))\n        raise UserWarning(('%s unrecognized_keys: %s' % (msg, unrecognized_keys)))\n", "label": 1}
{"function": "\n\ndef updateDropEffect(self, dataTransfer, event_type):\n    '\\n        http://dev.w3.org/html5/spec/dnd.html#dragevent\\n        '\n    dropEffect = 'none'\n    if (event_type in ['dragover', 'dragenter']):\n        ea = dataTransfer.getEffectAllowed()\n        if (ea == 'none'):\n            dropEffect = 'none'\n        elif (ea.startswith('copy') or (ea == 'all')):\n            dropEffect = 'copy'\n        elif ea.startswith('link'):\n            dropEffect = 'link'\n        elif (ea == 'move'):\n            dropEffect = 'move'\n        else:\n            dropEffect = 'copy'\n    elif (event_type in ['drop', 'dragend']):\n        dropEffect = self.currentDragOperation\n    dataTransfer.dropEffect = dropEffect\n", "label": 1}
{"function": "\n\ndef on_done(self, index):\n    if self.peek_file:\n        self.restore_view()\n    if (index == (- 1)):\n        return\n    item_name = self.get_name_by_index(index)\n    if self.isurl(item_name):\n        return self.open_url(item_name)\n    if self.isapp(item_name):\n        return self.open_app(item_name)\n    if path.isfile(item_name):\n        return self.open_file(item_name)\n    if path.isdir(item_name):\n        return self.open_dir(item_name)\n    sublime.status_message(('Invalid file/directory name: `%s`.' % item_name))\n", "label": 1}
{"function": "\n\ndef mustcontain(self, *strings, **kw):\n    '\\n        Assert that the response contains all of the strings passed\\n        in as arguments.\\n\\n        Equivalent to::\\n\\n            assert string in res\\n        '\n    if ('no' in kw):\n        no = kw['no']\n        del kw['no']\n        if isinstance(no, (six.binary_type, six.text_type)):\n            no = [no]\n    else:\n        no = []\n    if kw:\n        raise TypeError(\"The only keyword argument allowed is 'no'\")\n    for s in strings:\n        if (not (s in self)):\n            print(('Actual response (no %r):' % s), file=sys.stderr)\n            print(self, file=sys.stderr)\n            raise IndexError(('Body does not contain string %r' % s))\n    for no_s in no:\n        if (no_s in self):\n            print(('Actual response (has %r)' % no_s), file=sys.stderr)\n            print(self, file=sys.stderr)\n            raise IndexError(('Body contains string %r' % s))\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('SuperColumn')\n    if (self.name != None):\n        oprot.writeFieldBegin('name', TType.STRING, 1)\n        oprot.writeString(self.name)\n        oprot.writeFieldEnd()\n    if (self.columns != None):\n        oprot.writeFieldBegin('columns', TType.LIST, 2)\n        oprot.writeListBegin(TType.STRUCT, len(self.columns))\n        for iter6 in self.columns:\n            iter6.write(oprot)\n        oprot.writeListEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n\n    def validate(self):\n        if (self.name is None):\n            raise TProtocol.TProtocolException(message='Required field name is unset!')\n        if (self.columns is None):\n            raise TProtocol.TProtocolException(message='Required field columns is unset!')\n        return\n", "label": 1}
{"function": "\n\ndef lamdaContainer(x):\n    f = (lambda c: c)\n    g = (lambda c: (c if x else (c * c)))\n    y = f(x)\n    z = g(4)\n    print('Lambda with conditional expression gives', z)\n    if ('a' <= x <= y <= 'z'):\n        print('Four')\n    if ('a' <= x <= 'z'):\n        print('Yes')\n    if ('a' <= x > 'z'):\n        print('Yes1')\n    if ('a' <= ('1' if x else '2') > 'z'):\n        print('Yes2')\n    if ('a' <= ('1' if x else '2') > 'z' > undefined_global):\n        print('Yes3')\n    z = (lambda huhu=y: huhu)\n    print('Lambda defaulted gives', z())\n", "label": 1}
{"function": "\n\ndef parse_headers(message):\n    '\\n    Turn a Message object into a list of WSGI-style headers.\\n    '\n    headers_out = []\n    if six.PY3:\n        for (header, value) in message.items():\n            if (header.lower() not in filtered_headers):\n                headers_out.append((header, value))\n    else:\n        for full_header in message.headers:\n            if (not full_header):\n                continue\n            if full_header[0].isspace():\n                if (not headers_out):\n                    raise ValueError(('First header starts with a space (%r)' % full_header))\n                (last_header, last_value) = headers_out.pop()\n                value = ((last_value + ' ') + full_header.strip())\n                headers_out.append((last_header, value))\n                continue\n            try:\n                (header, value) = full_header.split(':', 1)\n            except:\n                raise ValueError(('Invalid header: %r' % full_header))\n            value = value.strip()\n            if (header.lower() not in filtered_headers):\n                headers_out.append((header, value))\n    return headers_out\n", "label": 1}
{"function": "\n\ndef parse_kwargs(kwargs):\n    \"\\n    Convert a list of kwargs into a dictionary. Duplicates of the same keyword\\n    get added to an list within the dictionary.\\n\\n    >>> parse_kwargs(['--var1=1', '--var2=2', '--var1=3']\\n    {'var1': [1, 3], 'var2': 2}\\n    \"\n    d = defaultdict(list)\n    for (k, v) in ((k.lstrip('-'), v) for (k, v) in (a.split('=') for a in kwargs)):\n        d[k].append(v)\n    ret = {\n        \n    }\n    for (k, v) in d.items():\n        if ((len(v) == 1) and (type(v) is list)):\n            ret[k] = v[0]\n        else:\n            ret[k] = v\n    return ret\n", "label": 1}
{"function": "\n\ndef _selection_rows_updated(self, event):\n    ' Handles multiple row selection changes.\\n        '\n    gfi = self.model.get_filtered_item\n    rio = self.model.raw_index_of\n    tl = self.grid._grid.GetSelectionBlockTopLeft()\n    br = iter(self.grid._grid.GetSelectionBlockBottomRight())\n    rows = len(self.model.get_filtered_items())\n    if self.auto_add:\n        rows -= 1\n    values = []\n    for (row0, col0) in tl:\n        (row1, col1) = br.next()\n        for row in xrange(row0, (row1 + 1)):\n            if (row < rows):\n                values.append((rio(row), gfi(row)))\n    values.sort(key=itemgetter(0))\n    self.trait_set(selected_row_indices=[v[0] for v in values], trait_change_notify=False)\n    rows = [v[1] for v in values]\n    self.setx(selected_rows=rows)\n    self._update_toolbar((len(values) > 0))\n    self.ui.evaluate(self.factory.on_select, rows)\n", "label": 1}
{"function": "\n\ndef __unicode__(self):\n    qs = ['DELETE']\n    if self.fields:\n        qs += [', '.join(['{0}'.format(f) for f in self.fields])]\n    qs += ['FROM', self.table]\n    delete_option = []\n    if self.timestamp:\n        delete_option += ['TIMESTAMP {0}'.format(self.timestamp_normalized)]\n    if delete_option:\n        qs += [' USING {0} '.format(' AND '.join(delete_option))]\n    if self.where_clauses:\n        qs += [self._where]\n    if self.conditionals:\n        qs += [self._get_conditionals()]\n    if self.if_exists:\n        qs += ['IF EXISTS']\n    return ' '.join(qs)\n", "label": 1}
{"function": "\n\ndef validate_timestamp_and_nonce(self, client_key, timestamp, nonce, request_token=None, access_token=None):\n    token = True\n    req_token = True\n    client = Client.find_one({\n        'client_key': client_key,\n    })\n    if client:\n        nonce = Nonce.find_one({\n            'nonce': nonce,\n            'timestamp': timestamp,\n            'client_id': client['_id'],\n        })\n        if nonce:\n            if request_token:\n                req_token = RequestToken.find_one({\n                    '_id': nonce['request_token_id'],\n                    'token': request_token,\n                })\n            if access_token:\n                token = RequestToken.find_one({\n                    '_id': nonce['request_token_id'],\n                    'token': access_token,\n                })\n    return (token and req_token and (nonce != None))\n", "label": 1}
{"function": "\n\ndef create_container(id_range=range(1, 255), subnet='10.0.1', os_template=None, description=None):\n    '\\n    Calls vzctl create  to create a new OpenVZ container with an id in the\\n    given range. Iff an available id could not be found a RuntimeError is\\n    raised.\\n\\n    The id of the newly created container is returned.\\n\\n    This function blocks until the container is fully created.\\n\\n    '\n    containers = set(get_containers())\n    for i in id_range:\n        if (i not in containers):\n            id = i\n            break\n    else:\n        raise RuntimeError(('Could not find availableVM ID in permissable range [%s, %s].' % config.getint('CreateContainer', 'idmin')))\n    parameters = []\n    if (subnet != None):\n        parameters += ['--ipadd', ((subnet + '.') + str(id))]\n    if (os_template != None):\n        parameters += ['--ostemplate', os_template]\n    if (description != None):\n        parameters += ['--description', description]\n    run_vzctl((['create', str(id)] + parameters))\n    return id\n", "label": 1}
{"function": "\n\ndef _pull_tenant_topology_from_db(self, tenant_id, lport_id):\n    switches = self.nb_api.get_all_logical_switches(tenant_id)\n    for switch in switches:\n        self.controller.logical_switch_updated(switch)\n    sg_groups = self.nb_api.get_security_groups(tenant_id)\n    for sg_group in sg_groups:\n        self.controller.security_group_updated(sg_group)\n    ports = self.nb_api.get_all_logical_ports(tenant_id)\n    for port in ports:\n        if (port.get_id() == lport_id):\n            continue\n        self.controller.logical_port_updated(port)\n    routers = self.nb_api.get_routers(tenant_id)\n    for router in routers:\n        self.controller.router_updated(router)\n    floating_ips = self.nb_api.get_floatingips(tenant_id)\n    for floating_ip in floating_ips:\n        self.controller.floatingip_updated(floating_ip)\n", "label": 1}
{"function": "\n\ndef InstallLibrary(name, version, explicit=True):\n    'Install a package.\\n\\n  If the installation is explicit then the user made the installation request,\\n  not a package as a dependency. Explicit installation leads to stricter\\n  version checking.\\n\\n  Args:\\n    name: Name of the requested package (already validated as available).\\n    version: The desired version (already validated as available).\\n    explicit: Explicitly requested by the user or implicitly because of a\\n      dependency.\\n  '\n    (installed_version, explicitly_installed) = installed.get(name, ([None] * 2))\n    if (name in sys.modules):\n        if explicit:\n            CheckInstalledVersion(name, version, explicit=True)\n        return\n    elif installed_version:\n        if (version == installed_version):\n            return\n        if explicit:\n            if explicitly_installed:\n                raise ValueError(('%s %s requested, but %s already in use' % (name, version, installed_version)))\n            RemoveLibrary(name)\n        else:\n            version_ob = distutils.version.LooseVersion(version)\n            installed_ob = distutils.version.LooseVersion(installed_version)\n            if (version_ob <= installed_ob):\n                return\n            else:\n                RemoveLibrary(name)\n    AddLibrary(name, version, explicit)\n    dep_details = PACKAGES[name][1][version]\n    if (not dep_details):\n        return\n    for (dep_name, dep_version) in dep_details:\n        InstallLibrary(dep_name, dep_version, explicit=False)\n", "label": 1}
{"function": "\n\n@register.inclusion_tag('core/includes/confirm_delete_form.html', takes_context=True)\ndef delete_form(context, *args, **kwargs):\n    action = (args[0] if (len(args) > 0) else kwargs.get('action'))\n    method = (args[1] if (len(args) > 1) else kwargs.get('method'))\n    form = context.get('form')\n    display_object = kwargs.get('object', context.get('object'))\n    if (action is None):\n        raise TemplateSyntaxError('delete_form template tag requires at least one argument: action, which is a URL.')\n    if (display_object is None):\n        raise TemplateSyntaxError('display_form needs object manually specified in this case.')\n    if hasattr(display_object, 'name'):\n        obj_title = display_object.name\n    elif hasattr(display_object, 'title'):\n        obj_title = display_object.title.title()\n    else:\n        obj_title = str(display_object)\n    obj_type = kwargs.get('obj_type', display_object._meta.verbose_name.title())\n    return {\n        'action': action,\n        'form': form,\n        'method': method,\n        'object': display_object,\n        'obj_title': obj_title,\n        'obj_type': obj_type,\n    }\n", "label": 1}
{"function": "\n\ndef launchFile(path, line=(- 1), col1=(- 1), col2=(- 1)):\n    if (sys.platform == 'darwin'):\n        cleanPath = os.path.abspath(path)\n        if os.path.isdir(cleanPath):\n            os.system(('open %s' % cleanPath))\n        elif os.path.isfile(cleanPath):\n            if (line >= 0):\n                os.system(('mate -l %s \"%s\"' % (line, cleanPath)))\n            else:\n                os.system(('mate \"%s\"' % cleanPath))\n    elif (sys.platform == 'win32'):\n        cleanPath = os.path.abspath(path)\n        if os.path.isdir(cleanPath):\n            os.system(('explorer %s' % cleanPath))\n        elif os.path.isfile(cleanPath):\n            scriptPath = 'editMSVC.py'\n            command = ('%s \"%s\" %s %s %s' % (scriptPath, cleanPath, line, col1, col2))\n            os.system(command)\n", "label": 1}
{"function": "\n\n@classmethod\ndef create(cls, host=None, binary=None, topic=None, manager=None, report_interval=None, periodic_interval=None, periodic_fuzzy_delay=None):\n    \"Instantiates class and passes back application object.\\n\\n        :param host: defaults to CONF.host\\n        :param binary: defaults to basename of executable\\n        :param topic: defaults to bin_name - 'neutron-' part\\n        :param manager: defaults to CONF.<topic>_manager\\n        :param report_interval: defaults to CONF.report_interval\\n        :param periodic_interval: defaults to CONF.periodic_interval\\n        :param periodic_fuzzy_delay: defaults to CONF.periodic_fuzzy_delay\\n\\n        \"\n    if (not host):\n        host = CONF.host\n    if (not binary):\n        binary = os.path.basename(inspect.stack()[(- 1)][1])\n    if (not topic):\n        topic = binary.rpartition('neutron-')[2]\n        topic = topic.replace('-', '_')\n    if (not manager):\n        manager = CONF.get(('%s_manager' % topic), None)\n    if (report_interval is None):\n        report_interval = CONF.report_interval\n    if (periodic_interval is None):\n        periodic_interval = CONF.periodic_interval\n    if (periodic_fuzzy_delay is None):\n        periodic_fuzzy_delay = CONF.periodic_fuzzy_delay\n    service_obj = cls(host, binary, topic, manager, report_interval=report_interval, periodic_interval=periodic_interval, periodic_fuzzy_delay=periodic_fuzzy_delay)\n    return service_obj\n", "label": 1}
{"function": "\n\ndef squash_unicode(obj):\n    'coerce unicode back to bytestrings.'\n    if isinstance(obj, dict):\n        for key in list(obj.keys()):\n            obj[key] = squash_unicode(obj[key])\n            if isinstance(key, str):\n                obj[squash_unicode(key)] = obj.pop(key)\n    elif isinstance(obj, list):\n        for (i, v) in enumerate(obj):\n            obj[i] = squash_unicode(v)\n    elif isinstance(obj, str):\n        obj = obj.encode('utf8')\n    return obj\n", "label": 1}
{"function": "\n\ndef display_time(seconds):\n    minute = 60\n    hour = (minute * 60)\n    day = (hour * 24)\n    month = (day * 31)\n    year = (month * 12)\n    century = (year * 100)\n    if (seconds < minute):\n        return 'instant'\n    elif (seconds < hour):\n        return (str((1 + math.ceil((seconds / minute)))) + ' minutes')\n    elif (seconds < day):\n        return (str((1 + math.ceil((seconds / hour)))) + ' hours')\n    elif (seconds < month):\n        return (str((1 + math.ceil((seconds / day)))) + ' days')\n    elif (seconds < year):\n        return (str((1 + math.ceil((seconds / month)))) + ' months')\n    elif (seconds < century):\n        return (str((1 + math.ceil((seconds / year)))) + ' years')\n    else:\n        return 'centuries'\n", "label": 1}
{"function": "\n\ndef repeatable_expr_SA(parser, node, children):\n    expr = children[0]\n    rule = expr\n    if (len(children) > 1):\n        repeat_op = children[1]\n        if (len(repeat_op) > 1):\n            (repeat_op, modifiers) = repeat_op\n        else:\n            repeat_op = repeat_op[0]\n            modifiers = None\n        if (repeat_op == '?'):\n            rule = Optional(nodes=[expr])\n        elif (repeat_op == '*'):\n            rule = ZeroOrMore(nodes=[expr])\n        else:\n            rule = OneOrMore(nodes=[expr])\n        if modifiers:\n            (modifiers, position) = modifiers\n            if (repeat_op == '?'):\n                (line, col) = parser.pos_to_linecol(position)\n                raise TextXSyntaxError('Modifiers are not allowed for \"?\" operator at {}'.format(text((line, col))), line, col)\n            if ('sep' in modifiers):\n                sep = modifiers['sep']\n                rule = Sequence(nodes=[expr, ZeroOrMore(nodes=[Sequence(nodes=[sep, expr])])])\n                if (repeat_op == '*'):\n                    rule = Optional(nodes=[rule])\n            if ('eolterm' in modifiers):\n                rule.eolterm = True\n    return rule\n", "label": 1}
{"function": "\n\ndef formfield_for_dbfield(self, db_field, **kwargs):\n    \"\\n        Hook for specifying the form Field instance for a given database Field\\n        instance.\\n\\n        If kwargs are given, they're passed to the form Field's constructor.\\n        \"\n    request = kwargs.pop('request', None)\n    if db_field.choices:\n        return self.formfield_for_choice_field(db_field, request, **kwargs)\n    if isinstance(db_field, (models.ForeignKey, models.ManyToManyField)):\n        if (db_field.__class__ in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[db_field.__class__], **kwargs)\n        if isinstance(db_field, models.ForeignKey):\n            formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)\n        elif isinstance(db_field, models.ManyToManyField):\n            formfield = self.formfield_for_manytomany(db_field, request, **kwargs)\n        if (formfield and (db_field.name not in self.raw_id_fields)):\n            formfield.widget = widgets.RelatedFieldWidgetWrapper(formfield.widget, db_field.rel, self.admin_site)\n        return formfield\n    for klass in db_field.__class__.mro():\n        if (klass in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[klass], **kwargs)\n            return db_field.formfield(**kwargs)\n    return db_field.formfield(**kwargs)\n", "label": 1}
{"function": "\n\ndef _mangle_prefix_result(self, res):\n    ' Mangle prefix result for easier testing\\n\\n            We can never predict the values of things like the ID (okay, that\\n            one is actually kind of doable) or the added and last_modified\\n            timestamp. This function will make sure the values are present but\\n            then strip them to make it easier to test against an expected\\n            result.\\n        '\n    if isinstance(res, list):\n        for p in res:\n            self.assertIn('added', p)\n            self.assertIn('last_modified', p)\n            del p['added']\n            del p['last_modified']\n            del p['total_addresses']\n            del p['used_addresses']\n            del p['free_addresses']\n    elif (isinstance(res, dict) and ('result' in res)):\n        for p in res['result']:\n            self.assertIn('added', p)\n            self.assertIn('last_modified', p)\n            del p['added']\n            del p['last_modified']\n            del p['total_addresses']\n            del p['used_addresses']\n            del p['free_addresses']\n    elif isinstance(res, dict):\n        self.assertIn('added', p)\n        self.assertIn('last_modified', p)\n        del p['added']\n        del p['last_modified']\n        del res['total_addresses']\n        del res['used_addresses']\n        del res['free_addresses']\n    return res\n", "label": 1}
{"function": "\n\ndef _cache_db_tables_iterator(tables, cache_alias, db_alias):\n    no_tables = (not tables)\n    cache_aliases = (settings.CACHES if (cache_alias is None) else (cache_alias,))\n    db_aliases = (settings.DATABASES if (db_alias is None) else (db_alias,))\n    for db_alias in db_aliases:\n        if no_tables:\n            tables = connections[db_alias].introspection.table_names()\n        if tables:\n            for cache_alias in cache_aliases:\n                (yield (cache_alias, db_alias, tables))\n", "label": 1}
{"function": "\n\ndef yield_sentences(self):\n    test_file = io.open(self.test_file, 'r').read()\n    inst2ans = self.get_answers()\n    for text in bsoup(test_file).findAll('text'):\n        if (not text):\n            continue\n        textid = text['id']\n        context_doc = ' '.join([remove_tags(i) for i in str(text).split('\\n') if remove_tags(i)])\n        for sent in text.findAll('sentence'):\n            context_sent = ' '.join([remove_tags(i) for i in str(sent).split('\\n') if remove_tags(i)])\n            (yield (sent, context_sent, context_doc, inst2ans, textid))\n", "label": 1}
{"function": "\n\ndef _group_matching(tlist, start_ttype, start_value, end_ttype, end_value, cls, include_semicolon=False, recurse=False):\n\n    def _find_matching(i, tl, stt, sva, ett, eva):\n        depth = 1\n        for n in range(i, len(tl.tokens)):\n            t = tl.tokens[n]\n            if t.match(stt, sva):\n                depth += 1\n            elif t.match(ett, eva):\n                depth -= 1\n                if (depth == 1):\n                    return t\n        return None\n    [_group_matching(sgroup, start_ttype, start_value, end_ttype, end_value, cls, include_semicolon) for sgroup in tlist.get_sublists() if recurse]\n    if isinstance(tlist, cls):\n        idx = 1\n    else:\n        idx = 0\n    token = tlist.token_next_match(idx, start_ttype, start_value)\n    while token:\n        tidx = tlist.token_index(token)\n        end = _find_matching(tidx, tlist, start_ttype, start_value, end_ttype, end_value)\n        if (end is None):\n            idx = (tidx + 1)\n        else:\n            if include_semicolon:\n                next_ = tlist.token_next(tlist.token_index(end))\n                if (next_ and next_.match(T.Punctuation, ';')):\n                    end = next_\n            group = tlist.group_tokens(cls, tlist.tokens_between(token, end))\n            _group_matching(group, start_ttype, start_value, end_ttype, end_value, cls, include_semicolon)\n            idx = (tlist.token_index(group) + 1)\n        token = tlist.token_next_match(idx, start_ttype, start_value)\n", "label": 1}
{"function": "\n\ndef _k_modes_iter(X, centroids, cl_attr_freq, membship):\n    'Single iteration of k-modes clustering algorithm'\n    moves = 0\n    for (ipoint, curpoint) in enumerate(X):\n        clust = np.argmin(matching_dissim(centroids, curpoint))\n        if membship[(clust, ipoint)]:\n            continue\n        moves += 1\n        old_clust = np.argwhere(membship[:, ipoint])[0][0]\n        (cl_attr_freq, membship) = move_point_cat(curpoint, ipoint, clust, old_clust, cl_attr_freq, membship)\n        for iattr in range(len(curpoint)):\n            for curc in (clust, old_clust):\n                centroids[(curc, iattr)] = get_max_value_key(cl_attr_freq[curc][iattr])\n        if (sum(membship[old_clust, :]) == 0):\n            from_clust = membship.sum(axis=1).argmax()\n            choices = [ii for (ii, ch) in enumerate(membship[from_clust, :]) if ch]\n            rindx = np.random.choice(choices)\n            (cl_attr_freq, membship) = move_point_cat(X[rindx], rindx, old_clust, from_clust, cl_attr_freq, membship)\n    return (centroids, moves)\n", "label": 1}
{"function": "\n\ndef _get_ssl_kwargs(ssl=False, ssl_keyfile=None, ssl_certfile=None, ssl_cert_reqs=None, ssl_ca_certs=None, ssl_match_hostname=True):\n    ssl_kwargs = {\n        'ssl': ssl,\n    }\n    if ssl_keyfile:\n        ssl_kwargs['ssl'] = True\n        ssl_kwargs['ssl_keyfile'] = ssl_keyfile\n    if ssl_certfile:\n        ssl_kwargs['ssl'] = True\n        ssl_kwargs['ssl_certfile'] = ssl_certfile\n    if ssl_cert_reqs:\n        if (ssl_cert_reqs is 'none'):\n            ssl_cert_reqs = ssl_lib.CERT_NONE\n        elif (ssl_cert_reqs is 'optional'):\n            ssl_cert_reqs = ssl_lib.CERT_OPTIONAL\n        elif (ssl_cert_reqs is 'required'):\n            ssl_cert_reqs = ssl_lib.CERT_REQUIRED\n        ssl_kwargs['ssl_cert_reqs'] = ssl_cert_reqs\n    if ssl_ca_certs:\n        ssl_kwargs['ssl'] = True\n        ssl_kwargs['ssl_ca_certs'] = ssl_ca_certs\n    if ssl_kwargs.get('ssl', False):\n        ssl_kwargs['ssl_match_hostname'] = ssl_match_hostname\n    return ssl_kwargs\n", "label": 1}
{"function": "\n\ndef test_NOT_purge_duplicate_comments(self):\n    self.explore(self.traj)\n    self.traj.f_get('purge_duplicate_comments').f_unlock()\n    self.traj.hdf5.purge_duplicate_comments = 0\n    self.traj.f_get('results_summary').f_unlock()\n    self.traj.overview.results_summary = 0\n    self.make_run()\n    hdf5file = pt.openFile(self.filename, mode='a')\n    ncomments = {\n        \n    }\n    try:\n        traj_group = hdf5file.getNode(where='/', name=self.traj.v_name)\n        for node in traj_group._f_walkGroups():\n            if (('/derived_parameters/' in node._v_pathname) or ('/results/' in node._v_pathname)):\n                if ('SRVC_LEAF' in node._v_attrs):\n                    if ('SRVC_INIT_COMMENT' in node._v_attrs):\n                        comment = node._v_attrs['SRVC_INIT_COMMENT']\n                        if (comment not in ncomments):\n                            ncomments[comment] = 0\n                        ncomments[comment] += 1\n    finally:\n        hdf5file.close()\n    self.assertGreaterEqual(len(ncomments), 1)\n    self.assertTrue(any(((x > 1) for x in ncomments.values())))\n", "label": 1}
{"function": "\n\ndef convertObjectArgs(self, objectArgs):\n    if isinstance(objectArgs, list):\n        return [self.convertObjectArgs(x) for x in objectArgs]\n    if isinstance(objectArgs, dict):\n        if ('objectDefinition_' in objectArgs):\n            obj = self.extractObjectDefinition(objectArgs['objectDefinition_'])\n            if ('objectId_' in objectArgs):\n                self.incomingObjectCache.addObjectById(objectArgs['objectId_'], obj)\n            return obj\n        if ('objectId_' in objectArgs):\n            obj = self.incomingObjectCache.lookupObjectById(objectArgs['objectId_'])\n            return obj\n        tr = {\n            \n        }\n        for (k, v) in objectArgs.iteritems():\n            tr[k] = self.convertObjectArgs(v)\n        return tr\n    return objectArgs\n", "label": 1}
{"function": "\n\n@classmethod\ndef _generate_jar_template(cls, jars):\n    global_dep_attributes = set((Dependency(org=jar.org, name=jar.name, rev=jar.rev, mutable=jar.mutable, force=jar.force, transitive=jar.transitive) for jar in jars))\n    if (len(global_dep_attributes) != 1):\n        conflicting_dependencies = sorted((str(g) for g in global_dep_attributes))\n        raise cls.IvyResolveConflictingDepsError('Found conflicting dependencies:\\n\\t{}'.format('\\n\\t'.join(conflicting_dependencies)))\n    jar_attributes = global_dep_attributes.pop()\n    excludes = set()\n    for jar in jars:\n        excludes.update(jar.excludes)\n    any_have_url = False\n    artifacts = OrderedDict()\n    for jar in jars:\n        ext = jar.ext\n        url = jar.url\n        if url:\n            any_have_url = True\n        classifier = jar.classifier\n        artifact = Artifact(name=jar.name, type_=(ext or 'jar'), ext=ext, url=url, classifier=classifier)\n        artifacts[(ext, url, classifier)] = artifact\n    template = TemplateData(org=jar_attributes.org, module=jar_attributes.name, version=jar_attributes.rev, mutable=jar_attributes.mutable, force=jar_attributes.force, transitive=jar_attributes.transitive, artifacts=artifacts.values(), any_have_url=any_have_url, excludes=[cls._generate_exclude_template(exclude) for exclude in excludes])\n    return template\n", "label": 1}
{"function": "\n\ndef vsphere(account):\n    myCredAccount = CredAccountVSphere()\n    if (not ('hostname' in account)):\n        printer.out('hostname in vcenter account not found', printer.ERROR)\n        return\n    if (not ('username' in account)):\n        printer.out('username in vcenter account not found', printer.ERROR)\n        return\n    if (not ('password' in account)):\n        printer.out('password in vcenter account not found', printer.ERROR)\n        return\n    if (not ('name' in account)):\n        printer.out('name in vcenter account not found', printer.ERROR)\n        return\n    if ('proxyHostname' in account):\n        myCredAccount.proxyHost = account['proxyHostname']\n    if ('proxyPort' in account):\n        myCredAccount.proxyPort = account['proxyPort']\n    if ('port' in account):\n        port = int(account['port'])\n    else:\n        port = 443\n    myCredAccount.name = account['name']\n    myCredAccount.login = account['username']\n    myCredAccount.password = account['password']\n    myCredAccount.hostname = account['hostname']\n    myCredAccount.port = port\n    return myCredAccount\n", "label": 1}
{"function": "\n\ndef set_job_lock(self, command_handler, lock):\n    \"Set a worker level job lock so we don't try to hold onto 2 jobs at anytime\"\n    if (command_handler not in self.handler_to_connection_map):\n        return False\n    failed_lock = bool((lock and (self.command_handler_holding_job_lock is not None)))\n    failed_unlock = bool(((not lock) and (self.command_handler_holding_job_lock != command_handler)))\n    if (failed_lock or failed_unlock):\n        return False\n    if lock:\n        self.command_handler_holding_job_lock = command_handler\n    else:\n        self.command_handler_holding_job_lock = None\n    return True\n", "label": 1}
{"function": "\n\ndef makeService(config):\n    s = service.MultiService()\n    if config['root']:\n        root = config['root']\n        if config['indexes']:\n            config['root'].indexNames = config['indexes']\n    else:\n        root = demo.Test()\n    if isinstance(root, static.File):\n        root.registry.setComponent(interfaces.IServiceCollection, s)\n    if config['logfile']:\n        site = server.Site(root, logPath=config['logfile'])\n    else:\n        site = server.Site(root)\n    site.displayTracebacks = (not config['notracebacks'])\n    if config['personal']:\n        personal = strports.service(config['port'], makePersonalServerFactory(site))\n        personal.setServiceParent(s)\n    else:\n        if config['https']:\n            from twisted.internet.ssl import DefaultOpenSSLContextFactory\n            i = internet.SSLServer(int(config['https']), site, DefaultOpenSSLContextFactory(config['privkey'], config['certificate']))\n            i.setServiceParent(s)\n        strports.service(config['port'], site).setServiceParent(s)\n    return s\n", "label": 1}
{"function": "\n\ndef as_sql(self, compiler, connection):\n    if isinstance(self.lhs, MultiColSource):\n        from django.db.models.sql.where import WhereNode, SubqueryConstraint, AND, OR\n        root_constraint = WhereNode(connector=OR)\n        if self.rhs_is_direct_value():\n            values = [get_normalized_value(value, self.lhs) for value in self.rhs]\n            for value in values:\n                value_constraint = WhereNode()\n                for (source, target, val) in zip(self.lhs.sources, self.lhs.targets, value):\n                    lookup_class = target.get_lookup('exact')\n                    lookup = lookup_class(target.get_col(self.lhs.alias, source), val)\n                    value_constraint.add(lookup, AND)\n                root_constraint.add(value_constraint, OR)\n        else:\n            root_constraint.add(SubqueryConstraint(self.lhs.alias, [target.column for target in self.lhs.targets], [source.name for source in self.lhs.sources], self.rhs), AND)\n        return root_constraint.as_sql(compiler, connection)\n    else:\n        return super(RelatedIn, self).as_sql(compiler, connection)\n", "label": 1}
{"function": "\n\ndef __init__(self, string):\n    chunks = string.split('\\x1b')\n    self.chars = []\n    chars = list(chunks[0])\n    if (len(chunks) > 1):\n        for chunk in chunks[1:]:\n            if (chunk == '(B'):\n                chars.append(('\\x1b' + chunk))\n            else:\n                p = chunk.find('m')\n                if (p > 0):\n                    chars.append(('\\x1b' + chunk[:(p + 1)]))\n                    chars.extend(list(chunk[(p + 1):]))\n                else:\n                    chars.extend(list(chunk))\n    ansi = []\n    for char in chars:\n        if (char[0] == '\\x1b'):\n            ansi.append(char)\n        else:\n            self.chars.append((''.join(ansi) + char))\n            ansi = []\n    if (len(self.chars) > 2):\n        if (self.chars[(- 1)][0] == '\\x1b'):\n            self.chars[(- 2)] = (self.chars[(- 2)] + self.chars[(- 1)])\n            self.chars = self.chars[:(- 1)]\n", "label": 1}
{"function": "\n\ndef constant_time_compare(val1, val2):\n    '\\n    Returns True if the two strings are equal, False otherwise.\\n    The time taken is independent of the number of characters that match.\\n    This code was borrowed from Django 1.5.4-final\\n    '\n    if (len(val1) != len(val2)):\n        return False\n    result = 0\n    if (six.PY3 and isinstance(val1, bytes) and isinstance(val2, bytes)):\n        for (x, y) in zip(val1, val2):\n            result |= (x ^ y)\n    else:\n        for (x, y) in zip(val1, val2):\n            result |= (ord(x) ^ ord(y))\n    return (result == 0)\n", "label": 1}
{"function": "\n\n@valign.setter\ndef valign(self, val):\n    if (not self._field_names):\n        self._valign = {\n            \n        }\n    elif ((val is None) or (isinstance(val, dict) and (len(val) is 0))):\n        for field in self._field_names:\n            self._valign[field] = 't'\n    else:\n        self._validate_valign(val)\n        for field in self._field_names:\n            self._valign[field] = val\n", "label": 1}
{"function": "\n\n@block1.setter\ndef block1(self, value):\n    '\\n        Set the Block1 option.\\n\\n        :param value: the Block1 value\\n        '\n    option = Option()\n    option.number = defines.OptionRegistry.BLOCK1.number\n    (num, m, size) = value\n    if (size > 512):\n        szx = 6\n    elif (256 < size <= 512):\n        szx = 5\n    elif (128 < size <= 256):\n        szx = 4\n    elif (64 < size <= 128):\n        szx = 3\n    elif (32 < size <= 64):\n        szx = 2\n    elif (16 < size <= 32):\n        szx = 1\n    else:\n        szx = 0\n    value = (num << 4)\n    value |= (m << 3)\n    value |= szx\n    option.value = value\n    self.add_option(option)\n", "label": 1}
{"function": "\n\ndef decode_json(payload):\n    if isinstance(payload, dict):\n        decoded = {\n            \n        }\n        for key in payload.keys():\n            decoded[key] = decode_json(payload[key])\n        return decoded\n    elif isinstance(payload, list):\n        decoded = []\n        for element in payload:\n            decoded.append(decode_json(element))\n        return decoded\n    elif isinstance(payload, str):\n        element = decode_xml(decode_url(payload))\n        try:\n            return decode_json(json.loads(element))\n        except:\n            return element\n    else:\n        return payload\n", "label": 1}
{"function": "\n\ndef GetNormalizedLibraries(self):\n    'Returns a list of normalized Library instances for this configuration.\\n\\n    Returns:\\n      The list of active Library instances for this configuration. This includes\\n      directly-specified libraries, their required dependencies as well as any\\n      libraries enabled by default. Any libraries with \"latest\" as their version\\n      will be replaced with the latest available version.\\n    '\n    libraries = self.GetAllLibraries()\n    enabled_libraries = set((library.name for library in libraries))\n    for library in _SUPPORTED_LIBRARIES:\n        if (library.default_version and (library.name not in enabled_libraries)):\n            libraries.append(Library(name=library.name, version=library.default_version))\n    for library in libraries:\n        if (library.version == 'latest'):\n            library.version = _NAME_TO_SUPPORTED_LIBRARY[library.name].supported_versions[(- 1)]\n    return libraries\n", "label": 1}
{"function": "\n\ndef test_edit_strings(self):\n    '\\n        Check access to view lotte for a resource in a private project\\n        '\n    URL = self.urls['translate_private']\n    for user in ['anonymous']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 302)\n    for user in ['registered']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 403)\n    for user in ['maintainer', 'team_coordinator', 'team_member']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 200)\n    URL = reverse('translate_resource', kwargs={\n        'project_slug': self.project_private.slug,\n        'resource_slug': self.resource_private.slug,\n        'lang_code': self.language_ar.code,\n    })\n    for user in ['anonymous']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 302)\n    for user in ['registered']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 403)\n    for user in ['team_coordinator', 'team_member']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 403)\n    for user in ['maintainer']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 200)\n", "label": 1}
{"function": "\n\ndef add(self, regions, forward, join):\n    total_bytes = sum((len(c) for c in regions))\n    if (total_bytes == 0):\n        return\n    index = self.index\n    try:\n        if (not join):\n            if (self.entries[index] and self.entries[index].same_as(regions)):\n                return\n        elif (self.entries[index] and self.entries[index].join_if_possible(regions, forward)):\n            return\n        index = ((index + 1) % self.KILL_RING_SIZE)\n        self.entries[index] = KillRing.Kill(regions)\n    finally:\n        self.index = index\n        self.entries[index].set_clipboard()\n", "label": 1}
{"function": "\n\ndef _redefine_external_snapshot(self, name=None):\n    snapshot = self._get_snapshot(name)\n    logger.info('Revert {0} ({1}) from external snapshot {2}'.format(self.name, snapshot.state, name))\n    self.destroy()\n    xml_domain = snapshot._xml_tree.find('domain')\n    if (snapshot.children_num == 0):\n        domain_disks = xml_domain.findall('./devices/disk')\n        for (s_disk, s_disk_data) in snapshot.disks.items():\n            for d_disk in domain_disks:\n                d_disk_dev = d_disk.find('target').get('dev')\n                d_disk_device = d_disk.get('device')\n                if ((d_disk_dev == s_disk) and (d_disk_device == 'disk')):\n                    d_disk.find('source').set('file', s_disk_data)\n    if (snapshot.state == 'shutoff'):\n        self.driver.conn.defineXML(ET.tostring(xml_domain))\n    else:\n        self.driver.conn.restoreFlags(snapshot.memory_file, dxml=ET.tostring(xml_domain), flags=libvirt.VIR_DOMAIN_SAVE_PAUSED)\n    self.set_snapshot_current(name)\n", "label": 1}
{"function": "\n\ndef body_languages(self, msg):\n    \"\\n        Find the language related headers in the message and return a\\n        string suitable for the 'body language' element of a\\n        bodystructure reply.\\n\\n        Arguments:\\n        - `msg`: the message we are looking in..\\n        \"\n    langs = []\n    for (hdr, value) in msg.items():\n        if (hdr[(- 9):].lower() != '-language'):\n            continue\n        if (',' in value):\n            langs.extend(value.split(','))\n        elif (';' in value):\n            langs.extend(value.split(';'))\n        else:\n            langs.append(value)\n    if (len(langs) == 0):\n        return 'NIL'\n    elif (len(langs) == 1):\n        return ('\"%s\"' % langs[0])\n    else:\n        return ('(%s)' % ' '.join([str(x).strip() for x in langs]))\n", "label": 1}
{"function": "\n\ndef _get_matches(self, key_presses):\n    '\\n        For a list of :class:`KeyPress` instances. Give the matching handlers\\n        that would handle this.\\n        '\n    keys = tuple((k.key for k in key_presses))\n    cli = self._cli_ref()\n    with_mode = [b for b in self._registry.get_bindings_for_keys(keys) if b.filter(cli)]\n    if with_mode:\n        return with_mode\n    keys_any = tuple((keys[:(- 1)] + (Keys.Any,)))\n    with_mode_any = [b for b in self._registry.get_bindings_for_keys(keys_any) if b.filter(cli)]\n    if with_mode_any:\n        return with_mode_any\n    return []\n", "label": 1}
{"function": "\n\ndef insertCustomRoles(self):\n    self.showStatus('insert role types')\n    roleTypesByIds = dict((((self.documentIds[roleType.modelDocument.uri], self.uriId[roleType.roleURI]), roleType) for roleTypes in self.modelXbrl.roleTypes.values() for roleType in roleTypes if (roleType.modelDocument.uri not in self.existingDocumentIds)))\n    table = self.getTable('custom_role_type', 'custom_role_type_id', ('document_id', 'uri_id', 'definition'), ('document_id', 'uri_id'), tuple(((roleTypeIDs[0], roleTypeIDs[1], roleType.definition) for (roleTypeIDs, roleType) in roleTypesByIds.items())))\n    table = self.getTable('custom_role_used_on', 'custom_role_used_on_id', ('custom_role_type_id', 'qname_id'), ('custom_role_type_id', 'qname_id'), tuple(((id, self.qnameId[usedOn]) for (id, docid, uriid) in table for usedOn in roleTypesByIds[(docid, uriid)].usedOns)))\n", "label": 1}
{"function": "\n\ndef file_filter(state, dirname, fnames):\n    if (args.dir_masks and (not any([re.search(x, dirname) for x in args.dir_masks]))):\n        return\n    for f in fnames:\n        p = os.path.abspath(os.path.join(os.path.realpath(dirname), f))\n        if (any([re.search(x, f) for x in args.file_masks]) or any([re.search(x, p) for x in args.path_masks])):\n            if os.path.isfile(p):\n                state['files'].append(p)\n", "label": 1}
{"function": "\n\n@wsgi.extends\ndef detail(self, req, resp_obj):\n    context = req.environ['nova.context']\n    authorize_extend = False\n    authorize_host_status = False\n    if authorize(context):\n        authorize_extend = True\n    if (api_version_request.is_supported(req, min_version='2.16') and soft_authorize(context, action='show:host_status')):\n        authorize_host_status = True\n    if (authorize_extend or authorize_host_status):\n        servers = list(resp_obj.obj['servers'])\n        instances = req.get_db_instances()\n        if authorize_host_status:\n            host_statuses = self.compute_api.get_instances_host_statuses(instances.values())\n        for server in servers:\n            if authorize_extend:\n                instance = instances[server['id']]\n                self._extend_server(context, server, instance, req)\n            if authorize_host_status:\n                server['host_status'] = host_statuses[server['id']]\n", "label": 1}
{"function": "\n\ndef test_main():\n    tests = []\n    test_suite = unittest.TestSuite()\n    tests.append(TestCase)\n    tests.append(TestFetchAllProcesses)\n    if POSIX:\n        from _posix import PosixSpecificTestCase\n        tests.append(PosixSpecificTestCase)\n    if LINUX:\n        from _linux import LinuxSpecificTestCase as stc\n    elif WINDOWS:\n        from _windows import WindowsSpecificTestCase as stc\n        from _windows import TestDualProcessImplementation\n        tests.append(TestDualProcessImplementation)\n    elif OSX:\n        from _osx import OSXSpecificTestCase as stc\n    elif BSD:\n        from _bsd import BSDSpecificTestCase as stc\n    tests.append(stc)\n    if hasattr(os, 'getuid'):\n        if (os.getuid() == 0):\n            tests.append(LimitedUserTestCase)\n        else:\n            atexit.register(warn, \"Couldn't run limited user tests (super-user privileges are required)\")\n    for test_class in tests:\n        test_suite.addTest(unittest.makeSuite(test_class))\n    unittest.TextTestRunner(verbosity=2).run(test_suite)\n", "label": 1}
{"function": "\n\ndef CheckRemoteGitState(self):\n    'Checks the state of the remote git repository.\\n\\n    Returns:\\n      A boolean value to indicate if the state is sane.\\n    '\n    if (self._command == 'close'):\n        if (not self._git_helper.SynchronizeWithUpstream()):\n            print('{0:s} aborted - unable to synchronize with upstream/master.'.format(self._command.title()))\n            return False\n    elif (self._command in ('create', 'update')):\n        if (not self._git_helper.CheckSynchronizedWithUpstream()):\n            if (not self._git_helper.SynchronizeWithUpstream()):\n                print('{0:s} aborted - unable to synchronize with upstream/master.'.format(self._command.title()))\n                return False\n            force_push = True\n        else:\n            force_push = False\n        if (not self._git_helper.PushToOrigin(self._active_branch, force=force_push)):\n            print('{0:s} aborted - unable to push updates to origin/{1:s}.'.format(self._command.title(), self._active_branch))\n            return False\n    elif (self._command == 'lint'):\n        self._git_helper.CheckSynchronizedWithUpstream()\n    elif (self._command == 'merge'):\n        if (not self._git_helper.SynchronizeWithOrigin()):\n            print('{0:s} aborted - unable to synchronize with origin/master.'.format(self._command.title()))\n            return False\n    return True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _validate_publish_parameters(body, exchange, immediate, mandatory, properties, routing_key):\n    'Validate Publish Parameters.\\n\\n        :param str|unicode body:\\n        :param str routing_key:\\n        :param str exchange:\\n        :param dict properties:\\n        :param bool mandatory:\\n        :param bool immediate:\\n        :raises  AMQPInvalidArgument: Invalid Parameters\\n        :return:\\n        '\n    if (not compatibility.is_string(body)):\n        raise AMQPInvalidArgument('body should be a string')\n    elif (not compatibility.is_string(routing_key)):\n        raise AMQPInvalidArgument('routing_key should be a string')\n    elif (not compatibility.is_string(exchange)):\n        raise AMQPInvalidArgument('exchange should be a string')\n    elif (properties and (not isinstance(properties, dict))):\n        raise AMQPInvalidArgument('properties should be a dict or None')\n    elif (not isinstance(mandatory, bool)):\n        raise AMQPInvalidArgument('mandatory should be a boolean')\n    elif (not isinstance(immediate, bool)):\n        raise AMQPInvalidArgument('immediate should be a boolean')\n", "label": 1}
{"function": "\n\ndef handle_form_and_email(request, form=None, form_template='', message_template='', subject='', recipients=None, redirect_to='/thanks/', sender=None, uses_captcha=True, *args, **kwargs):\n    '\\n    Abstracts the rendering and processing of e-mail forms.\\n    '\n    if uses_captcha:\n        remote_ip = request.META['REMOTE_ADDR']\n    if (request.method == 'POST'):\n        if uses_captcha:\n            form = form(remote_ip, request.POST)\n        else:\n            form = form(request.POST)\n        if form.is_valid():\n            try:\n                if form.cleaned_data['cc_self']:\n                    recipients.append(form.cleaned_data['self_email'])\n                if (sender == 'self'):\n                    sender = form.cleaned_data['self_email']\n            except:\n                pass\n            render_email_and_send(message_template=message_template, context=form.cleaned_data, subject=subject, recipients=recipients, sender=sender)\n            return HttpResponseRedirect('/thanks/')\n    elif uses_captcha:\n        form = form(remote_ip)\n    else:\n        form = form()\n    page = {\n        'form': form,\n    }\n    return render_to_response(form_template, page, context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef send_subscription_change_alert(domain, new_subscription, old_subscription, internal_change):\n    billing_account = (new_subscription.account if new_subscription else (old_subscription.account if old_subscription else None))\n    request = get_request()\n    email_context = {\n        'domain': domain,\n        'domain_url': get_default_domain_url(domain),\n        'old_plan': (old_subscription.plan_version if old_subscription else None),\n        'new_plan': (new_subscription.plan_version if new_subscription else None),\n        'old_subscription': old_subscription,\n        'new_subscription': new_subscription,\n        'billing_account': billing_account,\n        'username': (request.couch_user.username if getattr(request, 'couch_user', None) else None),\n        'referer': (request.META.get('HTTP_REFERER') if request else None),\n    }\n    email_subject = '{env}Subscription Change Alert: {domain} from {old_plan} to {new_plan}'.format(env=('[{}] '.format(settings.SERVER_ENVIRONMENT.upper()) if (settings.SERVER_ENVIRONMENT == 'staging') else ''), domain=email_context['domain'], old_plan=email_context['old_plan'], new_plan=email_context['new_plan'])\n    sub_change_email_address = (settings.INTERNAL_SUBSCRIPTION_CHANGE_EMAIL if internal_change else settings.SUBSCRIPTION_CHANGE_EMAIL)\n    send_html_email_async.delay(email_subject, sub_change_email_address, render_to_string('accounting/subscription_change_email.html', email_context), text_content=render_to_string('accounting/subscription_change_email.txt', email_context))\n", "label": 1}
{"function": "\n\ndef __init__(self, execute, name=None, provides=None, requires=None, auto_extract=True, rebind=None, revert=None, version=None, inject=None):\n    if (not six.callable(execute)):\n        raise ValueError('Function to use for executing must be callable')\n    if (revert is not None):\n        if (not six.callable(revert)):\n            raise ValueError('Function to use for reverting must be callable')\n    if (name is None):\n        name = reflection.get_callable_name(execute)\n    super(FunctorTask, self).__init__(name, provides=provides, inject=inject)\n    self._execute = execute\n    self._revert = revert\n    if (version is not None):\n        self.version = version\n    mapping = self._build_arg_mapping(execute, requires, rebind, auto_extract)\n    (self.rebind, exec_requires, self.optional) = mapping\n    if revert:\n        revert_mapping = self._build_arg_mapping(revert, requires, rebind, auto_extract)\n    else:\n        revert_mapping = (self.rebind, exec_requires, self.optional)\n    (self.revert_rebind, revert_requires, self.revert_optional) = revert_mapping\n    self.requires = exec_requires.union(revert_requires)\n", "label": 1}
{"function": "\n\ndef _handle_submit(self, *args):\n    'The SUBMIT handler'\n    job = args[1]\n    create_id = (job.job_id is None)\n    job_id = (self._next_id() if create_id else job.job_id)\n    job.job_id = job_id\n    job.stdout = self._resolve_log(job.id, job.stdout)\n    job.stderr = self._resolve_log(job.id, job.stderr)\n    self.queued[job_id] = job\n    for d in job.dependencies:\n        if (d in self.queued):\n            self.queued[d].children.append(job_id)\n        if (d in self.running):\n            self.running[d].children.append(job_id)\n    self.log.info('Master | Queue new job %s', job_id)\n    if create_id:\n        self.response.put(job_id)\n    if (self.slots_available >= 1):\n        self.schedule()\n    return True\n", "label": 1}
{"function": "\n\ndef send_frame(self, cmd, headers={\n    \n}, body=''):\n    frame = utils.Frame(cmd, headers, body)\n    if (cmd == CMD_BEGIN):\n        trans = headers[HDR_TRANSACTION]\n        if (trans in self.transactions):\n            self.notify('error', {\n                \n            }, ('Transaction %s already started' % trans))\n        else:\n            self.transactions[trans] = []\n    elif (cmd == CMD_COMMIT):\n        trans = headers[HDR_TRANSACTION]\n        if (trans not in self.transactions):\n            self.notify('error', {\n                \n            }, ('Transaction %s not started' % trans))\n        else:\n            for f in self.transactions[trans]:\n                self.transport.transmit(f)\n            del self.transactions[trans]\n    elif (cmd == CMD_ABORT):\n        trans = headers['transaction']\n        del self.transactions[trans]\n    elif ('transaction' in headers):\n        trans = headers['transaction']\n        if (trans not in self.transactions):\n            self.transport.notify('error', {\n                \n            }, ('Transaction %s not started' % trans))\n            return\n        else:\n            self.transactions[trans].append(frame)\n    else:\n        self.transport.transmit(frame)\n", "label": 1}
{"function": "\n\ndef get_query_string(self, new_params=None, remove=None):\n    if (new_params is None):\n        new_params = {\n            \n        }\n    if (remove is None):\n        remove = []\n    p = copy(self.params)\n    for r in remove:\n        for k in p.keys():\n            if k.startswith(r):\n                del p[k]\n    for (k, v) in new_params.items():\n        if (v is None):\n            if (k in p):\n                del p[k]\n        else:\n            p[k] = v\n    if hasattr(p, 'urlencode'):\n        return ('?%s' % p.urlencode())\n    return ('?%s' % urlencode(p))\n", "label": 1}
{"function": "\n\ndef minimalBases(classes):\n    'Reduce a list of base classes to its ordered minimum equivalent'\n    classes = [c for c in classes if (not _py2to3.is_old_style_class(c))]\n    candidates = []\n    for m in classes:\n        for n in classes:\n            if (issubclass(n, m) and (m is not n)):\n                break\n        else:\n            if (m in candidates):\n                candidates.remove(m)\n            candidates.append(m)\n    return candidates\n", "label": 1}
{"function": "\n\ndef check_ipv4(self, value):\n    parts = value.split('.')\n    if ((len(parts) == 4) and all((x.isdigit() for x in parts))):\n        numbers = list((int(x) for x in parts))\n        return all((((num >= 0) and (num < 256)) for num in numbers))\n    return False\n", "label": 1}
{"function": "\n\ndef __init__(self, name, fields, options=None, bases=None, managers=None):\n    self.fields = fields\n    self.options = (options or {\n        \n    })\n    self.bases = (bases or (models.Model,))\n    self.managers = (managers or [])\n    super(CreateModel, self).__init__(name)\n    _check_for_duplicates('fields', (name for (name, _) in self.fields))\n    _check_for_duplicates('bases', ((base._meta.label_lower if hasattr(base, '_meta') else (base.lower() if isinstance(base, six.string_types) else base)) for base in self.bases))\n    _check_for_duplicates('managers', (name for (name, _) in self.managers))\n", "label": 1}
{"function": "\n\n@mock.patch((MODPATH + '.BaseOpenStackService._get_meta_data'))\n@mock.patch((MODPATH + '.BaseOpenStackService.get_user_data'))\ndef _test_get_client_auth_certs(self, mock_get_user_data, mock_get_meta_data, meta_data, ret_value=None):\n    mock_get_meta_data.return_value = meta_data\n    mock_get_user_data.side_effect = [ret_value]\n    response = self._service.get_client_auth_certs()\n    mock_get_meta_data.assert_called_once_with()\n    if (isinstance(ret_value, bytes) and ret_value.startswith(x509constants.PEM_HEADER.encode())):\n        mock_get_user_data.assert_called_once_with()\n        self.assertEqual([ret_value.decode()], response)\n    elif (ret_value is base.NotExistingMetadataException):\n        self.assertFalse(response)\n    else:\n        expected = []\n        expectation = {\n            'meta': 'fake cert',\n            'keys': [key['data'].strip() for key in self._fake_keys if (key['type'] == 'x509')],\n        }\n        for (field, value) in expectation.items():\n            if (field in meta_data):\n                expected.extend((value if isinstance(value, list) else [value]))\n        self.assertEqual(sorted(list(set(expected))), sorted(response))\n", "label": 1}
{"function": "\n\ndef setup_databases(verbosity, interactive, **kwargs):\n    from django.db import connections, DEFAULT_DB_ALIAS\n    mirrored_aliases = {\n        \n    }\n    test_databases = {\n        \n    }\n    dependencies = {\n        \n    }\n    default_sig = connections[DEFAULT_DB_ALIAS].creation.test_db_signature()\n    for alias in connections:\n        connection = connections[alias]\n        if connection.settings_dict['TEST_MIRROR']:\n            mirrored_aliases[alias] = connection.settings_dict['TEST_MIRROR']\n        else:\n            item = test_databases.setdefault(connection.creation.test_db_signature(), (connection.settings_dict['NAME'], set()))\n            item[1].add(alias)\n            if ('TEST_DEPENDENCIES' in connection.settings_dict):\n                dependencies[alias] = connection.settings_dict['TEST_DEPENDENCIES']\n            elif ((alias != DEFAULT_DB_ALIAS) and (connection.creation.test_db_signature() != default_sig)):\n                dependencies[alias] = connection.settings_dict.get('TEST_DEPENDENCIES', [DEFAULT_DB_ALIAS])\n    old_names = []\n    mirrors = []\n    for (signature, (db_name, aliases)) in dependency_ordered(test_databases.items(), dependencies):\n        test_db_name = None\n        for alias in aliases:\n            connection = connections[alias]\n            if (test_db_name is None):\n                test_db_name = connection.creation.create_test_db(verbosity, autoclobber=(not interactive))\n                destroy = True\n            else:\n                connection.settings_dict['NAME'] = test_db_name\n                destroy = False\n            old_names.append((connection, db_name, destroy))\n    for (alias, mirror_alias) in mirrored_aliases.items():\n        mirrors.append((alias, connections[alias].settings_dict['NAME']))\n        connections[alias].settings_dict['NAME'] = connections[mirror_alias].settings_dict['NAME']\n    return (old_names, mirrors)\n", "label": 1}
{"function": "\n\ndef _to_volume(self, volume, node):\n    extra = {\n        'affinity_group': volume.affinity_group,\n        'os': volume.os,\n        'location': volume.location,\n        'media_link': volume.media_link,\n        'source_image_name': volume.source_image_name,\n    }\n    role_name = getattr(volume.attached_to, 'role_name', None)\n    hosted_service_name = getattr(volume.attached_to, 'hosted_service_name', None)\n    deployment_name = getattr(volume.attached_to, 'deployment_name', None)\n    if (role_name is not None):\n        extra['role_name'] = role_name\n    if (hosted_service_name is not None):\n        extra['hosted_service_name'] = hosted_service_name\n    if (deployment_name is not None):\n        extra['deployment_name'] = deployment_name\n    if node:\n        if ((role_name is not None) and (role_name == node.id)):\n            return StorageVolume(id=volume.name, name=volume.name, size=int(volume.logical_disk_size_in_gb), driver=self.connection.driver, extra=extra)\n    else:\n        return StorageVolume(id=volume.name, name=volume.name, size=int(volume.logical_disk_size_in_gb), driver=self.connection.driver, extra=extra)\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    meta = self._meta\n    pkname = meta.pk.name\n    setattr(self, pkname, kwargs.pop(pkname, None))\n    kwargs.pop(meta.pk.name, None)\n    for field in meta.scalarfields:\n        field.set_value(self, kwargs.pop(field.name, None))\n    attributes = meta.attributes\n    if args:\n        N = len(args)\n        if (N > len(attributes)):\n            raise ValueError('Too many attributes')\n        (attrs, attributes) = (attributes[:N], attributes[N:])\n        for (name, value) in zip(attrs, args):\n            setattr(self, name, value)\n    for name in attributes:\n        setattr(self, name, kwargs.pop(name, None))\n    if kwargs:\n        raise_kwargs(self, kwargs)\n", "label": 1}
{"function": "\n\n@login_and_domain_required\n@require_superuser\ndef add_forwarding_rule(request, domain, forwarding_rule_id=None):\n    forwarding_rule = None\n    if (forwarding_rule_id is not None):\n        forwarding_rule = ForwardingRule.get(forwarding_rule_id)\n        if (forwarding_rule.domain != domain):\n            raise Http404\n    if (request.method == 'POST'):\n        form = ForwardingRuleForm(request.POST)\n        if form.is_valid():\n            if (forwarding_rule is None):\n                forwarding_rule = ForwardingRule(domain=domain)\n            forwarding_rule.forward_type = form.cleaned_data.get('forward_type')\n            forwarding_rule.keyword = form.cleaned_data.get('keyword')\n            forwarding_rule.backend_id = form.cleaned_data.get('backend_id')\n            forwarding_rule.save()\n            return HttpResponseRedirect(reverse('list_forwarding_rules', args=[domain]))\n    else:\n        initial = {\n            \n        }\n        if (forwarding_rule is not None):\n            initial['forward_type'] = forwarding_rule.forward_type\n            initial['keyword'] = forwarding_rule.keyword\n            initial['backend_id'] = forwarding_rule.backend_id\n        form = ForwardingRuleForm(initial=initial)\n    context = {\n        'domain': domain,\n        'form': form,\n        'forwarding_rule_id': forwarding_rule_id,\n    }\n    return render(request, 'sms/add_forwarding_rule.html', context)\n", "label": 1}
{"function": "\n\ndef _read_unlocked(self, n=None):\n    nodata_val = b''\n    empty_values = (b'', None)\n    buf = self._read_buf\n    pos = self._read_pos\n    if ((n is None) or (n == (- 1))):\n        self._reset_read_buf()\n        chunks = [buf[pos:]]\n        current_size = 0\n        while True:\n            chunk = self.raw.read()\n            if (chunk in empty_values):\n                nodata_val = chunk\n                break\n            current_size += len(chunk)\n            chunks.append(chunk)\n        return (b''.join(chunks) or nodata_val)\n    avail = (len(buf) - pos)\n    if (n <= avail):\n        self._read_pos += n\n        return buf[pos:(pos + n)]\n    chunks = [buf[pos:]]\n    wanted = max(self.buffer_size, n)\n    while (avail < n):\n        chunk = self.raw.read(wanted)\n        if (chunk in empty_values):\n            nodata_val = chunk\n            break\n        avail += len(chunk)\n        chunks.append(chunk)\n    n = min(n, avail)\n    out = b''.join(chunks)\n    self._read_buf = out[n:]\n    self._read_pos = 0\n    return (out[:n] if out else nodata_val)\n", "label": 1}
{"function": "\n\ndef put(self, filedata, content_type, remote_path, force=False):\n    now = datetime.datetime.utcnow()\n    then = (now + datetime.timedelta(self.expiration_days))\n    expires = then.strftime('%a, %d %b %Y %H:%M:%S GMT')\n    if self.aws_prefix:\n        remote_path = ('%s/%s' % (self.aws_prefix, remote_path))\n    (hexdigest, b64digest) = mediasync.checksum(filedata)\n    raw_b64digest = b64digest\n    headers = {\n        'x-amz-acl': 'public-read',\n        'Content-Type': content_type,\n        'Expires': expires,\n        'Cache-Control': ('max-age=%d, public' % ((self.expiration_days * 24) * 3600)),\n    }\n    key = self._bucket.get_key(remote_path)\n    if (key is None):\n        key = Key(self._bucket, remote_path)\n    key_meta = (key.get_metadata('mediasync-checksum') or '')\n    s3_checksum = key_meta.replace(' ', '+')\n    if (force or (s3_checksum != raw_b64digest)):\n        key.set_metadata('mediasync-checksum', raw_b64digest)\n        key.set_contents_from_string(filedata, headers=headers, md5=(hexdigest, b64digest))\n        if (content_type in TYPES_TO_COMPRESS):\n            key = Key(self._bucket, ('%s.gzt' % remote_path))\n            filedata = mediasync.compress(filedata)\n            (hexdigest, b64digest) = mediasync.checksum(filedata)\n            headers['Content-Disposition'] = ('inline; filename=\"%sgzt\"' % remote_path.split('/')[(- 1)])\n            headers['Content-Encoding'] = 'gzip'\n            key.set_metadata('mediasync-checksum', raw_b64digest)\n            key.set_contents_from_string(filedata, headers=headers, md5=(hexdigest, b64digest))\n        return True\n", "label": 1}
{"function": "\n\ndef check_password(pw):\n    '\\n    Enforce password strength rules.\\n\\n    Returns the password if it passes, otherwises raises ``ValidationError``.\\n\\n    '\n    if (len(pw) < settings.MINIMUM_PASSWORD_CHARS):\n        raise forms.ValidationError('Your password must be a minimum of {0} characters.'.format(settings.MINIMUM_PASSWORD_CHARS))\n    if (settings.PASSWORD_REQUIRE_ALPHA_NUMERIC and (not (any((c.isdigit() for c in pw)) and any((c.isalpha() for c in pw))))):\n        raise forms.ValidationError('Your password must contain both letters and numbers.')\n    if (pw in settings.FORBIDDEN_PASSWORDS):\n        raise forms.ValidationError('That password is too easily guessed; please choose a different one.')\n    return pw\n", "label": 1}
{"function": "\n\ndef random_file():\n    files = [f for f in os.listdir(filesdir) if re.match('.*\\\\.(twt|tweet)$', f, re.IGNORECASE)]\n    jsonfiles = [f for f in os.listdir(filesdir) if re.match('.*\\\\.json$', f, re.IGNORECASE)]\n    jsonfiles = [jf.replace('.json', '') for jf in jsonfiles]\n    unprocessed_files = list((set(files) - set(jsonfiles)))\n    if (len(unprocessed_files) == 0):\n        return ('', len(unprocessed_files))\n    filename = random.choice(unprocessed_files)\n    return (filename, len(unprocessed_files))\n", "label": 1}
{"function": "\n\ndef _build_runlist(self):\n    'Build a runlist based on a uniform distribution.'\n    if (self.seed is not None):\n        np.random.seed(self.seed)\n    for i in moves.range(self.num_samples):\n        sample = []\n        for (key, meta) in iteritems(self.get_desvar_metadata()):\n            nval = meta['size']\n            values = []\n            for k in range(nval):\n                low = meta['lower']\n                high = meta['upper']\n                if isinstance(low, np.ndarray):\n                    low = low[k]\n                if isinstance(high, np.ndarray):\n                    high = high[k]\n                values.append(np.random.uniform(low, high))\n            sample.append([key, np.array(values)])\n        (yield sample)\n", "label": 1}
{"function": "\n\ndef _iter_one_range(self, range_spec, segment_iter):\n    client_start = range_spec['resp_client_start']\n    client_end = range_spec['resp_client_end']\n    segment_start = range_spec['resp_segment_start']\n    segment_end = range_spec['resp_segment_end']\n    segment_end = (min(segment_end, (self.obj_length - 1)) if (segment_end is not None) else (self.obj_length - 1))\n    client_end = (min(client_end, (self.obj_length - 1)) if (client_end is not None) else (self.obj_length - 1))\n    num_segments = int(math.ceil((float(((segment_end + 1) - segment_start)) / self.policy.ec_segment_size)))\n    start_overrun = (client_start - segment_start)\n    end_overrun = (segment_end - client_end)\n    for (i, next_seg) in enumerate(segment_iter):\n        if (start_overrun > 0):\n            seglen = len(next_seg)\n            if (seglen <= start_overrun):\n                start_overrun -= seglen\n                continue\n            else:\n                next_seg = next_seg[start_overrun:]\n                start_overrun = 0\n        if ((i == (num_segments - 1)) and end_overrun):\n            next_seg = next_seg[:(- end_overrun)]\n        (yield next_seg)\n", "label": 1}
{"function": "\n\ndef execute(self):\n    if (not self.args):\n        self.pc.update_output('Award what to whom?')\n        return\n    exp = '(?P<item>(\\\\w+|[ ])+)([ ]+to)([ ]+(?P<player>\\\\w+))([ ]+(?P<actor>\\\\\"(.*?)\\\\\")(:(?P<room>\\\\\"(.*?)\\\\\"))?)?'\n    match = re.match(exp, self.args, re.I)\n    if (not match):\n        self.pc.update_output('Type \"help award\" for help with this command.')\n        return\n    (item_kw, player_kw, actor, room) = match.group('item', 'player', 'actor', 'room')\n    player = self.pc.location.get_player(player_kw.lower())\n    if (not player):\n        self.pc.update_output(('Whom do you want to award %s to?' % item_kw))\n        return\n    item = self.pc.check_inv_for_keyword(item_kw)\n    if (not item):\n        self.pc.update_output((\"You don't have any %s.\" % item_kw))\n        return\n    self.pc.item_remove(item)\n    player.item_add(item)\n    self.pc.update_output(('%s has been awarded %s.' % (player.fancy_name(), item.name)))\n    if actor:\n        message = self.personalize(actor.strip('\"'), self.pc)\n        player.update_output(message)\n    if room:\n        message = self.personalize(room.strip('\"'), self.pc, player)\n        self.pc.location.tell_room(message, [player.name], self.pc)\n", "label": 1}
{"function": "\n\ndef _get_article_metadata(self, meta):\n    adict = PYConf()\n    adict.title = meta['title'][0]\n    adict.postid = meta['postid'][0]\n    adict.nicename = meta['nicename'][0]\n    adict.slug = meta['slug'][0]\n    adict.date = self.get_datetime(meta['date'][0])\n    adict.author = meta['author'][0]\n    tags = meta.get('tags')\n    if tags:\n        adict.tags = [tag.strip() for tag in tags[0].split(',')]\n    category = meta.get('category')\n    if category:\n        adict.category = [cat.strip() for cat in category[0].split(',')]\n    modified = meta.get('modified')\n    if modified:\n        adict.modified = self.get_datetime(modified[0])\n    posttype = meta.get('posttype')\n    if posttype:\n        adict.posttype = posttype[0]\n    else:\n        adict.posttype = 'post'\n    poststatus = meta.get('poststatus')\n    if poststatus:\n        adict.poststatus = poststatus[0]\n    else:\n        adict.poststatus = 'publish'\n    attachments = meta.get('attachments')\n    if attachments:\n        adict.attachments = [att.strip() for att in attachments[0].split(',')]\n    return adict\n", "label": 1}
{"function": "\n\ndef extra_client_expected_langs():\n    \"Return language list from http.request.header.Accept-Language,\\n    ordered by 'q'.\"\n    result = []\n    pieces = REQUEST_ACCEPT_LANGUAGE_RE.split(request.headers.get('Accept-Language', ''))\n    if pieces[(- 1)]:\n        return []\n    for i in range(0, (len(pieces) - 1), 3):\n        (first, lang, priority) = pieces[i:(i + 3)]\n        if (lang == '*'):\n            return []\n        if first:\n            return []\n        priority = ((priority and float(priority)) or 1.0)\n        result.append((lang, priority))\n    result.sort(key=(lambda k: k[1]), reverse=True)\n    return result\n", "label": 1}
{"function": "\n\ndef process_view(self, request, view_func, view_args, view_kwargs):\n    'Forwards unauthenticated requests to the admin page to the CAS\\n        login URL, as well as calls to django.contrib.auth.views.login and\\n        logout.\\n        '\n    if (view_func == login):\n        return cas_login(request, *view_args, **view_kwargs)\n    elif (view_func == logout):\n        return cas_logout(request, *view_args, **view_kwargs)\n    if (view_func in (cas_login, cas_logout)):\n        return None\n    if settings.CAS_ADMIN_PREFIX:\n        if (not request.path.startswith(settings.CAS_ADMIN_PREFIX)):\n            return None\n    elif (not view_func.__module__.startswith('django.contrib.admin.')):\n        return None\n    if request.user.is_authenticated():\n        if request.user.is_staff:\n            return None\n        else:\n            error = '<h1>Forbidden</h1><p>You do not have staff privileges.</p>'\n            return HttpResponseForbidden(error)\n    params = urllib_parse.urlencode({\n        REDIRECT_FIELD_NAME: request.get_full_path(),\n    })\n    return HttpResponseRedirect(((reverse(cas_login) + '?') + params))\n", "label": 1}
{"function": "\n\ndef _format_args(self, action, default_metavar):\n    get_metavar = self._metavar_formatter(action, default_metavar)\n    if (action.nargs is None):\n        result = ('%s' % get_metavar(1))\n    elif (action.nargs == OPTIONAL):\n        result = ('[%s]' % get_metavar(1))\n    elif (action.nargs == ZERO_OR_MORE):\n        result = ('[%s [%s ...]]' % get_metavar(2))\n    elif (action.nargs == ONE_OR_MORE):\n        result = ('%s [%s ...]' % get_metavar(2))\n    elif (action.nargs == REMAINDER):\n        result = '...'\n    elif (action.nargs == PARSER):\n        result = ('%s ...' % get_metavar(1))\n    else:\n        formats = ['%s' for _ in range(action.nargs)]\n        result = (' '.join(formats) % get_metavar(action.nargs))\n    return result\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _rebuild_ledger_value_from_transactions(ledger_value, transactions):\n    balance = 0\n    for transaction in transactions:\n        updated_values = _compute_ledger_values(balance, transaction)\n        new_balance = updated_values.balance\n        new_delta = (updated_values.balance - balance)\n        if ((new_balance != transaction.updated_balance) or (new_delta != transaction.delta)):\n            transaction.delta = new_delta\n            transaction.updated_balance = new_balance\n            ledger_value.track_update(transaction)\n        elif (not transaction.is_saved()):\n            ledger_value.track_create(transaction)\n        balance = new_balance\n    if ((balance != ledger_value.balance) or ledger_value.has_tracked_models()):\n        ledger_value.balance = balance\n    return ledger_value\n", "label": 1}
{"function": "\n\ndef get_aruba_data(self):\n    'Retrieve data from Aruba Access Point and return parsed result.'\n    import pexpect\n    connect = 'ssh {}@{}'\n    ssh = pexpect.spawn(connect.format(self.username, self.host))\n    query = ssh.expect(['password:', pexpect.TIMEOUT, pexpect.EOF, 'continue connecting (yes/no)?', 'Host key verification failed.', 'Connection refused', 'Connection timed out'], timeout=120)\n    if (query == 1):\n        _LOGGER.error('Timeout')\n        return\n    elif (query == 2):\n        _LOGGER.error('Unexpected response from router')\n        return\n    elif (query == 3):\n        ssh.sendline('yes')\n        ssh.expect('password:')\n    elif (query == 4):\n        _LOGGER.error('Host key Changed')\n        return\n    elif (query == 5):\n        _LOGGER.error('Connection refused by server')\n        return\n    elif (query == 6):\n        _LOGGER.error('Connection timed out')\n        return\n    ssh.sendline(self.password)\n    ssh.expect('#')\n    ssh.sendline('show clients')\n    ssh.expect('#')\n    devices_result = ssh.before.split(b'\\r\\n')\n    ssh.sendline('exit')\n    devices = {\n        \n    }\n    for device in devices_result:\n        match = _DEVICES_REGEX.search(device.decode('utf-8'))\n        if match:\n            devices[match.group('ip')] = {\n                'ip': match.group('ip'),\n                'mac': match.group('mac').upper(),\n                'name': match.group('name'),\n            }\n    return devices\n", "label": 1}
{"function": "\n\ndef _apply_common_rules(self, part, maxlength):\n    'This method contains the rules that must be applied to both the\\n        domain and the local part of the e-mail address.\\n        '\n    part = part.strip()\n    if self.fix:\n        part = part.strip('.')\n    if (not part):\n        return (part, 'It cannot be empty.')\n    if (len(part) > maxlength):\n        return (part, ('It cannot be longer than %i chars.' % maxlength))\n    if (part[0] == '.'):\n        return (part, 'It cannot start with a dot.')\n    if (part[(- 1)] == '.'):\n        return (part, 'It cannot end with a dot.')\n    if ('..' in part):\n        return (part, 'It cannot contain consecutive dots.')\n    return (part, '')\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TAlterSentryRoleRevokePrivilegeRequest')\n    if (self.protocol_version is not None):\n        oprot.writeFieldBegin('protocol_version', TType.I32, 1)\n        oprot.writeI32(self.protocol_version)\n        oprot.writeFieldEnd()\n    if (self.requestorUserName is not None):\n        oprot.writeFieldBegin('requestorUserName', TType.STRING, 2)\n        oprot.writeString(self.requestorUserName)\n        oprot.writeFieldEnd()\n    if (self.roleName is not None):\n        oprot.writeFieldBegin('roleName', TType.STRING, 3)\n        oprot.writeString(self.roleName)\n        oprot.writeFieldEnd()\n    if (self.component is not None):\n        oprot.writeFieldBegin('component', TType.STRING, 4)\n        oprot.writeString(self.component)\n        oprot.writeFieldEnd()\n    if (self.privilege is not None):\n        oprot.writeFieldBegin('privilege', TType.STRUCT, 5)\n        self.privilege.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef in_bbox(point, a, b):\n    'Return True if `point` is in the bounding box defined by `a`\\n        and `b`.\\n\\n        >>> bmin = (0, 0)\\n        >>> bmax = (100, 100)\\n        >>> Vector.in_bbox((50, 50), bmin, bmax)\\n        True\\n        >>> Vector.in_bbox((647, -10), bmin, bmax)\\n        False\\n\\n        '\n    return ((((point[0] <= a[0]) and (point[0] >= b[0])) or ((point[0] <= b[0]) and (point[0] >= a[0]))) and (((point[1] <= a[1]) and (point[1] >= b[1])) or ((point[1] <= b[1]) and (point[1] >= a[1]))))\n", "label": 1}
{"function": "\n\ndef insert_row(self, values, index=1):\n    '\"Adds a row to the worksheet at the specified index and populates it with values.\\n        Widens the worksheet if there are more values than columns.\\n\\n        :param values: List of values for the new row.\\n        '\n    if (index == (self.row_count + 1)):\n        return self.append_row(values)\n    elif (index > (self.row_count + 1)):\n        raise IndexError('Row index out of range')\n    self.add_rows(1)\n    data_width = len(values)\n    if (self.col_count < data_width):\n        self.resize(cols=data_width)\n    top_left = self.get_addr_int(index, 1)\n    bottom_right = self.get_addr_int(self.row_count, self.col_count)\n    range_str = ('%s:%s' % (top_left, bottom_right))\n    cells_after_insert = self.range(range_str)\n    for (ind, cell) in reversed(list(enumerate(cells_after_insert))):\n        if (ind < self.col_count):\n            new_val = (values[ind] if (ind < len(values)) else '')\n        else:\n            new_val = cells_after_insert[(ind - self.col_count)].value\n        cell.value = new_val\n    self.update_cells(cells_after_insert)\n", "label": 1}
{"function": "\n\ndef _check_1d(self, routine, dtype, shape, axis, overwritable_dtypes):\n    np.random.seed(1234)\n    if np.issubdtype(dtype, np.complexfloating):\n        data = (np.random.randn(*shape) + (1j * np.random.randn(*shape)))\n    else:\n        data = np.random.randn(*shape)\n    data = data.astype(dtype)\n    for type in [1, 2, 3]:\n        for overwrite_x in [True, False]:\n            for norm in [None, 'ortho']:\n                if ((type == 1) and (norm == 'ortho')):\n                    continue\n                should_overwrite = (overwrite_x and (dtype in overwritable_dtypes) and ((len(shape) == 1) or ((axis % len(shape)) == (len(shape) - 1))))\n                self._check(data, routine, type, None, axis, norm, overwrite_x, should_overwrite)\n", "label": 1}
{"function": "\n\ndef _print(self):\n    args = self.args\n    get_str = (lambda arg: (str(arg) if (arg.is_Atom or isinstance(arg, TensExpr)) else ('(%s)' % str(arg))))\n    if (not args):\n        return '1'\n    if (self.coeff == S.NegativeOne):\n        return ('-' + '*'.join([get_str(arg) for arg in args[1:]]))\n    return '*'.join([get_str(arg) for arg in self.args])\n", "label": 1}
{"function": "\n\ndef process_pass_fail(self):\n    '\\n        :return: etree element\\n        '\n    mods = (self.engine.reporters + self.engine.services)\n    pass_fail_objects = [_x for _x in mods if isinstance(_x, PassFailStatus)]\n    self.log.debug('Processing passfail objects: %s', pass_fail_objects)\n    fail_criterias = []\n    for pf_obj in pass_fail_objects:\n        if pf_obj.criterias:\n            for _fc in pf_obj.criterias:\n                fail_criterias.append(_fc)\n    root_xml_element = etree.Element('testsuite', name='bzt_pass_fail', package='bzt')\n    bza_report_info = self.get_bza_report_info()\n    classname = (bza_report_info[0][1] if bza_report_info else ('bzt-' + str(self.__hash__())))\n    report_urls = [info_item[0] for info_item in bza_report_info]\n    for fc_obj in fail_criterias:\n        testcase_etree = self.__process_criteria(classname, fc_obj, report_urls)\n        root_xml_element.append(testcase_etree)\n    return root_xml_element\n", "label": 1}
{"function": "\n\n@access_off(permission_denied)\n@login_required\ndef project_delete_permission_request(request, project_slug, permission_pk):\n    '\\n    View for deleting a request of permission of a user.\\n\\n    This view is an abstraction of a txpermissions.views method.\\n    '\n    (project, permission) = _get_project_and_permission(project_slug, permission_pk)\n    if (request.user.id == permission.user.id):\n        notice_type = 'project_submit_access_request_withdrawn'\n        sendto = project.maintainers.all()\n    else:\n        notice_type = 'project_submit_access_request_denied'\n        sendto = [permission.user]\n    notice = {\n        'type': notice_type,\n        'object': project,\n        'sendto': sendto,\n        'extra_context': {\n            'project': project,\n            'user_request': permission.user,\n            'user_action': request.user,\n        },\n    }\n    check = ProjectPermission(request.user)\n    if (check.maintain(project) or request.user.has_perm('authority.delete_permission') or (request.user.pk == permission.creator.pk)):\n        return delete_permission_or_request(request, permission, False, extra_context={\n            'notice': notice,\n        })\n    check = ProjectPermission(request.user)\n    if (check.maintain(project) or request.user.has_perm('authority.delete_permission') or (request.user.pk == permission.creator.pk)):\n        return delete_permission_or_request(request, permission, False)\n    return permission_denied(request)\n", "label": 1}
{"function": "\n\ndef field_repr(field, force_many=False):\n    kwargs = field._kwargs\n    if force_many:\n        kwargs = kwargs.copy()\n        kwargs['many'] = True\n        kwargs.pop('child', None)\n    if (kwargs.get('label', None) is None):\n        kwargs.pop('label', None)\n    if (kwargs.get('help_text', None) is None):\n        kwargs.pop('help_text', None)\n    arg_string = ', '.join([smart_repr(val) for val in field._args])\n    kwarg_string = ', '.join([('%s=%s' % (key, smart_repr(val))) for (key, val) in sorted(kwargs.items())])\n    if (arg_string and kwarg_string):\n        arg_string += ', '\n    if force_many:\n        class_name = force_many.__class__.__name__\n    else:\n        class_name = field.__class__.__name__\n    return ('%s(%s%s)' % (class_name, arg_string, kwarg_string))\n", "label": 1}
{"function": "\n\ndef to_rational(ctx, s, limit=True):\n    (p, q) = to_rational(s._mpf_)\n    if ((not limit) or (q <= ctx.max_denom)):\n        return (p, q)\n    (p0, q0, p1, q1) = (0, 1, 1, 0)\n    (n, d) = (p, q)\n    while True:\n        a = (n // d)\n        q2 = (q0 + (a * q1))\n        if (q2 > ctx.max_denom):\n            break\n        (p0, q0, p1, q1) = (p1, q1, (p0 + (a * p1)), q2)\n        (n, d) = (d, (n - (a * d)))\n    k = ((ctx.max_denom - q0) // q1)\n    number = mpq(p, q)\n    bound1 = mpq((p0 + (k * p1)), (q0 + (k * q1)))\n    bound2 = mpq(p1, q1)\n    if ((not bound2) or (not bound1)):\n        return (p, q)\n    elif (abs((bound2 - number)) <= abs((bound1 - number))):\n        return bound2._mpq_\n    else:\n        return bound1._mpq_\n", "label": 1}
{"function": "\n\ndef merge(a, b):\n    'return merged tuples or lists without duplicates\\n    note: ensure if admin theme is before admin\\n    '\n    if (isinstance(a, CONFIG_VALID) and isinstance(b, CONFIG_VALID)):\n        if (isinstance(a, dict) and isinstance(b, dict)):\n            a.update(b)\n            return a\n        _a = list(a)\n        for x in list(b):\n            if (x not in _a):\n                _a.append(x)\n        return _a\n    if (a and b):\n        raise Exception('Cannot merge')\n    raise NotImplementedError\n", "label": 1}
{"function": "\n\ndef _get_dtype(self, sqltype):\n    from sqlalchemy.types import Integer, Float, Boolean, DateTime, Date, TIMESTAMP\n    if isinstance(sqltype, Float):\n        return float\n    elif isinstance(sqltype, Integer):\n        return np.dtype('int64')\n    elif isinstance(sqltype, TIMESTAMP):\n        if (not sqltype.timezone):\n            return datetime\n        return DatetimeTZDtype\n    elif isinstance(sqltype, DateTime):\n        return datetime\n    elif isinstance(sqltype, Date):\n        return date\n    elif isinstance(sqltype, Boolean):\n        return bool\n    return object\n", "label": 1}
{"function": "\n\ndef __init__(self, corpus, num_best=None, dtype=numpy.float32, num_features=None, chunksize=256):\n    '\\n        `num_features` is the number of features in the corpus (will be determined\\n        automatically by scanning the corpus if not specified). See `Similarity`\\n        class for description of the other parameters.\\n\\n        '\n    if (num_features is None):\n        logger.warning('scanning corpus to determine the number of features (consider setting `num_features` explicitly)')\n        num_features = (1 + utils.get_max_id(corpus))\n    self.num_features = num_features\n    self.num_best = num_best\n    self.normalize = True\n    self.chunksize = chunksize\n    if (corpus is not None):\n        if (self.num_features <= 0):\n            raise ValueError('cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)')\n        logger.info(('creating matrix for %s documents and %i features' % (len(corpus), num_features)))\n        self.index = numpy.empty(shape=(len(corpus), num_features), dtype=dtype)\n        for (docno, vector) in enumerate(corpus):\n            if ((docno % 1000) == 0):\n                logger.debug(('PROGRESS: at document #%i/%i' % (docno, len(corpus))))\n            if isinstance(vector, numpy.ndarray):\n                pass\n            elif scipy.sparse.issparse(vector):\n                vector = vector.toarray().flatten()\n            else:\n                vector = matutils.unitvec(matutils.sparse2full(vector, num_features))\n            self.index[docno] = vector\n", "label": 1}
{"function": "\n\ndef __init__(self, topic, exchange, type_handlers=None, on_wait=None, url=None, transport=None, transport_options=None, retry_options=None):\n    self._topic = topic\n    self._exchange_name = exchange\n    self._on_wait = on_wait\n    self._running = threading.Event()\n    self._dispatcher = dispatcher.TypeDispatcher(requeue_filters=[(lambda data, message: (not self.is_running))], type_handlers=type_handlers)\n    ensure_options = self.DEFAULT_RETRY_OPTIONS.copy()\n    if (retry_options is not None):\n        for k in set(six.iterkeys(ensure_options)):\n            if (k in retry_options):\n                val = retry_options[k]\n                if (k in self._RETRY_INT_OPTS):\n                    tmp_val = int(val)\n                else:\n                    tmp_val = float(val)\n                if (tmp_val < 0):\n                    raise ValueError((\"Expected value greater or equal to zero for 'retry_options' %s; got %s instead\" % (k, val)))\n                ensure_options[k] = tmp_val\n    self._ensure_options = ensure_options\n    self._drain_events_timeout = DRAIN_EVENTS_PERIOD\n    if ((transport == 'memory') and transport_options):\n        polling_interval = transport_options.get('polling_interval')\n        if (polling_interval is not None):\n            self._drain_events_timeout = polling_interval\n    self._conn = kombu.Connection(url, transport=transport, transport_options=transport_options)\n    self._exchange = kombu.Exchange(name=self._exchange_name, durable=False, auto_delete=True)\n", "label": 1}
{"function": "\n\ndef _compute_underscore_max_num(exps):\n    max_num = 0\n    if (not issequence_except_str(exps)):\n        exps = (exps,)\n    for exp in exps:\n        if (isinstance(exp, Symbol) and exp.name.startswith('$')):\n            try:\n                n = int(exp.name[1:])\n            except:\n                n = 1\n        elif issequence_except_str(exp):\n            n = _compute_underscore_max_num(exp)\n        else:\n            n = 0\n        if (n > max_num):\n            max_num = n\n    return max_num\n", "label": 1}
{"function": "\n\ndef _filter_only(self, selector, elements, reverse=False, unique=False):\n    'Filters the selection set only, as opposed to also including\\n           descendants.\\n        '\n    if (selector is None):\n        results = elements\n    else:\n        xpath = selector_to_xpath(selector, 'self::')\n        results = []\n        for tag in elements:\n            results.extend(tag.xpath(xpath))\n    if reverse:\n        results.reverse()\n    if unique:\n        result_list = results\n        results = []\n        for item in result_list:\n            if (not (item in results)):\n                results.append(item)\n    return self.__class__(results, **dict(parent=self))\n", "label": 1}
{"function": "\n\ndef collect(self):\n    from unittest import TestLoader\n    cls = self.obj\n    if (not getattr(cls, '__test__', True)):\n        return\n    self.session._fixturemanager.parsefactories(self, unittest=True)\n    loader = TestLoader()\n    module = self.getparent(pytest.Module).obj\n    foundsomething = False\n    for name in loader.getTestCaseNames(self.obj):\n        x = getattr(self.obj, name)\n        funcobj = getattr(x, 'im_func', x)\n        transfer_markers(funcobj, cls, module)\n        (yield TestCaseFunction(name, parent=self))\n        foundsomething = True\n    if (not foundsomething):\n        runtest = getattr(self.obj, 'runTest', None)\n        if (runtest is not None):\n            ut = sys.modules.get('twisted.trial.unittest', None)\n            if ((ut is None) or (runtest != ut.TestCase.runTest)):\n                (yield TestCaseFunction('runTest', parent=self))\n", "label": 1}
{"function": "\n\ndef AllowInstalledLibrary(name, desired):\n    \"Allow the use of a package without performing a version check.\\n\\n  Needed to clear a package's dependencies in case the dependencies need to be\\n  imported in order to perform a version check. The version check is skipped on\\n  the dependencies because the assumption is that the package that triggered\\n  the call would not be installed without the proper dependencies (which might\\n  be a different version than what the package explicitly requires).\\n\\n  Args:\\n    name: Name of package.\\n    desired: Desired version.\\n\\n  Raises:\\n    UnacceptableVersion Error if the installed version of a package is\\n    unacceptable.\\n  \"\n    if ((name == 'django') and (desired != '0.96')):\n        tail = os.path.join('lib', 'django')\n        sys.path[:] = [dirname for dirname in sys.path if (not dirname.endswith(tail))]\n    CallSetAllowedModule(name, desired)\n    dependencies = PACKAGES[name][1][desired]\n    if dependencies:\n        for (dep_name, dep_version) in dependencies:\n            AllowInstalledLibrary(dep_name, dep_version)\n    installed[name] = (desired, False)\n", "label": 1}
{"function": "\n\ndef parallel_execute(objects, func, get_name, msg, get_deps=None):\n    'Runs func on objects in parallel while ensuring that func is\\n    ran on object only after it is ran on all its dependencies.\\n\\n    get_deps called on object must return a collection with its dependencies.\\n    get_name called on object must return its name.\\n    '\n    objects = list(objects)\n    stream = get_output_stream(sys.stderr)\n    writer = ParallelStreamWriter(stream, msg)\n    for obj in objects:\n        writer.initialize(get_name(obj))\n    events = parallel_execute_iter(objects, func, get_deps)\n    errors = {\n        \n    }\n    results = []\n    error_to_reraise = None\n    for (obj, result, exception) in events:\n        if (exception is None):\n            writer.write(get_name(obj), 'done')\n            results.append(result)\n        elif isinstance(exception, APIError):\n            errors[get_name(obj)] = exception.explanation\n            writer.write(get_name(obj), 'error')\n        elif isinstance(exception, UpstreamError):\n            writer.write(get_name(obj), 'error')\n        else:\n            errors[get_name(obj)] = exception\n            error_to_reraise = exception\n    for (obj_name, error) in errors.items():\n        stream.write('\\nERROR: for {}  {}\\n'.format(obj_name, error))\n    if error_to_reraise:\n        raise error_to_reraise\n    return results\n", "label": 1}
{"function": "\n\ndef get_scorm_object(authenticate=True, laboratory_identifier='', lms_path='/', lms_extension='/', html_body='<div id=\"gateway4labs_root\" />\\n'):\n    import labmanager\n    base_dir = os.path.dirname(labmanager.__file__)\n    base_scorm_dir = os.path.join(base_dir, 'data', 'scorm')\n    if (not os.path.exists(base_scorm_dir)):\n        flash(('Error: %s does not exist' % base_scorm_dir))\n        return render_template('lms_admin/scorm_errors.html')\n    sio = StringIO.StringIO('')\n    zf = zipfile.ZipFile(sio, 'w')\n    for (root, dir, files) in os.walk(base_scorm_dir):\n        for f in files:\n            file_name = os.path.join(root, f)\n            arc_name = os.path.join(root[(len(base_scorm_dir) + 1):], f)\n            content = codecs.open(file_name, 'rb', encoding='utf-8').read()\n            if ((f == 'lab.html') and (root == base_scorm_dir)):\n                content = (content % {\n                    'EXPERIMENT_COMMENT': ('//' if authenticate else ''),\n                    'AUTHENTICATE_COMMENT': ('//' if (not authenticate) else ''),\n                    'EXPERIMENT_IDENTIFIER': unicode(laboratory_identifier),\n                    'LMS_URL': unicode(lms_path),\n                    'LMS_EXTENSION': unicode(lms_extension),\n                    'HTML_CONTENT': unicode(html_body),\n                })\n            zf.writestr(arc_name, content.encode('utf-8'))\n    zf.close()\n    return sio.getvalue()\n", "label": 1}
{"function": "\n\ndef _build_root_message(self, message_cls=None, **kw):\n    msg = (message_cls or SafeMIMEMultipart)(**kw)\n    if self.policy:\n        msg.policy = self.policy\n    msg.preamble = self.ROOT_PREAMBLE\n    self.set_header(msg, 'Date', self.date, encode=False)\n    self.set_header(msg, 'Message-ID', self.message_id, encode=False)\n    if self._headers:\n        for (name, value) in self._headers.items():\n            self.set_header(msg, name, value)\n    subject = self.subject\n    if (subject is not None):\n        self.set_header(msg, 'Subject', subject)\n    self.set_header(msg, 'From', self.encode_address_header(self._mail_from), encode=False)\n    if self._mail_to:\n        self.set_header(msg, 'To', ', '.join([self.encode_address_header(addr) for addr in self._mail_to]), encode=False)\n    if self._cc:\n        self.set_header(msg, 'Cc', ', '.join([self.encode_address_header(addr) for addr in self._cc]), encode=False)\n    return msg\n", "label": 1}
{"function": "\n\ndef _get_context(self, context):\n    ' Gets the context to use for evaluating an expression.\\n        '\n    name = 'object'\n    n = len(context)\n    if ((n == 2) and ('handler' in context)):\n        for (name, value) in context.items():\n            if (name != 'handler'):\n                break\n    elif (n == 1):\n        name = context.keys()[0]\n    value = context.get(name)\n    if (value is not None):\n        context2 = value.trait_get()\n        context2.update(context)\n    else:\n        context2 = context.copy()\n    context2['ui'] = self\n    return context2\n", "label": 1}
{"function": "\n\ndef get_revision(self, location):\n    '\\n        Return the maximum revision for all files under a given location\\n        '\n    revision = 0\n    for (base, dirs, files) in os.walk(location):\n        if (self.dirname not in dirs):\n            dirs[:] = []\n            continue\n        dirs.remove(self.dirname)\n        entries_fn = os.path.join(base, self.dirname, 'entries')\n        if (not os.path.exists(entries_fn)):\n            continue\n        (dirurl, localrev) = self._get_svn_url_rev(base)\n        if (base == location):\n            base_url = (dirurl + '/')\n        elif ((not dirurl) or (not dirurl.startswith(base_url))):\n            dirs[:] = []\n            continue\n        revision = max(revision, localrev)\n    return revision\n", "label": 1}
{"function": "\n\ndef __expandparamstodict(self, params, kw):\n    self_parameters = self.parameters\n    parametervalues = dict(zip((p['name'] for p in self_parameters), params))\n    for (kw, kwval) in kw.items():\n        if (kw in parametervalues):\n            raise KeyError(('Multiple definitions of parameter %r' % kw))\n        parametervalues[kw] = kwval\n    for param_to_convert in self_parameters:\n        if (param_to_convert['name'] in parametervalues):\n            val = parametervalues[param_to_convert['name']]\n            if (val is None):\n                parametervalues[param_to_convert['name']] = ''\n            elif (not isinstance(val, param_to_convert['datatype'])):\n                conversion = param_to_convert['datatype'](val)\n                parametervalues[param_to_convert['name']] = getattr(conversion, '_json_struct', conversion)\n        elif (param_to_convert['parameterType'] != 'esriGPParameterTypeDerived'):\n            parametervalues[param_to_convert['name']] = ''\n    return parametervalues\n", "label": 1}
{"function": "\n\ndef attributeAsLDIF(attribute, value):\n    if (value.startswith('\\x00') or value.startswith('\\n') or value.startswith('\\r') or value.startswith(' ') or value.startswith(':') or value.startswith('<') or value.endswith(' ') or containsNonprintable(value)):\n        return attributeAsLDIF_base64(attribute, value)\n    else:\n        return ('%s: %s\\n' % (attribute, value))\n", "label": 1}
{"function": "\n\ndef collect(self, objs, source=None, nullable=False, collect_related=True, source_attr=None, collect_parents=True):\n    new_objs = self.add(objs, source, nullable)\n    if (not new_objs):\n        return\n    model = new_objs[0].__class__\n    if collect_related:\n        for related in model._meta.get_all_related_objects(include_hidden=True, local_only=True):\n            field = related.field\n            if related.model._meta.auto_created:\n                self.add_batch(related.model, field, new_objs)\n            else:\n                sub_objs = self.related_objects(related, new_objs)\n                if (not sub_objs):\n                    continue\n                field.rel.on_delete(self, field, sub_objs, self.using)\n        for relation in model._meta.many_to_many:\n            if (not relation.rel.through):\n                sub_objs = relation.bulk_related_objects(new_objs, self.using)\n                self.collect(sub_objs, source=model, source_attr=relation.rel.related_name, nullable=True)\n", "label": 1}
{"function": "\n\ndef _get_config(self):\n    config = None\n    possibles = [fs.abspath(os.path.join(self.basedir, 'boss.json')), fs.abspath(os.path.join(self.basedir, 'boss.yml'))]\n    for path in possibles:\n        if os.path.exists(path):\n            if (os.path.basename(path) == 'boss.json'):\n                config = self._get_json_config(path)\n                break\n            elif (os.path.basename(path) == 'boss.yml'):\n                config = self._get_yaml_config(path)\n                break\n    if (not config):\n        raise boss_exc.BossTemplateError('No supported config found.')\n    if ('delimiters' not in config.keys()):\n        config['delimiters'] = ('@', '@')\n        config['start_delimiter'] = config['delimiters'][0]\n        config['end_delimiter'] = config['delimiters'][1]\n    else:\n        config['start_delimiter'] = config['delimiters'][0]\n        config['end_delimiter'] = config['delimiters'][1]\n    return config\n", "label": 1}
{"function": "\n\ndef volume_create_attach(name, call=None, **kwargs):\n    '\\n    Create and attach volumes to created node\\n    '\n    if (call == 'function'):\n        raise SaltCloudSystemExit('The create_attach_volumes action must be called with -a or --action.')\n    if (type(kwargs['volumes']) is str):\n        volumes = yaml.safe_load(kwargs['volumes'])\n    else:\n        volumes = kwargs['volumes']\n    ret = []\n    for volume in volumes:\n        created = False\n        volume_dict = {\n            'name': volume['name'],\n        }\n        if ('volume_id' in volume):\n            volume_dict['volume_id'] = volume['volume_id']\n        elif ('snapshot' in volume):\n            volume_dict['snapshot'] = volume['snapshot']\n        else:\n            volume_dict['size'] = volume['size']\n            if ('type' in volume):\n                volume_dict['type'] = volume['type']\n            if ('iops' in volume):\n                volume_dict['iops'] = volume['iops']\n        if ('id' not in volume_dict):\n            created_volume = create_volume(**volume_dict)\n            created = True\n            volume_dict.update(created_volume)\n        attach = attach_volume(name=volume['name'], server_name=name, device=volume.get('device', None), call='action')\n        if attach:\n            msg = '{0} attached to {1} (aka {2})'.format(volume_dict['id'], name, volume_dict['name'])\n            log.info(msg)\n            ret.append(msg)\n    return ret\n", "label": 1}
{"function": "\n\ndef __init__(self, host, port=None, key_file=None, cert_file=None, ca_certs=None, strict=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n    if (not (os.path.isfile(cert_file) and os.access(cert_file, os.R_OK))):\n        raise Exception(('Could not find/access ' + cert_file))\n    if (not (os.path.isfile(key_file) and os.access(key_file, os.R_OK))):\n        raise Exception(('Could not find/access ' + key_file))\n    if (not (os.path.isfile(ca_certs) and os.access(ca_certs, os.R_OK))):\n        raise Exception(('Could not find/access ' + ca_certs))\n    httplib.HTTPSConnection.__init__(self, host=host, port=port, key_file=key_file, cert_file=cert_file, strict=strict, timeout=timeout, source_address=source_address)\n    self.ca_certs = ca_certs\n", "label": 1}
{"function": "\n\ndef setup(self, matrix, format):\n    if (matrix == 'Identity'):\n        if (format in ('lil', 'dok')):\n            raise NotImplementedError()\n        self.A = sparse.eye(10000, 10000, format=format)\n    elif (matrix == 'Poisson5pt'):\n        self.A = poisson2d(300, format=format)\n    elif (matrix == 'Block2x2'):\n        if (format not in ('csr', 'bsr')):\n            raise NotImplementedError()\n        b = (2, 2)\n        self.A = sparse.kron(poisson2d(150), ones(b)).tobsr(blocksize=b).asformat(format)\n    elif (matrix == 'Block3x3'):\n        if (format not in ('csr', 'bsr')):\n            raise NotImplementedError()\n        b = (3, 3)\n        self.A = sparse.kron(poisson2d(100), ones(b)).tobsr(blocksize=b).asformat(format)\n    else:\n        raise NotImplementedError()\n    self.x = ones(self.A.shape[1], dtype=float)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _simple_insert_many_txn(txn, table, values):\n    if (not values):\n        return\n    (keys, vals) = zip(*[zip(*sorted(i.items(), key=(lambda kv: kv[0]))) for i in values if i])\n    for k in keys:\n        if (k != keys[0]):\n            raise RuntimeError('All items must have the same keys')\n    sql = ('INSERT INTO %s (%s) VALUES(%s)' % (table, ', '.join((k for k in keys[0])), ', '.join(('?' for _ in keys[0]))))\n    txn.executemany(sql, vals)\n", "label": 1}
{"function": "\n\ndef mpf_pow(s, t, prec, rnd=round_fast):\n    '\\n    Compute s**t. Raises ComplexResult if s is negative and t is\\n    fractional.\\n    '\n    (ssign, sman, sexp, sbc) = s\n    (tsign, tman, texp, tbc) = t\n    if (ssign and (texp < 0)):\n        raise ComplexResult('negative number raised to a fractional power')\n    if (texp >= 0):\n        return mpf_pow_int(s, (((- 1) ** tsign) * (tman << texp)), prec, rnd)\n    if (texp == (- 1)):\n        if (tman == 1):\n            if tsign:\n                return mpf_div(fone, mpf_sqrt(s, (prec + 10), reciprocal_rnd[rnd]), prec, rnd)\n            return mpf_sqrt(s, prec, rnd)\n        else:\n            if tsign:\n                return mpf_pow_int(mpf_sqrt(s, (prec + 10), reciprocal_rnd[rnd]), (- tman), prec, rnd)\n            return mpf_pow_int(mpf_sqrt(s, (prec + 10), rnd), tman, prec, rnd)\n    c = mpf_log(s, (prec + 10), rnd)\n    return mpf_exp(mpf_mul(t, c), prec, rnd)\n", "label": 1}
{"function": "\n\ndef _check_true_dir(self, text):\n    is_rtl = False\n    is_ltr = False\n    quoted_text = False\n    last_inline_html_char_pos = text.rfind('>')\n    if (last_inline_html_char_pos > (- 1)):\n        it_here = text[(last_inline_html_char_pos + 1):]\n    else:\n        it_here = text\n    for ch in it_here:\n        res = UD.bidirectional(ch)\n        if (ch == '\"'):\n            quoted_text = (not quoted_text)\n        elif ((not quoted_text) and (res in {'R', 'AL'})):\n            is_rtl = True\n        elif ((not quoted_text) and (res == 'L')):\n            is_ltr = True\n    if is_rtl:\n        return 'rtl'\n    elif is_ltr:\n        return 'ltr'\n    else:\n        return 'auto'\n", "label": 1}
{"function": "\n\ndef _get_pages(self, locations, project_name):\n    '\\n        Yields (page, page_url) from the given locations, skipping\\n        locations that have errors, and adding download/homepage links\\n        '\n    all_locations = list(locations)\n    seen = set()\n    normalized = normalize_name(project_name)\n    while all_locations:\n        location = all_locations.pop(0)\n        if (location in seen):\n            continue\n        seen.add(location)\n        page = self._get_page(location)\n        if (page is None):\n            continue\n        (yield page)\n        for link in page.rel_links():\n            if ((normalized not in self.allow_external) and (not self.allow_all_external)):\n                self.need_warn_external = True\n                logger.debug('Not searching %s for files because external urls are disallowed.', link)\n                continue\n            if ((link.trusted is not None) and (not link.trusted) and (normalized not in self.allow_unverified)):\n                logger.debug('Not searching %s for urls, it is an untrusted link and cannot produce safe or verifiable files.', link)\n                self.need_warn_unverified = True\n                continue\n            all_locations.append(link)\n", "label": 1}
{"function": "\n\ndef extract_line_count(filename, target_dir):\n    example_file = os.path.join(target_dir, filename)\n    if six.PY2:\n        lines = open(example_file).readlines()\n    else:\n        lines = open(example_file, encoding='utf-8').readlines()\n    start_row = 0\n    if (lines and lines[0].startswith('#!')):\n        lines.pop(0)\n        start_row = 1\n    line_iterator = iter(lines)\n    tokens = tokenize.generate_tokens((lambda : next(line_iterator)))\n    check_docstring = True\n    erow_docstring = 0\n    for (tok_type, _, _, (erow, _), _) in tokens:\n        tok_type = token.tok_name[tok_type]\n        if (tok_type in ('NEWLINE', 'COMMENT', 'NL', 'INDENT', 'DEDENT')):\n            continue\n        elif ((tok_type == 'STRING') and check_docstring):\n            erow_docstring = erow\n            check_docstring = False\n    return (((erow_docstring + 1) + start_row), ((erow + 1) + start_row))\n", "label": 1}
{"function": "\n\ndef __init__(self, initset=set()):\n    self.__has = set()\n    if (not isinstance(initset, set)):\n        raise TypeError('Expected set of tuples')\n    for t in initset:\n        if ((not isinstance(t, tuple)) or (len(t) != 2) or (t[1] <= t[0]) or (t[1] < 0)):\n            raise ValueError('Your tuples are wrong :(')\n    self.__has = initset\n    self.__optimize()\n", "label": 1}
{"function": "\n\ndef test_basic_pycwl_statements(self):\n    h = CloudWatchLogHandler()\n    loggers = []\n    for i in range(5):\n        logger = logging.getLogger('logger{}'.format(i))\n        logger.addHandler(h)\n        loggers.append(logger)\n    for i in range(10001):\n        for logger in loggers:\n            logger.error('test')\n    import time\n    time.sleep(1)\n    for i in range(9000):\n        for logger in loggers:\n            logger.error('test')\n    for i in range(1001):\n        for logger in loggers:\n            logger.error('test')\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    '\\n        Dynamically create new classes and add them to the test module when\\n        multiple browsers specs are provided (e.g. --selenium=firefox,chrome).\\n        '\n    test_class = super(SeleniumTestCaseBase, cls).__new__(cls, name, bases, attrs)\n    if (test_class.browser or (not any(((name.startswith('test') and callable(value)) for (name, value) in attrs.items())))):\n        return test_class\n    elif test_class.browsers:\n        first_browser = test_class.browsers[0]\n        test_class.browser = first_browser\n        module = sys.modules[test_class.__module__]\n        for browser in test_class.browsers[1:]:\n            browser_test_class = cls.__new__(cls, str(('%s%s' % (capfirst(browser), name))), (test_class,), {\n                'browser': browser,\n                '__module__': test_class.__module__,\n            })\n            setattr(module, browser_test_class.__name__, browser_test_class)\n        return test_class\n    return unittest.skip('No browsers specified.')(test_class)\n", "label": 1}
{"function": "\n\ndef _check(self):\n    ' Checks data structure for consistency '\n    if (self.row.dtype.kind != 'i'):\n        warn(('row index array has non-integer dtype (%s)  ' % self.row.dtype.name))\n    if (self.col.dtype.kind != 'i'):\n        warn(('col index array has non-integer dtype (%s) ' % self.col.dtype.name))\n    idx_dtype = get_index_dtype(maxval=max(self.shape))\n    self.row = np.asarray(self.row, dtype=idx_dtype)\n    self.col = np.asarray(self.col, dtype=idx_dtype)\n    self.data = to_native(self.data)\n    if (self.nnz > 0):\n        if (self.row.max() >= self.shape[0]):\n            raise ValueError('row index exceeds matrix dimensions')\n        if (self.col.max() >= self.shape[1]):\n            raise ValueError('column index exceeds matrix dimensions')\n        if (self.row.min() < 0):\n            raise ValueError('negative row index found')\n        if (self.col.min() < 0):\n            raise ValueError('negative column index found')\n", "label": 1}
{"function": "\n\ndef build_controller_args(facts):\n    ' Build master controller_args '\n    cloud_cfg_path = os.path.join(facts['common']['config_base'], 'cloudprovider')\n    if ('master' in facts):\n        controller_args = {\n            \n        }\n        if ('cloudprovider' in facts):\n            if ('kind' in facts['cloudprovider']):\n                if (facts['cloudprovider']['kind'] == 'aws'):\n                    controller_args['cloud-provider'] = ['aws']\n                    controller_args['cloud-config'] = [(cloud_cfg_path + '/aws.conf')]\n                if (facts['cloudprovider']['kind'] == 'openstack'):\n                    controller_args['cloud-provider'] = ['openstack']\n                    controller_args['cloud-config'] = [(cloud_cfg_path + '/openstack.conf')]\n        if (controller_args != {\n            \n        }):\n            facts = merge_facts({\n                'master': {\n                    'controller_args': controller_args,\n                },\n            }, facts, [], [])\n    return facts\n", "label": 1}
{"function": "\n\ndef _parse_options(self, options):\n    'Parse an options string into a tuple containing the options.\\n\\n    Currently this only supports resize and crop.\\n\\n    Args:\\n      options: A str containing the url resize and crop options.\\n\\n    Returns:\\n      A tuple (resize, crop) parsed from the string.\\n\\n    Raises:\\n      InvalidRequestError: The requested resize is invalid.\\n    '\n    match = _OPTIONS_RE.search(options)\n    resize = None\n    crop = False\n    if match:\n        if match.group(1):\n            resize = int(match.group(1))\n        if match.group(2):\n            crop = True\n    if (resize and ((resize > _SIZE_LIMIT) or (resize < 0))):\n        logging.error('Invalid resize: %r', resize)\n        raise InvalidRequestError()\n    return (resize, crop)\n", "label": 1}
{"function": "\n\n@login_required\ndef vote(request):\n    env = {\n        \n    }\n    try:\n        online = VoteStatus.objects.all()[0].online\n    except:\n        online = False\n    if (not online):\n        return render(request, 'voting/vote_offline.html')\n    try:\n        cart = VoteCart.objects.get(user=request.user.id)\n    except:\n        cart = VoteCart(user=request.user, status=STATUS.ACTIVE)\n        cart.save()\n    if (cart.status == STATUS.COMPLETED):\n        return render(request, 'voting/vote_complete.html')\n    if (request.method == 'POST'):\n        form = VoteForm(request.POST)\n        if form.is_valid():\n            for (key, team) in form.cleaned_data.items():\n                category_id = key.replace('cat_', '')\n                category = Category.objects.get(id=category_id)\n                _insert_or_update_vote(cart, category, team)\n            return render(request, 'voting/vote_complete.html')\n    else:\n        votes = Vote.objects.filter(cart=cart.id)\n        form_init = {\n            \n        }\n        for vote in votes:\n            form_init[('cat_%s' % vote.category.id)] = vote.team.id\n        form = VoteForm(form_init)\n    env['form'] = form\n    return render(request, 'voting/vote.html', env)\n", "label": 1}
{"function": "\n\ndef _get_empty_context(self, context, instance, edit_fields, language, view_url, view_method, editmode=True):\n    '\\n        Inject in a copy of the context the data requested to trigger the edit.\\n\\n        `content` and `rendered_content` is emptied.\\n        '\n    if (not language):\n        language = get_language_from_request(context['request'])\n    if ((not instance) and editmode):\n        return context\n    extra_context = copy(context)\n    if (instance and isinstance(instance, Page)):\n        if (edit_fields == 'titles'):\n            edit_fields = 'title,page_title,menu_title'\n        view_url = 'admin:cms_page_edit_title_fields'\n    if (edit_fields == 'changelist'):\n        view_url = ('admin:%s_%s_changelist' % (instance._meta.app_label, instance._meta.model_name))\n    querystring = OrderedDict((('language', language),))\n    if edit_fields:\n        extra_context['edit_fields'] = edit_fields.strip().split(',')\n    if self._is_editable(context.get('request', None)):\n        extra_context.update(self._get_editable_context(extra_context, instance, language, edit_fields, view_method, view_url, querystring, editmode))\n    extra_context['content'] = ''\n    extra_context['rendered_content'] = ''\n    return extra_context\n", "label": 1}
{"function": "\n\ndef __rfcomm_advertise_service(self, name, service_id, service_classes, profiles, provider, description, protocols):\n    if (self._sdpservice is not None):\n        raise BluetoothError('Service already advertised')\n    if (not self.listening):\n        raise BluetoothError('Socket must be listening before advertised')\n    if protocols:\n        raise NotImplementedError('extra protocols not yet supported in Widcomm stack')\n    self._sdpservice = _widcomm._WCSdpService()\n    if service_classes:\n        service_classes = [to_full_uuid(s) for s in service_classes]\n        _sdp_checkraise(self._sdpservice.add_service_class_id_list(service_classes))\n    _sdp_checkraise(self._sdpservice.add_rfcomm_protocol_descriptor(self.port))\n    if profiles:\n        for (uuid, version) in profiles:\n            uuid = to_full_uuid(uuid)\n            _sdp_checkraise(self._sdpservice.add_profile_descriptor_list(uuid, version))\n    _sdp_checkraise(self._sdpservice.add_service_name(name))\n    _sdp_checkraise(self._sdpservice.make_public_browseable())\n", "label": 1}
{"function": "\n\ndef lookup(name, frame, locals):\n    'Find the value for a given name in the given environment.'\n    if (name in locals):\n        return ('local', locals[name])\n    if (name in frame.f_globals):\n        return ('global', frame.f_globals[name])\n    if ('__builtins__' in frame.f_globals):\n        builtins = frame.f_globals['__builtins__']\n        if (type(builtins) is type({\n            \n        })):\n            if (name in builtins):\n                return ('builtin', builtins[name])\n        elif hasattr(builtins, name):\n            return ('builtin', getattr(builtins, name))\n    return (None, __UNDEF__)\n", "label": 1}
{"function": "\n\ndef render(self):\n    met = (' metadata,' if (_flask_prepend == '') else '')\n    text = 't_{0} = {1}Table(\\n    {0!r},{2}\\n'.format(self.table.name, _flask_prepend, met)\n    for column in self.table.columns:\n        text += '    {0},\\n'.format(_render_column(column, True))\n    for constraint in sorted(self.table.constraints, key=_get_constraint_sort_key):\n        if isinstance(constraint, PrimaryKeyConstraint):\n            continue\n        if (isinstance(constraint, (ForeignKeyConstraint, UniqueConstraint)) and (len(constraint.columns) == 1)):\n            continue\n        text += '    {0},\\n'.format(_render_constraint(constraint))\n    for index in self.table.indexes:\n        if (len(index.columns) > 1):\n            text += '    {0},\\n'.format(_render_index(index))\n    if self.schema:\n        text += \"    schema='{0}',\\n\".format(self.schema)\n    return (text.rstrip('\\n,') + '\\n)')\n", "label": 1}
{"function": "\n\ndef include(arg, namespace=None, app_name=None):\n    if isinstance(arg, tuple):\n        if namespace:\n            raise ImproperlyConfigured('Cannot override the namespace for a dynamic module that provides a namespace')\n        (urlconf_module, app_name, namespace) = arg\n    else:\n        urlconf_module = arg\n    if isinstance(urlconf_module, six.string_types):\n        urlconf_module = import_module(urlconf_module)\n    patterns = getattr(urlconf_module, 'urlpatterns', urlconf_module)\n    if isinstance(patterns, (list, tuple)):\n        for url_pattern in patterns:\n            if isinstance(url_pattern, LocaleRegexURLResolver):\n                raise ImproperlyConfigured('Using i18n_patterns in an included URLconf is not allowed.')\n    return (urlconf_module, app_name, namespace)\n", "label": 1}
{"function": "\n\n@property\ndef _help_text(self):\n    \"Returns the help text displayed when '--help' is passed.\"\n    cmd_txt = ''\n    for label in self._visible_commands:\n        cmd = self._dispatch_map[label]\n        if ((len(cmd['aliases']) > 0) and cmd['aliases_only']):\n            if (len(cmd['aliases']) > 1):\n                first = cmd['aliases'].pop(0)\n                cmd_txt = (cmd_txt + ('  %s (aliases: %s)\\n' % (first, ', '.join(cmd['aliases']))))\n            else:\n                cmd_txt = (cmd_txt + ('  %s\\n' % cmd['aliases'][0]))\n        elif (len(cmd['aliases']) > 0):\n            cmd_txt = (cmd_txt + ('  %s (aliases: %s)\\n' % (label, ', '.join(cmd['aliases']))))\n        else:\n            cmd_txt = (cmd_txt + ('  %s\\n' % label))\n        if cmd['help']:\n            cmd_txt = (cmd_txt + ('    %s\\n\\n' % cmd['help']))\n        else:\n            cmd_txt = (cmd_txt + '\\n')\n    if (len(cmd_txt) > 0):\n        txt = ('%s\\n\\ncommands:\\n\\n%s\\n\\n\\n        ' % (self._meta.description, cmd_txt))\n    else:\n        txt = self._meta.description\n    return textwrap.dedent(txt)\n", "label": 1}
{"function": "\n\ndef get_methods(self, include_default=False):\n    for method in self.handler.allowed_methods:\n        met = getattr(self.handler, CsrfExemptResource.callmap.get(method))\n        stale = (inspect.getmodule(met) is handler)\n        if (not self.handler.is_anonymous):\n            if (met and ((not stale) or include_default)):\n                (yield HandlerMethod(met, stale))\n        elif ((not stale) or ((met.__name__ == 'read') and ('GET' in self.allowed_methods))):\n            (yield HandlerMethod(met, stale))\n", "label": 1}
{"function": "\n\ndef _handle_custom_action(self, method, remainder, request=None):\n    if (request is None):\n        self._raise_method_deprecation_warning(self._handle_custom_action)\n    remainder = [r for r in remainder if r]\n    if remainder:\n        if (method in ('put', 'delete')):\n            method_name = remainder[0]\n            remainder = remainder[1:]\n        else:\n            method_name = remainder[(- 1)]\n            remainder = remainder[:(- 1)]\n        if (method.upper() in self._custom_actions.get(method_name, [])):\n            controller = self._find_controller(('%s_%s' % (method, method_name)), method_name)\n            if controller:\n                return (controller, remainder)\n", "label": 1}
{"function": "\n\ndef _get_non_unique_error(self, queryset):\n    '\\n        Generate error message when some of the field has more than one value.\\n        '\n    non_unique = {\n        \n    }\n    for field in self._invoice_report_common_fields:\n        items = queryset.values(field).distinct()\n        if (items.count() != 1):\n            if (field == 'invoice_date'):\n                data = ', '.join((item[field].strftime('%Y-%m-%d') for item in items if item[field]))\n            else:\n                data = ', '.join((item[field] for item in items if item[field]))\n            non_unique[field] = data\n    non_unique_items = ' '.join(['{}: {}'.format(key, value) for (key, value) in non_unique.items() if value])\n    return '{}: {}'.format(_('Selected items have different'), non_unique_items)\n", "label": 1}
{"function": "\n\ndef list_tasks(self, app_id=None, **kwargs):\n    'List running tasks, optionally filtered by app_id.\\n\\n        :param str app_id: if passed, only show tasks for this application\\n        :param kwargs: arbitrary search filters\\n\\n        :returns: list of tasks\\n        :rtype: list[:class:`marathon.models.task.MarathonTask`]\\n        '\n    response = self._do_request('GET', '/v2/tasks')\n    tasks = self._parse_response(response, MarathonTask, is_list=True, resource_name='tasks')\n    if app_id:\n        tasks = [task for task in tasks if (task.app_id.lstrip('/') == app_id.lstrip('/'))]\n    [setattr(t, 'app_id', app_id) for t in tasks if (app_id and (t.app_id is None))]\n    for (k, v) in kwargs.items():\n        tasks = [o for o in tasks if (getattr(o, k) == v)]\n    return tasks\n", "label": 1}
{"function": "\n\n@classmethod\ndef decode_obj(cls, obj):\n    if isinstance(obj, dict):\n        obj = dict(((key, cls.decode_obj(val)) for (key, val) in obj.items()))\n    elif isinstance(obj, list):\n        obj = list((cls.decode_obj(val) for val in obj))\n    if ((not isinstance(obj, dict)) or (len(obj) != 1)):\n        return obj\n    obj_tag = next(iter(obj.keys()))\n    if (not obj_tag.startswith('!')):\n        return obj\n    if (obj_tag not in json_tags):\n        raise ValueError('Unknown tag', obj_tag)\n    obj_cls = json_tags[obj_tag]\n    return obj_cls.decode_json_obj(obj[obj_tag])\n", "label": 1}
{"function": "\n\ndef enableFutureFeature(object_name, future_spec, source_ref):\n    if (object_name == 'unicode_literals'):\n        future_spec.enableUnicodeLiterals()\n    elif (object_name == 'absolute_import'):\n        future_spec.enableAbsoluteImport()\n    elif (object_name == 'division'):\n        future_spec.enableFutureDivision()\n    elif (object_name == 'print_function'):\n        future_spec.enableFuturePrint()\n    elif ((object_name == 'barry_as_FLUFL') and (python_version >= 300)):\n        future_spec.enableBarry()\n    elif (object_name == 'generator_stop'):\n        future_spec.enableGeneratorStop()\n    elif (object_name == 'braces'):\n        SyntaxErrors.raiseSyntaxError('not a chance', source_ref)\n    elif (object_name in ('nested_scopes', 'generators', 'with_statement')):\n        pass\n    else:\n        SyntaxErrors.raiseSyntaxError(('future feature %s is not defined' % object_name), source_ref)\n", "label": 1}
{"function": "\n\ndef pretty(self, indent=0):\n    lines = list()\n    lines.append(('resource', self.uri))\n    if self.name:\n        lines.append(('identifier', repr(self.name)))\n    if self.resource_type:\n        lines.append(('resource type', self.resource_type))\n    if self.resource_size:\n        lines.append(('resource size', str(self.resource_size)))\n    for lang in sorted(self.title):\n        lines.append((('title[%s]' % lang), self.title[lang]))\n    for icon in sorted(self.icons):\n        info = '{0} ... ({1} bytes)'.format(repr(self.icons[icon][:10]).strip(\"'\"), len(self.icons[icon]))\n        lines.append((('icon[%s]' % icon), info))\n    lines.append(('action', self.action))\n    indent = (indent * ' ')\n    lwidth = max([len(line[0]) for line in lines])\n    lines = [((line[0].ljust(lwidth) + ' = ') + line[1]) for line in lines]\n    return '\\n'.join([(indent + line) for line in lines])\n", "label": 1}
{"function": "\n\ndef _restore_state(self, obj, instance):\n    state = self._restore(obj[tags.STATE])\n    has_slots = (isinstance(state, tuple) and (len(state) == 2) and isinstance(state[1], dict))\n    has_slots_and_dict = (has_slots and isinstance(state[0], dict))\n    if hasattr(instance, '__setstate__'):\n        instance.__setstate__(state)\n    elif isinstance(state, dict):\n        self._restore_from_dict(state, instance, ignorereserved=False)\n    elif has_slots:\n        self._restore_from_dict(state[1], instance, ignorereserved=False)\n        if has_slots_and_dict:\n            self._restore_from_dict(state[0], instance, ignorereserved=False)\n    elif ((not hasattr(instance, '__getnewargs__')) and (not hasattr(instance, '__getnewargs_ex__'))):\n        instance = state\n    return instance\n", "label": 1}
{"function": "\n\ndef result_processor(self, dialect, coltype):\n    wants_unicode = (self.convert_unicode or dialect.convert_unicode)\n    needs_convert = (wants_unicode and ((dialect.returns_unicode_strings is not True) or (self.convert_unicode in ('force', 'force_nocheck'))))\n    needs_isinstance = (needs_convert and dialect.returns_unicode_strings and (self.convert_unicode != 'force_nocheck'))\n    if needs_convert:\n        to_unicode = processors.to_unicode_processor_factory(dialect.encoding, self.unicode_error)\n        if needs_isinstance:\n            return processors.to_conditional_unicode_processor_factory(dialect.encoding, self.unicode_error)\n        else:\n            return processors.to_unicode_processor_factory(dialect.encoding, self.unicode_error)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef computeExpression(self, constraint_collection):\n    pos_arg = self.getPositionalArgument()\n    pairs = self.getNamedArgumentPairs()\n    if (pos_arg is None):\n        new_node = ExpressionMakeDict(pairs=self.getNamedArgumentPairs(), source_ref=self.source_ref)\n        return (new_node, 'new_expression', \"Replace 'dict' built-in call dictionary creation from arguments.\")\n    pos_iteration_length = pos_arg.getIterationLength()\n    if (pos_iteration_length == 0):\n        new_node = ExpressionMakeDict(pairs=self.getNamedArgumentPairs(), source_ref=self.source_ref)\n        new_node = wrapExpressionWithNodeSideEffects(old_node=ExpressionBuiltinIter1(value=pos_arg, source_ref=self.source_ref), new_node=new_node)\n        if pos_arg.mayRaiseExceptionIter(BaseException):\n            constraint_collection.onExceptionRaiseExit(BaseException)\n        return (new_node, 'new_expression', \"Replace 'dict' built-in call dictionary creation from arguments.\")\n    if ((pos_iteration_length is not None) and ((pos_iteration_length + len(pairs)) < 256) and self.hasOnlyConstantArguments()):\n        if (pos_arg is not None):\n            pos_args = (pos_arg,)\n        else:\n            pos_args = None\n        return constraint_collection.getCompileTimeComputationResult(node=self, computation=(lambda : builtin_dict_spec.simulateCall((pos_args, self.getNamedArgumentPairs()))), description=\"Replace 'dict' call with constant arguments.\")\n    else:\n        constraint_collection.onExceptionRaiseExit(BaseException)\n        return (self, None, None)\n", "label": 1}
{"function": "\n\ndef __lt__(self, other):\n    \" Used for custom sort on note's timestamp\\n        :param other: NoteListWidgetItem to compare against\\n        :return: boolean True if should be sorted below\\n        \"\n    if (other.notemodel.pinned and self.notemodel.pinned):\n        return (self.notemodel.timestamp < other.notemodel.timestamp)\n    elif ((not other.notemodel.pinned) and (not self.notemodel.pinned)):\n        return (self.notemodel.timestamp < other.notemodel.timestamp)\n    elif (other.notemodel.pinned and (not self.notemodel.pinned)):\n        return True\n    elif ((not other.notemodel.pinned) and self.notemodel.pinned):\n        return False\n", "label": 1}
{"function": "\n\ndef run(options):\n    options = opt_validate(options)\n    info('Read and build bedGraph...')\n    bio = BedGraphIO.bedGraphIO(options.ifile)\n    btrack = bio.build_bdgtrack(baseline_value=0)\n    info('Modify bedGraph...')\n    if (options.method.lower() == 'p2q'):\n        btrack.p2q()\n    elif (options.method.lower() == 'analen'):\n        btrack.analen()\n    else:\n        extraparam = float(options.extraparam[0])\n        if (options.method.lower() == 'multiply'):\n            btrack.apply_func((lambda x: (x * extraparam)))\n        elif (options.method.lower() == 'add'):\n            btrack.apply_func((lambda x: (x + extraparam)))\n        elif (options.method.lower() == 'max'):\n            btrack.apply_func((lambda x: (x if (x > extraparam) else extraparam)))\n        elif (options.method.lower() == 'min'):\n            btrack.apply_func((lambda x: (x if (x < extraparam) else extraparam)))\n    ofile = os.path.join(options.outdir, options.ofile)\n    info('Write bedGraph of modified scores...')\n    ofhd = open(ofile, 'wb')\n    btrack.write_bedGraph(ofhd, name=('%s_modified_scores' % options.method.upper()), description=('Scores calculated by %s' % options.method.upper()))\n    info((\"Finished '%s'! Please check '%s'!\" % (options.method, ofile)))\n", "label": 1}
{"function": "\n\ndef __init__(self, environment, source_manifest, target_manifest, pip_install_path, formula_dict=None):\n    ' generate a feature dict from Manifests <source_manifest> and <target_manifest> '\n    self._environment = environment\n    self._run_order = []\n    self._formula_dict = (formula_dict or {\n        \n    })\n    self._pip = Pip(pip_install_path)\n    self._pip.delete_all_eggs()\n    if target_manifest:\n        for feature in target_manifest.sections():\n            feature_key = self._instantiate_feature(feature, target_manifest, 'target')\n            if feature_key:\n                self._run_order.append(feature_key)\n    if source_manifest:\n        for feature in source_manifest.sections():\n            feature_key = self._instantiate_feature(feature, source_manifest, 'source')\n            if feature_key:\n                self._run_order.append(feature_key)\n", "label": 1}
{"function": "\n\ndef collect(self, objs, source=None, nullable=False, collect_related=True, source_attr=None, reverse_dependency=False):\n    if self.can_fast_delete(objs):\n        self.fast_deletes.append(objs)\n        return\n    new_objs = self.add(objs, source, nullable, reverse_dependency=reverse_dependency)\n    if (not new_objs):\n        return\n    model = new_objs[0].__class__\n    if collect_related:\n        for related in model._meta.get_all_related_objects(include_hidden=True, include_proxy_eq=True, local_only=True):\n            field = related.field\n            if (field.rel.on_delete == DO_NOTHING):\n                continue\n            sub_objs = self.related_objects(related, new_objs)\n            if self.can_fast_delete(sub_objs, from_field=field):\n                self.fast_deletes.append(sub_objs)\n            elif sub_objs:\n                field.rel.on_delete(self, field, sub_objs, self.using)\n        for field in model._meta.virtual_fields:\n            if hasattr(field, 'bulk_related_objects'):\n                sub_objs = field.bulk_related_objects(new_objs, self.using)\n                self.collect(sub_objs, source=model, source_attr=field.rel.related_name, nullable=True)\n", "label": 1}
{"function": "\n\ndef i2repr(self, pkt, x):\n    if ((type(x) is list) or (type(x) is tuple)):\n        return repr(x)\n    if self.multi:\n        r = []\n    else:\n        r = ''\n    i = 0\n    while x:\n        if (x & 1):\n            if self.multi:\n                r += [self.names[i]]\n            else:\n                r += self.names[i]\n        i += 1\n        x >>= 1\n    if self.multi:\n        r = '+'.join(r)\n    return r\n", "label": 1}
{"function": "\n\ndef _update_dbo_fields(cls, new_attrs):\n    for (name, attr) in new_attrs.items():\n        if hasattr(attr, 'hydrate'):\n            old_attr = cls.dbo_fields.get(name)\n            if (old_attr != attr):\n                if (old_attr and (old_attr.default == attr.default)):\n                    log.warn('Unnecessary override of attr {} in class {}', name, cls.__name__)\n                elif (old_attr and old_attr.default):\n                    log.info('Overriding default value of attr{} in class {}', name, cls.__name__)\n                cls.dbo_fields[name] = attr\n        elif isinstance(attr, DBOTField):\n            cls.dbot_fields[name] = attr\n    load_func = new_attrs.get('on_loaded')\n    if load_func:\n        cls.load_funcs.append(load_func)\n", "label": 1}
{"function": "\n\ndef has_perm(self, user, perm, obj=None):\n    if (not obj):\n        return\n    if isinstance(obj, TestObj):\n        if (user.username == 'test2'):\n            return True\n        elif (user.is_anonymous() and (perm == 'anon')):\n            return True\n        elif ((not user.is_active) and (perm == 'inactive')):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    exempt_exact_urls = getattr(settings, 'DOWNTIME_EXEMPT_EXACT_URLS', None)\n    if exempt_exact_urls:\n        for url in exempt_exact_urls:\n            if (request.path == url):\n                return None\n    exempt_paths = getattr(settings, 'DOWNTIME_EXEMPT_PATHS', ('/admin',))\n    for path in exempt_paths:\n        if request.path.startswith(path):\n            return None\n    objects = Period.objects.is_down()\n    if objects.count():\n        url_redirect = getattr(settings, 'DOWNTIME_URL_REDIRECT', None)\n        if url_redirect:\n            return redirect(url_redirect)\n        else:\n            return render(request, 'downtime/downtime.html', status=503)\n", "label": 1}
{"function": "\n\n@property\ndef describe_fips_code(self):\n    'Returns human-readable description'\n    fips = len(getattr(self, 'FipsCode', ''))\n    if (fips == 0):\n        return 'No FipsCode'\n    elif ((fips >= 1) and (fips <= 2)):\n        return 'State code'\n    elif ((fips >= 3) and (fips <= 5)):\n        return 'County code'\n    elif ((fips >= 6) and (fips <= 10)):\n        return 'City code'\n    else:\n        return 'Unknown'\n", "label": 1}
{"function": "\n\ndef _port_action_vxlan(self, port, segment, func):\n    'Verify configuration and then process event.'\n    if (segment is None):\n        self._log_missing_segment()\n        return\n    device_id = port.get('device_id')\n    mcast_group = segment.get(api.PHYSICAL_NETWORK)\n    host_id = port.get(portbindings.HOST_ID)\n    vni = segment.get(api.SEGMENTATION_ID)\n    if (vni and device_id and mcast_group and host_id):\n        func(vni, device_id, mcast_group, host_id)\n        return vni\n    else:\n        fields = ('vni ' if (not vni) else '')\n        fields += ('device_id ' if (not device_id) else '')\n        fields += ('mcast_group ' if (not mcast_group) else '')\n        fields += ('host_id' if (not host_id) else '')\n        raise excep.NexusMissingRequiredFields(fields=fields)\n", "label": 1}
{"function": "\n\ndef learn_from_treebanks(self, treebanks):\n    self.sym_count = Counter()\n    self.unary_count = Counter()\n    self.binary_count = Counter()\n    self.words_count = Counter()\n    for treebank in treebanks:\n        for s in open(treebank):\n            self.__count(loads(s))\n    for (word, count) in self.words_count.iteritems():\n        if (count >= PCFG.RARE_WORD_COUNT):\n            self.well_known_words.add(word)\n    norm = Counter()\n    for ((x, word), count) in self.unary_count.iteritems():\n        norm[(x, self.norm_word(word))] += count\n    self.unary_count = norm\n    for ((x, word), count) in self.unary_count.iteritems():\n        self.q1[(x, word)] = (self.unary_count[(x, word)] / self.sym_count[x])\n    for ((x, y1, y2), count) in self.binary_count.iteritems():\n        self.q2[(x, y1, y2)] = (self.binary_count[(x, y1, y2)] / self.sym_count[x])\n    self.__build_caches()\n", "label": 1}
{"function": "\n\ndef start_engines(opts, proc_mgr, proxy=None):\n    '\\n    Fire up the configured engines!\\n    '\n    if (opts['__role'] == 'master'):\n        runners = salt.loader.runner(opts)\n    else:\n        runners = []\n    utils = salt.loader.utils(opts)\n    funcs = salt.loader.minion_mods(opts, utils=utils)\n    engines = salt.loader.engines(opts, funcs, runners, proxy=proxy)\n    engines_opt = opts.get('engines', [])\n    if isinstance(engines_opt, dict):\n        engines_opt = [{\n            k: v,\n        } for (k, v) in engines_opt.items()]\n    if salt.utils.is_windows():\n        runners = None\n        utils = None\n        funcs = None\n    for engine in engines_opt:\n        if isinstance(engine, dict):\n            (engine, engine_opts) = next(iter(engine.items()))\n        else:\n            engine_opts = None\n        fun = '{0}.start'.format(engine)\n        if (fun in engines):\n            start_func = engines[fun]\n            name = '{0}.Engine({1})'.format(__name__, start_func.__module__)\n            log.info('Starting Engine {0}'.format(name))\n            proc_mgr.add_process(Engine, args=(opts, fun, engine_opts, funcs, runners, proxy), name=name)\n", "label": 1}
{"function": "\n\ndef _create_control(self, parent):\n    if ((len(self.default_path) != 0) and (len(self.default_directory) == 0) and (len(self.default_filename) == 0)):\n        (default_directory, default_filename) = os.path.split(self.default_path)\n    else:\n        default_directory = self.default_directory\n        default_filename = self.default_filename\n    filters = []\n    for filter_list in self.wildcard.split('|')[::2]:\n        filter_list = filter_list.replace(';', ' ')\n        filters.append(filter_list)\n    if (not default_directory):\n        default_directory = QtCore.QDir.currentPath()\n    dlg = QtGui.QFileDialog(parent, self.title, default_directory)\n    dlg.setViewMode(QtGui.QFileDialog.Detail)\n    dlg.selectFile(default_filename)\n    dlg.setNameFilters(filters)\n    if (self.wildcard_index < len(filters)):\n        dlg.selectNameFilter(filters[self.wildcard_index])\n    if (self.action == 'open'):\n        dlg.setAcceptMode(QtGui.QFileDialog.AcceptOpen)\n        dlg.setFileMode(QtGui.QFileDialog.ExistingFile)\n    elif (self.action == 'open files'):\n        dlg.setAcceptMode(QtGui.QFileDialog.AcceptOpen)\n        dlg.setFileMode(QtGui.QFileDialog.ExistingFiles)\n    else:\n        dlg.setAcceptMode(QtGui.QFileDialog.AcceptSave)\n        dlg.setFileMode(QtGui.QFileDialog.AnyFile)\n    return dlg\n", "label": 1}
{"function": "\n\ndef CalculateSlices(self, maxWidth, columnWidths):\n    '\\n        Return a list of integer pairs, where each pair represents\\n        the left and right columns that can fit into the width of one page\\n        '\n    firstColumn = 0\n    if (hasattr(self.lv, 'useExpansionColumn') and self.lv.useExpansionColumn):\n        firstColumn = 1\n    if (self.IsShrinkToFit() or (sum(columnWidths) <= maxWidth)):\n        return [[firstColumn, (len(columnWidths) - 1)]]\n    pairs = list()\n    left = firstColumn\n    right = firstColumn\n    while (right < len(columnWidths)):\n        if (sum(columnWidths[left:(right + 1)]) > maxWidth):\n            if (left == right):\n                pairs.append([left, right])\n                left += 1\n                right += 1\n            else:\n                pairs.append([left, (right - 1)])\n                left = right\n        else:\n            right += 1\n    if (left < len(columnWidths)):\n        pairs.append([left, (right - 1)])\n    return pairs\n", "label": 1}
{"function": "\n\ndef liupc(self):\n    \"Liu's algorithm, for pre-determination of the Elimination Tree of\\n        the given matrix, used in row-based symbolic Cholesky factorization.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.matrices import SparseMatrix\\n        >>> S = SparseMatrix([\\n        ... [1, 0, 3, 2],\\n        ... [0, 0, 1, 0],\\n        ... [4, 0, 0, 5],\\n        ... [0, 6, 7, 0]])\\n        >>> S.liupc()\\n        ([[0], [], [0], [1, 2]], [4, 3, 4, 4])\\n\\n        References\\n        ==========\\n\\n        Symbolic Sparse Cholesky Factorization using Elimination Trees,\\n        Jeroen Van Grondelle (1999)\\n        http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.7582\\n        \"\n    R = [[] for r in range(self.rows)]\n    for (r, c, _) in self.row_list():\n        if (c <= r):\n            R[r].append(c)\n    inf = len(R)\n    parent = ([inf] * self.rows)\n    virtual = ([inf] * self.rows)\n    for r in range(self.rows):\n        for c in R[r][:(- 1)]:\n            while (virtual[c] < r):\n                t = virtual[c]\n                virtual[c] = r\n                c = t\n            if (virtual[c] == inf):\n                parent[c] = virtual[c] = r\n    return (R, parent)\n", "label": 1}
{"function": "\n\ndef get_declared_fields(bases, attrs, with_base_fields=True):\n    \"\\n    Create a list of form field instances from the passed in 'attrs', plus any\\n    similar fields on the base classes (in 'bases'). This is used by both the\\n    Form and ModelForm metaclasses.\\n\\n    If 'with_base_fields' is True, all fields from the bases are used.\\n    Otherwise, only fields in the 'declared_fields' attribute on the bases are\\n    used. The distinction is useful in ModelForm subclassing.\\n    Also integrates any additional media definitions.\\n    \"\n    fields = [(field_name, attrs.pop(field_name)) for (field_name, obj) in list(six.iteritems(attrs)) if isinstance(obj, Field)]\n    fields.sort(key=(lambda x: x[1].creation_counter))\n    if with_base_fields:\n        for base in bases[::(- 1)]:\n            if hasattr(base, 'base_fields'):\n                fields = (list(six.iteritems(base.base_fields)) + fields)\n    else:\n        for base in bases[::(- 1)]:\n            if hasattr(base, 'declared_fields'):\n                fields = (list(six.iteritems(base.declared_fields)) + fields)\n    return OrderedDict(fields)\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef reserveCommit(self, header, connection_id, request_info=None):\n    log.msg('', system=LOG_SYSTEM)\n    log.msg(('ReserveCommit request. NSA: %s. Connection ID: %s' % (header.requester_nsa, connection_id)), system=LOG_SYSTEM)\n    conn = (yield self.getConnection(connection_id))\n    if (conn.lifecycle_state == state.TERMINATED):\n        raise error.ConnectionGoneError(('Connection %s has been terminated' % connection_id))\n    (yield state.reserveCommit(conn))\n    defs = []\n    sub_connections = (yield self.getSubConnectionsByConnectionKey(conn.id))\n    for sc in sub_connections:\n        provider = self.getProvider(sc.provider_nsa)\n        req_header = nsa.NSIHeader(self.nsa_.urn(), sc.provider_nsa, security_attributes=header.security_attributes)\n        d = provider.reserveCommit(req_header, sc.connection_id, request_info)\n        d.addErrback(_logErrorResponse, connection_id, sc.provider_nsa, 'provision')\n        defs.append(d)\n    results = (yield defer.DeferredList(defs, consumeErrors=True))\n    successes = [r[0] for r in results]\n    if all(successes):\n        log.msg(('Connection %s: ReserveCommit messages acked' % conn.connection_id), system=LOG_SYSTEM)\n        defer.returnValue(connection_id)\n    else:\n        n_success = sum([1 for s in successes if s])\n        log.msg(('Connection %s. Only %i of %i commit acked successfully' % (connection_id, n_success, len(defs))), system=LOG_SYSTEM)\n        provider_urns = [sc.provider_nsa for sc in sub_connections]\n        raise _createAggregateException(connection_id, 'committed', results, provider_urns, error.ConnectionError)\n", "label": 1}
{"function": "\n\ndef render(self, context):\n    request = context['request']\n    category = self.category.resolve(context)\n    try:\n        categories = map(int, category.split(','))\n    except:\n        context[self.var_name] = []\n        return ''\n    allowed_objects = get_allowed_objects(request.user, ArticleCategory, 'reader')\n    categories = [c for c in categories if (c in allowed_objects)]\n    if categories:\n        categories = ArticleCategory.objects.filter(pk__in=categories)\n        mao = get_allowed_objects(request.user, ArticleCategory, 'reader')\n        stories = ArticleStory.objects.select_related().filter(categories__in=categories).distinct().order_by('-time_updated')[:int(self.limit)]\n        for story in stories:\n            story.current_categories = story.categories.all()\n            story.current_category = story.current_categories[0]\n            story.url = reverse('view_story', args=[story.current_category.pk, story.pk, story.slug])\n            intersect = set(mao).intersection(set([category.pk for category in story.current_categories]))\n            if intersect:\n                story.edit__url = reverse('edit_story', args=[story.pk])\n    else:\n        stories = []\n    context[self.var_name] = stories\n    return ''\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_request(http_method, http_url, headers=None, parameters=None, query_string=None):\n    if (parameters is None):\n        parameters = {\n            \n        }\n    if (headers and ('HTTP_AUTHORIZATION' in headers)):\n        auth_header = headers['HTTP_AUTHORIZATION']\n        if (auth_header.index('OAuth') > (- 1)):\n            try:\n                header_params = OAuthRequest._split_header(auth_header)\n                parameters.update(header_params)\n            except:\n                raise OAuthError('Unable to parse OAuth parameters from Authorization header.')\n    if query_string:\n        query_params = OAuthRequest._split_url_string(query_string)\n        parameters.update(query_params)\n    param_str = urlparse.urlparse(http_url)[4]\n    url_params = OAuthRequest._split_url_string(param_str)\n    parameters.update(url_params)\n    if parameters:\n        return OAuthRequest(http_method, http_url, parameters)\n    return None\n", "label": 1}
{"function": "\n\ndef expand_variant(self, variant_call):\n    if (not self.is_snp(variant_call)):\n        return variant_call\n    expansion_calls = []\n    for sample_id in self.sample_refs.keys():\n        ref_call = self.sample_refs[sample_id]\n        if ((self.get_start(ref_call) <= self.get_start(variant_call)) and (self.get_end(ref_call) >= (self.get_start(variant_call) + 1))):\n            expansion_calls.extend(ref_call['call'])\n        else:\n            del self.sample_refs[sample_id]\n    if self.filter_ref_matches:\n        variant_sample_names = [call['call_set_name'] for call in variant_call['call']]\n        variant_call['call'].extend([call for call in expansion_calls if (call['call_set_name'] not in variant_sample_names)])\n    else:\n        variant_call['call'].extend(expansion_calls)\n    return variant_call\n", "label": 1}
{"function": "\n\ndef db_children(self, parent=(None, None), orphan=False):\n    children = []\n    if (self._db_location is not None):\n        children.extend(self._db_location.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_functions:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_annotations:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_portSpecs:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    if orphan:\n        for child in self._db_functions[:]:\n            self.db_delete_function(child)\n        for child in self._db_annotations[:]:\n            self.db_delete_annotation(child)\n        for child in self._db_portSpecs[:]:\n            self.db_delete_portSpec(child)\n        self._db_location = None\n    children.append((self, parent[0], parent[1]))\n    return children\n", "label": 1}
{"function": "\n\ndef match_rating_codex(s):\n    if isinstance(s, bytes):\n        raise TypeError(_no_bytes_err)\n    s = s.upper()\n    codex = []\n    prev = None\n    for (i, c) in enumerate(s):\n        if (((c != ' ') and ((i == 0) and (c in 'AEIOU'))) or ((c not in 'AEIOU') and (c != prev))):\n            codex.append(c)\n        prev = c\n    if (len(codex) > 6):\n        return ''.join((codex[:3] + codex[(- 3):]))\n    else:\n        return ''.join(codex)\n", "label": 1}
{"function": "\n\ndef convert_to_markdown(contributors_map, include_tickets=False):\n\n    def compare(item1, item2):\n        lastname1 = item1.split(' ')[(- 1)].lower()\n        lastname2 = item2.split(' ')[(- 1)].lower()\n        return cmp(lastname1, lastname2)\n    names = contributors_map.keys()\n    names = sorted(names, cmp=compare)\n    result = []\n    for name in names:\n        tickets = contributors_map[name]\n        tickets_string = []\n        for ticket in tickets:\n            if ('-' not in ticket):\n                continue\n            number = ticket.split('-')[1]\n            if ticket.startswith('LIBCLOUD-'):\n                url = (JIRA_URL % number)\n            elif (ticket.startswith('GITHUB-') or ticket.startswith('GH-')):\n                url = (GITHUB_URL % number)\n            values = {\n                'ticket': ticket,\n                'url': url,\n            }\n            tickets_string.append(('[%(ticket)s](%(url)s)' % values))\n        tickets_string = ', '.join(tickets_string)\n        if include_tickets:\n            line = ('* %(name)s: %(tickets)s' % {\n                'name': name,\n                'tickets': tickets_string,\n            })\n        else:\n            line = ('* %(name)s' % {\n                'name': name,\n            })\n        result.append(line.strip())\n    result = '\\n'.join(result)\n    return result\n", "label": 1}
{"function": "\n\ndef discover_client_scheme(self, context):\n    (ts, v0, v1, v2, v3) = _s_ts_ver.unpack_from(context, 0)\n    client_ver = (v0, v1, v2, v3)\n    if (client_ver == NO_VERSION):\n        log.info(\"no client version - won't use crypto handshake\")\n        return None\n    log.debug('client version: %s', client_ver)\n    scheme = find_client_offset_scheme(client_ver)\n    log.debug('scheme from ver: %s', scheme)\n    if (scheme is not None):\n        if self._check_client_scheme(scheme, context):\n            log.debug('verified client scheme: %s', scheme)\n            self._digest_offset_extractor = schemes[scheme][0]\n            return scheme\n    log.debug('trying all known schemes...')\n    for i in xrange(len(schemes)):\n        if (i == scheme):\n            continue\n        if self._check_client_scheme(i, context):\n            log.debug('verified client scheme: %s', i)\n            self._digest_offset_extractor = schemes[i][0]\n            return i\n    log.info(\"couldn't figure the client scheme out\")\n    if (not self.strict):\n        log.debug('selecting scheme anyway: %s', OFFSET_SCHEME_1)\n        return OFFSET_SCHEME_1\n    return None\n", "label": 1}
{"function": "\n\ndef extractParagraphProperties(self, style, parent=None):\n    ' Extracts paragraph properties from a style element. '\n    paraProps = ParagraphProps()\n    name = style.getAttribute('style:name')\n    if name.startswith('Heading_20_'):\n        level = name[11:]\n        try:\n            level = int(level)\n            paraProps.setHeading(level)\n        except:\n            level = 0\n    if (name == 'Title'):\n        paraProps.setTitle(True)\n    paraPropEl = style.getElementsByTagName('style:paragraph-properties')\n    if paraPropEl:\n        paraPropEl = paraPropEl[0]\n        leftMargin = paraPropEl.getAttribute('fo:margin-left')\n        if leftMargin:\n            try:\n                leftMargin = float(leftMargin[:(- 2)])\n                if (leftMargin > 0.01):\n                    paraProps.setIndented(True)\n            except:\n                pass\n    textProps = self.extractTextProperties(style)\n    if textProps.fixed:\n        paraProps.setCode(True)\n    return paraProps\n", "label": 1}
{"function": "\n\ndef add_palette(self, color, palette_type, palette_name):\n    'Add pallete.'\n    if (palette_type == '__special__'):\n        if (palette_name == 'Favorites'):\n            favs = util.get_favs()['colors']\n            if (color not in favs):\n                favs.append(color)\n            util.save_palettes(favs, favs=True)\n            self.show_color_info(update=True)\n    elif (palette_type in ('__global__', '__project__')):\n        if (palette_type == '__global__'):\n            color_palettes = util.get_palettes()\n        else:\n            color_palettes = util.get_project_palettes(self.view.window())\n        for palette in color_palettes:\n            if (palette_name == palette['name']):\n                if (color not in palette['colors']):\n                    palette['colors'].append(color)\n                    if (palette_type == '__global__'):\n                        util.save_palettes(color_palettes)\n                    else:\n                        util.save_project_palettes(self.view.window(), color_palettes)\n                    self.show_color_info(update=True)\n                    break\n", "label": 1}
{"function": "\n\ndef start_process(self, process, args=None, cwd=None, env=None):\n    '\\n        Starts a process interactively.\\n\\n        :param process: Process to run\\n        :type process: str\\n\\n        :param args: List of arguments (list of str)\\n        :type args: list\\n\\n        :param cwd: Working directory\\n        :type cwd: str\\n\\n        :param env: environment variables (dict).\\n        '\n    self.setReadOnly(False)\n    if (env is None):\n        env = {\n            \n        }\n    if (args is None):\n        args = []\n    if (not self._running):\n        self.process = QProcess()\n        self.process.finished.connect(self._on_process_finished)\n        self.process.started.connect(self.process_started.emit)\n        self.process.error.connect(self._write_error)\n        self.process.readyReadStandardError.connect(self._on_stderr)\n        self.process.readyReadStandardOutput.connect(self._on_stdout)\n        if cwd:\n            self.process.setWorkingDirectory(cwd)\n        e = self.process.systemEnvironment()\n        ev = QProcessEnvironment()\n        for v in e:\n            values = v.split('=')\n            ev.insert(values[0], '='.join(values[1:]))\n        for (k, v) in env.items():\n            ev.insert(k, v)\n        self.process.setProcessEnvironment(ev)\n        self._running = True\n        self._process_name = process\n        self._args = args\n        if self._clear_on_start:\n            self.clear()\n        self._user_stop = False\n        self._write_started()\n        self.process.start(process, args)\n        self.process.waitForStarted()\n    else:\n        _logger().warning('a process is already running')\n", "label": 1}
{"function": "\n\ndef update_created_pools(self):\n    \"\\n        When the set of live nodes change, the loadbalancer will change its\\n        mind on host distances. It might change it on the node that came/left\\n        but also on other nodes (for instance, if a node dies, another\\n        previously ignored node may be now considered).\\n\\n        This method ensures that all hosts for which a pool should exist\\n        have one, and hosts that shouldn't don't.\\n\\n        For internal use only.\\n        \"\n    for host in self.cluster.metadata.all_hosts():\n        distance = self._load_balancer.distance(host)\n        pool = self._pools.get(host)\n        if ((not pool) or pool.is_shutdown):\n            if ((distance != HostDistance.IGNORED) and host.is_up):\n                self.add_or_renew_pool(host, False)\n        elif (distance != pool.host_distance):\n            if (distance == HostDistance.IGNORED):\n                self.remove_pool(host)\n            else:\n                pool.host_distance = distance\n", "label": 1}
{"function": "\n\ndef generate_stylesheet(self):\n    for name in self.stylestack:\n        styles = self.styledict.get(name)\n        if (('__style-family' in styles) and (styles['__style-family'] in self.styledict)):\n            familystyle = self.styledict[styles['__style-family']].copy()\n            del styles['__style-family']\n            for (style, val) in list(styles.items()):\n                familystyle[style] = val\n            styles = familystyle\n        while (('__parent-style-name' in styles) and (styles['__parent-style-name'] in self.styledict)):\n            parentstyle = self.styledict[styles['__parent-style-name']].copy()\n            del styles['__parent-style-name']\n            for (style, val) in list(styles.items()):\n                parentstyle[style] = val\n            styles = parentstyle\n        self.styledict[name] = styles\n    self.writeout(self.default_styles)\n    for name in self.stylestack:\n        styles = self.styledict.get(name)\n        css2 = self.cs.convert_styles(styles)\n        self.writeout(('%s {\\n' % name))\n        for (style, val) in list(css2.items()):\n            self.writeout(('\\t%s: %s;\\n' % (style, val)))\n        self.writeout('}\\n')\n", "label": 1}
{"function": "\n\ndef escape(text, newline=False):\n    'Escape special html characters.'\n    if isinstance(text, str):\n        if ('&' in text):\n            text = text.replace('&', '&amp;')\n        if ('>' in text):\n            text = text.replace('>', '&gt;')\n        if ('<' in text):\n            text = text.replace('<', '&lt;')\n        if ('\"' in text):\n            text = text.replace('\"', '&quot;')\n        if (\"'\" in text):\n            text = text.replace(\"'\", '&quot;')\n        if newline:\n            if ('\\n' in text):\n                text = text.replace('\\n', '<br>')\n    return text\n", "label": 1}
{"function": "\n\ndef blockedSample(sampler, sample_size, predicates, *args):\n    blocked_sample = set()\n    remaining_sample = (sample_size - len(blocked_sample))\n    previous_sample_size = 0\n    while (remaining_sample and predicates):\n        random.shuffle(predicates)\n        new_sample = sampler(remaining_sample, predicates, *args)\n        filtered_sample = (subsample for subsample in new_sample if subsample)\n        blocked_sample.update(itertools.chain.from_iterable(filtered_sample))\n        growth = (len(blocked_sample) - previous_sample_size)\n        growth_rate = (growth / remaining_sample)\n        remaining_sample = (sample_size - len(blocked_sample))\n        previous_sample_size = len(blocked_sample)\n        if (growth_rate < 0.001):\n            warnings.warn(('%s blocked samples were requested, but only able to sample %s' % (sample_size, len(blocked_sample))))\n            break\n        predicates = [pred for (pred, pred_sample) in zip(predicates, new_sample) if (pred_sample or (pred_sample is None))]\n    return blocked_sample\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_deployment_tasks(cls, instance, graph_type=None):\n    'Get deployment graph based on release version.\\n\\n        :param instance: Release instance\\n        :type instance: models.Release\\n        :param graph_type: deployment graph type\\n        :type graph_type: basestring|None\\n        :returns: list of deployment tasks\\n        :rtype: list\\n        '\n    if (graph_type is None):\n        graph_type = consts.DEFAULT_DEPLOYMENT_GRAPH_TYPE\n    env_version = instance.environment_version\n    deployment_graph = DeploymentGraph.get_for_model(instance, graph_type)\n    if deployment_graph:\n        deployment_tasks = DeploymentGraph.get_tasks(deployment_graph)\n    else:\n        deployment_tasks = []\n    if ((graph_type == consts.DEFAULT_DEPLOYMENT_GRAPH_TYPE) and (not deployment_tasks)):\n        if env_version.startswith('5.0'):\n            deployment_tasks = yaml.load(graph_configuration.DEPLOYMENT_50)\n        elif (env_version.startswith('5.1') or env_version.startswith('6.0')):\n            deployment_tasks = yaml.load(graph_configuration.DEPLOYMENT_51_60)\n        if deployment_graph:\n            if deployment_tasks:\n                DeploymentGraph.update(deployment_graph, {\n                    'tasks': deployment_tasks,\n                })\n        else:\n            DeploymentGraph.create_for_model({\n                'tasks': deployment_tasks,\n            }, instance)\n    return deployment_tasks\n", "label": 1}
{"function": "\n\ndef reachable(stepFunction, start, destinations, _alreadyseen=None):\n    ' Determines the subset of destinations that can be reached from a set of starting positions,\\n    while using stepFunction (which produces a list of neighbor states) to navigate.\\n    Uses breadth-first search.\\n    Returns a dictionary with reachable destinations and their distances.\\n    '\n    if ((len(start) == 0) or (len(destinations) == 0)):\n        return {\n            \n        }\n    if (_alreadyseen is None):\n        _alreadyseen = []\n    _alreadyseen.extend(start)\n    res = {\n        \n    }\n    for s in start:\n        if (s in destinations):\n            res[s] = 0\n            start.remove(s)\n    new = set()\n    for s in start:\n        new.update(stepFunction(s))\n    new.difference_update(_alreadyseen)\n    ndestinations = list(destinations)\n    for s in list(new):\n        if (s in destinations):\n            res[s] = 1\n            new.remove(s)\n            ndestinations.remove(s)\n            _alreadyseen.append(s)\n    deeper = reachable(stepFunction, new, ndestinations, _alreadyseen)\n    for (k, val) in list(deeper.items()):\n        res[k] = (val + 1)\n    return res\n", "label": 1}
{"function": "\n\ndef _restore_from_dict(self, obj, instance, ignorereserved=True):\n    restore_key = self._restore_key_fn()\n    method = _obj_setattr\n    for (k, v) in sorted(obj.items(), key=util.itemgetter):\n        if (ignorereserved and (k in tags.RESERVED)):\n            continue\n        if isinstance(k, numeric_types):\n            str_k = unicode(k)\n        else:\n            str_k = k\n        self._namestack.append(str_k)\n        k = restore_key(k)\n        value = self._restore(v)\n        if (util.is_noncomplex(instance) or util.is_dictionary_subclass(instance)):\n            instance[k] = value\n        else:\n            setattr(instance, k, value)\n        if isinstance(value, _Proxy):\n            self._proxies.append((instance, k, value, method))\n        self._namestack.pop()\n", "label": 1}
{"function": "\n\ndef clean(self, value):\n    super(DEIdentityCardNumberField, self).clean(value)\n    if (value in EMPTY_VALUES):\n        return ''\n    match = re.match(id_re, value)\n    if (not match):\n        raise ValidationError(self.error_messages['invalid'])\n    gd = match.groupdict()\n    (residence, origin) = (gd['residence'], gd['origin'])\n    (birthday, validity, checksum) = (gd['birthday'], gd['validity'], gd['checksum'])\n    if ((residence == '0000000000') or (birthday == '0000000') or (validity == '0000000')):\n        raise ValidationError(self.error_messages['invalid'])\n    all_digits = ('%s%s%s%s' % (residence, birthday, validity, checksum))\n    if ((not self.has_valid_checksum(residence)) or (not self.has_valid_checksum(birthday)) or (not self.has_valid_checksum(validity)) or (not self.has_valid_checksum(all_digits))):\n        raise ValidationError(self.error_messages['invalid'])\n    return ('%s%s-%s-%s-%s' % (residence, origin, birthday, validity, checksum))\n", "label": 1}
{"function": "\n\ndef __init__(self, desired_capabilities=None, executable_path=None, port=0, service_log_path=None, service_args=None, opera_options=None):\n    engine = (desired_capabilities.get('engine', None) if desired_capabilities else None)\n    if ((engine == WebDriver.ServiceType.CHROMIUM) or (opera_options and opera_options.android_package_name)):\n        OperaDriver.__init__(self, executable_path=executable_path, port=port, opera_options=opera_options, service_args=service_args, desired_capabilities=desired_capabilities, service_log_path=service_log_path)\n    else:\n        if service_log_path:\n            print((\"Warning! service_log_path shouldn't be used \" + 'with Presto based Opera'))\n        if service_args:\n            print((\"Warning! service_args shouldn't be used with \" + 'Presto based Opera'))\n        if opera_options:\n            print((\"Warning! opera_options shouldn't be used with \" + 'Presto based Opera'))\n        if (not desired_capabilities):\n            desired_capabilities = DesiredCapabilities.OPERA\n        PrestoDriver.__init__(self, executable_path=executable_path, port=port, desired_capabilities=desired_capabilities)\n", "label": 1}
{"function": "\n\ndef receiveMessage(self, msg, sender):\n    if isinstance(msg, ActorExitRequest):\n        pass\n    elif ('what light' in msg):\n        self.send(sender, 'Ay me!')\n    elif (msg == 'She speaks!'):\n        self.send(sender, 'O Romeo, Romeo! wherefore art thou Romeo?')\n    elif (msg == 'Shall I hear more, or shall I speak at this?'):\n        self.send(sender, \"What's in a name? That which we call a rose\")\n        self.send(sender, 'By any other name would smell as sweet')\n    elif (msg == 'Like softest music to attending ears!'):\n        if self.nurse:\n            self.send(self.nurse, 'Anon, good nurse!')\n        else:\n            self.recalled = True\n    elif (msg == 'Mistress!'):\n        self.nurse = sender\n        if self.recalled:\n            self.send(self.nurse, 'Anon, good nurse!')\n    elif ('who_are_you' == msg):\n        self.send(sender, self.myAddress)\n", "label": 1}
{"function": "\n\ndef get_values(config_file='config.ini'):\n    if (not os.path.isfile(config_file)):\n        logger.error('config file is missing')\n        raise\n    source_csv = {\n        'csv_file': '',\n        'fieldnames': '',\n    }\n    for key in source_csv.iterkeys():\n        source_csv[key] = utils.get_config_value(config_file, 'CSV', key)\n    mapping = {\n        \n    }\n    for field in REDMINE_ISSUE_FIELDS:\n        mapping[field] = utils.get_config_value(config_file, 'CSV', field)\n    source_csv['fieldnames_mapping'] = mapping\n    dest_redmine = {\n        'username': '',\n        'password': '',\n        'id': '',\n        'url': '',\n        'name': '',\n        'sf_domain': '',\n        'versions_to_skip': [],\n        'issues_to_skip': [],\n    }\n    for key in dest_redmine.iterkeys():\n        dest_redmine[key] = utils.get_config_value(config_file, 'DEST_REDMINE', key)\n    if dest_redmine['url'].endswith('/'):\n        dest_redmine['url'] = dest_redmine['url'][:(- 1)]\n    versions_to_skip = utils.get_config_value(config_file, 'SKIP', 'version_id')\n    if versions_to_skip:\n        dest_redmine['versions_to_skip'] = versions_to_skip.split(',')\n    issues_to_skip = utils.get_config_value(config_file, 'SKIP', 'issue_id')\n    if issues_to_skip:\n        dest_redmine['issues_to_skip'] = issues_to_skip.split(',')\n    dest_redmine['mapper'] = utils.ConfigMapper('config.ini')\n    return (source_csv, dest_redmine)\n", "label": 1}
{"function": "\n\ndef _format_num(self, value):\n    if (value is None):\n        return None\n    num = decimal.Decimal(value)\n    if self.allow_nan:\n        if num.is_nan():\n            return decimal.Decimal('NaN')\n    elif (num.is_nan() or num.is_infinite()):\n        self.fail('special')\n    if ((self.places is not None) and num.is_finite()):\n        num = num.quantize(self.places, rounding=self.rounding)\n    return num\n", "label": 1}
{"function": "\n\n@transaction.atomic\ndef moderate(request, forum_id):\n    forum = get_object_or_404(Forum, pk=forum_id)\n    topics = forum.topics.order_by('-sticky', '-updated').select_related()\n    if (request.user.is_superuser or (request.user in forum.moderators.all())):\n        topic_ids = request.POST.getlist('topic_id')\n        if ('move_topics' in request.POST):\n            return render(request, 'djangobb_forum/move_topic.html', {\n                'categories': Category.objects.all(),\n                'topic_ids': topic_ids,\n                'exclude_forum': forum,\n            })\n        elif ('delete_topics' in request.POST):\n            for topic_id in topic_ids:\n                topic = get_object_or_404(Topic, pk=topic_id)\n                topic.delete()\n            messages.success(request, _('Topics deleted'))\n            return HttpResponseRedirect(reverse('djangobb:index'))\n        elif ('open_topics' in request.POST):\n            for topic_id in topic_ids:\n                open_close_topic(request, topic_id, 'o')\n            messages.success(request, _('Topics opened'))\n            return HttpResponseRedirect(reverse('djangobb:index'))\n        elif ('close_topics' in request.POST):\n            for topic_id in topic_ids:\n                open_close_topic(request, topic_id, 'c')\n            messages.success(request, _('Topics closed'))\n            return HttpResponseRedirect(reverse('djangobb:index'))\n        return render(request, 'djangobb_forum/moderate.html', {\n            'forum': forum,\n            'topics_page': get_page(topics, request, forum_settings.FORUM_PAGE_SIZE),\n            'posts': forum.posts.count(),\n        })\n    else:\n        raise Http404\n", "label": 1}
{"function": "\n\ndef ns_checker(self, i3s_output_list, i3s_config):\n    response = {\n        'full_text': '',\n    }\n    counter = 0\n    error = False\n    nameservers = []\n    if (not isinstance(self.nameservers, list)):\n        self.nameservers = self.nameservers.split(',')\n    if (not isinstance(self.resolvers, list)):\n        self.resolvers = self.resolvers.split(',')\n    my_resolver = dns.resolver.Resolver()\n    my_resolver.lifetime = self.lifetime\n    if self.resolvers:\n        my_resolver.nameservers = self.resolvers\n    my_ns = my_resolver.query(self.domain, 'NS')\n    for ns in my_ns:\n        nameservers.append(str(socket.gethostbyname(str(ns))))\n    for ns in self.nameservers:\n        nameservers.append(str(ns))\n    for ns in nameservers:\n        my_resolver.nameservers = [ns]\n        counter += 1\n        try:\n            my_resolver.query(self.domain, 'A')\n        except:\n            error = True\n    if error:\n        response['full_text'] = (str(counter) + ' NS NOK')\n        response['color'] = i3s_config['color_bad']\n    else:\n        response['full_text'] = (str(counter) + ' NS OK')\n        response['color'] = i3s_config['color_good']\n    return response\n", "label": 1}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    if (not view.match_selector(locations[0], 'text.html - source - meta.tag, punctuation.definition.tag.begin')):\n        return []\n    lines = [view.substr(sublime.Region(view.line(l).a, l)) for l in locations]\n    lines = [l[::(- 1)] for l in lines]\n    rex = re.compile('([\\\\w-]+)([.#])(\\\\w+)')\n    expr = match(rex, lines[0])\n    if (not expr):\n        return []\n    for i in xrange(1, len(lines)):\n        ex = match(rex, lines[i])\n        if (ex != expr):\n            return []\n    (arg, op, tag) = rex.match(expr).groups()\n    arg = arg[::(- 1)]\n    tag = tag[::(- 1)]\n    expr = expr[::(- 1)]\n    if (op == '.'):\n        snippet = '<{0} class=\"{1}\">$1</{0}>$0'.format(tag, arg)\n    else:\n        snippet = '<{0} id=\"{1}\">$1</{0}>$0'.format(tag, arg)\n    return [(expr, snippet)]\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@register_specialize\n@gof.local_optimizer([T.Rebroadcast])\ndef local_rebroadcast_lift(node):\n    '\\n    Lifts Rebroadcast through unary Elemwise operations,\\n    and merges consecutive Rebroadcasts.\\n\\n    Rebroadcast(Elemwise(x)) => Elemwise(Rebroadcast(x))\\n    Rebroadcast(Rebroadcast(x)) => Rebroadcast(x)\\n\\n    '\n    op = node.op\n    if (not isinstance(op, T.Rebroadcast)):\n        return False\n    input = node.inputs[0]\n    inode = input.owner\n    if (inode and isinstance(inode.op, Elemwise) and (len(inode.inputs) == 1)):\n        if (hasattr(input, 'clients') and (len(input.clients) == 1)):\n            rebroadcasted = T.Rebroadcast(*list(op.axis.items()))(inode.inputs[0])\n            copy_stack_trace(node.outputs, rebroadcasted)\n            rval = inode.op.make_node(rebroadcasted).outputs\n            copy_stack_trace((node.outputs + node.inputs), rval)\n            return rval\n    if (inode and isinstance(inode.op, T.Rebroadcast)):\n        axis = inode.op.axis.copy()\n        axis.update(op.axis)\n        iinput = inode.inputs[0]\n        rval = [T.Rebroadcast(*list(axis.items()))(iinput)]\n        copy_stack_trace((node.outputs + node.inputs), rval)\n        return rval\n", "label": 1}
{"function": "\n\ndef post_save_handler(self, sender, instance, **kwargs):\n    '\\n        Creates new moderation object if instance is created,\\n        If instance exists and is only updated then save instance as\\n        content_object of moderated_object\\n        '\n    if kwargs['raw']:\n        return\n    pk = instance.pk\n    moderator = self.get_moderator(sender)\n    if kwargs['created']:\n        old_object = sender._default_manager.get(pk=pk)\n        moderated_obj = ModeratedObject(content_object=old_object)\n        if (not moderator.visible_until_rejected):\n            moderated_obj.moderation_state = MODERATION_DRAFT_STATE\n        moderated_obj.save()\n        moderator.inform_moderator(instance)\n        return\n    moderated_obj = ModeratedObject.objects.get_for_instance(instance)\n    if ((moderated_obj.moderation_status == MODERATION_STATUS_APPROVED) and moderator.bypass_moderation_after_approval):\n        moderated_obj.changed_object = instance\n        moderated_obj.save()\n        return\n    if moderated_obj.has_object_been_changed(instance):\n        copied_instance = self._copy_model_instance(instance)\n        if (not moderator.visible_until_rejected):\n            moderated_obj.changed_object.save_base(raw=True)\n            moderated_obj.changed_object = copied_instance\n        moderated_obj.moderation_status = MODERATION_STATUS_PENDING\n        moderated_obj.save()\n        moderator.inform_moderator(instance)\n        instance._moderated_object = moderated_obj\n", "label": 1}
{"function": "\n\ndef __init__(self, url=None, method=None, body='', params=None, headers=None, cookies=None, auth=None, timeout=60, allow_redirects=False, proxies=None, files=None, verify=False):\n    if (url is None):\n        raise Exception('URL must be specified.')\n    if (method is None):\n        if (files or body):\n            method = 'POST'\n        else:\n            method = 'GET'\n    headers = (headers or {\n        \n    })\n    normalized_headers = self._normalize_headers(headers=headers)\n    if (body and ('content-length' not in normalized_headers)):\n        headers['Content-Length'] = len(body)\n    self.url = url\n    self.method = method\n    self.headers = headers\n    self.body = body\n    self.params = params\n    self.headers = headers\n    self.cookies = cookies\n    self.auth = auth\n    self.timeout = timeout\n    self.allow_redirects = allow_redirects\n    self.proxies = proxies\n    self.files = files\n    self.verify = verify\n", "label": 1}
{"function": "\n\ndef get_result(self):\n    op = self.expr.op()\n    if isinstance(op, ops.Join):\n        self._walk_join_tree(op)\n    else:\n        self.join_tables.append(self._format_table(self.expr))\n    result = self.join_tables[0]\n    for (jtype, table, preds) in zip(self.join_types, self.join_tables[1:], self.join_predicates):\n        if len(preds):\n            sqla_preds = [self._translate(pred) for pred in preds]\n            onclause = _and_all(sqla_preds)\n        else:\n            onclause = None\n        if (jtype in (ops.InnerJoin, ops.CrossJoin)):\n            result = result.join(table, onclause)\n        elif (jtype is ops.LeftJoin):\n            result = result.join(table, onclause, isouter=True)\n        elif (jtype is ops.RightJoin):\n            result = table.join(result, onclause, isouter=True)\n        elif (jtype is ops.OuterJoin):\n            result = result.outerjoin(table, onclause)\n        else:\n            raise NotImplementedError(jtype)\n    return result\n", "label": 1}
{"function": "\n\ndef __init__(self, default=missing_, attribute=None, load_from=None, dump_to=None, error=None, validate=None, required=False, allow_none=None, load_only=False, dump_only=False, missing=missing_, error_messages=None, **metadata):\n    self.default = default\n    self.attribute = attribute\n    self.load_from = load_from\n    self.dump_to = dump_to\n    self.validate = validate\n    if utils.is_iterable_but_not_string(validate):\n        if (not utils.is_generator(validate)):\n            self.validators = validate\n        else:\n            self.validators = list(validate)\n    elif callable(validate):\n        self.validators = [validate]\n    elif (validate is None):\n        self.validators = []\n    else:\n        raise ValueError(\"The 'validate' parameter must be a callable or a collection of callables.\")\n    self.required = required\n    if (allow_none is None):\n        if (missing is None):\n            self.allow_none = True\n        else:\n            self.allow_none = False\n    else:\n        self.allow_none = allow_none\n    self.load_only = load_only\n    self.dump_only = dump_only\n    self.missing = missing\n    self.metadata = metadata\n    self._creation_index = Field._creation_index\n    Field._creation_index += 1\n    messages = {\n        \n    }\n    for cls in reversed(self.__class__.__mro__):\n        messages.update(getattr(cls, 'default_error_messages', {\n            \n        }))\n    messages.update((error_messages or {\n        \n    }))\n    self.error_messages = messages\n", "label": 1}
{"function": "\n\n@classmethod\ndef assign(cls, cluster, member_metadata):\n    all_topics = set()\n    for metadata in six.itervalues(member_metadata):\n        all_topics.update(metadata.subscription)\n    all_topic_partitions = []\n    for topic in all_topics:\n        partitions = cluster.partitions_for_topic(topic)\n        if (partitions is None):\n            log.warning('No partition metadata for topic %s', topic)\n            continue\n        for partition in partitions:\n            all_topic_partitions.append(TopicPartition(topic, partition))\n    all_topic_partitions.sort()\n    assignment = collections.defaultdict((lambda : collections.defaultdict(list)))\n    member_iter = itertools.cycle(sorted(member_metadata.keys()))\n    for partition in all_topic_partitions:\n        member_id = next(member_iter)\n        while (partition.topic not in member_metadata[member_id].subscription):\n            member_id = next(member_iter)\n        assignment[member_id][partition.topic].append(partition.partition)\n    protocol_assignment = {\n        \n    }\n    for member_id in member_metadata:\n        protocol_assignment[member_id] = ConsumerProtocolMemberAssignment(cls.version, sorted(assignment[member_id].items()), b'')\n    return protocol_assignment\n", "label": 1}
{"function": "\n\n@classmethod\ndef recv_schema_change(cls, f, protocol_version):\n    change_type = read_string(f)\n    if (protocol_version >= 3):\n        target = read_string(f)\n        keyspace = read_string(f)\n        event = {\n            'target_type': target,\n            'change_type': change_type,\n            'keyspace': keyspace,\n        }\n        if (target != SchemaTargetType.KEYSPACE):\n            target_name = read_string(f)\n            if (target == SchemaTargetType.FUNCTION):\n                event['function'] = UserFunctionDescriptor(target_name, [read_string(f) for _ in range(read_short(f))])\n            elif (target == SchemaTargetType.AGGREGATE):\n                event['aggregate'] = UserAggregateDescriptor(target_name, [read_string(f) for _ in range(read_short(f))])\n            else:\n                event[target.lower()] = target_name\n    else:\n        keyspace = read_string(f)\n        table = read_string(f)\n        if table:\n            event = {\n                'target_type': SchemaTargetType.TABLE,\n                'change_type': change_type,\n                'keyspace': keyspace,\n                'table': table,\n            }\n        else:\n            event = {\n                'target_type': SchemaTargetType.KEYSPACE,\n                'change_type': change_type,\n                'keyspace': keyspace,\n            }\n    return event\n", "label": 1}
{"function": "\n\n@put('/s/notices/{notice_id}')\ndef mark_notice_route(request, notice_id):\n    '\\n    Mark notice as read or unread.\\n    Must be logged in as user, provide a valid ID, and own the notice.\\n    Return notice.\\n    '\n    db_conn = request['db_conn']\n    current_user = get_current_user(request)\n    if (not current_user):\n        return abort(401)\n    notice = Notice.get(db_conn, id=notice_id)\n    if (not notice):\n        return abort(404)\n    if (notice['user_id'] != current_user['id']):\n        return abort(403)\n    if ('read' not in request['params']):\n        errors = [{\n            'name': 'read',\n            'message': 'You must specify read or unread.',\n        }]\n    elif (request['params'].get('read') is True):\n        (notice, errors) = notice.mark_as_read(db_conn)\n    elif (request['params'].get('read') is False):\n        (notice, errors) = notice.mark_as_unread(db_conn)\n    if len(errors):\n        return (400, {\n            'errors': errors,\n            'ref': 'qR4CBtcfcYfWDTqK9JOXXLhO',\n        })\n    return (200, {\n        'notice': notice.deliver(access='private'),\n    })\n", "label": 1}
{"function": "\n\ndef test_mock_add_spec(self):\n\n    class _One(object):\n        one = 1\n\n    class _Two(object):\n        two = 2\n\n    class Anything(object):\n        one = two = three = 'four'\n    klasses = [Mock, MagicMock, NonCallableMock, NonCallableMagicMock]\n    for Klass in list(klasses):\n        klasses.append((lambda K=Klass: K(spec=Anything)))\n        klasses.append((lambda K=Klass: K(spec_set=Anything)))\n    for Klass in klasses:\n        for kwargs in (dict(), dict(spec_set=True)):\n            mock = Klass()\n            (mock.one, mock.two, mock.three)\n            for (One, Two) in [(_One, _Two), (['one'], ['two'])]:\n                for kwargs in (dict(), dict(spec_set=True)):\n                    mock.mock_add_spec(One, **kwargs)\n                    mock.one\n                    self.assertRaises(AttributeError, getattr, mock, 'two')\n                    self.assertRaises(AttributeError, getattr, mock, 'three')\n                    if ('spec_set' in kwargs):\n                        self.assertRaises(AttributeError, setattr, mock, 'three', None)\n                    mock.mock_add_spec(Two, **kwargs)\n                    self.assertRaises(AttributeError, getattr, mock, 'one')\n                    mock.two\n                    self.assertRaises(AttributeError, getattr, mock, 'three')\n                    if ('spec_set' in kwargs):\n                        self.assertRaises(AttributeError, setattr, mock, 'three', None)\n", "label": 1}
{"function": "\n\ndef get_contact_info(domain):\n    cache_key = ('sms-chat-contact-list-%s' % domain)\n    cache_expiration = (30 * 60)\n    try:\n        client = cache_core.get_redis_client()\n        cached_data = client.get(cache_key)\n        if cached_data:\n            return json.loads(cached_data)\n    except:\n        pass\n    verified_number_ids = VerifiedNumber.by_domain(domain, ids_only=True)\n    domain_obj = Domain.get_by_name(domain, strict=True)\n    case_ids = []\n    mobile_worker_ids = []\n    data = []\n    for doc in iter_docs(VerifiedNumber.get_db(), verified_number_ids):\n        owner_id = doc['owner_id']\n        if (doc['owner_doc_type'] == 'CommCareCase'):\n            case_ids.append(owner_id)\n            data.append([None, 'case', doc['phone_number'], owner_id, doc['_id']])\n        elif (doc['owner_doc_type'] == 'CommCareUser'):\n            mobile_worker_ids.append(owner_id)\n            data.append([None, 'mobile_worker', doc['phone_number'], owner_id, doc['_id']])\n    contact_data = get_case_contact_info(domain_obj, case_ids)\n    contact_data.update(get_mobile_worker_contact_info(domain_obj, mobile_worker_ids))\n    for row in data:\n        contact_info = contact_data.get(row[3])\n        row[0] = (contact_info[0] if contact_info else _('(unknown)'))\n    try:\n        client.set(cache_key, json.dumps(data))\n        client.expire(cache_key, cache_expiration)\n    except:\n        pass\n    return data\n", "label": 1}
{"function": "\n\n@staticmethod\ndef validate_type(value, type_name, force=False):\n    is_ok = False\n    if (not force):\n        if ((type_name == 'any') or (type_name == 'void') or (value is None)):\n            return\n        clazz = type(value)\n        if (type_name == 'function'):\n            is_ok = isinstance(value, types.FunctionType)\n        elif (type_name == 'str'):\n            is_ok = isinstance(value, (six.text_type, six.string_types))\n        elif (type_name in {'float', 'int', 'bool', 'dict', 'list'}):\n            is_ok = (type(value).__name__ == type_name)\n        else:\n            p = type_name.split('.')\n            if (1 < len(p)):\n                c = p.pop()\n                m = __import__('.'.join(p), fromlist=[c])\n                is_ok = isinstance(value, getattr(m, c))\n            else:\n                c = globals()['__builtins__'][type_name]\n                is_ok = isinstance(value, c)\n    if (not is_ok):\n        raise SaklientException('argument_type_mismatch', (((('Argument type mismatch (expected ' + type_name) + ', got ') + str(type(value))) + ')'))\n", "label": 1}
{"function": "\n\ndef append_from_cursor(self):\n    si = self.current\n    if ((len(si.search) > 0) and (not si.selected)):\n        return\n    view = self.view\n    limit = view.size()\n    if si.selected:\n        point = si.selected[(- 1)].end()\n    else:\n        point = self.point[0].b\n    if (point >= limit):\n        return\n    helper = self.util\n    search = si.search\n    separators = SettingsManager.get('sbp_word_separators', default_sbp_word_separators)\n    case_sensitive = (re.search('[A-Z]', search) is not None)\n\n    def append_one(ch):\n        if (not case_sensitive):\n            ch = ch.lower()\n        if (self.regex and (ch in '{}()[].*+')):\n            return ('\\\\' + ch)\n        return ch\n    if (point < limit):\n        search += append_one(view.substr(point))\n        point += 1\n        self.on_change(search)\n        while ((point < limit) and helper.is_word_char(point, True, separators)):\n            ch = view.substr(point)\n            search += append_one(ch)\n            self.on_change(search)\n            point += 1\n    self.set_text(self.current.search)\n", "label": 1}
{"function": "\n\ndef update_zone(self, zone, domain=None, type=None, ttl=None, extra=None):\n    extra = (extra if extra else {\n        \n    })\n    if domain:\n        raise LibcloudError('Domain cannot be changed', driver=self)\n    data = {\n        \n    }\n    if ttl:\n        data['ttl'] = int(ttl)\n    if ('email' in extra):\n        data['emailAddress'] = extra['email']\n    if ('comment' in extra):\n        data['comment'] = extra['comment']\n    type = (type if type else zone.type)\n    ttl = (ttl if ttl else zone.ttl)\n    self.connection.set_context({\n        'resource': 'zone',\n        'id': zone.id,\n    })\n    self.connection.async_request(action=('/domains/%s' % zone.id), method='PUT', data=data)\n    merged = merge_valid_keys(params=copy.deepcopy(zone.extra), valid_keys=VALID_ZONE_EXTRA_PARAMS, extra=extra)\n    updated_zone = get_new_obj(obj=zone, klass=Zone, attributes={\n        'type': type,\n        'ttl': ttl,\n        'extra': merged,\n    })\n    return updated_zone\n", "label": 1}
{"function": "\n\ndef to_utf8(obj):\n    'Walks a simple data structure, converting unicode to byte string.\\n\\n    Supports lists, tuples, and dictionaries.\\n    '\n    if isinstance(obj, unicode_type):\n        return _utf8(obj)\n    elif isinstance(obj, dict):\n        return dict(((to_utf8(k), to_utf8(v)) for (k, v) in obj.items()))\n    elif isinstance(obj, list):\n        return list((to_utf8(i) for i in obj))\n    elif isinstance(obj, tuple):\n        return tuple((to_utf8(i) for i in obj))\n    return obj\n", "label": 1}
{"function": "\n\ndef compat(self, token, iterable):\n    startline = False\n    indents = []\n    toks_append = self.tokens.append\n    (toknum, tokval) = token\n    if (toknum in (NAME, NUMBER)):\n        tokval += ' '\n    if (toknum in (NEWLINE, NL)):\n        startline = True\n    for tok in iterable:\n        (toknum, tokval) = tok[:2]\n        if (toknum in (NAME, NUMBER)):\n            tokval += ' '\n        if (toknum == INDENT):\n            indents.append(tokval)\n            continue\n        elif (toknum == DEDENT):\n            indents.pop()\n            continue\n        elif (toknum in (NEWLINE, NL)):\n            startline = True\n        elif (startline and indents):\n            toks_append(indents[(- 1)])\n            startline = False\n        toks_append(tokval)\n", "label": 1}
{"function": "\n\ndef compare(self, label, model, field):\n    return ((self.label or self.model or self.field) and ((self.label is None) or (self.label == label)) and ((self.model is None) or self.match_model(model)) and ((self.field is None) or (self.field == field)))\n", "label": 1}
{"function": "\n\ndef select_treasury_duration(start_date, end_date):\n    td = (end_date - start_date)\n    if (td.days <= 31):\n        treasury_duration = '1month'\n    elif (td.days <= 93):\n        treasury_duration = '3month'\n    elif (td.days <= 186):\n        treasury_duration = '6month'\n    elif (td.days <= 366):\n        treasury_duration = '1year'\n    elif (td.days <= ((365 * 2) + 1)):\n        treasury_duration = '2year'\n    elif (td.days <= ((365 * 3) + 1)):\n        treasury_duration = '3year'\n    elif (td.days <= ((365 * 5) + 2)):\n        treasury_duration = '5year'\n    elif (td.days <= ((365 * 7) + 2)):\n        treasury_duration = '7year'\n    elif (td.days <= ((365 * 10) + 2)):\n        treasury_duration = '10year'\n    else:\n        treasury_duration = '30year'\n    return treasury_duration\n", "label": 1}
{"function": "\n\ndef headerData(self, section, orientation, role=Qt.DisplayRole):\n    if (role == Qt.TextAlignmentRole):\n        if (orientation == Qt.Horizontal):\n            return to_qvariant(int((Qt.AlignHCenter | Qt.AlignVCenter)))\n        return to_qvariant(int((Qt.AlignRight | Qt.AlignVCenter)))\n    if (role != Qt.DisplayRole):\n        return to_qvariant()\n    if (orientation == Qt.Horizontal):\n        if (section == NAME):\n            return to_qvariant('Name')\n        elif (section == VERSION):\n            return to_qvariant('Version')\n        elif (section == ACTION):\n            return to_qvariant('Action')\n        elif (section == DESCRIPTION):\n            return to_qvariant('Description')\n    return to_qvariant()\n", "label": 1}
{"function": "\n\ndef replace(self, col):\n    if ((not self.magic_flag) and isinstance(col, FromClause) and self.selectable.is_derived_from(col)):\n        return self.selectable\n    elif (not isinstance(col, ColumnElement)):\n        return None\n    elif (self.include_fn and (not self.include_fn(col))):\n        return None\n    elif (self.exclude_fn and self.exclude_fn(col)):\n        return None\n    else:\n        return self._corresponding_column(col, True)\n", "label": 1}
{"function": "\n\ndef paste_config(gconfig, config_url, relative_to, global_conf=None):\n    sys.path.insert(0, relative_to)\n    pkg_resources.working_set.add_entry(relative_to)\n    config_url = config_url.split('#')[0]\n    cx = loadwsgi.loadcontext(SERVER, config_url, relative_to=relative_to, global_conf=global_conf)\n    (gc, lc) = (cx.global_conf.copy(), cx.local_conf.copy())\n    cfg = {\n        \n    }\n    (host, port) = (lc.pop('host', ''), lc.pop('port', ''))\n    if (host and port):\n        cfg['bind'] = ('%s:%s' % (host, port))\n    elif host:\n        cfg['bind'] = host.split(',')\n    cfg['default_proc_name'] = gc.get('__file__')\n    config_file = config_url.split(':')[1]\n    if _has_logging_config(config_file):\n        cfg.setdefault('logconfig', config_file)\n    for (k, v) in gc.items():\n        if (k not in gconfig.settings):\n            continue\n        cfg[k] = v\n    for (k, v) in lc.items():\n        if (k not in gconfig.settings):\n            continue\n        cfg[k] = v\n    return cfg\n", "label": 1}
{"function": "\n\ndef createLimitOrder(self, action, instrument, limitPrice, quantity):\n    if (instrument != common.btc_symbol):\n        raise Exception('Only BTC instrument is supported')\n    if (action == broker.Order.Action.BUY_TO_COVER):\n        action = broker.Order.Action.BUY\n    elif (action == broker.Order.Action.SELL_SHORT):\n        action = broker.Order.Action.SELL\n    if ((limitPrice * quantity) < BacktestingBroker.MIN_TRADE_USD):\n        raise Exception(('Trade must be >= %s' % BacktestingBroker.MIN_TRADE_USD))\n    if (action == broker.Order.Action.BUY):\n        fee = self.getCommission().calculate(None, limitPrice, quantity)\n        cashRequired = ((limitPrice * quantity) + fee)\n        if (cashRequired > self.getCash(False)):\n            raise Exception('Not enough cash')\n    elif (action == broker.Order.Action.SELL):\n        if (quantity > self.getShares(common.btc_symbol)):\n            raise Exception(('Not enough %s' % common.btc_symbol))\n    else:\n        raise Exception('Only BUY/SELL orders are supported')\n    return super(BacktestingBroker, self).createLimitOrder(action, instrument, limitPrice, quantity)\n", "label": 1}
{"function": "\n\ndef get_history(self, limit=None):\n    youngest_rev = self.repos.youngest_rev\n    rev = self.rev\n    path = self.path\n    while (rev > 0):\n        if (limit is not None):\n            if (limit <= 0):\n                return\n            limit -= 1\n        if (rev == 1):\n            change = Changeset.ADD\n        elif (rev == 40):\n            change = Changeset.MOVE\n        elif (rev == youngest_rev):\n            change = Changeset.DELETE\n        else:\n            change = Changeset.EDIT\n        (yield (path, rev, change))\n        if (rev == 40):\n            path += '-old'\n        rev -= 3\n", "label": 1}
{"function": "\n\ndef setup(self):\n    'Create the blob directory and database if necessary, start all desired services'\n    log.debug('Setting up the lbry session')\n    if (self.lbryid is None):\n        self.lbryid = generate_id()\n    if (self.wallet is None):\n        self.wallet = PTCWallet(self.db_dir)\n    if (self.peer_manager is None):\n        self.peer_manager = PeerManager()\n    if (self.use_upnp is True):\n        d = self._try_upnp()\n    else:\n        d = defer.succeed(True)\n    if (self.peer_finder is None):\n        d.addCallback((lambda _: self._setup_dht()))\n    elif ((self.hash_announcer is None) and (self.peer_port is not None)):\n        log.warning('The server has no way to advertise its available blobs.')\n        self.hash_announcer = DummyHashAnnouncer()\n    d.addCallback((lambda _: self._setup_other_components()))\n    return d\n", "label": 1}
{"function": "\n\ndef build(self, wait_time=60):\n    for container in self.start_order:\n        if (not self.config['containers'][container]):\n            sys.stderr.write((('Error: no configuration found for container: ' + container) + '\\n'))\n            exit(1)\n        config = self.config['containers'][container]\n        if ('base_image' not in config):\n            sys.stderr.write(('Error: no base image specified for container: ' + container))\n            exit(1)\n        base = config['base_image']\n        self._handleRequire(container, wait_time)\n        count = tag_name = 1\n        if ('count' in config):\n            count = tag_name = config['count']\n        while (count > 0):\n            name = container\n            if (tag_name > 1):\n                name = ((name + '__') + str(count))\n            self.log.info('Building container: %s using base template %s', name, base)\n            build = Container(name, copy.deepcopy(config))\n            dockerfile = None\n            if ('buildspec' in config):\n                if ('dockerfile' in config['buildspec']):\n                    dockerfile = config['buildspec']['dockerfile']\n                if ('url' in config['buildspec']):\n                    dockerfile_url = config['buildspec']['url']\n            build.build(dockerfile)\n            self.containers[name] = build\n            count = (count - 1)\n", "label": 1}
{"function": "\n\ndef fetch_item_bodies(self, item_ids, format='json', media_rss=False, authenticated=True):\n    query_params = {\n        'output': format,\n        'ann': 'false',\n        'likes': 'true',\n    }\n    if media_rss:\n        query_params['mediaRss'] = 'true'\n    post_params = {\n        'i': [i.decimal_form for i in item_ids],\n    }\n    result_text = self._fetch('stream/items/contents', query_params, post_params, authenticated=authenticated)\n    result = {\n        \n    }\n    if format.startswith('atom'):\n        feed = base.atom.parse(result_text)\n        for entry in feed.entries:\n            result[entry.item_id] = entry\n    else:\n        item_bodies_json = json.loads(result_text)\n        for item_body_json in item_bodies_json['items']:\n            item_id = item_id_from_atom_form(item_body_json['id'])\n            result[item_id] = item_body_json\n    for item_id in item_ids:\n        if ((item_id not in result) and (item_id not in not_found_items_ids_to_ignore)):\n            logging.warning('Requested item id %s (%s), but it was not found in the result', item_id.atom_form, item_id.decimal_form)\n    return result\n", "label": 1}
{"function": "\n\ndef _from_catalog(self):\n    'Initialize the dictionary of operators by querying the catalogs'\n    for oper in self.fetch():\n        (sch, opr, lft, rgt) = oper.key()\n        if (lft == '-'):\n            lft = oper.leftarg = 'NONE'\n        if (rgt == '-'):\n            rgt = oper.rightarg = 'NONE'\n        if (oper.commutator == '0'):\n            del oper.commutator\n        if (oper.negator == '0'):\n            del oper.negator\n        if (oper.restrict == '-'):\n            del oper.restrict\n        if (oper.join == '-'):\n            del oper.join\n        self[(sch, opr, lft, rgt)] = Operator(**oper.__dict__)\n", "label": 1}
{"function": "\n\ndef references_model(self, name, app_label=None):\n    strings_to_check = [self.name]\n    for base in self.bases:\n        if isinstance(base, six.string_types):\n            strings_to_check.append(base.split('.')[(- 1)])\n    for (fname, field) in self.fields:\n        if field.remote_field:\n            if isinstance(field.remote_field.model, six.string_types):\n                strings_to_check.append(field.remote_field.model.split('.')[(- 1)])\n    for string in strings_to_check:\n        if (string.lower() == name.lower()):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef isAuthorized(port, security_attributes, request_info, stp, start_time, end_time):\n    '\\n    Check if a request is authorized to use a certain port within the given criteria.\\n    '\n    default = (False if port.authz else True)\n    for rule in port.authz:\n        if (rule.type_ in HEADER_ATTRIBUTES):\n            if any([rule.match(sa) for sa in security_attributes]):\n                log.msg(('AuthZ granted for port %s: Using %s attribute' % (port.name, rule.type_)), system=LOG_SYSTEM)\n                return True\n        elif ((rule.type_ in REQUEST_ATTRIBUTES) and (rule.type_ == HOST_DN)):\n            if (rule.value == request_info.cert_host_dn):\n                log.msg(('AuthZ granted for port %s: Using certificate dn %s' % (port.name, request_info.cert_host_dn)), system=LOG_SYSTEM)\n                return True\n        else:\n            log.msg((\"Couldn't figure out what to do with rule of type %s\" % rule.type_), system='AuthZ')\n    return default\n", "label": 1}
{"function": "\n\ndef _workaround_for_old_pycparser(csource):\n    parts = []\n    while True:\n        match = _r_star_const_space.search(csource)\n        if (not match):\n            break\n        parts.append(csource[:match.start()])\n        parts.append('(')\n        closing = ')'\n        parts.append(match.group())\n        endpos = match.end()\n        if csource.startswith('*', endpos):\n            parts.append('(')\n            closing += ')'\n        level = 0\n        i = endpos\n        while (i < len(csource)):\n            c = csource[i]\n            if (c == '('):\n                level += 1\n            elif (c == ')'):\n                if (level == 0):\n                    break\n                level -= 1\n            elif (c in ',;='):\n                if (level == 0):\n                    break\n            i += 1\n        csource = ((csource[endpos:i] + closing) + csource[i:])\n    parts.append(csource)\n    return ''.join(parts)\n", "label": 1}
{"function": "\n\n@get_twit\ndef print_tweets(twit=None, settings=None):\n    'Find some tweets and print them to console.'\n    if (settings['stream'] and (settings['search'] or settings['user'])):\n        return streamed_search(twit, settings)\n    elif settings.get('uid'):\n        tweets = twit.statuses.user_timeline(user_id=settings['uid'])\n    elif settings['search']:\n        tweets = twit.search.tweets(q=','.join(settings['search']))['statuses']\n    else:\n        tweets = twit.statuses.home_timeline()\n    max_tweets = settings['max']\n    for (i, tweet) in enumerate(tweets):\n        if (not print_tweet(tweet, settings)):\n            max_tweets += 1\n        if ((i + 1) >= max_tweets):\n            break\n", "label": 1}
{"function": "\n\ndef get_all_permissions(self, user_obj, obj=None):\n    if ((not user_obj.is_active) or user_obj.is_anonymous()):\n        return set()\n    disable_cache = getattr(settings, 'DISABLE_GENERIC_PERMISSION_CACHE', False)\n    if user_obj.is_superuser:\n        perms = getattr(self, '_all_permissions', None)\n        if (perms is None):\n            perms = generate_perm_list(Permission.objects.all())\n            if (not disable_cache):\n                setattr(self, '_all_permissions', perms)\n        return perms\n    if (obj is None):\n        cache_field_name = ALL_PERMS_CACHE_FIELD\n    else:\n        cache_field_name = '{field}_{pk}_{ctype_id}'.format(field=ALL_PERMS_CACHE_FIELD, pk=obj.pk, ctype_id=ContentType.objects.get_for_model(obj).id)\n    if ((not hasattr(user_obj, cache_field_name)) or disable_cache):\n        perms = get_all_user_permissions(user_obj, obj)\n        setattr(user_obj, cache_field_name, perms)\n    return getattr(user_obj, cache_field_name)\n", "label": 1}
{"function": "\n\ndef describe_stacks(self, name_or_stack_id):\n    stacks = self.stacks.values()\n    if name_or_stack_id:\n        for stack in stacks:\n            if ((stack.name == name_or_stack_id) or (stack.stack_id == name_or_stack_id)):\n                return [stack]\n        if self.deleted_stacks:\n            deleted_stacks = self.deleted_stacks.values()\n            for stack in deleted_stacks:\n                if (stack.stack_id == name_or_stack_id):\n                    return [stack]\n        raise ValidationError(name_or_stack_id)\n    else:\n        return stacks\n", "label": 1}
{"function": "\n\ndef View(self, start, end):\n    'A generator over given lines in a buffer.\\n\\n    When vim messages are viewed in this fashion, the messenger object will be\\n    notified that those messages were not unexpected.\\n\\n    Args:\\n      start: The beginning of the range.\\n      end: A function to get the end of the range.\\n    Yields:\\n      An iterable over the range.\\n    Raises:\\n      NotEnoughOutput: when the range exceeds the buffer.\\n    '\n    self._line = ((- 1) if (self._line is None) else self._line)\n    if (start == vroom.controls.SPECIAL_RANGE.CURRENT_LINE):\n        start = (self.vim.GetCurrentLine() - 1)\n    else:\n        start = ((self._line + 1) if (start is None) else (start - 1))\n    end = ((start + 1) if (end is None) else end((start + 1)))\n    end = (len(self._data) if (end is 0) else end)\n    self._last_range = (start, end)\n    for i in range(start, end):\n        self._line = i\n        if (i < len(self._data)):\n            (yield self._data[i])\n        else:\n            raise NotEnoughOutput(self.GetContext())\n", "label": 1}
{"function": "\n\n@responder(pattern='^(\\\\?|help) ?(?P<regex>\\\\S+)?', form='? [regex]', auth_required=False, help='Display help, optionally filtered')\ndef help(conversation, regex):\n    help_text = ''\n    groups = {\n        \n    }\n    for func in responders.values():\n        if (func.module not in groups):\n            groups[func.module] = []\n        groups[func.module].append(func)\n    for (group, funcs) in sorted(groups.items()):\n        help_text += ('\\n<<< %s commands >>>\\n' % group)\n        for func in funcs:\n            help_text += ('%24s  %s\\n' % (func.form, func.help))\n    if regex:\n        regex = re.compile(regex)\n        output = ''\n        for line in help_text.split('\\n'):\n            if regex.search(line):\n                output += (line + '\\n')\n    else:\n        output = help_text\n    conversation.say(('\\n' + output), useHTML=False)\n    if (not regex):\n        doc_url = ('http://%s:%d/doc/index.html' % (config.HOSTNAME, config.DRONED_PORT))\n        conversation.say(('For more information read my online documentation at %s' % doc_url))\n", "label": 1}
{"function": "\n\ndef handle_status_result(self, cluster):\n    (cluster_status, status_times) = self.last_status.setdefault(str(cluster), (Status.OK, 0))\n    need_send_alert = False\n    if (cluster.last_status != cluster_status):\n        self.last_status[str(cluster)] = (cluster.last_status, 1)\n        if ((cluster.last_status == Status.OK) and (status_times >= ERROR_TIMES_FOR_ALERT)):\n            need_send_alert = True\n    else:\n        self.last_status[str(cluster)] = (cluster.last_status, (status_times + 1))\n        if ((cluster.last_status != Status.OK) and ((status_times + 1) == ERROR_TIMES_FOR_ALERT)):\n            need_send_alert = True\n    if need_send_alert:\n        self.alert_msg += ('[%s]Cluster[%s]\\n' % (('OK' if (cluster.last_status == Status.OK) else 'PROBLEM'), cluster))\n        for job in cluster.jobs.itervalues():\n            if (job.last_status != Status.OK):\n                self.alert_msg += ('Job[%s] not healthy: %s\\n' % (job.name, job.last_message))\n        self.alert_msg += '******\\n'\n", "label": 1}
{"function": "\n\ndef delete(self):\n    for prop in self.schema_visitor.properties:\n        if prop.is_value:\n            continue\n        if (prop.is_array and prop.items.inline):\n            for item in getattr(self, prop.name):\n                item.delete()\n        if (prop.is_object and prop.inline):\n            value = getattr(self, prop.name)\n            if (value is not None):\n                value.delete()\n    super(SchemaModel, self).delete()\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    started = time()\n    frequency = None\n    if (len(args) > 0):\n        frequency = int(args[0])\n    self.stdout.write('[*] Starting...\\n')\n    while True:\n        jobs = Cronjob.objects.all()\n        for job in jobs:\n            if (not job.need_run()):\n                continue\n            self.stdout.write((('[+] Running ' + job.description) + '...\\n'))\n            for entry in job.entry_set.filter(approved=True).exclude(travis_token=''):\n                self.stdout.write((('[+] Ping ' + entry.gh_project) + '\\n'))\n                try:\n                    ping(entry)\n                except:\n                    print_exc()\n            job.run_now()\n            job.save()\n        self.stdout.write('[-] Pinging myself as a Dead Man Snitch...\\n')\n        ping(Entry.objects.get(gh_project='FiloSottile/travis-cron'))\n        before_next = min(map((lambda job: job.before_next_run()), jobs))\n        before_next = (0 if (before_next < 0) else int((before_next + 1)))\n        if (frequency and ((time() + before_next) > (started + frequency))):\n            self.stdout.write('[-] Closing...\\n')\n            break\n        self.stdout.write((('[-] Sleeping ' + str(before_next)) + ' seconds...\\n'))\n        self.stdout.flush()\n        sleep(before_next)\n", "label": 1}
{"function": "\n\ndef find_binary(locations):\n    searchpath_sep = (';' if (sys.platform == 'win32') else ':')\n    searchpaths = os.environ['PATH'].split(searchpath_sep)\n    for location in locations:\n        if ('{PATH}' in location):\n            for searchpath in searchpaths:\n                s = location.replace('{PATH}', searchpath)\n                if (os.path.isfile(s) and os.access(s, os.X_OK)):\n                    (yield s)\n        elif (os.path.isfile(location) and os.access(location, os.X_OK)):\n            (yield location)\n", "label": 1}
{"function": "\n\ndef _create_control(self, parent):\n    dlg = QtGui.QMessageBox(parent)\n    dlg.setWindowTitle(self.title)\n    dlg.setText(self.message)\n    dlg.setInformativeText(self.informative)\n    dlg.setDetailedText(self.detail)\n    if (self.image is None):\n        dlg.setIcon(QtGui.QMessageBox.Warning)\n    else:\n        dlg.setIconPixmap(self.image.create_image())\n    if self.yes_label:\n        btn = dlg.addButton(self.yes_label, QtGui.QMessageBox.YesRole)\n    else:\n        btn = dlg.addButton(QtGui.QMessageBox.Yes)\n    self._button_result_map[btn] = YES\n    if (self.default == YES):\n        dlg.setDefaultButton(btn)\n    if self.no_label:\n        btn = dlg.addButton(self.no_label, QtGui.QMessageBox.NoRole)\n    else:\n        btn = dlg.addButton(QtGui.QMessageBox.No)\n    self._button_result_map[btn] = NO\n    if (self.default == NO):\n        dlg.setDefaultButton(btn)\n    if self.cancel:\n        if self.cancel_label:\n            btn = dlg.addButton(self.cancel_label, QtGui.QMessageBox.RejectRole)\n        else:\n            btn = dlg.addButton(QtGui.QMessageBox.Cancel)\n        self._button_result_map[btn] = CANCEL\n        if (self.default == CANCEL):\n            dlg.setDefaultButton(btn)\n    return dlg\n", "label": 1}
{"function": "\n\ndef process(self):\n    'Process the request entity based on its Content-Type.'\n    h = cherrypy.serving.request.headers\n    if (('Content-Length' not in h) and ('Transfer-Encoding' not in h)):\n        raise cherrypy.HTTPError(411)\n    self.fp = SizedReader(self.fp, self.length, self.maxbytes, bufsize=self.bufsize, has_trailers=('Trailer' in h))\n    super(RequestBody, self).process()\n    request_params = self.request_params\n    for (key, value) in self.params.items():\n        if (sys.version_info < (3, 0)):\n            if isinstance(key, unicode):\n                key = key.encode('ISO-8859-1')\n        if (key in request_params):\n            if (not isinstance(request_params[key], list)):\n                request_params[key] = [request_params[key]]\n            request_params[key].append(value)\n        else:\n            request_params[key] = value\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = RemoteDirectoryListingResult()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (len(self.jid_) != len(x.jid_)):\n        return 0\n    for (e1, e2) in zip(self.jid_, x.jid_):\n        if (e1 != e2):\n            return 0\n    if (self.has_from_jid_ != x.has_from_jid_):\n        return 0\n    if (self.has_from_jid_ and (self.from_jid_ != x.from_jid_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\n@lockutils.synchronized('cisco-nexus-portlock')\ndef update_port_postcommit(self, context):\n    'Update port non-database commit event.'\n    (vlan_segment, vxlan_segment) = self._get_segments(context.top_bound_segment, context.bottom_bound_segment)\n    (orig_vlan_segment, orig_vxlan_segment) = self._get_segments(context.original_top_bound_segment, context.original_bottom_bound_segment)\n    if self._is_vm_migrating(context, vlan_segment, orig_vlan_segment):\n        vni = (self._port_action_vxlan(context.original, orig_vxlan_segment, self._delete_nve_member) if orig_vxlan_segment else 0)\n        self._port_action_vlan(context.original, orig_vlan_segment, self._delete_switch_entry, vni)\n    elif (self._is_supported_deviceowner(context.current) and self._is_status_active(context.current)):\n        if self._is_baremetal(context.current):\n            self._init_baremetal_trunk_interfaces(context.current, vlan_segment, 0)\n            host_id = ''\n            (all_switches, active_switches) = self._get_baremetal_switches(context.current)\n        else:\n            host_id = context.current.get(portbindings.HOST_ID)\n            (all_switches, active_switches) = self._get_host_switches(host_id)\n        if ((not active_switches) and all_switches):\n            raise excep.NexusConnectFailed(nexus_host=all_switches[0], config='None', exc='Update Port Failed: Nexus Switch is down or replay in progress')\n        vni = (self._port_action_vxlan(context.current, vxlan_segment, self._configure_nve_member) if vxlan_segment else 0)\n        self._port_action_vlan(context.current, vlan_segment, self._configure_port_entries, vni)\n", "label": 1}
{"function": "\n\ndef __init__(self, country=None, state=None, locality=None, organization=None, organization_unit=None, name=None, email=None, digest='sha1', filename=None):\n    if (filename is None):\n        req = crypto.X509Req()\n        subject = req.get_subject()\n        if country:\n            subject.C = country\n        if state:\n            subject.ST = state\n        if locality:\n            subject.L = locality\n        if organization:\n            subject.O = organization\n        if organization_unit:\n            subject.OU = organization_unit\n        if name:\n            subject.CN = name\n        if email:\n            subject.emailAddress = email\n    else:\n        (ftype, text) = get_type_and_text(filename)\n        req = crypto.load_certificate_request(ftype, text)\n    self._req = req\n", "label": 1}
{"function": "\n\n@inherit_docstring_from(rv_continuous)\ndef fit(self, data, *args, **kwds):\n    f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))\n    floc = kwds.get('floc', None)\n    fscale = kwds.get('fscale', None)\n    if (floc is None):\n        return super(gamma_gen, self).fit(data, *args, **kwds)\n    if ((f0 is not None) and (fscale is not None)):\n        raise ValueError('All parameters fixed. There is nothing to optimize.')\n    data = np.asarray(data)\n    if np.any((data <= floc)):\n        raise FitDataError('gamma', lower=floc, upper=np.inf)\n    if (floc != 0):\n        data = (data - floc)\n    xbar = data.mean()\n    if (fscale is None):\n        if (f0 is not None):\n            a = f0\n        else:\n            s = (log(xbar) - log(data).mean())\n            func = (lambda a: ((log(a) - special.digamma(a)) - s))\n            aest = (((3 - s) + np.sqrt((((s - 3) ** 2) + (24 * s)))) / (12 * s))\n            xa = (aest * (1 - 0.4))\n            xb = (aest * (1 + 0.4))\n            a = optimize.brentq(func, xa, xb, disp=0)\n        scale = (xbar / a)\n    else:\n        c = (log(data).mean() - log(fscale))\n        a = _digammainv(c)\n        scale = fscale\n    return (a, floc, scale)\n", "label": 1}
{"function": "\n\ndef validates_log(obj):\n    checker = Checker()\n    check = True\n    _ = obj._\n    checker.errors = []\n    if is_param(obj.input, 's'):\n        check = (checker.check_datetime_string(_('Start Date'), obj.input.s, (CHECK_EMPTY | CHECK_VALID), obj.me.languages) and check)\n    if is_param(obj.input, 'e'):\n        check = (checker.check_datetime_string(_('End Date'), obj.input.e, (CHECK_EMPTY | CHECK_VALID), obj.me.languages) and check)\n    if is_param(obj.input, 'st'):\n        check = (checker.check_time_string(_('Start Time'), obj.input.st, (CHECK_EMPTY | CHECK_VALID)) and check)\n    if is_param(obj.input, 'et'):\n        check = (checker.check_time_string(_('End Time'), obj.input.et, (CHECK_EMPTY | CHECK_VALID)) and check)\n    obj.view.alert = checker.errors\n    return check\n", "label": 1}
{"function": "\n\ndef copy(x):\n    new_x = pycopy.copy(x)\n    new_x.params = [x for x in new_x.params]\n    new_x.params_grad_scale = [x for x in new_x.params_grad_scale]\n    new_x.noise_params = [x for x in new_x.noise_params]\n    new_x.noise_params_shape_fn = [x for x in new_x.noise_params_shape_fn]\n    new_x.updates = [x for x in new_x.updates]\n    new_x.additional_gradients = [x for x in new_x.additional_gradients]\n    new_x.inputs = [x for x in new_x.inputs]\n    new_x.schedules = [x for x in new_x.schedules]\n    new_x.properties = [x for x in new_x.properties]\n    return new_x\n", "label": 1}
{"function": "\n\ndef register(self, app=None, discovering_apps=False):\n    if (app is None):\n        return (lambda app: self.register(app, discovering_apps))\n    if (app.__module__.split('.')[(- 1)] == 'cms_app'):\n        warnings.warn('cms_app.py filename is deprecated, and it will be removed in version 3.4; please rename it to cms_apps.py', DeprecationWarning)\n    if (self.apphooks and (not discovering_apps)):\n        return app\n    if (app.__name__ in self.apps):\n        raise AppAlreadyRegistered(('A CMS application %r is already registered' % app.__name__))\n    if (not issubclass(app, CMSApp)):\n        raise ImproperlyConfigured(('CMS application must inherit from cms.app_base.CMSApp, but %r does not' % app.__name__))\n    if ((not hasattr(app, 'menus')) and hasattr(app, 'menu')):\n        warnings.warn((\"You define a 'menu' attribute on CMS application %r, but the 'menus' attribute is empty, did you make a typo?\" % app.__name__))\n    self.apps[app.__name__] = app()\n    return app\n", "label": 1}
{"function": "\n\ndef _validate_path_args(self):\n    params = self.parameters\n    if ((self.cmd == 'mv') and self._same_path(params['src'], params['dest'])):\n        raise ValueError((\"Cannot mv a file onto itself: '%s' - '%s'\" % (params['src'], params['dest'])))\n    if (('locals3' == params['paths_type']) and (not params['is_stream'])):\n        if (not os.path.exists(params['src'])):\n            raise RuntimeError(('The user-provided path %s does not exist.' % params['src']))\n    elif (('s3local' == params['paths_type']) and params['dir_op']):\n        if (not os.path.exists(params['dest'])):\n            os.makedirs(params['dest'])\n", "label": 1}
{"function": "\n\ndef split_template_path(template):\n    \"Split a path into segments and perform a sanity check.  If it detects\\n    '..' in the path it will raise a `TemplateNotFound` error.\\n    \"\n    pieces = []\n    for piece in template.split('/'):\n        if ((path.sep in piece) or (path.altsep and (path.altsep in piece)) or (piece == path.pardir)):\n            raise FileNotFound(template)\n        elif (piece and (piece != '.')):\n            pieces.append(piece)\n    return pieces\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_json(name, random=[], **json):\n    cls = None\n    if (name == 'play'):\n        cls = PlayMove\n    elif (name == 'attack'):\n        cls = AttackMove\n    elif (name == 'power'):\n        cls = PowerMove\n    elif (name == 'end'):\n        cls = TurnEndMove\n    elif (name == 'start'):\n        cls = TurnStartMove\n    elif (name == 'concede'):\n        cls = ConcedeMove\n    obj = cls.__new__(cls)\n    cls.__from_json__(obj, **json)\n    obj.random_numbers = []\n    for num in random:\n        if isinstance(num, dict):\n            obj.random_numbers.append(hearthbreaker.proxies.ProxyCharacter.from_json(**num))\n        else:\n            obj.random_numbers.append(num)\n    return obj\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.LIST):\n                self.success = []\n                (_etype31, _size28) = iprot.readListBegin()\n                for _i32 in xrange(_size28):\n                    _elem33 = Certificate()\n                    _elem33.read(iprot)\n                    self.success.append(_elem33)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\n@ComputedGraph.Function\ndef extractVectorDataAsNumpyArrayInChunks(self, stepSize=100000):\n    \"Return the data as a sequence of numpy arrays each of which is no larger than 'stepSize'.\\n\\n        This is used to prevent us from creating memory fragmentation when we are loading\\n        lots of arrays of different sizes.\\n        \"\n    if (self.computedValueVector.vectorImplVal is None):\n        return None\n    if ((len(self.vectorDataIds) > 0) and (not self.isLoaded)):\n        return None\n    if (not self.vdmThinksIsLoaded()):\n        return None\n    result = []\n    index = self.lowIndex\n    while ((index < self.highIndex) and (result is not None)):\n        tailResult = ComputedValueGateway.getGateway().extractVectorDataAsNumpyArray(self.computedValueVector, index, min(self.highIndex, (index + stepSize)))\n        index += stepSize\n        if (tailResult is not None):\n            result.append(tailResult)\n        else:\n            result = None\n    if ((result is None) and (not self.vdmThinksIsLoaded())):\n        logging.info('CumulusClient: %s was marked loaded but returned None', self)\n        self.isLoaded = False\n        ComputedValueGateway.getGateway().reloadVector(self)\n    return result\n", "label": 1}
{"function": "\n\ndef set_logging_parameters(args):\n    for (attr, catset, defaults) in (('info', args_info, INFO_ALL), ('debug', args_debug, DEBUG_ALL)):\n        option = getattr(args, attr)\n        if option:\n            catset.update((x.lower() for x in option.split(',')))\n            if ('all' in catset):\n                catset.update(defaults)\n    if (args.verbose is not None):\n        for level in sorted(verbose_categories):\n            if (args.verbose >= level):\n                args_info.update(verbose_categories[level])\n", "label": 1}
{"function": "\n\ndef optimize_locals(codelist):\n    ' Optimize the given code object for fast locals access.\\n\\n    All STORE_NAME opcodes will be replaced with STORE_FAST. Names which\\n    are stored and then loaded via LOAD_NAME are rewritten to LOAD_FAST\\n    and DELETE_NAME is rewritten to DELETE_FAST. This transformation is\\n    applied in-place.\\n\\n    Parameters\\n    ----------\\n    codelist : list\\n        The list of byteplay code ops to modify.\\n\\n    '\n    fast_locals = set()\n    for (idx, (op, op_arg)) in enumerate(codelist):\n        if (op == STORE_NAME):\n            fast_locals.add(op_arg)\n            codelist[idx] = (STORE_FAST, op_arg)\n    for (idx, (op, op_arg)) in enumerate(codelist):\n        if ((op == LOAD_NAME) and (op_arg in fast_locals)):\n            codelist[idx] = (LOAD_FAST, op_arg)\n        elif ((op == DELETE_NAME) and (op_arg in fast_locals)):\n            codelist[idx] = (DELETE_FAST, op_arg)\n", "label": 1}
{"function": "\n\ndef item(self, name, folderId, description=None, files=None, access=None, debug=False):\n    ret = {\n        \n    }\n    r = ItemResource(self, folderId)\n    valid_fields = [('name', name), ('description', description), ('folderId', folderId)]\n    if (self.module.params['state'] == 'present'):\n        if r.name_exists(name):\n            ret = r.update_by_name(name, {k: v for (k, v) in valid_fields if (v is not None)})\n        else:\n            ret = r.create({k: v for (k, v) in valid_fields if (v is not None)})\n    elif (self.module.params['state'] == 'absent'):\n        ret = r.delete_by_name(name)\n    return ret\n", "label": 1}
{"function": "\n\ndef is_cached(self, path, saltenv='base', cachedir=None):\n    '\\n        Returns the full path to a file if it is cached locally on the minion\\n        otherwise returns a blank string\\n        '\n    if path.startswith('salt://'):\n        (path, senv) = salt.utils.url.parse(path)\n        if senv:\n            saltenv = senv\n    escaped = (True if salt.utils.url.is_escaped(path) else False)\n    localsfilesdest = os.path.join(self.opts['cachedir'], 'localfiles', path.lstrip('|/'))\n    filesdest = os.path.join(self.opts['cachedir'], 'files', saltenv, path.lstrip('|/'))\n    extrndest = self._extrn_path(path, saltenv, cachedir=cachedir)\n    if os.path.exists(filesdest):\n        return (salt.utils.url.escape(filesdest) if escaped else filesdest)\n    elif os.path.exists(localsfilesdest):\n        return (salt.utils.url.escape(localsfilesdest) if escaped else localsfilesdest)\n    elif os.path.exists(extrndest):\n        return extrndest\n    return ''\n", "label": 1}
{"function": "\n\n@staticmethod\ndef zip_loader(filename, **kwargs):\n    'Read images from an zip file.\\n\\n        .. versionadded:: 1.0.8\\n\\n        Returns an Image with a list of type ImageData stored in Image._data\\n        '\n    _file = BytesIO(open(filename, 'rb').read())\n    z = zipfile.ZipFile(_file)\n    image_data = []\n    znamelist = z.namelist()\n    znamelist.sort()\n    image = None\n    for zfilename in znamelist:\n        try:\n            tmpfile = BytesIO(z.read(zfilename))\n            ext = zfilename.split('.')[(- 1)].lower()\n            im = None\n            for loader in ImageLoader.loaders:\n                if ((ext not in loader.extensions()) or (not loader.can_load_memory())):\n                    continue\n                Logger.debug(('Image%s: Load <%s> from <%s>' % (loader.__name__[11:], zfilename, filename)))\n                try:\n                    im = loader(zfilename, ext=ext, rawdata=tmpfile, inline=True, **kwargs)\n                except:\n                    continue\n                break\n            if (im is not None):\n                image_data.append(im._data[0])\n                image = im\n        except:\n            Logger.warning(('Image: Unable to load image<%s> in zip <%s> trying to continue...' % (zfilename, filename)))\n    z.close()\n    if (len(image_data) == 0):\n        raise Exception(('no images in zip <%s>' % filename))\n    image._data = image_data\n    image.filename = filename\n    return image\n", "label": 1}
{"function": "\n\ndef _add_edge(self, edge, minlvl=0):\n    '\\n        Add a single edge to the ChartView:\\n\\n            - Call analyze_edge to recalculate display parameters\\n            - Find an available level\\n            - Call _draw_edge\\n        '\n    if isinstance(edge, LeafEdge):\n        return\n    if (edge in self._edgetags):\n        return\n    self._analyze_edge(edge)\n    self._grow()\n    if (not self._compact):\n        self._edgelevels.append([edge])\n        lvl = (len(self._edgelevels) - 1)\n        self._draw_edge(edge, lvl)\n        self._resize()\n        return\n    lvl = 0\n    while True:\n        while (lvl >= len(self._edgelevels)):\n            self._edgelevels.append([])\n            self._resize()\n        if ((lvl >= minlvl) and (not self._edge_conflict(edge, lvl))):\n            self._edgelevels[lvl].append(edge)\n            break\n        lvl += 1\n    self._draw_edge(edge, lvl)\n", "label": 1}
{"function": "\n\ndef read_regexp_block(stream, start_re, end_re=None):\n    '\\n    Read a sequence of tokens from a stream, where tokens begin with\\n    lines that match ``start_re``.  If ``end_re`` is specified, then\\n    tokens end with lines that match ``end_re``; otherwise, tokens end\\n    whenever the next line matching ``start_re`` or EOF is found.\\n    '\n    while True:\n        line = stream.readline()\n        if (not line):\n            return []\n        if re.match(start_re, line):\n            break\n    lines = [line]\n    while True:\n        oldpos = stream.tell()\n        line = stream.readline()\n        if (not line):\n            return [''.join(lines)]\n        if ((end_re is not None) and re.match(end_re, line)):\n            return [''.join(lines)]\n        if ((end_re is None) and re.match(start_re, line)):\n            stream.seek(oldpos)\n            return [''.join(lines)]\n        lines.append(line)\n", "label": 1}
{"function": "\n\ndef __poisson_cdf_Q_large_lambda(k, a):\n    'Slower internal Poisson CDF evaluater for upper tail with large\\n    lambda.\\n    \\n    '\n    if (k < 0):\n        return 1\n    num_parts = int((a / LSTEP))\n    last_part = (a % LSTEP)\n    lastexp = exp((- last_part))\n    next = EXPSTEP\n    num_parts -= 1\n    for i in xrange(1, (k + 1)):\n        last = next\n        next = ((last * a) / i)\n        if (next > EXPTHRES):\n            if (num_parts >= 1):\n                next *= EXPSTEP\n                num_parts -= 1\n            else:\n                cdf *= lastexp\n                lastexp = 1\n    cdf = 0\n    i = (k + 1)\n    while (next > 0):\n        last = next\n        next = ((last * a) / i)\n        cdf += next\n        i += 1\n        if ((next > EXPTHRES) or (cdf > EXPTHRES)):\n            if (num_parts >= 1):\n                cdf *= EXPSTEP\n                next *= EXPSTEP\n                num_parts -= 1\n            else:\n                cdf *= lastexp\n                lastexp = 1\n    for i in xrange(num_parts):\n        cdf *= EXPSTEP\n    cdf *= lastexp\n    return cdf\n", "label": 1}
{"function": "\n\ndef get_or_post(uri, method, req, content_type=DEFAULT_POST_CONTENT_TYPE, accept=None, **kwargs):\n    '\\n\\n    :param uri:\\n    :param method:\\n    :param req:\\n    :param content_type:\\n    :param accept:\\n    :param kwargs:\\n    :return:\\n    '\n    if (method in ['GET', 'DELETE']):\n        _qp = req.to_urlencoded()\n        if _qp:\n            path = ((uri + '?') + _qp)\n        else:\n            path = uri\n        body = None\n    elif (method in ['POST', 'PUT']):\n        path = uri\n        if (content_type == URL_ENCODED):\n            body = req.to_urlencoded()\n        elif (content_type == JSON_ENCODED):\n            body = req.to_json()\n        else:\n            raise UnSupported((\"Unsupported content type: '%s'\" % content_type))\n        header_ext = {\n            'Content-Type': content_type,\n        }\n        if accept:\n            header_ext = {\n                'Accept': accept,\n            }\n        if ('headers' in kwargs.keys()):\n            kwargs['headers'].update(header_ext)\n        else:\n            kwargs['headers'] = header_ext\n    else:\n        raise UnSupported((\"Unsupported HTTP method: '%s'\" % method))\n    return (path, body, kwargs)\n", "label": 1}
{"function": "\n\ndef data_to_field(field, data):\n    if isinstance(field, fields.EmbeddedDocumentField):\n        return data_to_document(field.document_type_obj, data)\n    elif isinstance(field, (fields.ListField, fields.SequenceField, fields.SortedListField)):\n        l = []\n        for d in data:\n            l.append(data_to_field(field.field, d))\n        return l\n    elif isinstance(field, fields.FileField):\n        if data.filename:\n            gfs = field.proxy_class(db_alias=field.db_alias, collection_name=field.collection_name, instance=field.owner_document(), key=field.name)\n            gfs.put(data.stream, filename=secure_filename(data.filename), content_type=data.mimetype)\n            return gfs\n        elif data.clear:\n            return _remove_file_value\n        return _unset_value\n    elif (isinstance(field, (fields.ReferenceField, fields.ObjectIdField)) and isinstance(data, basestring)):\n        from bson.objectid import ObjectId\n        return ObjectId(data)\n    else:\n        return data\n", "label": 1}
{"function": "\n\ndef dtype_from_descr(descr, byteorder=None):\n    'Get a (nested) NumPy dtype from a description instance and byteorder.\\n\\n    The descr parameter can be a Description or IsDescription\\n    instance, sub-class of IsDescription or a dictionary.\\n\\n    '\n    if isinstance(descr, dict):\n        descr = Description(descr)\n    elif ((type(descr) == type(IsDescription)) and issubclass(descr, IsDescription)):\n        descr = Description(descr().columns)\n    elif isinstance(descr, IsDescription):\n        descr = Description(descr.columns)\n    elif (not isinstance(descr, Description)):\n        raise ValueError(('invalid description: %r' % descr))\n    dtype_ = descr._v_dtype\n    if (byteorder and (byteorder != '|')):\n        dtype_ = dtype_.newbyteorder(byteorder)\n    return dtype_\n", "label": 1}
{"function": "\n\ndef _set_hash_type(entity, value):\n    'Callback hook to set the hash type based on the length of the value.\\n\\n    If the `Hash` object already has a type, it is not changed.\\n\\n    Args:\\n        entity (Hash): The Hash object being modified.\\n        value (str): The hash value\\n    '\n    if entity.type_:\n        return\n    if (not value):\n        return\n    hashlen = len(value.value)\n    if (hashlen == 32):\n        entity.type_ = Hash.TYPE_MD5\n    elif (hashlen == 40):\n        entity.type_ = Hash.TYPE_SHA1\n    elif (hashlen == 56):\n        entity.type_ = Hash.TYPE_SHA224\n    elif (hashlen == 64):\n        entity.type_ = Hash.TYPE_SHA256\n    elif (hashlen == 96):\n        entity.type_ = Hash.TYPE_SHA384\n    elif (hashlen == 128):\n        entity.type_ = Hash.TYPE_SHA512\n    else:\n        entity.type_ = Hash.TYPE_OTHER\n", "label": 1}
{"function": "\n\n@register_specialize\n@gof.local_optimizer([T.neg])\ndef local_neg_div_neg(node):\n    '\\n    - (-a / b) -> a / b\\n\\n    Also performs - (c / b) -> ((-c) / b) when c is a scalar constant.\\n\\n    '\n    if (node.op == T.neg):\n        if (node.inputs[0].owner and (node.inputs[0].owner.op == T.true_div)):\n            frac = node.inputs[0]\n            (num, denom) = frac.owner.inputs\n            if (num.owner and (num.owner.op == T.neg)):\n                if (len(frac.clients) == 1):\n                    new_num = num.owner.inputs[0]\n                    return [T.true_div(new_num, denom)]\n            elif (numpy.all(num.broadcastable) and isinstance(num, Constant)):\n                if (len(frac.clients) == 1):\n                    new_num = (- num.data)\n                    return [T.true_div(new_num, denom)]\n", "label": 1}
{"function": "\n\ndef __init__(self, credentials, subscription_id, api_version='2016-03-30', accept_language='en-US', long_running_operation_retry_timeout=30, generate_client_request_id=True, base_url=None, filepath=None):\n    if (credentials is None):\n        raise ValueError(\"Parameter 'credentials' must not be None.\")\n    if (subscription_id is None):\n        raise ValueError(\"Parameter 'subscription_id' must not be None.\")\n    if (not isinstance(subscription_id, str)):\n        raise TypeError(\"Parameter 'subscription_id' must be str.\")\n    if ((api_version is not None) and (not isinstance(api_version, str))):\n        raise TypeError(\"Optional parameter 'api_version' must be str.\")\n    if ((accept_language is not None) and (not isinstance(accept_language, str))):\n        raise TypeError(\"Optional parameter 'accept_language' must be str.\")\n    if (not base_url):\n        base_url = 'https://management.azure.com'\n    super(NetworkManagementClientConfiguration, self).__init__(base_url, filepath)\n    self.add_user_agent('networkmanagementclient/{}'.format(VERSION))\n    self.add_user_agent('Azure-SDK-For-Python')\n    self.credentials = credentials\n    self.subscription_id = subscription_id\n    self.api_version = api_version\n    self.accept_language = accept_language\n    self.long_running_operation_retry_timeout = long_running_operation_retry_timeout\n    self.generate_client_request_id = generate_client_request_id\n", "label": 1}
{"function": "\n\n@classmethod\ndef __declare_first__(cls):\n    if hasattr(cls, '__mapper__'):\n        return\n    clsregistry.add_class(cls.__name__, cls)\n    mappers = []\n    stack = list(cls.__subclasses__())\n    while stack:\n        klass = stack.pop()\n        stack.extend(klass.__subclasses__())\n        mn = _mapper_or_none(klass)\n        if (mn is not None):\n            mappers.append(mn)\n    pjoin = cls._create_polymorphic_union(mappers)\n    cls.__mapper__ = m = mapper(cls, pjoin, polymorphic_on=pjoin.c.type)\n    for scls in cls.__subclasses__():\n        sm = _mapper_or_none(scls)\n        if (sm and sm.concrete and (cls in scls.__bases__)):\n            sm._set_concrete_base(m)\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    \"Emmulate GNU diff's format.\\n    Header: @@ -382,8 +481,9 @@\\n    Indicies are printed as 1-based, not 0-based.\\n\\n    Returns:\\n      The GNU diff string.\\n    \"\n    if (self.length1 == 0):\n        coords1 = (str(self.start1) + ',0')\n    elif (self.length1 == 1):\n        coords1 = str((self.start1 + 1))\n    else:\n        coords1 = ((str((self.start1 + 1)) + ',') + str(self.length1))\n    if (self.length2 == 0):\n        coords2 = (str(self.start2) + ',0')\n    elif (self.length2 == 1):\n        coords2 = str((self.start2 + 1))\n    else:\n        coords2 = ((str((self.start2 + 1)) + ',') + str(self.length2))\n    text = ['@@ -', coords1, ' +', coords2, ' @@\\n']\n    for (op, data) in self.diffs:\n        if (op == diff_match_patch.DIFF_INSERT):\n            text.append('+')\n        elif (op == diff_match_patch.DIFF_DELETE):\n            text.append('-')\n        elif (op == diff_match_patch.DIFF_EQUAL):\n            text.append(' ')\n        data = data.encode('utf-8')\n        text.append((urllib.quote(data, \"!~*'();/?:@&=+$,# \") + '\\n'))\n    return ''.join(text)\n", "label": 1}
{"function": "\n\ndef post(url, data, headers, auth):\n    if (url != 'https://outlook.office365.com/api/v1.0/me/calendars/0/events'):\n        raise BaseException('Url wrong')\n    if (auth[0] != 'test@unit.com'):\n        raise BaseException('wrong email')\n    if (auth[1] != 'pass'):\n        raise BaseException('wrong password')\n    if (headers['Content-type'] != 'application/json'):\n        raise BaseException('header wrong value for content-type.')\n    if (headers['Accept'] != 'application/json'):\n        raise BaseException('header accept wrong.')\n    if ((json.loads(data) != lough) and (json.loads(data) != oughter)):\n        raise BaseException('data is wrong.')\n    return Resp(data)\n", "label": 1}
{"function": "\n\ndef get_shard_member(self, shard):\n    for shard_member in self.shards:\n        if ((isinstance(shard, Server) and shard_member.get_server() and (shard_member.get_server().id == shard.id)) or (isinstance(shard, Cluster) and shard_member.get_cluster() and (shard_member.get_cluster().id == shard.id))):\n            return shard_member\n", "label": 1}
{"function": "\n\ndef _translations(self):\n    '\\n            Render the translations for all configured languages\\n\\n            @returns: translation tags\\n        '\n    T = current.T\n    translations = TAG['']()\n    strings = self._strings()\n    if (self.translate and strings):\n        append_translation = translations.append\n        languages = [l for l in current.response.s3.l10n_languages if (l != 'en')]\n        languages.insert(0, 'en')\n        for language in languages:\n            translation = TAG['translation'](_lang=language)\n            append_string = translation.append\n            for (key, string) in strings.items():\n                tstr = T((string.m if hasattr(string, 'm') else string), language=language)\n                append_string(TAG['text'](TAG['value'](tstr), _id=key))\n            if len(translation):\n                append_translation(translation)\n    return TAG['itext'](translations)\n", "label": 1}
{"function": "\n\ndef run_job(self, job_name, unzip=False, wait_for_workers=False):\n    if wait_for_workers:\n        while (not self.stopped.is_set()):\n            if (len(self.worker_tracker.workers) > 0):\n                break\n            stopped = self.stopped.wait(3)\n            if stopped:\n                return\n    if unzip:\n        self._unzip(job_name)\n    job_path = os.path.join(self.job_dir, job_name)\n    job_desc = import_job_desc(job_path)\n    job_master = JobMaster(self.ctx, job_name, job_desc, self.worker_tracker.workers.keys())\n    job_master.init()\n    self.job_tracker.register_job(job_name, job_master)\n    self._register_runned_job(job_name, job_desc)\n    zip_file = os.path.join(self.zip_dir, (job_name + '.zip'))\n    for worker in job_master.workers:\n        FileTransportClient(worker, zip_file).send_file()\n    self.logger.debug(('entering the master prepare stage, job id: %s' % job_name))\n    self.logger.debug(('job available workers: %s' % job_master.workers))\n    stage = Stage(job_master.workers, 'prepare', logger=self.logger)\n    prepared_ok = stage.barrier(True, job_name)\n    if (not prepared_ok):\n        self.logger.error('prepare for running failed')\n        return\n    self.logger.debug(('entering the master run_job stage, job id: %s' % job_name))\n    stage = Stage(job_master.workers, 'run_job', logger=self.logger)\n    run_ok = stage.barrier(True, job_name)\n    if (not run_ok):\n        self.logger.error(('run job failed, job id: %s' % job_name))\n", "label": 1}
{"function": "\n\ndef _create_request(self, uri, http_method, body, headers):\n    headers = (headers or {\n        \n    })\n    if (('Content-Type' in headers) and (CONTENT_TYPE_FORM_URLENCODED in headers['Content-Type'])):\n        request = Request(uri, http_method, body, headers)\n    else:\n        request = Request(uri, http_method, '', headers)\n    (signature_type, params, oauth_params) = self._get_signature_type_and_params(request)\n    if (len(dict(oauth_params)) != len(oauth_params)):\n        raise errors.InvalidRequestError(description='Duplicate OAuth1 entries.')\n    oauth_params = dict(oauth_params)\n    request.signature = oauth_params.get('oauth_signature')\n    request.client_key = oauth_params.get('oauth_consumer_key')\n    request.resource_owner_key = oauth_params.get('oauth_token')\n    request.nonce = oauth_params.get('oauth_nonce')\n    request.timestamp = oauth_params.get('oauth_timestamp')\n    request.redirect_uri = oauth_params.get('oauth_callback')\n    request.verifier = oauth_params.get('oauth_verifier')\n    request.signature_method = oauth_params.get('oauth_signature_method')\n    request.realm = dict(params).get('realm')\n    request.oauth_params = oauth_params\n    request.params = [(k, v) for (k, v) in params if (k != 'oauth_signature')]\n    if ('realm' in request.headers.get('Authorization', '')):\n        request.params = [(k, v) for (k, v) in request.params if (k != 'realm')]\n    return request\n", "label": 1}
{"function": "\n\n@validate_busy\n@validate_target\n@lock_host\ndef registers(self, target_id=0, thread_id=None, registers=[]):\n    '\\n            Get the register values for a given target/thread.\\n            `target_id` is ignored.\\n            '\n    arch = self.get_arch()\n    if (arch in self.reg_names):\n        if ('pc' in registers):\n            registers.remove('pc')\n            registers.append(self.reg_names[arch]['pc'])\n        if ('sp' in registers):\n            registers.remove('sp')\n            registers.append(self.reg_names[arch]['sp'])\n    else:\n        raise Exception('Unsupported architecture: {}'.format(target['arch']))\n    if (registers != []):\n        regs = {\n            \n        }\n        for reg in registers:\n            regs[reg] = self.get_register(reg)\n    else:\n        log.debug('Getting registers for arch {}'.format(arch))\n        if (arch == 'x86_64'):\n            regs = self.get_registers_x86_64()\n        elif (arch == 'x86'):\n            regs = self.get_registers_x86()\n        elif (arch == 'arm'):\n            regs = self.get_registers_arm()\n        else:\n            raise UnknownArchitectureException()\n    return regs\n", "label": 1}
{"function": "\n\ndef __init__(self, model=None, path=None, actions=None):\n    if model:\n        self.model = model\n    if (not self.model_name):\n        self.model_name = self.model._meta.object_name\n    if (not self.app_name):\n        self.app_name = self.model._meta.app_label\n    if ((not path) and (not self.path)):\n        self.path = self.model_name.lower()\n    if (not self.module_name):\n        parts = self.__class__.__module__.split('.')\n        self.module_name = parts[(- 2)]\n        if ((self.module_name == 'views') and (len(parts) >= 3)):\n            self.module_name = parts[(- 3)]\n    if actions:\n        self.actions = actions\n", "label": 1}
{"function": "\n\ndef getData(self, key, create_sub_data=False):\n    self.need_open()\n    if (key in self.db):\n        if (self.verbose > 1):\n            print('getData key exists')\n        (t, v) = self.get_value_and_value_type(key)\n        if (t == TYPE_SUB):\n            sub_db_name = v['name']\n            if (self.verbose > 1):\n                print('return subData stored as key', key, 'using name', sub_db_name)\n            return PersistentDataStructure(name=sub_db_name, path=os.path.join(self._dirname), verbose=self.verbose)\n        elif (t == TYPE_NPA):\n            if (self.verbose > 1):\n                print('return nparray value')\n            return self._loadNPA(v['fname'])\n        else:\n            if (self.verbose > 1):\n                print('return normal value')\n            return v\n    elif (not create_sub_data):\n        raise KeyError('key not found\\n{}'.format(key_to_str(key)))\n    else:\n        if (self.verbose > 1):\n            print('getData key does NOT exists -> create subData')\n        return self.newSubData(key)\n", "label": 1}
{"function": "\n\ndef get_project_stats(self, start_period, end_period):\n    projects_by_id = dict(((p.id, p) for p in self.projects))\n    project_ids = projects_by_id.keys()\n    query = db.session.query(Build.project_id, Build.result, func.count(Build.id).label('num'), func.avg(Build.duration).label('duration')).join(Source, (Source.id == Build.source_id)).filter(Build.project_id.in_(project_ids), (Build.status == Status.finished), Build.result.in_([Result.failed, Result.passed]), (Build.date_created >= start_period), (Build.date_created < end_period), *build_type.get_any_commit_build_filters()).group_by(Build.project_id, Build.result)\n    project_results = {\n        \n    }\n    for project in self.projects:\n        project_results[project] = {\n            'total_builds': 0,\n            'green_builds': 0,\n            'green_percent': None,\n            'avg_duration': 0,\n            'link': build_uri('/project/{0}/'.format(project.slug)),\n        }\n    for (project_id, result, num_builds, duration) in query:\n        if (duration is None):\n            duration = 0\n        project = projects_by_id[project_id]\n        if (result == Result.passed):\n            project_results[project]['avg_duration'] = duration\n        project_results[project]['total_builds'] += num_builds\n        if (result == Result.passed):\n            project_results[project]['green_builds'] += num_builds\n    for (project, stats) in project_results.iteritems():\n        if stats['total_builds']:\n            stats['green_percent'] = percent(stats['green_builds'], stats['total_builds'])\n        else:\n            stats['green_percent'] = None\n    return project_results\n", "label": 1}
{"function": "\n\ndef startup(self):\n    '\\n        Check and extract helper files\\n\\n        @param on_done: callback after loaded\\n        '\n    helpers = sublime.find_resources('JavatarAutocompleteHelper.jar')\n    installed = False\n    for helper in helpers:\n        if (helper[:9] == 'Packages/'):\n            helper = os.path.join(sublime.packages_path(), helper[9:])\n        if (os.path.exists(helper) and self.verify_helper(helper)):\n            installed = True\n            break\n    if (not installed):\n        Logger().info('Updating helper files...')\n        file_path = os.path.join(sublime.packages_path(), 'User', 'Javatar', 'Helper', 'JavatarAutocompleteHelper.jar')\n        if os.path.exists(file_path):\n            os.remove(file_path)\n        if (not os.path.isdir(os.path.dirname(file_path))):\n            try:\n                os.makedirs(os.path.dirname(file_path))\n            except:\n                pass\n        helper_file = open(file_path, 'wb')\n        helper_file.write(sublime.load_binary_resource('Packages/Javatar/binary/JavatarAutocompleteHelper.jar'))\n        helper_file.close()\n", "label": 1}
{"function": "\n\ndef update(*args, **kwds):\n    \"Like dict.update() but add counts instead of replacing them.\\n\\n        Source can be an iterable, a dictionary, or another Counter instance.\\n\\n        >>> c = Counter('which')\\n        >>> c.update('witch')           # add elements from another iterable\\n        >>> d = Counter('watch')\\n        >>> c.update(d)                 # add elements from another counter\\n        >>> c['h']                      # four 'h' in which, witch, and watch\\n        4\\n\\n        \"\n    if (not args):\n        raise TypeError(\"descriptor 'update' of 'Counter' object needs an argument\")\n    self = args[0]\n    args = args[1:]\n    if (len(args) > 1):\n        raise TypeError(('expected at most 1 arguments, got %d' % len(args)))\n    iterable = (args[0] if args else None)\n    if (iterable is not None):\n        if isinstance(iterable, Mapping):\n            if self:\n                self_get = self.get\n                for (elem, count) in iterable.items():\n                    self[elem] = (count + self_get(elem, 0))\n            else:\n                super(Counter, self).update(iterable)\n        else:\n            _count_elements(self, iterable)\n    if kwds:\n        self.update(kwds)\n", "label": 1}
{"function": "\n\ndef _passwd_opts(self):\n    '\\n        Return options to pass to ssh\\n        '\n    options = ['ControlMaster=auto', 'StrictHostKeyChecking=no']\n    if (self.opts['_ssh_version'] > (4, 9)):\n        options.append('GSSAPIAuthentication=no')\n    options.append('ConnectTimeout={0}'.format(self.timeout))\n    if self.opts.get('ignore_host_keys'):\n        options.append('StrictHostKeyChecking=no')\n    if self.opts.get('no_host_keys'):\n        options.extend(['StrictHostKeyChecking=no', 'UserKnownHostsFile=/dev/null'])\n    if self.passwd:\n        options.extend(['PasswordAuthentication=yes', 'PubkeyAuthentication=yes'])\n    else:\n        options.extend(['PasswordAuthentication=no', 'PubkeyAuthentication=yes', 'KbdInteractiveAuthentication=no', 'ChallengeResponseAuthentication=no', 'BatchMode=yes'])\n    if self.port:\n        options.append('Port={0}'.format(self.port))\n    if self.user:\n        options.append('User={0}'.format(self.user))\n    if self.identities_only:\n        options.append('IdentitiesOnly=yes')\n    ret = []\n    for option in options:\n        ret.append('-o {0} '.format(option))\n    return ''.join(ret)\n", "label": 1}
{"function": "\n\ndef _fail_ut_eof(self, name, end_token_stack, lineno):\n    expected = []\n    for exprs in end_token_stack:\n        expected.extend(map(describe_token_expr, exprs))\n    if end_token_stack:\n        currently_looking = ' or '.join(((\"'%s'\" % describe_token_expr(expr)) for expr in end_token_stack[(- 1)]))\n    else:\n        currently_looking = None\n    if (name is None):\n        message = ['Unexpected end of template.']\n    else:\n        message = [(\"Encountered unknown tag '%s'.\" % name)]\n    if currently_looking:\n        if ((name is not None) and (name in expected)):\n            message.append(('You probably made a nesting mistake. Jinja is expecting this tag, but currently looking for %s.' % currently_looking))\n        else:\n            message.append(('Jinja was looking for the following tags: %s.' % currently_looking))\n    if self._tag_stack:\n        message.append((\"The innermost block that needs to be closed is '%s'.\" % self._tag_stack[(- 1)]))\n    self.fail(' '.join(message), lineno)\n", "label": 1}
{"function": "\n\ndef _find_packages(where='.', exclude=()):\n    'Return a list all Python packages found within directory \\'where\\'\\n\\n        \\'where\\' should be supplied as a \"cross-platform\" (i.e. URL-style) path; it\\n        will be converted to the appropriate local path syntax.  \\'exclude\\' is a\\n        sequence of package names to exclude; \\'*\\' can be used as a wildcard in the\\n        names, such that \\'foo.*\\' will exclude all subpackages of \\'foo\\' (but not\\n        \\'foo\\' itself).\\n        '\n    out = []\n    stack = [(convert_path(where), '')]\n    while stack:\n        (where, prefix) = stack.pop(0)\n        for name in os.listdir(where):\n            fn = os.path.join(where, name)\n            if (('.' not in name) and os.path.isdir(fn) and os.path.isfile(os.path.join(fn, '__init__.py'))):\n                out.append((prefix + name))\n                stack.append((fn, ((prefix + name) + '.')))\n    for pat in (list(exclude) + ['ez_setup', 'distribute_setup']):\n        from fnmatch import fnmatchcase\n        out = [item for item in out if (not fnmatchcase(item, pat))]\n    return out\n", "label": 1}
{"function": "\n\n@classmethod\ndef search(cls, element, pattern):\n    '\\n        Helper method that returns a list of elements that match the\\n        given path pattern of form {type}.{group}.{label}.\\n\\n        The input may be a Layout, an Overlay type or a single\\n        Element.\\n        '\n    if isinstance(element, Layout):\n        return [el for cell in element for el in cls.search(cell, pattern)]\n    if isinstance(element, (NdOverlay, Overlay)):\n        return [el for el in element if el.matches(pattern)]\n    elif isinstance(element, Element):\n        return ([element] if element.matches(pattern) else [])\n", "label": 1}
{"function": "\n\ndef _print_execution_details(self, execution, args, **kwargs):\n    '\\n        Print the execution detail to stdout.\\n\\n        This method takes into account if an executed action was workflow or not\\n        and formats the output accordingly.\\n        '\n    runner_type = execution.action.get('runner_type', 'unknown')\n    is_workflow_action = (runner_type in WORKFLOW_RUNNER_TYPES)\n    show_tasks = getattr(args, 'show_tasks', False)\n    raw = getattr(args, 'raw', False)\n    detail = getattr(args, 'detail', False)\n    key = getattr(args, 'key', None)\n    attr = getattr(args, 'attr', [])\n    if (show_tasks and (not is_workflow_action)):\n        raise ValueError('--show-tasks option can only be used with workflow actions')\n    if ((not raw) and (not detail) and (show_tasks or is_workflow_action)):\n        self._run_and_print_child_task_list(execution=execution, args=args, **kwargs)\n    else:\n        instance = execution\n        if detail:\n            formatter = table.PropertyValueTable\n        else:\n            formatter = execution_formatter.ExecutionResult\n        if detail:\n            options = {\n                'attributes': copy.copy(self.display_attributes),\n            }\n        elif key:\n            options = {\n                'attributes': [('result.%s' % key)],\n                'key': key,\n            }\n        else:\n            options = {\n                'attributes': attr,\n            }\n        options['json'] = args.json\n        options['attribute_transform_functions'] = self.attribute_transform_functions\n        self.print_output(instance, formatter, **options)\n", "label": 1}
{"function": "\n\ndef _walk_TableConstructor(self, node):\n    (yield self._get_text(node, '{'))\n    self._indent += 1\n    if node.fields:\n        for t in self._walk(node.fields[0]):\n            (yield t)\n        if (len(node.fields) > 1):\n            for i in range(1, len(node.fields)):\n                (yield self._get_code_for_spaces(node))\n                if self._args.get('ignore_tokens'):\n                    (yield ', ')\n                else:\n                    (yield self._get_text(node, self._tokens[self._pos].code))\n                for t in self._walk(node.fields[i]):\n                    (yield t)\n    self._indent -= 1\n    (yield self._get_code_for_spaces(node))\n    if (not self._args.get('ignore_tokens')):\n        if (self._tokens[self._pos].matches(lexer.TokSymbol(',')) or self._tokens[self._pos].matches(lexer.TokSymbol(';'))):\n            (yield self._get_text(node, self._tokens[self._pos].code))\n    (yield self._get_text(node, '}'))\n", "label": 1}
{"function": "\n\ndef _make_table(self, ax, df, title, height=None):\n    if (df is None):\n        ax.set_visible(False)\n        return\n    import pandas.tools.plotting as plotting\n    idx_nlevels = df.index.nlevels\n    col_nlevels = df.columns.nlevels\n    df = self._insert_index(df)\n    tb = plotting.table(ax, df, loc=9)\n    tb.set_fontsize(self.font_size)\n    if (height is None):\n        height = (1.0 / (len(df) + 1))\n    props = tb.properties()\n    for ((r, c), cell) in compat.iteritems(props['celld']):\n        if (c == (- 1)):\n            cell.set_visible(False)\n        elif ((r < col_nlevels) and (c < idx_nlevels)):\n            cell.set_visible(False)\n        elif ((r < col_nlevels) or (c < idx_nlevels)):\n            cell.set_facecolor('#AAAAAA')\n        cell.set_height(height)\n    ax.set_title(title, size=self.font_size)\n    ax.axis('off')\n", "label": 1}
{"function": "\n\n@contextfilter\ndef number_format(context, value):\n    '\\n    Enforces 2 decimal places after a number if only one is given (adds a zero)\\n    also formats comma separators every 3rd digit before decimal place.\\n    '\n    value = str(value)\n    negative = False\n    addzero = None\n    if (value[0] == '-'):\n        value = value[1:]\n        negative = True\n    if ('.' in value):\n        point = value.rindex('.')\n        if (point == (len(value) - 2)):\n            addzero = True\n    else:\n        point = len(value)\n    while (point < (len(value) - 3)):\n        if (value[(len(value) - 1)] == '0'):\n            value = value[:(len(value) - 1)]\n        else:\n            break\n    while (point > 3):\n        value = ((value[:(point - 3)] + ',') + value[(point - 3):])\n        point = value.index(',')\n    if addzero:\n        value += '0'\n    if negative:\n        value = ('-' + value)\n    return value\n", "label": 1}
{"function": "\n\ndef createRC4(key, IV, implList=None):\n    'Create a new RC4 object.\\n\\n    @type key: str\\n    @param key: A 16 to 32 byte string.\\n\\n    @type IV: object\\n    @param IV: Ignored, whatever it is.\\n\\n    @rtype: L{tlslite.utils.RC4}\\n    @return: An RC4 object.\\n    '\n    if (implList == None):\n        implList = ['openssl', 'pycrypto', 'python']\n    if (len(IV) != 0):\n        raise AssertionError()\n    for impl in implList:\n        if ((impl == 'openssl') and cryptomath.m2cryptoLoaded):\n            return openssl_rc4.new(key)\n        elif ((impl == 'pycrypto') and cryptomath.pycryptoLoaded):\n            return pycrypto_rc4.new(key)\n        elif (impl == 'python'):\n            return python_rc4.new(key)\n    raise NotImplementedError()\n", "label": 1}
{"function": "\n\ndef watch_changes(model, ignore=[], master_key=None, **options):\n    'Declare the specified model to be \"observed\" (\"watched\") for changes.\\n    Each change to an object comprising at least one watched field\\n    will lead to an entry to the `Changes` table.\\n\\n    `ignore` should be a string with a space-separated list of field\\n    names to be ignored.\\n    \\n    All calls to watch_changes will be grouped by model.\\n\\n    '\n    if isinstance(ignore, basestring):\n        ignore = fields.fields_list(model, ignore)\n    if isinstance(master_key, basestring):\n        fld = model.get_data_elem(master_key)\n        if (fld is None):\n            raise Exception(('No field %r in %s' % (master_key, model)))\n        master_key = fld\n    if isinstance(master_key, fields.RemoteField):\n        get_master = master_key.func\n    elif (master_key is None):\n        get_master = return_self\n    else:\n\n        def get_master(obj):\n            return getattr(obj, master_key.name)\n    ignore = set(ignore)\n    cs = model.change_watcher_spec\n    if (cs is not None):\n        ignore |= cs.ignored_fields\n    for f in model._meta.fields:\n        if (not f.editable):\n            ignore.add(f.name)\n    model.change_watcher_spec = WatcherSpec(ignore, get_master)\n", "label": 1}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    argvals = list(args)\n    ops = []\n    tapes = set()\n    for (i, arg) in enumerate(args):\n        if isinstance(arg, Node):\n            argvals[i] = arg.value\n            if (i in self.zero_grads):\n                continue\n            for (tape, parent_rnode) in iteritems(arg.tapes):\n                if (not tape.complete):\n                    ops.append((tape, i, parent_rnode))\n                    tapes.add(tape)\n    result = self.fun(*argvals, **kwargs)\n    if (result is NotImplemented):\n        return result\n    if ops:\n        result = new_node(result, tapes)\n        for (tape, argnum, parent) in ops:\n            gradfun = self.gradmaker(argnum, result, args, kwargs)\n            rnode = result.tapes[tape]\n            rnode.parent_grad_ops.append((gradfun, parent))\n    return result\n", "label": 1}
{"function": "\n\ndef code_to_func(lang, code, out_parms, func_name, func_parms, symb_replace):\n    if (not isinstance(code[1], list)):\n        code = (code[0], [code[1]])\n    if (not isinstance(out_parms, list)):\n        out_parms = [out_parms]\n    lang = lang.lower()\n    if (lang in ['python', 'py']):\n        gen_func = gen_py_func\n    elif (lang in ['c', 'c++']):\n        gen_func = gen_c_func\n    elif (lang in ['julia', 'jl']):\n        gen_func = gen_julia_func\n    else:\n        raise Exception('chosen language not supported.')\n    if symb_replace:\n        sympified_replace = {\n            \n        }\n        for (k, v) in symb_replace.items():\n            if isinstance(k, str):\n                k = sympy.Symbol(k)\n            if isinstance(v, str):\n                v = sympy.Symbol(v)\n            sympified_replace[k] = v\n        code = xreplace(code, sympified_replace)\n    return gen_func(code, out_parms, func_parms, func_name)\n", "label": 1}
{"function": "\n\ndef __init__(self, hidden_size, input_type='sequence', output_type='sequence', inner_activation='sigmoid', outer_activation='tanh', inner_init=None, outer_init=None, steps=None, go_backwards=False, persistent_state=False, batch_size=0, reset_state_for_input=None, forget_bias=1, mask=None, second_input=None, second_input_size=None):\n    super(LSTM, self).__init__('lstm')\n    self._hidden_size = hidden_size\n    self._input_type = input_type\n    self._output_type = output_type\n    self._inner_activation = inner_activation\n    self._outer_activation = outer_activation\n    self._inner_init = inner_init\n    self._outer_init = outer_init\n    self._steps = steps\n    self.persistent_state = persistent_state\n    self.reset_state_for_input = reset_state_for_input\n    self.batch_size = batch_size\n    self.go_backwards = go_backwards\n    mask = (mask.tensor if (type(mask) == NeuralVariable) else mask)\n    self.mask = (mask.dimshuffle((1, 0)) if mask else None)\n    self._sequence_map = OrderedDict()\n    if (type(second_input) == NeuralVariable):\n        second_input_size = second_input.dim()\n        second_input = second_input.tensor\n    self.second_input = second_input\n    self.second_input_size = second_input_size\n    self.forget_bias = forget_bias\n    if (input_type not in INPUT_TYPES):\n        raise Exception(('Input type of LSTM is wrong: %s' % input_type))\n    if (output_type not in OUTPUT_TYPES):\n        raise Exception(('Output type of LSTM is wrong: %s' % output_type))\n    if (self.persistent_state and (not self.batch_size)):\n        raise Exception('Batch size must be set for persistent state mode')\n    if (mask and (input_type == 'one')):\n        raise Exception('Mask only works with sequence input')\n", "label": 1}
{"function": "\n\ndef save(self):\n    data = self.clean_data\n    article_url = data.get('article_url')\n    article_title = (data.get('article_title') or data.get('article_url'))\n    article = models.Article.get_or_insert(article_url, url=article_url, title=article_title)\n    if (article.title != article_title):\n        article.title = article_title\n        article.put()\n    author_email = data.get('author_email')\n    author_name = data.get('author_name')\n    author_url = data.get('author_url')\n    author_ip = data.get('author_ip')\n    author_key = ((author_email or '') + author_name)\n    author = models.Author.get_or_insert(author_key, name=author_name)\n    has_changes = False\n    if ((author.url != author_url) and author_url):\n        author.url = author_url\n        has_changes = True\n    if (author_email and (author_email != author.email)):\n        author.email = author_email\n        has_changes = True\n    if has_changes:\n        author.put()\n    comment = models.Comment(parent=article, comment=data.get('comment'), author=author, article=article, author_ip=author_ip)\n    comment.put()\n    params = urllib.urlencode({\n        'article_url': article_url,\n        'comment_id': comment.key().id(),\n    })\n    taskqueue.add(url=('/api/notify/?' + params), method='GET')\n    self._author = author\n    self._article = article\n    self._comment = comment\n    return comment\n", "label": 1}
{"function": "\n\ndef _extractBasicPluginInfo(self, directory, filename):\n    \"\\n        Gather some basic documentation about the plugin described by\\n        it's info file (found at 'directory/filename').\\n        \\n        Return a dictionary containing the core information (name and\\n        path) as well as as the 'documentation' info (version, author,\\n        description etc).\\n        \\n        See also:\\n        \\n          ``self._extractCorePluginInfo``\\n        \"\n    (infos, config_parser) = self._extractCorePluginInfo(directory, filename)\n    if (infos and config_parser and config_parser.has_section('Documentation')):\n        if config_parser.has_option('Documentation', 'Author'):\n            infos['author'] = config_parser.get('Documentation', 'Author')\n        if config_parser.has_option('Documentation', 'Version'):\n            infos['version'] = config_parser.get('Documentation', 'Version')\n        if config_parser.has_option('Documentation', 'Website'):\n            infos['website'] = config_parser.get('Documentation', 'Website')\n        if config_parser.has_option('Documentation', 'Copyright'):\n            infos['copyright'] = config_parser.get('Documentation', 'Copyright')\n        if config_parser.has_option('Documentation', 'Description'):\n            infos['description'] = config_parser.get('Documentation', 'Description')\n    return (infos, config_parser)\n", "label": 1}
{"function": "\n\ndef prepare(self):\n    self.lengths = [self.captions_len[(i, (- 1))] for i in xrange(self.captions_len.shape[0])]\n    self.len_unique = np.unique(self.lengths)\n    if ((self.minlen != None) and (self.maxlen != None)):\n        self.len_unique = [ll for ll in self.len_unique if ((ll >= self.minlen) and (ll < self.maxlen))]\n    else:\n        self.len_unique = [ll for ll in self.len_unique]\n    self.len2cap = dict()\n    for ll in self.len_unique:\n        self.len2cap[ll] = np.where((self.lengths == ll))[0]\n", "label": 1}
{"function": "\n\ndef _handle_execute_reply(self, msg):\n    ' Handles replies for code execution.\\n        '\n    info = self._request_info.get('execute')\n    if (info and (info.id == msg['parent_header']['msg_id']) and (info.kind == 'user') and (not self._hidden)):\n        self.kernel_manager.sub_channel.flush()\n        if self.ansi_codes:\n            self._ansi_processor.reset_sgr()\n        content = msg['content']\n        status = content['status']\n        if (status == 'ok'):\n            self._process_execute_ok(msg)\n        elif (status == 'error'):\n            self._process_execute_error(msg)\n        elif (status == 'abort'):\n            self._process_execute_abort(msg)\n        self._show_interpreter_prompt_for_reply(msg)\n        self.executed.emit(msg)\n", "label": 1}
{"function": "\n\ndef getnnz(self, axis=None):\n    if (axis is None):\n        nnz = len(self.data)\n        if ((nnz != len(self.row)) or (nnz != len(self.col))):\n            raise ValueError('row, column, and data array must all be the same length')\n        if ((self.data.ndim != 1) or (self.row.ndim != 1) or (self.col.ndim != 1)):\n            raise ValueError('row, column, and data arrays must be 1-D')\n        return int(nnz)\n    if (axis < 0):\n        axis += 2\n    if (axis == 0):\n        return np.bincount(downcast_intp_index(self.col), minlength=self.shape[1])\n    elif (axis == 1):\n        return np.bincount(downcast_intp_index(self.row), minlength=self.shape[0])\n    else:\n        raise ValueError('axis out of bounds')\n", "label": 1}
{"function": "\n\ndef _list_windows(self):\n    'Return list of windows in :py:obj:`dict` form.\\n\\n        Retrieved from ``$ tmux(1) list-windows`` stdout.\\n\\n        The :py:obj:`list` is derived from ``stdout`` in :class:`util.tmux_cmd`\\n        which wraps :py:class:`subprocess.Popen`.\\n\\n        :rtype: list\\n\\n        '\n    wformats = (['session_name', 'session_id'] + formats.WINDOW_FORMATS)\n    tmux_formats = [('#{%s}' % format) for format in wformats]\n    proc = self.cmd('list-windows', '-a', ('-F%s' % '\\t'.join(tmux_formats)))\n    if proc.stderr:\n        raise exc.TmuxpException(proc.stderr)\n    windows = proc.stdout\n    wformats = (['session_name', 'session_id'] + formats.WINDOW_FORMATS)\n    windows = [dict(zip(wformats, window.split('\\t'))) for window in windows]\n    windows = [dict(((k, v) for (k, v) in window.items() if v)) for window in windows]\n    for w in windows:\n        if (not ('window_id' in w)):\n            w['window_id'] = w['window_name']\n    if self._windows:\n        self._windows[:] = []\n    self._windows.extend(windows)\n    return self._windows\n", "label": 1}
{"function": "\n\ndef deselect(self, mode, data):\n    'Returns false if there was a data error when deselecting'\n    size = len(self.objects)\n    if (mode == 'None'):\n        return True\n    elif (mode == 'Mask'):\n        ' Deselect objects according to deselect mask '\n        mask = data\n        if (len(mask) < size):\n            mask = (mask + ([False] * (len(self.objects) - len(mask))))\n        self.logger.debug('Deselection Mask: {0}'.format(mask))\n        self.objects = [obj for (slct, obj) in filter((lambda slct_obj: (not slct_obj[0])), zip(mask, self.objects))]\n        return (len(mask) <= size)\n    elif (mode == 'OneIndices'):\n        ' Deselect objects according to indexes '\n        clean_data = list(filter((lambda i: (i < size)), data))\n        self.objects = [self.objects[i] for i in range(len(self.objects)) if (i not in clean_data)]\n        return (len(clean_data) == len(data))\n    elif (mode == 'ZeroIndices'):\n        ' Deselect objects according to indexes '\n        clean_data = list(filter((lambda i: (i < size)), data))\n        self.objects = [self.objects[i] for i in clean_data]\n        return (len(clean_data) == len(data))\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef _lift_TableColumn(self, expr, block=None):\n    node = expr.op()\n    tnode = node.table.op()\n    root = _base_table(tnode)\n    result = expr\n    if isinstance(root, ops.Selection):\n        can_lift = False\n        for val in root.selections:\n            if (isinstance(val.op(), ops.PhysicalTable) and (node.name in val.schema())):\n                can_lift = True\n                lifted_root = self.lift(val)\n            elif (isinstance(val.op(), ops.TableColumn) and (val.op().name == val.get_name()) and (node.name == val.get_name())):\n                can_lift = True\n                lifted_root = self.lift(val.op().table)\n        if (can_lift and (not block)):\n            lifted_node = ops.TableColumn(node.name, lifted_root)\n            result = expr._factory(lifted_node, name=expr._name)\n    return result\n", "label": 1}
{"function": "\n\ndef _handle_activating_mode(state, request_endpoint):\n    status = state['status']\n    if (request_endpoint == 'snapshots/<string:snapshot_id>'):\n        if (request.method in FORBIDDEN_METHODS):\n            return _return_maintenance_error(status)\n    if (request_endpoint == 'snapshots/<string:snapshot_id>/restore'):\n        return _return_maintenance_error(status)\n    if (request_endpoint == 'executions'):\n        if (request.method in FORBIDDEN_METHODS):\n            return _return_maintenance_error(status)\n    if (request_endpoint == 'deployments/<string:deployment_id>'):\n        if (request.method in FORBIDDEN_METHODS):\n            return _return_maintenance_error(status)\n    if (request_endpoint == 'deployment-modifications'):\n        if (request.method in FORBIDDEN_METHODS):\n            return _return_maintenance_error(status)\n", "label": 1}
{"function": "\n\ndef load(self, filename):\n    for line in open(filename):\n        if (line.startswith('#') or line.startswith('%')):\n            continue\n        if (line.strip() == ''):\n            continue\n        line = (line[:line.find('#')] + '\\n')\n        line = (line[:line.find('%')] + '\\n')\n        line = line.strip()\n        if line.startswith('@'):\n            pass\n        else:\n            semicolon = line.find(';')\n            charList = line[:semicolon].strip().split()\n            x = line[semicolon:]\n            collElements = []\n            while True:\n                begin = x.find('[')\n                if (begin == (- 1)):\n                    break\n                end = x[begin:].find(']')\n                collElement = x[begin:((begin + end) + 1)]\n                x = x[(begin + 1):]\n                alt = collElement[1]\n                chars = collElement[2:(- 1)].split('.')\n                collElements.append((alt, chars))\n            integer_points = [int(ch, 16) for ch in charList]\n            self.table.add(integer_points, collElements)\n", "label": 1}
{"function": "\n\n@renders('text/plain', 'text/x-cmd', 'text/x-irc')\ndef generic_text(self, result, errors):\n    out = []\n    kv_iterate = None\n    single_iterate = []\n    if hasattr(result, 'items'):\n        kv_iterate = result.items()\n    elif hasattr(result, 'lower'):\n        return result\n    elif (not result):\n        return ''\n    elif hasattr(result, 'append'):\n        single_iterate = result\n    elif hasattr(result, 'todict'):\n        kv_iterate = result.todict().items()\n    else:\n        kv_iterate = result.__dict__.items()\n    if kv_iterate:\n        for (key, value) in kv_iterate:\n            row = '{0} - {1}'.format(key, value)\n            out.append(row)\n    else:\n        for value in single_iterate:\n            out.append(self.generic_text(value, None))\n    return '\\n'.join(out)\n", "label": 1}
{"function": "\n\ndef get_point_of_reference(unit, count, epoch=None):\n    '\\n    Get a point-of-reference timestamp in epoch + milliseconds by deriving\\n    from a `unit` and a `count`, and an optional reference timestamp, `epoch`\\n\\n    :arg unit: One of ``seconds``, ``minutes``, ``hours``, ``days``, ``weeks``,\\n        ``months``, or ``years``.\\n    :arg unit_count: The number of ``units``. ``unit_count`` * ``unit`` will\\n        be calculated out to the relative number of seconds.\\n    :arg epoch: An epoch timestamp used in conjunction with ``unit`` and\\n        ``unit_count`` to establish a point of reference for calculations.\\n    :rtype: int\\n    '\n    if (unit == 'seconds'):\n        multiplier = 1\n    elif (unit == 'minutes'):\n        multiplier = 60\n    elif (unit == 'hours'):\n        multiplier = 3600\n    elif (unit == 'days'):\n        multiplier = (3600 * 24)\n    elif (unit == 'weeks'):\n        multiplier = ((3600 * 24) * 7)\n    elif (unit == 'months'):\n        multiplier = ((3600 * 24) * 30)\n    elif (unit == 'years'):\n        multiplier = ((3600 * 24) * 365)\n    else:\n        raise ValueError('Invalid unit: {0}.'.format(unit))\n    if (not epoch):\n        epoch = time.time()\n    epoch = fix_epoch(epoch)\n    return (epoch - (multiplier * count))\n", "label": 1}
{"function": "\n\ndef _json(self):\n    draw = self.get_integer_param('draw')\n    start = self.get_integer_param('start')\n    length = self.get_integer_param('length')\n    columns = self.query_into_dict('columns')\n    ordering = self.query_into_dict('order')\n    search = self.query_into_dict('search')\n    query = self.query\n    total_records = query.count()\n    if (callable(self.search_func) and search.get('value', None)):\n        query = self.search_func(query, str(search['value']))\n    for order in ordering.values():\n        (direction, column) = (order['dir'], order['column'])\n        if (column not in columns):\n            raise DataTablesError('Cannot order {}: column not found'.format(column))\n        if (not columns[column]['orderable']):\n            continue\n        column_name = columns[column]['data']\n        column = self.columns_dict[column_name]\n        model_column = self.get_column(column)\n        if isinstance(model_column, property):\n            raise DataTablesError('Cannot order by column {} as it is a property'.format(column.model_name))\n        query = query.order_by((model_column.desc() if (direction == 'desc') else model_column.asc()))\n    filtered_records = query.count()\n    query = query.slice(start, (start + length))\n    return {\n        'draw': draw,\n        'recordsTotal': total_records,\n        'recordsFiltered': filtered_records,\n        'data': [self.output_instance(instance) for instance in query.all()],\n    }\n", "label": 1}
{"function": "\n\ndef find_next_sym(next_bp, prev_bp, timeout):\n    iters = 100\n    found_sym = False\n    sample_time = 0\n    set_bp(next_bp, 0, 1)\n    set_bp(prev_bp, 1, iters)\n    while (not is_complete()):\n        pykd.go()\n        curr_bp = get_bp_hit()\n        target_time = get_proc_run_time()\n        log.debug('target time %0.2f', target_time)\n        if (curr_bp == 1):\n            if (target_time >= timeout):\n                break\n            iter_duration = (target_time - sample_time)\n            if (iter_duration < 0.5):\n                if (iters < 25600):\n                    iters *= 2\n                    log.debug('iter duration: %0.2f, (x2) prev_bp iters: %d', iter_duration, iters)\n            elif ((iter_duration >= 0.5) and (iter_duration < 0.85)):\n                iters += 100\n                log.debug('iter duration: %0.2f, (+100) prev_bp iters: %d', iter_duration, iters)\n            set_bp(prev_bp, 1, iters)\n        elif (curr_bp == 0):\n            found_sym = True\n            break\n        else:\n            log.debug('break not triggered by breakpoint')\n            if (pykd.dbgCommand('.lastevent').find('(!!! second chance !!!)') != (- 1)):\n                raise RuntimeError('Expected Timeout found Access violation!')\n        sample_time = target_time\n    pykd.removeBp(1)\n    pykd.removeBp(0)\n    return found_sym\n", "label": 1}
{"function": "\n\n@recordable\ndef open(self, filename, scene=None):\n    'Open a file given a filename if possible in either the\\n        current scene or the passed `scene`.\\n        '\n    passed_scene = scene\n    reader = registry.get_file_reader(filename)\n    if (reader is None):\n        msg = ('No suitable reader found for the file %s' % filename)\n        error(msg)\n    else:\n        src = None\n        if (scene is None):\n            scene = self.current_scene\n        if (scene is None):\n            scene = self.new_scene()\n        try:\n            sc = scene.scene\n            if (sc is not None):\n                sc.busy = True\n            callable = reader.get_callable()\n            if (reader.factory is None):\n                src = callable()\n                src.initialize(filename)\n            else:\n                src = callable(filename, self)\n            if (src is not None):\n                self.add_source(src, passed_scene)\n        finally:\n            if (sc is not None):\n                sc.busy = False\n        if (src is not None):\n            return src\n", "label": 1}
{"function": "\n\ndef compute(self):\n    ' compute() -> None\\n        Dispatch the vtkRenderer to the actual rendering widget\\n        '\n    if self.has_input('canvas'):\n        canvas = self.get_input('canvas')\n    else:\n        self.cellWidget = self.displayAndWait(QCDATWidget, (None,))\n        self.set_output('canvas', self.cellWidget.canvas)\n        return\n    self.set_output('canvas', canvas)\n    if (not self.has_input('gmName')):\n        return\n    if (not self.has_input('plotType')):\n        return\n    if (not self.has_input('slab1')):\n        return\n    if (not self.has_input('template')):\n        return\n    args = []\n    slab1 = self.get_input('slab1')\n    args.append(self.get_input('slab1'))\n    if self.has_input('slab2'):\n        args.append(self.get_input('slab2'))\n    args.append(self.get_input('template'))\n    args.append(self.get_input('plotType'))\n    args.append(self.get_input('gmName'))\n    kwargs = {\n        \n    }\n    if self.has_input('continents'):\n        kwargs['continents'] = self.get_input('continents')\n    self.location = CellLocation()\n    if self.has_input('row'):\n        self.location.row = self.get_input('row')\n    if self.has_input('col'):\n        self.location.col = self.get_input('col')\n    inputPorts = (canvas, args, kwargs)\n    self.displayAndWait(QCDATWidget, inputPorts)\n", "label": 1}
{"function": "\n\ndef run(self):\n    '\\n        Run the eventloop for the telnet server.\\n        '\n    listen_socket = self.create_socket(self.host, self.port)\n    logger.info('Listening for telnet connections on %s port %r', self.host, self.port)\n    try:\n        while True:\n            self.connections = set([c for c in self.connections if (not c.closed)])\n            connections = set([c for c in self.connections if (not c.handling_command)])\n            read_list = ([listen_socket, self._schedule_pipe[0]] + [c.conn for c in connections])\n            (read, _, _) = select.select(read_list, [], [])\n            for s in read:\n                if (s == listen_socket):\n                    self._accept(listen_socket)\n                elif (s == self._schedule_pipe[0]):\n                    self._process_callbacks()\n                else:\n                    self._handle_incoming_data(s)\n    finally:\n        listen_socket.close()\n", "label": 1}
{"function": "\n\n@classmethod\ndef find(cls, query=None, deleted=False, **kwargs):\n    from website.models import Node\n    if (query and getattr(query, 'nodes', False)):\n        for node in query.nodes:\n            replacement_attr = cls.attribute_map.get(node.attribute, False)\n            node.attribute = (replacement_attr or node.attribute)\n    elif isinstance(query, RawQuery):\n        replacement_attr = cls.attribute_map.get(query.attribute, False)\n        query.attribute = (replacement_attr or query.attribute)\n    query = ((query & Q('institution_id', 'ne', None)) if query else Q('institution_id', 'ne', None))\n    query = ((query & Q('is_deleted', 'ne', True)) if (not deleted) else query)\n    nodes = Node.find(query, allow_institution=True, **kwargs)\n    return InstitutionQuerySet(nodes)\n", "label": 1}
{"function": "\n\ndef parse_datatypes(f):\n    dt = set()\n    re_entry = re.compile('\\\\s*<entry><type>([^<]+)</type></entry>')\n    for line in f:\n        if ('<sect1' in line):\n            break\n        if ('<entry><type>' not in line):\n            continue\n        line = re.sub('<replaceable>[^<]+</replaceable>', '', line)\n        line = re.sub('<[^>]+>', '', line)\n        for tmp in [t for tmp in line.split('[') for t in tmp.split(']') if ('(' not in t)]:\n            for t in tmp.split(','):\n                t = t.strip()\n                if (not t):\n                    continue\n                dt.add(' '.join(t.split()))\n    dt = list(dt)\n    dt.sort()\n    return dt\n", "label": 1}
{"function": "\n\ndef FindRendererForObject(rdf_obj):\n    'Find the appropriate renderer for an RDFValue object.'\n    if (not semantic_renderer_cache):\n        for cls in RDFValueRenderer.classes.values():\n            if aff4.issubclass(cls, RDFValueArrayRenderer):\n                repeated_renderer_cache[cls.classname] = cls\n            elif aff4.issubclass(cls, RDFValueRenderer):\n                semantic_renderer_cache[cls.classname] = cls\n    rdf_obj_classname = rdf_obj.__class__.__name__\n    if isinstance(rdf_obj, rdf_protodict.RDFValueArray):\n        return repeated_renderer_cache.get(rdf_obj_classname, RDFValueArrayRenderer)(rdf_obj)\n    if isinstance(rdf_obj, rdf_structs.RepeatedFieldHelper):\n        rdf_obj_classname = rdf_obj.type_descriptor.type.__name__\n        return repeated_renderer_cache.get(rdf_obj_classname, RDFValueArrayRenderer)(rdf_obj)\n    if isinstance(rdf_obj, rdf_structs.RDFProtoStruct):\n        return semantic_renderer_cache.get(rdf_obj_classname, RDFProtoRenderer)(rdf_obj)\n    if isinstance(rdf_obj, rdfvalue.RDFValue):\n        return semantic_renderer_cache.get(rdf_obj_classname, RDFValueRenderer)(rdf_obj)\n    elif isinstance(rdf_obj, dict):\n        return DictRenderer(rdf_obj)\n    return RDFValueRenderer(rdf_obj)\n", "label": 1}
{"function": "\n\ndef setup(args, flags):\n    'Sets up arguments and flags for processing hashes.\\n\\n    Args:\\n        args: named to setup type, infile and outfile\\n\\n    Returns:\\n        boolean: true or false\\n\\n    '\n    if (args.type == 'MOBILEDEVICEID'):\n        flags['regex'] = re.compile('^[a-z0-9][a-z0-9\\\\-]+[a-z0-9]$')\n    elif (args.type == 'IDFA'):\n        flags['regex'] = re.compile('^[a-z0-9][a-z0-9\\\\-]+[a-z0-9]$')\n    elif (args.type == 'ADID'):\n        flags['regex'] = re.compile('^[a-z0-9][a-z0-9\\\\-]+[a-z0-9]$')\n    elif (args.type == 'ANDROID'):\n        flags['regex'] = re.compile('^[a-z0-9]+$')\n    elif (args.type == 'EMAIL'):\n        flags['regex'] = re.compile('^[a-z0-9][a-z0-9_\\\\-\\\\.\\\\+]+\\\\@[a-z0-9][a-z0-9\\\\.]+[a-z]$')\n    elif ((args.type == 'PHONE') or (args.type == 'TWITTERID')):\n        flags['dropleadingzeros'] = True\n        flags['regex'] = re.compile('^\\\\d+$')\n    elif (args.type == 'TWITTERSCREENNAME'):\n        flags['dropleadingat'] = True\n        flags['regex'] = re.compile('^[a-z0-9_]+$')\n    else:\n        print('ERROR: invalid type')\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef get_next_fire_time(self, previous_fire_time, now):\n    if previous_fire_time:\n        start_date = min(now, (previous_fire_time + timedelta(microseconds=1)))\n    else:\n        start_date = (max(now, self.start_date) if self.start_date else now)\n    fieldnum = 0\n    next_date = datetime_ceil(start_date).astimezone(self.timezone)\n    while (0 <= fieldnum < len(self.fields)):\n        field = self.fields[fieldnum]\n        curr_value = field.get_value(next_date)\n        next_value = field.get_next_value(next_date)\n        if (next_value is None):\n            (next_date, fieldnum) = self._increment_field_value(next_date, (fieldnum - 1))\n        elif (next_value > curr_value):\n            if field.REAL:\n                next_date = self._set_field_value(next_date, fieldnum, next_value)\n                fieldnum += 1\n            else:\n                (next_date, fieldnum) = self._increment_field_value(next_date, fieldnum)\n        else:\n            fieldnum += 1\n        if (self.end_date and (next_date > self.end_date)):\n            return None\n    if (fieldnum >= 0):\n        return next_date\n", "label": 1}
{"function": "\n\ndef _resolve_prefix(prefix, type):\n    osx_system_prefix = '/System/Library/Frameworks/Python.framework/Versions'\n    if (type == 'man'):\n        if (prefix == '/usr'):\n            return '/usr/share'\n        if sys.prefix.startswith(osx_system_prefix):\n            return '/usr/share'\n    elif (type == 'bash_comp'):\n        if (prefix == '/usr'):\n            return '/'\n        if sys.prefix.startswith(osx_system_prefix):\n            return '/'\n    elif (type == 'zsh_comp'):\n        if sys.prefix.startswith(osx_system_prefix):\n            return '/usr/local'\n    else:\n        raise ValueError('not supported type')\n    return prefix\n", "label": 1}
{"function": "\n\ndef update_tool(self, tool_number, **kwargs):\n    ' Change parameters of a tool\\n        '\n    if (kwargs.get('feed_rate') is not None):\n        self.tools[tool_number].feed_rate = kwargs.get('feed_rate')\n    if (kwargs.get('retract_rate') is not None):\n        self.tools[tool_number].retract_rate = kwargs.get('retract_rate')\n    if (kwargs.get('rpm') is not None):\n        self.tools[tool_number].rpm = kwargs.get('rpm')\n    if (kwargs.get('diameter') is not None):\n        self.tools[tool_number].diameter = kwargs.get('diameter')\n    if (kwargs.get('max_hit_count') is not None):\n        self.tools[tool_number].max_hit_count = kwargs.get('max_hit_count')\n    if (kwargs.get('depth_offset') is not None):\n        self.tools[tool_number].depth_offset = kwargs.get('depth_offset')\n    newtool = self.tools[tool_number]\n    for hit in self.hits:\n        if (hit.tool.number == newtool.number):\n            hit.tool = newtool\n", "label": 1}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.set_query_kind(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            length = d.getVarInt32()\n            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), (d.pos() + length))\n            d.skip(length)\n            self.mutable_query_ancestor().TryMerge(tmp)\n            continue\n        if (tt == 25):\n            self.set_query_thiscursor(d.get64())\n            continue\n        if (tt == 33):\n            self.set_query_nextcursor(d.get64())\n            continue\n        if (tt == 40):\n            self.add_get_successful_fetch(d.getBoolean())\n            continue\n        if (tt == 50):\n            length = d.getVarInt32()\n            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), (d.pos() + length))\n            d.skip(length)\n            self.add_keys_read().TryMerge(tmp)\n            continue\n        if (tt == 58):\n            length = d.getVarInt32()\n            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), (d.pos() + length))\n            d.skip(length)\n            self.add_keys_written().TryMerge(tmp)\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef AnnotationtoH5(anno, h5fh):\n    'Operate polymorphically on annotations'\n    if (anno.__class__ == annotation.AnnSynapse):\n        return SynapsetoH5(anno, h5fh)\n    elif (anno.__class__ == annotation.AnnSeed):\n        return SeedtoH5(anno, h5fh)\n    if (anno.__class__ == annotation.AnnSegment):\n        return SegmenttoH5(anno, h5fh)\n    if (anno.__class__ == annotation.AnnNeuron):\n        return NeurontoH5(anno, h5fh)\n    if (anno.__class__ == annotation.AnnOrganelle):\n        return OrganelletoH5(anno, h5fh)\n    if (anno.__class__ == annotation.AnnNode):\n        return NodetoH5(anno, h5fh)\n    if (anno.__class__ == annotation.AnnSkeleton):\n        return SkeletontoH5(anno, h5fh)\n    elif (anno.__class__ == annotation.Annotation):\n        return BasetoH5(anno, annotation.ANNO_ANNOTATION, h5fh)\n    else:\n        logger.warning(('(AnnotationtoH5) Does not support this annotation type yet. Type = %s' % anno.__class__))\n        raise OCPCAError(('(AnnotationtoH5) Does not support this annotation type yet. Type = %s' % anno.__class__))\n", "label": 1}
{"function": "\n\ndef on_activated(self, view):\n    '\\n        Timestamp each view when activated.\\n\\n        Detect if on_move event should be executed.\\n        '\n    if (not TabsExtraListener.extra_command_call):\n        window = view.window()\n        if (window is None):\n            return\n        s = window.active_sheet()\n        timestamp_view(window, s)\n    moving = view.settings().get('tabs_extra_moving', None)\n    if (moving is not None):\n        (win_id, group_id) = moving\n        window = view.window()\n        if (window is None):\n            return\n        active_group = window.get_view_index(view)[0]\n        if ((window.id() != win_id) or (int(group_id) != int(active_group))):\n            view.settings().erase('tabs_extra_moving')\n            last_index = view.settings().get('tabs_extra_last_activated_sheet_index', (- 1))\n            self.on_move(view, win_id, int(group_id), last_index)\n    elif (sort_on_load_save() and view.settings().get('tabsextra_to_sort')):\n        view.settings().erase('tabsextra_to_sort')\n        self.on_sort(view)\n", "label": 1}
{"function": "\n\ndef get_context_data(self, **kwargs):\n    context = kwargs\n    if ('view' not in context):\n        context['view'] = self\n    context['forum'] = self.get_forum()\n    context['topic'] = self.get_topic()\n    context['post'] = self.get_post()\n    if context['attachment_formset']:\n        if (hasattr(self, 'attachment_preview') and self.attachment_preview):\n            context['attachment_preview'] = self.attachment_preview\n            attachments = []\n            for form in context['attachment_formset'].forms:\n                if (form['DELETE'].value() or ((not (form['file'].html_name in self.request._files)) and (not form.instance.pk))):\n                    continue\n                attachments.append((form, (self.request._files[form['file'].html_name].name if (not form.instance) else form.instance.filename)))\n            context['attachment_file_previews'] = attachments\n    return context\n", "label": 1}
{"function": "\n\ndef filter_by_period(self, period):\n    ' Return a new ConsumptionData instance within the period.\\n\\n        Parameters\\n        ----------\\n        period : eemeter.evaluation.Period\\n            Period within which to get ConsumptionData\\n\\n        Returns\\n        -------\\n        consumption_data : eemeter.consumption.ConsumptionData\\n            ConsumptionData instance holding data within the requested period.\\n        '\n    filtered_data = None\n    filtered_estimated = None\n    if ((period.start is None) and (period.end is None)):\n        filtered_data = self.data.copy()\n        filtered_estimated = self.estimated.copy()\n    elif ((period.start is None) and (period.end is not None)):\n        filtered_data = self.data[:period.end].copy()\n        filtered_estimated = self.estimated[:period.end].copy()\n    elif ((period.start is not None) and (period.end is None)):\n        filtered_data = self.data[period.start:].copy()\n        filtered_estimated = self.estimated[period.start:].copy()\n    else:\n        filtered_data = self.data[period.start:period.end].copy()\n        filtered_estimated = self.estimated[period.start:period.end].copy()\n    if ((self.freq is None) and (filtered_data.shape[0] > 0)):\n        filtered_data.iloc[(- 1)] = np.nan\n        filtered_estimated.iloc[(- 1)] = np.nan\n    filtered_consumption_data = ConsumptionData(records=None, fuel_type=self.fuel_type, unit_name=self.unit_name, data=filtered_data, estimated=filtered_estimated)\n    return filtered_consumption_data\n", "label": 1}
{"function": "\n\ndef __init__(self, filename_or_obj, mode='r', format=None, group=None, writer=None, mmap=None):\n    import scipy\n    import scipy.io\n    if ((mode != 'r') and (scipy.__version__ < '0.13')):\n        warnings.warn(('scipy %s detected; the minimal recommended version is 0.13. Older version of this library do not reliably read and write files.' % scipy.__version__), ImportWarning)\n    if (group is not None):\n        raise ValueError('cannot save to a group with the scipy.io.netcdf backend')\n    if ((format is None) or (format == 'NETCDF3_64BIT')):\n        version = 2\n    elif (format == 'NETCDF3_CLASSIC'):\n        version = 1\n    else:\n        raise ValueError(('invalid format for scipy.io.netcdf backend: %r' % format))\n    if (isinstance(filename_or_obj, basestring) and filename_or_obj.startswith('CDF')):\n        filename_or_obj = BytesIO(filename_or_obj)\n    self.ds = scipy.io.netcdf_file(filename_or_obj, mode=mode, mmap=mmap, version=version)\n    super(ScipyDataStore, self).__init__(writer)\n", "label": 1}
{"function": "\n\ndef match_data(self, selector, data):\n    for (node, body) in self._iter_data(data):\n        match = self.match_node(selector, node)\n        if match:\n            next_selector = selector.next_selector\n            if next_selector:\n                if body:\n                    for node in self.match_data(next_selector, body):\n                        (yield node)\n            else:\n                (yield node)\n        if (body and (not (selector.combinator == self.selector_class.CHILD))):\n            for node in self.match_data(selector, body):\n                (yield node)\n", "label": 1}
{"function": "\n\ndef set_status(view):\n    infos = []\n    exts = ['ino', 'pde', 'cpp', 'c', '.S']\n    file_name = view.file_name()\n    if (file_name and (file_name.split('.')[(- 1)] in exts)):\n        arduino_info = st_base.get_arduino_info()\n        version_name = arduino_info.get_ide_dir().get_version_name()\n        version_text = ('Arduino %s' % version_name)\n        infos.append(version_text)\n        target_board_info = arduino_info.get_target_board_info()\n        target_board = target_board_info.get_target_board()\n        if target_board:\n            target_board_caption = target_board.get_caption()\n            infos.append(target_board_caption)\n            if target_board.has_options():\n                target_sub_boards = target_board_info.get_target_sub_boards()\n                for (index, target_sub_board) in enumerate(target_sub_boards):\n                    caption_text = target_sub_board.get_caption()\n                    if (index == 0):\n                        caption_text = ('[' + caption_text)\n                    if (index == (len(target_sub_boards) - 1)):\n                        caption_text += ']'\n                    infos.append(caption_text)\n        else:\n            target_board_caption = 'No board'\n            infos.append(target_board_caption)\n        settings = st_base.get_settings()\n        target_serial_port = settings.get('serial_port', 'No serial port')\n        serial_ports = pyarduino.base.serial_port.list_serial_ports()\n        if (not (target_serial_port in serial_ports)):\n            target_serial_port = 'No serial port'\n        serial_text = ('on %s' % target_serial_port)\n        infos.append(serial_text)\n        text = ', '.join(infos)\n        view.set_status('Arduino', text)\n", "label": 1}
{"function": "\n\ndef create_trie(self):\n    'Create a trie of source root patterns from options.'\n    options = self.get_options()\n    trie = SourceRootTrie(options.lang_canonicalizations)\n    for pattern in (options.source_root_patterns or []):\n        trie.add_pattern(pattern)\n    for pattern in (options.test_root_patterns or []):\n        trie.add_pattern(pattern)\n    for (path, langs) in (options.source_roots or {\n        \n    }).items():\n        trie.add_fixed(path, langs)\n    for (path, langs) in (options.test_roots or {\n        \n    }).items():\n        trie.add_fixed(path, langs)\n    return trie\n", "label": 1}
{"function": "\n\ndef process(self, challenge=None):\n    '\\n        '\n    if (challenge is None):\n        if self.has_values(['username', 'realm', 'nonce', 'key_hash', 'nc', 'cnonce', 'qops']):\n            self._qops = self.values['qops']\n            return self.response()\n        else:\n            return None\n    d = parse_challenge(challenge)\n    if (b'rspauth' in d):\n        self.mutual_auth(d[b'rspauth'])\n    else:\n        if (b'realm' not in d):\n            d[b'realm'] = self.sasl.def_realm\n        for key in ['nonce', 'realm']:\n            if (bytes(key) in d):\n                self.values[key] = d[bytes(key)]\n        self.values['nc'] = 0\n        self._qops = [b'auth']\n        if (b'qop' in d):\n            self._qops = [x.strip() for x in d[b'qop'].split(b',')]\n        self.values['qops'] = self._qops\n        if (b'maxbuf' in d):\n            self._max_buffer = int(d[b'maxbuf'])\n        return self.response()\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef _create_new_client_event(self, builder):\n    latest_ret = (yield self.store.get_latest_event_ids_and_hashes_in_room(builder.room_id))\n    if latest_ret:\n        depth = (max([d for (_, _, d) in latest_ret]) + 1)\n    else:\n        depth = 1\n    prev_events = [(event_id, prev_hashes) for (event_id, prev_hashes, _) in latest_ret]\n    builder.prev_events = prev_events\n    builder.depth = depth\n    state_handler = self.state_handler\n    context = (yield state_handler.compute_event_context(builder))\n    if ((not self.is_host_in_room(context.current_state)) and (builder.type == EventTypes.Member)):\n        prev_member_event = (yield self.store.get_room_member(builder.sender, builder.room_id))\n        context_event_ids = (e.event_id for e in context.current_state.values())\n        if (prev_member_event and (prev_member_event.event_id not in context_event_ids)):\n            builder.prev_events = (prev_member_event.event_id, prev_member_event.prev_events)\n            context = (yield state_handler.compute_event_context(builder, old_state=(prev_member_event,), outlier=True))\n    if builder.is_state():\n        builder.prev_state = (yield self.store.add_event_hashes(context.prev_state_events))\n    (yield self.auth.add_auth_events(builder, context))\n    add_hashes_and_signatures(builder, self.server_name, self.signing_key)\n    event = builder.build()\n    logger.debug('Created event %s with current state: %s', event.event_id, context.current_state)\n    defer.returnValue((event, context))\n", "label": 1}
{"function": "\n\ndef get_progressbar(self, label=None, maxval=None):\n    if self.args.get('show_porcelain'):\n        return _MachineReadableCounter(label=label, maxval=maxval)\n    elif (('progressbar' in sys.modules) and self.args.get('show_progress', False)):\n        widgets = []\n        if (label is not None):\n            widgets += [label, ' ']\n        if (maxval is not None):\n            widgets += [progressbar.Percentage(), ' ', progressbar.Bar(marker='='), ' ', _FileSize(), ' ', progressbar.FileTransferSpeed(), ' ']\n            if ('AdaptiveETA' in dir(progressbar)):\n                widgets.append(progressbar.AdaptiveETA())\n            else:\n                widgets.append(progressbar.ETA())\n            pbar = progressbar.ProgressBar(widgets=widgets, maxval=(maxval or sys.maxint), poll=0.05)\n            signal.siginterrupt(signal.SIGWINCH, False)\n            return pbar\n        else:\n            widgets += [_IndeterminateBouncingBar(marker='='), ' ', _FileSize(), ' ', progressbar.FileTransferSpeed(), ' ', progressbar.Timer(format='Time: %s')]\n            pbar = _IndeterminateProgressBar(widgets=widgets, maxval=(maxval or sys.maxint), poll=0.05)\n            signal.siginterrupt(signal.SIGWINCH, False)\n            return pbar\n    else:\n        return _EveryMethodObject()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.BOOL):\n                self.include_hadoop = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _plot_errorbars_trainset(self, canvas, which_data_rows='all', which_data_ycols='all', fixed_inputs=None, plot_raw=False, apply_link=False, label=None, projection='2d', predict_kw=None, **plot_kwargs):\n    ycols = get_which_data_ycols(self, which_data_ycols)\n    rows = get_which_data_rows(self, which_data_rows)\n    (X, _, Y) = get_x_y_var(self)\n    if (fixed_inputs is None):\n        fixed_inputs = []\n    free_dims = get_free_dims(self, None, fixed_inputs)\n    Xgrid = X.copy()\n    for (i, v) in fixed_inputs:\n        Xgrid[:, i] = v\n    plots = []\n    if ((len(free_dims) <= 2) and (projection == '2d')):\n        update_not_existing_kwargs(plot_kwargs, pl().defaults.yerrorbar)\n        if (predict_kw is None):\n            predict_kw = {\n                \n            }\n        if ('Y_metadata' not in predict_kw):\n            predict_kw['Y_metadata'] = (self.Y_metadata or {\n                \n            })\n        (mu, percs, _) = helper_predict_with_model(self, Xgrid, plot_raw, apply_link, (2.5, 97.5), ycols, predict_kw)\n        if (len(free_dims) == 1):\n            for d in ycols:\n                plots.append(pl().yerrorbar(canvas, X[(rows, free_dims[0])], mu[(rows, d)], np.vstack([(mu[(rows, d)] - percs[0][(rows, d)]), (percs[1][(rows, d)] - mu[(rows, d)])]), label=label, **plot_kwargs))\n    else:\n        raise NotImplementedError('Cannot plot in more then one dimensions, or 3d')\n    return dict(yerrorbars=plots)\n", "label": 1}
{"function": "\n\ndef op_max_until(self, ctx):\n    repeat = ctx.state.repeat\n    if (repeat is None):\n        raise RuntimeError('Internal re error: MAX_UNTIL without REPEAT.')\n    mincount = repeat.peek_code(2)\n    maxcount = repeat.peek_code(3)\n    ctx.state.string_position = ctx.string_position\n    count = (repeat.count + 1)\n    if (count < mincount):\n        repeat.count = count\n        child_context = repeat.push_new_context(4)\n        (yield False)\n        ctx.has_matched = child_context.has_matched\n        if (not ctx.has_matched):\n            repeat.count = (count - 1)\n            ctx.state.string_position = ctx.string_position\n        (yield True)\n    if (((count < maxcount) or (maxcount == MAXREPEAT)) and (ctx.state.string_position != repeat.last_position)):\n        repeat.count = count\n        ctx.state.marks_push()\n        save_last_position = repeat.last_position\n        repeat.last_position = ctx.state.string_position\n        child_context = repeat.push_new_context(4)\n        (yield False)\n        repeat.last_position = save_last_position\n        if child_context.has_matched:\n            ctx.state.marks_pop_discard()\n            ctx.has_matched = True\n            (yield True)\n        ctx.state.marks_pop()\n        repeat.count = (count - 1)\n        ctx.state.string_position = ctx.string_position\n    ctx.state.repeat = repeat.previous\n    child_context = ctx.push_new_context(1)\n    (yield False)\n    ctx.has_matched = child_context.has_matched\n    if (not ctx.has_matched):\n        ctx.state.repeat = repeat\n        ctx.state.string_position = ctx.string_position\n    (yield True)\n", "label": 1}
{"function": "\n\ndef extra_l33t_entropy(match):\n    if (('l33t' not in match) or (not match['l33t'])):\n        return 0\n    possibilities = 0\n    for (subbed, unsubbed) in match['sub'].items():\n        sub_len = len([x for x in match['token'] if (x == subbed)])\n        unsub_len = len([x for x in match['token'] if (x == unsubbed)])\n        possibilities += sum((binom((unsub_len + sub_len), i) for i in range(0, (min(unsub_len, sub_len) + 1))))\n    if (possibilities <= 1):\n        return 1\n    return lg(possibilities)\n", "label": 1}
{"function": "\n\n@require_POST\n@login_required\n@transaction.atomic\ndef modify(request, action, id):\n    '\\n    Attempt to modify the specified article.\\n    '\n    article = get_object_or_404(Article, pk=id)\n    if (action == 'submit'):\n        if ((article.author != request.user) or (article.status != Article.DRAFT)):\n            raise Http404\n        article.status = Article.UNAPPROVED\n        article.save()\n        template = render_to_string('articles/emails/submit.txt', {\n            'article': request.build_absolute_uri(article.get_absolute_url()),\n        })\n        mail_admins('2buntu Article Submitted', template)\n        messages.info(request, 'The article has been submitted for approval by a staff member.')\n    elif (action == 'publish'):\n        if (not request.user.is_staff):\n            raise Http404\n        article.status = Article.PUBLISHED\n        article.save()\n        messages.info(request, 'The article has been published.')\n    elif (action == 'release'):\n        if ((article.author != request.user) or article.cc_license):\n            raise Http404\n        article.cc_license = True\n        article.save()\n        messages.info(request, 'The article is now available under a CC BY-SA 4.0 license.')\n    return redirect(article)\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    output = StringIO()\n    response = []\n\n    def writer_start_response(status, headers, exc_info=None):\n        response.extend((status, headers))\n        start_response(status, headers, exc_info)\n        return output.write\n    app_iter = self.app(environ, writer_start_response)\n    try:\n        for s in app_iter:\n            output.write(s)\n    finally:\n        if hasattr(app_iter, 'close'):\n            app_iter.close()\n    page = output.getvalue()\n    (status, headers) = response\n    v = (header_value(headers, 'content-type') or '')\n    if ((not v.startswith('text/html')) and (not v.startswith('text/xhtml')) and (not v.startswith('application/xhtml'))):\n        return [page]\n    ops = []\n    if v.startswith('text/xhtml+xml'):\n        ops.append('--xml')\n    html_errors = self.call_wdg_validate(self.wdg_path, ops, page)\n    if html_errors:\n        page = self.add_error(page, html_errors)[0]\n        headers.remove(('Content-Length', str(header_value(headers, 'content-length'))))\n        headers.append(('Content-Length', str(len(page))))\n    return [page]\n", "label": 1}
{"function": "\n\ndef encode_model(obj, recursive=False):\n    if (obj is None):\n        return obj\n    import bson\n    import datetime\n    if isinstance(obj, (int, float, basestring)):\n        out = obj\n    elif isinstance(obj, list):\n        out = [encode_model(item) for item in obj]\n    elif isinstance(obj, dict):\n        out = dict([(k, encode_model(v)) for (k, v) in obj.items()])\n    elif isinstance(obj, (datetime.datetime, datetime.timedelta)):\n        out = str(obj)\n    elif isinstance(obj, bson.objectid.ObjectId):\n        out = {\n            'ObjectId': str(obj),\n        }\n    else:\n        raise NameError((\"Could not JSON-encode type '%s': %s\" % (type(obj), str(obj))))\n    return out\n", "label": 1}
{"function": "\n\ndef permutations(iterable, r=None):\n    pool = tuple(iterable)\n    n = len(pool)\n    r = (n if (r is None) else r)\n    if (r > n):\n        return\n    indices = range(n)\n    cycles = range(n, (n - r), (- 1))\n    (yield tuple((pool[i] for i in indices[:r])))\n    while n:\n        for i in reversed(range(r)):\n            cycles[i] -= 1\n            if (cycles[i] == 0):\n                indices[i:] = (indices[(i + 1):] + indices[i:(i + 1)])\n                cycles[i] = (n - i)\n            else:\n                j = cycles[i]\n                (indices[i], indices[(- j)]) = (indices[(- j)], indices[i])\n                (yield tuple((pool[i] for i in indices[:r])))\n                break\n        else:\n            return\n", "label": 1}
{"function": "\n\ndef status_notifciation(changed_status):\n    notifications = {\n        \n    }\n    for (category, services) in changed_status.iteritems():\n        for (name, st) in services.iteritems():\n            if (name == 'GameServer'):\n                notifications[category] = st\n    for (category, st) in notifications.iteritems():\n        status = ('Available' if st else 'Unavailable')\n        offset = 0\n        limit = 200\n        while True:\n            subscribers = load_model('subscribers').get_subscribers(limit, offset)\n            if (not subscribers):\n                break\n            for subscribe in subscribers:\n                if (category in subscribe.categorys):\n                    alert = _trans_alert('Diablo3 %s server status has changed to %s', category, status, subscribe.locale)\n                    apns_tasks.apns_push_task.delay(subscribe.token, {\n                        \n                    }, alert=alert, badge=1, sound='default')\n            offset += len(subscribers)\n", "label": 1}
{"function": "\n\ndef __init__(self, editor):\n    ' Initialise the object.\\n        '\n    QtGui.QTableView.__init__(self)\n    self._initial_size = False\n    self._editor = editor\n    self.setModel(editor.model)\n    factory = editor.factory\n    vheader = self.verticalHeader()\n    if factory.show_row_titles:\n        vheader.setHighlightSections(False)\n    else:\n        vheader.hide()\n    if (factory.show_row_titles and factory.auto_resize_rows):\n        vheader.setResizeMode(QtGui.QHeaderView.ResizeToContents)\n    else:\n        size = vheader.minimumSectionSize()\n        font = editor.adapter.get_font(editor.object, editor.name, 0)\n        if (font is not None):\n            size = max(size, QtGui.QFontMetrics(QtGui.QFont(font)).height())\n        vheader.setDefaultSectionSize(size)\n    hheader = self.horizontalHeader()\n    hheader.setStretchLastSection(factory.stretch_last_section)\n    if factory.show_titles:\n        hheader.setHighlightSections(False)\n    else:\n        hheader.hide()\n    self.setShowGrid(False)\n    self.setItemDelegate(_ItemDelegate(self))\n    self.setSelectionBehavior(QtGui.QAbstractItemView.SelectRows)\n    if factory.multi_select:\n        mode = QtGui.QAbstractItemView.ExtendedSelection\n    else:\n        mode = QtGui.QAbstractItemView.SingleSelection\n    self.setSelectionMode(mode)\n    self.setDragEnabled(True)\n    if factory.editable:\n        self.viewport().setAcceptDrops(True)\n    if factory.drag_move:\n        self.setDragDropMode(QtGui.QAbstractItemView.InternalMove)\n    self.setDropIndicatorShown(True)\n", "label": 1}
{"function": "\n\ndef add_vxlan(self, name, vni, group=None, dev=None, ttl=None, tos=None, local=None, port=None, proxy=False):\n    cmd = ['add', name, 'type', 'vxlan', 'id', vni]\n    if group:\n        cmd.extend(['group', group])\n    if dev:\n        cmd.extend(['dev', dev])\n    if ttl:\n        cmd.extend(['ttl', ttl])\n    if tos:\n        cmd.extend(['tos', tos])\n    if local:\n        cmd.extend(['local', local])\n    if proxy:\n        cmd.append('proxy')\n    if (port and (len(port) == 2)):\n        cmd.extend(['port', port[0], port[1]])\n    elif port:\n        raise n_exc.NetworkVxlanPortRangeError(vxlan_range=port)\n    self._as_root([], 'link', cmd)\n    return IPDevice(name, namespace=self.namespace)\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TAlterSentryRoleAddGroupsRequest')\n    if (self.protocol_version is not None):\n        oprot.writeFieldBegin('protocol_version', TType.I32, 1)\n        oprot.writeI32(self.protocol_version)\n        oprot.writeFieldEnd()\n    if (self.requestorUserName is not None):\n        oprot.writeFieldBegin('requestorUserName', TType.STRING, 2)\n        oprot.writeString(self.requestorUserName)\n        oprot.writeFieldEnd()\n    if (self.roleName is not None):\n        oprot.writeFieldBegin('roleName', TType.STRING, 3)\n        oprot.writeString(self.roleName)\n        oprot.writeFieldEnd()\n    if (self.component is not None):\n        oprot.writeFieldBegin('component', TType.STRING, 4)\n        oprot.writeString(self.component)\n        oprot.writeFieldEnd()\n    if (self.groups is not None):\n        oprot.writeFieldBegin('groups', TType.SET, 5)\n        oprot.writeSetBegin(TType.STRING, len(self.groups))\n        for iter13 in self.groups:\n            oprot.writeString(iter13)\n        oprot.writeSetEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef _is_all_zero_bit(self, type_, val):\n    if ((type_ == 'int') or (type_ == 'ipv4')):\n        return (val == 0)\n    elif (type_ == 'mac'):\n        for v in val:\n            if (v != b'\\x00'):\n                return False\n        return True\n    elif (type_ == 'ipv6'):\n        for v in val:\n            if (v != 0):\n                return False\n        return True\n    else:\n        raise Exception('Unknown type')\n", "label": 1}
{"function": "\n\ndef describe_xml(self):\n    input_elements = [i.describe_xml() for i in self.inputs]\n    output_elements = [i.describe_xml() for i in self.outputs]\n    doc = E.ProcessDescription(OWS.Identifier(self.identifier), OWS.Title(self.title))\n    doc.attrib['{http://www.opengis.net/wps/1.0.0}processVersion'] = self.version\n    if (self.store_supported == 'true'):\n        doc.attrib['storeSupported'] = self.store_supported\n    if (self.status_supported == 'true'):\n        doc.attrib['statusSupported'] = self.status_supported\n    if self.abstract:\n        doc.append(OWS.Abstract(self.abstract))\n    for m in self.metadata:\n        doc.append(OWS.Metadata({\n            '{http://www.w3.org/1999/xlink}title': m,\n        }))\n    for p in self.profile:\n        doc.append(WPS.Profile(p))\n    if input_elements:\n        doc.append(E.DataInputs(*input_elements))\n    doc.append(E.ProcessOutputs(*output_elements))\n    return doc\n", "label": 1}
{"function": "\n\ndef _match_long_opt(self, opt, explicit_value, state):\n    if (opt not in self._long_opt):\n        possibilities = [word for word in self._long_opt if word.startswith(opt)]\n        raise NoSuchOption(opt, possibilities=possibilities)\n    option = self._long_opt[opt]\n    if option.takes_value:\n        if (explicit_value is not None):\n            state.rargs.insert(0, explicit_value)\n        nargs = option.nargs\n        if (len(state.rargs) < nargs):\n            _error_opt_args(nargs, opt)\n        elif (nargs == 1):\n            value = state.rargs.pop(0)\n        else:\n            value = tuple(state.rargs[:nargs])\n            del state.rargs[:nargs]\n    elif (explicit_value is not None):\n        raise BadOptionUsage(opt, ('%s option does not take a value' % opt))\n    else:\n        value = None\n    option.process(value, state)\n", "label": 1}
{"function": "\n\ndef migrate_docker_facts(facts):\n    ' Apply migrations for docker facts '\n    params = {\n        'common': ('additional_registries', 'insecure_registries', 'blocked_registries', 'options'),\n        'node': ('log_driver', 'log_options'),\n    }\n    if ('docker' not in facts):\n        facts['docker'] = {\n            \n        }\n    for role in params.keys():\n        if (role in facts):\n            for param in params[role]:\n                old_param = ('docker_' + param)\n                if (old_param in facts[role]):\n                    facts['docker'][param] = facts[role].pop(old_param)\n    if (('node' in facts) and ('portal_net' in facts['node'])):\n        facts['docker']['hosted_registry_insecure'] = True\n        facts['docker']['hosted_registry_network'] = facts['node'].pop('portal_net')\n    if (('log_options' in facts['docker']) and isinstance(facts['docker']['log_options'], basestring)):\n        facts['docker']['log_options'] = facts['docker']['log_options'].split(',')\n    return facts\n", "label": 1}
{"function": "\n\ndef _ensure_inventory_uptodate(inventory, container_skel):\n    'Update inventory if needed.\\n\\n    Inspect the current inventory and ensure that all host items have all of\\n    the required entries.\\n\\n    :param inventory: ``dict`` Living inventory of containers and hosts\\n    '\n    for (key, value) in inventory['_meta']['hostvars'].iteritems():\n        if ('container_name' not in value):\n            value['container_name'] = key\n        for rh in REQUIRED_HOSTVARS:\n            if (rh not in value):\n                value[rh] = None\n                if (rh == 'container_networks'):\n                    value[rh] = {\n                        \n                    }\n    for (key, value) in container_skel.iteritems():\n        item = inventory.get(key)\n        hosts = item.get('hosts')\n        if hosts:\n            for host in hosts:\n                container = inventory['_meta']['hostvars'][host]\n                if ('properties' in value):\n                    container['properties'] = value['properties']\n", "label": 1}
{"function": "\n\n@reflection.cache\ndef get_foreign_keys(self, connection, table_name, schema=None, **kw):\n    parsed_state = self._parsed_state_or_create(connection, table_name, schema, **kw)\n    default_schema = None\n    fkeys = []\n    for spec in parsed_state.constraints:\n        ref_name = spec['table'][(- 1)]\n        ref_schema = (((len(spec['table']) > 1) and spec['table'][(- 2)]) or schema)\n        if (not ref_schema):\n            if (default_schema is None):\n                default_schema = connection.dialect.default_schema_name\n            if (schema == default_schema):\n                ref_schema = schema\n        loc_names = spec['local']\n        ref_names = spec['foreign']\n        con_kw = {\n            \n        }\n        for opt in ('onupdate', 'ondelete'):\n            if spec.get(opt, False):\n                con_kw[opt] = spec[opt]\n        fkey_d = {\n            'name': spec['name'],\n            'constrained_columns': loc_names,\n            'referred_schema': ref_schema,\n            'referred_table': ref_name,\n            'referred_columns': ref_names,\n            'options': con_kw,\n        }\n        fkeys.append(fkey_d)\n    return fkeys\n", "label": 1}
{"function": "\n\ndef walker(self, path=None, base_folder=None):\n    '\\n        This method walk a directory structure and create the\\n        Folders and Files as they appear.\\n        '\n    path = (path or self.path)\n    base_folder = (base_folder or self.base_folder)\n    path = os.path.normpath(upath(path))\n    if base_folder:\n        base_folder = os.path.normpath(upath(base_folder))\n        print(('The directory structure will be imported in %s' % (base_folder,)))\n    if (self.verbosity >= 1):\n        print(('Import the folders and files in %s' % (path,)))\n    root_folder_name = os.path.basename(path)\n    for (root, dirs, files) in os.walk(path):\n        rel_folders = root.partition(path)[2].strip(os.path.sep).split(os.path.sep)\n        while ('' in rel_folders):\n            rel_folders.remove('')\n        if base_folder:\n            folder_names = ((base_folder.split('/') + [root_folder_name]) + rel_folders)\n        else:\n            folder_names = ([root_folder_name] + rel_folders)\n        folder = self.get_or_create_folder(folder_names)\n        for file_obj in files:\n            dj_file = DjangoFile(open(os.path.join(root, file_obj), mode='rb'), name=file_obj)\n            self.import_file(file_obj=dj_file, folder=folder)\n    if (self.verbosity >= 1):\n        print((('folder_created #%s / file_created #%s / ' + 'image_created #%s') % (self.folder_created, self.file_created, self.image_created)))\n", "label": 1}
{"function": "\n\ndef fit(self, X, y, report_interval=1000):\n    if ((X is None) or (y is None)):\n        raise ValueError('X and/or y is None.')\n    xp = (np if (self.gpu < 0) else cuda.cupy)\n    data_size = len(X)\n    jump = (data_size // self.batch_size)\n    cur_log_perp = xp.zeros(())\n    start_at = time.time()\n    cur_at = start_at\n    accum_loss = 0\n    batch_idxs = list(range(self.batch_size))\n    self.model.predictor.reset_state()\n    self.model.zerograds()\n    for i in six.moves.range((jump * self.n_epoch)):\n        x = Variable(xp.asarray([X[(((jump * j) + i) % data_size)] for j in batch_idxs]))\n        t = Variable(xp.asarray([y[(((jump * j) + i) % data_size)] for j in batch_idxs]))\n        loss_i = self.model(x, t)\n        accum_loss += loss_i\n        cur_log_perp += loss_i.data\n        if (((i + 1) % self.bprop_len) == 0):\n            accum_loss.backward()\n            accum_loss.unchain_backward()\n            accum_loss = 0\n            self.optimizer.update()\n            self.model.predictor.reset_state()\n            self.model.zerograds()\n        if (((i + 1) % report_interval) == 0):\n            now = time.time()\n            throuput = (float(report_interval) / (now - cur_at))\n            perp = math.exp((float(cur_log_perp) / report_interval))\n            logger.info('iter {}/{} training perplexity: {:.2f} ({:.2f} iters/sec)'.format((i + 1), (jump * self.n_epoch), perp, throuput))\n            cur_at = now\n            cur_log_perp.fill(0)\n", "label": 1}
{"function": "\n\ndef skip(app, what, name, obj, skip, options):\n    if skip:\n        return True\n    if (name.startswith('_') and (name != '__init__')):\n        return True\n    if name.startswith('on_data'):\n        return True\n    if name.startswith('on_raw_'):\n        return True\n    if (name.startswith('on_ctcp') and (name not in ('on_ctcp', 'on_ctcp_reply'))):\n        return True\n    if name.startswith('on_isupport_'):\n        return True\n    if name.startswith('on_capability_'):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef set(self, key, val, to_global=False):\n    '\\n        Set a value to specified key in dictionary\\n\\n        @param key: a key to set value\\n        @param val: a value to be set, if provided as not None\\n            otherwise key will be deleted instead\\n        @param to_global: a boolean specified whether set the value to\\n            global dictionary or local dictionary\\n        '\n    if (val is None):\n        if self.has(key, in_global=False, in_local=True):\n            del self.local_dict[key]\n            self.local_change = True\n        elif self.has(key, in_global=True, in_local=False):\n            del self.global_dict[key]\n            self.global_change = True\n    elif to_global:\n        if ((key not in self.global_dict) or (self.global_dict[key] != val)):\n            self.global_dict[key] = val\n            self.global_change = True\n    elif ((key not in self.local_dict) or (self.local_dict[key] != val)):\n        self.local_dict[key] = val\n        self.local_change = True\n", "label": 1}
{"function": "\n\ndef _fontsize(self, key, label='fontsize', common=True):\n    if (not self.fontsize):\n        return {\n            \n        }\n    if (not isinstance(self.fontsize, dict)):\n        return ({\n            label: self.fontsize,\n        } if common else {\n            \n        })\n    unknown_keys = (set(self.fontsize.keys()) - set(self._fontsize_keys))\n    if unknown_keys:\n        msg = 'Popping unknown keys %r from fontsize dictionary.\\nValid keys: %r'\n        self.warning((msg % (list(unknown_keys), self._fontsize_keys)))\n        for key in unknown_keys:\n            self.fontsize.pop(key, None)\n    if (key in self.fontsize):\n        return {\n            label: self.fontsize[key],\n        }\n    elif ((key in ['ylabel', 'xlabel']) and ('labels' in self.fontsize)):\n        return {\n            label: self.fontsize['labels'],\n        }\n    else:\n        return {\n            \n        }\n", "label": 1}
{"function": "\n\ndef _get_offsets(self, node):\n    '\\n        @type node: C{PyNode}\\n        @param node: a node in the Graf graph\\n        @return: the offsets contained by a given node\\n        @rtype: C{pair} of C{int}, or C{None}\\n        '\n    if ((len(node._links) == 0) and (node._outEdgeList != [])):\n        offsets = []\n        edge_list = node._outEdgeList\n        edge_list.reverse()\n        for edge in edge_list:\n            temp_offsets = self._get_offsets(edge._toNode)\n            if (temp_offsets is not None):\n                offsets.extend(self._get_offsets(edge._toNode))\n        if (len(offsets) == 0):\n            return None\n        offsets.sort()\n        start = offsets[0]\n        end = offsets[(len(offsets) - 1)]\n        return [start, end]\n    elif (len(node._links) != 0):\n        offsets = []\n        for link in node._links:\n            for region in link._regions:\n                for anchor in region._anchors:\n                    offsets.append(int(anchor._offset))\n        offsets.sort()\n        start = offsets[0]\n        end = offsets[(len(offsets) - 1)]\n        return [start, end]\n    else:\n        return None\n", "label": 1}
{"function": "\n\n@classmethod\ndef find_one(cls, query=None, deleted=False, **kwargs):\n    from website.models import Node\n    if (query and getattr(query, 'nodes', False)):\n        for node in query.nodes:\n            replacement_attr = cls.attribute_map.get(node.attribute, False)\n            node.attribute = (replacement_attr if replacement_attr else node.attribute)\n    elif isinstance(query, RawQuery):\n        replacement_attr = cls.attribute_map.get(query.attribute, False)\n        query.attribute = (replacement_attr if replacement_attr else query.attribute)\n    query = ((query & Q('institution_id', 'ne', None)) if query else Q('institution_id', 'ne', None))\n    query = ((query & Q('is_deleted', 'ne', True)) if (not deleted) else query)\n    node = Node.find_one(query, allow_institution=True, **kwargs)\n    return cls(node)\n", "label": 1}
{"function": "\n\ndef embedly(url, max_width=None, key=None):\n    from embedly import Embedly\n    if (key is None):\n        key = settings.WAGTAILEMBEDS_EMBEDLY_KEY\n    client = Embedly(key=key)\n    if (max_width is not None):\n        oembed = client.oembed(url, maxwidth=max_width, better=False)\n    else:\n        oembed = client.oembed(url, better=False)\n    if oembed.get('error'):\n        if (oembed['error_code'] in [401, 403]):\n            raise AccessDeniedEmbedlyException\n        elif (oembed['error_code'] == 404):\n            raise EmbedNotFoundException\n        else:\n            raise EmbedlyException\n    if (oembed['type'] == 'photo'):\n        html = ('<img src=\"%s\" />' % (oembed['url'],))\n    else:\n        html = oembed.get('html')\n    return {\n        'title': (oembed['title'] if ('title' in oembed) else ''),\n        'author_name': (oembed['author_name'] if ('author_name' in oembed) else ''),\n        'provider_name': (oembed['provider_name'] if ('provider_name' in oembed) else ''),\n        'type': oembed['type'],\n        'thumbnail_url': oembed.get('thumbnail_url'),\n        'width': oembed.get('width'),\n        'height': oembed.get('height'),\n        'html': html,\n    }\n", "label": 1}
{"function": "\n\n@cache_readonly\ndef _var_beta_raw(self):\n    'Returns the raw covariance of beta.'\n    x = self._x\n    y = self._y\n    dates = x.index.levels[0]\n    cluster_axis = None\n    if (self._cluster == 'time'):\n        cluster_axis = 0\n    elif (self._cluster == 'entity'):\n        cluster_axis = 1\n    nobs = self._nobs\n    rmse = self._rmse_raw\n    beta = self._beta_raw\n    df = self._df_raw\n    window = self._window\n    if (not self._time_effects):\n        cum_xx = self._cum_xx(x)\n    results = []\n    for (n, i) in enumerate(self._valid_indices):\n        if (self._is_rolling and (i >= window)):\n            prior_date = dates[((i - window) + 1)]\n        else:\n            prior_date = dates[0]\n        date = dates[i]\n        x_slice = x.truncate(prior_date, date)\n        y_slice = y.truncate(prior_date, date)\n        if self._time_effects:\n            xx = _xx_time_effects(x_slice, y_slice)\n        else:\n            xx = cum_xx[i]\n            if (self._is_rolling and (i >= window)):\n                xx = (xx - cum_xx[(i - window)])\n        result = _var_beta_panel(y_slice, x_slice, beta[n], xx, rmse[n], cluster_axis, self._nw_lags, nobs[n], df[n], self._nw_overlap)\n        results.append(result)\n    return np.array(results)\n", "label": 1}
{"function": "\n\ndef generalized_active_forces(partials, forces, uaux=None):\n    ulist = partials.ulist\n    if (uaux is not None):\n        uaux_zero = dict(zip(uaux, ([0] * len(uaux))))\n    Fr = ([0] * len(ulist))\n    for pf in forces:\n        p = pf[0]\n        f = pf[1]\n        for (i, u) in enumerate(ulist):\n            if ((partials[p][u] != 0) and (f != 0)):\n                r = dot(partials[p][u], f)\n                if (len(pf) > 2):\n                    r = pf[2](r)\n                if ((uaux is not None) and (u not in uaux)):\n                    r = subs(r, uaux_zero)\n                Fr[i] += r\n    return (Fr, ulist)\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    if (not self.capture):\n        raise RuntimeError('Not capturing')\n    if (os.name == 'nt'):\n        q = Queue.Queue()\n\n        def reader(stream):\n            while 1:\n                line = stream.readline()\n                q.put(line)\n                if (not line):\n                    break\n        t1 = threading.Thread(target=reader, args=(self._cmd.stdout,))\n        t1.setDaemon(True)\n        t2 = threading.Thread(target=reader, args=(self._cmd.stderr,))\n        t2.setDaemon(True)\n        t1.start()\n        t2.start()\n        outstanding = 2\n        while outstanding:\n            item = q.get()\n            if (not item):\n                outstanding -= 1\n            else:\n                (yield item.rstrip().decode('utf-8', 'replace'))\n    else:\n        streams = [self._cmd.stdout, self._cmd.stderr]\n        while streams:\n            for l in select.select(streams, [], streams):\n                for stream in l:\n                    line = stream.readline()\n                    if (not line):\n                        if (stream in streams):\n                            streams.remove(stream)\n                        break\n                    (yield line.rstrip().decode('utf-8', 'replace'))\n", "label": 1}
{"function": "\n\ndef _args_for_opt_dest_subset(option_parser, args, dests=None):\n    'See docs for :py:func:`args_for_opt_dest_subset()`. This function allows\\n    us to write a compatibility wrapper for the old API\\n    (:py:func:`parse_and_save_options()`).\\n    '\n    values = deepcopy(option_parser.get_default_values())\n    rargs = [x for x in args]\n    option_parser.rargs = rargs\n    while rargs:\n        arg = rargs[0]\n        if (arg == '--'):\n            del rargs[0]\n            return\n        elif (arg[0:2] == '--'):\n            for item in _process_long_opt(option_parser, rargs, values, dests):\n                (yield item)\n        elif ((arg[:1] == '-') and (len(arg) > 1)):\n            for item in _process_short_opts(option_parser, rargs, values, dests):\n                (yield item)\n        else:\n            del rargs[0]\n", "label": 1}
{"function": "\n\ndef _plot_inducing(self, canvas, visible_dims, projection, label, **plot_kwargs):\n    if (visible_dims is None):\n        sig_dims = self.get_most_significant_input_dimensions()\n        visible_dims = [i for i in sig_dims if (i is not None)]\n    free_dims = get_free_dims(self, visible_dims, None)\n    Z = self.Z.values\n    plots = {\n        \n    }\n    if (len(free_dims) == 1):\n        update_not_existing_kwargs(plot_kwargs, pl().defaults.inducing_1d)\n        plots['inducing'] = pl().plot_axis_lines(canvas, Z[:, free_dims], label=label, **plot_kwargs)\n    elif ((len(free_dims) == 2) and (projection == '3d')):\n        update_not_existing_kwargs(plot_kwargs, pl().defaults.inducing_3d)\n        plots['inducing'] = pl().plot_axis_lines(canvas, Z[:, free_dims], label=label, **plot_kwargs)\n    elif (len(free_dims) == 2):\n        update_not_existing_kwargs(plot_kwargs, pl().defaults.inducing_2d)\n        plots['inducing'] = pl().scatter(canvas, Z[:, free_dims[0]], Z[:, free_dims[1]], label=label, **plot_kwargs)\n    elif (len(free_dims) == 0):\n        pass\n    else:\n        raise NotImplementedError('Cannot plot in more then two dimensions')\n    return plots\n", "label": 1}
{"function": "\n\ndef populate_parser(parser, defaults, funcsig, short_args, lexical_order):\n    'Populate parser according to function signature\\n\\n    Use the parameters accepted by the source function, according to the\\n    functions signature provided, to populating a corresponding command line\\n    argument parser.\\n    '\n    params = funcsig.parameters.values()\n    if lexical_order:\n        params = sorted(params, key=(lambda p: p.name))\n    for param in params:\n        if ((param.kind == param.POSITIONAL_OR_KEYWORD) or (param.kind == param.KEYWORD_ONLY) or (param.kind == param.POSITIONAL_ONLY)):\n            if isinstance(param.default, bool):\n                populate_flag(parser, param, defaults)\n            else:\n                populate_option(parser, param, defaults, short_args)\n        elif (param.kind == param.VAR_POSITIONAL):\n            kwargs = {\n                'nargs': '*',\n            }\n            if (param.annotation is not param.empty):\n                kwargs['help'] = param.annotation\n            parser.add_argument(param.name, **kwargs)\n        elif (param.kind == param.VAR_KEYWORD):\n            msg = 'Variable length keyword arguments not supported'\n            raise ValueError(msg)\n    return parser\n", "label": 1}
{"function": "\n\ndef GetArtifactPathDependencies(self):\n    'Return a set of knowledgebase path dependencies.\\n\\n    Returns:\\n      A set of strings for the required kb objects e.g.\\n      [\"users.appdata\", \"systemroot\"]\\n    '\n    deps = set()\n    for source in self.sources:\n        for (arg, value) in source.attributes.items():\n            paths = []\n            if (arg in ['path', 'query']):\n                paths.append(value)\n            if (arg == 'key_value_pairs'):\n                paths.extend([x['key'] for x in value])\n            if (arg in ['keys', 'paths', 'path_list', 'content_regex_list']):\n                paths.extend(value)\n            for path in paths:\n                for match in artifact_utils.INTERPOLATED_REGEX.finditer(path):\n                    deps.add(match.group()[2:(- 2)])\n    deps.update(self.GetArtifactParserDependencies())\n    return deps\n", "label": 1}
{"function": "\n\ndef on_post_text_command(self, view, cmd, args):\n    vs = ViewState.get(view)\n    cm = CmdUtil(view)\n    if (vs.active_mark and (vs.this_cmd != 'drag_select') and (vs.last_cmd == 'drag_select')):\n        if (cmd != 'context_menu'):\n            cm.toggle_active_mark_mode(False)\n    if (not cmd.startswith('sbp_')):\n        vs.argument_value = 0\n        vs.argument_supplied = False\n        vs.last_cmd = cmd\n    if vs.active_mark:\n        cm.set_cursors(cm.get_regions())\n    if ((cmd in ensure_visible_cmds) and cm.just_one_cursor()):\n        cm.ensure_visible(cm.get_last_cursor())\n", "label": 1}
{"function": "\n\ndef to_terminal(self):\n    'Yield lines to be printed to a terminal.'\n    sum_metrics = collections.defaultdict(int)\n    for (path, mod) in self.results:\n        if ('error' in mod):\n            (yield (path, (mod['error'],), {\n                'error': True,\n            }))\n            continue\n        (yield (path, (), {\n            \n        }))\n        for header in self.headers:\n            value = mod[header.lower().replace(' ', '_')]\n            (yield ('{0}: {1}', (header, value), {\n                'indent': 1,\n            }))\n            sum_metrics[header] += value\n        (loc, comments) = (mod['loc'], mod['comments'])\n        (yield ('- Comment Stats', (), {\n            'indent': 1,\n        }))\n        (yield ('(C % L): {0:.0%}', ((comments / (float(loc) or 1)),), {\n            'indent': 2,\n        }))\n        (yield ('(C % S): {0:.0%}', ((comments / (float(mod['sloc']) or 1)),), {\n            'indent': 2,\n        }))\n        (yield ('(C + M % L): {0:.0%}', (((comments + mod['multi']) / (float(loc) or 1)),), {\n            'indent': 2,\n        }))\n    if self.config.summary:\n        (yield ('** Total **', (), {\n            \n        }))\n        for header in self.headers:\n            (yield ('{0}: {1}', (header, sum_metrics[header]), {\n                'indent': 1,\n            }))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.LIST):\n                self.success = []\n                (_etype24, _size21) = iprot.readListBegin()\n                for _i25 in xrange(_size21):\n                    _elem26 = Archive()\n                    _elem26.read(iprot)\n                    self.success.append(_elem26)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef find_command(cmd, path=None, pathext=None):\n    if (path is None):\n        path = os.environ.get('PATH', []).split(os.pathsep)\n    if isinstance(path, basestring):\n        path = [path]\n    if (pathext is None):\n        pathext = os.environ.get('PATHEXT', '.COM;.EXE;.BAT;.CMD').split(os.pathsep)\n    for ext in pathext:\n        if cmd.endswith(ext):\n            pathext = ['']\n            break\n    for p in path:\n        f = os.path.join(p, cmd)\n        if os.path.isfile(f):\n            return f\n        for ext in pathext:\n            fext = (f + ext)\n            if os.path.isfile(fext):\n                return fext\n    return None\n", "label": 1}
{"function": "\n\ndef parse_from(self):\n    node = nodes.FromImport(lineno=next(self.stream).lineno)\n    node.template = self.parse_expression()\n    self.stream.expect('name:import')\n    node.names = []\n\n    def parse_context():\n        if ((self.stream.current.value in ('with', 'without')) and self.stream.look().test('name:context')):\n            node.with_context = (next(self.stream).value == 'with')\n            self.stream.skip()\n            return True\n        return False\n    while 1:\n        if node.names:\n            self.stream.expect('comma')\n        if (self.stream.current.type == 'name'):\n            if parse_context():\n                break\n            target = self.parse_assign_target(name_only=True)\n            if target.name.startswith('_'):\n                self.fail('names starting with an underline can not be imported', target.lineno, exc=TemplateAssertionError)\n            if self.stream.skip_if('name:as'):\n                alias = self.parse_assign_target(name_only=True)\n                node.names.append((target.name, alias.name))\n            else:\n                node.names.append(target.name)\n            if (parse_context() or (self.stream.current.type != 'comma')):\n                break\n        else:\n            break\n    if (not hasattr(node, 'with_context')):\n        node.with_context = False\n        self.stream.skip_if('comma')\n    return node\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('MetricsContext')\n    if (self.name is not None):\n        oprot.writeFieldBegin('name', TType.STRING, 1)\n        oprot.writeString(self.name)\n        oprot.writeFieldEnd()\n    if (self.isMonitoring is not None):\n        oprot.writeFieldBegin('isMonitoring', TType.BOOL, 2)\n        oprot.writeBool(self.isMonitoring)\n        oprot.writeFieldEnd()\n    if (self.period is not None):\n        oprot.writeFieldBegin('period', TType.I32, 3)\n        oprot.writeI32(self.period)\n        oprot.writeFieldEnd()\n    if (self.records is not None):\n        oprot.writeFieldBegin('records', TType.MAP, 4)\n        oprot.writeMapBegin(TType.STRING, TType.LIST, len(self.records))\n        for (kiter47, viter48) in self.records.items():\n            oprot.writeString(kiter47)\n            oprot.writeListBegin(TType.STRUCT, len(viter48))\n            for iter49 in viter48:\n                iter49.write(oprot)\n            oprot.writeListEnd()\n        oprot.writeMapEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef _configure_io_handler(self, handler):\n    'Register an io-handler at the polling object.'\n    if self.check_events():\n        return\n    if (handler in self._unprepared_handlers):\n        old_fileno = self._unprepared_handlers[handler]\n        prepared = self._prepare_io_handler(handler)\n    else:\n        old_fileno = None\n        prepared = True\n    fileno = handler.fileno()\n    if ((old_fileno is not None) and (fileno != old_fileno)):\n        del self._handlers[old_fileno]\n        self.poll.unregister(old_fileno)\n    if (not prepared):\n        self._unprepared_handlers[handler] = fileno\n    if (not fileno):\n        return\n    self._handlers[fileno] = handler\n    events = 0\n    if handler.is_readable():\n        logger.debug(' {0!r} readable'.format(handler))\n        events |= select.POLLIN\n    if handler.is_writable():\n        logger.debug(' {0!r} writable'.format(handler))\n        events |= select.POLLOUT\n    if events:\n        logger.debug(' registering {0!r} handler fileno {1} for events {2}'.format(handler, fileno, events))\n        self.poll.register(fileno, events)\n", "label": 1}
{"function": "\n\ndef _nmap_octet_target_values(spec):\n    values = set()\n    for element in spec.split(','):\n        if ('-' in element):\n            (left, right) = element.split('-', 1)\n            if (not left):\n                left = 0\n            if (not right):\n                right = 255\n            low = int(left)\n            high = int(right)\n            if (not ((0 <= low <= 255) and (0 <= high <= 255))):\n                raise ValueError(('octet value overflow for spec %s!' % spec))\n            if (low > high):\n                raise ValueError(('left side of hyphen must be <= right %r' % element))\n            for octet in _iter_range(low, (high + 1)):\n                values.add(octet)\n        else:\n            octet = int(element)\n            if (not (0 <= octet <= 255)):\n                raise ValueError(('octet value overflow for spec %s!' % spec))\n            values.add(octet)\n    return sorted(values)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if isinstance(other, Queue):\n        return ((self.name == other.name) and (self.exchange == other.exchange) and (self.routing_key == other.routing_key) and (self.queue_arguments == other.queue_arguments) and (self.binding_arguments == other.binding_arguments) and (self.consumer_arguments == other.consumer_arguments) and (self.durable == other.durable) and (self.exclusive == other.exclusive) and (self.auto_delete == other.auto_delete))\n    return NotImplemented\n", "label": 1}
{"function": "\n\ndef distribute(servers, repos, locations, run=0):\n    placed = False\n    for repo in repos:\n        if (repo not in repo.base_location.repos):\n            repo.base_location.repos.append(repo)\n        if (repo.base_location not in repo.locations):\n            repo.locations.append(repo.base_location)\n    repos = sorted(repos, key=(lambda r: r.local_size), reverse=True)\n    locations = sorted(locations, key=(lambda l: (l.max_size - l.committed_size())), reverse=True)\n    for repo in repos:\n        possible_locations = find_possible_locations(repo, locations)\n        print('Will try to place {repo} in {min_loc} of {pos}'.format(repo=repo, min_loc=((repo.min_locations - len(repo.locations)) if (run == 0) else 1), pos=possible_locations))\n        for placing in (range((repo.min_locations - 1)) if (run == 0) else [0]):\n            if (len(possible_locations) > 0):\n                print('Placing {repo} in {loc}'.format(repo=repo, loc=possible_locations[(- 1)]))\n                placed = True\n                repo.locations.append(possible_locations[(- 1)])\n                possible_locations[(- 1)].repos.append(repo)\n                possible_locations = possible_locations[:(- 1)]\n            elif (run == 0):\n                print('wine!')\n                break\n    return placed\n", "label": 1}
{"function": "\n\ndef format_help(self, formatter=None):\n    out = super(SubcommandsOptionParser, self).format_help(formatter)\n    if (formatter is None):\n        formatter = self.formatter\n    result = ['\\n']\n    result.append(formatter.format_heading('Commands'))\n    formatter.indent()\n    disp_names = []\n    help_position = 0\n    subcommands = [c for c in self.subcommands if (not c.hide)]\n    subcommands.sort(key=(lambda c: c.name))\n    for subcommand in subcommands:\n        name = subcommand.name\n        if subcommand.aliases:\n            name += (' (%s)' % ', '.join(subcommand.aliases))\n        disp_names.append(name)\n        proposed_help_position = ((len(name) + formatter.current_indent) + 2)\n        if (proposed_help_position <= formatter.max_help_position):\n            help_position = max(help_position, proposed_help_position)\n    for (subcommand, name) in zip(subcommands, disp_names):\n        name_width = ((help_position - formatter.current_indent) - 2)\n        if (len(name) > name_width):\n            name = ('%*s%s\\n' % (formatter.current_indent, '', name))\n            indent_first = help_position\n        else:\n            name = ('%*s%-*s  ' % (formatter.current_indent, '', name_width, name))\n            indent_first = 0\n        result.append(name)\n        help_width = (formatter.width - help_position)\n        help_lines = textwrap.wrap(subcommand.help, help_width)\n        result.append(('%*s%s\\n' % (indent_first, '', help_lines[0])))\n        result.extend([('%*s%s\\n' % (help_position, '', line)) for line in help_lines[1:]])\n    formatter.dedent()\n    return (out + ''.join(result))\n", "label": 1}
{"function": "\n\ndef _hostRequestCheck(self, host):\n    if (host not in self.pending_reqs):\n        return False\n    if (host in self.last_req):\n        if (host in self.min_req_interval_per_hosts):\n            if ((time.time() - self.last_req[host]) < self.min_req_interval_per_hosts[host]):\n                return False\n        elif ((time.time() - self.last_req[host]) < self.min_req_interval_per_host):\n            return False\n    if (host in self.active_reqs):\n        if (host in self.max_simul_reqs_per_hosts):\n            if (self.active_reqs[host] > self.max_simul_reqs_per_hosts[host]):\n                return False\n        elif (self.active_reqs[host] > self.max_simul_reqs_per_host):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef commit_translations(request):\n    translations = [(KeyValue.objects.get(pk=int(k.split('_')[1])), v) for (k, v) in request.POST.items() if ('translation_' in k)]\n    for (keyvalue, translation) in translations:\n        empty = (('empty_%d' % keyvalue.pk) in request.POST)\n        ignore = (('ignore_%d' % keyvalue.pk) in request.POST)\n        if ((translation != '') or empty or ignore):\n            if (keyvalue.value != translation):\n                if (not ignore):\n                    keyvalue.value = translation\n                keyvalue.fuzzy = False\n            if ignore:\n                keyvalue.fuzzy = False\n            keyvalue.edited = True\n            keyvalue.save()\n", "label": 1}
{"function": "\n\ndef parse_uri(uri_string):\n    'Creates a Uri object which corresponds to the URI string.\\n\\n    This method can accept partial URIs, but it will leave missing\\n    members of the Uri unset.\\n    '\n    parts = urlparse.urlparse(uri_string)\n    uri = Uri()\n    if parts[0]:\n        uri.scheme = parts[0]\n    if parts[1]:\n        host_parts = parts[1].split(':')\n        if host_parts[0]:\n            uri.host = host_parts[0]\n        if (len(host_parts) > 1):\n            uri.port = int(host_parts[1])\n    if parts[2]:\n        uri.path = parts[2]\n    if parts[4]:\n        param_pairs = parts[4].split('&')\n        for pair in param_pairs:\n            pair_parts = pair.split('=')\n            if (len(pair_parts) > 1):\n                uri.query[urllib.unquote_plus(pair_parts[0])] = urllib.unquote_plus(pair_parts[1])\n            elif (len(pair_parts) == 1):\n                uri.query[urllib.unquote_plus(pair_parts[0])] = None\n    return uri\n", "label": 1}
{"function": "\n\ndef adapt_like_to_iterable(self, obj):\n    'Converts collection-compatible objects to an iterable of values.\\n\\n        Can be passed any type of object, and if the underlying collection\\n        determines that it can be adapted into a stream of values it can\\n        use, returns an iterable of values suitable for append()ing.\\n\\n        This method may raise TypeError or any other suitable exception\\n        if adaptation fails.\\n\\n        If a converter implementation is not supplied on the collection,\\n        a default duck-typing-based implementation is used.\\n\\n        '\n    converter = self._data()._sa_converter\n    if (converter is not None):\n        return converter(obj)\n    setting_type = util.duck_type_collection(obj)\n    receiving_type = util.duck_type_collection(self._data())\n    if ((obj is None) or (setting_type != receiving_type)):\n        given = (((obj is None) and 'None') or obj.__class__.__name__)\n        if (receiving_type is None):\n            wanted = self._data().__class__.__name__\n        else:\n            wanted = receiving_type.__name__\n        raise TypeError(('Incompatible collection type: %s is not %s-like' % (given, wanted)))\n    if (getattr(obj, '_sa_adapter', None) is not None):\n        return obj._sa_adapter\n    elif (setting_type == dict):\n        if util.py3k:\n            return obj.values()\n        else:\n            return getattr(obj, 'itervalues', obj.values)()\n    else:\n        return iter(obj)\n", "label": 1}
{"function": "\n\ndef _mapData(self, data, maskPattern):\n    rows = list(range(self.moduleCount))\n    cols = [((col - 1) if (col <= 6) else col) for col in range((self.moduleCount - 1), 0, (- 2))]\n    maskFunc = QRUtil.getMaskFunction(maskPattern)\n    byteIndex = 0\n    bitIndex = 7\n    for col in cols:\n        rows.reverse()\n        for row in rows:\n            for c in range(2):\n                if (self.modules[row][(col - c)] == None):\n                    dark = False\n                    if (byteIndex < len(data)):\n                        dark = (((data[byteIndex] >> bitIndex) & 1) == 1)\n                    if maskFunc(row, (col - c)):\n                        dark = (not dark)\n                    self.modules[row][(col - c)] = dark\n                    bitIndex -= 1\n                    if (bitIndex == (- 1)):\n                        byteIndex += 1\n                        bitIndex = 7\n", "label": 1}
{"function": "\n\ndef update(*args, **kwds):\n    if (len(args) > 2):\n        raise TypeError('update() takes at most 2 positional arguments ({} given)'.format(len(args)))\n    elif (not args):\n        raise TypeError('update() takes at least 1 argument (0 given)')\n    self = args[0]\n    other = (args[1] if (len(args) >= 2) else ())\n    if isinstance(other, Mapping):\n        for key in other:\n            self[key] = other[key]\n    elif hasattr(other, 'keys'):\n        for key in other.keys():\n            self[key] = other[key]\n    else:\n        for (key, value) in other:\n            self[key] = value\n    for (key, value) in kwds.items():\n        self[key] = value\n", "label": 1}
{"function": "\n\ndef is_executable_file(path):\n    'Checks that path is an executable regular file (or a symlink to a file).\\n\\n    This is roughly ``os.path isfile(path) and os.access(path, os.X_OK)``, but\\n    on some platforms :func:`os.access` gives us the wrong answer, so this\\n    checks permission bits directly.\\n    '\n    fpath = os.path.realpath(path)\n    if (not os.path.isfile(fpath)):\n        return False\n    mode = os.stat(fpath).st_mode\n    if ((mode & stat.S_IROTH) and (mode & stat.S_IXOTH)):\n        return True\n    user_gids = (os.getgroups() + [os.getgid()])\n    if ((os.stat(fpath).st_gid in user_gids) and (mode & stat.S_IRGRP) and (mode & stat.S_IXGRP)):\n        return True\n    user_gids = (os.getgroups() + [os.getgid()])\n    if ((os.stat(fpath).st_uid == os.geteuid()) and (mode & stat.S_IRUSR) and (mode & stat.S_IXUSR)):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef geocode(q, api_key):\n    find = make_nsfind({\n        'ns': 'urn:yahoo:maps',\n    })\n    args = {\n        'location': q,\n        'appid': api_key,\n    }\n    url = ('http://local.yahooapis.com/MapsService/V1/geocode?%s' % urllib.urlencode(args))\n    et = ET.parse(urllib.urlopen(url))\n    result = find(et, '//ns:Result')\n    if (not result):\n        return (None, (None, None))\n    else:\n        namebits = {\n            \n        }\n        for field in ('Address', 'City', 'State', 'Zip', 'Country'):\n            bit = find(result, ('ns:%s' % field))\n            if ((bit is not None) and bit.text):\n                namebits[field] = bit.text.decode('utf8')\n        if ('Address' in namebits):\n            name = ('%(Address)s, %(City)s, %(State)s %(Zip)s, %(Country)s' % namebits)\n        elif ('Zip' in namebits):\n            name = ('%(City)s, %(State)s %(Zip)s, %(Country)s' % namebits)\n        elif ('City' in namebits):\n            name = ('%(City)s, %(State)s, %(Country)s' % namebits)\n        elif ('State' in namebits):\n            name = ('%(State)s, %(Country)s' % namebits)\n        elif ('Country' in namebits):\n            name = namebits['Country']\n        else:\n            return (None, (None, None))\n        lat = float(find(result, 'ns:Latitude').text)\n        lon = float(find(result, 'ns:Longitude').text)\n        return (name, (lat, lon))\n", "label": 1}
{"function": "\n\ndef _format_table(self):\n    cell_values = defaultdict(dict)\n    cell_widths = defaultdict(int)\n    for key in self.keys:\n        frame = self._get_frame(key)\n        if (frame is None):\n            continue\n        summarize = (frame.rows > self.max_rows)\n        half_rows = (self.max_rows // 2)\n        rows = min([self.max_rows, frame.rows])\n        for row in range(rows):\n            adjusted_row = row\n            for col in range(frame.cols):\n                if (summarize and (row == half_rows)):\n                    cell_text = '...'\n                else:\n                    if (summarize and (row > half_rows)):\n                        adjusted_row = ((frame.rows - self.max_rows) + row)\n                    value = frame.pprint_cell(adjusted_row, col)\n                    cell_text = self.pprint_value(value)\n                cell_values[key][(row, col)] = cell_text\n                if ((len(cell_text) + 2) > cell_widths[col]):\n                    cell_widths[col] = (len(cell_text) + 2)\n    return (cell_values, cell_widths)\n", "label": 1}
{"function": "\n\ndef _resolve_external_hdfs_urls(self, job_configs):\n    external_hdfs_urls = []\n    for (k, v) in six.iteritems(job_configs.get('configs', {\n        \n    })):\n        if (isinstance(v, six.string_types) and v.startswith('hdfs://')):\n            external_hdfs_urls.append(v)\n    for (k, v) in six.iteritems(job_configs.get('params', {\n        \n    })):\n        if (isinstance(v, six.string_types) and v.startswith('hdfs://')):\n            external_hdfs_urls.append(v)\n    for v in job_configs.get('args', []):\n        if (isinstance(v, six.string_types) and v.startswith('hdfs://')):\n            external_hdfs_urls.append(v)\n    return external_hdfs_urls\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    if isinstance(key, slice):\n        return Header([copy.copy(c) for c in self._cards[key]])\n    elif self._haswildcard(key):\n        return Header([copy.copy(self._cards[idx]) for idx in self._wildcardmatch(key)])\n    elif (isinstance(key, string_types) and (key.upper() in Card._commentary_keywords)):\n        key = key.upper()\n        return _HeaderCommentaryCards(self, key)\n    if isinstance(key, tuple):\n        keyword = key[0]\n    else:\n        keyword = key\n    card = self._cards[self._cardindex(key)]\n    if ((card.field_specifier is not None) and (keyword == card.rawkeyword)):\n        return card.rawvalue\n    return card.value\n", "label": 1}
{"function": "\n\n@register_gpu_opt()\n@local_optimizer([CumsumOp])\ndef use_gpu_cumsum(node):\n    if ((type(node.op) is CumsumOp) and (node.inputs[0].dtype == 'float32') and node.inputs[0].owner and isinstance(node.inputs[0].owner.op, HostFromGpu)):\n        axis = node.op.axis\n        x = node.inputs[0]\n        if ((axis is not None) and (x.ndim > GpuCumsum.SUPPORTED_NDIMS)):\n            return None\n        x = gpu_from_host(x)\n        if ((axis is None) and (x.ndim > 1)):\n            x = gpu_flatten(x)\n        if (axis is None):\n            axis = 0\n        ret = host_from_gpu(GpuCumsum(axis)(x))\n        ret.tag.values_eq_approx = values_eq_approx_high_tol\n        return [ret]\n", "label": 1}
{"function": "\n\ndef Push(self, branch_base, branch=None, dest_branch=None):\n    'Pushs the named branch.\\n    '\n    project = branch_base.project\n    if (branch is None):\n        branch = project.CurrentBranch\n    if (branch is None):\n        raise GitError('not currently on a branch')\n    branch = project.GetBranch(branch)\n    if (not branch.LocalMerge):\n        raise GitError(('branch %s does not track a remote' % branch.name))\n    if (dest_branch is None):\n        dest_branch = project.dest_branch\n    if (dest_branch is None):\n        dest_branch = branch.merge\n    if (not dest_branch.startswith(R_HEADS)):\n        dest_branch = (R_HEADS + dest_branch)\n    if (not branch.remote.projectname):\n        branch.remote.projectname = project.name\n        branch.remote.Save()\n    remote = branch.remote.name\n    cmd = ['push']\n    cmd.append(remote)\n    if dest_branch.startswith(R_HEADS):\n        dest_branch = dest_branch[len(R_HEADS):]\n    push_type = 'heads'\n    ref_spec = ('%s:refs/%s/%s' % ((R_HEADS + branch.name), push_type, dest_branch))\n    cmd.append(ref_spec)\n    if (GitCommand(project, cmd, bare=True).Wait() != 0):\n        raise UploadError('Push failed')\n", "label": 1}
{"function": "\n\ndef extract_params(self, command, paramstring):\n    if (command in 'Hf'):\n        params = tuple(((int(p) if (len(p) != 0) else 1) for p in paramstring.split(';')))\n        while (len(params) < 2):\n            params = (params + (1,))\n    else:\n        params = tuple((int(p) for p in paramstring.split(';') if (len(p) != 0)))\n        if (len(params) == 0):\n            if (command in 'JKm'):\n                params = (0,)\n            elif (command in 'ABCD'):\n                params = (1,)\n    return params\n", "label": 1}
{"function": "\n\ndef add_to_stack(settingsJson, string, startIndex, endIndex, oldStartIndex, oldEndIndex):\n    if ((settingsJson == '') or (settingsJson == None)):\n        settingsJson = '{\"stack\": [], \"hash\": \"\"}'\n    settings = json.loads(settingsJson)\n    lastStackItem = None\n    if (settings.get('stack') and (len(settings.get('stack')) > 0)):\n        lastStackItem = settings.get('stack')[(len(settings.get('stack')) - 1)]\n    else:\n        lastStackItem = {\n            'start': (- 1),\n            'end': (- 1),\n        }\n    stringHash = hashlib.md5(string).hexdigest()\n    if (stringHash != settings.get('hash')):\n        settings['hash'] = stringHash\n        settings['stack'] = [{\n            'start': startIndex,\n            'end': endIndex,\n        }]\n    elif ((stringHash == settings.get('hash')) and ((lastStackItem.get('start') != oldStartIndex) or (lastStackItem.get('end') != oldEndIndex))):\n        settings['stack'] = [{\n            'start': startIndex,\n            'end': endIndex,\n        }]\n    elif (stringHash == settings.get('hash')):\n        settings['stack'].append({\n            'start': startIndex,\n            'end': endIndex,\n        })\n    newSettingsJson = json.dumps(settings)\n    return newSettingsJson\n", "label": 1}
{"function": "\n\ndef take_action(self, action):\n    ' Take a step in the environment\\n\\n        :param action: The action to apply over the ball\\n        :type action: int\\n\\n        '\n    for i in xrange(20):\n        if (i == 0):\n            self.ball.add_impulse(*self.action_effects[action])\n        self.ball.step()\n        ncollision = 0\n        dxdy = np.array([0, 0])\n        for obs in self.obstacles:\n            if obs.collision(self.ball):\n                dxdy = (dxdy + obs.collision_effect(self.ball))\n                ncollision += 1\n        if (ncollision == 1):\n            self.ball.xdot = dxdy[0]\n            self.ball.ydot = dxdy[1]\n            if (i == 19):\n                self.ball.step()\n        elif (ncollision > 1):\n            self.ball.xdot = (- self.ball.xdot)\n            self.ball.ydot = (- self.ball.ydot)\n        if self.episode_ended():\n            return self.END_EPISODE\n    self.ball.add_drag()\n    self._check_bounds()\n    if (action == self.ACC_NONE):\n        return self.STEP_PENALTY\n    return self.THRUST_PENALTY\n", "label": 1}
{"function": "\n\ndef exception(self, msg, *args, **kwargs):\n    (_junk, exc, _junk) = sys.exc_info()\n    call = self.error\n    emsg = ''\n    if isinstance(exc, (OSError, socket.error)):\n        if (exc.errno in (errno.EIO, errno.ENOSPC)):\n            emsg = str(exc)\n        elif (exc.errno == errno.ECONNREFUSED):\n            emsg = _('Connection refused')\n        elif (exc.errno == errno.EHOSTUNREACH):\n            emsg = _('Host unreachable')\n        elif (exc.errno == errno.ETIMEDOUT):\n            emsg = _('Connection timeout')\n        else:\n            call = self._exception\n    elif isinstance(exc, eventlet.Timeout):\n        emsg = exc.__class__.__name__\n        if hasattr(exc, 'seconds'):\n            emsg += (' (%ss)' % exc.seconds)\n        if isinstance(exc, swift.common.exceptions.MessageTimeout):\n            if exc.msg:\n                emsg += (' %s' % exc.msg)\n    else:\n        call = self._exception\n    call(('%s: %s' % (msg, emsg)), *args, **kwargs)\n", "label": 1}
{"function": "\n\ndef calculate_column_widths(self, padding=0, max_width=None):\n    unscaled_widths = [(w + padding) for w in self._max_widths]\n    if (max_width is None):\n        return unscaled_widths\n    if (not unscaled_widths):\n        return unscaled_widths\n    else:\n        scale_factor = (max_width / float(sum(unscaled_widths)))\n        scaled = [int(round((scale_factor * w))) for w in unscaled_widths]\n        off_by = (sum(scaled) - max_width)\n        while (off_by != 0):\n            iter_order = range(len(scaled))\n            if (off_by < 0):\n                iter_order = reversed(iter_order)\n            for i in iter_order:\n                if (off_by > 0):\n                    scaled[i] -= 1\n                    off_by -= 1\n                else:\n                    scaled[i] += 1\n                    off_by += 1\n                if (off_by == 0):\n                    break\n        return scaled\n", "label": 1}
{"function": "\n\n@classmethod\ndef create_multi(cls, values):\n    \"Creates multiple unique values at once.\\n\\n        :param values:\\n            A sequence of values to be unique. See :meth:`create`.\\n        :returns:\\n            A tuple (bool, list_of_keys). If all values were created, bool is\\n            True and list_of_keys is empty. If one or more values weren't\\n            created, bool is False and the list contains all the values that\\n            already existed in datastore during the creation attempt.\\n        \"\n    keys = [model.Key(cls, value) for value in values]\n    entities = [cls(key=key) for key in keys]\n    func = (lambda e: (e.put() if (not e.key.get()) else None))\n    created = [model.transaction((lambda : func(e))) for e in entities]\n    if (created != keys):\n        model.delete_multi((k for k in created if k))\n        return (False, [k.id() for k in keys if (k not in created)])\n    return (True, [])\n", "label": 1}
{"function": "\n\ndef _make_message(ipython=True, quiet=False, source=None):\n    'Create a banner for an interactive session. '\n    from sympy import __version__ as sympy_version\n    from sympy.polys.domains import GROUND_TYPES\n    from sympy.utilities.misc import ARCH\n    from sympy import SYMPY_DEBUG\n    import sys\n    import os\n    if quiet:\n        return ''\n    python_version = ('%d.%d.%d' % sys.version_info[:3])\n    if ipython:\n        shell_name = 'IPython'\n    else:\n        shell_name = 'Python'\n    info = [('ground types: %s' % GROUND_TYPES)]\n    cache = os.getenv('SYMPY_USE_CACHE')\n    if ((cache is not None) and (cache.lower() == 'no')):\n        info.append('cache: off')\n    if SYMPY_DEBUG:\n        info.append('debugging: on')\n    args = (shell_name, sympy_version, python_version, ARCH, ', '.join(info))\n    message = ('%s console for SymPy %s (Python %s-%s) (%s)\\n' % args)\n    if (source is None):\n        source = preexec_source\n    _source = ''\n    for line in source.split('\\n')[:(- 1)]:\n        if (not line):\n            _source += '\\n'\n        else:\n            _source += (('>>> ' + line) + '\\n')\n    doc_version = sympy_version\n    if ('dev' in doc_version):\n        doc_version = 'dev'\n    else:\n        doc_version = ('%s/' % doc_version)\n    message += ('\\n' + (verbose_message % {\n        'source': _source,\n        'version': doc_version,\n    }))\n    return message\n", "label": 1}
{"function": "\n\ndef GenerateUncompactedItems(self, max_reversed_results=0, timestamp=None):\n    if self.IsAttributeSet(self.Schema.DATA):\n        freeze_timestamp = (timestamp or rdfvalue.RDFDatetime().Now())\n        results = []\n        for (_, value, _) in data_store.DB.ResolvePrefix(self.urn, self.Schema.DATA.predicate, token=self.token, timestamp=(0, freeze_timestamp)):\n            if (results is not None):\n                results.append(self.Schema.DATA(value).payload)\n                if (max_reversed_results and (len(results) > max_reversed_results)):\n                    for result in results:\n                        (yield result)\n                    results = None\n            else:\n                (yield self.Schema.DATA(value).payload)\n        if (results is not None):\n            for result in reversed(results):\n                (yield result)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _print_param(name, schema):\n    if (not schema):\n        raise ValueError(('Missing schema for parameter \"%s\"' % name))\n    wrapper = textwrap.TextWrapper(width=78)\n    wrapper.initial_indent = (' ' * 4)\n    wrapper.subsequent_indent = wrapper.initial_indent\n    print(wrapper.fill(name))\n    wrapper.initial_indent = (' ' * 8)\n    wrapper.subsequent_indent = wrapper.initial_indent\n    if (('description' in schema) and schema['description']):\n        print(wrapper.fill(schema['description']))\n    if (('type' in schema) and schema['type']):\n        print(wrapper.fill(('Type: %s' % schema['type'])))\n    if (('enum' in schema) and schema['enum']):\n        print(wrapper.fill(('Enum: %s' % ', '.join(schema['enum']))))\n    if (('default' in schema) and (schema['default'] is not None)):\n        print(wrapper.fill(('Default: %s' % schema['default'])))\n    print('')\n", "label": 1}
{"function": "\n\ndef write(self, data):\n    if self.headers_finished:\n        self.writer(data)\n        return\n    self.buffer += data\n    while (b'\\n' in self.buffer):\n        if ((b'\\r\\n' in self.buffer) and (self.buffer.find(b'\\r\\n') < self.buffer.find(b'\\n'))):\n            (line1, self.buffer) = self.buffer.split(b'\\r\\n', 1)\n        else:\n            (line1, self.buffer) = self.buffer.split(b'\\n', 1)\n        if (not line1):\n            self.headers_finished = True\n            self.writer = self.start_response(self.status, self.headers)\n            self.writer(self.buffer)\n            del self.buffer\n            del self.headers\n            del self.status\n            break\n        elif (b':' not in line1):\n            raise CGIError(('Bad header line: %r' % line1))\n        else:\n            (name, value) = line1.split(b':', 1)\n            value = value.lstrip()\n            name = name.strip()\n            if six.PY3:\n                name = name.decode('utf8')\n                value = value.decode('utf8')\n            if (name.lower() == 'status'):\n                if (' ' not in value):\n                    value = ('%s General' % value)\n                self.status = value\n            else:\n                self.headers.append((name, value))\n", "label": 1}
{"function": "\n\ndef include_config(include, orig_path, verbose):\n    '\\n    Parses extra configuration file(s) specified in an include list in the\\n    main config file.\\n    '\n    if (not include):\n        return {\n            \n        }\n    if (orig_path is None):\n        return {\n            \n        }\n    if isinstance(include, str):\n        include = [include]\n    configuration = {\n        \n    }\n    for path in include:\n        path = os.path.expanduser(path)\n        if (not os.path.isabs(path)):\n            path = os.path.join(os.path.dirname(orig_path), path)\n        if (len(glob.glob(path)) == 0):\n            if verbose:\n                log.warning('Warning parsing configuration file: \"include\" path/glob \\'{0}\\' matches no files'.format(path))\n        for fn_ in sorted(glob.glob(path)):\n            log.debug(\"Including configuration from '{0}'\".format(fn_))\n            opts = _read_conf_file(fn_)\n            include = opts.get('include', [])\n            if include:\n                opts.update(include_config(include, fn_, verbose))\n            salt.utils.dictupdate.update(configuration, opts)\n    return configuration\n", "label": 1}
{"function": "\n\ndef CheckArguments(self):\n    'Check arguments.\\n\\n    '\n    nodes = []\n    for inst in self.op.instances:\n        if (inst.iallocator is not None):\n            raise errors.OpPrereqError('iallocator are not allowed to be set on instance objects', errors.ECODE_INVAL)\n        nodes.append(bool(inst.pnode))\n        if (inst.disk_template in constants.DTS_INT_MIRROR):\n            nodes.append(bool(inst.snode))\n    has_nodes = compat.any(nodes)\n    if (compat.all(nodes) ^ has_nodes):\n        raise errors.OpPrereqError('There are instance objects providing pnode/snode while others do not', errors.ECODE_INVAL)\n    if ((not has_nodes) and (self.op.iallocator is None)):\n        default_iallocator = self.cfg.GetDefaultIAllocator()\n        if default_iallocator:\n            self.op.iallocator = default_iallocator\n        else:\n            raise errors.OpPrereqError('No iallocator or nodes on the instances given and no cluster-wide default iallocator found; please specify either an iallocator or nodes on the instances or set a cluster-wide default iallocator', errors.ECODE_INVAL)\n    CheckOpportunisticLocking(self.op)\n    dups = utils.FindDuplicates([op.instance_name for op in self.op.instances])\n    if dups:\n        raise errors.OpPrereqError(('There are duplicate instance names: %s' % utils.CommaJoin(dups)), errors.ECODE_INVAL)\n", "label": 1}
{"function": "\n\ndef get_context_files(app):\n    'Load static context from ``location``\\n    '\n    ctx = {\n        \n    }\n    location = app.config['CONTENT_PARTIALS']\n    if (location and os.path.isdir(location)):\n        for (dirpath, dirs, filenames) in os.walk(location, topdown=False):\n            if skipfile((os.path.basename(dirpath) or dirpath)):\n                continue\n            for filename in filenames:\n                if skipfile(filename):\n                    continue\n                file_bits = filename.split('.')\n                bits = [file_bits[0]]\n                prefix = get_rel_dir(dirpath, location)\n                while prefix:\n                    (prefix, tail) = os.path.split(prefix)\n                    bits.append(tail)\n                filename = os.path.join(dirpath, filename)\n                suffix = get_reader(app, filename).suffix\n                name = '_'.join(reversed(bits))\n                if suffix:\n                    name = ('%s_%s' % (suffix, name))\n                ctx[name] = filename\n    return ctx\n", "label": 1}
{"function": "\n\ndef buildChildren(self, child_, node, nodeName_, fromsubclass_=False):\n    if (nodeName_ == 'Title'):\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Title(obj_)\n    elif (nodeName_ == 'Author'):\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Author(obj_)\n    elif (nodeName_ == 'Subject'):\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Subject(obj_)\n    elif (nodeName_ == 'Keywords'):\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Keywords(obj_)\n    elif (nodeName_ == 'Creator'):\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Creator(obj_)\n    elif (nodeName_ == 'Producer'):\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Producer(obj_)\n    elif (nodeName_ == 'CreationDate'):\n        obj_ = cybox_common.DateTimeObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_CreationDate(obj_)\n    elif (nodeName_ == 'ModDate'):\n        obj_ = cybox_common.DateTimeObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_ModDate(obj_)\n    elif (nodeName_ == 'Trapped'):\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Trapped(obj_)\n", "label": 1}
{"function": "\n\ndef _debug(obj, params):\n    message = [('<%s' % obj.__class__.__name__)]\n    for (k, v) in params:\n        if isinstance(v, (list, tuple)):\n            message.append(('len(%s)=%d' % (k, len(v))))\n            if len(v):\n                k = ('%s[0]' % k)\n                v = v[0]\n        if v:\n            if isinstance(v, format.NetworkAddress):\n                text = ('%s:%d' % (v.address, v.port))\n            elif isinstance(v, format.InventoryVector):\n                obj_type = 'unknown'\n                if (v.object_type <= 2):\n                    obj_type = ['error', 'tx', 'block'][v.object_type]\n                text = ('%s:%s' % (obj_type, v.hash.encode('hex')))\n            elif isinstance(v, format.Txn):\n                text = v.hash.encode('hex')\n            elif isinstance(v, format.BlockHeader):\n                text = v.hash.encode('hex')\n            else:\n                text = str(v)\n            message.append(('%s=%s' % (k, text)))\n    return (' '.join(message) + '>')\n", "label": 1}
{"function": "\n\ndef indent_code(self, code):\n    'Accepts a string of code or a list of code lines'\n    if isinstance(code, string_types):\n        code_lines = self.indent_code(code.splitlines(True))\n        return ''.join(code_lines)\n    tab = '  '\n    inc_regex = ('^function ', '^if ', '^elseif ', '^else$', '^for ')\n    dec_regex = ('^end$', '^elseif ', '^else$')\n    code = [line.lstrip(' \\t') for line in code]\n    increase = [int(any([search(re, line) for re in inc_regex])) for line in code]\n    decrease = [int(any([search(re, line) for re in dec_regex])) for line in code]\n    pretty = []\n    level = 0\n    for (n, line) in enumerate(code):\n        if ((line == '') or (line == '\\n')):\n            pretty.append(line)\n            continue\n        level -= decrease[n]\n        pretty.append(('%s%s' % ((tab * level), line)))\n        level += increase[n]\n    return pretty\n", "label": 1}
{"function": "\n\ndef s_custom_shape(self, tag, attrs):\n    ' A <draw:custom-shape> is made into a <div> in HTML which is then styled\\n        '\n    anchor_type = attrs.get((TEXTNS, 'anchor-type'), 'notfound')\n    htmltag = 'div'\n    name = ('G-' + attrs.get((DRAWNS, 'style-name'), ''))\n    if (name == 'G-'):\n        name = ('PR-' + attrs.get((PRESENTATIONNS, 'style-name'), ''))\n    name = name.replace('.', '_')\n    if (anchor_type == 'paragraph'):\n        style = 'position:absolute;'\n    elif (anchor_type == 'char'):\n        style = 'position:absolute;'\n    elif (anchor_type == 'as-char'):\n        htmltag = 'div'\n        style = ''\n    else:\n        style = 'position: absolute;'\n    if ((SVGNS, 'width') in attrs):\n        style = (((style + 'width:') + attrs[(SVGNS, 'width')]) + ';')\n    if ((SVGNS, 'height') in attrs):\n        style = (((style + 'height:') + attrs[(SVGNS, 'height')]) + ';')\n    if ((SVGNS, 'x') in attrs):\n        style = (((style + 'left:') + attrs[(SVGNS, 'x')]) + ';')\n    if ((SVGNS, 'y') in attrs):\n        style = (((style + 'top:') + attrs[(SVGNS, 'y')]) + ';')\n    if self.generate_css:\n        self.opentag(htmltag, {\n            'class': name,\n            'style': style,\n        })\n    else:\n        self.opentag(htmltag)\n", "label": 1}
{"function": "\n\ndef append_token_line_indentation(self):\n    if self.just_added_newline():\n        line = self.output_lines[(- 1)]\n        if (self.opts.keep_array_indentation and self.is_array(self.flags.mode) and self.input_wanted_newline):\n            line.text.append('')\n            for item in self.whitespace_before_token:\n                line.text.append(item)\n        else:\n            if (self.preindent_string != ''):\n                line.text.append(self.preindent_string)\n            level = self.flags.indentation_level\n            if (self.flags.var_line and self.flags.var_line_reindented):\n                level += 1\n            if self.output_wrapped:\n                level += 1\n            self.append_indent_string(level)\n", "label": 1}
{"function": "\n\ndef fieldnames(self, table=None, dotted=False):\n    ret = set()\n    if (table is None):\n        for t in viewvalues(self.tables):\n            if dotted:\n                ret.update([('%s.%s' % (t.name, c.name)) for c in t.columns])\n            else:\n                ret.update([c.name for c in t.columns])\n        return ret\n    if (table not in self.tables):\n        return set()\n    t = self.tables[table]\n    if dotted:\n        return {('%s.%s' % (t.name, c.name)) for c in t.columns}\n    return {c.name for c in t.columns}\n", "label": 1}
{"function": "\n\ndef array_repr(arr):\n    if (hasattr(arr, 'name') and (arr.name is not None)):\n        name_str = ('%r ' % arr.name)\n    else:\n        name_str = ''\n    dim_summary = ', '.join((('%s: %s' % (k, v)) for (k, v) in zip(arr.dims, arr.shape)))\n    summary = [('<xarray.%s %s(%s)>' % (type(arr).__name__, name_str, dim_summary))]\n    if isinstance(getattr(arr, 'variable', arr)._data, dask_array_type):\n        summary.append(repr(arr.data))\n    elif (arr._in_memory or (arr.size < 100000.0)):\n        summary.append(repr(arr.values))\n    else:\n        summary.append(('[%s values with dtype=%s]' % (arr.size, arr.dtype)))\n    if hasattr(arr, 'coords'):\n        if arr.coords:\n            summary.append(repr(arr.coords))\n    if arr.attrs:\n        summary.append(attrs_repr(arr.attrs))\n    return '\\n'.join(summary)\n", "label": 1}
{"function": "\n\n@add_auth_token_to_kwargs_from_env\ndef query(self, **kwargs):\n    if (not kwargs):\n        raise Exception('Query parameter is not provided.')\n    if (('limit' in kwargs) and (kwargs.get('limit') <= 0)):\n        kwargs.pop('limit')\n    token = kwargs.get('token', None)\n    params = kwargs.get('params', {\n        \n    })\n    print(params)\n    for (k, v) in six.iteritems(kwargs):\n        if (k != 'token'):\n            params[k] = v\n    url = ('/%s/?%s' % (self.resource.get_url_path_name(), urllib.parse.urlencode(params)))\n    response = (self.client.get(url, token=token) if token else self.client.get(url))\n    if (response.status_code == 404):\n        return []\n    if (response.status_code != 200):\n        self.handle_error(response)\n    items = response.json()\n    instances = [self.resource.deserialize(item) for item in items]\n    return instances\n", "label": 1}
{"function": "\n\ndef __call__(self, args):\n    (options, args, context) = self.parse_args_and_create_context(args)\n    context.headers = self.options_list_to_lowered_dict(options.header)\n    context.query = self.options_list_to_lowered_dict(options.query)\n    context.input_ = options.input_\n    context.segment_size = options.segment_size\n    context.static_segments = False\n    if (context.segment_size and (context.segment_size[0].lower() == 's')):\n        context.static_segments = True\n        context.segment_size = context.segment_size[1:]\n    context.segment_size = int((context.segment_size or (((5 * 1024) * 1024) * 1024)))\n    if (context.segment_size < 1):\n        raise ReturnCode(('invalid segment size %s' % options.segment_size))\n    context.stdin_segmentation = options.stdin_segmentation\n    context.empty = options.empty\n    context.newer = options.newer\n    context.different = options.different\n    context.encrypt = options.encrypt\n    if (context.encrypt == '-'):\n        context.encrypt = os.environ.get('SWIFTLY_CRYPT_KEY')\n        if (not context.encrypt):\n            raise ReturnCode('A single dash \"-\" was given as the encryption key, but no key was found in the SWIFTLY_CRYPT_KEY environment variable.')\n    if (context.encrypt and context.different):\n        raise ReturnCode('--different will not work properly with --encrypt since encryption may change the object size')\n    path = (args.pop(0).lstrip('/') if args else None)\n    return cli_put(context, path)\n", "label": 1}
{"function": "\n\ndef check_color(c, greyscale, which):\n    'Checks that a colour argument for transparent or\\n    background options is the right form.  Returns the colour\\n    (which, if it\\'s a bar integer, is \"corrected\" to a 1-tuple).\\n    '\n    if (c is None):\n        return c\n    if greyscale:\n        try:\n            len(c)\n        except TypeError:\n            c = (c,)\n        if (len(c) != 1):\n            raise ValueError(('%s for greyscale must be 1-tuple' % which))\n        if (not isinteger(c[0])):\n            raise ValueError(('%s colour for greyscale must be integer' % which))\n    elif (not ((len(c) == 3) and isinteger(c[0]) and isinteger(c[1]) and isinteger(c[2]))):\n        raise ValueError(('%s colour must be a triple of integers' % which))\n    return c\n", "label": 1}
{"function": "\n\n@StateNameInit()\ndef __init__(self, model):\n    self.model = model\n    model.check_model()\n    if isinstance(model, JunctionTree):\n        self.variables = set(chain(*model.nodes()))\n    else:\n        self.variables = model.nodes()\n    self.cardinality = {\n        \n    }\n    self.factors = defaultdict(list)\n    if isinstance(model, BayesianModel):\n        for node in model.nodes():\n            cpd = model.get_cpds(node)\n            cpd_as_factor = cpd.to_factor()\n            self.cardinality[node] = cpd.variable_card\n            for var in cpd.variables:\n                self.factors[var].append(cpd_as_factor)\n    elif isinstance(model, (MarkovModel, FactorGraph, JunctionTree)):\n        self.cardinality = model.get_cardinality()\n        for factor in model.get_factors():\n            for var in factor.variables:\n                self.factors[var].append(factor)\n    elif isinstance(model, DynamicBayesianNetwork):\n        self.start_bayesian_model = BayesianModel(model.get_intra_edges(0))\n        self.start_bayesian_model.add_cpds(*model.get_cpds(time_slice=0))\n        cpd_inter = [model.get_cpds(node) for node in model.get_interface_nodes(1)]\n        self.interface_nodes = model.get_interface_nodes(0)\n        self.one_and_half_model = BayesianModel((model.get_inter_edges() + model.get_intra_edges(1)))\n        self.one_and_half_model.add_cpds(*(model.get_cpds(time_slice=1) + cpd_inter))\n", "label": 1}
{"function": "\n\ndef write_options_group(self, group, fp, with_title=True):\n\n    def is_positional_group(group):\n        return any(((not o.option_strings) for o in group._group_actions))\n\n    def get_help(option):\n        text = textwrap.dedent(((option.help or '') % option.__dict__))\n        return '\\n'.join((('| ' + line) for line in text.splitlines()))\n\n    def shipout(text):\n        fp.write(textwrap.indent('\\n'.join(text), (' ' * 4)))\n    if (not group._group_actions):\n        return\n    if with_title:\n        fp.write('\\n\\n')\n        fp.write((group.title + '\\n'))\n    text = []\n    if is_positional_group(group):\n        for option in group._group_actions:\n            text.append(option.metavar)\n            text.append(textwrap.indent((option.help or ''), (' ' * 4)))\n        shipout(text)\n        return\n    options = []\n    for option in group._group_actions:\n        if option.metavar:\n            option_fmt = ('``%%s %s``' % option.metavar)\n        else:\n            option_fmt = '``%s``'\n        option_str = ', '.join(((option_fmt % s) for s in option.option_strings))\n        options.append((option_str, option))\n    for (option_str, option) in options:\n        help = textwrap.indent(get_help(option), (' ' * 4))\n        text.append(option_str)\n        text.append(help)\n    shipout(text)\n", "label": 1}
{"function": "\n\ndef trim_docstring(docstring):\n    if (not docstring):\n        return ''\n    lines = docstring.expandtabs().splitlines()\n    indent = sys.maxsize\n    for line in lines[1:]:\n        stripped = line.lstrip()\n        if stripped:\n            indent = min(indent, (len(line) - len(stripped)))\n    trimmed = [lines[0].strip()]\n    if (indent < sys.maxsize):\n        for line in lines[1:]:\n            trimmed.append(line[indent:].rstrip())\n    while (trimmed and (not trimmed[(- 1)])):\n        trimmed.pop()\n    while (trimmed and (not trimmed[0])):\n        trimmed.pop(0)\n    return '\\n'.join(trimmed)\n", "label": 1}
{"function": "\n\ndef _build_metadata(self, year, elections):\n    meta_entries = []\n    for election in elections:\n        slug = election['slug']\n        year = int(election['start_date'].split('-')[0])\n        if (year <= 2006):\n            meta_entries.extend(self._build_metadata_preprocessed(election))\n        elif (slug == 'wa-2007-08-21-primary'):\n            meta_entries.extend(self._build_metadata_direct_links(election))\n        elif ((slug == 'wa-2007-11-06-general') or ((year >= 2008) and (year <= 2011))):\n            if (slug == 'wa-2011-08-16-primary'):\n                continue\n            meta_entries.extend(self._build_metadata_state_county(election))\n            meta_entries.extend(self._build_metadata_url_paths(election))\n        elif ((year >= 2012) and (year <= 2013)):\n            meta_entries.extend(self._build_metadata_url_paths(election))\n        else:\n            msg = 'Not sure how to define mappings for election {}.  Please update openelex.us.wa.datasource'.format(slug)\n            raise NotImplemented(msg)\n    return meta_entries\n", "label": 1}
{"function": "\n\ndef keyPressEvent(self, event):\n    '\\n        Handle key events\\n        '\n    if (self.model.player is None):\n        return\n    key_code = event.key()\n    player = self.model.player\n    next_creature = self.model.get_next_creature(self.rules_engine)\n    if (next_creature == player):\n        if (key_code in self.keymap):\n            self.keymap[key_code](key_code, event.modifiers())\n    next_creature = self.model.get_next_creature(self.rules_engine)\n    if (next_creature is None):\n        self.model.end_condition = DIED_IN_DUNGEON\n    while ((next_creature != player) and (next_creature is not None) and (self.model.end_condition == 0)):\n        next_creature.act()\n        next_creature = self.model.get_next_creature(self.rules_engine)\n        if (next_creature is None):\n            self.model.end_condition = DIED_IN_DUNGEON\n    if (self.model.end_condition != 0):\n        self.EndScreenRequested.emit()\n", "label": 1}
{"function": "\n\ndef find_command(cmd, paths=None, pathext=None):\n    'Searches the PATH for the given command and returns its path'\n    if (paths is None):\n        paths = os.environ.get('PATH', []).split(os.pathsep)\n    if isinstance(paths, string_types):\n        paths = [paths]\n    if (pathext is None):\n        pathext = os.environ.get('PATHEXT', '.COM;.EXE;.BAT;.CMD')\n    pathext = [ext for ext in pathext.lower().split(os.pathsep)]\n    if (os.path.splitext(cmd)[1].lower() in pathext):\n        pathext = ['']\n    for path in paths:\n        cmd_path = os.path.join(path, cmd)\n        for ext in pathext:\n            cmd_path_ext = (cmd_path + ext)\n            if os.path.isfile(cmd_path_ext):\n                return cmd_path_ext\n        if os.path.isfile(cmd_path):\n            return cmd_path\n    return None\n", "label": 1}
{"function": "\n\ndef apply(self, inv_plugin):\n    clicked = self.slot\n    cursor = inv_plugin.cursor_slot\n    if (clicked == cursor):\n        if (self.button == constants.INV_BUTTON_LEFT):\n            clicked.amount = 0\n        elif (self.button == constants.INV_BUTTON_RIGHT):\n            clicked.amount -= 1\n        self.cleanup_if_empty(clicked)\n    elif (self.button == constants.INV_BUTTON_LEFT):\n        if clicked.stacks_with(cursor):\n            self.transfer(cursor, clicked, cursor.amount)\n        else:\n            self.swap_slots(cursor, clicked)\n    elif (self.button == constants.INV_BUTTON_RIGHT):\n        if cursor.is_empty:\n            self.transfer(clicked, cursor, ((clicked.amount + 1) // 2))\n        elif (clicked.is_empty or clicked.stacks_with(cursor)):\n            self.transfer(cursor, clicked, 1)\n        else:\n            self.swap_slots(cursor, clicked)\n    else:\n        raise NotImplementedError(('Clicking with button %s not implemented' % self.button))\n", "label": 1}
{"function": "\n\ndef find_node(v, cls, ignore_clients=False):\n    if ((v.owner is not None) and (ignore_clients or (len(v.clients) == 1))):\n        if isinstance(v.owner.op, cls):\n            return v.owner\n        elif (isinstance(v.owner.op, GpuFromHost) and (v.owner.inputs[0].owner is not None) and (ignore_clients or (len(v.owner.inputs[0].clients) == 1)) and isinstance(v.owner.inputs[0].owner.op, HostFromGpu)):\n            return find_node(v.owner.inputs[0].owner.inputs[0], cls)\n        else:\n            return None\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw):\n    cause = None\n    if (args and isinstance(args[0], Exception)):\n        (cause, args) = (args[0], args[1:])\n    ret = super(ExceptionCauseMixin, cls).__new__(cls, *args, **kw)\n    ret.cause = cause\n    if (cause is None):\n        return ret\n    root_cause = getattr(cause, 'root_cause', None)\n    if (root_cause is None):\n        ret.root_cause = cause\n    else:\n        ret.root_cause = root_cause\n    full_trace = getattr(cause, 'full_trace', None)\n    if (full_trace is not None):\n        ret.full_trace = list(full_trace)\n        ret._tb = list(cause._tb)\n        ret._stack = list(cause._stack)\n        return ret\n    try:\n        (exc_type, exc_value, exc_tb) = sys.exc_info()\n        if ((exc_type is None) and (exc_value is None)):\n            return ret\n        if ((cause is exc_value) or (root_cause is exc_value)):\n            ret._tb = _extract_from_tb(exc_tb)\n            ret._stack = _extract_from_frame(exc_tb.tb_frame)\n            ret.full_trace = (ret._stack[:(- 1)] + ret._tb)\n    finally:\n        del exc_tb\n    return ret\n", "label": 1}
{"function": "\n\ndef fix_paths(app_path, python_lib_path):\n    \"Fix the __path__ attr of sys.modules entries.\\n\\n  Specifically this fixes the path of those sys.modules package entries that\\n  have __path__ attributes that point to the python library, but where there\\n  is a similar package in the application's code.\\n\\n  Args:\\n    app_path: The root path of the application code.\\n    python_lib_path: The root path of the python library.\\n  \"\n    if os.path.isfile(os.path.join(app_path, AUTO_IMPORT_FIXER_FILE)):\n        return\n    for (module_name, module) in sys.modules.items():\n        if (getattr(module, '__path__', None) is None):\n            continue\n        module_app_path = os.path.join(app_path, *module_name.split('.'))\n        module_init_file = os.path.join(module_app_path, '__init__.py')\n        if (not os.path.isfile(module_init_file)):\n            continue\n        found_python_lib_path = False\n        found_app_path = False\n        for path in module.__path__:\n            if path.startswith(python_lib_path):\n                found_python_lib_path = True\n            if path.startswith(app_path):\n                found_app_path = True\n        if (found_python_lib_path and (not found_app_path)):\n            module.__path__.append(module_app_path)\n", "label": 1}
{"function": "\n\ndef discover_relevant_flowblock_models(flow, pad, record, alt):\n    'Returns a dictionary of all relevant flow blocks.  If no list of\\n    flow block names is provided all flow blocks are returned.  Otherwise\\n    only flow blocks that are in the list or are children of flowblocks\\n    in the list are returned.\\n    '\n    flow_blocks = flow.flow_blocks\n    all_blocks = pad.db.flowblocks\n    if (flow_blocks is None):\n        return dict(((k, v.to_json(pad, record, alt)) for (k, v) in iteritems(all_blocks)))\n    wanted_blocks = set()\n    to_process = flow_blocks[:]\n    while to_process:\n        block_name = to_process.pop()\n        flowblock = all_blocks.get(block_name)\n        if ((block_name in wanted_blocks) or (flowblock is None)):\n            continue\n        wanted_blocks.add(block_name)\n        for field in flowblock.fields:\n            if isinstance(field.type, FlowType):\n                if (field.type.flow_blocks is None):\n                    raise RuntimeError('Nested flow-blocks require explicit list of involved blocks.')\n                to_process.extend(field.type.flow_blocks)\n    rv = {\n        \n    }\n    for block_name in wanted_blocks:\n        rv[block_name] = all_blocks[block_name].to_json(pad, record, alt)\n    return rv\n", "label": 1}
{"function": "\n\ndef which(filename):\n    'This takes a given filename; tries to find it in the environment path;\\n    then checks if it is executable. This returns the full path to the filename\\n    if found and executable. Otherwise this returns None.'\n    if ((os.path.dirname(filename) != '') and is_executable_file(filename)):\n        return filename\n    if (('PATH' not in os.environ) or (os.environ['PATH'] == '')):\n        p = os.defpath\n    else:\n        p = os.environ['PATH']\n    pathlist = p.split(os.pathsep)\n    for path in pathlist:\n        ff = os.path.join(path, filename)\n        if pty:\n            if is_executable_file(ff):\n                return ff\n        else:\n            pathext = os.environ.get('Pathext', '.exe;.com;.bat;.cmd')\n            pathext = (pathext.split(os.pathsep) + [''])\n            for ext in pathext:\n                if os.access((ff + ext), os.X_OK):\n                    return (ff + ext)\n    return None\n", "label": 1}
{"function": "\n\ndef sstpControlPacketReceived(self, messageType, attributes):\n    logging.info(('SSTP control packet (type %s) received.' % ord(messageType[1])))\n    if (messageType == SSTP_MSG_CALL_CONNECT_REQUEST):\n        protocolId = attributes[0][1]\n        self.sstpMsgCallConnectRequestReceived(protocolId)\n    elif (messageType == SSTP_MSG_CALL_CONNECTED):\n        attr = attributes[0][1]\n        hashType = attr[3:4]\n        nonce = attr[4:36]\n        certHash = attr[36:68]\n        macHash = attr[68:72]\n        self.sstpMsgCallConnectedReceived(hashType, nonce, certHash, macHash)\n    elif (messageType == SSTP_MSG_CALL_ABORT):\n        if attributes:\n            self.sstpMsgCallAbort(attributes[0][1])\n        else:\n            self.sstpMsgCallAbort()\n    elif (messageType == SSTP_MSG_CALL_DISCONNECT):\n        if attributes:\n            self.sstpMsgCallDisconnect(attributes[0][1])\n        else:\n            self.sstpMsgCallDisconnect()\n    elif (messageType == SSTP_MSG_CALL_DISCONNECT_ACK):\n        self.sstpMsgCallDisconnectAck()\n    elif (messageType == SSTP_MSG_ECHO_REQUEST):\n        self.sstpMsgEchoRequest()\n    elif (messageType == SSTP_MSG_ECHO_RESPONSE):\n        self.sstpMsgEchoResponse()\n    else:\n        logging.warn('Unknown type of SSTP control packet.')\n        self.abort(ATTRIB_STATUS_INVALID_FRAME_RECEIVED)\n", "label": 1}
{"function": "\n\ndef get_parent_child_types(self):\n    '\\n        Returns a dict of {parent case type: [subcase types...]}\\n        '\n    parent_child_types = {\n        \n    }\n    for app in self.apps:\n        for module in app.get_modules():\n            case_type = module.case_type\n            if (case_type not in parent_child_types):\n                parent_child_types[case_type] = []\n            if (module.module_type == 'basic'):\n                for form in module.get_forms():\n                    for subcase in form.actions.subcases:\n                        parent_child_types[case_type].append(subcase.case_type)\n            elif (module.module_type == 'advanced'):\n                for form in module.get_forms():\n                    for subcase in form.actions.get_open_subcase_actions(case_type):\n                        parent_child_types[case_type].append(subcase.case_type)\n    return self.clean_dict_list(parent_child_types)\n", "label": 1}
{"function": "\n\ndef minibatches(arr, batch_size, d=0):\n    'Return a list of views of the given arr.\\n\\n    Each view represents a mini bach of the data.\\n\\n    Parameters\\n    ----------\\n\\n    arr : array_like\\n        Array to obtain batches from. Needs to be slicable. If ``d > 0``, needs\\n        to have a ``.shape`` attribute from which the number of samples can\\n        be obtained.\\n\\n    batch_size : int\\n        Size of a batch. Last batch might be smaller if ``batch_size`` is not a\\n        divisor of ``arr``.\\n\\n    d : int, optional, default: 0\\n        Dimension along which the data samples are separated and thus slicing\\n        should be done.\\n\\n    Returns\\n    -------\\n\\n    mini_batches : list\\n        Each item of the list is a view of ``arr``. Views are ordered.\\n    '\n    if (d == 0):\n        (n_batches, rest) = divmod(len(arr), batch_size)\n    else:\n        (n_batches, rest) = divmod(arr.shape[d], batch_size)\n    if rest:\n        n_batches += 1\n    slices = (slice((i * batch_size), ((i + 1) * batch_size)) for i in range(n_batches))\n    if (d == 0):\n        res = [arr[i] for i in slices]\n    elif (d == 1):\n        res = [arr[:, i] for i in slices]\n    elif (d == 2):\n        res = [arr[:, :, i] for i in slices]\n    return res\n", "label": 1}
{"function": "\n\ndef do_effect(self, can_msg, args):\n    if can_msg.CANData:\n        if (('black_list' in args) and (can_msg.CANFrame.frame_id in args.get('black_list', []))):\n            can_msg.CANData = False\n            self.dprint(2, (((('Message ' + str(can_msg.CANFrame.frame_id)) + ' has been blocked(BL) (BUS = ') + str(can_msg.bus)) + ')'))\n        elif (('white_list' in args) and (can_msg.CANFrame.frame_id not in args.get('white_list', []))):\n            can_msg.CANData = False\n            self.dprint(2, (((('Message ' + str(can_msg.CANFrame.frame_id)) + ' has been blocked(WL) (BUS = ') + str(can_msg.bus)) + ')'))\n        if (('white_body' in args) and (can_msg.CANFrame.frame_data not in args.get('white_body', []))):\n            can_msg.CANData = False\n            self.dprint(2, (((('Message ' + str(can_msg.CANFrame.frame_id)) + ' has been blocked(WB) (BUS = ') + str(can_msg.bus)) + ')'))\n        elif (('black_body' in args) and (can_msg.CANFrame.frame_data in args.get('black_body', []))):\n            can_msg.CANData = False\n            self.dprint(2, (((('Message ' + str(can_msg.CANFrame.frame_id)) + ' has been blocked(BB) (BUS = ') + str(can_msg.bus)) + ')'))\n    return can_msg\n", "label": 1}
{"function": "\n\ndef net_msg_handle(self, msg):\n    if (msg[0:7] in ['UIDRESP']):\n        (_, uid) = msg.split(' ', 1)\n        if (self.Node == None):\n            self.Node = Node(uid, self.host, self.port, self)\n        self.finger.add(self.Node)\n    if (msg[0:6] in ['UIDREQ']):\n        resp = ('UIDRESP ' + self.finger.self.uid)\n        self.send(resp)\n    if (msg[0:4] in ['PING']):\n        self.send('PONG')\n    if (msg[0:4] in ['PONG']):\n        pass\n    if (self.Node == None):\n        self.send('UIDREQ')\n        return\n    else:\n        self.Node.seen()\n    if (msg[0:9] in ['REQ_LEVEL']):\n        (_, level) = msg.split(' ', 1)\n        node = self.finger.get_node_from_level(int(level), self.Node.uid)\n        if node:\n            resp = ((((('RESP_LEVEL ' + str(node.uid)) + ' ') + self.host) + ':') + str(self.port))\n            self.send(resp)\n    if (msg[0:10] in ['RESP_LEVEL']):\n        (_, uid, addr) = msg.split(' ', 1)\n        Connect(self.finger, self.set_handler, addr)\n    self.set_handler.handle_msg(self, msg)\n", "label": 1}
{"function": "\n\ndef read_fasta(ifile):\n    'iterate over id,title,seq from stream ifile'\n    id = None\n    isEmpty = True\n    for line in ifile:\n        if ('>' == line[0]):\n            if ((id is not None) and (len(seq) > 0)):\n                (yield (id, title, seq))\n                isEmpty = False\n            id = line[1:].split()[0]\n            title = line[(len(id) + 2):]\n            seq = ''\n        elif (id is not None):\n            for word in line.split():\n                seq += word\n    if ((id is not None) and (len(seq) > 0)):\n        (yield (id, title, seq))\n    elif isEmpty:\n        raise IOError('no readable sequence in FASTA file!')\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.MAP):\n                self.confOptions = {\n                    \n                }\n                (_ktype8, _vtype9, _size7) = iprot.readMapBegin()\n                for _i11 in xrange(_size7):\n                    _key12 = iprot.readString()\n                    _val13 = iprot.readString()\n                    self.confOptions[_key12] = _val13\n                iprot.readMapEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __init__(self, dr_input):\n    '\\n        Initializes an GDAL/OGR driver on either a string or integer input.\\n        '\n    if isinstance(dr_input, six.string_types):\n        self.ensure_registered()\n        if (dr_input.lower() in self._alias):\n            name = self._alias[dr_input.lower()]\n        else:\n            name = dr_input\n        for iface in (vcapi, rcapi):\n            driver = iface.get_driver_by_name(force_bytes(name))\n            if driver:\n                break\n    elif isinstance(dr_input, int):\n        self.ensure_registered()\n        for iface in (vcapi, rcapi):\n            driver = iface.get_driver(dr_input)\n            if driver:\n                break\n    elif isinstance(dr_input, c_void_p):\n        driver = dr_input\n    else:\n        raise GDALException(('Unrecognized input type for GDAL/OGR Driver: %s' % str(type(dr_input))))\n    if (not driver):\n        raise GDALException(('Could not initialize GDAL/OGR Driver on input: %s' % str(dr_input)))\n    self.ptr = driver\n", "label": 1}
{"function": "\n\ndef decode(self, input, final=False):\n    if self.buffer:\n        input = (self.buffer + input)\n    output = self.decoder.decode(input, final=final)\n    if (output.endswith('\\r') and (not final)):\n        output = output[:(- 1)]\n        self.buffer = b'\\r'\n    else:\n        self.buffer = b''\n    crlf = output.count('\\r\\n')\n    cr = (output.count('\\r') - crlf)\n    lf = (output.count('\\n') - crlf)\n    self.seennl |= (((lf and self._LF) | (cr and self._CR)) | (crlf and self._CRLF))\n    if self.translate:\n        if crlf:\n            output = output.replace('\\r\\n', '\\n')\n        if cr:\n            output = output.replace('\\r', '\\n')\n    return output\n", "label": 1}
{"function": "\n\ndef transform_Assign(self, stmt):\n    if ((stmt.lhs.__class__ is Var) and (stmt.rhs.__class__ is not Index)):\n        name = stmt.lhs.name\n        if (name in self.safe_to_move):\n            deps = self.analysis.depends_on[name]\n            if all(((d in self.binding_depth) for d in deps)):\n                if (len(deps) > 0):\n                    target_level = max((self.binding_depth[d] for d in deps))\n                else:\n                    target_level = 0\n                if ((target_level >= 0) and (target_level < self.blocks.depth())):\n                    self.blocks._blocks[target_level].append(stmt)\n                    self.binding_depth[name] = target_level\n                    return None\n    self.mark_binding_depths(collect_binding_names(stmt.lhs))\n    return stmt\n", "label": 1}
{"function": "\n\ndef _format(self, obj, indent, allowance, context, level):\n    level = (level + 1)\n    objid = id(obj)\n    if (objid in context):\n        self._io.write(_recursion(obj))\n        return\n    rep = self._repr(obj, context, (level - 1))\n    typ = type(obj)\n    sep_lines = (len(rep) > (((self._termwidth - 1) - indent) - allowance))\n    write = self._io.write\n    if sep_lines:\n        if (typ is dict):\n            write('{\\n  ')\n            length = len(obj)\n            if length:\n                context[objid] = 1\n                indent = (indent + 2)\n                items = obj.items()\n                items.sort()\n                (key, ent) = items[0]\n                rep = self._repr(key, context, level)\n                write(rep)\n                write(': ')\n                self._format(ent, ((indent + len(rep)) + 2), (allowance + 1), context, level)\n                if (length > 1):\n                    for (key, ent) in items[1:]:\n                        rep = self._repr(key, context, level)\n                        write((',\\n%s%s: ' % ((' ' * indent), rep)))\n                        self._format(ent, ((indent + len(rep)) + 2), (allowance + 1), context, level)\n                indent = (indent - 2)\n                del context[objid]\n            write('\\n}')\n            return\n        if (typ is list):\n            write('[\\n')\n            self.print_list(obj, 2)\n            write(']')\n            return\n        if (typ is tuple):\n            write('(\\n')\n            self.print_list(obj, 2)\n            if (len(obj) == 1):\n                write(',')\n            write(')')\n            return\n    write(rep)\n", "label": 1}
{"function": "\n\ndef add_user_to_subscription(self, user, notification_type, save=True):\n    for nt in NOTIFICATION_TYPES:\n        if (user in getattr(self, nt)):\n            if (nt != notification_type):\n                getattr(self, nt).remove(user)\n        elif (nt == notification_type):\n            getattr(self, nt).append(user)\n    if ((notification_type != 'none') and isinstance(self.owner, Node) and self.owner.parent_node):\n        user_subs = self.owner.parent_node.child_node_subscriptions\n        if (self.owner._id not in user_subs.setdefault(user._id, [])):\n            user_subs[user._id].append(self.owner._id)\n            self.owner.parent_node.save()\n    if save:\n        self.save()\n", "label": 1}

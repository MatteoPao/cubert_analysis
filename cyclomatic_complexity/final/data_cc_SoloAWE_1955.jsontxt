{"function": "\n\ndef __call__(self, *args, **kwargs):\n    try:\n        self._call(*args, **kwargs)\n    except Exception as err:\n        stats.incr('callback-failure', 1)\n        logging.exception('Callback Failed: %s', err)\n", "label": 0}
{"function": "\n\n@register.filter\ndef lookup(h, key):\n    try:\n        return h[key]\n    except KeyError:\n        return ''\n", "label": 0}
{"function": "\n\ndef process_response(self, request, response):\n    from django.conf import settings\n    from rest_framework.exceptions import PermissionDenied\n    from .access_control import upload_prefix_for_request\n    cookie_name = getattr(settings, 'UPLOAD_PREFIX_COOKIE_NAME', 'upload_prefix')\n    try:\n        response.set_cookie(cookie_name, upload_prefix_for_request(request))\n    except PermissionDenied:\n        response.delete_cookie(cookie_name)\n    return response\n", "label": 0}
{"function": "\n\ndef test_content_url_encoding_safe(self):\n    s = Site(self.SITE_PATH, config=self.config)\n    s.load()\n    path = '\".jpg/abc'\n    print(s.content_url(path, ''))\n    print(('/' + quote(path, '')))\n    assert (s.content_url(path, '') == ('/' + quote(path, '')))\n", "label": 0}
{"function": "\n\ndef getUpdatedBatchJob(self, maxWait):\n    while True:\n        try:\n            (jobID, status, wallTime) = self.updatedJobsQueue.get(timeout=maxWait)\n        except Empty:\n            return None\n        try:\n            self.runningJobs.remove(jobID)\n        except KeyError:\n            pass\n        else:\n            return (jobID, status, wallTime)\n", "label": 0}
{"function": "\n\ndef _validate_simple_authn(self, username, credentials):\n    '\\n        When the login() method is called, this method is used with the \\n        username and credentials (e.g., password, IP address, etc.). This\\n        method will only check the SimpleAuthn instances.\\n        '\n    try:\n        (login, role_name, user_auths) = self._db.retrieve_role_and_user_auths(username)\n    except DbUserNotFoundError:\n        return self._process_invalid()\n    errors = False\n    for user_auth in user_auths:\n        if user_auth.is_simple_authn():\n            try:\n                authenticated = user_auth.authenticate(login, credentials)\n            except:\n                log.log(LoginManager, log.level.Warning, ('Username: %s with user_auth %s: ERROR' % (login, user_auth)))\n                log.log_exc(LoginManager, log.level.Warning)\n                errors = True\n                traceback.print_exc()\n                continue\n            if authenticated:\n                log.log(LoginManager, log.level.Debug, ('Username: %s with user_auth %s: SUCCESS' % (login, user_auth)))\n                return ValidDatabaseSessionId(login, role_name)\n            else:\n                log.log(LoginManager, log.level.Warning, ('Username: %s with user_auth %s: FAIL' % (login, user_auth)))\n    if errors:\n        raise LoginErrors.LoginError('Error checking credentials. Contact administrators!')\n    return self._process_invalid()\n", "label": 0}
{"function": "\n\ndef test_elemwise_thats_also_a_column():\n    t = symbol('t', 'var * {x: int, time: datetime, y: int}')\n    expr = t[(t.x > 0)].time.truncate(months=1)\n    expected = t[['time', 'x']]\n    result = lean_projection(expr)\n    assert result._child._child._child.isidentical(t[['time', 'x']])\n", "label": 0}
{"function": "\n\ndef test_error_load_single_field_type(single_schema):\n    (data, errors) = single_schema.load({\n        'child': {\n            'id': 'foo',\n        },\n    })\n    assert (not data)\n    assert (errors == {\n        'child': {\n            'id': [fields.Integer().error_messages['invalid']],\n        },\n    })\n", "label": 0}
{"function": "\n\ndef test_reraise(self):\n    self.assertRaises(RuntimeError, reraise, RuntimeError, RuntimeError())\n    try:\n        raise RuntimeError('bla')\n    except Exception:\n        exc_info = sys.exc_info()\n    self.assertRaises(RuntimeError, reraise, *exc_info)\n", "label": 0}
{"function": "\n\ndef format_author(self, entry):\n    try:\n        persons = entry.persons['author']\n        if (sys.version_info[0] == 2):\n            authors = [unicode(au) for au in persons]\n        elif (sys.version_info[0] == 3):\n            authors = [str(au) for au in persons]\n    except KeyError:\n        authors = ['']\n    authors = self.strip_chars('; '.join(authors))\n    return authors\n", "label": 0}
{"function": "\n\ndef synchro_connect(self):\n    try:\n        self.synchronize(self.delegate.open)()\n    except AutoReconnect as e:\n        raise ConnectionFailure(str(e))\n", "label": 0}
{"function": "\n\ndef test_try_failure_bad_arg(self):\n    rv = self.app.get('/trivial_fn?nothing=1')\n    assert (rv.status_code == 200)\n    data = rv.data.decode('utf8')\n    jsn = json.loads(data)\n    assert (jsn['success'] == False), 'We expect this call failed as it has the wrong argument'\n", "label": 0}
{"function": "\n\ndef test_delete_all_lines_inversed(self):\n    text = '1\\n22\\n3\\n44\\n5\\n66\\n'\n    self.fillAndClear(text)\n    self.buffer.delete(Range(6, 1))\n    assert (str(self.buffer) == '')\n    assert (self.buffer.lines == [])\n    assert (self.deleted('afterPosition') == Position(1, 1))\n    assert (self.deleted('startPosition') == Position(7, 1))\n", "label": 0}
{"function": "\n\ndef run_osprey(self, config):\n    '\\n        Run osprey-worker.\\n\\n        Parameters\\n        ----------\\n        config : str\\n            Configuration string.\\n        '\n    (fh, filename) = tempfile.mkstemp(dir=self.temp_dir)\n    with open(filename, 'wb') as f:\n        f.write(config)\n    args = Namespace(config=filename, n_iters=1, output='json')\n    execute_worker.execute(args, None)\n    dump = json.loads(execute_dump.execute(args, None))\n    assert (len(dump) == 1)\n    assert (dump[0]['status'] == 'SUCCEEDED'), dump[0]['status']\n", "label": 0}
{"function": "\n\ndef test_download_file_proxies_to_transfer_object(self):\n    with mock.patch('boto3.s3.inject.S3Transfer') as transfer:\n        inject.download_file(mock.sentinel.CLIENT, Bucket='bucket', Key='key', Filename='filename')\n        transfer.return_value.download_file.assert_called_with(bucket='bucket', key='key', filename='filename', extra_args=None, callback=None)\n", "label": 0}
{"function": "\n\ndef test_no_repeats(self):\n    with self.assertNumQueries(2):\n        authors = Author.objects.sql_calc_found_rows().sql_calc_found_rows()[:5]\n        list(authors)\n        assert (authors.found_rows == 10)\n", "label": 0}
{"function": "\n\ndef test_reindex():\n    s = pd.Series([0.5, 1.0, 1.5], index=[2, 1, 3])\n    s2 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n    assert (list(reindex(s, s2).values) == [1.0, 0.5, 1.5])\n", "label": 0}
{"function": "\n\ndef _is_numeric(self, value):\n    try:\n        int(value)\n    except (TypeError, ValueError):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef set_service_target(context, policy_target_id, relationship):\n    session = context.session\n    with session.begin(subtransactions=True):\n        owner = ServiceTarget(policy_target_id=policy_target_id, servicechain_instance_id=context.instance['id'], servicechain_node_id=context.current_node['id'], position=context.current_position, relationship=relationship)\n        session.add(owner)\n", "label": 0}
{"function": "\n\ndef test_add_listener_exception(self):\n    cap = [':candidate']\n    obj = Session(cap)\n    listener = Session(None)\n    with self.assertRaises(SessionError):\n        obj.add_listener(listener)\n", "label": 0}
{"function": "\n\ndef test_scan_clear_product(self):\n    with HTTMock(wechat_api_mock):\n        res = self.client.scan.clear_product('ean13', '6900873042720')\n    self.assertEqual(0, res['errcode'])\n", "label": 0}
{"function": "\n\ndef __init__(self, dateTime, frequency):\n    super(IntraDayRange, self).__init__()\n    assert isinstance(frequency, int)\n    assert (frequency > 1)\n    assert (frequency < bar.Frequency.DAY)\n    ts = int(dt.datetime_to_timestamp(dateTime))\n    slot = int((ts / frequency))\n    slotTs = (slot * frequency)\n    self.__begin = dt.timestamp_to_datetime(slotTs, (not dt.datetime_is_naive(dateTime)))\n    if (not dt.datetime_is_naive(dateTime)):\n        self.__begin = dt.localize(self.__begin, dateTime.tzinfo)\n    self.__end = (self.__begin + datetime.timedelta(seconds=frequency))\n", "label": 0}
{"function": "\n\ndef sample(problem, N, num_levels, grid_jump, optimal_trajectories=None, local_optimization=False):\n    \"Generates model inputs using for Method of Morris.\\n    \\n    Returns a NumPy matrix containing the model inputs required for Method of\\n    Morris.  The resulting matrix has N rows and D columns, where D is the\\n    number of parameters.  These model inputs are intended to be used with\\n    :func:`SALib.analyze.morris.analyze`.\\n    \\n    Three variants of Morris' sampling for elementary effects is supported:\\n    \\n    - Vanilla Morris\\n    - Optimised trajectories when optimal_trajectories is set (using \\n      Campolongo's enhancements from 2007 and optionally Ruano's enhancement from 2012)\\n    - Groups with optimised trajectories when optimal_trajectores is set and \\n      the problem definition specifies groups\\n    \\n    At present, optimised trajectories is implemented using a brute-force\\n    approach, which can be very slow, especially if you require more than four\\n    trajectories.  Note that the number of factors makes little difference,\\n    but the ratio between number of optimal trajectories and the sample size\\n    results in an exponentially increasing number of scores that must be\\n    computed to find the optimal combination of trajectories.  We suggest going\\n    no higher than 4 from a pool of 100 samples.\\n    \\n    Update: With local_optimization = True, it is possible to go higher than the previously suggested 4 from 100.\\n    \\n    Parameters\\n    ----------\\n    problem : dict\\n        The problem definition\\n    N : int\\n        The number of samples to generate\\n    num_levels : int\\n        The number of grid levels\\n    grid_jump : int\\n        The grid jump size\\n    optimal_trajectories : int\\n        The number of optimal trajectories to sample (between 2 and N)\\n    local_optimization : bool\\n        Flag whether to use local optimization according to Ruano et al. (2012) \\n        Speeds up the process tremendously for bigger N and num_levels.\\n        Stating this variable to be true causes the function to ignore gurobi.\\n    \"\n    if (grid_jump >= num_levels):\n        raise ValueError('grid_jump must be less than num_levels')\n    if problem.get('groups'):\n        sample = sample_groups(problem, N, num_levels, grid_jump)\n    else:\n        sample = sample_oat(problem, N, num_levels, grid_jump)\n    if optimal_trajectories:\n        assert (type(optimal_trajectories) == int), 'Number of optimal trajectories should be an integer'\n        if (optimal_trajectories < 2):\n            raise ValueError('The number of optimal trajectories must be set to 2 or more.')\n        if (optimal_trajectories >= N):\n            raise ValueError('The number of optimal trajectories should be less than the number of samples.')\n        if ((_has_gurobi == False) and (local_optimization == False) and (optimal_trajectories > 10)):\n            raise ValueError('Running optimal trajectories greater than values of 10 will take a long time.')\n        sample = compute_optimised_trajectories(problem, sample, N, optimal_trajectories, local_optimization)\n    scale_samples(sample, problem['bounds'])\n    return sample\n", "label": 0}
{"function": "\n\ndef test_deprecated_simple(self):\n\n    @deprecated()\n    def f(arg):\n        return arg\n    ARG = object()\n    with warnings.catch_warnings(record=True) as recorded:\n        returned = f(ARG)\n    self.assertIs(returned, ARG)\n    self.assertEqual(len(recorded), 1)\n", "label": 0}
{"function": "\n\ndef load_proposal_rlp(self, blockhash):\n    try:\n        prlp = self.chainservice.db.get(('blockproposal:%s' % blockhash))\n        assert isinstance(prlp, bytes)\n        return prlp\n    except KeyError:\n        return None\n", "label": 0}
{"function": "\n\ndef test_realtime_with_batch_computation(self):\n    with self._get_swap_context():\n        user_id = 'uid'\n        exp_id = 'eid'\n        self.save_new_valid_exploration(exp_id, 'owner')\n        feedback_services.create_thread(exp_id, 'a_state_name', None, 'a subject', 'some text')\n        self.process_and_flush_pending_tasks()\n        ModifiedFeedbackAnalyticsAggregator.start_computation()\n        self.assertEqual(self.count_jobs_in_taskqueue(), 1)\n        self.process_and_flush_pending_tasks()\n        ModifiedFeedbackAnalyticsAggregator.stop_computation(user_id)\n        self.assertEqual(self.count_jobs_in_taskqueue(), 0)\n        self._flush_tasks_and_check_analytics(exp_id, {\n            'num_open_threads': 1,\n            'num_total_threads': 1,\n        })\n        feedback_services.create_thread(exp_id, 'a_state_name', None, 'a subject', 'some text')\n        self._flush_tasks_and_check_analytics(exp_id, {\n            'num_open_threads': 2,\n            'num_total_threads': 2,\n        })\n", "label": 0}
{"function": "\n\ndef test_dispose1():\n    h = event.HasEvents()\n\n    @h.connect('x1', 'x2')\n    def handler(*events):\n        pass\n    handler_ref = weakref.ref(handler)\n    del handler\n    gc.collect()\n    assert (handler_ref() is not None)\n    handler_ref().dispose()\n    gc.collect()\n    assert (handler_ref() is None)\n", "label": 0}
{"function": "\n\ndef test_issue(self):\n    'Show that one can retrieve the associated issue of a PR.'\n    cassette_name = self.cassette_name('issue')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        issue = p.issue()\n        assert isinstance(issue, github3.issues.Issue)\n", "label": 0}
{"function": "\n\ndef facettupletrees(table, key, start='start', stop='stop', value=None):\n    '\\n    Construct faceted interval trees for the given table, where each node in\\n    the tree is a row of the table.\\n\\n    '\n    import intervaltree\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    assert (start in flds), 'start field not recognised'\n    assert (stop in flds), 'stop field not recognised'\n    getstart = itemgetter(flds.index(start))\n    getstop = itemgetter(flds.index(stop))\n    if (value is None):\n        getvalue = tuple\n    else:\n        valueindices = asindices(hdr, value)\n        assert (len(valueindices) > 0), 'invalid value field specification'\n        getvalue = itemgetter(*valueindices)\n    keyindices = asindices(hdr, key)\n    assert (len(keyindices) > 0), 'invalid key'\n    getkey = itemgetter(*keyindices)\n    trees = dict()\n    for row in it:\n        k = getkey(row)\n        if (k not in trees):\n            trees[k] = intervaltree.IntervalTree()\n        trees[k].addi(getstart(row), getstop(row), getvalue(row))\n    return trees\n", "label": 0}
{"function": "\n\ndef test_sys_stderr_should_have_no_output_when_no_logger_is_set(memcached):\n    mc = cmemcached.Client([memcached])\n    with patch('sys.stderr') as mock_stderr:\n        mc.get('test_key_with_no_logger')\n        mc.set('test_key_with_no_logger', 'test_value_with_no_logger')\n        assert (not mock_stderr.write.called)\n", "label": 0}
{"function": "\n\ndef test_get_children_duplicates(self):\n    from psutil._compat import defaultdict\n    table = defaultdict(int)\n    for p in psutil.process_iter():\n        try:\n            table[p.ppid] += 1\n        except psutil.Error:\n            pass\n    pid = max(sorted(table, key=(lambda x: table[x])))\n    p = psutil.Process(pid)\n    try:\n        c = p.get_children(recursive=True)\n    except psutil.AccessDenied:\n        pass\n    else:\n        self.assertEqual(len(c), len(set(c)))\n", "label": 0}
{"function": "\n\ndef __call__(self, driver):\n    try:\n        return _element_if_visible(_find_element(driver, self.locator))\n    except StaleElementReferenceException:\n        return False\n", "label": 0}
{"function": "\n\ndef _test_update_routing_table(self, is_snat_host=True):\n    router = l3_test_common.prepare_router_data()\n    uuid = router['id']\n    s_netns = ('snat-' + uuid)\n    q_netns = ('qrouter-' + uuid)\n    fake_route1 = {\n        'destination': '135.207.0.0/16',\n        'nexthop': '19.4.4.200',\n    }\n    calls = [mock.call('replace', fake_route1, q_netns)]\n    agent = l3_agent.L3NATAgent(HOSTNAME, self.conf)\n    ri = dvr_router.DvrEdgeRouter(agent, HOSTNAME, uuid, router, **self.ri_kwargs)\n    ri._update_routing_table = mock.Mock()\n    with mock.patch.object(ri, '_is_this_snat_host') as snat_host:\n        snat_host.return_value = is_snat_host\n        ri.update_routing_table('replace', fake_route1)\n        if is_snat_host:\n            ri._update_routing_table('replace', fake_route1, s_netns)\n            calls += [mock.call('replace', fake_route1, s_netns)]\n        ri._update_routing_table.assert_has_calls(calls, any_order=True)\n", "label": 0}
{"function": "\n\ndef install_ssl_certs(instances):\n    certs = []\n    if CONF.object_store_access.public_identity_ca_file:\n        certs.append(CONF.object_store_access.public_identity_ca_file)\n    if CONF.object_store_access.public_object_store_ca_file:\n        certs.append(CONF.object_store_access.public_object_store_ca_file)\n    if (not certs):\n        return\n    with context.ThreadGroup() as tg:\n        for inst in instances:\n            tg.spawn(('configure-ssl-cert-%s' % inst.instance_id), _install_ssl_certs, inst, certs)\n", "label": 0}
{"function": "\n\ndef test_node_site():\n    s = Site(TEST_SITE_ROOT)\n    r = RootNode(TEST_SITE_ROOT.child_folder('content'), s)\n    assert (r.site == s)\n    n = Node(r.source_folder.child_folder('blog'), r)\n    assert (n.site == s)\n", "label": 0}
{"function": "\n\ndef create_debianization(distribution):\n    if exists('debian'):\n        raise NotImplementedError()\n    name = distribution.get_name()\n    name = ('python-%s' % name.replace('_', '-').lower())\n    maintainer = distribution.get_maintainer()\n    maintainer_email = distribution.get_maintainer_email()\n    if (maintainer == 'UNKNOWN'):\n        maintainer = 'CH content team'\n    if (maintainer_email == 'UNKNOWN'):\n        maintainer_email = 'pg-content-dev@chconf.com'\n    maintainer = ('%s <%s>' % (maintainer, maintainer_email))\n    version = distribution.get_version()\n    if (not version):\n        version = '0.0.0'\n    now = datetime.now()\n    utcnow = datetime.utcnow()\n    tzdiff = get_tzdiff(now, utcnow)\n    nowstring = ('%s %s' % (now.strftime('%a, %d %b %Y %H:%M:%S'), tzdiff))\n    description = distribution.get_description()\n    description = description.strip().replace('\\n', '\\n ')\n    architecture = 'all'\n    if distribution.has_ext_modules():\n        architecture = 'any'\n    copytree(join(dirname(__file__), 'default_debianization'), 'debian')\n    for (root, dirs, files) in os.walk('debian'):\n        for f in files:\n            file = join(root, f)\n            with open(file) as fin:\n                content = fin.read()\n            for (key, value) in (('#NAME#', name), ('#MAINTAINER#', maintainer), ('#VERSION#', version), ('#DATE#', nowstring)):\n                content = content.replace(key, value)\n            with open(file, 'w') as fout:\n                fout.write(content)\n    cf = ControlFile(filename='debian/control')\n    src = cf.source\n    p = cf.packages[0]\n    src['Source'] = p['Package'] = name\n    src['Maintainer'] = maintainer\n    p['Description'] = description\n    p['Architecture'] = architecture\n    install_requires = distribution.install_requires\n    if install_requires:\n        for package in install_requires:\n            p['Depends'].append(parse_setuppy_dependency(package))\n    cf.dump('debian/control')\n", "label": 1}
{"function": "\n\ndef on_request(self, context, request):\n    if ('PowerView.ps1' == request.path[1:]):\n        request.send_response(200)\n        request.end_headers()\n        with open('data/PowerSploit/Recon/PowerView.ps1', 'r') as ps_script:\n            ps_script = obfs_ps_script(ps_script.read())\n            request.wfile.write(ps_script)\n    else:\n        request.send_response(404)\n        request.end_headers()\n", "label": 0}
{"function": "\n\ndef test_column_expr(self):\n    c = Column('x', Integer)\n    is_(inspect(c), c)\n    assert (not c.is_selectable)\n    assert (not hasattr(c, 'selectable'))\n", "label": 0}
{"function": "\n\ndef test_oldPythonPy3(self):\n    '\\n        L{_checkRequirements} raises L{ImportError} when run on a version of\\n        Python that is too old.\\n        '\n    sys.version_info = self.Py3unsupportedPythonVersion\n    with self.assertRaises(ImportError) as raised:\n        _checkRequirements()\n    self.assertEqual(('Twisted on Python 3 requires Python %d.%d or later.' % self.Py3supportedPythonVersion), str(raised.exception))\n", "label": 0}
{"function": "\n\ndef _apply_filters(self, query, count_query, joins, count_joins, filters):\n    for (idx, flt_name, value) in filters:\n        flt = self._filters[idx]\n        alias = None\n        count_alias = None\n        if isinstance(flt, sqla_filters.BaseSQLAFilter):\n            path = self._filter_joins.get(flt.column, [])\n            (query, joins, alias) = self._apply_path_joins(query, joins, path, inner_join=False)\n            if (count_query is not None):\n                (count_query, count_joins, count_alias) = self._apply_path_joins(count_query, count_joins, path, inner_join=False)\n        clean_value = flt.clean(value)\n        try:\n            query = flt.apply(query, clean_value, alias)\n        except TypeError:\n            spec = inspect.getargspec(flt.apply)\n            if (len(spec.args) == 3):\n                warnings.warn(('Please update your custom filter %s to include additional `alias` parameter.' % repr(flt)))\n            else:\n                raise\n            query = flt.apply(query, clean_value)\n        if (count_query is not None):\n            try:\n                count_query = flt.apply(count_query, clean_value, count_alias)\n            except TypeError:\n                count_query = flt.apply(count_query, clean_value)\n    return (query, count_query, joins, count_joins)\n", "label": 0}
{"function": "\n\n@property\ndef vcf(self):\n    'serialize to VCARD as specified in RFC2426,\\n        if no UID is specified yet, one will be added (as a UID is mandatory\\n        for carddav as specified in RFC6352\\n        TODO make shure this random uid is unique'\n    import string\n    import random\n\n    def generate_random_uid():\n        \"generate a random uid, when random isn't broken, getting a\\n            random UID from a pool of roughly 10^56 should be good enough\"\n        choice = (string.ascii_uppercase + string.digits)\n        return ''.join([random.choice(choice) for _ in range(36)])\n    if ('UID' not in self.keys()):\n        self['UID'] = [(generate_random_uid(), dict())]\n    collector = list()\n    collector.append('BEGIN:VCARD')\n    collector.append('VERSION:3.0')\n    for key in ['FN', 'N']:\n        try:\n            collector.append(((key + ':') + self[key][0][0]))\n        except IndexError:\n            collector.append((key + ':'))\n    for prop in self.alt_keys():\n        for line in self[prop]:\n            types = self._line_helper(line)\n            collector.append((((prop + types) + ':') + line[0]))\n    collector.append('END:VCARD')\n    return '\\n'.join(collector)\n", "label": 0}
{"function": "\n\ndef draw_outlines(context, box, enable_hinting):\n    width = box.style.outline_width\n    color = box.style.get_color('outline_color')\n    style = box.style.outline_style\n    if ((box.style.visibility == 'visible') and (width != 0) and (color.alpha != 0)):\n        outline_box = ((box.border_box_x() - width), (box.border_box_y() - width), (box.border_width() + (2 * width)), (box.border_height() + (2 * width)))\n        for side in SIDES:\n            with stacked(context):\n                clip_border_segment(context, enable_hinting, style, width, side, outline_box)\n                draw_rect_border(context, outline_box, (4 * (width,)), style, styled_color(style, color, side))\n    if isinstance(box, boxes.ParentBox):\n        for child in box.children:\n            if isinstance(child, boxes.Box):\n                draw_outlines(context, child, enable_hinting)\n", "label": 0}
{"function": "\n\ndef test_existing_spawn(self):\n    child = pexpect.spawnu('bash', timeout=5, echo=False)\n    repl = replwrap.REPLWrapper(child, re.compile('[$#]'), \"PS1='{0}' PS2='{1}' PROMPT_COMMAND=''\")\n    res = repl.run_command('echo $HOME')\n    assert res.startswith('/'), res\n", "label": 0}
{"function": "\n\ndef freeze(self, skipSet=None):\n    assert (len(self()) in self.allowedSize)\n    return StringStream.freeze(self, skipSet=skipSet)\n", "label": 0}
{"function": "\n\ndef test_write_job3():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'write': ['test.test', 'test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef get_dates(self, resource):\n    '\\n        Retrieve dates from mercurial\\n        '\n    try:\n        commits = subprocess.check_output(['hg', 'log', '--template={date|isodatesec}\\n', resource.path]).split('\\n')\n        commits = commits[:(- 1)]\n    except subprocess.CalledProcessError:\n        self.logger.warning(('Unable to get mercurial history for [%s]' % resource))\n        commits = None\n    if (not commits):\n        self.logger.warning(('No mercurial history for [%s]' % resource))\n        return (None, None)\n    created = parse(commits[(- 1)].strip())\n    modified = parse(commits[0].strip())\n    return (created, modified)\n", "label": 0}
{"function": "\n\ndef _unit_file(self, name):\n    for extension in ['service', 'yaml']:\n        file_path = '{0}.{1}'.format(name, extension)\n        if path.exists(file_path):\n            with open(file_path) as handle:\n                if (extension == 'service'):\n                    return handle.read()\n                data = yaml.load(handle)\n                if (self._global and ('global' in data)):\n                    return data['global']\n                if (self._name in data):\n                    return data[self._name]\n                raise ValueError('No unit found for {0}'.format(self._name))\n    raise ValueError('No unit file: '.format(name))\n", "label": 0}
{"function": "\n\ndef test_key_has_correct_repr(self):\n    '\\n        Calling repr on a Key instance returns the proper string.\\n        '\n    key = pem.Key(b'test')\n    assert ('<Key({0})>'.format(TEST_DIGEST) == repr(key))\n", "label": 0}
{"function": "\n\ndef do_access_token_response(self, access_token, atinfo, state, refresh_token=None):\n    _tinfo = {\n        'access_token': access_token,\n        'expires_in': atinfo['exp'],\n        'token_type': 'bearer',\n        'state': state,\n    }\n    try:\n        _tinfo['scope'] = atinfo['scope']\n    except KeyError:\n        pass\n    if refresh_token:\n        _tinfo['refresh_token'] = refresh_token\n    return AccessTokenResponse(**by_schema(AccessTokenResponse, **_tinfo))\n", "label": 0}
{"function": "\n\ndef test_handle_error_401_sends_challege_default_realm(self):\n    api = restplus.Api(self.app, serve_challenge_on_401=True)\n    exception = HTTPException()\n    exception.code = 401\n    exception.data = {\n        'foo': 'bar',\n    }\n    with self.app.test_request_context('/foo'):\n        resp = api.handle_error(exception)\n        self.assertEqual(resp.status_code, 401)\n        self.assertEqual(resp.headers['WWW-Authenticate'], 'Basic realm=\"flask-restplus\"')\n", "label": 0}
{"function": "\n\ndef test_coerce_on_select(nyc):\n    t = symbol('t', discover(nyc))\n    t = t[(((((((((t.pickup_latitude >= 40.477399) & (t.pickup_latitude <= 40.917577)) & (t.dropoff_latitude >= 40.477399)) & (t.dropoff_latitude <= 40.917577)) & (t.pickup_longitude >= (- 74.25909))) & (t.pickup_longitude <= (- 73.700272))) & (t.dropoff_longitude >= (- 74.25909))) & (t.dropoff_longitude <= (- 73.700272))) & (t.passenger_count < 6))]\n    t = transform(t, pass_count=(t.passenger_count + 1))\n    result = compute(t.pass_count.coerce('float64'), nyc, return_type='native')\n    s = odo(result, pd.Series)\n    expected = (compute(t, nyc, return_type=pd.DataFrame).passenger_count.astype('float64') + 1.0)\n    assert (list(s) == list(expected))\n", "label": 0}
{"function": "\n\ndef get_dir(self, path, dest='', saltenv='base', gzip=None, cachedir=None):\n    '\\n        Get a directory recursively from the salt-master\\n        '\n    ret = []\n    path = self._check_proto(path).rstrip('/')\n    separated = path.rsplit('/', 1)\n    if (len(separated) != 2):\n        prefix = ''\n    else:\n        prefix = separated[0]\n    for fn_ in self.file_list(saltenv, prefix=path):\n        try:\n            if (fn_[len(path)] != '/'):\n                continue\n        except IndexError:\n            continue\n        minion_relpath = fn_[len(prefix):].lstrip('/')\n        ret.append(self.get_file(salt.utils.url.create(fn_), '{0}/{1}'.format(dest, minion_relpath), True, saltenv, gzip))\n    try:\n        for fn_ in self.file_list_emptydirs(saltenv, prefix=path):\n            try:\n                if (fn_[len(path)] != '/'):\n                    continue\n            except IndexError:\n                continue\n            minion_relpath = fn_[len(prefix):].lstrip('/')\n            minion_mkdir = '{0}/{1}'.format(dest, minion_relpath)\n            if (not os.path.isdir(minion_mkdir)):\n                os.makedirs(minion_mkdir)\n            ret.append(minion_mkdir)\n    except TypeError:\n        pass\n    ret.sort()\n    return ret\n", "label": 0}
{"function": "\n\n@require_creds(True)\n@rpcmethod(signature=[SUCCESS_TYPE, URN_TYPE, CREDENTIALS_TYPE], url_name='openflow_gapi')\ndef DeleteSliver(slice_urn, credentials, **kwargs):\n    logger.debug('Called DeleteSliver')\n    try:\n        return gapi.DeleteSliver(slice_urn, kwargs['request'].user)\n    except Slice.DoesNotExist:\n        no_such_slice(slice_urn)\n", "label": 0}
{"function": "\n\ndef identity_provider_create(request, idp_id, description=None, enabled=False, remote_ids=None):\n    manager = keystoneclient(request, admin=True).federation.identity_providers\n    try:\n        return manager.create(id=idp_id, description=description, enabled=enabled, remote_ids=remote_ids)\n    except keystone_exceptions.Conflict:\n        raise exceptions.Conflict()\n", "label": 0}
{"function": "\n\ndef test_past_datetime(self):\n    value = self.sd.past_datetime()\n    self.assertTrue(isinstance(value, datetime.datetime))\n    self.assertTrue((value <= datetime.datetime.utcnow().replace(tzinfo=utc)))\n    self.assertTrue((value >= (datetime.datetime.utcnow().replace(tzinfo=utc) - datetime.timedelta(minutes=1440))))\n    value = self.sd.past_datetime(0, 10)\n    self.assertTrue((value <= datetime.datetime.utcnow().replace(tzinfo=utc)))\n    self.assertTrue((value >= (datetime.datetime.utcnow().replace(tzinfo=utc) - datetime.timedelta(minutes=10))))\n    with self.assertRaises(ParameterError):\n        self.sd.past_datetime(100, 0)\n    with self.assertRaises(ParameterError):\n        self.sd.past_datetime((- 10), 10)\n", "label": 0}
{"function": "\n\ndef _download_pdf(self, url, base_path):\n    local_file_path = os.path.join(base_path, 'billing-temp-document.pdf')\n    response = requests.get(url, stream=True)\n    should_wipe_bad_headers = True\n    with open(local_file_path, 'wb') as out_file:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                if should_wipe_bad_headers:\n                    pdf_header_pos = chunk.find('%PDF-')\n                    if (pdf_header_pos > 0):\n                        chunk = chunk[pdf_header_pos:]\n                    should_wipe_bad_headers = False\n                out_file.write(chunk)\n                out_file.flush()\n    return local_file_path\n", "label": 0}
{"function": "\n\ndef _dispatch(self, inst, kws):\n    assert (self.current_block is not None)\n    fname = ('op_%s' % inst.opname.replace('+', '_'))\n    try:\n        fn = getattr(self, fname)\n    except AttributeError:\n        raise NotImplementedError(inst)\n    else:\n        try:\n            return fn(inst, **kws)\n        except errors.NotDefinedError as e:\n            if (e.loc is None):\n                e.loc = self.loc\n            raise e\n", "label": 0}
{"function": "\n\ndef test_review_comments(self):\n    \"Show that one can iterate over a PR's review comments.\"\n    cassette_name = self.cassette_name('review_comments')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        for comment in p.review_comments():\n            assert isinstance(comment, github3.pulls.ReviewComment)\n", "label": 0}
{"function": "\n\ndef test_input_extra_rewrite(self):\n    self.client_job_description.rewrite_paths = True\n    extra_file = os.path.join(self.input1_files_path, 'moo', 'cow.txt')\n    os.makedirs(os.path.dirname(extra_file))\n    open(extra_file, 'w').write('Hello World!')\n    command_line = ('test.exe %s' % extra_file)\n    self.client_job_description.command_line = command_line\n    self.client.expect_command_line('test.exe /pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt')\n    self.client.expect_put_paths(['/pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt'])\n    self._submit()\n    uploaded_file1 = self.client.put_files[0]\n    assert (uploaded_file1[1] == 'input')\n    assert (uploaded_file1[0] == extra_file)\n", "label": 0}
{"function": "\n\ndef test_grad_s(self):\n    'tests that the gradients with respect to s_i are 0 after doing a mean field update of s_i '\n    model = self.model\n    e_step = self.e_step\n    X = self.X\n    assert (X.shape[0] == self.m)\n    model.test_batch_size = X.shape[0]\n    init_H = e_step.init_H_hat(V=X)\n    init_Mu1 = e_step.init_S_hat(V=X)\n    prev_setting = config.compute_test_value\n    config.compute_test_value = 'off'\n    (H, Mu1) = function([], outputs=[init_H, init_Mu1])()\n    config.compute_test_value = prev_setting\n    H = broadcast(H, self.m)\n    Mu1 = broadcast(Mu1, self.m)\n    H = np.cast[config.floatX](self.model.rng.uniform(0.0, 1.0, H.shape))\n    Mu1 = np.cast[config.floatX](self.model.rng.uniform((- 5.0), 5.0, Mu1.shape))\n    H_var = T.matrix(name='H_var')\n    H_var.tag.test_value = H\n    Mu1_var = T.matrix(name='Mu1_var')\n    Mu1_var.tag.test_value = Mu1\n    idx = T.iscalar()\n    idx.tag.test_value = 0\n    S = e_step.infer_S_hat(V=X, H_hat=H_var, S_hat=Mu1_var)\n    s_idx = S[:, idx]\n    s_i_func = function([H_var, Mu1_var, idx], s_idx)\n    sigma0 = (1.0 / model.alpha)\n    Sigma1 = e_step.infer_var_s1_hat()\n    mu0 = T.zeros_like(model.mu)\n    trunc_kl = ((- model.entropy_hs(H_hat=H_var, var_s0_hat=sigma0, var_s1_hat=Sigma1)) + model.expected_energy_vhs(V=X, H_hat=H_var, S_hat=Mu1_var, var_s0_hat=sigma0, var_s1_hat=Sigma1))\n    grad_Mu1 = T.grad(trunc_kl.sum(), Mu1_var)\n    grad_Mu1_idx = grad_Mu1[:, idx]\n    grad_func = function([H_var, Mu1_var, idx], grad_Mu1_idx)\n    for i in xrange(self.N):\n        Mu1[:, i] = s_i_func(H, Mu1, i)\n        g = grad_func(H, Mu1, i)\n        assert (not contains_nan(g))\n        g_abs_max = np.abs(g).max()\n        if (g_abs_max > self.tol):\n            raise Exception(((('after mean field step, gradient of kl divergence wrt mean field parameter should be 0, but here the max magnitude of a gradient element is ' + str(g_abs_max)) + ' after updating s_') + str(i)))\n", "label": 0}
{"function": "\n\n@app.task(bind=True)\n@only_one(key='analyze_databases_service_task', timeout=6000)\ndef analyze_databases(self, task_history=None):\n    (endpoint, healh_check_route, healh_check_string) = get_analyzing_credentials()\n    user = User.objects.get(username='admin')\n    worker_name = get_worker_name()\n    task_history = TaskHistory.register(task_history=task_history, request=self.request, user=user, worker_name=worker_name)\n    task_history.update_details(persist=True, details='Loading Process...')\n    AuditRequest.new_request('analyze_databases', user, 'localhost')\n    try:\n        analyze_service = AnalyzeService(endpoint, healh_check_route, healh_check_string)\n        with transaction.atomic():\n            databases = Database.objects.filter(is_in_quarantine=False)\n            today = datetime.now()\n            for database in databases:\n                (database_name, engine, instances, environment_name, databaseinfra_name) = setup_database_info(database)\n                for execution_plan in ExecutionPlan.objects.all():\n                    if database_can_not_be_resized(database, execution_plan):\n                        continue\n                    params = execution_plan.setup_execution_params()\n                    result = analyze_service.run(engine=engine, database=database_name, instances=instances, **params)\n                    if (result['status'] == 'success'):\n                        task_history.update_details(persist=True, details='\\nDatabase {} {} was analised.'.format(database, execution_plan.plan_name))\n                        if (result['msg'] != instances):\n                            continue\n                        for instance in result['msg']:\n                            insert_analyze_repository_record(today, database_name, instance, engine, databaseinfra_name, environment_name, execution_plan)\n                    else:\n                        raise Exception('Check your service logs..')\n        task_history.update_status_for(TaskHistory.STATUS_SUCCESS, details='Analisys ok!')\n    except Exception:\n        try:\n            task_history.update_details(persist=True, details='\\nDatabase {} {} could not be analised.'.format(database, execution_plan.plan_name))\n            task_history.update_status_for(TaskHistory.STATUS_ERROR, details='Analisys finished with errors!\\nError: {}'.format(result['msg']))\n        except UnboundLocalError:\n            task_history.update_details(persist=True, details='\\nProccess crashed')\n            task_history.update_status_for(TaskHistory.STATUS_ERROR, details='Analisys could not be started')\n    finally:\n        AuditRequest.cleanup_request()\n", "label": 0}
{"function": "\n\ndef create_initial_revisions(self, app, model_class, comment, batch_size, verbosity=2, database=None, **kwargs):\n    'Creates the set of initial revisions for the given model.'\n    try:\n        import_module(('%s.admin' % app.__name__.rsplit('.', 1)[0]))\n    except ImportError:\n        pass\n    if default_revision_manager.is_registered(model_class):\n        if (verbosity >= 2):\n            print(('Creating initial revision(s) for model %s ...' % force_text(model_class._meta.verbose_name)))\n        created_count = 0\n        content_type = ContentType.objects.db_manager(database).get_for_model(model_class)\n        versioned_pk_queryset = Version.objects.using(database).filter(content_type=content_type).all()\n        live_objs = model_class._default_manager.using(database).all()\n        if has_int_pk(model_class):\n            live_objs = live_objs.exclude(pk__in=versioned_pk_queryset.values_list('object_id_int', flat=True))\n        else:\n            live_objs = live_objs.exclude(pk__in=list(versioned_pk_queryset.values_list('object_id', flat=True).iterator()))\n        ids = list(live_objs.values_list(model_class._meta.pk.name, flat=True).order_by())\n        total = len(ids)\n        for i in range(0, total, batch_size):\n            chunked_ids = ids[i:(i + batch_size)]\n            objects = live_objs.in_bulk(chunked_ids)\n            for (id, obj) in objects.items():\n                try:\n                    default_revision_manager.save_revision((obj,), comment=comment, db=database)\n                except:\n                    print(('ERROR: Could not save initial version for %s %s.' % (model_class.__name__, obj.pk)))\n                    raise\n                created_count += 1\n            reset_queries()\n            if (verbosity >= 2):\n                print(('Created %s of %s.' % (created_count, total)))\n        if (verbosity >= 2):\n            print(('Created %s initial revision(s) for model %s.' % (created_count, force_text(model_class._meta.verbose_name))))\n    elif (verbosity >= 2):\n        print(('Model %s is not registered.' % force_text(model_class._meta.verbose_name)))\n", "label": 1}
{"function": "\n\ndef clean_message(self):\n    message = self.cleaned_data['message']\n    try:\n        message = message.decode('base64')\n    except TypeError as e:\n        raise ValidationError(('Cannot convert to binary: %r' % e.msg))\n    if (len(message) % 16):\n        raise ValidationError('Wrong block size for message !')\n    if (len(message) <= 16):\n        raise ValidationError('Message too short or missing IV !')\n    return message\n", "label": 0}
{"function": "\n\ndef _bump_version(self, version):\n    try:\n        parts = map(int, version.split('.'))\n    except ValueError:\n        self._fail('Current version is not numeric')\n    parts[(- 1)] += 1\n    return '.'.join(map(str, parts))\n", "label": 0}
{"function": "\n\ndef get_ud(self, cardinal, user, channel, msg):\n    try:\n        word = msg.split(' ', 1)[1]\n    except IndexError:\n        cardinal.sendMsg(channel, 'Syntax: .ud <word>')\n        return\n    try:\n        url = (URBANDICT_API_PREFIX + word)\n        f = urlopen(url).read()\n        data = json.loads(f)\n        word_def = data['list'][0]['definition']\n        link = data['list'][0]['permalink']\n        response = ('UD for %s: %s (%s)' % (word, word_def, link))\n        cardinal.sendMsg(channel, response.encode('utf-8'))\n    except Exception:\n        cardinal.sendMsg(channel, ('Could not retrieve definition for %s' % word))\n", "label": 0}
{"function": "\n\ndef store_and_use_artifact(self, cache_key, src, results_dir=None):\n    'Read the content of a tarball from an iterator and return an artifact stored in the cache.'\n    with self._tmpfile(cache_key, 'read') as tmp:\n        for chunk in src:\n            tmp.write(chunk)\n        tmp.close()\n        tarball = self._store_tarball(cache_key, tmp.name)\n        artifact = self._artifact(tarball)\n        if (results_dir is not None):\n            safe_rmtree(results_dir)\n        artifact.extract()\n        return True\n", "label": 0}
{"function": "\n\ndef _prepare_ivy_xml(self, frozen_resolution, ivyxml, resolve_hash_name_for_report):\n    default_resolution = frozen_resolution.get('default')\n    if (default_resolution is None):\n        raise IvyUtils.IvyError(\"Couldn't find the frozen resolution for the 'default' ivy conf.\")\n    try:\n        jars = default_resolution.jar_dependencies\n        IvyUtils.generate_fetch_ivy(jars, ivyxml, self.confs, resolve_hash_name_for_report)\n    except Exception as e:\n        raise IvyUtils.IvyError('Failed to prepare ivy resolve: {}'.format(e))\n", "label": 0}
{"function": "\n\ndef loadWordFile(self, pre_processor=None):\n    filename = self.getDictionaryPath()\n    with codecs.open(filename, 'r', 'utf-8') as fp:\n        for word in fp.readlines():\n            if pre_processor:\n                self.add(pre_processor(word.strip()))\n            else:\n                self.add(word.strip())\n    return\n", "label": 0}
{"function": "\n\ndef create_security_groups(self):\n    for hostdef in self.blueprint.host_definitions.all():\n        sg_name = 'stackdio-managed-{0}-stack-{1}'.format(hostdef.slug, self.pk)\n        sg_description = 'stackd.io managed security group'\n        account = hostdef.cloud_image.account\n        if (not account.create_security_groups):\n            logger.debug('Skipping creation of {0} because security group creation is turned off for the account'.format(sg_name))\n            continue\n        driver = account.get_driver()\n        try:\n            sg_id = driver.create_security_group(sg_name, sg_description, delete_if_exists=True)\n        except Exception as e:\n            err_msg = 'Error creating security group: {0}'.format(str(e))\n            self.set_status('create_security_groups', self.ERROR, err_msg, Level.ERROR)\n        logger.debug('Created security group {0}: {1}'.format(sg_name, sg_id))\n        for access_rule in hostdef.access_rules.all():\n            driver.authorize_security_group(sg_id, {\n                'protocol': access_rule.protocol,\n                'from_port': access_rule.from_port,\n                'to_port': access_rule.to_port,\n                'rule': access_rule.rule,\n            })\n        self.security_groups.create(account=account, blueprint_host_definition=hostdef, name=sg_name, description=sg_description, group_id=sg_id, is_managed=True)\n", "label": 0}
{"function": "\n\ndef test_perturb_inv(self):\n    pmat = perturb_inv(closure(self.cdata1), closure([0.1, 0.1, 0.1]))\n    imat = perturb(closure(self.cdata1), closure([10, 10, 10]))\n    npt.assert_allclose(pmat, imat)\n    pmat = perturb_inv(closure(self.cdata1), closure([1, 1, 1]))\n    npt.assert_allclose(pmat, closure([[0.2, 0.2, 0.6], [0.4, 0.4, 0.2]]))\n    pmat = perturb_inv(closure(self.cdata5), closure([0.1, 0.1, 0.1]))\n    imat = perturb(closure(self.cdata1), closure([10, 10, 10]))\n    npt.assert_allclose(pmat, imat)\n    with self.assertRaises(ValueError):\n        perturb_inv(closure(self.cdata1), self.bad1)\n    perturb_inv(self.cdata2, [1, 2, 3])\n    npt.assert_allclose(self.cdata2, np.array([2, 2, 6]))\n", "label": 0}
{"function": "\n\ndef get_style(self, attribute):\n    \"Get the document's named style at the caret's current position.\\n\\n        If there is a text selection and the style varies over the selection,\\n        `pyglet.text.document.STYLE_INDETERMINATE` is returned.\\n\\n        :Parameters:\\n            `attribute` : str\\n                Name of style attribute to retrieve.  See\\n                `pyglet.text.document` for a list of recognised attribute\\n                names.\\n\\n        :rtype: object\\n        \"\n    if ((self._mark is None) or (self._mark == self._position)):\n        try:\n            return self._next_attributes[attribute]\n        except KeyError:\n            return self._layout.document.get_style(attribute, self._position)\n    start = min(self._position, self._mark)\n    end = max(self._position, self._mark)\n    return self._layout.document.get_style_range(attribute, start, end)\n", "label": 0}
{"function": "\n\ndef __call__(self, feature=None):\n    if (not current_app):\n        log.warn(\"Got a request to check for {feature} but we're outside the request context. Returning False\".format(feature=feature))\n        return False\n    try:\n        return self.model.check(feature)\n    except NoResultFound:\n        raise NoFeatureFlagFound()\n", "label": 0}
{"function": "\n\ndef run_and_expect(self, joined_params, retcode, extra_args=['--local-scheduler', '--no-lock']):\n    with self.assertRaises(SystemExit) as cm:\n        luigi_run((joined_params.split(' ') + extra_args))\n    self.assertEqual(cm.exception.code, retcode)\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('model_class', (ModelWithVanillaMoneyField, ModelWithChoicesMoneyField))\ndef test_currency_querying(self, model_class):\n    model_class.objects.create(money=Money('100.0', moneyed.ZWN))\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.USD)).count() == 0)\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.ZWN)).count() == 1)\n", "label": 0}
{"function": "\n\ndef _create_hdfs_workflow_dir(self, where, job):\n    constructed_dir = ('/user/%s/' % self.get_hdfs_user())\n    constructed_dir = self._add_postfix(constructed_dir)\n    constructed_dir += ('%s/%s' % (job.name, six.text_type(uuid.uuid4())))\n    with remote.get_remote(where) as r:\n        self.create_hdfs_dir(r, constructed_dir)\n    return constructed_dir\n", "label": 0}
{"function": "\n\n@mock.patch('pushmanager.servlets.testtag.urllib2.urlopen')\ndef test_generate_test_tag_no_url(self, mock_urlopen):\n    m = mock.Mock()\n    m.read.side_effect = ['{\"tag\" : \"tag 0 fails\"}', '{\"url\" : \"\"}']\n    mock_urlopen.return_value = m\n    MockedSettings['tests_tag'] = {\n        \n    }\n    MockedSettings['tests_tag']['tag'] = 'test'\n    MockedSettings['tests_tag']['tag_api_endpoint'] = 'example.com'\n    MockedSettings['tests_tag']['tag_api_body'] = '{ \"sha\" : \"%SHA%\" }'\n    MockedSettings['tests_tag']['url_api_endpoint'] = 'http://example.com/api/v1/test_results_url'\n    MockedSettings['tests_tag']['url_api_body'] = '{ \"sha\" : \"%SHA%\" }'\n    MockedSettings['tests_tag']['url_tmpl'] = 'www.example.com/%ID%'\n    request_info = {\n        'tags': 'test',\n        'branch': 'test',\n        'revision': 'abc123',\n    }\n    with mock.patch.dict(Settings, MockedSettings):\n        gen_tags = TestTagServlet._gen_test_tag_resp(request_info)\n        T.assert_equals({\n            'tag': 'tag 0 fails',\n            'url': '',\n        }, gen_tags)\n", "label": 0}
{"function": "\n\ndef query_lookupd(self):\n    self.logger.debug('querying lookupd...')\n    lookupd = next(self.iterlookupds)\n    try:\n        producers = lookupd.lookup(self.topic)['producers']\n        self.logger.debug(('found %d producers' % len(producers)))\n    except Exception as error:\n        msg = 'Failed to lookup %s on %s (%s)'\n        self.logger.warn((msg % (self.topic, lookupd.address, error)))\n        return\n    for producer in producers:\n        conn = Nsqd((producer.get('broadcast_address') or producer['address']), producer['tcp_port'], producer['http_port'], **self.conn_kwargs)\n        self.connect_to_nsqd(conn)\n", "label": 0}
{"function": "\n\ndef get_infra_name(host_id):\n    'Return DATABASE_INFRA_NAME'\n    from physical.models import Host\n    host = Host.objects.filter(id=host_id).select_related('instance').select_related('databaseinfra')\n    try:\n        host = host[0]\n    except IndexError as e:\n        LOG.warn('Host id does not exists: {}. {}'.format(host_id, e))\n        return None\n    return host.instance_set.all()[0].databaseinfra.name\n", "label": 0}
{"function": "\n\ndef full_clean(self):\n    '\\n        Cleans all of self.data and populates self._errors.\\n        '\n    self._errors = []\n    if (not self.is_bound):\n        return\n    for i in range(0, self.total_form_count()):\n        form = self.forms[i]\n        self._errors.append(form.errors)\n    try:\n        self.clean()\n    except ValidationError as e:\n        self._non_form_errors = self.error_class(e.messages)\n", "label": 0}
{"function": "\n\ndef test_deepcopy_shared_container(self):\n    (a, x) = T.scalars('ax')\n    h = function([In(a, value=0.0)], a)\n    f = function([x, In(a, value=h.container[a], implicit=True)], (x + a))\n    try:\n        memo = {\n            \n        }\n        ac = copy.deepcopy(a)\n        memo.update({\n            id(a): ac,\n        })\n        hc = copy.deepcopy(h, memo=memo)\n        memo.update({\n            id(h): hc,\n        })\n        fc = copy.deepcopy(f, memo=memo)\n    except NotImplementedError as e:\n        if e[0].startswith('DebugMode is not picklable'):\n            return\n        else:\n            raise\n    h[a] = 1\n    hc[ac] = 2\n    self.assertTrue((f[a] == 1))\n    self.assertTrue((fc[ac] == 2))\n", "label": 0}
{"function": "\n\ndef test_rerun_after_depletion_calls_once(self):\n    'Ensure MessageIterator works when used manually.'\n    from furious.batcher import MessageIterator\n    payload = '[\"test\"]'\n    task = Mock(payload=payload, tag='tag')\n    iterator = MessageIterator('tag', 'qn', 1)\n    with patch.object(iterator, 'queue') as queue:\n        queue.lease_tasks_by_tag.return_value = [task]\n        results = [payload for payload in iterator]\n        self.assertEqual(results, [payload])\n        results = [payload for payload in iterator]\n    queue.lease_tasks_by_tag.assert_called_once_with(60, 1, tag='tag', deadline=10)\n", "label": 0}
{"function": "\n\ndef update(self, action, action_id):\n    updated_action = {\n        \n    }\n    updated_action['freezer_action'] = utils.create_dict(**action)\n    try:\n        if (action['mandatory'] != ''):\n            updated_action['mandatory'] = action['mandatory']\n    except KeyError:\n        pass\n    try:\n        if (action['max_retries'] != ''):\n            updated_action['max_retries'] = action['max_retries']\n    except KeyError:\n        pass\n    try:\n        if (action['max_retries_interval'] != ''):\n            updated_action['max_retries_interval'] = action['max_retries_interval']\n    except KeyError:\n        pass\n    return self.client.actions.update(action_id, updated_action)\n", "label": 0}
{"function": "\n\ndef test_remove_unpickables_http_exception(self):\n    try:\n        urllib2.urlopen('http://localhost/this.does.not.exist')\n        self.fail('exception expected')\n    except urllib2.URLError as e:\n        pass\n    except urllib2.HTTPError as e:\n        pass\n    removed = mapper.remove_unpickables(e)\n    pickled = pickle.dumps(removed)\n    pickle.loads(pickled)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(CreateFlavorInfoAction, self).clean()\n    name = cleaned_data.get('name')\n    flavor_id = cleaned_data.get('flavor_id')\n    try:\n        flavors = api.nova.flavor_list(self.request, None)\n    except Exception:\n        flavors = []\n        msg = _('Unable to get flavor list')\n        exceptions.check_message(['Connection', 'refused'], msg)\n        raise\n    if (flavors is not None):\n        for flavor in flavors:\n            if (flavor.name == name):\n                raise forms.ValidationError((_('The name \"%s\" is already used by another flavor.') % name))\n            if (flavor.id == flavor_id):\n                raise forms.ValidationError((_('The ID \"%s\" is already used by another flavor.') % flavor_id))\n    return cleaned_data\n", "label": 0}
{"function": "\n\ndef test_start_run(self):\n    assert_false(os.path.exists(self.result_file_path))\n    self.run_results.start_run(self.scenario)\n    assert_equal(len(self.scenario.packb()), self._current_size())\n    assert_greater(self._current_size(), 0)\n    with open(self.result_file_path, 'rb') as f:\n        unpacker = msgpack.Unpacker(file_like=f)\n        got_scenario = Scenario.unpackb(unpacker)\n        for attr in ['name', '_scenario_data', 'user_count', 'operation_count', 'run_seconds', 'container_base', 'container_count', 'containers', 'container_concurrency', 'sizes_by_name', 'version', 'bench_size_thresholds']:\n            assert_equal(getattr(got_scenario, attr), getattr(self.scenario, attr))\n", "label": 0}
{"function": "\n\ndef __init__(self, n):\n    assert (n == 1)\n", "label": 0}
{"function": "\n\ndef get_extractor(coarse, fine):\n    log.debug(\"getting fine extractor for '{}: {}'\".format(coarse, fine))\n    try:\n        extractor = importlib.import_module(((__package__ + '.') + question_types[fine]))\n    except (ImportError, KeyError):\n        log.warn(\"Extractor for fine type '{}: {}' not implemented\".format(coarse, fine))\n        raise NoExtractorError(coarse, fine)\n    return extractor.Extractor\n", "label": 0}
{"function": "\n\ndef _get_method(self, request, action, content_type, body):\n    'Look up the action-specific method and its extensions.'\n    try:\n        if (not self.controller):\n            meth = getattr(self, action)\n        else:\n            meth = getattr(self.controller, action)\n    except AttributeError:\n        if ((not self.wsgi_actions) or (action not in (_ROUTES_METHODS + ['action']))):\n            raise\n    else:\n        return (meth, self.wsgi_extensions.get(action, []))\n    if (action == 'action'):\n        action_name = action_peek(body)\n    else:\n        action_name = action\n    return (self.wsgi_actions[action_name], self.wsgi_action_extensions.get(action_name, []))\n", "label": 0}
{"function": "\n\ndef test_id(self):\n    'Each test annotation should be created with a unique ID.'\n    annotation_1 = factories.Annotation()\n    annotation_2 = factories.Annotation()\n    assert annotation_1.get('id')\n    assert annotation_2.get('id')\n    assert (annotation_1['id'] != annotation_2['id'])\n", "label": 0}
{"function": "\n\n@httprettified\ndef test_likes_with_after(self):\n    HTTPretty.register_uri(HTTPretty.GET, 'https://api.tumblr.com/v2/user/likes', body='{\"meta\": {\"status\": 200, \"msg\": \"OK\"}, \"response\": []}')\n    response = self.client.likes(after=1418684291)\n    assert (response == [])\n", "label": 0}
{"function": "\n\ndef do_undef(self, t):\n    '\\n        Default handling of a #undef line.\\n        '\n    try:\n        del self.cpp_namespace[t[1]]\n    except KeyError:\n        pass\n", "label": 0}
{"function": "\n\ndef run(self, command_line=''):\n    assert command_line, 'expected non-empty command line'\n    parsed = parse_command_line(command_line)\n    self.window.run_command('tab_control', {\n        'command': 'only',\n        'forced': parsed.command.forced,\n    })\n", "label": 0}
{"function": "\n\n@testing.requires.threading_with_mock\n@testing.requires.timing_intensive\ndef test_timeout_race(self):\n    dbapi = MockDBAPI()\n    p = pool.QueuePool(creator=(lambda : dbapi.connect(delay=0.05)), pool_size=2, max_overflow=1, use_threadlocal=False, timeout=3)\n    timeouts = []\n\n    def checkout():\n        for x in range(1):\n            now = time.time()\n            try:\n                c1 = p.connect()\n            except tsa.exc.TimeoutError:\n                timeouts.append((time.time() - now))\n                continue\n            time.sleep(4)\n            c1.close()\n    threads = []\n    for i in range(10):\n        th = threading.Thread(target=checkout)\n        th.start()\n        threads.append(th)\n    for th in threads:\n        th.join(join_timeout)\n    assert (len(timeouts) > 0)\n    for t in timeouts:\n        assert (t >= 3), ('Not all timeouts were >= 3 seconds %r' % timeouts)\n        assert (t < 14), ('Not all timeouts were < 14 seconds %r' % timeouts)\n", "label": 0}
{"function": "\n\ndef test_length(session):\n    set_ = session.set(key('test_sortedset_length'), S('abc'), SortedSet)\n    assert (len(set_) == 3)\n    setx = session.set(key('test_sortedsetx_length'), S([1, 2, 3]), IntSet)\n    assert (len(setx) == 3)\n", "label": 0}
{"function": "\n\n@mock.patch.object(manager_utils, 'node_power_action', autospec=True)\ndef test_tear_down(self, node_power_action_mock):\n    with task_manager.acquire(self.context, self.node.uuid, shared=False) as task:\n        state = task.driver.deploy.tear_down(task)\n        self.assertEqual(state, states.DELETED)\n        node_power_action_mock.assert_called_once_with(task, states.POWER_OFF)\n", "label": 0}
{"function": "\n\ndef test_skips_unknown_permission_quietly(self):\n    'Skips unknown permission silently with verbosity 0.'\n    with patch('moztrap.model.core.management.commands.create_default_roles.ROLES', {\n        'Foo': ['foo.foo'],\n    }):\n        output = self.call_command(verbosity=0)\n    self.assertEqual(output, '')\n", "label": 0}
{"function": "\n\ndef cold_evacuate(config, compute_api, instance_id, dst_host):\n    '\\n    Evacuate VM by shutting it down, booting another VM with same ephemeral\\n    volume on different host and deleting original VM.\\n\\n    :param config: CloudFerry configuration\\n    :param compute_api: Compute API client (NovaClient) instance\\n    :param instance_id: VM instance identifier to evacuate\\n    :param dst_host: destination host name\\n    '\n    LOG.debug('Cold evacuating VM %s to %s', instance_id, dst_host)\n    state_change_timeout = cfglib.CONF.evacuation.state_change_timeout\n    migration_timeout = cfglib.CONF.evacuation.migration_timeout\n    if (not change_to_pre_migration_state(compute_api, instance_id)):\n        instance = compute_api.servers.get(instance_id)\n        LOG.warning(\"Can't migrate VM in %s status\", instance.status)\n        return\n    instance = compute_api.servers.get(instance_id)\n    src_host = getattr(instance, INSTANCE_HOST_ATTRIBUTE)\n    if (src_host == dst_host):\n        LOG.warning('Skipping migration to the same host')\n        return\n    original_status = instance.status.lower()\n    if (original_status != SHUTOFF):\n        compute_api.servers.stop(instance)\n        wait_for_condition(is_vm_status_in, compute_api, instance, [SHUTOFF], timeout=state_change_timeout)\n    fix_post_cobalt_ephemeral_disk(config, instance)\n    with install_ssh_keys(config, src_host, dst_host):\n        with disable_all_nova_compute_services(compute_api):\n            with enable_nova_compute_services(compute_api, dst_host, src_host):\n                compute_api.servers.migrate(instance)\n                wait_for_condition(is_vm_status_in, compute_api, instance, [VERIFY_RESIZE], timeout=migration_timeout)\n                compute_api.servers.confirm_resize(instance)\n                wait_for_condition(is_vm_status_in, compute_api, instance, [ACTIVE], timeout=state_change_timeout)\n    if (original_status == SHUTOFF.lower()):\n        LOG.debug('Starting replacement VM %s', instance_id)\n        compute_api.servers.stop(instance_id)\n", "label": 0}
{"function": "\n\ndef _validate_python(self, field_dict, state):\n    try:\n        ref = field_dict[self.field_names[0]]\n    except TypeError:\n        raise Invalid(self.message('notDict', state), field_dict, state)\n    except KeyError:\n        ref = ''\n    errors = {\n        \n    }\n    for name in self.field_names[1:]:\n        if (field_dict.get(name, '') != ref):\n            if self.show_match:\n                errors[name] = self.message('invalid', state, match=ref)\n            else:\n                errors[name] = self.message('invalidNoMatch', state)\n    if errors:\n        error_list = sorted(six.iteritems(errors))\n        error_message = '<br>\\n'.join((('%s: %s' % (name, value)) for (name, value) in error_list))\n        raise Invalid(error_message, field_dict, state, error_dict=errors)\n", "label": 0}
{"function": "\n\ndef iter_keys(self, filename):\n    with open(filename, 'rb') as f:\n        header = f.read(8)\n        self._verify_header(header)\n        current_offset = 8\n        file_size_bytes = os.path.getsize(filename)\n        while True:\n            current_contents = f.read(8)\n            current_offset += 8\n            if (len(current_contents) < 8):\n                if (len(current_contents) > 0):\n                    raise DBMLoadError('Error loading db: partial header read')\n                else:\n                    return\n            (key_size, val_size) = struct.unpack('!ii', current_contents)\n            key = f.read(key_size)\n            if (len(key) != key_size):\n                raise DBMLoadError(('Error loading db: key size does not match (expected %s bytes, got %s instead.' % (key_size, len(key))))\n            value_offset = (current_offset + key_size)\n            if ((value_offset + val_size) > file_size_bytes):\n                return\n            (yield (key, value_offset, val_size))\n            if (val_size == _DELETED):\n                val_size = 0\n            skip_ahead = ((key_size + val_size) + 4)\n            current_offset += skip_ahead\n            if (current_offset > file_size_bytes):\n                raise DBMLoadError('Error loading db: reading past the end of the file (file possibly truncated)')\n            f.seek(current_offset)\n", "label": 0}
{"function": "\n\ndef test_iter_smart_pk_range(self):\n    seen = []\n    for (start_pk, end_pk) in Author.objects.iter_smart_pk_ranges():\n        seen.extend(Author.objects.filter(id__gte=start_pk, id__lt=end_pk).values_list('id', flat=True))\n    all_ids = list(Author.objects.order_by('id').values_list('id', flat=True))\n    assert (seen == all_ids)\n", "label": 0}
{"function": "\n\ndef get_command(self, ctx, name):\n    \"\\n        Get a specific command by looking up the module.\\n\\n        :param ctx: Click context\\n        :param name: Command name\\n        :return: Module's cli function\\n        \"\n    try:\n        if (sys.version_info[0] == 2):\n            name = name.encode('ascii', 'replace')\n        mod = __import__(('cli.commands.cmd_' + name), None, None, ['cli'])\n    except ImportError as e:\n        logging.error('Error importing module {0}:\\n{0}'.format(name, e))\n        exit(1)\n    return mod.cli\n", "label": 0}
{"function": "\n\ndef test_diff_nans(self):\n    'Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/204'\n    arr = np.empty((10, 10), dtype=np.float64)\n    arr[:5] = 1.0\n    arr[5:] = np.nan\n    arr2 = arr.copy()\n    table = np.rec.array([(1.0, 2.0), (3.0, np.nan), (np.nan, np.nan)], names=['cola', 'colb']).view(fits.FITS_rec)\n    table2 = table.copy()\n    assert ImageDataDiff(arr, arr2).identical\n    assert TableDataDiff(table, table2).identical\n    arr2[0][0] = 2.0\n    arr2[5][0] = 2.0\n    table2[0][0] = 2.0\n    table2[1][1] = 2.0\n    diff = ImageDataDiff(arr, arr2)\n    assert (not diff.identical)\n    assert (diff.diff_pixels[0] == ((0, 0), (1.0, 2.0)))\n    assert (diff.diff_pixels[1][0] == (5, 0))\n    assert np.isnan(diff.diff_pixels[1][1][0])\n    assert (diff.diff_pixels[1][1][1] == 2.0)\n    diff = TableDataDiff(table, table2)\n    assert (not diff.identical)\n    assert (diff.diff_values[0] == (('cola', 0), (1.0, 2.0)))\n    assert (diff.diff_values[1][0] == ('colb', 1))\n    assert np.isnan(diff.diff_values[1][1][0])\n    assert (diff.diff_values[1][1][1] == 2.0)\n", "label": 1}
{"function": "\n\ndef bayesdb_generator_column_stattype(bdb, generator_id, colno):\n    'Return the statistical type of the column `colno` in `generator_id`.'\n    sql = '\\n        SELECT stattype FROM bayesdb_generator_column\\n            WHERE generator_id = ? AND colno = ?\\n    '\n    cursor = bdb.sql_execute(sql, (generator_id, colno))\n    try:\n        row = cursor.next()\n    except StopIteration:\n        generator = bayesdb_generator_name(bdb, generator_id)\n        sql = '\\n            SELECT COUNT(*)\\n                FROM bayesdb_generator AS g, bayesdb_column AS c\\n                WHERE g.id = :generator_id\\n                    AND g.tabname = c.tabname\\n                    AND c.colno = :colno\\n        '\n        cursor = bdb.sql_execute(sql, {\n            'generator_id': generator_id,\n            'colno': colno,\n        })\n        if (cursor_value(cursor) == 0):\n            raise ValueError(('No such column in generator %s: %d' % (generator, colno)))\n        else:\n            raise ValueError(('Column not modelled in generator %s: %d' % (generator, colno)))\n    else:\n        assert (len(row) == 1)\n        return row[0]\n", "label": 0}
{"function": "\n\ndef test_serialization_type(self):\n    activity_object = Pin(id=1)\n    activity = Activity(1, LoveVerb, activity_object)\n    assert isinstance(activity.serialization_id, (six.integer_types, float))\n", "label": 0}
{"function": "\n\n@pytest.mark.xfail\ndef test_fails(self):\n    contact = models.Contact(name='Example')\n    contact.put()\n    models.PhoneNumber(contact=self.contact_key, phone_type='home', number='(650) 555 - 2200').put()\n    numbers = contact.phone_numbers.fetch()\n    assert (1 == len(numbers))\n", "label": 0}
{"function": "\n\ndef _mount_shares_to_instance(self, instance):\n    for share in self.shares:\n        share.handler.allow_access_to_instance(instance, share.share_config)\n    with instance.remote() as remote:\n        share_types = set((type(share.handler) for share in self.shares))\n        for share_type in share_types:\n            share_type.setup_instance(remote)\n        for share in self.shares:\n            share.handler.mount_to_instance(remote, share.share_config)\n", "label": 0}
{"function": "\n\ndef parse(cls, signed_request, application_secret_key):\n    'Parse a signed request, returning a dictionary describing its payload.'\n\n    def decode(encoded):\n        padding = ('=' * (len(encoded) % 4))\n        return base64.urlsafe_b64decode((encoded + padding))\n    try:\n        (encoded_signature, encoded_payload) = (str(string) for string in signed_request.split('.', 2))\n        signature = decode(encoded_signature)\n        signed_request_data = json.loads(decode(encoded_payload).decode('utf-8'))\n    except (TypeError, ValueError):\n        raise SignedRequestError('Signed request had a corrupt payload')\n    if (signed_request_data.get('algorithm', '').upper() != 'HMAC-SHA256'):\n        raise SignedRequestError('Signed request is using an unknown algorithm')\n    expected_signature = hmac.new(application_secret_key.encode('utf-8'), msg=encoded_payload.encode('utf-8'), digestmod=hashlib.sha256).digest()\n    if (signature != expected_signature):\n        raise SignedRequestError('Signed request signature mismatch')\n    return signed_request_data\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    \" Add an issue to the test repository and save it's id.\"\n    super(IssueCommentAuthenticatedMethodsTest, self).setUp()\n    (success, result) = self.bb.issue.create(title='Test Issue Bitbucket API', content='Test Issue Bitbucket API', responsible=self.bb.username, status='new', kind='bug')\n    assert success\n    self.bb.issue.comment.issue_id = result['local_id']\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    assert (not cls._meta.has_auto_field), \"A model can't have more than one AutoField.\"\n    super(AutoField, self).contribute_to_class(cls, name)\n    cls._meta.has_auto_field = True\n    cls._meta.auto_field = self\n", "label": 0}
{"function": "\n\ndef make_node(self, images):\n    '\\n        .. todo::\\n\\n            WRITEME\\n        '\n    images = as_cuda_ndarray_variable(images)\n    assert (images.ndim == 4)\n    channels_broadcastable = images.type.broadcastable[0]\n    batch_broadcastable = images.type.broadcastable[3]\n    rows_broadcastable = False\n    cols_broadcastable = False\n    targets_broadcastable = (channels_broadcastable, rows_broadcastable, cols_broadcastable, batch_broadcastable)\n    targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)\n    targets = targets_type()\n    return Apply(self, [images], [targets])\n", "label": 0}
{"function": "\n\ndef _get_url(self, url):\n    if (self.access == 'public'):\n        url = url.replace('https://', 'http://')\n        req = urllib.request.Request(url)\n        try:\n            return urllib.request.urlopen(req).read()\n        except urllib.error.HTTPError:\n            raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n    else:\n        raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n", "label": 0}
{"function": "\n\ndef LVMPathSpecGetVolumeIndex(path_spec):\n    'Retrieves the volume index from the path specification.\\n\\n  Args:\\n    path_spec: the path specification (instance of PathSpec).\\n  '\n    volume_index = getattr(path_spec, 'volume_index', None)\n    if (volume_index is None):\n        location = getattr(path_spec, 'location', None)\n        if ((location is None) or (not location.startswith('/lvm'))):\n            return\n        volume_index = None\n        try:\n            volume_index = (int(location[4:], 10) - 1)\n        except ValueError:\n            pass\n        if ((volume_index is None) or (volume_index < 0)):\n            return\n    return volume_index\n", "label": 0}
{"function": "\n\ndef handle_socket_write(self):\n    'Write to socket'\n    try:\n        count = self.socket.send(bytes(self.buffer_ser2net))\n        self.buffer_ser2net = self.buffer_ser2net[count:]\n    except socket.error:\n        self.handle_socket_error()\n", "label": 0}
{"function": "\n\n@responses.activate\ndef test_bitly_total_clicks_bad_response():\n    body = '20'\n    params = urlencode(dict(link=shorten, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/link/clicks', params)\n    responses.add(responses.GET, url, body=body, status=400, match_querystring=True)\n    body = shorten\n    params = urlencode(dict(uri=expanded, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/shorten', params)\n    responses.add(responses.GET, url, body=body, match_querystring=True)\n    s.short(expanded)\n    assert (s.total_clicks() == 0)\n    assert (s.total_clicks(shorten) == 0)\n", "label": 0}
{"function": "\n\ndef test_RosdepDatabase():\n    from rosdep2.model import RosdepDatabase\n    db = RosdepDatabase()\n    assert (not db.is_loaded('foo'))\n    data = {\n        'a': 1,\n    }\n    db.set_view_data('foo', data, [], 'origin1')\n    assert db.is_loaded('foo')\n    entry = db.get_view_data('foo')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin1')\n    assert (entry.view_dependencies == [])\n    data['a'] = 2\n    assert (entry.rosdep_data != data)\n    data = {\n        'b': 2,\n    }\n    db.set_view_data('bar', data, ['foo'], 'origin2')\n    assert db.is_loaded('bar')\n    entry = db.get_view_data('bar')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin2')\n    assert (entry.view_dependencies == ['foo'])\n    data = {\n        'b': 3,\n    }\n    assert db.is_loaded('bar')\n    db.set_view_data('bar', data, ['baz', 'blah'], 'origin3')\n    assert db.is_loaded('bar')\n    entry = db.get_view_data('bar')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin3')\n    assert (set(entry.view_dependencies) == set(['baz', 'blah']))\n", "label": 1}
{"function": "\n\ndef prune_overridden(ansi_string):\n    'Remove color codes that are rendered ineffective by subsequent codes in one escape sequence then sort codes.\\n\\n    :param str ansi_string: Incoming ansi_string with ANSI color codes.\\n\\n    :return: Color string with pruned color sequences.\\n    :rtype: str\\n    '\n    multi_seqs = set((p for p in RE_ANSI.findall(ansi_string) if (';' in p[1])))\n    for (escape, codes) in multi_seqs:\n        r_codes = list(reversed(codes.split(';')))\n        try:\n            r_codes = r_codes[:(r_codes.index('0') + 1)]\n        except ValueError:\n            pass\n        for group in CODE_GROUPS:\n            for pos in reversed([i for (i, n) in enumerate(r_codes) if (n in group)][1:]):\n                r_codes.pop(pos)\n        reduced_codes = ';'.join(sorted(r_codes, key=int))\n        if (codes != reduced_codes):\n            ansi_string = ansi_string.replace(escape, (('\\x1b[' + reduced_codes) + 'm'))\n    return ansi_string\n", "label": 0}
{"function": "\n\ndef test_dehydrate(self):\n    note = Note()\n    bundle_1 = Bundle(obj=note)\n    field_1 = ToManyField(SubjectResource, 'subjects')\n    field_1.instance_name = 'm2m'\n    with self.assertRaises(ApiFieldError):\n        field_1.dehydrate(bundle_1)\n    field_2 = ToManyField(SubjectResource, 'subjects', null=True)\n    field_2.instance_name = 'm2m'\n    self.assertEqual(field_2.dehydrate(bundle_1), [])\n    field_3 = ToManyField(SubjectResource, 'subjects')\n    field_3.instance_name = 'm2m'\n    bundle_3 = Bundle(obj=self.note_1)\n    self.assertEqual(field_3.dehydrate(bundle_3), ['/api/v1/subjects/1/', '/api/v1/subjects/2/'])\n    field_4 = ToManyField(SubjectResource, 'subjects', full=True)\n    field_4.instance_name = 'm2m'\n    request = MockRequest()\n    request.path = ('/api/v1/subjects/%(pk)s/' % {\n        'pk': self.note_1.pk,\n    })\n    bundle_4 = Bundle(obj=self.note_1, request=request)\n    subject_bundle_list = field_4.dehydrate(bundle_4)\n    self.assertEqual(len(subject_bundle_list), 2)\n    self.assertEqual(isinstance(subject_bundle_list[0], Bundle), True)\n    self.assertEqual(subject_bundle_list[0].data['name'], 'News')\n    self.assertEqual(subject_bundle_list[0].data['url'], '/news/')\n    self.assertEqual(subject_bundle_list[0].obj.name, 'News')\n    self.assertEqual(subject_bundle_list[0].obj.url, '/news/')\n    self.assertEqual(isinstance(subject_bundle_list[1], Bundle), True)\n    self.assertEqual(subject_bundle_list[1].data['name'], 'Photos')\n    self.assertEqual(subject_bundle_list[1].data['url'], '/photos/')\n    self.assertEqual(subject_bundle_list[1].obj.name, 'Photos')\n    self.assertEqual(subject_bundle_list[1].obj.url, '/photos/')\n    field_5 = ToManyField(SubjectResource, 'subjects')\n    field_5.instance_name = 'm2m'\n    bundle_5 = Bundle(obj=self.note_2)\n    self.assertEqual(field_5.dehydrate(bundle_5), ['/api/v1/subjects/1/', '/api/v1/subjects/3/'])\n    field_6 = ToManyField(SubjectResource, 'subjects')\n    field_6.instance_name = 'm2m'\n    bundle_6 = Bundle(obj=self.note_3)\n    self.assertEqual(field_6.dehydrate(bundle_6), [])\n    field_7 = ToManyField(SubjectResource, None)\n    field_7.instance_name = 'm2m'\n    bundle_7 = Bundle(obj=self.note_3)\n    with self.assertRaises(ApiFieldError):\n        field_7.dehydrate(bundle_7)\n", "label": 0}
{"function": "\n\ndef capture(self, money, authorization, options=None):\n    options = (options or {\n        \n    })\n    params = {\n        'checkout_id': authorization,\n    }\n    token = options.pop('access_token', self.we_pay_settings['ACCESS_TOKEN'])\n    try:\n        response = self.we_pay.call('/checkout/capture', params, token=token)\n    except WePayError as error:\n        transaction_was_unsuccessful.send(sender=self, type='capture', response=error)\n        return {\n            'status': 'FAILURE',\n            'response': error,\n        }\n    transaction_was_successful.send(sender=self, type='capture', response=response)\n    return {\n        'status': 'SUCCESS',\n        'response': response,\n    }\n", "label": 0}
{"function": "\n\ndef update_success(self, update_dict, raw=False):\n    ' This method serves as an easy way to update your success attributes\\n        that are passed to the start Node rendering context, or passed back in\\n        JSON. It automatically recalls whether the last validation call was to\\n        json_validate or validate_render and modifys the correct dictionary\\n        accordingly.\\n\\n        :param update_dict: The dictionary of values to update/add.\\n        :type data: dictionary\\n\\n        :param raw: Whether you would like a pre-compiled JSON\\n            string returned, or the raw dictionary.\\n        :type raw: bool\\n\\n        :return: Return value is either the new JSON string (or raw dict if\\n            requested) if json_validate was your last validation call, or a\\n            re-render of the form with updated error messages if validate_render\\n            was your last call.\\n        '\n    if (self._last_valid == 'render'):\n        try:\n            self.start.errors[(- 1)].update(update_dict)\n        except IndexError:\n            raise IndexError('Error updating your error dictionary for the start Node. There were no errors to modify.')\n        except AttributeError:\n            raise AttributeError('This method is designed to update an error dictionary, yet your errors are not dictionaries')\n        return self.render()\n    else:\n        try:\n            self._last_raw_json['success_blob'].update(update_dict)\n        except KeyError:\n            raise KeyError('Either your json_validate method has not been run yet, or your success_header_generate does not produce output')\n        if raw:\n            return self._last_raw_json\n        else:\n            return json.dumps(self._last_raw_json)\n", "label": 0}
{"function": "\n\ndef test_getitem_slice_big():\n    slt = SortedList(range(4))\n    lst = list(range(4))\n    itr = ((start, stop, step) for start in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for stop in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for step in [(- 3), (- 2), (- 1), 1, 2, 3])\n    for (start, stop, step) in itr:\n        assert (slt[start:stop:step] == lst[start:stop:step])\n", "label": 0}
{"function": "\n\n@users_delete_fixtures\ndef test_users_delete_redirect(User):\n    request = DummyRequest(params={\n        'username': 'bob',\n    })\n    User.get_by_username.return_value = None\n    result = views.users_delete(request)\n    assert (result.__class__ == httpexceptions.HTTPFound)\n", "label": 0}
{"function": "\n\ndef get_primary_address(self):\n    'Return the primary address of this partner.\\n\\n        '\n    Address = rt.modules.addresses.Address\n    try:\n        return Address.objects.get(partner=self, primary=True)\n    except Address.DoesNotExist:\n        pass\n", "label": 0}
{"function": "\n\ndef clear(self):\n    'od.clear() -> None.  Remove all items from od.'\n    try:\n        for node in self.__map.itervalues():\n            del node[:]\n        root = self.__root\n        root[:] = [root, root, None]\n        self.__map.clear()\n    except AttributeError:\n        pass\n    dict.clear(self)\n", "label": 0}
{"function": "\n\ndef Validate(self):\n    'Attempt to validate the artifact has been well defined.\\n\\n    This is used to enforce Artifact rules. Since it checks all dependencies are\\n    present, this method can only be called once all artifacts have been loaded\\n    into the registry. Use ValidateSyntax to check syntax for each artifact on\\n    import.\\n\\n    Raises:\\n      ArtifactDefinitionError: If artifact is invalid.\\n    '\n    self.ValidateSyntax()\n    try:\n        for dependency in self.GetArtifactDependencies():\n            dependency_obj = REGISTRY.GetArtifact(dependency)\n            if dependency_obj.error_message:\n                raise ArtifactDefinitionError(('Dependency %s has an error!' % dependency))\n    except ArtifactNotRegisteredError as e:\n        raise ArtifactDefinitionError(e)\n", "label": 0}
{"function": "\n\ndef dfs_labeled_edges(G, source=None):\n    \"Produce edges in a depth-first-search (DFS) labeled by type.\\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n\\n    source : node, optional\\n       Specify starting node for depth-first search and return edges in\\n       the component reachable from source.\\n\\n    Returns\\n    -------\\n    edges: generator\\n       A generator of edges in the depth-first-search labeled with 'forward',\\n       'nontree', and 'reverse'.\\n\\n    Examples\\n    --------\\n    >>> G = nx.path_graph(3)\\n    >>> edges = (list(nx.dfs_labeled_edges(G,0)))\\n\\n    Notes\\n    -----\\n    Based on http://www.ics.uci.edu/~eppstein/PADS/DFS.py\\n    by D. Eppstein, July 2004.\\n\\n    If a source is not specified then a source is chosen arbitrarily and\\n    repeatedly until all components in the graph are searched.\\n    \"\n    if (source is None):\n        nodes = G\n    else:\n        nodes = [source]\n    visited = set()\n    for start in nodes:\n        if (start in visited):\n            continue\n        (yield (start, start, {\n            'dir': 'forward',\n        }))\n        visited.add(start)\n        stack = [(start, iter(G[start]))]\n        while stack:\n            (parent, children) = stack[(- 1)]\n            try:\n                child = next(children)\n                if (child in visited):\n                    (yield (parent, child, {\n                        'dir': 'nontree',\n                    }))\n                else:\n                    (yield (parent, child, {\n                        'dir': 'forward',\n                    }))\n                    visited.add(child)\n                    stack.append((child, iter(G[child])))\n            except StopIteration:\n                stack.pop()\n                if stack:\n                    (yield (stack[(- 1)][0], parent, {\n                        'dir': 'reverse',\n                    }))\n        (yield (start, start, {\n            'dir': 'reverse',\n        }))\n", "label": 0}
{"function": "\n\ndef __init__(self, pattern, flags=0):\n    'The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.'\n    super(Regex, self).__init__()\n    if isinstance(pattern, basestring):\n        if (len(pattern) == 0):\n            warnings.warn('null string passed to Regex; use Empty() instead', SyntaxWarning, stacklevel=2)\n        self.pattern = pattern\n        self.flags = flags\n        try:\n            self.re = re.compile(self.pattern, self.flags)\n            self.reString = self.pattern\n        except sre_constants.error:\n            warnings.warn(('invalid pattern (%s) passed to Regex' % pattern), SyntaxWarning, stacklevel=2)\n            raise\n    elif isinstance(pattern, Regex.compiledREtype):\n        self.re = pattern\n        self.pattern = self.reString = str(pattern)\n        self.flags = flags\n    else:\n        raise ValueError('Regex may only be constructed with a string or a compiled RE object')\n    self.name = _ustr(self)\n    self.errmsg = ('Expected ' + self.name)\n    self.mayIndexError = False\n    self.mayReturnEmpty = True\n", "label": 0}
{"function": "\n\ndef get_cpu_state(self):\n    '\\n        Retrieves CPU state from client\\n        '\n    state = c_int(0)\n    self.library.Cli_GetPlcStatus(self.pointer, byref(state))\n    try:\n        status_string = cpu_statuses[state.value]\n    except KeyError:\n        status_string = None\n    if (not status_string):\n        raise Snap7Exception(('The cpu state (%s) is invalid' % state.value))\n    logging.debug(('CPU state is %s' % status_string))\n    return status_string\n", "label": 0}
{"function": "\n\ndef test_local_bower_json_dependencies():\n    bower = bowerstatic.Bower()\n    components = bower.components('components', os.path.join(os.path.dirname(__file__), 'bower_components'))\n    local = bower.local_components('local', components)\n    path = os.path.join(os.path.dirname(__file__), 'local_component_deps')\n    local.component(path, version='2.0')\n\n    def wsgi(environ, start_response):\n        start_response('200 OK', [('Content-Type', 'text/html;charset=UTF-8')])\n        include = local.includer(environ)\n        include('local_component')\n        return [b'<html><head></head><body>Hello!</body></html>']\n    wrapped = bower.wrap(wsgi)\n    c = Client(wrapped)\n    response = c.get('/')\n    assert (response.body == b'<html><head><script type=\"text/javascript\" src=\"/bowerstatic/components/jquery/2.1.1/dist/jquery.js\"></script>\\n<script type=\"text/javascript\" src=\"/bowerstatic/local/local_component/2.0/local.js\"></script></head><body>Hello!</body></html>')\n", "label": 0}
{"function": "\n\ndef main():\n    '\\n    %prog database.fa query.fa [options]\\n\\n    Wrapper for NCBI BLAST+.\\n    '\n    p = OptionParser(main.__doc__)\n    p.add_option('--format', default=\" '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore' \", help='0-11, learn more with \"blastp -help\". [default: %default]')\n    p.add_option('--path', dest='blast_path', default=None, help='specify BLAST+ path including the program name')\n    p.add_option('--prog', dest='blast_program', default='blastp', help='specify BLAST+ program to use. See complete list here: http://www.ncbi.nlm.nih.gov/books/NBK52640/#chapter1.Installation [default: %default]')\n    p.set_align(evalue=0.01)\n    p.add_option('--best', default=1, type='int', help='Only look for best N hits [default: %default]')\n    p.set_cpus()\n    p.add_option('--nprocs', default=1, type='int', help=(('number of BLAST processes to run in parallel. ' + 'split query.fa into `nprocs` chunks, ') + 'each chunk uses -num_threads=`cpus`'))\n    p.set_params()\n    p.set_outfile()\n    (opts, args) = p.parse_args()\n    if ((len(args) != 2) or (opts.blast_program is None)):\n        sys.exit((not p.print_help()))\n    (bfasta_fn, afasta_fn) = args\n    for fn in (afasta_fn, bfasta_fn):\n        assert op.exists(fn)\n    afasta_fn = op.abspath(afasta_fn)\n    bfasta_fn = op.abspath(bfasta_fn)\n    out_fh = must_open(opts.outfile, 'w')\n    extra = opts.extra\n    blast_path = opts.blast_path\n    blast_program = opts.blast_program\n    blast_bin = (blast_path or blast_program)\n    if (op.basename(blast_bin) != blast_program):\n        blast_bin = op.join(blast_bin, blast_program)\n    (nprocs, cpus) = (opts.nprocs, opts.cpus)\n    if (nprocs > 1):\n        logging.debug(('Dispatch job to %d processes' % nprocs))\n        outdir = 'outdir'\n        fs = split([afasta_fn, outdir, str(nprocs)])\n        queries = fs.names\n    else:\n        queries = [afasta_fn]\n    dbtype = ('prot' if (op.basename(blast_bin) in ('blastp', 'blastx')) else 'nucl')\n    db = bfasta_fn\n    if (dbtype == 'prot'):\n        nin = (db + '.pin')\n    else:\n        nin = (db + '.nin')\n        nin00 = (db + '.00.nin')\n        nin = (nin00 if op.exists(nin00) else (db + '.nin'))\n    run_formatdb(infile=db, outfile=nin, dbtype=dbtype)\n    lock = Lock()\n    blastplus_template = '{0} -db {1} -outfmt {2}'\n    blast_cmd = blastplus_template.format(blast_bin, bfasta_fn, opts.format)\n    blast_cmd += ' -evalue {0} -max_target_seqs {1}'.format(opts.evalue, opts.best)\n    blast_cmd += ' -num_threads {0}'.format(cpus)\n    if extra:\n        blast_cmd += (' ' + extra.strip())\n    args = [(out_fh, blast_cmd, query, lock) for query in queries]\n    g = Jobs(target=blastplus, args=args)\n    g.run()\n", "label": 1}
{"function": "\n\ndef test_basic(self):\n    'messages are sent and received properly'\n    question = b'sucess?'\n    answer = b'yeah, success'\n\n    def handler(sock):\n        text = sock.recv(1000)\n        assert (text == question)\n        sock.sendall(answer)\n    with Server(handler) as (host, port):\n        sock = socket.socket()\n        sock.connect((host, port))\n        sock.sendall(question)\n        text = sock.recv(1000)\n        assert (text == answer)\n        sock.close()\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    from pennyblack.models import Link, Newsletter\n    if ('mail' not in context):\n        return '#'\n    mail = context['mail']\n    newsletter = mail.job.newsletter\n    if newsletter.is_workflow():\n        job = newsletter.get_default_job()\n    else:\n        job = mail.job\n    try:\n        link = job.links.get(identifier=self.identifier)\n    except job.links.model.DoesNotExist:\n        link = Newsletter.add_view_link_to_job(self.identifier, job)\n    return (context['base_url'] + reverse('pennyblack.redirect_link', args=(mail.mail_hash, link.link_hash)))\n", "label": 0}
{"function": "\n\n@plug.handler()\ndef delete_file(metadata):\n    try:\n        ignore_delete.add(metadata.path)\n        os.unlink(metadata.path)\n    except (IOError, OSError) as e:\n        ignore_delete.discard(metadata.path)\n        raise ServiceError(\"Error deleting file '{}': {}\".format(metadata.path, e))\n", "label": 0}
{"function": "\n\ndef test_write_a_file(self, gitfs_log):\n    content = 'Just a small file'\n    filename = '{}/new_file'.format(self.current_path)\n    with gitfs_log('SyncWorker: Set push_successful'):\n        with open(filename, 'w') as f:\n            f.write(content)\n    with open(filename) as f:\n        assert (f.read() == content)\n    with pull(self.sh):\n        self.assert_new_commit()\n        self.assert_commit_message('Update /new_file')\n", "label": 0}
{"function": "\n\ndef test_context(self):\n    self.client.quit()\n    ctx = ssl.SSLContext(ssl.PROTOCOL_TLSv1)\n    self.assertRaises(ValueError, ftplib.FTP_TLS, keyfile=CERTFILE, context=ctx)\n    self.assertRaises(ValueError, ftplib.FTP_TLS, certfile=CERTFILE, context=ctx)\n    self.assertRaises(ValueError, ftplib.FTP_TLS, certfile=CERTFILE, keyfile=CERTFILE, context=ctx)\n    self.client = ftplib.FTP_TLS(context=ctx, timeout=2)\n    self.client.connect(self.server.host, self.server.port)\n    self.assertNotIsInstance(self.client.sock, ssl.SSLSocket)\n    self.client.auth()\n    self.assertIs(self.client.sock.context, ctx)\n    self.assertIsInstance(self.client.sock, ssl.SSLSocket)\n    self.client.prot_p()\n    with self.client.transfercmd('list') as sock:\n        self.assertIs(sock.context, ctx)\n        self.assertIsInstance(sock, ssl.SSLSocket)\n", "label": 0}
{"function": "\n\ndef _setup_units(self, connections, params_dict, unknowns_dict):\n    '\\n        Calculate unit conversion factors for any connected\\n        variables having different units and store them in params_dict.\\n\\n        Args\\n        ----\\n        connections : dict\\n            A dict of target variables (absolute name) mapped\\n            to the absolute name of their source variable and the\\n            relevant indices of that source if applicable.\\n\\n        params_dict : OrderedDict\\n            A dict of parameter metadata for the whole `Problem`.\\n\\n        unknowns_dict : OrderedDict\\n            A dict of unknowns metadata for the whole `Problem`.\\n        '\n    to_prom_name = self.root._sysdata.to_prom_name\n    for (target, (source, idxs)) in iteritems(connections):\n        tmeta = params_dict[target]\n        smeta = unknowns_dict[source]\n        if (('units' not in tmeta) or ('units' not in smeta)):\n            continue\n        src_unit = smeta['units']\n        tgt_unit = tmeta['units']\n        try:\n            (scale, offset) = get_conversion_tuple(src_unit, tgt_unit)\n        except TypeError as err:\n            if (str(err) == 'Incompatible units'):\n                msg = \"Unit '{0}' in source {1} is incompatible with unit '{2}' in target {3}.\".format(src_unit, _both_names(smeta, to_prom_name), tgt_unit, _both_names(tmeta, to_prom_name))\n                self._setup_errors.append(msg)\n                continue\n            else:\n                raise\n        if ((scale != 1.0) or (offset != 0.0)):\n            tmeta['unit_conv'] = (scale, offset)\n", "label": 0}
{"function": "\n\n@classmethod\ndef load_missing(cls, cloud, object_id):\n    identity_client = clients.identity_client(cloud)\n    try:\n        raw_tenant = identity_client.tenants.get(object_id.id)\n        return cls.load_from_cloud(cloud, raw_tenant)\n    except exceptions.NotFound:\n        return None\n", "label": 0}
{"function": "\n\ndef Check(self):\n    'Assertion verification for options.'\n    try:\n        assert (self.m0 >= 0), 'margin0'\n        assert (self.m1 >= self.m0), 'margin1'\n        assert (self.c0 >= 0), 'cost0'\n        assert (self.c1 >= 0), 'cost1'\n        assert (self.cb >= 0), 'costb'\n        assert (self.ind >= 0), 'indent'\n        assert (self.adj_comment >= 0), 'adj_comment'\n        assert (self.adj_flow >= 0), 'adj_flow'\n        assert (self.adj_call >= 0), 'adj_call'\n        assert (self.adj_arg >= 0), 'adj_arg'\n        assert (self.cpack >= 0), 'cpack'\n    except AssertionError as e:\n        raise Error((\"Illegal option value for '%s'\" % e.args[0]))\n", "label": 1}
{"function": "\n\ndef message_user(self, request, message, level=messages.INFO, extra_tags='', fail_silently=False):\n    '\\n        Send a message to the user. The default implementation\\n        posts a message using the django.contrib.messages backend.\\n\\n        Exposes almost the same API as messages.add_message(), but accepts the\\n        positional arguments in a different order to maintain backwards\\n        compatibility. For convenience, it accepts the `level` argument as\\n        a string rather than the usual level number.\\n        '\n    if (not isinstance(level, int)):\n        try:\n            level = getattr(messages.constants, level.upper())\n        except AttributeError:\n            levels = messages.constants.DEFAULT_TAGS.values()\n            levels_repr = ', '.join((('`%s`' % l) for l in levels))\n            raise ValueError(('Bad message level string: `%s`. Possible values are: %s' % (level, levels_repr)))\n    messages.add_message(request, level, message, extra_tags=extra_tags, fail_silently=fail_silently)\n", "label": 0}
{"function": "\n\ndef test_active_contributors(self):\n    'Test the active_contributors util method.'\n    start_date = self.start_date\n    en_us_contributors = active_contributors(from_date=start_date, locale='en-US')\n    es_contributors = active_contributors(from_date=start_date, locale='es')\n    all_contributors = active_contributors(from_date=start_date)\n    eq_(3, len(en_us_contributors))\n    assert (self.user in en_us_contributors)\n    assert (self.en_us_old.creator not in en_us_contributors)\n    eq_(4, len(es_contributors))\n    assert (self.user in es_contributors)\n    assert (self.es_old.creator not in es_contributors)\n    eq_(6, len(all_contributors))\n    assert (self.user in all_contributors)\n    assert (self.en_us_old.creator not in all_contributors)\n    assert (self.es_old.creator not in all_contributors)\n", "label": 0}
{"function": "\n\ndef _cleanup_groups(self):\n    exception_list = list()\n    for group in self.cloud.list_groups():\n        if group['name'].startswith(self.group_prefix):\n            try:\n                self.cloud.delete_group(group['id'])\n            except Exception as e:\n                exception_list.append(str(e))\n                continue\n    if exception_list:\n        raise OpenStackCloudException('\\n'.join(exception_list))\n", "label": 0}
{"function": "\n\ndef _expand_glob_path(file_roots):\n    '\\n    Applies shell globbing to a set of directories and returns\\n    the expanded paths\\n    '\n    unglobbed_path = []\n    for path in file_roots:\n        try:\n            if glob.has_magic(path):\n                unglobbed_path.extend(glob.glob(path))\n            else:\n                unglobbed_path.append(path)\n        except Exception:\n            unglobbed_path.append(path)\n    return unglobbed_path\n", "label": 0}
{"function": "\n\ndef listKeysAndSizes(self, bucketName):\n    'Return a list of (name, size) pairs of keys in the bucket'\n    with self.state_.lock:\n        if (bucketName not in self.state_.buckets_):\n            raise S3Interface.BucketNotFound(bucketName)\n        self.state_.validateAccess(bucketName, self.credentials_)\n        return [(key, len(val.value), val.mtime) for (key, val) in self.state_.buckets_[bucketName].iteritems()]\n", "label": 0}
{"function": "\n\ndef test_issue_7754(self):\n    old_cwd = os.getcwd()\n    config_dir = os.path.join(integration.TMP, 'issue-7754')\n    if (not os.path.isdir(config_dir)):\n        os.makedirs(config_dir)\n    os.chdir(config_dir)\n    for fname in ('master', 'minion'):\n        pid_path = os.path.join(config_dir, '{0}.pid'.format(fname))\n        with salt.utils.fopen(self.get_config_file_path(fname), 'r') as fhr:\n            config = yaml.load(fhr.read())\n            config['log_file'] = config['syndic_log_file'] = 'file:///tmp/log/LOG_LOCAL3'\n            config['root_dir'] = config_dir\n            if ('ret_port' in config):\n                config['ret_port'] = (int(config['ret_port']) + 10)\n                config['publish_port'] = (int(config['publish_port']) + 10)\n            with salt.utils.fopen(os.path.join(config_dir, fname), 'w') as fhw:\n                fhw.write(yaml.dump(config, default_flow_style=False))\n    ret = self.run_script(self._call_binary_, '--config-dir={0} --pid-file={1} -l debug'.format(config_dir, pid_path), timeout=5, catch_stderr=True, with_retcode=True)\n    if os.path.exists(pid_path):\n        with salt.utils.fopen(pid_path) as fhr:\n            try:\n                os.kill(int(fhr.read()), signal.SIGKILL)\n            except OSError:\n                pass\n    try:\n        self.assertFalse(os.path.isdir(os.path.join(config_dir, 'file:')))\n        self.assertIn('Failed to setup the Syslog logging handler', '\\n'.join(ret[1]))\n        self.assertEqual(ret[2], 2)\n    finally:\n        self.chdir(old_cwd)\n        if os.path.isdir(config_dir):\n            shutil.rmtree(config_dir)\n", "label": 0}
{"function": "\n\n@classmethod\ndef _from_line(cls, remote, line):\n    'Create a new PushInfo instance as parsed from line which is expected to be like\\n            refs/heads/master:refs/heads/master 05d2687..1d0568e as bytes'\n    (control_character, from_to, summary) = line.split('\\t', 3)\n    flags = 0\n    try:\n        flags |= cls._flag_map[control_character]\n    except KeyError:\n        raise ValueError(('Control character %r unknown as parsed from line %r' % (control_character, line)))\n    (from_ref_string, to_ref_string) = from_to.split(':')\n    if (flags & cls.DELETED):\n        from_ref = None\n    else:\n        from_ref = Reference.from_path(remote.repo, from_ref_string)\n    old_commit = None\n    if summary.startswith('['):\n        if ('[rejected]' in summary):\n            flags |= cls.REJECTED\n        elif ('[remote rejected]' in summary):\n            flags |= cls.REMOTE_REJECTED\n        elif ('[remote failure]' in summary):\n            flags |= cls.REMOTE_FAILURE\n        elif ('[no match]' in summary):\n            flags |= cls.ERROR\n        elif ('[new tag]' in summary):\n            flags |= cls.NEW_TAG\n        elif ('[new branch]' in summary):\n            flags |= cls.NEW_HEAD\n    else:\n        split_token = '...'\n        if (control_character == ' '):\n            split_token = '..'\n        (old_sha, new_sha) = summary.split(' ')[0].split(split_token)\n        old_commit = remote.repo.commit(old_sha)\n    return PushInfo(flags, from_ref, to_ref_string, remote, old_commit, summary)\n", "label": 1}
{"function": "\n\ndef init(self, modelDocument):\n    super(ModelRssItem, self).init(modelDocument)\n    try:\n        if (self.modelXbrl.modelManager.rssWatchOptions.latestPubDate and (self.pubDate <= self.modelXbrl.modelManager.rssWatchOptions.latestPubDate)):\n            self.status = _('tested')\n        else:\n            self.status = _('not tested')\n    except AttributeError:\n        self.status = _('not tested')\n    self.results = None\n    self.assertions = None\n", "label": 0}
{"function": "\n\n@attr('numpy')\ndef test_empty(self):\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest('numpy not available.')\n    G = networkx.Graph()\n    assert_equal(networkx.hits(G), ({\n        \n    }, {\n        \n    }))\n    assert_equal(networkx.hits_numpy(G), ({\n        \n    }, {\n        \n    }))\n    assert_equal(networkx.authority_matrix(G).shape, (0, 0))\n    assert_equal(networkx.hub_matrix(G).shape, (0, 0))\n", "label": 0}
{"function": "\n\ndef test_pos_list_append_with_nonexistent_key(self):\n    '\\n        Invoke list_append() with non-existent key\\n        '\n    charSet = 'abcdefghijklmnopqrstuvwxyz1234567890'\n    minLength = 5\n    maxLength = 30\n    length = random.randint(minLength, maxLength)\n    key = ('test', 'demo', (''.join(map((lambda unused: random.choice(charSet)), range(length))) + '.com'))\n    status = self.as_connection.list_append(key, 'abc', 122)\n    assert (status == 0)\n    (key, _, bins) = self.as_connection.get(key)\n    self.as_connection.remove(key)\n    assert (status == 0)\n    assert (bins == {\n        'abc': [122],\n    })\n", "label": 0}
{"function": "\n\ndef start(self):\n    'Start watching the directory for changes.'\n    with self._inotify_fd_lock:\n        if (self._inotify_fd < 0):\n            return\n        self._inotify_poll.register(self._inotify_fd, select.POLLIN)\n        for directory in self._directories:\n            self._add_watch_for_path(directory)\n", "label": 0}
{"function": "\n\ndef test_one(self):\n    red = self.make_target('red', RedTarget)\n    with self.mutex_group(targets=[red]) as (red_viewer, blue_viewer, green_viewer):\n        red_viewer.execute()\n        blue_viewer.execute()\n        green_viewer.execute()\n        self.assertEqual([red], red_viewer.executed)\n        self.assertIsNone(blue_viewer.executed)\n        self.assertIsNone(green_viewer.executed)\n", "label": 0}
{"function": "\n\ndef test_invalid_base_fields(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.ForeignKey('testapp.Author'), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E005')\n    assert ('Base field for list must be' in errors[0].msg)\n", "label": 0}
{"function": "\n\n@contextlib.contextmanager\ndef _assert_warns_context_manager(warning_class=None, warnings_test=None):\n    '\\n    Builds a context manager for testing code that should throw a warning.\\n    This will look for a given class, call a custom test, or both.\\n\\n    Args:\\n        warning_class - a class or subclass of Warning. If not None, then\\n            the context manager will raise an AssertionError if the block\\n            does not throw at least one warning of that type.\\n        warnings_test - a function which takes a list of warnings caught,\\n            and makes a number of assertions about the result. If the function\\n            returns without an exception, the context manager will consider\\n            this a successful assertion.\\n    '\n    with warnings.catch_warnings(record=True) as caught:\n        warnings.resetwarnings()\n        if warning_class:\n            warnings.simplefilter('ignore')\n            warnings.simplefilter('always', category=warning_class)\n        else:\n            warnings.simplefilter('always')\n        (yield)\n        assert_gt(len(caught), 0, 'expected at least one warning to be thrown')\n        if warnings_test:\n            warnings_test(caught)\n", "label": 0}
{"function": "\n\ndef _AtNonLeaf(self, attr_value, path):\n    'Called when at a non-leaf value. Should recurse and yield values.'\n    try:\n        if isinstance(attr_value, collections.Mapping):\n            sub_obj = attr_value.get(path[1])\n            if (len(path) > 2):\n                sub_obj = self.Expand(sub_obj, path[2:])\n            if isinstance(sub_obj, basestring):\n                (yield sub_obj)\n            elif isinstance(sub_obj, collections.Mapping):\n                for (k, v) in sub_obj.items():\n                    (yield {\n                        k: v,\n                    })\n            else:\n                for value in sub_obj:\n                    (yield value)\n        else:\n            for sub_obj in attr_value:\n                for value in self.Expand(sub_obj, path[1:]):\n                    (yield value)\n    except TypeError:\n        for value in self.Expand(attr_value, path[1:]):\n            (yield value)\n", "label": 1}
{"function": "\n\ndef write_file(filename, content):\n    'Write content to file.'\n    (_dir, _) = os.path.split(filename)\n    if (not os.path.exists(_dir)):\n        logging.debug('The directory %s not exists, create it', _dir)\n        mkdir_p(_dir)\n    with io.open(filename, 'wt', encoding='utf-8') as fd:\n        fd.write(content)\n", "label": 0}
{"function": "\n\ndef longRunHighGrayLevelEmphasis(self, P_glrl, ivector, jvector, sumP_glrl, meanFlag=True):\n    try:\n        lrhgle = (numpy.sum(numpy.sum(((P_glrl * (ivector ** 2)[:, None, None]) * (jvector ** 2)[None, :, None]), 0), 0) / sumP_glrl[None, None, :])\n    except ZeroDivisionError:\n        lrhgle = 0\n    if meanFlag:\n        return lrhgle.mean()\n    else:\n        return lrhgle\n", "label": 0}
{"function": "\n\ndef clean_votes(self, value):\n    assert (value > 0), 'Must be greater than 0.'\n    assert (value < 51), 'Must be less than 51.'\n    return value\n", "label": 0}
{"function": "\n\ndef test_validate_bad_choice_in_list(self):\n    config = _root({\n        'foo': 3,\n    })\n    with self.assertRaises(confuse.ConfigValueError):\n        config['foo'].get(confuse.Choice([1, 2, 4, 8, 16]))\n", "label": 0}
{"function": "\n\ndef host_to_ip(host):\n    '\\n    Returns the IP address of a given hostname\\n    '\n    try:\n        (family, socktype, proto, canonname, sockaddr) = socket.getaddrinfo(host, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0]\n        if (family == socket.AF_INET):\n            (ip, port) = sockaddr\n        elif (family == socket.AF_INET6):\n            (ip, port, flow_info, scope_id) = sockaddr\n    except Exception:\n        ip = None\n    return ip\n", "label": 0}
{"function": "\n\ndef load(self, path):\n    with open(path, 'r') as in_f:\n        buf = in_f.read()\n    self.history = buf.split('\\n')\n", "label": 0}
{"function": "\n\ndef test_post(self):\n    req = self.factory.post('/?from=/foo/')\n    with mock.patch.object(Form, 'execute') as form_execute:\n        form_execute.return_value = object()\n        resp = handle_form(req, form_node_pk=self.form.node.pk)\n    (args, kwargs) = form_execute.call_args\n    self.assertIs(args[0], req)\n    self.assertIsInstance(args[1], forms.BaseForm)\n    self.assertIs(resp, form_execute.return_value)\n", "label": 0}
{"function": "\n\ndef test_customservice_get_session_list(self):\n    with HTTMock(wechat_api_mock):\n        result = self.client.customservice.get_session_list('test1@test')\n        self.assertEqual(2, len(result))\n", "label": 0}
{"function": "\n\ndef test_gzip():\n    res = app.get('/', extra_environ=dict(HTTP_ACCEPT_ENCODING='gzip'))\n    assert (int(res.header('content-length')) == len(res.body))\n    assert (res.body != b'this is a test')\n    actual = gzip.GzipFile(fileobj=six.BytesIO(res.body)).read()\n    assert (actual == b'this is a test')\n", "label": 0}
{"function": "\n\ndef __init__(self, callback):\n    _CallbackExceptionHelper.__init__(self)\n\n    @wraps(callback)\n    def wrapper(ssl, out, outlen, in_, inlen, arg):\n        try:\n            conn = Connection._reverse_mapping[ssl]\n            instr = _ffi.buffer(in_, inlen)[:]\n            protolist = []\n            while instr:\n                encoded_len = indexbytes(instr, 0)\n                proto = instr[1:(encoded_len + 1)]\n                protolist.append(proto)\n                instr = instr[(encoded_len + 1):]\n            outstr = callback(conn, protolist)\n            if (not isinstance(outstr, _binary_type)):\n                raise TypeError('ALPN callback must return a bytestring.')\n            conn._alpn_select_callback_args = [_ffi.new('unsigned char *', len(outstr)), _ffi.new('unsigned char[]', outstr)]\n            outlen[0] = conn._alpn_select_callback_args[0][0]\n            out[0] = conn._alpn_select_callback_args[1]\n            return 0\n        except Exception as e:\n            self._problems.append(e)\n            return 2\n    self.callback = _ffi.callback('int (*)(SSL *, unsigned char **, unsigned char *, const unsigned char *, unsigned int, void *)', wrapper)\n", "label": 0}
{"function": "\n\ndef to_ctype(self, parakeet_type):\n    if isinstance(parakeet_type, (NoneT, ScalarT)):\n        return type_mappings.to_ctype(parakeet_type)\n    elif isinstance(parakeet_type, TupleT):\n        return self.struct_type_from_fields(parakeet_type.elt_types)\n    elif isinstance(parakeet_type, PtrT):\n        return self.ptr_struct_type(parakeet_type.elt_type)\n    elif isinstance(parakeet_type, ArrayT):\n        elt_t = parakeet_type.elt_type\n        rank = parakeet_type.rank\n        return self.array_struct_type(elt_t, rank)\n    elif isinstance(parakeet_type, SliceT):\n        return self.slice_struct_type()\n    elif isinstance(parakeet_type, ClosureT):\n        return self.struct_type_from_fields(parakeet_type.arg_types)\n    elif isinstance(parakeet_type, TypeValueT):\n        return 'int'\n    else:\n        assert False, (\"Don't know how to make C type for %s\" % parakeet_type)\n", "label": 0}
{"function": "\n\n@mock.patch.object(iscsi_deploy, '_save_disk_layout', autospec=True)\n@mock.patch.object(iscsi_deploy, 'LOG', autospec=True)\n@mock.patch.object(iscsi_deploy, 'get_deploy_info', autospec=True)\n@mock.patch.object(iscsi_deploy, 'InstanceImageCache', autospec=True)\n@mock.patch.object(manager_utils, 'node_power_action', autospec=True)\n@mock.patch.object(deploy_utils, 'deploy_partition_image', autospec=True)\ndef test_continue_deploy(self, deploy_mock, power_mock, mock_image_cache, mock_deploy_info, mock_log, mock_disk_layout):\n    kwargs = {\n        'address': '123456',\n        'iqn': 'aaa-bbb',\n        'key': 'fake-56789',\n    }\n    self.node.provision_state = states.DEPLOYWAIT\n    self.node.target_provision_state = states.ACTIVE\n    self.node.save()\n    mock_deploy_info.return_value = {\n        'address': '123456',\n        'boot_option': 'netboot',\n        'configdrive': \"I've got the power\",\n        'ephemeral_format': None,\n        'ephemeral_mb': 0,\n        'image_path': '/var/lib/ironic/images/1be26c0b-03f2-4d2e-ae87-c02d7f33c123/disk',\n        'iqn': 'aaa-bbb',\n        'lun': '1',\n        'node_uuid': '1be26c0b-03f2-4d2e-ae87-c02d7f33c123',\n        'port': '3260',\n        'preserve_ephemeral': True,\n        'root_mb': 102400,\n        'swap_mb': 0,\n    }\n    log_params = mock_deploy_info.return_value.copy()\n    log_params['configdrive'] = '***'\n    expected_dict = {\n        'node': self.node.uuid,\n        'params': log_params,\n    }\n    uuid_dict_returned = {\n        'root uuid': '12345678-87654321',\n    }\n    deploy_mock.return_value = uuid_dict_returned\n    with task_manager.acquire(self.context, self.node.uuid, shared=False) as task:\n        mock_log.isEnabledFor.return_value = True\n        retval = iscsi_deploy.continue_deploy(task, **kwargs)\n        mock_log.debug.assert_called_once_with(mock.ANY, expected_dict)\n        self.assertEqual(states.DEPLOYWAIT, task.node.provision_state)\n        self.assertEqual(states.ACTIVE, task.node.target_provision_state)\n        self.assertIsNone(task.node.last_error)\n        mock_image_cache.assert_called_once_with()\n        mock_image_cache.return_value.clean_up.assert_called_once_with()\n        self.assertEqual(uuid_dict_returned, retval)\n        mock_disk_layout.assert_called_once_with(task.node, mock.ANY)\n", "label": 0}
{"function": "\n\ndef test_some_incompatible(self):\n    red = self.make_target('red', RedTarget)\n    blue = self.make_target('blue', BlueTarget)\n    with self.mutex_group(targets=[red, blue]) as (red_viewer, blue_viewer, green_viewer):\n        self.assert_incompatible_activations(red_viewer)\n        self.assert_incompatible_activations(blue_viewer)\n        green_viewer.execute()\n        self.assertIsNone(green_viewer.executed)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    TestCase.__init__(self, *args, **kwargs)\n    for attr in [x for x in dir(self) if x.startswith('test')]:\n        meth = getattr(self, attr)\n\n        def test_(self):\n            try:\n                meth()\n            except psutil.AccessDenied:\n                pass\n        setattr(self, attr, types.MethodType(test_, self))\n", "label": 0}
{"function": "\n\ndef write_file(self, filename):\n    '\\n        Pass schema object to template engine to be rendered for use.\\n\\n        :param filename: output filename\\n        :return:\\n        '\n    template = self.template_env.get_template('settings.py.j2')\n    settings = template.render(endpoints=OrderedDict([(endpoint, self.format_endpoint(schema)) for (endpoint, schema) in self.endpoints.iteritems()]))\n    with open(filename, 'w') as ofile:\n        ofile.write((settings + '\\n'))\n", "label": 0}
{"function": "\n\ndef test_install_one_host(self):\n    args = self.parser.parse_args('install host1'.split())\n    assert (args.host == ['host1'])\n", "label": 0}
{"function": "\n\ndef SaveTemporaryFile(fp):\n    'Store incoming database file in a temporary directory.'\n    loc = data_store.DB.Location()\n    if (not os.path.exists(loc)):\n        return False\n    if (not os.path.isdir(loc)):\n        return False\n    filecopy_len_str = fp.read(sutils.SIZE_PACKER.size)\n    filecopy_len = sutils.SIZE_PACKER.unpack(filecopy_len_str)[0]\n    filecopy = rdf_data_server.DataServerFileCopy(fp.read(filecopy_len))\n    rebdir = _CreateDirectory(loc, filecopy.rebalance_id)\n    filedir = utils.JoinPath(rebdir, filecopy.directory)\n    try:\n        os.makedirs(filedir)\n    except OSError:\n        pass\n    filepath = utils.JoinPath(filedir, filecopy.filename)\n    logging.info('Writing to file %s', filepath)\n    with open(filepath, 'wb') as wp:\n        decompressor = zlib.decompressobj()\n        while True:\n            block_len_str = fp.read(sutils.SIZE_PACKER.size)\n            block_len = sutils.SIZE_PACKER.unpack(block_len_str)[0]\n            if (not block_len):\n                break\n            block = fp.read(block_len)\n            to_decompress = (decompressor.unconsumed_tail + block)\n            while to_decompress:\n                decompressed = decompressor.decompress(to_decompress)\n                if decompressed:\n                    wp.write(decompressed)\n                    to_decompress = decompressor.unconsumed_tail\n                else:\n                    to_decompress = ''\n        remaining = decompressor.flush()\n        if remaining:\n            wp.write(remaining)\n    if (os.path.getsize(filepath) != filecopy.size):\n        logging.error('Size of file %s is not %d', filepath, filecopy.size)\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef __int(value):\n    'validate an integer'\n    (valid, _value) = (False, value)\n    try:\n        _value = int(value)\n        valid = True\n    except ValueError:\n        pass\n    return (valid, _value, 'integer')\n", "label": 0}
{"function": "\n\ndef test_can_update_status_via_trigger_on_participant_balance(self):\n    self.db.run(\"UPDATE participants SET balance=10, status_of_1_0_payout='pending-application' WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 10)\n    assert (alice.status_of_1_0_payout == 'pending-application')\n    self.db.run(\"UPDATE participants SET balance=0 WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 0)\n    assert (alice.status_of_1_0_payout == 'completed')\n", "label": 0}
{"function": "\n\n@app.route('/v1/repositories/<path:repository>/tags', methods=['GET'])\n@toolkit.parse_repository_name\n@toolkit.requires_auth\n@mirroring.source_lookup_tag\ndef _get_tags(namespace, repository):\n    logger.debug('[get_tags] namespace={0}; repository={1}'.format(namespace, repository))\n    try:\n        data = get_tags(namespace=namespace, repository=repository)\n    except exceptions.FileNotFoundError:\n        return toolkit.api_error('Repository not found', 404)\n    return toolkit.response(data)\n", "label": 0}
{"function": "\n\ndef test_unknown_apps_are_ignored(self):\n    'Unknown engines get ignored.'\n    self.create_appversion('firefox', '33.0a1')\n    self.create_appversion('thunderbird', '33.0a1')\n    data = {\n        'engines': {\n            'firefox': '>=33.0a1',\n            'thunderbird': '>=33.0a1',\n            'node': '>=0.10',\n        },\n    }\n    apps = self.parse(data)['apps']\n    engines = [app.appdata.short for app in apps]\n    assert (sorted(engines) == ['firefox', 'thunderbird'])\n", "label": 0}
{"function": "\n\ndef _trending_for_month(metric=None):\n    this_month_date = month_for_date(datetime.date.today())\n    previous_month_date = get_previous_month(this_month_date)\n    previous_month_year_date = get_previous_year(this_month_date)\n    data = {\n        'month': 0,\n        'previous_month': 0,\n        'previous_month_year': 0,\n    }\n    try:\n        month = MetricMonth.objects.get(metric=metric, created=this_month_date)\n        data['month'] = month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)\n        data['previous_month'] = previous_month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)\n        data['previous_month_year'] = previous_month_year.num\n    except ObjectDoesNotExist:\n        pass\n    return data\n", "label": 0}
{"function": "\n\ndef report_to_ci_server(self, project):\n    for report in self.reports:\n        test_name = report['test']\n        test_failed = (report['success'] is not True)\n        with test_proxy_for(project).and_test_name(('Integrationtest.%s' % test_name)) as test:\n            if test_failed:\n                test.fails(report['exception'])\n", "label": 0}
{"function": "\n\n@patch('paasta_tools.cli.cmds.check.read_service_configuration')\n@patch('paasta_tools.cli.cmds.check.is_file_in_dir')\n@patch('sys.stdout', new_callable=StringIO)\ndef test_check_smartstack_check_missing_instance(mock_stdout, mock_is_file_in_dir, mock_read_service_info):\n    mock_is_file_in_dir.return_value = True\n    smartstack_dict = {\n        \n    }\n    mock_read_service_info.return_value = smartstack_dict\n    expected_output = ('%s\\n%s\\n' % (PaastaCheckMessages.SMARTSTACK_YAML_FOUND, PaastaCheckMessages.SMARTSTACK_PORT_MISSING))\n    smartstack_check(service='fake_service', service_path='path', soa_dir='path')\n    output = mock_stdout.getvalue()\n    assert (output == expected_output)\n", "label": 0}
{"function": "\n\ndef test_None_on_sys_path(self):\n    new_path = sys.path[:]\n    new_path.insert(0, None)\n    new_path_importer_cache = sys.path_importer_cache.copy()\n    new_path_importer_cache.pop(None, None)\n    new_path_hooks = [zipimport.zipimporter, _bootstrap.FileFinder.path_hook(*_bootstrap._get_supported_file_loaders())]\n    missing = object()\n    email = sys.modules.pop('email', missing)\n    try:\n        with util.import_state(meta_path=sys.meta_path[:], path=new_path, path_importer_cache=new_path_importer_cache, path_hooks=new_path_hooks):\n            module = import_module('email')\n            self.assertIsInstance(module, ModuleType)\n    finally:\n        if (email is not missing):\n            sys.modules['email'] = email\n", "label": 0}
{"function": "\n\ndef compare_file_contents(self, artifact_path, directory):\n    '\\n    Tests the ivy.xml and pom\\n    :param artifact_path: Path of the artifact\\n    :param directory: Directory where the artifact resides.\\n    :return:\\n    '\n    [package_dir, artifact_name, version] = directory.rsplit(os.path.sep, 2)\n    file_name = os.path.basename(artifact_path)\n    golden_file_nm = os.path.join(JarPublishIntegrationTest.GOLDEN_DATA_DIR, package_dir.replace(os.path.sep, '.'), artifact_name, file_name)\n    with open(artifact_path, 'r') as test_file:\n        generated_file = test_file.read()\n        with open(golden_file_nm, 'r') as golden_file:\n            golden_file_contents = golden_file.read()\n            if artifact_path.endswith('.xml'):\n                generated_file = re.sub('publication=.*', '/>', generated_file)\n        return self.assertMultiLineEqual(generated_file, golden_file_contents)\n", "label": 0}
{"function": "\n\ndef application(environ, start_response):\n    '\\n    The main WSGI application. Dispatch the current request to\\n    the functions from above and store the regular expression\\n    captures in the WSGI environment as  `oic.url_args` so that\\n    the functions from above can access the url placeholders.\\n\\n    If nothing matches call the `not_found` function.\\n\\n    :param environ: The HTTP application environment\\n    :param start_response: The application to run when the handling of the\\n        request is done\\n    :return: The response as a list of lines\\n    '\n    global OAS\n    path = environ.get('PATH_INFO', '').lstrip('/')\n    logger = logging.getLogger('oicServer')\n    if (path == 'robots.txt'):\n        return static(environ, start_response, 'static/robots.txt')\n    environ['oic.oas'] = OAS\n    if path.startswith('static/'):\n        return static(environ, start_response, path)\n    for (regex, callback) in URLS:\n        match = re.search(regex, path)\n        if (match is not None):\n            try:\n                environ['oic.url_args'] = match.groups()[0]\n            except IndexError:\n                environ['oic.url_args'] = path\n            logger.info(('callback: %s' % callback))\n            try:\n                return callback(environ, start_response, logger)\n            except Exception as err:\n                print(('%s' % err))\n                message = traceback.format_exception(*sys.exc_info())\n                print(message)\n                logger.exception(('%s' % err))\n                resp = ServiceError(('%s' % err))\n                return resp(environ, start_response)\n    LOGGER.debug(('unknown side: %s' % path))\n    resp = NotFound(\"Couldn't find the side you asked for!\")\n    return resp(environ, start_response)\n", "label": 0}
{"function": "\n\ndef delete_policy_port_binding(context, policy_id, port_id):\n    try:\n        with context.session.begin(subtransactions=True):\n            db_object = db.model_query(context, models.QosPortPolicyBinding).filter_by(policy_id=policy_id, port_id=port_id).one()\n            context.session.delete(db_object)\n    except orm_exc.NoResultFound:\n        raise n_exc.PortQosBindingNotFound(port_id=port_id, policy_id=policy_id)\n", "label": 0}
{"function": "\n\ndef decode(self, file):\n    fStart = file.tell()\n    identifier = None\n    try:\n        identifier = self.iEIEncoder.decode(file)\n    except UDHInformationElementIdentifierUnknownError:\n        pass\n    length = self.int8Encoder.decode(file)\n    data = None\n    if (identifier in self.dataEncoders):\n        data = self.dataEncoders[identifier].decode(file)\n    elif (length > 0):\n        data = self.read(file, length)\n    parsed = (file.tell() - fStart)\n    if (parsed != (length + 2)):\n        raise UDHParseError(('Invalid length: expected %d, parsed %d' % ((length + 2), parsed)))\n    if (identifier is None):\n        return None\n    return gsm_types.InformationElement(identifier, data)\n", "label": 0}
{"function": "\n\ndef test_task_loader_has_cmd_list(self, monkeypatch):\n    cmd_names = []\n\n    def save_cmd_names(self, params, args):\n        cmd_names.extend(self.loader.cmd_names)\n    monkeypatch.setattr(Run, 'execute', save_cmd_names)\n    cmd_main([])\n    assert ('list' in cmd_names)\n", "label": 0}
{"function": "\n\ndef is_iterable(maybe_iter, unless=(string_types, dict)):\n    \" Return whether ``maybe_iter`` is an iterable, unless it's an instance of one\\n    of the base class, or tuple of base classes, given in ``unless``.\\n\\n    Example::\\n\\n        >>> is_iterable('foo')\\n        False\\n        >>> is_iterable(['foo'])\\n        True\\n        >>> is_iterable(['foo'], unless=list)\\n        False\\n        >>> is_iterable(xrange(5))\\n        True\\n    \"\n    try:\n        iter(maybe_iter)\n    except TypeError:\n        return False\n    return (not isinstance(maybe_iter, unless))\n", "label": 0}
{"function": "\n\ndef test_func_adds_roots(self):\n\n    def add_roots(doc):\n        doc.add_root(AnotherModelInTestFunction())\n        doc.add_root(SomeModelInTestFunction())\n    handler = FunctionHandler(add_roots)\n    doc = Document()\n    handler.modify_document(doc)\n    if handler.failed:\n        raise RuntimeError(handler.error)\n    assert (len(doc.roots) == 2)\n", "label": 0}
{"function": "\n\ndef listen(self):\n    'Listen to incoming clients until\\n        self._running is set to False\\n        '\n    l = self.listener\n    self._running = True\n    try:\n        while self._running:\n            log.debug('Accept connection')\n            c = l.accept()\n            try:\n                action = c.recv()\n            except EOFError:\n                c.close()\n                continue\n            if isinstance(action, basestring):\n                args = ()\n                kwargs = {\n                    \n                }\n            else:\n                args = action.get('args', ())\n                kwargs = (action.get('kwargs') or {\n                    \n                })\n                action = action.get('action')\n            log.info(('Dispatch action \"%s\"' % action))\n            method = getattr(self, ('dispatch_%s' % action), None)\n            if method:\n                try:\n                    result = method(*args, **kwargs)\n                except Exception as err:\n                    log.exception(err)\n                    c.send({\n                        'error': True,\n                        'message': ('Exception in action %s - %s' % (action, err)),\n                    })\n                else:\n                    c.send({\n                        'error': False,\n                        'message': 'ok',\n                        'result': result,\n                    })\n            else:\n                log.warn(('No action %s' % action))\n                c.send({\n                    'error': True,\n                    'message': ('No action %s' % action),\n                })\n            c.close()\n    finally:\n        self._listener = None\n        l.close()\n    log.info('Exiting event loop')\n", "label": 0}
{"function": "\n\ndef convert_fragment(self, fragment, fd):\n    mdat = None\n    try:\n        f4v = F4V(fd, raw_payload=True)\n        for box in f4v:\n            if (box.type == 'mdat'):\n                mdat = box.payload.data\n                break\n    except F4VError as err:\n        self.logger.error('Failed to parse fragment {0}-{1}: {2}', fragment.segment, fragment.fragment, err)\n        return\n    if (not mdat):\n        self.logger.error('No MDAT box found in fragment {0}-{1}', fragment.segment, fragment.fragment)\n        return\n    try:\n        for chunk in self.concater.iter_chunks(buf=mdat, skip_header=True):\n            self.reader.buffer.write(chunk)\n            if self.closed:\n                break\n        else:\n            self.logger.debug('Download of fragment {0}-{1} complete', fragment.segment, fragment.fragment)\n    except IOError as err:\n        if ('Unknown tag type' in str(err)):\n            self.logger.error('Unknown tag type found, this stream is probably encrypted')\n            self.close()\n            return\n        self.logger.error('Error reading fragment {0}-{1}: {2}', fragment.segment, fragment.fragment, err)\n", "label": 0}
{"function": "\n\ndef test_os_stat(self):\n    'Test sizing os.stat and os.statvfs objects.\\n        '\n    try:\n        stat = os.stat(__file__)\n    except Exception:\n        pass\n    else:\n        stat_size = asizeof.asizeof(stat)\n        self.assertTrue((stat_size > 0), stat_size)\n        refs = asizeof.named_refs(stat)\n        ref_names = set([name for (name, _) in refs])\n        self.assertTrue((set(['st_mode', 'st_size', 'st_mtime']) <= ref_names), ref_names)\n    try:\n        stat = os.statvfs(__file__)\n    except Exception:\n        pass\n    else:\n        stat_size = asizeof.asizeof(stat)\n        self.assertTrue((stat_size > 0), stat_size)\n        refs = asizeof.named_refs(stat)\n        ref_names = set([name for (name, _) in refs])\n        self.assertTrue((set(['f_bsize', 'f_blocks']) <= ref_names), ref_names)\n", "label": 0}
{"function": "\n\ndef encrypt(self, data):\n    ' encrypt data with convergence encryption.\\n\\n            Args\\n                data: str, the plain text to be encrypted\\n        \\n            Returns\\n                key: hash(block), encryption key\\n                id: hash(hash(block), block ID\\n                ciphertext: enc(key, block)\\n        '\n    assert isinstance(data, str)\n    (key, id) = self.__sec_key(data)\n    return (key, id, aes(key, data))\n", "label": 0}
{"function": "\n\ndef _unshorten_lnxlu(self, uri):\n    try:\n        r = requests.get(uri, headers=self._headers, timeout=self._timeout)\n        html = r.text\n        code = re.findall('/\\\\?click\\\\=(.*)\\\\.\"', html)\n        if (len(code) > 0):\n            payload = {\n                'click': code[0],\n            }\n            r = requests.get('http://lnx.lu/', params=payload, headers=self._headers, timeout=self._timeout)\n            return (r.url, r.status_code)\n        else:\n            return (uri, 'No click variable found')\n    except Exception as e:\n        return (uri, str(e))\n", "label": 0}
{"function": "\n\ndef test_encode_fields():\n    msg = APIEncodeMsg()\n    msg.enc = six.b('').join([six.int2byte(x) for x in range(0, 255)])\n    assert (msg.to_dict()['data']['enc'] == six.text_type('AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWprbG1ub3BxcnN0dXZ3eHl6e3x9fn+AgYKDhIWGh4iJiouMjY6PkJGSk5SVlpeYmZqbnJ2en6ChoqOkpaanqKmqq6ytrq+wsbKztLW2t7i5uru8vb6/wMHCw8TFxsfIycrLzM3Oz9DR0tPU1dbX2Nna29zd3t/g4eLj5OXm5+jp6uvs7e7v8PHy8/T19vf4+fr7/P3+'))\n    assert (msg.to_json() == '{\"data\": {\"enc\": \"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWprbG1ub3BxcnN0dXZ3eHl6e3x9fn+AgYKDhIWGh4iJiouMjY6PkJGSk5SVlpeYmZqbnJ2en6ChoqOkpaanqKmqq6ytrq+wsbKztLW2t7i5uru8vb6/wMHCw8TFxsfIycrLzM3Oz9DR0tPU1dbX2Nna29zd3t/g4eLj5OXm5+jp6uvs7e7v8PHy8/T19vf4+fr7/P3+\"}, \"type\": null}')\n    msg2 = APIEncodeMsg(data=msg.to_json())\n    assert (msg.to_dict() == msg2.to_dict())\n    assert (msg.to_json() == msg2.to_json())\n    assert (msg2.enc == msg.enc)\n    msg3 = APIEncodeMsg()\n    msg3.enc = six.u('xxxx')\n    assert (msg3.to_dict() == {\n        'data': {\n            'enc': 'eHh4eA==',\n        },\n        'type': None,\n    })\n    msg3.enc = six.b('xxxx')\n    assert (msg3.to_dict() == {\n        'data': {\n            'enc': 'eHh4eA==',\n        },\n        'type': None,\n    })\n    msg4 = APIEncodeMsg()\n    msg4.from_dict(msg.to_dict())\n    assert (msg4.to_dict() == msg.to_dict())\n", "label": 0}
{"function": "\n\ndef test_phone(self):\n    value = self.sd.phone(locale='es')\n    self.assertTrue(isinstance(value, six.string_types))\n    self.assertEqual(len(value), 9)\n    self.assertTrue((value[0] in ['6', '9']))\n    value = self.sd.phone(locale='es', country_code=True)\n    self.assertTrue(isinstance(value, six.string_types))\n    self.assertEqual(len(value), 13)\n    self.assertTrue((value[0:5] in ['+34 6', '+34 9']))\n    with self.assertRaises(ParameterError):\n        value = self.sd.phone(locale='not-valid-locale')\n", "label": 0}
{"function": "\n\ndef process_get_results_metadata(self, seqid, iprot, oprot):\n    args = get_results_metadata_args()\n    args.read(iprot)\n    iprot.readMessageEnd()\n    result = get_results_metadata_result()\n    try:\n        result.success = self._handler.get_results_metadata(args.handle)\n        msg_type = TMessageType.REPLY\n    except (TTransport.TTransportException, KeyboardInterrupt, SystemExit):\n        raise\n    except QueryNotFoundException as error:\n        msg_type = TMessageType.REPLY\n        result.error = error\n    except Exception as ex:\n        msg_type = TMessageType.EXCEPTION\n        logging.exception(ex)\n        result = TApplicationException(TApplicationException.INTERNAL_ERROR, 'Internal error')\n    oprot.writeMessageBegin('get_results_metadata', msg_type, seqid)\n    result.write(oprot)\n    oprot.writeMessageEnd()\n    oprot.trans.flush()\n", "label": 0}
{"function": "\n\n@add_method(Return)\ndef handle(self, ch):\n    msg = ch.message\n    msg.rx_channel = ch\n    if ch.on_return:\n        try:\n            ch.on_return(msg)\n        except Exception:\n            logger.error('ERROR in on_return() callback', exc_info=True)\n", "label": 0}
{"function": "\n\ndef shutdown(self):\n    if self.stopped:\n        return\n    self.stopped = True\n    try:\n        for handle in self.map_handles.values():\n            if (handle is not None):\n                handle.close()\n        for handle in self.file_handles.values():\n            if (handle is not None):\n                handle.close()\n    finally:\n        with self.lock:\n            if (self.create_lock_file is True):\n                os.remove(self.lock_file)\n    self.inited = False\n", "label": 0}
{"function": "\n\ndef unschedule(self, watch):\n    'Unschedules a watch.\\n\\n    :param watch:\\n        The watch to unschedule.\\n    :type watch:\\n        An instance of :class:`ObservedWatch` or a subclass of\\n        :class:`ObservedWatch`\\n    '\n    with self._lock:\n        try:\n            emitter = self._get_emitter_for_watch(watch)\n            self._remove_handlers_for_watch(watch)\n            self._remove_emitter(emitter)\n            self._watches.remove(watch)\n        except KeyError:\n            raise\n", "label": 0}
{"function": "\n\ndef test_considers_a_forum_without_tracks_as_unread_if_it_has_topics(self):\n    new_topic = create_topic(forum=self.forum_2_child_2, poster=self.u2)\n    PostFactory.create(topic=new_topic, poster=self.u2)\n    unread_forums = ForumReadTrack.objects.get_unread_forums_from_list(self.top_level_cat_1.get_descendants(include_self=True), self.u1)\n    assert (self.forum_2_child_2 in unread_forums)\n", "label": 0}
{"function": "\n\ndef fetch(self, _id=False):\n    if (not os.path.isfile(self.settings.file)):\n        print(\"TypeTodo: 'file' db does not exist, should be created.\")\n        return False\n    todoA = {\n        \n    }\n    try:\n        with codecs.open(self.settings.file, 'r', 'UTF-8') as f:\n            ctxTodo = None\n            for ln in f:\n                ln = ln.splitlines()[0]\n                matchParse = RE_TODO_STORED.match(ln)\n                if matchParse:\n                    cId = int(matchParse.group('id'))\n                    if (_id and (_id != cId)):\n                        continue\n                    rxEDate = matchParse.group('editdate')\n                    rxETime = matchParse.group('edittime')\n                    rxESecs = (matchParse.group('editsecs') or ':00')\n                    gmtTime = time.mktime(time.strptime(('%s %s%s' % (rxEDate, rxETime, rxESecs)), '%y/%m/%d %H:%M:%S'))\n                    if (cId not in todoA):\n                        todoA[cId] = TodoTask(cId, self.parentDB.config.projectName, self.parentDB)\n                    ctxTodo = matchParse\n                    if (cId > self.maxId):\n                        self.maxIdSaved = self.maxId = cId\n                    continue\n                if ctxTodo:\n                    __state = ctxTodo.group('prefix')\n                    if (ctxTodo.group(1) == '-'):\n                        __state = ''\n                    matchComment = RE_TODO_STORED_COMMENT.match(ln)\n                    todoA[int(ctxTodo.group('id'))].set(__state, ctxTodo.group('tags').split(','), int(ctxTodo.group('priority')), ctxTodo.group('context'), matchComment.group('comment'), ctxTodo.group('editor'), gmtTime)\n                    ctxTodo = None\n                    continue\n                maxIdParse = RE_TODO_FILE_MAXID.match(ln)\n                if maxIdParse:\n                    cId = int(maxIdParse.group('maxid'))\n                    if (cId > self.maxId):\n                        self.maxIdSaved = self.maxId = cId\n                    continue\n    except Exception as e:\n        print(\"TypeTodo: 'file' db experienced error while fetching\")\n        print(e)\n        self.dbOk = False\n        return False\n    return todoA\n", "label": 1}
{"function": "\n\ndef __dump_xml(self, filename):\n    self.log.info('Dumping final status as XML: %s', filename)\n    root = etree.Element('FinalStatus')\n    if self.last_sec:\n        for (label, kpiset) in iteritems(self.last_sec[DataPoint.CUMULATIVE]):\n            root.append(self.__get_xml_summary(label, kpiset))\n    with open(get_full_path(filename), 'wb') as fhd:\n        tree = etree.ElementTree(root)\n        tree.write(fhd, pretty_print=True, encoding='UTF-8', xml_declaration=True)\n", "label": 0}
{"function": "\n\ndef missing_datetimes(self, finite_datetimes):\n    try:\n        complete_parameters = self.of_cls.bulk_complete(map(self.datetime_to_parameter, finite_datetimes))\n        return (set(finite_datetimes) - set(map(self.parameter_to_datetime, complete_parameters)))\n    except NotImplementedError:\n        return infer_bulk_complete_from_fs(finite_datetimes, (lambda d: self._instantiate_task_cls(self.datetime_to_parameter(d))), (lambda d: d.strftime('(%Y).*(%m).*(%d)')))\n", "label": 0}
{"function": "\n\ndef backward_cpu(self, xs, gys):\n    assert (len(xs) == self.n_in)\n    assert (len(gys) == self.n_out)\n    return tuple((np.zeros_like(xs).astype(np.float32) for _ in six.moves.range(self.n_in)))\n", "label": 0}
{"function": "\n\ndef update(self):\n    ' Update loop for sensors->perception->control(->vehicle). '\n    try:\n        self.read_time()\n    except Exception as ex:\n        self.data.has_time = False\n        logging.exception(('CORE:\\tError in update loop (TIME) - %s' % ex))\n    try:\n        self.read_GPS()\n    except Exception as ex:\n        self.data.has_GPS = False\n        logging.exception(('CORE:\\tError in update loop (GPS) - %s' % ex))\n    try:\n        self.read_compass()\n    except Exception as ex:\n        self.data.has_compass = False\n        logging.exception(('CORE:\\tError in update loop (COMPASS) - %s' % ex))\n    try:\n        self.read_magnetometer()\n    except Exception as ex:\n        self.data.has_magnetometer = False\n        logging.exception(('CORE:\\tError in update loop (MAGNETOMETER) - %s' % ex))\n    try:\n        self.read_accelerometer()\n    except Exception as ex:\n        self.data.has_accelerometer = False\n        logging.exception(('CORE:\\tError in update loop (ACCELEROMETER) - %s' % ex))\n    try:\n        self.read_gyro()\n    except Exception as ex:\n        self.data.has_gyro = False\n        logging.exception(('CORE:\\tError in update loop (GYROSCOPE) - %s' % ex))\n    try:\n        self.read_temperature()\n    except Exception as ex:\n        self.data.has_temperature = False\n        logging.exception(('CORE:\\tError in update loop (TEMPERATURE) - %s' % ex))\n    try:\n        self._perception_unit.update(self.data)\n    except Exception as ex:\n        logging.exception(('CORE:\\tError in update loop (PERCEPTION) - %s' % ex))\n    try:\n        self._navigation_unit.update()\n    except Exception as ex:\n        logging.exception(('CORE:\\tError in update loop (NAVIGATION) - %s' % ex))\n", "label": 0}
{"function": "\n\ndef ssq_error(correct, estimate, mask):\n    'Compute the sum-squared-error for an image, where the estimate is\\n    multiplied by a scalar which minimizes the error. Sums over all pixels\\n    where mask is True. If the inputs are color, each color channel can be\\n    rescaled independently.'\n    assert (correct.ndim == 2)\n    if (np.sum(((estimate ** 2) * mask)) > 1e-05):\n        alpha = (np.sum(((correct * estimate) * mask)) / np.sum(((estimate ** 2) * mask)))\n    else:\n        alpha = 0.0\n    return np.sum((mask * ((correct - (alpha * estimate)) ** 2)))\n", "label": 0}
{"function": "\n\ndef transform(self, node):\n    if (self._tracks is not None):\n        if (not isinstance(node.op, tuple(self._tracks))):\n            return\n    givens = {\n        \n    }\n    missing = set()\n    for input in node.inputs:\n        if isinstance(input, theano.compile.SharedVariable):\n            pass\n        elif hasattr(input.tag, 'test_value'):\n            givens[input] = theano.shared(input.type.filter(input.tag.test_value), input.name, broadcastable=input.broadcastable, borrow=True)\n        else:\n            missing.add(input)\n    if missing:\n        givens.update(self.provide_inputs(node, missing))\n        missing.difference_update(givens.keys())\n    if missing:\n        if self.verbose:\n            print(('%s cannot meta-optimize %s, %d of %d input shapes unknown' % (self.__class__.__name__, node, len(missing), node.nin)))\n        return\n    if self.verbose:\n        print(('%s meta-optimizing %s (%d choices):' % (self.__class__.__name__, node, len(self.optimizers))))\n    timings = []\n    for opt in self.optimizers:\n        outputs = opt.transform(node)\n        if outputs:\n            try:\n                fn = theano.function([], outputs, givens=givens, on_unused_input='ignore')\n                timing = min((self.time_call(fn) for _ in range(3)))\n            except Exception as e:\n                if self.verbose:\n                    print(('* %s: exception' % opt), e)\n                continue\n            else:\n                if self.verbose:\n                    print(('* %s: %.5g sec' % (opt, timing)))\n                timings.append((timing, outputs, opt))\n        elif self.verbose:\n            print(('* %s: not applicable' % opt))\n    if timings:\n        timings.sort()\n        if self.verbose:\n            print(('= %s' % timings[0][2]))\n        return timings[0][1]\n    return\n", "label": 1}
{"function": "\n\ndef test_createindex_with_incorrect_set(self):\n    '\\n            Invoke createindex() with incorrect set\\n        '\n    policy = {\n        \n    }\n    retobj = TestIndex.client.index_integer_create('test', 'demo1', 'age', 'age_index', policy)\n    assert (retobj == 0)\n    TestIndex.client.index_remove('test', 'age_index', policy)\n", "label": 0}
{"function": "\n\ndef _send_mail(self, handler, trap, is_duplicate):\n    if (is_duplicate and (not handler['mail_on_duplicate'])):\n        return\n    mail = handler['mail']\n    if (not mail):\n        return\n    recipients = handler['mail'].get('recipients')\n    if (not recipients):\n        return\n    subject = (handler['mail']['subject'] % {\n        'trap_oid': trap.oid,\n        'trap_name': ObjectId(trap.oid).name,\n        'ipaddress': trap.host,\n        'hostname': self.resolver.hostname_or_ip(trap.host),\n    })\n    ctxt = dict(trap=trap, dest_host=self.hostname)\n    try:\n        stats.incr('mail_sent_attempted', 1)\n        send_trap_email(recipients, 'trapperkeeper', subject, self.template_env, ctxt)\n        stats.incr('mail_sent_successful', 1)\n    except socket.error as err:\n        stats.incr('mail_sent_failed', 1)\n        logging.warning('Failed to send e-mail for trap: %s', err)\n", "label": 0}
{"function": "\n\ndef test_build_graph(self, huang_darwiche_nodes):\n    bbn = build_bbn(huang_darwiche_nodes)\n    nodes = dict([(node.name, node) for node in bbn.nodes])\n    assert (nodes['f_a'].parents == [])\n    assert (nodes['f_b'].parents == [nodes['f_a']])\n    assert (nodes['f_c'].parents == [nodes['f_a']])\n    assert (nodes['f_d'].parents == [nodes['f_b']])\n    assert (nodes['f_e'].parents == [nodes['f_c']])\n    assert (nodes['f_f'].parents == [nodes['f_d'], nodes['f_e']])\n    assert (nodes['f_g'].parents == [nodes['f_c']])\n    assert (nodes['f_h'].parents == [nodes['f_e'], nodes['f_g']])\n", "label": 0}
{"function": "\n\ndef test_explicitmany_pass(self):\n    serializer = ExplicitManySerializer(data={\n        'embedded': [{\n            'test_field': 'Test',\n        }],\n    })\n    assert serializer.is_valid(), serializer.errors\n", "label": 0}
{"function": "\n\ndef _faster_to_representation(self, instance):\n    'Modified to_representation with optimizations.\\n\\n        1) Returns a plain old dict as opposed to OrderedDict.\\n            (Constructing ordered dict is ~100x slower than `{}`.)\\n        2) Ensure we use a cached list of fields\\n            (this optimization exists in DRF 3.2 but not 3.1)\\n\\n        Arguments:\\n            instance: a model instance or data object\\n        Returns:\\n            Dict of primitive datatypes.\\n        '\n    ret = {\n        \n    }\n    fields = self._readable_fields\n    for field in fields:\n        try:\n            attribute = field.get_attribute(instance)\n        except SkipField:\n            continue\n        if (attribute is None):\n            ret[field.field_name] = None\n        else:\n            ret[field.field_name] = field.to_representation(attribute)\n    return ret\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('reject_spec', reject_specs)\ndef test_reject_spec(self, reject_spec):\n    with pytest.raises(ValueError):\n        EntryPoint.parse(reject_spec)\n", "label": 0}
{"function": "\n\ndef store_catch_412_HTTPError(entity):\n    'Returns the stored Entity if the function succeeds or None if the 412 is caught.'\n    try:\n        return syn.store(entity)\n    except SynapseHTTPError as err:\n        if (err.response.status_code == 412):\n            return None\n        raise\n", "label": 0}
{"function": "\n\ndef unlink(self, name):\n    symlinks = self.read_bootstrap().get('symlinks', {\n        \n    })\n    removed_targets = set()\n    found = False\n    for (source_glob, target) in symlinks.items():\n        if self._islinkkey(source_glob, name):\n            found = True\n            for (source, target) in self.expandtargets(source_glob, target):\n                self._remove_link_target(source, target)\n                removed_targets.add(target)\n                shutil.move(source, target)\n                print(tty.progress('Moved {0} -> {1}'.format(collapseuser(source), collapseuser(target))))\n    if (not found):\n        raise StowError('No symlink found with name: {0}'.format(name))\n    try:\n        os.rmdir(os.path.join(self.symlink_dir, name))\n    except OSError as e:\n        if (e.errno != errno.ENOTEMPTY):\n            raise e\n    self.remove_symlink(name)\n    self._update_target_cache((set(self._cached_targets()) - removed_targets))\n", "label": 0}
{"function": "\n\ndef _make_table(self, table, fields):\n    'Set up the schema of the database. `fields` is a mapping\\n        from field names to `Type`s. Columns are added if necessary.\\n        '\n    with self.transaction() as tx:\n        rows = tx.query(('PRAGMA table_info(%s)' % table))\n    current_fields = set([row[1] for row in rows])\n    field_names = set(fields.keys())\n    if current_fields.issuperset(field_names):\n        return\n    if (not current_fields):\n        columns = []\n        for (name, typ) in fields.items():\n            columns.append('{0} {1}'.format(name, typ.sql))\n        setup_sql = 'CREATE TABLE {0} ({1});\\n'.format(table, ', '.join(columns))\n    else:\n        setup_sql = ''\n        for (name, typ) in fields.items():\n            if (name in current_fields):\n                continue\n            setup_sql += 'ALTER TABLE {0} ADD COLUMN {1} {2};\\n'.format(table, name, typ.sql)\n    with self.transaction() as tx:\n        tx.script(setup_sql)\n", "label": 0}
{"function": "\n\ndef test_record_addon_invalid(self, capfd):\n    open(os.path.join(os.getcwd(), 'mockOF', 'addons', 'ofxSomeAddon', 'untracked.txt'), 'w').close()\n    (_, err) = run_ofSM('record -p mockProject', capfd=capfd, desired_exit_status=1)\n    assert ('Repository has untracked files' in err)\n", "label": 0}
{"function": "\n\n@classmethod\ndef ensure_db_num(cls, db_num):\n    try:\n        return int(db_num)\n    except ValueError:\n        raise cls.InvalidDBNum()\n", "label": 0}
{"function": "\n\ndef undo(self, workflow_dict):\n    LOG.info('Running undo...')\n    try:\n        if (('databaseinfra' not in workflow_dict) and ('hosts' not in workflow_dict)):\n            LOG.info('We could not find a databaseinfra inside the workflow_dict')\n            return False\n        if (len(workflow_dict['hosts']) == 1):\n            return True\n        databaseinfraattr = DatabaseInfraAttr.objects.filter(databaseinfra=workflow_dict['databaseinfra'])\n        cs_credentials = get_credentials_for(environment=workflow_dict['environment'], credential_type=CredentialType.CLOUDSTACK)\n        networkapi_credentials = get_credentials_for(environment=workflow_dict['environment'], credential_type=CredentialType.NETWORKAPI)\n        cs_provider = CloudStackProvider(credentials=cs_credentials, networkapi_credentials=networkapi_credentials)\n        networkapi_equipment_id = workflow_dict.get('networkapi_equipment_id')\n        for infra_attr in databaseinfraattr:\n            networkapi_equipment_id = infra_attr.networkapi_equipment_id\n            networkapi_ip_id = infra_attr.networkapi_ip_id\n            if networkapi_ip_id:\n                LOG.info(('Removing network api IP for %s' % networkapi_ip_id))\n                if (not cs_provider.remove_networkapi_ip(equipment_id=networkapi_equipment_id, ip_id=networkapi_ip_id)):\n                    return False\n            LOG.info(('Removing secondary_ip for %s' % infra_attr.cs_ip_id))\n            if (not cs_provider.remove_secondary_ips(infra_attr.cs_ip_id)):\n                return False\n            LOG.info('Secondary ip deleted!')\n            infra_attr.delete()\n            LOG.info('Databaseinfraattr deleted!')\n        if networkapi_equipment_id:\n            cs_provider.remove_networkapi_equipment(equipment_id=networkapi_equipment_id)\n        return True\n    except Exception:\n        traceback = full_stack()\n        workflow_dict['exceptions']['error_codes'].append(DBAAS_0010)\n        workflow_dict['exceptions']['traceback'].append(traceback)\n        return False\n", "label": 0}
{"function": "\n\n@mock.patch('pyghmi.ipmi.command.Command', autospec=True)\ndef test_get_power_state(self, ipmi_mock):\n    cmd_mock = ipmi_mock.return_value\n    get_power_mock = cmd_mock.get_power\n    return_values = [{\n        'powerstate': 'error',\n    }, {\n        'powerstate': 'on',\n    }, {\n        'powerstate': 'off',\n    }]\n    get_power_mock.side_effect = (lambda : return_values.pop())\n    with task_manager.acquire(self.context, self.node.uuid) as task:\n        pstate = self.driver.power.get_power_state(task)\n        self.assertEqual(states.POWER_OFF, pstate)\n        pstate = self.driver.power.get_power_state(task)\n        self.assertEqual(states.POWER_ON, pstate)\n        pstate = self.driver.power.get_power_state(task)\n        self.assertEqual(states.ERROR, pstate)\n        self.assertEqual(3, get_power_mock.call_count, 'pyghmi.ipmi.command.Command.get_power was not called 3 times.')\n", "label": 0}
{"function": "\n\ndef manual_no_op_chain_test():\n    points = PointCloud(np.random.random([10, 2]))\n    t = Translation([3, 4])\n    chain = TransformChain([t, t.pseudoinverse()])\n    points_applied = chain.apply(points)\n    assert np.allclose(points_applied.points, points.points)\n", "label": 0}
{"function": "\n\ndef start(self, environment=None, user=None):\n    'Mark this result started.'\n    envs = [environment]\n    try:\n        latest = self.results.get(is_latest=True, tester=user, environment=environment)\n        if (latest.status == Result.STATUS.skipped):\n            envs = self.environments.all()\n    except ObjectDoesNotExist:\n        pass\n    for env in envs:\n        Result.objects.create(runcaseversion=self, tester=user, environment=env, status=Result.STATUS.started, user=user)\n", "label": 0}
{"function": "\n\ndef generate_conflicting_plot_options_with_json_writes_of_config():\n    \" if the user wrote their own options in the config file,\\n    then we'll raise the error when the call plot or iplot through\\n    _plot_option_logic\\n    \"\n\n    def gen_test(plot_options):\n\n        def test(self):\n            config = json.load(open(CONFIG_FILE))\n            with open(CONFIG_FILE, 'w') as f:\n                config.update(plot_options)\n                f.write(json.dumps(config))\n            self.assertRaises(PlotlyError, py._plot_option_logic, {\n                \n            })\n        return test\n    for (i, plot_options) in enumerate(TestPlotOptionLogic.conflicting_option_set):\n        setattr(TestPlotOptionLogic, 'test_conflicting_plot_options_with_json_writes_of_config{}'.format(i), gen_test(plot_options))\n", "label": 0}
{"function": "\n\ndef __getitem__(self, position):\n    with self.lock:\n        return self.vterm.vtscreen[position]\n", "label": 0}
{"function": "\n\n@pytest.mark.timeout(10)\n@pytest.mark.parametrize('ssh_command', [None, 'fake_ssh_command'])\ndef test_sshkey(tmpdir, ssh_command):\n    tmpdir = str(tmpdir)\n    test_path = os.path.dirname(os.path.realpath(__file__))\n    with open(os.path.join(test_path, 'private_key')) as k:\n        private_key = k.read()\n    abs_source = os.path.join(tmpdir, 'deploy~git')\n    dest = os.path.join(tmpdir, 'dest-dir')\n    with mock.patch('subprocess.check_call') as submock:\n        with mock.patch('git.Repo') as gitmock:\n\n            def check_environ(*args):\n                if ssh_command:\n                    with open(os.environ['GIT_SSH']) as gitfile:\n                        assert (ssh_command in gitfile.read())\n            submock.check_call.side_effect = check_environ\n            url = 'git@example.org:squadron/test-repo.git'\n            version = 'a057eb0faaa8'\n            with open(abs_source, 'w') as gfile:\n                gfile.write(get_git_file(url, '@version', 'filename', '--depth=1'))\n            dest = os.path.join(tmpdir, 'dest-dir2')\n            if ssh_command:\n                os.environ['GIT_SSH'] = ssh_command\n            finalfile = ext_git(abs_source, dest, {\n                'version': version,\n            }, get_loader(), {\n                'filename': (lambda : private_key),\n            })\n            expected_sub_calls = [mock.call('git clone --depth=1 -- {} {} '.format(url, finalfile).split())]\n            expected_git_calls = [mock.call(finalfile), mock.call().git.checkout(version)]\n            assert (expected_sub_calls == submock.mock_calls)\n            assert (expected_git_calls == gitmock.mock_calls)\n            if ssh_command:\n                assert ('GIT_SSH' in os.environ)\n                assert (os.environ['GIT_SSH'] == ssh_command)\n            else:\n                assert ('GIT_SSH' not in os.environ)\n", "label": 0}
{"function": "\n\ndef runtests(*test_args):\n    import django\n    try:\n        django.setup()\n    except AttributeError:\n        pass\n    import django.test.utils\n    runner_class = django.test.utils.get_runner(settings)\n    test_runner = runner_class(verbosity=1, interactive=True)\n    failures = test_runner.run_tests(['recommends'])\n    sys.exit(failures)\n", "label": 0}
{"function": "\n\ndef result_fail(self, environment=None, comment='', stepnumber=None, bug='', user=None):\n    'Create a failed result for this case.'\n    result = Result.objects.create(runcaseversion=self, tester=user, environment=environment, status=Result.STATUS.failed, comment=comment, user=user)\n    if (stepnumber is not None):\n        try:\n            step = self.caseversion.steps.get(number=stepnumber)\n        except CaseStep.DoesNotExist:\n            pass\n        else:\n            stepresult = StepResult(result=result, step=step)\n            stepresult.status = StepResult.STATUS.failed\n            stepresult.bug_url = bug\n            stepresult.save(user=user)\n    self.save(force_update=True, user=user)\n", "label": 0}
{"function": "\n\ndef test_upload_patched_500(self):\n    self.stream._upload_form_encoded = mock.MagicMock()\n    self.stream._upload_form_encoded.return_value()\n    self.stream._upload_form_encoded.return_value.status_code = 500\n\n    def _fivehundred():\n        self.stream.upload(self.uploadfile)\n    with self.assertRaises(ResponseError) as ex:\n        _fivehundred()\n        self.assertEqual(ex.message, 'Server error.')\n", "label": 0}
{"function": "\n\ndef main():\n    (options, args) = parser.parse_args()\n    tmpl = ConfigParser()\n    tmpl.read(options.input)\n    with open(options.output, 'wb') as outfile:\n        render(tmpl).write(outfile)\n", "label": 0}
{"function": "\n\ndef test_iter_deferred_language(self):\n    with translation.override('en'):\n        qs = Normal.objects.language()\n    with translation.override('ja'):\n        for (index, obj) in enumerate(qs, 1):\n            self.assertEqual(obj.shared_field, NORMAL[index].shared_field)\n            self.assertEqual(obj.translated_field, NORMAL[index].translated_field['ja'])\n", "label": 0}
{"function": "\n\ndef it_also_has_dict_style_get_rel_by_rId(self, rels_with_known_rel):\n    (rels, rId, known_rel) = rels_with_known_rel\n    assert (rels[rId] == known_rel)\n", "label": 0}
{"function": "\n\ndef ValidateTree(self, queries, additional_visitors=(), max_alter_rows=100000, allowed_engines=('InnoDB',), fail_fast=False):\n    'Validate a parse tree.\\n\\n    Args:\\n      tokens: pyparsing parse tree\\n\\n    Returns:\\n      Whether the tree validated\\n    '\n    visitors = ([ShardSetChecker(self._schema), AlterChecker(self._schema, max_alter_rows=max_alter_rows), CreateDatabaseChecker(self._schema), DropDatabaseChecker(self._schema), CreateTableChecker(self._schema, allowed_engines=allowed_engines), DropTableChecker(self._schema), ReplaceChecker(self._schema), ColumnChecker(self._schema)] + list(additional_visitors))\n    for query in queries:\n        self._CheckCancelled()\n        assert (query.getName() == 'query'), ('Invalid second-level token: %s' % query.getName())\n        logging.debug('Visiting: %s', query)\n        for visitor in visitors:\n            visitor.visit([query])\n            if (fail_fast and (visitor.Errors() or visitor.Warnings())):\n                self._Finalize(visitors)\n                return False\n        if self._callback:\n            self._callback(self._loc)\n    self._Finalize(visitors)\n    return ((not self._errors) and (not self._warnings))\n", "label": 0}
{"function": "\n\ndef get(self, *args):\n    self.preflight()\n    self.set_header('Content-Type', 'application/json')\n    m = self.settings.get('manager')\n    pname = ('%s.%s' % (args[0], args[1]))\n    if (not self.api_key.can_read(pname)):\n        raise HTTPError(403)\n    try:\n        info = m.info(pname)\n    except ProcessError:\n        self.set_status(404)\n        return self.write({\n            'error': 'not_found',\n        })\n    self.write(info)\n", "label": 0}
{"function": "\n\ndef send_test(self, test=True):\n    if (not test):\n        try:\n            del self._data['test']\n        except KeyError:\n            pass\n    else:\n        self._data['test'] = 1\n    return self\n", "label": 0}
{"function": "\n\ndef test_context_manager_before_rollover(self):\n    with tempfile.SpooledTemporaryFile(max_size=1) as f:\n        self.assertFalse(f._rolled)\n        self.assertFalse(f.closed)\n    self.assertTrue(f.closed)\n\n    def use_closed():\n        with f:\n            pass\n    self.assertRaises(ValueError, use_closed)\n", "label": 0}
{"function": "\n\ndef test_makemigrations_with_size(self):\n    field = SetCharField(models.CharField(max_length=5), max_length=32, size=5)\n    (statement, imports) = MigrationWriter.serialize(field)\n    assert re.compile('^django_mysql\\\\.models\\\\.SetCharField\\\\(\\n                models\\\\.CharField\\\\(max_length=5\\\\),\\\\ # space here\\n                (\\n                    max_length=32,\\\\ size=5|\\n                    size=5,\\\\ max_length=32\\n                )\\n                \\\\)$\\n            ', re.VERBOSE).match(statement)\n", "label": 0}
{"function": "\n\ndef print_stats(stats):\n    console_logger.info((((' %-' + str(STATS_NAME_WIDTH)) + 's %7s %12s %7s %7s %7s  | %7s %7s') % ('Name', '# reqs', '# fails', 'Avg', 'Min', 'Max', 'Median', 'req/s')))\n    console_logger.info(('-' * (80 + STATS_NAME_WIDTH)))\n    total_rps = 0\n    total_reqs = 0\n    total_failures = 0\n    for key in sorted(stats.iterkeys()):\n        r = stats[key]\n        total_rps += r.current_rps\n        total_reqs += r.num_requests\n        total_failures += r.num_failures\n        console_logger.info(r)\n    console_logger.info(('-' * (80 + STATS_NAME_WIDTH)))\n    try:\n        fail_percent = ((total_failures / float(total_reqs)) * 100)\n    except ZeroDivisionError:\n        fail_percent = 0\n    console_logger.info((((' %-' + str(STATS_NAME_WIDTH)) + 's %7d %12s %42.2f') % ('Total', total_reqs, ('%d(%.2f%%)' % (total_failures, fail_percent)), total_rps)))\n    console_logger.info('')\n", "label": 0}
{"function": "\n\ndef __getattr__(self, name):\n    t = type(self)\n    try:\n        i = t.name_to_idx[name]\n    except KeyError:\n        raise AttributeError(name)\n    f = t.fields[i]\n    if (i < len(self.data)):\n        v = self.data[i]\n    else:\n        v = ''\n    if (len(f) >= 3):\n        if (v == ''):\n            return None\n        return f[2](v)\n    else:\n        return v\n", "label": 0}
{"function": "\n\ndef test_public_school_detail_charter_funding(self, public_school_detail):\n    assert (public_school_detail.get('charter_funding') == 'Directly funded')\n", "label": 0}
{"function": "\n\ndef _test_sync_state_helper(self, known_net_ids, active_net_ids):\n    active_networks = set((mock.Mock(id=netid) for netid in active_net_ids))\n    with mock.patch(DHCP_PLUGIN) as plug:\n        mock_plugin = mock.Mock()\n        mock_plugin.get_active_networks_info.return_value = active_networks\n        plug.return_value = mock_plugin\n        dhcp = dhcp_agent.DhcpAgent(HOSTNAME)\n        attrs_to_mock = dict([(a, mock.DEFAULT) for a in ['disable_dhcp_helper', 'cache', 'safe_configure_dhcp_for_network']])\n        with mock.patch.multiple(dhcp, **attrs_to_mock) as mocks:\n            mocks['cache'].get_network_ids.return_value = known_net_ids\n            dhcp.sync_state()\n            diff = (set(known_net_ids) - set(active_net_ids))\n            exp_disable = [mock.call(net_id) for net_id in diff]\n            mocks['cache'].assert_has_calls([mock.call.get_network_ids()])\n            mocks['disable_dhcp_helper'].assert_has_calls(exp_disable)\n", "label": 0}
{"function": "\n\ndef test_iter_smart_fk_string_primary_key_fails(self):\n    with pytest.raises(ValueError) as excinfo:\n        NameAuthorExtra.objects.iter_smart()\n    assert ('non-integer primary key' in str(excinfo.value))\n", "label": 0}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (node in self.nodes_seen):\n        self.nodes_seen.discard(node)\n        self.process_node(fgraph, node)\n    if (not isinstance(node, string_types)):\n        assert node.inputs\n    if isinstance(new_r, graph.Constant):\n        self.process_constant(fgraph, new_r)\n", "label": 0}
{"function": "\n\ndef run_migrations_online():\n    \"Run migrations in 'online' mode.\\n\\n    In this scenario we need to create an Engine\\n    and associate a connection with the context.\\n\\n    \"\n    set_mysql_engine()\n    engine = session.create_engine(neutron_config.database.connection)\n    connection = engine.connect()\n    context.configure(connection=connection, target_metadata=target_metadata, include_object=include_object, version_table=alembic_migrations.VERSION_TABLE)\n    try:\n        with context.begin_transaction():\n            context.run_migrations()\n    finally:\n        connection.close()\n        engine.dispose()\n", "label": 0}
{"function": "\n\ndef _initialize_trunk_interfaces_to_none(self, switch_ip):\n    'Initialize all nexus interfaces to trunk allowed none.'\n    try:\n        switch_ifs = self._mdriver._get_switch_interfaces(switch_ip)\n        if (not switch_ifs):\n            LOG.debug('Skipping switch %s which has no configured interfaces', switch_ip)\n            return\n        self._driver.initialize_all_switch_interfaces(switch_ifs)\n    except Exception:\n        with excutils.save_and_reraise_exception():\n            LOG.warning(_LW('Unable to initialize interfaces to switch %(switch_ip)s'), {\n                'switch_ip': switch_ip,\n            })\n            self._mdriver.register_switch_as_inactive(switch_ip, 'replay init_interface')\n    for (switch_ip, intf_type, port, is_native, ch_grp) in switch_ifs:\n        try:\n            reserved = nxos_db.get_reserved_port_binding(switch_ip, self._mdriver.format_interface_name(intf_type, port))\n        except excep.NexusPortBindingNotFound:\n            continue\n        if (reserved[0].channel_group != ch_grp):\n            self._mdriver._change_baremetal_interfaces(switch_ip, intf_type, port, reserved[0].channel_group, ch_grp)\n    if self._mdriver.is_replay_enabled():\n        return\n    try:\n        mgr = self._driver.nxos_connect(switch_ip)\n        self._driver._close_session(mgr, switch_ip)\n    except Exception:\n        LOG.warning(_LW('Failed to release connection after initialize interfaces for switch %(switch_ip)s'), {\n            'switch_ip': switch_ip,\n        })\n", "label": 0}
{"function": "\n\ndef test_current_time(self):\n    self.skipTest('time.xmlrpc.com is unreliable')\n    server = xmlrpclib.ServerProxy('http://time.xmlrpc.com/RPC2')\n    try:\n        t0 = server.currentTime.getCurrentTime()\n    except socket.error as e:\n        self.skipTest(('network error: %s' % e))\n        return\n    t1 = xmlrpclib.DateTime()\n    dt0 = xmlrpclib._datetime_type(t0.value)\n    dt1 = xmlrpclib._datetime_type(t1.value)\n    if (dt0 > dt1):\n        delta = (dt0 - dt1)\n    else:\n        delta = (dt1 - dt0)\n    self.assertTrue((delta.days <= 1))\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(Client, self).__init__(*args, **kwargs)\n    self.aws_bucket = msettings['AWS_BUCKET']\n    self.aws_prefix = msettings.get('AWS_PREFIX', '').strip('/')\n    self.aws_bucket_cname = msettings.get('AWS_BUCKET_CNAME', False)\n    assert self.aws_bucket\n", "label": 0}
{"function": "\n\ndef handle(self, request, data):\n    volume_type_id = self.initial['id']\n    try:\n        cinder.volume_type_update(request, volume_type_id, data['name'], data['description'])\n        message = _('Successfully updated volume type.')\n        messages.success(request, message)\n        return True\n    except Exception as ex:\n        redirect = reverse('horizon:admin:volumes:index')\n        if (ex.code == 409):\n            error_message = _('New name conflicts with another volume type.')\n        else:\n            error_message = _('Unable to update volume type.')\n        exceptions.handle(request, error_message, redirect=redirect)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, position):\n    (i, j) = self._get_indexes(position)\n    if isinstance(i, slice):\n        if (j is None):\n            return Table(self.data[position])\n        else:\n            return Table((cells[j] for cells in self.data[i]))\n    else:\n        try:\n            row = self.data[i]\n        except IndexError:\n            msg = 'no row at index %r of %d-row table'\n            raise IndexError((msg % (position, len(self))))\n        if (j is None):\n            return row\n        else:\n            return row[j]\n", "label": 0}
{"function": "\n\ndef test_exists_many_with_none_keys(self):\n    try:\n        TestExistsMany.client.exists_many(None, {\n            \n        })\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'Keys should be specified as a list or tuple.')\n", "label": 0}
{"function": "\n\ndef _handle_message(self, opcode, data):\n    if self.client_terminated:\n        return\n    if (opcode == 1):\n        try:\n            decoded = data.decode('utf-8')\n        except UnicodeDecodeError:\n            self._abort()\n            return\n        self._run_callback(self.handler.on_message, decoded)\n    elif (opcode == 2):\n        self._run_callback(self.handler.on_message, data)\n    elif (opcode == 8):\n        self.client_terminated = True\n        if (len(data) >= 2):\n            self.handler.close_code = struct.unpack('>H', data[:2])[0]\n        if (len(data) > 2):\n            self.handler.close_reason = to_unicode(data[2:])\n        self.close()\n    elif (opcode == 9):\n        self._write_frame(True, 10, data)\n    elif (opcode == 10):\n        self._run_callback(self.handler.on_pong, data)\n    else:\n        self._abort()\n", "label": 0}
{"function": "\n\ndef mergeStatementsSequence(self, statement_sequence):\n    assert (statement_sequence.parent is self)\n    old_statements = list(self.getStatements())\n    assert (statement_sequence in old_statements), (statement_sequence, self)\n    merge_index = old_statements.index(statement_sequence)\n    new_statements = ((tuple(old_statements[:merge_index]) + statement_sequence.getStatements()) + tuple(old_statements[(merge_index + 1):]))\n    self.setChild('statements', new_statements)\n", "label": 0}
{"function": "\n\ndef test_get(self):\n    (id1, id2) = (uuid(), uuid())\n\n    def gather():\n        (yield id1)\n        (yield id2)\n    ares = self.get_async_result()\n    ares.gather = Mock(return_value=['1'])\n    ares.get()\n    ares.gather.assert_called_once_with(limit=1)\n    ares.gather.reset_mock()\n    kwargs = {\n        'timeout': 100,\n        'ignore_timeout': False,\n        'foo': 'bar',\n        'propaget': True,\n    }\n    ares.get(**kwargs)\n    ares.gather.assert_called_once_with(**dict(kwargs, limit=1))\n    ares.gather.reset_mock()\n    kwargs = {\n        'timeout': 100,\n        'ignore_timeout': False,\n        'limit': 10,\n    }\n    ares.get(**kwargs)\n    ares.gather.assert_called_once_with(**kwargs)\n    ares.gather.reset_mock()\n    ares.gather = Mock(return_value=gather())\n    res = ares.get()\n    self.assertEqual(res, id1)\n    ares.gather = Mock(return_value=None)\n    with self.assertRaises(ares.NoReplyError):\n        ares.get()\n    ares.gather.reset_mock()\n    ares.gather = Mock(return_value={\n        \n    })\n    with self.assertRaises(ares.NoReplyError):\n        ares.get()\n", "label": 0}
{"function": "\n\ndef _handle_message(self, message, buffer, record_fn):\n    'Build a response calling record_fn on all the TlsRecords in message\\n\\n        message: bytes to parse as TlsRecords\\n        record_fn: one of on_tls_request, on_tls_response to handle the record\\n        Returns tuple containing the bytes to send for all the records handled and any remaining unparsed data\\n        '\n    out = ''\n    message = (buffer.buffer + message)\n    buffer.buffer = ''\n    remaining = message\n    while remaining:\n        record = None\n        try:\n            (record, remaining) = tls.parse_tls(remaining, throw_on_incomplete=True)\n        except tls.types.TlsNotEnoughDataError:\n            if buffer.should_buffer:\n                buffer.buffer = remaining\n                if (len(buffer.buffer) >= buffer.MAX_BUFFER):\n                    buffer.buffer = ''\n            return out\n        if (not record):\n            return out\n        record_bytes = record_fn(record)\n        if record_bytes:\n            out += record_bytes\n        if (record.content_type == record.CONTENT_TYPE.CHANGE_CIPHER_SPEC):\n            buffer.should_buffer = False\n    return out\n", "label": 0}
{"function": "\n\n@classmethod\n@quickcache(['domain', 'app_id'])\ndef _app_data(cls, domain, app_id):\n    defaults = MaltAppData(AMPLIFIES_NOT_SET, AMPLIFIES_NOT_SET, 15, 3, False)\n    if (not app_id):\n        return defaults\n    try:\n        app = get_app(domain, app_id)\n    except Http404:\n        logger.debug(('App not found %s' % app_id))\n        return defaults\n    return MaltAppData(getattr(app, 'amplifies_workers', AMPLIFIES_NOT_SET), getattr(app, 'amplifies_project', AMPLIFIES_NOT_SET), getattr(app, 'minimum_use_threshold', 15), getattr(app, 'experienced_threshold', 3), app.is_deleted())\n", "label": 0}
{"function": "\n\ndef is_not_found(self, request, response, **kwargs):\n    try:\n        return (self.get_object(request, **kwargs) is None)\n    except ValueError:\n        return True\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _aggregator_counter_kind(combine_fn):\n    \"Returns the counter aggregation kind for the combine_fn passed in.\\n\\n    Args:\\n      combine_fn: The combining function used in an Aggregator.\\n\\n    Returns:\\n      The aggregation_kind (to use in a Counter) that matches combine_fn.\\n\\n    Raises:\\n      ValueError if the combine_fn doesn't map to any supported\\n      aggregation kind.\\n    \"\n    combine_kind_map = {\n        sum: Counter.SUM,\n        max: Counter.MAX,\n        min: Counter.MIN,\n        combiners.Mean: Counter.MEAN,\n    }\n    try:\n        return combine_kind_map[combine_fn]\n    except KeyError:\n        try:\n            return combine_kind_map[combine_fn.__class__]\n        except KeyError:\n            raise ValueError(('combine_fn %r (class %r) does not map to a supported aggregation kind' % (combine_fn, combine_fn.__class__)))\n", "label": 0}
{"function": "\n\ndef run(self):\n    '\\n        Kill any open Redshift sessions for the given database.\\n        '\n    connection = self.output().connect()\n    query = \"select pg_terminate_backend(process) from STV_SESSIONS where db_name=%s and user_name != 'rdsdb' and process != pg_backend_pid()\"\n    cursor = connection.cursor()\n    logger.info('Killing all open Redshift sessions for database: %s', self.database)\n    try:\n        cursor.execute(query, (self.database,))\n        cursor.close()\n        connection.commit()\n    except psycopg2.DatabaseError as e:\n        if (e.message and ('EOF' in e.message)):\n            connection.close()\n            logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n            time.sleep(self.connection_reset_wait_seconds)\n            logger.info('Reconnecting to Redshift')\n            connection = self.output().connect()\n        else:\n            raise\n    try:\n        self.output().touch(connection)\n        connection.commit()\n    finally:\n        connection.close()\n    logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n", "label": 0}
{"function": "\n\n@AccountRequired\n@PostOnly\ndef unfollow(request, user_id):\n    followed = get_object_or_404(User, pk=user_id)\n    try:\n        follow_instance = UserToUserFollow.objects.get(follower=request.user, followed=followed)\n        follow_instance.is_following = False\n        follow_instance.stopped_following = datetime.datetime.now()\n        follow_instance.save()\n    except UserToUserFollow.DoesNotExist:\n        pass\n    cache.bust(followed)\n    if request.is_ajax():\n        button = render_inclusiontag(request, 'follow_button followed', 'users_tags', {\n            'followed': followed,\n        })\n        return json_response({\n            'button': button,\n        })\n    else:\n        return redirect(followed)\n", "label": 0}
{"function": "\n\ndef configure_formatter(self, config):\n    'Configure a formatter from a dictionary.'\n    if ('()' in config):\n        factory = config['()']\n        try:\n            result = self.configure_custom(config)\n        except TypeError as te:\n            if (\"'format'\" not in str(te)):\n                raise\n            config['fmt'] = config.pop('format')\n            config['()'] = factory\n            result = self.configure_custom(config)\n    else:\n        fmt = config.get('format', None)\n        dfmt = config.get('datefmt', None)\n        result = logging.Formatter(fmt, dfmt)\n    return result\n", "label": 0}
{"function": "\n\ndef test_chrome_headers_loading(self):\n    self.app.config['DRIVER_NAME'] = 'Chrome'\n    dcap = dict(DesiredCapabilities.PHANTOMJS)\n    with self.assertRaises(Exception):\n        Headers(self.config).set_headers(dcap)\n", "label": 0}
{"function": "\n\ndef _on_auth(self, user):\n    if (not user):\n        raise tornado.web.HTTPError(403, 'Google auth failed')\n    access_token = user['access_token']\n    try:\n        response = httpclient.HTTPClient().fetch('https://www.googleapis.com/plus/v1/people/me', headers={\n            'Authorization': ('Bearer %s' % access_token),\n        })\n    except Exception as e:\n        raise tornado.web.HTTPError(403, ('Google auth failed: %s' % e))\n    email = json.loads(response.body.decode('utf-8'))['emails'][0]['value']\n    if (not re.match(self.application.options.auth, email)):\n        message = \"Access denied to '{email}'. Please use another account or ask your admin to add your email to flower --auth.\".format(email=email)\n        raise tornado.web.HTTPError(403, message)\n    self.set_secure_cookie('user', str(email))\n    next = self.get_argument('next', '/')\n    self.redirect(next)\n", "label": 0}
{"function": "\n\n@httprettified\ndef test_blogLikes(self):\n    HTTPretty.register_uri(HTTPretty.GET, 'https://api.tumblr.com/v2/blog/codingjester.tumblr.com/likes', body='{\"meta\": {\"status\": 200, \"msg\": \"OK\"}, \"response\": {\"liked_posts\": [] } }')\n    response = self.client.blog_likes('codingjester.tumblr.com')\n    assert (response['liked_posts'] == [])\n", "label": 0}
{"function": "\n\ndef __contains__(self, ref_or_name):\n    with self._lock:\n        if isinstance(ref_or_name, six.string_types):\n            return (ref_or_name in self._registered_names)\n        else:\n            return (ref_or_name in self._by_ref)\n", "label": 0}
{"function": "\n\ndef handle_entityref(self, ref):\n    if (not self.elementstack):\n        return\n    if (ref in ('lt', 'gt', 'quot', 'amp', 'apos')):\n        text = ('&%s;' % ref)\n    elif (ref in self.entities):\n        text = self.entities[ref]\n        if (text.startswith('&#') and text.endswith(';')):\n            return self.handle_entityref(text)\n    else:\n        try:\n            name2codepoint[ref]\n        except KeyError:\n            text = ('&%s;' % ref)\n        else:\n            text = chr(name2codepoint[ref]).encode('utf-8')\n    self.elementstack[(- 1)][2].append(text)\n", "label": 0}
{"function": "\n\ndef _open_url(url, file=None):\n    respond = urllib.request.urlopen(urllib.request.Request(url, headers=USER_AGENT))\n    if file:\n        with open(file, 'wb') as destination:\n            destination.write(respond.read())\n    return respond\n", "label": 0}
{"function": "\n\ndef test_domain_name_with_whitespaces(self):\n    site = Site(name='test name', domain='test test')\n    with self.assertRaises(ValidationError):\n        site.full_clean()\n    site.domain = 'test\\ttest'\n    with self.assertRaises(ValidationError):\n        site.full_clean()\n    site.domain = 'test\\ntest'\n    with self.assertRaises(ValidationError):\n        site.full_clean()\n", "label": 0}
{"function": "\n\ndef clean_email(self):\n    value = self.cleaned_data['email']\n    if (UNIQUE_EMAIL or EMAIL_AUTHENTICATION):\n        try:\n            User.objects.get(email__iexact=value)\n        except User.DoesNotExist:\n            return value\n        raise forms.ValidationError(_('A user is registered with this e-mail address.'))\n    return value\n", "label": 0}
{"function": "\n\ndef get_contents_if_file(contents_or_file_name):\n    'Get the contents of a file.\\n\\n    If the value passed in is a file name or file URI, return the\\n    contents. If not, or there is an error reading the file contents,\\n    return the value passed in as the contents.\\n\\n    For example, a workflow definition will be returned if either the\\n    workflow definition file name, or file URI are passed in, or the\\n    actual workflow definition itself is passed in.\\n    '\n    try:\n        if parse.urlparse(contents_or_file_name).scheme:\n            definition_url = contents_or_file_name\n        else:\n            path = os.path.abspath(contents_or_file_name)\n            definition_url = parse.urljoin('file:', request.pathname2url(path))\n        return request.urlopen(definition_url).read().decode('utf8')\n    except Exception:\n        return contents_or_file_name\n", "label": 0}
{"function": "\n\ndef test_invalid_sequence_type(self):\n    config = _root({\n        'foo': ['bar', 2126],\n    })\n    with self.assertRaises(confuse.ConfigTypeError):\n        config['foo'].get(confuse.StrSeq())\n", "label": 0}
{"function": "\n\ndef get_module_path(module_name):\n    try:\n        module = import_module(module_name)\n    except ImportError as e:\n        raise ImproperlyConfigured(('Error importing HIDE_IN_STACKTRACES: %s' % (e,)))\n    else:\n        source_path = inspect.getsourcefile(module)\n        if source_path.endswith('__init__.py'):\n            source_path = os.path.dirname(source_path)\n        return os.path.realpath(source_path)\n", "label": 0}
{"function": "\n\n@raises(ValueError)\ndef test_execute_steps_should_fail_when_called_without_feature(self):\n    doc = '\\nGiven a passes\\nThen a step passes\\n'.lstrip()\n    with patch('behave.step_registry.registry', self.step_registry):\n        self.context.feature = None\n        self.context.execute_steps(doc)\n", "label": 0}
{"function": "\n\ndef _read_bytes_from_socket(self, msglen):\n    ' Read bytes from the socket. '\n    chunks = []\n    bytes_recd = 0\n    while (bytes_recd < msglen):\n        if self.stop.is_set():\n            raise InterruptLoop('Stopped while reading from socket')\n        try:\n            chunk = self.socket.recv(min((msglen - bytes_recd), 2048))\n            if (chunk == b''):\n                raise socket.error('socket connection broken')\n            chunks.append(chunk)\n            bytes_recd += len(chunk)\n        except socket.timeout:\n            continue\n        except ssl.SSLError as exc:\n            if _is_ssl_timeout(exc):\n                continue\n            raise\n    return b''.join(chunks)\n", "label": 0}
{"function": "\n\ndef test_many_to_one(self, session, objects, calls):\n    users = session.query(models.User).all()\n    users[0].addresses\n    assert (len(calls) == 1)\n    call = calls[0]\n    assert (call.objects == (models.User, 'User:1', 'addresses'))\n    assert ('users[0].addresses' in ''.join(call.frame[4]))\n", "label": 0}
{"function": "\n\ndef __update_yaml(self):\n    conf_file = os.path.join(self.get_path(), 'resources', 'dse', 'conf', 'dse.yaml')\n    with open(conf_file, 'r') as f:\n        data = yaml.load(f)\n    data['system_key_directory'] = os.path.join(self.get_path(), 'keys')\n    full_options = common.merge_configuration(self.cluster._dse_config_options, self._dse_config_options, delete_empty=False)\n    data = common.merge_configuration(data, full_options)\n    with open(conf_file, 'w') as f:\n        yaml.safe_dump(data, f, default_flow_style=False)\n", "label": 0}
{"function": "\n\n@register.tag\ndef shardtype(parser, token):\n    try:\n        (tag_name, shard_type) = token.split_contents()\n    except ValueError:\n        raise template.TemplateSyntaxError(('%r tag requires a single argument' % token.contents.split()[0]))\n    if (not ((shard_type[0] == shard_type[(- 1)]) and (shard_type[0] in ('\"', \"'\")))):\n        raise template.TemplateSyntaxError((\"%r tag's argument should be in quotes\" % tag_name))\n    shard_type = shard_type[1:(- 1)]\n    nodelist = parser.parse(('end{0}'.format(tag_name),))\n    parser.delete_first_token()\n    return EmailShardTypeNode(shard_type, nodelist)\n", "label": 0}
{"function": "\n\ndef _parse_changelog(self, pkg_name):\n    with open('ChangeLog.md') as f:\n        lineiter = iter(f)\n        for line in lineiter:\n            match = re.search(('^%s\\\\s+(.*)' % pkg_name), line.strip())\n            if (match is None):\n                continue\n            length = len(match.group(1))\n            version = match.group(1).strip()\n            if (lineiter.next().count('-') != len(match.group(0))):\n                continue\n            while 1:\n                change_info = lineiter.next().strip()\n                if change_info:\n                    break\n            match = re.search('released on (\\\\w+\\\\s+\\\\d+\\\\w+\\\\s+\\\\d+)', change_info)\n            if (match is None):\n                continue\n            datestr = match.group(1)\n            return (version, self._parse_date(datestr))\n", "label": 0}
{"function": "\n\ndef _run(self):\n    result = StatusCheckResult(check=self)\n    auth = None\n    if (self.username or self.password):\n        auth = (self.username, self.password)\n    try:\n        resp = requests.get(self.endpoint, timeout=self.timeout, verify=self.verify_ssl_certificate, auth=auth, headers={\n            'User-Agent': settings.HTTP_USER_AGENT,\n        })\n    except requests.RequestException as e:\n        result.error = ('Request error occurred: %s' % (e.message,))\n        result.succeeded = False\n    else:\n        if (self.status_code and (resp.status_code != int(self.status_code))):\n            result.error = ('Wrong code: got %s (expected %s)' % (resp.status_code, int(self.status_code)))\n            result.succeeded = False\n            result.raw_data = resp.content\n        elif self.text_match:\n            if (not re.search(self.text_match, resp.content)):\n                result.error = ('Failed to find match regex /%s/ in response body' % self.text_match)\n                result.raw_data = resp.content\n                result.succeeded = False\n            else:\n                result.succeeded = True\n        else:\n            result.succeeded = True\n    return result\n", "label": 0}
{"function": "\n\n@authorization_required(is_admin=True)\n@threaded\ndef put(self, uid):\n    try:\n        user = Users.get(id=uid)\n    except DoesNotExist:\n        raise HTTPError(404)\n    try:\n        user.login = self.json.get('login', user.login)\n        user.email = self.json.get('email', user.email)\n        user.is_admin = bool(self.json.get('is_admin', user.is_admin))\n        user.password = self.json.get('password', user.password)\n        if (not all((isinstance(user.login, text_type), isinstance(user.email, text_type), (LOGIN_EXP.match(str(user.login)) is not None), (user.password and (len(user.password) > 3)), (EMAIL_EXP.match(str(user.email)) is not None)))):\n            raise HTTPError(400)\n    except:\n        raise HTTPError(400)\n    user.save()\n    self.response({\n        'id': user.id,\n        'login': user.login,\n        'email': user.email,\n        'is_admin': user.is_admin,\n    })\n", "label": 0}
{"function": "\n\n@pytest.fixture(scope='module')\ndef smtp_servers(request):\n    try:\n        from .local_smtp_severs import SERVERS\n    except ImportError:\n        from .smtp_servers import SERVERS\n    return dict([(k, SMTPTestParams(**v)) for (k, v) in SERVERS.items()])\n", "label": 0}
{"function": "\n\ndef _version_str_to_list(version):\n    'convert a version string to a list of ints\\n\\n    non-int segments are excluded\\n    '\n    v = []\n    for part in version.split('.'):\n        try:\n            v.append(int(part))\n        except ValueError:\n            pass\n    return v\n", "label": 0}
{"function": "\n\ndef test_read_job2():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test.test', 'test1'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\n@classmethod\ndef award_points(cls, user, addon, status, **kwargs):\n    'Awards points to user based on an event and the queue.\\n\\n        `event` is one of the `REVIEWED_` keys in constants.\\n        `status` is one of the `STATUS_` keys in constants.\\n\\n        '\n    event = cls.get_event(addon, status, **kwargs)\n    score = amo.REVIEWED_SCORES.get(event)\n    try:\n        vq = ViewQueue.objects.get(addon_slug=addon.slug)\n        if (vq.waiting_time_days > amo.REVIEWED_OVERDUE_LIMIT):\n            days_over = (vq.waiting_time_days - amo.REVIEWED_OVERDUE_LIMIT)\n            bonus = (days_over * amo.REVIEWED_OVERDUE_BONUS)\n            score = (score + bonus)\n    except ViewQueue.DoesNotExist:\n        pass\n    if score:\n        cls.objects.create(user=user, addon=addon, score=score, note_key=event)\n        cls.get_key(invalidate=True)\n        user_log.info(('Awarding %s points to user %s for \"%s\" for addon %s' % (score, user, amo.REVIEWED_CHOICES[event], addon.id)).encode('utf-8'))\n    return score\n", "label": 0}
{"function": "\n\ndef test_DELETE(self):\n    res = self.app.delete('/crud.json?ref.id=1', expect_errors=False)\n    print('Received:', res.body)\n    result = json.loads(res.text)\n    print(result)\n    assert (result['data']['id'] == 1)\n    assert (result['data']['name'] == u('test'))\n    assert (result['message'] == 'delete')\n", "label": 0}
{"function": "\n\n@login_required\n@permission_required('workshops.add_todoitem', raise_exception=True)\ndef todos_add(request, event_ident):\n    'Add a standard TodoItems for a specific event.'\n    try:\n        event = Event.get_by_ident(event_ident)\n    except Event.DoesNotExist:\n        raise Http404('Event matching query does not exist.')\n    dt = datetime.datetime\n    timedelta = datetime.timedelta\n    initial = []\n    base = dt.now()\n    if ((not event.start) or (not event.end)):\n        initial = [{\n            'title': 'Set date with host',\n            'due': (dt.now() + timedelta(days=30)),\n            'event': event,\n        }]\n    formset = TodoFormSet(queryset=TodoItem.objects.none(), initial=(initial + [{\n        'title': 'Set up a workshop website',\n        'due': (base + timedelta(days=7)),\n        'event': event,\n    }, {\n        'title': 'Find instructor #1',\n        'due': (base + timedelta(days=14)),\n        'event': event,\n    }, {\n        'title': 'Find instructor #2',\n        'due': (base + timedelta(days=14)),\n        'event': event,\n    }, {\n        'title': 'Follow up that instructors have booked travel',\n        'due': (base + timedelta(days=21)),\n        'event': event,\n    }, {\n        'title': 'Set up pre-workshop survey',\n        'due': ((event.start - timedelta(days=7)) if event.start else ''),\n        'event': event,\n    }, {\n        'title': 'Make sure instructors are set with materials',\n        'due': ((event.start - timedelta(days=1)) if event.start else ''),\n        'event': event,\n    }, {\n        'title': 'Submit invoice',\n        'due': ((event.end + timedelta(days=2)) if event.end else ''),\n        'event': event,\n    }, {\n        'title': 'Make sure instructors are reimbursed',\n        'due': ((event.end + timedelta(days=7)) if event.end else ''),\n        'event': event,\n    }, {\n        'title': 'Get attendee list',\n        'due': ((event.end + timedelta(days=7)) if event.end else ''),\n        'event': event,\n    }]))\n    if (request.method == 'POST'):\n        formset = TodoFormSet(request.POST)\n        if formset.is_valid():\n            formset.save()\n            messages.success(request, 'Successfully added a bunch of TODOs.', extra_tags='todos')\n            return redirect(reverse(event_details, args=(event.get_ident(),)))\n        else:\n            messages.error(request, 'Fix errors below.')\n    context = {\n        'title': 'Add standard TODOs to the event',\n        'formset': formset,\n        'helper': bootstrap_helper_inline_formsets,\n        'event': event,\n    }\n    return render(request, 'workshops/todos_add.html', context)\n", "label": 1}
{"function": "\n\ndef connect_to_nsqd(self, conn):\n    if (not self.is_running):\n        return\n    if (conn in self.conns):\n        self.logger.debug(('[%s] already connected' % conn))\n        return\n    if (conn in self.pending):\n        self.logger.debug(('[%s] already pending' % conn))\n        return\n    self.logger.debug(('[%s] connecting...' % conn))\n    conn.on_response.connect(self.handle_response)\n    conn.on_error.connect(self.handle_error)\n    conn.on_finish.connect(self.handle_finish)\n    conn.on_requeue.connect(self.handle_requeue)\n    conn.on_auth.connect(self.handle_auth)\n    if self.max_concurrency:\n        conn.on_message.connect(self.queue_message)\n    else:\n        conn.on_message.connect(self.handle_message)\n    self.pending.add(conn)\n    try:\n        conn.connect()\n        conn.identify()\n        if (conn.max_ready_count < self.max_in_flight):\n            msg = ' '.join(['[%s] max RDY count %d < reader max in flight %d,', 'truncation possible'])\n            self.logger.warning((msg % (conn, conn.max_ready_count, self.max_in_flight)))\n        conn.subscribe(self.topic, self.channel)\n        self.send_ready(conn, 1)\n    except NSQException as error:\n        self.logger.warn(('[%s] connection failed (%r)' % (conn, error)))\n        self.handle_connection_failure(conn)\n        return\n    finally:\n        self.pending.remove(conn)\n    if (not self.is_running):\n        conn.close_stream()\n        return\n    self.logger.info(('[%s] connection successful' % conn))\n    self.handle_connection_success(conn)\n", "label": 0}
{"function": "\n\ndef find_by(self, finder, selector, original_find=None, original_query=None):\n    elements = None\n    end_time = (time.time() + self.wait_time)\n    func_name = getattr(getattr(finder, _meth_func), _func_name)\n    find_by = (original_find or func_name[(func_name.rfind('_by_') + 4):])\n    query = (original_query or selector)\n    while (time.time() < end_time):\n        try:\n            elements = finder(selector)\n            if (not isinstance(elements, list)):\n                elements = [elements]\n        except NoSuchElementException:\n            pass\n        if elements:\n            return ElementList([self.element_class(element, self) for element in elements], find_by=find_by, query=query)\n    return ElementList([], find_by=find_by, query=query)\n", "label": 0}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_version():\n        self.set_version(x.version())\n", "label": 0}
{"function": "\n\ndef __call__(self, driver):\n    try:\n        if isinstance(self.frame_locator, tuple):\n            driver.switch_to.frame(_find_element(driver, self.frame_locator))\n        else:\n            driver.switch_to.frame(self.frame_locator)\n        return True\n    except NoSuchFrameException:\n        return False\n", "label": 0}
{"function": "\n\ndef get_version():\n    INIT = os.path.abspath(os.path.join(HERE, '../psutil/__init__.py'))\n    with open(INIT, 'r') as f:\n        for line in f:\n            if line.startswith('__version__'):\n                ret = eval(line.strip().split(' = ')[1])\n                assert (ret.count('.') == 2), ret\n                for num in ret.split('.'):\n                    assert num.isdigit(), ret\n                return ret\n        else:\n            raise ValueError(\"couldn't find version string\")\n", "label": 0}
{"function": "\n\ndef test_search_products(self):\n    p = ProductFactory(title='Product One', slug='product')\n    doc = DocumentFactory(title='cookies', locale='en-US', category=10, products=[p])\n    RevisionFactory(document=doc, is_approved=True)\n    self.refresh()\n    response = self.client.get(reverse('search.advanced'), {\n        'a': '1',\n        'product': 'product',\n        'q': 'cookies',\n        'w': '1',\n    })\n    assert (\"We couldn't find any results for\" not in response.content)\n    eq_(200, response.status_code)\n    assert ('Product One' in response.content)\n", "label": 0}
{"function": "\n\ndef testAdminsCanStartAdminOnlyFlow(self):\n    admin_token = access_control.ACLToken(username='adminuser', reason='testing')\n    with self.ACLChecksDisabled():\n        client_id = self.SetupClients(1)[0]\n        self.CreateAdminUser('adminuser')\n        self.RequestAndGrantClientApproval(client_id, token=admin_token)\n    flow.GRRFlow.StartFlow(flow_name=AdminOnlyFlow.__name__, client_id=client_id, token=admin_token, sync=False)\n", "label": 0}
{"function": "\n\ndef valid(self, order=None):\n    '\\n        Can do complex validation about whether or not this option is valid.\\n        For example, may check to see if the recipient is in an allowed country\\n        or location.\\n        '\n    if order:\n        try:\n            for item in order.orderitem_set.all():\n                p = item.product\n                price = self.carrier.price(p)\n        except ProductShippingPriceException:\n            return False\n    elif self.cart:\n        try:\n            price = self.cost()\n        except ProductShippingPriceException:\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef reset(self):\n    'Stop the zookeeper instance, cleaning out its on disk-data.'\n    self.stop()\n    shutil.rmtree(os.path.join(self.working_path, 'data'), True)\n    os.mkdir(os.path.join(self.working_path, 'data'))\n    with open(os.path.join(self.working_path, 'data', 'myid'), 'w') as fh:\n        fh.write(str(self.server_info.server_id))\n", "label": 0}
{"function": "\n\ndef test_compute_centroid_quantile(self, empty_tdigest, example_centroids):\n    empty_tdigest.C = example_centroids\n    empty_tdigest.n = 4\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 1.1)]) == (((1 / 2.0) + 0) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 0.5)]) == (((1 / 2.0) + 1) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[0.1]) == (((1 / 2.0) + 2) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[1.5]) == (((1 / 2.0) + 3) / 4))\n", "label": 0}
{"function": "\n\ndef cycle(self):\n    '\\n        Perform full thermostat cycle and return state.\\n        '\n    units = cherrypy.config['units']\n    retry_count = cherrypy.config['retry_count']\n    retry_delay = cherrypy.config['retry_delay']\n    envcontroller = cherrypy.config['envcontroller']\n    thermometer = cherrypy.config['thermometer']\n    thermostat = cherrypy.config['thermostat']\n    (current_heat, current_cool) = envcontroller.get_power_levels()\n    for i in range(retry_count):\n        try:\n            current_temp = thermometer.get_temperature(units=units)\n            break\n        except braubuddy.thermometer.ReadError as err:\n            cherrypy.log.error(err.message)\n            time.sleep(retry_delay)\n    else:\n        cherrypy.request.app.log.error('Unable to collect temperature after {0} tries'.format(retry_count))\n        return False\n    (required_heat, required_cool) = thermostat.get_required_state(current_temp, current_heat, current_cool, units=units)\n    envcontroller.set_heater_level(required_heat)\n    envcontroller.set_cooler_level(required_cool)\n    target = thermostat.target\n    for (name, output) in cherrypy.request.app.config['outputs'].iteritems():\n        try:\n            output.publish_status(target, current_temp, current_heat, current_cool)\n        except braubuddy.output.OutputError as err:\n            cherrypy.log.error(err.message)\n    return True\n", "label": 0}
{"function": "\n\ndef test_record_no_OF_location(self, capfd):\n    fobj = open(os.path.join(os.getcwd(), 'mockProject', 'config.make'), 'w')\n    fobj.close()\n    (_, err) = run_ofSM('record -p mockProject', capfd=capfd, desired_exit_status=1)\n    assert ('Did not find OF location in config.make in' in err)\n", "label": 0}
{"function": "\n\ndef _generate_image_id(self):\n    while True:\n        image_id = hashlib.sha256(str(random.getrandbits(128)).encode('utf8')).hexdigest()\n        try:\n            int(image_id[0:10])\n        except ValueError:\n            return image_id\n", "label": 0}
{"function": "\n\ndef test_version_minor_older(self):\n    with patch('subprocess.check_output') as mock_subprocess:\n        p = P.PyTesseract({\n            \n        })\n        p.required = '3.02.02'\n        mock_subprocess.return_value = 'tesseract 3.02.01'\n        (uptodate, ver) = p._is_version_uptodate()\n        assert (not uptodate)\n", "label": 0}
{"function": "\n\n@classmethod\ndef _is_socket(cls, stream):\n    'Check if the given stream is a socket.'\n    try:\n        fd = stream.fileno()\n    except ValueError:\n        return False\n    sock = socket.fromfd(fd, socket.AF_INET, socket.SOCK_RAW)\n    try:\n        sock.getsockopt(socket.SOL_SOCKET, socket.SO_TYPE)\n    except socket.error as ex:\n        if (ex.args[0] != errno.ENOTSOCK):\n            return True\n    else:\n        return True\n", "label": 0}
{"function": "\n\n@mock.patch('certbot.main.sys')\ndef test_handle_exception(self, mock_sys):\n    from acme import messages\n    config = mock.MagicMock()\n    mock_open = mock.mock_open()\n    with mock.patch('certbot.main.open', mock_open, create=True):\n        exception = Exception('detail')\n        config.verbose_count = 1\n        main._handle_exception(Exception, exc_value=exception, trace=None, config=None)\n        mock_open().write.assert_called_once_with(''.join(traceback.format_exception_only(Exception, exception)))\n        error_msg = mock_sys.exit.call_args_list[0][0][0]\n        self.assertTrue(('unexpected error' in error_msg))\n    with mock.patch('certbot.main.open', mock_open, create=True):\n        mock_open.side_effect = [KeyboardInterrupt]\n        error = errors.Error('detail')\n        main._handle_exception(errors.Error, exc_value=error, trace=None, config=None)\n        mock_sys.exit.assert_any_call(''.join(traceback.format_exception_only(errors.Error, error)))\n    exception = messages.Error(detail='alpha', typ='urn:acme:error:triffid', title='beta')\n    config = mock.MagicMock(debug=False, verbose_count=(- 3))\n    main._handle_exception(messages.Error, exc_value=exception, trace=None, config=config)\n    error_msg = mock_sys.exit.call_args_list[(- 1)][0][0]\n    self.assertTrue(('unexpected error' in error_msg))\n    self.assertTrue(('acme:error' not in error_msg))\n    self.assertTrue(('alpha' in error_msg))\n    self.assertTrue(('beta' in error_msg))\n    config = mock.MagicMock(debug=False, verbose_count=1)\n    main._handle_exception(messages.Error, exc_value=exception, trace=None, config=config)\n    error_msg = mock_sys.exit.call_args_list[(- 1)][0][0]\n    self.assertTrue(('unexpected error' in error_msg))\n    self.assertTrue(('acme:error' in error_msg))\n    self.assertTrue(('alpha' in error_msg))\n    interrupt = KeyboardInterrupt('detail')\n    main._handle_exception(KeyboardInterrupt, exc_value=interrupt, trace=None, config=None)\n    mock_sys.exit.assert_called_with(''.join(traceback.format_exception_only(KeyboardInterrupt, interrupt)))\n", "label": 0}
{"function": "\n\ndef run_cli(*args, **kw):\n    runner = CliRunner()\n    with patch('planet.scripts.client', (lambda : client)):\n        return runner.invoke(scripts.cli, *args, **kw)\n", "label": 0}
{"function": "\n\ndef test_profile_exception(self):\n\n    def f():\n        raise ValueError\n\n    def main():\n        try:\n            f()\n        except ValueError:\n            pass\n    with tracebin.record(profile=True) as recorder:\n        main()\n    [_, call, _] = recorder.calls\n    assert (call.func_name == 'main')\n    [subcall] = call.subcalls\n    assert (subcall.func_name == 'f')\n", "label": 0}
{"function": "\n\ndef test_user_data_dir_osx(self, monkeypatch):\n    monkeypatch.setattr(appdirs, 'WINDOWS', False)\n    monkeypatch.setattr(os, 'path', posixpath)\n    monkeypatch.setenv('HOME', '/home/test')\n    monkeypatch.setattr(sys, 'platform', 'darwin')\n    assert (appdirs.user_data_dir('pip') == '/home/test/Library/Application Support/pip')\n", "label": 0}
{"function": "\n\ndef test_should_override_by_env(self, settings_dict_to_override):\n\n    def mock_env_side_effect(k, d=None):\n        return ('simple from env' if (k == 'SIMPLE_STRING') else d)\n    with patch('os.environ.get', side_effect=mock_env_side_effect):\n        override_settings_by_env(settings_dict_to_override)\n    assert (settings_dict_to_override['SIMPLE_STRING'] == 'simple from env')\n    assert (settings_dict_to_override['SIMPLE_INTEGER'] == 1)\n", "label": 0}
{"function": "\n\ndef test_get_unique_constraints(self):\n    constraints = self.person_tools._get_unique_constraints()\n    assert (len(constraints) == 1)\n    assert (constraints[0] == 'person_name')\n", "label": 0}
{"function": "\n\ndef do(self, workflow_dict):\n    try:\n        mongodbkey = ''.join((random.choice(string.hexdigits) for i in range(50)))\n        workflow_dict['replicasetname'] = ('RepicaSet_' + workflow_dict['databaseinfra'].name)\n        statsd_credentials = get_credentials_for(environment=workflow_dict['environment'], credential_type=CredentialType.STATSD)\n        (statsd_host, statsd_port) = statsd_credentials.endpoint.split(':')\n        mongodb_password = get_credentials_for(environment=workflow_dict['environment'], credential_type=CredentialType.MONGODB).password\n        for (index, instance) in enumerate(workflow_dict['instances']):\n            host = instance.hostname\n            LOG.info('Getting vm credentials...')\n            host_csattr = CsHostAttr.objects.get(host=host)\n            LOG.info('Cheking host ssh...')\n            host_ready = check_ssh(server=host.address, username=host_csattr.vm_user, password=host_csattr.vm_password, wait=5, interval=10)\n            if (not host_ready):\n                LOG.warn(('Host %s is not ready...' % host))\n                return False\n            if instance.is_arbiter:\n                contextdict = {\n                    'HOST': workflow_dict['hosts'][index].hostname.split('.')[0],\n                    'DATABASENAME': workflow_dict['name'],\n                    'ENGINE': 'mongodb',\n                    'STATSD_HOST': statsd_host,\n                    'STATSD_PORT': statsd_port,\n                    'IS_HA': workflow_dict['databaseinfra'].plan.is_ha,\n                }\n                databaserule = 'ARBITER'\n            else:\n                host_nfsattr = HostAttr.objects.get(host=host)\n                contextdict = {\n                    'EXPORTPATH': host_nfsattr.nfsaas_path,\n                    'HOST': workflow_dict['hosts'][index].hostname.split('.')[0],\n                    'DATABASENAME': workflow_dict['name'],\n                    'ENGINE': 'mongodb',\n                    'DBPASSWORD': mongodb_password,\n                    'STATSD_HOST': statsd_host,\n                    'STATSD_PORT': statsd_port,\n                    'IS_HA': workflow_dict['databaseinfra'].plan.is_ha,\n                }\n                if (index == 0):\n                    databaserule = 'PRIMARY'\n                else:\n                    databaserule = 'SECONDARY'\n            if (len(workflow_dict['hosts']) > 1):\n                LOG.info(('Updating contexdict for %s' % host))\n                contextdict.update({\n                    'REPLICASETNAME': workflow_dict['replicasetname'],\n                    'HOST01': workflow_dict['hosts'][0],\n                    'HOST02': workflow_dict['hosts'][1],\n                    'HOST03': workflow_dict['hosts'][2],\n                    'MONGODBKEY': mongodbkey,\n                    'DATABASERULE': databaserule,\n                    'HOST': workflow_dict['hosts'][index].hostname.split('.')[0],\n                })\n            else:\n                contextdict.update({\n                    'DATABASERULE': databaserule,\n                })\n            planattr = PlanAttr.objects.get(plan=workflow_dict['plan'])\n            scripts = (planattr.initialization_script, planattr.configuration_script, planattr.start_database_script)\n            for script in scripts:\n                LOG.info(('Executing script on %s' % host))\n                script = build_context_script(contextdict, script)\n                return_code = exec_remote_command(server=host.address, username=host_csattr.vm_user, password=host_csattr.vm_password, command=script)\n                if (return_code != 0):\n                    return False\n        if (len(workflow_dict['hosts']) > 1):\n            scripts_to_run = planattr.start_replication_script\n            contextdict.update({\n                'DBPASSWORD': mongodb_password,\n                'DATABASERULE': 'PRIMARY',\n            })\n            scripts_to_run = build_context_script(contextdict, scripts_to_run)\n            host = workflow_dict['hosts'][0]\n            host_csattr = CsHostAttr.objects.get(host=host)\n            return_code = exec_remote_command(server=host.address, username=host_csattr.vm_user, password=host_csattr.vm_password, command=scripts_to_run)\n            if (return_code != 0):\n                return False\n        return True\n    except Exception:\n        traceback = full_stack()\n        workflow_dict['exceptions']['error_codes'].append(DBAAS_0014)\n        workflow_dict['exceptions']['traceback'].append(traceback)\n        return False\n", "label": 1}
{"function": "\n\ndef test_neg_list_trim_policy_is_string(self):\n    '\\n        Invoke list_trim() with policy is string\\n        '\n    key = ('test', 'demo', 1)\n    try:\n        self.as_connection.list_trim(key, 'contact_no', 0, 1, {\n            \n        }, '')\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'policy must be a dict')\n", "label": 0}
{"function": "\n\ndef get_usage(self, mountpoint):\n    results = []\n    with FileDescriptor(mountpoint) as fd:\n        ret = sized_array(TWO_LONGS_STRUCT.size)\n        fcntl.ioctl(fd, BTRFS_IOC_SPACE_INFO, ret)\n        (_, total_spaces) = TWO_LONGS_STRUCT.unpack(ret)\n        buffer_size = (TWO_LONGS_STRUCT.size + (total_spaces * THREE_LONGS_STRUCT.size))\n        data = sized_array(buffer_size)\n        TWO_LONGS_STRUCT.pack_into(data, 0, total_spaces, 0)\n        fcntl.ioctl(fd, BTRFS_IOC_SPACE_INFO, data)\n    (_, total_spaces) = TWO_LONGS_STRUCT.unpack_from(ret, 0)\n    for offset in xrange(TWO_LONGS_STRUCT.size, buffer_size, THREE_LONGS_STRUCT.size):\n        (flags, total_bytes, used_bytes) = THREE_LONGS_STRUCT.unpack_from(data, offset)\n        results.append((flags, total_bytes, used_bytes))\n    return results\n", "label": 0}
{"function": "\n\ndef _send_email(self, check):\n    check_name = check['check']\n    hostname = check['hostname']\n    if (check['status'] is True):\n        status = 'UP'\n    else:\n        status = 'DOWN'\n    subject = ('[stalker] %s on %s is %s' % (check_name, hostname, status))\n    message = ('From: %s\\n        To: %s\\n        Subject: %s\\n\\n        %s\\n        ' % (self.from_addr, self.recipients, subject, check))\n    try:\n        conn = smtplib.SMTP(self.smtp_host, self.smtp_port)\n        conn.ehlo()\n        conn.sendmail(self.from_addr, self.recipients, message)\n        conn.close()\n        self.logger.info(('Email sent for: %s' % check))\n        return True\n    except Exception:\n        self.logger.exception('Email notification error.')\n        return False\n", "label": 0}
{"function": "\n\ndef _do_login(backend, user, social_user):\n    user.backend = '{0}.{1}'.format(backend.__module__, backend.__class__.__name__)\n    login(backend.strategy.request, user)\n    if backend.setting('SESSION_EXPIRATION', False):\n        expiration = social_user.expiration_datetime()\n        if expiration:\n            try:\n                backend.strategy.request.session.set_expiry((expiration.seconds + (expiration.days * 86400)))\n            except OverflowError:\n                backend.strategy.request.session.set_expiry(None)\n", "label": 0}
{"function": "\n\ndef test_get_output_shape_without_arguments(self, layer, get_output_shape):\n    assert (get_output_shape(layer) == (3, 2))\n", "label": 0}
{"function": "\n\ndef test_metadata_tags(self):\n    self.v = self.klass(self.filename, meta=True)\n    fields = self.v.metadata.fields\n    assert ('PixelsPhysicalSizeX' in fields)\n", "label": 0}
{"function": "\n\ndef test_create_by_admin(self):\n    url = admin_reverse('cms_staticplaceholder_add')\n    with self.login_user_context(self.get_superuser()):\n        response = self.client.post(url, data={\n            'name': 'Name',\n            'code': 'content',\n        })\n        self.assertEqual(response.status_code, 302)\n", "label": 0}
{"function": "\n\ndef windows_shell(chan):\n    import threading\n    stdout.write('*** Emulating terminal on Windows; press F6 or Ctrl+Z then enter to send EOF,\\r\\nor at the end of the execution.\\r\\n')\n    stdout.flush()\n    out_lock = threading.RLock()\n\n    def write(recv, std):\n        while True:\n            data = recv(256)\n            if (not data):\n                if std:\n                    with out_lock:\n                        stdout.write('\\r\\n*** EOF reached; (press F6 or ^Z then enter to end)\\r\\n')\n                        stdout.flush()\n                break\n            stream = [stderr_bytes, stdout_bytes][std]\n            with out_lock:\n                stream.write(data)\n                stream.flush()\n    threading.Thread(target=write, args=(chan.recv, True)).start()\n    threading.Thread(target=write, args=(chan.recv_stderr, False)).start()\n    try:\n        while True:\n            d = stdin_bytes.read(1)\n            if (not d):\n                chan.shutdown_write()\n                break\n            try:\n                chan.send(d)\n            except socket.error:\n                break\n    except EOFError:\n        pass\n", "label": 0}
{"function": "\n\ndef test_exists(self):\n    block_val = predicates.existsPredicate(self.latlong1)\n    assert (block_val == ('1',))\n    block_val = predicates.existsPredicate((0, 0))\n    assert (block_val == ('0',))\n", "label": 0}
{"function": "\n\ndef get_current(self):\n    \"\\n        Returns the current ``Site`` based on the SITE_ID in the\\n        project's settings. The ``Site`` object is cached the first\\n        time it's retrieved from the database.\\n        \"\n    from django.conf import settings\n    try:\n        sid = settings.SITE_ID\n    except AttributeError:\n        from django.core.exceptions import ImproperlyConfigured\n        raise ImproperlyConfigured('You\\'re using the Django \"sites framework\" without having set the SITE_ID setting. Create a site in your database and set the SITE_ID setting to fix this error.')\n    try:\n        current_site = SITE_CACHE[sid]\n    except KeyError:\n        current_site = self.get(pk=sid)\n        SITE_CACHE[sid] = current_site\n    return current_site\n", "label": 0}
{"function": "\n\ndef parse_original_topology(topology_path):\n    with open(topology_path) as f:\n        yaml_spec = yaml.load(f)\n    return TopologySpec(yaml_spec)\n", "label": 0}
{"function": "\n\ndef test_set_serializer_default_config(tmpdir):\n    my_vcr = vcr.VCR(serializer='json')\n    with my_vcr.use_cassette(str(tmpdir.join('test.json'))):\n        assert (my_vcr.serializer == 'json')\n        urlopen('http://httpbin.org/get')\n    with open(str(tmpdir.join('test.json'))) as f:\n        assert json.loads(f.read())\n", "label": 0}
{"function": "\n\ndef test_do_revert_fail(demoenv, capsys):\n    (c, d) = demoenv\n    Cmddocs(c).do_revert('test')\n    (out, err) = capsys.readouterr()\n    assert (out == 'Error: Could not find given commit reference\\n')\n", "label": 0}
{"function": "\n\ndef pytest_funcarg__huang_darwiche_jt(request):\n\n    def priority_func_override(node):\n        introduced_arcs = 0\n        cluster = ([node] + node.neighbours)\n        for (node_a, node_b) in combinations(cluster, 2):\n            if (node_a not in node_b.neighbours):\n                assert (node_b not in node_a.neighbours)\n                introduced_arcs += 1\n        if (node.name == 'f_h'):\n            return [introduced_arcs, 0]\n        if (node.name == 'f_g'):\n            return [introduced_arcs, 1]\n        if (node.name == 'f_c'):\n            return [introduced_arcs, 2]\n        if (node.name == 'f_b'):\n            return [introduced_arcs, 3]\n        if (node.name == 'f_d'):\n            return [introduced_arcs, 4]\n        if (node.name == 'f_e'):\n            return [introduced_arcs, 5]\n        return [introduced_arcs, 10]\n    dag = pytest_funcarg__huang_darwiche_dag(request)\n    jt = build_join_tree(dag, priority_func_override)\n    return jt\n", "label": 0}
{"function": "\n\n@name_validator\ndef create_folder(self, _path, name, validate=True):\n    try:\n        folder_path = os.path.join(_path, name)\n        os.mkdir(folder_path)\n    except OSError:\n        self.errors.append((('Creating skipped: ' + name) + ' already exists'))\n", "label": 0}
{"function": "\n\ndef _extract_sections(self):\n    '\\n        Here is an example of what a section header looks like in the\\n        html of a Google Document:\\n\\n        <h3 class=\"c1\"><a name=\"h.699ffpepx6zs\"></a><span>Hello World\\n        </span></h3>\\n\\n        We split the content of the Google Document up using a regular\\n        expression that matches the above header. re.split is a pretty\\n        cool function if you haven\\'t tried it before. It puts the\\n        matching groups into the list as well as the content between\\n        the matches. Check it out here:\\n\\n        http://docs.python.org/library/re.html#re.split\\n\\n        One big thing we do in this method is replace the ugly section\\n        id that Google creates with a nicely slugified version of the\\n        section title. This makes for pretty urls.\\n        '\n    self._sections = []\n    header = '<h(?P<level>\\\\d) class=\"[^\"]+\"><a name=\"(?P<id>[^\"]+)\"></a><span>(?P<title>[^<]+)</span></h\\\\d>'\n    l = re.split(header, self._content)\n    l.pop(0)\n    while l:\n        section = Section(level=(int(l.pop(0)) - 2), id=l.pop(0), title=l.pop(0).decode('utf8'), content=l.pop(0))\n        section['id'] = slugify(section['title'])\n        if (section['level'] >= 1):\n            self._sections.append(section)\n", "label": 0}
{"function": "\n\ndef startTag(self, namespace, name, attrs):\n    assert ((namespace is None) or isinstance(namespace, string_types)), type(namespace)\n    assert isinstance(name, string_types), type(name)\n    assert all(((((namespace is None) or isinstance(namespace, string_types)) and isinstance(name, string_types) and isinstance(value, string_types)) for ((namespace, name), value) in attrs.items()))\n    return {\n        'type': 'StartTag',\n        'name': text_type(name),\n        'namespace': to_text(namespace),\n        'data': dict((((to_text(namespace, False), to_text(name)), to_text(value, False)) for ((namespace, name), value) in attrs.items())),\n    }\n", "label": 0}
{"function": "\n\n@shared_task()\ndef unpublish_object(content_type_pk, obj_pk):\n    \"\\n    Unbuild all views related to a object and then sync to S3.\\n\\n    Accepts primary keys to retrieve a model object that\\n    inherits bakery's BuildableModel class.\\n    \"\n    ct = ContentType.objects.get_for_id(content_type_pk)\n    obj = ct.get_object_for_this_type(pk=obj_pk)\n    try:\n        logger.info(('unpublish_object task has received %s' % obj))\n        obj.unbuild()\n        if (not getattr(settings, 'ALLOW_BAKERY_AUTO_PUBLISHING', True)):\n            logger.info('Not running publish command because ALLOW_BAKERY_AUTO_PUBLISHING is False')\n        else:\n            management.call_command('publish')\n    except Exception:\n        logger.error('Task Error: unpublish_object', exc_info=True)\n", "label": 0}
{"function": "\n\ndef stderr_to_parser_error(parse_args, *args, **kwargs):\n    if (isinstance(sys.stderr, StdIOBuffer) or isinstance(sys.stdout, StdIOBuffer)):\n        return parse_args(*args, **kwargs)\n    old_stdout = sys.stdout\n    old_stderr = sys.stderr\n    sys.stdout = StdIOBuffer()\n    sys.stderr = StdIOBuffer()\n    try:\n        try:\n            result = parse_args(*args, **kwargs)\n            for key in list(vars(result)):\n                if (getattr(result, key) is sys.stdout):\n                    setattr(result, key, old_stdout)\n                if (getattr(result, key) is sys.stderr):\n                    setattr(result, key, old_stderr)\n            return result\n        except SystemExit:\n            code = sys.exc_info()[1].code\n            stdout = sys.stdout.getvalue()\n            stderr = sys.stderr.getvalue()\n            raise ArgumentParserError('SystemExit', stdout, stderr, code)\n    finally:\n        sys.stdout = old_stdout\n        sys.stderr = old_stderr\n", "label": 0}
{"function": "\n\ndef __contains__(self, path):\n    '\\n        Determines whether a :class:`KeventDescriptor has been registered\\n        for the specified path.\\n\\n        :param path:\\n            Path for which the descriptor will be obtained.\\n        '\n    with self._lock:\n        path = absolute_path(path)\n        return self._has_path(path)\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    try:\n        message = record.getMessage()\n        assert isinstance(message, basestring_type)\n        record.message = _safe_unicode(message)\n    except Exception as e:\n        record.message = ('Bad message (%r): %r' % (e, record.__dict__))\n    record.asctime = self.formatTime(record, self.datefmt)\n    if (record.levelno in self._colors):\n        record.color = self._colors[record.levelno]\n        record.end_color = self._normal\n    else:\n        record.color = record.end_color = ''\n    formatted = (self._fmt % record.__dict__)\n    if record.exc_info:\n        if (not record.exc_text):\n            record.exc_text = self.formatException(record.exc_info)\n    if record.exc_text:\n        lines = [formatted.rstrip()]\n        lines.extend((_safe_unicode(ln) for ln in record.exc_text.split('\\n')))\n        formatted = '\\n'.join(lines)\n    return formatted.replace('\\n', '\\n    ')\n", "label": 0}
{"function": "\n\ndef it_adds_func_names_to_all(self):\n    base = 'uber'\n    namespace = {\n        '__all__': [],\n    }\n    generate_generic_calls(base, namespace)\n    base_funcs = (m.split('.', 1)[1] for m in METHODS if m.startswith(base))\n    assert (sorted(namespace['__all__']) == list(sorted(base_funcs)))\n", "label": 0}
{"function": "\n\ndef test_logsoftmax_grad_3():\n    npr.seed(5)\n    for ii in xrange(NUM_TRIALS):\n        np_X = npr.randn(5, 6)\n        np_T = npr.randint(0, 10, np_X.shape)\n        X = kayak.Parameter(np_X)\n        T = kayak.Targets(np_T)\n        Y = kayak.LogSoftMax(X)\n        Z = kayak.MatSum(kayak.LogMultinomialLoss(Y, T))\n        assert (kayak.util.checkgrad(X, Z) < MAX_GRAD_DIFF)\n", "label": 0}
{"function": "\n\ndef do43x(self, msg, problem):\n    if (not self.afterConnect):\n        newNick = self._getNextNick()\n        assert (newNick != self.nick)\n        log.info('Got %s: %s %s.  Trying %s.', msg.command, self.nick, problem, newNick)\n        self.sendMsg(ircmsgs.nick(newNick))\n", "label": 0}
{"function": "\n\ndef __init__(self, plug, parenting=None):\n    row = GafferUI.ListContainer(GafferUI.ListContainer.Orientation.Horizontal)\n    GafferUI.PlugValueWidget.__init__(self, row, plug, parenting=parenting)\n    with row:\n        self.__enabledWidget = GafferUI.BoolPlugValueWidget(plug['enabled'], displayMode=GafferUI.BoolWidget.DisplayMode.Switch)\n        self.__cameraWidget = GafferSceneUI.ScenePathPlugValueWidget(plug['camera'], path=GafferScene.ScenePath(plug.node()['in'], plug.node().getContext(), '/', filter=GafferScene.ScenePath.createStandardFilter(['__cameras'], 'Show only cameras')))\n        self.__cameraWidget.pathWidget().setFixedCharacterWidth(13)\n        if hasattr(self.__cameraWidget.pathWidget()._qtWidget(), 'setPlaceholderText'):\n            self.__cameraWidget.pathWidget()._qtWidget().setPlaceholderText('Render Camera')\n    self._updateFromPlug()\n", "label": 0}
{"function": "\n\ndef handle_noargs(self, *args, **options):\n    r = get_r()\n    try:\n        keys = r.r.smembers(r._metric_slugs_key)\n        slugs = set((s.split(':')[1] for s in keys))\n        r.r.srem(r._metric_slugs_key, *keys)\n        r.r.sadd(r._metric_slugs_key, *slugs)\n        p = '\\nMetrics: Converted {0} Keys to {1} Slugs\\n'\n        self.stdout.write(p.format(len(keys), len(slugs)))\n    except (IndexError, ResponseError):\n        pass\n    try:\n        keys = r.r.smembers(r._gauge_slugs_key)\n        slugs = set((s.split(':')[1] for s in keys))\n        r.r.srem(r._gauge_slugs_key, *keys)\n        r.r.sadd(r._gauge_slugs_key, *slugs)\n        p = 'Gauges: Converted {0} Keys to {1} Slugs\\n'\n        self.stdout.write(p.format(len(keys), len(slugs)))\n    except (IndexError, ResponseError):\n        pass\n    i = 0\n    categories = r.categories()\n    for category in categories:\n        try:\n            k = r._category_key(category)\n            data = r.r.get(k)\n            if data:\n                data = json.loads(data)\n                r.r.delete(k)\n                r.r.sadd(k, *set(data))\n                i += 1\n        except ResponseError:\n            pass\n    if (i > 0):\n        p = 'Converted {0} Categories from JSON -> Redis Sets\\n'\n        self.stdout.write(p.format(i))\n", "label": 0}
{"function": "\n\ndef __init__(self, env, name=None, version=None):\n    'Create a new page object or retrieves an existing page.\\n\\n        :param env: an `Environment` object.\\n        :param name: the page name or a `Resource` object.\\n        :param version: the page version. The value takes precedence over the\\n                        `Resource` version when both are specified.\\n        '\n    self.env = env\n    if version:\n        try:\n            version = int(version)\n        except ValueError:\n            version = None\n    if isinstance(name, Resource):\n        resource = name\n        name = resource.id\n        if ((version is None) and (resource.version is not None)):\n            try:\n                version = int(resource.version)\n            except ValueError:\n                version = None\n    self.name = name\n    self._resource_version = version\n    if name:\n        self._fetch(name, version)\n    else:\n        self.version = 0\n        self.text = self.comment = self.author = ''\n        self.time = None\n        self.readonly = 0\n    self.old_text = self.text\n    self.old_readonly = self.readonly\n", "label": 0}
{"function": "\n\ndef __call__(self, value):\n    if (value is None):\n        return None\n    try:\n        p = value.split(':', 2)\n        _60 = Duration._60\n        _unsigned = Duration._unsigned\n        if (len(p) == 1):\n            result = _unsigned(p[0])\n        if (len(p) == 2):\n            result = ((60 * _unsigned(p[0])) + _60(p[1]))\n        if (len(p) == 3):\n            result = (((3600 * _unsigned(p[0])) + (60 * _60(p[1]))) + _60(p[2]))\n    except ValueError:\n        raise ValueError('Invalid duration value: %s', value)\n    return result\n", "label": 0}
{"function": "\n\n@patch('homeassistant.components.sensor.tcp.Sensor.update')\ndef test_init_calls_update(self, mock_update):\n    'Should call update() method during __init__().'\n    tcp.Sensor(self.hass, TEST_CONFIG)\n    assert mock_update.called\n", "label": 0}
{"function": "\n\ndef _create_gradebook_csv(csv_id, requester, class_id, fill=0):\n    csv_id = ObjectId(csv_id)\n    csv_file = temp_directory = ''\n    deleted_files = []\n    for i in CSV.objects(expires__lt=datetime.datetime.today()):\n        deleted_files.append(i.file_location)\n        if i.file_location:\n            try:\n                os.remove(i.file_location)\n            except OSError as e:\n                logger.warning('Could not remove expired csv file at %s: %s.', i.file_location, str(e))\n        i.delete()\n    if deleted_files:\n        logger.info('Deleted csv files %s.', str(deleted_files))\n    new_csv = CSV(id=csv_id, requester=requester)\n    temp_directory = csv_file = None\n    try:\n        csv_file = open(os.path.join(config['CSV_DIRECTORY'], str(csv_id)), 'w')\n        the_class = Class.objects.get(id=ObjectId(class_id))\n        assns = list(Assignment.objects(for_class=the_class.id))\n        ((print >> csv_file), ('%s,%s' % ('Username', ','.join(('\"{0}\"'.format(i.name) for i in assns)))))\n        users = list(User.objects(account_type='student', classes=the_class.id))\n        assn_ids = [i.id for i in assns]\n        for user in users:\n            query = {\n                'assignment__in': assn_ids,\n                'most_recent': True,\n                'user': user.id,\n            }\n            submissions = list(Submission.objects(**query))\n            assn_to_score = OrderedDict(((i, str(fill)) for i in assn_ids))\n            for sub in submissions:\n                if sub.test_results:\n                    test_result = TestResult.objects.get(id=sub.test_results)\n                    if (test_result.score is not None):\n                        assn_to_score[sub.assignment] = str(test_result.score)\n            ((print >> csv_file), ('%s,%s' % (user.email, ','.join(assn_to_score.values()))))\n        csv_file.close()\n        new_csv.file_location = os.path.join(config['CSV_DIRECTORY'], str(csv_id))\n        new_csv.expires = (datetime.datetime.today() + config['TEACHER_CSV_LIFETIME'])\n        new_csv.save(force_insert=True)\n    except Exception as e:\n        new_csv.file_location = None\n        os.remove(os.path.join(config['CSV_DIRECTORY'], str(csv_id)))\n        new_csv.error_string = str(e)\n        new_csv.save(force_insert=True)\n        raise\n", "label": 1}
{"function": "\n\ndef test_exp1_complex128(self):\n    z = np.asarray((np.random.rand(4, 4) + (1j * np.random.rand(4, 4))), np.complex128)\n    z_gpu = gpuarray.to_gpu(z)\n    e_gpu = special.exp1(z_gpu)\n    assert np.allclose(sp.special.exp1(z), e_gpu.get())\n", "label": 0}
{"function": "\n\n@patch('pypdfocr.pypdfocr_tesseract.subprocess.call')\n@patch('pypdfocr.pypdfocr_tesseract.PyTesseract._is_version_uptodate')\n@patch('pypdfocr.pypdfocr_tesseract.os.name')\n@patch('pypdfocr.pypdfocr_tesseract.os.path.exists')\ndef test_tesseract_fail(self, mock_os_path_exists, mock_os_name, mock_uptodate, mock_subprocess_call, capsys):\n    '\\n            Get all the checks past and make sure we report the case where tesseract returns a non-zero status\\n        '\n    mock_os_name.__str__.return_value = 'nt'\n    p = P.PyTesseract({\n        \n    })\n    assert ('tesseract.exe' in p.binary)\n    mock_os_path_exists.return_value = True\n    mock_uptodate.return_value = (True, '')\n    mock_subprocess_call.return_value = (- 1)\n    with pytest.raises(SystemExit):\n        p.make_hocr_from_pnm('blah.tiff')\n    (out, err) = capsys.readouterr()\n    assert (p.msgs['TS_FAILED'] in out)\n", "label": 0}
{"function": "\n\ndef _safe_eval(self, code, event):\n    '\\n        Try to evaluate the given code on the given frame. If failure occurs, returns some ugly string with exception.\\n        '\n    try:\n        return eval(code, (event.globals if self.globals else {\n            \n        }), event.locals)\n    except Exception as exc:\n        return '{internal-failure}FAILED EVAL: {internal-detail}{!r}'.format(exc, **self.event_colors)\n", "label": 0}
{"function": "\n\ndef test_render_pdf(client, test_handwriting):\n    image = client.render_pdf({\n        'handwriting_id': test_handwriting['id'],\n        'text': 'Hello world!',\n    })\n    assert image\n    assert isinstance(image, bytes)\n", "label": 0}
{"function": "\n\ndef DownloadActivity(self, serviceRecord, activity):\n    workoutID = activity.ServiceData['WorkoutID']\n    logger.debug(('DownloadActivity for %s' % workoutID))\n    session = self._get_session(record=serviceRecord)\n    resp = session.get((self._urlRoot + ('/api/workout/%d' % workoutID)))\n    try:\n        res = resp.json()\n    except ValueError:\n        raise APIException(('Parse failure in Motivato activity (%d) download: %s' % (workoutID, res.text)))\n    lap = Lap(stats=activity.Stats, startTime=activity.StartTime, endTime=activity.EndTime)\n    activity.Laps = [lap]\n    activity.GPS = False\n    if (('track' in res) and ('points' in res['track'])):\n        for pt in res['track']['points']:\n            wp = Waypoint()\n            if ('moment' not in pt):\n                continue\n            wp.Timestamp = self._parseDateTime(pt['moment'])\n            if ((('lat' in pt) and ('lon' in pt)) or ('ele' in pt)):\n                wp.Location = Location()\n                if (('lat' in pt) and ('lon' in pt)):\n                    wp.Location.Latitude = pt['lat']\n                    wp.Location.Longitude = pt['lon']\n                    activity.GPS = True\n                if ('ele' in pt):\n                    wp.Location.Altitude = float(pt['ele'])\n            if ('bpm' in pt):\n                wp.HR = pt['bpm']\n            lap.Waypoints.append(wp)\n    activity.Stationary = (len(lap.Waypoints) == 0)\n    return activity\n", "label": 1}
{"function": "\n\ndef terminal(self, ret):\n    if (ret is not None):\n        assert os.path.isabs(ret), ret\n        assert os.path.exists(ret), ret\n", "label": 0}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_content():\n        self.set_content(x.content())\n    if x.has_statuscode():\n        self.set_statuscode(x.statuscode())\n    for i in xrange(x.header_size()):\n        self.add_header().CopyFrom(x.header(i))\n    if x.has_contentwastruncated():\n        self.set_contentwastruncated(x.contentwastruncated())\n    if x.has_externalbytessent():\n        self.set_externalbytessent(x.externalbytessent())\n    if x.has_externalbytesreceived():\n        self.set_externalbytesreceived(x.externalbytesreceived())\n    if x.has_finalurl():\n        self.set_finalurl(x.finalurl())\n", "label": 0}
{"function": "\n\ndef get(self, key_path, default=None):\n    try:\n        value = self._data\n        for k in key_path.split('.'):\n            value = value[k]\n        return value\n    except KeyError:\n        if (default is not None):\n            return default\n        else:\n            raise ConfigKeyError(key_path)\n", "label": 0}
{"function": "\n\ndef get_session_locking(self, session_id):\n    (lock, sessions) = self._get_lock_and_sessions(session_id)\n    session_locks = self._get_session_lock(session_id)\n    with lock:\n        session = self.get_session(session_id)\n        lck = session_locks[session_id]\n        acquired = lck.acquire(False)\n    if (not acquired):\n        lck.acquire()\n        session = self.get_session(session_id)\n    return session\n", "label": 0}
{"function": "\n\n@editor_open(__file__)\ndef test_extended_selection(editor):\n    QTest.qWait(1000)\n    mode = get_mode(editor)\n    TextHelper(editor).goto_line(0, 10)\n    QTest.qWait(1000)\n    try:\n        event = QtGui.QMouseEvent(QtCore.QEvent.MouseButtonDblClick, QtCore.QPointF(0, 0), QtCore.Qt.LeftButton, QtCore.Qt.LeftButton, QtCore.Qt.ControlModifier)\n    except TypeError:\n        event = QtGui.QMouseEvent(QtCore.QEvent.MouseButtonDblClick, QtCore.QPoint(0, 0), QtCore.Qt.LeftButton, QtCore.Qt.LeftButton, QtCore.Qt.ControlModifier)\n    mode._on_double_click(event)\n    assert (editor.textCursor().selectedText() == 'pyqode.qt')\n    QTest.qWait(1000)\n", "label": 0}
{"function": "\n\n@mock.patch('flask_via.import_module')\ndef test_add_to_app(self, _import_module):\n    foo = mock.MagicMock(__name__='foo', methods=['GET'], return_value='foo')\n    bar = mock.MagicMock(__name__='bar', methods=['GET'], return_value='bar')\n    routes = [default.Functional('/foo', foo, 'foo'), default.Functional('/bar', bar, 'bar')]\n    _import_module.side_effect = [mock.MagicMock(routes=routes)]\n    route = default.Blueprint('foo', 'foo.bar')\n    with mock.patch('flask.helpers.pkgutil.get_loader'):\n        route.add_to_app(self.app)\n    self.assertEqual(url_for('foo.foo'), '/foo')\n    self.assertEqual(url_for('foo.bar'), '/bar')\n", "label": 0}
{"function": "\n\ndef test_field_checks(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.CharField(), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E004')\n    assert ('Base field for list has errors' in errors[0].msg)\n    assert ('max_length' in errors[0].msg)\n", "label": 0}
{"function": "\n\ndef test_initialises_to_none(self):\n    r = result.SerfResult()\n    assert (r.head is None)\n    assert (r.body is None)\n", "label": 0}
{"function": "\n\ndef _safe_int(r, default=0):\n    if (r is None):\n        return default\n    try:\n        return int(r)\n    except ValueError:\n        return default\n", "label": 0}
{"function": "\n\ndef createVM(request_id, hostname, recipe, updateProgress):\n    configParser = configparser.RawConfigParser()\n    configFilePath = '/opt/chef-tools/createvm/createvm.config'\n    configParser.read(configFilePath)\n    subdomain = configParser.get('cocreate_config', 'subdomain')\n    progress = 0\n    validHostname = checkHostname(hostname)\n    if (validHostname == False):\n        return (None, None, 'Invalid hostname', progress)\n    fqdn = ((hostname + '.') + subdomain)\n    validRecipe = checkRecipe(configParser, recipe)\n    if (validRecipe == False):\n        return (None, None, 'Unsupported template', progress)\n    updateProgress(request_id, progress, 'Beginning VM template cloning')\n    try:\n        cloneVM(configParser, hostname)\n    except subprocess.CalledProcessError:\n        print('A cloning error occurred')\n        return (None, None, 'VM cloning failed', progress)\n    progress = 33\n    updateProgress(request_id, progress, 'Waiting for new VM IP address')\n    ipAddress = None\n    try:\n        ipAddress = getIP(configParser, hostname)\n    except:\n        print('Could not get IP Address')\n        return (None, None, 'Could not obtain VM IP address', progress)\n    progress = 67\n    updateProgress(request_id, progress, 'Beginning VM bootstrap')\n    try:\n        bootstrapVM(configParser, ipAddress, hostname, recipe)\n    except subprocess.CalledProcessError:\n        print('An error occurred during bootstrap')\n        return (None, None, 'VM bootstrap failed', progress)\n    url = ((('http://' + fqdn) + '/') + recipe)\n    progress = 100\n    updateProgress(request_id, progress, 'VM creation complete', url)\n    return (ipAddress, fqdn, None, progress)\n", "label": 0}
{"function": "\n\ndef test_dumping(self):\n    instance = CharSetModel(field={'big', 'comfy'})\n    data = json.loads(serializers.serialize('json', [instance]))[0]\n    field = data['fields']['field']\n    assert (sorted(field.split(',')) == ['big', 'comfy'])\n", "label": 0}
{"function": "\n\ndef opt_rulers_parser(value):\n    try:\n        converted = json.loads(value)\n        if isinstance(converted, list):\n            return converted\n        else:\n            raise ValueError\n    except ValueError:\n        raise\n    except TypeError:\n        raise ValueError\n", "label": 0}
{"function": "\n\ndef terminate(self):\n    ' Method should be called to terminate the client before the reactor\\n            is stopped.\\n\\n            @return:            Deferred which fires as soon as the client is\\n                                ready to stop the reactor.\\n            @rtype:             twisted.internet.defer.Deferred\\n        '\n    for call in self._deathCandidates.itervalues():\n        call.cancel()\n    self._deathCandidates = {\n        \n    }\n    for connection in self._connections.copy():\n        connection.destroy()\n    assert (len(self._connections) == 0)\n    Endpoint.terminate(self)\n", "label": 0}
{"function": "\n\ndef main():\n    import sys\n    import json\n    import os\n    import boto3.session\n    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')\n    if (not os.path.isdir(data_dir)):\n        os.makedirs(data_dir)\n    session = boto3.session.Session()\n    loader = session._loader\n    builder = ResourceIndexBuilder()\n    for resource_name in session.get_available_resources():\n        api_version = loader.determine_latest_version(resource_name, 'resources-1')\n        model = loader.load_service_model(resource_name, 'resources-1', api_version)\n        index = builder.build_index(model)\n        output_file = os.path.join(data_dir, resource_name, api_version, 'completions-1.json')\n        if (not os.path.isdir(os.path.dirname(output_file))):\n            os.makedirs(os.path.dirname(output_file))\n        with open(output_file, 'w') as f:\n            f.write(json.dumps(index, indent=2))\n", "label": 0}
{"function": "\n\ndef test_load_module_file(self):\n    with override_settings(MIGRATION_MODULES={\n        'migrations': 'migrations.faulty_migrations.file',\n    }):\n        loader = MigrationLoader(connection)\n        self.assertIn('migrations', loader.unmigrated_apps, 'App with migrations module file not in unmigrated apps.')\n", "label": 0}
{"function": "\n\ndef test_getset_metadata(self):\n    m = meta.Metadata()\n    md = m.get_metadata('files/one')\n    assert (md == 'r--------')\n    d = self.tmpdir()\n    p = os.path.join(d, 'test')\n    utils.touch(p)\n    assert (m.get_metadata(p) != md)\n    m.set_metadata(p, md)\n    assert (m.get_metadata(p) == md)\n", "label": 0}
{"function": "\n\ndef create(self, validated_data):\n    try:\n        document = Document.objects.get(pk=validated_data['document'])\n        validated_data['folder'].documents.add(document)\n    except Exception as exception:\n        raise ValidationError(exception)\n    return {\n        'document': document.pk,\n    }\n", "label": 0}
{"function": "\n\ndef wait_till_stopped(self, conf, container_id, timeout=10, message=None, waiting=True):\n    'Wait till a container is stopped'\n    stopped = False\n    inspection = None\n    for _ in until(timeout=timeout, action=message):\n        try:\n            inspection = conf.harpoon.docker_context.inspect_container(container_id)\n            if (not isinstance(inspection, dict)):\n                log.error('Weird response from inspecting the container\\tresponse=%s', inspection)\n            elif (not inspection['State']['Running']):\n                stopped = True\n                conf.container_id = None\n                break\n            else:\n                break\n        except (socket.timeout, ValueError):\n            log.warning('Failed to inspect the container\\tcontainer_id=%s', container_id)\n        except DockerAPIError as error:\n            if (error.response.status_code != 404):\n                raise\n            else:\n                break\n    if (not inspection):\n        log.warning('Failed to inspect the container!')\n        stopped = True\n        exit_code = 1\n    else:\n        exit_code = inspection['State']['ExitCode']\n    return (stopped, exit_code)\n", "label": 0}
{"function": "\n\ndef test_binder_provider_for_type_with_metaclass():\n    A = abc.ABCMeta('A', (object,), {\n        \n    })\n    injector = Injector()\n    binder = injector.binder\n    assert isinstance(binder.provider_for(A, None).get(injector), A)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args):\n    if (len(args) == 1):\n        (start, stop, step) = (0, args[0], 1)\n    elif (len(args) == 2):\n        (start, stop, step) = (args[0], args[1], 1)\n    elif (len(args) == 3):\n        (start, stop, step) = args\n    else:\n        raise TypeError('range() requires 1-3 int arguments')\n    try:\n        (start, stop, step) = (int(start), int(stop), int(step))\n    except ValueError:\n        raise TypeError('an integer is required')\n    if (step == 0):\n        raise ValueError('range() arg 3 must not be zero')\n    elif (step < 0):\n        stop = min(stop, start)\n    else:\n        stop = max(stop, start)\n    self._start = start\n    self._stop = stop\n    self._step = step\n    self._len = (((stop - start) // step) + bool(((stop - start) % step)))\n", "label": 0}
{"function": "\n\ndef test_history_form_view_without_getting_history(self):\n    request = RequestFactory().post('/')\n    request.session = 'session'\n    request._messages = FallbackStorage(request)\n    request.user = self.user\n    poll = Poll.objects.create(question='why?', pub_date=today)\n    poll.question = 'how?'\n    poll.save()\n    history = poll.history.all()[0]\n    admin_site = AdminSite()\n    admin = SimpleHistoryAdmin(Poll, admin_site)\n    with patch('simple_history.admin.render') as mock_render:\n        admin.history_form_view(request, poll.id, history.pk)\n    context = {\n        'original': poll,\n        'change_history': False,\n        'title': ('Revert %s' % force_text(poll)),\n        'adminform': ANY,\n        'object_id': poll.id,\n        'is_popup': False,\n        'media': ANY,\n        'errors': ANY,\n        'app_label': 'tests',\n        'original_opts': ANY,\n        'changelist_url': '/admin/tests/poll/',\n        'change_url': ANY,\n        'history_url': '/admin/tests/poll/1/history/',\n        'add': False,\n        'change': True,\n        'has_add_permission': admin.has_add_permission(request),\n        'has_change_permission': admin.has_change_permission(request, poll),\n        'has_delete_permission': admin.has_delete_permission(request, poll),\n        'has_file_field': True,\n        'has_absolute_url': False,\n        'form_url': '',\n        'opts': ANY,\n        'content_type_id': ANY,\n        'save_as': admin.save_as,\n        'save_on_top': admin.save_on_top,\n        'root_path': getattr(admin_site, 'root_path', None),\n    }\n    mock_render.assert_called_once_with(request, template_name=admin.object_history_form_template, dictionary=context, current_app=admin_site.name)\n", "label": 0}
{"function": "\n\ndef doCapNew(self, msg):\n    if (len(msg.args) != 3):\n        log.warning('Bad CAP NEW from server: %r', msg)\n        return\n    caps = msg.args[2].split()\n    assert caps, 'Empty list of capabilities'\n    self._addCapabilities(msg.args[2])\n    if ((not self.sasl_authenticated) and ('sasl' in self.state.capabilities_ls)):\n        self.resetSasl()\n        s = self.state.capabilities_ls['sasl']\n        if (s is not None):\n            self.filterSaslMechanisms(set(s.split(',')))\n    common_supported_unrequested_capabilities = (set(self.state.capabilities_ls) & (self.REQUEST_CAPABILITIES - self.state.capabilities_ack))\n    if common_supported_unrequested_capabilities:\n        caps = ' '.join(sorted(common_supported_unrequested_capabilities))\n        self.sendMsg(ircmsgs.IrcMsg(command='CAP', args=('REQ', caps)))\n", "label": 0}
{"function": "\n\ndef test_flags_that_cause_program_abort(self):\n    argv_1 = ['--boo!']\n    self.assertRaises(SystemExit, ParseArgs, argv_1, self.function)\n    argv_2 = ['--version']\n    try:\n        ParseArgs(argv_2, self.function)\n        raise\n    except SystemExit:\n        pass\n", "label": 0}
{"function": "\n\ndef test_json():\n    response = auto_namedtuple('Response', data='{\"foo\": \"bar\"}')\n    instance = Response(response)\n    assert (instance.json == {\n        'foo': 'bar',\n    })\n", "label": 0}
{"function": "\n\n@classmethod\ndef detect_worktree(cls, binary='git', subdir=None):\n    \"Detect the git working tree above cwd and return it; else, return None.\\n\\n    :param string binary: The path to the git binary to use, 'git' by default.\\n    :param string subdir: The path to start searching for a git repo.\\n    :returns: path to the directory where the git working tree is rooted.\\n    :rtype: string\\n    \"\n    cmd = [binary, 'rev-parse', '--show-toplevel']\n    try:\n        if subdir:\n            with pushd(subdir):\n                (process, out) = cls._invoke(cmd)\n        else:\n            (process, out) = cls._invoke(cmd)\n        cls._check_result(cmd, process.returncode, raise_type=Scm.ScmException)\n    except Scm.ScmException:\n        return None\n    return cls._cleanse(out)\n", "label": 0}
{"function": "\n\ndef test_key_has_correct_str(self):\n    '\\n        Calling str on a Key instance returns the proper string.\\n        '\n    key = pem.Key(b'test')\n    assert (str(key) == 'test')\n", "label": 0}
{"function": "\n\ndef value(self, key, value=1):\n    'Set value of a counter by counter key'\n    if isinstance(key, six.string_types):\n        key = (key,)\n    assert isinstance(key, tuple), 'event key type error'\n    if (key not in self.counters):\n        self.counters[key] = self.cls()\n    self.counters[key].value(value)\n    return self\n", "label": 0}
{"function": "\n\ndef cached(self, timeout=None, extra=None, key_func=func_cache_key):\n    '\\n        A decorator for caching function calls\\n        '\n    if callable(timeout):\n        return self.cached(key_func=key_func)(timeout)\n\n    def decorator(func):\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            cache_key = ('c:' + key_func(func, args, kwargs, extra))\n            try:\n                result = self.get(cache_key)\n            except CacheMiss:\n                result = func(*args, **kwargs)\n                self.set(cache_key, result, timeout)\n            return result\n\n        def invalidate(*args, **kwargs):\n            cache_key = ('c:' + key_func(func, args, kwargs, extra))\n            self.delete(cache_key)\n        wrapper.invalidate = invalidate\n\n        def key(*args, **kwargs):\n            cache_key = ('c:' + key_func(func, args, kwargs, extra))\n            return CacheKey.make(cache_key, cache=self, timeout=timeout)\n        wrapper.key = key\n        return wrapper\n    return decorator\n", "label": 0}
{"function": "\n\n@transaction.atomic\ndef post(self, request, *args, **kwargs):\n    referer_url = request.GET.get('referer', '')\n    nextpage = request.POST.get('next')\n    form = DomainRegistrationForm(request.POST)\n    context = self.get_context_data(form=form)\n    if form.is_valid():\n        reqs_today = RegistrationRequest.get_requests_today()\n        max_req = settings.DOMAIN_MAX_REGISTRATION_REQUESTS_PER_DAY\n        if (reqs_today >= max_req):\n            context.update({\n                'current_page': {\n                    'page_name': _('Oops!'),\n                },\n                'error_msg': (_('Number of domains requested today exceeds limit (%d) - contact Dimagi') % max_req),\n                'show_homepage_link': 1,\n            })\n            return render(request, 'error.html', context)\n        try:\n            domain_name = request_new_domain(request, form, is_new_user=self.is_new_user)\n        except NameUnavailableException:\n            context.update({\n                'current_page': {\n                    'page_name': _('Oops!'),\n                },\n                'error_msg': _('Project name already taken - please try another'),\n                'show_homepage_link': 1,\n            })\n            return render(request, 'error.html', context)\n        if self.is_new_user:\n            context.update({\n                'requested_domain': domain_name,\n                'track_domain_registration': True,\n                'current_page': {\n                    'page_name': _('Confirm Account'),\n                },\n            })\n            return render(request, 'registration/confirmation_sent.html', context)\n        else:\n            if nextpage:\n                return HttpResponseRedirect(nextpage)\n            if referer_url:\n                return redirect(referer_url)\n            return HttpResponseRedirect(reverse('domain_homepage', args=[domain_name]))\n    return self.render_to_response(context)\n", "label": 0}
{"function": "\n\ndef _get(self, key, default=None):\n    with lockutils.lock(key):\n        return self._get_unlocked(key, default)[1]\n", "label": 0}
{"function": "\n\ndef test_manage_session():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test'],\n        }, key='test')\n        key = Key.load(h.get_key('test'))\n        assert (key.api_key == 'test')\n        assert (key.is_admin() == False)\n        assert (key.can_create_key() == False)\n        assert (key.can_create_user() == False)\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == False)\n        assert (key.can_write('test') == True)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == True)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef slurp(self):\n    '\\n        :returns: item_list - list of Redshift Policies.\\n        :returns: exception_map - A dict where the keys are a tuple containing the\\n            location of the exception and the value is the actual exception\\n\\n        '\n    self.prep_for_slurp()\n    from security_monkey.common.sts_connect import connect\n    item_list = []\n    exception_map = {\n        \n    }\n    for account in self.accounts:\n        for region in regions():\n            app.logger.debug('Checking {}/{}/{}'.format(self.index, account, region.name))\n            try:\n                redshift = connect(account, 'redshift', region=region)\n                all_clusters = []\n                marker = None\n                while True:\n                    response = self.wrap_aws_rate_limited_call(redshift.describe_clusters, marker=marker)\n                    all_clusters.extend(response['DescribeClustersResponse']['DescribeClustersResult']['Clusters'])\n                    if (response['DescribeClustersResponse']['DescribeClustersResult']['Marker'] is not None):\n                        marker = response['DescribeClustersResponse']['DescribeClustersResult']['Marker']\n                    else:\n                        break\n            except Exception as e:\n                if (region.name not in TROUBLE_REGIONS):\n                    exc = BotoConnectionIssue(str(e), 'redshift', account, region.name)\n                    self.slurp_exception((self.index, account, region.name), exc, exception_map)\n                continue\n            app.logger.debug('Found {} {}'.format(len(all_clusters), Redshift.i_am_plural))\n            for cluster in all_clusters:\n                cluster_id = cluster['ClusterIdentifier']\n                if self.check_ignore_list(cluster_id):\n                    continue\n                item = RedshiftCluster(region=region.name, account=account, name=cluster_id, config=dict(cluster))\n                item_list.append(item)\n    return (item_list, exception_map)\n", "label": 0}
{"function": "\n\ndef generate_models(self, skip_invalid=False, table_names=None):\n    database = self.introspect(table_names=table_names)\n    models = {\n        \n    }\n\n    class BaseModel(Model):\n\n        class Meta():\n            database = self.metadata.database\n\n    def _create_model(table, models):\n        for foreign_key in database.foreign_keys[table]:\n            dest = foreign_key.dest_table\n            if ((dest not in models) and (dest != table)):\n                _create_model(dest, models)\n        primary_keys = []\n        columns = database.columns[table]\n        for (db_column, column) in columns.items():\n            if column.primary_key:\n                primary_keys.append(column.name)\n        multi_column_indexes = database.multi_column_indexes(table)\n        column_indexes = database.column_indexes(table)\n\n        class Meta():\n            indexes = multi_column_indexes\n        composite_key = False\n        if (len(primary_keys) == 0):\n            primary_keys = columns.keys()\n        if (len(primary_keys) > 1):\n            Meta.primary_key = CompositeKey(*[field.name for (col, field) in columns.items() if (col in primary_keys)])\n            composite_key = True\n        attrs = {\n            'Meta': Meta,\n        }\n        for (db_column, column) in columns.items():\n            FieldClass = column.field_class\n            if (FieldClass is UnknownField):\n                FieldClass = BareField\n            params = {\n                'db_column': db_column,\n                'null': column.nullable,\n            }\n            if (column.primary_key and composite_key):\n                if (FieldClass is PrimaryKeyField):\n                    FieldClass = IntegerField\n                params['primary_key'] = False\n            elif (column.primary_key and (FieldClass is not PrimaryKeyField)):\n                params['primary_key'] = True\n            if column.is_foreign_key():\n                if column.is_self_referential_fk():\n                    params['rel_model'] = 'self'\n                else:\n                    dest_table = column.foreign_key.dest_table\n                    params['rel_model'] = models[dest_table]\n                if column.to_field:\n                    params['to_field'] = column.to_field\n                params['related_name'] = ('%s_%s_rel' % (table, db_column))\n            if ((db_column in column_indexes) and (not column.is_primary_key())):\n                if column_indexes[db_column]:\n                    params['unique'] = True\n                elif (not column.is_foreign_key()):\n                    params['index'] = True\n            attrs[column.name] = FieldClass(**params)\n        try:\n            models[table] = type(str(table), (BaseModel,), attrs)\n        except ValueError:\n            if (not skip_invalid):\n                raise\n    for (table, model) in sorted(database.model_names.items()):\n        if (table not in models):\n            _create_model(table, models)\n    return models\n", "label": 0}
{"function": "\n\ndef unquote(string):\n    if (not string):\n        return b''\n    res = string.split(b'%')\n    if (len(res) != 1):\n        string = res[0]\n        for item in res[1:]:\n            try:\n                string += (bytes([int(item[:2], 16)]) + item[2:])\n            except ValueError:\n                string += (b'%' + item)\n    return string\n", "label": 0}
{"function": "\n\ndef __hash__(self):\n    try:\n        return self.__hash\n    except AttributeError:\n        h = self.__hash = hash(self._items)\n        return h\n", "label": 0}
{"function": "\n\ndef create_settings(settings_path):\n    'Create settings file.'\n    err = False\n    default_theme = {\n        'enabled': False,\n        'themes': [],\n    }\n    j = json.dumps(default_theme, sort_keys=True, indent=4, separators=(',', ': '))\n    try:\n        with open(settings_path, 'w') as f:\n            f.write((j + '\\n'))\n    except Exception:\n        err = True\n    return err\n", "label": 0}
{"function": "\n\ndef create(self, req, body):\n    'Create or import keypair.\\n\\n        Sending name will generate a key and return private_key\\n        and fingerprint.\\n\\n        You can send a public_key to add an existing ssh key\\n\\n        params: keypair object with:\\n            name (required) - string\\n            public_key (optional) - string\\n        '\n    context = req.environ['nova.context']\n    authorize(context, action='create')\n    try:\n        params = body['keypair']\n        name = params['name']\n    except KeyError:\n        msg = _('Invalid request body')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        if ('public_key' in params):\n            keypair = self.api.import_key_pair(context, context.user_id, name, params['public_key'])\n            keypair = self._filter_keypair(keypair, user_id=True)\n        else:\n            (keypair, private_key) = self.api.create_key_pair(context, context.user_id, name)\n            keypair = self._filter_keypair(keypair, user_id=True)\n            keypair['private_key'] = private_key\n        return {\n            'keypair': keypair,\n        }\n    except exception.KeypairLimitExceeded:\n        msg = _('Quota exceeded, too many key pairs.')\n        raise webob.exc.HTTPForbidden(explanation=msg)\n    except exception.InvalidKeypair as exc:\n        raise webob.exc.HTTPBadRequest(explanation=exc.format_message())\n    except exception.KeyPairExists as exc:\n        raise webob.exc.HTTPConflict(explanation=exc.format_message())\n", "label": 0}
{"function": "\n\ndef convertPyClassOrFunctionDefinitionToForaFunctionExpression(self, classOrFunctionDefinition, objectIdToObjectDefinition):\n    pyAst = self.convertClassOrFunctionDefinitionToNativePyAst(classOrFunctionDefinition, objectIdToObjectDefinition)\n    assert (pyAst is not None)\n    sourcePath = objectIdToObjectDefinition[classOrFunctionDefinition.sourceFileId].path\n    tr = None\n    if isinstance(classOrFunctionDefinition, TypeDescription.FunctionDefinition):\n        if (isinstance(pyAst, ForaNative.PythonAstStatement) and pyAst.isFunctionDef()):\n            tr = self.nativeConverter.convertPythonAstFunctionDefToForaOrParseError(pyAst.asFunctionDef, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]))\n        else:\n            assert pyAst.isLambda()\n            tr = self.nativeConverter.convertPythonAstLambdaToForaOrParseError(pyAst.asLambda, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]))\n    elif isinstance(classOrFunctionDefinition, TypeDescription.ClassDefinition):\n        objectIdToFreeVar = {v: k for (k, v) in classOrFunctionDefinition.freeVariableMemberAccessChainsToId.iteritems()}\n        baseClasses = [objectIdToFreeVar[baseId].split('.') for baseId in classOrFunctionDefinition.baseClassIds]\n        tr = self.nativeConverter.convertPythonAstClassDefToForaOrParseError(pyAst.asClassDef, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]), baseClasses)\n    else:\n        assert False\n    if isinstance(tr, ForaNative.PythonToForaConversionError):\n        raise convertNativePythonToForaConversionError(tr, sourcePath)\n    return tr\n", "label": 1}
{"function": "\n\ndef get_users(self):\n    node = utils.get_repo_node(self.env, self.gitolite_admin_reponame, 'keydir')\n    assert node.isdir, ('Node %s at /keydir/ is not a directory' % node)\n    for child in node.get_entries():\n        name = child.get_name()\n        assert name.endswith('.pub'), ('Node %s' % name)\n        name = name[:(- 4)]\n        (yield name)\n", "label": 0}
{"function": "\n\ndef test_astar_directed2(self):\n    XG2 = nx.DiGraph()\n    XG2.add_edges_from([[1, 4, {\n        'weight': 1,\n    }], [4, 5, {\n        'weight': 1,\n    }], [5, 6, {\n        'weight': 1,\n    }], [6, 3, {\n        'weight': 1,\n    }], [1, 3, {\n        'weight': 50,\n    }], [1, 2, {\n        'weight': 100,\n    }], [2, 3, {\n        'weight': 100,\n    }]])\n    assert (nx.astar_path(XG2, 1, 3) == [1, 4, 5, 6, 3])\n", "label": 0}
{"function": "\n\ndef get_profile(self):\n    '\\n        Returns site-specific profile for this user. Raises\\n        SiteProfileNotAvailable if this site does not allow profiles.\\n        '\n    warnings.warn('The use of AUTH_PROFILE_MODULE to define user profiles has been deprecated.', PendingDeprecationWarning)\n    if (not hasattr(self, '_profile_cache')):\n        from django.conf import settings\n        if (not getattr(settings, 'AUTH_PROFILE_MODULE', False)):\n            raise SiteProfileNotAvailable('You need to set AUTH_PROFILE_MODULE in your project settings')\n        try:\n            (app_label, model_name) = settings.AUTH_PROFILE_MODULE.split('.')\n        except ValueError:\n            raise SiteProfileNotAvailable('app_label and model_name should be separated by a dot in the AUTH_PROFILE_MODULE setting')\n        try:\n            model = models.get_model(app_label, model_name)\n            if (model is None):\n                raise SiteProfileNotAvailable('Unable to load the profile model, check AUTH_PROFILE_MODULE in your project settings')\n            self._profile_cache = model._default_manager.using(self._state.db).get(user__id__exact=self.id)\n            self._profile_cache.user = self\n        except (ImportError, ImproperlyConfigured):\n            raise SiteProfileNotAvailable\n    return self._profile_cache\n", "label": 0}
{"function": "\n\ndef run_is_false(self, action):\n    try:\n        value = self._lookup(action)\n    except AssertionError:\n        pass\n    else:\n        self.assertIn(value, ('', None, False, 0))\n", "label": 0}
{"function": "\n\n@mock.patch.object(iscsi_deploy, 'continue_deploy', autospec=True)\n@mock.patch.object(iscsi_deploy, 'build_deploy_ramdisk_options', autospec=True)\ndef test_do_agent_iscsi_deploy_preserve_ephemeral(self, build_options_mock, continue_deploy_mock):\n    'Ensure the disk is not wiped if preserve_ephemeral is True.'\n    build_options_mock.return_value = {\n        'deployment_key': 'abcdef',\n        'iscsi_target_iqn': 'iqn-qweqwe',\n        'iscsi_portal_port': 3260,\n    }\n    agent_client_mock = mock.MagicMock(spec_set=agent_client.AgentClient)\n    agent_client_mock.start_iscsi_target.return_value = {\n        'command_status': 'SUCCESS',\n        'command_error': None,\n    }\n    driver_internal_info = {\n        'agent_url': 'http://1.2.3.4:1234',\n    }\n    self.node.driver_internal_info = driver_internal_info\n    self.node.save()\n    uuid_dict_returned = {\n        'root uuid': 'some-root-uuid',\n    }\n    continue_deploy_mock.return_value = uuid_dict_returned\n    with task_manager.acquire(self.context, self.node.uuid, shared=False) as task:\n        task.node.instance_info['preserve_ephemeral'] = True\n        iscsi_deploy.do_agent_iscsi_deploy(task, agent_client_mock)\n        build_options_mock.assert_called_once_with(task.node)\n        agent_client_mock.start_iscsi_target.assert_called_once_with(task.node, 'iqn-qweqwe', 3260, wipe_disk_metadata=False)\n", "label": 0}
{"function": "\n\ndef test_http_default_port():\n    with InstalledApp(wsgi_app.simple_app, host=HOST, port=80) as app:\n        http = httplib2.Http()\n        (resp, content) = http.request('http://some_hopefully_nonexistant_domain/')\n        assert (content == b'WSGI intercept successful!\\n')\n        assert app.success()\n", "label": 0}
{"function": "\n\ndef test_equality():\n    (a, b, c) = (Identity(3), eye(3), ImmutableMatrix(eye(3)))\n    for x in [a, b, c]:\n        for y in [a, b, c]:\n            assert x.equals(y)\n", "label": 0}
{"function": "\n\ndef test_connect(self):\n    with expect(mock(gevent_transport, 'super')).args(is_arg(GeventTransport), GeventTransport).returns(mock()) as parent:\n        expect(parent.connect).args(('host', 'port'), klass=is_arg(socket.socket)).returns('somedata')\n    self.transport.connect(('host', 'port'))\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **kwargs):\n    valid_domains = ['@stud.ntnu.no', '@ntnu.no']\n    for campain_id in args:\n        try:\n            campain = Campaign.objects.filter(pk=int(campain_id))\n        except Campaign.DoesNotExist:\n            raise CommandError(('Campaign ID %s does not exits' % campain_id))\n        signatures_qs = Signature.objects.filter(campaign__pk=int(campain_id))\n        for domain in valid_domains:\n            signatures_qs = signatures_qs.exclude(email__endswith=domain)\n        for signature in signatures_qs:\n            signature.delete()\n", "label": 0}
{"function": "\n\ndef get(self, build_id):\n    build = Build.query.options(joinedload('project', innerjoin=True), joinedload('author'), joinedload('source').joinedload('revision'), subqueryload_all('stats')).get(build_id)\n    if (build is None):\n        return ('', 404)\n    try:\n        most_recent_run = Build.query.filter((Build.project == build.project), (Build.date_created < build.date_created), (Build.status == Status.finished), (Build.id != build.id)).join(Source, (Build.source_id == Source.id)).filter(*build_type.get_any_commit_build_filters()).options(contains_eager('source').joinedload('revision'), joinedload('author')).order_by(Build.date_created.desc())[0]\n    except IndexError:\n        most_recent_run = None\n    jobs = list(Job.query.filter((Job.build_id == build.id)))\n    if (not jobs):\n        test_failures = []\n        num_test_failures = 0\n    else:\n        test_failures = TestCase.query.options(joinedload('job', innerjoin=True)).filter(TestCase.job_id.in_([j.id for j in jobs]), (TestCase.result == Result.failed)).order_by(TestCase.name.asc())\n        num_test_failures = test_failures.count()\n        test_failures = test_failures[:25]\n    failures_by_job = defaultdict(list)\n    for failure in test_failures:\n        failures_by_job[failure.job].append(failure)\n    failure_origins = find_failure_origins(build, test_failures)\n    for test_failure in test_failures:\n        test_failure.origin = failure_origins.get(test_failure)\n    if (most_recent_run and (build.status == Status.finished)):\n        changed_tests = find_changed_tests(build, most_recent_run)\n    else:\n        changed_tests = []\n    seen_by = list(User.query.join(BuildSeen, (BuildSeen.user_id == User.id)).filter((BuildSeen.build_id == build.id)))\n    extended_serializers = {\n        TestCase: TestCaseWithOriginCrumbler(),\n    }\n    event_list = list(Event.query.filter((Event.item_id == build.id)).order_by(Event.date_created.desc()))\n    context = self.serialize(build)\n    context.update({\n        'jobs': jobs,\n        'seenBy': seen_by,\n        'events': event_list,\n        'failures': get_failure_reasons(build),\n        'testFailures': {\n            'total': num_test_failures,\n            'tests': self.serialize(test_failures, extended_serializers),\n        },\n        'testChanges': self.serialize(changed_tests, extended_serializers),\n        'parents': self.serialize(get_parents_last_builds(build)),\n    })\n    return self.respond(context)\n", "label": 0}
{"function": "\n\ndef test_adding_many(self):\n    with CaptureLastQuery() as cap:\n        list(Author.objects.straight_join().sql_cache().sql_big_result().sql_buffer_result())\n    assert cap.query.startswith('SELECT STRAIGHT_JOIN SQL_BIG_RESULT SQL_BUFFER_RESULT SQL_CACHE ')\n", "label": 0}
{"function": "\n\ndef __call__(self, item, context=None):\n    array_value = self._array_expression(item, context)\n    if (not isinstance(array_value, list)):\n        return None\n    index_value = self._index_expression(item, context)\n    if (not isinstance(index_value, int)):\n        return None\n    try:\n        return array_value[index_value]\n    except IndexError:\n        return None\n", "label": 0}
{"function": "\n\ndef _parse_settings_bond_6(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond6.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '6',\n    }\n    for binding in ['miimon', 'downdelay', 'updelay']:\n        if (binding in opts):\n            try:\n                int(opts[binding])\n                bond.update({\n                    binding: opts[binding],\n                })\n            except ValueError:\n                _raise_error_iface(iface, binding, ['integer'])\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({\n                binding: bond_def[binding],\n            })\n    if ('use_carrier' in opts):\n        if (opts['use_carrier'] in _CONFIG_TRUE):\n            bond.update({\n                'use_carrier': '1',\n            })\n        elif (opts['use_carrier'] in _CONFIG_FALSE):\n            bond.update({\n                'use_carrier': '0',\n            })\n        else:\n            valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({\n            'use_carrier': bond_def['use_carrier'],\n        })\n    return bond\n", "label": 0}
{"function": "\n\ndef _verify_tombstones(self, tx_objs, policy):\n    for (o_name, diskfiles) in tx_objs.items():\n        try:\n            self._open_tx_diskfile(o_name, policy)\n            self.fail('DiskFileDeleted expected')\n        except DiskFileDeleted as exc:\n            tx_delete_time = exc.timestamp\n        try:\n            self._open_rx_diskfile(o_name, policy)\n            self.fail('DiskFileDeleted expected')\n        except DiskFileDeleted as exc:\n            rx_delete_time = exc.timestamp\n        self.assertEqual(tx_delete_time, rx_delete_time)\n", "label": 0}
{"function": "\n\ndef test_validate_invalid_root_device_hints(self):\n    with task_manager.acquire(self.context, self.node.uuid, shared=True) as task:\n        task.node.properties['root_device'] = {\n            'size': 'not-int',\n        }\n        self.assertRaises(exception.InvalidParameterValue, iscsi_deploy.validate, task)\n", "label": 0}
{"function": "\n\ndef test_authorize(self):\n    'Test the ability to create an authorization.'\n    from ..conftest import credentials\n    (username, password) = credentials\n    cassette_name = self.cassette_name('authorize')\n    with self.recorder.use_cassette(cassette_name):\n        auth = self.gh.authorize(username, password, note='Test authorization', note_url='http://example.com')\n    assert isinstance(auth, github3.auths.Authorization)\n", "label": 0}
{"function": "\n\ndef _get_axis_class(axis_type, range_input):\n    if (axis_type is None):\n        return None\n    elif (axis_type == 'linear'):\n        return LinearAxis\n    elif (axis_type == 'log'):\n        return LogAxis\n    elif (axis_type == 'datetime'):\n        return DatetimeAxis\n    elif (axis_type == 'auto'):\n        if isinstance(range_input, FactorRange):\n            return CategoricalAxis\n        elif isinstance(range_input, Range1d):\n            try:\n                Datetime.validate(Datetime(), range_input.start)\n                return DatetimeAxis\n            except ValueError:\n                pass\n        return LinearAxis\n    else:\n        raise ValueError((\"Unrecognized axis_type: '%r'\" % axis_type))\n", "label": 0}
{"function": "\n\ndef generate_x509_cert(user_id, project_id, bits=2048):\n    'Generate and sign a cert for user in project.'\n    subject = _user_cert_subject(user_id, project_id)\n    with utils.tempdir() as tmpdir:\n        keyfile = os.path.abspath(os.path.join(tmpdir, 'temp.key'))\n        csrfile = os.path.abspath(os.path.join(tmpdir, 'temp.csr'))\n        utils.execute('openssl', 'genrsa', '-out', keyfile, str(bits))\n        utils.execute('openssl', 'req', '-new', '-key', keyfile, '-out', csrfile, '-batch', '-subj', subject)\n        with open(keyfile) as f:\n            private_key = f.read()\n        with open(csrfile) as f:\n            csr = f.read()\n    (serial, signed_csr) = sign_csr(csr, project_id)\n    fname = os.path.join(ca_folder(project_id), ('newcerts/%s.pem' % serial))\n    cert = {\n        'user_id': user_id,\n        'project_id': project_id,\n        'file_name': fname,\n    }\n    db.certificate_create(context.get_admin_context(), cert)\n    return (private_key, signed_csr)\n", "label": 0}
{"function": "\n\n@classmethod\ndef get_param_type(cls, param):\n    try:\n        param.aliases\n    except AttributeError:\n        return LABEL_POS\n    else:\n        return LABEL_OPT\n", "label": 0}
{"function": "\n\ndef test_basic_properties(self):\n    data = ['a', 'b', 'c', 'b', 'b', 'c', 'a', 'c', 'd']\n    cat_comp = CategoricalComponent(data)\n    np.testing.assert_equal(cat_comp.labels, data)\n    np.testing.assert_equal(cat_comp.codes, [0, 1, 2, 1, 1, 2, 0, 2, 3])\n    np.testing.assert_equal(cat_comp.categories, ['a', 'b', 'c', 'd'])\n    with warnings.catch_warnings(record=True) as w:\n        cat_comp.data\n    assert (len(w) == 1)\n    assert (str(w[0].message) == \"The 'data' attribute is deprecated. Use 'codes' instead to access the underlying index of the categories\")\n", "label": 0}
{"function": "\n\n@staticmethod\ndef write_to_csvfile(headers, rows):\n    filename = 'cycli {}.csv'.format(datetime.now().strftime('%Y-%m-%d at %I.%M.%S %p'))\n    with open(filename, 'wt') as csvfile:\n        csvwriter = csv.writer(csvfile, quotechar=str('\"'), quoting=csv.QUOTE_NONNUMERIC, delimiter=str(','))\n        csvwriter.writerow(headers)\n        for row in rows:\n            csvwriter.writerow(row)\n    csvfile.close()\n", "label": 0}
{"function": "\n\ndef test_create_with_foreign_key(self):\n    'Create a table with a foreign key constraint'\n    inmap = self.std_map()\n    inmap['schema public'].update({\n        'table t1': {\n            'columns': [{\n                'c11': {\n                    'type': 'integer',\n                },\n            }, {\n                'c12': {\n                    'type': 'text',\n                },\n            }],\n        },\n        'table t2': {\n            'columns': [{\n                'c21': {\n                    'type': 'integer',\n                },\n            }, {\n                'c22': {\n                    'type': 'text',\n                },\n            }, {\n                'c23': {\n                    'type': 'integer',\n                },\n            }],\n            'foreign_keys': {\n                't2_c23_fkey': {\n                    'columns': ['c23'],\n                    'references': {\n                        'columns': ['c11'],\n                        'table': 't1',\n                    },\n                },\n            },\n        },\n    })\n    sql = self.to_sql(inmap)\n    crt1 = 0\n    crt2 = 1\n    if ('t1' in sql[1]):\n        crt1 = 1\n        crt2 = 0\n    assert (fix_indent(sql[crt1]) == 'CREATE TABLE t1 (c11 integer, c12 text)')\n    assert (fix_indent(sql[crt2]) == 'CREATE TABLE t2 (c21 integer, c22 text, c23 integer)')\n    assert (fix_indent(sql[2]) == 'ALTER TABLE t2 ADD CONSTRAINT t2_c23_fkey FOREIGN KEY (c23) REFERENCES t1 (c11)')\n", "label": 0}
{"function": "\n\n@staticmethod\ndef specialize(args, gtl_map, loop_id):\n    \"Given ``args``, instances of some :class:`fusion.Arg` superclass,\\n        create and return specialized :class:`fusion.Arg` objects.\\n\\n        :param args: either a single :class:`host.Arg` object or an iterator\\n                     (accepted: list, tuple) of :class:`host.Arg` objects.\\n        :gtl_map: a dict associating global maps' names to local maps' c_names.\\n        :param loop_id: indicates the position of the args` loop in the loop\\n                        chain\\n        \"\n\n    def convert(arg, gtl_map, loop_id):\n        maps = as_tuple(arg.map, Map)\n        c_local_maps = ([None] * len(maps))\n        for (i, map) in enumerate(maps):\n            c_local_maps[i] = ([None] * len(map))\n            for (j, m) in enumerate(map):\n                c_local_maps[i][j] = gtl_map[('%s%d_%d' % (m.name, i, j))]\n        _arg = Arg(arg.data, arg.map, arg.idx, arg.access, arg._flatten)\n        _arg._loop_position = loop_id\n        _arg.position = arg.position\n        _arg.indirect_position = arg.indirect_position\n        _arg._c_local_maps = c_local_maps\n        return _arg\n    try:\n        return [convert(arg, gtl_map, loop_id) for arg in args]\n    except TypeError:\n        return convert(args, gtl_map, loop_id)\n", "label": 0}
{"function": "\n\ndef test_disk_usage(self):\n    usage = psutil.disk_usage(os.getcwd())\n    assert (usage.total > 0), usage\n    assert (usage.used > 0), usage\n    assert (usage.free > 0), usage\n    assert (usage.total > usage.used), usage\n    assert (usage.total > usage.free), usage\n    assert (0 <= usage.percent <= 100), usage.percent\n    fname = tempfile.mktemp()\n    try:\n        psutil.disk_usage(fname)\n    except OSError:\n        err = sys.exc_info()[1]\n        if (err.args[0] != errno.ENOENT):\n            raise\n    else:\n        self.fail('OSError not raised')\n", "label": 0}
{"function": "\n\ndef test_repair_corrupted_commit_segment(self):\n    self.add_objects([[1, 2, 3], [4, 5, 6]])\n    with open(os.path.join(self.tmppath, 'repository', 'data', '0', '1'), 'r+b') as fd:\n        fd.seek((- 1), os.SEEK_END)\n        fd.write(b'X')\n    self.assert_raises(Repository.ObjectNotFound, (lambda : self.get_objects(4)))\n    self.check(status=True)\n    self.get_objects(3)\n    self.assert_equal(set([1, 2, 3]), self.list_objects())\n", "label": 0}
{"function": "\n\ndef _make_window(window_dict):\n    '\\n    Creates a new class for that window and registers it at this module.\\n    '\n    cls_name = ('%sWindow' % camel_case(str(window_dict['name'])))\n    bases = (Window,)\n    attrs = {\n        '__module__': sys.modules[__name__],\n        'name': str(window_dict['name']),\n        'inv_type': str(window_dict['id']),\n        'inv_data': window_dict,\n    }\n\n    def make_slot_method(index, size=1):\n        if (size == 1):\n            return (lambda self: self.slots[index])\n        else:\n            return (lambda self: self.slots[index:(index + size)])\n    for slots in window_dict.get('slots', []):\n        index = slots['index']\n        size = slots.get('size', 1)\n        attr_name = snake_case(str(slots['name']))\n        attr_name += ('_slot' if (size == 1) else '_slots')\n        slots_method = make_slot_method(index, size)\n        slots_method.__name__ = attr_name\n        attrs[attr_name] = property(slots_method)\n    for (i, prop_name) in enumerate(window_dict.get('properties', [])):\n\n        def make_prop_method(i):\n            return (lambda self: self.properties[i])\n        prop_method = make_prop_method(i)\n        prop_name = snake_case(str(prop_name))\n        prop_method.__name__ = prop_name\n        attrs[prop_name] = property(prop_method)\n    cls = type(cls_name, bases, attrs)\n    assert (not hasattr(sys.modules[__name__], cls_name)), ('Window \"%s\" already registered at %s' % (cls_name, __name__))\n    setattr(sys.modules[__name__], cls_name, cls)\n    return cls\n", "label": 0}
{"function": "\n\ndef _connect(self):\n    '[Internal]'\n    password = None\n    if (len(self.server_list[0]) > 2):\n        password = self.server_list[0][2]\n    try:\n        self.connect(self.server_list[0][0], self.server_list[0][1], self._nickname, password, ircname=self._realname)\n    except ServerConnectionError:\n        pass\n", "label": 0}
{"function": "\n\ndef bn_relu_conv_backward(dout, cache, dres_ref=None):\n    '\\n    Backward pass for the conv-batchnorm-relu convenience layer.\\n    '\n    assert ((dres_ref is None) or (len(dres_ref) == 0))\n    (conv_cache, batchnorm_cache, relu_cache) = cache\n    if (dres_ref is not None):\n        dres_ref.append(dout)\n    (dx, dw, db) = conv_backward_fast(dout, conv_cache)\n    dout = relu_backward(dout, relu_cache)\n    (dout, dgamma, dbeta) = spatial_batchnorm_backward(dout, batchnorm_cache)\n    return (dx, dw, db, dgamma, dbeta)\n", "label": 0}
{"function": "\n\ndef value_ds_to_numpy(ds, count):\n    ret = None\n    try:\n        values = ds[(count * (- 1)):]\n        ret = numpy.array([float(value) for value in values])\n    except IndexError:\n        pass\n    except TypeError:\n        pass\n    return ret\n", "label": 0}
{"function": "\n\ndef test_bad_querysets(self):\n    with pytest.raises(ValueError) as excinfo:\n        Author.objects.all().order_by('name').iter_smart_chunks()\n    assert ('ordering' in str(excinfo.value))\n    with pytest.raises(ValueError) as excinfo:\n        Author.objects.all()[:5].iter_smart_chunks()\n    assert ('sliced QuerySet' in str(excinfo.value))\n    with pytest.raises(ValueError) as excinfo:\n        NameAuthor.objects.all().iter_smart_chunks()\n    assert ('non-integer primary key' in str(excinfo.value))\n", "label": 0}
{"function": "\n\ndef test_retrieve(client):\n    response = client.get('/widgets/1')\n    assert (response.status_code == 200)\n    assert (helpers.get_data(response) == {\n        'id': '1',\n        'name': 'Foo',\n        'description': 'foo widget',\n    })\n", "label": 0}
{"function": "\n\ndef test_server(args, prepare_func=None, family=socket.AF_INET):\n    try:\n        try:\n            sock = socket.socket(family=family)\n            sock.settimeout(args.timeout)\n            sock.connect((args.host, args.port))\n        except socket.error as e:\n            print('Unable to connect to {0}:{1}: {2}'.format(args.host, args.port, e))\n            return False\n        (remote_addr, remote_port) = sock.getpeername()[:2]\n        print('Connected to: {0}:{1}'.format(remote_addr, remote_port))\n        if (prepare_func is not None):\n            prepare_func(sock)\n            print('Pre-TLS stage completed, continuing with handshake')\n        return handle_ssl(sock, args)\n    except (Failure, socket.error) as e:\n        print(('Unable to check for vulnerability: ' + str(e)))\n        return False\n    finally:\n        if sock:\n            sock.close()\n", "label": 0}
{"function": "\n\ndef test_fold_stdev(self):\n    '\\n        Tests the standard deviations of counters is properly computed.\\n        '\n    now = 10\n    metrics = [Timer('k', 10), Timer('k', 15), Timer('j', 7.9), Timer('j', 8)]\n    result = Timer.fold(metrics, now)\n    assert (('timers.k.stdev', Timer._stdev([10, 15], 12.5), now) == self._get_metric('timers.k.stdev', result))\n    assert (('timers.j.stdev', Timer._stdev([7.9, 8], 7.95), now) == self._get_metric('timers.j.stdev', result))\n", "label": 0}
{"function": "\n\ndef exchange(self, pdu, timeout):\n    if ((not isinstance(pdu, Symmetry)) or (self.cfg.get('symm-log') is True)):\n        log.debug('SEND {0}'.format(pdu))\n    data = (pdu.to_string() if pdu else None)\n    try:\n        data = self.mac.exchange(data, timeout)\n        if (data is None):\n            return None\n    except nfc.clf.DigitalProtocolError as error:\n        log.debug('{0!r}'.format(error))\n        return None\n    pdu = ProtocolDataUnit.from_string(data)\n    if ((not isinstance(pdu, Symmetry)) or (self.cfg.get('symm-log') is True)):\n        log.debug('RECV {0}'.format(pdu))\n    return pdu\n", "label": 0}
{"function": "\n\ndef test_mreg_interpolation(self):\n    park_id = NREL.park_id['tehachapi']\n    windpark = NREL().get_windpark(park_id, 3, 2004)\n    target = windpark.get_target()\n    timestep = 600\n    measurements = target.get_measurements()[300:500]\n    (damaged, indices) = MARDestroyer().destroy(measurements, percentage=0.5)\n    before_misses = MissingDataFinder().find(damaged, timestep)\n    neighbors = windpark.get_turbines()[:(- 1)]\n    reg = 'knn'\n    regargs = {\n        'n': 10,\n        'variant': 'uniform',\n    }\n    nseries = [t.get_measurements()[300:500] for t in neighbors]\n    t_hat = MRegInterpolation().interpolate(damaged, timestep=timestep, neighbor_series=nseries, reg=reg, regargs=regargs)\n    after_misses = MissingDataFinder().find(t_hat, timestep)\n    assert (len(after_misses) < 1)\n", "label": 0}
{"function": "\n\ndef __init__(self, thread):\n    self.ident = 0\n    try:\n        self.ident = thread.ident\n    except AttributeError:\n        pass\n    if (not self.ident):\n        for (tid, athread) in threading._active.items():\n            if (athread is thread):\n                self.ident = tid\n                break\n    try:\n        self.name = thread.name\n    except AttributeError:\n        self.name = thread.getName()\n    try:\n        self.daemon = thread.daemon\n    except AttributeError:\n        self.daemon = thread.isDaemon()\n", "label": 0}
{"function": "\n\ndef __init__(self, obj, *args, **kwds):\n    assert ((len(args) % 2) == 0)\n    attrs = kwds.get('attrs', {\n        \n    })\n    for (k, v) in zip(args[::2], args[1::2]):\n        attrs[k] = v\n    self.attrs = attrs\n    self.obj = obj\n", "label": 0}
{"function": "\n\ndef indication(self, pdu):\n    if _debug:\n        deferred(ConsoleServer._debug, 'Indication %r', pdu)\n    try:\n        sys.stdout.write(pdu.pduData)\n    except Exception as err:\n        ConsoleServer._exception('Indication sys.stdout.write exception: %r', err)\n", "label": 0}
{"function": "\n\ndef test_werkzeug_upload():\n    try:\n        import werkzeug\n    except ImportError:\n        return\n    storage = app_storage()\n    object_name = 'my-txt-hello.txt'\n    filepath = (CWD + '/data/hello.txt')\n    file = None\n    with open(filepath, 'rb') as fp:\n        file = werkzeug.datastructures.FileStorage(fp)\n        file.filename = object_name\n        o = storage.upload(file, overwrite=True)\n        assert isinstance(o, Object)\n        assert (o.name == object_name)\n", "label": 0}
{"function": "\n\ndef test_match_unrolled(self):\n    ' tests that inference with scan matches result using unrolled loops '\n    unrolled_e_step = E_Step(h_new_coeff_schedule=self.h_new_coeff_schedule)\n    unrolled_e_step.register_model(self.model)\n    V = T.matrix()\n    scan_result = self.e_step.infer(V)\n    unrolled_result = unrolled_e_step.infer(V)\n    outputs = []\n    for key in scan_result:\n        outputs.append(scan_result[key])\n        outputs.append(unrolled_result[key])\n    f = function([V], outputs)\n    outputs = f(self.X)\n    assert ((len(outputs) % 2) == 0)\n    for i in xrange(0, len(outputs), 2):\n        assert np.allclose(outputs[i], outputs[(i + 1)])\n", "label": 0}
{"function": "\n\ndef handle(self, request, data):\n    try:\n        if (data['cipher'] == ''):\n            data['cipher'] = None\n        volume_type = cinder.volume_encryption_type_create(request, data['volume_type_id'], data)\n        messages.success(request, (_('Successfully created encryption for volume type: %s') % data['name']))\n        return volume_type\n    except Exception:\n        redirect = reverse('horizon:admin:volumes:index')\n        exceptions.handle(request, _('Unable to create encrypted volume type.'), redirect=redirect)\n", "label": 0}
{"function": "\n\ndef test_manage_job():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == False)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef _setup_extensions(self):\n    self.extensions = []\n    if try_murmur3:\n        self.extensions.append(murmur3_ext)\n    if try_libev:\n        self.extensions.append(libev_ext)\n    if try_cython:\n        try:\n            from Cython.Build import cythonize\n            cython_candidates = ['cluster', 'concurrent', 'connection', 'cqltypes', 'metadata', 'pool', 'protocol', 'query', 'util']\n            compile_args = ([] if is_windows else ['-Wno-unused-function'])\n            self.extensions.extend(cythonize([Extension(('cassandra.%s' % m), [('cassandra/%s.py' % m)], extra_compile_args=compile_args) for m in cython_candidates], nthreads=build_concurrency, exclude_failures=True))\n            self.extensions.extend(cythonize(NoPatchExtension('*', ['cassandra/*.pyx'], extra_compile_args=compile_args), nthreads=build_concurrency))\n        except Exception:\n            sys.stderr.write('Failed to cythonize one or more modules. These will not be compiled as extensions (optional).\\n')\n", "label": 0}
{"function": "\n\ndef test_service_delete(self):\n    with test.nested(mock.patch.object(objects.Service, 'get_by_id', return_value=objects.Service()), mock.patch.object(objects.Service, 'destroy')) as (get_by_id, destroy):\n        self.host_api.service_delete(self.ctxt, 1)\n        get_by_id.assert_called_once_with(self.ctxt, 1)\n        destroy.assert_called_once_with()\n", "label": 0}
{"function": "\n\ndef test_bad_fancy_index(self):\n    msa = TabularMSA([DNA('ABCD'), DNA('GHKM'), DNA('NRST')])\n    with self.assertRaises((KeyError, TypeError)):\n        self.get(msa, [0, 'foo'])\n    with self.assertRaises(IndexError):\n        self.get(msa, (Ellipsis, [0, 'foo']))\n", "label": 0}
{"function": "\n\ndef test_empty_update_fields(self):\n    s = Person.objects.create(name='Sara', gender='F')\n    pre_save_data = []\n\n    def pre_save_receiver(**kwargs):\n        pre_save_data.append(kwargs['update_fields'])\n    pre_save.connect(pre_save_receiver)\n    post_save_data = []\n\n    def post_save_receiver(**kwargs):\n        post_save_data.append(kwargs['update_fields'])\n    post_save.connect(post_save_receiver)\n    with self.assertNumQueries(0):\n        s.save(update_fields=[])\n    self.assertEqual(len(pre_save_data), 0)\n    self.assertEqual(len(post_save_data), 0)\n    pre_save.disconnect(pre_save_receiver)\n    post_save.disconnect(post_save_receiver)\n", "label": 0}
{"function": "\n\ndef test_delete_removes_watches(self):\n    t = ThreadFactory()\n    NewPostEvent.notify('me@me.com', t)\n    assert NewPostEvent.is_notifying('me@me.com', t)\n    t.delete()\n    assert (not NewPostEvent.is_notifying('me@me.com', t))\n", "label": 0}
{"function": "\n\ndef _emit(self, cond_br, target=None, reg=None, absolute=False, link=REGISTERS['null']):\n    if (target is None):\n        imm = 0\n    elif isinstance(target, Label):\n        target.pinned = False\n        self.asm._add_backpatch_item(target.name)\n        imm = 0\n    elif isinstance(target, int):\n        imm = target\n    else:\n        raise AssembleError('Invalid branch target: {}'.format(target))\n    if reg:\n        if ((not (reg.spec & _REG_AR)) or (reg.name not in GENERAL_PURPOSE_REGISTERS)):\n            raise AssembleError('Must be general purpose regfile A register {}'.format(reg))\n        assert (reg.addr < 32)\n        raddr_a = reg.addr\n        use_reg = True\n    else:\n        raddr_a = 0\n        use_reg = False\n    (waddr_add, waddr_mul, write_swap, pack) = self._encode_write_operands(link)\n    if pack:\n        raise AssembleError('Packing is not available for link register')\n    insn = BranchInsn(sig=15, cond_br=cond_br, rel=(not absolute), reg=use_reg, raddr_a=raddr_a, ws=write_swap, waddr_add=waddr_add, waddr_mul=waddr_mul, immediate=imm)\n    self.asm._emit(insn)\n", "label": 0}
{"function": "\n\ndef pop(self, *args, **kwargs):\n    with self.transaction():\n        return dict.pop(self, *args, **kwargs)\n", "label": 0}
{"function": "\n\n@mock.patch('st2actions.runners.windows_script_runner.run_command')\ndef test_get_share_absolute_path(self, mock_run_command):\n    runner = self._get_script_runner()\n    fixture_path = os.path.join(FIXTURES_DIR, 'net_share_C_stdout.txt')\n    with open(fixture_path, 'r') as fp:\n        stdout = fp.read()\n    mock_run_command.return_value = (2, '', '', False)\n    self.assertRaises(Exception, runner._get_share_absolute_path, share='C$')\n    mock_run_command.return_value = (0, '', '', False)\n    self.assertRaises(Exception, runner._get_share_absolute_path, share='C$')\n    mock_run_command.return_value = (0, stdout, '', False)\n    share_path = runner._get_share_absolute_path(share='C$')\n    self.assertEqual(share_path, 'C:\\\\')\n", "label": 0}
{"function": "\n\ndef save(self, filename=None, directory=None):\n    \"Save the DOT source to file.\\n\\n        Args:\\n            filename: Filename for saving the source (defaults to name + '.gv')\\n            directory: (Sub)directory for source saving and rendering.\\n        Returns:\\n            The (possibly relative) path of the saved source file.\\n        \"\n    if (filename is not None):\n        self.filename = filename\n    if (directory is not None):\n        self.directory = directory\n    filepath = self.filepath\n    tools.mkdirs(filepath)\n    data = text_type(self.source)\n    with io.open(filepath, 'w', encoding=self.encoding) as fd:\n        fd.write(data)\n    return filepath\n", "label": 0}
{"function": "\n\ndef _test_timer_context_rate(cl, proto):\n    with cl.timer('foo', rate=0.5):\n        pass\n    _timer_check(cl._sock, 1, proto, 'foo', 'ms|@0.5')\n", "label": 0}
{"function": "\n\ndef __init__(self, sliceDB, seqDB, annotationType=None, itemClass=AnnotationSeq, itemSliceClass=AnnotationSlice, itemAttrDict=None, sliceAttrDict=None, maxCache=None, autoGC=True, checkFirstID=True, **kwargs):\n    'sliceDB must map identifier to a sliceInfo object;\\n        sliceInfo must have attributes: id, start, stop, orientation;\\n        seqDB must map sequence ID to a sliceable sequence object;\\n        sliceAttrDict gives optional dict of item attributes that\\n        should be mapped to sliceDB item attributes.\\n        maxCache specfies the maximum number of annotation objects\\n        to keep in the cache.'\n    if autoGC:\n        self._weakValueDict = classutil.RecentValueDictionary(autoGC)\n    else:\n        self._weakValueDict = {\n            \n        }\n    self.autoGC = autoGC\n    if (sliceAttrDict is None):\n        sliceAttrDict = {\n            \n        }\n    if (sliceDB is not None):\n        self.sliceDB = sliceDB\n    else:\n        self.sliceDB = classutil.get_shelve_or_dict(**kwargs)\n    self.seqDB = seqDB\n    self.annotationType = annotationType\n    self.itemClass = itemClass\n    self.itemSliceClass = itemSliceClass\n    self.sliceAttrDict = sliceAttrDict\n    if (maxCache is not None):\n        self.maxCache = maxCache\n    if checkFirstID:\n        try:\n            k = iter(self).next()\n            self.get_annot_obj(k, self.sliceDB[k])\n        except KeyError:\n            raise KeyError(('cannot create annotation object %s; sequence database %s may not be correct' % (k, repr(seqDB))))\n        except StopIteration:\n            pass\n", "label": 0}
{"function": "\n\ndef get_field_history_user(self, instance):\n    try:\n        return instance._field_history_user\n    except AttributeError:\n        try:\n            if self.thread.request.user.is_authenticated():\n                return self.thread.request.user\n            return None\n        except AttributeError:\n            return None\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    ploader = PluginLoaderMock()\n    self.plug = ChatPlugin(ploader, {\n        \n    })\n    assert (ploader.provides_ident == 'Chat')\n    assert isinstance(ploader.provides_obj, ChatCore)\n", "label": 0}
{"function": "\n\ndef get_files(self, s3_conn=None):\n    logger = log.get_logger('Jobs')\n    for load_path in self.paths:\n        if (load_path.scheme == 's3'):\n            bucket = s3_conn.get_bucket(load_path.bucket)\n            s3_globber = glob2.S3Globber(bucket)\n            for keyname in s3_globber.glob(load_path.pattern):\n                if (not s3_globber.isdir(keyname)):\n                    try:\n                        key = s3_globber.get_key(keyname)\n                        if (key is not None):\n                            (yield AttrDict({\n                                'scheme': 's3',\n                                'name': key.name,\n                                'etag': key.etag,\n                                'size': key.size,\n                                'bucket': bucket,\n                            }))\n                        else:\n                            logger.warning('Key `%s` not found, skipping', keyname)\n                    except S3ResponseError as e:\n                        logger.warning('Received %s %s accessing `%s`, skipping', e.status, e.reason, keyname)\n        elif (load_path.scheme == 'file'):\n            fs_globber = glob2.Globber()\n            for fname in fs_globber.glob(load_path.pattern):\n                if (not fs_globber.isdir(fname)):\n                    (yield AttrDict({\n                        'scheme': 'file',\n                        'name': fname,\n                        'etag': None,\n                        'size': os.path.getsize(fs_globber._normalize_string(fname)),\n                        'bucket': None,\n                    }))\n        elif (load_path.scheme == 'hdfs'):\n            hdfs_host = self.spec.source.hdfs_host\n            webhdfs_port = self.spec.source.webhdfs_port\n            hdfs_user = self.spec.source.hdfs_user\n            client = PyWebHdfsClient(hdfs_host, webhdfs_port, user_name=hdfs_user)\n            hdfs_globber = glob2.HDFSGlobber(client)\n            for fname in hdfs_globber.glob(load_path.pattern):\n                if (not hdfs_globber.isdir(fname)):\n                    fileinfo = hdfs_globber.get_fileinfo(fname)\n                    (yield AttrDict({\n                        'scheme': 'hdfs',\n                        'name': fileinfo['path'],\n                        'etag': fileinfo['etag'],\n                        'size': fileinfo['length'],\n                        'bucket': None,\n                    }))\n        else:\n            assert False, ('Unknown scheme %s' % load_path.scheme)\n", "label": 1}
{"function": "\n\ndef test_min_length(self):\n    h = Hashids(min_length=25)\n    assert (h.encode(7452, 2967, 21401) == 'pO3K69b86jzc6krI416enr2B5')\n    assert (h.encode(1, 2, 3) == 'gyOwl4B97bo2fXhVaDR0Znjrq')\n    assert (h.encode(6097) == 'Nz7x3VXyMYerRmWeOBQn6LlRG')\n    assert (h.encode(99, 25) == 'k91nqP3RBe3lKfDaLJrvy8XjV')\n", "label": 0}
{"function": "\n\ndef test_root_name(webapp):\n    (status, content) = make_request(webapp, ('%s/name' % webapp.server.http.base))\n    assert (status == 200)\n    assert (content == b'Earth')\n", "label": 0}
{"function": "\n\ndef test_getattr_raise_attribute_error(self):\n    r = Request(URI, body='foo bar')\n    with self.assertRaises(AttributeError):\n        getattr(r, 'does_not_exist')\n", "label": 0}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    buildutil.linux.BuildEnvironment.add_arguments(parser)\n    args = parser.parse_args()\n    retval = (- 1)\n    env = buildutil.linux.BuildEnvironment(args)\n    env.cmake_flags = '-DMESSAGE=\"Hello, World!\"'\n    try:\n        env.git_clean()\n        env.run_cmake()\n        env.run_make()\n        env.make_archive(['Hello'], 'output.zip')\n        retval = 0\n    except buildutil.common.Error as e:\n        ((print >> sys.stderr), ('Caught buildutil error: %s' % e.error_message))\n        retval = e.error_code\n    except IOError as e:\n        ((print >> sys.stderr), ('Caught IOError for file %s: %s' % (e.filename, e.strerror)))\n        retval = (- 1)\n    return retval\n", "label": 0}
{"function": "\n\ndef _deserialize(self, value, attr, data):\n    if (not self.truthy):\n        return bool(value)\n    else:\n        try:\n            if (value in self.truthy):\n                return True\n            elif (value in self.falsy):\n                return False\n        except TypeError:\n            pass\n    self.fail('invalid')\n", "label": 0}
{"function": "\n\ndef test_array_CONMIN(self):\n    try:\n        from pyopt_driver.pyopt_driver import pyOptDriver\n    except ImportError:\n        raise SkipTest('this test requires pyOpt to be installed')\n    self.top = ArrayOpt()\n    set_as_top(self.top)\n    try:\n        self.top.driver.optimizer = 'CONMIN'\n    except ValueError:\n        raise SkipTest('CONMIN not present on this system')\n    self.top.driver.title = 'Little Test'\n    optdict = {\n        \n    }\n    self.top.driver.options = optdict\n    self.top.run()\n    assert_rel_error(self, self.top.paraboloid.x[0], 7.175775, 0.01)\n    assert_rel_error(self, self.top.paraboloid.x[1], (- 7.824225), 0.01)\n", "label": 0}
{"function": "\n\ndef test_registry_uris_param_v2(self):\n    spec = CommonSpec()\n    spec.set_params(registry_uris=['http://registry.example.com:5000/v2'], user=TEST_USER)\n    registry = spec.registry_uris.value[0]\n    assert (registry.uri == 'http://registry.example.com:5000')\n    assert (registry.docker_uri == 'registry.example.com:5000')\n    assert (registry.version == 'v2')\n", "label": 0}
{"function": "\n\n@testing.requires.predictable_gc\n@testing.uses_deprecated('.*Use event.listen')\ndef test_listeners(self):\n\n    class InstrumentingListener(object):\n\n        def __init__(self):\n            if hasattr(self, 'connect'):\n                self.connect = self.inst_connect\n            if hasattr(self, 'first_connect'):\n                self.first_connect = self.inst_first_connect\n            if hasattr(self, 'checkout'):\n                self.checkout = self.inst_checkout\n            if hasattr(self, 'checkin'):\n                self.checkin = self.inst_checkin\n            self.clear()\n\n        def clear(self):\n            self.connected = []\n            self.first_connected = []\n            self.checked_out = []\n            self.checked_in = []\n\n        def assert_total(innerself, conn, fconn, cout, cin):\n            eq_(len(innerself.connected), conn)\n            eq_(len(innerself.first_connected), fconn)\n            eq_(len(innerself.checked_out), cout)\n            eq_(len(innerself.checked_in), cin)\n\n        def assert_in(innerself, item, in_conn, in_fconn, in_cout, in_cin):\n            self.assert_(((item in innerself.connected) == in_conn))\n            self.assert_(((item in innerself.first_connected) == in_fconn))\n            self.assert_(((item in innerself.checked_out) == in_cout))\n            self.assert_(((item in innerself.checked_in) == in_cin))\n\n        def inst_connect(self, con, record):\n            print(('connect(%s, %s)' % (con, record)))\n            assert (con is not None)\n            assert (record is not None)\n            self.connected.append(con)\n\n        def inst_first_connect(self, con, record):\n            print(('first_connect(%s, %s)' % (con, record)))\n            assert (con is not None)\n            assert (record is not None)\n            self.first_connected.append(con)\n\n        def inst_checkout(self, con, record, proxy):\n            print(('checkout(%s, %s, %s)' % (con, record, proxy)))\n            assert (con is not None)\n            assert (record is not None)\n            assert (proxy is not None)\n            self.checked_out.append(con)\n\n        def inst_checkin(self, con, record):\n            print(('checkin(%s, %s)' % (con, record)))\n            assert (record is not None)\n            self.checked_in.append(con)\n\n    class ListenAll(tsa.interfaces.PoolListener, InstrumentingListener):\n        pass\n\n    class ListenConnect(InstrumentingListener):\n\n        def connect(self, con, record):\n            pass\n\n    class ListenFirstConnect(InstrumentingListener):\n\n        def first_connect(self, con, record):\n            pass\n\n    class ListenCheckOut(InstrumentingListener):\n\n        def checkout(self, con, record, proxy, num):\n            pass\n\n    class ListenCheckIn(InstrumentingListener):\n\n        def checkin(self, con, record):\n            pass\n\n    def assert_listeners(p, total, conn, fconn, cout, cin):\n        for instance in (p, p.recreate()):\n            self.assert_((len(instance.dispatch.connect) == conn))\n            self.assert_((len(instance.dispatch.first_connect) == fconn))\n            self.assert_((len(instance.dispatch.checkout) == cout))\n            self.assert_((len(instance.dispatch.checkin) == cin))\n    p = self._queuepool_fixture()\n    assert_listeners(p, 0, 0, 0, 0, 0)\n    p.add_listener(ListenAll())\n    assert_listeners(p, 1, 1, 1, 1, 1)\n    p.add_listener(ListenConnect())\n    assert_listeners(p, 2, 2, 1, 1, 1)\n    p.add_listener(ListenFirstConnect())\n    assert_listeners(p, 3, 2, 2, 1, 1)\n    p.add_listener(ListenCheckOut())\n    assert_listeners(p, 4, 2, 2, 2, 1)\n    p.add_listener(ListenCheckIn())\n    assert_listeners(p, 5, 2, 2, 2, 2)\n    del p\n    snoop = ListenAll()\n    p = self._queuepool_fixture(listeners=[snoop])\n    assert_listeners(p, 1, 1, 1, 1, 1)\n    c = p.connect()\n    snoop.assert_total(1, 1, 1, 0)\n    cc = c.connection\n    snoop.assert_in(cc, True, True, True, False)\n    c.close()\n    snoop.assert_in(cc, True, True, True, True)\n    del c, cc\n    snoop.clear()\n    c = p.connect()\n    cc = c.connection\n    snoop.assert_in(cc, False, False, True, False)\n    snoop.assert_total(0, 0, 1, 0)\n    del c, cc\n    lazy_gc()\n    snoop.assert_total(0, 0, 1, 1)\n    p.dispose()\n    snoop.clear()\n    c = p.connect()\n    c.close()\n    c = p.connect()\n    snoop.assert_total(1, 0, 2, 1)\n    c.close()\n    snoop.assert_total(1, 0, 2, 2)\n    p.dispose()\n    snoop.clear()\n    c = p.connect()\n    snoop.assert_total(1, 0, 1, 0)\n    c.invalidate()\n    snoop.assert_total(1, 0, 1, 1)\n    c.close()\n    snoop.assert_total(1, 0, 1, 1)\n    del c\n    lazy_gc()\n    snoop.assert_total(1, 0, 1, 1)\n    c = p.connect()\n    snoop.assert_total(2, 0, 2, 1)\n    c.close()\n    del c\n    lazy_gc()\n    snoop.assert_total(2, 0, 2, 2)\n    p.dispose()\n    snoop.clear()\n    c = p.connect()\n    snoop.assert_total(1, 0, 1, 0)\n    c.detach()\n    snoop.assert_total(1, 0, 1, 0)\n    c.close()\n    del c\n    snoop.assert_total(1, 0, 1, 0)\n    c = p.connect()\n    snoop.assert_total(2, 0, 2, 0)\n    c.close()\n    del c\n    snoop.assert_total(2, 0, 2, 1)\n    p = p.recreate()\n    snoop.clear()\n    c = p.connect()\n    snoop.assert_total(1, 1, 1, 0)\n    c.close()\n    snoop.assert_total(1, 1, 1, 1)\n    c = p.connect()\n    snoop.assert_total(1, 1, 2, 1)\n    c.close()\n    snoop.assert_total(1, 1, 2, 2)\n", "label": 0}
{"function": "\n\ndef test_number_to_string(self):\n    ' Numbers are turned into strings.\\n        '\n    cleaner = Cleaners()\n    in_int = 85\n    in_float = 82.12\n    in_string = 'big frame, small spirit!'\n    in_list = ['hands', 'by', 'the', 'halyards']\n    in_none = None\n    assert (cleaner.number_to_string(in_int) == str(in_int))\n    assert (cleaner.number_to_string(in_float) == str(in_float))\n    assert (cleaner.number_to_string(in_string) == in_string)\n    assert (cleaner.number_to_string(in_list) == in_list)\n    assert (cleaner.number_to_string(in_none) is None)\n", "label": 0}
{"function": "\n\ndef __call__(self, action, *args, **kwargs):\n    module_name = ('%s.%s' % (self.package, action.replace('-', '_')))\n    cwd = kwargs.get('cwd')\n    if cwd:\n        self.chdir(cwd)\n    if kwargs.get('raises'):\n        with pytest.raises(qisys.error.Error) as error:\n            qisys.script.run_action(module_name, args)\n        return str(error.value)\n    if kwargs.get('retcode'):\n        try:\n            qisys.script.run_action(module_name, args)\n        except SystemExit as e:\n            return e.code\n        return 0\n    else:\n        return qisys.script.run_action(module_name, args)\n", "label": 0}
{"function": "\n\ndef spawn(self, func, *args, **kwargs):\n\n    def _inner():\n        try:\n            return func(*args, **kwargs)\n        except gevent.GreenletExit:\n            raise\n        except:\n            self.error_hook(sys.exc_info())\n            raise\n    return self.pool.spawn(_inner)\n", "label": 0}
{"function": "\n\ndef connection_made(self, transport):\n    \"\\n        asyncio.Protocol member - called upon when there is a new socket\\n        connection. This creates a new responder (as determined by the member\\n        'responder_type') and stores in a list. Incoming data from this\\n        connection will always call\\n        on_data to the last element of this list.\\n\\n        Parameters\\n        ----------\\n        transport : asyncio.Transport\\n            The Transport handling the socket communication\\n        \"\n    self.transport = transport\n    self.responders = [self.make_responder(self)]\n    try:\n        good_func = callable(self.responders[0].on_data)\n    except AttributeError:\n        good_func = False\n    if (not good_func):\n        err_str = \"Provided responder MUST implement an 'on_data' method\"\n        raise TypeError(err_str)\n    log_info = (id(self), self.remote_hostname, self.remote_port)\n    log.info(('%d connection from %s:%s' % log_info))\n", "label": 0}
{"function": "\n\ndef __init__(self, request, subscription):\n    '\\n\\n        :param request: The WAMP request ID of this request.\\n        :type request: int\\n        :param subscription: The subscription ID for the subscription to unsubscribe from.\\n        :type subscription: int\\n        '\n    assert (type(request) in six.integer_types)\n    assert (type(subscription) in six.integer_types)\n    Message.__init__(self)\n    self.request = request\n    self.subscription = subscription\n", "label": 0}
{"function": "\n\n@testing.requires_testing_data\ndef test_head_translation():\n    'Test Maxwell filter head translation'\n    raw = Raw(raw_fname, allow_maxshield='yes').crop(0.0, 1.0, copy=False)\n    raw_sss = maxwell_filter(raw, destination=raw_fname, origin=mf_head_origin, regularize=None, bad_condition='ignore')\n    assert_meg_snr(raw_sss, Raw(sss_std_fname).crop(0.0, 1.0), 200.0)\n    with warnings.catch_warnings(record=True):\n        with catch_logging() as log:\n            raw_sss = maxwell_filter(raw, destination=mf_head_origin, origin=mf_head_origin, regularize=None, bad_condition='ignore', verbose='warning')\n    assert_true(('over 25 mm' in log.getvalue()))\n    assert_meg_snr(raw_sss, Raw(sss_trans_default_fname), 125.0)\n    destination = np.eye(4)\n    destination[(2, 3)] = 0.04\n    assert_allclose(raw_sss.info['dev_head_t']['trans'], destination)\n    with warnings.catch_warnings(record=True):\n        with catch_logging() as log:\n            raw_sss = maxwell_filter(raw, destination=sample_fname, origin=mf_head_origin, regularize=None, bad_condition='ignore', verbose='warning')\n    assert_true(('= 25.6 mm' in log.getvalue()))\n    assert_meg_snr(raw_sss, Raw(sss_trans_sample_fname), 350.0)\n    assert_allclose(raw_sss.info['dev_head_t']['trans'], read_info(sample_fname)['dev_head_t']['trans'])\n    assert_raises(RuntimeError, maxwell_filter, raw, destination=mf_head_origin, coord_frame='meg')\n    assert_raises(ValueError, maxwell_filter, raw, destination=([0.0] * 4))\n", "label": 0}
{"function": "\n\ndef password_forgotten_view(request):\n    '\\n  forgotten password view and submit.\\n  includes return_url\\n  '\n    from helios_auth.view_utils import render_template\n    from helios_auth.models import User\n    if (request.method == 'GET'):\n        return render_template(request, 'password/forgot', {\n            'return_url': request.GET.get('return_url', ''),\n        })\n    else:\n        username = request.POST['username']\n        return_url = request.POST['return_url']\n        try:\n            user = User.get_by_type_and_id('password', username)\n        except User.DoesNotExist:\n            return render_template(request, 'password/forgot', {\n                'return_url': request.GET.get('return_url', ''),\n                'error': 'no such username',\n            })\n        body = ('\\n\\nThis is a password reminder:\\n\\nYour username: %s\\nYour password: %s\\n\\n--\\n%s\\n' % (user.user_id, user.info['password'], settings.SITE_TITLE))\n        send_mail('password reminder', body, settings.SERVER_EMAIL, [('%s <%s>' % (user.info['name'], user.info['email']))], fail_silently=False)\n        return HttpResponseRedirect(return_url)\n", "label": 0}
{"function": "\n\ndef vim_choice(prompt, default, choices):\n    default = (choices.index(default) + 1)\n    choices_str = '\\n'.join([('&%s' % choice) for choice in choices])\n    try:\n        choice = int(vim.eval(('confirm(\"%s\", \"%s\", %s)' % (prompt, choices_str, default))))\n    except KeyboardInterrupt:\n        return None\n    if (choice == 0):\n        return None\n    return choices[(choice - 1)]\n", "label": 0}
{"function": "\n\ndef test_does_not_send_enroll_signal_again(self):\n    participant(user=self.user).enroll(EXPERIMENT_NAME, ['red', 'blue'])\n    with WatchSignal(user_enrolled) as signal:\n        participant(user=self.user).enroll(EXPERIMENT_NAME, ['red', 'blue'])\n        self.assertFalse(signal.called)\n", "label": 0}
{"function": "\n\ndef get_submitted_exams(course, student):\n    try:\n        exams = Exam.objects.filter(course=course).order_by('exam_num')\n    except Exam.DoesNotExist:\n        exams = None\n    try:\n        submitted_exams = ExamSubmission.objects.filter(exam__course=course, student=student)\n    except ExamSubmission.DoesNotExist:\n        submitted_exams = None\n    if (len(exams) != len(submitted_exams)):\n        for exam in exams:\n            found_exam = False\n            for submitted_exam in submitted_exams:\n                if (exam.id == submitted_exam.exam_id):\n                    found_exam = True\n            if (not found_exam):\n                submission = ExamSubmission.objects.create(student=student, exam=exam)\n                submission.save()\n        submitted_exams = ExamSubmission.objects.filter(exam__course=course, student=student)\n    return submitted_exams\n", "label": 0}
{"function": "\n\ndef testExceptions(self):\n    request = roots.Request()\n    try:\n        request.write(b'blah')\n    except NotImplementedError:\n        pass\n    else:\n        self.fail()\n    try:\n        request.finish()\n    except NotImplementedError:\n        pass\n    else:\n        self.fail()\n", "label": 0}
{"function": "\n\ndef execute(self, cmd):\n    if (len(cmd) > 237):\n        print_error('Your command must be at most 237 characters long. Longer strings might crash the server.')\n        return\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    sock.bind(('0.0.0.0', 9999))\n    sock.settimeout(2)\n    packet = ((((b'\\x0c\\x153\\x00' + os.urandom(4)) + (b'\\x00' * 38)) + struct.pack('<H', len(cmd))) + cmd).ljust(512, b'\\x00')\n    try:\n        sock.sendto(packet, (self.target, 9999))\n    except socket.error:\n        return ''\n    while True:\n        try:\n            (data, addr) = sock.recvfrom(512)\n        except socket.timeout:\n            sock.close()\n            raise\n        if ((len(data) == 512) and (data[1] == '\\x16')):\n            break\n    length = struct.unpack('<H', data[14:16])[0]\n    output = data[16:(16 + length)]\n    sock.close()\n    return output\n", "label": 0}
{"function": "\n\ndef test_int_deserialization(self):\n    s = DecimalSerializer\n    self.assertEqual(s.deserialize('1'), Decimal('1'))\n    self.assertEqual(s.deserialize('-1'), Decimal('-1'))\n    self.assertEqual(s.deserialize('-666.6'), Decimal('-666.6'))\n    self.assertEqual(s.deserialize('666.6'), Decimal('666.6'))\n    with self.assertRaises(s.exception):\n        s.serialize(\"I'm a decimal!\")\n", "label": 0}
{"function": "\n\ndef get_request_params(what, request=None, **kwargs):\n    'Returns requested argument value'\n    args = {\n        'app_name': 1,\n        'model_name': 2,\n        'pk': 4,\n    }\n    try:\n        return kwargs.get(what, (request.path.split('/')[args[what]] if request else None))\n    except IndexError:\n        pass\n", "label": 0}
{"function": "\n\ndef test_options(self):\n    app = self.build_app()\n    app.config['RAPTOR_TRIGGER'] = 'konami-code'\n    app.config['RAPTOR_DELAY'] = 5000\n    init_app(app)\n    response = app.test_client().get('/')\n    assert ('enterOn: \"konami-code\"' in response.data)\n    assert ('delayTime: 5000' in response.data)\n", "label": 0}
{"function": "\n\n@skipUnlessDBFeature('has_AsKML_function')\ndef test_askml(self):\n    with self.assertRaises(TypeError):\n        City.objects.annotate(kml=functions.AsKML('name'))\n    ptown = City.objects.annotate(kml=functions.AsKML('point', precision=9)).get(name='Pueblo')\n    self.assertEqual('<Point><coordinates>-104.609252,38.255001</coordinates></Point>', ptown.kml)\n", "label": 0}
{"function": "\n\ndef plot(figure_or_data, show_link=True, link_text='Export to plot.ly', validate=True, output_type='file', include_plotlyjs=True, filename='temp-plot.html', auto_open=True):\n    \" Create a plotly graph locally as an HTML document or string.\\n\\n    Example:\\n    ```\\n    from plotly.offline import plot\\n    import plotly.graph_objs as go\\n\\n    plot([go.Scatter(x=[1, 2, 3], y=[3, 2, 6])], filename='my-graph.html')\\n    ```\\n    More examples below.\\n\\n    figure_or_data -- a plotly.graph_objs.Figure or plotly.graph_objs.Data or\\n                      dict or list that describes a Plotly graph.\\n                      See https://plot.ly/python/ for examples of\\n                      graph descriptions.\\n\\n    Keyword arguments:\\n    show_link (default=True) -- display a link in the bottom-right corner of\\n        of the chart that will export the chart to Plotly Cloud or\\n        Plotly Enterprise\\n    link_text (default='Export to plot.ly') -- the text of export link\\n    validate (default=True) -- validate that all of the keys in the figure\\n        are valid? omit if your version of plotly.js has become outdated\\n        with your version of graph_reference.json or if you need to include\\n        extra, unnecessary keys in your figure.\\n    output_type ('file' | 'div' - default 'file') -- if 'file', then\\n        the graph is saved as a standalone HTML file and `plot`\\n        returns None.\\n        If 'div', then `plot` returns a string that just contains the\\n        HTML <div> that contains the graph and the script to generate the\\n        graph.\\n        Use 'file' if you want to save and view a single graph at a time\\n        in a standalone HTML file.\\n        Use 'div' if you are embedding these graphs in an HTML file with\\n        other graphs or HTML markup, like a HTML report or an website.\\n    include_plotlyjs (default=True) -- If True, include the plotly.js\\n        source code in the output file or string.\\n        Set as False if your HTML file already contains a copy of the plotly.js\\n        library.\\n    filename (default='temp-plot.html') -- The local filename to save the\\n        outputted chart to. If the filename already exists, it will be\\n        overwritten. This argument only applies if `output_type` is 'file'.\\n    auto_open (default=True) -- If True, open the saved file in a\\n        web browser after saving.\\n        This argument only applies if `output_type` is 'file'.\\n    \"\n    if (output_type not in ['div', 'file']):\n        raise ValueError(((\"`output_type` argument must be 'div' or 'file'. You supplied `\" + output_type) + '``'))\n    if ((not filename.endswith('.html')) and (output_type == 'file')):\n        warnings.warn((('Your filename `' + filename) + \"` didn't end with .html. Adding .html to the end of your file.\"))\n        filename += '.html'\n    (plot_html, plotdivid, width, height) = _plot_html(figure_or_data, show_link, link_text, validate, '100%', '100%', global_requirejs=False)\n    resize_script = ''\n    if ((width == '100%') or (height == '100%')):\n        resize_script = '<script type=\"text/javascript\">window.removeEventListener(\"resize\");window.addEventListener(\"resize\", function(){{Plotly.Plots.resize(document.getElementById(\"{id}\"));}});</script>'.format(id=plotdivid)\n    if (output_type == 'file'):\n        with open(filename, 'w') as f:\n            if include_plotlyjs:\n                plotly_js_script = ''.join(['<script type=\"text/javascript\">', get_plotlyjs(), '</script>'])\n            else:\n                plotly_js_script = ''\n            f.write(''.join(['<html>', '<head><meta charset=\"utf-8\" /></head>', '<body>', plotly_js_script, plot_html, resize_script, '</body>', '</html>']))\n        url = ('file://' + os.path.abspath(filename))\n        if auto_open:\n            webbrowser.open(url)\n        return url\n    elif (output_type == 'div'):\n        if include_plotlyjs:\n            return ''.join(['<div>', '<script type=\"text/javascript\">', get_plotlyjs(), '</script>', plot_html, '</div>'])\n        else:\n            return plot_html\n", "label": 1}
{"function": "\n\ndef deploy_file(self, file_name, calc_md5=True, calc_sha1=True, parameters={\n    \n}):\n    '\\n        Upload the given file to this path\\n        '\n    if calc_md5:\n        md5 = md5sum(file_name)\n    if calc_sha1:\n        sha1 = sha1sum(file_name)\n    target = self\n    if self.is_dir():\n        target = (self / pathlib.Path(file_name).name)\n    with open(file_name, 'rb') as fobj:\n        target.deploy(fobj, md5, sha1, parameters)\n", "label": 0}
{"function": "\n\ndef gradient(self, x, Y):\n    '\\n        Computes the gradient of the Polynomial kernel wrt. to the left argument, i.e.\\n        \\nabla_x k(x,y)=\\nabla_x (1+x^Ty)^d=d(1+x^Ty)^(d-1) y\\n        \\n        x - single sample on right hand side (1D vector)\\n        Y - samples on left hand side (2D matrix)\\n        '\n    assert (len(x.shape) == 1)\n    assert (len(Y.shape) == 2)\n    assert (len(x) == Y.shape[1])\n    return ((self.degree * pow((1 + x.dot(Y.T)), self.degree)) * Y)\n", "label": 0}
{"function": "\n\ndef test_invalid_content_disposition(self):\n    data = b'--1234\\nContent-Disposition: invalid; name=\"files\"; filename=\"ab.txt\"\\n\\nFoo\\n--1234--'.replace(b'\\n', b'\\r\\n')\n    args = {\n        \n    }\n    files = {\n        \n    }\n    with ExpectLog(gen_log, 'Invalid multipart/form-data'):\n        parse_multipart_form_data(b'1234', data, args, files)\n    self.assertEqual(files, {\n        \n    })\n", "label": 0}
{"function": "\n\ndef _describe_table(self, connection, table, charset=None, full_name=None):\n    'Run DESCRIBE for a ``Table`` and return processed rows.'\n    if (full_name is None):\n        full_name = self.identifier_preparer.format_table(table)\n    st = ('DESCRIBE %s' % full_name)\n    (rp, rows) = (None, None)\n    try:\n        try:\n            rp = connection.execution_options(skip_user_error_events=True).execute(st)\n        except exc.DBAPIError as e:\n            if (self._extract_error_code(e.orig) == 1146):\n                raise exc.NoSuchTableError(full_name)\n            else:\n                raise\n        rows = self._compat_fetchall(rp, charset=charset)\n    finally:\n        if rp:\n            rp.close()\n    return rows\n", "label": 0}
{"function": "\n\ndef get_model(app_label, model_name):\n    '\\n    Given an app label and a model name, returns the corresponding model class.\\n    '\n    try:\n        return apps.get_model(app_label, model_name)\n    except AppRegistryNotReady:\n        if (apps.apps_ready and (not apps.models_ready)):\n            app_config = apps.get_app_config(app_label)\n            import_module(('%s.%s' % (app_config.name, MODELS_MODULE_NAME)))\n            return apps.get_registered_model(app_label, model_name)\n        else:\n            raise\n", "label": 0}
{"function": "\n\n@contextfunction\ndef htform(context, form):\n    'Set time zone'\n    request = context['request']\n    user = None\n    if request.user.username:\n        try:\n            user = request.user.profile\n        except Exception:\n            pass\n    default_timezone = settings.HARDTREE_SERVER_DEFAULT_TIMEZONE\n    try:\n        conf = ModuleSetting.get('default_timezone')[0]\n        default_timezone = conf.value\n    except:\n        pass\n    try:\n        conf = ModuleSetting.get('default_timezone', user=user)[0]\n        default_timezone = conf.value\n    except Exception:\n        default_timezone = getattr(settings, 'HARDTREE_SERVER_TIMEZONE')[default_timezone][0]\n    all_timezones = getattr(settings, 'HARDTREE_SERVER_TIMEZONE', [(1, '(GMT-11:00) International Date Line West')])\n    title = all_timezones[int(default_timezone)][1]\n    GMT = title[4:10]\n    sign = GMT[0:1]\n    hours = int(GMT[1:3])\n    mins = int(GMT[4:6])\n    if (not form.errors):\n        for field in form:\n            try:\n                date = datetime.strptime(str(field.form.initial[field.name]), '%Y-%m-%d %H:%M:%S')\n                if date:\n                    if (sign == '-'):\n                        field.form.initial[field.name] = (date - timedelta(hours=hours, minutes=mins))\n                    else:\n                        field.form.initial[field.name] = (date + timedelta(hours=hours, minutes=mins))\n            except:\n                pass\n    return form\n", "label": 0}
{"function": "\n\ndef test_bad_mtime_check_interval(self):\n    fname = 'container-sync-realms.conf'\n    fcontents = '\\n[DEFAULT]\\nmtime_check_interval = invalid\\n'\n    with temptree([fname], [fcontents]) as tempdir:\n        logger = FakeLogger()\n        fpath = os.path.join(tempdir, fname)\n        csr = ContainerSyncRealms(fpath, logger)\n        self.assertEqual(logger.all_log_lines(), {\n            'error': [(\"Error in '%s' with mtime_check_interval: invalid literal for int() with base 10: 'invalid'\" % fpath)],\n        })\n        self.assertEqual(csr.mtime_check_interval, 300)\n", "label": 0}
{"function": "\n\ndef test_form(self):\n    form_data = {\n        \n    }\n    with open(SYM_FILE, 'rb') as f:\n        form_file_data = {\n            'file': SimpleUploadedFile('BreakpadTestApp.sym', f.read()),\n        }\n    form = SymbolsAdminForm(form_data, form_file_data)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['debug_id'], 'C1C0FA629EAA4B4D9DD2ADE270A231CC1')\n    self.assertEqual(form.cleaned_data['debug_file'], 'BreakpadTestApp.pdb')\n    self.assertEqual(form.cleaned_data['file_size'], 68149)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(UserSettingsForm, self).__init__(*args, **kwargs)\n\n    def get_language_display_name(code, desc):\n        try:\n            desc = translation.get_language_info(code)['name_local']\n            desc = string.capwords(desc)\n        except KeyError:\n            pass\n        return ('%s (%s)' % (desc, code))\n    languages = [(k, get_language_display_name(k, v)) for (k, v) in settings.LANGUAGES]\n    self.fields['language'].choices = languages\n    timezones = []\n    language = translation.get_language()\n    current_locale = translation.to_locale(language)\n    babel_locale = babel.Locale.parse(current_locale)\n    for (tz, offset) in self._sorted_zones():\n        try:\n            utc_offset = (_('UTC %(hour)s:%(min)s') % {\n                'hour': offset[:3],\n                'min': offset[3:],\n            })\n        except Exception:\n            utc_offset = ''\n        if (tz == 'UTC'):\n            tz_name = _('UTC')\n        elif (tz == 'GMT'):\n            tz_name = _('GMT')\n        else:\n            tz_label = babel.dates.get_timezone_location(tz, locale=babel_locale)\n            tz_name = (_('%(offset)s: %(label)s') % {\n                'offset': utc_offset,\n                'label': tz_label,\n            })\n        timezones.append((tz, tz_name))\n    self.fields['timezone'].choices = timezones\n", "label": 0}
{"function": "\n\n@mock.patch.object(vif, 'get_network_device', return_value='device')\n@mock.patch.object(vm_util, 'reconfigure_vm')\n@mock.patch.object(vm_util, 'get_network_detach_config_spec', return_value='fake-detach-spec')\n@mock.patch.object(vm_util, 'get_vm_detach_port_index', return_value=1)\n@mock.patch.object(vm_util, 'get_vm_ref', return_value='fake-ref')\ndef test_detach_interface(self, mock_get_vm_ref, mock_get_detach_port_index, mock_get_network_detach_config_spec, mock_reconfigure_vm, mock_get_network_device):\n    _network_api = mock.Mock()\n    self._vmops._network_api = _network_api\n    with mock.patch.object(self._session, '_call_method', return_value='hardware-devices'):\n        self._vmops.detach_interface(self._instance, self._network_values)\n    mock_get_vm_ref.assert_called_once_with(self._session, self._instance)\n    mock_get_detach_port_index(self._session, 'fake-ref')\n    mock_get_network_detach_config_spec.assert_called_once_with(self._session.vim.client.factory, 'device', 1)\n    mock_reconfigure_vm.assert_called_once_with(self._session, 'fake-ref', 'fake-detach-spec')\n    _network_api.update_instance_vnic_index(mock.ANY, self._instance, self._network_values, None)\n", "label": 0}
{"function": "\n\ndef test_helper_label():\n    f1 = [h for h in link_helper if (h[0] is helper)][0]\n    assert (helper_label(f1) == 'test helper')\n", "label": 0}
{"function": "\n\ndef test_vector_norm():\n    x = np.arange(30).reshape((5, 6))\n    s = symbol('x', discover(x))\n    assert eq(compute(s.vnorm(), x), np.linalg.norm(x))\n    assert eq(compute(s.vnorm(ord=1), x), np.linalg.norm(x.flatten(), ord=1))\n    assert eq(compute(s.vnorm(ord=4, axis=0), x), np.linalg.norm(x, ord=4, axis=0))\n    expr = s.vnorm(ord=4, axis=0, keepdims=True)\n    assert (expr.shape == compute(expr, x).shape)\n", "label": 0}
{"function": "\n\ndef assert_no_warnings(func, *args, **kw):\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        result = func(*args, **kw)\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            w = [e for e in w if (e.category is not np.VisibleDeprecationWarning)]\n        if (len(w) > 0):\n            raise AssertionError(('Got warnings when calling %s: %s' % (func.__name__, w)))\n    return result\n", "label": 0}
{"function": "\n\ndef test_check_migrations(self):\n    requires_migrations_checks = dance.Command.requires_migrations_checks\n    self.assertEqual(requires_migrations_checks, False)\n    try:\n        with mock.patch.object(BaseCommand, 'check_migrations') as check_migrations:\n            management.call_command('dance', verbosity=0)\n            self.assertFalse(check_migrations.called)\n            dance.Command.requires_migrations_checks = True\n            management.call_command('dance', verbosity=0)\n            self.assertTrue(check_migrations.called)\n    finally:\n        dance.Command.requires_migrations_checks = requires_migrations_checks\n", "label": 0}
{"function": "\n\ndef do_list(self, arg):\n    'list [arg]: lists last command issued\\n\\n        no arg -> list most recent command\\n        arg is integer -> list one history item, by index\\n        a..b, a:b, a:, ..b -> list spans from a (or start) to b (or end)\\n        arg is string -> list all commands matching string search\\n        arg is /enclosed in forward-slashes/ -> regular expression search\\n        '\n    try:\n        history = self.history.span((arg or '-1'))\n    except IndexError:\n        history = self.history.search(arg)\n    for hi in history:\n        self.poutput(hi.pr())\n", "label": 0}
{"function": "\n\ndef testWithXEffectsAndDroppedDummies(self):\n    with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n        result = ols(y=self.panel_y2, x=self.panel_x2, x_effects=['x1'], dropped_dummies={\n            'x1': 30,\n        })\n    res = result._x\n    assert_almost_equal(result._y.values.flat, [1, 4, 5])\n    exp_x = DataFrame([[1.0, 0.0, 14.0, 1.0], [0, 1, 17, 1], [0, 0, 48, 1]], columns=['x1_6', 'x1_9', 'x2', 'intercept'], index=res.index, dtype=float)\n    assert_frame_equal(res, exp_x.reindex(columns=res.columns))\n", "label": 0}
{"function": "\n\n@Cache\ndef GetTypeManager(self):\n    ' Get dynamic type manager '\n    dynTypeMgr = None\n    if self.hostSystem:\n        try:\n            dynTypeMgr = self.hostSystem.RetrieveDynamicTypeManager()\n        except vmodl.fault.MethodNotFound as err:\n            pass\n    if (not dynTypeMgr):\n        cmdlineTypesMoId = 'ha-dynamic-type-manager'\n        dynTypeMgr = vmodl.reflect.DynamicTypeManager(cmdlineTypesMoId, self.stub)\n    return dynTypeMgr\n", "label": 0}
{"function": "\n\ndef compute_files(hive_holder, output, settings):\n    ' given the current hive_holder, compute the files that have to be saved in disk\\n    return: {BlockCellName: StrOrBytes to be saved in disk}\\n    param output: something that supports info, warn, error\\n    '\n    new_files = {\n        \n    }\n    for (block_cell_name, (cell, content)) in hive_holder.resources.iteritems():\n        if isinstance(cell, VirtualCell):\n            try:\n                target = cell.evaluate(settings)\n            except ConfigurationFileError as e:\n                output.error(('Error evaluating virtual %s: %s' % (block_cell_name, e.message)))\n                continue\n            content = hive_holder[target.block_name][target.cell_name].content\n            new_files[block_cell_name] = content.load.load\n        elif content.blob_updated:\n            new_files[block_cell_name] = content.load.load\n    return new_files\n", "label": 0}
{"function": "\n\ndef _update_project_members(self, request, data, project_id):\n    users_to_add = 0\n    try:\n        available_roles = api.keystone.role_list(request)\n        member_step = self.get_step(PROJECT_USER_MEMBER_SLUG)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_to_add += len(role_list)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_added = 0\n            for user in role_list:\n                api.keystone.add_tenant_user_role(request, project=project_id, user=user, role=role.id)\n                users_added += 1\n            users_to_add -= users_added\n    except Exception:\n        if PROJECT_GROUP_ENABLED:\n            group_msg = _(', add project groups')\n        else:\n            group_msg = ''\n        exceptions.handle(request, (_('Failed to add %(users_to_add)s project members%(group_msg)s and set project quotas.') % {\n            'users_to_add': users_to_add,\n            'group_msg': group_msg,\n        }))\n", "label": 0}
{"function": "\n\ndef _execute_multi_task(self, gen, executor, task):\n    if task.unordered:\n        results_gen = self._execute_multi_gen_task(gen, executor, task)\n        return gen.send(results_gen)\n    future_tasks = [executor.submit(t) for t in task.tasks]\n    while True:\n        if (not task.wait(executor, future_tasks, self.engine.pool_timeout)):\n            self.engine.update_gui()\n        else:\n            break\n    if task.skip_errors:\n        results = []\n        for f in future_tasks:\n            try:\n                results.append(f.result())\n            except Exception:\n                pass\n    else:\n        try:\n            results = [f.result() for f in future_tasks]\n        except Exception:\n            return gen.throw(*sys.exc_info())\n    return gen.send(results)\n", "label": 0}
{"function": "\n\ndef nested_update(d, u):\n    for (k, v) in u.iteritems():\n        if isinstance(v, collections.Mapping):\n            r = nested_update(d.get(k, {\n                \n            }), v)\n            d[k] = r\n        elif isinstance(v, collections.Iterable):\n            try:\n                d[k].extend(u[k])\n            except KeyError:\n                d[k] = u[k]\n        else:\n            d[k] = u[k]\n    return d\n", "label": 0}
{"function": "\n\n@mock.patch('hm.lb_managers.networkapi_cloudstack.CloudStack')\n@mock.patch('networkapiclient.EnvironmentVIP.EnvironmentVIP')\n@mock.patch('networkapiclient.Ip.Ip')\n@mock.patch('networkapiclient.Vip.Vip')\ndef test_create_vip_failure(self, Vip, Ip, EnvironmentVIP, CloudStack):\n    client_evip = mock.Mock()\n    client_evip.search.return_value = {\n        'environment_vip': {\n            'id': 500,\n        },\n    }\n    EnvironmentVIP.return_value = client_evip\n    client_ip = mock.Mock()\n    client_ip.get_available_ip4_for_vip.return_value = {\n        'ip': {\n            'id': 303,\n        },\n    }\n    Ip.return_value = client_ip\n    client_vip = mock.Mock()\n    client_vip.add.side_effect = Exception('Cannot create vip')\n    Vip.return_value = client_vip\n    manager = networkapi_cloudstack.NetworkApiCloudstackLB(self.conf)\n    with self.assertRaises(Exception) as cm:\n        manager.create_load_balancer('tsuru')\n    exc = cm.exception\n    self.assertEqual(('Cannot create vip',), exc.args)\n    client_evip.search.assert_called_with(ambiente_p44_txt=self.conf['NETWORKAPI_AMBIENTE_P44_TXT'], cliente_txt=self.conf['NETWORKAPI_CLIENTE_TXT'], finalidade_txt=self.conf['NETWORKAPI_FINALIDADE_TXT'])\n    client_ip.get_available_ip4_for_vip.assert_called_with(500, 'tsuru hm tsuru')\n    client_ip.delete_ip4.assert_called_with(303)\n", "label": 0}
{"function": "\n\ndef test_read_file(self):\n    rel_test_path = os.path.relpath(os.path.join(self.tmp_dir, 'foo'))\n    self.assertRaises(argparse.ArgumentTypeError, cli.read_file, rel_test_path)\n    test_contents = 'bar\\n'\n    with open(rel_test_path, 'w') as f:\n        f.write(test_contents)\n    (path, contents) = cli.read_file(rel_test_path)\n    self.assertEqual(path, os.path.abspath(path))\n    self.assertEqual(contents, test_contents)\n", "label": 0}
{"function": "\n\ndef cleanup(self):\n    'Destroys benchmark environment.'\n    ctxlst = (self._visited or self._get_sorted_context_lst())\n    for ctx in ctxlst[::(- 1)]:\n        try:\n            ctx.cleanup()\n        except Exception as e:\n            LOG.error(('Context %s failed during cleanup.' % ctx.get_name()))\n            LOG.exception(e)\n", "label": 0}
{"function": "\n\n@register.simple_tag(takes_context=True)\ndef render_paginator(context, page, page_var='page', hashtag=''):\n    query_dict = context['request'].GET.copy()\n    try:\n        del query_dict[page_var]\n    except KeyError:\n        pass\n    extra_query = ''\n    if query_dict:\n        extra_query = ('&%s' % query_dict.urlencode())\n    if hashtag:\n        hashtag = ('#%s' % hashtag)\n    new_context = {\n        'page': page,\n        'page_var': page_var,\n        'hashtag': hashtag,\n        'extra_query': extra_query,\n    }\n    if isinstance(page, Page):\n        template = 'spirit/utils/paginator/_paginator.html'\n    else:\n        template = 'spirit/utils/paginator/_yt_paginator.html'\n    return render_to_string(template, new_context)\n", "label": 0}
{"function": "\n\ndef test_option():\n    kernel = get_kernel()\n    d = Dummy(kernel)\n    assert ('Options:' in d.line_dummy.__doc__)\n    assert ('--size' in d.line_dummy.__doc__)\n    ret = d.call_magic('line', 'dummy', '', 'hey -s400,200')\n    assert (ret == d)\n    assert (d.foo == 'hey'), d.foo\n    assert (d.size == (400, 200))\n    ret = d.call_magic('line', 'dummy', '', 'hey there')\n    assert (d.foo == 'hey there')\n    ret = d.call_magic('line', 'dummy', '', 'range(1, 10)')\n    assert (d.foo == range(1, 10))\n    ret = d.call_magic('line', 'dummy', '', '[1, 2, 3]')\n    assert (d.foo == [1, 2, 3])\n    ret = d.call_magic('line', 'dummy', '', 'hey -l -s400,200')\n    assert (d.size == (400, 200))\n    assert (d.foo == 'hey -l')\n    ret = d.call_magic('line', 'dummy', '', 'hey -s -- -s400,200')\n    assert (d.size == (400, 200))\n    assert (d.foo == 'hey -s')\n", "label": 1}
{"function": "\n\ndef slugize(slug):\n    'Convert a string to a string for an URL.\\n    '\n    assert (name_pattern.match(slug) is not None)\n    slug = slug.lower()\n    for c in (' ', ',', '.', '_'):\n        slug = slug.replace(c, '-')\n    while ('--' in slug):\n        slug = slug.replace('--', '-')\n    slug = slug.strip('-')\n    return slug\n", "label": 0}
{"function": "\n\ndef test_connect_checkout_handler_always_gets_info(self):\n    'test [ticket:3497]'\n    (dbapi, p) = self._queuepool_dbapi_fixture(pool_size=2, max_overflow=2)\n    c1 = p.connect()\n    c2 = p.connect()\n    c1.close()\n    c2.close()\n    dbapi.shutdown(True)\n    bad = p.connect()\n    p._invalidate(bad)\n    bad.close()\n    assert p._invalidate_time\n\n    @event.listens_for(p, 'connect')\n    def connect(conn, conn_rec):\n        conn_rec.info['x'] = True\n\n    @event.listens_for(p, 'checkout')\n    def checkout(conn, conn_rec, conn_f):\n        assert ('x' in conn_rec.info)\n    assert_raises(Exception, p.connect)\n    p._pool.queue = collections.deque([c for c in p._pool.queue if (c.connection is not None)])\n    dbapi.shutdown(False)\n    c = p.connect()\n    c.close()\n", "label": 0}
{"function": "\n\ndef encodeFiles(array):\n    IDfile = readIDfile()\n    dictionary = readDictionaryFile()\n    for thisfile in array:\n        try:\n            input = pickle.load(open(thisfile))\n        except ValueError:\n            logging.warn(((('unable to unpicked ' + thisfile) + '... dangerously just ') + 'skipping, some texts may be lost'))\n            continue\n        except:\n            logging.warn((('Some problem: fix if ' + thisfile) + ' should be unpicklable'))\n            continue\n        for level in input.levels:\n            input.encode(level, IDfile, dictionary)\n", "label": 0}
{"function": "\n\ndef LoadServerCertificate(self, server_certificate=None, ca_certificate=None):\n    'Loads and verifies the server certificate.'\n    try:\n        server_cert = X509.load_cert_string(str(server_certificate))\n        ca_cert = X509.load_cert_string(str(ca_certificate))\n        if (server_cert.verify(ca_cert.get_pubkey()) != 1):\n            self.server_name = None\n            raise IOError('Server cert is invalid.')\n        server_cert_serial = server_cert.get_serial_number()\n        if (server_cert_serial < config_lib.CONFIG['Client.server_serial_number']):\n            raise IOError('Server cert is too old.')\n        elif (server_cert_serial > config_lib.CONFIG['Client.server_serial_number']):\n            logging.info('Server serial number updated to %s', server_cert_serial)\n            config_lib.CONFIG.Set('Client.server_serial_number', server_cert_serial)\n            config_lib.CONFIG.Write()\n    except X509.X509Error:\n        raise IOError('Server cert is invalid.')\n    self.server_name = self.pub_key_cache.GetCNFromCert(server_cert)\n    self.server_certificate = server_certificate\n    self.ca_certificate = ca_certificate\n    self.pub_key_cache.Put(self.server_name, self.pub_key_cache.PubKeyFromCert(server_cert))\n", "label": 0}
{"function": "\n\ndef print_ctypes_struct(struct, name='', ident=0, hexa=False):\n    if isinstance(struct, _ctypes._Pointer):\n        if (ctypes.cast(struct, ctypes.c_void_p).value is None):\n            print('{0} -> NULL'.format(name))\n            return\n        return print_ctypes_struct(struct[0], (name + '<deref>'), hexa=hexa)\n    if (not hasattr(struct, '_fields_')):\n        value = struct\n        if hasattr(struct, 'value'):\n            value = struct.value\n        if isinstance(value, basestring):\n            value = repr(value)\n        if hexa:\n            try:\n                print('{0} -> {1}'.format(name, hex(value)))\n                return\n            except TypeError:\n                pass\n        print('{0} -> {1}'.format(name, value))\n        return\n    for (fname, ftype) in struct._fields_:\n        value = getattr(struct, fname)\n        print_ctypes_struct(value, '{0}.{1}'.format(name, fname), hexa=hexa)\n", "label": 0}
{"function": "\n\ndef test_column_optimizations_with_bcolz_and_rewrite():\n    try:\n        import bcolz\n    except ImportError:\n        return\n    bc = bcolz.ctable([[1, 2, 3], [10, 20, 30]], names=['a', 'b'])\n    func = (lambda x: x)\n    for cols in [None, 'abc', ['abc']]:\n        dsk2 = merge(dict(((('x', i), (dataframe_from_ctable, bc, slice(0, 2), cols, {\n            \n        })) for i in [1, 2, 3])), dict(((('y', i), (getitem, ('x', i), (list, ['a', 'b']))) for i in [1, 2, 3])))\n        expected = dict(((('y', i), (dataframe_from_ctable, bc, slice(0, 2), (list, ['a', 'b']), {\n            \n        })) for i in [1, 2, 3]))\n        result = dd.optimize(dsk2, [('y', i) for i in [1, 2, 3]])\n        assert (result == expected)\n", "label": 0}
{"function": "\n\ndef check_scripts(debug, scripts):\n    with s3gis_tests.Change(current.session.s3, {\n        'debug': debug,\n    }):\n        with s3gis_tests.InsertedRecord(db, db.gis_layer_google, google_layer):\n            with s3gis_tests.AddedRole(session, session.s3.system_roles.MAP_ADMIN):\n                actual_output = str(s3base.GIS().show_map(window=True, catalogue_toolbar=True, toolbar=True, search=True, catalogue_layers=True, projection=900913))\n                s3gis_tests.check_scripts(actual_output, scripts, request)\n", "label": 0}
{"function": "\n\n@celery.task\ndef send_async_email(msg):\n    app = create_app('default')\n    with app.app_context():\n        mail.send(msg)\n", "label": 0}
{"function": "\n\n@pytest.mark.browser\ndef test_can_be_added_with_new_image_and_link(admin_user, live_server, browser):\n    fancypage = factories.FancyPageFactory()\n    im = Image.new('RGB', (320, 240), 'red')\n    (__, filename) = tempfile.mkstemp(suffix='.jpg', dir=TEMP_MEDIA_ROOT)\n    im.save(filename, 'JPEG')\n    image_filename = os.path.basename(filename)\n    ImageAsset.objects.create(name='test image', image=image_filename, creator=admin_user)\n    second_page = factories.FancyPageFactory(node__name='Another page', node__slug='another-page')\n    browser.visit((live_server.url + fancypage.get_absolute_url()))\n    find_and_click_by_css(browser, '#editor-handle')\n    find_and_click_by_css(browser, 'div[class=block-add-control]>a')\n    find_and_click_by_css(browser, \"a[href='#content']\")\n    find_and_click_by_css(browser, 'button[data-block-code=image]')\n    wait_for_editor_reload()\n    default_image_text = 'Add An Image'\n    if (not browser.is_text_present(default_image_text, 2)):\n        raise AssertionError('Could not find image block on page')\n    if (not browser.is_element_present_by_css('.edit-button', 2)):\n        raise AssertionError('Could not find edit button for block')\n    find_and_click_by_css(browser, '.edit-button')\n    wait_for_editor_reload()\n    find_and_click_by_css(browser, 'a[data-behaviours=load-asset-modal]')\n    wait_for_editor_reload()\n    with browser.get_iframe(0) as iframe:\n        wait_for_editor_reload()\n        find_and_click_by_css(iframe, 'li[data-behaviours=selectable-asset]')\n    wait_for_editor_reload()\n    find_and_click_by_css(browser, '.glyphicon-share')\n    find_and_click_by_css(browser, 'a[href*={}]'.format(second_page.slug))\n    find_and_click_by_css(browser, 'button[type=submit]')\n    wait_for_editor_reload()\n    shows_image = browser.is_element_present_by_css(\"img[src$='{}']\".format(image_filename))\n    if (not shows_image):\n        raise AssertionError('Image not added to image block')\n    link_exists = browser.is_element_present_by_css('div.block-wrapper>a[href*={}]'.format(second_page.slug))\n    if (not link_exists):\n        raise AssertionError('image block is not wrapped in link to other page')\n", "label": 0}
{"function": "\n\n@register_canonicalize\n@register_specialize\n@gof.local_optimizer([Subtensor])\ndef local_subtensor_of_alloc(node):\n    'alloc[x:y] -> alloc'\n    if (not isinstance(node.op, Subtensor)):\n        return False\n    u = node.inputs[0]\n    if (u.owner is None):\n        return False\n    if (not isinstance(u.owner.op, T.Alloc)):\n        return False\n    slices = get_idx_list(node.inputs, node.op.idx_list)\n    val = u.owner.inputs[0]\n    dims = u.owner.inputs[1:]\n    assert (len(slices) <= len(dims))\n    n_added_dims = (u.ndim - val.ndim)\n    nw_dims = []\n    val_slices = []\n    for (i, (sl, dim)) in enumerate(zip(slices, dims)):\n        if (i >= n_added_dims):\n            if ((val.type.ndim > (i - n_added_dims)) and val.type.broadcastable[(i - n_added_dims)]):\n                val_slices.append(slice(None))\n            else:\n                val_slices.append(sl)\n        (csl, _) = get_canonical_form_slice(sl, dim)\n        if (type(csl) is not slice):\n            pass\n        else:\n            nw_dim = (csl.stop - csl.start)\n            if (csl.step != 1):\n                nw_dim = T.ceil_intdiv(nw_dim, csl.step)\n            nw_dims += [nw_dim]\n    nw_val = val[tuple(val_slices)]\n    nw_dims += dims[len(slices):]\n    if (nw_val.ndim > len(nw_dims)):\n        return False\n    rval = T.alloc(nw_val, *nw_dims)\n    if (type(rval) not in (list, tuple)):\n        rval = [rval]\n    if (rval[0].type != node.outputs[0].type):\n        rval[0] = theano.tensor.unbroadcast(rval[0], *[i for (i, (b1, b2)) in enumerate(zip(rval[0].broadcastable, node.outputs[0].broadcastable)) if (b1 and (not b2))])\n    return rval\n", "label": 1}
{"function": "\n\ndef _query_metadata_proxy(self, machine):\n    url = ('http://%(host)s:%(port)s' % {\n        'host': dhcp.METADATA_DEFAULT_IP,\n        'port': dhcp.METADATA_PORT,\n    })\n    cmd = ('curl', '--max-time', METADATA_REQUEST_TIMEOUT, '-D-', url)\n    i = 0\n    CONNECTION_REFUSED_TIMEOUT = (METADATA_REQUEST_TIMEOUT // 2)\n    while (i <= CONNECTION_REFUSED_TIMEOUT):\n        try:\n            raw_headers = machine.execute(cmd)\n            break\n        except RuntimeError as e:\n            if ('Connection refused' in str(e)):\n                time.sleep(METADATA_REQUEST_SLEEP)\n                i += METADATA_REQUEST_SLEEP\n            else:\n                self.fail(('metadata proxy unreachable on %s before timeout' % url))\n    if (i > CONNECTION_REFUSED_TIMEOUT):\n        self.fail('Timed out waiting metadata proxy to become available')\n    return raw_headers.splitlines()[0]\n", "label": 0}
{"function": "\n\ndef update(self, value, timestamp=None):\n    if (timestamp is None):\n        timestamp = now()\n    self.rescale_if_necessary()\n    with self.lock:\n        try:\n            priority = (self.weight(timestamp) / random.random())\n        except (OverflowError, ZeroDivisionError):\n            priority = sys.float_info.max\n        if (len(self.values) < self.reservoir_size):\n            heapq.heappush(self.values, (priority, value))\n        else:\n            heapq.heappushpop(self.values, (priority, value))\n", "label": 0}
{"function": "\n\ndef get(self, resource):\n    '\\n        Get a resource into the cache,\\n\\n        :param resource: A :class:`Resource` instance.\\n        :return: The pathname of the resource in the cache.\\n        '\n    (prefix, path) = resource.finder.get_cache_info(resource)\n    if (prefix is None):\n        result = path\n    else:\n        result = os.path.join(self.base, self.prefix_to_dir(prefix), path)\n        dirname = os.path.dirname(result)\n        if (not os.path.isdir(dirname)):\n            os.makedirs(dirname)\n        if (not os.path.exists(result)):\n            stale = True\n        else:\n            stale = self.is_stale(resource, path)\n        if stale:\n            with open(result, 'wb') as f:\n                f.write(resource.bytes)\n    return result\n", "label": 0}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_freq():\n    ' reading/writing of 3D RNMRTK frequency domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_3d', 'freq_3d.sec'))\n    assert (data.shape == (128, 128, 4096))\n    assert (np.abs((data[(0, 1, 2)] - 3.23)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)] - 1.16)) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef _digest(self, alg, password, salt=None):\n    \"\\n        Helper method to perform the password digest.\\n\\n        :param alg: The hash algorithm to use.\\n        :type alg: str - 'sha512' | 'bcrypt'\\n        :param password: The password to digest.\\n        :type password: str\\n        :param salt: The salt to use. In the case of bcrypt,\\n                     when storing the password, pass None;\\n                     when testing the password, pass the hashed value.\\n        :type salt: None or str\\n        :returns: The hashed value as a string.\\n        \"\n    cur_config = config.getConfig()\n    if (alg == 'sha512'):\n        return hashlib.sha512((password + salt).encode('utf8')).hexdigest()\n    elif (alg == 'bcrypt'):\n        try:\n            import bcrypt\n        except ImportError:\n            raise Exception('Bcrypt module is not installed. See girder.local.cfg.')\n        password = password.encode('utf8')\n        if (salt is None):\n            rounds = int(cur_config['auth']['bcrypt_rounds'])\n            return bcrypt.hashpw(password, bcrypt.gensalt(rounds))\n        else:\n            if isinstance(salt, six.text_type):\n                salt = salt.encode('utf8')\n            return bcrypt.hashpw(password, salt)\n    else:\n        raise Exception(('Unsupported hash algorithm: %s' % alg))\n", "label": 0}
{"function": "\n\ndef get_image_parent(image_id):\n    if (image_id in images_cache):\n        return images_cache[image_id]\n    image_json = store.image_json_path(image_id)\n    parent_id = None\n    try:\n        info = store.get_json(image_json)\n        if (info['id'] != image_id):\n            warning(('image_id != json image_id for image_id: ' + image_id))\n        parent_id = info.get('parent')\n    except exceptions.FileNotFoundError:\n        warning('graph is broken for image_id: {0}'.format(image_id))\n    images_cache[image_id] = parent_id\n    return parent_id\n", "label": 0}
{"function": "\n\ndef read_data(self, offset, length):\n    end = (offset + length)\n    eof = self['size']\n    if (end > eof):\n        end = eof\n        length = (end - offset)\n        if (length <= 0):\n            return ''\n    first_block = (offset / self.blocksize)\n    last_block = (end / self.blocksize)\n    output = StringIO()\n    for n_block in range(first_block, (last_block + 1)):\n        block_offset = (n_block * self.blocksize)\n        fragment_offset = 0\n        if (n_block == first_block):\n            fragment_offset = (offset - block_offset)\n        fragment_end = self.blocksize\n        if (n_block == last_block):\n            fragment_end = (end - block_offset)\n        block_data = self.read_block(n_block)\n        fragment = block_data[fragment_offset:fragment_end]\n        assert (len(fragment) == (fragment_end - fragment_offset))\n        output.write(fragment)\n    output = output.getvalue()\n    assert (len(output) == length)\n    return output\n", "label": 0}
{"function": "\n\n@classmethod\ndef create_user(cls, name, email, password, email_verified=True):\n    'Create (and save) a new user with the given password and\\n        email address.\\n        '\n    now = datetime.datetime.utcnow()\n    try:\n        (email_name, domain_part) = email.strip().split('@', 1)\n    except ValueError:\n        pass\n    else:\n        email = '@'.join([email_name.lower(), domain_part.lower()])\n    user = User(name=name, email=email, date_joined=now)\n    if (not password):\n        password = generate_password()\n    user.set_password(password)\n    if (not email_verified):\n        user.mark_email_for_activation()\n    else:\n        user.is_email_activated = True\n    user.save()\n    return user\n", "label": 0}
{"function": "\n\ndef test_set_reuse_addr(self):\n    if (HAS_UNIX_SOCKETS and (self.family == socket.AF_UNIX)):\n        self.skipTest('Not applicable to AF_UNIX sockets.')\n    sock = socket.socket(self.family)\n    try:\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    except socket.error:\n        unittest.skip('SO_REUSEADDR not supported on this platform')\n    else:\n        s = asyncore.dispatcher(socket.socket(self.family))\n        self.assertFalse(s.socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR))\n        s.socket.close()\n        s.create_socket(self.family)\n        s.set_reuse_addr()\n        self.assertTrue(s.socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR))\n    finally:\n        sock.close()\n", "label": 0}
{"function": "\n\ndef test_login_logout_save():\n    (auth, app, user) = _get_flask_app()\n    auth.session = Session()\n    client = app.test_client()\n    auth.session.saved = 0\n    resp = client.get('/protected/')\n    print(resp.data)\n    assert (auth.session_key not in auth.session)\n    assert (auth.redirect_key in auth.session)\n    assert (auth.session.saved == 1)\n    data = {\n        'login': user.login,\n        'password': 'foobar',\n        '_csrf_token': auth.get_csrf_token(),\n    }\n    client.post(auth.url_sign_in, data=data)\n    assert (auth.session_key in auth.session)\n    assert (auth.session.saved == 4)\n    data = {\n        '_csrf_token': auth.get_csrf_token(),\n    }\n    client.post(auth.url_sign_out, data=data)\n    assert (auth.session_key not in auth.session)\n    assert (auth.session.saved == 5)\n", "label": 0}
{"function": "\n\ndef AllowPort(self, vm, port):\n    'Opens a port on the firewall.\\n\\n    Args:\\n      vm: The BaseVirtualMachine object to open the port for.\\n      port: The local port to open.\\n    '\n    if vm.is_static:\n        return\n    entry = (port, vm.group_id)\n    if (entry in self.firewall_set):\n        return\n    with self._lock:\n        if (entry in self.firewall_set):\n            return\n        authorize_cmd = (util.AWS_PREFIX + ['ec2', 'authorize-security-group-ingress', ('--region=%s' % vm.region), ('--group-id=%s' % vm.group_id), ('--port=%s' % port), '--cidr=0.0.0.0/0'])\n        util.IssueRetryableCommand((authorize_cmd + ['--protocol=tcp']))\n        util.IssueRetryableCommand((authorize_cmd + ['--protocol=udp']))\n        self.firewall_set.add(entry)\n", "label": 0}
{"function": "\n\ndef test_response_change(self):\n    '\\n        Test the response_change method that it works with a _change_history\\n        in the POST and settings.SIMPLE_HISTORY_EDIT set to True\\n        '\n    request = RequestFactory().post('/')\n    request.POST = {\n        '_change_history': True,\n    }\n    request.session = 'session'\n    request._messages = FallbackStorage(request)\n    request.path = '/awesome/url/'\n    poll = Poll.objects.create(question='why?', pub_date=today)\n    poll.question = 'how?'\n    poll.save()\n    admin_site = AdminSite()\n    admin = SimpleHistoryAdmin(Poll, admin_site)\n    with patch('simple_history.admin.SIMPLE_HISTORY_EDIT', True):\n        response = admin.response_change(request, poll)\n    self.assertEqual(response['Location'], '/awesome/url/')\n", "label": 0}
{"function": "\n\ndef testSessionAlreadyClosed(self):\n    with tdrest.connect(host=host, system=system, username=self.username, password=self.password, autoCommit=True) as conn:\n        self.assertIsNotNone(conn)\n        with conn.template.connect() as http:\n            http.delete('/systems/{}/sessions/{}'.format(conn.system, conn.sessionId))\n", "label": 0}
{"function": "\n\ndef _ensure_path(self):\n    result = self.client.ensure_path(self.path)\n    self.assured_path = True\n    if (result is True):\n        (data, _) = self.client.get(self.path)\n        try:\n            leases = int(data.decode('utf-8'))\n        except (ValueError, TypeError):\n            pass\n        else:\n            if (leases != self.max_leases):\n                raise ValueError(('Inconsistent max leases: %s, expected: %s' % (leases, self.max_leases)))\n    else:\n        self.client.set(self.path, str(self.max_leases).encode('utf-8'))\n", "label": 0}
{"function": "\n\ndef test_size(self):\n    self.multi_map.put('key', 'value-1')\n    self.multi_map.put('key', 'value-2')\n    self.multi_map.put('key', 'value-3')\n    with self.client.new_transaction() as tx:\n        tx_multi_map = tx.get_multi_map(self.multi_map.name)\n        self.assertTrue(tx_multi_map.size(), 3)\n", "label": 0}
{"function": "\n\ndef test_eval_size_zero(self, TrainSplit, nn):\n    (X, y) = (np.random.random((100, 10)), np.repeat([0, 1, 2, 3], 25))\n    (X_train, X_valid, y_train, y_valid) = TrainSplit(0.0)(X, y, nn)\n    assert (len(X_train) == len(X))\n    assert (len(y_train) == len(y))\n    assert (len(X_valid) == 0)\n    assert (len(y_valid) == 0)\n", "label": 0}
{"function": "\n\ndef test_mismatched_dtype(self):\n    msa = TabularMSA([DNA('ACGT'), DNA('TGCA')])\n    with self.assertRaisesRegex(TypeError, 'matching type.*RNA.*DNA'):\n        msa.extend([DNA('----'), RNA('UUUU')])\n    self.assertEqual(msa, TabularMSA([DNA('ACGT'), DNA('TGCA')]))\n", "label": 0}
{"function": "\n\ndef to_python(self, value):\n    try:\n        if (value is not None):\n            return json.loads(value)\n    except TypeError:\n        raise ValidationError(_('String type is required.'))\n    except ValueError:\n        raise ValidationError(_('Enter a valid value.'))\n", "label": 0}
{"function": "\n\ndef test_max(self):\n    validator = RangeMaxValueValidator(5)\n    validator(NumericRange(0, 5))\n    with self.assertRaises(exceptions.ValidationError) as cm:\n        validator(NumericRange(0, 10))\n    self.assertEqual(cm.exception.messages[0], 'Ensure that this range is completely less than or equal to 5.')\n    self.assertEqual(cm.exception.code, 'max_value')\n", "label": 0}
{"function": "\n\ndef compile_template(func):\n    spec = inspect.getargspec(func)\n    assert (len(spec.args) == len((spec.defaults or []))), 'All template args should have AST classes'\n    compiler = TemplateCompiler(zipdict(spec.args, (spec.defaults or [])))\n    template = map(compiler.visit, get_body_ast(func))\n    if ((len(template) == 1) and isinstance(template[0], ast.Expr)):\n        return template[0].value\n    return template\n", "label": 0}
{"function": "\n\ndef match_or_trust(self, host, der_encoded_certificate):\n    base64_encoded_certificate = b64encode(der_encoded_certificate)\n    if isfile(self.path):\n        with open(self.path) as f_in:\n            for line in f_in:\n                (known_host, _, known_cert) = line.strip().partition(':')\n                known_cert = known_cert.encode('utf-8')\n                if (host == known_host):\n                    return (base64_encoded_certificate == known_cert)\n    try:\n        makedirs(dirname(self.path))\n    except OSError:\n        pass\n    f_out = os_open(self.path, ((O_CREAT | O_APPEND) | O_WRONLY), 384)\n    if isinstance(host, bytes):\n        os_write(f_out, host)\n    else:\n        os_write(f_out, host.encode('utf-8'))\n    os_write(f_out, b':')\n    os_write(f_out, base64_encoded_certificate)\n    os_write(f_out, b'\\n')\n    os_close(f_out)\n    return True\n", "label": 0}
{"function": "\n\ndef remove(target, identifier, fn):\n    'Remove an event listener.\\n\\n    The arguments here should match exactly those which were sent to\\n    :func:`.listen`; all the event registration which proceeded as a result\\n    of this call will be reverted by calling :func:`.remove` with the same\\n    arguments.\\n\\n    e.g.::\\n\\n        # if a function was registered like this...\\n        @event.listens_for(SomeMappedClass, \"before_insert\", propagate=True)\\n        def my_listener_function(*arg):\\n            pass\\n\\n        # ... it\\'s removed like this\\n        event.remove(SomeMappedClass, \"before_insert\", my_listener_function)\\n\\n    Above, the listener function associated with ``SomeMappedClass`` was also\\n    propagated to subclasses of ``SomeMappedClass``; the :func:`.remove`\\n    function will revert all of these operations.\\n\\n    .. versionadded:: 0.9.0\\n\\n    '\n    _event_key(target, identifier, fn).remove()\n", "label": 0}
{"function": "\n\ndef get_fqhostname():\n    '\\n    Returns the fully qualified hostname\\n    '\n    l = []\n    l.append(socket.getfqdn())\n    try:\n        addrinfo = socket.getaddrinfo(socket.gethostname(), 0, socket.AF_UNSPEC, socket.SOCK_STREAM, socket.SOL_TCP, socket.AI_CANONNAME)\n        for info in addrinfo:\n            if (len(info) >= 4):\n                l.append(info[3])\n    except socket.gaierror:\n        pass\n    l = _sort_hostnames(l)\n    if (len(l) > 0):\n        return l[0]\n    return None\n", "label": 0}
{"function": "\n\ndef _plot_html(figure_or_data, show_link, link_text, validate, default_width, default_height, global_requirejs):\n    figure = tools.return_figure_from_figure_or_data(figure_or_data, validate)\n    width = figure.get('layout', {\n        \n    }).get('width', default_width)\n    height = figure.get('layout', {\n        \n    }).get('height', default_height)\n    try:\n        float(width)\n    except (ValueError, TypeError):\n        pass\n    else:\n        width = (str(width) + 'px')\n    try:\n        float(width)\n    except (ValueError, TypeError):\n        pass\n    else:\n        width = (str(width) + 'px')\n    plotdivid = uuid.uuid4()\n    jdata = json.dumps(figure.get('data', []), cls=utils.PlotlyJSONEncoder)\n    jlayout = json.dumps(figure.get('layout', {\n        \n    }), cls=utils.PlotlyJSONEncoder)\n    config = {\n        \n    }\n    config['showLink'] = show_link\n    config['linkText'] = link_text\n    jconfig = json.dumps(config)\n    plotly_platform_url = plotly.plotly.get_config().get('plotly_domain', 'https://plot.ly')\n    if ((plotly_platform_url != 'https://plot.ly') and (link_text == 'Export to plot.ly')):\n        link_domain = plotly_platform_url.replace('https://', '').replace('http://', '')\n        link_text = link_text.replace('plot.ly', link_domain)\n    script = 'Plotly.newPlot(\"{id}\", {data}, {layout}, {config})'.format(id=plotdivid, data=jdata, layout=jlayout, config=jconfig)\n    optional_line1 = ('require([\"plotly\"], function(Plotly) {{ ' if global_requirejs else '')\n    optional_line2 = ('}});' if global_requirejs else '')\n    plotly_html_div = (((((('<div id=\"{id}\" style=\"height: {height}; width: {width};\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">' + optional_line1) + 'window.PLOTLYENV=window.PLOTLYENV || {{}};window.PLOTLYENV.BASE_URL=\"') + plotly_platform_url) + '\";{script}') + optional_line2) + '</script>').format(id=plotdivid, script=script, height=height, width=width)\n    return (plotly_html_div, plotdivid, width, height)\n", "label": 0}
{"function": "\n\ndef _build_doc(self):\n    '\\n        Raises\\n        ------\\n        ValueError\\n            * If a URL that lxml cannot parse is passed.\\n\\n        Exception\\n            * Any other ``Exception`` thrown. For example, trying to parse a\\n              URL that is syntactically correct on a machine with no internet\\n              connection will fail.\\n\\n        See Also\\n        --------\\n        pandas.io.html._HtmlFrameParser._build_doc\\n        '\n    from lxml.html import parse, fromstring, HTMLParser\n    from lxml.etree import XMLSyntaxError\n    parser = HTMLParser(recover=False, encoding=self.encoding)\n    try:\n        r = parse(self.io, parser=parser)\n        try:\n            r = r.getroot()\n        except AttributeError:\n            pass\n    except (UnicodeDecodeError, IOError):\n        if (not _is_url(self.io)):\n            r = fromstring(self.io, parser=parser)\n            try:\n                r = r.getroot()\n            except AttributeError:\n                pass\n        else:\n            scheme = parse_url(self.io).scheme\n            if (scheme not in _valid_schemes):\n                msg = ('%r is not a valid url scheme, valid schemes are %s' % (scheme, _valid_schemes))\n                raise ValueError(msg)\n            else:\n                raise\n    else:\n        if (not hasattr(r, 'text_content')):\n            raise XMLSyntaxError('no text parsed from document', 0, 0, 0)\n    return r\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if (not self.pk):\n        try:\n            self.position = (self.siblings(include_self=True).only('position').order_by('-position')[0].position + 1)\n        except IndexError:\n            if self.parent:\n                self.position = (self.parent.position + 1)\n            else:\n                try:\n                    self.position = (Forum.objects.only('position').order_by('-position')[0].position + 1)\n                except IndexError:\n                    self.position = 1\n        if (self.position != 1):\n            Forum.objects.update_position(self.position)\n        if self.parent:\n            self.level = (self.parent.level + 1)\n    super(Forum, self).save(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef test_update_to_none(self):\n    t = TestConditionalModel.create(text='something', count=5)\n    self.assertEqual(TestConditionalModel.objects(id=t.id).count(), 1)\n    with self.assertRaises(LWTException):\n        t.iff(count=9999).update(text=None)\n    self.assertIsNotNone(TestConditionalModel.objects(id=t.id).first().text)\n    t.iff(count=5).update(text=None)\n    self.assertIsNone(TestConditionalModel.objects(id=t.id).first().text)\n    t = TestConditionalModel.create(text='something', count=5)\n    self.assertEqual(TestConditionalModel.objects(id=t.id).count(), 1)\n    with self.assertRaises(LWTException):\n        TestConditionalModel.objects(id=t.id).iff(count=9999).update(text=None)\n    self.assertIsNotNone(TestConditionalModel.objects(id=t.id).first().text)\n    TestConditionalModel.objects(id=t.id).iff(count=5).update(text=None)\n    self.assertIsNone(TestConditionalModel.objects(id=t.id).first().text)\n", "label": 0}
{"function": "\n\ndef fsync_dir(dirpath):\n    '\\n    Sync directory entries to disk.\\n\\n    :param dirpath: Path to the directory to be synced.\\n    '\n    dirfd = None\n    try:\n        dirfd = os.open(dirpath, (os.O_DIRECTORY | os.O_RDONLY))\n        fsync(dirfd)\n    except OSError as err:\n        if (err.errno == errno.ENOTDIR):\n            raise\n        logging.warning(_('Unable to perform fsync() on directory %(dir)s: %(err)s'), {\n            'dir': dirpath,\n            'err': os.strerror(err.errno),\n        })\n    finally:\n        if dirfd:\n            os.close(dirfd)\n", "label": 0}
{"function": "\n\ndef test_model_time_res_uniform(self):\n    override = '\\n            time:\\n                resolution: 24\\n        '\n    model = create_and_run_model(override)\n    assert (len(model.data.time_res_series) == 4)\n    assert (str(model.results.solver.termination_condition) == 'optimal')\n    sol = model.solution\n    assert (sol['e'].loc[dict(c='power', y='ccgt')].sum(dim=['x', 't']) == 1320)\n", "label": 0}
{"function": "\n\ndef download_set(set_id, get_filename, size_label=None):\n    \"\\n    Download the set with 'set_id' to the current directory.\\n\\n    @param set_id: str, id of the photo set\\n    @param get_filename: Function, function that creates a filename for the photo\\n    @param size_label: str|None, size to download (or None for largest available)\\n    \"\n    suffix = (' ({})'.format(size_label) if size_label else '')\n    pset = Flickr.Photoset(id=set_id)\n    photos = pset.getPhotos()\n    pagenum = 2\n    while True:\n        try:\n            page = pset.getPhotos(page=pagenum)\n            photos.extend(page)\n            pagenum += 1\n        except FlickrAPIError as ex:\n            if (ex.code == 1):\n                break\n            raise\n    dirname = pset.title.replace(os.sep, '_')\n    if (not os.path.exists(dirname)):\n        os.mkdir(dirname)\n    for photo in photos:\n        fname = get_full_path(dirname, get_filename(pset, photo, suffix))\n        if os.path.exists(fname):\n            print('Skipping {0}, as it exists already'.format(fname))\n            continue\n        print('Saving: {0}'.format(fname))\n        photo.save(fname, size_label)\n        info = photo.getInfo()\n        taken = parser.parse(info['taken'])\n        taken_unix = time.mktime(taken.timetuple())\n        os.utime(fname, (taken_unix, taken_unix))\n", "label": 0}
{"function": "\n\ndef refreshTextboxes(self, reset=False):\n    ' Update the information of the textboxes that give information about the measurements\\n        '\n    self.aortaTextBox.setText('0')\n    self.paTextBox.setText('0')\n    self.ratioTextBox.setText('0')\n    self.ratioTextBox.setStyleSheet(' QLineEdit { background-color: white; color: black}')\n    volumeId = self.volumeSelector.currentNodeId\n    if (volumeId not in self.logic.currentVolumesLoaded):\n        return\n    if volumeId:\n        self.logic.changeColor(volumeId, self.logic.defaultColor)\n    aorta = None\n    pa = None\n    if (not reset):\n        (rulerAorta, newAorta) = self.logic.getRulerNodeForVolumeAndStructure(self.volumeSelector.currentNodeId, self.logic.AORTA, createIfNotExist=False)\n        (rulerPA, newPA) = self.logic.getRulerNodeForVolumeAndStructure(self.volumeSelector.currentNodeId, self.logic.PA, createIfNotExist=False)\n        if rulerAorta:\n            aorta = rulerAorta.GetDistanceMeasurement()\n            self.aortaTextBox.setText(str(aorta))\n        if rulerPA:\n            pa = rulerPA.GetDistanceMeasurement()\n            self.paTextBox.setText(str(pa))\n        if ((aorta is not None) and (aorta != 0)):\n            try:\n                ratio = (pa / aorta)\n                self.ratioTextBox.setText(str(ratio))\n                if (ratio > 1):\n                    self.ratioTextBox.setStyleSheet(' QLineEdit { background-color: rgb(255, 0, 0); color: white}')\n                    self.logic.changeColor(volumeId, self.logic.defaultWarningColor)\n            except Exception:\n                Util.print_last_exception()\n", "label": 0}
{"function": "\n\ndef create_readonly_file(self, filename):\n    file_path = os.path.join(self.temp_dir, filename)\n    with open(file_path, 'w') as file:\n        file.write(filename)\n    os.chmod(file_path, stat.S_IREAD)\n", "label": 0}
{"function": "\n\ndef check_pid(pidfile):\n    if (pidfile and os.path.exists(pidfile)):\n        try:\n            pid = int(open(pidfile).read().strip())\n            os.kill(pid, 0)\n            return pid\n        except BaseException:\n            return 0\n    return 0\n", "label": 0}
{"function": "\n\ndef test_launch_vpn_instance(self):\n    self.stubs.Set(self.cloudpipe.compute_api, 'create', (lambda *a, **kw: (None, 'r-fakeres')))\n    with utils.tempdir() as tmpdir:\n        self.flags(ca_path=tmpdir, keys_path=tmpdir)\n        crypto.ensure_ca_filesystem()\n        self.cloudpipe.launch_vpn_instance(self.context)\n", "label": 0}
{"function": "\n\n@mock.patch('ironic.drivers.modules.ucs.helper.ucs_helper', spec_set=True, autospec=True)\n@mock.patch('ironic.drivers.modules.ucs.management.ucs_mgmt.BootDeviceHelper', spec_set=True, autospec=True)\ndef test_get_boot_device(self, mock_ucs_mgmt, mock_helper):\n    mock_helper.generate_ucsm_handle.return_value = (True, mock.Mock())\n    mock_mgmt = mock_ucs_mgmt.return_value\n    mock_mgmt.get_boot_device.return_value = {\n        'boot_device': 'disk',\n        'persistent': False,\n    }\n    with task_manager.acquire(self.context, self.node.uuid, shared=False) as task:\n        expected_device = boot_devices.DISK\n        expected_response = {\n            'boot_device': expected_device,\n            'persistent': False,\n        }\n        self.assertEqual(expected_response, self.interface.get_boot_device(task))\n    mock_mgmt.get_boot_device.assert_called_once_with()\n", "label": 0}
{"function": "\n\n@permission_required('core.manage_shop')\ndef send_rating_mails(request):\n    'Send rating mails for given orders.\\n    '\n    if (request.method == 'POST'):\n        ctype = ContentType.objects.get_for_model(Product)\n        site = ('http://%s' % Site.objects.get(id=settings.SITE_ID))\n        shop = lfs.core.utils.get_default_shop()\n        subject = _('Please rate your products on ')\n        subject += shop.name\n        from_email = shop.from_email\n        orders_sent = []\n        for order in lfs.marketing.utils.get_orders():\n            try:\n                OrderRatingMail.objects.get(order=order)\n            except OrderRatingMail.DoesNotExist:\n                pass\n            else:\n                continue\n            orders_sent.append(order)\n            if request.POST.get('test'):\n                to = shop.get_notification_emails()\n                bcc = []\n            else:\n                to = [order.customer_email]\n                if request.POST.get('bcc'):\n                    bcc = shop.get_notification_emails()\n                else:\n                    bcc = []\n                OrderRatingMail.objects.create(order=order)\n            text = render_to_string('lfs/reviews/rating_mail.txt', {\n                'order': order,\n                'content_type_id': ctype.id,\n                'site': site,\n            })\n            mail = EmailMultiAlternatives(subject=subject, body=text, from_email=from_email, to=to, bcc=bcc)\n            order_items = []\n            for order_item in order.items.all():\n                product = order_item.product\n                if product.is_variant():\n                    product = product.parent\n                order_items.append({\n                    'product_id': product.id,\n                    'product_name': product.name,\n                })\n            html = render_to_string('lfs/reviews/rating_mail.html', {\n                'order': order,\n                'order_items': order_items,\n                'content_type_id': ctype.id,\n                'site': site,\n            })\n            mail.attach_alternative(html, 'text/html')\n            mail.send()\n        return render_to_response('manage/marketing/rating_mails.html', RequestContext(request, {\n            'display_orders_sent': True,\n            'orders_sent': orders_sent,\n        }))\n", "label": 0}
{"function": "\n\ndef test_xmpp_connect(self):\n    with self.fake_xmpp_connect() as jabber_client:\n        T.assert_equal(jabber_client.connect.call_count, 1)\n        T.assert_equal(jabber_client.auth.call_count, 1)\n", "label": 0}
{"function": "\n\ndef run_tests():\n    for each_size in testlib.sizes:\n        print(('Testing with %s graphs' % each_size))\n        suite = unittest.TestSuite()\n        testlib.use_size = each_size\n        for each_module in test_modules():\n            try:\n                suite.addTests(unittest.TestLoader().loadTestsFromName(each_module))\n            except ImportError as ie:\n                log.exception(ie)\n                continue\n        tr = unittest.TextTestRunner(verbosity=2)\n        result = tr.run(suite)\n        del suite\n", "label": 0}
{"function": "\n\ndef test_create_different_index_multiple_times_same_bin(self):\n    '\\n            Invoke createindex() with multiple times on same bin with different\\nname\\n        '\n    policy = {\n        \n    }\n    retobj = TestIndex.client.index_integer_create('test', 'demo', 'age', 'age_index', policy)\n    if (retobj == 0):\n        retobj = TestIndex.client.index_integer_create('test', 'demo', 'age', 'age_index1', policy)\n        assert (retobj == 0)\n        TestIndex.client.index_remove('test', 'age_index', policy)\n        TestIndex.client.index_remove('test', 'age_index1', policy)\n    else:\n        assert (True is False)\n", "label": 0}
{"function": "\n\ndef test_send_event(self):\n    fake_service = 'fake_service'\n    fake_instance = 'fake_instance'\n    fake_status = '42'\n    fake_output = 'The http port is not open'\n    fake_soa_dir = ''\n    expected_check_name = ('setup_marathon_job.%s' % compose_job_id(fake_service, fake_instance))\n    with contextlib.nested(mock.patch('paasta_tools.monitoring_tools.send_event', autospec=True), mock.patch('paasta_tools.marathon_tools.load_marathon_service_config', autospec=True), mock.patch('paasta_tools.setup_marathon_job.load_system_paasta_config', autospec=True)) as (send_event_patch, load_marathon_service_config_patch, load_system_paasta_config_patch):\n        load_system_paasta_config_patch.return_value.get_cluster = mock.Mock(return_value='fake_cluster')\n        load_marathon_service_config_patch.return_value.get_monitoring.return_value = {\n            \n        }\n        setup_marathon_job.send_event(fake_service, fake_instance, fake_soa_dir, fake_status, fake_output)\n        send_event_patch.assert_called_once_with(fake_service, expected_check_name, {\n            'alert_after': '10m',\n            'check_every': '10s',\n        }, fake_status, fake_output, fake_soa_dir)\n        load_marathon_service_config_patch.assert_called_once_with(fake_service, fake_instance, load_system_paasta_config_patch.return_value.get_cluster.return_value, load_deployments=False, soa_dir=fake_soa_dir)\n", "label": 0}
{"function": "\n\ndef unpack_args(self, arg, kwarg_name, kwargs):\n    try:\n        new_args = kwargs[kwarg_name]\n        if (not isinstance(new_args, list)):\n            new_args = [new_args]\n        for i in new_args:\n            if (not isinstance(i, str)):\n                raise MesonException('html_args values must be strings.')\n    except KeyError:\n        return []\n    if (len(new_args) > 0):\n        return [(arg + '@@'.join(new_args))]\n    return []\n", "label": 0}
{"function": "\n\ndef test_preproc_pkl():\n    (fd, fname) = tempfile.mkstemp()\n    with os.fdopen(fd, 'wb') as f:\n        d = ('a', 1)\n        cPickle.dump(d, f)\n    environ['TEST_VAR'] = fname\n    loaded = load('a: !pkl: \"${TEST_VAR}\"')\n    assert_((loaded['a'] == d))\n    del environ['TEST_VAR']\n", "label": 0}
{"function": "\n\ndef invokeCli(argv, username='', password='', useApiUrl=False):\n    '\\n    Invoke the Girder Python client CLI with a set of arguments.\\n    '\n    if useApiUrl:\n        apiUrl = ('http://localhost:%s/api/v1' % os.environ['GIRDER_PORT'])\n        argsList = (['girder-client', '--api-url', apiUrl, '--username', username, '--password', password] + list(argv))\n    else:\n        argsList = (['girder-client', '--port', os.environ['GIRDER_PORT'], '--username', username, '--password', password] + list(argv))\n    exitVal = 0\n    with mock.patch.object(sys, 'argv', argsList), mock.patch('sys.exit', side_effect=SysExitException) as exit, captureOutput() as output:\n        try:\n            girder_client.cli.main()\n        except SysExitException:\n            args = exit.mock_calls[0][1]\n            exitVal = (args[0] if len(args) else 0)\n    return {\n        'exitVal': exitVal,\n        'stdout': output[0],\n        'stderr': output[1],\n    }\n", "label": 0}
{"function": "\n\ndef test_variable_access_before_setup(self):\n    prob = Problem(root=ExampleGroup())\n    try:\n        prob['G2.C1.x'] = 5.0\n    except AttributeError as err:\n        msg = \"'unknowns' has not been initialized, setup() must be called before 'G2.C1.x' can be accessed\"\n        self.assertEqual(text_type(err), msg)\n    else:\n        self.fail('Exception expected')\n    try:\n        prob.run()\n    except RuntimeError as err:\n        msg = 'setup() must be called before running the model.'\n        self.assertEqual(text_type(err), msg)\n    else:\n        self.fail('Exception expected')\n", "label": 0}
{"function": "\n\ndef test_buffer(self):\n    expect(self.transport._read_lock.acquire)\n    with expect(mock(gevent_transport, 'super')).args(is_arg(GeventTransport), GeventTransport).returns(mock()) as parent:\n        expect(parent.buffer).args('datas')\n    expect(self.transport._read_lock.release)\n    self.transport.buffer('datas')\n", "label": 0}
{"function": "\n\ndef __init__(self, env_spec, hidden_sizes=(32,), state_include_action=True, hidden_nonlinearity=NL.tanh):\n    '\\n        :param env_spec: A spec for the env.\\n        :param hidden_sizes: list of sizes for the fully connected hidden layers\\n        :param hidden_nonlinearity: nonlinearity used for each hidden layer\\n        :return:\\n        '\n    assert isinstance(env_spec.action_space, Discrete)\n    Serializable.quick_init(self, locals())\n    super(CategoricalGRUPolicy, self).__init__(env_spec)\n    assert (len(hidden_sizes) == 1)\n    if state_include_action:\n        input_shape = ((env_spec.observation_space.flat_dim + env_spec.action_space.flat_dim),)\n    else:\n        input_shape = (env_spec.observation_space.flat_dim,)\n    prob_network = GRUNetwork(input_shape=input_shape, output_dim=env_spec.action_space.n, hidden_dim=hidden_sizes[0], hidden_nonlinearity=hidden_nonlinearity, output_nonlinearity=NL.softmax)\n    self._prob_network = prob_network\n    self._state_include_action = state_include_action\n    self._f_step_prob = ext.compile_function([prob_network.step_input_layer.input_var, prob_network.step_prev_hidden_layer.input_var], L.get_output([prob_network.step_output_layer, prob_network.step_hidden_layer]))\n    self._prev_action = None\n    self._prev_hidden = None\n    self._hidden_sizes = hidden_sizes\n    self._dist = RecurrentCategorical()\n    self.reset()\n    LasagnePowered.__init__(self, [prob_network.output_layer])\n", "label": 0}
{"function": "\n\ndef test_options_not_supported(self):\n    factory = enginefacade._TransactionFactory()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        factory.configure(fake1='x', idle_timeout=200, wrong2='y')\n    self.assertEqual(1, len(w))\n    self.assertTrue(issubclass(w[(- 1)].category, exception.NotSupportedWarning))\n    self.assertEqual(\"Configuration option(s) ['fake1', 'wrong2'] not supported\", str(w[(- 1)].message))\n", "label": 0}
{"function": "\n\ndef test_libvirt_quobyte_driver_mount_non_quobyte_volume(self):\n    mnt_base = '/mnt'\n    self.flags(quobyte_mount_point_base=mnt_base, group='libvirt')\n    libvirt_driver = quobyte.LibvirtQuobyteVolumeDriver(self.fake_conn)\n    export_string = 'quobyte://192.168.1.1/volume-00001'\n    connection_info = {\n        'data': {\n            'export': export_string,\n            'name': self.name,\n        },\n    }\n\n    def exe_side_effect(*cmd, **kwargs):\n        if (cmd == mock.ANY):\n            raise exception.NovaException()\n    with mock.patch.object(quobyte, 'validate_volume') as mock_execute:\n        mock_execute.side_effect = exe_side_effect\n        self.assertRaises(exception.NovaException, libvirt_driver.connect_volume, connection_info, self.disk_info)\n", "label": 0}
{"function": "\n\ndef _register(self, dp):\n    assert (dp.id is not None)\n    self.dps[dp.id] = dp\n    if (dp.id not in self.port_state):\n        self.port_state[dp.id] = PortState()\n        for port in dp.ports.values():\n            self.port_state[dp.id].add(port.port_no, port)\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    '\\n        A custom save that publishes or unpublishes the object where\\n        appropriate.\\n\\n        Save with keyword argument obj.save(publish=False) to skip the process.\\n        '\n    from bakery import tasks\n    from django.contrib.contenttypes.models import ContentType\n    if (not kwargs.pop('publish', True)):\n        super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n    else:\n        try:\n            preexisting = self.__class__.objects.get(pk=self.pk)\n        except self.__class__.DoesNotExist:\n            preexisting = None\n        if (not preexisting):\n            if self.get_publication_status():\n                action = 'publish'\n            else:\n                action = None\n        elif ((not self.get_publication_status()) and preexisting.get_publication_status()):\n            action = 'unpublish'\n        elif self.get_publication_status():\n            action = 'publish'\n        else:\n            action = None\n        with transaction.atomic():\n            super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n        ct = ContentType.objects.get_for_model(self.__class__)\n        if (action == 'publish'):\n            tasks.publish_object.delay(ct.pk, self.pk)\n        elif (action == 'unpublish'):\n            tasks.unpublish_object.delay(ct.pk, self.pk)\n", "label": 0}
{"function": "\n\ndef testRunNotebooks(self):\n    notebook_dir = path.join('tests', 'input')\n    for notebook_path in glob(path.join(notebook_dir, '*.ipynb')):\n        notebook_file = path.basename(notebook_path)\n        print(notebook_file)\n        expected_file = path.join('tests', 'expected', notebook_file)\n        notebook = ''\n        with open(notebook_path) as notebook_file:\n            notebook = notebook_file.read()\n        try:\n            notebook = reads(notebook, 3)\n        except (TypeError, NBFormatError):\n            notebook = reads(notebook, 'json')\n        runner = NotebookRunner(notebook, working_dir=notebook_dir)\n        runner.run_notebook(True)\n        expected = ''\n        with open(expected_file) as notebook_file:\n            expected = notebook_file.read()\n        try:\n            expected = reads(expected, 3)\n        except (TypeError, NBFormatError):\n            expected = reads(expected, 'json')\n        self.assert_notebooks_equal(expected, runner.nb)\n", "label": 0}
{"function": "\n\ndef write_image(results, output_filename=None):\n    print(('Gathered %s results that represents a mandelbrot image (using %s chunks that are computed jointly by %s workers).' % (len(results), CHUNK_COUNT, WORKERS)))\n    if (not output_filename):\n        return\n    try:\n        from PIL import Image\n    except ImportError as e:\n        raise RuntimeError(('Pillow is required to write image files: %s' % e))\n    color_max = 0\n    for (_point, color) in results:\n        color_max = max(color, color_max)\n    img = Image.new('L', IMAGE_SIZE, 'black')\n    pixels = img.load()\n    for ((x, y), color) in results:\n        if (color_max == 0):\n            color = 0\n        else:\n            color = int(((float(color) / color_max) * 255.0))\n        pixels[(x, y)] = color\n    img.save(output_filename)\n", "label": 0}
{"function": "\n\ndef get_param_as_date(self, name, format_string='%Y-%m-%d', required=False, store=None):\n    'Return the value of a query string parameter as a date.\\n\\n        Args:\\n            name (str): Parameter name, case-sensitive (e.g., \\'ids\\').\\n            format_string (str): String used to parse the param value into a\\n                date.\\n                Any format recognized by strptime() is supported.\\n                (default ``\"%Y-%m-%d\"``)\\n            required (bool, optional): Set to ``True`` to raise\\n                ``HTTPBadRequest`` instead of returning ``None`` when the\\n                parameter is not found (default ``False``).\\n            store (dict, optional): A ``dict``-like object in which to place\\n                the value of the param, but only if the param is found (default\\n                ``None``).\\n        Returns:\\n            datetime.date: The value of the param if it is found and can be\\n                converted to a ``date`` according to the supplied format\\n                string. If the param is not found, returns ``None`` unless\\n                required is ``True``.\\n\\n        Raises:\\n            HTTPBadRequest: A required param is missing from the request.\\n            HTTPInvalidParam: A transform function raised an instance of\\n                ``ValueError``.\\n        '\n    param_value = self.get_param(name, required=required)\n    if (param_value is None):\n        return None\n    try:\n        date = strptime(param_value, format_string).date()\n    except ValueError:\n        msg = 'The date value does not match the required format'\n        raise HTTPInvalidParam(msg, name)\n    if (store is not None):\n        store[name] = date\n    return date\n", "label": 0}
{"function": "\n\ndef test_selector_auto_multi(self):\n    rule = AutoLineDisableRule()\n    assert rule.regex_match_any('# flake8: disable=E101, E102', ['E101'])\n    assert rule.regex_match_any('# flake8: disable=E101, E102', ['E102'])\n    assert (not rule.regex_match_any('# flake8: disable=E10', ['E102']))\n", "label": 0}
{"function": "\n\ndef Validate(self, value, unused_key=None):\n    'Validates a subnet.'\n    if (value is None):\n        raise validation.MissingAttribute('subnet must be specified')\n    if (not isinstance(value, basestring)):\n        raise validation.ValidationError((\"subnet must be a string, not '%r'\" % type(value)))\n    try:\n        ipaddr.IPNetwork(value)\n    except ValueError:\n        raise validation.ValidationError(('%s is not a valid IPv4 or IPv6 subnet' % value))\n    parts = value.split('/')\n    if ((len(parts) == 2) and (not re.match('^[0-9]+$', parts[1]))):\n        raise validation.ValidationError(('Prefix length of subnet %s must be an integer (quad-dotted masks are not supported)' % value))\n    return value\n", "label": 0}
{"function": "\n\n@override_settings(MIGRATION_MODULES={\n    'migrations': 'migrations.test_migrations',\n})\ndef test_name_match(self):\n    'Tests prefix name matching'\n    migration_loader = MigrationLoader(connection)\n    self.assertEqual(migration_loader.get_migration_by_prefix('migrations', '0001').name, '0001_initial')\n    with self.assertRaises(AmbiguityError):\n        migration_loader.get_migration_by_prefix('migrations', '0')\n    with self.assertRaises(KeyError):\n        migration_loader.get_migration_by_prefix('migrations', 'blarg')\n", "label": 0}
{"function": "\n\ndef do_rollback_test(self, new_func):\n    'Perform the rollback test with an exception in a new function.'\n    test_data = self.create_run_needing_refresh()\n\n    class SurpriseException(RuntimeError):\n        pass\n\n    def raise_exception(*args, **kwargs):\n        raise SurpriseException('Surprise!')\n    new_func.side_effect = raise_exception\n    with self.assertRaises(SurpriseException):\n        test_data['r'].refresh()\n    self.assert_rolled_back(test_data)\n", "label": 0}
{"function": "\n\ndef test_gdb(board_id=None):\n    result = GdbTestResult()\n    with MbedBoard.chooseBoard(board_id=board_id) as board:\n        memory_map = board.target.getMemoryMap()\n        ram_regions = [region for region in memory_map if (region.type == 'ram')]\n        ram_region = ram_regions[0]\n        rom_region = memory_map.getBootMemory()\n        target_type = board.getTargetType()\n        binary_file = os.path.join(parentdir, 'binaries', board.getTestBinary())\n        if (board_id is None):\n            board_id = board.getUniqueID()\n        test_clock = 10000000\n        test_port = 3334\n        error_on_invalid_access = True\n        ignore_hw_bkpt_result = (1 if (ram_region.start >= 536870912) else 0)\n        if (target_type == 'nrf51'):\n            test_clock = 1000000\n            error_on_invalid_access = False\n        board.flash.flashBinary(binary_file, rom_region.start)\n        board.uninit(False)\n    test_params = {\n        \n    }\n    test_params['rom_start'] = rom_region.start\n    test_params['rom_length'] = rom_region.length\n    test_params['ram_start'] = ram_region.start\n    test_params['ram_length'] = ram_region.length\n    test_params['invalid_start'] = 4294901760\n    test_params['invalid_length'] = 4096\n    test_params['expect_error_on_invalid_access'] = error_on_invalid_access\n    test_params['ignore_hw_bkpt_result'] = ignore_hw_bkpt_result\n    with open(TEST_PARAM_FILE, 'wb') as f:\n        f.write(json.dumps(test_params))\n    gdb = [PYTHON_GDB, '--command=gdb_script.py']\n    with open('output.txt', 'wb') as f:\n        program = Popen(gdb, stdin=PIPE, stdout=f, stderr=STDOUT)\n        args = [('-p=%i' % test_port), ('-f=%i' % test_clock), ('-b=%s' % board_id)]\n        server = GDBServerTool()\n        server.run(args)\n        program.wait()\n    with open(TEST_RESULT_FILE, 'rb') as f:\n        test_result = json.loads(f.read())\n    if set(TEST_RESULT_KEYS).issubset(test_result):\n        print('----------------Test Results----------------')\n        print(('HW breakpoint count: %s' % test_result['breakpoint_count']))\n        print(('Watchpoint count: %s' % test_result['watchpoint_count']))\n        print(('Average instruction step time: %s' % test_result['step_time_si']))\n        print(('Average single step time: %s' % test_result['step_time_s']))\n        print(('Average over step time: %s' % test_result['step_time_n']))\n        print(('Failure count: %i' % test_result['fail_count']))\n        result.passed = (test_result['fail_count'] == 0)\n    else:\n        result.passed = False\n    os.remove(TEST_RESULT_FILE)\n    os.remove(TEST_PARAM_FILE)\n    return result\n", "label": 0}
{"function": "\n\ndef test_cache(self):\n    mocked_repo = MagicMock()\n    mocked_commit = MagicMock()\n    mocked_repo.lookup_reference().resolve().target = 'head'\n    mocked_repo.walk.return_value = [mocked_commit]\n    mocked_commit.commit_time = 1411135000\n    mocked_commit.hex = '1111111111'\n    cache = CommitCache(mocked_repo)\n    cache.update()\n    cache['2014-09-20'] = Commit(1, 1, '1111111111')\n    assert (sorted(cache.keys()) == ['2014-09-19', '2014-09-20'])\n    asserted_time = datetime.fromtimestamp(mocked_commit.commit_time)\n    asserted_time = '{}-{}-{}'.format(asserted_time.hour, asserted_time.minute, asserted_time.second)\n    assert (repr(cache['2014-09-19']) == ('[%s-1111111111]' % asserted_time))\n    del cache['2014-09-20']\n    for commit_date in cache:\n        assert (commit_date == '2014-09-19')\n    mocked_repo.lookup_reference.has_calls([call('HEAD')])\n    mocked_repo.walk.assert_called_once_with('head', GIT_SORT_TIME)\n    assert (mocked_repo.lookup_reference().resolve.call_count == 2)\n", "label": 0}
{"function": "\n\ndef draw_collapsed_borders(context, table, enable_hinting):\n    'Draw borders of table cells when they collapse.'\n    row_heights = [row.height for row_group in table.children for row in row_group.children]\n    column_widths = table.column_widths\n    if (not (row_heights and column_widths)):\n        return\n    row_positions = [row.position_y for row_group in table.children for row in row_group.children]\n    column_positions = list(table.column_positions)\n    grid_height = len(row_heights)\n    grid_width = len(column_widths)\n    assert (grid_width == len(column_positions))\n    column_positions += [(column_positions[(- 1)] + column_widths[(- 1)])]\n    row_positions.append((row_positions[(- 1)] + row_heights[(- 1)]))\n    (vertical_borders, horizontal_borders) = table.collapsed_border_grid\n    if table.children[0].is_header:\n        header_rows = len(table.children[0].children)\n    else:\n        header_rows = 0\n    if table.children[(- 1)].is_footer:\n        footer_rows = len(table.children[(- 1)].children)\n    else:\n        footer_rows = 0\n    skipped_rows = table.skipped_rows\n    if skipped_rows:\n        body_rows_offset = (skipped_rows - header_rows)\n    else:\n        body_rows_offset = 0\n    if (header_rows == 0):\n        header_rows = (- 1)\n    if footer_rows:\n        first_footer_row = ((grid_height - footer_rows) - 1)\n    else:\n        first_footer_row = (grid_height + 1)\n    original_grid_height = len(vertical_borders)\n    footer_rows_offset = (original_grid_height - grid_height)\n\n    def row_number(y, horizontal):\n        if (y < (header_rows + int(horizontal))):\n            return y\n        elif (y >= (first_footer_row + int(horizontal))):\n            return (y + footer_rows_offset)\n        else:\n            return (y + body_rows_offset)\n    segments = []\n\n    def half_max_width(border_list, yx_pairs, vertical=True):\n        result = 0\n        for (y, x) in yx_pairs:\n            if (((0 <= y < grid_height) and (0 <= x <= grid_width)) if vertical else ((0 <= y <= grid_height) and (0 <= x < grid_width))):\n                yy = row_number(y, horizontal=(not vertical))\n                (_, (_, width, _)) = border_list[yy][x]\n                result = max(result, width)\n        return (result / 2)\n\n    def add_vertical(x, y):\n        yy = row_number(y, horizontal=False)\n        (score, (style, width, color)) = vertical_borders[yy][x]\n        if ((width == 0) or (color.alpha == 0)):\n            return\n        pos_x = column_positions[x]\n        pos_y1 = (row_positions[y] - half_max_width(horizontal_borders, [(y, (x - 1)), (y, x)], vertical=False))\n        pos_y2 = (row_positions[(y + 1)] + half_max_width(horizontal_borders, [((y + 1), (x - 1)), ((y + 1), x)], vertical=False))\n        segments.append((score, style, width, color, 'left', ((pos_x - (width / 2)), pos_y1, 0, (pos_y2 - pos_y1))))\n\n    def add_horizontal(x, y):\n        yy = row_number(y, horizontal=True)\n        (score, (style, width, color)) = horizontal_borders[yy][x]\n        if ((width == 0) or (color.alpha == 0)):\n            return\n        pos_y = row_positions[y]\n        pos_x1 = (column_positions[x] - half_max_width(vertical_borders, [((y - 1), x), (y, x)]))\n        pos_x2 = (column_positions[(x + 1)] + half_max_width(vertical_borders, [((y - 1), (x + 1)), (y, (x + 1))]))\n        segments.append((score, style, width, color, 'top', (pos_x1, (pos_y - (width / 2)), (pos_x2 - pos_x1), 0)))\n    for x in xrange(grid_width):\n        add_horizontal(x, 0)\n    for y in xrange(grid_height):\n        add_vertical(0, y)\n        for x in xrange(grid_width):\n            add_vertical((x + 1), y)\n            add_horizontal(x, (y + 1))\n    segments.sort(key=operator.itemgetter(0))\n    for segment in segments:\n        (_, style, width, color, side, border_box) = segment\n        if (side == 'top'):\n            widths = (width, 0, 0, 0)\n        else:\n            widths = (0, 0, 0, width)\n        with stacked(context):\n            clip_border_segment(context, enable_hinting, style, width, side, border_box, widths)\n            draw_rect_border(context, border_box, widths, style, styled_color(style, color, side))\n", "label": 1}
{"function": "\n\ndef test_draft4_validator_is_the_default(self):\n    with mock.patch.object(Draft4Validator, 'check_schema') as chk_schema:\n        validate({\n            \n        }, {\n            \n        })\n        chk_schema.assert_called_once_with({\n            \n        })\n", "label": 0}
{"function": "\n\ndef test_changelog(self):\n    with open(os.path.join(self.package_dir, 'ChangeLog'), 'r') as f:\n        body = f.read()\n    self.assertEqual(body, 'CHANGES\\n=======\\n\\n')\n", "label": 0}
{"function": "\n\n@override_settings(INSTALLED_APPS=TEST_INSTALLED_APPS)\ndef test_uninstall_apphooks_with_apphook(self):\n    with apphooks(SampleApp):\n        out = StringIO()\n        create_page('Hello Title', 'nav_playground.html', 'en', apphook=APPHOOK)\n        self.assertEqual(Page.objects.filter(application_urls=APPHOOK).count(), 1)\n        management.call_command('cms', 'uninstall', 'apphooks', APPHOOK, interactive=False, stdout=out)\n        self.assertEqual(out.getvalue(), \"1 'SampleApp' apphooks uninstalled\\n\")\n        self.assertEqual(Page.objects.filter(application_urls=APPHOOK).count(), 0)\n", "label": 0}
{"function": "\n\ndef candidates(self, items, artist, album, va_likely):\n    'Returns a list of AlbumInfo objects for discogs search results\\n        matching an album and artist (if not various).\\n        '\n    if (not self.discogs_client):\n        return\n    if va_likely:\n        query = album\n    else:\n        query = ('%s %s' % (artist, album))\n    try:\n        return self.get_albums(query)\n    except DiscogsAPIError as e:\n        self._log.debug('API Error: {0} (query: {1})', e, query)\n        if (e.status_code == 401):\n            self.reset_auth()\n            return self.candidates(items, artist, album, va_likely)\n        else:\n            return []\n    except CONNECTION_ERRORS:\n        self._log.debug('Connection error in album search', exc_info=True)\n        return []\n", "label": 0}
{"function": "\n\ndef Execute(self, opt, args):\n    self.opt = opt\n    project_list = self.GetProjects(args)\n    pending = []\n    branch = None\n    if opt.branch:\n        branch = opt.branch\n    for project in project_list:\n        if opt.current_branch:\n            cbr = project.CurrentBranch\n            up_branch = project.GetUploadableBranch(cbr)\n            if up_branch:\n                avail = [up_branch]\n            else:\n                avail = None\n                print(('ERROR: Current branch (%s) not pushable. You may be able to type \"git branch --set-upstream-to m/master\" to fix your branch.' % str(cbr)), file=sys.stderr)\n        else:\n            avail = project.GetUploadableBranches(branch)\n        if avail:\n            pending.append((project, avail))\n    if (pending and (not opt.bypass_hooks)):\n        hook = RepoHook('pre-push', self.manifest.repo_hooks_project, self.manifest.topdir, abort_if_user_denies=True)\n        pending_proj_names = [project.name for (project, avail) in pending]\n        pending_worktrees = [project.worktree for (project, avail) in pending]\n        try:\n            hook.Run(opt.allow_all_hooks, project_list=pending_proj_names, worktree_list=pending_worktrees)\n        except HookError as e:\n            print(('ERROR: %s' % str(e)), file=sys.stderr)\n            return\n    if (not pending):\n        print('no branches ready for push', file=sys.stderr)\n    elif ((len(pending) == 1) and (len(pending[0][1]) == 1)):\n        self._SingleBranch(opt, pending[0][1][0])\n    else:\n        self._MultipleBranches(opt, pending)\n", "label": 1}
{"function": "\n\ndef to_json(self, user):\n    ret = super(GitHubNodeSettings, self).to_json(user)\n    user_settings = user.get_addon('github')\n    ret.update({\n        'user_has_auth': (user_settings and user_settings.has_auth),\n        'is_registration': self.owner.is_registration,\n    })\n    if (self.user_settings and self.user_settings.has_auth):\n        valid_credentials = False\n        owner = self.user_settings.owner\n        connection = GitHubClient(external_account=self.external_account)\n        valid_credentials = True\n        try:\n            repos = itertools.chain.from_iterable((connection.repos(), connection.my_org_repos()))\n            repo_names = ['{0} / {1}'.format(repo.owner.login, repo.name) for repo in repos]\n        except GitHubError:\n            repo_names = []\n            valid_credentials = False\n        if (owner == user):\n            ret.update({\n                'repo_names': repo_names,\n            })\n        ret.update({\n            'node_has_auth': True,\n            'github_user': (self.user or ''),\n            'github_repo': (self.repo or ''),\n            'github_repo_full_name': ('{0} / {1}'.format(self.user, self.repo) if (self.user and self.repo) else ''),\n            'auth_osf_name': owner.fullname,\n            'auth_osf_url': owner.url,\n            'auth_osf_id': owner._id,\n            'github_user_name': self.external_account.display_name,\n            'github_user_url': self.external_account.profile_url,\n            'is_owner': (owner == user),\n            'valid_credentials': valid_credentials,\n            'addons_url': web_url_for('user_addons'),\n            'files_url': self.owner.web_url_for('collect_file_trees'),\n        })\n    return ret\n", "label": 1}
{"function": "\n\ndef emit(events, stream=None, Dumper=Dumper, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None):\n    '\\n    Emit YAML parsing events into a stream.\\n    If stream is None, return the produced string instead.\\n    '\n    getvalue = None\n    if (stream is None):\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        stream = StringIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break)\n    for event in events:\n        dumper.emit(event)\n    if getvalue:\n        return getvalue()\n", "label": 0}
{"function": "\n\ndef suspend_job(job_execution_id):\n    ctx = context.ctx()\n    job_execution = conductor.job_execution_get(ctx, job_execution_id)\n    if (job_execution.info['status'] not in edp.JOB_STATUSES_SUSPENDIBLE):\n        raise e.SuspendingFailed(_('Suspending operation can not be performed on status: {status}')).format(status=job_execution.info['status'])\n    cluster = conductor.cluster_get(ctx, job_execution.cluster_id)\n    engine = get_job_engine(cluster, job_execution)\n    job_execution = conductor.job_execution_update(ctx, job_execution_id, {\n        'info': {\n            'status': edp.JOB_STATUS_TOBESUSPENDED,\n        },\n    })\n    try:\n        job_info = engine.suspend_job(job_execution)\n    except Exception as ex:\n        job_info = None\n        conductor.job_execution_update(ctx, job_execution_id, {\n            'info': {\n                'status': edp.JOB_STATUS_SUSPEND_FAILED,\n            },\n        })\n        raise e.SuspendingFailed(_('Error during suspending of job execution: {error}')).format(error=ex)\n    if (job_info is not None):\n        job_execution = _write_job_status(job_execution, job_info)\n        LOG.info(_LI('Job execution was suspended successfully'))\n        return job_execution\n    conductor.job_execution_update(ctx, job_execution_id, {\n        'info': {\n            'status': edp.JOB_STATUS_SUSPEND_FAILED,\n        },\n    })\n    raise e.SuspendingFailed(_('Failed to suspend job execution{jid}')).format(jid=job_execution_id)\n", "label": 0}
{"function": "\n\ndef __init__(self, default, *items):\n    DiagramItem.__init__(self, 'g')\n    assert (default < len(items))\n    self.default = default\n    self.items = [wrapString(item) for item in items]\n    self.width = ((ARC_RADIUS * 4) + max((item.width for item in self.items)))\n    self.up = 0\n    self.down = 0\n    self.yAdvance = self.items[self.default].yAdvance\n    for (i, item) in enumerate(self.items):\n        if (i < default):\n            self.up += max(ARC_RADIUS, ((item.up + item.down) + VERTICAL_SEPARATION))\n        elif (i == default):\n            self.up += max(ARC_RADIUS, item.up)\n            self.down += max(ARC_RADIUS, item.down)\n        else:\n            assert (i > default)\n            self.down += max(ARC_RADIUS, ((VERTICAL_SEPARATION + item.up) + item.down))\n    if DEBUG:\n        self.attrs['data-updown'] = '{0} {1}'.format(self.up, self.down)\n        self.attrs['data-type'] = 'choice'\n", "label": 0}
{"function": "\n\n@cached_property\ndef api_version(self):\n    metas = [x for x in self.parsed.findall('.//meta') if (x.get('name', '').lower() == 'api-version')]\n    if metas:\n        try:\n            return int(metas[0].get('value', None))\n        except (TypeError, ValueError):\n            pass\n    return None\n", "label": 0}
{"function": "\n\n@common.check_cells_enabled\ndef create(self, req, body):\n    'Create a child cell entry.'\n    context = req.environ['nova.context']\n    authorize(context)\n    authorize(context, action='create')\n    nova_context.require_admin_context(context)\n    if ('cell' not in body):\n        msg = _('No cell information in request')\n        raise exc.HTTPBadRequest(explanation=msg)\n    cell = body['cell']\n    if ('name' not in cell):\n        msg = _('No cell name in request')\n        raise exc.HTTPBadRequest(explanation=msg)\n    self._validate_cell_name(cell['name'])\n    self._normalize_cell(cell)\n    try:\n        cell = self.cells_rpcapi.cell_create(context, cell)\n    except exception.CellsUpdateUnsupported as e:\n        raise exc.HTTPForbidden(explanation=e.format_message())\n    return dict(cell=_scrub_cell(cell))\n", "label": 0}
{"function": "\n\ndef _dump_json(data):\n    options = getattr(settings, 'JSON_OPTIONS', {\n        \n    })\n    if ('cls' in options):\n        if isinstance(options['cls'], six.string_types):\n            options['cls'] = import_string(options['cls'])\n    else:\n        try:\n            use_django = getattr(settings, 'JSON_USE_DJANGO_SERIALIZER')\n        except AttributeError:\n            use_django = True\n        else:\n            warnings.warn(\"JSON_USE_DJANGO_SERIALIZER is deprecated and will be removed. Please use JSON_OPTIONS['cls'] instead.\", DeprecationWarning)\n        if use_django:\n            options['cls'] = DjangoJSONEncoder\n    return json.dumps(data, **options)\n", "label": 0}
{"function": "\n\n@gen.engine\ndef Transform(self, client, device_photo, callback):\n    from device import Device\n    from user_photo import UserPhoto\n    device_id = device_photo.device_id\n    if (device_id not in MoveDevicePhoto._device_to_user_cache):\n        query_expr = ('device.device_id={t}', {\n            't': device_id,\n        })\n        devices = (yield gen.Task(Device.IndexQuery, client, query_expr, None))\n        assert (len(devices) == 1)\n        MoveDevicePhoto._device_to_user_cache[device_id] = devices[0].user_id\n    user_id = MoveDevicePhoto._device_to_user_cache[device_id]\n    existing = (yield gen.Task(UserPhoto.Query, client, user_id, device_photo.photo_id, None, must_exist=False))\n    if (existing is None):\n        logging.info('Creating user photo for photo %s, device %s, user %s', device_photo.photo_id, device_id, user_id)\n        user_photo = UserPhoto.CreateFromKeywords(photo_id=device_photo.photo_id, user_id=user_id, asset_keys=device_photo.asset_keys)\n    else:\n        logging.info('Photo %s, device %s, user %s already has user photo', device_photo.photo_id, device_id, user_id)\n        user_photo = None\n    if (user_photo is not None):\n        self._LogUpdate(user_photo)\n    if (Version._mutate_items and (user_photo is not None)):\n        (yield gen.Task(user_photo.Update, client))\n    callback(device_photo)\n", "label": 0}
{"function": "\n\n@wrap_exception()\n@wrap_instance_fault\ndef attach_interface(self, context, instance, network_id, port_id, requested_ip):\n    'Use hotplug to add an network adapter to an instance.'\n    bind_host_id = self.driver.network_binding_host_id(context, instance)\n    network_info = self.network_api.allocate_port_for_instance(context, instance, port_id, network_id, requested_ip, bind_host_id=bind_host_id)\n    if (len(network_info) != 1):\n        LOG.error(_LE('allocate_port_for_instance returned %(ports)s ports'), {\n            'ports': len(network_info),\n        })\n        raise exception.InterfaceAttachFailed(instance_uuid=instance.uuid)\n    image_meta = objects.ImageMeta.from_instance(instance)\n    try:\n        self.driver.attach_interface(instance, image_meta, network_info[0])\n    except exception.NovaException as ex:\n        port_id = network_info[0].get('id')\n        LOG.warning(_LW('attach interface failed , try to deallocate port %(port_id)s, reason: %(msg)s'), {\n            'port_id': port_id,\n            'msg': ex,\n        }, instance=instance)\n        try:\n            self.network_api.deallocate_port_for_instance(context, instance, port_id)\n        except Exception:\n            LOG.warning(_LW('deallocate port %(port_id)s failed'), {\n                'port_id': port_id,\n            }, instance=instance)\n        raise exception.InterfaceAttachFailed(instance_uuid=instance.uuid)\n    return network_info[0]\n", "label": 0}
{"function": "\n\n@setup_cache\ndef test_set_get_delete(cache):\n    for value in range(100):\n        cache.set(value, value)\n    cache.check()\n    for value in range(100):\n        assert (cache.get(value) == value)\n    cache.check()\n    for value in range(100):\n        assert (value in cache)\n    cache.check()\n    for value in range(100):\n        assert cache.delete(value)\n    assert (cache.delete(100) == False)\n    cache.check()\n    for value in range(100):\n        cache[value] = value\n    cache.check()\n    for value in range(100):\n        assert (cache[value] == value)\n    cache.check()\n    cache.clear()\n    assert (len(cache) == 0)\n    cache.check()\n", "label": 1}
{"function": "\n\ndef delete_mistyped_role():\n    '\\n    Delete \" system_admin\" role which was fat fingered.\\n    '\n    role_name = ' system_admin'\n    assert role_name.startswith(' ')\n    try:\n        role_db = Role.get_by_name(role_name)\n    except:\n        return\n    if (not role_db):\n        return\n    try:\n        Role.delete(role_db)\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef upload_file(self, upload_url, upload_auth_token, file_name, content_length, content_type, content_sha1, file_infos, data_stream):\n    assert (upload_url == upload_auth_token)\n    url_match = re.match('https://upload.example.com/([^/]*)/([^/]*)', upload_url)\n    if (url_match is None):\n        raise BadUploadUrl(upload_url)\n    if (len(self.upload_errors) != 0):\n        raise self.upload_errors.pop(0)\n    (bucket_id, upload_id) = url_match.groups()\n    bucket = self._get_bucket_by_id(bucket_id)\n    response = bucket.upload_file(upload_id, upload_auth_token, file_name, content_length, content_type, content_sha1, file_infos, data_stream)\n    file_id = response['fileId']\n    self.file_id_to_bucket_id[file_id] = bucket_id\n    return response\n", "label": 0}
{"function": "\n\n@public\ndef gcd(f, g=None, *gens, **args):\n    '\\n    Compute GCD of ``f`` and ``g``.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy import gcd\\n    >>> from sympy.abc import x\\n\\n    >>> gcd(x**2 - 1, x**2 - 3*x + 2)\\n    x - 1\\n\\n    '\n    if hasattr(f, '__iter__'):\n        if (g is not None):\n            gens = ((g,) + gens)\n        return gcd_list(f, *gens, **args)\n    elif (g is None):\n        raise TypeError('gcd() takes 2 arguments or a sequence of arguments')\n    options.allowed_flags(args, ['polys'])\n    try:\n        ((F, G), opt) = parallel_poly_from_expr((f, g), *gens, **args)\n    except PolificationFailed as exc:\n        (domain, (a, b)) = construct_domain(exc.exprs)\n        try:\n            return domain.to_sympy(domain.gcd(a, b))\n        except NotImplementedError:\n            raise ComputationFailed('gcd', 2, exc)\n    result = F.gcd(G)\n    if (not opt.polys):\n        return result.as_expr()\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef _showmodule(module_name):\n    module = sys.modules[module_name]\n    if (not hasattr(module, '__file__')):\n        raise ValueError('cannot display module {0} (no __file__)'.format(module_name))\n    if module.__file__.endswith('.py'):\n        fname = module.__file__\n    else:\n        if (not module.__file__.endswith('.pyc')):\n            raise ValueError('cannot display module file {0} for {1}'.format(module.__file__, module_name))\n        fname = module.__file__[:(- 1)]\n    if (not os.path.exists(fname)):\n        raise ValueError('could not find file {0} for {1}'.format(fname, module_name))\n    (leaf_count, branch_count) = _get_samples_by_line(fname)\n    lines = []\n    with open(fname) as f:\n        for (i, line) in enumerate(f):\n            lines.append(((i + 1), cgi.escape(line), leaf_count[(i + 1)], branch_count[(i + 1)]))\n    return lines\n", "label": 0}
{"function": "\n\ndef test_get_magics():\n    kernel = get_kernel()\n    d = Dummy(kernel)\n    line = d.get_magics('line')\n    cell = d.get_magics('cell')\n    assert ('dummy' in line)\n    assert ('spam' in cell)\n    assert ('eggs' in line)\n", "label": 0}
{"function": "\n\ndef test0(self):\n    x = zvector()\n    rng = numpy.random.RandomState(23)\n    xval = numpy.asarray(list((numpy.complex(rng.randn(), rng.randn()) for i in xrange(10))))\n    assert numpy.all((xval.real == theano.function([x], real(x))(xval)))\n    assert numpy.all((xval.imag == theano.function([x], imag(x))(xval)))\n", "label": 0}
{"function": "\n\ndef test_user_and_group_title(self):\n    u_title = 'User Title'\n    g_title = 'Group Title'\n    t = Title(name=u_title)\n    t.save()\n    t.users.add(self.user)\n    t = Title(name=g_title)\n    t.save()\n    t.groups.add(self.group)\n    titles = [k.name for k in karma_titles(self.user)]\n    eq_(2, len(titles))\n    assert (u_title in titles)\n    assert (g_title in titles)\n", "label": 0}
{"function": "\n\n@expose(help='Change directory to site webroot')\ndef cd(self):\n    if (not self.app.pargs.site_name):\n        try:\n            while (not self.app.pargs.site_name):\n                self.app.pargs.site_name = input('Enter site name : ').strip()\n        except IOError as e:\n            Log.error(self, 'Unable to read input, please try again')\n    self.app.pargs.site_name = self.app.pargs.site_name.strip()\n    (ee_domain, ee_www_domain) = ValidateDomain(self.app.pargs.site_name)\n    if (not check_domain_exists(self, ee_domain)):\n        Log.error(self, 'site {0} does not exist'.format(ee_domain))\n    ee_site_webroot = getSiteInfo(self, ee_domain).site_path\n    EEFileUtils.chdir(self, ee_site_webroot)\n    try:\n        subprocess.call(['bash'])\n    except OSError as e:\n        Log.debug(self, '{0}{1}'.format(e.errno, e.strerror))\n        Log.error(self, 'unable to change directory')\n", "label": 0}
{"function": "\n\ndef create_cloud(oname, words, maxsize=120, fontname='Lobster'):\n    'Creates a word cloud (when pytagcloud is installed)\\n\\n    Parameters\\n    ----------\\n    oname : output filename\\n    words : list of (value,str)\\n    maxsize : int, optional\\n        Size of maximum word. The best setting for this parameter will often\\n        require some manual tuning for each input.\\n    fontname : str, optional\\n        Font to use.\\n    '\n    try:\n        from pytagcloud import create_tag_image, make_tags\n    except ImportError:\n        if (not warned_of_error):\n            print('Could not import pytagcloud. Skipping cloud generation')\n        return\n    words = [(w, int((v * 10000))) for (v, w) in words]\n    tags = make_tags(words, maxsize=maxsize)\n    create_tag_image(tags, oname, size=(1800, 1200), fontname=fontname)\n", "label": 0}
{"function": "\n\ndef connectionLost(self, reason):\n    if (self.bodyDecoder is not None):\n        try:\n            try:\n                self.bodyDecoder.noMoreData()\n            except PotentialDataLoss:\n                self.response._bodyDataFinished(Failure())\n            except _DataLoss:\n                self.response._bodyDataFinished(Failure(ResponseFailed([reason, Failure()], self.response)))\n            else:\n                self.response._bodyDataFinished()\n        except:\n            log.err()\n    elif (self.state != DONE):\n        if self._everReceivedData:\n            exceptionClass = ResponseFailed\n        else:\n            exceptionClass = ResponseNeverReceived\n        self._responseDeferred.errback(Failure(exceptionClass([reason])))\n        del self._responseDeferred\n", "label": 0}
{"function": "\n\ndef __init__(self, generator, name=None, storage=None, cachefile_backend=None, cachefile_strategy=None):\n    '\\n        :param generator: The object responsible for generating a new image.\\n        :param name: The filename\\n        :param storage: A Django storage object that will be used to save the\\n            file.\\n        :param cachefile_backend: The object responsible for managing the\\n            state of the file.\\n        :param cachefile_strategy: The object responsible for handling events\\n            for this file.\\n\\n        '\n    self.generator = generator\n    if (not name):\n        try:\n            name = generator.cachefile_name\n        except AttributeError:\n            fn = get_by_qname(settings.IMAGEKIT_CACHEFILE_NAMER, 'namer')\n            name = fn(generator)\n    self.name = name\n    storage = (storage or getattr(generator, 'cachefile_storage', None) or get_singleton(settings.IMAGEKIT_DEFAULT_FILE_STORAGE, 'file storage backend'))\n    self.cachefile_backend = (cachefile_backend or getattr(generator, 'cachefile_backend', None) or get_singleton(settings.IMAGEKIT_DEFAULT_CACHEFILE_BACKEND, 'cache file backend'))\n    self.cachefile_strategy = (cachefile_strategy or getattr(generator, 'cachefile_strategy', None) or get_singleton(settings.IMAGEKIT_DEFAULT_CACHEFILE_STRATEGY, 'cache file strategy'))\n    super(ImageCacheFile, self).__init__(storage=storage)\n", "label": 0}
{"function": "\n\ndef assert_lte(lval, rval, message=None):\n    'Assert that lval is less than or equal to rval'\n    if message:\n        assert (lval <= rval), message\n    else:\n        assert (lval <= rval), ('assertion failed: %r <= %r' % (lval, rval))\n", "label": 0}
{"function": "\n\ndef _edit_running_config(self, conf_str, snippet):\n    conn = self._get_connection()\n    LOG.info(_LI('Config generated for [%(device)s] %(snip)s is:%(conf)s caller:%(caller)s'), {\n        'device': self.hosting_device['id'],\n        'snip': snippet,\n        'conf': conf_str,\n        'caller': self.caller_name(),\n    })\n    try:\n        rpc_obj = conn.edit_config(target='running', config=conf_str)\n        self._check_response(rpc_obj, snippet, conf_str=conf_str)\n    except Exception as e:\n        if re.search('REMOVE_|DELETE_', snippet):\n            LOG.warning(_LW('Pass exception for %s'), snippet)\n            pass\n        elif isinstance(e, ncclient.operations.rpc.RPCError):\n            e_tag = e.tag\n            e_type = e.type\n            params = {\n                'snippet': snippet,\n                'type': e_type,\n                'tag': e_tag,\n                'dev_id': self.hosting_device['id'],\n                'ip': self._host_ip,\n                'confstr': conf_str,\n            }\n            raise cfg_exc.CSR1kvConfigException(**params)\n", "label": 0}
{"function": "\n\ndef test_profiler():\n    with prof:\n        out = get(dsk, 'e')\n    assert (out == 6)\n    prof_data = sorted(prof.results, key=(lambda d: d.key))\n    keys = [i.key for i in prof_data]\n    assert (keys == ['c', 'd', 'e'])\n    tasks = [i.task for i in prof_data]\n    assert (tasks == [(add, 'a', 'b'), (mul, 'a', 'b'), (mul, 'c', 'd')])\n    prof.clear()\n    assert (prof.results == [])\n", "label": 0}
{"function": "\n\ndef assert_raises_and_contains(expected_exception_class, strings, callable_obj, *args, **kwargs):\n    'Assert an exception is raised by passing in a callable and its\\n    arguments and that the string representation of the exception\\n    contains the case-insensitive list of passed in strings.\\n\\n    Args\\n        strings -- can be a string or an iterable of strings\\n    '\n    try:\n        callable_obj(*args, **kwargs)\n    except expected_exception_class as e:\n        message = str(e).lower()\n        try:\n            is_string = isinstance(strings, basestring)\n        except NameError:\n            is_string = isinstance(strings, str)\n        if is_string:\n            strings = [strings]\n        for string in strings:\n            assert_in(string.lower(), message)\n    else:\n        assert_not_reached(('No exception was raised (expected %s)' % expected_exception_class))\n", "label": 0}
{"function": "\n\ndef calc_gradient(self, indep_list, unknown_list, mode='auto', return_format='array', dv_scale=None, cn_scale=None, sparsity=None):\n    \" Returns the gradient for the system that is specified in\\n        self.root. This function is used by the optimizer but also can be\\n        used for testing derivatives on your model.\\n\\n        Args\\n        ----\\n        indep_list : iter of strings\\n            Iterator of independent variable names that derivatives are to\\n            be calculated with respect to. All params must have a IndepVarComp.\\n\\n        unknown_list : iter of strings\\n            Iterator of output or state names that derivatives are to\\n            be calculated for. All must be valid unknowns in OpenMDAO.\\n\\n        mode : string, optional\\n            Deriviative direction, can be 'fwd', 'rev', 'fd', or 'auto'.\\n            Default is 'auto', which uses mode specified on the linear solver\\n            in root.\\n\\n        return_format : string, optional\\n            Format for the derivatives, can be 'array' or 'dict'.\\n\\n        dv_scale : dict, optional\\n            Dictionary of driver-defined scale factors on the design variables.\\n\\n        cn_scale : dict, optional\\n            Dictionary of driver-defined scale factors on the constraints.\\n\\n        sparsity : dict, optional\\n            Dictionary that gives the relevant design variables for each\\n            constraint. This option is only supported in the `dict` return\\n            format.\\n\\n        Returns\\n        -------\\n        ndarray or dict\\n            Jacobian of unknowns with respect to params.\\n        \"\n    if (mode not in ['auto', 'fwd', 'rev', 'fd']):\n        msg = \"mode must be 'auto', 'fwd', 'rev', or 'fd'\"\n        raise ValueError(msg)\n    if (return_format not in ['array', 'dict']):\n        msg = \"return_format must be 'array' or 'dict'\"\n        raise ValueError(msg)\n    with self.root._dircontext:\n        if ((mode == 'fd') or self.root.fd_options['force_fd']):\n            return self._calc_gradient_fd(indep_list, unknown_list, return_format, dv_scale=dv_scale, cn_scale=cn_scale, sparsity=sparsity)\n        else:\n            return self._calc_gradient_ln_solver(indep_list, unknown_list, return_format, mode, dv_scale=dv_scale, cn_scale=cn_scale, sparsity=sparsity)\n", "label": 0}
{"function": "\n\ndef _async_connect(self, *args):\n    \"Internal use only; use 'connect' with 'yield' instead.\\n\\n        Asynchronous version of socket connect method.\\n        \"\n\n    def _connect(self, *args):\n        err = self._rsock.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n        if err:\n            self._notifier.unregister(self)\n            self._write_task = None\n            (coro, self._write_coro) = (self._write_coro, None)\n            coro.throw(socket.error(err))\n        elif self._certfile:\n\n            def _ssl_handshake(self):\n                try:\n                    self._rsock.do_handshake()\n                except ssl.SSLError as err:\n                    if ((err.args[0] == ssl.SSL_ERROR_WANT_READ) or (err.args[0] == ssl.SSL_ERROR_WANT_WRITE)):\n                        pass\n                    else:\n                        self._read_task = self._write_task = None\n                        (coro, self._write_coro) = (self._write_coro, None)\n                        self._read_coro = None\n                        self.close()\n                        coro.throw(*sys.exc_info())\n                except:\n                    self._read_task = self._write_task = None\n                    (coro, self._write_coro) = (self._write_coro, None)\n                    self._read_coro = None\n                    self.close()\n                    coro.throw(*sys.exc_info())\n                else:\n                    self._notifier.clear(self)\n                    self._read_task = self._write_task = None\n                    (coro, self._write_coro) = (self._write_coro, None)\n                    self._read_coro = None\n                    coro._proceed_(0)\n            try:\n                self._rsock = ssl.wrap_socket(self._rsock, ca_certs=self._certfile, cert_reqs=ssl.CERT_REQUIRED, server_side=False, do_handshake_on_connect=False)\n            except:\n                (coro, self._write_coro) = (self._write_coro, None)\n                self.close()\n                coro.throw(*sys.exc_info())\n            else:\n                self._read_task = self._write_task = functools.partial(_ssl_handshake, self)\n                self._read_coro = self._write_coro\n                self._notifier.add(self, _AsyncPoller._Read)\n                self._write_task()\n        else:\n            self._write_task = None\n            (coro, self._write_coro) = (self._write_coro, None)\n            self._notifier.clear(self, _AsyncPoller._Write)\n            coro._proceed_(0)\n    self._write_task = functools.partial(_connect, self, *args)\n    self._write_coro = AsynCoro.cur_coro()\n    self._write_coro._await_()\n    self._notifier.add(self, _AsyncPoller._Write)\n    try:\n        self._rsock.connect(*args)\n    except socket.error as e:\n        if ((e.args[0] == EINPROGRESS) or (e.args[0] == EWOULDBLOCK)):\n            pass\n        else:\n            raise\n", "label": 0}
{"function": "\n\ndef _result(self, response, json=False, binary=False):\n    assert (not (json and binary))\n    self._raise_for_status(response)\n    if json:\n        return response.json()\n    if binary:\n        return response.content\n    return response.text\n", "label": 0}
{"function": "\n\n@classmethod\n@unguarded\ndef validate_url(cls, url):\n    '\\n        Return a boolean indicating whether the URL has a protocol and hostname.\\n        If a port is specified, ensure it is an integer.\\n\\n        Arguments:\\n            url (str): The URL to check.\\n\\n        Returns:\\n            Boolean indicating whether the URL has a protocol and hostname.\\n        '\n    result = urlparse.urlsplit(url)\n    if ((not result.scheme) or (not result.netloc)):\n        return False\n    try:\n        if (result.port is not None):\n            int(result.port)\n        elif result.netloc.endswith(':'):\n            return False\n    except ValueError:\n        return False\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef test_reflect_entity_overrides():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    b = Symbol('b')\n    m = Symbol('m')\n    l = Line((0, b), slope=m)\n    p = Point(x, y)\n    r = p.reflect(l)\n    c = Circle((x, y), 3)\n    cr = c.reflect(l)\n    assert (cr == Circle(r, (- 3)))\n    assert (c.area == (- cr.area))\n    pent = RegularPolygon((1, 2), 1, 5)\n    l = Line((0, pi), slope=sqrt(2))\n    rpent = pent.reflect(l)\n    assert (rpent.center == pent.center.reflect(l))\n    assert (str([w.n(3) for w in rpent.vertices]) == '[Point2D(-0.586, 4.27), Point2D(-1.69, 4.66), Point2D(-2.41, 3.73), Point2D(-1.74, 2.76), Point2D(-0.616, 3.10)]')\n    assert pent.area.equals((- rpent.area))\n", "label": 0}
{"function": "\n\ndef test_dispatch():\n\n    class App(morepath.App):\n        pass\n\n    class Foo(object):\n        pass\n\n    class Bar(object):\n        pass\n\n    class Other(object):\n        pass\n\n    @reg.dispatch('obj')\n    def f(obj):\n        return 'fallback'\n\n    @App.function(f, obj=Foo)\n    def f_foo(obj):\n        return 'foo'\n\n    @App.function(f, obj=Bar)\n    def f_bar(obj):\n        return 'bar'\n    a = App()\n    lookup = a.lookup\n    assert (f(Foo(), lookup=lookup) == 'foo')\n    assert (f(Bar(), lookup=lookup) == 'bar')\n    assert (f(Other(), lookup=lookup) == 'fallback')\n", "label": 0}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(UpdateProjectMembersAction, self).__init__(request, *args, **kwargs)\n    err_msg = _('Unable to retrieve user list. Please try again later.')\n    domain_id = self.initial.get('domain_id', None)\n    project_id = ''\n    if ('project_id' in self.initial):\n        project_id = self.initial['project_id']\n    try:\n        default_role = keystone.get_default_role(self.request)\n        if (default_role is None):\n            default = getattr(settings, 'OPENSTACK_KEYSTONE_DEFAULT_ROLE', None)\n            msg = (_('Could not find default role \"%s\" in Keystone') % default)\n            raise exceptions.NotFound(msg)\n    except Exception:\n        exceptions.handle(self.request, err_msg, redirect=reverse(INDEX_URL))\n    default_role_name = self.get_default_role_field_name()\n    self.fields[default_role_name] = forms.CharField(required=False)\n    self.fields[default_role_name].initial = default_role.id\n    all_users = []\n    try:\n        all_users = api.keystone.user_list(request, domain=domain_id)\n    except Exception:\n        exceptions.handle(request, err_msg)\n    users_list = [(user.id, user.name) for user in all_users]\n    role_list = []\n    try:\n        role_list = api.keystone.role_list(request)\n    except Exception:\n        exceptions.handle(request, err_msg, redirect=reverse(INDEX_URL))\n    for role in role_list:\n        field_name = self.get_member_field_name(role.id)\n        label = role.name\n        self.fields[field_name] = forms.MultipleChoiceField(required=False, label=label)\n        self.fields[field_name].choices = users_list\n        self.fields[field_name].initial = []\n    if project_id:\n        try:\n            users_roles = api.keystone.get_project_users_roles(request, project_id)\n        except Exception:\n            exceptions.handle(request, err_msg, redirect=reverse(INDEX_URL))\n        for user_id in users_roles:\n            roles_ids = users_roles[user_id]\n            for role_id in roles_ids:\n                field_name = self.get_member_field_name(role_id)\n                self.fields[field_name].initial.append(user_id)\n", "label": 1}
{"function": "\n\ndef test_list_extend_meta_type_integer(self):\n    '\\n        Invoke list_extend() with metadata input is of type integer\\n        '\n    key = ('test', 'demo', 1)\n    try:\n        TestListExtend.client.list_extend(key, 'contact_no', [85], 888)\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'Metadata should be of type dictionary')\n", "label": 0}
{"function": "\n\ndef __init__(self, vm_spec):\n    'Initialize a CloudStack virtual machine.\\n\\n    Args:\\n      vm_spec: virtual_machine.BaseVirtualMachineSpec object of the vm.\\n    '\n    super(CloudStackVirtualMachine, self).__init__(vm_spec)\n    self.network = cloudstack_network.CloudStackNetwork.GetNetwork(self)\n    self.cs = util.CsClient(FLAGS.CS_API_URL, FLAGS.CS_API_KEY, FLAGS.CS_API_SECRET)\n    self.project_id = None\n    if FLAGS.project:\n        project = self.cs.get_project(FLAGS.project)\n        assert project, 'Project not found'\n        self.project_id = project['id']\n    zone = self.cs.get_zone(self.zone)\n    assert zone, 'Zone not found'\n    self.zone_id = zone['id']\n    self.user_name = self.DEFAULT_USER_NAME\n    self.image = (self.image or self.DEFAULT_IMAGE)\n    self.disk_counter = 0\n", "label": 0}
{"function": "\n\ndef fetch(self, dataset, name, model, forwarders, db):\n    collect = (self.collect or forwarders)\n    try:\n        data_mapping = collections.defaultdict(list)\n        t1 = time.time()\n        for obj in dataset:\n            for field in forwarders:\n                obj = getattr(obj, field, None)\n            if (not obj):\n                continue\n            if collect:\n                data_mapping[self.mapper(obj)].append(obj)\n            else:\n                data_mapping[self.mapper(obj)] = obj\n            self.decorator(obj)\n        t2 = time.time()\n        logger.debug('Creating data_mapping for %s query took %.3f secs for the %s prefetcher.', model.__name__, (t2 - t1), name)\n        t1 = time.time()\n        related_data = self.filter(data_mapping.keys())\n        if (db is not None):\n            related_data = related_data.using(db)\n        related_data_len = len(related_data)\n        t2 = time.time()\n        logger.debug('Filtering for %s related objects for %s query took %.3f secs for the %s prefetcher.', related_data_len, model.__name__, (t2 - t1), name)\n        relation_mapping = collections.defaultdict(list)\n        t1 = time.time()\n        for obj in related_data:\n            for id_ in self.reverse_mapper(obj):\n                if id_:\n                    relation_mapping[id_].append(obj)\n        for (id_, related_items) in relation_mapping.items():\n            if (id_ in data_mapping):\n                if collect:\n                    for item in data_mapping[id_]:\n                        self.decorator(item, related_items)\n                else:\n                    self.decorator(data_mapping[id_], related_items)\n        t2 = time.time()\n        logger.debug('Adding the related objects on the %s query took %.3f secs for the %s prefetcher.', model.__name__, (t2 - t1), name)\n        return dataset\n    except Exception:\n        logger.exception('Prefetch failed for %s prefetch on the %s model:', name, model.__name__)\n        raise\n", "label": 1}
{"function": "\n\ndef test_Matrix_printing():\n    mat = Matrix([(x * y), Piecewise(((2 + x), (y > 0)), (y, True)), sin(z)])\n    A = MatrixSymbol('A', 3, 1)\n    assert (ccode(mat, A) == 'A[0] = x*y;\\nif (y > 0) {\\n   A[1] = x + 2;\\n}\\nelse {\\n   A[1] = y;\\n}\\nA[2] = sin(z);')\n    expr = ((Piecewise(((2 * A[(2, 0)]), (x > 0)), (A[(2, 0)], True)) + sin(A[(1, 0)])) + A[(0, 0)])\n    assert (ccode(expr) == '((x > 0) ? (\\n   2*A[2]\\n)\\n: (\\n   A[2]\\n)) + sin(A[1]) + A[0]')\n    q = MatrixSymbol('q', 5, 1)\n    M = MatrixSymbol('M', 3, 3)\n    m = Matrix([[sin(q[(1, 0)]), 0, cos(q[(2, 0)])], [(q[(1, 0)] + q[(2, 0)]), q[(3, 0)], 5], [((2 * q[(4, 0)]) / q[(1, 0)]), (sqrt(q[(0, 0)]) + 4), 0]])\n    assert (ccode(m, M) == 'M[0] = sin(q[1]);\\nM[1] = 0;\\nM[2] = cos(q[2]);\\nM[3] = q[1] + q[2];\\nM[4] = q[3];\\nM[5] = 5;\\nM[6] = 2*q[4]*1.0/q[1];\\nM[7] = 4 + sqrt(q[0]);\\nM[8] = 0;')\n", "label": 0}
{"function": "\n\ndef pseudo_raw_input(self, prompt):\n    \"copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout\"\n    if self.use_rawinput:\n        try:\n            line = raw_input(prompt)\n        except EOFError:\n            line = 'EOF'\n    else:\n        self.stdout.write(prompt)\n        self.stdout.flush()\n        line = self.stdin.readline()\n        if (not len(line)):\n            line = 'EOF'\n        elif (line[(- 1)] == '\\n'):\n            line = line[:(- 1)]\n    return line\n", "label": 0}
{"function": "\n\ndef get_stack(message=None, stack_first_frame=1, max_frames=15):\n    try:\n        stack = inspect.stack()\n        frame_num = stack_first_frame\n        context = []\n        while ((len(stack) > frame_num) and (frame_num < (max_frames + stack_first_frame))):\n            exec_line = ('%s:%s:%s' % (stack[frame_num][1], stack[frame_num][2], stack[frame_num][3]))\n            context.insert(0, exec_line)\n            if (exec_line.endswith('_control_flow') or exec_line.endswith('load_ion') or exec_line.endswith('spawn_process') or exec_line.endswith(':main') or exec_line.endswith(':dispatch_request')):\n                break\n            frame_num += 1\n        stack_str = '\\n '.join(context)\n        if message:\n            stack_str = ((message + '\\n') + stack_str)\n        return stack_str\n    except Exception as ex:\n        stack_str = ('ERROR: ' + str(ex))\n        if message:\n            stack_str = ((message + '\\n') + stack_str)\n        return stack_str\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    if (self.length is None):\n        length = '*'\n    else:\n        length = self.length\n    if (self.start is None):\n        assert (self.stop is None)\n        return ('bytes */%s' % length)\n    stop = (self.stop - 1)\n    return ('bytes %s-%s/%s' % (self.start, stop, length))\n", "label": 0}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(SelectPluginAction, self).__init__(request, *args, **kwargs)\n    try:\n        plugins = saharaclient.plugin_list(request)\n    except Exception:\n        plugins = []\n        exceptions.handle(request, _('Unable to fetch plugin list.'))\n    plugin_choices = [(plugin.name, plugin.title) for plugin in plugins]\n    self.fields['plugin_name'] = forms.ChoiceField(label=_('Plugin name'), choices=plugin_choices, widget=forms.Select(attrs={\n        'class': 'plugin_name_choice',\n    }))\n    for plugin in plugins:\n        field_name = (plugin.name + '_version')\n        choice_field = forms.ChoiceField(label=_('Version'), choices=[(version, version) for version in plugin.versions], widget=forms.Select(attrs={\n            'class': (('plugin_version_choice ' + field_name) + '_choice'),\n        }))\n        self.fields[field_name] = choice_field\n", "label": 0}
{"function": "\n\ndef run(self):\n    super(FakeDeletionThread, self).run()\n    receiver = NailgunReceiver\n    kwargs = {\n        'task_uuid': self.task_uuid,\n        'nodes': self.data['args']['nodes'],\n        'status': 'ready',\n    }\n    nodes_to_restore = copy.deepcopy(self.data['args'].get('nodes_to_restore', []))\n    resp_method = getattr(receiver, self.respond_to)\n    try:\n        resp_method(**kwargs)\n        db().commit()\n    except Exception:\n        db().rollback()\n        raise\n    recover_nodes = self.params.get('recover_nodes', True)\n    recover_offline_nodes = self.params.get('recover_offline_nodes', True)\n    if (not recover_nodes):\n        db().commit()\n        return\n    for node_data in nodes_to_restore:\n        is_offline = (('online' in node_data) and (not node_data['online']))\n        if (is_offline and (not recover_offline_nodes)):\n            continue\n        node_data['status'] = 'discover'\n        objects.Node.create(node_data)\n    db().commit()\n", "label": 0}
{"function": "\n\ndef add_jumpbox(host):\n    if jb.cluster.locate(host):\n        return\n    print('Connecting to jumpbox:', host)\n    try:\n        t = ssh.connection_worker(host, None, init_data['auth'])\n        retries = 3\n        while ((not t.is_authenticated()) and (retries > 0)):\n            print(('Failed to authenticate to Jumpbox (%s)' % host))\n            jb_user = raw_input(('Enter username for [%s]: ' % host))\n            jb_passwd = getpass.getpass(('Enter password for %s@%s: ' % (jb_user, host)))\n            reauth = AuthManager(jb_user, auth_file=None, include_agent=False, include_userkeys=False, default_password=jb_passwd)\n            t = ssh.connection_worker(host, None, reauth)\n            retries -= 1\n    except Exception as e:\n        print(host, repr(e))\n    finally:\n        jb.cluster.connections[host] = t\n", "label": 0}
{"function": "\n\ndef _finish_query_lookupd(self, response, lookupd_url):\n    if response.error:\n        logger.warning('[%s] lookupd %s query error: %s', self.name, lookupd_url, response.error)\n        return\n    try:\n        lookup_data = json.loads(response.body)\n    except ValueError:\n        logger.warning('[%s] lookupd %s failed to parse JSON: %r', self.name, lookupd_url, response.body)\n        return\n    if (lookup_data['status_code'] != 200):\n        logger.warning('[%s] lookupd %s responded with %d', self.name, lookupd_url, lookup_data['status_code'])\n        return\n    for producer in lookup_data['data']['producers']:\n        address = producer.get('broadcast_address', producer.get('address'))\n        assert address\n        self.connect_to_nsqd(address, producer['tcp_port'])\n", "label": 0}
{"function": "\n\ndef populate(self):\n    config = self.__get_newsblog_config()\n    if (not config):\n        return\n    user = getattr(self.request, 'user', None)\n    try:\n        view_name = self.request.resolver_match.view_name\n    except AttributeError:\n        view_name = None\n    if (user and view_name):\n        language = get_language_from_request(self.request, check_path=True)\n        if (view_name == '{0}:article-detail'.format(config.namespace)):\n            article = get_object_from_request(Article, self.request)\n        else:\n            article = None\n        menu = self.toolbar.get_or_create_menu('newsblog-app', config.get_app_title())\n        change_config_perm = user.has_perm('aldryn_newsblog.change_newsblogconfig')\n        add_config_perm = user.has_perm('aldryn_newsblog.add_newsblogconfig')\n        config_perms = [change_config_perm, add_config_perm]\n        change_article_perm = user.has_perm('aldryn_newsblog.change_article')\n        delete_article_perm = user.has_perm('aldryn_newsblog.delete_article')\n        add_article_perm = user.has_perm('aldryn_newsblog.add_article')\n        article_perms = [change_article_perm, add_article_perm, delete_article_perm]\n        if change_config_perm:\n            url_args = {\n                \n            }\n            if language:\n                url_args = {\n                    'language': language,\n                }\n            url = get_admin_url('aldryn_newsblog_newsblogconfig_change', [config.pk], **url_args)\n            menu.add_modal_item(_('Configure addon'), url=url)\n        if (any(config_perms) and any(article_perms)):\n            menu.add_break()\n        if change_article_perm:\n            url_args = {\n                \n            }\n            if config:\n                url_args = {\n                    'app_config__id__exact': config.pk,\n                }\n            url = get_admin_url('aldryn_newsblog_article_changelist', **url_args)\n            menu.add_sideframe_item(_('Article list'), url=url)\n        if add_article_perm:\n            url_args = {\n                'app_config': config.pk,\n                'owner': user.pk,\n            }\n            if language:\n                url_args.update({\n                    'language': language,\n                })\n            url = get_admin_url('aldryn_newsblog_article_add', **url_args)\n            menu.add_modal_item(_('Add new article'), url=url)\n        if (change_article_perm and article):\n            url_args = {\n                \n            }\n            if language:\n                url_args = {\n                    'language': language,\n                }\n            url = get_admin_url('aldryn_newsblog_article_change', [article.pk], **url_args)\n            menu.add_modal_item(_('Edit this article'), url=url, active=True)\n        if (delete_article_perm and article):\n            redirect_url = self.get_on_delete_redirect_url(article, language=language)\n            url = get_admin_url('aldryn_newsblog_article_delete', [article.pk])\n            menu.add_modal_item(_('Delete this article'), url=url, on_close=redirect_url)\n", "label": 1}
{"function": "\n\ndef update_record(self, record, name, type, data, extra=None):\n    '\\n        Update an existing record.\\n\\n        :param record: Record to update.\\n        :type  record: :class:`Record`\\n\\n        :param name: FQDN of the new record, for example \"www.example.com\".\\n        :type  name: ``str``\\n\\n        :param type: DNS record type (A, AAAA, ...).\\n        :type  type: :class:`RecordType`\\n\\n        :param data: Data for the record (depends on the record type).\\n        :type  data: ``str``\\n\\n        :param extra: (optional) Extra attributes (driver specific).\\n        :type  extra: ``dict``\\n\\n        :rtype: :class:`Record`\\n        '\n    action = ('%s/servers/%s/zones/%s' % (self.api_root, self.ex_server, record.zone.id))\n    if ((extra is None) or (extra.get('ttl', None) is None)):\n        raise ValueError('PowerDNS requires a ttl value for every record')\n    updated_record = {\n        'content': data,\n        'disabled': False,\n        'name': name,\n        'ttl': extra['ttl'],\n        'type': type,\n    }\n    payload = {\n        'rrsets': [{\n            'name': record.name,\n            'type': record.type,\n            'changetype': 'DELETE',\n        }, {\n            'name': name,\n            'type': type,\n            'changetype': 'REPLACE',\n            'records': [updated_record],\n        }],\n    }\n    try:\n        self.connection.request(action=action, data=json.dumps(payload), method='PATCH')\n    except BaseHTTPError:\n        e = sys.exc_info()[1]\n        if ((e.code == httplib.UNPROCESSABLE_ENTITY) and e.message.startswith('Could not find domain')):\n            raise ZoneDoesNotExistError(zone_id=record.zone.id, driver=self, value=e.message)\n        raise e\n    return Record(id=None, name=name, data=data, type=type, zone=record.zone, driver=self, ttl=extra['ttl'])\n", "label": 0}
{"function": "\n\n@xmlrpc_func(returns='string[]', args=['string'])\ndef pingback_extensions_get_pingbacks(target):\n    \"\\n    pingback.extensions.getPingbacks(url) => '[url, url, ...]'\\n\\n    Returns an array of URLs that link to the specified url.\\n\\n    See: http://www.aquarionics.com/misc/archives/blogite/0198.html\\n    \"\n    site = Site.objects.get_current()\n    (scheme, netloc, path, query, fragment) = urlsplit(target)\n    if (netloc != site.domain):\n        return TARGET_DOES_NOT_EXIST\n    try:\n        (view, args, kwargs) = resolve(path)\n    except Resolver404:\n        return TARGET_DOES_NOT_EXIST\n    try:\n        entry = Entry.published.get(slug=kwargs['slug'], publication_date__year=kwargs['year'], publication_date__month=kwargs['month'], publication_date__day=kwargs['day'])\n    except (KeyError, Entry.DoesNotExist):\n        return TARGET_IS_NOT_PINGABLE\n    return [pingback.user_url for pingback in entry.pingbacks]\n", "label": 0}
{"function": "\n\ndef profile_head_single(benchmark):\n    import gc\n    results = []\n    gc.collect()\n    try:\n        from ctypes import cdll, CDLL\n        cdll.LoadLibrary('libc.so.6')\n        libc = CDLL('libc.so.6')\n        libc.malloc_trim(0)\n    except:\n        pass\n    N = (args.hrepeats + args.burnin)\n    results = []\n    try:\n        for i in range(N):\n            gc.disable()\n            d = dict()\n            try:\n                d = benchmark.run()\n            except KeyboardInterrupt:\n                raise\n            except Exception as e:\n                err = ''\n                try:\n                    err = d.get('traceback')\n                    if (err is None):\n                        err = str(e)\n                except:\n                    pass\n                print(('%s died with:\\n%s\\nSkipping...\\n' % (benchmark.name, err)))\n            results.append(d.get('timing', np.nan))\n            gc.enable()\n            gc.collect()\n    finally:\n        gc.enable()\n    if results:\n        results = results[args.burnin:]\n    sys.stdout.write('.')\n    sys.stdout.flush()\n    return Series(results, name=benchmark.name)\n", "label": 0}
{"function": "\n\ndef test_cpu_stats_interrupts(self):\n    with open('/proc/stat', 'rb') as f:\n        for line in f:\n            if line.startswith(b'intr'):\n                interrupts = int(line.split()[1])\n                break\n        else:\n            raise ValueError(\"couldn't find line\")\n    self.assertAlmostEqual(psutil.cpu_stats().interrupts, interrupts, delta=1000)\n", "label": 0}
{"function": "\n\ndef replace(self, new):\n    'Replaces this node with a new one in the parent.'\n    assert (self.parent is not None), str(self)\n    assert (new is not None)\n    if (not isinstance(new, list)):\n        new = [new]\n    l_children = []\n    found = False\n    for ch in self.parent.children:\n        if (ch is self):\n            assert (not found), (self.parent.children, self, new)\n            if (new is not None):\n                l_children.extend(new)\n            found = True\n        else:\n            l_children.append(ch)\n    assert found, (self.children, self, new)\n    self.parent.changed()\n    self.parent.children = l_children\n    for x in new:\n        x.parent = self.parent\n    self.parent = None\n", "label": 0}
{"function": "\n\ndef test_swap_SharedVariable(self):\n    i = T.iscalar()\n    x_list = theano.shared(value=numpy.random.rand(10).astype(config.floatX))\n    x = T.scalar('x')\n    y = theano.shared(value=1, name='y')\n    z = theano.shared(value=2, name='z')\n    m = theano.shared(value=0, name='m')\n    y_rpl = theano.shared(value=3, name='y_rpl')\n    z_rpl = theano.shared(value=4, name='z_rpl')\n    swap = {\n        y: y_rpl,\n        z: z_rpl,\n    }\n    map_SV = {\n        'y_rpl': y_rpl,\n        'z_rpl': z_rpl,\n    }\n    out = (((x + y) + z) + m)\n    second_time = False\n    for mode in ['FAST_RUN', 'FAST_COMPILE']:\n        ori = theano.function([i], [out], mode=mode, updates=[(z, (z + 1)), (m, (m + 2))], givens={\n            x: x_list[i],\n        })\n        cpy = ori.copy(swap=swap)\n        (ori(1), cpy(1), cpy(2))\n        if (not second_time):\n            assert (m.get_value() == 6)\n            assert (z.get_value() == 3)\n            assert (z_rpl.get_value() == 6)\n            assert (y_rpl.get_value() == 3)\n            assert (y.get_value() == 1)\n        elif second_time:\n            assert (m.get_value() == 12)\n            assert (z.get_value() == 4)\n            assert (z_rpl.get_value() == 8)\n            assert (y_rpl.get_value() == 3)\n        names = map_SV.keys()\n        for key in cpy.fn.storage_map:\n            if (key.name in names):\n                assert (map_SV[key.name].container.storage[0] == cpy.fn.storage_map[key][0])\n        second_time = True\n", "label": 1}
{"function": "\n\ndef main():\n    global imap_account\n    global routes\n    if (conf is None):\n        return 1\n    logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n    for routing in routings:\n        (methods, regex, app) = routing[:3]\n        if isinstance(methods, six.string_types):\n            methods = (methods,)\n        vars = (routing[3] if (len(routing) >= 4) else {\n            \n        })\n        routes.append((methods, re.compile(regex), app, vars))\n        log.info('Route {} openned'.format(regex[1:(- 1)]))\n    try:\n        imap_account = imap_cli.connect(**conf)\n        httpd = simple_server.make_server('127.0.0.1', 8000, router)\n        log.info('Serving on http://127.0.0.1:8000')\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        log.info('Interupt by user, exiting')\n    return 0\n", "label": 0}
{"function": "\n\ndef test_time_dep_bra():\n    b = TimeDepBra(0, t)\n    assert isinstance(b, TimeDepBra)\n    assert isinstance(b, BraBase)\n    assert isinstance(b, StateBase)\n    assert isinstance(b, QExpr)\n    assert (b.label == (Integer(0),))\n    assert (b.args == (Integer(0), t))\n    assert (b.time == t)\n    assert (b.dual_class() == TimeDepKet)\n    assert (b.dual == TimeDepKet(0, t))\n    k = TimeDepBra(x, 0.5)\n    assert (k.label == (x,))\n    assert (k.args == (x, sympify(0.5)))\n    assert (TimeDepBra() == TimeDepBra('psi', 't'))\n", "label": 1}
{"function": "\n\ndef __call__(self, request, *args, **kwargs):\n    '\\n        In case of the wrapped view being determined as a class (that it has\\n        a dispatch attribute) we return a new instance of the class with all\\n        view arguments passed to the dispatch method. The case of a view\\n        function things are much simpler, we just call the view function with\\n        the view arguments.\\n\\n        For debugging purposes we insert some additional information that is\\n        useful for view classes in the raised exception.\\n        '\n    try:\n        if isclass(self.view):\n            view = self.view()\n            if hasattr(view, 'dispatch'):\n                return view.dispatch(request, *args, **kwargs)\n        else:\n            view = self.view\n        if callable(view):\n            return view(request, *args, **kwargs)\n    except (Http404, PermissionDenied, SystemExit):\n        raise\n    except Exception as ex:\n        try:\n            (cls, e, trace) = sys.exc_info()\n            msg = ('%s in %s.%s: %s' % (cls.__name__, self.view.__module__, self.view.__name__, e))\n        except Exception:\n            raise ex\n        else:\n            raise UtkikException(msg).with_traceback(trace)\n    raise ImproperlyConfigured(('%s.%s does not define a view function or class view.' % (self.view.__module__, self.view.__name__)))\n", "label": 0}
{"function": "\n\ndef _safeio(self, callback):\n    if self.closed:\n        raise ValueError('I/O operation on closed file')\n    for i in range(self.retries):\n        try:\n            if (not self.stream):\n                self.stream = self._reconnect()\n            return callback(self.stream)\n        except (EOFError, IOError, OSError, socket.error):\n            if (i >= (self.retries - 1)):\n                raise\n            if self.stream:\n                self.stream.close()\n            self.stream = None\n            time.sleep(0.5)\n", "label": 0}
{"function": "\n\ndef main(args=None):\n    try:\n        if (args is None):\n            args = sys.argv[1:]\n        MonascaShell().main(args)\n    except Exception as e:\n        if (('--debug' in args) or ('-d' in args)):\n            raise\n        else:\n            print(e, file=sys.stderr)\n        sys.exit(1)\n", "label": 0}
{"function": "\n\ndef run(self):\n    arguments = self.arguments\n    wrong_sorted_files = False\n    arguments['check'] = True\n    for path in self.distribution_files():\n        for python_file in glob.iglob(os.path.join(path, '*.py')):\n            try:\n                incorrectly_sorted = SortImports(python_file, **arguments).incorrectly_sorted\n                if incorrectly_sorted:\n                    wrong_sorted_files = True\n            except IOError as e:\n                print('WARNING: Unable to parse file {0} due to {1}'.format(file_name, e))\n    if wrong_sorted_files:\n        exit(1)\n", "label": 0}
{"function": "\n\ndef _Push(self, opt, todo):\n    have_errors = False\n    for branch in todo:\n        try:\n            changes = branch.project.UncommitedFiles()\n            if changes:\n                sys.stdout.write(('Uncommitted changes in ' + branch.project.name))\n                sys.stdout.write(' (did you forget to amend?):\\n')\n                sys.stdout.write(('\\n'.join(changes) + '\\n'))\n                sys.stdout.write('Continue pushing? (y/N) ')\n                a = sys.stdin.readline().strip().lower()\n                if (a not in ('y', 'yes', 't', 'true', 'on')):\n                    print('skipping push', file=sys.stderr)\n                    branch.uploaded = False\n                    branch.error = 'User aborted'\n                    continue\n            destination = (opt.dest_branch or branch.project.dest_branch)\n            merge_branch = self._GetMergeBranch(branch.project)\n            if destination:\n                full_dest = ('refs/heads/%s' % destination)\n                if ((not opt.dest_branch) and merge_branch and (merge_branch != full_dest)):\n                    print(('merge branch %s does not match destination branch %s' % (merge_branch, full_dest)))\n                    print('skipping push.')\n                    print(('Please use `--destination %s` if this is intentional' % destination))\n                    branch.uploaded = False\n                    continue\n            self.Push(branch, dest_branch=destination)\n            branch.uploaded = True\n        except UploadError as e:\n            branch.error = e\n            branch.uploaded = False\n            have_errors = True\n    print(file=sys.stderr)\n    print('----------------------------------------------------------------------', file=sys.stderr)\n    if have_errors:\n        for branch in todo:\n            if (not branch.uploaded):\n                if (len(str(branch.error)) <= 30):\n                    fmt = ' (%s)'\n                else:\n                    fmt = '\\n       (%s)'\n                print((('[FAILED] %-15s %-15s' + fmt) % ((branch.project.relpath + '/'), branch.name, str(branch.error))), file=sys.stderr)\n        print()\n    for branch in todo:\n        if branch.uploaded:\n            print(('[OK    ] %-15s %s' % ((branch.project.relpath + '/'), branch.name)), file=sys.stderr)\n    if have_errors:\n        sys.exit(1)\n", "label": 1}
{"function": "\n\ndef download_file(url, chunk_size=(100 * 1024)):\n    ' Helper method to download a file displaying a progress bar '\n    print('Fetching:', url)\n    file_content = None\n    progressbar = None\n    if (sys.version_info.major <= 2):\n        from rplibs.progressbar import FileTransferSpeed, ETA, ProgressBar, Percentage\n        from rplibs.progressbar import Bar\n        widgets = ['\\tDownloading: ', FileTransferSpeed(), ' ', Bar(), Percentage(), '   ', ETA()]\n        file_content = []\n        bytes_read = 0\n        try:\n            usock = urllib.request.urlopen(url)\n            file_size = int(usock.headers.get('Content-Length', 10000000000.0))\n            print('File size is', round((file_size / (1024 ** 2)), 2), 'MB')\n            progressbar = ProgressBar(widgets=widgets, maxval=file_size).start()\n            while True:\n                data = usock.read(chunk_size)\n                file_content.append(data)\n                bytes_read += len(data)\n                progressbar.update(bytes_read)\n                if (not data):\n                    break\n            usock.close()\n        except Exception:\n            print('ERROR: Could not fetch', url, '!', file=sys.stderr)\n            raise\n    else:\n        print('Downloading .. (progressbar disabled due to python 3 build)')\n        try:\n            usock = urllib.request.urlopen(url)\n            file_content = []\n            while True:\n                data = usock.read(chunk_size)\n                file_content.append(data)\n                if (not data):\n                    break\n            usock.close()\n        except Exception:\n            print('ERROR: Could not fetch', url, '!', file=sys.stderr)\n            raise\n    if progressbar:\n        progressbar.finish()\n    return binary_type().join(file_content)\n", "label": 0}
{"function": "\n\ndef rangeRingsFromList(centerFC, rangeList, distanceUnits, numRadials, outputRingFeatures, outputRadialFeatures, sr):\n    ' Make range ring features from a center, and list of distances '\n    try:\n        if ((sr == '#') or (sr == '') or (sr == None)):\n            sr = srDefault\n        rm = RingMaker(centerFC, rangeList, distanceUnits, sr)\n        numCenterPoints = arcpy.GetCount_management(centerFC).getOutput(0)\n        numRingsPerCenter = len(rangeList)\n        totalNumRings = (int(numCenterPoints) * int(numRingsPerCenter))\n        totalNumRadials = (int(numCenterPoints) * int(numRadials))\n        arcpy.AddMessage((((((('Making rings ' + str(totalNumRings)) + ' (') + str(numRingsPerCenter)) + ' for ') + str(numCenterPoints)) + ' centers)...'))\n        rm.makeRingsFromDistances()\n        outRings = rm.saveRingsAsFeatures(outputRingFeatures)\n        arcpy.AddMessage((((((('Making radials ' + str(totalNumRadials)) + ' (') + str(numRadials)) + ' for ') + str(numCenterPoints)) + ' centers)...'))\n        rm.makeRadials(numRadials)\n        outRadials = rm.saveRadialsAsFeatures(outputRadialFeatures)\n        return [outRings, outRadials]\n    except arcpy.ExecuteError:\n        msgs = arcpy.GetMessages()\n        arcpy.AddError(msgs)\n        print(msgs)\n    except:\n        tb = sys.exc_info()[2]\n        tbinfo = traceback.format_tb(tb)[0]\n        pymsg = ((('PYTHON ERRORS:\\nTraceback info:\\n' + tbinfo) + '\\nError Info:\\n') + str(sys.exc_info()[1]))\n        msgs = (('ArcPy ERRORS:\\n' + arcpy.GetMessages()) + '\\n')\n        arcpy.AddError(pymsg)\n        arcpy.AddError(msgs)\n        print((pymsg + '\\n'))\n        print(msgs)\n", "label": 0}
{"function": "\n\ndef execute(self):\n    context = Gaffer.Context.current()\n    fileName = self['fileName'].getValue()\n    directory = os.path.dirname(fileName)\n    if directory:\n        try:\n            os.makedirs(directory)\n        except OSError:\n            if (not os.path.isdir(directory)):\n                raise\n    text = self.__processText(context)\n    with file(fileName, self['mode'].getValue()) as f:\n        f.write(text)\n", "label": 0}
{"function": "\n\ndef diff(self, report_a, report_b):\n    '\\n        Generate a diff for two data reports.\\n        '\n    arguments = (GLOBAL_ARGUMENTS + ['run_date'])\n    output = OrderedDict([('a', OrderedDict([(arg, report_a[arg]) for arg in arguments])), ('b', OrderedDict([(arg, report_b[arg]) for arg in arguments])), ('queries', [])])\n    output['a']\n    for query_a in report_a['queries']:\n        for query_b in report_b['queries']:\n            if (query_a['config'] == query_b['config']):\n                diff = OrderedDict()\n                diff['config'] = query_a['config']\n                diff['data_types'] = query_a['data_types']\n                diff['data'] = OrderedDict()\n                for (metric, values) in query_a['data'].items():\n                    data_type = diff['data_types'][metric]\n                    diff['data'][metric] = OrderedDict()\n                    total_a = values['total']\n                    total_b = query_b['data'][metric]['total']\n                    for (label, value) in values.items():\n                        a = value\n                        try:\n                            b = query_b['data'][metric][label]\n                        except KeyError:\n                            continue\n                        change = (b - a)\n                        percent_change = ((float(change) / a) if (a > 0) else None)\n                        percent_a = ((float(a) / total_a) if (total_a > 0) else None)\n                        percent_b = ((float(b) / total_b) if (total_b > 0) else None)\n                        if ((label == 'total') or (data_type == 'TIME') or (percent_a is None) or (percent_b is None)):\n                            point_change = None\n                        else:\n                            point_change = (percent_b - percent_a)\n                        diff['data'][metric][label] = OrderedDict([('change', change), ('percent_change', percent_change), ('point_change', point_change)])\n                output['queries'].append(diff)\n        query_b = report_b['queries']\n    return output\n", "label": 1}
{"function": "\n\n@blueprint.route('/bundle/<bundle_name>/asset/<int:asset_index>_v<asset_hash>.<bundle_extension>')\ndef render_asset(bundle_name, bundle_extension, asset_index, asset_hash):\n    ' Render a single source from an asset.\\n\\n    Args:\\n        bundle_name: name of the bundle to render\\n        bundle_extension: file extension for the bundle\\n        asset_index: index of the asset in `Bundle.assets`\\n        asset_hash: calculated hash from asset content\\n    '\n    compressor = current_app.extensions['compressor']\n    try:\n        bundle = compressor.get_bundle(bundle_name)\n    except CompressorException:\n        abort(404)\n    try:\n        asset = bundle.assets[asset_index]\n    except IndexError:\n        abort(404)\n    if (asset.hash != asset_hash):\n        abort(404)\n    if (bundle.extension != bundle_extension):\n        abort(404)\n    content = asset.content\n    return Response(content, mimetype=bundle.mimetype)\n", "label": 0}
{"function": "\n\ndef test_compute():\n    a = (delayed(1) + 5)\n    b = (a + 1)\n    c = (a + 2)\n    assert (compute(b, c) == (7, 8))\n    assert (compute(b) == (7,))\n    assert (compute([a, b], c) == ([6, 7], 8))\n", "label": 0}
{"function": "\n\ndef compute(self):\n    port_object = None\n    if self.has_input('SetInputConnection0'):\n        ic = self.get_input('SetInputConnection0')\n        if hasattr(ic, 'vtkInstance'):\n            ic = ic.vtkInstance\n        producer = ic.GetProducer()\n        try:\n            port_object = producer.GetOutput()\n        except AttributeError:\n            raise ModuleError(self, 'expected a module that supports GetOutput')\n    elif self.has_input('SetInput'):\n        port_object = self.get_input('SetInput')\n        if hasattr(port_object, 'vtkInstance'):\n            port_object = port_object.vtkInstance\n    if port_object:\n        self.auto_set_results(port_object)\n", "label": 0}
{"function": "\n\n@common.check_cells_enabled\ndef sync_instances(self, req, body):\n    'Tell all cells to sync instance info.'\n    context = req.environ['nova.context']\n    authorize(context)\n    authorize(context, action='sync_instances')\n    project_id = body.pop('project_id', None)\n    deleted = body.pop('deleted', False)\n    updated_since = body.pop('updated_since', None)\n    if body:\n        msg = _(\"Only 'updated_since', 'project_id' and 'deleted' are understood.\")\n        raise exc.HTTPBadRequest(explanation=msg)\n    if isinstance(deleted, six.string_types):\n        try:\n            deleted = strutils.bool_from_string(deleted, strict=True)\n        except ValueError as err:\n            raise exc.HTTPBadRequest(explanation=six.text_type(err))\n    if updated_since:\n        try:\n            timeutils.parse_isotime(updated_since)\n        except ValueError:\n            msg = _('Invalid changes-since value')\n            raise exc.HTTPBadRequest(explanation=msg)\n    self.cells_rpcapi.sync_instances(context, project_id=project_id, updated_since=updated_since, deleted=deleted)\n", "label": 0}
{"function": "\n\ndef __init__(self, name, *fields, **kwargs):\n    self.name = name\n    self.fields = fields\n    assert (kwargs.get('join') is not 'and'), 'And joiner is not implemented!'\n    self.query_joiner = 'or'\n    self.search_in_field = kwargs.get('search_in_field')\n    self.search_in_values = kwargs.get('search_in_values')\n    assert self.search_in_field, 'Please specify search_in kwargs.'\n    assert self.search_in_values, 'Please specify a search_in_values kwarg.'\n    if (self.query_joiner not in ['and', 'or']):\n        raise TypeError(('Unknown query joiner: %s' % self.query_joiner))\n", "label": 0}
{"function": "\n\ndef handle_text(self):\n    '\\n        Takes care of converting body text to unicode, if its text at all.\\n        Sets self.original_encoding to original char encoding, and converts body\\n        to unicode if possible. Must come after handle_compression, and after\\n        self.mediaType is valid.\\n        '\n    self.encoding = None\n    if (self.mediaType and ((self.mediaType.type == 'text') or ((self.mediaType.type == 'application') and ('xml' in self.mediaType.subtype)))):\n        if ('charset' in self.mediaType.params):\n            override_encodings = [self.mediaType.params['charset']]\n        else:\n            override_encodings = []\n        if (self.body != ''):\n            if UnicodeDammit:\n                dammit = UnicodeDammit(self.body, override_encodings)\n                if dammit.unicode:\n                    self.text = dammit.unicode\n                    self.originalEncoding = dammit.originalEncoding\n                else:\n                    pass\n            else:\n                u = None\n                for e in (override_encodings + ['utf8', 'iso-8859-1']):\n                    try:\n                        u = self.body.decode(e, 'strict')\n                        self.originalEncoding = e\n                        break\n                    except UnicodeError:\n                        pass\n                if (not u):\n                    u = self.body.decode('utf8', 'replace')\n                    self.originalEncoding = None\n                self.text = (u or None)\n    else:\n        self.text = b64encode(self.body)\n        self.encoding = 'base64'\n", "label": 1}
{"function": "\n\n@classmethod\ndef connection_pool(cls, **kwargs):\n    pool_key = cls.key_for_kwargs(kwargs)\n    if (pool_key in cls.pools):\n        return cls.pools[pool_key]\n    location = kwargs.get('location', None)\n    if (not location):\n        raise ImproperlyConfigured('no `location` key on connection kwargs')\n    params = {\n        'connection_class': Connection,\n        'db': kwargs.get('db', 0),\n        'password': kwargs.get('password', None),\n    }\n    if location.startswith('unix:'):\n        params['connection_class'] = UnixDomainSocketConnection\n        params['path'] = location[5:]\n    else:\n        try:\n            (params['host'], params['port']) = location.split(':')\n            params['port'] = int(params['port'])\n        except ValueError:\n            raise ImproperlyConfigured('Invalid `location` key syntax on connection kwargs')\n    cls.pools[pool_key] = RedisConnectionPool(**params)\n    return cls.pools[pool_key]\n", "label": 0}
{"function": "\n\ndef _maxbansParser(s):\n    if (':' in s):\n        modes = ''\n        limits = []\n        pairs = s.split(',')\n        for pair in pairs:\n            (mode, limit) = pair.split(':', 1)\n            modes += mode\n            limits += ((int(limit),) * len(mode))\n        d = dict(list(zip(modes, limits)))\n        assert ('b' in d)\n        return d['b']\n    else:\n        return int(s)\n", "label": 0}
{"function": "\n\ndef auth(self, irc, msg, args, url):\n    '<url>\\n\\n            Check the GPG signature at the <url> and authenticates you if\\n            the key used is associated to a user.'\n    self._expire_tokens()\n    content = utils.web.getUrl(url)\n    if (minisix.PY3 and isinstance(content, bytes)):\n        content = content.decode()\n    match = self._auth_re.search(content)\n    if (not match):\n        irc.error(_('Signature or token not found.'), Raise=True)\n    data = match.group(0)\n    token = match.group(1)\n    if (token not in self._tokens):\n        irc.error(_('Unknown token. It may have expired before you submit it.'), Raise=True)\n    if (self._tokens[token][0] != msg.prefix):\n        irc.error(_('Your hostname/nick changed in the process. Authentication aborted.'), Raise=True)\n    verified = gpg.keyring.verify(data)\n    if (verified and verified.valid):\n        keyid = verified.pubkey_fingerprint[(- 16):]\n        (prefix, expiry) = self._tokens.pop(token)\n        found = False\n        for (id, user) in ircdb.users.items():\n            if (keyid in [x[(- len(keyid)):] for x in user.gpgkeys]):\n                try:\n                    user.addAuth(msg.prefix)\n                except ValueError:\n                    irc.error(_(\"Your secure flag is true and your hostmask doesn't match any of your known hostmasks.\"), Raise=True)\n                ircdb.users.setUser(user, flush=False)\n                irc.reply((_('You are now authenticated as %s.') % user.name))\n                return\n        irc.error(_('Unknown GPG key.'), Raise=True)\n    else:\n        irc.error(_('Signature could not be verified. Make sure this is a valid GPG signature and the URL is valid.'))\n", "label": 1}
{"function": "\n\ndef __init__(self, short=None, long=None, argcount=0, value=False):\n    assert (argcount in (0, 1))\n    (self.short, self.long) = (short, long)\n    (self.argcount, self.value) = (argcount, value)\n    self.value = (None if ((value is False) and argcount) else value)\n", "label": 0}
{"function": "\n\ndef closeUnusedFiles(transport):\n    import os, sys\n    notouch = transport.protectedFileNumList()\n    for each in [sys.stdin, sys.stderr, sys.stdout]:\n        try:\n            notouch.append(each.fileno())\n        except AttributeError:\n            pass\n    for fdnum in range(3, 255):\n        if (fdnum not in notouch):\n            try:\n                os.close(fdnum)\n            except OSError:\n                pass\n", "label": 0}
{"function": "\n\ndef get_context(self, bot, update, **kwargs):\n    queryset = self.get_queryset()\n    if (not self.slug_field):\n        raise AttributeError(('Generic detail view %s must be called with a slug.' % self.__class__.__name__))\n    slug_field = self.get_slug_field(**kwargs)\n    slug = self.get_slug(**kwargs)\n    if slug:\n        try:\n            object = queryset.get(**{\n                slug_field: slug,\n            })\n        except FieldError:\n            raise FieldError(('Field %s not in valid. Review slug_field' % slug_field))\n        except ObjectDoesNotExist:\n            object = None\n    else:\n        object = None\n    context = {\n        'context_object_name': object,\n    }\n    if self.context_object_name:\n        context[self.context_object_name] = object\n    return context\n", "label": 0}
{"function": "\n\ndef test_do_bounce_when_apps_to_kill(self):\n    fake_bounce_func_return = {\n        'create_app': False,\n        'tasks_to_drain': [],\n    }\n    fake_bounce_func = mock.create_autospec(bounce_lib.brutal_bounce, return_value=fake_bounce_func_return)\n    fake_config = {\n        'instances': 5,\n    }\n    fake_new_app_running = True\n    fake_happy_new_tasks = ['fake_one', 'fake_two', 'fake_three']\n    fake_old_app_live_happy_tasks = {\n        'fake_app_to_kill_1': set(),\n    }\n    fake_old_app_live_unhappy_tasks = {\n        'fake_app_to_kill_1': set(),\n    }\n    fake_old_app_draining_tasks = {\n        'fake_app_to_kill_1': set(),\n    }\n    fake_service = 'fake_service'\n    fake_serviceinstance = 'fake_service.fake_instance'\n    self.fake_cluster = 'fake_cluster'\n    fake_instance = 'fake_instance'\n    fake_bounce_method = 'fake_bounce_method'\n    fake_drain_method = mock.Mock()\n    fake_marathon_jobid = 'fake.marathon.jobid'\n    fake_client = mock.create_autospec(marathon.MarathonClient)\n    expected_new_task_count = (fake_config['instances'] - len(fake_happy_new_tasks))\n    with contextlib.nested(mock.patch('paasta_tools.setup_marathon_job._log', autospec=True), mock.patch('paasta_tools.setup_marathon_job.bounce_lib.create_marathon_app', autospec=True), mock.patch('paasta_tools.setup_marathon_job.bounce_lib.kill_old_ids', autospec=True)) as (mock_log, mock_create_marathon_app, mock_kill_old_ids):\n        setup_marathon_job.do_bounce(bounce_func=fake_bounce_func, drain_method=fake_drain_method, config=fake_config, new_app_running=fake_new_app_running, happy_new_tasks=fake_happy_new_tasks, old_app_live_happy_tasks=fake_old_app_live_happy_tasks, old_app_live_unhappy_tasks=fake_old_app_live_unhappy_tasks, old_app_draining_tasks=fake_old_app_draining_tasks, service=fake_service, bounce_method=fake_bounce_method, serviceinstance=fake_serviceinstance, cluster=self.fake_cluster, instance=fake_instance, marathon_jobid=fake_marathon_jobid, client=fake_client, soa_dir='fake_soa_dir')\n        assert (mock_log.call_count == 3)\n        first_logged_line = mock_log.mock_calls[0][2]['line']\n        assert (('%s new tasks' % expected_new_task_count) in first_logged_line)\n        second_logged_line = mock_log.mock_calls[1][2]['line']\n        assert (('removing old unused apps with app_ids: %s' % 'fake_app_to_kill_1') in second_logged_line)\n        assert (mock_create_marathon_app.call_count == 0)\n        assert (fake_client.kill_task.call_count == len(fake_bounce_func_return['tasks_to_drain']))\n        assert (mock_kill_old_ids.call_count == 1)\n        third_logged_line = mock_log.mock_calls[2][2]['line']\n        assert (('%s bounce on %s finish' % (fake_bounce_method, fake_serviceinstance)) in third_logged_line)\n        assert (('Now running %s' % fake_marathon_jobid) in third_logged_line)\n", "label": 0}
{"function": "\n\ndef test_multiple_numbers(self):\n    h = Hashids()\n    assert (h.encode(683, 94108, 123, 5) == 'vJvi7On9cXGtD')\n    assert (h.encode(1, 2, 3) == 'o2fXhV')\n    assert (h.encode(2, 4, 6) == 'xGhmsW')\n    assert (h.encode(99, 25) == '3lKfD')\n", "label": 0}
{"function": "\n\ndef test_include_1():\n    sub_ffi = FFI()\n    sub_ffi.cdef('static const int k2 = 121212;')\n    sub_ffi.include(original_ffi)\n    assert ('macro FOOBAR' in original_ffi._parser._declarations)\n    assert ('macro FOOBAZ' in original_ffi._parser._declarations)\n    sub_ffi.set_source('re_python_pysrc', None)\n    sub_ffi.emit_python_code(str(tmpdir.join('_re_include_1.py')))\n    if (sys.version_info[:2] >= (3, 3)):\n        import importlib\n        importlib.invalidate_caches()\n    from _re_include_1 import ffi\n    assert (ffi.integer_const('FOOBAR') == (- 42))\n    assert (ffi.integer_const('FOOBAZ') == (- 43))\n    assert (ffi.integer_const('k2') == 121212)\n    lib = ffi.dlopen(extmod)\n    assert (lib.FOOBAR == (- 42))\n    assert (lib.FOOBAZ == (- 43))\n    assert (lib.k2 == 121212)\n    p = ffi.new('bar_t *', [5, b'foobar'])\n    assert (p.a[4] == ord('a'))\n", "label": 1}
{"function": "\n\ndef test_Distinct():\n    t = Symbol('t', 'var * {name: string, amount: int32}')\n    r = distinct(t['name'])\n    print(r.dshape)\n    assert (r.dshape == dshape('var * string'))\n    assert (r._name == 'name')\n    r = t.distinct()\n    assert (r.dshape == t.dshape)\n", "label": 0}
{"function": "\n\ndef process_request(self, request):\n    if request.user.is_authenticated():\n        try:\n            profile = util.get_pybb_profile(request.user)\n        except ObjectDoesNotExist:\n            from pybb.signals import user_saved\n            user_saved(request.user, created=True)\n            profile = util.get_pybb_profile(request.user)\n        if (not profile.language):\n            profile.language = translation.get_language_from_request(request)\n            profile.save()\n        request.session['django_language'] = profile.language\n        translation.activate(profile.language)\n        request.LANGUAGE_CODE = translation.get_language()\n", "label": 0}
{"function": "\n\ndef remind(self, card, force=False, last_state=None):\n    last_reminder_sent = self.last_reminded(card)\n    if ((not force) and last_reminder_sent and ((datetime.now() - last_reminder_sent.sent_date).days < 1)):\n        app.logger.info(('%s Reminder for card %s has already been sent today. Last sent: %s' % (self.type, card.last_four, last_reminder_sent)))\n        return\n    app.logger.info(('Begin %s reminder check for %s' % (self.type, card.last_four)))\n    if (not self.check_reminder(card, last_state=last_state)):\n        app.logger.info(('Card %s does not need a %s reminder' % (card.last_four, self.type)))\n        return\n    from flask import render_template\n    from breezeminder.app import context, filters\n    from breezeminder.models.messaging import Messaging\n    app.logger.info(('Reminding user %s of %s at or below threshold' % (self.owner.id, self.type)))\n    subject = ('[BreezeMinder] %s reminder' % self.type.name)\n    messages = []\n    context = {\n        'card': card,\n        'reminder': self,\n        'last_state': last_state,\n    }\n    if (self.type == 'EXP'):\n        delta = self._exp_delta()\n        expiring = {\n            'card': self._will_expire(card.expiration_date, delta),\n        }\n        expiring['products'] = [p for p in card.products if self._will_expire(p.expiration_date, delta)]\n        context['expiring'] = expiring\n    if self.send_email:\n        template = ('messages/email/reminders/%s.html' % self.type.key.lower())\n        with app.test_request_context():\n            content = render_template(template, **context)\n        Messaging.objects.create(recipients=[self.owner.email], sender=app.config['DEFAULT_MAIL_SENDER'], subject=subject, message=content)\n    if self.send_sms:\n        recipients = []\n        if self.owner.can_receive_sms:\n            recipients.append(self.owner.cell_phone.sms_address)\n        if (len(recipients) > 0):\n            template = ('messages/sms/reminders/%s.html' % self.type.key.lower())\n            with app.test_request_context():\n                content = render_template(template, **context)\n            content = re.sub('\\\\s{2,}', ' ', content)\n            subject = ('%s reminder' % self.type.name)\n            Messaging.objects.create(recipients=recipients, sender=app.config['DEFAULT_MAIL_SENDER'], subject=subject.upper(), message=content, is_plain=True)\n    try:\n        with app.test_request_context():\n            web_message = render_template(('messages/web/reminders/%s.html' % self.type.key.lower()), **context)\n        ReminderHistory.objects.create(reminder=self, card=card, message=web_message, sent_date=datetime.now(), owner=self.owner)\n    except:\n        app.logger.exception('Could not save a reminder history object')\n    self.save()\n", "label": 1}
{"function": "\n\ndef initialize_locksets(self):\n    log.debug('initializing locksets')\n    v = self.sign(VoteBlock(0, 0, self.chainservice.chain.genesis.hash))\n    self.add_vote(v)\n    head_proposal = self.load_proposal(self.head.hash)\n    if head_proposal:\n        assert (head_proposal.blockhash == self.head.hash)\n        for v in head_proposal.signing_lockset:\n            self.add_vote(v)\n        assert self.heights[(self.head.header.number - 1)].has_quorum\n    last_committing_lockset = self.load_last_committing_lockset()\n    if last_committing_lockset:\n        assert (last_committing_lockset.has_quorum == self.head.hash)\n        for v in last_committing_lockset.votes:\n            self.add_vote(v)\n        assert self.heights[self.head.header.number].has_quorum\n    else:\n        assert (self.head.header.number == 0)\n    assert self.highest_committing_lockset\n    assert self.last_committing_lockset\n    assert self.last_valid_lockset\n", "label": 1}
{"function": "\n\ndef distrib_id():\n    '\\n    Get the OS distribution ID.\\n\\n    Returns a string such as ``\"Debian\"``, ``\"Ubuntu\"``, ``\"RHEL\"``,\\n    ``\"CentOS\"``, ``\"SLES\"``, ``\"Fedora\"``, ``\"Arch\"``, ``\"Gentoo\"``,\\n    ``\"SunOS\"``...\\n\\n    Example::\\n\\n        from fabtools.system import distrib_id\\n\\n        if distrib_id() != \\'Debian\\':\\n            abort(u\"Distribution is not supported\")\\n\\n    '\n    with settings(hide('running', 'stdout')):\n        kernel = run('uname -s')\n        if (kernel == 'Linux'):\n            if is_file('/usr/bin/lsb_release'):\n                id_ = run('lsb_release --id --short')\n                if (id in ['arch', 'Archlinux']):\n                    id_ = 'Arch'\n                return id_\n            elif is_file('/etc/debian_version'):\n                return 'Debian'\n            elif is_file('/etc/fedora-release'):\n                return 'Fedora'\n            elif is_file('/etc/arch-release'):\n                return 'Arch'\n            elif is_file('/etc/redhat-release'):\n                release = run('cat /etc/redhat-release')\n                if release.startswith('Red Hat Enterprise Linux'):\n                    return 'RHEL'\n                elif release.startswith('CentOS'):\n                    return 'CentOS'\n                elif release.startswith('Scientific Linux'):\n                    return 'SLES'\n            elif is_file('/etc/gentoo-release'):\n                return 'Gentoo'\n        elif (kernel == 'SunOS'):\n            return 'SunOS'\n", "label": 1}
{"function": "\n\ndef populate(self):\n    self.page = get_page_draft(self.request.current_page)\n    if (not self.page):\n        return\n    if get_cms_setting('PERMISSION'):\n        has_global_current_page_change_permission = has_page_change_permission(self.request)\n    else:\n        has_global_current_page_change_permission = False\n    can_change = (self.request.current_page and self.request.current_page.has_change_permission(self.request))\n    if (has_global_current_page_change_permission or can_change):\n        try:\n            mypageextension = MyPageExtension.objects.get(extended_object_id=self.page.id)\n        except MyPageExtension.DoesNotExist:\n            mypageextension = None\n        try:\n            if mypageextension:\n                url = admin_reverse('extensionapp_mypageextension_change', args=(mypageextension.pk,))\n            else:\n                url = (admin_reverse('extensionapp_mypageextension_add') + ('?extended_object=%s' % self.page.pk))\n        except NoReverseMatch:\n            pass\n        else:\n            not_edit_mode = (not self.toolbar.edit_mode)\n            current_page_menu = self.toolbar.get_or_create_menu('page')\n            current_page_menu.add_modal_item(_('Page Extension'), url=url, disabled=not_edit_mode)\n", "label": 0}
{"function": "\n\ndef _updateFromPlug(self):\n    plug = self.getPlug()\n    if (plug is not None):\n        with self.getContext():\n            try:\n                value = plug.getValue()\n            except:\n                value = None\n        if (value is not None):\n            with Gaffer.BlockedConnection(self.__valueChangedConnection):\n                self.__numericWidget.setValue(value)\n        self.__numericWidget.setErrored((value is None))\n        animated = Gaffer.Animation.isAnimated(plug)\n        widgetAnimated = (GafferUI._Variant.fromVariant(self.__numericWidget._qtWidget().property('gafferAnimated')) or False)\n        if (widgetAnimated != animated):\n            self.__numericWidget._qtWidget().setProperty('gafferAnimated', GafferUI._Variant.toVariant(bool(animated)))\n            self.__numericWidget._repolish()\n    self.__numericWidget.setEditable(self._editable(canEditAnimation=True))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef format_percent(ratio, denom=1, unit=False):\n    try:\n        ratio /= float(denom)\n    except ZeroDivisionError:\n        ratio = 0\n    if (round(ratio, 2) >= 1):\n        precision = 0\n    elif (round(ratio, 2) >= 0.1):\n        precision = 1\n    else:\n        precision = 2\n    string = (('{:.' + str(precision)) + 'f}').format((ratio * 100))\n    if unit:\n        return (string + '%')\n    else:\n        return string\n", "label": 0}
{"function": "\n\ndef update(self, req, id, body):\n    context = req.environ['nova.context']\n    authorize(context)\n    try:\n        utils.check_string_length(id, 'quota_class_name', min_length=1, max_length=255)\n    except exception.InvalidInput as e:\n        raise webob.exc.HTTPBadRequest(explanation=e.format_message())\n    quota_class = id\n    bad_keys = []\n    if (not self.is_valid_body(body, 'quota_class_set')):\n        msg = _('quota_class_set not specified')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    quota_class_set = body['quota_class_set']\n    for key in quota_class_set.keys():\n        if (key not in self.supported_quotas):\n            bad_keys.append(key)\n            continue\n        try:\n            body['quota_class_set'][key] = utils.validate_integer(body['quota_class_set'][key], key, max_value=db.MAX_INT)\n        except exception.InvalidInput as e:\n            raise webob.exc.HTTPBadRequest(explanation=e.format_message())\n    if bad_keys:\n        msg = (_('Bad key(s) %s in quota_set') % ','.join(bad_keys))\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        nova.context.require_admin_context(context)\n    except exception.AdminRequired:\n        raise webob.exc.HTTPForbidden()\n    for (key, value) in quota_class_set.items():\n        try:\n            db.quota_class_update(context, quota_class, key, value)\n        except exception.QuotaClassNotFound:\n            db.quota_class_create(context, quota_class, key, value)\n    values = QUOTAS.get_class_quotas(context, quota_class)\n    return self._format_quota_set(None, values)\n", "label": 0}
{"function": "\n\ndef test_output_skip(capsys, dst):\n    render(dst)\n    (out, err) = capsys.readouterr()\n    render(dst, quiet=False, skip=True)\n    (out, err) = capsys.readouterr()\n    print(out)\n    assert re.search('conflict[^\\\\s]+  config\\\\.py', out)\n    assert re.search('skip[^\\\\s]+  config\\\\.py', out)\n    assert re.search('identical[^\\\\s]+  setup\\\\.py', out)\n    assert re.search('identical[^\\\\s]+  doc/images/nslogo\\\\.gif', out)\n", "label": 0}
{"function": "\n\ndef __call__(self, parser, namespace, value, option_string=None):\n    try:\n        assert (self.dest == 'spam'), ('dest: %s' % self.dest)\n        assert (option_string == '-s'), ('flag: %s' % option_string)\n        expected_ns = NS(spam=0.25)\n        if (value in [0.125, 0.625]):\n            expected_ns.badger = 2\n        elif (value in [2.0]):\n            expected_ns.badger = 84\n        else:\n            raise AssertionError(('value: %s' % value))\n        assert (expected_ns == namespace), ('expected %s, got %s' % (expected_ns, namespace))\n    except AssertionError:\n        e = sys.exc_info()[1]\n        raise ArgumentParserError(('opt_action failed: %s' % e))\n    setattr(namespace, 'spam', value)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    for (name, value) in self._iter_contents():\n        if (key == name):\n            break\n    else:\n        raise KeyError(('Folder entry %s not found' % repr(key)))\n    if (value == '/'):\n        qname = quote(name)\n        child_ls = self.sub_tree[(qname + '.ls')]\n        try:\n            child_sub = self.sub_tree[(qname + '.sub')]\n        except KeyError:\n            child_sub = self.sub_tree.new_tree((qname + '.sub'))\n            self.storage._autocommit()\n        return StorageDir(name, child_ls, child_sub, ((self.path + name) + '/'), self.storage, self)\n    else:\n        inode = self.storage.get_inode(value)\n        return StorageFile(name, inode, self)\n", "label": 0}
{"function": "\n\ndef get_image_service(image_href, client=None, version=1, context=None):\n    'Get image service instance to download the image.\\n\\n    :param image_href: String containing href to get image service for.\\n    :param client: Glance client to be used for download, used only if\\n        image_href is Glance href.\\n    :param version: Version of Glance API to use, used only if image_href is\\n        Glance href.\\n    :param context: request context, used only if image_href is Glance href.\\n    :raises: exception.ImageRefValidationFailed if no image service can\\n        handle specified href.\\n    :returns: Instance of an image service class that is able to download\\n        specified image.\\n    '\n    scheme = urlparse.urlparse(image_href).scheme.lower()\n    try:\n        cls = protocol_mapping[(scheme or 'glance')]\n    except KeyError:\n        raise exception.ImageRefValidationFailed(image_href=image_href, reason=(_('Image download protocol %s is not supported.') % scheme))\n    if (cls == GlanceImageService):\n        return cls(client, version, context)\n    return cls()\n", "label": 0}
{"function": "\n\ndef domain_recipients_valid(domain_recipients=[]):\n    'Confirm that the first email recipient @smtp_server_domain could correspond to a valid project (i.e., it a new project or an int) and return it'\n    result = None\n    try:\n        if ((domain_recipients[0] in action_mailboxes.keys()) or (domain_recipients[0] in pass_through_mailboxes) or (default_mailbox is not None)):\n            result = domain_recipients[0]\n    except IndexError:\n        pass\n    return result\n", "label": 0}
{"function": "\n\ndef _LoadArtifactsFromDatastore(self, source_urns=None, token=None, overwrite_if_exists=True):\n    'Load artifacts from the data store.'\n    if (token is None):\n        token = access_control.ACLToken(username='GRRArtifactRegistry', reason='Managing Artifacts.')\n    loaded_artifacts = []\n    for artifact_coll_urn in (source_urns or []):\n        with aff4.FACTORY.Create(artifact_coll_urn, aff4_type='RDFValueCollection', token=token, mode='rw') as artifact_coll:\n            for artifact_value in artifact_coll:\n                self.RegisterArtifact(artifact_value, source=('datastore:%s' % artifact_coll_urn), overwrite_if_exists=overwrite_if_exists)\n                loaded_artifacts.append(artifact_value)\n                logging.debug('Loaded artifact %s from %s', artifact_value.name, artifact_coll_urn)\n    revalidate = True\n    while revalidate:\n        revalidate = False\n        for artifact_obj in loaded_artifacts[:]:\n            try:\n                artifact_obj.Validate()\n            except ArtifactDefinitionError as e:\n                logging.error('Artifact %s did not validate: %s', artifact_obj.name, e)\n                artifact_obj.error_message = utils.SmartStr(e)\n                loaded_artifacts.remove(artifact_obj)\n                revalidate = True\n", "label": 0}
{"function": "\n\ndef kmeans_init(X, Y, n_labels, n_hidden_states, latent_node_features=False):\n    all_feats = []\n    for (x, y) in zip(X, Y):\n        if (len(x) == 3):\n            (features, edges, n_hidden) = x\n        elif (len(x) == 4):\n            (features, edges, _, n_hidden) = x\n        else:\n            raise ValueError('Something is fishy!')\n        n_visible = features.shape[0]\n        if latent_node_features:\n            n_visible -= n_hidden\n        if (np.max(edges) != ((n_hidden + n_visible) - 1)):\n            raise ValueError(\"Edges don't add up\")\n        labels_one_hot = np.zeros((n_visible, n_labels), dtype=np.int)\n        y = y.ravel()\n        gx = np.ogrid[:n_visible]\n        labels_one_hot[(gx, y)] = 1\n        graph = sparse.coo_matrix((np.ones(edges.shape[0]), edges.T), ((n_visible + n_hidden), (n_visible + n_hidden)))\n        graph = (graph + graph.T)[(- n_hidden):, :n_visible]\n        neighbors = (graph * labels_one_hot.reshape(n_visible, (- 1)))\n        neighbors /= np.maximum(neighbors.sum(axis=1)[:, np.newaxis], 1)\n        all_feats.append(neighbors)\n    all_feats_stacked = np.vstack(all_feats)\n    try:\n        km = KMeans(n_clusters=n_hidden_states)\n    except TypeError:\n        km = KMeans(k=n_hidden_states)\n    km.fit(all_feats_stacked)\n    H = []\n    for (y, feats) in zip(Y, all_feats):\n        H.append(np.hstack([y, (km.predict(feats) + n_labels)]))\n    return H\n", "label": 0}
{"function": "\n\ndef send_status(test_result, status_url, repo_token, pending=False):\n    if (status_url and repo_token):\n        commit_id = status_url.rstrip('/').split('/')[(- 1)]\n        log_url = (cfg.CONF.worker.log_url_prefix + commit_id)\n        headers = {\n            'Authorization': ('token ' + repo_token),\n            'Content-Type': 'application/json',\n        }\n        if pending:\n            data = {\n                'state': 'pending',\n                'description': 'Solum says: Testing in progress',\n                'target_url': log_url,\n            }\n        elif (test_result == 0):\n            data = {\n                'state': 'success',\n                'description': 'Solum says: Tests passed',\n                'target_url': log_url,\n            }\n        else:\n            data = {\n                'state': 'failure',\n                'description': 'Solum says: Tests failed',\n                'target_url': log_url,\n            }\n        try:\n            conn = get_http_connection()\n            (resp, _) = conn.request(status_url, 'POST', headers=headers, body=json.dumps(data))\n            if (resp['status'] != '201'):\n                LOG.debug(('Failed to send back status. Error code %s,status_url %s, repo_token %s' % (resp['status'], status_url, repo_token)))\n        except (httplib2.HttpLib2Error, socket.error) as ex:\n            LOG.warning(('Error in sending status, status url: %s, repo token: %s, error: %s' % (status_url, repo_token, ex)))\n    else:\n        LOG.debug('No url or token available to send back status')\n", "label": 0}
{"function": "\n\ndef do_migration(records):\n    database['boxnodesettings'].update({\n        'user_settings': {\n            '$type': 2,\n        },\n    }, {\n        '$rename': {\n            'user_settings': 'foreign_user_settings',\n        },\n    }, multi=True)\n    for user_addon in records:\n        user = user_addon.owner\n        old_account = user_addon.oauth_settings\n        logger.info('Record found for user {}'.format(user._id))\n        try:\n            account = ExternalAccount(provider='box', provider_name='Box', display_name=old_account.username, oauth_key=old_account.access_token, refresh_token=old_account.refresh_token, provider_id=old_account.user_id, expires_at=old_account.expires_at)\n            account.save()\n        except KeyExistsException:\n            account = ExternalAccount.find_one((Q('provider', 'eq', 'box') & Q('provider_id', 'eq', old_account.user_id)))\n            assert (account is not None)\n        user.external_accounts.append(account)\n        user.save()\n        user_addon.oauth_settings = None\n        user_addon.save()\n        logger.info('Added external account {0} to user {1}'.format(account._id, user._id))\n    for node in BoxNodeSettings.find():\n        if (node.foreign_user_settings is None):\n            continue\n        logger.info('Migrating user_settings for box {}'.format(node._id))\n        node.user_settings = node.foreign_user_settings\n        node.save()\n", "label": 0}
{"function": "\n\ndef visualize(self, complex, color_function='', path_html='mapper_visualization_output.html', title='My Data', graph_link_distance=30, graph_gravity=0.1, graph_charge=(- 120), custom_tooltips=None, width_html=0, height_html=0, show_tooltips=True, show_title=True, show_meta=True):\n    json_s = {\n        \n    }\n    json_s['nodes'] = []\n    json_s['links'] = []\n    k2e = {\n        \n    }\n    for (e, k) in enumerate(complex['nodes']):\n        if (custom_tooltips is not None):\n            tooltip_s = (('<h2>Cluster %s</h2>' % k) + ' '.join([str(f) for f in custom_tooltips[complex['nodes'][k]]]))\n            if (color_function == 'average_signal_cluster'):\n                tooltip_i = int(((sum([f for f in custom_tooltips[complex['nodes'][k]]]) / len(custom_tooltips[complex['nodes'][k]])) * 30))\n                json_s['nodes'].append({\n                    'name': str(k),\n                    'tooltip': tooltip_s,\n                    'group': (2 * int(np.log(len(complex['nodes'][k])))),\n                    'color': str(tooltip_i),\n                })\n            else:\n                json_s['nodes'].append({\n                    'name': str(k),\n                    'tooltip': tooltip_s,\n                    'group': (2 * int(np.log(len(complex['nodes'][k])))),\n                    'color': str(k.split('_')[0]),\n                })\n        else:\n            tooltip_s = ('<h2>Cluster %s</h2>Contains %s members.' % (k, len(complex['nodes'][k])))\n            json_s['nodes'].append({\n                'name': str(k),\n                'tooltip': tooltip_s,\n                'group': (2 * int(np.log(len(complex['nodes'][k])))),\n                'color': str(k.split('_')[0]),\n            })\n        k2e[k] = e\n    for k in complex['links']:\n        for link in complex['links'][k]:\n            json_s['links'].append({\n                'source': k2e[k],\n                'target': k2e[link],\n                'value': 1,\n            })\n    if (width_html == 0):\n        width_css = '100%'\n        width_js = 'document.getElementById(\"holder\").offsetWidth-20'\n    else:\n        width_css = ('%spx' % width_html)\n        width_js = ('%s' % width_html)\n    if (height_html == 0):\n        height_css = '100%'\n        height_js = 'document.getElementById(\"holder\").offsetHeight-20'\n    else:\n        height_css = ('%spx' % height_html)\n        height_js = ('%s' % height_html)\n    if (show_tooltips == False):\n        tooltips_display = 'display: none;'\n    else:\n        tooltips_display = ''\n    if (show_meta == False):\n        meta_display = 'display: none;'\n    else:\n        meta_display = ''\n    if (show_title == False):\n        title_display = 'display: none;'\n    else:\n        title_display = ''\n    with open(path_html, 'wb') as outfile:\n        html = ('<!DOCTYPE html>\\n    <meta charset=\"utf-8\">\\n    <meta name=\"generator\" content=\"KeplerMapper\">\\n    <title>%s | KeplerMapper</title>\\n    <link href=\\'https://fonts.googleapis.com/css?family=Roboto:700,300\\' rel=\\'stylesheet\\' type=\\'text/css\\'>\\n    <style>\\n    * {margin: 0; padding: 0;}\\n    html { height: 100%%;}\\n    body {background: #111; height: 100%%; font: 100 16px Roboto, Sans-serif;}\\n    .link { stroke: #999; stroke-opacity: .333;  }\\n    .divs div { border-radius: 50%%; background: red; position: absolute; }\\n    .divs { position: absolute; top: 0; left: 0; }\\n    #holder { position: relative; width: %s; height: %s; background: #111; display: block;}\\n    h1 { %s padding: 20px; color: #fafafa; text-shadow: 0px 1px #000,0px -1px #000; position: absolute; font: 300 30px Roboto, Sans-serif;}\\n    h2 { text-shadow: 0px 1px #000,0px -1px #000; font: 700 16px Roboto, Sans-serif;}\\n    .meta {  position: absolute; opacity: 0.9; width: 220px; top: 80px; left: 20px; display: block; %s background: #000; line-height: 25px; color: #fafafa; border: 20px solid #000; font: 100 16px Roboto, Sans-serif;}\\n    div.tooltip { position: absolute; width: 380px; display: block; %s padding: 20px; background: #000; border: 0px; border-radius: 3px; pointer-events: none; z-index: 999; color: #FAFAFA;}\\n    }\\n    </style>\\n    <body>\\n    <div id=\"holder\">\\n      <h1>%s</h1>\\n      <p class=\"meta\">\\n      <b>Lens</b><br>%s<br><br>\\n      <b>Cubes per dimension</b><br>%s<br><br>\\n      <b>Overlap percentage</b><br>%s%%<br><br>\\n      <b>Color Function</b><br>%s( %s )<br><br>\\n      <b>Clusterer</b><br>%s<br><br>\\n      <b>Scaler</b><br>%s\\n      </p>\\n    </div>\\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\"></script>\\n    <script>\\n    var width = %s,\\n      height = %s;\\n    var color = d3.scale.ordinal()\\n      .domain([\"0\",\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"])\\n      .range([\"#FF0000\",\"#FF1400\",\"#FF2800\",\"#FF3c00\",\"#FF5000\",\"#FF6400\",\"#FF7800\",\"#FF8c00\",\"#FFa000\",\"#FFb400\",\"#FFc800\",\"#FFdc00\",\"#FFf000\",\"#fdff00\",\"#b0ff00\",\"#65ff00\",\"#17ff00\",\"#00ff36\",\"#00ff83\",\"#00ffd0\",\"#00e4ff\",\"#00c4ff\",\"#00a4ff\",\"#00a4ff\",\"#0084ff\",\"#0064ff\",\"#0044ff\",\"#0022ff\",\"#0002ff\",\"#0100ff\",\"#0300ff\",\"#0500ff\"]);\\n    var force = d3.layout.force()\\n      .charge(%s)\\n      .linkDistance(%s)\\n      .gravity(%s)\\n      .size([width, height]);\\n    var svg = d3.select(\"#holder\").append(\"svg\")\\n      .attr(\"width\", width)\\n      .attr(\"height\", height);\\n    \\n    var div = d3.select(\"#holder\").append(\"div\")   \\n      .attr(\"class\", \"tooltip\")               \\n      .style(\"opacity\", 0.0);\\n    \\n    var divs = d3.select(\\'#holder\\').append(\\'div\\')\\n      .attr(\\'class\\', \\'divs\\')\\n      .attr(\\'style\\', function(d) { return \\'overflow: hidden; width: \\' + width + \\'px; height: \\' + height + \\'px;\\'; });  \\n    \\n      graph = %s;\\n      force\\n        .nodes(graph.nodes)\\n        .links(graph.links)\\n        .start();\\n      var link = svg.selectAll(\".link\")\\n        .data(graph.links)\\n        .enter().append(\"line\")\\n        .attr(\"class\", \"link\")\\n        .style(\"stroke-width\", function(d) { return Math.sqrt(d.value); });\\n      var node = divs.selectAll(\\'div\\')\\n      .data(graph.nodes)\\n        .enter().append(\\'div\\')\\n        .on(\"mouseover\", function(d) {      \\n          div.transition()        \\n            .duration(200)      \\n            .style(\"opacity\", .9);\\n          div .html(d.tooltip + \"<br/>\")  \\n            .style(\"left\", (d3.event.pageX + 100) + \"px\")     \\n            .style(\"top\", (d3.event.pageY - 28) + \"px\");    \\n          })                  \\n        .on(\"mouseout\", function(d) {       \\n          div.transition()        \\n            .duration(500)      \\n            .style(\"opacity\", 0);   \\n        })\\n        .call(force.drag);\\n      \\n      node.append(\"title\")\\n        .text(function(d) { return d.name; });\\n      force.on(\"tick\", function() {\\n      link.attr(\"x1\", function(d) { return d.source.x; })\\n        .attr(\"y1\", function(d) { return d.source.y; })\\n        .attr(\"x2\", function(d) { return d.target.x; })\\n        .attr(\"y2\", function(d) { return d.target.y; });\\n      node.attr(\"cx\", function(d) { return d.x; })\\n        .attr(\"cy\", function(d) { return d.y; })\\n        .attr(\\'style\\', function(d) { return \\'width: \\' + (d.group * 2) + \\'px; height: \\' + (d.group * 2) + \\'px; \\' + \\'left: \\'+(d.x-(d.group))+\\'px; \\' + \\'top: \\'+(d.y-(d.group))+\\'px; background: \\'+color(d.color)+\\'; box-shadow: 0px 0px 3px #111; box-shadow: 0px 0px 33px \\'+color(d.color)+\\', inset 0px 0px 5px rgba(0, 0, 0, 0.2);\\'})\\n        ;\\n      });\\n    </script>' % (title, width_css, height_css, title_display, meta_display, tooltips_display, title, complex['meta'], self.nr_cubes, (self.overlap_perc * 100), color_function, complex['meta'], str(self.clusterer), str(self.scaler), width_js, height_js, graph_charge, graph_link_distance, graph_gravity, json.dumps(json_s)))\n        outfile.write(html.encode('utf-8'))\n    if (self.verbose > 0):\n        print((\"\\nWrote d3.js graph to '%s'\" % path_html))\n", "label": 1}
{"function": "\n\n@login_required()\ndef submit_r_assignment_answer(request, course_id, assignment_id):\n    if request.is_ajax():\n        if (request.method == 'POST'):\n            question_id = int(request.POST['question_id'])\n            answer = request.POST['answer']\n            course = Course.objects.get(id=course_id)\n            assignment = Assignment.objects.get(assignment_id=assignment_id)\n            student = Student.objects.get(user=request.user)\n            try:\n                question = ResponseQuestion.objects.get(assignment=assignment, question_id=question_id)\n            except ResponseQuestion.DoesNotExist:\n                response_data = {\n                    'status': 'failed',\n                    'message': 'cannot find question',\n                }\n                return HttpResponse(json.dumps(response_data), content_type='application/json')\n            try:\n                submission = ResponseSubmission.objects.get(student=student, question_id=question_id)\n            except ResponseSubmission.DoesNotExist:\n                submission = ResponseSubmission.objects.create(student=student, question_id=question_id)\n            submission.answer = answer\n            submission.save()\n            response_data = {\n                'status': 'success',\n                'message': 'submitted',\n            }\n    return HttpResponse(json.dumps(response_data), content_type='application/json')\n", "label": 0}
{"function": "\n\ndef safely_reserve_a_username(cursor, gen_usernames=gen_random_usernames, reserve=insert_into_participants):\n    'Safely reserve a username.\\n\\n    :param cursor: a :py:class:`psycopg2.cursor` managed as a :py:mod:`postgres`\\n        transaction\\n    :param gen_usernames: a generator of usernames to try\\n    :param reserve: a function that takes the cursor and does the SQL\\n        stuff\\n    :database: one ``INSERT`` on average\\n    :returns: a 12-hex-digit unicode\\n    :raises: :py:class:`FailedToReserveUsername` if no acceptable username is found\\n        within 100 attempts, or :py:class:`RanOutOfUsernameAttempts` if the username\\n        generator runs out first\\n\\n    The returned value is guaranteed to have been reserved in the database.\\n\\n    '\n    cursor.execute('SAVEPOINT safely_reserve_a_username')\n    seatbelt = 0\n    for username in gen_usernames():\n        seatbelt += 1\n        if (seatbelt > 100):\n            raise FailedToReserveUsername\n        try:\n            check = reserve(cursor, username)\n        except IntegrityError:\n            cursor.execute('ROLLBACK TO safely_reserve_a_username')\n            continue\n        else:\n            assert (check == username)\n            break\n    else:\n        raise RanOutOfUsernameAttempts\n    cursor.execute('RELEASE safely_reserve_a_username')\n    return username\n", "label": 0}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01, gamma=10, cof=4000):\n    ''\n    samplerate = self.audio.samplerate\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for y in super(Y4, self).walk(N, freq_base, freq_max, start, end, each=False, combine=False, gamma=gamma, cof=cof):\n        y = np.maximum(0, y)\n        if (not combine):\n            if each:\n                for v in y:\n                    (yield v)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\ndef send_data(sock, data, address=None):\n    if (address is None):\n        _rep = []\n    else:\n        _rep = list(address)\n    for a in data:\n        if isinstance(a, str):\n            _rep.append(a.encode('utf-8'))\n        else:\n            _rep.append(a)\n    while True:\n        try:\n            sock.send_multipart(_rep, zmq.NOBLOCK)\n        except zmq.ZMQError as e:\n            if (e.errno == errno.EAGAIN):\n                gethub().do_write(sock)\n                continue\n            elif (e.errno == errno.EINTR):\n                continue\n            else:\n                raise\n        else:\n            break\n", "label": 0}
{"function": "\n\ndef log(self, message, args=None, level=logging.INFO, sublogger=None):\n    if (args is None):\n        args = []\n    module_name = inspect.getmodule(inspect.stack()[1][0]).__name__\n    logger_name = module_name\n    if sublogger:\n        logger_name = '{module_name}:{sublogger}'.format(module_name=module_name, sublogger=sublogger)\n    logger.log(level, message, *args)\n    with io.open(self.log_path, 'a', encoding='utf-8') as log_file:\n        log_file.write(six.text_type('{date}\\t{level}\\t{module}\\t{message}\\n'.format(date=datetime.datetime.utcnow().isoformat(), level=logging.getLevelName(level), module=logger_name, message=(message % args).replace('\\n', '\\\\n'))))\n    if ((level >= logging.INFO) and (not self.quiet)):\n        print(('[%s %s] %s' % (logging.getLevelName(level), self.issue, (message % args))))\n", "label": 0}
{"function": "\n\ndef test_initialization_legacy(self, NeuralNet):\n    input = Mock(__name__='InputLayer', __bases__=(InputLayer,))\n    (hidden1, hidden2, output) = [Mock(__name__='MockLayer', __bases__=(Layer,)) for i in range(3)]\n    nn = NeuralNet(layers=[('input', input), ('hidden1', hidden1), ('hidden2', hidden2), ('output', output)], input_shape=(10, 10), hidden1_some='param')\n    out = nn.initialize_layers(nn.layers)\n    input.assert_called_with(name='input', shape=(10, 10))\n    assert (nn.layers_['input'] is input.return_value)\n    hidden1.assert_called_with(incoming=input.return_value, name='hidden1', some='param')\n    assert (nn.layers_['hidden1'] is hidden1.return_value)\n    hidden2.assert_called_with(incoming=hidden1.return_value, name='hidden2')\n    assert (nn.layers_['hidden2'] is hidden2.return_value)\n    output.assert_called_with(incoming=hidden2.return_value, name='output')\n    assert (out is nn.layers_['output'])\n", "label": 0}
{"function": "\n\ndef test_indexSearch(self, dataFrame):\n    datasearch = DataSearch('Test', dataFrame=dataFrame)\n    filterString = 'indexSearch([0])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 1)\n    filterString = 'indexSearch([0, 2])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 2)\n    filterString = 'indexSearch([0, 1, 2])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 3)\n    filterString = 'indexSearch([99])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 0)\n", "label": 0}
{"function": "\n\ndef lineReceived(self, line):\n    line = line.strip()\n    self.log.debug('[sref:%s] Received line: %s', self.sessionRef, line)\n    (cmd, arg, line) = self.parseline(line)\n    if (self.sessionLineCallback is not None):\n        return self.sessionLineCallback(cmd, arg, line)\n    if (not line):\n        return self.sendData()\n    if ((cmd is None) or (cmd not in self.findCommands())):\n        return self.default(line)\n    funcName = ('do_' + cmd)\n    try:\n        func = getattr(self, funcName)\n    except AttributeError:\n        return self.default(line)\n    self.log.debug('[sref:%s] Running %s with arg:%s', self.sessionRef, funcName, arg)\n    return func(arg)\n", "label": 0}
{"function": "\n\ndef pre_build_check():\n    '\\n    Try to verify build tools\\n    '\n    if os.environ.get('CASS_DRIVER_NO_PRE_BUILD_CHECK'):\n        return True\n    try:\n        from distutils.ccompiler import new_compiler\n        from distutils.sysconfig import customize_compiler\n        from distutils.dist import Distribution\n        be = build_ext(Distribution())\n        be.initialize_options()\n        be.finalize_options()\n        have_python_include = any((os.path.isfile(os.path.join(p, 'Python.h')) for p in be.include_dirs))\n        if (not have_python_include):\n            sys.stderr.write((\"Did not find 'Python.h' in %s.\\n\" % (be.include_dirs,)))\n            return False\n        compiler = new_compiler(compiler=be.compiler)\n        customize_compiler(compiler)\n        executables = []\n        if (compiler.compiler_type in ('unix', 'cygwin')):\n            executables = [compiler.executables[exe][0] for exe in ('compiler_so', 'linker_so')]\n        elif (compiler.compiler_type == 'nt'):\n            executables = [getattr(compiler, exe) for exe in ('cc', 'linker')]\n        if executables:\n            from distutils.spawn import find_executable\n            for exe in executables:\n                if (not find_executable(exe)):\n                    sys.stderr.write(('Failed to find %s for compiler type %s.\\n' % (exe, compiler.compiler_type)))\n                    return False\n    except Exception as exc:\n        sys.stderr.write(('%s\\n' % str(exc)))\n        sys.stderr.write('Failed pre-build check. Attempting anyway.\\n')\n    return True\n", "label": 1}
{"function": "\n\ndef doNick(self, msg):\n    'Handles NICK messages.'\n    if (msg.nick == self.nick):\n        newNick = msg.args[0]\n        self.nick = newNick\n        (nick, user, domain) = ircutils.splitHostmask(msg.prefix)\n        self.prefix = ircutils.joinHostmask(self.nick, user, domain)\n    elif conf.supybot.followIdentificationThroughNickChanges():\n        try:\n            id = ircdb.users.getUserId(msg.prefix)\n            u = ircdb.users.getUser(id)\n        except KeyError:\n            return\n        if u.auth:\n            (_, user, host) = ircutils.splitHostmask(msg.prefix)\n            newhostmask = ircutils.joinHostmask(msg.args[0], user, host)\n            for (i, (when, authmask)) in enumerate(u.auth[:]):\n                if ircutils.strEqual(msg.prefix, authmask):\n                    log.info('Following identification for %s: %s -> %s', u.name, authmask, newhostmask)\n                    u.auth[i] = (u.auth[i][0], newhostmask)\n                    ircdb.users.setUser(u)\n", "label": 0}
{"function": "\n\ndef acquire(self, blocking=True):\n    \"Must be used with 'yield' as 'yield cv.acquire()'.\\n        \"\n    coro = AsynCoro.cur_coro()\n    if (self._owner == coro):\n        self._depth += 1\n        raise StopIteration(True)\n    if ((not blocking) and (self._owner is not None)):\n        raise StopIteration(False)\n    while (self._owner is not None):\n        self._waitlist.append(coro)\n        (yield coro._await_())\n    assert (self._depth == 0)\n    self._owner = coro\n    self._depth = 1\n    raise StopIteration(True)\n", "label": 0}
{"function": "\n\n@float32_floatX\ndef test_correctness():\n    '\\n    Test the forward pass Op against theano graph implementation\\n    '\n    rng = np.random.RandomState([2012, 7, 19])\n    batch_size_list = [1, 5]\n    channels = 16\n    rows_list = [2, 24]\n    pool_rows_list = [2, 3]\n    for batch_size in batch_size_list:\n        for (rows, pool_rows) in zip(rows_list, pool_rows_list):\n            cols = rows\n            pool_cols = pool_rows\n            zv = rng.randn(channels, rows, cols, batch_size).astype(config.floatX)\n            z = T.tensor4()\n            (p, h) = prob_max_pool_c01b(z, (pool_rows, pool_cols))\n            func = function([z], [p, h], mode=mode_with_gpu)\n            (p_op, h_op) = func(zv)\n            (p, h) = max_pool_c01b(z, (pool_rows, pool_cols))\n            func = function([z], [p, h], mode=mode_without_gpu)\n            (p_th, h_th) = func(zv)\n            assert np.allclose(p_op, p_th)\n            assert np.allclose(h_op, h_th)\n", "label": 0}
{"function": "\n\n@permission_required('core.manage_shop')\ndef manage_export(request, export_id, template_name='manage/export/export.html'):\n    'The main view to display exports.\\n    '\n    export = Export.objects.get(pk=export_id)\n    categories = []\n    for category in Category.objects.filter(parent=None):\n        options = []\n        try:\n            category_option = CategoryOption.objects.get(export=export, category=category)\n        except CategoryOption.DoesNotExist:\n            variants_option = None\n        else:\n            variants_option = category_option.variants_option\n        for option in CATEGORY_VARIANTS_CHOICES:\n            options.append({\n                'name': option[1],\n                'value': option[0],\n                'selected': (option[0] == variants_option),\n            })\n        (checked, klass) = _get_category_state(export, category)\n        categories.append({\n            'id': category.id,\n            'name': category.name,\n            'checked': checked,\n            'klass': klass,\n            'options': options,\n        })\n    data_form = ExportDataForm(instance=export)\n    return render_to_response(template_name, RequestContext(request, {\n        'categories': categories,\n        'export_id': export_id,\n        'slug': export.slug,\n        'selectable_exports_inline': selectable_exports_inline(request, export_id),\n        'export_data_inline': export_data_inline(request, export_id, data_form),\n    }))\n", "label": 0}
{"function": "\n\ndef _verify_data_file(self, fp):\n    \"\\n        Verify the metadata's name value matches what we think the object is\\n        named.\\n\\n        :raises DiskFileCollision: if the metadata stored name does not match\\n                                   the referenced name of the file\\n        :raises DiskFileNotExist: if the object has expired\\n        :raises DiskFileQuarantined: if data inconsistencies were detected\\n                                     between the metadata and the file-system\\n                                     metadata\\n        \"\n    try:\n        mname = self._metadata['name']\n    except KeyError:\n        raise self._quarantine(self._name, 'missing name metadata')\n    else:\n        if (mname != self._name):\n            raise DiskFileCollision('Client path does not match path stored in object metadata')\n    try:\n        x_delete_at = int(self._metadata['X-Delete-At'])\n    except KeyError:\n        pass\n    except ValueError:\n        raise self._quarantine(self._name, ('bad metadata x-delete-at value %s' % self._metadata['X-Delete-At']))\n    else:\n        if (x_delete_at <= time.time()):\n            raise DiskFileNotExist('Expired')\n    try:\n        metadata_size = int(self._metadata['Content-Length'])\n    except KeyError:\n        raise self._quarantine(self._name, 'missing content-length in metadata')\n    except ValueError:\n        raise self._quarantine(self._name, ('bad metadata content-length value %s' % self._metadata['Content-Length']))\n    try:\n        fp.seek(0, 2)\n        obj_size = fp.tell()\n        fp.seek(0, 0)\n    except OSError as err:\n        raise self._quarantine(self._name, ('not stat-able: %s' % err))\n    if (obj_size != metadata_size):\n        raise self._quarantine(self._name, ('metadata content-length %s does not match actual object size %s' % (metadata_size, obj_size)))\n    return fp\n", "label": 1}
{"function": "\n\ndef test_pop_dict_t(session):\n    session2 = get_session()\n    keyid = key('test_sortedset_dict_set_t')\n    set_ = session.set(keyid, {\n        'h': 1,\n        'o': 1,\n        'n': 3,\n        'g': 4,\n    }, SortedSet)\n    set2 = session2.get(keyid, SortedSet)\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop('o')\n        assert (popped == 1)\n        assert (dict(set2) == {\n            'h': 1,\n            'o': 1,\n            'n': 3,\n            'g': 4,\n        })\n        with raises(CommitError):\n            len(set_)\n    assert (dict(set_) == dict(set2) == {\n        'h': 1,\n        'n': 3,\n        'g': 4,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop('n', score=0.5)\n        assert (popped == 3)\n        assert (dict(set2) == {\n            'h': 1,\n            'n': 3,\n            'g': 4,\n        })\n        with raises(CommitError):\n            len(set_)\n    assert (dict(set_) == dict(set2) == {\n        'h': 1,\n        'n': 2.5,\n        'g': 4,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop('n', remove=1.5)\n        assert (popped == 2.5)\n        assert (dict(set2) == {\n            'h': 1,\n            'n': 2.5,\n            'g': 4,\n        })\n        with raises(CommitError):\n            len(set_)\n    assert (dict(set_) == dict(set2) == {\n        'h': 1,\n        'g': 4,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop('g', remove=None)\n        assert (popped == 4)\n        assert (dict(set2) == {\n            'h': 1,\n            'g': 4,\n        })\n        with raises(CommitError):\n            len(set_)\n    assert (dict(set_) == dict(set2) == {\n        'h': 1,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop('n')\n        assert (popped is None)\n        assert (dict(set2) == {\n            'h': 1,\n        })\n    assert (dict(set_) == dict(set2) == {\n        'h': 1,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop('n', default='default value')\n        assert (popped == 'default value')\n        assert (dict(set2) == {\n            'h': 1,\n        })\n    assert (dict(set_) == dict(set2) == {\n        'h': 1,\n    })\n", "label": 1}
{"function": "\n\ndef get_context_data(self, **kwargs):\n    context = super(InputStockView, self).get_context_data(**kwargs)\n    try:\n        sql_location = SQLLocation.objects.get(domain=self.domain, site_code=kwargs.get('site_code'))\n    except SQLLocation.DoesNotExist:\n        raise Http404()\n    InputStockFormSet = formset_factory(InputStockForm, extra=0)\n    initial_data = []\n    for product in sql_location.products.order_by('name'):\n        try:\n            stock_state = StockState.objects.get(case_id=sql_location.supply_point_id, product_id=product.product_id)\n            stock_on_hand = stock_state.stock_on_hand\n            monthly_consumption = stock_state.get_monthly_consumption()\n        except StockState.DoesNotExist:\n            stock_on_hand = 0\n            monthly_consumption = 0\n        initial_data.append({\n            'product_id': product.product_id,\n            'product': product.name,\n            'stock_on_hand': int(stock_on_hand),\n            'monthly_consumption': (round(monthly_consumption) if monthly_consumption else 0),\n            'default_consumption': get_default_monthly_consumption(self.domain, product.product_id, sql_location.location_type.name, sql_location.supply_point_id),\n            'units': product.units,\n        })\n    context['formset'] = InputStockFormSet(initial=initial_data)\n    return context\n", "label": 0}
{"function": "\n\ndef MergeFlags(self, args, unique=1, dict=None):\n    '\\n        Merge the dict in args into the construction variables of this\\n        env, or the passed-in dict.  If args is not a dict, it is\\n        converted into a dict using ParseFlags.  If unique is not set,\\n        the flags are appended rather than merged.\\n        '\n    if (dict is None):\n        dict = self\n    if (not SCons.Util.is_Dict(args)):\n        args = self.ParseFlags(args)\n    if (not unique):\n        self.Append(**args)\n        return self\n    for (key, value) in args.items():\n        if (not value):\n            continue\n        try:\n            orig = self[key]\n        except KeyError:\n            orig = value\n        else:\n            if (not orig):\n                orig = value\n            elif value:\n                try:\n                    orig = (orig + value)\n                except (KeyError, TypeError):\n                    try:\n                        add_to_orig = orig.append\n                    except AttributeError:\n                        value.insert(0, orig)\n                        orig = value\n                    else:\n                        add_to_orig(value)\n        t = []\n        if (key[(- 4):] == 'PATH'):\n            for v in orig:\n                if (v not in t):\n                    t.append(v)\n        else:\n            orig.reverse()\n            for v in orig:\n                if (v not in t):\n                    t.insert(0, v)\n        self[key] = t\n    return self\n", "label": 1}
{"function": "\n\ndef check_char_lookup(self, lookup):\n    lname = ('field__' + lookup)\n    mymodel = CharListModel.objects.create(field=['mouldy', 'rotten'])\n    mouldy = CharListModel.objects.filter(**{\n        lname: 'mouldy',\n    })\n    assert (mouldy.count() == 1)\n    assert (mouldy[0] == mymodel)\n    rotten = CharListModel.objects.filter(**{\n        lname: 'rotten',\n    })\n    assert (rotten.count() == 1)\n    assert (rotten[0] == mymodel)\n    clean = CharListModel.objects.filter(**{\n        lname: 'clean',\n    })\n    assert (clean.count() == 0)\n    with pytest.raises(ValueError):\n        list(CharListModel.objects.filter(**{\n            lname: ['a', 'b'],\n        }))\n    both = CharListModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) & Q(**{\n        lname: 'rotten',\n    })))\n    assert (both.count() == 1)\n    assert (both[0] == mymodel)\n    either = CharListModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) | Q(**{\n        lname: 'clean',\n    })))\n    assert (either.count() == 1)\n    not_clean = CharListModel.objects.exclude(**{\n        lname: 'clean',\n    })\n    assert (not_clean.count() == 1)\n    not_mouldy = CharListModel.objects.exclude(**{\n        lname: 'mouldy',\n    })\n    assert (not_mouldy.count() == 0)\n", "label": 1}
{"function": "\n\ndef enqueue(self, pdu):\n    self.log('enqueue {pdu.name} PDU'.format(pdu=pdu))\n    if (not (pdu.type in connection_mode_pdu_types)):\n        self.err('non connection mode pdu on data link connection')\n        pdu = FrameReject.from_pdu(pdu, flags='W', dlc=self)\n        self.close()\n        self.send_queue.append(pdu)\n        return\n    if self.state.CLOSED:\n        pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=1)\n        self.send_queue.append(pdu)\n    if self.state.LISTEN:\n        if isinstance(pdu, Connect):\n            if (super(DataLinkConnection, self).enqueue(pdu) == False):\n                log.warn('full backlog on listening socket')\n                pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=32)\n                self.send_queue.append(pdu)\n                return False\n            return True\n    if self.state.CONNECT:\n        if (isinstance(pdu, ConnectionComplete) or isinstance(pdu, DisconnectedMode)):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.DISCONNECT:\n        if isinstance(pdu, DisconnectedMode):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.ESTABLISHED:\n        return self._enqueue_state_established(pdu)\n", "label": 1}
{"function": "\n\ndef __call__(self, event):\n    '\\n        Write event to file.\\n\\n        @param event: An event.\\n        @type event: L{dict}\\n        '\n    text = self.formatEvent(event)\n    if (text is None):\n        text = ''\n    if ('log_failure' in event):\n        try:\n            traceback = event['log_failure'].getTraceback()\n        except Exception:\n            traceback = '(UNABLE TO OBTAIN TRACEBACK FROM EVENT)\\n'\n        text = '\\n'.join((text, traceback))\n    if (self._encoding is not None):\n        text = text.encode(self._encoding)\n    if text:\n        self._outFile.write(text)\n        self._outFile.flush()\n", "label": 0}
{"function": "\n\ndef add_image_info_cb(self, viewer, channel, info):\n    save_thumb = self.settings.get('cache_thumbs', False)\n    chname = channel.name\n    thumbkey = self.get_thumb_key(chname, info.name, info.path)\n    thumbpath = self.get_thumbpath(info.path)\n    with self.thmblock:\n        try:\n            bnch = self.thumbDict[thumbkey]\n            if (bnch.thumbpath == thumbpath):\n                return\n        except KeyError:\n            pass\n    image = None\n    if ((thumbpath is not None) and os.path.exists(thumbpath)):\n        save_thumb = False\n        try:\n            image = self.fv.load_image(thumbpath)\n        except Exception as e:\n            pass\n    try:\n        if (image is None):\n            image = info.image_loader(info.path)\n        image.set(name=info.name)\n        self.fv.gui_do(self._make_thumb, chname, image, info.name, info.path, thumbkey, info.image_future, save_thumb=save_thumb, thumbpath=thumbpath)\n    except Exception as e:\n        self.logger.error((\"Error generating thumbnail for '%s': %s\" % (info.path, str(e))))\n", "label": 0}
{"function": "\n\ndef update_text_object(self, event):\n    ' Handles the user typing text into the combo box text entry field.\\n        '\n    if (self._no_enum_update == 0):\n        value = self.control.GetValue()\n        try:\n            value = self.mapping[value]\n        except:\n            try:\n                value = self.factory.evaluate(value)\n            except Exception as excp:\n                self.error(excp)\n                return\n        self._no_enum_update += 1\n        try:\n            self.value = value\n            self.control.SetBackgroundColour(OKColor)\n            self.control.Refresh()\n        except:\n            pass\n        self._no_enum_update -= 1\n", "label": 0}
{"function": "\n\ndef incidence_matrix(G, nodelist=None, edgelist=None, oriented=False, weight=None):\n    'Return incidence matrix of G.\\n\\n    The incidence matrix assigns each row to a node and each column to an edge.\\n    For a standard incidence matrix a 1 appears wherever a row\\'s node is\\n    incident on the column\\'s edge.  For an oriented incidence matrix each\\n    edge is assigned an orientation (arbitrarily for undirected and aligning to\\n    direction for directed).  A -1 appears for the tail of an edge and 1\\n    for the head of the edge.  The elements are zero otherwise.\\n\\n    Parameters\\n    ----------\\n    G : graph\\n       A NetworkX graph\\n\\n    nodelist : list, optional   (default= all nodes in G)\\n       The rows are ordered according to the nodes in nodelist.\\n       If nodelist is None, then the ordering is produced by G.nodes().\\n\\n    edgelist : list, optional (default= all edges in G)\\n       The columns are ordered according to the edges in edgelist.\\n       If edgelist is None, then the ordering is produced by G.edges().\\n\\n    oriented: bool, optional (default=False)\\n       If True, matrix elements are +1 or -1 for the head or tail node\\n       respectively of each edge.  If False, +1 occurs at both nodes.\\n\\n    weight : string or None, optional (default=None)\\n       The edge data key used to provide each value in the matrix.\\n       If None, then each edge has weight 1.  Edge weights, if used,\\n       should be positive so that the orientation can provide the sign.\\n\\n    Returns\\n    -------\\n    A : SciPy sparse matrix\\n      The incidence matrix of G.\\n\\n    Notes\\n    -----\\n    For MultiGraph/MultiDiGraph, the edges in edgelist should be\\n    (u,v,key) 3-tuples.\\n\\n    \"Networks are the best discrete model for so many problems in\\n    applied mathematics\" [1]_.\\n\\n    References\\n    ----------\\n    .. [1] Gil Strang, Network applications: A = incidence matrix,\\n       http://academicearth.org/lectures/network-applications-incidence-matrix\\n    '\n    import scipy.sparse\n    if (nodelist is None):\n        nodelist = list(G)\n    if (edgelist is None):\n        if G.is_multigraph():\n            edgelist = list(G.edges(keys=True))\n        else:\n            edgelist = list(G.edges())\n    A = scipy.sparse.lil_matrix((len(nodelist), len(edgelist)))\n    node_index = dict(((node, i) for (i, node) in enumerate(nodelist)))\n    for (ei, e) in enumerate(edgelist):\n        (u, v) = e[:2]\n        if (u == v):\n            continue\n        try:\n            ui = node_index[u]\n            vi = node_index[v]\n        except KeyError:\n            raise NetworkXError('node %s or %s in edgelist but not in nodelist\"%(u,v)')\n        if (weight is None):\n            wt = 1\n        elif G.is_multigraph():\n            ekey = e[2]\n            wt = G[u][v][ekey].get(weight, 1)\n        else:\n            wt = G[u][v].get(weight, 1)\n        if oriented:\n            A[(ui, ei)] = (- wt)\n            A[(vi, ei)] = wt\n        else:\n            A[(ui, ei)] = wt\n            A[(vi, ei)] = wt\n    return A.asformat('csc')\n", "label": 1}
{"function": "\n\ndef getPassword(keyDesc):\n    if (not _keyutils):\n        return None\n    _setupSession()\n    try:\n        keyId = _keyutils.request_key(keyDesc, _keyring)\n    except _keyutils.Error as err:\n        if (err.args[0] != _keyutils.EKEYREVOKED):\n            raise\n        return None\n    if (keyId is not None):\n        return _keyutils.read_key(keyId)\n    return None\n", "label": 0}
{"function": "\n\ndef onEnterNode(self, node):\n    for variable in node.getClosureVariables():\n        assert (not variable.isModuleVariable())\n        current = node\n        while (current is not variable.getOwner()):\n            if current.isParentVariableProvider():\n                if (variable not in current.getClosureVariables()):\n                    current.addClosureVariable(variable)\n            assert (current.getParentVariableProvider() is not current)\n            current = current.getParentVariableProvider()\n            assert (current is not None), variable\n", "label": 0}
{"function": "\n\ndef test_log_message(self):\n    logger = logging.getLogger('django.server')\n    original_handlers = logger.handlers\n    logger.handlers = [logging.NullHandler()]\n    try:\n        request = WSGIRequest(RequestFactory().get('/').environ)\n        request.makefile = (lambda *args, **kwargs: BytesIO())\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        level_status_codes = {\n            'info': [200, 301, 304],\n            'warning': [400, 403, 404],\n            'error': [500, 503],\n        }\n\n        def _log_level_code(level, status_code):\n            with patch_logger('django.server', level) as messages:\n                handler.log_message('GET %s %s', 'A', str(status_code))\n            return messages\n        for (level, status_codes) in level_status_codes.items():\n            for status_code in status_codes:\n                messages = _log_level_code(level, status_code)\n                self.assertIn(('GET A %d' % status_code), messages[0])\n                for wrong_level in level_status_codes.keys():\n                    if (wrong_level != level):\n                        messages = _log_level_code(wrong_level, status_code)\n                        self.assertEqual(len(messages), 0)\n    finally:\n        logger.handlers = original_handlers\n", "label": 0}
{"function": "\n\ndef getCovMatrix(self, x=None, z=None, mode=None):\n    self.checkInputGetCovMatrix(x, z, mode)\n    c = np.exp(self.hyp[0])\n    sf2 = np.exp((2.0 * self.hyp[1]))\n    ord = self.para[0]\n    if (np.abs((ord - np.round(ord))) < 1e-08):\n        ord = int(round(ord))\n    assert (ord >= 1.0)\n    ord = int(ord)\n    if (mode == 'self_test'):\n        (nn, D) = z.shape\n        A = np.reshape(np.sum((z * z), 1), (nn, 1))\n    elif (mode == 'train'):\n        (n, D) = x.shape\n        A = (np.dot(x, x.T) + (np.eye(n) * 1e-10))\n    elif (mode == 'cross'):\n        A = np.dot(x, z.T)\n    A = (sf2 * ((c + A) ** ord))\n    return A\n", "label": 0}
{"function": "\n\ndef test_404_tried_urls_have_names(self):\n    '\\n        Verifies that the list of URLs that come back from a Resolver404\\n        exception contains a list in the right format for printing out in\\n        the DEBUG 404 page with both the patterns and URL names, if available.\\n        '\n    urls = 'regressiontests.urlpatterns_reverse.named_urls'\n    url_types_names = [[{\n        'type': RegexURLPattern,\n        'name': 'named-url1',\n    }], [{\n        'type': RegexURLPattern,\n        'name': 'named-url2',\n    }], [{\n        'type': RegexURLPattern,\n        'name': None,\n    }], [{\n        'type': RegexURLResolver,\n    }, {\n        'type': RegexURLPattern,\n        'name': 'named-url3',\n    }], [{\n        'type': RegexURLResolver,\n    }, {\n        'type': RegexURLPattern,\n        'name': 'named-url4',\n    }], [{\n        'type': RegexURLResolver,\n    }, {\n        'type': RegexURLPattern,\n        'name': None,\n    }], [{\n        'type': RegexURLResolver,\n    }, {\n        'type': RegexURLResolver,\n    }]]\n    try:\n        resolve('/included/non-existent-url', urlconf=urls)\n        self.fail('resolve did not raise a 404')\n    except Resolver404 as e:\n        self.assertTrue(('tried' in e.args[0]))\n        tried = e.args[0]['tried']\n        self.assertEqual(len(e.args[0]['tried']), len(url_types_names), ('Wrong number of tried URLs returned.  Expected %s, got %s.' % (len(url_types_names), len(e.args[0]['tried']))))\n        for (tried, expected) in zip(e.args[0]['tried'], url_types_names):\n            for (t, e) in zip(tried, expected):\n                self.assertTrue(isinstance(t, e['type']), (str('%s is not an instance of %s') % (t, e['type'])))\n                if ('name' in e):\n                    if (not e['name']):\n                        self.assertTrue((t.name is None), ('Expected no URL name but found %s.' % t.name))\n                    else:\n                        self.assertEqual(t.name, e['name'], ('Wrong URL name.  Expected \"%s\", got \"%s\".' % (e['name'], t.name)))\n", "label": 0}
{"function": "\n\ndef test_upload_image(client, mocker):\n    today = datetime.date.today()\n    mocker.patch('requests.get')\n    mocker.patch('redwind.tasks.create_queue')\n    rv = client.post('/save_new', data={\n        'photo': (open('tests/image.jpg', 'rb'), 'image.jpg', 'image/jpeg'),\n        'post_type': 'photo',\n        'content': 'High score',\n        'action': 'publish_quietly',\n    })\n    assert (rv.status_code == 302)\n    assert (rv.location == 'http://example.com/{}/{:02d}/high-score'.format(today.year, today.month))\n    permalink = rv.location\n    rv = client.get(permalink)\n    assert (rv.status_code == 200)\n    content = rv.get_data(as_text=True)\n    assert ('High score' in content)\n    assert ('<img' in content)\n    rv = client.get((permalink + '/files/image.jpg'))\n    assert (rv.status_code == 200)\n", "label": 0}
{"function": "\n\n@set_ev_cls(ofp_event.EventOFPPacketIn, MAIN_DISPATCHER)\ndef lldp_packet_in_handler(self, ev):\n    if (not self.link_discovery):\n        return\n    msg = ev.msg\n    try:\n        (src_dpid, src_port_no) = LLDPPacket.lldp_parse(msg.data)\n    except LLDPPacket.LLDPUnknownFormat as e:\n        return\n    dst_dpid = msg.datapath.id\n    if (msg.datapath.ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION):\n        dst_port_no = msg.in_port\n    elif (msg.datapath.ofproto.OFP_VERSION >= ofproto_v1_2.OFP_VERSION):\n        dst_port_no = msg.match['in_port']\n    else:\n        LOG.error('cannot accept LLDP. unsupported version. %x', msg.datapath.ofproto.OFP_VERSION)\n    src = self._get_port(src_dpid, src_port_no)\n    if ((not src) or (src.dpid == dst_dpid)):\n        return\n    try:\n        self.ports.lldp_received(src)\n    except KeyError:\n        pass\n    dst = self._get_port(dst_dpid, dst_port_no)\n    if (not dst):\n        return\n    old_peer = self.links.get_peer(src)\n    if (old_peer and (old_peer != dst)):\n        old_link = Link(src, old_peer)\n        del self.links[old_link]\n        self.send_event_to_observers(event.EventLinkDelete(old_link))\n    link = Link(src, dst)\n    if (link not in self.links):\n        self.send_event_to_observers(event.EventLinkAdd(link))\n        for host in self.hosts.values():\n            if (not self._is_edge_port(host.port)):\n                del self.hosts[host.mac]\n    if (not self.links.update_link(src, dst)):\n        self.ports.move_front(dst)\n        self.lldp_event.set()\n    if self.explicit_drop:\n        self._drop_packet(msg)\n", "label": 1}
{"function": "\n\ndef _get_text(self, encoding):\n    lines = self.editor.toPlainText().splitlines()\n    if self.clean_trailing_whitespaces:\n        lines = [l.rstrip() for l in lines]\n    try:\n        last_line = lines[(- 1)]\n    except IndexError:\n        pass\n    else:\n        while (last_line == ''):\n            try:\n                lines.pop()\n                last_line = lines[(- 1)]\n            except IndexError:\n                last_line = None\n    text = (self._eol.join(lines) + self._eol)\n    return text.encode(encoding)\n", "label": 0}
{"function": "\n\ndef OnOk(self, hwnd):\n    \"When OK is pressed, if this isn't a progress dialog then simply\\n        gather the results and return. If this is a progress dialog then\\n        start a thread to handle progress via the progress iterator.\\n        \"\n\n    def progress_thread(iterator, cancelled):\n        'Handle the progress side of the dialog by iterating over a supplied\\n            iterator(presumably a generator) sending generated values as messages\\n            to the progress box -- these might be percentages or files processed\\n            or whatever.\\n\\n            If the user cancels, an event will be fired which is detected here and\\n            the iteration broken. Likewise an exception will be logged to the usual\\n            places and a suitable message sent.\\n            '\n        try:\n            for message in iterator:\n                if (wrapped(win32event.WaitForSingleObject, cancelled, 0) != win32event.WAIT_TIMEOUT):\n                    self._progress_complete('User cancelled')\n                    break\n                else:\n                    self._progress_message(message)\n        except:\n            info_dialog('An error occurred: please contact the Helpdesk', traceback.format_exc(), hwnd)\n            self._progress_complete('An error occurred')\n        else:\n            self._progress_complete('Complete')\n    self.results = []\n    for (i, (field, default_value, callback)) in enumerate(self.fields):\n        value = self._get_item((self.IDC_FIELD_BASE + i))\n        if isinstance(default_value, datetime.date):\n            try:\n                if value:\n                    value = datetime.datetime.strptime(value, '%d %b %Y').date()\n                else:\n                    value = None\n            except ValueError:\n                win32api.MessageBox(hwnd, ('Dates must look like:\\n%s' % datetime.date.today().strftime('%d %b %Y').lstrip('0')), 'Invalid Date')\n                return\n        self.results.append(value)\n    if self.progress_callback:\n        self._set_item(self._progress_id, 'Working...')\n        for i in range(len(self.fields)):\n            self._enable((self.IDC_FIELD_BASE + i), False)\n        self._enable(win32con.IDOK, False)\n        wrapped(win32gui.SetFocus, wrapped(win32gui.GetDlgItem, hwnd, win32con.IDCANCEL))\n        progress_iterator = self.progress_callback(*self.results)\n        self.progress_callback = None\n        self.progress_thread = threading.Thread(target=progress_thread, args=(progress_iterator, self.progress_cancelled))\n        self.progress_thread.setDaemon(True)\n        self.progress_thread.start()\n    else:\n        wrapped(win32gui.EndDialog, hwnd, win32con.IDOK)\n", "label": 0}
{"function": "\n\ndef mon_status(conn, logger, hostname, args, silent=False):\n    '\\n    run ``ceph daemon mon.`hostname` mon_status`` on the remote end and provide\\n    not only the output, but be able to return a boolean status of what is\\n    going on.\\n    ``False`` represents a monitor that is not doing OK even if it is up and\\n    running, while ``True`` would mean the monitor is up and running correctly.\\n    '\n    mon = ('mon.%s' % hostname)\n    try:\n        out = mon_status_check(conn, logger, hostname, args)\n        if (not out):\n            logger.warning(('monitor: %s, might not be running yet' % mon))\n            return False\n        if (not silent):\n            logger.debug(('*' * 80))\n            logger.debug(('status for monitor: %s' % mon))\n            for line in json.dumps(out, indent=2, sort_keys=True).split('\\n'):\n                logger.debug(line)\n            logger.debug(('*' * 80))\n        if (out['rank'] >= 0):\n            logger.info(('monitor: %s is running' % mon))\n            return True\n        if ((out['rank'] == (- 1)) and out['state']):\n            logger.info(('monitor: %s is currently at the state of %s' % (mon, out['state'])))\n            return True\n        logger.info(('monitor: %s is not running' % mon))\n        return False\n    except RuntimeError:\n        logger.info(('monitor: %s is not running' % mon))\n        return False\n", "label": 0}
{"function": "\n\ndef execute(self, cluster, commands):\n    pipes = {\n        \n    }\n    pool = self.get_pool(commands)\n    for (db_num, command_list) in commands.iteritems():\n        pipes[db_num] = cluster[db_num].get_pipeline()\n        for command in command_list:\n            pipes[db_num].add(command.clone())\n    for (db_num, pipe) in pipes.iteritems():\n        pool.add(db_num, pipe.execute, (), {\n            \n        })\n    db_result_map = pool.join()\n    results = defaultdict(list)\n    for (db_num, db_results) in db_result_map.iteritems():\n        assert (len(db_results) == 1)\n        db_results = db_results[0]\n        if isinstance(db_results, Exception):\n            for command in commands[db_num]:\n                results[command].append(db_results)\n            continue\n        for (command, result) in db_results.iteritems():\n            results[command].append(result)\n    return results\n", "label": 0}
{"function": "\n\ndef _buildSequenceUnpacking(provider, node, source_ref):\n    kind = getKind(node)\n    if (kind == 'List'):\n        return buildListUnpacking(provider, node.elts, source_ref)\n    elif (kind == 'Tuple'):\n        return _buildTupleUnpacking(provider, node.elts, source_ref)\n    elif (kind == 'Set'):\n        return _buildSetUnpacking(provider, node.elts, source_ref)\n    else:\n        assert False, kind\n", "label": 0}
{"function": "\n\ndef load_gray(random_seed=123522):\n    data_path = os.path.join(os.path.split(__file__)[0], 'data')\n    if (not os.path.exists(data_path)):\n        os.makedirs(data_path)\n    dataset = 'train.zip'\n    data_file = os.path.join(data_path, dataset)\n    if os.path.isfile(data_file):\n        dataset = data_file\n    if (not os.path.isfile(data_file)):\n        try:\n            import urllib\n            urllib.urlretrieve('http://google.com')\n        except AttributeError:\n            import urllib.request as urllib\n        url = 'https://dl.dropboxusercontent.com/u/15378192/train.zip'\n        print(('Downloading data from %s' % url))\n        urllib.urlretrieve(url, data_file)\n    data_dir = os.path.join(data_path, 'cvd')\n    if (not os.path.exists(data_dir)):\n        os.makedirs(data_dir)\n        zf = zipfile.ZipFile(data_file)\n        zf.extractall(data_dir)\n    data_file = os.path.join(data_path, 'cvd_gray.npy')\n    label_file = os.path.join(data_path, 'cvd_gray_labels.npy')\n    if (not os.path.exists(data_file)):\n        print('... loading data')\n        cat_matches = []\n        dog_matches = []\n        for (root, dirname, filenames) in os.walk(data_dir):\n            for filename in fnmatch.filter(filenames, 'cat*'):\n                cat_matches.append(os.path.join(root, filename))\n            for filename in fnmatch.filter(filenames, 'dog*'):\n                dog_matches.append(os.path.join(root, filename))\n        sort_key = (lambda x: int(x.split('.')[(- 2)]))\n        cat_matches = sorted(cat_matches, key=sort_key)\n        dog_matches = sorted(dog_matches, key=sort_key)\n\n        def square_and_gray(X):\n            gray_consts = np.array([[0.299], [0.587], [0.144]])\n            return imresize(X, (48, 48)).dot(gray_consts).squeeze()\n        X_cat = np.asarray([square_and_gray(mpimg.imread(f)) for f in cat_matches])\n        y_cat = np.zeros((len(X_cat),))\n        X_dog = np.asarray([square_and_gray(mpimg.imread(f)) for f in dog_matches])\n        y_dog = np.ones((len(X_dog),))\n        X = np.concatenate((X_cat, X_dog), axis=0).astype('float32')\n        y = np.concatenate((y_cat, y_dog), axis=0).astype('int32')\n        np.save(data_file, X)\n        np.save(label_file, y)\n    else:\n        X = np.load(data_file)\n        y = np.load(label_file)\n    random_state = np.random.RandomState(random_seed)\n    idx = random_state.permutation(len(X))\n    X_s = X[idx].reshape(len(X), (- 1))\n    y_s = y[idx]\n    train_x = X_s[:20000]\n    valid_x = X_s[20000:22500]\n    test_x = X_s[22500:]\n    train_y = y_s[:20000]\n    valid_y = y_s[20000:22500]\n    test_y = y_s[22500:]\n    test_x = test_x.astype('float32')\n    test_y = test_y.astype('int32')\n    valid_x = valid_x.astype('float32')\n    valid_y = valid_y.astype('int32')\n    train_x = train_x.astype('float32')\n    train_y = train_y.astype('int32')\n    rval = [(train_x, train_y), (valid_x, valid_y), (test_x, test_y)]\n    return rval\n", "label": 1}
{"function": "\n\ndef test_spike_terms():\n    rules = {\n        'threshold_ref': 5,\n        'spike_height': 2,\n        'timeframe': datetime.timedelta(minutes=10),\n        'spike_type': 'both',\n        'use_count_query': False,\n        'timestamp_field': 'ts',\n        'query_key': 'username',\n        'use_term_query': True,\n    }\n    terms1 = {\n        ts_to_dt('2014-01-01T00:01:00Z'): [{\n            'key': 'userA',\n            'doc_count': 10,\n        }, {\n            'key': 'userB',\n            'doc_count': 5,\n        }],\n    }\n    terms2 = {\n        ts_to_dt('2014-01-01T00:10:00Z'): [{\n            'key': 'userA',\n            'doc_count': 22,\n        }, {\n            'key': 'userB',\n            'doc_count': 5,\n        }],\n    }\n    terms3 = {\n        ts_to_dt('2014-01-01T00:25:00Z'): [{\n            'key': 'userA',\n            'doc_count': 25,\n        }, {\n            'key': 'userB',\n            'doc_count': 27,\n        }],\n    }\n    terms4 = {\n        ts_to_dt('2014-01-01T00:27:00Z'): [{\n            'key': 'userA',\n            'doc_count': 10,\n        }, {\n            'key': 'userB',\n            'doc_count': 12,\n        }, {\n            'key': 'userC',\n            'doc_count': 100,\n        }],\n    }\n    terms5 = {\n        ts_to_dt('2014-01-01T00:30:00Z'): [{\n            'key': 'userD',\n            'doc_count': 100,\n        }, {\n            'key': 'userC',\n            'doc_count': 100,\n        }],\n    }\n    rule = SpikeRule(rules)\n    rule.add_terms_data(terms1)\n    assert (len(rule.matches) == 0)\n    rule.add_terms_data(terms2)\n    assert (len(rule.matches) == 0)\n    rule.add_terms_data(terms3)\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0].get('username') == 'userB')\n    rules.pop('threshold_ref')\n    rules['threshold_cur'] = 50\n    rule = SpikeRule(rules)\n    rule.add_terms_data(terms1)\n    rule.add_terms_data(terms2)\n    rule.add_terms_data(terms3)\n    rule.add_terms_data(terms4)\n    assert (len(rule.matches) == 0)\n    rules['alert_on_new_data'] = True\n    rule = SpikeRule(rules)\n    rule.add_terms_data(terms1)\n    rule.add_terms_data(terms2)\n    rule.add_terms_data(terms3)\n    rule.add_terms_data(terms4)\n    assert (len(rule.matches) == 1)\n    rule.matches = []\n    rule.add_terms_data(terms5)\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0]['username'] == 'userD')\n", "label": 0}
{"function": "\n\ndef load_ctfs(saved_data, volume_property):\n    ' Given the saved data produced via `save_ctfs`, this sets the\\n    state of the passed volume_property appropriately.\\n\\n    It returns the new color transfer function and piecewise function.\\n    '\n    rgb = saved_data['rgb']\n    a = saved_data['alpha']\n    new_ctf = True\n    ctf = volume_property.rgb_transfer_function\n    if isinstance(ctf, ColorTransferFunction):\n        new_ctf = False\n        ctf.remove_all_points()\n    else:\n        ctf = ColorTransferFunction()\n    nc = len(rgb)\n    for i in range(nc):\n        ctf.add_rgb_point(rgb[i][0], *rgb[i][1:])\n    if new_ctf:\n        volume_property.set_color(ctf)\n    try:\n        ctf.range = saved_data['range']\n    except Exception:\n        pass\n    na = len(a)\n    new_otf = True\n    otf = volume_property.get_scalar_opacity()\n    if isinstance(otf, PiecewiseFunction):\n        new_otf = False\n        otf.remove_all_points()\n    else:\n        otf = PiecewiseFunction()\n    for i in range(na):\n        otf.add_point(a[i][0], a[i][1])\n    if new_otf:\n        volume_property.set_scalar_opacity(otf)\n    return (ctf, otf)\n", "label": 0}
{"function": "\n\ndef _validate_ip(af, ip):\n    '\\n    Common logic for IPv4\\n    '\n    if ((af == socket.AF_INET6) and (not socket.has_ipv6)):\n        raise RuntimeError('IPv6 not supported')\n    try:\n        socket.inet_pton(af, ip)\n    except AttributeError:\n        if (af == socket.AF_INET6):\n            raise RuntimeError('socket.inet_pton not available')\n        try:\n            socket.inet_aton(ip)\n        except socket.error:\n            return False\n    except (socket.error, TypeError):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef load(s):\n    try:\n        yml_dict = yaml.load(s, yaml_loader)\n    except yaml.YAMLError as exc:\n        msg = 'An error occurred during YAML parsing.'\n        if hasattr(exc, 'problem_mark'):\n            msg += (' Error position: (%s:%s)' % ((exc.problem_mark.line + 1), (exc.problem_mark.column + 1)))\n        raise ValueError(msg)\n    if ((not isinstance(yml_dict, dict)) and (not isinstance(yml_dict, list))):\n        raise ValueError('The source is not a YAML mapping or list.')\n    if (isinstance(yml_dict, dict) and (len(yml_dict) < 1)):\n        raise ValueError('Could not find any element in your YAML mapping.')\n    return yml_dict\n", "label": 0}
{"function": "\n\n@decorator\ndef retry(call, tries, errors=Exception, timeout=0):\n    if isinstance(errors, list):\n        errors = tuple(errors)\n    for attempt in xrange(tries):\n        try:\n            return call()\n        except errors:\n            if ((attempt + 1) == tries):\n                raise\n            else:\n                timeout_value = (timeout(attempt) if callable(timeout) else timeout)\n                if (timeout_value > 0):\n                    time.sleep(timeout_value)\n", "label": 0}
{"function": "\n\ndef test_default_tag(temp_pkg):\n    subprocess.check_call([sys.executable, 'setup.py', 'bdist_wheel'], cwd=str(temp_pkg))\n    dist_dir = temp_pkg.join('dist')\n    assert dist_dir.check(dir=1)\n    wheels = dist_dir.listdir()\n    assert (len(wheels) == 1)\n    assert wheels[0].basename.startswith(('Test-1.0-py%s-' % (sys.version[0],)))\n    assert (wheels[0].ext == '.whl')\n", "label": 0}
{"function": "\n\ndef _hostmaskPatternEqual(pattern, hostmask):\n    try:\n        return (_patternCache[pattern](hostmask) is not None)\n    except KeyError:\n        fd = minisix.io.StringIO()\n        for c in pattern:\n            if (c == '*'):\n                fd.write('.*')\n            elif (c == '?'):\n                fd.write('.')\n            elif (c in '[{'):\n                fd.write('[[{]')\n            elif (c in '}]'):\n                fd.write('[}\\\\]]')\n            elif (c in '|\\\\'):\n                fd.write('[|\\\\\\\\]')\n            elif (c in '^~'):\n                fd.write('[~^]')\n            else:\n                fd.write(re.escape(c))\n        fd.write('$')\n        f = re.compile(fd.getvalue(), re.I).match\n        _patternCache[pattern] = f\n        return (f(hostmask) is not None)\n", "label": 0}
{"function": "\n\ndef get_by_index(src_gpu, ind):\n    '\\n    Get values in a GPUArray by index.\\n\\n    Parameters\\n    ----------\\n    src_gpu : pycuda.gpuarray.GPUArray\\n        GPUArray instance from which to extract values.\\n    ind : pycuda.gpuarray.GPUArray or numpy.ndarray\\n        Array of element indices to set. Must have an integer dtype.\\n\\n    Returns\\n    -------\\n    res_gpu : pycuda.gpuarray.GPUArray\\n        GPUArray with length of `ind` and dtype of `src_gpu` containing\\n        selected values.\\n\\n    Examples\\n    --------\\n    >>> import pycuda.gpuarray as gpuarray\\n    >>> import pycuda.autoinit\\n    >>> import numpy as np\\n    >>> import misc\\n    >>> src = np.random.rand(5).astype(np.float32)\\n    >>> src_gpu = gpuarray.to_gpu(src)\\n    >>> ind = gpuarray.to_gpu(np.array([0, 2, 4]))\\n    >>> res_gpu = misc.get_by_index(src_gpu, ind)\\n    >>> np.allclose(res_gpu.get(), src[[0, 2, 4]])\\n    True\\n\\n    Notes\\n    -----\\n    Only supports 1D index arrays.\\n\\n    May not be efficient for certain index patterns because of lack of inability\\n    to coalesce memory operations.\\n    '\n    assert (len(np.shape(ind)) == 1)\n    assert issubclass(ind.dtype.type, numbers.Integral)\n    N = len(ind)\n    if (not isinstance(ind, gpuarray.GPUArray)):\n        ind = gpuarray.to_gpu(ind)\n    dest_gpu = gpuarray.empty(N, dtype=src_gpu.dtype)\n    if (N == 0):\n        return dest_gpu\n    try:\n        func = get_by_index.cache[(src_gpu.dtype, ind.dtype)]\n    except KeyError:\n        data_ctype = tools.dtype_to_ctype(src_gpu.dtype)\n        ind_ctype = tools.dtype_to_ctype(ind.dtype)\n        v = '{data_ctype} *dest, {ind_ctype} *ind, {data_ctype} *src'.format(data_ctype=data_ctype, ind_ctype=ind_ctype)\n        func = elementwise.ElementwiseKernel(v, 'dest[i] = src[ind[i]]')\n        get_by_index.cache[(src_gpu.dtype, ind.dtype)] = func\n    func(dest_gpu, ind, src_gpu, range=slice(0, N, 1))\n    return dest_gpu\n", "label": 0}
{"function": "\n\ndef __init__(self, dataset, subset_iterator, preprocessor=None, fit_preprocessor=False, which_set=None, return_dict=True):\n    self.dataset = dataset\n    self.subset_iterator = list(subset_iterator)\n    dataset_iterator = dataset.iterator(mode='sequential', num_batches=1, data_specs=dataset.data_specs, return_tuple=True)\n    self._data = dataset_iterator.next()\n    self.preprocessor = preprocessor\n    self.fit_preprocessor = fit_preprocessor\n    self.which_set = which_set\n    if (which_set is not None):\n        which_set = np.atleast_1d(which_set)\n        assert len(which_set)\n        for label in which_set:\n            if (label not in ['train', 'valid', 'test']):\n                raise ValueError(\"Unrecognized subset '{}'\".format(label))\n        self.which_set = which_set\n    self.return_dict = return_dict\n", "label": 0}
{"function": "\n\n@staticmethod\ndef canMatch(l1, l2):\n    if ((l1 is None) and (l2 is None)):\n        return True\n    elif ((l1 is None) or (l2 is None)):\n        return False\n    try:\n        l1.intersect(l2)\n        return True\n    except EmptyLabelSet:\n        return False\n", "label": 0}
{"function": "\n\ndef rs_series_reversion(p, x, n, y):\n    \"\\n    Reversion of a series.\\n\\n    ``p`` is a series with ``O(x**n)`` of the form `p = a*x + f(x)`\\n    where `a` is a number different from 0.\\n\\n    `f(x) = sum( a\\\\_k*x\\\\_k, k in range(2, n))`\\n\\n      a_k : Can depend polynomially on other variables, not indicated.\\n      x : Variable with name x.\\n      y : Variable with name y.\\n\\n    Solve `p = y`, that is, given `a*x + f(x) - y = 0`,\\n    find the solution x = r(y) up to O(y**n)\\n\\n    Algorithm:\\n\\n    If `r\\\\_i` is the solution at order i, then:\\n    `a*r\\\\_i + f(r\\\\_i) - y = O(y**(i + 1))`\\n\\n    and if r_(i + 1) is the solution at order i + 1, then:\\n    `a*r\\\\_(i + 1) + f(r\\\\_(i + 1)) - y = O(y**(i + 2))`\\n\\n    We have, r_(i + 1) = r_i + e, such that,\\n    `a*e + f(r\\\\_i) = O(y**(i + 2))`\\n    or `e = -f(r\\\\_i)/a`\\n\\n    So we use the recursion relation:\\n    `r\\\\_(i + 1) = r\\\\_i - f(r\\\\_i)/a`\\n    with the boundary condition: `r\\\\_1 = y`\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_series_reversion, rs_trunc\\n    >>> R, x, y, a, b = ring('x, y, a, b', QQ)\\n    >>> p = x - x**2 - 2*b*x**2 + 2*a*b*x**2\\n    >>> p1 = rs_series_reversion(p, x, 3, y); p1\\n    -2*y**2*a*b + 2*y**2*b + y**2 + y\\n    >>> rs_trunc(p.compose(x, p1), y, 3)\\n    y\\n    \"\n    if rs_is_puiseux(p, x):\n        raise NotImplementedError\n    R = p.ring\n    nx = R.gens.index(x)\n    y = R(y)\n    ny = R.gens.index(y)\n    if _has_constant_term(p, x):\n        raise ValueError('p must not contain a constant term in the series variable')\n    a = _coefficient_t(p, (nx, 1))\n    zm = R.zero_monom\n    assert ((zm in a) and (len(a) == 1))\n    a = a[zm]\n    r = (y / a)\n    for i in range(2, n):\n        sp = rs_subs(p, {\n            x: r,\n        }, y, (i + 1))\n        sp = (_coefficient_t(sp, (ny, i)) * (y ** i))\n        r -= (sp / a)\n    return r\n", "label": 0}
{"function": "\n\ndef __init__(self, config_files=None, refresh=False, private=False, config_key=None, config_defaults=None, cloud=None):\n    if (config_files is None):\n        config_files = []\n    config = os_client_config.config.OpenStackConfig(config_files=(os_client_config.config.CONFIG_FILES + config_files))\n    self.extra_config = config.get_extra_config(config_key, config_defaults)\n    if (cloud is None):\n        self.clouds = [shade.OpenStackCloud(cloud_config=cloud_config) for cloud_config in config.get_all_clouds()]\n    else:\n        try:\n            self.clouds = [shade.OpenStackCloud(cloud_config=config.get_one_cloud(cloud))]\n        except os_client_config.exceptions.OpenStackConfigException as e:\n            raise shade.OpenStackCloudException(e)\n    if private:\n        for cloud in self.clouds:\n            cloud.private = True\n    if refresh:\n        for cloud in self.clouds:\n            cloud._cache.invalidate()\n", "label": 0}
{"function": "\n\ndef decode(message, pblite, ignore_first_item=False):\n    \"Decode pblite to Protocol Buffer message.\\n\\n    This method is permissive of decoding errors and will log them as warnings\\n    and continue decoding where possible.\\n\\n    The first element of the outer pblite list must often be ignored using the\\n    ignore_first_item parameter because it contains an abbreviation of the name\\n    of the protobuf message (eg.  cscmrp for ClientSendChatMessageResponseP)\\n    that's not part of the protobuf.\\n\\n    Args:\\n        message: protocol buffer message instance to decode into.\\n        pblite: list representing a pblite-serialized message.\\n        ignore_first_item: If True, ignore the item at index 0 in the pblite\\n            list, making the item at index 1 correspond to field 1 in the\\n            message.\\n    \"\n    if (not isinstance(pblite, list)):\n        logger.warning('Ignoring invalid message: expected list, got %r', type(pblite))\n        return\n    if ignore_first_item:\n        pblite = pblite[1:]\n    if ((len(pblite) > 0) and isinstance(pblite[(- 1)], dict)):\n        extra_fields = {int(field_number): value for (field_number, value) in pblite[(- 1)].items()}\n        pblite = pblite[:(- 1)]\n    else:\n        extra_fields = {\n            \n        }\n    fields_values = itertools.chain(enumerate(pblite, start=1), extra_fields.items())\n    for (field_number, value) in fields_values:\n        if (value is None):\n            continue\n        try:\n            field = message.DESCRIPTOR.fields_by_number[field_number]\n        except KeyError:\n            if (value not in [[], '', 0]):\n                logger.debug('Message %r contains unknown field %s with value %r', message.__class__.__name__, field_number, value)\n            continue\n        if (field.label == FieldDescriptor.LABEL_REPEATED):\n            _decode_repeated_field(message, field, value)\n        else:\n            _decode_field(message, field, value)\n", "label": 1}
{"function": "\n\ndef test_read_session():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test'],\n        }, key='test')\n        key = Key.load(h.get_key('test'))\n        assert (key.api_key == 'test')\n        assert (key.is_admin() == False)\n        assert (key.can_create_key() == False)\n        assert (key.can_create_user() == False)\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == False)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef test_ignore_table_fields(self):\n    c1 = Column('A', format='L', array=[True, False])\n    c2 = Column('B', format='X', array=[[0], [1]])\n    c3 = Column('C', format='4I', dim='(2, 2)', array=[[0, 1, 2, 3], [4, 5, 6, 7]])\n    c4 = Column('B', format='X', array=[[1], [0]])\n    c5 = Column('C', format='4I', dim='(2, 2)', array=[[1, 2, 3, 4], [5, 6, 7, 8]])\n    ta = BinTableHDU.from_columns([c1, c2, c3])\n    tb = BinTableHDU.from_columns([c1, c4, c5])\n    diff = TableDataDiff(ta.data, tb.data, ignore_fields=['B', 'C'])\n    assert diff.identical\n    assert (len(diff.common_columns) == 1)\n    assert (diff.common_column_names == set(['a']))\n    assert (diff.diff_ratio == 0)\n    assert (diff.diff_total == 0)\n", "label": 0}
{"function": "\n\ndef _copyFile(syn, entity, destinationId, version=None, replace=False, setProvenance='traceback'):\n    \"\\n    Copies most recent version of a file to a specified synapse ID.\\n\\n    :param entity:          A synapse ID of a File entity\\n\\n    :param destinationId:   Synapse ID of a folder/project that the file wants to be copied to\\n\\n    :param version:         Can specify version of a file. \\n                            Default to None\\n\\n    :param replace:         Can choose to replace files that have the same name \\n                            Default to False\\n    \\n    :param setProvenance:   Has three values to set the provenance of the copied entity:\\n                                traceback: Sets to the source entity\\n                                existing: Sets to source entity's original provenance (if it exists)\\n                                None: No provenance is set\\n    \"\n    ent = syn.get(entity, downloadFile=False, version=version, followLink=False)\n    if (not replace):\n        search = syn.query(('select name from entity where parentId ==\"%s\"' % destinationId))\n        for i in search['results']:\n            if (i['entity.name'] == ent.name):\n                raise ValueError(('An item named \"%s\" already exists in this location. File could not be copied' % ent.name))\n    profile = syn.getUserProfile()\n    if (setProvenance == 'traceback'):\n        act = Activity('Copied file', used=ent)\n    elif (setProvenance == 'existing'):\n        try:\n            act = syn.getProvenance(ent.id)\n        except Exception as e:\n            if (e.message.find('No activity') >= 0):\n                act = None\n            else:\n                raise e\n    elif ((setProvenance is None) or (setProvenance.lower() == 'none')):\n        act = None\n    else:\n        raise ValueError('setProvenance must be one of None, existing, or traceback')\n    fileHandleList = syn.restGET(('/entity/%s/version/%s/filehandles' % (ent.id, ent.versionNumber)))\n    for fileHandle in fileHandleList['list']:\n        if (fileHandle['id'] == ent.dataFileHandleId):\n            createdBy = fileHandle['createdBy']\n            break\n    else:\n        createdBy = None\n    if (profile.ownerId == createdBy):\n        new_ent = File(name=ent.name, parentId=destinationId)\n        new_ent.dataFileHandleId = ent.dataFileHandleId\n    else:\n        if (ent.externalURL is None):\n            store = True\n            ent = syn.get(entity, downloadFile=store, version=version)\n            path = ent.path\n        else:\n            store = False\n            ent = syn.get(entity, downloadFile=store, version=version)\n            path = ent.externalURL\n        new_ent = File(path, name=ent.name, parentId=destinationId, synapseStore=store)\n    new_ent.annotations = ent.annotations\n    if (act is not None):\n        new_ent = syn.store(new_ent, activity=act)\n    else:\n        new_ent = syn.store(new_ent)\n    return new_ent['id']\n", "label": 1}
{"function": "\n\n@patch('nefertari.view.BaseView._run_init_actions')\ndef test_init(self, run):\n    request = Mock(content_type='application/json', json={\n        'param1.foo': 'val1',\n        'param3': 'val3',\n    }, method='POST', accept=[''])\n    request.params.mixed.return_value = {\n        'param2.foo': 'val2',\n    }\n    view = DummyBaseView(context={\n        'foo': 'bar',\n    }, request=request)\n    run.assert_called_once_with()\n    assert (request.override_renderer == 'nefertari_json')\n    assert (list(sorted(view._params.keys())) == ['param1', 'param2', 'param3'])\n    assert (view._params['param1'] == {\n        'foo': 'val1',\n    })\n    assert (view._params['param2'] == {\n        'foo': 'val2',\n    })\n    assert (view._params['param3'] == 'val3')\n    assert (view.request == request)\n    assert (view.context == {\n        'foo': 'bar',\n    })\n    assert (view._before_calls == {\n        \n    })\n    assert (view._after_calls == {\n        \n    })\n", "label": 0}
{"function": "\n\ndef _query(function, api_url=None, api_key=None, api_version=None, room_id=None, method='GET', data=None):\n    '\\n    HipChat object method function to construct and execute on the API URL.\\n\\n    :param api_url:     The HipChat API URL.\\n    :param api_key:     The HipChat api key.\\n    :param function:    The HipChat api function to perform.\\n    :param api_version: The HipChat api version (v1 or v2).\\n    :param method:      The HTTP method, e.g. GET or POST.\\n    :param data:        The data to be sent for POST method.\\n    :return:            The json response from the API call or False.\\n    '\n    headers = {\n        \n    }\n    query_params = {\n        \n    }\n    if (not api_url):\n        try:\n            options = __salt__['config.option']('hipchat')\n            api_url = options.get('api_url')\n        except (NameError, KeyError, AttributeError):\n            pass\n    if ((not api_key) or (not api_version)):\n        try:\n            options = __salt__['config.option']('hipchat')\n            if (not api_key):\n                api_key = options.get('api_key')\n            if (not api_version):\n                api_version = options.get('api_version')\n        except (NameError, KeyError, AttributeError):\n            log.error('No HipChat api key or version found.')\n            return False\n    if room_id:\n        room_id = 'room/{0}/notification'.format(str(room_id))\n    else:\n        room_id = 'room/0/notification'\n    hipchat_functions = {\n        'v1': {\n            'rooms': {\n                'request': 'rooms/list',\n                'response': 'rooms',\n            },\n            'users': {\n                'request': 'users/list',\n                'response': 'users',\n            },\n            'message': {\n                'request': 'rooms/message',\n                'response': 'status',\n            },\n        },\n        'v2': {\n            'rooms': {\n                'request': 'room',\n                'response': 'items',\n            },\n            'users': {\n                'request': 'user',\n                'response': 'items',\n            },\n            'message': {\n                'request': room_id,\n                'response': None,\n            },\n        },\n    }\n    use_api_url = 'https://api.hipchat.com'\n    if api_url:\n        use_api_url = api_url\n    base_url = _urljoin(use_api_url, (api_version + '/'))\n    path = hipchat_functions.get(api_version).get(function).get('request')\n    url = _urljoin(base_url, path, False)\n    if (api_version == 'v1'):\n        query_params['format'] = 'json'\n        query_params['auth_token'] = api_key\n        if (method == 'POST'):\n            headers['Content-Type'] = 'application/x-www-form-urlencoded'\n        if data:\n            if data.get('notify', None):\n                data['notify'] = 1\n            data = _urlencode(data)\n    elif (api_version == 'v2'):\n        headers['Authorization'] = 'Bearer {0}'.format(api_key)\n        if data:\n            data = json.dumps(data)\n        if (method == 'POST'):\n            headers['Content-Type'] = 'application/json'\n    else:\n        log.error('Unsupported HipChat API version')\n        return False\n    result = salt.utils.http.query(url, method, params=query_params, data=data, decode=True, status=True, header_dict=headers, opts=__opts__)\n    if (result.get('status', None) == salt.ext.six.moves.http_client.OK):\n        response = hipchat_functions.get(api_version).get(function).get('response')\n        return result.get('dict', {\n            \n        }).get(response, None)\n    elif (result.get('status', None) == salt.ext.six.moves.http_client.NO_CONTENT):\n        return False\n    else:\n        log.debug(url)\n        log.debug(query_params)\n        log.debug(data)\n        log.debug(result)\n        if result.get('error'):\n            log.error(result)\n        return False\n", "label": 1}
{"function": "\n\ndef evacuate_host(request, host, target=None, on_shared_storage=False):\n    hypervisors = novaclient(request).hypervisors.search(host, True)\n    response = []\n    err_code = None\n    for hypervisor in hypervisors:\n        hyper = Hypervisor(hypervisor)\n        for server in hyper.servers:\n            try:\n                novaclient(request).servers.evacuate(server['uuid'], target, on_shared_storage)\n            except nova_exceptions.ClientException as err:\n                err_code = err.code\n                msg = _('Name: %(name)s ID: %(uuid)s')\n                msg = (msg % {\n                    'name': server['name'],\n                    'uuid': server['uuid'],\n                })\n                response.append(msg)\n    if err_code:\n        msg = (_('Failed to evacuate instances: %s') % ', '.join(response))\n        raise nova_exceptions.ClientException(err_code, msg)\n    return True\n", "label": 0}
{"function": "\n\ndef add_proof(self, lhs, rhs):\n    'Adds a proof obligation to show the rhs is the representation of the lhs'\n    assert isinstance(lhs, Gen)\n    assert (lhs.prove == False)\n    assert isinstance(rhs, Gen)\n    assert (rhs.prove == True)\n    assert (self == lhs.zkp == rhs.zkp)\n    self.proofs += [(lhs, rhs)]\n", "label": 0}
{"function": "\n\ndef seed(self, seed=None):\n    '\\n        Re-initialize each random stream.\\n\\n        Parameters\\n        ----------\\n        seed : None or integer in range 0 to 2**30\\n            Each random stream will be assigned a unique state that depends\\n            deterministically on this value.\\n\\n        Returns\\n        -------\\n        None\\n\\n        '\n    if (seed is None):\n        seed = self.default_instance_seed\n    self.set_rstate(seed)\n    for (old_r, new_r, size, nstreams) in self.state_updates:\n        if (nstreams is None):\n            nstreams = self.n_streams(size)\n        rstates = self.get_substream_rstates(nstreams, new_r.owner.outputs[1].dtype)\n        assert (old_r.get_value(borrow=True, return_internal_type=True).shape == rstates.shape)\n        assert (rstates.dtype == old_r.dtype)\n        old_r.set_value(rstates, borrow=True)\n", "label": 0}
{"function": "\n\ndef findMode_Mult_faster(Nvec, Wmat):\n    ' Find modes to multinomial with given parameters\\n        vectorized to solve many problems simultaneously\\n\\n      Args\\n      ------\\n      Nvec : 1D array, size P\\n             contains non-negative integers\\n      Wmat : 2D array, size P x K\\n             each row is valid probability vector of length K\\n             Wmat[p] has non-negative entries, sums to one\\n\\n      Returns\\n      -------\\n      Nmat : 2D sparse array, size P x K\\n          Nmat[p,:] = argmax_{n} \\\\log p_{mult}( Nmat[p] | Nvec[p], Wmat[p] ) \\n  '\n    Nvec = np.asarray(Nvec, dtype=np.int64)\n    if (Nvec.ndim < 1):\n        Nvec = Nvec[np.newaxis]\n    Wmat = np.asarray(Wmat, dtype=np.float64)\n    if (Wmat.ndim < 2):\n        Wmat = Wmat[np.newaxis, :]\n    Nmat = np.zeros(Wmat.shape, dtype=np.int64)\n    np.floor((Nvec[:, np.newaxis] * Wmat), out=Nmat)\n    activeRows = np.flatnonzero((Nmat.sum(axis=1) < Nvec))\n    if (len(activeRows) > 0):\n        f = ((Nvec[:, np.newaxis] * Wmat) - Nmat)\n        q = ((1 - f) / Wmat)\n        q = np.take(q, activeRows, axis=0)\n    while (len(activeRows) > 0):\n        minIDs = np.argmin(q, axis=1)\n        ids = np.ravel_multi_index([activeRows, minIDs], Nmat.shape)\n        Nmat.ravel()[ids] += 1\n        stillActiveMask = (np.take(Nmat, activeRows, axis=0).sum(axis=1) < np.take(Nvec, activeRows))\n        activeRows = activeRows[stillActiveMask]\n        ids = ids[stillActiveMask]\n        minIDs = minIDs[stillActiveMask]\n        q = np.take(q, np.flatnonzero(stillActiveMask), axis=0)\n        qids = np.ravel_multi_index([np.arange(len(minIDs)), minIDs], q.shape)\n        q.ravel()[qids] += (1.0 / Wmat.ravel()[ids])\n    assert np.all((np.abs((Nmat.sum(axis=1) - Nvec)) < 0.0001))\n    return Nmat\n", "label": 0}
{"function": "\n\ndef run(self):\n    try:\n        process = subprocess.Popen(['esformatter'], bufsize=(160 * len(self.code)), stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, startupinfo=getStartupInfo())\n        if ST2:\n            (stdout, stderr) = process.communicate(self.code)\n            self.result = re.sub('(\\\\r|\\\\r\\\\n|\\\\n)\\\\Z', '', stdout).decode('utf-8')\n        else:\n            (stdout, stderr) = process.communicate(self.code)\n            self.result = re.sub('(\\\\r|\\\\r\\\\n|\\\\n)\\\\Z', '', str(stdout, encoding='utf-8'))\n        if stderr:\n            self.result = False\n            if ST2:\n                self.error = str(stderr.decode('utf-8'))\n            else:\n                self.error = str(stderr, encoding='utf-8')\n    except Exception as e:\n        self.result = False\n        self.error = str(e)\n", "label": 0}
{"function": "\n\ndef parse_vexrc(inp, environ):\n    'Iterator yielding key/value pairs from given stream.\\n\\n    yields tuples of heading, key, value.\\n    '\n    heading = None\n    errors = []\n    with inp:\n        for (line_number, line) in enumerate(inp):\n            line = line.decode('utf-8')\n            if (not line.strip()):\n                continue\n            extracted_heading = extract_heading(line)\n            if (extracted_heading is not None):\n                heading = extracted_heading\n                continue\n            kv_tuple = extract_key_value(line, environ)\n            if (kv_tuple is None):\n                errors.append((line_number, line))\n                continue\n            try:\n                (yield (heading, kv_tuple[0], kv_tuple[1]))\n            except GeneratorExit:\n                break\n    if errors:\n        raise InvalidConfigError(inp.name, errors)\n", "label": 0}
{"function": "\n\ndef main(args=None):\n    'Run the main command-line interface for beets. Includes top-level\\n    exception handlers that print friendly error messages.\\n    '\n    try:\n        _raw_main(args)\n    except UserError as exc:\n        message = (exc.args[0] if exc.args else None)\n        log.error('error: {0}', message)\n        sys.exit(1)\n    except util.HumanReadableException as exc:\n        exc.log(log)\n        sys.exit(1)\n    except library.FileOperationError as exc:\n        log.debug('{}', traceback.format_exc())\n        log.error('{}', exc)\n        sys.exit(1)\n    except confit.ConfigError as exc:\n        log.error('configuration error: {0}', exc)\n        sys.exit(1)\n    except db_query.InvalidQueryError as exc:\n        log.error('invalid query: {0}', exc)\n        sys.exit(1)\n    except IOError as exc:\n        if (exc.errno == errno.EPIPE):\n            pass\n        else:\n            raise\n    except KeyboardInterrupt:\n        log.debug('{}', traceback.format_exc())\n", "label": 0}
{"function": "\n\ndef test_merge_tiny_output_opt(tiffs):\n    outputname = str(tiffs.join('merged.tif'))\n    inputs = [str(x) for x in tiffs.listdir()]\n    inputs.sort()\n    runner = CliRunner()\n    result = runner.invoke(merge, (inputs + ['-o', outputname]))\n    assert (result.exit_code == 0)\n    with rasterio.open(outputname) as src:\n        data = src.read()\n        assert (data[0][0:2, 1] == 120).all()\n        assert (data[0][0:2, 2:4] == 90).all()\n        assert (data[0][2][1] == 60)\n        assert (data[0][3][0] == 40)\n", "label": 0}
{"function": "\n\ndef execute(self, *tasks):\n    '\\n        Execute one or more ``tasks`` in sequence.\\n\\n        :param tasks:\\n            An all-purpose iterable of \"tasks to execute\", each member of which\\n            may take one of the following forms:\\n\\n            **A string** naming a task from the Executor\\'s `.Collection`. This\\n            name may contain dotted syntax appropriate for calling namespaced\\n            tasks, e.g. ``subcollection.taskname``. Such tasks are executed\\n            without arguments.\\n\\n            **A two-tuple** whose first element is a task name string (as\\n            above) and whose second element is a dict suitable for use as\\n            ``**kwargs`` when calling the named task. E.g.::\\n\\n                [\\n                    (\\'task1\\', {}),\\n                    (\\'task2\\', {\\'arg1\\': \\'val1\\'}),\\n                    ...\\n                ]\\n\\n            is equivalent, roughly, to::\\n\\n                task1()\\n                task2(arg1=\\'val1\\')\\n\\n            **A `.ParserContext`** instance, whose ``.name`` attribute is used\\n            as the task name and whose ``.as_kwargs`` attribute is used as the\\n            task kwargs (again following the above specifications).\\n\\n            .. note::\\n                When called without any arguments at all (i.e. when ``*tasks``\\n                is empty), the default task from ``self.collection`` is used\\n                instead, if defined.\\n\\n        :returns:\\n            A dict mapping task objects to their return values.\\n\\n            This dict may include pre- and post-tasks if any were executed. For\\n            example, in a collection with a ``build`` task depending on another\\n            task named ``setup``, executing ``build`` will result in a dict\\n            with two keys, one for ``build`` and one for ``setup``.\\n        '\n    debug('Examining top level tasks {0!r}'.format([x for x in tasks]))\n    calls = self.normalize(tasks)\n    debug('Tasks (now Calls) with kwargs: {0!r}'.format(calls))\n    direct = list(calls)\n    config = self.config.clone()\n    expanded = self.expand_calls(calls, config)\n    try:\n        dedupe = config.tasks.dedupe\n    except AttributeError:\n        dedupe = True\n    calls = (self.dedupe(expanded) if dedupe else expanded)\n    results = {\n        \n    }\n    for call in calls:\n        autoprint = ((call in direct) and call.autoprint)\n        args = call.args\n        debug('Executing {0!r}'.format(call))\n        if call.contextualized:\n            args = ((call.context,) + args)\n        result = call.task(*args, **call.kwargs)\n        if autoprint:\n            print(result)\n        results[call.task] = result\n    return results\n", "label": 0}
{"function": "\n\ndef start(name):\n    '\\n    Start a named virtual machine\\n    '\n    ret = {\n        \n    }\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    data = vm_info(name, quiet=True)\n    if (not data):\n        __jid_event__.fire_event({\n            'message': 'Failed to find VM {0} to start'.format(name),\n        }, 'progress')\n        return 'fail'\n    host = next(six.iterkeys(data))\n    if (data[host][name]['state'] == 'running'):\n        print('VM {0} is already running'.format(name))\n        return 'bad state'\n    try:\n        cmd_ret = client.cmd_iter(host, 'virt.start', [name], timeout=600)\n    except SaltClientError as client_error:\n        return 'Virtual machine {0} not started: {1}'.format(name, client_error)\n    for comp in cmd_ret:\n        ret.update(comp)\n    __jid_event__.fire_event({\n        'message': 'Started VM {0}'.format(name),\n    }, 'progress')\n    return 'good'\n", "label": 0}
{"function": "\n\ndef ami(account):\n    myCredAccount = CredAccountAws()\n    if (not ('x509Cert' in account)):\n        printer.out('x509Cert in ami account not found', printer.ERROR)\n        return\n    if (not ('x509PrivateKey' in account)):\n        printer.out('x509PrivateKey in ami account not found', printer.ERROR)\n        return\n    if (not ('accessKey' in account)):\n        printer.out('accessKey in ami account not found', printer.ERROR)\n        return\n    if (not ('secretAccessKey' in account)):\n        printer.out('secretAccessKey in ami account not found', printer.ERROR)\n        return\n    if (not ('accountNumber' in account)):\n        printer.out('accountNumber for ami account not found', printer.ERROR)\n        return\n    if (not ('name' in account)):\n        printer.out('name for ami account not found', printer.ERROR)\n        return\n    myCredAccount.accountNumber = account['accountNumber']\n    myCredAccount.secretAccessKeyID = account['secretAccessKey']\n    myCredAccount.accessKeyID = account['accessKey']\n    myCredAccount.name = account['name']\n    myCertificates = certificates()\n    myCredAccount.certificates = myCertificates\n    try:\n        myCertificate = certificate()\n        with open(account['x509Cert'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'x509'\n        myCertificate.name = ntpath.basename(account['x509Cert'])\n        myCertificates.add_certificate(myCertificate)\n        myCertificate = certificate()\n        with open(account['x509PrivateKey'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'ec2PrivateKey'\n        myCertificate.name = ntpath.basename(account['x509PrivateKey'])\n        myCertificates.add_certificate(myCertificate)\n        if ('keyPairPrivateKey' in account):\n            myCertificate = certificate()\n            with open(account['keyPairPrivateKey'], 'r') as myfile:\n                myCertificate.certStr = myfile.read()\n            myCertificate.type_ = 'ec2KeyPairPrivateKey'\n            myCertificate.name = ntpath.basename(account['keyPairPrivateKey'])\n            myCertificates.add_certificate(myCertificate)\n            myCredAccount.keyPairName = os.path.splitext(myCertificate.name)[0]\n    except IOError as e:\n        printer.out(('File error: ' + str(e)), printer.ERROR)\n        return\n    return myCredAccount\n", "label": 0}
{"function": "\n\ndef test_graph_equality_attributes(self):\n    '\\n        Graph equality test. This one checks node equality. \\n        '\n    gr = graph()\n    gr.add_nodes([0, 1, 2])\n    gr.add_edge((0, 1))\n    gr.add_node_attribute(1, ('a', 'x'))\n    gr.add_node_attribute(2, ('b', 'y'))\n    gr.add_edge_attribute((0, 1), ('c', 'z'))\n    gr2 = deepcopy(gr)\n    gr3 = deepcopy(gr)\n    gr3.del_edge((0, 1))\n    gr3.add_edge((0, 1))\n    gr4 = deepcopy(gr)\n    gr4.del_edge((0, 1))\n    gr4.add_edge((0, 1))\n    gr4.add_edge_attribute((0, 1), ('d', 'k'))\n    gr5 = deepcopy(gr)\n    gr5.del_node(2)\n    gr5.add_node(2)\n    gr5.add_node_attribute(0, ('d', 'k'))\n    assert (gr == gr2)\n    assert (gr2 == gr)\n    assert (gr != gr3)\n    assert (gr3 != gr)\n    assert (gr != gr4)\n    assert (gr4 != gr)\n    assert (gr != gr5)\n    assert (gr5 != gr)\n", "label": 0}
{"function": "\n\ndef write(self, data):\n    if (self.paused or ((not self.iAmStreaming) and (not self.outstandingPull))):\n        self._buffer.append(data)\n    elif (self.consumer is not None):\n        assert (not self._buffer), 'Writing fresh data to consumer before my buffer is empty!'\n        bytesSent = self._writeSomeData(data)\n        self.outstandingPull = False\n        if (not (bytesSent == len(data))):\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer.append(data[bytesSent:])\n    if ((self.producer is not None) and self.producerIsStreaming):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (bytesBuffered >= self.bufferSize):\n            self.producer.pauseProducing()\n            self.producerPaused = True\n", "label": 1}
{"function": "\n\ndef from_iso_format(string):\n    '\\n    Parse a string representing the date and time in ISO 8601 format,\\n    YYYY-MM-DDTHH:MM:SS.mmmmmm or, if microsecond is 0, YYYY-MM-DDTHH:MM:SS\\n\\n    If utcoffset() does not return None, a 6-character string is appended,\\n    giving the UTC offset in (signed) hours and minutes:\\n    YYYY-MM-DDTHH:MM:SS.mmmmmm+HH:MM or, if microsecond is 0\\n    YYYY-MM-DDTHH:MM:SS+HH:MM\\n    '\n    patterns = [('\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2}.\\\\d{0,6}$', '%Y-%m-%d %H:%M:%S.%f'), ('\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2}$', '%Y-%m-%d %H:%M:%S')]\n    result = None\n    for (regex_pattern, strptime_pattern) in patterns:\n        match = re.match(regex_pattern, string)\n        if match:\n            result = datetime.datetime.strptime(string, strptime_pattern)\n            break\n        elif (len(string) >= 6):\n            timezone = string[(- 6):]\n            if re.match('[+-]\\\\d{2}:\\\\d{2}$', timezone):\n                match = re.match(regex_pattern, string[:(- 6)])\n                if match:\n                    result = datetime.datetime.strptime(string[:(- 6)], strptime_pattern)\n                    offset_hours = int(timezone[0:3])\n                    offset_minutes = int(timezone[4:6])\n                    if (offset_hours < 0):\n                        offset_minutes *= (- 1)\n                    offset_minutes += (offset_hours * 60)\n                    result = result.replace(tzinfo=TimezoneInfo(offset_minutes))\n                    break\n    if (not result):\n        raise ValueError('String not in ISO 8601 format')\n    assert result, string\n    return result\n", "label": 0}
{"function": "\n\n@patch('os.getcwd', return_value='{}/tests/testsrc/someuser/src/some_package'.format(base))\n@setup_test\ndef test_sync_package_and_clean(mock_os):\n    args = argparse.Namespace()\n    args.package = ['some_package/test.py']\n    sync(args)\n    assert (os.path.isdir('./tests/testsrc/someuser/.envies/someenv/some_package') == True)\n    assert (os.path.isfile('./tests/testsrc/someuser/.virtualenvs/someenv/lib/python2.7/site-packages/some_package/test.py') == True)\n    args.all = False\n    clean(args)\n    assert (os.path.isdir('./tests/testsrc/someuser/.envies/someenv/some_package') == False)\n    assert (os.path.isfile('./tests/testsrc/someuser/.virtualenvs/someenv/lib/python2.7/site-packages/some_package/test.py') == False)\n", "label": 0}
{"function": "\n\ndef write_fasta(ofile, s, chunk=60, id=None, reformatter=None):\n    'Trivial FASTA output'\n    if (id is None):\n        try:\n            id = str(s.id)\n        except AttributeError:\n            id = new_seq_id()\n    ofile.write((('>' + id) + '\\n'))\n    seq = str(s)\n    if (reformatter is not None):\n        seq = reformatter(seq)\n    end = len(seq)\n    pos = 0\n    while 1:\n        ofile.write((seq[pos:(pos + chunk)] + '\\n'))\n        pos += chunk\n        if (pos >= end):\n            break\n    return id\n", "label": 0}
{"function": "\n\ndef _check_skip_installed(self, req_to_install, finder):\n    \"Check if req_to_install should be skipped.\\n\\n        This will check if the req is installed, and whether we should upgrade\\n        or reinstall it, taking into account all the relevant user options.\\n\\n        After calling this req_to_install will only have satisfied_by set to\\n        None if the req_to_install is to be upgraded/reinstalled etc. Any\\n        other value will be a dist recording the current thing installed that\\n        satisfies the requirement.\\n\\n        Note that for vcs urls and the like we can't assess skipping in this\\n        routine - we simply identify that we need to pull the thing down,\\n        then later on it is pulled down and introspected to assess upgrade/\\n        reinstalls etc.\\n\\n        :return: A text reason for why it was skipped, or None.\\n        \"\n    req_to_install.check_if_exists()\n    if req_to_install.satisfied_by:\n        skip_reason = 'satisfied (use --upgrade to upgrade)'\n        if self.upgrade:\n            best_installed = False\n            if (not (self.force_reinstall or req_to_install.link)):\n                try:\n                    finder.find_requirement(req_to_install, self.upgrade)\n                except BestVersionAlreadyInstalled:\n                    skip_reason = 'up-to-date'\n                    best_installed = True\n                except DistributionNotFound:\n                    pass\n            if (not best_installed):\n                if (not (self.use_user_site and (not dist_in_usersite(req_to_install.satisfied_by)))):\n                    req_to_install.conflicts_with = req_to_install.satisfied_by\n                req_to_install.satisfied_by = None\n        return skip_reason\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef get_recipient_info(self, message, contact_cache):\n    recipient_id = message.couch_recipient\n    if (recipient_id in contact_cache):\n        return contact_cache[recipient_id]\n    doc = None\n    if (recipient_id not in [None, '']):\n        try:\n            if (message.couch_recipient_doc_type == 'CommCareCase'):\n                doc = CommCareCase.get(recipient_id)\n            else:\n                doc = CouchUser.get_by_user_id(recipient_id)\n        except Exception:\n            pass\n    if doc:\n        doc_info = get_doc_info(doc.to_json(), self.domain)\n    else:\n        doc_info = None\n    contact_cache[recipient_id] = doc_info\n    return doc_info\n", "label": 0}
{"function": "\n\ndef copy_file(self, path, prefixed_path, source_storage, **options):\n    '\\n        Attempt to copy ``path`` with storage\\n        '\n    if (prefixed_path in self.copied_files):\n        return self.log((\"Skipping '%s' (already copied earlier)\" % path))\n    if (not self.delete_file(path, prefixed_path, source_storage, **options)):\n        return\n    source_path = source_storage.path(path)\n    if options['dry_run']:\n        self.log((\"Pretending to copy '%s'\" % source_path), level=1)\n    else:\n        self.log((\"Copying '%s'\" % source_path), level=1)\n        if self.local:\n            full_path = self.storage.path(prefixed_path)\n            try:\n                os.makedirs(os.path.dirname(full_path))\n            except OSError:\n                pass\n            shutil.copy2(source_path, full_path)\n        else:\n            source_file = source_storage.open(path)\n            self.storage.save(prefixed_path, source_file)\n    if (not (prefixed_path in self.copied_files)):\n        self.copied_files.append(prefixed_path)\n", "label": 0}
{"function": "\n\ndef _check_criterion(self, criterion_k, criterion_v, payload_lookup):\n    if ('type' not in criterion_v):\n        return False\n    criteria_operator = criterion_v['type']\n    criteria_pattern = criterion_v.get('pattern', None)\n    try:\n        criteria_pattern = self._render_criteria_pattern(criteria_pattern=criteria_pattern)\n    except Exception:\n        LOG.exception(('Failed to render pattern value \"%s\" for key \"%s\"' % (criteria_pattern, criterion_k)), extra=self._base_logger_context)\n        return False\n    try:\n        matches = payload_lookup.get_value(criterion_k)\n        if matches:\n            payload_value = (matches[0] if (len(matches) > 0) else matches)\n        else:\n            payload_value = None\n    except:\n        LOG.exception('Failed transforming criteria key %s', criterion_k, extra=self._base_logger_context)\n        return False\n    op_func = criteria_operators.get_operator(criteria_operator)\n    try:\n        result = op_func(value=payload_value, criteria_pattern=criteria_pattern)\n    except:\n        LOG.exception('There might be a problem with critera in rule %s.', self.rule, extra=self._base_logger_context)\n        return False\n    return (result, payload_value, criteria_pattern)\n", "label": 0}
{"function": "\n\ndef cleanup(self, _warn=False):\n    if (self.name and (not self._closed)):\n        try:\n            self._rmtree(self.name)\n        except (TypeError, AttributeError) as ex:\n            if ('None' not in str(ex)):\n                raise\n            print_('ERROR: {!r} while cleaning up {!r}'.format(ex, self), file=_sys.stderr)\n            return\n        self._closed = True\n        if _warn:\n            self._warn('Implicitly cleaning up {!r}'.format(self), Warning)\n", "label": 0}
{"function": "\n\ndef test_as_coeff_exponent():\n    assert ((3 * (x ** 4)).as_coeff_exponent(x) == (3, 4))\n    assert ((2 * (x ** 3)).as_coeff_exponent(x) == (2, 3))\n    assert ((4 * (x ** 2)).as_coeff_exponent(x) == (4, 2))\n    assert ((6 * (x ** 1)).as_coeff_exponent(x) == (6, 1))\n    assert ((3 * (x ** 0)).as_coeff_exponent(x) == (3, 0))\n    assert ((2 * (x ** 0)).as_coeff_exponent(x) == (2, 0))\n    assert ((1 * (x ** 0)).as_coeff_exponent(x) == (1, 0))\n    assert ((0 * (x ** 0)).as_coeff_exponent(x) == (0, 0))\n    assert (((- 1) * (x ** 0)).as_coeff_exponent(x) == ((- 1), 0))\n    assert (((- 2) * (x ** 0)).as_coeff_exponent(x) == ((- 2), 0))\n    assert (((2 * (x ** 3)) + (pi * (x ** 3))).as_coeff_exponent(x) == ((2 + pi), 3))\n    assert (((x * log(2)) / ((2 * x) + (pi * x))).as_coeff_exponent(x) == ((log(2) / (2 + pi)), 0))\n    D = Derivative\n    f = Function('f')\n    fx = D(f(x), x)\n    assert (fx.as_coeff_exponent(f(x)) == (fx, 0))\n", "label": 1}
{"function": "\n\ndef test_caching(self):\n    changed = False\n\n    class TestLoader(loaders.BaseLoader):\n\n        def get_source(self, environment, template):\n            return ('foo', None, (lambda : (not changed)))\n    env = Environment(loader=TestLoader(), cache_size=(- 1))\n    tmpl = env.get_template('template')\n    assert (tmpl is env.get_template('template'))\n    changed = True\n    assert (tmpl is not env.get_template('template'))\n    changed = False\n    env = Environment(loader=TestLoader(), cache_size=0)\n    assert (env.get_template('template') is not env.get_template('template'))\n    env = Environment(loader=TestLoader(), cache_size=2)\n    t1 = env.get_template('one')\n    t2 = env.get_template('two')\n    assert (t2 is env.get_template('two'))\n    assert (t1 is env.get_template('one'))\n    t3 = env.get_template('three')\n    assert ('one' in env.cache)\n    assert ('two' not in env.cache)\n    assert ('three' in env.cache)\n", "label": 0}
{"function": "\n\ndef __init__(self, cgroups, kill_process_fn, hardtimelimit, softtimelimit, walltimelimit, pid_to_kill, cores, callbackFn=(lambda reason: None)):\n    super(_TimelimitThread, self).__init__()\n    if (hardtimelimit or softtimelimit):\n        assert (CPUACCT in cgroups)\n    assert (walltimelimit is not None)\n    if cores:\n        self.cpuCount = len(cores)\n    else:\n        try:\n            self.cpuCount = multiprocessing.cpu_count()\n        except NotImplementedError:\n            self.cpuCount = 1\n    self.daemon = True\n    self.cgroups = cgroups\n    self.timelimit = (hardtimelimit or ((((60 * 60) * 24) * 365) * 100))\n    self.softtimelimit = (softtimelimit or ((((60 * 60) * 24) * 365) * 100))\n    self.latestKillTime = (util.read_monotonic_time() + walltimelimit)\n    self.pid_to_kill = pid_to_kill\n    self.callback = callbackFn\n    self.kill_process = kill_process_fn\n    self.finished = threading.Event()\n", "label": 0}
{"function": "\n\ndef doAuthenticate(self, msg):\n    if (not self.authenticate_decoder):\n        self.authenticate_decoder = ircutils.AuthenticateDecoder()\n    self.authenticate_decoder.feed(msg)\n    if (not self.authenticate_decoder.ready):\n        return\n    string = self.authenticate_decoder.get()\n    self.authenticate_decoder = None\n    mechanism = self.sasl_current_mechanism\n    if (mechanism == 'ecdsa-nist256p-challenge'):\n        if (string == b''):\n            self.sendSaslString(self.sasl_username.encode('utf-8'))\n            return\n        try:\n            with open(self.sasl_ecdsa_key) as fd:\n                private_key = SigningKey.from_pem(fd.read())\n            authstring = private_key.sign(base64.b64decode(msg.args[0].encode()))\n            self.sendSaslString(authstring)\n        except (BadDigestError, OSError, ValueError):\n            self.sendMsg(ircmsgs.IrcMsg(command='AUTHENTICATE', args=('*',)))\n            self.tryNextSaslMechanism()\n    elif (mechanism == 'external'):\n        self.sendSaslString(b'')\n    elif (mechanism == 'plain'):\n        authstring = b'\\x00'.join([self.sasl_username.encode('utf-8'), self.sasl_username.encode('utf-8'), self.sasl_password.encode('utf-8')])\n        self.sendSaslString(authstring)\n", "label": 0}
{"function": "\n\ndef _attach_unknown_suggestions(self, unknown_forms):\n    'If there are any unknown forms, try and find the best possible matches\\n        from deleted apps or copied apps. If no suggestion is found, say so\\n        but provide the xmlns.\\n        '\n    if unknown_forms:\n        possibilities = self._get_unknown_form_possibilities()\n\n        class AppCache(dict):\n\n            def __init__(self, domain):\n                super(AppCache, self).__init__()\n                self.domain = domain\n\n            def __getitem__(self, item):\n                if (item not in self):\n                    try:\n                        self[item] = get_app(app_id=item, domain=self.domain)\n                    except Http404:\n                        pass\n                return super(AppCache, self).__getitem__(item)\n        app_cache = AppCache(self.domain)\n        for form in unknown_forms:\n            app = None\n            if form['app']['id']:\n                try:\n                    app = app_cache[form['app']['id']]\n                    form['has_app'] = True\n                except KeyError:\n                    form['app_does_not_exist'] = True\n                    form['possibilities'] = possibilities[form['xmlns']]\n                    if form['possibilities']:\n                        form['duplicate'] = True\n                else:\n                    if (app.domain != self.domain):\n                        logging.error(('submission tagged with app from wrong domain: %s' % app.get_id))\n                    else:\n                        if app.copy_of:\n                            try:\n                                app = app_cache[app.copy_of]\n                                form['app_copy'] = {\n                                    'id': app.get_id,\n                                    'name': app.name,\n                                }\n                            except KeyError:\n                                form['app_copy'] = {\n                                    'id': app.copy_of,\n                                    'name': '?',\n                                }\n                        if app.is_deleted():\n                            form['app_deleted'] = {\n                                'id': app.get_id,\n                            }\n                        try:\n                            app_forms = app.get_xmlns_map()[form['xmlns']]\n                        except AttributeError:\n                            app_forms = None\n                            form['has_app'] = True\n                        if app_forms:\n                            app_form = app_forms[0]\n                            if (app_form.doc_type == 'UserRegistrationForm'):\n                                form['is_user_registration'] = True\n                            else:\n                                app_module = app_form.get_module()\n                                form['module'] = app_module\n                                form['form'] = app_form\n                            form['show_xmlns'] = False\n                        if ((not form.get('app_copy')) and (not form.get('app_deleted'))):\n                            form['no_suggestions'] = True\n                if app:\n                    form['app'] = {\n                        'id': app.get_id,\n                        'name': app.name,\n                        'langs': app.langs,\n                    }\n            else:\n                form['possibilities'] = possibilities[form['xmlns']]\n                if form['possibilities']:\n                    form['duplicate'] = True\n                else:\n                    form['no_suggestions'] = True\n    return unknown_forms\n", "label": 1}
{"function": "\n\ndef add_block_proposal(self, p):\n    assert isinstance(p, BlockProposal)\n    if self.has_blockproposal(p.blockhash):\n        self.log('known block_proposal')\n        return\n    assert p.signing_lockset.has_quorum\n    assert (p.signing_lockset.height == (p.height - 1))\n    for v in p.signing_lockset:\n        self.add_vote(v)\n    self.block_candidates[p.blockhash] = p\n", "label": 0}
{"function": "\n\ndef render_image(self, rgbobj, dst_x, dst_y):\n    for ax in self.figure.axes:\n        if (not (ax in (self.ax_img, self.ax_util))):\n            if hasattr(ax, 'lines'):\n                for line in ax.lines:\n                    try:\n                        line._transformed_path.invalidate()\n                    except AttributeError:\n                        pass\n    if self.in_axes:\n        self.render_image2(rgbobj, dst_x, dst_y)\n    else:\n        self.render_image1(rgbobj, dst_x, dst_y)\n    self.ax_util.cla()\n    if self.t_['show_pan_position']:\n        self.ax_util.add_line(self.cross1)\n        self.ax_util.add_line(self.cross2)\n    if self.message:\n        self.draw_message(self.message)\n    self.figure.canvas.draw()\n", "label": 0}
{"function": "\n\ndef dates(self):\n    'returns the years and months for which there are posts'\n    if o_settings.CACHE_ENABLED:\n        key = get_key('posts_dates')\n        cached = cache.get(key)\n        if cached:\n            return cached\n    posts = self.published()\n    dates = OrderedDict()\n    for post in posts:\n        key = post.created.strftime('%Y_%m')\n        try:\n            dates[key][1] = (dates[key][1] + 1)\n        except KeyError:\n            dates[key] = [post.created, 1]\n    if o_settings.CACHE_ENABLED:\n        cache.set(key, dates, o_settings.CACHE_TIMEOUT)\n    return dates\n", "label": 0}
{"function": "\n\n@eventmgr_rfc1459.message('AUTHENTICATE', min_params=1, allow_unregistered=True)\ndef m_AUTHENTICATE(cli, ev_msg):\n    if ((len(ev_msg['params']) == 1) and (ev_msg['params'][0] == '*')):\n        if getattr(cli, 'sasl', None):\n            cli.dump_numeric('906', ['SASL authentication aborted'])\n        else:\n            cli.dump_numeric('904', ['SASL authentication failed'])\n        cli.sasl = None\n        cli.sasl_tmp = ''\n        return\n    if getattr(cli, 'sasl', None):\n        raw_data = ev_msg['params'][0]\n        if (len(raw_data) > 400):\n            cli.dump_numeric('905', ['SASL message too long'])\n            cli.sasl = None\n            cli.sasl_tmp = ''\n            return\n        elif (len(raw_data) == 400):\n            if (not hasattr(cli, 'sasl_tmp')):\n                cli.sasl_tmp = ''\n            cli.sasl_tmp += raw_data\n            if (len(cli.sasl_tmp) > (400 * 4)):\n                cli.dump_numeric('904', ['SASL authentication failed: Password too long'])\n                cli.sasl = None\n                cli.sasl_tmp = ''\n            return\n        elif getattr(cli, 'sasl_tmp', None):\n            if (raw_data != '+'):\n                cli.sasl_tmp += raw_data\n        try:\n            if hasattr(cli, 'sasl_tmp'):\n                data = base64.b64decode(cli.sasl_tmp)\n            else:\n                data = base64.b64decode(raw_data)\n        except binascii.Error:\n            cli.dump_numeric('904', ['SASL authentication failed'])\n            return\n        cli.sasl_tmp = ''\n        eventmgr_core.dispatch('sasl authenticate {}'.format(cli.sasl.casefold()), {\n            'source': cli,\n            'mechanism': cli.sasl,\n            'data': data,\n        })\n    else:\n        mechanism = ev_msg['params'][0].upper()\n        if (mechanism in valid_mechanisms):\n            cli.sasl = mechanism\n            cli.dump_verb('AUTHENTICATE', '+')\n        else:\n            cli.dump_numeric('904', ['SASL authentication failed'])\n            return\n", "label": 1}
{"function": "\n\ndef execute(self, grades, moduleDict, solutionDict):\n    self.addMessage(('Grading agent using command:  python pacman.py %s' % (self.pacmanParams,)))\n    startTime = time.time()\n    games = pacman.runGames(**pacman.readCommand(self.pacmanParams.split(' ')))\n    totalTime = (time.time() - startTime)\n    numGames = len(games)\n    stats = {\n        'time': totalTime,\n        'wins': [g.state.isWin() for g in games].count(True),\n        'games': games,\n        'scores': [g.state.getScore() for g in games],\n        'timeouts': [g.agentTimeout for g in games].count(True),\n        'crashes': [g.agentCrashed for g in games].count(True),\n    }\n    averageScore = (sum(stats['scores']) / float(len(stats['scores'])))\n    nonTimeouts = (numGames - stats['timeouts'])\n    wins = stats['wins']\n\n    def gradeThreshold(value, minimum, thresholds, name):\n        points = 0\n        passed = ((minimum is None) or (value >= minimum))\n        if passed:\n            for t in thresholds:\n                if (value >= t):\n                    points += 1\n        return (passed, points, value, minimum, thresholds, name)\n    results = [gradeThreshold(averageScore, self.scoreMinimum, self.scoreThresholds, 'average score'), gradeThreshold(nonTimeouts, self.nonTimeoutMinimum, self.nonTimeoutThresholds, 'games not timed out'), gradeThreshold(wins, self.winsMinimum, self.winsThresholds, 'wins')]\n    totalPoints = 0\n    for (passed, points, value, minimum, thresholds, name) in results:\n        if ((minimum is None) and (len(thresholds) == 0)):\n            continue\n        totalPoints += points\n        if (not passed):\n            assert (points == 0)\n            self.addMessage(('%s %s (fail: below minimum value %s)' % (value, name, minimum)))\n        else:\n            self.addMessage(('%s %s (%s of %s points)' % (value, name, points, len(thresholds))))\n        if (minimum is not None):\n            self.addMessage('    Grading scheme:')\n            self.addMessage(('     < %s:  fail' % (minimum,)))\n            if ((len(thresholds) == 0) or (minimum != thresholds[0])):\n                self.addMessage(('    >= %s:  0 points' % (minimum,)))\n            for (idx, threshold) in enumerate(thresholds):\n                self.addMessage(('    >= %s:  %s points' % (threshold, (idx + 1))))\n        elif (len(thresholds) > 0):\n            self.addMessage('    Grading scheme:')\n            self.addMessage(('     < %s:  0 points' % (thresholds[0],)))\n            for (idx, threshold) in enumerate(thresholds):\n                self.addMessage(('    >= %s:  %s points' % (threshold, (idx + 1))))\n    if any([(not passed) for (passed, _, _, _, _, _) in results]):\n        totalPoints = 0\n    return self.testPartial(grades, totalPoints, self.maxPoints)\n", "label": 1}
{"function": "\n\ndef deserialize(self, raw_value):\n    try:\n        if isinstance(self._value_option, BaseOption):\n            deserialized_dict = {\n                \n            }\n            for (k, v) in raw_value.items():\n                deserialized_dict.update({\n                    k: self._value_option.deserialize(v),\n                })\n            value = deserialized_dict\n        else:\n            value = raw_value\n    except DeserializationError:\n        raise DeserializationError('Unable to deserialize \"{}\" into dict for \"{}\"!'.format(raw_value, self.name), raw_value, self.name)\n    else:\n        return value\n", "label": 0}
{"function": "\n\n@setup_cache\ndef test_stats(cache):\n    for value in range(100):\n        cache[value] = value\n    assert (cache.stats(enable=True) == (0, 0))\n    for value in range(100):\n        cache[value]\n    for value in range(100, 110):\n        cache.get(value)\n    assert (cache.stats(reset=True) == (100, 10))\n    assert (cache.stats(enable=False) == (0, 0))\n    for value in range(100):\n        cache[value]\n    for value in range(100, 110):\n        cache.get(value)\n    assert (cache.stats() == (0, 0))\n    assert (len(cache.check()) == 0)\n", "label": 1}
{"function": "\n\ndef test_path_in_several_ways(self):\n    alice = Node(name='Alice')\n    bob = Node(name='Bob')\n    carol = Node(name='Carol')\n    dave = Node(name='Dave')\n    path = Path(alice, 'LOVES', bob, Relationship(carol, 'HATES', bob), carol, 'KNOWS', dave)\n    assert path.__bool__()\n    assert path.__nonzero__()\n    assert (path[0] == Relationship(alice, 'LOVES', bob))\n    assert (path[1] == Relationship(carol, 'HATES', bob))\n    assert (path[2] == Relationship(carol, 'KNOWS', dave))\n    assert (path[(- 1)] == Relationship(carol, 'KNOWS', dave))\n    assert (path[0:1] == Path(alice, 'LOVES', bob))\n    assert (path[0:2] == Path(alice, 'LOVES', bob, Relationship(carol, 'HATES', bob), carol))\n    try:\n        _ = path[7]\n    except IndexError:\n        assert True\n    else:\n        assert False\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    if (':' in request.get_host()):\n        (domain, port) = request.get_host().split(':')\n        if (int(port) not in (80, 443)):\n            domain = request.get_host()\n    else:\n        domain = request.get_host().split(':')[0]\n    domain = domain.lower()\n    cache_key = ('Site:domain:%s' % domain)\n    site = cache.get(cache_key)\n    if site:\n        SITE_ID.value = site\n    else:\n        try:\n            site = Site.objects.get(domain=domain)\n        except Site.DoesNotExist:\n            site = None\n        if (not site):\n            if domain.startswith('www.'):\n                fallback_domain = domain[4:]\n            else:\n                fallback_domain = ('www.' + domain)\n            try:\n                site = Site.objects.get(domain=fallback_domain)\n            except Site.DoesNotExist:\n                site = None\n        if ((not site) and getattr(settings, 'CREATE_SITES_AUTOMATICALLY', True)):\n            site = Site(domain=domain, name=domain)\n            site.save()\n        if site:\n            SITE_ID.value = site.pk\n        else:\n            SITE_ID.value = _default_site_id\n        cache.set(cache_key, SITE_ID.value, (5 * 60))\n", "label": 1}
{"function": "\n\ndef main():\n    '\\n    Run the evennia launcher main program.\\n\\n    '\n    parser = ArgumentParser(description=CMDLINE_HELP)\n    parser.add_argument('-v', '--version', action='store_true', dest='show_version', default=False, help='Show version info.')\n    parser.add_argument('-i', '--interactive', action='store_true', dest='interactive', default=False, help='Start given processes in interactive mode.')\n    parser.add_argument('-l', '--log', action='store_true', dest='logserver', default=False, help='Log Server data to log file.')\n    parser.add_argument('--init', action='store', dest='init', metavar='name', help=\"Creates a new game directory 'name' at the current location.\")\n    parser.add_argument('--list', nargs='+', action='store', dest='listsetting', metavar='key', help=\"List values for server settings. Use 'all' to list all available keys.\")\n    parser.add_argument('--profiler', action='store_true', dest='profiler', default=False, help='Start given server component under the Python profiler.')\n    parser.add_argument('--dummyrunner', nargs=1, action='store', dest='dummyrunner', metavar='N', help='Tests a running server by connecting N dummy players to it.')\n    parser.add_argument('--settings', nargs=1, action='store', dest='altsettings', default=None, metavar='filename.py', help='Start evennia with alternative settings file in gamedir/server/conf/.')\n    parser.add_argument('option', nargs='?', default='noop', help=\"Operational mode: 'start', 'stop', 'restart' or 'menu'.\")\n    parser.add_argument('service', metavar='component', nargs='?', default='all', help=\"Server component to operate on: 'server', 'portal' or 'all' (default).\")\n    parser.epilog = \"Example django-admin commands: 'migrate', 'flush', 'shell' and 'dbshell'. See the django documentation for more django-admin commands.\"\n    (args, unknown_args) = parser.parse_known_args()\n    (option, service) = (args.option, args.service)\n    check_main_evennia_dependencies()\n    if (not args):\n        print(CMDLINE_HELP)\n        sys.exit()\n    elif args.init:\n        create_game_directory(args.init)\n        print(CREATED_NEW_GAMEDIR.format(gamedir=args.init, settings_path=os.path.join(args.init, SETTINGS_PATH)))\n        sys.exit()\n    if args.show_version:\n        print(show_version_info((option == 'help')))\n        sys.exit()\n    if args.altsettings:\n        sfile = args.altsettings[0]\n        global SETTINGSFILE, SETTINGS_DOTPATH\n        SETTINGSFILE = sfile\n        SETTINGS_DOTPATH = ('server.conf.%s' % sfile.rstrip('.py'))\n        print((\"Using settings file '%s' (%s).\" % (SETTINGSFILE, SETTINGS_DOTPATH)))\n    if args.dummyrunner:\n        init_game_directory(CURRENT_DIR, check_db=True)\n        run_dummyrunner(args.dummyrunner[0])\n    elif args.listsetting:\n        init_game_directory(CURRENT_DIR, check_db=False)\n        list_settings(args.listsetting)\n    elif (option == 'menu'):\n        init_game_directory(CURRENT_DIR, check_db=True)\n        run_menu()\n    elif (option in ('start', 'reload', 'stop')):\n        init_game_directory(CURRENT_DIR, check_db=True)\n        server_operation(option, service, args.interactive, args.profiler, args.logserver)\n    elif (option != 'noop'):\n        check_db = False\n        if (option in ('runserver', 'testserver')):\n            print(WARNING_RUNSERVER)\n        if (option == 'shell'):\n            check_db = True\n        init_game_directory(CURRENT_DIR, check_db=check_db)\n        args = [option]\n        kwargs = {\n            \n        }\n        if (service not in ('all', 'server', 'portal')):\n            args.append(service)\n        if unknown_args:\n            for arg in unknown_args:\n                if arg.startswith('--'):\n                    print('arg:', arg)\n                    if ('=' in arg):\n                        (arg, value) = [p.strip() for p in arg.split('=', 1)]\n                    else:\n                        value = True\n                    kwargs[arg.lstrip('--')] = [value]\n                else:\n                    args.append(arg)\n        try:\n            django.core.management.call_command(*args, **kwargs)\n        except django.core.management.base.CommandError as exc:\n            args = ', '.join(args)\n            kwargs = ', '.join([('--%s' % kw) for kw in kwargs])\n            print(ERROR_INPUT.format(traceback=exc, args=args, kwargs=kwargs))\n    else:\n        print(ABOUT_INFO)\n", "label": 1}
{"function": "\n\ndef _load(self):\n    ' Read the metadata from the binary package and store them in the\\n        instance.\\n\\n        :return: the metadata dictionary\\n\\n        '\n    with qisys.sh.TempDir() as work_dir:\n        pkg = portage.xpak.tbz2(self.path)\n        pkg.decompose(work_dir, cleanup=0)\n        (arch, arch_variant) = _get_pkg_arch(work_dir)\n        with open(os.path.join(work_dir, 'PF'), 'r') as fpf:\n            pf = fpf.readline().strip()\n        (name, version, revision) = portage.versions.pkgsplit(pf)\n        dependency = dict()\n        for (dep, dep_filename) in _DEPENDENCY.iteritems():\n            dep_path = os.path.join(work_dir, dep_filename)\n            if (not os.path.exists(dep_path)):\n                dependency[dep] = list()\n                continue\n            with open(dep_path, 'r') as fdep:\n                dependency[dep] = fdep.read().strip().split()\n    dependency['all'] = list()\n    for dep_list in _DEPENDENCY.keys():\n        dependency['all'].extend(dependency[dep_list])\n    for (dep, dep_list) in dependency.iteritems():\n        dependency[dep] = list(set(dep_list))\n    metadata = {\n        'name': name,\n        'version': version,\n        'revision': revision,\n        'arch': arch,\n        'arch_variant': arch_variant,\n        'dependencies': dependency,\n    }\n    self.metadata = metadata\n", "label": 0}
{"function": "\n\ndef to_python(self, data):\n    '\\n        Checks that the file-upload field data contains a valid image (GIF, JPG,\\n        PNG, possibly others -- whatever the Python Imaging Library supports).\\n        '\n    f = super(ImageField, self).to_python(data)\n    if (f is None):\n        return None\n    from PIL import Image\n    if hasattr(data, 'temporary_file_path'):\n        file = data.temporary_file_path()\n    elif hasattr(data, 'read'):\n        file = BytesIO(data.read())\n    else:\n        file = BytesIO(data['content'])\n    try:\n        image = Image.open(file)\n        image.verify()\n        f.image = image\n        f.content_type = Image.MIME.get(image.format)\n    except Exception:\n        six.reraise(ValidationError, ValidationError(self.error_messages['invalid_image'], code='invalid_image'), sys.exc_info()[2])\n    if (hasattr(f, 'seek') and callable(f.seek)):\n        f.seek(0)\n    return f\n", "label": 0}
{"function": "\n\ndef test_repeat_get_url_interval(url_prefix):\n    session = RateLimitRequests(url_interval=1)\n    t = datetime.now()\n    resp1 = session.get((url_prefix + '/cookies'))\n    session.get((url_prefix + '/cookies/set?a=b'))\n    resp2 = session.get((url_prefix + '/cookies'))\n    assert ((datetime.now() - t) < timedelta(seconds=1))\n    assert (resp1 == resp2)\n    assert (resp1.json()['cookies'] == resp2.json()['cookies'])\n    time.sleep(1)\n    resp3 = session.get((url_prefix + '/cookies'))\n    assert (resp1 != resp3)\n    assert (resp1.json()['cookies'] != resp3.json()['cookies'])\n", "label": 0}
{"function": "\n\ndef _create_test_db(self, verbosity, autoclobber, keepdb=False):\n    '\\n        Internal implementation - creates the test db tables.\\n        '\n    suffix = self.sql_table_creation_suffix()\n    test_database_name = self._get_test_db_name()\n    qn = self.connection.ops.quote_name\n    with self._nodb_connection.cursor() as cursor:\n        try:\n            cursor.execute(('CREATE DATABASE %s %s' % (qn(test_database_name), suffix)))\n        except Exception as e:\n            if keepdb:\n                return test_database_name\n            sys.stderr.write(('Got an error creating the test database: %s\\n' % e))\n            if (not autoclobber):\n                confirm = input((\"Type 'yes' if you would like to try deleting the test database '%s', or 'no' to cancel: \" % test_database_name))\n            if (autoclobber or (confirm == 'yes')):\n                try:\n                    if (verbosity >= 1):\n                        print(('Destroying old test database for alias %s...' % (self._get_database_display_str(verbosity, test_database_name),)))\n                    cursor.execute(('DROP DATABASE %s' % qn(test_database_name)))\n                    cursor.execute(('CREATE DATABASE %s %s' % (qn(test_database_name), suffix)))\n                except Exception as e:\n                    sys.stderr.write(('Got an error recreating the test database: %s\\n' % e))\n                    sys.exit(2)\n            else:\n                print('Tests cancelled.')\n                sys.exit(1)\n    return test_database_name\n", "label": 0}
{"function": "\n\ndef get_variable_name(self, abbreviated_xpath):\n    '\\n        If the abbreviated_xpath has been renamed in\\n        self.variable_names_json return that new name, otherwise\\n        return the original abbreviated_xpath.\\n        '\n    if (not hasattr(self, '_keys')):\n        self._keys = self.get_keys()\n    if (not hasattr(self, '_headers')):\n        self._headers = self.get_headers()\n    assert (abbreviated_xpath in self._keys), abbreviated_xpath\n    i = self._keys.index(abbreviated_xpath)\n    header = self._headers[i]\n    if (not hasattr(self, '_variable_names')):\n        self._variable_names = ColumnRename.get_dict()\n        assert (type(self._variable_names) == dict)\n    if ((header in self._variable_names) and self._variable_names[header]):\n        return self._variable_names[header]\n    return header\n", "label": 0}
{"function": "\n\ndef get_or_head_response(self, req, x_object_manifest, response_headers=None):\n    if (response_headers is None):\n        response_headers = self._response_headers\n    (container, obj_prefix) = x_object_manifest.split('/', 1)\n    container = unquote(container)\n    obj_prefix = unquote(obj_prefix)\n    req.acl = None\n    (version, account, _junk) = req.split_path(2, 3, True)\n    (error_response, segments) = self._get_container_listing(req, version, account, container, obj_prefix)\n    if error_response:\n        return error_response\n    have_complete_listing = (len(segments) < constraints.CONTAINER_LISTING_LIMIT)\n    first_byte = last_byte = None\n    actual_content_length = None\n    content_length_for_swob_range = None\n    if (req.range and (len(req.range.ranges) == 1)):\n        content_length_for_swob_range = sum((o['bytes'] for o in segments))\n        (_junk, range_end) = req.range.ranges_for_length(float('inf'))[0]\n        if (have_complete_listing or (range_end < content_length_for_swob_range)):\n            byteranges = req.range.ranges_for_length(content_length_for_swob_range)\n            if (not byteranges):\n                return HTTPRequestedRangeNotSatisfiable(request=req)\n            (first_byte, last_byte) = byteranges[0]\n            last_byte -= 1\n            actual_content_length = ((last_byte - first_byte) + 1)\n        else:\n            actual_content_length = None\n            content_length_for_swob_range = None\n            req.range = None\n    response_headers = [(h, v) for (h, v) in response_headers if (h.lower() not in ('content-length', 'content-range'))]\n    if (content_length_for_swob_range is not None):\n        response_headers.append(('Content-Length', str(content_length_for_swob_range)))\n    elif have_complete_listing:\n        actual_content_length = sum((o['bytes'] for o in segments))\n        response_headers.append(('Content-Length', str(actual_content_length)))\n    if have_complete_listing:\n        response_headers = [(h, v) for (h, v) in response_headers if (h.lower() != 'etag')]\n        etag = md5()\n        for seg_dict in segments:\n            etag.update(seg_dict['hash'].strip('\"'))\n        response_headers.append(('Etag', ('\"%s\"' % etag.hexdigest())))\n    app_iter = None\n    if (req.method == 'GET'):\n        listing_iter = RateLimitedIterator(self._segment_listing_iterator(req, version, account, container, obj_prefix, segments, first_byte=first_byte, last_byte=last_byte), self.dlo.rate_limit_segments_per_sec, limit_after=self.dlo.rate_limit_after_segment)\n        app_iter = SegmentedIterable(req, self.dlo.app, listing_iter, ua_suffix='DLO MultipartGET', swift_source='DLO', name=req.path, logger=self.logger, max_get_time=self.dlo.max_get_time, response_body_length=actual_content_length)\n        try:\n            app_iter.validate_first_segment()\n        except (SegmentError, ListingIterError):\n            return HTTPConflict(request=req)\n    resp = Response(request=req, headers=response_headers, conditional_response=True, app_iter=app_iter)\n    return resp\n", "label": 1}
{"function": "\n\ndef export_to_csv(self, model_name):\n    self.header('Exporting models ...')\n    today = datetime.datetime.today()\n    model = get_model('calaccess_campaign_browser', model_name)\n    fieldnames = ([f.name for f in model._meta.fields] + ['committee_name', 'filer_name', 'filer_id', 'filer_id_raw'])\n    relation_names = ([f.name for f in model._meta.fields] + ['committee__name', 'committee__filer__name', 'committee__filer__id', 'committee__filer__filer_id_raw'])\n    filename = '{}-{}-{}-{}.csv'.format(today.year, today.month, today.day, model_name.lower())\n    filepath = os.path.join(self.data_dir, filename)\n    self.header('  Exporting {} model ...'.format(model_name.capitalize()))\n    with open(filepath, 'wb') as csvfile:\n        writer = csv.writer(csvfile, delimiter='\\t')\n        writer.writerow(fieldnames)\n        if (model_name != 'summary'):\n            for cycle in Cycle.objects.all():\n                self.log('    Looking at cycle {} ...'.format(cycle.name))\n                rows = model.objects.filter(cycle=cycle).exclude(is_duplicate=True).values_list(*relation_names)\n                if (not rows):\n                    self.failure('      No data for {}'.format(cycle.name))\n                else:\n                    rows = self.encoded(rows)\n                    writer.writerows(rows)\n                    self.success('      Added {} {} data'.format(cycle.name, model_name))\n        else:\n            rows = self.encoded(model.objects.values_list())\n            writer.writerows(rows)\n    self.success('  Exported {}!'.format(model_name.capitalize()))\n", "label": 0}
{"function": "\n\ndef data(self):\n    if (not hasattr(self, '_data')):\n        request = URLRequest(self.url)\n        if (self.env and self.env.cache):\n            headers = self.env.cache.get(('url', 'headers', self.url))\n            if headers:\n                (etag, lmod) = headers\n                if etag:\n                    request.add_header('If-None-Match', etag)\n                if lmod:\n                    request.add_header('If-Modified-Since', lmod)\n        try:\n            response = urlopen(request)\n        except HTTPError as e:\n            if (e.code != 304):\n                raise\n            self._data = self.env.cache.get(('url', 'contents', self.url))\n        else:\n            with contextlib.closing(response):\n                self._data = response.read()\n            if (self.env and self.env.cache):\n                self.env.cache.set(('url', 'headers', self.url), (response.headers.get('ETag'), response.headers.get('Last-Modified')))\n                self.env.cache.set(('url', 'contents', self.url), self._data)\n    return self._data\n", "label": 1}
{"function": "\n\ndef find_diverge_commits(self, first_branch, second_branch):\n    \"\\n        Take two branches and find diverge commits.\\n\\n             2--3--4--5\\n            /\\n        1--+              Return:\\n            \\\\               - common parent: 1\\n             6              - first list of commits: (2, 3, 4, 5)\\n                            - second list of commits: (6)\\n\\n        :param first_branch: first branch to look for common parent\\n        :type first_branch: `pygit2.Branch`\\n        :param second_branch: second branch to look for common parent\\n        :type second_branch: `pygit2.Branch`\\n        :returns: a namedtuple with common parent, a list of first's branch\\n        commits and another list with second's branch commits\\n        :rtype: DivergeCommits (namedtuple)\\n        \"\n    common_parent = None\n    first_commits = CommitsList()\n    second_commits = CommitsList()\n    walker = self.walk_branches(GIT_SORT_TOPOLOGICAL, first_branch, second_branch)\n    for (first_commit, second_commit) in walker:\n        if ((first_commit in second_commits) or (second_commit in first_commits)):\n            break\n        if (first_commit not in first_commits):\n            first_commits.append(first_commit)\n        if (second_commit not in second_commits):\n            second_commits.append(second_commit)\n        if (second_commit.hex == first_commit.hex):\n            break\n    try:\n        index = second_commits.index(first_commit)\n    except ValueError:\n        pass\n    else:\n        second_commits = second_commits[:index]\n        common_parent = first_commit\n    try:\n        index = first_commits.index(second_commit)\n    except ValueError:\n        pass\n    else:\n        first_commits = first_commits[:index]\n        common_parent = second_commit\n    return DivergeCommits(common_parent, first_commits, second_commits)\n", "label": 1}
{"function": "\n\ndef call_for_each_element(data, function, args=[], data_type='sequential'):\n    if (data_type == 'plain'):\n        return function(data, *args)\n    elif (data_type == 'sequential'):\n        assert list_of_lists(data)\n        return [function(d, *args) for d in data]\n    elif (data_type == 'token'):\n        assert (type(data) == dict)\n    return {token: function(contexts, *args) for (token, contexts) in data.items()}\n", "label": 0}
{"function": "\n\ndef headers_received(self, start_line, headers):\n    if (start_line.code != 101):\n        return super(WebSocketClientConnection, self).headers_received(start_line, headers)\n    self.headers = headers\n    assert (self.headers['Upgrade'].lower() == 'websocket')\n    assert (self.headers['Connection'].lower() == 'upgrade')\n    accept = WebSocketProtocol13.compute_accept_value(self.key)\n    assert (self.headers['Sec-Websocket-Accept'] == accept)\n    self.protocol = WebSocketProtocol13(self, mask_outgoing=True)\n    self.protocol._receive_frame()\n    if (self._timeout is not None):\n        self.io_loop.remove_timeout(self._timeout)\n        self._timeout = None\n    self.stream = self.connection.detach()\n    self.stream.set_close_callback(self.on_connection_close)\n    self.final_callback = None\n    self.connect_future.set_result(self)\n", "label": 0}
{"function": "\n\n@classmethod\n@postonly\n@multiplayer_service\n@jsonify\ndef client_leave(cls):\n    remote_addr = get_remote_addr(request)\n    params = request.params\n    try:\n        session_id = params['session']\n        player_id = params['client']\n        hmac = params['hmac']\n    except KeyError:\n        response.status_int = 400\n        return {\n            'ok': False,\n            'msg': 'Missing session information.',\n        }\n    calculated_hmac = _calculate_client_hmac(cls.secret, remote_addr, session_id, player_id)\n    if (hmac != calculated_hmac):\n        response.status_int = 400\n        return {\n            'ok': False,\n            'msg': 'Invalid server information.',\n        }\n    sessions = cls.sessions\n    try:\n        session = sessions[session_id]\n    except KeyError:\n        response.status_int = 404\n        return {\n            'ok': False,\n            'msg': 'Unknown session.',\n        }\n    with cls.lock:\n        if session.has_player(player_id):\n            request_ip = get_remote_addr(request)\n            stored_ip = session.get_player_ip(player_id)\n            if ((stored_ip is not None) and (request_ip != stored_ip)):\n                response.status_int = 401\n                return {\n                    'ok': False,\n                }\n            LOG.info('Player %s left session %s', player_id, session_id)\n            session.remove_player(player_id)\n            cls._clean_empty_sessions()\n    return {\n        'ok': True,\n    }\n", "label": 0}
{"function": "\n\ndef test_hi_level_transaction(self):\n    factory = pyorient.OrientDB('localhost', 2424)\n    factory.get_message(pyorient.CONNECT).prepare(('root', 'root')).send().fetch_response()\n    db_name = 'test_transactions'\n    exists = factory.get_message(pyorient.DB_EXIST).prepare([db_name, pyorient.STORAGE_TYPE_MEMORY]).send().fetch_response()\n    print(('Before %r' % exists))\n    try:\n        factory.get_message(pyorient.DB_DROP).prepare([db_name]).send().fetch_response()\n        assert True\n    except pyorient.PyOrientCommandException as e:\n        print(str(e))\n    finally:\n        factory.get_message(pyorient.DB_CREATE).prepare((db_name, pyorient.DB_TYPE_GRAPH, pyorient.STORAGE_TYPE_MEMORY)).send().fetch_response()\n    msg = factory.get_message(pyorient.DB_OPEN)\n    cluster_info = msg.prepare((db_name, 'admin', 'admin', pyorient.DB_TYPE_GRAPH, '')).send().fetch_response()\n    rec = {\n        'alloggio': 'baita',\n        'lavoro': 'no',\n        'vacanza': 'lago',\n    }\n    rec_position = factory.get_message(pyorient.RECORD_CREATE).prepare((3, rec)).send().fetch_response()\n    rec3 = {\n        'alloggio': 'albergo',\n        'lavoro': 'ufficio',\n        'vacanza': 'montagna',\n    }\n    update_success = factory.get_message(pyorient.RECORD_UPDATE).prepare((3, rec_position._rid, rec3, rec_position._version))\n    rec1 = {\n        'alloggio': 'casa',\n        'lavoro': 'ufficio',\n        'vacanza': 'mare',\n    }\n    rec_position1 = factory.get_message(pyorient.RECORD_CREATE).prepare(((- 1), rec1))\n    rec2 = {\n        'alloggio': 'baita',\n        'lavoro': 'no',\n        'vacanza': 'lago',\n    }\n    rec_position2 = factory.get_message(pyorient.RECORD_CREATE).prepare(((- 1), rec2))\n    rec = {\n        'alloggio': 'baita',\n        'lavoro': 'no',\n        'vacanza': 'lago',\n    }\n    rec_position = factory.get_message(pyorient.RECORD_CREATE).prepare((3, rec)).send().fetch_response()\n    delete_msg = factory.get_message(pyorient.RECORD_DELETE)\n    delete_msg.prepare((3, rec_position._rid))\n    tx = factory.get_message(pyorient.TX_COMMIT)\n    tx.begin()\n    tx.attach(rec_position1)\n    tx.attach(rec_position1)\n    tx.attach(rec_position2)\n    tx.attach(update_success)\n    tx.attach(delete_msg)\n    res = tx.commit()\n    for (k, v) in res.items():\n        print(((k + ' -> ') + v.vacanza))\n    assert (len(res) == 4)\n    assert (res['#3:0'].vacanza == 'montagna')\n    assert (res['#3:2'].vacanza == 'mare')\n    assert (res['#3:3'].vacanza == 'mare')\n    assert (res['#3:4'].vacanza == 'lago')\n    sid = factory.get_message(pyorient.CONNECT).prepare(('root', 'root')).send().fetch_response()\n    factory.get_message(pyorient.DB_DROP).prepare([db_name, pyorient.STORAGE_TYPE_MEMORY]).send().fetch_response()\n", "label": 0}
{"function": "\n\ndef test_listen_targets_per_subclass(self):\n    'test that listen() called on a subclass remains specific to\\n        that subclass.'\n    canary = []\n\n    def listen_one(*args):\n        canary.append('listen_one')\n\n    def listen_two(*args):\n        canary.append('listen_two')\n\n    def listen_three(*args):\n        canary.append('listen_three')\n    event.listen(pool.Pool, 'connect', listen_one)\n    event.listen(pool.QueuePool, 'connect', listen_two)\n    event.listen(pool.SingletonThreadPool, 'connect', listen_three)\n    p1 = pool.QueuePool(creator=MockDBAPI().connect)\n    p2 = pool.SingletonThreadPool(creator=MockDBAPI().connect)\n    assert (listen_one in p1.dispatch.connect)\n    assert (listen_two in p1.dispatch.connect)\n    assert (listen_three not in p1.dispatch.connect)\n    assert (listen_one in p2.dispatch.connect)\n    assert (listen_two not in p2.dispatch.connect)\n    assert (listen_three in p2.dispatch.connect)\n    p1.connect()\n    eq_(canary, ['listen_one', 'listen_two'])\n    p2.connect()\n    eq_(canary, ['listen_one', 'listen_two', 'listen_one', 'listen_three'])\n", "label": 0}
{"function": "\n\n@expose('/mkdir/', methods=('GET', 'POST'))\n@expose('/mkdir/<path:path>', methods=('GET', 'POST'))\ndef mkdir(self, path=None):\n    '\\n            Directory creation view method\\n\\n            :param path:\\n                Optional directory path. If not provided, will use the base directory\\n        '\n    (base_path, directory, path) = self._normalize_path(path)\n    dir_url = self._get_dir_url('.index', path)\n    if (not self.can_mkdir):\n        flash(gettext('Directory creation is disabled.'), 'error')\n        return redirect(dir_url)\n    if (not self.is_accessible_path(path)):\n        flash(gettext('Permission denied.'), 'error')\n        return redirect(self._get_dir_url('.index'))\n    form = self.name_form()\n    if self.validate_form(form):\n        try:\n            os.mkdir(op.join(directory, form.name.data))\n            self.on_mkdir(directory, form.name.data)\n            return redirect(dir_url)\n        except Exception as ex:\n            flash(gettext('Failed to create directory: %(error)s', error=ex), 'error')\n    else:\n        helpers.flash_errors(form, message='Failed to create directory: %(error)s')\n    return self.render(self.mkdir_template, form=form, dir_url=dir_url)\n", "label": 0}
{"function": "\n\ndef _validate_int_format(self, name, val):\n    if (val == ''):\n        return\n    try:\n        assert (type(val) in (str, unicode))\n        assert val.isdigit()\n    except AssertionError:\n        raise Exception(('Invalid value for %s!  Must be an integer format string.' % name))\n", "label": 0}
{"function": "\n\ndef getexecutable(name, cache={\n    \n}):\n    try:\n        return cache[name]\n    except KeyError:\n        executable = py.path.local.sysfind(name)\n        if executable:\n            if (name == 'jython'):\n                import subprocess\n                popen = subprocess.Popen([str(executable), '--version'], universal_newlines=True, stderr=subprocess.PIPE)\n                (out, err) = popen.communicate()\n                if ((not err) or ('2.5' not in err)):\n                    executable = None\n                if ('2.5.2' in err):\n                    executable = None\n        cache[name] = executable\n        return executable\n", "label": 0}
{"function": "\n\ndef main(argv=None):\n    'script main.\\n    parses command line options in sys.argv, unless *argv* is given.\\n    '\n    if (not argv):\n        argv = sys.argv\n    parser = E.OptionParser(version='%prog version: $Id$', usage=globals()['__doc__'])\n    parser.add_option('-t', '--design-file-tsv', dest='design_file', type='string', help='filename with design [default=%default].')\n    parser.add_option('-i', '--change-bin-min', dest='min_cbin', type='float', help='minimum bin for change bins [default=%default].')\n    parser.add_option('-x', '--change-bin-max', dest='max_cbin', type='float', help='maximum bin for change bins [default=%default].')\n    parser.add_option('-w', '--change-bin-width', dest='width_cbin', type='float', help='bin width for change bins [default=%default].')\n    parser.add_option('-s', '--initial-bin-min', dest='min_ibin', type='float', help='minimum bin for initial bins[default=%default].')\n    parser.add_option('-e', '--initial-bin-max', dest='max_ibin', type='float', help='maximum bin for intitial bins[default=%default].')\n    parser.add_option('-y', '--initial-bin-width', dest='width_ibin', type='float', help='bin width intitial bins[default=%default].')\n    parser.add_option('-l', '--spike-minimum', dest='min_spike', type='int', help='minimum number of spike-ins required within each bin                      [default=%default].')\n    parser.add_option('-u', '--spike-maximum', dest='max_spike', type='int', help='maximum number of spike-ins allowed within each bin                      [default=%default].')\n    parser.add_option('-d', '--difference-method', dest='difference', type='choice', choices=('relative', 'logfold'), help='method to use for calculating difference                      [default=%default].')\n    parser.add_option('-r', '--iterations', dest='iterations', type='int', help='number of iterations [default=%default].')\n    parser.add_option('-a', '--id_columns', dest='id', action='append', help='name of identification column(s)                      [default=%default].')\n    parser.add_option('-c', '--cluster-maximum-distance', dest='cluster_max_distance', type='int', help='maximum distance between adjacent loci in cluster                      [default=%default].')\n    parser.add_option('-b', '--cluster-minimum-size', dest='cluster_min_size', type='int', help='minimum number of loci required per cluster                      [default=%default].')\n    parser.add_option('-f', '--spike-type', dest='spike_type', type='choice', choices=('row', 'cluster'), help='spike in type [default=%default].')\n    parser.add_option('-g', '--subcluster-min-size', dest='min_sbin', type='int', help='minimum size of subcluster                      [default=%default].')\n    parser.add_option('-j', '--subcluster-max-size', dest='max_sbin', type='int', help='maximum size of subcluster                      [default=%default].')\n    parser.add_option('-k', '--subcluster-bin-width', dest='width_sbin', type='int', help='bin width for subcluster size                      [default=%default].')\n    parser.add_option('-o', '--output-method', dest='output_method', type='choice', choices=('append', 'seperate'), help='defines whether the spike-ins should be appended                      to the original table or seperately [default=%default].')\n    parser.add_option('-p', '--shuffle-column-suffix', dest='shuffle_suffix', type='string', help='the suffix of the columns which are to be shuffled                      [default=%default].')\n    parser.add_option('-q', '--keep-column-suffix', dest='keep_suffix', type='string', help='a list of suffixes for the columns which are to be                      keep along with the shuffled columns[default=%default].')\n    parser.add_option('-z', '--spike-regex', dest='spike_regex', type='string', action='append', help='a regex to identify tracks for shuffling                      [default=%default].')\n    parser.set_defaults(design_file=None, output_method='seperate', difference='relative', spike_type='row', genome_file=None, min_cbin=None, max_cbin=None, width_cbin=None, min_ibin=None, max_ibin=None, width_ibin=None, id_columns=None, max_spike=100, min_spike=None, iterations=1, cluster_max_distance=100, cluster_min_size=10, min_sbin=1, max_sbin=9, width_sbin=2, id_column=None, shuffle_suffix=None, keep_suffix=None, spike_regex='.*')\n    (options, args) = E.Start(parser, argv=argv, add_output_options=True)\n    df = pd.read_table(sys.stdin, sep='\\t')\n    if (options.spike_type == 'cluster'):\n        df_sort = df.sort(columns=['contig', 'position'])\n        df_sort.set_index(df.index, inplace=True)\n    else:\n        df_sort = df\n    if (not options.design_file):\n        raise ValueError('a design table is required.')\n    if (options.spike_type == 'cluster'):\n        if (options.max_sbin > options.cluster_min_size):\n            raise ValueError(('maximum size of subscluster bin: %s            is greater than minimum size of cluster: %s' % (options.max_sbin, options.cluster_min_size)))\n        df_columns = set(df.columns.values.tolist())\n        msg = \"cluster analysis requires columns named 'contig' and        'position' in the dataframe\"\n        assert (('contig' in df_columns) and ('position' in df_columns)), msg\n        msg2 = ('the minimum cluster size (%s) must be larger than the maximum\\n        size of subcluster to be shuffled (%s)' % (options.cluster_min_size, options.max_sbin))\n        assert (options.max_sbin < options.cluster_min_size), msg2\n        df_sort = df.sort(columns=['contig', 'position'])\n        df_sort.set_index(df.index, inplace=True)\n    if (not options.id_column):\n        if ((options.output_method == 'append') and (options.spike_type == 'row')):\n            raise ValueError('id column must be specified to append rows')\n        elif (options.output_method != 'append'):\n            options.id_column = 'id'\n    if (not options.min_spike):\n        options.min_spike = options.max_spike\n    design_table = pd.read_table(options.design_file, sep='\\t')\n    (groups, g_to_keep_tracks, g_to_spike_tracks) = groupMappers(design_table, options.spike_regex, options.shuffle_suffix, options.keep_suffix)\n    change_bins = np.arange(options.min_cbin, options.max_cbin, options.width_cbin)\n    E.info(('Column boundaries are: %s' % str(change_bins)))\n    initial_bins = np.arange(options.min_ibin, options.max_ibin, options.width_ibin)\n    E.info(('Row boundaries are: %s' % str(initial_bins)))\n    if (options.spike_type == 'cluster'):\n        E.info('looking for clusters...')\n        clusters_dict = findClusters(df_sort, options.cluster_max_distance, options.cluster_min_size, g_to_spike_tracks, groups)\n        if (len(clusters_dict) == 0):\n            raise Exception('no clusters were found, check parameters')\n        E.info('repeatedly shuffling subcluster regions...')\n        (output_indices, counts) = shuffleCluster(initial_bins, change_bins, g_to_spike_tracks, groups, options.difference, options.max_spike, options.iterations, clusters_dict, options.max_sbin, options.min_sbin, options.width_sbin)\n    elif (options.spike_type == 'row'):\n        E.info('repeatedly shuffling rows...')\n        (output_indices, counts) = shuffleRows(df, initial_bins, change_bins, g_to_spike_tracks, groups, options.difference, options.max_spike, options.iterations)\n    filled_bins = thresholdBins(output_indices, counts, options.min_spike)\n    if (len(filled_bins) == 0):\n        raise Exception('No bins contained a sufficient number of spike-ins')\n    E.info('outputting spike-ins...')\n    outputSpikes(df_sort, options.id_column, filled_bins, g_to_keep_tracks, groups, method=options.output_method, spike_type=options.spike_type, min_cbin=options.min_cbin, width_cbin=options.width_cbin, min_ibin=options.min_ibin, width_ibin=options.width_ibin, min_sbin=options.min_sbin, width_sbin=options.width_sbin)\n    E.Stop()\n", "label": 1}
{"function": "\n\ndef test_multi_column_join():\n    a = Symbol('a', 'var * {x: int, y: int, z: int}')\n    b = Symbol('b', 'var * {w: int, x: int, y: int}')\n    j = join(a, b, ['x', 'y'])\n    assert (set(j.fields) == set('wxyz'))\n    assert (j.on_left == j.on_right == ['x', 'y'])\n    assert hash(j)\n    assert (j.fields == ['x', 'y', 'z', 'w'])\n", "label": 0}
{"function": "\n\ndef __init__(self, method):\n    '\\n        :param method: One of SSLv2_METHOD, SSLv3_METHOD, SSLv23_METHOD, or\\n            TLSv1_METHOD.\\n        '\n    if (not isinstance(method, integer_types)):\n        raise TypeError('method must be an integer')\n    try:\n        method_func = self._methods[method]\n    except KeyError:\n        raise ValueError('No such protocol')\n    method_obj = method_func()\n    if (method_obj == _ffi.NULL):\n        _raise_current_error()\n    context = _lib.SSL_CTX_new(method_obj)\n    if (context == _ffi.NULL):\n        _raise_current_error()\n    context = _ffi.gc(context, _lib.SSL_CTX_free)\n    self._context = context\n    self._passphrase_helper = None\n    self._passphrase_callback = None\n    self._passphrase_userdata = None\n    self._verify_helper = None\n    self._verify_callback = None\n    self._info_callback = None\n    self._tlsext_servername_callback = None\n    self._app_data = None\n    self._npn_advertise_helper = None\n    self._npn_advertise_callback = None\n    self._npn_select_helper = None\n    self._npn_select_callback = None\n    self._alpn_select_helper = None\n    self._alpn_select_callback = None\n    self.set_mode(_lib.SSL_MODE_ENABLE_PARTIAL_WRITE)\n", "label": 0}
{"function": "\n\ndef api_public_resolve(self):\n    try:\n        ticket = Ticket.objects.get(id=self.request.POST.get('ticket', False))\n    except Ticket.DoesNotExist:\n        return api_return(STATUS_ERROR, 'Invalid ticket ID')\n    resolution = self.request.POST.get('resolution', None)\n    if (not resolution):\n        return api_return(STATUS_ERROR, 'Blank resolution')\n    f = FollowUp(ticket=ticket, date=timezone.now(), comment=resolution, user=self.request.user, title='Resolved', public=True)\n    f.save()\n    context = safe_template_context(ticket)\n    context['resolution'] = f.comment\n    subject = ('%s %s (Resolved)' % (ticket.ticket, ticket.title))\n    messages_sent_to = []\n    if ticket.submitter_email:\n        send_templated_mail('resolved_submitter', context, recipients=ticket.submitter_email, sender=ticket.queue.from_address, fail_silently=True)\n        messages_sent_to.append(ticket.submitter_email)\n        for cc in ticket.ticketcc_set.all():\n            if (cc.email_address not in messages_sent_to):\n                send_templated_mail('resolved_submitter', context, recipients=cc.email_address, sender=ticket.queue.from_address, fail_silently=True)\n                messages_sent_to.append(cc.email_address)\n    if (ticket.queue.updated_ticket_cc and (ticket.queue.updated_ticket_cc not in messages_sent_to)):\n        send_templated_mail('resolved_cc', context, recipients=ticket.queue.updated_ticket_cc, sender=ticket.queue.from_address, fail_silently=True)\n        messages_sent_to.append(ticket.queue.updated_ticket_cc)\n    if (ticket.assigned_to and (self.request.user != ticket.assigned_to) and getattr(ticket.assigned_to.usersettings.settings, 'email_on_ticket_apichange', False) and ticket.assigned_to.email and (ticket.assigned_to.email not in messages_sent_to)):\n        send_templated_mail('resolved_resolved', context, recipients=ticket.assigned_to.email, sender=ticket.queue.from_address, fail_silently=True)\n    ticket.resoltuion = f.comment\n    ticket.status = Ticket.RESOLVED_STATUS\n    ticket.save()\n    return api_return(STATUS_OK)\n", "label": 1}
{"function": "\n\ndef discover(self, start_dir, pattern='test*.py', top_level_dir=None):\n    \"Find and return all test modules from the specified start\\n        directory, recursing into subdirectories to find them and return all\\n        tests found within them. Only test files that match the pattern will\\n        be loaded. (Using shell style pattern matching.)\\n\\n        All test modules must be importable from the top level of the project.\\n        If the start directory is not the top level directory then the top\\n        level directory must be specified separately.\\n\\n        If a test package name (directory with '__init__.py') matches the\\n        pattern then the package will be checked for a 'load_tests' function. If\\n        this exists then it will be called with loader, tests, pattern.\\n\\n        If load_tests exists then discovery does  *not* recurse into the package,\\n        load_tests is responsible for loading all tests in the package.\\n\\n        The pattern is deliberately not stored as a loader attribute so that\\n        packages can continue discovery themselves. top_level_dir is stored so\\n        load_tests does not need to pass this argument in to loader.discover().\\n        \"\n    set_implicit_top = False\n    if ((top_level_dir is None) and (self._top_level_dir is not None)):\n        top_level_dir = self._top_level_dir\n    elif (top_level_dir is None):\n        set_implicit_top = True\n        top_level_dir = start_dir\n    top_level_dir = os.path.abspath(top_level_dir)\n    if (not (top_level_dir in sys.path)):\n        sys.path.insert(0, top_level_dir)\n    self._top_level_dir = top_level_dir\n    is_not_importable = False\n    if os.path.isdir(os.path.abspath(start_dir)):\n        start_dir = os.path.abspath(start_dir)\n        if (start_dir != top_level_dir):\n            is_not_importable = (not os.path.isfile(os.path.join(start_dir, '__init__.py')))\n    else:\n        try:\n            __import__(start_dir)\n        except ImportError:\n            is_not_importable = True\n        else:\n            the_module = sys.modules[start_dir]\n            top_part = start_dir.split('.')[0]\n            start_dir = os.path.abspath(os.path.dirname(the_module.__file__))\n            if set_implicit_top:\n                self._top_level_dir = self._get_directory_containing_module(top_part)\n                sys.path.remove(top_level_dir)\n    if is_not_importable:\n        raise ImportError(('Start directory is not importable: %r' % start_dir))\n    tests = list(self._find_tests(start_dir, pattern))\n    return self.suiteClass(tests)\n", "label": 1}
{"function": "\n\ndef attempt(self, explain):\n    \"Attempts to execute the goal's tasks in installed order.\\n\\n    :param bool explain: If ``True`` then the goal plan will be explained instead of being\\n                         executed.\\n    \"\n    goal_workdir = os.path.join(self._context.options.for_global_scope().pants_workdir, self._goal.name)\n    with self._context.new_workunit(name=self._goal.name, labels=[WorkUnitLabel.GOAL]):\n        for (name, task_type) in reversed(self._tasktypes_by_name.items()):\n            task_workdir = os.path.join(goal_workdir, name)\n            task = task_type(self._context, task_workdir)\n            log_config = WorkUnit.LogConfig(level=task.get_options().level, colors=task.get_options().colors)\n            with self._context.new_workunit(name=name, labels=[WorkUnitLabel.TASK], log_config=log_config):\n                if explain:\n                    self._context.log.debug('Skipping execution of {} in explain mode'.format(name))\n                else:\n                    task.execute()\n        if explain:\n            reversed_tasktypes_by_name = reversed(self._tasktypes_by_name.items())\n            goal_to_task = ', '.join(('{}->{}'.format(name, task_type.__name__) for (name, task_type) in reversed_tasktypes_by_name))\n            print('{goal} [{goal_to_task}]'.format(goal=self._goal.name, goal_to_task=goal_to_task))\n", "label": 0}
{"function": "\n\ndef test_postgres_search_path_parsing(self):\n    url = 'postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn?currentSchema=otherschema'\n    url = dj_database_url.parse(url)\n    assert (url['ENGINE'] == 'django.db.backends.postgresql_psycopg2')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 5431)\n    assert (url['OPTIONS']['options'] == '-c search_path=otherschema')\n", "label": 0}
{"function": "\n\ndef sslcli(argv):\n    b'sslcli [-?D]\\nProvides an interactive CLI for managing SSL certificates and CA operations.\\n\\nOptions:\\n   -?        = This help text.\\n   -D        = Debug on.\\n    '\n    import os\n    import getopt\n    try:\n        (optlist, args) = getopt.getopt(argv[1:], b'?hD')\n    except getopt.GetoptError:\n        print(sslcli.__doc__)\n        return\n    for (opt, val) in optlist:\n        if (opt in (b'-?', b'-h')):\n            print(sslcli.__doc__)\n            return\n        elif (opt == b'-D'):\n            from pycopia import autodebug\n    io = CLI.ConsoleIO()\n    ui = CLI.UserInterface(io)\n    ui._env[b'PS1'] = b'SSL> '\n    cmd = SSLCommands(ui)\n    parser = CLI.CommandParser(cmd, historyfile=os.path.expandvars(b'$HOME/.hist_sslcli'))\n    parser.interact()\n", "label": 0}
{"function": "\n\ndef load_color(random_seed=123522):\n    data_path = os.path.join(os.path.split(__file__)[0], 'data')\n    if (not os.path.exists(data_path)):\n        os.makedirs(data_path)\n    dataset = 'train.zip'\n    data_file = os.path.join(data_path, dataset)\n    if os.path.isfile(data_file):\n        dataset = data_file\n    if (not os.path.isfile(data_file)):\n        try:\n            import urllib\n            urllib.urlretrieve('http://google.com')\n        except AttributeError:\n            import urllib.request as urllib\n        url = 'https://dl.dropboxusercontent.com/u/15378192/train.zip'\n        print(('Downloading data from %s' % url))\n        urllib.urlretrieve(url, data_file)\n    data_dir = os.path.join(data_path, 'cvd')\n    if (not os.path.exists(data_dir)):\n        os.makedirs(data_dir)\n        zf = zipfile.ZipFile(data_file)\n        zf.extractall(data_dir)\n    data_file = os.path.join(data_path, 'cvd_color.hdf5')\n    label_file = os.path.join(data_path, 'cvd_color_labels.npy')\n    if (not os.path.exists(data_file)):\n        print('... loading data')\n        cat_matches = []\n        dog_matches = []\n        for (root, dirname, filenames) in os.walk(data_dir):\n            for filename in fnmatch.filter(filenames, 'cat*'):\n                cat_matches.append(os.path.join(root, filename))\n            for filename in fnmatch.filter(filenames, 'dog*'):\n                dog_matches.append(os.path.join(root, filename))\n        sort_key = (lambda x: int(x.split('.')[(- 2)]))\n        cat_matches = sorted(cat_matches, key=sort_key)\n        dog_matches = sorted(dog_matches, key=sort_key)\n\n        def square(X):\n            resize_shape = (64, 64)\n            slice_size = (48, 48)\n            slice_left = ((resize_shape[0] - slice_size[0]) / 2)\n            slice_upper = ((resize_shape[1] - slice_size[1]) / 2)\n            return imresize(X, resize_shape, interp='nearest')[slice_left:(slice_left + slice_size[0]), slice_upper:(slice_upper + slice_size[1])].transpose(2, 0, 1).astype('float32')\n        matches = (cat_matches + dog_matches)\n        matches = np.array(matches)\n        random_state = np.random.RandomState(random_seed)\n        idx = random_state.permutation(len(matches))\n        c = ([0] * len(cat_matches))\n        d = ([1] * len(dog_matches))\n        y = np.array((c + d)).astype('int32')\n        matches = matches[idx]\n        y = y[idx]\n        compression_filter = tables.Filters(complevel=5, complib='blosc')\n        h5_file = tables.openFile(data_file, mode='w')\n        example = square(mpimg.imread(matches[0]))\n        image_storage = h5_file.createEArray(h5_file.root, 'images', tables.Float32Atom(), shape=((0,) + example.shape), filters=compression_filter)\n        for (n, f) in enumerate(matches):\n            print(('Processing image %i of %i' % (n, len(matches))))\n            X = square(mpimg.imread(f)).astype('float32')\n            image_storage.append(X[None])\n        h5_file.close()\n        np.save(label_file, y)\n    h5_file = tables.openFile(data_file, mode='r')\n    X_s = h5_file.root.images\n    y_s = np.load(label_file)\n    return (X_s, y_s)\n", "label": 1}
{"function": "\n\ndef _autodiscover(self):\n    'Discovers modules to register from ``settings.INSTALLED_APPS``.\\n\\n        This makes sure that the appropriate modules get imported to register\\n        themselves with Horizon.\\n        '\n    if (not getattr(self, '_registerable_class', None)):\n        raise ImproperlyConfigured('You must set a \"_registerable_class\" property in order to use autodiscovery.')\n    for mod_name in ('dashboard', 'panel'):\n        for app in settings.INSTALLED_APPS:\n            mod = import_module(app)\n            try:\n                before_import_registry = copy.copy(self._registry)\n                import_module(('%s.%s' % (app, mod_name)))\n            except Exception:\n                self._registry = before_import_registry\n                if module_has_submodule(mod, mod_name):\n                    raise\n", "label": 0}
{"function": "\n\ndef __get__(self, instance, cls=None):\n    if (instance is None):\n        return self\n    try:\n        return getattr(instance, self.cache_attr)\n    except AttributeError:\n        rel_obj = None\n        f = self.model._meta.get_field(self.ct_field)\n        ct_id = getattr(instance, f.get_attname(), None)\n        if (ct_id is not None):\n            ct = self.get_content_type(id=ct_id, using=instance._state.db)\n            try:\n                rel_obj = ct.get_object_for_this_type(pk=getattr(instance, self.fk_field))\n            except ObjectDoesNotExist:\n                pass\n        setattr(instance, self.cache_attr, rel_obj)\n        return rel_obj\n", "label": 0}
{"function": "\n\n@classmethod\ndef get_page(cls, link, skip_archives=True, session=None):\n    if (session is None):\n        raise TypeError(\"get_page() missing 1 required keyword argument: 'session'\")\n    url = link.url\n    url = url.split('#', 1)[0]\n    from pip.vcs import VcsSupport\n    for scheme in VcsSupport.schemes:\n        if (url.lower().startswith(scheme) and (url[len(scheme)] in '+:')):\n            logger.debug('Cannot look at %s URL %s', scheme, link)\n            return None\n    try:\n        if skip_archives:\n            filename = link.filename\n            for bad_ext in ARCHIVE_EXTENSIONS:\n                if filename.endswith(bad_ext):\n                    content_type = cls._get_content_type(url, session=session)\n                    if content_type.lower().startswith('text/html'):\n                        break\n                    else:\n                        logger.debug('Skipping page %s because of Content-Type: %s', link, content_type)\n                        return\n        logger.debug('Getting page %s', url)\n        (scheme, netloc, path, params, query, fragment) = urllib_parse.urlparse(url)\n        if ((scheme == 'file') and os.path.isdir(urllib_request.url2pathname(path))):\n            if (not url.endswith('/')):\n                url += '/'\n            url = urllib_parse.urljoin(url, 'index.html')\n            logger.debug(' file: URL is directory, getting %s', url)\n        resp = session.get(url, headers={\n            'Accept': 'text/html',\n            'Cache-Control': 'max-age=600',\n        })\n        resp.raise_for_status()\n        content_type = resp.headers.get('Content-Type', 'unknown')\n        if (not content_type.lower().startswith('text/html')):\n            logger.debug('Skipping page %s because of Content-Type: %s', link, content_type)\n            return\n        inst = cls(resp.content, resp.url, resp.headers, trusted=link.trusted)\n    except requests.HTTPError as exc:\n        level = (2 if (exc.response.status_code == 404) else 1)\n        cls._handle_fail(link, exc, url, level=level)\n    except requests.ConnectionError as exc:\n        cls._handle_fail(link, ('connection error: %s' % exc), url)\n    except requests.Timeout:\n        cls._handle_fail(link, 'timed out', url)\n    except SSLError as exc:\n        reason = ('There was a problem confirming the ssl certificate: %s' % exc)\n        cls._handle_fail(link, reason, url, level=2, meth=logger.info)\n    else:\n        return inst\n", "label": 1}
{"function": "\n\n@then('the tag expression selects model elements with')\ndef step_given_named_model_elements_with_tags(context):\n    '\\n    .. code-block:: gherkin\\n\\n        Then the tag expression select model elements with:\\n            | tag expression | selected?    |\\n            |  @foo          | S1, S3       |\\n            | -@foo          | S0, S2, S3   |\\n    '\n    assert context.model_elements, 'REQUIRE: context attribute'\n    assert context.table, 'REQUIRE: context.table'\n    context.table.require_columns(['tag expression', 'selected?'])\n    for (row_index, row) in enumerate(context.table.rows):\n        tag_expression_text = row['tag expression']\n        tag_expression = convert_tag_expression(tag_expression_text)\n        expected_selected_names = convert_comma_list(row['selected?'])\n        actual_selected = []\n        for model_element in context.model_elements:\n            if tag_expression.check(model_element.tags):\n                actual_selected.append(model_element.name)\n        assert_that(actual_selected, equal_to(expected_selected_names), ('tag_expression=%s (row=%s)' % (tag_expression_text, row_index)))\n", "label": 0}
{"function": "\n\ndef set_system_date_time(years=None, months=None, days=None, hours=None, minutes=None, seconds=None):\n    \"\\n    Set the system date and time. Each argument is an element of the date, but\\n    not required. If an element is not passed, the current system value for that\\n    element will be used. For example, if you don't pass the year, the current\\n    system year will be used. (Used by set_system_date and set_system_time)\\n\\n    :param int years: Years digit, ie: 2015\\n    :param int months: Months digit: 1 - 12\\n    :param int days: Days digit: 1 - 31\\n    :param int hours: Hours digit: 0 - 23\\n    :param int minutes: Minutes digit: 0 - 59\\n    :param int seconds: Seconds digit: 0 - 59\\n\\n    :return: True if successful. Otherwise False.\\n    :rtype: bool\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' system.set_system_date_ time 2015 5 12 11 37 53\\n    \"\n    try:\n        date_time = win32api.GetLocalTime()\n    except win32api.error as exc:\n        (number, context, message) = exc\n        log.error('Failed to get local time')\n        log.error('nbr: {0}'.format(number))\n        log.error('ctx: {0}'.format(context))\n        log.error('msg: {0}'.format(message))\n        return False\n    if (not years):\n        years = date_time[0]\n    if (not months):\n        months = date_time[1]\n    if (not days):\n        days = date_time[3]\n    if (not hours):\n        hours = date_time[4]\n    if (not minutes):\n        minutes = date_time[5]\n    if (not seconds):\n        seconds = date_time[6]\n    time_tuple = (years, months, days, hours, minutes, seconds, 0)\n    try:\n        win32api.SetLocalTime(time_tuple)\n    except win32api.error as exc:\n        (number, context, message) = exc\n        log.error('Failed to set local time')\n        log.error('nbr: {0}'.format(number))\n        log.error('ctx: {0}'.format(context))\n        log.error('msg: {0}'.format(message))\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef rs_atan(p, x, prec):\n    \"\\n    The arctangent of a series\\n\\n    Return the series expansion of the atan of ``p``, about 0.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_atan\\n    >>> R, x, y = ring('x, y', QQ)\\n    >>> rs_atan(x + x*y, x, 4)\\n    -1/3*x**3*y**3 - x**3*y**2 - x**3*y - 1/3*x**3 + x*y + x\\n\\n    See Also\\n    ========\\n\\n    atan\\n    \"\n    if rs_is_puiseux(p, x):\n        return rs_puiseux(rs_atan, p, x, prec)\n    R = p.ring\n    const = 0\n    if _has_constant_term(p, x):\n        zm = R.zero_monom\n        c = p[zm]\n        if (R.domain is EX):\n            c_expr = c.as_expr()\n            const = atan(c_expr)\n        elif isinstance(c, PolyElement):\n            try:\n                c_expr = c.as_expr()\n                const = R(atan(c_expr))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        else:\n            try:\n                const = R(atan(c))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n    dp = p.diff(x)\n    p1 = (rs_square(p, x, prec) + R(1))\n    p1 = rs_series_inversion(p1, x, (prec - 1))\n    p1 = rs_mul(dp, p1, x, (prec - 1))\n    return (rs_integrate(p1, x) + const)\n", "label": 0}
{"function": "\n\ndef sync_ldap_groups(self, ldap_groups):\n    '\\n        Synchronize LDAP groups with local group database.\\n        '\n    attributes = getattr(settings, 'LDAP_SYNC_GROUP_ATTRIBUTES', None)\n    groupname_field = 'name'\n    if (groupname_field not in attributes.values()):\n        error_msg = (\"LDAP_SYNC_GROUP_ATTRIBUTES must contain the group name field '%s'\" % groupname_field)\n        raise ImproperlyConfigured(error_msg)\n    for (cname, attrs) in ldap_groups:\n        try:\n            items = attrs.items()\n        except AttributeError:\n            continue\n        group_attr = {\n            \n        }\n        for (name, attr) in items:\n            group_attr[attributes[name]] = attr[0].decode('utf-8')\n        try:\n            groupname = group_attr[groupname_field]\n            group_attr[groupname_field] = groupname.lower()\n        except KeyError:\n            logger.warning((\"Group is missing a required attribute '%s'\" % groupname_field))\n            continue\n        kwargs = {\n            (groupname_field + '__iexact'): groupname,\n            'defaults': group_attr,\n        }\n        try:\n            (group, created) = Group.objects.get_or_create(**kwargs)\n        except IntegrityError as e:\n            logger.error(('Error creating group %s' % e))\n        else:\n            if created:\n                logger.debug(('Created group %s' % groupname))\n    logger.info('Groups are synchronized')\n", "label": 0}
{"function": "\n\ndef _extra_config(user_defined_config, base_dir):\n    'Discover new items in any extra directories and add the new values.\\n\\n    :param user_defined_config: ``dict``\\n    :param base_dir: ``str``\\n    '\n    for (root_dir, _, files) in os.walk(base_dir):\n        for name in files:\n            if name.endswith(('.yml', '.yaml')):\n                with open(os.path.join(root_dir, name), 'rb') as f:\n                    _merge_dict(user_defined_config, (yaml.safe_load(f.read()) or {\n                        \n                    }))\n", "label": 0}
{"function": "\n\ndef test_process_commit():\n    (m, ctl, config) = init()\n    config['numprocesses'] = 0\n    m.load(config, start=False)\n    cmd = TestCommand('commit', ['dummy'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    state = m._get_locked_state('dummy')\n    assert (len(state.running) == 0)\n    assert (state.numprocesses == 0)\n    assert (len(state.running_out) == 1)\n    assert (m.pids() == [1])\n    m.stop()\n    m.run()\n    assert (cmd.result['pid'] == 1)\n", "label": 0}
{"function": "\n\ndef bind_params(self, I, E, O, alpha):\n    assert (I.dtype == E.dtype)\n    if (not self.initialized):\n        self.initialized = True\n        self.autotune(I, E, O)\n    if ((O.dtype.type is not np.float32) or self.determ_size):\n        updat_temp = self.lib.scratch_buffer(self.output_size)\n        image_temp = self.lib.scratch_buffer_offset(self.image_size)\n        delta_temp = self.lib.scratch_buffer_offset(self.delta_size)\n        self.convert_args = [updat_temp, 'f4', O, self.determ_shape]\n    else:\n        updat_temp = O.gpudata\n        image_temp = self.lib.scratch_buffer(self.image_size)\n        delta_temp = self.lib.scratch_buffer_offset(self.delta_size)\n        self.convert_args = False\n    self.image_args[2:5] = (self.lib.stream, image_temp, I.gpudata)\n    self.delta_args[2:5] = (self.lib.stream, delta_temp, E.gpudata)\n    if self.zero:\n        self.zero_args = [updat_temp, 0, O.size, self.lib.stream]\n    self.kernel[3:8] = (self.lib.stream, updat_temp, image_temp, delta_temp, alpha)\n", "label": 0}
{"function": "\n\ndef validate(self, object, name, value):\n    ' Validates that the value is a valid font descriptor string.\\n        '\n    try:\n        point_size = family = style = weight = underline = ''\n        facename = ['']\n        for word in value.split():\n            lword = word.lower()\n            if (lword in font_families):\n                family = (' ' + lword)\n            elif (lword in font_styles):\n                style = (' ' + lword)\n            elif (lword in font_weights):\n                weight = (' ' + lword)\n            elif (lword == 'underline'):\n                underline = (' ' + lword)\n            elif (lword not in font_noise):\n                try:\n                    int(lword)\n                    point_size = (lword + ' pt')\n                except:\n                    facename.append(word)\n        fontstr = ('%s%s%s%s%s%s' % (point_size, family, style, weight, underline, ' '.join(facename))).strip()\n        return fontstr\n    except Exception:\n        pass\n    raise TraitError(object, name, 'a font descriptor string', repr(value))\n", "label": 0}
{"function": "\n\ndef read_events(self, timeout=None):\n    timeout_ms = 2147483647\n    if (timeout is not None):\n        timeout_ms = int((timeout * 1000))\n        if ((timeout_ms < 0) or (timeout_ms >= 2147483647)):\n            raise ValueError('Timeout value out of range')\n    try:\n        events = []\n        (rc, num, key, _) = win32file.GetQueuedCompletionStatus(self.__cphandle, timeout_ms)\n        if (rc == 0):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled and (not watch._removed)):\n                    events.extend(process_events(watch, num))\n        elif (rc == 5):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled):\n                    close_watch(watch)\n                    del self.__key_to_watch[key]\n                    events.append(FSEvent(watch, FSEvent.DeleteSelf))\n        return events\n    except pywintypes.error as e:\n        raise FSMonitorWindowsError(*e.args)\n", "label": 1}
{"function": "\n\ndef _addPhantomContents(self, changeSet, trv, header):\n    'Fabricate files for the given RPM header'\n    for (path, owner, group, mode, size, rdev, flags, vflags, linkto, mtime) in itertools.izip(header[rpmhelper.OLDFILENAMES], header[rpmhelper.FILEUSERNAME], header[rpmhelper.FILEGROUPNAME], header[rpmhelper.FILEMODES], header[rpmhelper.FILESIZES], header[rpmhelper.FILERDEVS], header[rpmhelper.FILEFLAGS], header[rpmhelper.FILEVERIFYFLAGS], header[rpmhelper.FILELINKTOS], header[rpmhelper.FILEMTIMES]):\n        fullPath = util.joinPaths(self.root, path)\n        fakestat = FakeStat(mode, 0, None, 1, owner, group, size, mtime, mtime, mtime, st_rdev=rdev, linkto=linkto)\n        pathId = os.urandom(16)\n        kind = 'regular'\n        if (flags & rpmhelper.RPMFILE_GHOST):\n            kind = 'initial'\n        elif (flags & ((rpmhelper.RPMFILE_CONFIG | rpmhelper.RPMFILE_MISSINGOK) | rpmhelper.RPMFILE_NOREPLACE)):\n            if size:\n                kind = 'config'\n            else:\n                kind = 'initial'\n        elif vflags:\n            if ((stat.S_ISREG(mode) and (not (vflags & rpmhelper.RPMVERIFY_FILEDIGEST))) or (stat.S_ISLNK(mode) and (not (vflags & rpmhelper.RPMVERIFY_LINKTO)))):\n                kind = 'initial'\n        fileStream = files.FileFromFilesystem(fullPath, pathId, statBuf=fakestat, sha1FailOk=True)\n        if (kind == 'config'):\n            fileStream.flags.isConfig(set=True)\n        elif (kind == 'initial'):\n            fileStream.flags.isInitialContents(set=True)\n        else:\n            assert (kind == 'regular')\n        if (isinstance(fileStream, files.RegularFile) and (not fileStream.flags.isConfig()) and (not (fileStream.flags.isInitialContents() and (not fileStream.contents.size())))):\n            fileStream.flags.isEncapsulatedContent(set=True)\n        fileId = fileStream.fileId()\n        trv.addFile(pathId, path, trv.getVersion(), fileId)\n        changeSet.addFile(None, fileId, fileStream.freeze())\n        if (fileStream.hasContents and (not fileStream.flags.isEncapsulatedContent())):\n            if (fileStream.contents.sha1() == sha1helper.sha1Empty):\n                contents = filecontents.FromString('')\n            else:\n                contents = filecontents.FromFilesystem(fullPath)\n            changeSet.addFileContents(pathId, fileId, contType=changeset.ChangedFileTypes.file, contents=contents, cfgFile=fileStream.flags.isConfig())\n", "label": 1}
{"function": "\n\n@property\ndef select_options(self):\n    start_year = getattr(settings, 'START_YEAR', 2008)\n    years = [dict(val=unicode(y), text=y) for y in range(start_year, (datetime.utcnow().year + 1))]\n    years.reverse()\n    months = [dict(val=('%02d' % m), text=calendar.month_name[m]) for m in range(1, 13)]\n    now = datetime.now()\n    three_month_earlier = (now - relativedelta(months=3))\n    first_friday = rrule(DAILY, dtstart=datetime(three_month_earlier.year, three_month_earlier.month, 1), until=now, byweekday=FR)[0]\n    if (first_friday.day != 1):\n        first_friday = (first_friday - relativedelta(days=7))\n    fridays = rrule(WEEKLY, dtstart=first_friday, until=now, byweekday=FR)\n    weeks = []\n    value = None\n    text = None\n    for (idx, val) in enumerate(fridays):\n        try:\n            value = '{0}|{1}'.format(val.strftime('%Y-%m-%d'), fridays[(idx + 1)].strftime('%Y-%m-%d'))\n            text = '{0} - {1}'.format(ews_date_format(val), ews_date_format((fridays[(idx + 1)] - relativedelta(days=1))))\n        except IndexError:\n            next_thursday = (val + relativedelta(days=6))\n            value = '{0}|{1}'.format(val.strftime('%Y-%m-%d'), next_thursday.strftime('%Y-%m-%d'))\n            text = '{0} - {1}'.format(ews_date_format(val), ews_date_format(next_thursday))\n        finally:\n            weeks.append(dict(val=value, text=text))\n    return [{\n        'text': 'Week (Friday - Thursday)',\n        'val': 2,\n        'firstOptions': weeks,\n        'secondOptions': [],\n    }, {\n        'text': 'Month',\n        'val': 1,\n        'firstOptions': months,\n        'secondOptions': years,\n    }]\n", "label": 0}
{"function": "\n\ndef test_mixedset_cache_miss(self, backend, base_set, base_set2):\n    ms = op2.MixedSet([base_set, base_set2])\n    ms2 = op2.MixedSet([base_set2, base_set])\n    assert (ms is not ms2)\n    assert (ms != ms2)\n    assert (not (ms == ms2))\n    ms3 = op2.MixedSet([base_set, base_set2])\n    assert (ms is ms3)\n    assert (not (ms != ms3))\n    assert (ms == ms3)\n", "label": 0}
{"function": "\n\ndef image_style(image, style):\n    ' Return the url of different image style\\n    Construct appropriately if not exist\\n    See 9cms-crop.odt\\n\\n    Available styles\\n     - thumbnail: create a thumbnail restricted to the smaller dimension\\n     - thumbnail-upscale: create a thumbnail that is upscaled if smaller\\n     - thumbnail-crop: create a thumbnail that is cropped to the exact dimension\\n\\n    :param image: ImageFieldFile\\n    :param style: Specify style to return image\\n    :return: image url of specified style\\n    '\n    if (not image):\n        return image\n    url = image.url\n    url_path = '/'.join(url.split('/')[:(- 1)])\n    img_path_file_name = str(image.file)\n    img_file_name = os.path.basename(img_path_file_name)\n    style_url_path = '/'.join((url_path, style))\n    style_url = '/'.join((style_url_path, img_file_name))\n    style_path = os.path.join(os.path.dirname(img_path_file_name), style)\n    style_path_file_name = os.path.join(style_path, img_file_name)\n    style_def = settings.IMAGE_STYLES[style]\n    if (not os.path.exists(style_path_file_name)):\n        if (not os.path.exists(style_path)):\n            os.makedirs(style_path)\n        by = chr(120)\n        plus = chr(43)\n        try:\n            source_size_str = check_output(['identify', img_path_file_name]).decode()\n        except CalledProcessError:\n            return url\n        source_size_str = source_size_str[len(img_path_file_name):].split(' ')[2]\n        source_size_array = source_size_str.split(by)\n        source_size_x = int(source_size_array[0])\n        source_size_y = int(source_size_array[1])\n        target_size_x = style_def['size'][0]\n        target_size_y = style_def['size'][1]\n        target_size_str = ((str(target_size_x) + by) + str(target_size_y))\n        if (style_def['type'] == 'thumbnail'):\n            if ((target_size_x > source_size_x) and (target_size_y > source_size_y)):\n                target_size_str = source_size_str\n            call(['convert', img_path_file_name, '-thumbnail', target_size_str, '-antialias', style_path_file_name])\n        elif (style_def['type'] == 'thumbnail-upscale'):\n            call(['convert', img_path_file_name, '-thumbnail', target_size_str, '-antialias', style_path_file_name])\n        elif (style_def['type'] == 'thumbnail-crop'):\n            source_ratio = (float(source_size_x) / float(source_size_y))\n            target_ratio = (float(target_size_x) / float(target_size_y))\n            if (source_ratio > target_ratio):\n                crop_target_size_x = (source_size_y * target_ratio)\n                crop_target_size_y = source_size_y\n                offset = ((source_size_x - crop_target_size_x) / 2)\n                crop_size_str = ((((((str(crop_target_size_x) + by) + str(crop_target_size_y)) + plus) + str(offset)) + plus) + '0')\n            else:\n                crop_target_size_x = source_size_x\n                crop_target_size_y = (source_size_x / target_ratio)\n                offset = ((source_size_y - crop_target_size_y) / 2)\n                crop_size_str = ((((((str(crop_target_size_x) + by) + str(crop_target_size_y)) + plus) + '0') + plus) + str(offset))\n            call(['convert', img_path_file_name, '-crop', crop_size_str, style_path_file_name])\n            call(['convert', style_path_file_name, '-thumbnail', target_size_str, '-antialias', style_path_file_name])\n    return style_url\n", "label": 1}
{"function": "\n\ndef getTempPath(file_name=False):\n    tmp_path = '/tmp'\n    os_name = Tools.getOsName()\n    if (os_name == 'windows'):\n        tmp_path = os.environ['tmp']\n    tmp_path = os.path.join(tmp_path, 'Deviot')\n    if file_name:\n        tmp_path = os.path.join(tmp_path, file_name)\n    try:\n        os.makedirs(tmp_path)\n    except OSError as exc:\n        if (exc.errno != errno.EEXIST):\n            raise exc\n        pass\n    return tmp_path\n", "label": 0}
{"function": "\n\ndef getas(self, convert, name, default=None, valuetype=None):\n    'Converts the value of user-data parameter from a string into a\\n        specific value type.\\n\\n        :param convert: Converter function to use (string to value-type).\\n        :param name:    Variable name to use.\\n        :param default: Default value, used if parameter is not found.\\n        :param valuetype: Value type(s), needed if convert != valuetype()\\n        :return: Converted textual value (type: valuetype)\\n        :return: Default value, if parameter is unknown.\\n        :raises ValueError: If type conversion fails.\\n        '\n    if (valuetype is None):\n        valuetype = convert\n    value = self.get(name, Unknown)\n    if (value is Unknown):\n        return default\n    elif isinstance(value, valuetype):\n        return value\n    else:\n        assert callable(convert)\n        return convert(value)\n", "label": 0}
{"function": "\n\ndef test_augmented_assignment(self, space):\n    self.assert_compiles(space, 'i = 0; i += 1', '\\n        LOAD_CONST 0\\n        STORE_DEREF 0\\n        DISCARD_TOP\\n\\n        LOAD_DEREF 0\\n        LOAD_CONST 1\\n        SEND 2 1\\n        STORE_DEREF 0\\n\\n        RETURN\\n        ')\n    bc = self.assert_compiles(space, 'self.x.y += 1', '\\n        LOAD_SELF\\n        SEND 0 0\\n        DUP_TOP\\n        SEND 1 0\\n        LOAD_CONST 2\\n        SEND 3 1\\n        SEND 4 1\\n\\n        RETURN\\n        ')\n    assert (space.symbol_w(bc.consts_w[0]) == 'x')\n    assert (space.symbol_w(bc.consts_w[1]) == 'y')\n    assert (space.symbol_w(bc.consts_w[3]) == '+')\n    assert (space.symbol_w(bc.consts_w[4]) == 'y=')\n    self.assert_compiles(space, '@a += 2', '\\n        LOAD_SELF\\n        DUP_TOP\\n        LOAD_INSTANCE_VAR 0\\n        LOAD_CONST 1\\n        SEND 2 1\\n        STORE_INSTANCE_VAR 0\\n\\n        RETURN\\n        ')\n", "label": 0}
{"function": "\n\ndef shell_exec_monitor(self, Command, PluginInfo):\n    CommandInfo = self.StartCommand(Command, Command)\n    (Target, CanRun) = self.CanRunCommand(CommandInfo)\n    if (not CanRun):\n        Message = ('The command was already run for target: ' + str(Target))\n        return Message\n    logging.info('')\n    logging.info('Executing :\\n\\n%s\\n\\n', Command)\n    logging.info('')\n    logging.info(('------> Execution Start Date/Time: ' + self.timer.get_start_date_time_as_str('Command')))\n    logging.info('')\n    Output = ''\n    Cancelled = False\n    try:\n        proc = self.create_subprocess(Command)\n        while True:\n            line = proc.stdout.readline()\n            if (not line):\n                break\n            logging.warn(line.strip())\n            Output += line\n    except KeyboardInterrupt:\n        os.killpg(proc.pid, signal.SIGINT)\n        (outdata, errdata) = proc.communicate()\n        logging.warn(outdata)\n        Output += outdata\n        try:\n            os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n        except OSError:\n            pass\n        Cancelled = True\n        Output += self.error_handler.UserAbort('Command', Output)\n    finally:\n        self.FinishCommand(CommandInfo, Cancelled, PluginInfo)\n    return scrub_output(Output)\n", "label": 0}
{"function": "\n\n@services.route('/services/fetch_profile')\ndef fetch_profile():\n    url = request.args.get('url')\n    if (not url):\n        return '\\n<html><body>\\n<h1>Fetch Profile</h1>\\n<form><label>URL to fetch: <input name=\"url\"></label>\\n<input type=\"Submit\">\\n</form></body></html>'\n    try:\n        name = None\n        image = None\n        d = mf2py.Parser(url=url).to_dict()\n        relmes = d['rels'].get('me', [])\n        hfeed = next((item for item in d['items'] if ('h-feed' in item['type'])), None)\n        if hfeed:\n            authors = hfeed.get('properties', {\n                \n            }).get('author')\n            images = hfeed.get('properties', {\n                \n            }).get('photo')\n            if authors:\n                if isinstance(authors[0], dict):\n                    name = authors[0].get('properties', {\n                        \n                    }).get('name')\n                    image = authors[0].get('properties', {\n                        \n                    }).get('photo')\n                else:\n                    name = authors[0]\n            if (images and (not image)):\n                image = images[0]\n        for item in d['items']:\n            if ('h-card' in item.get('type', [])):\n                if (not name):\n                    name = item.get('properties', {\n                        \n                    }).get('name')\n                if (not image):\n                    image = item.get('properties', {\n                        \n                    }).get('photo')\n        return jsonify({\n            'name': name,\n            'image': image,\n            'social': relmes,\n        })\n    except BaseException as e:\n        resp = jsonify({\n            'error': str(e),\n        })\n        resp.status_code = 400\n        return resp\n", "label": 1}
{"function": "\n\ndef _after_flush(app, changes):\n    bytype = {\n        \n    }\n    for change in changes:\n        update = (change[1] in ('update', 'insert'))\n        if hasattr(change[0].__class__, __searchable__):\n            bytype.setdefault(change[0].__class__.__name__, []).append((update, change[0]))\n    for (model, values) in bytype.iteritems():\n        index = whoosh_index(app, values[0][1].__class__)\n        with index.writer() as writer:\n            primary_field = values[0][1].pure_whoosh.primary_key_name\n            searchable = values[0][1].__searchable__\n            for (update, v) in values:\n                if update:\n                    attrs = {\n                        \n                    }\n                    for key in searchable:\n                        try:\n                            attrs[key] = unicode(getattr(v, key))\n                        except AttributeError:\n                            raise AttributeError('{0} does not have {1} field {2}'.format(model, __searchable__, key))\n                    attrs[primary_field] = unicode(getattr(v, primary_field))\n                    writer.update_document(**attrs)\n                else:\n                    writer.delete_by_term(primary_field, unicode(getattr(v, primary_field)))\n", "label": 0}
{"function": "\n\ndef ssl_settings(host, config_file, env=os.environ):\n    \"\\n    Function wcich generates SSL setting for cassandra.Cluster\\n\\n    Params:\\n    * host .........: hostname of Cassandra node.\\n    * env ..........: environment variables. SSL factory will use, if passed,\\n                      SSL_CERTFILE and SSL_VALIDATE variables.\\n    * config_file ..: path to cqlsh config file (usually ~/.cqlshrc).\\n                      SSL factory will use, if set, certfile and validate\\n                      options in [ssl] section, as well as host to certfile\\n                      mapping in [certfiles] section.\\n\\n    [certfiles] section is optional, 'validate' setting in [ssl] section is\\n    optional too. If validation is enabled then SSL certfile must be provided\\n    either in the config file or as an environment variable.\\n    Environment variables override any options set in cqlsh config file.\\n    \"\n    configs = ConfigParser.SafeConfigParser()\n    configs.read(config_file)\n\n    def get_option(section, option):\n        try:\n            return configs.get(section, option)\n        except ConfigParser.Error:\n            return None\n    ssl_validate = env.get('SSL_VALIDATE')\n    if (ssl_validate is None):\n        ssl_validate = get_option('ssl', 'validate')\n    ssl_validate = ((ssl_validate is None) or (ssl_validate.lower() != 'false'))\n    ssl_certfile = env.get('SSL_CERTFILE')\n    if (ssl_certfile is None):\n        ssl_certfile = get_option('certfiles', host)\n    if (ssl_certfile is None):\n        ssl_certfile = get_option('ssl', 'certfile')\n    if (ssl_validate and (ssl_certfile is None)):\n        sys.exit((\"Validation is enabled; SSL transport factory requires a valid certfile to be specified. Please provide path to the certfile in [ssl] section as 'certfile' option in %s (or use [certfiles] section) or set SSL_CERTFILE environment variable.\" % (config_file,)))\n    if (not (ssl_certfile is None)):\n        ssl_certfile = os.path.expanduser(ssl_certfile)\n    userkey = get_option('ssl', 'userkey')\n    if userkey:\n        userkey = os.path.expanduser(userkey)\n    usercert = get_option('ssl', 'usercert')\n    if usercert:\n        usercert = os.path.expanduser(usercert)\n    return dict(ca_certs=ssl_certfile, cert_reqs=(ssl.CERT_REQUIRED if ssl_validate else ssl.CERT_NONE), ssl_version=ssl.PROTOCOL_TLSv1, keyfile=userkey, certfile=usercert)\n", "label": 1}
{"function": "\n\ndef poll(self, event, timeout):\n    if (event == 'recv'):\n        with self.recv_ready:\n            if (len(self.recv_queue) == 0):\n                self.recv_ready.wait(timeout)\n            if (len(self.recv_queue) > 0):\n                return self.recv_queue[0].type\n            return None\n    if (event == 'send'):\n        with self.send_ready:\n            if (len(self.send_queue) >= self.send_buf):\n                self.send_ready.wait(timeout)\n            return (len(self.send_queue) < self.send_buf)\n", "label": 0}
{"function": "\n\n@newick.reader(TreeNode)\ndef _newick_to_tree_node(fh, convert_underscores=True):\n    tree_stack = []\n    current_depth = 0\n    last_token = ''\n    next_is_distance = False\n    root = TreeNode()\n    tree_stack.append((root, current_depth))\n    for token in _tokenize_newick(fh, convert_underscores=convert_underscores):\n        if (last_token not in '(,):'):\n            if (not next_is_distance):\n                tree_stack[(- 1)][0].name = (last_token if last_token else None)\n            else:\n                next_is_distance = False\n        if (token == ':'):\n            next_is_distance = True\n        elif (last_token == ':'):\n            try:\n                tree_stack[(- 1)][0].length = float(token)\n            except ValueError:\n                raise NewickFormatError(('Could not read length as numeric type: %s.' % token))\n        elif (token == '('):\n            current_depth += 1\n            tree_stack.append((TreeNode(), current_depth))\n        elif (token == ','):\n            tree_stack.append((TreeNode(), current_depth))\n        elif (token == ')'):\n            if (len(tree_stack) < 2):\n                raise NewickFormatError('Could not parse file as newick. Parenthesis are unbalanced.')\n            children = []\n            while (current_depth == tree_stack[(- 1)][1]):\n                (node, _) = tree_stack.pop()\n                children.insert(0, node)\n            parent = tree_stack[(- 1)][0]\n            if parent.children:\n                raise NewickFormatError('Could not parse file as newick. Contains unnested children.')\n            for child in children:\n                child.parent = parent\n            parent.children = children\n            current_depth -= 1\n        elif (token == ';'):\n            if (len(tree_stack) == 1):\n                return root\n            break\n        last_token = token\n    raise NewickFormatError(\"Could not parse file as newick. `(Parenthesis)`, `'single-quotes'`, `[comments]` may be unbalanced, or tree may be missing its root.\")\n", "label": 1}
{"function": "\n\ndef reset(self, **traits):\n    'Creates the dataset afresh or resets existing data source.'\n    self.set(trait_change_notify=False, **traits)\n    points = self.points\n    scalars = self.scalars\n    (x, y, z) = (self.x, self.y, self.z)\n    points = np.c_[(x.ravel(), y.ravel(), z.ravel())].ravel()\n    points.shape = ((- 1), 3)\n    self.set(points=points, trait_change_notify=False)\n    triangles = self.triangles\n    assert (triangles.shape[1] == 3), 'The shape of the triangles array must be (X, 3)'\n    assert (triangles.max() < len(points)), 'The triangles indices must be smaller that the number of points'\n    assert (triangles.min() >= 0), 'The triangles indices must be positive or null'\n    if (self.dataset is None):\n        pd = tvtk.PolyData()\n    else:\n        pd = self.dataset\n    pd.set(points=points)\n    pd.set(polys=triangles)\n    if ((not ('scalars' in traits)) and (scalars is not None) and (scalars.shape != x.shape)):\n        scalars = z\n    if ((scalars is not None) and (len(scalars) > 0)):\n        if (not scalars.flags.contiguous):\n            scalars = scalars.copy()\n            self.set(scalars=scalars, trait_change_notify=False)\n        assert (x.shape == scalars.shape)\n        pd.point_data.scalars = scalars.ravel()\n        pd.point_data.scalars.name = 'scalars'\n    self.dataset = pd\n", "label": 1}
{"function": "\n\ndef parse_cluster_position(_cluster_position):\n    try:\n        if isinstance(_cluster_position, str):\n            pass\n        elif isinstance(_cluster_position, int):\n            _cluster_position = str(_cluster_position)\n        elif isinstance(_cluster_position, bytes):\n            _cluster_position = _cluster_position.decode('utf-8')\n        elif isinstance(_cluster_position, OrientRecordLink):\n            _cluster_position = _cluster_position.get()\n        (_cluster, _position) = _cluster_position.split(':')\n    except (AttributeError, ValueError):\n        _position = _cluster_position\n    return _position\n", "label": 0}
{"function": "\n\ndef test_read_write(self):\n    record_type = FCGI_DATA\n    request_id = randint(1, 65535)\n    data = binary_data()\n    record = Record(record_type, data, request_id)\n    conn = Connection(self.sock)\n    conn.write_record(record)\n    self.sock.flip()\n    record = conn.read_record()\n    assert (record.type == record_type)\n    assert (record.content == data)\n    assert (record.request_id == request_id)\n    assert (conn.read_record() is None)\n", "label": 0}
{"function": "\n\ndef test_nested_block(self, space):\n    bc = self.assert_compiles(space, '\\n        sums = []\\n        [].each do |x|\\n            [].each do |y|\\n                sums << x + y\\n            end\\n        end\\n        ', '\\n        BUILD_ARRAY 0\\n        STORE_DEREF 0\\n        DISCARD_TOP\\n\\n        BUILD_ARRAY 0\\n        LOAD_CONST 0\\n        LOAD_CLOSURE 0\\n        BUILD_BLOCK 1\\n        SEND_BLOCK 1 1\\n\\n        RETURN\\n        ')\n    assert (bc.freevars == [])\n    assert (bc.cellvars == ['sums'])\n    self.assert_compiled(bc.consts_w[0], '\\n        BUILD_ARRAY 0\\n        LOAD_CONST 0\\n        LOAD_CLOSURE 0\\n        LOAD_CLOSURE 1\\n        BUILD_BLOCK 2\\n        SEND_BLOCK 1 1\\n        RETURN\\n        ')\n    assert (bc.consts_w[0].freevars == ['sums'])\n    assert (bc.consts_w[0].cellvars == ['x'])\n    self.assert_compiled(bc.consts_w[0].consts_w[0], '\\n        LOAD_DEREF 1\\n        LOAD_DEREF 2\\n        LOAD_DEREF 0\\n        SEND 0 1\\n        SEND 1 1\\n        RETURN\\n        ')\n    assert (bc.consts_w[0].consts_w[0].freevars == ['sums', 'x'])\n    assert (bc.consts_w[0].consts_w[0].cellvars == ['y'])\n", "label": 0}
{"function": "\n\n@classmethod\ndef validate(cls, data):\n    try:\n        su.schema_validate(data, cls.SCHEMA)\n    except su.ValidationError as e:\n        cls_name = reflection.get_class_name(cls, fully_qualified=False)\n        excp.raise_with_cause(excp.InvalidFormat, ('%s message response data not of the expected format: %s' % (cls_name, e.message)), cause=e)\n    else:\n        failures = []\n        if ('failures' in data):\n            failures.extend(six.itervalues(data['failures']))\n        result = data.get('result')\n        if (result is not None):\n            (result_data_type, result_data) = result\n            if (result_data_type == 'failure'):\n                failures.append(result_data)\n        for fail_data in failures:\n            ft.Failure.validate(fail_data)\n", "label": 0}
{"function": "\n\ndef isDBPort(host, port, db, timeout=10):\n    if (host == JSONFILE_HOSTNAME):\n        return True\n    t = 2\n    while (t < timeout):\n        try:\n            conn = urllib.request.urlopen('http://{0}:{1}/{2}/status'.format(host, (port or '80'), db))\n            return True\n        except HTTPError:\n            return False\n        except URLError:\n            return False\n        except socket.timeout:\n            t = (t + 2)\n    return False\n", "label": 0}
{"function": "\n\ndef test_mysql_database_url_with_sslca_options(self):\n    os.environ['DATABASE_URL'] = 'mysql://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:3306/d8r82722r2kuvn?ssl-ca=rds-combined-ca-bundle.pem'\n    url = dj_database_url.config()\n    assert (url['ENGINE'] == 'django.db.backends.mysql')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 3306)\n    assert (url['OPTIONS'] == {\n        'ssl': {\n            'ca': 'rds-combined-ca-bundle.pem',\n        },\n    })\n    os.environ['DATABASE_URL'] = 'mysql://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:3306/d8r82722r2kuvn?'\n    url = dj_database_url.config()\n    assert ('OPTIONS' not in url)\n", "label": 0}
{"function": "\n\ndef test_flatline_query_key():\n    rules = {\n        'timeframe': datetime.timedelta(seconds=30),\n        'threshold': 1,\n        'use_query_key': True,\n        'query_key': 'qk',\n        'timestamp_field': '@timestamp',\n    }\n    rule = FlatlineRule(rules)\n    rule.add_data(hits(1, qk='key1'))\n    rule.add_data(hits(1, qk='key2'))\n    rule.add_data(hits(1, qk='key3'))\n    assert (rule.matches == [])\n    rule.garbage_collect(ts_to_dt('2014-09-26T12:00:11Z'))\n    assert (rule.matches == [])\n    rule.add_data([create_event(ts_to_dt('2014-09-26T12:00:20Z'), qk='key3')])\n    timestamp = '2014-09-26T12:00:45Z'\n    rule.garbage_collect(ts_to_dt(timestamp))\n    assert (len(rule.matches) == 2)\n    assert (set(['key1', 'key2']) == set([m['key'] for m in rule.matches if (m['@timestamp'] == timestamp)]))\n    timestamp = '2014-09-26T12:01:20Z'\n    rule.garbage_collect(ts_to_dt(timestamp))\n    assert (len(rule.matches) == 3)\n    assert (set(['key3']) == set([m['key'] for m in rule.matches if (m['@timestamp'] == timestamp)]))\n", "label": 0}
{"function": "\n\ndef parse_data(infile):\n    'Parse data from `infile`.'\n    blocks = re.compile(' '.join(([('=' * 9)] * 8)))\n    dashes = re.compile('^-{79}$')\n    title = re.compile('^Timings for (.*)$')\n    row = re.compile((' '.join((['(.{9})'] * 7)) + ' (.{8,9})'))\n    lines = infile.readlines()\n    data = co.OrderedDict()\n    index = 0\n    while (index < len(lines)):\n        line = lines[index]\n        if blocks.match(line):\n            try:\n                name = title.match(lines[(index + 1)]).group(1)\n            except:\n                index += 1\n                continue\n            data[name] = {\n                \n            }\n            assert dashes.match(lines[(index + 2)])\n            cols = parse_row(row, lines[(index + 3)])\n            assert blocks.match(lines[(index + 4)])\n            get_row = parse_row(row, lines[(index + 5)])\n            assert (get_row[0] == 'get')\n            set_row = parse_row(row, lines[(index + 6)])\n            assert (set_row[0] == 'set')\n            delete_row = parse_row(row, lines[(index + 7)])\n            assert (delete_row[0] == 'delete')\n            assert blocks.match(lines[(index + 9)])\n            data[name]['get'] = dict(zip(cols, get_row))\n            data[name]['set'] = dict(zip(cols, set_row))\n            data[name]['delete'] = dict(zip(cols, delete_row))\n            index += 10\n        else:\n            index += 1\n    return data\n", "label": 0}
{"function": "\n\ndef run(self, _tokens=1, **kwargs):\n    restart_limit = self.restart_limit\n    errors = (self.connection.connection_errors + self.connection.channel_errors)\n    while (not self.should_stop):\n        try:\n            if restart_limit.can_consume(_tokens):\n                for _ in self.consume(limit=None, **kwargs):\n                    pass\n            else:\n                sleep(restart_limit.expected_time(_tokens))\n        except errors:\n            warn(W_CONN_LOST, exc_info=1)\n", "label": 0}
{"function": "\n\ndef _find_exe_in_registry(self):\n    try:\n        from _winreg import OpenKey, QueryValue, HKEY_LOCAL_MACHINE, HKEY_CURRENT_USER\n    except ImportError:\n        from winreg import OpenKey, QueryValue, HKEY_LOCAL_MACHINE, HKEY_CURRENT_USER\n    import shlex\n    keys = ('SOFTWARE\\\\Classes\\\\FirefoxHTML\\\\shell\\\\open\\\\command', 'SOFTWARE\\\\Classes\\\\Applications\\\\firefox.exe\\\\shell\\\\open\\\\command')\n    command = ''\n    for path in keys:\n        try:\n            key = OpenKey(HKEY_LOCAL_MACHINE, path)\n            command = QueryValue(key, '')\n            break\n        except OSError:\n            try:\n                key = OpenKey(HKEY_CURRENT_USER, path)\n                command = QueryValue(key, '')\n                break\n            except OSError:\n                pass\n    else:\n        return ''\n    if (not command):\n        return ''\n    return shlex.split(command)[0]\n", "label": 0}
{"function": "\n\ndef update_database_info(self):\n    ' Queries the AppController for information about what datastore is used\\n    to implement support for the Google App Engine Datastore API, placing this\\n    info in the Datastore for later viewing.\\n\\n    This update is only performed if there is no data in the Datastore about the\\n    current location of the head node, as this is unlikely to dynamically change\\n    at this time.\\n\\n    Returns:\\n      A dict containing the name of the database used (a str), as well as the\\n      number of replicas for each piece of data (an int).\\n    '\n    dashboard_root = self.get_by_id(DashboardDataRoot, self.ROOT_KEYNAME)\n    if (dashboard_root and (dashboard_root.table is not None) and (dashboard_root.replication is not None)):\n        return {\n            'table': dashboard_root.table,\n            'replication': dashboard_root.replication,\n        }\n    try:\n        acc = self.helper.get_appcontroller_client()\n        db_info = acc.get_database_information()\n        if (dashboard_root is None):\n            dashboard_root = DashboardDataRoot(id=self.ROOT_KEYNAME)\n        dashboard_root.table = db_info['table']\n        dashboard_root.replication = int(db_info['replication'])\n        dashboard_root.put()\n        return {\n            'table': dashboard_root.table,\n            'replication': dashboard_root.replication,\n        }\n    except Exception as err:\n        logging.exception(err)\n        return {\n            'table': 'unknown',\n            'replication': 0,\n        }\n", "label": 0}
{"function": "\n\ndef parse(self, parse_until=None):\n    \"\\n        Iterate through the parser tokens and compiles each one into a node.\\n\\n        If parse_until is provided, parsing will stop once one of the\\n        specified tokens has been reached. This is formatted as a list of\\n        tokens, e.g. ['elif', 'else', 'endif']. If no matching token is\\n        reached, raise an exception with the unclosed block tag details.\\n        \"\n    if (parse_until is None):\n        parse_until = []\n    nodelist = NodeList()\n    while self.tokens:\n        token = self.next_token()\n        if (token.token_type == 0):\n            self.extend_nodelist(nodelist, TextNode(token.contents), token)\n        elif (token.token_type == 1):\n            if (not token.contents):\n                raise self.error(token, ('Empty variable tag on line %d' % token.lineno))\n            try:\n                filter_expression = self.compile_filter(token.contents)\n            except TemplateSyntaxError as e:\n                raise self.error(token, e)\n            var_node = VariableNode(filter_expression)\n            self.extend_nodelist(nodelist, var_node, token)\n        elif (token.token_type == 2):\n            try:\n                command = token.contents.split()[0]\n            except IndexError:\n                raise self.error(token, ('Empty block tag on line %d' % token.lineno))\n            if (command in parse_until):\n                self.prepend_token(token)\n                return nodelist\n            self.command_stack.append((command, token))\n            try:\n                compile_func = self.tags[command]\n            except KeyError:\n                self.invalid_block_tag(token, command, parse_until)\n            try:\n                compiled_result = compile_func(self, token)\n            except Exception as e:\n                raise self.error(token, e)\n            self.extend_nodelist(nodelist, compiled_result, token)\n            self.command_stack.pop()\n    if parse_until:\n        self.unclosed_block_tag(parse_until)\n    return nodelist\n", "label": 1}
{"function": "\n\ndef run(self):\n    if (not have_bonjour):\n        logging.warn('Cannot run bonjour server!  Maybe some packages need to be installed?')\n        return\n    sdRef = pybonjour.DNSServiceRegister(name=self.name, regtype=self.regtype, port=self.port, callBack=self.register_callback)\n    try:\n        try:\n            while True:\n                ready = select.select([sdRef], [], [])\n                if (sdRef in ready[0]):\n                    pybonjour.DNSServiceProcessResult(sdRef)\n        except KeyboardInterrupt:\n            pass\n    finally:\n        sdRef.close()\n", "label": 0}
{"function": "\n\ndef test_notch_filters():\n    'Test notch filters\\n    '\n    sfreq = 487.0\n    sig_len_secs = 20\n    t = (np.arange(0, int((sig_len_secs * sfreq))) / sfreq)\n    freqs = np.arange(60, 241, 60)\n    a = rng.randn(int((sig_len_secs * sfreq)))\n    orig_power = np.sqrt(np.mean((a ** 2)))\n    a += np.sum([np.sin((((2 * np.pi) * f) * t)) for f in freqs], axis=0)\n    assert_raises(ValueError, notch_filter, a, sfreq, None, 'fft')\n    assert_raises(ValueError, notch_filter, a, sfreq, None, 'iir')\n    methods = ['spectrum_fit', 'spectrum_fit', 'fft', 'fft', 'iir']\n    filter_lengths = [None, None, None, 8192, None]\n    line_freqs = [None, freqs, freqs, freqs, freqs]\n    tols = [2, 1, 1, 1]\n    for (meth, lf, fl, tol) in zip(methods, line_freqs, filter_lengths, tols):\n        with catch_logging() as log_file:\n            b = notch_filter(a, sfreq, lf, filter_length=fl, method=meth, verbose='INFO')\n        if (lf is None):\n            out = log_file.getvalue().split('\\n')[:(- 1)]\n            if ((len(out) != 2) and (len(out) != 3)):\n                raise ValueError('Detected frequencies not logged properly')\n            out = np.fromstring(out[(- 1)], sep=', ')\n            assert_array_almost_equal(out, freqs)\n        new_power = np.sqrt((sum_squared(b) / b.size))\n        assert_almost_equal(new_power, orig_power, tol)\n", "label": 0}
{"function": "\n\ndef __init__(self, op_tree, ad):\n    '\\n        op_tree: the op_tree at this grad_node\\n        ad: the autodiff object with global op_tree, next_error and dicts\\n        '\n    assert (op_tree is not None)\n    self.op_tree = op_tree\n    self.grad_op_tree = None\n    self.ad = ad\n    self.left = None\n    self.right = None\n    if isinstance(op_tree, Tensor):\n        if (op_tree._original_base not in ad.map_tensor_grad_node):\n            ad.map_tensor_grad_node[op_tree._original_base] = self\n    elif (type(op_tree) == OpTreeNode):\n        if (op_tree[1] is not None):\n            if (isinstance(op_tree[1], Tensor) and (op_tree[1]._original_base in ad.map_tensor_grad_node)):\n                self.left = ad.map_tensor_grad_node[op_tree[1]._original_base]\n            else:\n                self.left = GradNode(op_tree[1], ad)\n        if (op_tree[2] is not None):\n            if (isinstance(op_tree[2], Tensor) and (op_tree[2]._original_base in ad.map_tensor_grad_node)):\n                self.right = ad.map_tensor_grad_node[op_tree[2]._original_base]\n            else:\n                self.right = GradNode(op_tree[2], ad)\n", "label": 1}
{"function": "\n\ndef request_new_domain(request, form, is_new_user=True):\n    now = datetime.utcnow()\n    current_user = CouchUser.from_django_user(request.user)\n    dom_req = RegistrationRequest()\n    if is_new_user:\n        dom_req.request_time = now\n        dom_req.request_ip = get_ip(request)\n        dom_req.activation_guid = uuid.uuid1().hex\n    name = name_to_url(form.cleaned_data['hr_name'], 'project')\n    with CriticalSection(['request_domain_name_{}'.format(name)]):\n        name = Domain.generate_name(name)\n        new_domain = Domain(name=name, hr_name=form.cleaned_data['hr_name'], is_active=False, date_created=datetime.utcnow(), creating_user=current_user.username, secure_submissions=True)\n        if form.cleaned_data.get('domain_timezone'):\n            new_domain.default_timezone = form.cleaned_data['domain_timezone']\n        if (not is_new_user):\n            new_domain.is_active = True\n        new_domain.save(**get_safe_write_kwargs())\n    if (not new_domain.name):\n        new_domain.name = new_domain._id\n        new_domain.save()\n    if is_new_user:\n        create_30_day_advanced_trial(new_domain)\n    UserRole.init_domain_with_presets(new_domain.name)\n    dom_req.domain = new_domain.name\n    if request.user.is_authenticated():\n        if (not current_user):\n            current_user = WebUser()\n            current_user.sync_from_django_user(request.user)\n            current_user.save()\n        current_user.add_domain_membership(new_domain.name, is_admin=True)\n        current_user.save()\n        dom_req.requesting_user_username = request.user.username\n        dom_req.new_user_username = request.user.username\n    if is_new_user:\n        dom_req.save()\n        send_domain_registration_email(request.user.email, dom_req.domain, dom_req.activation_guid, request.user.get_full_name())\n    else:\n        send_global_domain_registration_email(request.user, new_domain.name)\n    send_new_request_update_email(request.user, get_ip(request), new_domain.name, is_new_user=is_new_user)\n    meta = get_meta(request)\n    track_created_new_project_space_on_hubspot.delay(current_user, request.COOKIES, meta)\n    return new_domain.name\n", "label": 0}
{"function": "\n\n@task()\ndef update_podcast(self, podcast):\n    atom = self.atom\n\n    def gct(node, names):\n        for name in names:\n            if (node.find(name) is None):\n                continue\n            value = node.find(name).text\n            if (name == 'pubDate'):\n                value = datetime.fromtimestamp(email.utils.mktime_tz(email.utils.parsedate_tz(value)))\n            elif (name == atom('published')):\n                value = dateutil.parser.parse(value)\n            elif (name == '{itunes:}duration'):\n                value = int(value)\n            return value\n        return None\n    xml = etree.parse(urllib.urlopen(podcast.rss_url)).getroot()\n    try:\n        podcast.title = xml.find('.//channel/title').text\n        podcast.description = xml.find('.//channel/description').text\n    except AttributeError:\n        podcast.title = xml.find(atom('title')).text\n        podcast.description = xml.find(atom('subtitle')).text\n    podcast.license = self.determine_license(xml.find('.//channel'))\n    if (self.medium is not None):\n        podcast.medium = self.medium\n    logo = xml.find('.//channel/image/url')\n    podcast.logo = (logo.text if (logo is not None) else None)\n    ids = []\n    for item in (xml.findall('.//channel/item') or xml.findall(atom('entry'))):\n        id = gct(item, ('guid', atom('id')))\n        if (not id):\n            continue\n        try:\n            (podcast_item, created) = PodcastItem.objects.get_or_create(podcast=podcast, guid=id)\n        except PodcastItem.MultipleObjectsReturned:\n            PodcastItem.objects.filter(podcast=podcast, guid=id).delete()\n            (podcast_item, created) = PodcastItem.objects.get_or_create(podcast=podcast, guid=id)\n        old_order = podcast_item.order\n        try:\n            podcast_item.order = int(item.find('{http://ns.ox.ac.uk/namespaces/oxitems/TopDownloads}position').text)\n        except (AttributeError, TypeError):\n            pass\n        require_save = (old_order != podcast_item.order)\n        for (attr, x_attrs) in self.PODCAST_ATTRS:\n            if (getattr(podcast_item, attr) != gct(item, x_attrs)):\n                setattr(podcast_item, attr, gct(item, x_attrs))\n                require_save = True\n        license = self.determine_license(item)\n        if (require_save or (podcast_item.license != license)):\n            podcast_item.license = license\n            podcast_item.save()\n        enc_urls = []\n        for enc in (item.findall('enclosure') or item.findall(atom('link'))):\n            attrib = enc.attrib\n            url = attrib.get('url', attrib.get('href'))\n            (podcast_enc, updated) = PodcastEnclosure.objects.get_or_create(podcast_item=podcast_item, url=url)\n            try:\n                podcast_enc.length = int(attrib['length'])\n            except ValueError:\n                podcast_enc.length = None\n            podcast_enc.mimetype = attrib['type']\n            podcast_enc.save()\n            enc_urls.append(url)\n        encs = PodcastEnclosure.objects.filter(podcast_item=podcast_item)\n        for enc in encs:\n            if (not (enc.url in enc_urls)):\n                enc.delete()\n        ids.append(id)\n    for podcast_item in PodcastItem.objects.filter(podcast=podcast):\n        if (not (podcast_item.guid in ids)):\n            podcast_item.podcastenclosure_set.all().delete()\n            podcast_item.delete()\n    podcast.save()\n", "label": 1}
{"function": "\n\ndef test_04_ore_init_with_statement(self):\n    s = Ore_Sword_Statement(ORE_TEST_STATEMENT)\n    assert (len(s.states) == 1)\n    assert (len(s.original_deposits) == 1)\n    assert (len(s.resources) == 1)\n    assert (s.xml_document != None)\n    assert (s.dom != None)\n    assert s.parsed\n    assert s.valid\n    (uri, description) = s.states[0]\n    assert (uri == 'http://purl.org/net/sword/terms/state/Testing')\n    assert (description == 'The work has passed through review and is now in the archive')\n    t = datetime.strptime('2011-03-02T20:50:06Z', '%Y-%m-%dT%H:%M:%SZ')\n    entry = s.resources[0]\n    assert (len(entry.packaging) == 1)\n    assert (entry.deposited_by == 'sword')\n    assert (entry.deposited_on_behalf_of == 'jbloggs')\n    assert (entry.deposited_on == t)\n    assert (entry.uri == 'http://localhost:8080/part-IRI/43/my_deposit/example.zip')\n    assert (entry.packaging[0] == 'http://purl.org/net/sword/package/SimpleZip')\n", "label": 1}
{"function": "\n\ndef make_node(self, *inputs):\n    assert (self.nout == 1)\n    assert (len(inputs) == 2)\n    _inputs = [gpu_contiguous(as_cuda_ndarray_variable(i)) for i in inputs]\n    if ((self.nin > 0) and (len(_inputs) != self.nin)):\n        raise TypeError('Wrong argument count', (self.nin, len(_inputs)))\n    for i in _inputs[1:]:\n        if (i.type.ndim != inputs[0].type.ndim):\n            raise TypeError('different ranks among inputs')\n    if any([any(i.type.broadcastable) for i in inputs]):\n        raise Exception(\"pycuda don't support broadcasted dimensions\")\n    otype = CudaNdarrayType(broadcastable=([False] * _inputs[0].type.ndim))\n    out_node = Apply(self, _inputs, [otype() for o in xrange(self.nout)])\n    return out_node\n", "label": 1}
{"function": "\n\ndef fuzzy_load(name, merge_inherited=True):\n    localedata._cache_lock.acquire()\n    try:\n        data = localedata._cache.get(name)\n        if (not data):\n            if ((name == 'root') or (not merge_inherited)):\n                data = {\n                    \n                }\n            else:\n                parts = name.split('_')\n                if (len(parts) == 1):\n                    parent = 'root'\n                else:\n                    parent = '_'.join(parts[:(- 1)])\n                data = fuzzy_load(parent).copy()\n            filename = os.path.join(localedata._dirname, ('%s.dat' % name))\n            try:\n                fileobj = open(filename, 'rb')\n                try:\n                    if ((name != 'root') and merge_inherited):\n                        localedata.merge(data, pickle.load(fileobj))\n                    else:\n                        data = pickle.load(fileobj)\n                    localedata._cache[name] = data\n                finally:\n                    fileobj.close()\n            except IOError:\n                pass\n        return data\n    finally:\n        localedata._cache_lock.release()\n", "label": 0}
{"function": "\n\ndef test_integer_int(self):\n    if _debug:\n        TestInteger._debug('test_integer_int')\n    obj = Integer(1)\n    assert (obj.value == 1)\n    assert (str(obj) == 'Integer(1)')\n    obj = Integer((- 1))\n    assert (obj.value == (- 1))\n    assert (str(obj) == 'Integer(-1)')\n", "label": 0}
{"function": "\n\ndef test_result_generation_when_one_test_has_two_cases():\n    jobstep = JobStep(id=uuid.uuid4(), project_id=uuid.uuid4(), job_id=uuid.uuid4())\n    fp = StringIO(SAMPLE_XUNIT_DOUBLE_CASES)\n    handler = XunitHandler(jobstep)\n    results = handler.get_tests(fp)\n    assert (len(results) == 2)\n    r1 = results[0]\n    assert (type(r1) is TestResult)\n    assert (r1.step == jobstep)\n    assert (r1.package is None)\n    assert (r1.name == 'test_simple.SampleTest.test_falsehood')\n    assert (r1.duration == 750.0)\n    assert (r1.result == Result.failed)\n    assert (r1.message == 'test_simple.py:8: in test_falsehood\\n    assert False\\nE   AssertionError: assert False\\n\\ntest_simple.py:4: in tearDown\\n    1/0\\nE   ZeroDivisionError: integer division or modulo by zero')\n    assert (r1.reruns == 3)\n    r2 = results[1]\n    assert (type(r2) is TestResult)\n    assert (r2.step == jobstep)\n    assert (r2.package is None)\n    assert (r2.name == 'test_simple.SampleTest.test_truth')\n    assert (r2.duration == 1250.0)\n    assert (r2.result == Result.failed)\n    assert (r2.message == 'test_simple.py:4: in tearDown\\n    1/0\\nE   ZeroDivisionError: integer division or modulo by zero')\n    assert (r2.reruns == 0)\n", "label": 1}
{"function": "\n\ndef OnAuthTumblr(self):\n    self.User = self.le_mail.text().trimmed()\n    self.Password = self.le_password.text()\n    self.Blog = self.le_url.text().trimmed()\n    self.error = None\n    if (not ((self.User.isEmpty() | self.Password.isEmpty()) | self.Blog.isEmpty())):\n        self.api = Api(self.Blog, self.User, self.Password)\n        try:\n            self.auth = self.api.auth_check()\n            if QtGui.QSystemTrayIcon.isSystemTrayAvailable():\n                self.hide()\n                tray = TumblrTray(self)\n            else:\n                dashboard = Dashboard(self)\n                self.hide()\n                dashboard.show()\n            if (self.rememberme.checkState() == 2):\n                file = open((QtCore.QDir().homePath() + '/.opentumblr'), 'w')\n                file.write(self.le_mail.text())\n                file.write(self.le_url.text())\n        except TumblrAuthError:\n            self.error = errors['403']\n        except urllib2.HTTPError:\n            self.error = errors['404']\n        except urllib2.URLError:\n            self.error = errors['urlopen']\n        finally:\n            if (self.error != None):\n                QtGui.QMessageBox.warning(self, 'Error', ('Occurrio un error: \\n' + self.error), QtGui.QMessageBox.Ok)\n    else:\n        QtGui.QMessageBox.warning(self, 'Error', 'Todos los Campos son necesarios', QtGui.QMessageBox.Ok)\n", "label": 0}
{"function": "\n\ndef test_quartiles(self):\n    '\\n        CDF quartile tests from:\\n        http://www.amstat.org/publications/jse/v14n3/langford.html#Parzen1979\\n        '\n    warnings.simplefilter('error')\n    with self.assertRaises(NullCalculationWarning):\n        Quartiles('one').validate(self.table)\n    with self.assertRaises(DataTypeError):\n        Quartiles('three').validate(self.table)\n    Quartiles('two').validate(self.table)\n    rows = [(n,) for n in [1, 2, 3, 4]]\n    table = Table(rows, ['ints'], [self.number_type])\n    quartiles = Quartiles('ints').run(table)\n    for (i, v) in enumerate(['1', '1.5', '2.5', '3.5', '4']):\n        self.assertEqual(quartiles[i], Decimal(v))\n    rows = [(n,) for n in [1, 2, 3, 4, 5]]\n    table = Table(rows, ['ints'], [self.number_type])\n    quartiles = Quartiles('ints').run(table)\n    for (i, v) in enumerate(['1', '2', '3', '4', '5']):\n        self.assertEqual(quartiles[i], Decimal(v))\n    rows = [(n,) for n in [1, 2, 3, 4, 5, 6]]\n    table = Table(rows, ['ints'], [self.number_type])\n    quartiles = Quartiles('ints').run(table)\n    for (i, v) in enumerate(['1', '2', '3.5', '5', '6']):\n        self.assertEqual(quartiles[i], Decimal(v))\n    rows = [(n,) for n in [1, 2, 3, 4, 5, 6, 7]]\n    table = Table(rows, ['ints'], [self.number_type])\n    quartiles = Quartiles('ints').run(table)\n    for (i, v) in enumerate(['1', '2', '4', '6', '7']):\n        self.assertEqual(quartiles[i], Decimal(v))\n    rows = [(n,) for n in [1, 1, 2, 2, 3, 3, 4, 4]]\n    table = Table(rows, ['ints'], [self.number_type])\n    quartiles = Quartiles('ints').run(table)\n    for (i, v) in enumerate(['1', '1.5', '2.5', '3.5', '4']):\n        self.assertEqual(quartiles[i], Decimal(v))\n    rows = [(n,) for n in [1, 1, 2, 2, 3, 3, 4, 4, 5, 5]]\n    table = Table(rows, ['ints'], [self.number_type])\n    quartiles = Quartiles('ints').run(table)\n    for (i, v) in enumerate(['1', '2', '3', '4', '5']):\n        self.assertEqual(quartiles[i], Decimal(v))\n    rows = [(n,) for n in [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]]\n    table = Table(rows, ['ints'], [self.number_type])\n    quartiles = Quartiles('ints').run(table)\n    for (i, v) in enumerate(['1', '2', '3.5', '5', '6']):\n        self.assertEqual(quartiles[i], Decimal(v))\n    rows = [(n,) for n in [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7]]\n    table = Table(rows, ['ints'], [self.number_type])\n    quartiles = Quartiles('ints').run(table)\n    for (i, v) in enumerate(['1', '2', '4', '6', '7']):\n        self.assertEqual(quartiles[i], Decimal(v))\n", "label": 1}
{"function": "\n\ndef optimizeJumps(irdata):\n    instrs = irdata.flat_instructions\n    jump_instrs = [ins for ins in instrs if isinstance(ins, ir.LazyJumpBase)]\n    while 1:\n        done = True\n        (posd, _) = _calcMinimumPositions(instrs)\n        for ins in jump_instrs:\n            if ((ins.min < ins.max) and ins.widenIfNecessary(irdata.labels, posd)):\n                done = False\n        if done:\n            break\n    for ins in jump_instrs:\n        assert (ins.min <= ins.max)\n        ins.max = ins.min\n", "label": 0}
{"function": "\n\ndef handle(self):\n    message_format = '%s     %3d%%     [ %d / %d ]'\n    last_status_message_len = 0\n    status_message = ''\n    message_sent = False\n    self.server.binwalk.status.running = True\n    while True:\n        time.sleep(0.1)\n        try:\n            self.request.send(binwalk.core.compat.str2bytes(('\\x08' * last_status_message_len)))\n            self.request.send(binwalk.core.compat.str2bytes((' ' * last_status_message_len)))\n            self.request.send(binwalk.core.compat.str2bytes(('\\x08' * last_status_message_len)))\n            if self.server.binwalk.status.shutdown:\n                self.server.binwalk.status.finished = True\n                break\n            if (self.server.binwalk.status.total != 0):\n                percentage = ((float(self.server.binwalk.status.completed) / float(self.server.binwalk.status.total)) * 100)\n                status_message = (message_format % (self.server.binwalk.status.fp.path, percentage, self.server.binwalk.status.completed, self.server.binwalk.status.total))\n            elif (not message_sent):\n                status_message = 'No status information available at this time!'\n            else:\n                continue\n            last_status_message_len = len(status_message)\n            self.request.send(binwalk.core.compat.str2bytes(status_message))\n            message_sent = True\n        except IOError as e:\n            if (e.errno == errno.EPIPE):\n                break\n        except Exception as e:\n            binwalk.core.common.debug((('StatusRequestHandler exception: ' + str(e)) + '\\n'))\n        except KeyboardInterrupt as e:\n            raise e\n    self.server.binwalk.status.running = False\n    return\n", "label": 0}
{"function": "\n\ndef reproject(source, destination, src_transform=None, src_crs=None, src_nodata=None, dst_transform=None, dst_crs=None, dst_nodata=None, resampling=Resampling.nearest, **kwargs):\n    \"\\n    Reproject a source raster to a destination raster.\\n\\n    If the source and destination are ndarrays, coordinate reference\\n    system definitions and affine transformation parameters are required\\n    for reprojection.\\n\\n    If the source and destination are rasterio Bands, shorthand for\\n    bands of datasets on disk, the coordinate reference systems and\\n    transforms will be read from the appropriate datasets.\\n\\n    Parameters\\n    ------------\\n    source: ndarray or rasterio Band\\n        Source raster.\\n    destination: ndarray or rasterio Band\\n        Target raster.\\n    src_transform: affine transform object, optional\\n        Source affine transformation.  Required if source and destination\\n        are ndarrays.  Will be derived from source if it is a rasterio Band.\\n    src_crs: dict, optional\\n        Source coordinate reference system, in rasterio dict format.\\n        Required if source and destination are ndarrays.\\n        Will be derived from source if it is a rasterio Band.\\n        Example: {'init': 'EPSG:4326'}\\n    src_nodata: int or float, optional\\n        The source nodata value.  Pixels with this value will not be used\\n        for interpolation.  If not set, it will be default to the\\n        nodata value of the source image if a masked ndarray or rasterio band,\\n        if available.  Must be provided if dst_nodata is not None.\\n    dst_transform: affine transform object, optional\\n        Target affine transformation.  Required if source and destination\\n        are ndarrays.  Will be derived from target if it is a rasterio Band.\\n    dst_crs: dict, optional\\n        Target coordinate reference system.  Required if source and destination\\n        are ndarrays.  Will be derived from target if it is a rasterio Band.\\n    dst_nodata: int or float, optional\\n        The nodata value used to initialize the destination; it will remain\\n        in all areas not covered by the reprojected source.  Defaults to the\\n        nodata value of the destination image (if set), the value of\\n        src_nodata, or 0 (GDAL default).\\n    resampling: int\\n        Resampling method to use.  One of the following:\\n            Resampling.nearest,\\n            Resampling.bilinear,\\n            Resampling.cubic,\\n            Resampling.cubic_spline,\\n            Resampling.lanczos,\\n            Resampling.average,\\n            Resampling.mode\\n    kwargs:  dict, optional\\n        Additional arguments passed to transformation function.\\n\\n    Returns\\n    ---------\\n    out: None\\n        Output is written to destination.\\n    \"\n    try:\n        Resampling(resampling)\n        if (resampling == 7):\n            raise ValueError\n    except ValueError:\n        raise ValueError('resampling must be one of: {0}'.format(', '.join(['Resampling.{0}'.format(k) for k in Resampling.__members__.keys() if (k != 'gauss')])))\n    if src_transform:\n        src_transform = guard_transform(src_transform).to_gdal()\n    if dst_transform:\n        dst_transform = guard_transform(dst_transform).to_gdal()\n    rasterio.env.setenv()\n    _reproject(source, destination, src_transform, src_crs, src_nodata, dst_transform, dst_crs, dst_nodata, resampling, **kwargs)\n", "label": 0}
{"function": "\n\n@classmethod\ndef load_dotenv(cls):\n    '\\n        Pulled from Honcho code with minor updates, reads local default\\n        environment variables from a .env file located in the project root\\n        or provided directory.\\n\\n        http://www.wellfireinteractive.com/blog/easier-12-factor-django/\\n        https://gist.github.com/bennylope/2999704\\n        '\n    dotenv = getattr(cls, 'DOTENV', None)\n    if (not dotenv):\n        return\n    try:\n        with open(dotenv, 'r') as f:\n            content = f.read()\n    except IOError as e:\n        raise ImproperlyConfigured(\"Couldn't read .env file with the path {}. Error: {}\".format(dotenv, e))\n    else:\n        for line in content.splitlines():\n            m1 = re.match('\\\\A([A-Za-z_0-9]+)=(.*)\\\\Z', line)\n            if (not m1):\n                continue\n            (key, val) = (m1.group(1), m1.group(2))\n            m2 = re.match(\"\\\\A'(.*)'\\\\Z\", val)\n            if m2:\n                val = m2.group(1)\n            m3 = re.match('\\\\A\"(.*)\"\\\\Z', val)\n            if m3:\n                val = re.sub('\\\\\\\\(.)', '\\\\1', m3.group(1))\n            os.environ.setdefault(key, val)\n        cls.DOTENV_LOADED = dotenv\n", "label": 0}
{"function": "\n\ndef clean(self):\n    username = self.cleaned_data.get('username')\n    password = self.cleaned_data.get('password')\n    message = ERROR_MESSAGE\n    if (username and password):\n        self.user_cache = authenticate(username=username, password=password)\n        if (self.user_cache is None):\n            if ('@' in username):\n                try:\n                    user = User.objects.get(email=username)\n                except (User.DoesNotExist, User.MultipleObjectsReturned):\n                    pass\n                else:\n                    if user.check_password(password):\n                        message = (_(\"Your e-mail address is not your username. Try '%s' instead.\") % user.username)\n            raise forms.ValidationError(message)\n        elif ((not self.user_cache.is_active) or (not self.user_cache.is_staff)):\n            raise forms.ValidationError(message)\n    self.check_for_test_cookie()\n    return self.cleaned_data\n", "label": 0}
{"function": "\n\n@never_cache\n@login_required\ndef delete(request, app_label, model_name, instance_id, delete_form=DeleteRequestForm):\n    instance_return = _get_instance(request, 'delete', app_label, model_name, instance_id)\n    if isinstance(instance_return, HttpResponseForbidden):\n        return instance_return\n    (model, instance_form, instance) = instance_return\n    cancel = _handle_cancel(request)\n    if cancel:\n        return cancel\n    if (request.method == 'POST'):\n        form = delete_form(request.POST)\n        if form.is_valid():\n            instance.delete()\n            msg = (_('Your %(model_name)s was deleted.') % {\n                'model_name': model._meta.verbose_name,\n            })\n            try:\n                request.user.message_set.create(message=msg)\n            except AttributeError:\n                messages.success(request, msg)\n            if request.is_ajax():\n                return success_delete(request)\n            return _handle_response(request, instance)\n    else:\n        form = delete_form()\n    template_context = {\n        'action': 'delete',\n        'action_url': request.get_full_path(),\n        'model_title': model._meta.verbose_name,\n        'form': form,\n    }\n    return render_to_response(_get_template(request, None, None), template_context, RequestContext(request))\n", "label": 0}
{"function": "\n\ndef ensure_ssh_key_added(key_files):\n    need_adding = set((os.path.abspath(os.path.expanduser(p)) for p in key_files))\n    with settings(hide('warnings', 'running', 'stdout', 'stderr'), warn_only=True):\n        res = local('ssh-add -l', capture=True)\n        if res.succeeded:\n            for line in res.splitlines():\n                m = SSH_KEY_LIST_RE.match(line)\n                if (not m):\n                    continue\n                path = os.path.abspath(os.path.expanduser(m.group('key_file')))\n                need_adding.discard(path)\n    with settings(hide('warnings', 'running', 'stdout', 'stderr')):\n        if need_adding:\n            key_string = ' '.join(need_adding)\n            start_ssh_agent = ('eval `ssh-agent` && echo $SSH_AUTH_SOCK && ssh-add %s' % key_string)\n            info_agent = local(start_ssh_agent, capture=True).splitlines()\n            os.environ['SSH_AGENT_PID'] = info_agent[0].split()[(- 1)]\n            os.environ['SSH_AUTH_SOCK'] = info_agent[1]\n            return False\n        else:\n            return True\n", "label": 0}
{"function": "\n\ndef dialect_of(data, **kwargs):\n    ' CSV dialect of a CSV file stored in SSH, HDFS, or a Directory. '\n    keys = set(['delimiter', 'doublequote', 'escapechar', 'lineterminator', 'quotechar', 'quoting', 'skipinitialspace', 'strict', 'has_header'])\n    if isinstance(data, (HDFS(CSV), SSH(CSV))):\n        with sample(data) as fn:\n            d = dialect_of(CSV(fn, **data.dialect))\n    elif isinstance(data, (HDFS(Directory(CSV)), SSH(Directory(CSV)))):\n        with sample(data) as fn:\n            d = dialect_of(CSV(fn, **data.kwargs))\n    elif isinstance(data, Directory(CSV)):\n        d = dialect_of(next(data))\n    else:\n        assert isinstance(data, CSV)\n        with open(data.path, 'r') as f:\n            text = f.read()\n        result = dict()\n        d = sniffer.sniff(text)\n        d = dict(((k, getattr(d, k)) for k in keys if hasattr(d, k)))\n        if (data.has_header is None):\n            d['has_header'] = sniffer.has_header(text)\n        else:\n            d['has_header'] = data.has_header\n        d.update(data.dialect)\n    d.update(kwargs)\n    d = dict(((k, v) for (k, v) in d.items() if (k in keys)))\n    return d\n", "label": 0}
{"function": "\n\ndef test_posts_atom(client, silly_posts):\n    rv = client.get('/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('Probably a &lt;i&gt;dumb&lt;/i&gt; joke' in content)\n    assert ('First interesting article' in content)\n    rv = client.get('/everything/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('mal.colm/reynolds' in content)\n    assert ('First interesting article' in content)\n    rv = client.get('/notes/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('Probably a &lt;i&gt;dumb&lt;/i&gt; joke' in content)\n    assert ('First interesting article' not in content)\n", "label": 1}
{"function": "\n\ndef _on_calculate_verts(self):\n    self.u_interval = self.intervals[0]\n    self.u_set = list(self.u_interval.frange())\n    self.v_interval = self.intervals[1]\n    self.v_set = list(self.v_interval.frange())\n    self.bounds = [[S.Infinity, (- S.Infinity), 0], [S.Infinity, (- S.Infinity), 0], [S.Infinity, (- S.Infinity), 0]]\n    evaluate = self._get_evaluator()\n    self._calculating_verts_pos = 0.0\n    self._calculating_verts_len = float((self.u_interval.v_len * self.v_interval.v_len))\n    verts = list()\n    b = self.bounds\n    for u in self.u_set:\n        column = list()\n        for v in self.v_set:\n            try:\n                _e = evaluate(u, v)\n            except ZeroDivisionError:\n                _e = None\n            if (_e is not None):\n                for axis in range(3):\n                    b[axis][0] = min([b[axis][0], _e[axis]])\n                    b[axis][1] = max([b[axis][1], _e[axis]])\n            column.append(_e)\n            self._calculating_verts_pos += 1.0\n        verts.append(column)\n    for axis in range(3):\n        b[axis][2] = (b[axis][1] - b[axis][0])\n        if (b[axis][2] == 0.0):\n            b[axis][2] = 1.0\n    self.verts = verts\n    self.push_wireframe(self.draw_verts(False, False))\n    self.push_solid(self.draw_verts(False, True))\n", "label": 0}
{"function": "\n\ndef _read(obj):\n    'Try to read from a url, file or string.\\n\\n    Parameters\\n    ----------\\n    obj : str, unicode, or file-like\\n\\n    Returns\\n    -------\\n    raw_text : str\\n    '\n    if _is_url(obj):\n        with urlopen(obj) as url:\n            text = url.read()\n    elif hasattr(obj, 'read'):\n        text = obj.read()\n    elif isinstance(obj, char_types):\n        text = obj\n        try:\n            if os.path.isfile(text):\n                with open(text, 'rb') as f:\n                    return f.read()\n        except (TypeError, ValueError):\n            pass\n    else:\n        raise TypeError(('Cannot read object of type %r' % type(obj).__name__))\n    return text\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _ExtractMetadata(image, parse_metadata):\n    'Extract EXIF metadata from the image.\\n\\n    Note that this is a much simplified version of metadata extraction. After\\n    deployment applications have access to a more powerful parser that can\\n    parse hundreds of fields from images.\\n\\n    Args:\\n      image: PIL Image object.\\n      parse_metadata: bool, True if metadata parsing has been requested. If\\n        False the result will contain image dimensions.\\n    Returns:\\n      str, JSON encoded values with various metadata fields.\\n    '\n\n    def ExifTimeToUnixtime(exif_time):\n        'Convert time in EXIF to unix time.\\n\\n      Args:\\n        exif_time: str, the time from the EXIF block formated by EXIF standard.\\n          E.g., \"2011:02:20 10:23:12\", seconds are optional.\\n\\n      Returns:\\n        Integer, the time in unix fromat: seconds since the epoch.\\n      '\n        regexp = re.compile('^([0-9]{4}):([0-9]{1,2}):([0-9]{1,2}) ([0-9]{1,2}):([0-9]{1,2})(?::([0-9]{1,2}))?')\n        match = regexp.match(exif_time)\n        if (match is None):\n            return None\n        try:\n            date = datetime.datetime(*map(int, filter(None, match.groups())))\n        except ValueError:\n            logging.info('Invalid date in EXIF: %s', exif_time)\n            return None\n        return int(time.mktime(date.timetuple()))\n    metadata_dict = ((parse_metadata and ImagesServiceStub._GetExifFromImage(image)) or {\n        \n    })\n    (metadata_dict[256], metadata_dict[257]) = image.size\n    if (_EXIF_DATETIMEORIGINAL_TAG in metadata_dict):\n        date_ms = ExifTimeToUnixtime(metadata_dict[_EXIF_DATETIMEORIGINAL_TAG])\n        if date_ms:\n            metadata_dict[_EXIF_DATETIMEORIGINAL_TAG] = date_ms\n        else:\n            del metadata_dict[_EXIF_DATETIMEORIGINAL_TAG]\n    metadata = dict([(_EXIF_TAGS[k], v) for (k, v) in metadata_dict.iteritems() if (k in _EXIF_TAGS)])\n    return simplejson.dumps(metadata)\n", "label": 0}
{"function": "\n\ndef test_deploy_service_known_bounce(self):\n    fake_bounce = 'areallygoodbouncestrategy'\n    fake_drain_method_name = 'noop'\n    fake_name = 'how_many_strings'\n    fake_instance = 'will_i_need_to_think_of'\n    fake_id = marathon_tools.format_job_id(fake_name, fake_instance, 'git11111111', 'config11111111')\n    fake_config = {\n        'id': fake_id,\n        'instances': 2,\n    }\n    old_app_id = marathon_tools.format_job_id(fake_name, fake_instance, 'git22222222', 'config22222222')\n    old_task_to_drain = mock.Mock(id='old_task_to_drain', app_id=old_app_id)\n    old_task_is_draining = mock.Mock(id='old_task_is_draining', app_id=old_app_id)\n    old_task_dont_drain = mock.Mock(id='old_task_dont_drain', app_id=old_app_id)\n    old_app = mock.Mock(id=('/%s' % old_app_id), tasks=[old_task_to_drain, old_task_is_draining, old_task_dont_drain])\n    fake_client = mock.MagicMock(list_apps=mock.Mock(return_value=[old_app]), kill_given_tasks=mock.Mock(spec=(lambda task_ids, scale=False: None)))\n    fake_bounce_func = mock.create_autospec(bounce_lib.brutal_bounce, return_value={\n        'create_app': True,\n        'tasks_to_drain': [old_task_to_drain],\n    })\n    fake_drain_method = mock.Mock(is_draining=(lambda t: (t is old_task_is_draining)), is_safe_to_kill=(lambda t: True))\n    with contextlib.nested(mock.patch('paasta_tools.bounce_lib.get_bounce_method_func', return_value=fake_bounce_func, autospec=True), mock.patch('paasta_tools.bounce_lib.bounce_lock_zookeeper', autospec=True), mock.patch('paasta_tools.bounce_lib.get_happy_tasks', autospec=True, side_effect=(lambda x, _, __, ___, **kwargs: x.tasks)), mock.patch('paasta_tools.bounce_lib.kill_old_ids', autospec=True), mock.patch('paasta_tools.bounce_lib.create_marathon_app', autospec=True), mock.patch('paasta_tools.setup_marathon_job._log', autospec=True), mock.patch('paasta_tools.setup_marathon_job.load_system_paasta_config', autospec=True), mock.patch('paasta_tools.drain_lib.get_drain_method', return_value=fake_drain_method)) as (_, _, _, kill_old_ids_patch, create_marathon_app_patch, mock_log, mock_load_system_paasta_config, _):\n        mock_load_system_paasta_config.return_value.get_cluster = mock.Mock(return_value='fake_cluster')\n        result = setup_marathon_job.deploy_service(service=fake_name, instance=fake_instance, marathon_jobid=fake_id, config=fake_config, client=fake_client, bounce_method=fake_bounce, drain_method_name=fake_drain_method_name, drain_method_params={\n            \n        }, nerve_ns=fake_instance, bounce_health_params={\n            \n        }, soa_dir='fake_soa_dir')\n        assert (result[0] == 0), ('Expected successful result; got (%d, %s)' % result)\n        fake_client.list_apps.assert_called_once_with(embed_failures=True)\n        assert (fake_client.create_app.call_count == 0)\n        fake_bounce_func.assert_called_once_with(new_config=fake_config, new_app_running=False, happy_new_tasks=[], old_app_live_happy_tasks={\n            old_app.id: set([old_task_to_drain, old_task_dont_drain]),\n        }, old_app_live_unhappy_tasks={\n            old_app.id: set(),\n        })\n        assert (fake_drain_method.drain.call_count == 2)\n        fake_drain_method.drain.assert_any_call(old_task_is_draining)\n        fake_drain_method.drain.assert_any_call(old_task_to_drain)\n        assert (fake_client.kill_given_tasks.call_count == 1)\n        assert (set([old_task_to_drain.id, old_task_is_draining.id]) == set(fake_client.kill_given_tasks.call_args[1]['task_ids']))\n        assert (fake_client.kill_given_tasks.call_args[1]['scale'] is True)\n        create_marathon_app_patch.assert_called_once_with(fake_config['id'], fake_config, fake_client)\n        assert (kill_old_ids_patch.call_count == 0)\n        assert (mock_log.call_count == 5)\n", "label": 0}
{"function": "\n\ndef wait(self, timeout=None):\n    'Block until the instance is ready.\\n\\n        If this instance already holds a value / an exception, return immediatelly.\\n        Otherwise, block until another thread calls :meth:`set` or :meth:`set_exception` or\\n        until the optional timeout occurs.\\n\\n        When the *timeout* argument is present and not ``None``, it should be a\\n        floating point number specifying a timeout for the operation in seconds\\n        (or fractions thereof).\\n\\n        Return :attr:`value`.\\n        '\n    if (self._exception is not _NONE):\n        return self.value\n    else:\n        switch = greenlet.getcurrent().switch\n        self.rawlink(switch)\n        try:\n            timer = Timeout(timeout)\n            try:\n                result = self.hub.switch()\n                assert (result is self), ('Invalid switch into AsyncResult.wait(): %r' % (result,))\n            finally:\n                timer.cancel()\n        except Timeout as exc:\n            self.unlink(switch)\n            if (exc is not timer):\n                raise\n        except:\n            self.unlink(switch)\n            raise\n    return self.value\n", "label": 0}
{"function": "\n\n@utils.synchronized('dhcp-agent')\ndef sync_state(self, networks=None):\n    \"Sync the local DHCP state with Neutron. If no networks are passed,\\n        or 'None' is one of the networks, sync all of the networks.\\n        \"\n    only_nets = set(([] if ((not networks) or (None in networks)) else networks))\n    LOG.info(_LI('Synchronizing state'))\n    pool = eventlet.GreenPool(self.conf.num_sync_threads)\n    known_network_ids = set(self.cache.get_network_ids())\n    try:\n        active_networks = self.plugin_rpc.get_active_networks_info()\n        LOG.info(_LI('All active networks have been fetched through RPC.'))\n        active_network_ids = set((network.id for network in active_networks))\n        for deleted_id in (known_network_ids - active_network_ids):\n            try:\n                self.disable_dhcp_helper(deleted_id)\n            except Exception as e:\n                self.schedule_resync(e, deleted_id)\n                LOG.exception(_LE('Unable to sync network state on deleted network %s'), deleted_id)\n        for network in active_networks:\n            if ((not only_nets) or (network.id not in known_network_ids) or (network.id in only_nets)):\n                pool.spawn(self.safe_configure_dhcp_for_network, network)\n        pool.waitall()\n        LOG.info(_LI('Synchronizing state complete'))\n    except Exception as e:\n        if only_nets:\n            for network_id in only_nets:\n                self.schedule_resync(e, network_id)\n        else:\n            self.schedule_resync(e)\n        LOG.exception(_LE('Unable to sync network state.'))\n", "label": 1}
{"function": "\n\ndef __init__(self, session, roles, realm=None, authid=None, authrole=None, authmethod=None, authprovider=None, authextra=None, custom=None):\n    '\\n\\n        :param session: The WAMP session ID the other peer is assigned.\\n        :type session: int\\n        :param roles: The WAMP roles to announce.\\n        :type roles: dict of :class:`autobahn.wamp.role.RoleFeatures`\\n        :param realm: The effective realm the session is joined on.\\n        :type realm: unicode or None\\n        :param authid: The authentication ID assigned.\\n        :type authid: unicode or None\\n        :param authrole: The authentication role assigned.\\n        :type authrole: unicode or None\\n        :param authmethod: The authentication method in use.\\n        :type authmethod: unicode or None\\n        :param authprovider: The authentication provided in use.\\n        :type authprovider: unicode or None\\n        :param authextra: Application-specific \"extra data\" to be forwarded to the client.\\n        :type authextra: arbitrary or None\\n        :param custom: Implementation-specific \"custom attributes\" (`x_my_impl_attribute`) to be set.\\n        :type custom: dict or None\\n        '\n    assert (type(session) in six.integer_types)\n    assert (type(roles) == dict)\n    assert (len(roles) > 0)\n    for role in roles:\n        assert (role in ['broker', 'dealer'])\n        assert isinstance(roles[role], autobahn.wamp.role.ROLE_NAME_TO_CLASS[role])\n    assert ((realm is None) or (type(realm) == six.text_type))\n    assert ((authid is None) or (type(authid) == six.text_type))\n    assert ((authrole is None) or (type(authrole) == six.text_type))\n    assert ((authmethod is None) or (type(authmethod) == six.text_type))\n    assert ((authprovider is None) or (type(authprovider) == six.text_type))\n    assert ((authextra is None) or (type(authextra) == dict))\n    assert ((custom is None) or (type(custom) == dict))\n    if custom:\n        for k in custom:\n            assert _CUSTOM_ATTRIBUTE.match(k)\n    Message.__init__(self)\n    self.session = session\n    self.roles = roles\n    self.realm = realm\n    self.authid = authid\n    self.authrole = authrole\n    self.authmethod = authmethod\n    self.authprovider = authprovider\n    self.authextra = authextra\n    self.custom = (custom or {\n        \n    })\n", "label": 1}
{"function": "\n\ndef safe_create_instance(username, xml_file, media_files, uuid, request):\n    'Create an instance and catch exceptions.\\n\\n    :returns: A list [error, instance] where error is None if there was no\\n        error.\\n    '\n    error = instance = None\n    try:\n        instance = create_instance(username, xml_file, media_files, uuid=uuid, request=request)\n    except InstanceInvalidUserError:\n        error = OpenRosaResponseBadRequest(_('Username or ID required.'))\n    except InstanceEmptyError:\n        error = OpenRosaResponseBadRequest(_('Received empty submission. No instance was created'))\n    except FormInactiveError:\n        error = OpenRosaResponseNotAllowed(_('Form is not active'))\n    except XForm.DoesNotExist:\n        error = OpenRosaResponseNotFound(_('Form does not exist on this account'))\n    except ExpatError as e:\n        error = OpenRosaResponseBadRequest(_('Improperly formatted XML.'))\n    except DuplicateInstance:\n        response = OpenRosaResponse(_('Duplicate submission'))\n        response.status_code = 202\n        response['Location'] = request.build_absolute_uri(request.path)\n        error = response\n    except PermissionDenied as e:\n        error = OpenRosaResponseForbidden(e)\n    except InstanceMultipleNodeError as e:\n        error = OpenRosaResponseBadRequest(e)\n    except DjangoUnicodeDecodeError:\n        error = OpenRosaResponseBadRequest(_('File likely corrupted during transmission, please try later.'))\n    return [error, instance]\n", "label": 0}
{"function": "\n\ndef _async_recv_msg(self):\n    \"Internal use only; use 'recv_msg' with 'yield' instead.\\n\\n        Message is tagged with length of the payload (data). This\\n        method receives length of payload, then the payload and\\n        returns the payload.\\n        \"\n    n = struct.calcsize('>L')\n    try:\n        data = (yield self.recvall(n))\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            raise StopIteration('')\n        else:\n            raise\n    if (len(data) != n):\n        raise StopIteration('')\n    n = struct.unpack('>L', data)[0]\n    assert (n >= 0)\n    try:\n        data = (yield self.recvall(n))\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            raise StopIteration('')\n        else:\n            raise\n    if (len(data) != n):\n        raise StopIteration('')\n    raise StopIteration(data)\n", "label": 0}
{"function": "\n\ndef test_delete_user_in_backends_by_username(self):\n    ' Delete a user previously registered user by username\\n        '\n    try:\n        self.msu.create_user('bootsy', 'collins', 'funk@mothership.com')\n    except NotImplementedError:\n        skip('user management not supported in this version of managesf')\n    self.logout()\n    self.login('bootsy', 'collins', config.GATEWAY_URL)\n    self.assertEqual('funk@mothership.com', self.gu.get_account('bootsy').get('email'))\n    if has_issue_tracker():\n        users = self.rm.active_users()\n        users = [u for u in users if (u[0] == 'bootsy')]\n        self.assertEqual(1, len(users))\n        user = users[0]\n        self.assertEqual('funk@mothership.com', user[1])\n    del_url = (config.GATEWAY_URL + 'manage/services_users/?username=bootsy')\n    auth_cookie = get_cookie(config.GATEWAY_HOST, 'user5', config.ADMIN_PASSWORD)\n    d = requests.delete(del_url, cookies={\n        'auth_pubtkt': auth_cookie,\n    })\n    self.assertEqual(401, int(d.status_code))\n    auth_cookie = config.USERS[config.ADMIN_USER]['auth_cookie']\n    d = requests.delete(del_url, cookies={\n        'auth_pubtkt': auth_cookie,\n    })\n    self.assertTrue((int(d.status_code) < 400), d.status_code)\n    self.assertEqual(False, self.gu.get_account('bootsy'))\n    if has_issue_tracker():\n        users = self.rm.active_users()\n        self.assertEqual(0, len([u for u in users if (u[0] == 'bootsy')]))\n", "label": 0}
{"function": "\n\ndef set_options(self, **options):\n    try:\n        super_set_options = super(StorageOverrideMixin, self).set_options\n    except AttributeError:\n        pass\n    else:\n        super_set_options(**options)\n    storage_override = options.get('storage_override')\n    if storage_override:\n        cls = get_storage_class(storage_override)\n        self.storage = cls()\n    else:\n        self.storage = staticfiles_storage\n    try:\n        self.storage.path('')\n    except NotImplementedError:\n        self.local = False\n    else:\n        self.local = True\n", "label": 0}
{"function": "\n\ndef _SelectViaPoll(_, rfds, wfds, efds, timeout):\n    \"poll() based replacement for pexpect.spawn.__select().\\n\\n  As mentioned in the module docstring, this is required since Python's select\\n  is unable to wait for events on high-numbered file descriptors.  The API is\\n  as per select.select(), however if we are interrupted by a signal, we wait\\n  again for the remaining time.\\n\\n  Args:\\n    _: An object, self, unused.\\n    rfds: A list, file descriptors to check for read.\\n    wfds: A list, file descriptors to check for write.\\n    efds: A list, file descriptors to check for exceptions.\\n    timeout: A float, timeout (seconds).\\n\\n  Returns:\\n    A tuple of three lists, being the descriptors in each of the incoming lists\\n    which are ready for read, write or have an exception, respectively.\\n  \"\n    if (wfds or efds):\n        logging.fatal('Unexpected code change in pexpect: __select called with wfds=%s efds=%s', wfds, efds)\n    p = select.poll()\n    for fd in rfds:\n        p.register(fd, select.POLLIN)\n    if (timeout is not None):\n        end_time = (time.time() + timeout)\n    while True:\n        try:\n            fdstate = p.poll((int((timeout * 1000)) if (timeout is not None) else None))\n            rrfds = []\n            for (fd, state) in fdstate:\n                if ((state & select.POLLIN) or (state & select.POLLHUP)):\n                    rrfds.append(fd)\n            return (rrfds, [], [])\n        except select.error as e:\n            if (e[0] == errno.EINTR):\n                if (timeout is not None):\n                    timeout = (end_time - time.time())\n                    if (timeout < 0):\n                        return ([], [], [])\n            else:\n                raise\n", "label": 1}
{"function": "\n\ndef _create_x509_extension(self, handlers, extension):\n    if isinstance(extension.value, x509.UnrecognizedExtension):\n        obj = _txt2obj_gc(self, extension.oid.dotted_string)\n        value = _encode_asn1_str_gc(self, extension.value.value, len(extension.value.value))\n        return self._lib.X509_EXTENSION_create_by_OBJ(self._ffi.NULL, obj, (1 if extension.critical else 0), value)\n    else:\n        try:\n            encode = handlers[extension.oid]\n        except KeyError:\n            raise NotImplementedError('Extension not supported: {0}'.format(extension.oid))\n        ext_struct = encode(self, extension.value)\n        nid = self._lib.OBJ_txt2nid(extension.oid.dotted_string.encode('ascii'))\n        backend.openssl_assert((nid != self._lib.NID_undef))\n        x509_extension = self._lib.X509V3_EXT_i2d(nid, (1 if extension.critical else 0), ext_struct)\n        if ((x509_extension == self._ffi.NULL) and (extension.oid == x509.OID_CERTIFICATE_ISSUER)):\n            self._consume_errors()\n            pp = backend._ffi.new('unsigned char **')\n            r = self._lib.i2d_GENERAL_NAMES(ext_struct, pp)\n            backend.openssl_assert((r > 0))\n            pp = backend._ffi.gc(pp, (lambda pointer: backend._lib.OPENSSL_free(pointer[0])))\n            obj = _txt2obj_gc(self, extension.oid.dotted_string)\n            return self._lib.X509_EXTENSION_create_by_OBJ(self._ffi.NULL, obj, (1 if extension.critical else 0), _encode_asn1_str_gc(self, pp[0], r))\n        return x509_extension\n", "label": 0}
{"function": "\n\ndef getDerMatrix(self, x=None, z=None, mode=None, der=None):\n    self.checkInputGetDerMatrix(x, z, mode, der)\n    Q = self.para[0]\n    if (mode == 'self_test'):\n        (nn, D) = z.shape\n    else:\n        (nn, D) = x.shape\n    assert (Q == old_div(len(self.hyp), (1 + (2 * D))))\n    w = np.exp(self.hyp[:Q])\n    m = np.exp(np.reshape(self.hyp[Q:(Q + (Q * D))], (D, Q)))\n    v = np.exp((2 * np.reshape(self.hyp[(Q + (Q * D)):], (D, Q))))\n    if (mode == 'self_test'):\n        d2 = np.zeros((nn, 1, D))\n    elif (mode == 'train'):\n        d2 = np.zeros((nn, nn, D))\n        for j in range(D):\n            xslice = np.atleast_2d(x[:, j]).T\n            d2[:, :, j] = spdist.cdist(xslice, xslice, 'sqeuclidean')\n    elif (mode == 'cross'):\n        d2 = np.zeros((nn, z.shape[0], D))\n        for j in range(D):\n            xslice = np.atleast_2d(x[:, j]).T\n            zslice = np.atleast_2d(z[:, j]).T\n            d2[:, :, j] = spdist.cdist(xslice, zslice, 'sqeuclidean')\n    d = np.sqrt(d2)\n    k = (lambda d2v_dm1: (np.exp((((- 2) * (np.pi ** 2)) * d2v_dm1[0])) * np.cos(((2 * np.pi) * d2v_dm1[1]))))\n    km = (lambda dm: ((((- 2) * np.pi) * np.tan(((2 * np.pi) * dm))) * dm))\n    kv = (lambda d2v: ((- d2v) * ((2 * np.pi) ** 2)))\n    A = 0.0\n    c = 1.0\n    qq = list(range(Q))\n    if (der < Q):\n        c = 1\n        qq = [der]\n    elif (der < (Q + (Q * D))):\n        p = ((der - Q) % D)\n        q = old_div(((der - Q) - p), D)\n        c = km((d[:, :, p] * m[(p, q)]))\n        qq = [q]\n    elif (der < (((2 * Q) * D) + Q)):\n        p = ((der - ((D + 1) * Q)) % D)\n        q = old_div(((der - ((D + 1) * Q)) - p), D)\n        c = kv((d2[:, :, p] * v[(p, q)]))\n        qq = [q]\n    else:\n        raise Exception('Wrong derivative entry in SM')\n    for q in qq:\n        C = (w[q] * c)\n        for j in range(D):\n            C = (C * k(((d2[:, :, j] * v[(j, q)]), (d[:, :, j] * m[(j, q)]))))\n            A = (A + C)\n    return A\n", "label": 1}
{"function": "\n\ndef test_docopt_parser_with_opts():\n    help_string = '    Some Tool\\n\\n    Usage: tools [-t] [-i <input>...] [-o <output>] <cmd>\\n\\n    Inputs:\\n        -i, --input <input>    The input\\n\\n    Outputs:\\n        -o, --output <output>  The output\\n\\n    Options:\\n        -t, --test             Some option\\n        -h, --help             Show help\\n\\n    '\n    opts = Options.from_docopt(help_string)\n    assert (len(opts) == 4)\n    assert (opts['input'] is not None)\n    assert (opts['input'].nargs == '*')\n    assert (opts['input'].option_type == TYPE_INPUT)\n    assert (not opts['input'].required)\n    assert (opts['output'] is not None)\n    assert (opts['output'].nargs == 1)\n    assert (opts['output'].option_type == TYPE_OUTPUT)\n    assert (not opts['output'].required)\n    assert (opts['test'] is not None)\n    assert (opts['test'].nargs == 0)\n    assert (not opts['test'].required)\n    assert (opts['test'].option_type == TYPE_OPTION)\n    assert (opts['cmd'] is not None)\n    assert (opts['cmd'].nargs == 1)\n    assert opts['cmd'].required\n    assert (opts['cmd'].option_type == TYPE_OPTION)\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    for status in ('active', 'failed', 'invalidated'):\n        resp = self.client.post(self.path, data={\n            'status': status,\n        })\n        assert (resp.status_code == 200)\n        data = self.unserialize(resp)\n        assert (data['id'] == self.image.id.hex)\n        assert (data['status']['id'] == status)\n        db.session.expire(self.image)\n        image = SnapshotImage.query.get(self.image.id)\n        assert (image.status == SnapshotStatus[status])\n", "label": 0}
{"function": "\n\ndef __init__(self, node):\n    'Create the basic structures to keep the attribute information.\\n\\n        Reads all the HDF5 attributes (if any) on disk for the node \"node\".\\n\\n        Parameters\\n        ----------\\n        node\\n            The parent node\\n\\n        '\n    if (not node._v_isopen):\n        raise ClosedNodeError('the node for attribute set is closed')\n    dict_ = self.__dict__\n    self._g_new(node)\n    dict_['_v__nodefile'] = node._v_file\n    dict_['_v__nodepath'] = node._v_pathname\n    dict_['_v_attrnames'] = self._g_list_attr(node)\n    dict_['_v_unimplemented'] = []\n    try:\n        format_version = node._v_file.format_version\n    except AttributeError:\n        parsed_version = None\n    else:\n        if (format_version == 'unknown'):\n            parsed_version = None\n        else:\n            parsed_version = tuple(map(int, format_version.split('.')))\n    dict_['_v__format_version'] = parsed_version\n    dict_['_v_attrnamessys'] = []\n    dict_['_v_attrnamesuser'] = []\n    for attr in self._v_attrnames:\n        self.__getattr__(attr)\n        if issysattrname(attr):\n            self._v_attrnamessys.append(attr)\n        else:\n            self._v_attrnamesuser.append(attr)\n    self._v_attrnames.sort()\n    self._v_attrnamessys.sort()\n    self._v_attrnamesuser.sort()\n", "label": 0}
{"function": "\n\ndef _do_ssl_handshake(self):\n    try:\n        sock_debug('Doing SSL handshake')\n        self._sock.do_handshake()\n    except ssl.SSLError as e:\n        sock_debug('Floobits: ssl.SSLError. This is expected sometimes.')\n        if (e.args[0] in [ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE]):\n            return False\n        self.stop()\n        editor.error_message(('Floobits SSL handshake error: %s' % str(e)))\n        sock_debug(('SSLError args: %s' % ''.join([str(a) for a in e.args])))\n    except Exception as e:\n        msg.error('Error in SSL handshake: ', str_e(e))\n    else:\n        sock_debug('Successful handshake')\n        self._needs_handshake = False\n        editor.status_message(('%s:%s: SSL handshake completed' % (self.host, self.port)))\n        return True\n    self.reconnect()\n    return False\n", "label": 0}
{"function": "\n\ndef post(self, user_id=None, name=None):\n    user = User.get(self.session, user_id, name)\n    if (not user):\n        return self.notfound()\n    if ((user.name != self.current_user.name) and (not self.current_user.user_admin)):\n        return self.forbidden()\n    form = PublicKeyForm(self.request.arguments)\n    if (not form.validate()):\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    try:\n        pubkey = public_key.add_public_key(self.session, user, form.data['public_key'])\n    except public_key.PublicKeyParseError:\n        form.public_key.errors.append('Key failed to parse and is invalid.')\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    except public_key.DuplicateKey:\n        form.public_key.errors.append('Key already in use. Public keys must be unique.')\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    AuditLog.log(self.session, self.current_user.id, 'add_public_key', 'Added public key: {}'.format(pubkey.fingerprint), on_user_id=user.id)\n    email_context = {\n        'actioner': self.current_user.name,\n        'changed_user': user.name,\n        'action': 'added',\n    }\n    send_email(self.session, [user.name], 'Public SSH key added', 'ssh_keys_changed', settings, email_context)\n    return self.redirect('/users/{}?refresh=yes'.format(user.name))\n", "label": 0}
{"function": "\n\n@property\ndef _contents(self):\n    try:\n        return super(GenerateToken, self)._contents\n    except compat.urllib2.HTTPError:\n        if self._html_login:\n            (username, password) = (self._username, self._password)\n            payload = {\n                'username': username,\n                'password': password,\n                'redirect': '/',\n            }\n            for formurl in re.findall('action=\"(.*?)\"', compat.urllib2.urlopen(self._referer).read()):\n                relurl = compat.urljoin(self._referer, formurl)\n                try:\n                    html_request = compat.urllib2.Request(relurl, compat.urlencode(payload), {\n                        'Referer': self._referer,\n                    })\n                    html_response = compat.urllib2.urlopen(html_request).read()\n                    redirect_value = [re.findall('value=\"(.*?)\"', input) for input in re.findall('<input.*?name=\"redirect\".*?>', html_response)]\n                    if redirect_value:\n                        redirect = redirect_value[0][0]\n                    else:\n                        redirect = None\n                    for href in re.findall('(?:href|action)=\"(.*?)\"', html_response):\n                        if ('generatetoken' in href.lower()):\n                            try:\n                                gentokenurl = compat.urljoin(relurl, href)\n                                gentokenpayload = {\n                                    'username': username,\n                                    'password': password,\n                                    'expiration': str(self._expiration),\n                                    'client': 'requestip',\n                                    'f': 'json',\n                                }\n                                if redirect:\n                                    gentokenpayload['redirect'] = redirect\n                                self.__urldata__ = compat.urlopen(gentokenurl, compat.urlencode(gentokenpayload)).read()\n                                if self.__urldata__:\n                                    return self.__urldata__\n                            except compat.HTTPError:\n                                pass\n                except compat.urllib2.HTTPError:\n                    pass\n            return (self.__urldata__ or '{}')\n        else:\n            raise\n", "label": 1}
{"function": "\n\ndef the_local_prediction_is(step, prediction):\n    if (isinstance(world.local_prediction, list) or isinstance(world.local_prediction, tuple)):\n        local_prediction = world.local_prediction[0]\n    elif isinstance(world.local_prediction, dict):\n        local_prediction = world.local_prediction['prediction']\n    else:\n        local_prediction = world.local_prediction\n    try:\n        local_model = world.local_model\n        if (not isinstance(world.local_model, LogisticRegression)):\n            if isinstance(local_model, MultiModel):\n                local_model = local_model.models[0]\n            if local_model.tree.regression:\n                local_prediction = round(float(local_prediction), 4)\n                prediction = round(float(prediction), 4)\n    except AttributeError:\n        local_model = world.local_ensemble.multi_model.models[0]\n        if local_model.tree.regression:\n            local_prediction = round(float(local_prediction), 4)\n            prediction = round(float(prediction), 4)\n    if (local_prediction == prediction):\n        assert True\n    else:\n        assert False, ('found: %s, expected %s' % (local_prediction, prediction))\n", "label": 1}
{"function": "\n\ndef test_updating_state(db, tmpdir):\n    if (not db.startswith('mysql')):\n        db = os.path.join(str(tmpdir), db)\n    jip.db.init(db)\n    j = jip.db.Job()\n    jip.db.save(j)\n    j = jip.db.get(j.id)\n    assert (j is not None)\n    assert (j.create_date is not None)\n    assert (j.start_date is None)\n    assert (j.finish_date is None)\n    assert (j.job_id is None)\n    assert (j.state == jip.db.STATE_HOLD)\n    assert (len(j.pipe_to) == 0)\n    assert (len(j.pipe_from) == 0)\n    date = datetime.datetime.now()\n    j.job_id = 10\n    j.start_date = date\n    j.finish_date = date\n    j.state = jip.db.STATE_DONE\n    jip.db.update_job_states(j)\n    fresh = jip.db.get(j.id)\n    assert (fresh is not None)\n    assert (fresh.job_id == '10')\n    assert (fresh.state == jip.db.STATE_DONE)\n    assert (len(jip.db.get_all()) == 1)\n", "label": 1}
{"function": "\n\ndef test_tweet_ordering():\n    now = datetime.now(timezone.utc)\n    tweet_1 = Tweet('A', now)\n    tweet_2 = Tweet('B', (now + timedelta(hours=1)))\n    tweet_3 = Tweet('C', (now + timedelta(hours=2)))\n    tweet_4 = Tweet('D', (now + timedelta(hours=2)))\n    tweet_5 = Tweet('D', (now + timedelta(hours=2)))\n    source = Source('foo', 'bar')\n    with pytest.raises(TypeError):\n        (tweet_1 < source)\n    with pytest.raises(TypeError):\n        (tweet_1 <= source)\n    with pytest.raises(TypeError):\n        (tweet_1 > source)\n    with pytest.raises(TypeError):\n        (tweet_1 >= source)\n    assert (tweet_1 != source)\n    assert (tweet_1 < tweet_2)\n    assert (tweet_1 <= tweet_2)\n    assert (tweet_2 > tweet_1)\n    assert (tweet_2 >= tweet_1)\n    assert (tweet_3 != tweet_4)\n    assert (tweet_5 == tweet_4)\n    assert (tweet_5 >= tweet_4)\n    assert (tweet_5 <= tweet_4)\n    assert (not (tweet_3 <= tweet_4))\n    assert (not (tweet_3 >= tweet_4))\n", "label": 1}
{"function": "\n\ndef cli_put_container(context, path):\n    '\\n    Performs a PUT on the container.\\n\\n    See :py:mod:`swiftly.cli.put` for context usage information.\\n\\n    See :py:class:`CLIPut` for more information.\\n    '\n    path = path.rstrip('/')\n    if ('/' in path):\n        raise ReturnCode(('called cli_put_container with object %r' % path))\n    body = None\n    if context.input_:\n        if (context.input_ == '-'):\n            body = context.io_manager.get_stdin()\n        else:\n            body = open(context.input_, 'rb')\n    with context.client_manager.with_client() as client:\n        (status, reason, headers, contents) = client.put_container(path, headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n        if hasattr(contents, 'read'):\n            contents.read()\n    if ((status // 100) != 2):\n        raise ReturnCode(('putting container %r: %s %s' % (path, status, reason)))\n", "label": 0}
{"function": "\n\ndef _on_headers(self, data):\n    try:\n        data = native_str(data.decode('latin1'))\n        eol = data.find('\\r\\n')\n        start_line = data[:eol]\n        try:\n            (method, uri, version) = start_line.split(' ')\n        except ValueError:\n            raise _BadRequestException('Malformed HTTP request line')\n        if (not version.startswith('HTTP/')):\n            raise _BadRequestException('Malformed HTTP version in HTTP Request-Line')\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            raise _BadRequestException('Malformed HTTP headers')\n        if (self.address_family in (socket.AF_INET, socket.AF_INET6)):\n            remote_ip = self.address[0]\n        else:\n            remote_ip = '0.0.0.0'\n        self._request = HTTPRequest(connection=self, method=method, uri=uri, version=version, headers=headers, remote_ip=remote_ip, protocol=self.protocol)\n        content_length = headers.get('Content-Length')\n        if content_length:\n            content_length = int(content_length)\n            if (content_length > self.stream.max_buffer_size):\n                raise _BadRequestException('Content-Length too long')\n            if (headers.get('Expect') == '100-continue'):\n                self.stream.write(b'HTTP/1.1 100 (Continue)\\r\\n\\r\\n')\n            self.stream.read_bytes(content_length, self._on_request_body)\n            return\n        self.request_callback(self._request)\n    except _BadRequestException as e:\n        gen_log.info('Malformed HTTP request from %s: %s', self.address[0], e)\n        self.close()\n        return\n", "label": 0}
{"function": "\n\n@mock.patch('changes.config.queue.delay')\n@mock.patch.object(HistoricalImmutableStep, 'get_implementation')\n@responses.activate\ndef test_finished(self, get_implementation, queue_delay):\n    responses.add(responses.GET, SyncJobStepTest.ARTIFACTSTORE_REQUEST_RE, body='', status=404)\n    implementation = mock.Mock()\n    get_implementation.return_value = implementation\n\n    def mark_finished(step):\n        step.status = Status.finished\n        step.result = Result.failed\n    implementation.update_step.side_effect = mark_finished\n    project = self.create_project()\n    build = self.create_build(project=project)\n    job = self.create_job(build=build)\n    plan = self.create_plan(project)\n    self.create_step(plan, implementation='test', order=0)\n    self.create_job_plan(job, plan)\n    phase = self.create_jobphase(job)\n    step = self.create_jobstep(phase)\n    task = self.create_task(parent_id=job.id, task_id=step.id, task_name='sync_job_step', status=Status.finished)\n    db.session.add(TestCase(name='test', step_id=step.id, job_id=job.id, project_id=project.id, result=Result.failed))\n    db.session.add(FileCoverage(job=job, step=step, project=job.project, filename='foo.py', data='CCCUUUCCCUUNNN', lines_covered=6, lines_uncovered=5, diff_lines_covered=3, diff_lines_uncovered=2))\n    db.session.commit()\n    sync_job_step(step_id=step.id.hex, task_id=step.id.hex, parent_task_id=job.id.hex)\n    implementation.update_step.assert_called_once_with(step=step)\n    db.session.expire(step)\n    db.session.expire(task)\n    step = JobStep.query.get(step.id)\n    assert (step.status == Status.finished)\n    task = Task.query.get(task.id)\n    assert (task.status == Status.finished)\n    assert (len(queue_delay.mock_calls) == 0)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'tests_missing')).first()\n    assert (stat.value == 0)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'lines_covered')).first()\n    assert (stat.value == 6)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'lines_uncovered')).first()\n    assert (stat.value == 5)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'diff_lines_covered')).first()\n    assert (stat.value == 3)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'diff_lines_uncovered')).first()\n    assert (stat.value == 2)\n    assert FailureReason.query.filter((FailureReason.step_id == step.id), (FailureReason.reason == 'test_failures'))\n", "label": 0}
{"function": "\n\ndef polyhedra(self, wm):\n    'Iterates through the polyhedra that make up the closest volume to a certain vertex'\n    for (p, facerow) in enumerate(self.connected):\n        faces = facerow.indices\n        (pts, polys) = (_ptset(), _quadset())\n        if (len(faces) > 0):\n            poly = np.roll(self.polys[faces[0]], (- np.nonzero((self.polys[faces[0]] == p))[0][0]))\n            assert (pts[wm[p]] == 0)\n            assert (pts[self.pts[p]] == 1)\n            pts[wm[poly[[0, 1]]].mean(0)]\n            pts[self.pts[poly[[0, 1]]].mean(0)]\n            for face in faces:\n                poly = np.roll(self.polys[face], (- np.nonzero((self.polys[face] == p))[0][0]))\n                a = pts[wm[poly].mean(0)]\n                b = pts[self.pts[poly].mean(0)]\n                c = pts[wm[poly[[0, 2]]].mean(0)]\n                d = pts[self.pts[poly[[0, 2]]].mean(0)]\n                e = pts[wm[poly[[0, 1]]].mean(0)]\n                f = pts[self.pts[poly[[0, 1]]].mean(0)]\n                polys((0, c, a, e))\n                polys((1, f, b, d))\n                polys((1, d, c, 0))\n                polys((1, 0, e, f))\n                polys((f, e, a, b))\n                polys((d, b, a, c))\n        (yield (pts.points, np.array(list(polys.triangles))))\n", "label": 0}
{"function": "\n\ndef io_connection_pattern(inputs, outputs):\n    '\\n    Returns the connection pattern of a subgraph defined by given\\n    inputs and outputs.\\n\\n    '\n    inner_nodes = io_toposort(inputs, outputs)\n    connect_pattern_by_var = {\n        \n    }\n    nb_inputs = len(inputs)\n    for i in range(nb_inputs):\n        input = inputs[i]\n        inp_connection_pattern = [(i == j) for j in range(nb_inputs)]\n        connect_pattern_by_var[input] = inp_connection_pattern\n    for n in inner_nodes:\n        try:\n            op_connection_pattern = n.op.connection_pattern(n)\n        except AttributeError:\n            op_connection_pattern = ([([True] * len(n.outputs))] * len(n.inputs))\n        for out_idx in range(len(n.outputs)):\n            out = n.outputs[out_idx]\n            out_connection_pattern = ([False] * nb_inputs)\n            for inp_idx in range(len(n.inputs)):\n                inp = n.inputs[inp_idx]\n                if (inp in connect_pattern_by_var):\n                    inp_connection_pattern = connect_pattern_by_var[inp]\n                    if op_connection_pattern[inp_idx][out_idx]:\n                        out_connection_pattern = [(out_connection_pattern[i] or inp_connection_pattern[i]) for i in range(nb_inputs)]\n            connect_pattern_by_var[out] = out_connection_pattern\n    global_connection_pattern = [[] for o in range(len(inputs))]\n    for out in outputs:\n        out_connection_pattern = connect_pattern_by_var[out]\n        for i in range(len(inputs)):\n            global_connection_pattern[i].append(out_connection_pattern[i])\n    return global_connection_pattern\n", "label": 1}
{"function": "\n\ndef _apt_packages(to_install=None, pkg_list=None):\n    '\\n    Install packages available via apt-get.\\n    Note that ``to_install`` and ``pkg_list`` arguments cannot be used simultaneously.\\n\\n    :type to_install:  list\\n    :param to_install: A list of strings (ie, groups) present in the ``main.yaml``\\n                       config file that will be used to filter out the specific\\n                       packages to be installed.\\n\\n    :type pkg_list:  list\\n    :param pkg_list: An explicit list of packages to install. No other files,\\n                     flavors are considered.\\n    '\n    if ('minimal' not in env.flavor.short_name):\n        env.logger.info('Update the system')\n        with settings(warn_only=True):\n            env.safe_sudo('apt-get update')\n    if (to_install is not None):\n        config_file = get_config_file(env, 'packages.yaml')\n        if (('minimal' not in env.flavor.name) and ('minimal' not in env.flavor.short_name)):\n            env.flavor.apt_upgrade_system(env=env)\n        (packages, _) = _yaml_to_packages(config_file.base, to_install, config_file.dist)\n        packages = env.flavor.rewrite_config_items('packages', packages)\n    elif (pkg_list is not None):\n        env.logger.info('Will install specific packages: {0}'.format(pkg_list))\n        packages = pkg_list\n    else:\n        raise ValueError('Need a file with packages or a list of packages')\n    group_size = 30\n    i = 0\n    env.logger.info(('Installing %i packages' % len(packages)))\n    while (i < len(packages)):\n        env.logger.info('Package install progress: {0}/{1}'.format(i, len(packages)))\n        env.safe_sudo(('apt-get -y --force-yes install %s' % ' '.join(packages[i:(i + group_size)])))\n        i += group_size\n    env.safe_sudo('apt-get clean')\n", "label": 0}
{"function": "\n\n@expose('/delete/', methods=('POST',))\ndef delete(self):\n    '\\n            Delete view method\\n        '\n    form = self.delete_form()\n    path = form.path.data\n    if path:\n        return_url = self._get_dir_url('.index', op.dirname(path))\n    else:\n        return_url = self.get_url('.index')\n    if self.validate_form(form):\n        (base_path, full_path, path) = self._normalize_path(path)\n        if (not self.can_delete):\n            flash(gettext('Deletion is disabled.'), 'error')\n            return redirect(return_url)\n        if (not self.is_accessible_path(path)):\n            flash(gettext('Permission denied.'), 'error')\n            return redirect(self._get_dir_url('.index'))\n        if op.isdir(full_path):\n            if (not self.can_delete_dirs):\n                flash(gettext('Directory deletion is disabled.'), 'error')\n                return redirect(return_url)\n            try:\n                shutil.rmtree(full_path)\n                self.on_directory_delete(full_path, path)\n                flash(gettext('Directory \"%(path)s\" was successfully deleted.', path=path))\n            except Exception as ex:\n                flash(gettext('Failed to delete directory: %(error)s', error=ex), 'error')\n        else:\n            try:\n                os.remove(full_path)\n                self.on_file_delete(full_path, path)\n                flash(gettext('File \"%(name)s\" was successfully deleted.', name=path))\n            except Exception as ex:\n                flash(gettext('Failed to delete file: %(name)s', name=ex), 'error')\n    else:\n        helpers.flash_errors(form, message='Failed to delete file. %(error)s')\n    return redirect(return_url)\n", "label": 0}
{"function": "\n\ndef the_local_prediction_confidence_is(step, confidence):\n    if (isinstance(world.local_prediction, list) or isinstance(world.local_prediction, tuple)):\n        local_confidence = world.local_prediction[1]\n    else:\n        local_confidence = world.local_prediction['confidence']\n    local_confidence = round(float(local_confidence), 4)\n    confidence = round(float(confidence), 4)\n    if (local_confidence == confidence):\n        assert True\n    else:\n        assert False, ('found: %s, expected %s' % (local_confidence, confidence))\n", "label": 0}
{"function": "\n\n@skipif(('GPy' not in sys.modules), 'this test requires hyperopt')\ndef test_gp():\n    searchspace = SearchSpace()\n    searchspace.add_float('x', (- 10), 10)\n    searchspace.add_float('y', 1, 10, warp='log')\n    searchspace.add_int('z', (- 10), 10)\n    searchspace.add_enum('w', ['opt1', 'opt2'])\n    history = [(searchspace.rvs(), np.random.random(), 'SUCCEEDED') for _ in range(4)]\n    params = GP().suggest(history, searchspace)\n    for (k, v) in iteritems(params):\n        assert (k in searchspace.variables)\n        if isinstance(searchspace[k], EnumVariable):\n            assert (v in searchspace[k].choices)\n        elif isinstance(searchspace[k], FloatVariable):\n            assert (searchspace[k].min <= v <= searchspace[k].max)\n        elif isinstance(searchspace[k], IntVariable):\n            assert (searchspace[k].min <= v <= searchspace[k].max)\n        else:\n            assert False\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    assert name.endswith('PropMap'), 'Please use convention: ___PropMap, e.g. ElectromagneticPropMap'\n    _properties = {\n        \n    }\n    for base in bases:\n        for baseProp in getattr(base, '_properties', {\n            \n        }):\n            _properties[baseProp] = base._properties[baseProp]\n    keys = [key for key in attrs]\n    for attr in keys:\n        if isinstance(attrs[attr], Property):\n            attrs[attr].name = attr\n            attrs[(attr + 'Map')] = attrs[attr]._getMapProperty()\n            attrs[(attr + 'Index')] = attrs[attr]._getIndexProperty()\n            _properties[attr] = attrs[attr]\n            attrs.pop(attr)\n    attrs['_properties'] = _properties\n    defaultInvProps = []\n    for p in _properties:\n        prop = _properties[p]\n        if prop.defaultInvProp:\n            defaultInvProps += [p]\n        if (prop.propertyLink is not None):\n            assert (prop.propertyLink[0] in _properties), (\"You can only link to things that exist: '%s' is trying to link to '%s'\" % (prop.name, prop.propertyLink[0]))\n    if (len(defaultInvProps) > 1):\n        raise Exception(('You have more than one default inversion property: %s' % defaultInvProps))\n    newClass = super(_PropMapMetaClass, cls).__new__(cls, name, bases, attrs)\n    newClass.PropModel = cls.createPropModelClass(newClass, name, _properties)\n    _PROPMAPCLASSREGISTRY[name] = newClass\n    return newClass\n", "label": 1}
{"function": "\n\n@classmethod\ndef delete(cls, repo, path):\n    'Delete the reference at the given path\\n\\n        :param repo:\\n            Repository to delete the reference from\\n\\n        :param path:\\n            Short or full path pointing to the reference, i.e. refs/myreference\\n            or just \"myreference\", hence \\'refs/\\' is implied.\\n            Alternatively the symbolic reference to be deleted'\n    full_ref_path = cls.to_full_path(path)\n    abs_path = join(repo.git_dir, full_ref_path)\n    if exists(abs_path):\n        os.remove(abs_path)\n    else:\n        pack_file_path = cls._get_packed_refs_path(repo)\n        try:\n            reader = open(pack_file_path, 'rb')\n        except (OSError, IOError):\n            pass\n        else:\n            new_lines = list()\n            made_change = False\n            dropped_last_line = False\n            for line in reader:\n                line = line.decode(defenc)\n                if ((line.startswith('#') or (full_ref_path not in line)) and ((not dropped_last_line) or (dropped_last_line and (not line.startswith('^'))))):\n                    new_lines.append(line)\n                    dropped_last_line = False\n                    continue\n                made_change = True\n                dropped_last_line = True\n            reader.close()\n            if made_change:\n                open(pack_file_path, 'wb').writelines((l.encode(defenc) for l in new_lines))\n    reflog_path = RefLog.path(cls(repo, full_ref_path))\n    if os.path.isfile(reflog_path):\n        os.remove(reflog_path)\n", "label": 1}
{"function": "\n\ndef test_new_term_with_terms():\n    rules = {\n        'fields': ['a'],\n        'timestamp_field': '@timestamp',\n        'es_host': 'example.com',\n        'es_port': 10,\n        'index': 'logstash',\n        'query_key': 'a',\n    }\n    mock_res = {\n        'aggregations': {\n            'filtered': {\n                'values': {\n                    'buckets': [{\n                        'key': 'key1',\n                        'doc_count': 1,\n                    }, {\n                        'key': 'key2',\n                        'doc_count': 5,\n                    }],\n                },\n            },\n        },\n    }\n    with mock.patch('elastalert.ruletypes.Elasticsearch') as mock_es:\n        mock_es.return_value = mock.Mock()\n        mock_es.return_value.search.return_value = mock_res\n        rule = NewTermsRule(rules)\n        assert (rule.es.search.call_count == 1)\n    terms = {\n        ts_now(): [{\n            'key': 'key1',\n            'doc_count': 1,\n        }, {\n            'key': 'key2',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (rule.matches == [])\n    terms = {\n        ts_now(): [{\n            'key': 'key3',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0]['new_field'] == 'a')\n    assert (rule.matches[0]['a'] == 'key3')\n    rule.matches = []\n    terms = {\n        ts_now(): [{\n            'key': 'key3',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (rule.matches == [])\n", "label": 0}
{"function": "\n\ndef test_remove_role():\n    db = SQLAlchemy('sqlite:///:memory:')\n    auth = authcode.Auth(SECRET_KEY, db=db, roles=True)\n    User = auth.User\n    Role = auth.Role\n    db.create_all()\n    user = User(login='meh', password='foobar')\n    db.session.add(user)\n    db.session.commit()\n    assert hasattr(auth, 'Role')\n    assert hasattr(User, 'roles')\n    user.add_role('admin')\n    db.session.commit()\n    assert user.has_role('admin')\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('foobar')\n    db.session.commit()\n    assert (db.query(Role).count() == 1)\n", "label": 1}
{"function": "\n\ndef test_virtual_memory(self):\n    mem = psutil.virtual_memory()\n    assert (mem.total > 0), mem\n    assert (mem.available > 0), mem\n    assert (0 <= mem.percent <= 100), mem\n    assert (mem.used > 0), mem\n    assert (mem.free >= 0), mem\n    for name in mem._fields:\n        if (name != 'total'):\n            value = getattr(mem, name)\n            if (not (value >= 0)):\n                self.fail(('%r < 0 (%s)' % (name, value)))\n            if (value > mem.total):\n                self.fail(('%r > total (total=%s, %s=%s)' % (name, mem.total, name, value)))\n", "label": 0}
{"function": "\n\ndef static(environ, start_response, path):\n    logger.info(('[static]sending: %s' % (path,)))\n    try:\n        text = open(path).read()\n        if path.endswith('.ico'):\n            start_response('200 OK', [('Content-Type', 'image/x-icon')])\n        elif path.endswith('.html'):\n            start_response('200 OK', [('Content-Type', 'text/html')])\n        elif path.endswith('.json'):\n            start_response('200 OK', [('Content-Type', 'application/json')])\n        elif path.endswith('.txt'):\n            start_response('200 OK', [('Content-Type', 'text/plain')])\n        elif path.endswith('.css'):\n            start_response('200 OK', [('Content-Type', 'text/css')])\n        else:\n            start_response('200 OK', [('Content-Type', 'text/xml')])\n        return [text]\n    except IOError:\n        resp = NotFound()\n        return resp(environ, start_response)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef shutdown(dbpool, filename, archive_db_dir):\n    '\\n        Facilitates cleaning up filesystem objects after the last\\n        user of the connection pool goes away.\\n        '\n    try:\n        dbpool.close()\n    except Exception as e:\n        logging.info('SqlConnections: dbpool.close failed in shutdown: %s', e)\n    if ((filename != ':memory:') and os.path.exists(filename)):\n        if archive_db_dir:\n            logging.info(\"SqlConnections: Archiving db file '%s' to '%s'\", filename, archive_db_dir)\n            try:\n                os.renames(filename, archive_db_dir)\n            except Exception as e:\n                logging.info('SqlConnections: ERROR: Archive failed! %s', e)\n        else:\n            logging.info(\"SqlConnections: Removing db file '%s'\", filename)\n            try:\n                os.remove(filename)\n            except Exception as e:\n                logging.info('SqlConnections: ERROR: Could not remove db file! %s', e)\n", "label": 0}
{"function": "\n\ndef get_passenger_memory_stats(self):\n    '\\n        Execute passenger-memory-stats, parse its output, return dictionary with\\n        stats.\\n        '\n    command = [self.config['bin']]\n    if str_to_bool(self.config['use_sudo']):\n        command.insert(0, self.config['sudo_cmd'])\n    try:\n        proc1 = subprocess.Popen(command, stdout=subprocess.PIPE)\n        (std_out, std_err) = proc1.communicate()\n    except OSError:\n        return {\n            \n        }\n    if (std_out is None):\n        return {\n            \n        }\n    dict_stats = {\n        'apache_procs': [],\n        'nginx_procs': [],\n        'passenger_procs': [],\n        'apache_mem_total': 0.0,\n        'nginx_mem_total': 0.0,\n        'passenger_mem_total': 0.0,\n    }\n    re_colour = re.compile('\\x1b\\\\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]')\n    re_digit = re.compile('^\\\\d')\n    apache_flag = 0\n    nginx_flag = 0\n    passenger_flag = 0\n    for raw_line in std_out.splitlines():\n        line = re_colour.sub('', raw_line)\n        if ('Apache processes' in line):\n            apache_flag = 1\n        elif ('Nginx processes' in line):\n            nginx_flag = 1\n        elif ('Passenger processes' in line):\n            passenger_flag = 1\n        elif re_digit.match(line):\n            line_splitted = line.split()\n            if (apache_flag == 1):\n                dict_stats['apache_procs'].append(line_splitted[0])\n                dict_stats['apache_mem_total'] += float(line_splitted[4])\n            elif (nginx_flag == 1):\n                dict_stats['nginx_procs'].append(line_splitted[0])\n                dict_stats['nginx_mem_total'] += float(line_splitted[4])\n            elif (passenger_flag == 1):\n                dict_stats['passenger_procs'].append(line_splitted[0])\n                dict_stats['passenger_mem_total'] += float(line_splitted[3])\n        elif ('Processes:' in line):\n            passenger_flag = 0\n            apache_flag = 0\n            nginx_flag = 0\n    return dict_stats\n", "label": 1}
{"function": "\n\ndef main():\n    (options, args) = parse_args()\n    names = sorted((name for name in resource_listdir('trac.wiki', 'default-pages') if (not name.startswith('.'))))\n    if args:\n        args = sorted((set(names) & set(map(os.path.basename, args))))\n    else:\n        args = names\n    if options.download:\n        download_default_pages(args, options.prefix)\n    env = EnvironmentStub(disable=['trac.mimeview.pygments.*'])\n    load_components(env)\n    with env.db_transaction:\n        for name in names:\n            wiki = WikiPage(env, name)\n            wiki.text = resource_string('trac.wiki', ('default-pages/' + name)).decode('utf-8')\n            if wiki.text:\n                wiki.save('trac', '')\n            else:\n                printout(('%s: Skipped empty page' % name))\n    req = Mock(href=Href('/'), abs_href=Href('http://localhost/'), perm=MockPerm())\n    for name in args:\n        wiki = WikiPage(env, name)\n        if (not wiki.exists):\n            continue\n        context = web_context(req, wiki.resource)\n        out = DummyIO()\n        DefaultWikiChecker(env, context, name).format(wiki.text, out)\n", "label": 0}
{"function": "\n\n@expose(help='Display Nginx configuration of example.com')\ndef show(self):\n    if (not self.app.pargs.site_name):\n        try:\n            while (not self.app.pargs.site_name):\n                self.app.pargs.site_name = input('Enter site name : ').strip()\n        except IOError as e:\n            Log.error(self, 'could not input site name')\n    self.app.pargs.site_name = self.app.pargs.site_name.strip()\n    (ee_domain, ee_www_domain) = ValidateDomain(self.app.pargs.site_name)\n    if (not check_domain_exists(self, ee_domain)):\n        Log.error(self, 'site {0} does not exist'.format(ee_domain))\n    if os.path.isfile('/etc/nginx/sites-available/{0}'.format(ee_domain)):\n        Log.info(self, 'Display NGINX configuration for {0}'.format(ee_domain))\n        f = open('/etc/nginx/sites-available/{0}'.format(ee_domain), encoding='utf-8', mode='r')\n        text = f.read()\n        Log.info(self, (Log.ENDC + text))\n        f.close()\n    else:\n        Log.error(self, 'nginx configuration file does not exists'.format(ee_domain))\n", "label": 0}
{"function": "\n\ndef fix_lib64(lib_dir):\n    \"\\n    Some platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y\\n    instead of lib/pythonX.Y.  If this is such a platform we'll just create a\\n    symlink so lib64 points to lib\\n    \"\n    if [p for p in distutils.sysconfig.get_config_vars().values() if (isinstance(p, basestring) and ('lib64' in p))]:\n        logger.debug('This system uses lib64; symlinking lib64 to lib')\n        assert (os.path.basename(lib_dir) == ('python%s' % sys.version[:3])), ('Unexpected python lib dir: %r' % lib_dir)\n        lib_parent = os.path.dirname(lib_dir)\n        assert (os.path.basename(lib_parent) == 'lib'), ('Unexpected parent dir: %r' % lib_parent)\n        copyfile(lib_parent, os.path.join(os.path.dirname(lib_parent), 'lib64'))\n", "label": 0}
{"function": "\n\ndef _PopulateX509(self):\n    with self._x509_init_lock:\n        if (self._x509 is None):\n            url = ('https://www.googleapis.com/service_accounts/v1/metadata/x509/%s' % urllib.unquote_plus(self._credentials.service_account_email))\n            response = urlfetch.fetch(url=url, validate_certificate=True, method=urlfetch.GET)\n            if (response.status_code != 200):\n                raise apiproxy_errors.ApplicationError(app_identity_service_pb.AppIdentityServiceError.UNKNOWN_ERROR, ('Unable to load X509 cert: %s Response code: %i, Content: %s' % (url, response.status_code, response.content)))\n            message = 'dummy'\n            (_, signature) = self._credentials.sign_blob(message)\n            for (signing_key, x509) in json.loads(response.content).items():\n                der = rsa.pem.load_pem(x509, 'CERTIFICATE')\n                (asn1_cert, _) = decoder.decode(der, asn1Spec=Certificate())\n                key_bitstring = asn1_cert['tbsCertificate']['subjectPublicKeyInfo']['subjectPublicKey']\n                key_bytearray = BitStringToByteString(key_bitstring)\n                public_key = rsa.PublicKey.load_pkcs1(key_bytearray, 'DER')\n                try:\n                    if rsa.pkcs1.verify(message, signature, public_key):\n                        self._x509 = x509\n                        self._signing_key = signing_key\n                        return\n                except rsa.pkcs1.VerificationError:\n                    pass\n            raise apiproxy_errors.ApplicationError(app_identity_service_pb.AppIdentityServiceError.UNKNOWN_ERROR, ('Unable to find matching X509 cert for private key: %s' % url))\n", "label": 0}
{"function": "\n\ndef DeclareLocks(self, level):\n    if (level == locking.LEVEL_NODEGROUP):\n        assert (not self.needed_locks[locking.LEVEL_NODEGROUP])\n        if self.req_target_uuids:\n            lock_groups = set(self.req_target_uuids)\n            instance_groups = self.cfg.GetInstanceNodeGroups(self.op.instance_uuid)\n            lock_groups.update(instance_groups)\n        else:\n            lock_groups = locking.ALL_SET\n        self.needed_locks[locking.LEVEL_NODEGROUP] = lock_groups\n    elif (level == locking.LEVEL_NODE):\n        if self.req_target_uuids:\n            self.recalculate_locks[locking.LEVEL_NODE] = constants.LOCKS_APPEND\n            self._LockInstancesNodes()\n            lock_groups = (frozenset(self.owned_locks(locking.LEVEL_NODEGROUP)) | self.cfg.GetInstanceNodeGroups(self.op.instance_uuid))\n            member_nodes = [node_uuid for group in lock_groups for node_uuid in self.cfg.GetNodeGroup(group).members]\n            self.needed_locks[locking.LEVEL_NODE].extend(member_nodes)\n        else:\n            self.needed_locks[locking.LEVEL_NODE] = locking.ALL_SET\n", "label": 0}
{"function": "\n\ndef test_admin_inheritor_documents(suite, administrator, docs):\n    jwt = _login(administrator)\n    headers = _auth_header(jwt)\n    docs = [{\n        'name': 'Added Document {0}'.format(x),\n        'date': datetime.utcnow().isoformat(),\n    } for x in range(101)]\n    inheritor_documents = 'http://localhost:5000/api/authorized-app/simple-documents'\n    confirmed_dangerous_delete = requests.delete(inheritor_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    post = requests.post(inheritor_documents, headers=headers, data=json.dumps(docs))\n    assert post.ok\n    get_all = requests.get((inheritor_documents + ';json'), headers=headers)\n    assert get_all.ok\n    confirmed_dangerous_delete = requests.delete(inheritor_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    get_empty = requests.get((inheritor_documents + ';json'), headers=headers)\n    assert get_empty.ok\n    _logout(jwt)\n", "label": 0}
{"function": "\n\ndef complete_struct_or_union(self, CTypesStructOrUnion, fields, tp, totalsize=(- 1), totalalignment=(- 1), sflags=0):\n    if ((totalsize >= 0) or (totalalignment >= 0)):\n        raise NotImplementedError('the ctypes backend of CFFI does not support structures completed by verify(); please compile and install the _cffi_backend module.')\n    struct_or_union = CTypesStructOrUnion._ctype\n    fnames = [fname for (fname, BField, bitsize) in fields]\n    btypes = [BField for (fname, BField, bitsize) in fields]\n    bitfields = [bitsize for (fname, BField, bitsize) in fields]\n    bfield_types = {\n        \n    }\n    cfields = []\n    for (fname, BField, bitsize) in fields:\n        if (bitsize < 0):\n            cfields.append((fname, BField._ctype))\n            bfield_types[fname] = BField\n        else:\n            cfields.append((fname, BField._ctype, bitsize))\n            bfield_types[fname] = Ellipsis\n    if (sflags & 8):\n        struct_or_union._pack_ = 1\n    struct_or_union._fields_ = cfields\n    CTypesStructOrUnion._bfield_types = bfield_types\n\n    @staticmethod\n    def _create_ctype_obj(init):\n        result = struct_or_union()\n        if (init is not None):\n            initialize(result, init)\n        return result\n    CTypesStructOrUnion._create_ctype_obj = _create_ctype_obj\n\n    def initialize(blob, init):\n        if is_union:\n            if (len(init) > 1):\n                raise ValueError(('union initializer: %d items given, but only one supported (use a dict if needed)' % (len(init),)))\n        if (not isinstance(init, dict)):\n            if isinstance(init, (bytes, unicode)):\n                raise TypeError('union initializer: got a str')\n            init = tuple(init)\n            if (len(init) > len(fnames)):\n                raise ValueError(('too many values for %s initializer' % CTypesStructOrUnion._get_c_name()))\n            init = dict(zip(fnames, init))\n        addr = ctypes.addressof(blob)\n        for (fname, value) in init.items():\n            (BField, bitsize) = name2fieldtype[fname]\n            assert (bitsize < 0), 'not implemented: initializer with bit fields'\n            offset = CTypesStructOrUnion._offsetof(fname)\n            PTR = ctypes.POINTER(BField._ctype)\n            p = ctypes.cast((addr + offset), PTR)\n            BField._initialize(p.contents, value)\n    is_union = (CTypesStructOrUnion._kind == 'union')\n    name2fieldtype = dict(zip(fnames, zip(btypes, bitfields)))\n    for (fname, BField, bitsize) in fields:\n        if (fname == ''):\n            raise NotImplementedError('nested anonymous structs/unions')\n        if hasattr(CTypesStructOrUnion, fname):\n            raise ValueError(('the field name %r conflicts in the ctypes backend' % fname))\n        if (bitsize < 0):\n\n            def getter(self, fname=fname, BField=BField, offset=CTypesStructOrUnion._offsetof(fname), PTR=ctypes.POINTER(BField._ctype)):\n                addr = ctypes.addressof(self._blob)\n                p = ctypes.cast((addr + offset), PTR)\n                return BField._from_ctypes(p.contents)\n\n            def setter(self, value, fname=fname, BField=BField):\n                setattr(self._blob, fname, BField._to_ctypes(value))\n            if issubclass(BField, CTypesGenericArray):\n                setter = None\n                if (BField._declared_length == 0):\n\n                    def getter(self, fname=fname, BFieldPtr=BField._CTPtr, offset=CTypesStructOrUnion._offsetof(fname), PTR=ctypes.POINTER(BField._ctype)):\n                        addr = ctypes.addressof(self._blob)\n                        p = ctypes.cast((addr + offset), PTR)\n                        return BFieldPtr._from_ctypes(p)\n        else:\n\n            def getter(self, fname=fname, BField=BField):\n                return BField._from_ctypes(getattr(self._blob, fname))\n\n            def setter(self, value, fname=fname, BField=BField):\n                value = BField._to_ctypes(value)\n                oldvalue = getattr(self._blob, fname)\n                setattr(self._blob, fname, value)\n                if (value != getattr(self._blob, fname)):\n                    setattr(self._blob, fname, oldvalue)\n                    raise OverflowError('value too large for bitfield')\n        setattr(CTypesStructOrUnion, fname, property(getter, setter))\n    CTypesPtr = self.ffi._get_cached_btype(model.PointerType(tp))\n    for fname in fnames:\n        if hasattr(CTypesPtr, fname):\n            raise ValueError(('the field name %r conflicts in the ctypes backend' % fname))\n\n        def getter(self, fname=fname):\n            return getattr(self[0], fname)\n\n        def setter(self, value, fname=fname):\n            setattr(self[0], fname, value)\n        setattr(CTypesPtr, fname, property(getter, setter))\n", "label": 1}
{"function": "\n\ndef prepare_on_all_hosts(self, query, excluded_host):\n    '\\n        Prepare the given query on all hosts, excluding ``excluded_host``.\\n        Intended for internal use only.\\n        '\n    futures = []\n    for host in self._pools.keys():\n        if ((host != excluded_host) and host.is_up):\n            future = ResponseFuture(self, PrepareMessage(query=query), None, self.default_timeout)\n            try:\n                request_id = future._query(host)\n            except Exception:\n                log.exception('Error preparing query for host %s:', host)\n                continue\n            if (request_id is None):\n                log.debug('Failed to prepare query for host %s: %r', host, future._errors.get(host))\n                continue\n            futures.append((host, future))\n    for (host, future) in futures:\n        try:\n            future.result()\n        except Exception:\n            log.exception('Error preparing query for host %s:', host)\n", "label": 0}
{"function": "\n\ndef _destroy(self, instance, vm_ref, network_info=None, destroy_disks=True, block_device_info=None):\n    'Destroys VM instance by performing:\\n\\n            1. A shutdown\\n            2. Destroying associated VDIs.\\n            3. Destroying kernel and ramdisk files (if necessary).\\n            4. Destroying that actual VM record.\\n\\n        '\n    if (vm_ref is None):\n        LOG.warning(_LW('VM is not present, skipping destroy...'), instance=instance)\n        bdms = (block_device_info['block_device_mapping'] or [])\n        if (not bdms):\n            return\n        for bdm in bdms:\n            volume_id = bdm['connection_info']['data']['volume_id']\n            sr_uuid = ('FA15E-D15C-%s' % volume_id)\n            sr_ref = None\n            try:\n                sr_ref = volume_utils.find_sr_by_uuid(self._session, sr_uuid)\n                if (not sr_ref):\n                    connection_data = bdm['connection_info']['data']\n                    (sr_uuid, _, _) = volume_utils.parse_sr_info(connection_data)\n                    sr_ref = volume_utils.find_sr_by_uuid(self._session, sr_uuid)\n            except Exception:\n                LOG.exception(_LE('Failed to find an SR for volume %s'), volume_id, instance=instance)\n            try:\n                if sr_ref:\n                    volume_utils.forget_sr(self._session, sr_ref)\n                else:\n                    LOG.error(_LE('Volume %s is associated with the instance but no SR was found for it'), volume_id, instance=instance)\n            except Exception:\n                LOG.exception(_LE('Failed to forget the SR for volume %s'), volume_id, instance=instance)\n        return\n    if (block_device_info and block_device_info['block_device_mapping']):\n        if (not vm_utils.clean_shutdown_vm(self._session, instance, vm_ref)):\n            LOG.debug('Clean shutdown did not complete successfully, trying hard shutdown.', instance=instance)\n            vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)\n    else:\n        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)\n    if destroy_disks:\n        self._volumeops.detach_all(vm_ref)\n        self._destroy_vdis(instance, vm_ref)\n        self._destroy_kernel_ramdisk(instance, vm_ref)\n    self.unplug_vifs(instance, network_info, vm_ref)\n    self.firewall_driver.unfilter_instance(instance, network_info=network_info)\n    vm_utils.destroy_vm(self._session, instance, vm_ref)\n", "label": 1}
{"function": "\n\ndef put(self, objects, force=False, commit=True):\n    if self.stopped:\n        return\n    self.init()\n    if (isinstance(objects, basestring) or (not iterable(objects))):\n        return self.put_one(objects, force, commit)\n    remains = []\n    for obj in objects:\n        result = self.put_one(obj, force, commit=False)\n        if (result is not None):\n            remains.append(result)\n    m = self.map_handles[WRITE_ENTRANCE]\n    if ((len(remains) > 0) and (m is not None)):\n        with self.lock:\n            m.flush()\n    return remains\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('auth_scopes', (('users:write',), ('users:read',), ('users:read', 'users:write')))\ndef test_getting_list_of_users_by_unauthorized_user_must_fail(flask_app_client, regular_user, auth_scopes):\n    with flask_app_client.login(regular_user, auth_scopes=auth_scopes):\n        response = flask_app_client.get('/api/v1/users/')\n    if ('users:read' in auth_scopes):\n        assert (response.status_code == 403)\n    else:\n        assert (response.status_code == 401)\n    assert (response.content_type == 'application/json')\n    assert (set(response.json.keys()) >= {'status', 'message'})\n", "label": 0}
{"function": "\n\ndef unique_messages():\n    basedir = None\n    if os.path.isdir(os.path.join('conf', 'locale')):\n        basedir = os.path.abspath(os.path.join('conf', 'locale'))\n    elif os.path.isdir('locale'):\n        basedir = os.path.abspath('locale')\n    else:\n        print('This script should be run from the Django Git tree or your project or app tree.')\n        sys.exit(1)\n    for (dirpath, dirnames, filenames) in os.walk(basedir):\n        for f in filenames:\n            if f.endswith('.po'):\n                sys.stderr.write(('processing file %s in %s\\n' % (f, dirpath)))\n                pf = os.path.splitext(os.path.join(dirpath, f))[0]\n                cmd = ('msguniq \"%s.po\"' % pf)\n                stdout = os.popen(cmd)\n                msg = stdout.read()\n                with open(('%s.po' % pf), 'w') as fp:\n                    fp.write(msg)\n", "label": 0}
{"function": "\n\ndef cli_post(context, path, body=None):\n    '\\n    Performs a POST on the item (account, container, or object).\\n\\n    See :py:mod:`swiftly.cli.post` for context usage information.\\n\\n    See :py:class:`CLIPost` for more information.\\n\\n    :param context: The :py:class:`swiftly.cli.context.CLIContext` to\\n        use.\\n    :param path: The path to the item to issue a POST for.\\n    :param body: The body send along with the POST.\\n    '\n    path = (path.lstrip('/') if path else '')\n    (status, reason, headers, contents) = (0, 'Unknown', {\n        \n    }, '')\n    with context.client_manager.with_client() as client:\n        if (not path):\n            (status, reason, headers, contents) = client.post_account(headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n            if ((status // 100) != 2):\n                raise ReturnCode(('posting account: %s %s' % (status, reason)))\n        elif ('/' not in path.rstrip('/')):\n            path = path.rstrip('/')\n            (status, reason, headers, contents) = client.post_container(path, headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n            if ((status // 100) != 2):\n                raise ReturnCode(('posting container %r: %s %s' % (path, status, reason)))\n        else:\n            (status, reason, headers, contents) = client.post_object(*path.split('/', 1), headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n            if ((status // 100) != 2):\n                raise ReturnCode(('posting object %r: %s %s' % (path, status, reason)))\n", "label": 0}
{"function": "\n\n@not_implemented_for('directed')\ndef generate_edgelist(G, delimiter=' ', data=True):\n    \"Generate a single line of the bipartite graph G in edge list format.\\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n       The graph is assumed to have node attribute `part` set to 0,1 representing\\n       the two graph parts\\n\\n    delimiter : string, optional\\n       Separator for node labels\\n\\n    data : bool or list of keys\\n       If False generate no edge data.  If True use a dictionary\\n       representation of edge data.  If a list of keys use a list of data\\n       values corresponding to the keys.\\n\\n    Returns\\n    -------\\n    lines : string\\n        Lines of data in adjlist format.\\n\\n    Examples\\n    --------\\n    >>> from networkx.algorithms import bipartite\\n    >>> G = nx.path_graph(4)\\n    >>> G.add_nodes_from([0,2], bipartite=0)\\n    >>> G.add_nodes_from([1,3], bipartite=1)\\n    >>> G[1][2]['weight'] = 3\\n    >>> G[2][3]['capacity'] = 12\\n    >>> for line in bipartite.generate_edgelist(G, data=False):\\n    ...     print(line)\\n    0 1\\n    2 1\\n    2 3\\n\\n    >>> for line in bipartite.generate_edgelist(G):\\n    ...     print(line)\\n    0 1 {}\\n    2 1 {'weight': 3}\\n    2 3 {'capacity': 12}\\n\\n    >>> for line in bipartite.generate_edgelist(G,data=['weight']):\\n    ...     print(line)\\n    0 1\\n    2 1 3\\n    2 3\\n    \"\n    try:\n        part0 = [n for (n, d) in G.node.items() if (d['bipartite'] == 0)]\n    except:\n        raise AttributeError('Missing node attribute `bipartite`')\n    if ((data is True) or (data is False)):\n        for n in part0:\n            for e in G.edges(n, data=data):\n                (yield delimiter.join(map(make_str, e)))\n    else:\n        for n in part0:\n            for (u, v, d) in G.edges(n, data=True):\n                e = [u, v]\n                try:\n                    e.extend((d[k] for k in data))\n                except KeyError:\n                    pass\n                (yield delimiter.join(map(make_str, e)))\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('grid_type', ALL_GRID_TYPES)\ndef test_append_two_quantities_invalid(self, grid_type):\n    g = self.grid[grid_type]\n    g['density'] = []\n    g['energy'] = []\n    g['density'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    assert g['density'][0]\n    assert g['density'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['density'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert g['energy'][0]\n    assert g['energy'][1]\n    assert g['energy'][(- 1)]\n    assert g['energy'][(- 2)]\n    with pytest.raises(IndexError) as exc:\n        assert g['energy'][2]\n    assert (exc_msg(exc) == 'list index out of range')\n    with pytest.raises(ValueError) as exc:\n        g.n_dust\n    assert (exc_msg(exc) == 'Not all dust lists in the grid have the same size')\n", "label": 1}
{"function": "\n\ndef add_task(self, task, raise_error=False):\n    '\\n        Add task to the task queue.\\n        '\n    if self.parser_mode:\n        self.parser_result_queue.put((task, None))\n        return\n    if (self.task_queue is None):\n        raise SpiderMisuseError('You should configure task queue before adding tasks. Use `setup_queue` method.')\n    if ((task.priority is None) or (not task.priority_is_custom)):\n        task.priority = self.generate_task_priority()\n        task.priority_is_custom = False\n    else:\n        task.priority_is_custom = True\n    try:\n        if (not task.url.startswith(('http://', 'https://', 'ftp://', 'file://', 'feed://'))):\n            if (self.base_url is None):\n                msg = ('Could not resolve relative URL because base_url is not specified. Task: %s, URL: %s' % (task.name, task.url))\n                raise SpiderError(msg)\n            else:\n                warn('Class attribute `Spider::base_url` is deprecated. Use Task objects with absolute URLs')\n                task.url = urljoin(self.base_url, task.url)\n                if task.grab_config:\n                    task.grab_config['url'] = task.url\n    except Exception as ex:\n        self.stat.collect('task-with-invalid-url', task.url)\n        if raise_error:\n            raise\n        else:\n            logger.error('', exc_info=ex)\n            return False\n    self.task_queue.put(task, task.priority, schedule_time=task.schedule_time)\n    return True\n", "label": 0}
{"function": "\n\ndef commit_dirtiness_flags(self):\n    '\\n        Updates any dirtiness flags in the database.\\n        '\n    if self.domain:\n        flags_to_save = self.get_flags_to_save()\n        if should_create_flags_on_submission(self.domain):\n            assert settings.UNIT_TESTING\n            all_touched_ids = (set(flags_to_save.keys()) | self.get_clean_owner_ids())\n            to_update = {f.owner_id: f for f in OwnershipCleanlinessFlag.objects.filter(domain=self.domain, owner_id__in=list(all_touched_ids))}\n            for owner_id in all_touched_ids:\n                if (owner_id not in to_update):\n                    flag = OwnershipCleanlinessFlag(domain=self.domain, owner_id=owner_id, is_clean=True)\n                    if (owner_id in flags_to_save):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                    flag.save()\n                else:\n                    flag = to_update[owner_id]\n                    if ((owner_id in flags_to_save) and (flag.is_clean or (not flag.hint))):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                        flag.save()\n        else:\n            flags_to_update = OwnershipCleanlinessFlag.objects.filter(Q(domain=self.domain), Q(owner_id__in=flags_to_save.keys()), (Q(is_clean=True) | Q(hint__isnull=True)))\n            for flag in flags_to_update:\n                flag.is_clean = False\n                flag.hint = flags_to_save[flag.owner_id]\n                flag.save()\n", "label": 1}
{"function": "\n\ndef _compute_generator_info(self):\n    \"\\n        Compute the generator's state variables as the union of live variables\\n        at all yield points.\\n        \"\n    gi = self.generator_info\n    for yp in gi.get_yield_points():\n        live_vars = set(self.block_entry_vars[yp.block])\n        weak_live_vars = set()\n        stmts = iter(yp.block.body)\n        for stmt in stmts:\n            if isinstance(stmt, ir.Assign):\n                if (stmt.value is yp.inst):\n                    break\n                live_vars.add(stmt.target.name)\n            elif isinstance(stmt, ir.Del):\n                live_vars.remove(stmt.value)\n        else:\n            assert 0, \"couldn't find yield point\"\n        for stmt in stmts:\n            if isinstance(stmt, ir.Del):\n                name = stmt.value\n                if (name in live_vars):\n                    live_vars.remove(name)\n                    weak_live_vars.add(name)\n            else:\n                break\n        yp.live_vars = live_vars\n        yp.weak_live_vars = weak_live_vars\n    st = set()\n    for yp in gi.get_yield_points():\n        st |= yp.live_vars\n        st |= yp.weak_live_vars\n    gi.state_vars = sorted(st)\n", "label": 1}
{"function": "\n\ndef __init__(self, source, filename=None, lineno=(- 1), lookup='strict', xform=None):\n    'Create the code object, either from a string, or from an AST node.\\n        \\n        :param source: either a string containing the source code, or an AST\\n                       node\\n        :param filename: the (preferably absolute) name of the file containing\\n                         the code\\n        :param lineno: the number of the line on which the code was found\\n        :param lookup: the lookup class that defines how variables are looked\\n                       up in the context; can be either \"strict\" (the default),\\n                       \"lenient\", or a custom lookup class\\n        :param xform: the AST transformer that should be applied to the code;\\n                      if `None`, the appropriate transformation is chosen\\n                      depending on the mode\\n        '\n    if isinstance(source, str):\n        self.source = source\n        node = _parse(source, mode=self.mode)\n    else:\n        assert isinstance(source, _ast.AST), ('Expected string or AST node, but got %r' % source)\n        self.source = '?'\n        if (self.mode == 'eval'):\n            node = _ast.Expression()\n            node.body = source\n        else:\n            node = _ast.Module()\n            node.body = [source]\n    self.ast = node\n    self.code = _compile(node, self.source, mode=self.mode, filename=filename, lineno=lineno, xform=xform)\n    if (lookup is None):\n        lookup = LenientLookup\n    elif isinstance(lookup, str):\n        lookup = {\n            'lenient': LenientLookup,\n            'strict': StrictLookup,\n        }[lookup]\n    self._globals = lookup.globals\n", "label": 0}
{"function": "\n\ndef transform_Assign(self, stmt):\n    lhs = stmt.lhs\n    lhs_class = lhs.__class__\n    rhs = self.transform_expr(stmt.rhs)\n    if (lhs_class is Tuple):\n        for (i, _) in enumerate(lhs.type.elt_types):\n            lhs_i = self.tuple_proj(lhs, i)\n            rhs_i = self.tuple_proj(rhs, i)\n            assert (lhs_i.__class__ not in (ArrayView, Tuple))\n            self.assign(lhs_i, rhs_i)\n        return None\n    elif (lhs_class is Index):\n        lhs = self.transform_Index(lhs)\n        if (lhs.__class__ is ArrayView):\n            copy_loop = self.array_copy(src=rhs, dest=lhs, return_stmt=True)\n            copy_loop = self.transform_stmt(copy_loop)\n            return copy_loop\n    elif ((lhs_class is Var) and (stmt.rhs.__class__ in (Slice, Struct, ArrayView, Tuple))):\n        self.bindings[lhs.name] = rhs\n    return Assign(lhs, rhs)\n", "label": 0}
{"function": "\n\ndef pil_from_ndarray(ndarray):\n    '\\n    Converts an ndarray to a PIL image.\\n\\n    Parameters\\n    ----------\\n    ndarray : ndarray\\n        An ndarray containing an image.\\n\\n    Returns\\n    -------\\n    pil : PIL Image\\n        A PIL Image containing the image.\\n    '\n    try:\n        if ((ndarray.dtype == 'float32') or (ndarray.dtype == 'float64')):\n            assert (ndarray.min() >= 0.0)\n            assert (ndarray.max() <= 1.0)\n            ndarray = np.cast['uint8']((ndarray * 255))\n            if ((len(ndarray.shape) == 3) and (ndarray.shape[2] == 1)):\n                ndarray = ndarray[:, :, 0]\n        ensure_Image()\n        rval = Image.fromarray(ndarray)\n        return rval\n    except Exception as e:\n        logger.exception('original exception: ')\n        logger.exception(e)\n        logger.exception('ndarray.dtype: {0}'.format(ndarray.dtype))\n        logger.exception('ndarray.shape: {0}'.format(ndarray.shape))\n        raise\n    assert False\n", "label": 0}
{"function": "\n\ndef test_SeqExprOp():\n    form = SeqFormula((n ** 2), (n, 0, 10))\n    per = SeqPer((1, 2, 3), (m, 5, 10))\n    s = SeqExprOp(form, per)\n    assert (s.gen == ((n ** 2), (1, 2, 3)))\n    assert (s.interval == Interval(5, 10))\n    assert (s.start == 5)\n    assert (s.stop == 10)\n    assert (s.length == 6)\n    assert (s.variables == (n, m))\n", "label": 0}
{"function": "\n\ndef make_app(repo_paths, site_name, use_smarthttp=False, htdigest_file=None, require_browser_auth=False, disable_push=False, unauthenticated_push=False, ctags_policy='none'):\n    '\\n    Returns a WSGI app with all the features (smarthttp, authentication)\\n    already patched in.\\n\\n    :param repo_paths: List of paths of repositories to serve.\\n    :param site_name: Name of the Web site (e.g. \"John Doe\\'s Git Repositories\")\\n    :param use_smarthttp: Enable Git Smart HTTP mode, which makes it possible to\\n        pull from the served repositories. If `htdigest_file` is set as well,\\n        also allow to push for authenticated users.\\n    :param require_browser_auth: Require HTTP authentication according to the\\n        credentials in `htdigest_file` for ALL access to the Web interface.\\n        Requires the `htdigest_file` option to be set.\\n    :param disable_push: Disable push support. This is required in case both\\n        `use_smarthttp` and `require_browser_auth` (and thus `htdigest_file`)\\n        are set, but push should not be supported.\\n    :param htdigest_file: A *file-like* object that contains the HTTP auth credentials.\\n    :param unauthenticated_push: Allow push\\'ing without authentication. DANGER ZONE!\\n    :param ctags_policy: The ctags policy to use, may be one of:\\n        - \\'none\\': never use ctags\\n        - \\'tags-and-branches\\': use ctags for revisions that are the HEAD of\\n          a tag or branc\\n        - \\'ALL\\': use ctags for all revisions, may result in high server load!\\n    '\n    if unauthenticated_push:\n        if (not use_smarthttp):\n            raise ValueError(\"'unauthenticated_push' set without 'use_smarthttp'\")\n        if disable_push:\n            raise ValueError(\"'unauthenticated_push' set with 'disable_push'\")\n        if require_browser_auth:\n            raise ValueError(\"Incompatible options 'unauthenticated_push' and 'require_browser_auth'\")\n    if (htdigest_file and (not (require_browser_auth or use_smarthttp))):\n        raise ValueError(\"'htdigest_file' set without 'use_smarthttp' or 'require_browser_auth'\")\n    app = Klaus(repo_paths, site_name, use_smarthttp, ctags_policy)\n    app.wsgi_app = utils.ProxyFix(app.wsgi_app)\n    if use_smarthttp:\n        dulwich_backend = dulwich.server.DictBackend(dict(((('/' + name), repo) for (name, repo) in app.repos.items())))\n        dulwich_wrapped_app = dulwich.web.make_wsgi_chain(backend=dulwich_backend, fallback_app=app.wsgi_app)\n        dulwich_wrapped_app = utils.ProxyFix(dulwich_wrapped_app)\n        PATTERN = '^/[^/]+/(info/refs\\\\?service=git-receive-pack|git-receive-pack)$'\n        if unauthenticated_push:\n            app.wsgi_app = dulwich_wrapped_app\n        elif (htdigest_file and (not disable_push)):\n            if require_browser_auth:\n                app.wsgi_app = dulwich_wrapped_app\n            else:\n                app.wsgi_app = httpauth.DigestFileHttpAuthMiddleware(htdigest_file, wsgi_app=dulwich_wrapped_app, routes=[PATTERN])\n        else:\n            app.wsgi_app = httpauth.AlwaysFailingAuthMiddleware(wsgi_app=dulwich_wrapped_app, routes=[PATTERN])\n    if require_browser_auth:\n        app.wsgi_app = httpauth.DigestFileHttpAuthMiddleware(htdigest_file, wsgi_app=app.wsgi_app)\n    return app\n", "label": 1}
{"function": "\n\ndef save(self, **kwargs):\n    if ('encode_attachments' not in kwargs):\n        kwargs['encode_attachments'] = False\n    RETRIES = 10\n    SLEEP = 0.5\n    tries = 0\n    while True:\n        try:\n            return super(XFormInstance, self).save(**kwargs)\n        except PreconditionFailed:\n            if (tries == 0):\n                logging.error(('doc %s got a precondition failed' % self._id))\n            if (tries < RETRIES):\n                tries += 1\n                time.sleep(SLEEP)\n            else:\n                raise\n", "label": 0}
{"function": "\n\ndef _new_actor(self, actor_type, actor_id=None, credentials=None):\n    \"Return a 'bare' actor of actor_type, raises an exception on failure.\"\n    if (credentials is not None):\n        sec = Security()\n        sec.set_principal(credentials)\n        sec.authenticate_principal()\n    else:\n        sec = None\n    (found, is_primitive, class_) = ActorStore(security=sec).lookup(actor_type)\n    if (not found):\n        _log.analyze(self.node.id, '+ NOT FOUND CREATE SHADOW ACTOR', {\n            'class': class_,\n        })\n        found = True\n        is_primitive = True\n        class_ = ShadowActor\n    if ((not found) or (not is_primitive)):\n        _log.error(('Requested actor %s is not available' % actor_type))\n        raise Exception('ERROR_NOT_FOUND')\n    try:\n        a = class_(actor_type, actor_id=actor_id)\n    except Exception as e:\n        _log.exception('')\n        _log.error((\"The actor %s(%s) can't be instantiated.\" % (actor_type, class_.__init__)))\n        raise e\n    try:\n        a.set_credentials(credentials, security=sec)\n        a._calvinsys = self.node.calvinsys()\n        a.check_requirements()\n    except Exception as e:\n        _log.exception('Catched new from state')\n        _log.analyze(self.node.id, '+ FAILED REQS CREATE SHADOW ACTOR', {\n            'class': class_,\n        })\n        a = ShadowActor(actor_type, actor_id=actor_id)\n        a.set_credentials(credentials, security=sec)\n        a._calvinsys = self.node.calvinsys()\n    return a\n", "label": 0}
{"function": "\n\ndef WordsTableCreate(maxDictionaryLength=1000000, maxMemoryStorage=20000000):\n    \"\\n    This function reads already-tokenized words from sys.stdin,\\n    and uses them to count all the words in the document.\\n\\n    The only real challenge here is that there may be more unique words than can fit\\n    in memory, so we need some kind of buffering on disk.\\n\\n    It does this by maintaining a dictionary in memory, and periodically \\n    writing the least used portions of that to disk; once it's read everything,\\n    it writes everything to disk, sorts the list of all words so that \\n    \"\n    database = open('.bookworm/texts/wordlist/raw.txt', 'w')\n    start_time = timeit.default_timer()\n    n = 1\n    keepThreshold = 2\n    wordcounts = dict()\n    for row in sys.stdin:\n        for item in row.split(' '):\n            n += 1\n            if ((n % 100000000) == 0):\n                elapsed = (timeit.default_timer() - start_time)\n                logging.info((((((str((float(len(wordcounts)) / 1000000)) + ' million distinct entries at ') + str((float(n) / 1000000000))) + ' billion words---') + str(elapsed)) + ' seconds since last print'))\n                start_time = timeit.default_timer()\n            item = item.rstrip('\\n')\n            try:\n                wordcounts[item] += 1\n            except KeyError:\n                wordcounts[item] = 1\n            while (len(wordcounts) > maxMemoryStorage):\n                logging.info((('exporting to disk at ' + str((float(len(wordcounts)) / 1000000))) + ' million words'))\n                wordcounts = exportToDisk(wordcounts, diskFile=database, keepThreshold=keepThreshold)\n                logging.info(((\"after export, it's \" + str((float(len(wordcounts)) / 1000000))) + ' million words'))\n                if (len(wordcounts) > (0.8 * float(maxMemoryStorage))):\n                    keepThreshold = (keepThreshold * 2)\n                    logging.info(('upping the keep threshold to ' + str(keepThreshold)))\n    nothing = exportToDisk(wordcounts, diskFile=database, keepThreshold=float('inf'))\n    database.close()\n    sortWordlist(maxDictionaryLength=maxDictionaryLength)\n", "label": 0}
{"function": "\n\ndef test_fermionoperator():\n    c = FermionOp('c')\n    d = FermionOp('d')\n    assert isinstance(c, FermionOp)\n    assert isinstance(Dagger(c), FermionOp)\n    assert c.is_annihilation\n    assert (not Dagger(c).is_annihilation)\n    assert (FermionOp('c') == FermionOp('c'))\n    assert (FermionOp('c') != FermionOp('d'))\n    assert (FermionOp('c', True) != FermionOp('c', False))\n    assert (AntiCommutator(c, Dagger(c)).doit() == 1)\n    assert (AntiCommutator(c, Dagger(d)).doit() == ((c * Dagger(d)) + (Dagger(d) * c)))\n", "label": 0}
{"function": "\n\ndef _fetch(self, model_cls, query=None, sort=None):\n    'Fetch the objects of type `model_cls` matching the given\\n        query. The query may be given as a string, string sequence, a\\n        Query object, or None (to fetch everything). `sort` is an\\n        `Sort` object.\\n        '\n    query = (query or TrueQuery())\n    sort = (sort or NullSort())\n    (where, subvals) = query.clause()\n    order_by = sort.order_clause()\n    sql = 'SELECT * FROM {0} WHERE {1} {2}'.format(model_cls._table, (where or '1'), ('ORDER BY {0}'.format(order_by) if order_by else ''))\n    with self.transaction() as tx:\n        rows = tx.query(sql, subvals)\n    return Results(model_cls, rows, self, (None if where else query), (sort if sort.is_slow() else None))\n", "label": 0}
{"function": "\n\ndef test_getting_user_info_by_authorized_user(flask_app_client, regular_user, admin_user):\n    with flask_app_client.login(admin_user, auth_scopes=('users:read',)):\n        response = flask_app_client.get(('/api/v1/users/%d' % regular_user.id))\n    assert (response.status_code == 200)\n    assert (response.content_type == 'application/json')\n    assert isinstance(response.json, dict)\n    assert (set(response.json.keys()) >= {'id', 'username'})\n    assert ('password' not in response.json.keys())\n", "label": 0}
{"function": "\n\ndef __init__(self, subscription, publication, args=None, kwargs=None, payload=None, publisher=None, publisher_authid=None, publisher_authrole=None, topic=None, enc_algo=None, enc_key=None, enc_serializer=None):\n    '\\n\\n        :param subscription: The subscription ID this event is dispatched under.\\n        :type subscription: int\\n        :param publication: The publication ID of the dispatched event.\\n        :type publication: int\\n        :param args: Positional values for application-defined exception.\\n           Must be serializable using any serializers in use.\\n        :type args: list or tuple or None\\n        :param kwargs: Keyword values for application-defined exception.\\n           Must be serializable using any serializers in use.\\n        :param payload: Alternative, transparent payload. If given, `args` and `kwargs` must be left unset.\\n        :type payload: unicode or bytes\\n        :type kwargs: dict or None\\n        :param publisher: The WAMP session ID of the pubisher. Only filled if pubisher is disclosed.\\n        :type publisher: None or int\\n        :param publisher_authid: The WAMP authid of the pubisher. Only filled if pubisher is disclosed.\\n        :type publisher_authid: None or unicode\\n        :param publisher_authrole: The WAMP authrole of the pubisher. Only filled if pubisher is disclosed.\\n        :type publisher_authrole: None or unicode\\n        :param topic: For pattern-based subscriptions, the event MUST contain the actual topic published to.\\n        :type topic: unicode or None\\n        :param enc_algo: If using payload encryption, the algorithm used (currently, only \"cryptobox\" is valid).\\n        :type enc_algo: unicode\\n        :param enc_key: If using payload encryption, the message encryption key.\\n        :type enc_key: unicode or binary\\n        :param enc_serializer: If using payload encryption, the encrypted payload object serializer.\\n        :type enc_serializer: unicode\\n        '\n    assert (type(subscription) in six.integer_types)\n    assert (type(publication) in six.integer_types)\n    assert ((args is None) or (type(args) in [list, tuple]))\n    assert ((kwargs is None) or (type(kwargs) == dict))\n    assert ((payload is None) or (type(payload) in [six.text_type, six.binary_type]))\n    assert ((payload is None) or ((payload is not None) and (args is None) and (kwargs is None)))\n    assert ((publisher is None) or (type(publisher) in six.integer_types))\n    assert ((publisher_authid is None) or (type(publisher_authid) == six.text_type))\n    assert ((publisher_authrole is None) or (type(publisher_authrole) == six.text_type))\n    assert ((topic is None) or (type(topic) == six.text_type))\n    assert ((enc_algo is None) or (enc_algo in [PAYLOAD_ENC_CRYPTO_BOX]))\n    assert ((enc_key is None) or (type(enc_key) in [six.text_type, six.binary_type]))\n    assert ((enc_serializer is None) or (enc_serializer in ['json', 'msgpack', 'cbor', 'ubjson']))\n    assert (((enc_algo is None) and (enc_key is None) and (enc_serializer is None)) or ((enc_algo is not None) and (payload is not None)))\n    Message.__init__(self)\n    self.subscription = subscription\n    self.publication = publication\n    self.args = args\n    self.kwargs = kwargs\n    self.payload = payload\n    self.publisher = publisher\n    self.publisher_authid = publisher_authid\n    self.publisher_authrole = publisher_authrole\n    self.topic = topic\n    self.enc_algo = enc_algo\n    self.enc_key = enc_key\n    self.enc_serializer = enc_serializer\n", "label": 1}
{"function": "\n\ndef test_percentile_algorithm_extremes():\n    ' Unsorted so it is expected to sort it for us. '\n    snapshot = PercentileSnapshot(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 800, 768, 657, 700, 867)\n    print('0.01', snapshot.percentile(0.01))\n    print('10th', snapshot.percentile(10))\n    print('Median', snapshot.percentile(50))\n    print('75th', snapshot.percentile(75))\n    print('90th', snapshot.percentile(90))\n    print('99th', snapshot.percentile(99))\n    print('99.5th', snapshot.percentile(99.5))\n    print('99.99', snapshot.percentile(99.99))\n    assert (snapshot.percentile(50) == 2)\n    assert (snapshot.percentile(10) == 2)\n    assert (snapshot.percentile(75) == 2)\n    if (snapshot.percentile(95) < 600):\n        msg = 'We expect 90th to be over 600 to show the extremes but got: {}'\n        pytest.fail(msg.format(snapshot.percentile(95)))\n    if (snapshot.percentile(99) < 600):\n        msg = 'We expect 99th to be over 600 to show the extremes but got: {}'\n        pytest.fail(msg.format(snapshot.percentile(99)))\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    r = []\n    try:\n        for el in self:\n            c = el.get('class')\n            c = ((c and ('.' + '.'.join(c.split(' ')))) or '')\n            id = el.get('id')\n            id = ((id and ('#' + id)) or '')\n            r.append(('<%s%s%s>' % (el.tag, id, c)))\n        return (('[' + ', '.join(r)) + ']')\n    except AttributeError:\n        if PY3k:\n            return list.__repr__(self)\n        else:\n            for el in self:\n                if isinstance(el, unicode):\n                    r.append(el.encode('utf-8'))\n                else:\n                    r.append(el)\n            return repr(r)\n", "label": 0}
{"function": "\n\ndef _configure_port_entries(self, port, vlan_id, device_id, host_id, vni, is_provider_vlan):\n    'Create a nexus switch entry.\\n\\n        if needed, create a VLAN in the appropriate switch or port and\\n        configure the appropriate interfaces for this VLAN.\\n\\n        Called during update postcommit port event.\\n        '\n    connections = self._get_active_port_connections(port, host_id)\n    vlan_already_created = []\n    starttime = time.time()\n    for (switch_ip, intf_type, nexus_port, is_native) in connections:\n        all_bindings = nxos_db.get_nexusvlan_binding(vlan_id, switch_ip)\n        previous_bindings = [row for row in all_bindings if (row.instance_id != device_id)]\n        if (previous_bindings and (switch_ip in vlan_already_created)):\n            duplicate_type = const.DUPLICATE_VLAN\n        else:\n            vlan_already_created.append(switch_ip)\n            duplicate_type = const.NO_DUPLICATE\n        port_starttime = time.time()\n        try:\n            self._configure_port_binding(is_provider_vlan, duplicate_type, is_native, switch_ip, vlan_id, intf_type, nexus_port, vni)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                self.driver.capture_and_print_timeshot(port_starttime, 'port_configerr', switch=switch_ip)\n                self.driver.capture_and_print_timeshot(starttime, 'configerr', switch=switch_ip)\n        self.driver.capture_and_print_timeshot(port_starttime, 'port_config', switch=switch_ip)\n    self.driver.capture_and_print_timeshot(starttime, 'config')\n", "label": 0}
{"function": "\n\ndef handle(self, environ, start_response, url_map, match, request_id, request_type):\n    'Serves this request by forwarding it to the runtime process.\\n\\n    Args:\\n      environ: An environ dict for the request as defined in PEP-333.\\n      start_response: A function with semantics defined in PEP-333.\\n      url_map: An appinfo.URLMap instance containing the configuration for the\\n          handler matching this request.\\n      match: A re.MatchObject containing the result of the matched URL pattern.\\n      request_id: A unique string id associated with the request.\\n      request_type: The type of the request. See instance.*_REQUEST module\\n          constants.\\n\\n    Yields:\\n      A sequence of strings containing the body of the HTTP response.\\n    '\n    environ[http_runtime_constants.SCRIPT_HEADER] = match.expand(url_map.script)\n    if (request_type == instance.BACKGROUND_REQUEST):\n        environ[http_runtime_constants.REQUEST_TYPE_HEADER] = 'background'\n    elif (request_type == instance.SHUTDOWN_REQUEST):\n        environ[http_runtime_constants.REQUEST_TYPE_HEADER] = 'shutdown'\n    elif (request_type == instance.INTERACTIVE_REQUEST):\n        environ[http_runtime_constants.REQUEST_TYPE_HEADER] = 'interactive'\n    for name in http_runtime_constants.ENVIRONS_TO_PROPAGATE:\n        if ((http_runtime_constants.INTERNAL_ENVIRON_PREFIX + name) not in environ):\n            value = environ.get(name, None)\n            if (value is not None):\n                environ[(http_runtime_constants.INTERNAL_ENVIRON_PREFIX + name)] = value\n    headers = util.get_headers_from_environ(environ)\n    if environ.get('QUERY_STRING'):\n        url = ('%s?%s' % (urllib.quote(environ['PATH_INFO']), environ['QUERY_STRING']))\n    else:\n        url = urllib.quote(environ['PATH_INFO'])\n    if ('CONTENT_LENGTH' in environ):\n        headers['CONTENT-LENGTH'] = environ['CONTENT_LENGTH']\n        data = environ['wsgi.input'].read(int(environ['CONTENT_LENGTH']))\n    else:\n        data = ''\n    cookies = environ.get('HTTP_COOKIE')\n    (user_email, admin, user_id) = login.get_user_info(cookies)\n    if user_email:\n        (nickname, organization) = user_email.split('@', 1)\n    else:\n        nickname = ''\n        organization = ''\n    headers[http_runtime_constants.REQUEST_ID_HEADER] = request_id\n    headers[(http_runtime_constants.INTERNAL_HEADER_PREFIX + 'User-Id')] = user_id\n    headers[(http_runtime_constants.INTERNAL_HEADER_PREFIX + 'User-Email')] = user_email\n    headers[(http_runtime_constants.INTERNAL_HEADER_PREFIX + 'User-Is-Admin')] = str(int(admin))\n    headers[(http_runtime_constants.INTERNAL_HEADER_PREFIX + 'User-Nickname')] = nickname\n    headers[(http_runtime_constants.INTERNAL_HEADER_PREFIX + 'User-Organization')] = organization\n    headers['X-AppEngine-Country'] = 'ZZ'\n    connection = httplib.HTTPConnection(self._host, self._port)\n    with contextlib.closing(connection):\n        try:\n            connection.connect()\n            connection.request(environ.get('REQUEST_METHOD', 'GET'), url, data, dict(headers.items()))\n            response = connection.getresponse()\n            headers = []\n            for name in response.msg:\n                for value in response.msg.getheaders(name):\n                    headers.append((name, value))\n            response_headers = wsgiref.headers.Headers(headers)\n            error_file = self._get_error_file()\n            if (error_file and (http_runtime_constants.ERROR_CODE_HEADER in response_headers)):\n                try:\n                    with open(error_file) as f:\n                        content = f.read()\n                except IOError:\n                    content = 'Failed to load error handler'\n                    logging.exception('failed to load error file: %s', error_file)\n                start_response('500 Internal Server Error', [('Content-Type', 'text/html'), ('Content-Length', str(len(content)))])\n                (yield content)\n                return\n            del response_headers[http_runtime_constants.ERROR_CODE_HEADER]\n            start_response(('%s %s' % (response.status, response.reason)), response_headers.items())\n            block = response.read(512)\n            while block:\n                (yield block)\n                block = response.read(512)\n        except Exception:\n            with self._process_lock:\n                if (self._process and (self._process.poll() is not None)):\n                    message = ('the runtime process for the instance running on port %d has unexpectedly quit; exiting the development server' % self._port)\n                    logging.error(message)\n                    start_response('500 Internal Server Error', [('Content-Type', 'text/plain'), ('Content-Length', str(len(message)))])\n                    shutdown.async_quit()\n                    (yield message)\n                else:\n                    raise\n", "label": 1}
{"function": "\n\ndef show(self, req, id):\n    s = self.os_helper.get_server(req, id)\n    flavor = self.os_helper.get_flavor(req, s['flavor']['id'])\n    res_tpl = templates.OpenStackResourceTemplate(flavor['id'], flavor['name'], flavor['vcpus'], flavor['ram'], flavor['disk'])\n    img_id = s['image']['id']\n    try:\n        image = self.os_helper.get_image(req, img_id)\n    except webob.exc.HTTPNotFound:\n        image = {\n            'id': img_id,\n            'name': (\"None (Image with ID '%s' not found)\" % img_id),\n        }\n    os_tpl = templates.OpenStackOSTemplate(image['id'], image['name'])\n    comp = compute.ComputeResource(title=s['name'], id=s['id'], cores=flavor['vcpus'], hostname=s['name'], memory=flavor['ram'], state=helpers.vm_state(s['status']), mixins=[os_tpl, res_tpl])\n    vols = self.os_helper.get_server_volumes_link(req, s['id'])\n    for v in vols:\n        st = storage.StorageResource(title='storage', id=v['volumeId'])\n        comp.add_link(storage_link.StorageLink(comp, st, deviceid=v['device']))\n    addresses = s.get('addresses', {\n        \n    })\n    if addresses:\n        for addr_set in addresses.values():\n            for addr in addr_set:\n                comp.add_link(_create_network_link(addr, comp))\n    return [comp]\n", "label": 0}
{"function": "\n\ndef stomp_prompt(self, changed_bufs, missing_bufs, new_files, ignored, cb):\n\n    def pluralize(arg):\n        return (((arg != 1) and 's') or '')\n    overwrite_local = ''\n    overwrite_remote = ''\n    missing = [buf['path'] for buf in missing_bufs]\n    changed = [buf['path'] for buf in changed_bufs]\n    to_upload = set((new_files + changed)).difference(set(ignored))\n    to_remove = (missing + ignored)\n    to_fetch = (changed + missing)\n    to_upload_len = len(to_upload)\n    to_remove_len = len(to_remove)\n    remote_len = (to_remove_len + to_upload_len)\n    to_fetch_len = len(to_fetch)\n    if (not to_fetch):\n        overwrite_local = 'Fetch nothing'\n    elif (to_fetch_len < 5):\n        overwrite_local = ('Fetch %s' % ', '.join(to_fetch))\n    else:\n        overwrite_local = ('Fetch %s file%s' % (to_fetch_len, pluralize(to_fetch_len)))\n    if (to_upload_len < 5):\n        to_upload_str = ('upload %s' % ', '.join(to_upload))\n    else:\n        to_upload_str = ('upload %s' % to_upload_len)\n    if (to_remove_len < 5):\n        to_remove_str = ('remove %s' % ', '.join(to_remove))\n    else:\n        to_remove_str = ('remove %s' % to_remove_len)\n    if to_upload:\n        overwrite_remote += to_upload_str\n        if to_remove:\n            overwrite_remote += ' and '\n    if to_remove:\n        overwrite_remote += to_remove_str\n    if ((remote_len >= 5) and overwrite_remote):\n        overwrite_remote += ' files'\n    overwrite_remote = overwrite_remote.capitalize()\n    action = 'Overwrite'\n    choices = ['remote', 'local', 'cancel']\n    prompt = 'The workspace is out of sync. You may:\\n'\n    prompt += ('\\t%s %s (r)emote file%s (%s)\\n' % (action, remote_len, pluralize(remote_len), overwrite_remote))\n    prompt += ('\\t%s %s (l)ocal file%s (%s)\\n' % (action, to_fetch_len, pluralize(to_fetch_len), overwrite_local))\n    prompt += '\\t(c)ancel\\n'\n    choice = editor.vim_choice(prompt, choices[0], choices)\n    try:\n        return cb(choices.index(choice))\n    except ValueError:\n        return cb((- 1))\n", "label": 1}
{"function": "\n\ndef load_middleware(self):\n    '\\n        Populate middleware lists from settings.MIDDLEWARE_CLASSES.\\n\\n        Must be called after the environment is fixed (see __call__ in subclasses).\\n        '\n    self._view_middleware = []\n    self._template_response_middleware = []\n    self._response_middleware = []\n    self._exception_middleware = []\n    request_middleware = []\n    for middleware_path in settings.MIDDLEWARE_CLASSES:\n        mw_class = import_string(middleware_path)\n        try:\n            mw_instance = mw_class()\n        except MiddlewareNotUsed as exc:\n            if settings.DEBUG:\n                if six.text_type(exc):\n                    logger.debug('MiddlewareNotUsed(%r): %s', middleware_path, exc)\n                else:\n                    logger.debug('MiddlewareNotUsed: %r', middleware_path)\n            continue\n        if hasattr(mw_instance, 'process_request'):\n            request_middleware.append(mw_instance.process_request)\n        if hasattr(mw_instance, 'process_view'):\n            self._view_middleware.append(mw_instance.process_view)\n        if hasattr(mw_instance, 'process_template_response'):\n            self._template_response_middleware.insert(0, mw_instance.process_template_response)\n        if hasattr(mw_instance, 'process_response'):\n            self._response_middleware.insert(0, mw_instance.process_response)\n        if hasattr(mw_instance, 'process_exception'):\n            self._exception_middleware.insert(0, mw_instance.process_exception)\n    self._request_middleware = request_middleware\n", "label": 0}
{"function": "\n\ndef salvage_broken_user_settings_document(document):\n    if ((not document['access_token']) or (not document['dropbox_id'])):\n        return False\n    if ((not document['owner']) or (not User.load(document['owner']).is_active)):\n        return False\n    if document['deleted']:\n        return False\n    if ((not document.get('dropbox_info')) or (not document['dropbox_info']['display_name'])):\n        logger.info('Attempting dropbox_info population for document (id:{0})'.format(document['_id']))\n        client = DropboxClient(document['access_token'])\n        document['dropbox_info'] = {\n            \n        }\n        try:\n            database['dropboxusersettings'].find_and_modify({\n                '_id': document['_id'],\n            }, {\n                '$set': {\n                    'dropbox_info': client.account_info(),\n                },\n            })\n        except Exception:\n            return True\n        else:\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef _save(filepath, obj):\n    '\\n    .. todo::\\n\\n        WRITEME\\n    '\n    try:\n        import joblib\n        joblib_available = True\n    except ImportError:\n        joblib_available = False\n    if filepath.endswith('.npy'):\n        np.save(filepath, obj)\n        return\n    save_dir = os.path.dirname(filepath)\n    if (save_dir == ''):\n        save_dir = '.'\n    if (not os.path.exists(save_dir)):\n        os.makedirs(save_dir)\n    if (os.path.exists(save_dir) and (not os.path.isdir(save_dir))):\n        raise IOError(('save path %s exists, not a directory' % save_dir))\n    elif (not os.access(save_dir, os.W_OK)):\n        raise IOError(('permission error creating %s' % filepath))\n    try:\n        if (joblib_available and filepath.endswith('.joblib')):\n            joblib.dump(obj, filepath)\n        else:\n            if filepath.endswith('.joblib'):\n                warnings.warn('Warning: .joblib suffix specified but joblib unavailable. Using ordinary pickle.')\n            with open(filepath, 'wb') as filehandle:\n                cPickle.dump(obj, filehandle, get_pickle_protocol())\n    except Exception as e:\n        logger.exception('cPickle has failed to write an object to {0}'.format(filepath))\n        if (str(e).find('maximum recursion depth exceeded') != (- 1)):\n            raise\n        try:\n            logger.info('retrying with pickle')\n            with open(filepath, 'wb') as f:\n                pickle.dump(obj, f)\n        except Exception as e2:\n            if ((str(e) == '') and (str(e2) == '')):\n                logger.exception('neither cPickle nor pickle could write to {0}'.format(filepath))\n                logger.exception('moreover, neither of them raised an exception that can be converted to a string')\n                logger.exception('now re-attempting to write with cPickle outside the try/catch loop so you can see if it prints anything when it dies')\n                with open(filepath, 'wb') as f:\n                    cPickle.dump(obj, f, get_pickle_protocol())\n                logger.info('Somehow or other, the file write worked once we quit using the try/catch.')\n            else:\n                if (str(e2) == 'env'):\n                    raise\n                import pdb\n                tb = pdb.traceback.format_exc()\n                reraise_as(IOError(((((((((str(obj) + ' could not be written to ') + str(filepath)) + ' by cPickle due to ') + str(e)) + ' nor by pickle due to ') + str(e2)) + '. \\nTraceback ') + tb)))\n        logger.warning('{0} was written by pickle instead of cPickle, due to {1} (perhaps your object is really big?)'.format(filepath, e))\n", "label": 1}
{"function": "\n\ndef test_loader_channels():\n    (n_samples_trace, n_channels) = (1000, 10)\n    n_samples_waveforms = 20\n    traces = artificial_traces(n_samples_trace, n_channels)\n    loader = WaveformLoader(traces, n_samples_waveforms=n_samples_waveforms)\n    loader.traces = traces\n    channels = [2, 5, 7]\n    loader.channels = channels\n    assert (loader.channels == channels)\n    assert (loader[500].shape == (1, n_samples_waveforms, 3))\n    assert (loader[[500, 501, 600, 300]].shape == (4, n_samples_waveforms, 3))\n    assert (loader[3].shape == (1, n_samples_waveforms, 3))\n    assert (loader[995].shape == (1, n_samples_waveforms, 3))\n    with raises(NotImplementedError):\n        loader[500:510]\n", "label": 0}
{"function": "\n\ndef layer_test(layer_cls, kwargs={\n    \n}, input_shape=None, input_dtype=None, input_data=None, expected_output=None, expected_output_dtype=None):\n    'Test routine for a layer with a single input tensor\\n    and single output tensor.\\n    '\n    if (input_data is None):\n        assert input_shape\n        if (not input_dtype):\n            input_dtype = K.floatx()\n        input_data = (10 * np.random.random(input_shape)).astype(input_dtype)\n    elif (input_shape is None):\n        input_shape = input_data.shape\n    if (expected_output_dtype is None):\n        expected_output_dtype = input_dtype\n    layer = layer_cls(**kwargs)\n    weights = layer.get_weights()\n    layer.set_weights(weights)\n    if ('weights' in inspect.getargspec(layer_cls.__init__)):\n        kwargs['weights'] = weights\n        layer = layer_cls(**kwargs)\n    x = Input(shape=input_shape[1:], dtype=input_dtype)\n    y = layer(x)\n    assert (K.dtype(y) == expected_output_dtype)\n    model = Model(input=x, output=y)\n    model.compile('rmsprop', 'mse')\n    expected_output_shape = layer.get_output_shape_for(input_shape)\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    assert (expected_output_shape == actual_output_shape)\n    if (expected_output is not None):\n        assert_allclose(actual_output, expected_output, rtol=0.001)\n    model_config = model.get_config()\n    model = Model.from_config(model_config)\n    model.compile('rmsprop', 'mse')\n    layer_config = layer.get_config()\n    layer_config['batch_input_shape'] = input_shape\n    layer = layer.__class__.from_config(layer_config)\n    model = Sequential()\n    model.add(layer)\n    model.compile('rmsprop', 'mse')\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    assert (expected_output_shape == actual_output_shape)\n    if (expected_output is not None):\n        assert_allclose(actual_output, expected_output, rtol=0.001)\n    json_model = model.to_json()\n    model = model_from_json(json_model)\n    return actual_output\n", "label": 1}
{"function": "\n\ndef finalize(genomes, env):\n    'Provide symlinks back to reference genomes so tophat avoids generating FASTA genomes.\\n    '\n    genome_dir = os.path.join(env.data_files, 'genomes')\n    for (orgname, gid, manager) in genomes:\n        org_dir = os.path.join(genome_dir, orgname)\n        for aligner in ['bowtie', 'bowtie2']:\n            aligner_dir = os.path.join(org_dir, gid, aligner)\n            if env.safe_exists(aligner_dir):\n                with cd(aligner_dir):\n                    for ext in ['', '.fai']:\n                        orig_seq = os.path.join(os.pardir, 'seq', ('%s.fa%s' % (gid, ext)))\n                        if (env.safe_exists(orig_seq) and (not env.safe_exists(os.path.basename(orig_seq)))):\n                            env.safe_run(('ln -sf %s' % orig_seq))\n", "label": 0}
{"function": "\n\n@login_required\ndef profileupdaterequest_details(request, request_id):\n    update_request = get_object_or_404(ProfileUpdateRequest, pk=request_id)\n    person_selected = False\n    person = None\n    form = None\n    try:\n        person = Person.objects.get(email=update_request.email)\n    except Person.DoesNotExist:\n        try:\n            person = Person.objects.get(personal=update_request.personal, family=update_request.family)\n        except (Person.DoesNotExist, Person.MultipleObjectsReturned):\n            try:\n                form = PersonLookupForm(request.GET)\n                person = Person.objects.get(pk=int(request.GET['person_1']))\n                person_selected = True\n            except KeyError:\n                person = None\n                form = PersonLookupForm()\n            except (ValueError, Person.DoesNotExist):\n                person = None\n    if person:\n        person.has_instructor_badge = Award.objects.filter(badge__in=Badge.objects.instructor_badges(), person=person).exists()\n    try:\n        airport = Airport.objects.get(iata=update_request.airport_iata)\n    except Airport.DoesNotExist:\n        airport = None\n    context = {\n        'title': 'Instructor profile update request #{}'.format(update_request.pk),\n        'new': update_request,\n        'old': person,\n        'person_form': form,\n        'person_selected': person_selected,\n        'form_helper': bootstrap_helper_get,\n        'airport': airport,\n    }\n    return render(request, 'workshops/profileupdaterequest.html', context)\n", "label": 0}
{"function": "\n\n@weblab_api.route_webclient('/labs/<category_name>/<experiment_name>/')\ndef lab(category_name, experiment_name):\n    '\\n    Renders a specific laboratory.\\n    '\n    federated_reservation_id = session.get('reservation_id')\n    federated_mode = (federated_reservation_id is not None)\n    if federated_mode:\n        back_url = session.get('back_url')\n    else:\n        back_url = None\n    experiment = None\n    if federated_mode:\n        weblab_api.ctx.reservation_id = federated_reservation_id\n        try:\n            experiment = _get_experiment_data(weblab_api.api.get_reservation_experiment_info())\n            reservation_status = weblab_api.api.get_reservation_status()\n        except SessionNotFoundError:\n            session.pop('reservation_id', None)\n            session.pop('back_url', None)\n            federated_mode = False\n            try:\n                experiment_list = weblab_api.api.list_experiments(experiment_name, category_name)\n            except SessionNotFoundError:\n                return render_template('webclient/error.html', error_message=gettext('The reservation you used has expired.'), federated_mode=True, back_url=back_url)\n        else:\n            if ((experiment is not None) and (reservation_status is not None) and (experiment['type'] == 'redirect')):\n                if (reservation_status.status == Reservation.CONFIRMED):\n                    local_url = reservation_status.url\n                    if (local_url is not None):\n                        return redirect(local_url.replace('TIME_REMAINING', unicode(local_url.time)))\n    if (experiment is None):\n        try:\n            experiment_list = weblab_api.api.list_experiments(experiment_name, category_name)\n        except SessionNotFoundError:\n            flash(gettext('You are not logged in'), 'danger')\n            return redirect(url_for('.login', next=request.url))\n        for exp_allowed in experiment_list:\n            if ((exp_allowed.experiment.name == experiment_name) and (exp_allowed.experiment.category.name == category_name)):\n                experiment = _get_experiment(exp_allowed)\n        if (experiment is None):\n            if weblab_api.db.check_experiment_exists(experiment_name, category_name):\n                flash(gettext(\"You don't have permission on this laboratory\"), 'danger')\n            else:\n                flash(gettext('Experiment does not exist'), 'danger')\n            return redirect(url_for('.labs'))\n    return render_template('webclient/lab.html', experiment=experiment, federated_mode=federated_mode, back_url=back_url, federated_reservation_id=federated_reservation_id)\n", "label": 1}
{"function": "\n\ndef _set_body(self, body):\n    ctype = self.headers.ipop('content-type', None)\n    clen = self.headers.ipop('content-length', None)\n    if isinstance(body, dict):\n        if ((ctype is not None) and ctype.startswith('multipart/form-data')):\n            (type_, opts) = cgi.parse_header(ctype)\n            boundary = opts.get('boundary', uuid.uuid4().hex)\n            (self._body, self.headers) = multipart_form_encode(body, self.headers, boundary)\n            ctype = self.headers.ipop('content-type', None)\n        else:\n            ctype = 'application/x-www-form-urlencoded; charset=utf-8'\n            self._body = form_encode(body)\n    elif (hasattr(body, 'boundary') and hasattr(body, 'get_size')):\n        ctype = ('multipart/form-data; boundary=%s' % body.boundary)\n        clen = body.get_size()\n        self._body = body\n    else:\n        self._body = body\n    if (not ctype):\n        ctype = 'application/octet-stream'\n        if hasattr(self.body, 'name'):\n            ctype = mimetypes.guess_type(body.name)[0]\n    if (not clen):\n        if hasattr(self._body, 'fileno'):\n            try:\n                self._body.flush()\n            except IOError:\n                pass\n            try:\n                fno = self._body.fileno()\n                clen = str(os.fstat(fno)[6])\n            except IOError:\n                if (not self.is_chunked()):\n                    clen = len(self._body.read())\n        elif (hasattr(self._body, 'getvalue') and (not self.is_chunked())):\n            clen = len(self._body.getvalue())\n        elif isinstance(self._body, types.StringTypes):\n            self._body = to_bytestring(self._body)\n            clen = len(self._body)\n    if (clen is not None):\n        self.headers['Content-Length'] = clen\n    if (ctype is not None):\n        self.headers['Content-Type'] = ctype\n", "label": 1}
{"function": "\n\ndef test_tb_option(self, testdir, option):\n    testdir.makepyfile('\\n            import pytest\\n            def g():\\n                raise IndexError\\n            def test_func():\\n                print (6*7)\\n                g()  # --calling--\\n        ')\n    for tbopt in ['long', 'short', 'no']:\n        print(('testing --tb=%s...' % tbopt))\n        result = testdir.runpytest(('--tb=%s' % tbopt))\n        s = result.stdout.str()\n        if (tbopt == 'long'):\n            assert ('print (6*7)' in s)\n        else:\n            assert ('print (6*7)' not in s)\n        if (tbopt != 'no'):\n            assert ('--calling--' in s)\n            assert ('IndexError' in s)\n        else:\n            assert ('FAILURES' not in s)\n            assert ('--calling--' not in s)\n            assert ('IndexError' not in s)\n", "label": 1}
{"function": "\n\ndef contests():\n    '\\n        Show the upcoming contests\\n    '\n    today = datetime.datetime.today()\n    today = datetime.datetime.strptime(str(today)[:(- 7)], '%Y-%m-%d %H:%M:%S')\n    start_date = today.date()\n    end_date = (start_date + datetime.timedelta(90))\n    url = 'https://contesttrackerapi.herokuapp.com/'\n    response = requests.get(url)\n    if (response.status_code == 200):\n        response = response.json()['result']\n    else:\n        return dict()\n    ongoing = response['ongoing']\n    upcoming = response['upcoming']\n    contests = []\n    cal = pdt.Calendar()\n    table = TABLE(_class='centered striped')\n    thead = THEAD(TR(TH('Contest Name'), TH('Site'), TH('Start'), TH('Duration/Ending'), TH('Link'), TH('Add Reminder')))\n    table.append(thead)\n    tbody = TBODY()\n    for i in ongoing:\n        if (i['Platform'] in ('TOPCODER', 'OTHER')):\n            continue\n        try:\n            endtime = datetime.datetime.strptime(i['EndTime'], '%a, %d %b %Y %H:%M')\n        except ValueError:\n            continue\n        tr = TR()\n        span = SPAN(_class='green tooltipped', data={\n            'position': 'right',\n            'delay': '50',\n            'tooltip': 'Live Contest',\n        }, _style=(((('cursor: pointer; ' + 'float:right; ') + 'height:10px; ') + 'width:10px; ') + 'border-radius: 50%;'))\n        tr.append(TD(i['Name'], span))\n        tr.append(TD(i['Platform'].capitalize()))\n        tr.append(TD('-'))\n        tr.append(TD(str(endtime).replace('-', '/'), _class='contest-end-time'))\n        tr.append(TD(A(I(_class='fa fa-external-link-square fa-lg'), _class='btn-floating btn-small green accent-4 tooltipped', _href=i['url'], data={\n            'position': 'left',\n            'tooltip': 'Contest Link',\n            'delay': '50',\n        }, _target='_blank')))\n        tr.append(TD(BUTTON(I(_class='fa fa-calendar-plus-o'), _class='btn-floating btn-small orange accent-4 tooltipped disabled', data={\n            'position': 'left',\n            'tooltip': 'Already started!',\n            'delay': '50',\n        })))\n        tbody.append(tr)\n    button_id = 1\n    for i in upcoming:\n        if (i['Platform'] in ('TOPCODER', 'OTHER')):\n            continue\n        start_time = datetime.datetime.strptime(i['StartTime'], '%a, %d %b %Y %H:%M')\n        tr = TR(_id=('contest-' + str(button_id)))\n        tr.append(TD(i['Name']))\n        tr.append(TD(i['Platform'].capitalize()))\n        tr.append(TD(str(start_time)))\n        duration = i['Duration']\n        duration = duration.replace(' days', 'd')\n        duration = duration.replace(' day', 'd')\n        tr.append(TD(duration))\n        tr.append(TD(A(I(_class='fa fa-external-link-square fa-lg'), _class='btn-floating btn-small green accent-4 tooltipped', _href=i['url'], data={\n            'position': 'left',\n            'tooltip': 'Contest Link',\n            'delay': '50',\n        }, _target='_blank')))\n        tr.append(TD(BUTTON(I(_class='fa fa-calendar-plus-o'), _class='btn-floating btn-small orange accent-4 tooltipped', data={\n            'position': 'left',\n            'tooltip': 'Set Reminder to Google Calendar',\n            'delay': '50',\n        }, _id=('set-reminder-' + str(button_id)))))\n        tbody.append(tr)\n        button_id += 1\n    table.append(tbody)\n    return dict(table=table, upcoming=upcoming)\n", "label": 0}
{"function": "\n\ndef sleep_while_copying(self, timeoutSec=3600):\n    Util.validate_type(timeoutSec, 'int')\n    step = 3\n    while (0 < timeoutSec):\n        try:\n            self.reload()\n        except saklient.errors.httpexception.HttpException:\n            pass\n        a = self.get_availability()\n        if (a == EAvailability.available):\n            return True\n        if (a != EAvailability.migrating):\n            timeoutSec = 0\n        timeoutSec -= step\n        if (0 < timeoutSec):\n            Util.sleep(step)\n    return False\n", "label": 0}
{"function": "\n\ndef test_basic_authenticated_documents(suite, basic_user, docs):\n    jwt = _login(basic_user)\n    headers = _auth_header(jwt)\n    authenticated_documents = 'http://localhost:5000/api/authenticated-app/authenticated-documents'\n    confirmed_dangerous_delete = requests.delete(authenticated_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    dangerous_delete = requests.delete(authenticated_documents, headers=headers, params={\n        'delete_all': False,\n    })\n    assert (not dangerous_delete.ok)\n    post = requests.post(authenticated_documents, headers=headers, data=json.dumps(docs))\n    assert post.ok\n    get_all = requests.get((authenticated_documents + ';json'), headers=headers)\n    assert get_all.ok\n    confirmed_dangerous_delete = requests.delete(authenticated_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    _logout(jwt)\n", "label": 0}
{"function": "\n\ndef read_nonblocking(self, size=1, timeout=(- 1)):\n    \"This reads at most size characters from the child application. It\\n        includes a timeout. If the read does not complete within the timeout\\n        period then a TIMEOUT exception is raised. If the end of file is read\\n        then an EOF exception will be raised. If a log file was set using\\n        setlog() then all data will also be written to the log file.\\n\\n        If timeout is None then the read may block indefinitely.\\n        If timeout is -1 then the self.timeout value is used. If timeout is 0\\n        then the child is polled and if there is no data immediately ready\\n        then this will raise a TIMEOUT exception.\\n\\n        The timeout refers only to the amount of time to read at least one\\n        character. This is not effected by the 'size' parameter, so if you call\\n        read_nonblocking(size=100, timeout=30) and only one character is\\n        available right away then one character will be returned immediately.\\n        It will not wait for 30 seconds for another 99 characters to come in.\\n\\n        This is a wrapper around os.read(). It uses select.select() to\\n        implement the timeout. \"\n    if self.closed:\n        raise ValueError('I/O operation on closed file.')\n    if (timeout == (- 1)):\n        timeout = self.timeout\n    if (not pty):\n        return self._winread(size, timeout)\n    if (not self.isalive()):\n        (r, w, e) = self.__select([self.child_fd], [], [], 0)\n        if (not r):\n            self.flag_eof = True\n            raise EOF('End Of File (EOF). Braindead platform.')\n    elif self.__irix_hack:\n        (r, w, e) = self.__select([self.child_fd], [], [], 2)\n        if ((not r) and (not self.isalive())):\n            self.flag_eof = True\n            raise EOF('End Of File (EOF). Slow platform.')\n    (r, w, e) = self.__select([self.child_fd], [], [], timeout)\n    if (not r):\n        if (not self.isalive()):\n            self.flag_eof = True\n            raise EOF('End of File (EOF). Very slow platform.')\n        else:\n            raise TIMEOUT('Timeout exceeded.')\n    if (self.child_fd in r):\n        try:\n            s = os.read(self.child_fd, size)\n        except OSError as err:\n            if (err.args[0] == errno.EIO):\n                self.flag_eof = True\n                raise EOF('End Of File (EOF). Exception style platform.')\n            raise\n        if (s == b''):\n            self.flag_eof = True\n            raise EOF('End Of File (EOF). Empty string style platform.')\n        s = self._coerce_read_string(s)\n        self._log(s, 'read')\n        return s\n    raise ExceptionPexpect('Reached an unexpected state.')\n", "label": 1}
{"function": "\n\ndef acquire(self, specification, arguments=None):\n    \"\\n        Returns an object for `specification` injecting its provider\\n        with a mix of its :term:`dependencies <dependency>` and given\\n        `arguments`. If there is a conflict between the injectable\\n        dependencies and `arguments`, the value from `arguments` is\\n        used.\\n\\n        When one of `arguments` keys is neither an integer nor a string\\n        a `TypeError` is raised.\\n\\n        :param specification:\\n            An object :term:`specification`.\\n        :param arguments:\\n            A dictionary of arguments given to the object :term:`provider`,\\n            overriding those that would be injected or filling in for those\\n            that wouldn't.  Positional arguments should be stored under 0-based\\n            integer keys.\\n        :raises:\\n            TypeError\\n        \"\n    if (arguments is None):\n        realized_dependencies = {\n            \n        }\n    else:\n        realized_dependencies = copy.copy(arguments)\n    provider = self.providers[specification]\n    scope = None\n    if (provider.scope is not None):\n        try:\n            scope = self.scopes[provider.scope]\n        except KeyError:\n            raise UnknownScopeError(provider.scope)\n    if ((scope is not None) and (specification in scope)):\n        return scope[specification]\n    dependencies = six.iteritems(provider.dependencies)\n    for (argument, dependency_specification) in dependencies:\n        if (argument not in realized_dependencies):\n            if isinstance(dependency_specification, Factory):\n                realized_dependencies[argument] = self.FactoryProxy(self, dependency_specification.specification)\n            else:\n                realized_dependencies[argument] = self.acquire(dependency_specification)\n    args = []\n    kwargs = {\n        \n    }\n    for (argument, value) in six.iteritems(realized_dependencies):\n        if isinstance(argument, six.integer_types):\n            if (len(args) <= argument):\n                args.extend(([None] * ((argument + 1) - len(args))))\n            args[argument] = value\n        elif isinstance(argument, six.string_types):\n            kwargs[argument] = value\n        else:\n            raise TypeError('{} is not a valid argument key'.format(repr(argument)))\n    instance = provider(*args, **kwargs)\n    if (scope is not None):\n        scope[specification] = instance\n    return instance\n", "label": 1}
{"function": "\n\ndef left_context(token_list, token, context_size, idx):\n    left_window = []\n    if (idx <= 0):\n        return ['<s>' for i in range(context_size)]\n    assert (token_list[idx] == token)\n    for i in range((idx - context_size), idx):\n        if (i < 0):\n            left_window.append('<s>')\n        else:\n            left_window.append(token_list[i])\n    return left_window\n", "label": 0}
{"function": "\n\ndef _on_process_finished(self, exit_code, exit_status):\n    if (self is None):\n        return\n    self._running = False\n    if (not self._user_stop):\n        if self._write_app_messages:\n            self._writer(self, ('\\nProcess finished with exit code %d' % self.exit_code), self._app_msg_col)\n    _logger().debug('process finished (exit_code=%r, exit_status=%r)', exit_code, exit_status)\n    try:\n        self.process_finished.emit(exit_code)\n    except TypeError:\n        pass\n    else:\n        self.setReadOnly(True)\n", "label": 0}
{"function": "\n\ndef run(self, sync):\n    app = sync.app\n    with app.db.getSession() as session:\n        for c in session.getPendingTopics():\n            sync.submitTask(SetTopicTask(c.key, self.priority))\n        for c in session.getPendingRebases():\n            sync.submitTask(RebaseChangeTask(c.key, self.priority))\n        for c in session.getPendingStatusChanges():\n            sync.submitTask(ChangeStatusTask(c.key, self.priority))\n        for c in session.getPendingStarred():\n            sync.submitTask(ChangeStarredTask(c.key, self.priority))\n        for c in session.getPendingCherryPicks():\n            sync.submitTask(SendCherryPickTask(c.key, self.priority))\n        for r in session.getPendingCommitMessages():\n            sync.submitTask(ChangeCommitMessageTask(r.key, self.priority))\n        for m in session.getPendingMessages():\n            sync.submitTask(UploadReviewTask(m.key, self.priority))\n", "label": 0}
{"function": "\n\ndef rs_tanh(p, x, prec):\n    \"\\n    Hyperbolic tangent of a series\\n\\n    Return the series expansion of the tanh of ``p``, about 0.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_tanh\\n    >>> R, x, y = ring('x, y', QQ)\\n    >>> rs_tanh(x + x*y, x, 4)\\n    -1/3*x**3*y**3 - x**3*y**2 - x**3*y - 1/3*x**3 + x*y + x\\n\\n    See Also\\n    ========\\n\\n    tanh\\n    \"\n    if rs_is_puiseux(p, x):\n        return rs_puiseux(rs_tanh, p, x, prec)\n    R = p.ring\n    const = 0\n    if _has_constant_term(p, x):\n        zm = R.zero_monom\n        c = p[zm]\n        if (R.domain is EX):\n            c_expr = c.as_expr()\n            const = tanh(c_expr)\n        elif isinstance(c, PolyElement):\n            try:\n                c_expr = c.as_expr()\n                const = R(tanh(c_expr))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        else:\n            try:\n                const = R(tanh(c))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        p1 = (p - c)\n        t1 = rs_tanh(p1, x, prec)\n        t = rs_series_inversion((1 + (const * t1)), x, prec)\n        return rs_mul((const + t1), t, x, prec)\n    if (R.ngens == 1):\n        return _tanh(p, x, prec)\n    else:\n        return rs_fun(p, _tanh, x, prec)\n", "label": 0}
{"function": "\n\ndef _coverall(self, fr, analysis):\n    try:\n        with open(fr.filename) as fp:\n            source = fp.readlines()\n    except IOError:\n        if (not self.config.ignore_errors):\n            raise\n    if self.config.strip_dirs:\n        filename = fr.filename\n        for dir in self.config.strip_dirs:\n            if filename.startswith(dir):\n                filename = filename.replace(dir, '').lstrip('/')\n                break\n    else:\n        filename = fr.relname\n    self.ret.append({\n        'name': filename,\n        'source': ''.join(source).rstrip(),\n        'coverage': list(self._coverage_list(source, analysis)),\n    })\n", "label": 0}
{"function": "\n\ndef _InvokeGitkitApi(self, method, params=None, need_service_account=True):\n    'Invokes Gitkit API, with optional access token for service account.\\n\\n    Args:\\n      method: string, the api method name.\\n      params: dict of optional parameters for the API.\\n      need_service_account: false if service account is not needed.\\n\\n    Raises:\\n      GitkitClientError: if the request is bad.\\n      GitkitServerError: if Gitkit can not handle the request.\\n\\n    Returns:\\n      API response as dict.\\n    '\n    body = (simplejson.dumps(params) if params else None)\n    req = urllib_request.Request((self.google_api_url + method))\n    req.add_header('Content-type', 'application/json')\n    if need_service_account:\n        if self.credentials:\n            access_token = self.credentials.get_access_token().access_token\n        elif (self.service_account_email and self.service_account_key):\n            access_token = self._GetAccessToken()\n        else:\n            raise errors.GitkitClientError('Missing service account credentials')\n        req.add_header('Authorization', ('Bearer ' + access_token))\n    try:\n        binary_body = (body.encode('utf-8') if body else None)\n        raw_response = urllib_request.urlopen(req, binary_body).read()\n    except urllib_request.HTTPError as err:\n        if (err.code == 400):\n            raw_response = err.read()\n        else:\n            raise\n    return self._CheckGitkitError(raw_response)\n", "label": 0}
{"function": "\n\ndef build_channels(api_user, nick, channels, path, taxonomy, country=False):\n    res = []\n    tags = build_tags(channels, taxonomy, path)\n    for channel in channels:\n        tags = [path, ((path + '/') + channel)]\n        if country:\n            rel_ref = Relation(owner='/tag_geo/Global', relation='tags_hierarchical', target=((path + '/') + channel))\n            rel_ref = rel_ref.put()\n            tags.append('/tag_geo/Global')\n        description = ('Group P.O.Box for %s' % channel)\n        try:\n            channel_ref = api.channel_create(api_user, nick=nick, channel=channel.title().replace(' ', ''), tags=tags, description=description)\n        except exception.ApiException:\n            logging.info('Channel Exists')\n            nick = api.clean.channel(channel.title().replace(' ', ''))\n            query = Actor.gql('WHERE nick=:1', nick)\n            channel_ref = query.get()\n            if channel_ref:\n                if (not (path in channel_ref.tags)):\n                    channel_ref.tags.append(path)\n                    channel_key = channel_ref.put()\n        res.append(channel_ref)\n    logging.info('End Creating Channels')\n    return res\n", "label": 0}
{"function": "\n\ndef kl(Y, Y_hat, batch_axis):\n    \"\\n    Warning: This function expects a sigmoid nonlinearity in the\\n    output layer. Returns a batch (vector) of mean across units of\\n    KL divergence for each example,\\n    KL(P || Q) where P is defined by Y and Q is defined by Y_hat:\\n\\n    p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)\\n    For binary p, some terms drop out:\\n    - p log q - (1-p) log (1-q)\\n    - p log sigmoid(z) - (1-p) log sigmoid(-z)\\n    p softplus(-z) + (1-p) softplus(z)\\n\\n    Parameters\\n    ----------\\n    Y : Variable\\n        targets for the sigmoid outputs. Currently Y must be purely binary.\\n        If it's not, you'll still get the right gradient, but the\\n        value in the monitoring channel will be wrong.\\n    Y_hat : Variable\\n        predictions made by the sigmoid layer. Y_hat must be generated by\\n        fprop, i.e., it must be a symbolic sigmoid.\\n    batch_axis : list\\n        list of axes to compute average kl divergence across.\\n\\n    Returns\\n    -------\\n    ave : Variable\\n        average kl divergence between Y and Y_hat.\\n    \"\n    assert hasattr(Y_hat, 'owner')\n    assert (batch_axis is not None)\n    owner = Y_hat.owner\n    assert (owner is not None)\n    op = owner.op\n    if (not hasattr(op, 'scalar_op')):\n        raise ValueError(((('Expected Y_hat to be generated by an Elemwise op, got ' + str(op)) + ' of type ') + str(type(op))))\n    assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)\n    for Yv in get_debug_values(Y):\n        if (not ((Yv.min() >= 0.0) and (Yv.max() <= 1.0))):\n            raise ValueError(('Expected Y to be between 0 and 1. Either Y' + '< 0 or Y > 1 was found in the input.'))\n    (z,) = owner.inputs\n    term_1 = (Y * T.nnet.softplus((- z)))\n    term_2 = ((1 - Y) * T.nnet.softplus(z))\n    total = (term_1 + term_2)\n    naxes = total.ndim\n    axes_to_reduce = list(range(naxes))\n    del axes_to_reduce[batch_axis]\n    ave = total.mean(axis=axes_to_reduce)\n    return ave\n", "label": 0}
{"function": "\n\ndef test_egg_packages():\n    el = EggPackage('psutil-0.4.1-py2.6-macosx-10.7-intel.egg')\n    assert (el.name == 'psutil')\n    assert (el.raw_version == '0.4.1')\n    assert (el.py_version == '2.6')\n    assert (el.platform == 'macosx-10.7-intel')\n    for req in ('psutil', 'psutil>0.4', 'psutil==0.4.1', 'psutil>0.4.0,<0.4.2'):\n        assert el.satisfies(req)\n    for req in ('foo', 'bar==0.4.1'):\n        assert (not el.satisfies(req))\n    el = EggPackage('pyfoo-1.0.0_bar-py2.7-linux-x86_64.egg')\n    assert (el.name == 'pyfoo')\n    assert (el.raw_version == '1.0.0-bar')\n    assert (el.py_version == '2.7')\n    assert (el.platform == 'linux-x86_64')\n    for req in ('pyfoo', 'pyfoo==1.0.0-bar'):\n        assert el.satisfies(req)\n    el = EggPackage('pytz-2012b-py2.6.egg')\n    assert (el.name == 'pytz')\n    assert (el.raw_version == '2012b0')\n    assert (el.py_version == '2.6')\n    assert (el.platform is None)\n    with pytest.raises(EggPackage.InvalidPackage):\n        EggPackage('bar.egg')\n    with pytest.raises(EggPackage.InvalidPackage):\n        EggPackage('bar-1.egg')\n    with pytest.raises(EggPackage.InvalidPackage):\n        EggPackage('bar-py2.6.egg')\n", "label": 1}
{"function": "\n\n@bacpypes_debugging\ndef octet_string_endec(x):\n    'Pass the value to OctetString, construct a tag from the hex string,\\n    and compare results of encode and decoding each other.'\n    if _debug:\n        octet_string_endec._debug('octet_string_endec %r', x)\n    tag = octet_string_tag(x)\n    if _debug:\n        octet_string_endec._debug('    - tag: %r, %r', tag, tag.tagData)\n    obj = OctetString(xtob(x))\n    if _debug:\n        octet_string_endec._debug('    - obj: %r, %r', obj, obj.value)\n    assert (octet_string_encode(obj) == tag)\n    assert (octet_string_decode(tag) == obj)\n", "label": 0}
{"function": "\n\ndef test_extract_simple():\n    weak = 1.0\n    strong = 2.0\n    nc = 4\n    ns = 20\n    data = np.random.uniform(size=(ns, nc), low=0.0, high=1.0)\n    data[(10, 0)] = 0.5\n    data[(11, 0)] = 1.5\n    data[(12, 0)] = 1.0\n    data[(10, 1)] = 1.5\n    data[(11, 1)] = 2.5\n    data[(12, 1)] = 2.0\n    component = np.array([[10, 0], [10, 1], [11, 0], [11, 1], [12, 0], [12, 1]])\n    we = WaveformExtractor(extract_before=3, extract_after=5)\n    we.set_thresholds(weak=weak, strong=strong)\n    comp = we._component(component, n_samples=ns)\n    ae(comp.comp_s, [10, 10, 11, 11, 12, 12])\n    ae(comp.comp_ch, [0, 1, 0, 1, 0, 1])\n    assert ((comp.s_min, comp.s_max) == ((10 - 3), (12 + 4)))\n    assert (we._normalize(weak) == 0)\n    assert (we._normalize(strong) == 1)\n    ae(we._normalize([((weak + strong) / 2.0)]), [0.5])\n    wave = we._comp_wave(data, comp)\n    assert (wave.shape == (((3 + 5) + 1), nc))\n    ae(wave[3:6, :], [[0.5, 1.5, 0.0, 0.0], [1.5, 2.5, 0.0, 0.0], [1.0, 2.0, 0.0, 0.0]])\n    masks = we.masks(data, wave, comp)\n    ae(masks, [0.5, 1.0, 0, 0])\n    s = we.spike_sample_aligned(wave, comp)\n    assert (11 <= s < 12)\n    wave_e = we.extract(data, s)\n    assert (wave_e.shape[1] == wave.shape[1])\n    ae(wave[3:6, :2], wave_e[3:6, :2])\n    wave_a = we.align(wave_e, s)\n    assert (wave_a.shape == ((3 + 5), nc))\n    (s_f, masks_f, wave_f) = we(component, data=data, data_t=data)\n    assert (s_f == s)\n    ae(masks_f, masks)\n    ae(wave_f, wave_a)\n    we = WaveformExtractor(extract_before=3, extract_after=5, thresholds={\n        'weak': weak,\n        'strong': strong,\n    })\n    (s_f_o, masks_f_o, wave_f_o) = we(component, data=data, data_t=data)\n    assert (s_f == s_f_o)\n    assert np.allclose(wave_f, wave_f_o)\n    ae(masks_f_o, [0.5, 1.0, 0.0, 0.0])\n", "label": 1}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    pos = locations[0]\n    scopes = view.scope_name(pos).split()\n    if ('source.json' not in scopes):\n        return []\n    else:\n        doc = view.substr(sublime.Region(0, view.size()))\n        whitespaces = 0\n        for (index, char) in enumerate(doc[0:pos]):\n            if ((char == '\\n') or (char == '\\r') or (char == ' ') or (char == '\\t')):\n                whitespaces += 1\n        doc = doc.replace('\\n', '').replace('\\r', '').replace(' ', '').replace('\\t', '')\n        pos = (pos - whitespaces)\n        depth = (- 1)\n        tokens = []\n        token_regex = re.compile('\"([-a-zA-Z0-9+]+)\":{')\n        for (index, char) in enumerate(doc):\n            if (char == '{'):\n                depth += 1\n                token_regex = '\"([-a-zA-Z0-9+]+)\":{$'\n                match = re.search(token_regex, doc[0:(index + 1)])\n                try:\n                    tokens.append(match.group(1))\n                except AttributeError as e:\n                    pass\n            if (char == '}'):\n                depth -= 1\n                try:\n                    tokens.pop()\n                except IndexError as e:\n                    pass\n            if (index == pos):\n                if ((depth == 1) and (tokens[0] == 'dependencies')):\n                    version_regex = '(?:,|{|\\\\[])\"([-a-zA-Z0-9.*]+)\":\"[-a-zA-Z0-9.*]*$'\n                    try:\n                        package_name = re.search(version_regex, doc[0:index]).group(1)\n                        return (self.result[package_name], AC_OPTS)\n                    except AttributeError as e:\n                        pass\n                    except KeyError as e:\n                        return AC_OPTS\n                    return (self.cache, AC_OPTS)\n                elif (depth == 0):\n                    return ([('version', 'version\" : \"$1\"'), ('dependencies', 'dependencies\" : {\\n\\t\"$1\": \"$2\"\\n}'), ('commands', 'commands\" : {\\n\\t\"$1\": \"$2\"\\n}'), ('configurations', 'configurations\" : {\\n\\t\"$1\": {}\\n}'), ('compilationOptions', 'compilationOptions'), ('frameworks', 'frameworks\" : {\\n\\t\"$1\": \"$2\"\\n}'), ('description', 'description\" : \"$1\"'), ('authors', 'authors\" : [\\n\\t\"$1\"\\n]'), ('code', 'code\" : {\\n\\t\"$1\": \"$2\"\\n}'), ('shared', 'shared\" : {\\n\\t\"$1\": \"$2\"\\n}'), ('exclude', 'exclude\" : {\\n\\t\"$1\": \"$2\"\\n}'), ('preprocess', 'preprocess\" : {\\n\\t\"$1\": \"$2\"\\n}'), ('resources', 'resources')], AC_OPTS)\n                else:\n                    break\n        return AC_OPTS\n", "label": 1}
{"function": "\n\ndef test_log_pdf(self):\n    X_1 = numpy.array([[1], [0]])\n    parameters = dict(mu=0.0, rho=1.0)\n    log_pdf = self.component_class.log_pdf(X_1, parameters)\n    assert (len(log_pdf) == 2)\n    assert (math.fabs(((- 1.4189385332046727) - log_pdf[(0, 0)])) < 1e-08)\n    assert (math.fabs(((- 0.9189385332046727) - log_pdf[(1, 0)])) < 1e-08)\n    parameters = dict(mu=2.2, rho=12.1)\n    log_pdf = self.component_class.log_pdf(X_1, parameters)\n    assert (len(log_pdf) == 2)\n    assert (math.fabs(((- 8.38433580690333) - log_pdf[(0, 0)])) < 1e-08)\n    assert (math.fabs(((- 28.954335806903334) - log_pdf[(1, 0)])) < 1e-08)\n    parameters = dict(mu=0.0, rho=1.0)\n    lspc = numpy.linspace(0, 10, num=20)\n    X_2 = numpy.array([[x] for x in lspc])\n    log_pdf = self.component_class.log_pdf(X_2, parameters)\n    assert (len(log_pdf) == 20)\n    for n in range(1, 20):\n        assert (log_pdf[((n - 1), 0)] > log_pdf[(n, 0)])\n", "label": 1}
{"function": "\n\ndef checkout(path, branch='master', use_sudo=False, user=None, force=False):\n    '\\n    Checkout a branch to the working directory.\\n\\n    :param path: Path of the working copy directory.  This directory must exist\\n                 and be a Git working copy.\\n    :type path: str\\n\\n    :param branch: Name of the branch to checkout.\\n    :type branch: str\\n\\n    :param use_sudo: If ``True`` execute ``git`` with\\n                     :func:`fabric.operations.sudo`, else with\\n                     :func:`fabric.operations.run`.\\n    :type use_sudo: bool\\n\\n    :param user: If ``use_sudo is True``, run :func:`fabric.operations.sudo`\\n                 with the given user.  If ``use_sudo is False`` this parameter\\n                 has no effect.\\n    :type user: str\\n    :param force: If ``True``, append the ``--force`` option to the command.\\n    :type force: bool\\n    '\n    if (path is None):\n        raise ValueError('Path to the working copy is needed to checkout a branch')\n    options = []\n    if force:\n        options.append('--force')\n    options = ' '.join(options)\n    cmd = ('git checkout %s %s' % (branch, options))\n    with cd(path):\n        if (use_sudo and (user is None)):\n            run_as_root(cmd)\n        elif use_sudo:\n            sudo(cmd, user=user)\n        else:\n            run(cmd)\n", "label": 0}
{"function": "\n\ndef load(self, filename):\n    if (not filename):\n        import traceback\n        traceback.print_stack()\n        return\n    try:\n        im = None\n        if self._inline:\n            im = pygame.image.load(filename, 'x.{}'.format(self._ext))\n        elif isfile(filename):\n            with open(filename, 'rb') as fd:\n                im = pygame.image.load(fd)\n        elif isinstance(filename, bytes):\n            try:\n                fname = filename.decode()\n                if isfile(fname):\n                    with open(fname, 'rb') as fd:\n                        im = pygame.image.load(fd)\n            except UnicodeDecodeError:\n                pass\n        if (im is None):\n            im = pygame.image.load(filename)\n    except:\n        raise\n    fmt = ''\n    if (im.get_bytesize() == 3):\n        fmt = 'rgb'\n    elif (im.get_bytesize() == 4):\n        fmt = 'rgba'\n    if (fmt not in ('rgb', 'rgba')):\n        try:\n            imc = im.convert(32)\n            fmt = 'rgba'\n        except:\n            try:\n                imc = im.convert_alpha()\n                fmt = 'rgba'\n            except:\n                Logger.warning(('Image: Unable to convert image %r to rgba (was %r)' % (filename, im.fmt)))\n                raise\n        im = imc\n    if (not self._inline):\n        self.filename = filename\n    data = pygame.image.tostring(im, fmt.upper())\n    return [ImageData(im.get_width(), im.get_height(), fmt, data, source=filename)]\n", "label": 1}
{"function": "\n\ndef _populate_vars(self):\n    if ('variables' in self.config.keys()):\n        for (var, question) in self.config['variables'].items():\n            if (self.app.pargs.defaults is True):\n                try:\n                    res = self.app.config.get('answers', var)\n                except ConfigParser.NoOptionError as e:\n                    res = 'MISSING VARIABLE'\n            elif (var.lower() in self.app.config.keys('answers')):\n                default = self.app.config.get('answers', var.lower())\n                res = input(('%s: [%s] ' % (question, default)))\n                if (len(res) == 0):\n                    res = default\n            else:\n                res = input(('%s: ' % question))\n            self._vars[var] = res.strip()\n", "label": 0}
{"function": "\n\ndef apply(self, fgraph):\n    '\\n        WRITEME\\n\\n        Applies each L{Optimizer} in self in turn.\\n\\n        '\n    l = []\n    if fgraph.profile:\n        validate_before = fgraph.profile.validate_time\n        sub_validate_time = [validate_before]\n    else:\n        sub_validate_time = []\n    callback_before = fgraph.execute_callbacks_time\n    nb_node_before = len(fgraph.apply_nodes)\n    sub_profs = []\n    for optimizer in self:\n        try:\n            t0 = time.time()\n            sub_prof = optimizer.optimize(fgraph)\n            l.append(float((time.time() - t0)))\n            sub_profs.append(sub_prof)\n            if fgraph.profile:\n                sub_validate_time.append(fgraph.profile.validate_time)\n        except AssertionError:\n            raise\n        except Exception as e:\n            if self.failure_callback:\n                self.failure_callback(e, self, optimizer)\n                continue\n            else:\n                raise\n    if fgraph.profile:\n        validate_time = (fgraph.profile.validate_time - validate_before)\n    else:\n        validate_time = None\n    callback_time = (fgraph.execute_callbacks_time - callback_before)\n    return (self, l, validate_time, callback_time, nb_node_before, len(fgraph.apply_nodes), sub_profs, sub_validate_time)\n", "label": 0}
{"function": "\n\ndef test_file_metadata_drive(basepath):\n    item = fixtures.list_file['items'][0]\n    path = basepath.child(item['title'])\n    parsed = GoogleDriveFileMetadata(item, path)\n    assert (parsed.provider == 'googledrive')\n    assert (parsed.id == item['id'])\n    assert (path.name == item['title'])\n    assert (parsed.name == item['title'])\n    assert (parsed.size == item['fileSize'])\n    assert (parsed.modified == item['modifiedDate'])\n    assert (parsed.content_type == item['mimeType'])\n    assert (parsed.extra == {\n        'revisionId': item['version'],\n        'webView': item['alternateLink'],\n    })\n    assert (parsed.path == ('/' + os.path.join(*[x.raw for x in path.parts])))\n    assert (parsed.materialized_path == str(path))\n    assert (parsed.is_google_doc == False)\n    assert (parsed.export_name == item['title'])\n", "label": 1}
{"function": "\n\ndef extend_list(self, data, parsed_args):\n    'Add subnet information to a network list.'\n    neutron_client = self.get_client()\n    search_opts = {\n        'fields': ['id', 'cidr'],\n    }\n    if self.pagination_support:\n        page_size = parsed_args.page_size\n        if page_size:\n            search_opts.update({\n                'limit': page_size,\n            })\n    subnet_ids = []\n    for n in data:\n        if ('subnets' in n):\n            subnet_ids.extend(n['subnets'])\n\n    def _get_subnet_list(sub_ids):\n        search_opts['id'] = sub_ids\n        return neutron_client.list_subnets(**search_opts).get('subnets', [])\n    try:\n        subnets = _get_subnet_list(subnet_ids)\n    except exceptions.RequestURITooLong as uri_len_exc:\n        subnet_count = len(subnet_ids)\n        max_size = ((self.subnet_id_filter_len * subnet_count) - uri_len_exc.excess)\n        chunk_size = (max_size // self.subnet_id_filter_len)\n        subnets = []\n        for i in range(0, subnet_count, chunk_size):\n            subnets.extend(_get_subnet_list(subnet_ids[i:(i + chunk_size)]))\n    subnet_dict = dict([(s['id'], s) for s in subnets])\n    for n in data:\n        if ('subnets' in n):\n            n['subnets'] = [(subnet_dict.get(s) or {\n                'id': s,\n            }) for s in n['subnets']]\n", "label": 1}
{"function": "\n\ndef _load_config(self):\n    try:\n        scf = self.conf.glance_store.swift_store_config_file\n        conf_file = self.conf.find_file(scf)\n        CONFIG.read(conf_file)\n    except Exception as e:\n        msg = (i18n._('swift config file %(conf)s:%(exc)s not found') % {\n            'conf': self.conf.glance_store.swift_store_config_file,\n            'exc': e,\n        })\n        LOG.error(msg)\n        raise exceptions.BadStoreConfiguration(store_name='swift', reason=msg)\n    account_params = {\n        \n    }\n    account_references = CONFIG.sections()\n    for ref in account_references:\n        reference = {\n            \n        }\n        try:\n            for param in ('auth_address', 'user', 'key', 'project_domain_id', 'project_domain_name', 'user_domain_id', 'user_domain_name'):\n                reference[param] = CONFIG.get(ref, param)\n            try:\n                reference['auth_version'] = CONFIG.get(ref, 'auth_version')\n            except configparser.NoOptionError:\n                av = self.conf.glance_store.swift_store_auth_version\n                reference['auth_version'] = av\n            account_params[ref] = reference\n        except (ValueError, SyntaxError, configparser.NoOptionError) as e:\n            LOG.exception(i18n._('Invalid format of swift store configcfg'))\n    return account_params\n", "label": 0}
{"function": "\n\ndef iterbusinessdays(self, d1, d2):\n    '\\n        Date iterator returning dates in d1 <= x < d2, excluding weekends and holidays\\n        '\n    assert (d2 >= d1)\n    if ((d1.date() == d2.date()) and (d2.time() < self.business_hours[0])):\n        return\n    first = True\n    for dt in self.iterdays(d1, d2):\n        if (first and (d1.time() > self.business_hours[1])):\n            first = False\n            continue\n        first = False\n        if ((not self.isweekend(dt)) and (not self.isholiday(dt))):\n            (yield dt)\n", "label": 0}
{"function": "\n\ndef test_vecwrapper(self):\n    unknowns_dict = OrderedDict()\n    unknowns_dict['y1'] = {\n        'shape': (3, 2),\n        'size': 6,\n        'val': np.ones((3, 2)),\n    }\n    unknowns_dict['y2'] = {\n        'shape': 1,\n        'size': 1,\n        'val': 2.0,\n    }\n    unknowns_dict['y3'] = {\n        'size': 0,\n        'val': 'foo',\n        'pass_by_obj': True,\n    }\n    unknowns_dict['y4'] = {\n        'shape': (2, 1),\n        'size': 2,\n        'val': np.zeros((2, 1)),\n    }\n    unknowns_dict['s1'] = {\n        'shape': 1,\n        'size': 1,\n        'val': (- 1.0),\n        'state': True,\n    }\n    sd = _SysData('')\n    for (u, meta) in unknowns_dict.items():\n        meta['pathname'] = u\n        meta['top_promoted_name'] = u\n        sd.to_prom_name[u] = u\n    u = SrcVecWrapper(sd, pbd)\n    u.setup(unknowns_dict, store_byobjs=True)\n    self.assertEqual(u.vec.size, 10)\n    self.assertEqual(len(u), 5)\n    self.assertEqual(list(u.keys()), ['y1', 'y2', 'y3', 'y4', 's1'])\n    self.assertTrue(np.all((u['y1'] == np.ones((3, 2)))))\n    self.assertEqual(u['y2'], 2.0)\n    self.assertEqual(u['y3'], 'foo')\n    self.assertTrue(np.all((u['y4'] == np.zeros((2, 1)))))\n    self.assertEqual(u['s1'], (- 1.0))\n    self.assertEqual([t[0] for t in u.vec_val_iter()], ['y1', 'y2', 'y4', 's1'])\n    u['y1'] = (np.ones((3, 2)) * 3.0)\n    u['y2'] = 2.5\n    u['y3'] = 'bar'\n    u['y4'] = (np.ones((2, 1)) * 7.0)\n    u['s1'] = 5.0\n    self.assertTrue(np.all((u['y1'] == (np.ones((3, 2)) * 3.0))))\n    self.assertTrue(np.all((u['y4'] == (np.ones((2, 1)) * 7.0))))\n    self.assertEqual(u['y2'], 2.5)\n    self.assertEqual(u['y3'], 'bar')\n    self.assertEqual(u['s1'], 5.0)\n    try:\n        u['y1'] = np.ones((3, 3))\n    except Exception as err:\n        self.assertEqual(str(err), 'could not broadcast input array from shape (9) into shape (6)')\n    else:\n        self.fail('Exception expected')\n    params = OrderedDict()\n    params['y1'] = {\n        'shape': (3, 2),\n        'size': 6,\n        'val': np.ones((3, 2)),\n    }\n    params['y2'] = {\n        'shape': 1,\n        'size': 1,\n        'val': 2.0,\n    }\n    params['y3'] = {\n        'size': 0,\n        'val': 'foo',\n    }\n    params['y4'] = {\n        'shape': (2, 1),\n        'size': 6,\n        'val': np.zeros((2, 1)),\n    }\n    for (p, meta) in params.items():\n        meta['pathname'] = p\n        meta['top_promoted_name'] = p\n        sd.to_prom_name[u] = u\n    connections = {\n        \n    }\n    for p in params:\n        connections[p] = (p, None)\n    s = _SysData('')\n    s._unknowns_dict = u._dat\n    p = TgtVecWrapper(s, pbd)\n    p.setup(None, params, u, params.keys(), connections, store_byobjs=True)\n    self.assertEqual(p.vec.size, 9)\n    self.assertEqual(len(p), 4)\n    self.assertEqual(list(p.keys()), ['y1', 'y2', 'y3', 'y4'])\n    self.assertTrue(np.all((p['y1'] == np.zeros((3, 2)))))\n    self.assertEqual(p['y2'], 0.0)\n    self.assertEqual(p['y3'], 'bar')\n    self.assertTrue(np.all((p['y4'] == np.zeros((2, 1)))))\n    p['y1'] = (np.ones((3, 2)) * 9.0)\n    self.assertTrue(np.all((p['y1'] == (np.ones((3, 2)) * 9.0))))\n", "label": 0}
{"function": "\n\ndef transaction(self, func, *watches, **kwargs):\n    \"\\n        Convenience method for executing the callable `func` as a transaction\\n        while watching all keys specified in `watches`. The 'func' callable\\n        should expect a single argument which is a Pipeline object.\\n        \"\n    shard_hint = kwargs.pop('shard_hint', None)\n    value_from_callable = kwargs.pop('value_from_callable', False)\n    watch_delay = kwargs.pop('watch_delay', None)\n    with self.pipeline(True, shard_hint) as pipe:\n        while 1:\n            try:\n                if watches:\n                    pipe.watch(*watches)\n                func_value = func(pipe)\n                exec_value = pipe.execute()\n                return (func_value if value_from_callable else exec_value)\n            except WatchError:\n                if ((watch_delay is not None) and (watch_delay > 0)):\n                    time.sleep(watch_delay)\n                continue\n", "label": 0}
{"function": "\n\ndef transform_Index(self, expr):\n    arr = self.transform_expr(expr.value)\n    idx = self.transform_expr(expr.index)\n    idx = self.assign_name(idx, 'idx')\n    arr_t = arr.type\n    if (arr_t.__class__ is PtrT):\n        assert isinstance(idx.type, IntT)\n        return expr\n    assert (arr_t.__class__ is ArrayT), ('Unexpected array %s : %s' % (arr, arr.type))\n    if self.is_tuple(idx):\n        indices = self.tuple_elts(idx)\n    else:\n        indices = [idx]\n    n_given = len(indices)\n    n_required = arr_t.rank\n    if (n_given < n_required):\n        extra_indices = ([syntax.helpers.slice_none] * (n_required - n_given))\n        indices.extend(extra_indices)\n    if syntax.helpers.all_scalars(indices):\n        data_ptr = self.attr(arr, 'data')\n        strides = self.attr(arr, 'strides')\n        offset_elts = self.attr(arr, 'offset')\n        for (i, idx_i) in enumerate(indices):\n            stride_i = self.tuple_proj(strides, i)\n            elts_i = self.mul(stride_i, idx_i, ('offset_elts_%d' % i))\n            offset_elts = self.add(offset_elts, elts_i, 'total_offset')\n        return self.index(data_ptr, offset_elts, temp=False)\n    else:\n        return self.array_slice(arr, indices)\n", "label": 0}
{"function": "\n\ndef test_scale():\n    (m, ctl, config) = init()\n    m.load(config)\n    pids = m.pids()\n    cmd = TestCommand('scale', ['dummy', 1])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids1 = m.pids()\n    cmd = TestCommand('scale', ['dummy', (- 1)])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids2 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '+4'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids3 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '-1'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids4 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '=1'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids5 = m.pids()\n    m.stop()\n    m.run()\n    assert (len(pids) == 1)\n    assert (len(pids1) == 2)\n    assert (len(pids2) == 1)\n    assert (len(pids3) == 5)\n    assert (len(pids4) == 4)\n    assert (len(pids5) == 1)\n", "label": 0}
{"function": "\n\ndef test_load_from_entry_point():\n    from giblets.core import ComponentManager, Component, ExtensionPoint\n    from giblets.search import find_plugins_by_entry_point\n\n    class PluginFinder(Component):\n        found_plugins = ExtensionPoint(TestEggInterface)\n    mgr = ComponentManager()\n    pf = PluginFinder(mgr)\n    assert (len(pf.found_plugins) == 0)\n    find_plugins_by_entry_point('giblets_load_from_entry_point_test')\n    expected_plugins = ['TestEggPlugin1', 'TestEggPlugin2', 'TestEggPlugin3']\n    got_plugins = set()\n    assert (len(pf.found_plugins) == len(expected_plugins))\n    for plugin in pf.found_plugins:\n        plugin_name = plugin.__class__.__name__\n        assert (plugin_name in expected_plugins)\n        got_plugins.add(plugin_name)\n    for plugin_name in expected_plugins:\n        assert (plugin_name in got_plugins)\n", "label": 0}
{"function": "\n\ndef install(self, install_options, global_options=(), *args, **kwargs):\n    '\\n        Install everything in this set (after having downloaded and unpacked\\n        the packages)\\n        '\n    to_install = self._to_install()\n    if to_install:\n        logger.info('Installing collected packages: %s', ', '.join([req.name for req in to_install]))\n    with indent_log():\n        for requirement in to_install:\n            if requirement.conflicts_with:\n                logger.info('Found existing installation: %s', requirement.conflicts_with)\n                with indent_log():\n                    requirement.uninstall(auto_confirm=True)\n            try:\n                requirement.install(install_options, global_options, *args, **kwargs)\n            except:\n                if (requirement.conflicts_with and (not requirement.install_succeeded)):\n                    requirement.rollback_uninstall()\n                raise\n            else:\n                if (requirement.conflicts_with and requirement.install_succeeded):\n                    requirement.commit_uninstall()\n            requirement.remove_temporary_source()\n    self.successfully_installed = to_install\n", "label": 1}
{"function": "\n\n@ignore_warnings\ndef test_warm_start():\n    (X, y) = (iris.data, iris.target)\n    solvers = ['newton-cg', 'sag']\n    if (sp_version >= (0, 12)):\n        solvers.append('lbfgs')\n    for warm_start in [True, False]:\n        for fit_intercept in [True, False]:\n            for solver in solvers:\n                for multi_class in ['ovr', 'multinomial']:\n                    clf = LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept)\n                    clf.fit(X, y)\n                    coef_1 = clf.coef_\n                    clf.max_iter = 1\n                    with ignore_warnings():\n                        clf.fit(X, y)\n                    cum_diff = np.sum(np.abs((coef_1 - clf.coef_)))\n                    msg = ('Warm starting issue with %s solver in %s mode with fit_intercept=%s and warm_start=%s' % (solver, multi_class, str(fit_intercept), str(warm_start)))\n                    if warm_start:\n                        assert_greater(2.0, cum_diff, msg)\n                    else:\n                        assert_greater(cum_diff, 2.0, msg)\n", "label": 0}
{"function": "\n\ndef _addedDataToIncoming(self, inc, skipFinish=False):\n    if (not inc.receivedAllData()):\n        return inc\n    (rdata, extra) = ('', '')\n    try:\n        (rdata, extra) = inc.data\n        if isControlMessage(rdata):\n            raise ValueError(('Error: received control message \"%s\"; expecting incoming data.' % str(rdata)))\n        rEnv = ReceiveEnvelope(*rdata)\n    except Exception:\n        import traceback\n        thesplog('OUCH!  Error deserializing received data: %s  (rdata=\"%s\", extra=\"%s\")', traceback.format_exc(), rdata, extra)\n        try:\n            inc.socket.sendall(ackDataErrMsg)\n        except Exception:\n            pass\n        inc.close()\n        return None\n    inc.socket.sendall(ackMsg)\n    inc.fromAddress = rdata[0]\n    self._processReceivedEnvelope(rEnv)\n    if (extra and isinstance(inc, TCPIncomingPersistent)):\n        newinc = TCPIncomingPersistent(inc.fromAddress, inc.socket)\n        newinc.addData(rdata)\n        return self._addedDataToIncoming(newinc)\n    if (not skipFinish):\n        self._finishIncoming(inc, rEnv.sender)\n    return None\n", "label": 0}
{"function": "\n\ndef assertDictMatch(self, d1, d2, approx_equal=False, tolerance=0.001):\n    \"Assert two dicts are equivalent.\\n\\n        This is a 'deep' match in the sense that it handles nested\\n        dictionaries appropriately.\\n\\n        NOTE:\\n\\n            If you don't care (or don't know) a given value, you can specify\\n            the string DONTCARE as the value. This will cause that dict-item\\n            to be skipped.\\n\\n        \"\n\n    def raise_assertion(msg):\n        d1str = str(d1)\n        d2str = str(d2)\n        base_msg = ('Dictionaries do not match. %(msg)s d1: %(d1str)s d2: %(d2str)s' % locals())\n        raise AssertionError(base_msg)\n    d1keys = set(d1.keys())\n    d2keys = set(d2.keys())\n    if (d1keys != d2keys):\n        d1only = (d1keys - d2keys)\n        d2only = (d2keys - d1keys)\n        raise_assertion(('Keys in d1 and not d2: %(d1only)s. Keys in d2 and not d1: %(d2only)s' % locals()))\n    for key in d1keys:\n        d1value = d1[key]\n        d2value = d2[key]\n        try:\n            error = abs((float(d1value) - float(d2value)))\n            within_tolerance = (error <= tolerance)\n        except (ValueError, TypeError):\n            within_tolerance = False\n        if (hasattr(d1value, 'keys') and hasattr(d2value, 'keys')):\n            self.assertDictMatch(d1value, d2value)\n        elif ('DONTCARE' in (d1value, d2value)):\n            continue\n        elif (approx_equal and within_tolerance):\n            continue\n        elif (d1value != d2value):\n            raise_assertion((\"d1['%(key)s']=%(d1value)s != d2['%(key)s']=%(d2value)s\" % locals()))\n", "label": 0}
{"function": "\n\n@periodic_task.periodic_task(spacing=CONF.instance_delete_interval)\ndef _cleanup_incomplete_migrations(self, context):\n    'Delete instance files on failed resize/revert-resize operation\\n\\n        During resize/revert-resize operation, if that instance gets deleted\\n        in-between then instance files might remain either on source or\\n        destination compute node because of race condition.\\n        '\n    LOG.debug('Cleaning up deleted instances with incomplete migration ')\n    migration_filters = {\n        'host': CONF.host,\n        'status': 'error',\n    }\n    migrations = objects.MigrationList.get_by_filters(context, migration_filters)\n    if (not migrations):\n        return\n    inst_uuid_from_migrations = set([migration.instance_uuid for migration in migrations])\n    inst_filters = {\n        'deleted': True,\n        'soft_deleted': False,\n        'uuid': inst_uuid_from_migrations,\n        'host': CONF.host,\n    }\n    attrs = ['info_cache', 'security_groups', 'system_metadata']\n    with utils.temporary_mutation(context, read_deleted='yes'):\n        instances = objects.InstanceList.get_by_filters(context, inst_filters, expected_attrs=attrs, use_slave=True)\n    for instance in instances:\n        for migration in migrations:\n            if (instance.uuid == migration.instance_uuid):\n                self.driver.delete_instance_files(instance)\n                try:\n                    migration.status = 'failed'\n                    with migration.obj_as_admin():\n                        migration.save()\n                except exception.MigrationNotFound:\n                    LOG.warning(_LW('Migration %s is not found.'), migration.id, context=context, instance=instance)\n                break\n", "label": 0}
{"function": "\n\ndef _test_renewal_common(self, due_for_renewal, extra_args, log_out=None, args=None, should_renew=True, error_expected=False):\n    cert_path = 'certbot/tests/testdata/cert.pem'\n    chain_path = '/etc/letsencrypt/live/foo.bar/fullchain.pem'\n    mock_lineage = mock.MagicMock(cert=cert_path, fullchain=chain_path)\n    mock_lineage.should_autorenew.return_value = due_for_renewal\n    mock_certr = mock.MagicMock()\n    mock_key = mock.MagicMock(pem='pem_key')\n    mock_client = mock.MagicMock()\n    stdout = None\n    mock_client.obtain_certificate.return_value = (mock_certr, 'chain', mock_key, 'csr')\n    try:\n        with mock.patch('certbot.main._find_duplicative_certs') as mock_fdc:\n            mock_fdc.return_value = (mock_lineage, None)\n            with mock.patch('certbot.main._init_le_client') as mock_init:\n                mock_init.return_value = mock_client\n                get_utility_path = 'certbot.main.zope.component.getUtility'\n                with mock.patch(get_utility_path) as mock_get_utility:\n                    with mock.patch('certbot.main.renewal.OpenSSL') as mock_ssl:\n                        mock_latest = mock.MagicMock()\n                        mock_latest.get_issuer.return_value = 'Fake fake'\n                        mock_ssl.crypto.load_certificate.return_value = mock_latest\n                        with mock.patch('certbot.main.renewal.crypto_util'):\n                            if (not args):\n                                args = ['-d', 'isnot.org', '-a', 'standalone', 'certonly']\n                            if extra_args:\n                                args += extra_args\n                            try:\n                                (ret, stdout, _, _) = self._call(args)\n                                if ret:\n                                    print('Returned', ret)\n                                    raise AssertionError(ret)\n                                assert (not error_expected), 'renewal should have errored'\n                            except:\n                                if (not error_expected):\n                                    raise AssertionError(('Unexpected renewal error:\\n' + traceback.format_exc()))\n        if should_renew:\n            mock_client.obtain_certificate.assert_called_once_with(['isnot.org'])\n        else:\n            self.assertEqual(mock_client.obtain_certificate.call_count, 0)\n    except:\n        self._dump_log()\n        raise\n    finally:\n        if log_out:\n            with open(os.path.join(self.logs_dir, 'letsencrypt.log')) as lf:\n                self.assertTrue((log_out in lf.read()))\n    return (mock_lineage, mock_get_utility, stdout)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef parse(config):\n    \" Create a validator that does an extract from body and applies a comparator,\\n            Then does comparison vs expected value\\n            Syntax sample:\\n              { jsonpath_mini: 'node.child',\\n                operator: 'eq',\\n                expected: 'myValue'\\n              }\\n        \"\n    output = ComparatorValidator()\n    config = parsing.lowercase_keys(parsing.flatten_dictionaries(config))\n    output.config = config\n    output.extractor = _get_extractor(config)\n    if (output.extractor is None):\n        raise ValueError('Extract function for comparison is not valid or not found!')\n    if ('comparator' not in config):\n        output.comparator_name = 'eq'\n    else:\n        output.comparator_name = config['comparator'].lower()\n    output.comparator = COMPARATORS[output.comparator_name]\n    if (not output.comparator):\n        raise ValueError('Invalid comparator given!')\n    try:\n        expected = config['expected']\n    except KeyError:\n        raise ValueError('No expected value found in comparator validator config, one must be!')\n    if (isinstance(expected, basestring) or isinstance(expected, (int, long, float, complex))):\n        output.expected = expected\n    elif isinstance(expected, dict):\n        expected = parsing.lowercase_keys(expected)\n        template = expected.get('template')\n        if template:\n            if (not isinstance(template, basestring)):\n                raise ValueError(\"Can't template a comparator-validator unless template value is a string\")\n            output.isTemplateExpected = True\n            output.expected = template\n        else:\n            output.expected = _get_extractor(expected)\n            if (not output.expected):\n                raise ValueError(\"Can't supply a non-template, non-extract dictionary to comparator-validator\")\n    return output\n", "label": 1}
{"function": "\n\n@contextmanager\ndef build_with_altered_context(self, name, conf, context, stream, dockerfile, volumes_from=None, command=None, tag=False):\n    conf_image_name = conf.prefixed_image_name\n    new_conf = conf.clone()\n    if (name is not None):\n        if tag:\n            new_name = '{0}-{1}'.format(conf.prefixed_image_name, name)\n        else:\n            new_name = None\n        new_conf.name = name\n        new_conf.image_name = new_name\n        new_conf.container_id = None\n        new_conf.container_name = '{0}-{1}'.format(new_name, str(uuid.uuid1())).replace('/', '__')\n    else:\n        new_name = conf.image_name\n    new_conf.bash = NotSpecified\n    new_conf.command = NotSpecified\n    if (command is not None):\n        new_conf.bash = command\n    if volumes_from:\n        new_conf.volumes = new_conf.volumes.clone()\n        new_conf.volumes.share_with = (list(conf.volumes.share_with) + volumes_from)\n    if (context is not None):\n        maker = context.clone_with_new_dockerfile(conf, dockerfile)\n    else:\n        new_conf.context = Context(enabled=False, parent_dir=new_conf.context.parent_dir)\n        maker = new_conf.make_context(docker_file=dockerfile)\n\n    @contextmanager\n    def remover(conf):\n        (yield)\n    if (new_name is not None):\n        remover = self.remove_replaced_images\n    with remover(new_conf):\n        cached = False\n        with maker as new_context:\n            cached = NormalBuilder(new_name).build(new_conf, new_context, stream)\n            new_conf.image_name = stream.current_container\n    (yield (new_conf, cached))\n", "label": 0}
{"function": "\n\ndef test_multinomial_coefficients():\n    assert (multinomial_coefficients(1, 1) == {\n        (1,): 1,\n    })\n    assert (multinomial_coefficients(1, 2) == {\n        (2,): 1,\n    })\n    assert (multinomial_coefficients(1, 3) == {\n        (3,): 1,\n    })\n    assert (multinomial_coefficients(2, 0) == {\n        (0, 0): 1,\n    })\n    assert (multinomial_coefficients(2, 1) == {\n        (0, 1): 1,\n        (1, 0): 1,\n    })\n    assert (multinomial_coefficients(2, 2) == {\n        (2, 0): 1,\n        (0, 2): 1,\n        (1, 1): 2,\n    })\n    assert (multinomial_coefficients(2, 3) == {\n        (3, 0): 1,\n        (1, 2): 3,\n        (0, 3): 1,\n        (2, 1): 3,\n    })\n    assert (multinomial_coefficients(3, 1) == {\n        (1, 0, 0): 1,\n        (0, 1, 0): 1,\n        (0, 0, 1): 1,\n    })\n    assert (multinomial_coefficients(3, 2) == {\n        (0, 1, 1): 2,\n        (0, 0, 2): 1,\n        (1, 1, 0): 2,\n        (0, 2, 0): 1,\n        (1, 0, 1): 2,\n        (2, 0, 0): 1,\n    })\n    mc = multinomial_coefficients(3, 3)\n    assert (mc == {\n        (2, 1, 0): 3,\n        (0, 3, 0): 1,\n        (1, 0, 2): 3,\n        (0, 2, 1): 3,\n        (0, 1, 2): 3,\n        (3, 0, 0): 1,\n        (2, 0, 1): 3,\n        (1, 2, 0): 3,\n        (1, 1, 1): 6,\n        (0, 0, 3): 1,\n    })\n    assert (dict(multinomial_coefficients_iterator(2, 0)) == {\n        (0, 0): 1,\n    })\n    assert (dict(multinomial_coefficients_iterator(2, 1)) == {\n        (0, 1): 1,\n        (1, 0): 1,\n    })\n    assert (dict(multinomial_coefficients_iterator(2, 2)) == {\n        (2, 0): 1,\n        (0, 2): 1,\n        (1, 1): 2,\n    })\n    assert (dict(multinomial_coefficients_iterator(3, 3)) == mc)\n    it = multinomial_coefficients_iterator(7, 2)\n    assert ([next(it) for i in range(4)] == [((2, 0, 0, 0, 0, 0, 0), 1), ((1, 1, 0, 0, 0, 0, 0), 2), ((0, 2, 0, 0, 0, 0, 0), 1), ((1, 0, 1, 0, 0, 0, 0), 2)])\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_dict(values):\n    '\\n        Instantiate a BlockadeConfig instance based on\\n        a given dictionary of configuration values\\n        '\n    try:\n        containers = values['containers']\n        parsed_containers = {\n            \n        }\n        for (name, container_dict) in containers.items():\n            try:\n                for cnt in BlockadeContainerConfig.from_dict(name, container_dict):\n                    if cnt.container_name:\n                        cname = cnt.container_name\n                        existing = [c for c in parsed_containers.values() if (c.container_name == cname)]\n                        if existing:\n                            raise BlockadeConfigError((\"Duplicate 'container_name' definition: %s\" % cname))\n                    parsed_containers[cnt.name] = cnt\n            except Exception as err:\n                raise BlockadeConfigError((\"Container '%s' config problem: %s\" % (name, err)))\n        network = values.get('network')\n        if network:\n            defaults = _DEFAULT_NETWORK_CONFIG.copy()\n            defaults.update(network)\n            network = defaults\n        else:\n            network = _DEFAULT_NETWORK_CONFIG.copy()\n        return BlockadeConfig(parsed_containers, network=network)\n    except KeyError as err:\n        raise BlockadeConfigError(('Config missing value: ' + str(err)))\n    except Exception as err:\n        raise BlockadeConfigError(('Failed to load config: ' + str(err)))\n", "label": 1}
{"function": "\n\ndef _stata_elapsed_date_to_datetime(date, fmt):\n    '\\n    Convert from SIF to datetime. http://www.stata.com/help.cgi?datetime\\n\\n    Parameters\\n    ----------\\n    date : int\\n        The Stata Internal Format date to convert to datetime according to fmt\\n    fmt : str\\n        The format to convert to. Can be, tc, td, tw, tm, tq, th, ty\\n\\n    Examples\\n    --------\\n    >>> _stata_elapsed_date_to_datetime(52, \"%tw\")                                datetime.datetime(1961, 1, 1, 0, 0)\\n\\n    Notes\\n    -----\\n    datetime/c - tc\\n        milliseconds since 01jan1960 00:00:00.000, assuming 86,400 s/day\\n    datetime/C - tC - NOT IMPLEMENTED\\n        milliseconds since 01jan1960 00:00:00.000, adjusted for leap seconds\\n    date - td\\n        days since 01jan1960 (01jan1960 = 0)\\n    weekly date - tw\\n        weeks since 1960w1\\n        This assumes 52 weeks in a year, then adds 7 * remainder of the weeks.\\n        The datetime value is the start of the week in terms of days in the\\n        year, not ISO calendar weeks.\\n    monthly date - tm\\n        months since 1960m1\\n    quarterly date - tq\\n        quarters since 1960q1\\n    half-yearly date - th\\n        half-years since 1960h1 yearly\\n    date - ty\\n        years since 0000\\n\\n    If you don\\'t have pandas with datetime support, then you can\\'t do\\n    milliseconds accurately.\\n    '\n    date = int(date)\n    stata_epoch = datetime.datetime(1960, 1, 1)\n    if (fmt in ['%tc', 'tc']):\n        from dateutil.relativedelta import relativedelta\n        return (stata_epoch + relativedelta(microseconds=(date * 1000)))\n    elif (fmt in ['%tC', 'tC']):\n        from warnings import warn\n        warn('Encountered %tC format. Leaving in Stata Internal Format.', UserWarning)\n        return date\n    elif (fmt in ['%td', 'td']):\n        return (stata_epoch + datetime.timedelta(int(date)))\n    elif (fmt in ['%tw', 'tw']):\n        year = datetime.datetime((stata_epoch.year + (date // 52)), 1, 1)\n        day_delta = ((date % 52) * 7)\n        return (year + datetime.timedelta(int(day_delta)))\n    elif (fmt in ['%tm', 'tm']):\n        year = (stata_epoch.year + (date // 12))\n        month_delta = ((date % 12) + 1)\n        return datetime.datetime(year, month_delta, 1)\n    elif (fmt in ['%tq', 'tq']):\n        year = (stata_epoch.year + (date // 4))\n        month_delta = (((date % 4) * 3) + 1)\n        return datetime.datetime(year, month_delta, 1)\n    elif (fmt in ['%th', 'th']):\n        year = (stata_epoch.year + (date // 2))\n        month_delta = (((date % 2) * 6) + 1)\n        return datetime.datetime(year, month_delta, 1)\n    elif (fmt in ['%ty', 'ty']):\n        if (date > 0):\n            return datetime.datetime(date, 1, 1)\n        else:\n            raise ValueError('Year 0 and before not implemented')\n    else:\n        raise ValueError(('Date fmt %s not understood' % fmt))\n", "label": 0}
{"function": "\n\ndef test_success(self):\n    count = CompatReport.objects.count()\n    r = self.client.post(self.url, self.json, content_type='application/json')\n    assert (r.status_code == 204)\n    assert (CompatReport.objects.count() == (count + 1))\n    cr = CompatReport.objects.order_by('-id')[0]\n    assert (cr.app_build == incoming_data['appBuild'])\n    assert (cr.app_guid == incoming_data['appGUID'])\n    assert (cr.works_properly == incoming_data['worksProperly'])\n    assert (cr.comments == incoming_data['comments'])\n    assert (cr.client_ip == '127.0.0.1')\n    vals = CompatReport.objects.filter(id=cr.id).values('other_addons')\n    assert (vals[0]['other_addons'] == json.dumps(incoming_data['otherAddons'], separators=(',', ':')))\n", "label": 0}
{"function": "\n\ndef _sync_recv_msg(self):\n    \"Internal use only; use 'recv_msg' instead.\\n\\n        Synchronous version of async_recv_msg.\\n        \"\n    n = struct.calcsize('>L')\n    try:\n        data = self._sync_recvall(n)\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            return ''\n        else:\n            raise\n    if (len(data) != n):\n        return ''\n    n = struct.unpack('>L', data)[0]\n    assert (n >= 0)\n    try:\n        data = self._sync_recvall(n)\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            return ''\n        else:\n            raise\n    if (len(data) != n):\n        return ''\n    return data\n", "label": 0}
{"function": "\n\ndef get_header(self, create=True):\n    try:\n        hdr = self.metadata['header']\n        if (self.inherit_primary_header and (self._primary_hdr is not None)):\n            displayhdr = AstroHeader()\n            for key in hdr.keyorder:\n                card = hdr.get_card(key)\n                bnch = displayhdr.__setitem__(card.key, card.value)\n                bnch.comment = card.comment\n            for key in self._primary_hdr.keyorder:\n                if (key not in hdr):\n                    card = self._primary_hdr.get_card(key)\n                    bnch = displayhdr.__setitem__(card.key, card.value)\n                    bnch.comment = card.comment\n        else:\n            displayhdr = hdr\n    except KeyError as e:\n        if (not create):\n            raise e\n        hdr = AstroHeader()\n        self.metadata['header'] = hdr\n        displayhdr = hdr\n    return displayhdr\n", "label": 0}
{"function": "\n\ndef new(self, key=None, data=None, content_type='application/json', encoded_data=None):\n    'A shortcut for manually instantiating a new\\n        :class:`~riak.riak_object.RiakObject` or a new\\n        :class:`~riak.datatypes.Datatype`, based on the presence and value\\n        of the :attr:`datatype <BucketType.datatype>` bucket property. When\\n        the bucket contains a :class:`~riak.datatypes.Datatype`, all\\n        arguments are ignored except ``key``, otherwise they are used to\\n        initialize the :class:`~riak.riak_object.RiakObject`.\\n\\n        :param key: Name of the key. Leaving this to be None (default)\\n                    will make Riak generate the key on store.\\n        :type key: str\\n        :param data: The data to store in a\\n           :class:`~riak.riak_object.RiakObject`, see\\n           :attr:`RiakObject.data <riak.riak_object.RiakObject.data>`.\\n        :type data: object\\n        :param content_type: The media type of the data stored in the\\n           :class:`~riak.riak_object.RiakObject`, see\\n           :attr:`RiakObject.content_type\\n           <riak.riak_object.RiakObject.content_type>`.\\n        :type content_type: str\\n        :param encoded_data: The encoded data to store in a\\n           :class:`~riak.riak_object.RiakObject`, see\\n           :attr:`RiakObject.encoded_data\\n           <riak.riak_object.RiakObject.encoded_data>`.\\n        :type encoded_data: str\\n        :rtype: :class:`~riak.riak_object.RiakObject` or\\n                :class:`~riak.datatypes.Datatype`\\n\\n        '\n    from riak import RiakObject\n    if self.bucket_type.datatype:\n        return TYPES[self.bucket_type.datatype](bucket=self, key=key)\n    if PY2:\n        try:\n            if isinstance(data, string_types):\n                data = data.encode('ascii')\n        except UnicodeError:\n            raise TypeError('Unicode data values are not supported.')\n    obj = RiakObject(self._client, self, key)\n    obj.content_type = content_type\n    if (data is not None):\n        obj.data = data\n    if (encoded_data is not None):\n        obj.encoded_data = encoded_data\n    return obj\n", "label": 0}
{"function": "\n\ndef test_sqrt_rounding():\n    for i in [2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15]:\n        i = from_int(i)\n        for dps in [7, 15, 83, 106, 2000]:\n            mp.dps = dps\n            a = mpf_pow_int(mpf_sqrt(i, mp.prec, round_down), 2, mp.prec, round_down)\n            b = mpf_pow_int(mpf_sqrt(i, mp.prec, round_up), 2, mp.prec, round_up)\n            assert mpf_lt(a, i)\n            assert mpf_gt(b, i)\n    random.seed(1234)\n    prec = 100\n    for rnd in [round_down, round_nearest, round_ceiling]:\n        for i in range(100):\n            a = mpf_rand(prec)\n            b = mpf_mul(a, a)\n            assert (mpf_sqrt(b, prec, rnd) == a)\n    mp.dps = 100\n    a = (mpf(9) + 1e-90)\n    b = (mpf(9) - 1e-90)\n    mp.dps = 15\n    assert (sqrt(a, rounding='d') == 3)\n    assert (sqrt(a, rounding='n') == 3)\n    assert (sqrt(a, rounding='u') > 3)\n    assert (sqrt(b, rounding='d') < 3)\n    assert (sqrt(b, rounding='n') == 3)\n    assert (sqrt(b, rounding='u') == 3)\n    assert (sqrt(mpf('7.0503726185518891')) == mpf('2.655253776675949'))\n", "label": 1}
{"function": "\n\ndef _unpack_response(response, cursor_id=None, as_class=dict, tz_aware=False, uuid_subtype=OLD_UUID_SUBTYPE, compile_re=True):\n    'Unpack a response from the database.\\n\\n    Check the response for errors and unpack, returning a dictionary\\n    containing the response data.\\n\\n    :Parameters:\\n      - `response`: byte string as returned from the database\\n      - `cursor_id` (optional): cursor_id we sent to get this response -\\n        used for raising an informative exception when we get cursor id not\\n        valid at server response\\n      - `as_class` (optional): class to use for resulting documents\\n    '\n    response_flag = struct.unpack('<i', response[:4])[0]\n    if (response_flag & 1):\n        assert (cursor_id is not None)\n        raise CursorNotFound((\"cursor id '%s' not valid at server\" % cursor_id))\n    elif (response_flag & 2):\n        error_object = bson.BSON(response[20:]).decode()\n        if error_object['$err'].startswith('not master'):\n            raise AutoReconnect(error_object['$err'])\n        elif (error_object.get('code') == 50):\n            raise ExecutionTimeout(error_object.get('$err'), error_object.get('code'), error_object)\n        raise OperationFailure(('database error: %s' % error_object.get('$err')), error_object.get('code'), error_object)\n    result = {\n        \n    }\n    result['cursor_id'] = struct.unpack('<q', response[4:12])[0]\n    result['starting_from'] = struct.unpack('<i', response[12:16])[0]\n    result['number_returned'] = struct.unpack('<i', response[16:20])[0]\n    result['data'] = bson.decode_all(response[20:], as_class, tz_aware, uuid_subtype, compile_re)\n    assert (len(result['data']) == result['number_returned'])\n    return result\n", "label": 0}
{"function": "\n\ndef reverted(name, snapshot=None, cleanup=False):\n    '\\n    .. deprecated:: 2016.3.0\\n\\n    Reverts to the particular snapshot.\\n\\n    .. versionadded:: 2016.3.0\\n\\n    .. code-block:: yaml\\n\\n        domain_name:\\n          virt.reverted:\\n            - cleanup: True\\n\\n        domain_name_1:\\n          virt.reverted:\\n            - snapshot: snapshot_name\\n            - cleanup: False\\n    '\n    ret = {\n        'name': name,\n        'changes': {\n            \n        },\n        'result': False,\n        'comment': '',\n    }\n    try:\n        domains = fnmatch.filter(__salt__['virt.list_domains'](), name)\n        if (not domains):\n            ret['comment'] = 'No domains found for criteria \"{0}\"'.format(name)\n        else:\n            ignored_domains = list()\n            if (len(domains) > 1):\n                ret['changes'] = {\n                    'reverted': list(),\n                }\n            for domain in domains:\n                result = {\n                    \n                }\n                try:\n                    result = __salt__['virt.revert_snapshot'](domain, snapshot=snapshot, cleanup=cleanup)\n                    result = {\n                        'domain': domain,\n                        'current': result['reverted'],\n                        'deleted': result['deleted'],\n                    }\n                except CommandExecutionError as err:\n                    if (len(domains) > 1):\n                        ignored_domains.append({\n                            'domain': domain,\n                            'issue': str(err),\n                        })\n                if (len(domains) > 1):\n                    if result:\n                        ret['changes']['reverted'].append(result)\n                else:\n                    ret['changes'] = result\n                    break\n            ret['result'] = (len(domains) != len(ignored_domains))\n            if ret['result']:\n                ret['comment'] = 'Domain{0} has been reverted'.format((((len(domains) > 1) and 's') or ''))\n            if ignored_domains:\n                ret['changes']['ignored'] = ignored_domains\n            if (not ret['changes']['reverted']):\n                ret['changes'].pop('reverted')\n    except libvirt.libvirtError as err:\n        ret['comment'] = str(err)\n    except CommandExecutionError as err:\n        ret['comment'] = str(err)\n    return ret\n", "label": 1}
{"function": "\n\ndef _discoro_process(_discoro_config, _discoro_server_id, _discoro_mp_queue, _discoro_auth):\n    import os\n    import logging\n    import asyncoro.disasyncoro as asyncoro\n    if _discoro_config['loglevel']:\n        asyncoro.logger.setLevel(logging.DEBUG)\n    else:\n        asyncoro.logger.setLevel(logging.INFO)\n    del _discoro_config['loglevel']\n    _discoro_config_msg = {\n        'req': 'config',\n        'id': _discoro_server_id,\n        'phoenix': _discoro_config.pop('phoenix', False),\n        'serve': _discoro_config.pop('serve', (- 1)),\n        'peers': _discoro_config.pop('peers', []),\n        'min_pulse_interval': _discoro_config.pop('min_pulse_interval'),\n        'max_pulse_interval': _discoro_config.pop('max_pulse_interval'),\n        'auth': _discoro_auth,\n        'mp_queue': _discoro_mp_queue,\n    }\n    _discoro_scheduler = asyncoro.AsynCoro(**_discoro_config)\n    _discoro_coro = asyncoro.Coro(_discoro_proc)\n    for _discoro_var in globals().keys():\n        if _discoro_var.startswith('_discoro_'):\n            globals().pop(_discoro_var)\n    del logging, os, _discoro_config, _discoro_var\n    (req_queue, _discoro_mp_queue) = (_discoro_mp_queue, None)\n    while 1:\n        try:\n            req = req_queue.get()\n        except KeyboardInterrupt:\n            req = {\n                'req': 'terminate',\n                'auth': _discoro_auth,\n            }\n            req_queue.put(req)\n            asyncoro.logger.debug('%s terminating', asyncoro.AsynCoro.instance()._location)\n        if ((not isinstance(req, dict)) or (req.get('auth') != _discoro_auth)):\n            asyncoro.logger.warning('Ignoring invalid request: \"%s\"', type(req))\n            continue\n        cmd = req.get('req')\n        if ((cmd == 'status') or (cmd == 'close')):\n            _discoro_coro.send(req)\n        elif (cmd == 'start'):\n            _discoro_config_msg['timer_coro'] = req.get('timer_coro', None)\n            _discoro_coro.send(_discoro_config_msg)\n            del _discoro_config_msg\n        elif ((cmd == 'quit') or (cmd == 'terminate')):\n            _discoro_coro.send(req)\n            break\n    _discoro_scheduler.finish()\n", "label": 1}
{"function": "\n\ndef optwrap(text):\n    'Wrap all paragraphs in the provided text.'\n    if (not BODY_WIDTH):\n        return text\n    assert wrap, 'Requires Python 2.3.'\n    result = ''\n    newlines = 0\n    for para in text.split('\\n'):\n        if (len(para) > 0):\n            if ((para[0] != ' ') and (para[0] != '-') and (para[0] != '*')):\n                for line in wrap(para, BODY_WIDTH):\n                    result += (line + '\\n')\n                result += '\\n'\n                newlines = 2\n            elif (not onlywhite(para)):\n                result += (para + '\\n')\n                newlines = 1\n        elif (newlines < 2):\n            result += '\\n'\n            newlines += 1\n    return result\n", "label": 1}
{"function": "\n\ndef _callback_write(self, buf, key=PiVideoFrameType.frame):\n    '\\n        Extended to implement video frame meta-data tracking, and to handle\\n        splitting video recording to the next output when :meth:`split` is\\n        called.\\n        '\n    self.frame = PiVideoFrame(index=((self.frame.index + 1) if self.frame.complete else self.frame.index), frame_type=(PiVideoFrameType.key_frame if (buf[0].flags & mmal.MMAL_BUFFER_HEADER_FLAG_KEYFRAME) else (PiVideoFrameType.sps_header if (buf[0].flags & mmal.MMAL_BUFFER_HEADER_FLAG_CONFIG) else (PiVideoFrameType.motion_data if (buf[0].flags & mmal.MMAL_BUFFER_HEADER_FLAG_CODECSIDEINFO) else PiVideoFrameType.frame))), frame_size=(buf[0].length if self.frame.complete else (self.frame.frame_size + buf[0].length)), video_size=(self.frame.video_size if (buf[0].flags & mmal.MMAL_BUFFER_HEADER_FLAG_CODECSIDEINFO) else (self.frame.video_size + buf[0].length)), split_size=(self.frame.split_size if (buf[0].flags & mmal.MMAL_BUFFER_HEADER_FLAG_CODECSIDEINFO) else (self.frame.split_size + buf[0].length)), timestamp=(None if (buf[0].pts in (0, mmal.MMAL_TIME_UNKNOWN)) else buf[0].pts), complete=bool((buf[0].flags & mmal.MMAL_BUFFER_HEADER_FLAG_FRAME_END)))\n    if ((self.format != 'h264') or (buf[0].flags & mmal.MMAL_BUFFER_HEADER_FLAG_CONFIG)):\n        with self.outputs_lock:\n            try:\n                new_outputs = self._next_output.pop(0)\n            except IndexError:\n                new_outputs = None\n        if new_outputs:\n            for (new_key, new_output) in new_outputs.items():\n                self._close_output(new_key)\n                self._open_output(new_output, new_key)\n                if (new_key == PiVideoFrameType.frame):\n                    self.frame = PiVideoFrame(index=self.frame.index, frame_type=self.frame.frame_type, frame_size=self.frame.frame_size, video_size=self.frame.video_size, split_size=0, timestamp=self.frame.timestamp, complete=self.frame.complete)\n            self.event.set()\n    if (buf[0].flags & mmal.MMAL_BUFFER_HEADER_FLAG_CODECSIDEINFO):\n        key = PiVideoFrameType.motion_data\n    return super(PiVideoEncoder, self)._callback_write(buf, key)\n", "label": 1}
{"function": "\n\ndef test_set_cookie():\n    cookiejar = FileCookieJar()\n    _cookie = {\n        'value_0': 'v_0',\n        'value_1': 'v_1',\n        'value_2': 'v_2',\n    }\n    c = SimpleCookie(_cookie)\n    domain_0 = '.test_domain'\n    domain_1 = 'test_domain'\n    max_age = '09 Feb 1994 22:23:32 GMT'\n    expires = http2time(max_age)\n    path = 'test/path'\n    c['value_0']['max-age'] = max_age\n    c['value_0']['domain'] = domain_0\n    c['value_0']['path'] = path\n    c['value_1']['domain'] = domain_1\n    util.set_cookie(cookiejar, c)\n    cookies = cookiejar._cookies\n    c_0 = cookies[domain_0][path]['value_0']\n    c_1 = cookies[domain_1]['']['value_1']\n    c_2 = cookies['']['']['value_2']\n    assert (not (c_2.domain_specified and c_2.path_specified))\n    assert (c_1.domain_specified and (not c_1.domain_initial_dot) and (not c_1.path_specified))\n    assert (c_0.domain_specified and c_0.domain_initial_dot and c_0.path_specified)\n    assert (c_0.expires == expires)\n    assert (c_0.domain == domain_0)\n    assert (c_0.name == 'value_0')\n    assert (c_0.path == path)\n    assert (c_0.value == 'v_0')\n    assert (not c_1.expires)\n    assert (c_1.domain == domain_1)\n    assert (c_1.name == 'value_1')\n    assert (c_1.path == '')\n    assert (c_1.value == 'v_1')\n    assert (not c_2.expires)\n    assert (c_2.domain == '')\n    assert (c_2.name == 'value_2')\n    assert (c_2.path == '')\n    assert (c_2.value == 'v_2')\n", "label": 1}
{"function": "\n\ndef _parse_settings_bond_0(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond0.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '0',\n    }\n    valid = ['list of ips (up to 16)']\n    if ('arp_ip_target' in opts):\n        if isinstance(opts['arp_ip_target'], list):\n            if (1 <= len(opts['arp_ip_target']) <= 16):\n                bond.update({\n                    'arp_ip_target': '',\n                })\n                for ip in opts['arp_ip_target']:\n                    if (len(bond['arp_ip_target']) > 0):\n                        bond['arp_ip_target'] = ((bond['arp_ip_target'] + ',') + ip)\n                    else:\n                        bond['arp_ip_target'] = ip\n            else:\n                _raise_error_iface(iface, 'arp_ip_target', valid)\n        else:\n            _raise_error_iface(iface, 'arp_ip_target', valid)\n    else:\n        _raise_error_iface(iface, 'arp_ip_target', valid)\n    if ('arp_interval' in opts):\n        try:\n            int(opts['arp_interval'])\n            bond.update({\n                'arp_interval': opts['arp_interval'],\n            })\n        except ValueError:\n            _raise_error_iface(iface, 'arp_interval', ['integer'])\n    else:\n        _log_default_iface(iface, 'arp_interval', bond_def['arp_interval'])\n        bond.update({\n            'arp_interval': bond_def['arp_interval'],\n        })\n    return bond\n", "label": 0}
{"function": "\n\ndef generate_gcd_app(app_id):\n    'Generates an app in tmp for a cloud datastore implementation.'\n    if (sys.platform == 'win32'):\n        user_format = ''\n    else:\n        try:\n            user_name = getpass.getuser()\n        except Exception:\n            user_format = ''\n        else:\n            user_format = ('.%s' % user_name)\n    tempdir = tempfile.gettempdir()\n    version = sdk_update_checker.GetVersionObject()\n    sdk_version = (version['release'] if version else 'unknown')\n    gcd_path = os.path.join(tempdir, ('appengine-gcd-war.%s%s%s' % (sdk_version, app_id, user_format)))\n    if (not os.path.exists(gcd_path)):\n        os.mkdir(gcd_path)\n        webinf_path = os.path.join(gcd_path, 'WEB-INF')\n        os.mkdir(webinf_path)\n        filter_path = os.path.join(webinf_path, 'lib')\n        os.mkdir(filter_path)\n        if runtime_factories.java_runtime:\n            filter_jar = _get_filter_jar()\n            shutil.copy(filter_jar, filter_path)\n    with open(os.path.join(gcd_path, 'WEB-INF', 'web.xml'), 'w') as f:\n        f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<web-app version=\"2.5\" xmlns=\"http://java.sun.com/xml/ns/javaee\"\\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n         xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee\\n         http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\">\\n\\n  <security-constraint>\\n    <web-resource-collection>\\n      <web-resource-name>datastore_constraint</web-resource-name>\\n      <url-pattern>/datastore/*</url-pattern>\\n    </web-resource-collection>\\n    <user-data-constraint>\\n      <transport-guarantee>CONFIDENTIAL</transport-guarantee>\\n    </user-data-constraint>\\n  </security-constraint>\\n\\n  <filter>\\n    <filter-name>ProtoJsonFilter</filter-name>\\n    <filter-class>\\n      com.google.apphosting.client.datastoreservice.app.filter.ProtoJsonFilter\\n    </filter-class>\\n  </filter>\\n\\n  <filter-mapping>\\n    <filter-name>ProtoJsonFilter</filter-name>\\n    <url-pattern>/datastore/*</url-pattern>\\n  </filter-mapping>\\n\\n  <servlet>\\n    <servlet-name>DatastoreApiServlet</servlet-name>\\n    <servlet-class>\\n      com.google.apphosting.client.datastoreservice.app.DatastoreApiServlet\\n    </servlet-class>\\n    <load-on-startup>1</load-on-startup>\\n  </servlet>\\n\\n  <servlet-mapping>\\n    <servlet-name>DatastoreApiServlet</servlet-name>\\n    <url-pattern>/datastore/*</url-pattern>\\n  </servlet-mapping>\\n\\n</web-app>')\n    gcd_app_xml = os.path.join(gcd_path, 'WEB-INF', 'appengine-web.xml')\n    with open(gcd_app_xml, 'w') as f:\n        f.write(('<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<appengine-web-app xmlns=\"http://appengine.google.com/ns/1.0\">\\n  <application>%s</application>\\n  <version>1</version>\\n  <module>google-cloud-datastore</module>\\n\\n  <precompilation-enabled>true</precompilation-enabled>\\n  <threadsafe>true</threadsafe>\\n\\n</appengine-web-app>' % app_id))\n    return gcd_app_xml\n", "label": 0}
{"function": "\n\ndef mapKeyword2Script(path):\n    'collect keywords from scripts.'\n    map_keyword2script = collections.defaultdict(list)\n    for script in glob.glob(os.path.join(path, '*.py')):\n        s = os.path.basename(script)[:(- 3)]\n        with open(script, 'r') as inf:\n            data = [x for x in inf.readlines(10000) if x.startswith(':Tags:')]\n            if data:\n                keywords = [x.strip() for x in data[0][6:].split(' ')]\n                for x in keywords:\n                    if x:\n                        map_keyword2script[x].append(s)\n    return map_keyword2script\n", "label": 0}
{"function": "\n\ndef test_define(self):\n    assert (self.l2_net_dev.forward.mode == 'nat')\n    self.l2_net_dev.define()\n    assert isinstance(self.l2_net_dev.uuid, str)\n    assert (len(self.l2_net_dev.uuid) == 36)\n    assert (self.l2_net_dev.network_name() == 'test_env_test_l2_net_dev')\n    assert (self.l2_net_dev.exists() is True)\n    assert (self.l2_net_dev.is_active() == 0)\n    assert (self.l2_net_dev.bridge_name() == 'virbr1')\n    assert (self.l2_net_dev._libvirt_network.autostart() == 1)\n    xml = self.l2_net_dev._libvirt_network.XMLDesc(0)\n    assert (xml == \"<network>\\n  <name>test_env_test_l2_net_dev</name>\\n  <uuid>{0}</uuid>\\n  <forward mode='nat'/>\\n  <bridge name='virbr1' stp='on' delay='0'/>\\n  <ip address='172.0.0.1' prefix='24'>\\n  </ip>\\n</network>\\n\".format(self.l2_net_dev.uuid))\n", "label": 0}
{"function": "\n\ndef _get_conn(self, timeout=None):\n    '\\n        Get a connection. Will return a pooled connection if one is available.\\n\\n        If no connections are available and :prop:`.block` is ``False``, then a\\n        fresh connection is returned.\\n\\n        :param timeout:\\n            Seconds to wait before giving up and raising\\n            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and\\n            :prop:`.block` is ``True``.\\n        '\n    conn = None\n    try:\n        conn = self.pool.get(block=self.block, timeout=timeout)\n    except AttributeError:\n        raise ClosedPoolError(self, 'Pool is closed.')\n    except Empty:\n        if self.block:\n            raise EmptyPoolError(self, 'Pool reached maximum size and no more connections are allowed.')\n        pass\n    if (conn and is_connection_dropped(conn)):\n        log.info(('Resetting dropped connection: %s' % self.host))\n        conn.close()\n    return (conn or self._new_conn())\n", "label": 0}
{"function": "\n\ndef get_placeholders(template_name):\n    'Return a list of PlaceholderNode found in the given template.\\n\\n    :param template_name: the name of the template file\\n    '\n    dummy_context.template = template.Template('')\n    try:\n        temp_wrapper = template.loader.get_template(template_name)\n    except template.TemplateDoesNotExist:\n        return []\n    (plist, blist) = ([], [])\n    temp = temp_wrapper.template\n    _placeholders_recursif(temp.nodelist, plist, blist)\n    previous = {\n        \n    }\n    block_to_remove = []\n    for block in blist:\n        if (block.name in previous):\n            if (not hasattr(block, 'has_super_var')):\n                block_to_remove.append(previous[block.name])\n        previous[block.name] = block\n\n    def keep(p):\n        return (not (p.found_in_block in block_to_remove))\n    placeholders = [p for p in plist if keep(p)]\n    names = []\n    pfiltered = []\n    for p in placeholders:\n        if (p.ctype not in names):\n            pfiltered.append(p)\n            names.append(p.ctype)\n    return pfiltered\n", "label": 0}
{"function": "\n\ndef test_generate_with_params(self):\n    api = APISpecification(version='v1', base_url='http://api.globo.com')\n    api.add_resource(Resource('dogs', paths=[Path('/dogs/{key}', params=[Param('key')], methods=[Method('POST'), Method('GET')])]))\n    result = self.gen(api)\n    doc = ElementTree.fromstring(result)\n    resources = doc.getchildren()[0]\n    resource = resources.getchildren()[0]\n    param = resource.getchildren()[0]\n    assert param.tag.endswith('param')\n    assert (param.get('name') == 'key')\n    assert (param.get('required') == 'true')\n    assert (param.get('type') == 'xsd:string')\n    assert (param.get('style') == 'template')\n    method_1 = resource.getchildren()[1]\n    assert method_1.tag.endswith('method')\n    assert (method_1.get('name') == 'POST')\n    method_2 = resource.getchildren()[2]\n    assert method_2.tag.endswith('method')\n    assert (method_2.get('name') == 'GET')\n", "label": 0}
{"function": "\n\ndef visit_Const(self, expr):\n    t = expr.type\n    c = t.__class__\n    if (c == BoolT):\n        return ('1' if expr.value else '0')\n    elif (c == NoneT):\n        return '0'\n    assert isinstance(t, ScalarT), (\"Don't know how to translate Const %s : %s\" % (expr, t))\n    v = expr.value\n    if np.isinf(v):\n        return 'INFINITY'\n    elif np.isnan(v):\n        return 'NAN'\n    return ('%s' % expr.value)\n", "label": 0}
{"function": "\n\ndef _update_tickets(self, tickets, changeset, comment, date):\n    'Update the tickets with the given comment.'\n    authname = self._authname(changeset)\n    perm = PermissionCache(self.env, authname)\n    for (tkt_id, cmds) in tickets.iteritems():\n        try:\n            self.log.debug('Updating ticket #%d', tkt_id)\n            save = False\n            with self.env.db_transaction:\n                ticket = Ticket(self.env, tkt_id)\n                ticket_perm = perm(ticket.resource)\n                for cmd in cmds:\n                    if (cmd(ticket, changeset, ticket_perm) is not False):\n                        save = True\n                if save:\n                    ticket.save_changes(authname, comment, date)\n            if save:\n                self._notify(ticket, date, changeset.author, comment)\n        except Exception as e:\n            self.log.error('Unexpected error while processing ticket #%s: %s', tkt_id, exception_to_unicode(e))\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    try:\n        record.message = record.getMessage()\n    except Exception as e:\n        record.message = ('Bad message (%r): %r' % (e, record.__dict__))\n    record.asctime = time.strftime('%y%m%d %H:%M:%S', self.converter(record.created))\n    prefix = ('[%(levelname)1.1s %(asctime)s %(module)s:%(lineno)d]' % record.__dict__)\n    if self._color:\n        prefix = ((self._colors.get(record.levelno, self._normal) + prefix) + self._normal)\n\n    def safe_unicode(s):\n        try:\n            return unicode(s)\n        except UnicodeDecodeError:\n            return repr(s)\n    formatted = ((prefix + ' ') + safe_unicode(record.message))\n    if record.exc_info:\n        if (not record.exc_text):\n            record.exc_text = self.formatException(record.exc_info)\n    if record.exc_text:\n        lines = [formatted.rstrip()]\n        lines.extend((safe_unicode(ln) for ln in record.exc_text.split('\\n')))\n        formatted = '\\n'.join(lines)\n    return formatted.replace('\\n', '\\n    ')\n", "label": 0}
{"function": "\n\n@csrf_protect_m\n@transaction.commit_on_success\ndef add_view(self, request, form_url='', extra_context=None):\n    \"The 'add' admin view for this model.\"\n    model = self.model\n    opts = model._meta\n    if (not self.has_add_permission(request)):\n        raise PermissionDenied\n    ModelForm = self.get_form(request)\n    formsets = []\n    if (request.method == 'POST'):\n        form = ModelForm(request.POST, request.FILES)\n        if form.is_valid():\n            new_object = self.save_form(request, form, change=False)\n            form_validated = True\n        else:\n            form_validated = False\n            new_object = self.model()\n        prefixes = {\n            \n        }\n        for (FormSet, inline) in zip(self.get_formsets(request), self.inline_instances):\n            prefix = FormSet.get_default_prefix()\n            prefixes[prefix] = (prefixes.get(prefix, 0) + 1)\n            if (prefixes[prefix] != 1):\n                prefix = ('%s-%s' % (prefix, prefixes[prefix]))\n            formset = FormSet(data=request.POST, files=request.FILES, instance=new_object, save_as_new=request.POST.has_key('_saveasnew'), prefix=prefix, queryset=inline.queryset(request))\n            formsets.append(formset)\n        if (all_valid(formsets) and form_validated):\n            self.save_model(request, new_object, form, change=False)\n            form.save_m2m()\n            for formset in formsets:\n                self.save_formset(request, form, formset, change=False)\n            self.log_addition(request, new_object)\n            return self.response_add(request, new_object)\n    else:\n        initial = dict(request.GET.items())\n        for k in initial:\n            try:\n                f = opts.get_field(k)\n            except models.FieldDoesNotExist:\n                continue\n            if isinstance(f, models.ManyToManyField):\n                initial[k] = initial[k].split(',')\n        form = ModelForm(initial=initial)\n        prefixes = {\n            \n        }\n        for (FormSet, inline) in zip(self.get_formsets(request), self.inline_instances):\n            prefix = FormSet.get_default_prefix()\n            prefixes[prefix] = (prefixes.get(prefix, 0) + 1)\n            if (prefixes[prefix] != 1):\n                prefix = ('%s-%s' % (prefix, prefixes[prefix]))\n            formset = FormSet(instance=self.model(), prefix=prefix, queryset=inline.queryset(request))\n            formsets.append(formset)\n    adminForm = helpers.AdminForm(form, list(self.get_fieldsets(request)), self.prepopulated_fields, self.get_readonly_fields(request), model_admin=self)\n    media = (self.media + adminForm.media)\n    inline_admin_formsets = []\n    for (inline, formset) in zip(self.inline_instances, formsets):\n        fieldsets = list(inline.get_fieldsets(request))\n        readonly = list(inline.get_readonly_fields(request))\n        inline_admin_formset = helpers.InlineAdminFormSet(inline, formset, fieldsets, readonly, model_admin=self)\n        inline_admin_formsets.append(inline_admin_formset)\n        media = (media + inline_admin_formset.media)\n    context = {\n        'title': (_('Add %s') % force_unicode(opts.verbose_name)),\n        'adminform': adminForm,\n        'is_popup': request.REQUEST.has_key('_popup'),\n        'show_delete': False,\n        'media': mark_safe(media),\n        'inline_admin_formsets': inline_admin_formsets,\n        'errors': helpers.AdminErrorList(form, formsets),\n        'root_path': self.admin_site.root_path,\n        'app_label': opts.app_label,\n    }\n    context.update((extra_context or {\n        \n    }))\n    return self.render_change_form(request, context, form_url=form_url, add=True)\n", "label": 1}
{"function": "\n\ndef collect(self):\n    '\\n        Collect metrics\\n        '\n    collected = {\n        \n    }\n    files = []\n    if isinstance(self.config['dir'], basestring):\n        dirs = [d.strip() for d in self.config['dir'].split(',')]\n    elif isinstance(self.config['dir'], list):\n        dirs = self.config['dir']\n    if isinstance(self.config['files'], basestring):\n        files = [f.strip() for f in self.config['files'].split(',')]\n    elif isinstance(self.config['files'], list):\n        files = self.config['files']\n    for sdir in dirs:\n        for sfile in files:\n            if sfile.endswith('conntrack_count'):\n                metric_name = 'ip_conntrack_count'\n            elif sfile.endswith('conntrack_max'):\n                metric_name = 'ip_conntrack_max'\n            else:\n                self.log.error('Unknown file for collection: %s', sfile)\n                continue\n            fpath = os.path.join(sdir, sfile)\n            if (not os.path.exists(fpath)):\n                continue\n            try:\n                with open(fpath, 'r') as fhandle:\n                    metric = float(fhandle.readline().rstrip('\\n'))\n                    collected[metric_name] = metric\n            except Exception as exception:\n                self.log.error(\"Failed to collect from '%s': %s\", fpath, exception)\n    if (not collected):\n        self.log.error('No metric was collected, looks like nf_conntrack/ip_conntrack kernel module was not loaded')\n    else:\n        for key in collected.keys():\n            self.publish(key, collected[key])\n", "label": 1}
{"function": "\n\n@register.inclusion_tag('auxiliary/_bar.html')\ndef bar(quantity, start, end, bar_class=None, show_label=True):\n    '\\n    Draws a bar.\\n\\n    :param quantity: The quantity to represent\\n    :param start: Start of scale\\n    :param end: End of scale\\n    :param bar_class: Bootstrap bar class (One of: info, success, warning,\\n                      danger). If not passed, will be calculated from percentage\\n    :param show_label: Show the descriptive label ?\\n    '\n    assert (quantity <= end), 'bar: quantity > end'\n    try:\n        value = (((quantity - start) * 100) / (end - start))\n    except (TypeError, ZeroDivisionError):\n        return {\n            'applicable': False,\n        }\n    VALUES = ((20, 'danger', _('Extremely below average')), (40, 'warning', _('Below average')), (60, 'info', _('Average')), (80, 'info', _('Above average')), (101, 'success', _('Extremely above average')))\n    for (boundary, css_class, label) in VALUES:\n        if (value < boundary):\n            break\n    if (not bar_class):\n        bar_class = css_class\n    if (not show_label):\n        label = ''\n    return {\n        'width': value,\n        'bar_class': bar_class,\n        'label': label,\n        'quantity': quantity,\n        'applicable': True,\n    }\n", "label": 0}
{"function": "\n\ndef RunTest():\n    try:\n        arcpy.AddMessage('Starting Test: TestRemoveDuplicateData')\n        inputTrackPointsFC = os.path.join(TestUtilities.inputGDB, 'GPSData')\n        outputPointsFC = os.path.join(TestUtilities.outputGDB, 'GPSData_Duplicates')\n        try:\n            desc = arcpy.Describe(outputPointsFC)\n            if (desc != None):\n                print(('Deleting: ' + str(outputPointsFC)))\n                arcpy.Delete_management(outputPointsFC)\n        except:\n            print(('Delete failed for: ' + str(outputPointsFC)))\n        try:\n            print(((('Copying ' + str(inputTrackPointsFC)) + ' --> ') + str(outputPointsFC)))\n            arcpy.Copy_management(inputTrackPointsFC, outputPointsFC)\n        except:\n            print(((('Copy failed for: ' + str(inputTrackPointsFC)) + ':') + str(outputPointsFC)))\n        toolbox = TestUtilities.toolbox\n        print(('Running from: ' + str(TestUtilities.currentPath)))\n        print(('Geodatabase path: ' + str(TestUtilities.geodatabasePath)))\n        arcpy.env.overwriteOutput = True\n        arcpy.ImportToolbox(toolbox, 'pdc')\n        arcpy.RemoveDuplicateGPSData_pdc(outputPointsFC)\n        inputFeatureCount = int(arcpy.GetCount_management(inputTrackPointsFC).getOutput(0))\n        print(('Input FeatureClass: ' + str(inputTrackPointsFC)))\n        print(('Input Feature Count: ' + str(inputFeatureCount)))\n        if (inputFeatureCount < 1):\n            print(('Invalid Output Feature Count: ' + str(inputFeatureCount)))\n            raise Exception('Test Failed')\n        outputFeatureCount = int(arcpy.GetCount_management(outputPointsFC).getOutput(0))\n        print(('Output FeatureClass: ' + str(outputPointsFC)))\n        print(('Output Feature Count: ' + str(outputFeatureCount)))\n        if (outputFeatureCount >= inputFeatureCount):\n            print(('Output Feature Count >= Input Feature Count: ' + str(outputFeatureCount)))\n            raise Exception('Test Failed')\n        print('Test Successful')\n    except arcpy.ExecuteError:\n        msgs = arcpy.GetMessages()\n        arcpy.AddError(msgs)\n        sys.exit((- 1))\n    except Exception as e:\n        tb = sys.exc_info()[2]\n        tbinfo = traceback.format_tb(tb)[0]\n        pymsg = ((('PYTHON ERRORS:\\nTraceback info:\\n' + tbinfo) + '\\nError Info:\\n') + str(sys.exc_info()[1]))\n        msgs = (('ArcPy ERRORS:\\n' + arcpy.GetMessages()) + '\\n')\n        arcpy.AddError(pymsg)\n        arcpy.AddError(msgs)\n        sys.exit((- 1))\n", "label": 0}
{"function": "\n\ndef download_default_pages(names, prefix):\n    from httplib import HTTPSConnection\n    host = 'trac.edgewall.org'\n    if (prefix and (not prefix.endswith('/'))):\n        prefix += '/'\n    conn = HTTPSConnection(host)\n    for name in names:\n        if (name in ('WikiStart', 'SandBox')):\n            continue\n        sys.stdout.write(('Downloading %s%s' % (prefix, name)))\n        conn.request('GET', ('/wiki/%s%s?format=txt' % (prefix, name)))\n        response = conn.getresponse()\n        content = response.read()\n        if (prefix and ((response.status != 200) or (not content))):\n            sys.stdout.write((' %s' % name))\n            conn.request('GET', ('/wiki/%s?format=txt' % name))\n            response = conn.getresponse()\n            content = response.read()\n        if ((response.status == 200) and content):\n            with open(('trac/wiki/default-pages/' + name), 'w') as f:\n                lines = content.replace('\\r\\n', '\\n').splitlines(True)\n                f.write(''.join((line for line in lines if (line.strip() != '[[TranslatedPages]]'))))\n            sys.stdout.write('\\tdone.\\n')\n        else:\n            sys.stdout.write('\\tmissing or empty.\\n')\n    conn.close()\n", "label": 1}
{"function": "\n\ndef test_reset_repeating(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, repeat=True, _time_function=time)\n    time.time = 12\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 8)\n    timer.reset()\n    assert (timer.sleep_time() == 10)\n    time.time = 34\n    assert (timer.sleep_time() == 0)\n    timer.reset()\n    assert (timer.sleep_time() == 10)\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 10)\n", "label": 0}
{"function": "\n\ndef test_contsant_sleep(self):\n    time = TestingTimeFunction()\n    manager = TimerManager(_time_function=time)\n    c5 = MockCallback()\n    manager.add_timer(5, c5, repeat=True)\n    c7 = MockCallback()\n    manager.add_timer(7, c7, repeat=True)\n    c13 = MockCallback()\n    manager.add_timer(13, c13, repeat=True)\n    for time.time in xrange(100):\n        manager.run()\n        if (time.time == 42):\n\n            def check_time():\n                assert (time.time == 48)\n            one_shot = MockCallback()\n            manager.add_timer(6, check_time)\n            manager.add_timer(6, one_shot)\n    assert (c5.nb_calls == 19)\n    assert (c7.nb_calls == 14)\n    assert (c13.nb_calls == 7)\n    assert (one_shot.nb_calls == 1)\n", "label": 0}
{"function": "\n\ndef _packagePaths(self):\n    '\\n        Yield a sequence of FilePath-like objects which represent path segments.\\n        '\n    if (not self.isPackage()):\n        return\n    if self.isLoaded():\n        load = self.load()\n        if hasattr(load, '__path__'):\n            for fn in load.__path__:\n                if (fn == self.parentPath.path):\n                    assert self.parentPath.exists()\n                    (yield self.parentPath)\n                else:\n                    smp = self.pathEntry.pythonPath._smartPath(fn)\n                    if smp.exists():\n                        (yield smp)\n    else:\n        (yield self.parentPath)\n", "label": 0}
{"function": "\n\ndef _thread_run(self, **kwargs):\n    input_fn = kwargs.get('input_fn')\n    queue = kwargs.get('queue')\n    device = kwargs.get('device')\n    drs = kwargs.get('default_ranges').get\n    touches = {\n        \n    }\n    touches_sent = []\n    point = {\n        \n    }\n    l_points = {\n        \n    }\n\n    def assign_coord(point, value, invert, coords):\n        (cx, cy) = coords\n        if invert:\n            value = (1.0 - value)\n        if (rotation == 0):\n            point[cx] = value\n        elif (rotation == 90):\n            point[cy] = value\n        elif (rotation == 180):\n            point[cx] = (1.0 - value)\n        elif (rotation == 270):\n            point[cy] = (1.0 - value)\n\n    def process(points):\n        for args in points:\n            if ('id' not in args):\n                continue\n            tid = args['id']\n            try:\n                touch = touches[tid]\n            except KeyError:\n                touch = MTDMotionEvent(device, tid, args)\n                touches[touch.id] = touch\n            touch.move(args)\n            action = 'update'\n            if (tid not in touches_sent):\n                action = 'begin'\n                touches_sent.append(tid)\n            if ('delete' in args):\n                action = 'end'\n                del args['delete']\n                del touches[touch.id]\n                touches_sent.remove(tid)\n                touch.update_time_end()\n            queue.append((action, touch))\n\n    def normalize(value, vmin, vmax):\n        return ((value - vmin) / float((vmax - vmin)))\n    _fn = input_fn\n    _slot = 0\n    _device = Device(_fn)\n    _changes = set()\n    ab = _device.get_abs(MTDEV_ABS_POSITION_X)\n    range_min_position_x = drs('min_position_x', ab.minimum)\n    range_max_position_x = drs('max_position_x', ab.maximum)\n    Logger.info(('MTD: <%s> range position X is %d - %d' % (_fn, range_min_position_x, range_max_position_x)))\n    ab = _device.get_abs(MTDEV_ABS_POSITION_Y)\n    range_min_position_y = drs('min_position_y', ab.minimum)\n    range_max_position_y = drs('max_position_y', ab.maximum)\n    Logger.info(('MTD: <%s> range position Y is %d - %d' % (_fn, range_min_position_y, range_max_position_y)))\n    ab = _device.get_abs(MTDEV_ABS_TOUCH_MAJOR)\n    range_min_major = drs('min_touch_major', ab.minimum)\n    range_max_major = drs('max_touch_major', ab.maximum)\n    Logger.info(('MTD: <%s> range touch major is %d - %d' % (_fn, range_min_major, range_max_major)))\n    ab = _device.get_abs(MTDEV_ABS_TOUCH_MINOR)\n    range_min_minor = drs('min_touch_minor', ab.minimum)\n    range_max_minor = drs('max_touch_minor', ab.maximum)\n    Logger.info(('MTD: <%s> range touch minor is %d - %d' % (_fn, range_min_minor, range_max_minor)))\n    range_min_pressure = drs('min_pressure', 0)\n    range_max_pressure = drs('max_pressure', 255)\n    Logger.info(('MTD: <%s> range pressure is %d - %d' % (_fn, range_min_pressure, range_max_pressure)))\n    invert_x = int(bool(drs('invert_x', 0)))\n    invert_y = int(bool(drs('invert_y', 0)))\n    Logger.info(('MTD: <%s> axes invertion: X is %d, Y is %d' % (_fn, invert_x, invert_y)))\n    rotation = drs('rotation', 0)\n    Logger.info(('MTD: <%s> rotation set to %d' % (_fn, rotation)))\n    while _device:\n        while _device.idle(1000):\n            continue\n        while True:\n            data = _device.get()\n            if (data is None):\n                break\n            if ((data.type == MTDEV_TYPE_EV_ABS) and (data.code == MTDEV_CODE_SLOT)):\n                _slot = data.value\n                continue\n            if (not (_slot in l_points)):\n                l_points[_slot] = dict()\n            point = l_points[_slot]\n            ev_value = data.value\n            ev_code = data.code\n            if (ev_code == MTDEV_CODE_POSITION_X):\n                val = normalize(ev_value, range_min_position_x, range_max_position_x)\n                assign_coord(point, val, invert_x, 'xy')\n            elif (ev_code == MTDEV_CODE_POSITION_Y):\n                val = (1.0 - normalize(ev_value, range_min_position_y, range_max_position_y))\n                assign_coord(point, val, invert_y, 'yx')\n            elif (ev_code == MTDEV_CODE_PRESSURE):\n                point['pressure'] = normalize(ev_value, range_min_pressure, range_max_pressure)\n            elif (ev_code == MTDEV_CODE_TOUCH_MAJOR):\n                point['size_w'] = normalize(ev_value, range_min_major, range_max_major)\n            elif (ev_code == MTDEV_CODE_TOUCH_MINOR):\n                point['size_h'] = normalize(ev_value, range_min_minor, range_max_minor)\n            elif (ev_code == MTDEV_CODE_TRACKING_ID):\n                if (ev_value == (- 1)):\n                    point['delete'] = True\n                    _changes.add(_slot)\n                    process([l_points[x] for x in _changes])\n                    _changes.clear()\n                    continue\n                else:\n                    point['id'] = ev_value\n            else:\n                continue\n            _changes.add(_slot)\n        if _changes:\n            process([l_points[x] for x in _changes])\n            _changes.clear()\n", "label": 1}
{"function": "\n\ndef parse(self, path):\n    try:\n        if (path.startswith('http://') or path.startswith('https://')):\n            if ('requests' not in IMPORTS):\n                e = 'HTTP library not found: requests'\n                raise ImportError(e)\n            headers = {\n                'User-Agent': 'Mozilla/5.0 Gecko Firefox',\n            }\n            r = requests.get(path, headers=headers)\n            r.raise_for_status()\n            f = StringIO(r.content)\n            self.parser_func(f, path)\n            return\n        elif os.path.isfile(path):\n            with open(path, 'rb') as f:\n                self.parser_func(f, path)\n            return\n        elif os.path.isdir(path):\n            for (walk_root, walk_dirs, walk_files) in os.walk(path):\n                for walk_file in fnmatch.filter(walk_files, self.ext_filter):\n                    fpath = os.path.join(walk_root, walk_file)\n                    with open(fpath, 'rb') as f:\n                        self.parser_func(f, fpath)\n            return\n        e = ('File path is not a file, directory or URL: %s' % path)\n        raise IOError(e)\n    except (KeyboardInterrupt, SystemExit):\n        raise\n    except Exception as e:\n        self.handler.print_error(path, e)\n", "label": 0}
{"function": "\n\n@messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)\n@wrap_exception()\n@reverts_task_state\n@wrap_instance_event\n@wrap_instance_fault\ndef rebuild_instance(self, context, instance, orig_image_ref, image_ref, injected_files, new_pass, orig_sys_metadata, bdms, recreate, on_shared_storage=None, preserve_ephemeral=False, migration=None, scheduled_node=None, limits=None):\n    \"Destroy and re-make this instance.\\n\\n        A 'rebuild' effectively purges all existing data from the system and\\n        remakes the VM with given 'metadata' and 'personalities'.\\n\\n        :param context: `nova.RequestContext` object\\n        :param instance: Instance object\\n        :param orig_image_ref: Original image_ref before rebuild\\n        :param image_ref: New image_ref for rebuild\\n        :param injected_files: Files to inject\\n        :param new_pass: password to set on rebuilt instance\\n        :param orig_sys_metadata: instance system metadata from pre-rebuild\\n        :param bdms: block-device-mappings to use for rebuild\\n        :param recreate: True if the instance is being recreated (e.g. the\\n            hypervisor it was on failed) - cleanup of old state will be\\n            skipped.\\n        :param on_shared_storage: True if instance files on shared storage.\\n                                  If not provided then information from the\\n                                  driver will be used to decide if the instance\\n                                  files are available or not on the target host\\n        :param preserve_ephemeral: True if the default ephemeral storage\\n                                   partition must be preserved on rebuild\\n        :param migration: a Migration object if one was created for this\\n                          rebuild operation (if it's a part of evacaute)\\n        :param scheduled_node: A node of the host chosen by the scheduler. If a\\n                               host was specified by the user, this will be\\n                               None\\n        :param limits: Overcommit limits set by the scheduler. If a host was\\n                       specified by the user, this will be None\\n        \"\n    context = context.elevated()\n    LOG.info(_LI('Rebuilding instance'), context=context, instance=instance)\n    if (scheduled_node is not None):\n        rt = self._get_resource_tracker(scheduled_node)\n        rebuild_claim = rt.rebuild_claim\n    else:\n        rebuild_claim = claims.NopClaim\n    image_meta = {\n        \n    }\n    if image_ref:\n        image_meta = self.image_api.get(context, image_ref)\n    if (not scheduled_node):\n        try:\n            compute_node = self._get_compute_info(context, self.host)\n            scheduled_node = compute_node.hypervisor_hostname\n        except exception.ComputeHostNotFound:\n            LOG.exception(_LE('Failed to get compute_info for %s'), self.host)\n    with self._error_out_instance_on_exception(context, instance):\n        try:\n            claim_ctxt = rebuild_claim(context, instance, limits=limits, image_meta=image_meta, migration=migration)\n            self._do_rebuild_instance_with_claim(claim_ctxt, context, instance, orig_image_ref, image_ref, injected_files, new_pass, orig_sys_metadata, bdms, recreate, on_shared_storage, preserve_ephemeral)\n        except exception.ComputeResourcesUnavailable as e:\n            LOG.debug('Could not rebuild instance on this host, not enough resources available.', instance=instance)\n            self._set_migration_status(migration, 'failed')\n            self._notify_about_instance_usage(context, instance, 'rebuild.error', fault=e)\n            raise exception.BuildAbortException(instance_uuid=instance.uuid, reason=e.format_message())\n        except (exception.InstanceNotFound, exception.UnexpectedDeletingTaskStateError) as e:\n            LOG.debug('Instance was deleted while rebuilding', instance=instance)\n            self._set_migration_status(migration, 'failed')\n            self._notify_about_instance_usage(context, instance, 'rebuild.error', fault=e)\n        except Exception as e:\n            self._set_migration_status(migration, 'failed')\n            self._notify_about_instance_usage(context, instance, 'rebuild.error', fault=e)\n            raise\n        else:\n            instance.apply_migration_context()\n            instance.host = self.host\n            instance.node = scheduled_node\n            instance.save()\n            instance.drop_migration_context()\n            self._set_migration_status(migration, 'done')\n", "label": 0}
{"function": "\n\ndef setecho(self, state):\n    \"This sets the terminal echo mode on or off. Note that anything the\\n        child sent before the echo will be lost, so you should be sure that\\n        your input buffer is empty before you call setecho(). For example, the\\n        following will work as expected::\\n\\n            p = pexpect.spawn('cat') # Echo is on by default.\\n            p.sendline('1234') # We expect see this twice from the child...\\n            p.expect(['1234']) # ... once from the tty echo...\\n            p.expect(['1234']) # ... and again from cat itself.\\n            p.setecho(False) # Turn off tty echo\\n            p.sendline('abcd') # We will set this only once (echoed by cat).\\n            p.sendline('wxyz') # We will set this only once (echoed by cat)\\n            p.expect(['abcd'])\\n            p.expect(['wxyz'])\\n\\n        The following WILL NOT WORK because the lines sent before the setecho\\n        will be lost::\\n\\n            p = pexpect.spawn('cat')\\n            p.sendline('1234')\\n            p.setecho(False) # Turn off tty echo\\n            p.sendline('abcd') # We will set this only once (echoed by cat).\\n            p.sendline('wxyz') # We will set this only once (echoed by cat)\\n            p.expect(['1234'])\\n            p.expect(['1234'])\\n            p.expect(['abcd'])\\n            p.expect(['wxyz'])\\n\\n\\n        Not supported on platforms where ``isatty()`` returns False.\\n        \"\n    if (not self.isatty()):\n        self.echo = state\n        return\n    errmsg = 'setecho() may not be called on this platform'\n    try:\n        attr = termios.tcgetattr(self.child_fd)\n    except termios.error as err:\n        if (err.args[0] == errno.EINVAL):\n            raise IOError(err.args[0], ('%s: %s.' % (err.args[1], errmsg)))\n        raise\n    if state:\n        attr[3] = (attr[3] | termios.ECHO)\n    else:\n        attr[3] = (attr[3] & (~ termios.ECHO))\n    try:\n        termios.tcsetattr(self.child_fd, termios.TCSANOW, attr)\n    except IOError as err:\n        if (err.args[0] == errno.EINVAL):\n            raise IOError(err.args[0], ('%s: %s.' % (err.args[1], errmsg)))\n        raise\n    self.echo = state\n", "label": 0}
{"function": "\n\ndef get_field_types(field_list, iterations=3):\n    assert (iterations > 0), 'iterations should be a positive integer (not a negative integer or 0)'\n\n    def test_boolean(value):\n        if (value.lower() not in ('false', 'true')):\n            raise ValueError((_('%s is not a boolean value') % value))\n\n    def test_timestamp(value):\n        if (not value):\n            raise ValueError()\n        if ((len(value) > 50) or (len(value) < 3)):\n            raise ValueError()\n        if (value.startswith('[') and value.endswith(']')):\n            value = value[1:(- 1)]\n        try:\n            parse(value)\n        except OverflowError:\n            raise ValueError()\n\n    def test_int(value):\n        if ((len(bin(int(value))) - 2) > 32):\n            raise ValueError()\n\n    def test_string(value):\n        if (len(smart_str(value).split(' ')) > 4):\n            raise ValueError()\n    test_fns = [('boolean', test_boolean), ('tint', test_int), ('tlong', int), ('tdouble', float), ('tdate', test_timestamp), ('string', test_string), ('text_general', any)]\n    all_field_types = []\n    for row in field_list:\n        if (iterations == 0):\n            break\n        iterations -= 1\n        row_field_types = []\n        for field in row:\n            field_type_index = None\n            for index in range(0, len(test_fns)):\n                try:\n                    test_fns[index][1](field)\n                    field_type_index = index\n                    break\n                except ValueError:\n                    pass\n            row_field_types.append(field_type_index)\n        all_field_types.append(row_field_types)\n    if (not all_field_types):\n        return []\n    final_field_types = all_field_types[0]\n    for row_field_types in all_field_types:\n        for index in range(0, len(row_field_types)):\n            if (row_field_types[index] > final_field_types[index]):\n                final_field_types[index] = row_field_types[index]\n    return [test_fns[index][0] for index in final_field_types]\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef get_script(report_name):\n    report = get_report_doc(report_name)\n    module = (report.module or frappe.db.get_value('DocType', report.ref_doctype, 'module'))\n    module_path = get_module_path(module)\n    report_folder = os.path.join(module_path, 'report', scrub(report.name))\n    script_path = os.path.join(report_folder, (scrub(report.name) + '.js'))\n    print_path = os.path.join(report_folder, (scrub(report.name) + '.html'))\n    script = None\n    if os.path.exists(script_path):\n        with open(script_path, 'r') as f:\n            script = f.read()\n    html_format = get_html_format(print_path)\n    if ((not script) and report.javascript):\n        script = report.javascript\n    if (not script):\n        script = (\"frappe.query_reports['%s']={}\" % report_name)\n    if (frappe.lang != 'en'):\n        send_translations(frappe.get_lang_dict('report', report_name))\n    return {\n        'script': script,\n        'html_format': html_format,\n    }\n", "label": 0}
{"function": "\n\n@app.route('/v1/credentials/<id>', methods=['PUT'])\n@authnz.require_auth\n@authnz.require_csrf_token\ndef update_credential(id):\n    try:\n        _cred = Credential.get(id)\n    except Credential.DoesNotExist:\n        return (jsonify({\n            'error': 'Credential not found.',\n        }), 404)\n    if (_cred.data_type != 'credential'):\n        msg = 'id provided is not a credential.'\n        return (jsonify({\n            'error': msg,\n        }), 400)\n    data = request.get_json()\n    update = {\n        \n    }\n    revision = (_cred.revision + 1)\n    update['name'] = data.get('name', _cred.name)\n    if ('enabled' in data):\n        if (not isinstance(data['enabled'], bool)):\n            return (jsonify({\n                'error': 'Enabled must be a boolean.',\n            }), 400)\n        update['enabled'] = data['enabled']\n    else:\n        update['enabled'] = _cred.enabled\n    services = _get_services_for_credential(id)\n    if ('credential_pairs' in data):\n        credential_pairs = _lowercase_credential_pairs(data['credential_pairs'])\n        if (not _check_credential_pair_uniqueness(credential_pairs)):\n            ret = {\n                'error': 'credential pairs must be key: value',\n            }\n            return (jsonify(ret), 400)\n        conflicts = _pair_key_conflicts_for_services(id, credential_pairs, services)\n        if conflicts:\n            ret = {\n                'error': 'Conflicting key pairs in mapped service.',\n                'conflicts': conflicts,\n            }\n            return (jsonify(ret), 400)\n        update['credential_pairs'] = json.dumps(credential_pairs)\n    else:\n        data_key = keymanager.decrypt_key(_cred.data_key, encryption_context={\n            'id': id,\n        })\n        cipher_version = _cred.cipher_version\n        cipher = CipherManager(data_key, cipher_version)\n        update['credential_pairs'] = cipher.decrypt(_cred.credential_pairs)\n    data_key = keymanager.create_datakey(encryption_context={\n        'id': id,\n    })\n    cipher = CipherManager(data_key['plaintext'], version=2)\n    credential_pairs = cipher.encrypt(update['credential_pairs'])\n    try:\n        Credential(id='{0}-{1}'.format(id, revision), name=update['name'], data_type='archive-credential', credential_pairs=credential_pairs, enabled=update['enabled'], revision=revision, data_key=data_key['ciphertext'], cipher_version=2, modified_by=authnz.get_logged_in_user_email()).save(id__null=True)\n    except PutError as e:\n        logging.error(e)\n        return (jsonify({\n            'error': 'Failed to add credential to archive.',\n        }), 500)\n    try:\n        cred = Credential(id=id, name=update['name'], data_type='credential', credential_pairs=credential_pairs, enabled=update['enabled'], revision=revision, data_key=data_key['ciphertext'], cipher_version=2, modified_by=authnz.get_logged_in_user_email())\n        cred.save()\n    except PutError as e:\n        logging.error(e)\n        return (jsonify({\n            'error': 'Failed to update active credential.',\n        }), 500)\n    if services:\n        service_names = [x.id for x in services]\n        msg = 'Updated credential \"{0}\" ({1}); Revision {2}'\n        msg = msg.format(cred.name, cred.id, cred.revision)\n        graphite.send_event(service_names, msg)\n    return jsonify({\n        'id': cred.id,\n        'name': cred.name,\n        'credential_pairs': json.loads(cipher.decrypt(cred.credential_pairs)),\n        'revision': cred.revision,\n        'enabled': cred.enabled,\n        'modified_date': cred.modified_date,\n        'modified_by': cred.modified_by,\n    })\n", "label": 1}
{"function": "\n\ndef email(request, form_class=AddEmailForm, template_name='account/email.html'):\n    if ((request.method == 'POST') and request.user.is_authenticated()):\n        if (request.POST['action'] == 'add'):\n            add_email_form = form_class(request.user, request.POST)\n            if add_email_form.is_valid():\n                add_email_form.save()\n                add_email_form = form_class()\n        else:\n            add_email_form = form_class()\n            if (request.POST['action'] == 'send'):\n                email = request.POST['email']\n                try:\n                    email_address = EmailAddress.objects.get(user=request.user, email=email)\n                    request.user.message_set.create(message=('Confirmation email sent to %s' % email))\n                    EmailConfirmation.objects.send_confirmation(email_address)\n                except EmailAddress.DoesNotExist:\n                    pass\n            elif (request.POST['action'] == 'remove'):\n                email = request.POST['email']\n                try:\n                    email_address = EmailAddress.objects.get(user=request.user, email=email)\n                    email_address.delete()\n                    request.user.message_set.create(message=('Removed email address %s' % email))\n                except EmailAddress.DoesNotExist:\n                    pass\n            elif (request.POST['action'] == 'primary'):\n                email = request.POST['email']\n                email_address = EmailAddress.objects.get(user=request.user, email=email)\n                email_address.set_as_primary()\n    else:\n        add_email_form = form_class()\n    return render_to_response(template_name, {\n        'add_email_form': add_email_form,\n    }, context_instance=RequestContext(request))\n", "label": 0}
{"function": "\n\ndef disable(**kwargs):\n    \"\\n    Disable all beaconsd jobs on the minion\\n\\n    :return:                Boolean and status message on success or failure of disable.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' beacons.disable\\n    \"\n    ret = {\n        'comment': [],\n        'result': True,\n    }\n    if (('test' in kwargs) and kwargs['test']):\n        ret['comment'] = 'Beacons would be disabled.'\n    else:\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({\n                'func': 'disable',\n            }, 'manage_beacons')\n            if res:\n                event_ret = eventer.get_event(tag='/salt/minion/minion_beacons_disabled_complete', wait=30)\n                log.debug('event_ret {0}'.format(event_ret))\n                if (event_ret and event_ret['complete']):\n                    beacons = event_ret['beacons']\n                    if (('enabled' in beacons) and (not beacons['enabled'])):\n                        ret['result'] = True\n                        ret['comment'] = 'Disabled beacons on minion.'\n                    else:\n                        ret['result'] = False\n                        ret['comment'] = 'Failed to disable beacons on minion.'\n                    return ret\n        except KeyError:\n            ret['comment'] = 'Event module not available. Beacons enable job failed.'\n    return ret\n", "label": 0}
{"function": "\n\ndef get_versions(default=DEFAULT, verbose=False):\n    assert (versionfile_source is not None), 'please set versioneer.versionfile_source'\n    assert (tag_prefix is not None), 'please set versioneer.tag_prefix'\n    assert (parentdir_prefix is not None), 'please set versioneer.parentdir_prefix'\n    root = get_root()\n    versionfile_abs = os.path.join(root, versionfile_source)\n    variables = get_expanded_variables(versionfile_abs)\n    if variables:\n        ver = versions_from_expanded_variables(variables, tag_prefix)\n        if ver:\n            if verbose:\n                print(('got version from expanded variable %s' % ver))\n            return ver\n    ver = versions_from_file(versionfile_abs)\n    if ver:\n        if verbose:\n            print(('got version from file %s %s' % (versionfile_abs, ver)))\n        return ver\n    ver = versions_from_vcs(tag_prefix, root, verbose)\n    if ver:\n        if verbose:\n            print(('got version from git %s' % ver))\n        return ver\n    ver = versions_from_parentdir(parentdir_prefix, root, verbose)\n    if ver:\n        if verbose:\n            print(('got version from parentdir %s' % ver))\n        return ver\n    if verbose:\n        print(('got version from default %s' % ver))\n    return default\n", "label": 1}
{"function": "\n\ndef test_assertRaises(self):\n\n    def _raise(e):\n        raise e\n    self.assertRaises(KeyError, _raise, KeyError)\n    self.assertRaises(KeyError, _raise, KeyError('key'))\n    try:\n        self.assertRaises(KeyError, (lambda : None))\n    except self.failureException as e:\n        self.assertIn('KeyError not raised', str(e))\n    else:\n        self.fail(\"assertRaises() didn't fail\")\n    try:\n        self.assertRaises(KeyError, _raise, ValueError)\n    except ValueError:\n        pass\n    else:\n        self.fail(\"assertRaises() didn't let exception pass through\")\n    with self.assertRaises(KeyError) as cm:\n        try:\n            raise KeyError\n        except Exception as e:\n            exc = e\n            raise\n    self.assertIs(cm.exception, exc)\n    with self.assertRaises(KeyError):\n        raise KeyError('key')\n    try:\n        with self.assertRaises(KeyError):\n            pass\n    except self.failureException as e:\n        self.assertIn('KeyError not raised', str(e))\n    else:\n        self.fail(\"assertRaises() didn't fail\")\n    try:\n        with self.assertRaises(KeyError):\n            raise ValueError\n    except ValueError:\n        pass\n    else:\n        self.fail(\"assertRaises() didn't let exception pass through\")\n", "label": 0}
{"function": "\n\ndef import_string(import_name, silent=False):\n    'Imports an object based on a string.  This is useful if you want to\\n    use import paths as endpoints or something similar.  An import path can\\n    be specified either in dotted notation (``xml.sax.saxutils.escape``)\\n    or with a colon as object delimiter (``xml.sax.saxutils:escape``).\\n\\n    If `silent` is True the return value will be `None` if the import fails.\\n\\n    :param import_name: the dotted name for the object to import.\\n    :param silent: if set to `True` import errors are ignored and\\n                   `None` is returned instead.\\n    :return: imported object\\n    '\n    assert isinstance(import_name, string_types)\n    import_name = str(import_name)\n    try:\n        if (':' in import_name):\n            (module, obj) = import_name.split(':', 1)\n        elif ('.' in import_name):\n            (module, obj) = import_name.rsplit('.', 1)\n        else:\n            return __import__(import_name)\n        if (PY2 and isinstance(obj, unicode)):\n            obj = obj.encode('utf-8')\n        try:\n            return getattr(__import__(module, None, None, [obj]), obj)\n        except (ImportError, AttributeError):\n            modname = ((module + '.') + obj)\n            __import__(modname)\n            return sys.modules[modname]\n    except ImportError as e:\n        if (not silent):\n            raise\n", "label": 0}
{"function": "\n\ndef indication(self, pdu):\n    if _debug:\n        MiddleMan._debug('indication %r', pdu)\n    if (not pdu.pduData):\n        stop()\n        return\n    line = pdu.pduData.decode('utf_8')[:(- 1)]\n    if _debug:\n        MiddleMan._debug('    - line: %r', line)\n    line_parts = line.split(' ', 1)\n    if _debug:\n        MiddleMan._debug('    - line_parts: %r', line_parts)\n    if (len(line_parts) != 2):\n        sys.stderr.write(('err: invalid line: %r\\n' % (line,)))\n        return\n    (addr, msg) = line_parts\n    if (addr == '*'):\n        dest = local_broadcast_tuple\n    elif (':' in addr):\n        (addr, port) = addr.split(':')\n        if (addr == '*'):\n            dest = (local_broadcast_tuple[0], int(port))\n        else:\n            dest = (addr, int(port))\n    else:\n        dest = (addr, local_unicast_tuple[1])\n    if _debug:\n        MiddleMan._debug('    - dest: %r', dest)\n    try:\n        self.request(PDU(msg.encode('utf_8'), destination=dest))\n    except Exception as err:\n        sys.stderr.write(('err: %r\\n' % (err,)))\n        return\n", "label": 1}
{"function": "\n\ndef ensure_new_type(obj):\n    from future.types.newbytes import newbytes\n    from future.types.newstr import newstr\n    from future.types.newint import newint\n    from future.types.newdict import newdict\n    native_type = type(native(obj))\n    if issubclass(native_type, type(obj)):\n        if (native_type == str):\n            return newbytes(obj)\n        elif (native_type == unicode):\n            return newstr(obj)\n        elif (native_type == int):\n            return newint(obj)\n        elif (native_type == long):\n            return newint(obj)\n        elif (native_type == dict):\n            return newdict(obj)\n        else:\n            return NotImplementedError(('type %s not supported' % type(obj)))\n    else:\n        assert (type(obj) in [newbytes, newstr])\n        return obj\n", "label": 0}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(UpdatePolicyRuleForm, self).__init__(request, *args, **kwargs)\n    try:\n        policyrule_id = self.initial['policyrule_id']\n        rule = client.policyrule_get(request, policyrule_id)\n        for item in ['name', 'description', 'policy_classifier_id', 'policy_actions', 'shared']:\n            self.fields[item].initial = getattr(rule, item)\n        actions = client.policyaction_list(request, tenant_id=request.user.tenant_id)\n        action_list = [a.id for a in actions]\n        for action in actions:\n            action.set_id_as_name_if_empty()\n        actions = sorted(actions, key=(lambda action: action.name))\n        action_list = [(a.id, a.name) for a in actions]\n        self.fields['policy_actions'].choices = action_list\n        classifiers = client.policyclassifier_list(request, tenant_id=request.user.tenant_id)\n        classifier_list = [(c.id, c.name) for c in classifiers]\n        self.fields['policy_classifier_id'].choices = classifier_list\n    except Exception:\n        exceptions.handle(request, _('Unable to retrive policy rule details.'))\n", "label": 0}
{"function": "\n\ndef test_deprecated_worker(self):\n    account_sid = 'AC123'\n    auth_token = 'foobar'\n    workspace_sid = 'WS456'\n    worker_sid = 'WK789'\n    capability = TaskRouterCapability(account_sid, auth_token, workspace_sid, worker_sid)\n    capability.generate_token()\n    token = capability.generate_token()\n    self.assertNotEqual(None, token)\n    decoded = jwt.decode(token, auth_token)\n    self.assertNotEqual(None, decoded)\n    self.check_decoded(decoded, account_sid, workspace_sid, worker_sid, worker_sid)\n    policies = decoded['policies']\n    self.assertEqual(len(policies), 6)\n    for (method, url, policy) in [('GET', 'https://event-bridge.twilio.com/v1/wschannels/AC123/WK789', policies[0]), ('POST', 'https://event-bridge.twilio.com/v1/wschannels/AC123/WK789', policies[1]), ('GET', 'https://taskrouter.twilio.com/v1/Workspaces/WS456/Activities', policies[2]), ('GET', 'https://taskrouter.twilio.com/v1/Workspaces/WS456/Tasks/**', policies[3]), ('GET', 'https://taskrouter.twilio.com/v1/Workspaces/WS456/Workers/WK789/Reservations/**', policies[4]), ('GET', 'https://taskrouter.twilio.com/v1/Workspaces/WS456/Workers/WK789', policies[5])]:\n        (yield (self.check_policy, method, url, policy))\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        capability.allow_worker_fetch_attributes()\n        assert (len(w) == 1)\n        assert issubclass(w[(- 1)].category, DeprecationWarning)\n        assert ('deprecated' in str(w[(- 1)].message))\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        capability.allow_worker_activity_updates()\n        assert (len(w) == 1)\n        assert issubclass(w[(- 1)].category, DeprecationWarning)\n        assert ('deprecated' in str(w[(- 1)].message))\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        capability.allow_task_reservation_updates()\n        assert (len(w) == 1)\n        assert issubclass(w[(- 1)].category, DeprecationWarning)\n        assert ('deprecated' in str(w[(- 1)].message))\n", "label": 1}
{"function": "\n\ndef process_group(hg, crawl_id, requesters, processed_groups, dbpool):\n    'Gevent worker that should process single hitgroup.\\n\\n    This should write some data into database and do not return any important\\n    data.\\n    '\n    hg['keywords'] = ', '.join(hg['keywords'])\n    hg['qualifications'] = ', '.join(hg['qualifications'])\n    conn = dbpool.getconn(thread.get_ident())\n    db = DB(conn)\n    try:\n        hit_group_content_id = db.hit_group_content_id(hg['group_id'])\n        if (hit_group_content_id is None):\n            profile = requesters.get(hg['requester_id'], None)\n            if (profile and (profile.is_public is False)):\n                hg['is_public'] = False\n            else:\n                hg['is_public'] = True\n            hg['occurrence_date'] = datetime.datetime.now()\n            hg['first_crawl_id'] = crawl_id\n            if (not hg['group_id_hashed']):\n                hg.update(hits_group_info(hg['group_id']))\n            else:\n                hg['html'] = ''\n            hit_group_content_id = db.insert_hit_group_content(hg)\n            log.debug('new hit group content: %s;;%s', hit_group_content_id, hg['group_id'])\n        hg['hit_group_content_id'] = hit_group_content_id\n        hg['crawl_id'] = crawl_id\n        hg['now'] = datetime.datetime.now()\n        db.insert_hit_group_status(hg)\n        conn.commit()\n    except Exception:\n        processed_groups.remove(hg['group_id'])\n        log.exception('process_group fail - rollback')\n        conn.rollback()\n    finally:\n        db.curr.close()\n        dbpool.putconn(conn, thread.get_ident())\n        msg = 'This really should not happen, Hitgroupstatus was processed but is not on the list, race condition?'\n        assert (hg['group_id'] in processed_groups), msg\n    return True\n", "label": 0}
{"function": "\n\ndef get_verb_aliases(path=catkin_config_path):\n    if (not os.path.isdir(path)):\n        raise RuntimeError(\"Cannot get verb aliases because the catkin config path ('{0}') does not exist or is a file.\".format(path))\n    verb_aliases_path = os.path.join(path, 'verb_aliases')\n    if (not os.path.isdir(verb_aliases_path)):\n        raise RuntimeError(\"Cannot get verb aliases because the verb aliases config path ('{0}') does not exist or is a file.\".format(verb_aliases_path))\n    verb_aliases = {\n        \n    }\n    for file_name in sorted(os.listdir(verb_aliases_path)):\n        if file_name.endswith('.yaml'):\n            full_path = os.path.join(verb_aliases_path, file_name)\n            with open(full_path, 'r') as f:\n                yaml_dict = yaml.load(f)\n            if (yaml_dict is None):\n                continue\n            if (not isinstance(yaml_dict, dict)):\n                raise RuntimeError(\"Invalid alias file ('{0}'), expected a dict but got a {1}\".format(full_path, type(yaml_dict)))\n            for (key, value) in yaml_dict.items():\n                if (not isinstance(key, string_type)):\n                    raise RuntimeError(\"Invalid alias in file ('{0}'), expected a string but got '{1}' of type {2}\".format(full_path, key, type(key)))\n                if ((not isinstance(value, string_type)) and (not isinstance(value, type(None)))):\n                    raise RuntimeError(\"Invalid alias expansion in file ('{0}'), expected a string but got '{1}' of type {2}\".format(full_path, value, type(value)))\n            verb_aliases.update(yaml_dict)\n    for (alias, value) in dict(verb_aliases).items():\n        if (not value):\n            del verb_aliases[alias]\n    return verb_aliases\n", "label": 1}
{"function": "\n\ndef _runner(init, shape, target_mean=None, target_std=None, target_max=None, target_min=None):\n    variable = init(shape)\n    output = K.get_value(variable)\n    lim = 0.01\n    if (target_std is not None):\n        assert (abs((output.std() - target_std)) < lim)\n    if (target_mean is not None):\n        assert (abs((output.mean() - target_mean)) < lim)\n    if (target_max is not None):\n        assert (abs((output.max() - target_max)) < lim)\n    if (target_min is not None):\n        assert (abs((output.min() - target_min)) < lim)\n", "label": 0}
{"function": "\n\ndef blame(self, extensions=None, ignore_dir=None, committer=True, by='repository'):\n    '\\n        Returns the blame from the current HEAD of the repositories as a DataFrame.  The DataFrame is grouped by committer\\n        name, so it will be the sum of all contributions to all repositories by each committer. As with the commit history\\n        method, extensions and ignore_dirs parameters can be passed to exclude certain directories, or focus on certain\\n        file extensions. The DataFrame will have the columns:\\n\\n         * committer\\n         * loc\\n\\n        :param extensions: (optional, default=None) a list of file extensions to return commits for\\n        :param ignore_dir: (optional, default=None) a list of directory names to ignore\\n        :param committer: (optional, default=True) true if committer should be reported, false if author\\n        :param by: (optional, default=repository) whether to group by repository or by file\\n        :return: DataFrame\\n        '\n    df = None\n    for repo in self.repos:\n        try:\n            if (df is None):\n                df = repo.blame(extensions=extensions, ignore_dir=ignore_dir, committer=committer, by=by)\n            else:\n                df = df.append(repo.blame(extensions=extensions, ignore_dir=ignore_dir, committer=committer, by=by))\n        except GitCommandError as err:\n            print(('Warning! Repo: %s couldnt be blamed' % (repo,)))\n            pass\n    df = df.reset_index(level=1)\n    df = df.reset_index(level=1)\n    if committer:\n        if (by == 'repository'):\n            df = df.groupby('committer').agg({\n                'loc': np.sum,\n            })\n        elif (by == 'file'):\n            df = df.groupby(['committer', 'file']).agg({\n                'loc': np.sum,\n            })\n    elif (by == 'repository'):\n        df = df.groupby('author').agg({\n            'loc': np.sum,\n        })\n    elif (by == 'file'):\n        df = df.groupby(['author', 'file']).agg({\n            'loc': np.sum,\n        })\n    df = df.sort_values(by=['loc'], ascending=False)\n    return df\n", "label": 0}
{"function": "\n\ndef drag_release_frame(self, frame, pos):\n    ' Handle the dock frame being released by the user.\\n\\n        This method is called by the framework at the appropriate times\\n        and should not be called directly by user code. It will redock\\n        a floating dock item if it is released over a dock guide.\\n\\n        Parameters\\n        ----------\\n        frame : QDockFrame\\n            The dock frame being dragged by the user.\\n\\n        pos : QPoint\\n            The global coordinates of the mouse position.\\n\\n        '\n    overlay = self._overlay\n    overlay.hide()\n    guide = overlay.guide_at(pos)\n    if (guide == QGuideRose.Guide.NoGuide):\n        return\n    if self._proximity_handler.hasLinkedFrames(frame):\n        return\n    builder = LayoutBuilder(self)\n    target = self._dock_target(frame, pos)\n    if isinstance(target, QDockArea):\n        if (target.maximizedWidget() is not None):\n            return\n        with builder.drop_frame(frame):\n            local = target.mapFromGlobal(pos)\n            widget = layout_hit_test(target, local)\n            plug_frame(target, widget, frame, guide)\n    elif isinstance(target, QDockContainer):\n        with builder.dock_context(target):\n            with builder.drop_frame(frame):\n                area = target.parentDockArea()\n                if (area is not None):\n                    plug_frame(area, target, frame, guide)\n", "label": 0}
{"function": "\n\ndef get_ssh(self, host):\n    '\\n        Ssh connection. The actual connection to the host is established\\n        only after the first call of this function.\\n        '\n    with self._lock:\n        if (not (self._ssh_cache and self._ssh_cache._transport and self._ssh_cache._transport.is_active())):\n            progress_bar = self._create_connect_progress_bar(host)\n            with progress_bar:\n                h = host\n                self._ssh_cache = paramiko.SSHClient()\n                if (not h.reject_unknown_hosts):\n                    self._ssh_cache.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n                try:\n                    kw = {\n                        \n                    }\n                    if h.config_filename:\n                        try:\n                            config_file = file(os.path.expanduser(h.config_filename))\n                        except IOError:\n                            pass\n                        else:\n                            ssh_config = paramiko.config.SSHConfig()\n                            ssh_config.parse(config_file)\n                            host_config = ssh_config.lookup(h.address)\n                            config_map = {\n                                'identityfile': 'key_filename',\n                                'user': 'username',\n                                'port': 'port',\n                                'connecttimeout': 'timeout',\n                            }\n                            for (ck, pk) in config_map.items():\n                                if (ck in host_config):\n                                    kw[pk] = host_config[ck]\n                    if h.port:\n                        kw['port'] = h.port\n                    if h.username:\n                        kw['username'] = h.username\n                    if h.timeout:\n                        kw['timeout'] = h.timeout\n                    if h.rsa_key:\n                        rsa_key_file_obj = StringIO.StringIO(h.rsa_key)\n                        kw['pkey'] = paramiko.RSAKey.from_private_key(rsa_key_file_obj, h.rsa_key_password)\n                    elif h.key_filename:\n                        kw['key_filename'] = h.key_filename\n                    elif h.password:\n                        kw['password'] = h.password\n                    from .paramiko_connect_patch import connect as connect_patch\n                    kw['progress_bar_callback'] = progress_bar.set_progress\n                    connect_patch(self._ssh_cache, h.address, **kw)\n                except (paramiko.SSHException, Exception) as e:\n                    self._ssh_cache = None\n                    raise ConnectionFailedException(('Could not connect to host %s:%s (%s) username=%s\\n%s' % (h.address, h.port, h.slug, h.username, unicode(e))))\n        return self._ssh_cache\n", "label": 1}
{"function": "\n\ndef __init__(self, domain, sensor, backscatter_model=None):\n    self.domain = domain\n    self.sensor = sensor\n    self.backscatter_model = []\n    for i in range(len(sensor.band_names)):\n        try:\n            if (backscatter_model != None):\n                self.backscatter_model.append(backscatter_model)\n            elif (sensor.water_distributions[sensor.band_names[i]]['model'] == 'peak'):\n                self.backscatter_model.append(RadarHistogram.BACKSCATTER_MODEL_PEAK)\n            elif (sensor.water_distributions[sensor.band_names[i]]['model'] == 'dip'):\n                self.backscatter_model.append(RadarHistogram.BACKSCATTER_MODEL_DIP)\n            elif (sensor.water_distributions[sensor.band_names[i]]['model'] == 'lambda'):\n                self.backscatter_model.append(RadarHistogram.BACKSCATTER_MODEL_GAMMA)\n            else:\n                self.backscatter_model.append(RadarHistogram.BACKSCATTER_MODEL_GAUSSIAN)\n        except KeyError:\n            pass\n    self.hist_image = self.__preprocess_image(sensor)\n    self.histograms = self.__compute_histogram(self.hist_image)\n    self.__find_thresholds()\n", "label": 0}
{"function": "\n\ndef move_page(self, direction, n_windows):\n    '\\n        Move the page down (positive direction) or up (negative direction).\\n\\n        Paging down:\\n            The post on the bottom of the page becomes the post at the top of\\n            the page and the cursor is moved to the top.\\n        Paging up:\\n            The post at the top of the page becomes the post at the bottom of\\n            the page and the cursor is moved to the bottom.\\n        '\n    assert (direction in ((- 1), 1))\n    assert (n_windows >= 0)\n    if ((self.absolute_index < 0) | (n_windows == 0)):\n        (valid, redraw) = self.move(direction, n_windows)\n    else:\n        if ((self.absolute_index < n_windows) and (direction < 0)):\n            self.page_index = (- 1)\n            self.cursor_index = 0\n            self.inverted = False\n            if (not self._is_valid(self.absolute_index)):\n                self.page_index = 0\n            valid = True\n        else:\n            if (((direction > 0) & (self.inverted is True)) | ((direction < 0) & (self.inverted is False))):\n                self.page_index += (self.step * (n_windows - 1))\n                self.inverted = (not self.inverted)\n                self.cursor_index = ((n_windows - (direction < 0)) - self.cursor_index)\n            valid = False\n            adj = 0\n            while (not valid):\n                n_move = (n_windows - adj)\n                if (n_move == 0):\n                    break\n                self.page_index += (n_move * direction)\n                valid = self._is_valid(self.absolute_index)\n                if (not valid):\n                    self.page_index -= (n_move * direction)\n                    adj += 1\n        redraw = True\n    return (valid, redraw)\n", "label": 1}
{"function": "\n\ndef test_profile(self):\n\n    def g():\n        pass\n\n    def f():\n        g()\n\n    def main():\n        f()\n        f()\n        f()\n    with tracebin.record() as recorder:\n        main()\n    assert (recorder.calls is None)\n    start = time.time()\n    with tracebin.record(profile=True) as recorder:\n        main()\n    assert (len(recorder.calls) == 3)\n    [call1, call2, call3] = recorder.calls\n    assert (call1.func_name == 'high_res_time')\n    assert (len(call1.subcalls) == 1)\n    assert (call2.func_name == 'main')\n    assert ((call2.start_time - start) < 1)\n    assert ((call2.end_time - call2.start_time) < 1)\n    assert (len(call2.subcalls) == 3)\n    assert ({c.func_name for c in call2.subcalls} == {'f'})\n    assert (call2.subcalls[1].subcalls[0].func_name == 'g')\n", "label": 1}
{"function": "\n\ndef _upload_job_files_to_hdfs(self, where, job_dir, job, configs, proxy_configs=None):\n    mains = (job.mains or [])\n    libs = (job.libs or [])\n    builtin_libs = edp.get_builtin_binaries(job, configs)\n    uploaded_paths = []\n    hdfs_user = self.get_hdfs_user()\n    job_dir_suffix = ('lib' if (job.type != edp.JOB_TYPE_SHELL) else '')\n    lib_dir = os.path.join(job_dir, job_dir_suffix)\n    with remote.get_remote(where) as r:\n        for main in mains:\n            raw_data = dispatch.get_raw_binary(main, proxy_configs=proxy_configs, remote=r)\n            if (isinstance(raw_data, dict) and (raw_data['type'] == 'path')):\n                h.copy_from_local(r, raw_data['path'], job_dir, hdfs_user)\n            else:\n                h.put_file_to_hdfs(r, raw_data, main.name, job_dir, hdfs_user)\n            uploaded_paths.append(((job_dir + '/') + main.name))\n        if (len(libs) and job_dir_suffix):\n            self.create_hdfs_dir(r, lib_dir)\n        for lib in libs:\n            raw_data = dispatch.get_raw_binary(lib, proxy_configs=proxy_configs, remote=remote)\n            if (isinstance(raw_data, dict) and (raw_data['type'] == 'path')):\n                h.copy_from_local(r, raw_data['path'], lib_dir, hdfs_user)\n            else:\n                h.put_file_to_hdfs(r, raw_data, lib.name, lib_dir, hdfs_user)\n            uploaded_paths.append(((lib_dir + '/') + lib.name))\n        for lib in builtin_libs:\n            h.put_file_to_hdfs(r, lib['raw'], lib['name'], lib_dir, hdfs_user)\n            uploaded_paths.append(((lib_dir + '/') + lib['name']))\n    return uploaded_paths\n", "label": 1}
{"function": "\n\ndef attach(self, timeout, wait=True):\n    if (self.attachment_status in (apiAttachPendingAuthorization, apiAttachSuccess)):\n        return\n    self.acquire()\n    try:\n        try:\n            self.start()\n        except AssertionError:\n            pass\n        t = threading.Timer(timeout2float(timeout), (lambda : setattr(self, 'wait', False)))\n        try:\n            self.init_observer()\n            self.client_id = (- 1)\n            self.set_attachment_status(apiAttachPendingAuthorization)\n            self.post('SKSkypeAPIAttachRequest')\n            self.wait = True\n            if wait:\n                t.start()\n            while (self.wait and (self.attachment_status == apiAttachPendingAuthorization)):\n                if self.run_main_loop:\n                    time.sleep(1.0)\n                else:\n                    EventLoop.run(1.0)\n        finally:\n            t.cancel()\n        if (not self.wait):\n            self.set_attachment_status(apiAttachUnknown)\n            raise SkypeAPIError('Skype attach timeout')\n    finally:\n        self.release()\n    command = Command(('PROTOCOL %s' % self.protocol), Blocking=True)\n    self.send_command(command)\n    self.protocol = int(command.Reply.rsplit(None, 1)[(- 1)])\n", "label": 0}
{"function": "\n\ndef _get_container_description(self, state, name, network_state=True, ip_partitions=None):\n    state_container = state.containers[name]\n    container_id = state_container['id']\n    try:\n        container = self.docker_client.inspect_container(container_id)\n    except docker.errors.APIError as err:\n        if (err.response.status_code == 404):\n            return Container(name, container_id, ContainerState.MISSING)\n        else:\n            raise\n    state_dict = container.get('State')\n    if (state_dict and state_dict.get('Running')):\n        container_state = ContainerState.UP\n    else:\n        container_state = ContainerState.DOWN\n    extras = {\n        \n    }\n    network = container.get('NetworkSettings')\n    ip = None\n    if network:\n        ip = network.get('IPAddress')\n        if ip:\n            extras['ip_address'] = ip\n    if (network_state and (name in state.containers) and (container_state == ContainerState.UP)):\n        device = state_container['device']\n        extras['device'] = device\n        extras['network_state'] = self.network.network_state(device)\n        if (ip_partitions and ip):\n            extras['partition'] = ip_partitions.get(ip)\n    else:\n        extras['network_state'] = NetworkState.UNKNOWN\n        extras['device'] = None\n    cfg_container = self.config.containers.get(name)\n    extras['neutral'] = (cfg_container.neutral if cfg_container else False)\n    extras['holy'] = (cfg_container.holy if cfg_container else False)\n    return Container(name, container_id, container_state, **extras)\n", "label": 1}
{"function": "\n\ndef query_library(self, query, tie_breaker=no_tiebreak, modifiers=None, auto=False):\n    'Queries the library for songs.\\n        returns a list of matches, or None.\\n        '\n    if (not modifiers):\n        modifiers = []\n    try:\n        if (not auto):\n            return self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, modifiers, auto))\n        else:\n            current_mods = modifiers[:]\n            future_mods = (m for m in self.auto_modifiers if (m not in modifiers))\n            while True:\n                results = self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, current_mods, auto))\n                if (not results):\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        return results\n                elif (len(results) == 1):\n                    return results\n                else:\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        raise self.TieBroken(tie_breaker(query, results))\n                    next_results = self.query_library(query, tie_breaker, current_mods, auto)\n                    if (not next_results):\n                        raise self.TieBroken(tie_breaker(query, results))\n                    else:\n                        return next_results\n    except self.TieBroken as tie:\n        return tie.results\n", "label": 1}
{"function": "\n\ndef walk_branches(self, sort, *branches):\n    \"\\n        Simple iterator which take a sorting strategy and some branch and\\n        iterates through those branches one commit at a time, yielding a list\\n        of commits\\n\\n        :param sort: a sorting option `GIT_SORT_NONE, GIT_SORT_TOPOLOGICAL,\\n        GIT_SORT_TIME, GIT_SORT_REVERSE`. Default is 'GIT_SORT_TOPOLOGICAL'\\n        :param branches: branch to iterate through\\n        :type branches: list\\n        :returns: yields a list of commits corresponding to given branches\\n        :rtype: list\\n\\n        \"\n    iterators = [self._repo.walk(branch.target, sort) for branch in branches]\n    stop_iteration = [False for branch in branches]\n    commits = []\n    for iterator in iterators:\n        try:\n            commit = next(iterator)\n        except StopIteration:\n            commit = None\n        commits.append(commit)\n    (yield (commit for commit in commits))\n    while (not all(stop_iteration)):\n        for (index, iterator) in enumerate(iterators):\n            try:\n                commit = next(iterator)\n                commits[index] = commit\n            except StopIteration:\n                stop_iteration[index] = True\n        if (not all(stop_iteration)):\n            (yield (commit for commit in commits))\n", "label": 1}
{"function": "\n\ndef login(self, server, username, password='', original_prompt='[#$]', login_timeout=10, auto_prompt_reset=True, sync_multiplier=1, port=23):\n    cmd = 'telnet -l {} {} {}'.format(username, server, port)\n    spawn._spawn(self, cmd)\n    try:\n        i = self.expect('(?i)(?:password)', timeout=login_timeout)\n        if (i == 0):\n            self.sendline(password)\n            i = self.expect([original_prompt, 'Login incorrect'], timeout=login_timeout)\n        if i:\n            raise pxssh.ExceptionPxssh('could not log in: password was incorrect')\n    except TIMEOUT:\n        if (not password):\n            pass\n        else:\n            raise pxssh.ExceptionPxssh('could not log in: did not see a password prompt')\n    if (not self.sync_original_prompt(sync_multiplier)):\n        self.close()\n        raise pxssh.ExceptionPxssh('could not synchronize with original prompt')\n    if auto_prompt_reset:\n        if (not self.set_unique_prompt()):\n            self.close()\n            message = 'could not set shell prompt (recieved: {}, expected: {}).'\n            raise pxssh.ExceptionPxssh(message.format(self.before, self.PROMPT))\n    return True\n", "label": 0}
{"function": "\n\ndef _addNote(self, nick, whence, text, at=None, maximum=None):\n    if (at is None):\n        at = time.time()\n    if (maximum is None):\n        maximum = self.registryValue('maximum')\n    try:\n        notes = self._notes[nick]\n        if (maximum and (len(notes) >= maximum)):\n            raise QueueIsFull()\n        else:\n            notes.append((at, whence, text))\n    except KeyError:\n        self._notes[nick] = [(at, whence, text)]\n    if (('?' in nick) or (('*' in nick) and (nick not in self.wildcards))):\n        self.wildcards.append(nick)\n    self._flushNotes()\n", "label": 0}
{"function": "\n\n@login_required\n@permission_required('workshops.add_person', raise_exception=True)\ndef person_bulk_add(request):\n    if (request.method == 'POST'):\n        form = PersonBulkAddForm(request.POST, request.FILES)\n        if form.is_valid():\n            charset = (request.FILES['file'].charset or settings.DEFAULT_CHARSET)\n            stream = io.TextIOWrapper(request.FILES['file'].file, charset)\n            try:\n                (persons_tasks, empty_fields) = upload_person_task_csv(stream)\n            except csv.Error as e:\n                messages.add_message(request, messages.ERROR, 'Error processing uploaded .CSV file: {}'.format(e))\n            except UnicodeDecodeError as e:\n                messages.add_message(request, messages.ERROR, 'Please provide a file in {} encoding.'.format(charset))\n            else:\n                if empty_fields:\n                    msg_template = 'The following required fields were not found in the uploaded file: {}'\n                    msg = msg_template.format(', '.join(empty_fields))\n                    messages.add_message(request, messages.ERROR, msg)\n                else:\n                    request.session['bulk-add-people'] = persons_tasks\n                    return redirect('person_bulk_add_confirmation')\n    else:\n        form = PersonBulkAddForm()\n    context = {\n        'title': 'Bulk Add People',\n        'form': form,\n        'charset': settings.DEFAULT_CHARSET,\n    }\n    return render(request, 'workshops/person_bulk_add_form.html', context)\n", "label": 0}
{"function": "\n\ndef _detect_loop(self, sim_run, new_tpl, simrun_key, new_call_stack_suffix, new_addr, new_jumpkind, current_exit_wrapper, current_function_addr):\n    '\\n        Loop detection.\\n\\n        :param sim_run:\\n        :param new_tpl:\\n        :param simrun_key:\\n        :param new_call_stack_suffix:\\n        :param new_addr:\\n        :param new_jumpkind:\\n        :param current_exit_wrapper:\\n        :param current_function_addr:\\n        :return:\\n        '\n    assert isinstance(sim_run, simuvex.SimIRSB)\n    if new_jumpkind.startswith('Ijk_Sys'):\n        return\n    if (new_tpl == simrun_key):\n        l.debug(\"%s is branching to itself. That's a loop.\", sim_run)\n        if ((sim_run.addr, sim_run.addr) not in self._loop_back_edges_set):\n            self._loop_back_edges_set.add((sim_run.addr, sim_run.addr))\n            self._loop_back_edges.append((simrun_key, new_tpl))\n    elif ((new_jumpkind != 'Ijk_Call') and (new_jumpkind != 'Ijk_Ret') and current_exit_wrapper.bbl_in_stack(new_call_stack_suffix, current_function_addr, new_addr)):\n        \"\\n            There are two cases:\\n            # The loop header we found is a single IRSB that doesn't overlap with\\n            other IRSBs\\n            or\\n            # The loop header we found is a subset of the original loop header IRSB,\\n            as IRSBa could be inside IRSBb if they don't start at the same address but\\n            end at the same address\\n            We should take good care of these two cases.\\n            \"\n        next_irsb = self._nodes[new_tpl]\n        other_preds = set()\n        for (node_key, node) in self._nodes_by_addr[next_irsb.addr]:\n            predecessors = self.graph.predecessors(node)\n            for pred in predecessors:\n                if (pred.addr != sim_run.addr):\n                    other_preds.add(pred)\n        if (len(other_preds) > 0):\n            is_overlapping = False\n            for p in other_preds:\n                if isinstance(p, simuvex.SimIRSB):\n                    if ((p.addr + p.irsb.size()) == (sim_run.addr + sim_run.irsb.size())):\n                        is_overlapping = True\n                        break\n            if is_overlapping:\n                self._overlapped_loop_headers.append(sim_run)\n                l.debug('Found an overlapped loop header %s', sim_run)\n            elif ((sim_run.addr, next_irsb.addr) not in self._loop_back_edges_set):\n                self._loop_back_edges_set.add((sim_run.addr, next_irsb.addr))\n                self._loop_back_edges.append((simrun_key, new_tpl))\n                l.debug('Found a loop, back edge %s --> %s', sim_run, next_irsb)\n        else:\n            if ((sim_run.addr, next_irsb.addr) not in self._loop_back_edges_set):\n                self._loop_back_edges_set.add((sim_run.addr, next_irsb.addr))\n                self._loop_back_edges.append((simrun_key, new_tpl))\n            l.debug('Found a loop, back edge %s --> %s', sim_run, next_irsb)\n", "label": 1}
{"function": "\n\ndef main(model, out_fpath):\n    store = pd.HDFStore(model)\n    from_ = store['from_'][0][0]\n    to = store['to'][0][0]\n    assert (from_ == 0)\n    trace_fpath = store['trace_fpath'][0][0]\n    kernel_class = store['kernel_class'][0][0]\n    kernel_class = eval(kernel_class)\n    Theta_zh = store['Theta_zh'].values\n    Psi_sz = store['Psi_sz'].values\n    count_z = store['count_z'].values[:, 0]\n    P = store['P'].values\n    residency_priors = store['residency_priors'].values[:, 0]\n    previous_stamps = StampLists(count_z.shape[0])\n    mem_size = store['Dts'].values.shape[1]\n    tstamps = store['Dts'].values[:, 0]\n    assign = store['assign'].values[:, 0]\n    for z in xrange(count_z.shape[0]):\n        idx = (assign == z)\n        previous_stamps._extend(z, tstamps[idx])\n    hyper2id = dict(store['hyper2id'].values)\n    obj2id = dict(store['source2id'].values)\n    HSDs = []\n    Dts = []\n    with open(trace_fpath) as trace_file:\n        for (i, l) in enumerate(trace_file):\n            if (i < to):\n                continue\n            spl = l.strip().split('\\t')\n            dts_line = [float(x) for x in spl[:mem_size]]\n            h = spl[mem_size]\n            d = spl[(- 1)]\n            sources = spl[(mem_size + 1):(- 1)]\n            all_in = ((h in hyper2id) and (d in obj2id))\n            for s in sources:\n                all_in = (all_in and (s in obj2id))\n            if all_in:\n                trace_line = (([hyper2id[h]] + [obj2id[s] for s in sources]) + [obj2id[d]])\n                HSDs.append(trace_line)\n                Dts.append(dts_line)\n    trace_size = sum(count_z)\n    kernel = kernel_class()\n    kernel.build(trace_size, count_z.shape[0], residency_priors)\n    kernel.update_state(P)\n    num_queries = min(10000, len(HSDs))\n    queries = np.random.choice(len(HSDs), size=num_queries)\n    HSDs = np.array(HSDs, dtype='i4')[queries].copy()\n    Dts = np.array(Dts, dtype='d')[queries].copy()\n    rrs = _learn.reciprocal_rank(Dts, HSDs, previous_stamps, Theta_zh, Psi_sz, count_z, kernel)\n    np.savetxt(out_fpath, rrs)\n    print(rrs.mean(axis=0))\n    store.close()\n", "label": 1}
{"function": "\n\ndef make_diff(self, merge_base, base, tip, include_files, exclude_patterns):\n    'Performs a diff on a particular branch range.'\n    rev_range = ('%s..%s' % (base, tip))\n    if include_files:\n        include_files = (['--'] + include_files)\n    git_cmd = [self.git]\n    if self._supports_git_config_flag():\n        git_cmd.extend(['-c', 'core.quotepath=false'])\n    if (self.type in ('svn', 'perforce')):\n        diff_cmd_params = ['--no-color', '--no-prefix', '-r', '-u']\n    elif (self.type == 'git'):\n        diff_cmd_params = ['--no-color', '--full-index', '--ignore-submodules']\n        if self._supports_git_config_flag():\n            git_cmd.extend(['-c', 'diff.noprefix=false'])\n        if ((self.capabilities is not None) and self.capabilities.has_capability('diffs', 'moved_files')):\n            diff_cmd_params.append('-M')\n        else:\n            diff_cmd_params.append('--no-renames')\n    else:\n        assert False\n    if (not self.config.get('GIT_USE_EXT_DIFF', False)):\n        diff_cmd_params.append('--no-ext-diff')\n    diff_cmd = ((git_cmd + ['diff']) + diff_cmd_params)\n    if exclude_patterns:\n        changed_files_cmd = ((git_cmd + ['diff-tree']) + diff_cmd_params)\n        if (self.type == 'git'):\n            changed_files_cmd.append('-r')\n        changed_files = execute(((changed_files_cmd + [rev_range]) + include_files), split_lines=True, with_errors=False, ignore_errors=True, none_on_ignored_error=True, log_output_on_error=False)\n        changed_files = remove_filenames_matching_patterns((filename.split()[(- 1)] for filename in changed_files), exclude_patterns, base_dir=self._get_root_directory())\n        diff_lines = []\n        for filename in changed_files:\n            lines = execute((diff_cmd + [rev_range, '--', filename]), split_lines=True, with_errors=False, ignore_errors=True, none_on_ignored_error=True, log_output_on_error=False, results_unicode=False)\n            if (lines is None):\n                logging.error(('Could not get diff for all files (git-diff failed for \"%s\"). Refusing to return a partial diff.' % filename))\n                diff_lines = None\n                break\n            diff_lines += lines\n    else:\n        diff_lines = execute(((diff_cmd + [rev_range]) + include_files), split_lines=True, with_errors=False, ignore_errors=True, none_on_ignored_error=True, log_output_on_error=False, results_unicode=False)\n    if (self.type == 'svn'):\n        return self.make_svn_diff(merge_base, diff_lines)\n    elif (self.type == 'perforce'):\n        return self.make_perforce_diff(merge_base, diff_lines)\n    else:\n        return b''.join(diff_lines)\n", "label": 1}
{"function": "\n\ndef _next_method(self):\n    'Read the next method from the source and process it\\n\\n        Once one complete method has been assembled, it is placed in the internal queue. This\\n        method will block until a complete `Method` has been constructed, which may consist of one\\n        or more frames.\\n        '\n    while (not self.method_queue):\n        try:\n            frame = self.transport.read_frame()\n        except Exception as exc:\n            if six.PY2:\n                (_, _, tb) = sys.exc_info()\n                exc.tb = tb\n            self.method_queue.append(exc)\n            break\n        self.frames_recv += 1\n        if (frame.frame_type not in (self.expected_types[frame.channel], 8)):\n            msg = 'Received frame type {} while expecting type: {}'.format(frame.frame_type, self.expected_types[frame.channel])\n            self.method_queue.append(UnexpectedFrame(msg, channel_id=frame.channel))\n        elif (frame.frame_type == FrameType.METHOD):\n            self._process_method_frame(frame)\n        elif (frame.frame_type == FrameType.HEADER):\n            self._process_content_header(frame)\n        elif (frame.frame_type == FrameType.BODY):\n            self._process_content_body(frame)\n", "label": 0}
{"function": "\n\ndef test_basics(backend):\n    backend.init_schema()\n    backend.create_schema()\n    francis_coppola = Director({\n        'name': 'Francis Coppola',\n    })\n    stanley_kubrick = Director({\n        'name': 'Stanley Kubrick',\n    })\n    robert_de_niro = Actor({\n        'name': 'Robert de Niro',\n        'movies': [],\n    })\n    harrison_ford = Actor({\n        'name': 'Harrison Ford',\n    })\n    brian_de_palma = Director({\n        'name': 'Brian de Palma',\n    })\n    al_pacino = Actor({\n        'name': 'Al Pacino',\n        'movies': [],\n    })\n    scarface = Movie({\n        'title': 'Scarface',\n        'director': brian_de_palma,\n    })\n    the_godfather = Movie({\n        'title': 'The Godfather',\n        'director': francis_coppola,\n    })\n    space_odyssey = Movie({\n        'title': '2001 - A space odyssey',\n        'director': stanley_kubrick,\n    })\n    clockwork_orange = Movie({\n        'title': 'A Clockwork Orange',\n        'director': stanley_kubrick,\n    })\n    robert_de_niro.movies.append(the_godfather)\n    al_pacino.movies.append(the_godfather)\n    al_pacino.movies.append(scarface)\n    apocalypse_now = Movie({\n        'title': 'Apocalypse Now',\n    })\n    star_wars_v = Movie({\n        'title': 'Star Wars V: The Empire Strikes Back',\n    })\n    harrison_ford.movies = [star_wars_v]\n    backend.save(robert_de_niro)\n    backend.save(al_pacino)\n    backend.save(francis_coppola)\n    backend.save(stanley_kubrick)\n    backend.save(brian_de_palma)\n    backend.save(harrison_ford)\n    backend.update(stanley_kubrick, {\n        'favorite_actor': al_pacino,\n    })\n    backend.update(francis_coppola, {\n        'favorite_actor': robert_de_niro,\n    })\n    backend.save(the_godfather)\n    backend.save(clockwork_orange)\n    backend.save(space_odyssey)\n    backend.save(scarface)\n    backend.commit()\n    actor = backend.get(Actor, {\n        'name': 'Al Pacino',\n    })\n    assert isinstance(actor.movies, ManyToManyProxy)\n    assert (the_godfather in actor.movies)\n    assert (scarface in actor.movies)\n    assert (len(actor.movies) == 2)\n    with backend.transaction():\n        actor.movies.remove(scarface)\n    assert (scarface not in actor.movies)\n    assert (len(actor.movies) == 1)\n    actor.movies.append(scarface)\n    assert (len(actor.movies) == 2)\n    assert (scarface in actor.movies)\n    actor.movies.append(scarface)\n    assert (len(actor.movies) == 2)\n    assert (scarface in actor.movies)\n    actor.movies.extend([scarface, the_godfather])\n    assert (len(actor.movies) == 2)\n    assert (scarface in actor.movies)\n    actor.movies.extend([scarface, the_godfather, star_wars_v])\n    assert (len(actor.movies) == 3)\n    assert (star_wars_v in actor.movies)\n    assert (len(actor.movies[1:]) == 2)\n    assert (len(actor.movies[1:2]) == 1)\n    assert (len(actor.movies[:(- 1)]) == 2)\n    assert (len(actor.movies[1:(- 1)]) == 1)\n", "label": 1}
{"function": "\n\ndef __init__(self, optimizers, failure_callback=None, ignore_newtrees=True, max_use_ratio=None, final_optimizers=None, cleanup_optimizers=None):\n    super(EquilibriumOptimizer, self).__init__(None, ignore_newtrees=ignore_newtrees, failure_callback=failure_callback)\n    self.local_optimizers_map = OrderedDict()\n    self.local_optimizers_all = []\n    self.global_optimizers = []\n    self.final_optimizers = []\n    self.cleanup_optimizers = []\n    for opt in optimizers:\n        if isinstance(opt, LocalOptimizer):\n            if (opt.tracks() is None):\n                self.local_optimizers_all.append(opt)\n            else:\n                for c in opt.tracks():\n                    self.local_optimizers_map.setdefault(c, []).append(opt)\n        else:\n            self.global_optimizers.append(opt)\n    if final_optimizers:\n        self.final_optimizers = final_optimizers\n    if cleanup_optimizers:\n        self.cleanup_optimizers = cleanup_optimizers\n    self.max_use_ratio = max_use_ratio\n    assert (self.max_use_ratio is not None), 'max_use_ratio has to be a number'\n", "label": 0}
{"function": "\n\ndef process_regular_2d_scalars(*args, **kwargs):\n    ' Converts different signatures to (x, y, s). '\n    args = convert_to_arrays(args)\n    for (index, arg) in enumerate(args):\n        if (not callable(arg)):\n            args[index] = np.atleast_2d(arg)\n    if (len(args) == 1):\n        s = args[0]\n        assert (len(s.shape) == 2), '2D array required'\n        (x, y) = np.indices(s.shape)\n    elif (len(args) == 3):\n        (x, y, s) = args\n        if callable(s):\n            s = s(x, y)\n    else:\n        raise ValueError('wrong number of arguments')\n    assert (len(s.shape) == 2), '2D array required'\n    if ('mask' in kwargs):\n        mask = kwargs['mask']\n        s[mask.astype('bool')] = np.nan\n        s = s.astype('float')\n    return (x, y, s)\n", "label": 0}
{"function": "\n\ndef draw_networkx_labels(G, pos, labels=None, font_size=12, font_color='k', font_family='sans-serif', font_weight='normal', alpha=1.0, ax=None, **kwds):\n    \"Draw node labels on the graph G.\\n\\n    Parameters\\n    ----------\\n    G : graph\\n       A networkx graph\\n\\n    pos : dictionary, optional\\n       A dictionary with nodes as keys and positions as values.\\n       If not specified a spring layout positioning will be computed.\\n       See networkx.layout for functions that compute node positions.\\n\\n    labels : dictionary, optional (default=None)\\n       Node labels in a dictionary keyed by node of text labels\\n\\n    font_size : int\\n       Font size for text labels (default=12)\\n\\n    font_color : string\\n       Font color string (default='k' black)\\n\\n    font_family : string\\n       Font family (default='sans-serif')\\n\\n    font_weight : string\\n       Font weight (default='normal')\\n\\n    alpha : float\\n       The text transparency (default=1.0)\\n\\n    ax : Matplotlib Axes object, optional\\n       Draw the graph in the specified Matplotlib axes.\\n\\n\\n    Examples\\n    --------\\n    >>> G=nx.dodecahedral_graph()\\n    >>> labels=nx.draw_networkx_labels(G,pos=nx.spring_layout(G))\\n\\n    Also see the NetworkX drawing examples at\\n    http://networkx.lanl.gov/gallery.html\\n\\n\\n    See Also\\n    --------\\n    draw()\\n    draw_networkx()\\n    draw_networkx_nodes()\\n    draw_networkx_edges()\\n    draw_networkx_edge_labels()\\n    \"\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.cbook as cb\n    except ImportError:\n        raise ImportError('Matplotlib required for draw()')\n    except RuntimeError:\n        print('Matplotlib unable to open display')\n        raise\n    if (ax is None):\n        ax = plt.gca()\n    if (labels is None):\n        labels = dict(((n, n) for n in G.nodes()))\n    horizontalalignment = kwds.get('horizontalalignment', 'center')\n    verticalalignment = kwds.get('verticalalignment', 'center')\n    text_items = {\n        \n    }\n    for (n, label) in labels.items():\n        (x, y) = pos[n]\n        if (not cb.is_string_like(label)):\n            label = str(label)\n        t = ax.text(x, y, label, size=font_size, color=font_color, family=font_family, weight=font_weight, horizontalalignment=horizontalalignment, verticalalignment=verticalalignment, transform=ax.transData, clip_on=True)\n        text_items[n] = t\n    return text_items\n", "label": 0}
{"function": "\n\ndef norm_cspace_id(cspace):\n    try:\n        cspace = ALIASES[cspace]\n    except (KeyError, TypeError):\n        pass\n    if isinstance(cspace, str):\n        if _CIECAM02_axes.issuperset(cspace):\n            return {\n                'name': 'CIECAM02-subset',\n                'ciecam02_space': CIECAM02Space.sRGB,\n                'axes': cspace,\n            }\n        else:\n            return {\n                'name': cspace,\n            }\n    elif isinstance(cspace, CIECAM02Space):\n        return {\n            'name': 'CIECAM02',\n            'ciecam02_space': cspace,\n        }\n    elif isinstance(cspace, LuoEtAl2006UniformSpace):\n        return {\n            'name': \"J'a'b'\",\n            'ciecam02_space': CIECAM02Space.sRGB,\n            'luoetal2006_space': cspace,\n        }\n    elif isinstance(cspace, dict):\n        if (cspace['name'] in ALIASES):\n            base = ALIASES[cspace['name']]\n            if (isinstance(base, dict) and (base['name'] == cspace['name'])):\n                return cspace\n            else:\n                base = norm_cspace_id(base)\n                cspace = dict(cspace)\n                del cspace['name']\n                base = dict(base)\n                base.update(cspace)\n                return base\n        return cspace\n    else:\n        raise ValueError(('unrecognized color space %r' % (cspace,)))\n", "label": 0}
{"function": "\n\ndef _create_encoder(self, bitrate=17000000, intra_period=None, profile='high', quantization=0, quality=0, inline_headers=True, sei=False, motion_output=None, intra_refresh=None):\n    '\\n        Extends the base :meth:`~PiEncoder._create_encoder` implementation to\\n        configure the video encoder for H.264 or MJPEG output.\\n        '\n    super(PiVideoEncoder, self)._create_encoder()\n    quality = (quality or quantization)\n    try:\n        self.output_port[0].format[0].encoding = {\n            'h264': mmal.MMAL_ENCODING_H264,\n            'mjpeg': mmal.MMAL_ENCODING_MJPEG,\n        }[self.format]\n    except KeyError:\n        raise PiCameraValueError(('Unsupported format %s' % self.format))\n    if (not (0 <= bitrate <= 25000000)):\n        raise PiCameraValueError('bitrate must be between 0 and 25Mbps')\n    self.output_port[0].format[0].bitrate = bitrate\n    self.output_port[0].format[0].es[0].video.frame_rate.num = 0\n    self.output_port[0].format[0].es[0].video.frame_rate.den = 1\n    mmal_check(mmal.mmal_port_format_commit(self.output_port), prefix='Unable to set format on encoder output port')\n    if (self.format == 'h264'):\n        mp = mmal.MMAL_PARAMETER_VIDEO_PROFILE_T(mmal.MMAL_PARAMETER_HEADER_T(mmal.MMAL_PARAMETER_PROFILE, ct.sizeof(mmal.MMAL_PARAMETER_VIDEO_PROFILE_T)))\n        try:\n            mp.profile[0].profile = {\n                'baseline': mmal.MMAL_VIDEO_PROFILE_H264_BASELINE,\n                'main': mmal.MMAL_VIDEO_PROFILE_H264_MAIN,\n                'high': mmal.MMAL_VIDEO_PROFILE_H264_HIGH,\n                'constrained': mmal.MMAL_VIDEO_PROFILE_H264_CONSTRAINED_BASELINE,\n            }[profile]\n        except KeyError:\n            raise PiCameraValueError(('Invalid H.264 profile %s' % profile))\n        mp.profile[0].level = mmal.MMAL_VIDEO_LEVEL_H264_4\n        mmal_check(mmal.mmal_port_parameter_set(self.output_port, mp.hdr), prefix='Unable to set encoder H.264 profile')\n        if inline_headers:\n            mmal_check(mmal.mmal_port_parameter_set_boolean(self.output_port, mmal.MMAL_PARAMETER_VIDEO_ENCODE_INLINE_HEADER, mmal.MMAL_TRUE), prefix='Unable to set inline_headers')\n        if sei:\n            mmal_check(mmal.mmal_port_parameter_set_boolean(self.output_port, mmal.MMAL_PARAMETER_VIDEO_ENCODE_SEI_ENABLE, mmal.MMAL_TRUE), prefix='Unable to set SEI')\n        if motion_output:\n            mmal_check(mmal.mmal_port_parameter_set_boolean(self.output_port, mmal.MMAL_PARAMETER_VIDEO_ENCODE_INLINE_VECTORS, mmal.MMAL_TRUE), prefix='Unable to set inline motion vectors')\n        if (intra_period is not None):\n            mp = mmal.MMAL_PARAMETER_UINT32_T(mmal.MMAL_PARAMETER_HEADER_T(mmal.MMAL_PARAMETER_INTRAPERIOD, ct.sizeof(mmal.MMAL_PARAMETER_UINT32_T)), intra_period)\n            mmal_check(mmal.mmal_port_parameter_set(self.output_port, mp.hdr), prefix='Unable to set encoder intra_period')\n            self._intra_period = intra_period\n        else:\n            mp = mmal.MMAL_PARAMETER_UINT32_T(mmal.MMAL_PARAMETER_HEADER_T(mmal.MMAL_PARAMETER_INTRAPERIOD, ct.sizeof(mmal.MMAL_PARAMETER_UINT32_T)))\n            mmal_check(mmal.mmal_port_parameter_get(self.output_port, mp.hdr), prefix='Unable to get encoder intra_period')\n            self._intra_period = mp.value\n        if (intra_refresh is not None):\n            mp = mmal.MMAL_PARAMETER_VIDEO_INTRA_REFRESH_T(mmal.MMAL_PARAMETER_HEADER_T(mmal.MMAL_PARAMETER_VIDEO_INTRA_REFRESH, ct.sizeof(mmal.MMAL_PARAMETER_VIDEO_INTRA_REFRESH_T)))\n            mmal.mmal_port_parameter_get(self.output_port, mp.hdr)\n            try:\n                mp.refresh_mode = {\n                    'cyclic': mmal.MMAL_VIDEO_INTRA_REFRESH_CYCLIC,\n                    'adaptive': mmal.MMAL_VIDEO_INTRA_REFRESH_ADAPTIVE,\n                    'both': mmal.MMAL_VIDEO_INTRA_REFRESH_BOTH,\n                    'cyclicrows': mmal.MMAL_VIDEO_INTRA_REFRESH_CYCLIC_MROWS,\n                }[intra_refresh]\n            except KeyError:\n                raise PiCameraValueError(('Invalid intra_refresh %s' % intra_refresh))\n            mmal_check(mmal.mmal_port_parameter_set(self.output_port, mp.hdr), prefix='Unable to set encoder intra_refresh')\n    elif (self.format == 'mjpeg'):\n        self._intra_period = 1\n    if quality:\n        mp = mmal.MMAL_PARAMETER_UINT32_T(mmal.MMAL_PARAMETER_HEADER_T(mmal.MMAL_PARAMETER_VIDEO_ENCODE_INITIAL_QUANT, ct.sizeof(mmal.MMAL_PARAMETER_UINT32_T)), quality)\n        mmal_check(mmal.mmal_port_parameter_set(self.output_port, mp.hdr), prefix='Unable to set initial quality')\n        mp = mmal.MMAL_PARAMETER_UINT32_T(mmal.MMAL_PARAMETER_HEADER_T(mmal.MMAL_PARAMETER_VIDEO_ENCODE_MIN_QUANT, ct.sizeof(mmal.MMAL_PARAMETER_UINT32_T)), quality)\n        mmal_check(mmal.mmal_port_parameter_set(self.output_port, mp.hdr), prefix='Unable to set minimum quality')\n        mp = mmal.MMAL_PARAMETER_UINT32_T(mmal.MMAL_PARAMETER_HEADER_T(mmal.MMAL_PARAMETER_VIDEO_ENCODE_MAX_QUANT, ct.sizeof(mmal.MMAL_PARAMETER_UINT32_T)), quality)\n        mmal_check(mmal.mmal_port_parameter_set(self.output_port, mp.hdr), prefix='Unable to set maximum quality')\n    mmal_check(mmal.mmal_port_parameter_set_boolean(self.encoder[0].input[0], mmal.MMAL_PARAMETER_VIDEO_IMMUTABLE_INPUT, 1), prefix='Unable to set immutable flag on encoder input port')\n    mmal_check(mmal.mmal_component_enable(self.encoder), prefix='Unable to enable video encoder component')\n", "label": 1}
{"function": "\n\ndef eucalyptus(account):\n    myCredAccount = CredAccountEws()\n    if (not ('secretKey' in account)):\n        printer.out('secretKey in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('queryId' in account)):\n        printer.out('queryId in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('endpoint' in account)):\n        printer.out('endpoint in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('cloudCert' in account)):\n        printer.out('cloudCert in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('x509Cert' in account)):\n        printer.out('x509Cert in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('x509PrivateKey' in account)):\n        printer.out('x509PrivateKey in azure eucalyptus not found', printer.ERROR)\n        return\n    if (not ('accountNumber' in account)):\n        printer.out('accountNumber for eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('name' in account)):\n        printer.out('name for eucalyptus account not found', printer.ERROR)\n        return\n    myCredAccount.accountNumber = account['accountNumber']\n    myCredAccount.name = account['name']\n    myCredAccount.hostname = account['endpoint']\n    myCredAccount.secretAccessKeyID = account['secretKey']\n    myCredAccount.accessKeyID = account['queryId']\n    myCertificates = certificates()\n    myCredAccount.certificates = myCertificates\n    try:\n        myCertificate = certificate()\n        with open(account['x509Cert'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'x509'\n        myCertificate.name = ntpath.basename(account['x509Cert'])\n        myCertificates.add_certificate(myCertificate)\n        myCertificate = certificate()\n        with open(account['x509PrivateKey'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'ec2PrivateKey'\n        myCertificate.name = ntpath.basename(account['x509PrivateKey'])\n        myCertificates.add_certificate(myCertificate)\n        myCertificate = certificate()\n        with open(account['cloudCert'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'eucCert'\n        myCertificate.name = ntpath.basename(account['cloudCert'])\n        myCertificates.add_certificate(myCertificate)\n    except IOError as e:\n        printer.out(('File error: ' + str(e)), printer.ERROR)\n        return\n    return myCredAccount\n", "label": 0}
{"function": "\n\ndef _do_rebuild_instance(self, context, instance, orig_image_ref, image_ref, injected_files, new_pass, orig_sys_metadata, bdms, recreate, on_shared_storage, preserve_ephemeral):\n    orig_vm_state = instance.vm_state\n    if recreate:\n        if (not self.driver.capabilities['supports_recreate']):\n            raise exception.InstanceRecreateNotSupported\n        self._check_instance_exists(context, instance)\n        if (on_shared_storage is None):\n            LOG.debug('on_shared_storage is not provided, using driverinformation to decide if the instance needs tobe recreated')\n            on_shared_storage = self.driver.instance_on_disk(instance)\n        elif (on_shared_storage != self.driver.instance_on_disk(instance)):\n            raise exception.InvalidSharedStorage(_('Invalid state of instance files on shared storage'))\n        if on_shared_storage:\n            LOG.info(_LI('disk on shared storage, recreating using existing disk'))\n        else:\n            image_ref = orig_image_ref = instance.image_ref\n            LOG.info(_LI(\"disk not on shared storage, rebuilding from: '%s'\"), str(image_ref))\n    if image_ref:\n        image_meta = objects.ImageMeta.from_image_ref(context, self.image_api, image_ref)\n    else:\n        image_meta = objects.ImageMeta.from_dict({\n            \n        })\n    orig_image_ref_url = glance.generate_image_url(orig_image_ref)\n    extra_usage_info = {\n        'image_ref_url': orig_image_ref_url,\n    }\n    compute_utils.notify_usage_exists(self.notifier, context, instance, current_period=True, system_metadata=orig_sys_metadata, extra_usage_info=extra_usage_info)\n    extra_usage_info = {\n        'image_name': self._get_image_name(image_meta),\n    }\n    self._notify_about_instance_usage(context, instance, 'rebuild.start', extra_usage_info=extra_usage_info)\n    instance.power_state = self._get_power_state(context, instance)\n    instance.task_state = task_states.REBUILDING\n    instance.save(expected_task_state=[task_states.REBUILDING])\n    if recreate:\n        self.network_api.setup_networks_on_host(context, instance, self.host)\n        self.network_api.setup_instance_network_on_host(context, instance, self.host)\n    network_info = compute_utils.get_nw_info_for_instance(instance)\n    if (bdms is None):\n        bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(context, instance.uuid)\n    block_device_info = self._get_instance_block_device_info(context, instance, bdms=bdms)\n\n    def detach_block_devices(context, bdms):\n        for bdm in bdms:\n            if bdm.is_volume:\n                self._detach_volume(context, bdm.volume_id, instance, destroy_bdm=False)\n    files = self._decode_files(injected_files)\n    kwargs = dict(context=context, instance=instance, image_meta=image_meta, injected_files=files, admin_password=new_pass, bdms=bdms, detach_block_devices=detach_block_devices, attach_block_devices=self._prep_block_device, block_device_info=block_device_info, network_info=network_info, preserve_ephemeral=preserve_ephemeral, recreate=recreate)\n    try:\n        with instance.mutated_migration_context():\n            self.driver.rebuild(**kwargs)\n    except NotImplementedError:\n        self._rebuild_default_impl(**kwargs)\n    self._update_instance_after_spawn(context, instance)\n    instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])\n    if (orig_vm_state == vm_states.STOPPED):\n        LOG.info(_LI(\"bringing vm to original state: '%s'\"), orig_vm_state, instance=instance)\n        instance.vm_state = vm_states.ACTIVE\n        instance.task_state = task_states.POWERING_OFF\n        instance.progress = 0\n        instance.save()\n        self.stop_instance(context, instance, False)\n    self._update_scheduler_instance_info(context, instance)\n    self._notify_about_instance_usage(context, instance, 'rebuild.end', network_info=network_info, extra_usage_info=extra_usage_info)\n", "label": 1}
{"function": "\n\n@classmethod\ndef create_user(cls, auth_id, unique_properties=None, **user_values):\n    'Creates a new user.\\n\\n        :param auth_id:\\n            A string that is unique to the user. Users may have multiple\\n            auth ids. Example auth ids:\\n\\n            - own:username\\n            - own:email@example.com\\n            - google:username\\n            - yahoo:username\\n\\n            The value of `auth_id` must be unique.\\n        :param unique_properties:\\n            Sequence of extra property names that must be unique.\\n        :param user_values:\\n            Keyword arguments to create a new user entity. Since the model is\\n            an ``Expando``, any provided custom properties will be saved.\\n            To hash a plain password, pass a keyword ``password_raw``.\\n        :returns:\\n            A tuple (boolean, info). The boolean indicates if the user\\n            was created. If creation succeeds, ``info`` is the user entity;\\n            otherwise it is a list of duplicated unique properties that\\n            caused creation to fail.\\n        '\n    assert (user_values.get('password') is None), 'Use password_raw instead of password to create new users.'\n    assert (not isinstance(auth_id, list)), 'Creating a user with multiple auth_ids is not allowed, please provide a single auth_id.'\n    if ('password_raw' in user_values):\n        user_values['password'] = security.generate_password_hash(user_values.pop('password_raw'), length=12)\n    user_values['auth_ids'] = [auth_id]\n    user = cls(**user_values)\n    uniques = [(('%s.auth_id:%s' % (cls.__name__, auth_id)), 'auth_id')]\n    if unique_properties:\n        for name in unique_properties:\n            key = ('%s.%s:%s' % (cls.__name__, name, user_values[name]))\n            uniques.append((key, name))\n    (ok, existing) = cls.unique_model.create_multi((k for (k, v) in uniques))\n    if ok:\n        user.put()\n        return (True, user)\n    else:\n        properties = [v for (k, v) in uniques if (k in existing)]\n        return (False, properties)\n", "label": 0}
{"function": "\n\ndef __setitem__(self, key, value):\n    if (isinstance(value, bytearray) or isinstance(value, str)):\n        start_addr = 0\n        stop_addr = 0\n        if isinstance(key, slice):\n            start_addr = int(key.start)\n            stop_addr = int(key.stop)\n        else:\n            start_addr = int(key)\n            stop_addr = (int(key) + 1)\n        self.mem[start_addr:stop_addr] = value\n    else:\n        start_addr = 0\n        num_bytes = 0\n        if isinstance(key, slice):\n            start_addr = int(key.start)\n            num_bytes = (int(key.stop) - int(key.start))\n        else:\n            start_addr = int(key)\n            num_bytes = 1\n        if isinstance(value, Bits):\n            bits = value\n            assert ((value.nbits % 8) == 0)\n        else:\n            bits = Bits((num_bytes * 8), value)\n        for i in range(num_bytes):\n            self.mem[(start_addr + i)] = bits[(i * 8):((i * 8) + 8)]\n", "label": 0}
{"function": "\n\ndef _next_XMIT_1(self, intent):\n    if hasattr(self, '_openSockets'):\n        if (intent.targetAddr in self._openSockets):\n            intent.socket = self._openSockets[intent.targetAddr].socket\n            del self._openSockets[intent.targetAddr]\n            intent.stage = self._XMITStepSendData\n            intent.amtSent = 0\n            return self._nextTransmitStep(intent)\n        if any((T for T in self._transmitIntents.values() if ((T.targetAddr == intent.targetAddr) and hasattr(T, 'socket')))):\n            intent.awaitingTXSlot()\n            return True\n    if (isinstance(intent.targetAddr.addressDetails, TXOnlyAdminTCPv4ActorAddress) and (intent.targetAddr != self._adminAddr)):\n        intent.backoffPause(True)\n        intent.stage = self._XMITStepRetry\n        return self._nextTransmitStep(intent)\n    intent.socket = socket.socket(*intent.targetAddr.addressDetails.socketArgs)\n    intent.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    intent.socket.setblocking(0)\n    intent.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n    try:\n        intent.socket.connect(*intent.targetAddr.addressDetails.connectArgs)\n    except socket.error as err:\n        if (not err_inprogress(err.errno)):\n            thesplog('Socket connect failure %s to %s on %s (returning %s)', err, intent.targetAddr, intent.socket, intent.completionCallback, level=logging.WARNING)\n            return self._finishIntent(intent, (SendStatus.DeadTarget if err_conn_refused(err) else SendStatus.Failed))\n        intent.backoffPause(True)\n    except Exception as ex:\n        thesplog('Unexpected TCP socket connect exception: %s', ex, level=logging.ERROR)\n        return self._finishIntent(intent, SendStatus.BadPacket)\n    intent.stage = self._XMITStepSendData\n    intent.amtSent = 0\n    return True\n", "label": 1}
{"function": "\n\ndef test_get_open_files(self):\n    p = psutil.Process(os.getpid())\n    files = p.get_open_files()\n    self.assertFalse((TESTFN in files))\n    f = open(TESTFN, 'w')\n    call_until(p.get_open_files, ('len(ret) != %i' % len(files)))\n    filenames = [x.path for x in p.get_open_files()]\n    self.assertIn(TESTFN, filenames)\n    f.close()\n    for file in filenames:\n        assert os.path.isfile(file), file\n    cmdline = (\"import time; f = open(r'%s', 'r'); time.sleep(100);\" % TESTFN)\n    sproc = get_test_subprocess([PYTHON, '-c', cmdline], wait=True)\n    p = psutil.Process(sproc.pid)\n    for x in range(100):\n        filenames = [x.path for x in p.get_open_files()]\n        if (TESTFN in filenames):\n            break\n        time.sleep(0.01)\n    else:\n        self.assertIn(TESTFN, filenames)\n    for file in filenames:\n        assert os.path.isfile(file), file\n", "label": 0}
{"function": "\n\ndef expand_macro(self, formatter, name, content, args=None):\n    args = (args or {\n        \n    })\n    reponame = (args.get('repository') or '')\n    rev = args.get('revision')\n    repos = RepositoryManager(self.env).get_repository(reponame)\n    try:\n        changeset = repos.get_changeset(rev)\n        message = changeset.message\n        rev = changeset.rev\n        resource = repos.resource\n    except Exception:\n        message = content\n        resource = Resource('repository', reponame)\n    if (formatter.context.resource.realm == 'ticket'):\n        ticket_re = CommitTicketUpdater.ticket_re\n        if (not any(((int(tkt_id) == int(formatter.context.resource.id)) for tkt_id in ticket_re.findall(message)))):\n            return tag.p(_(\"(The changeset message doesn't reference this ticket)\"), class_='hint')\n    if ChangesetModule(self.env).wiki_format_messages:\n        return tag.div(format_to_html(self.env, formatter.context.child('changeset', rev, parent=resource), message, escape_newlines=True), class_='message')\n    else:\n        return tag.pre(message, class_='message')\n", "label": 0}
{"function": "\n\ndef run(self, command_line=''):\n    assert command_line, 'expected non-empty command_line'\n    parsed = parse_command_line(command_line)\n    global_range = None\n    if parsed.line_range.is_empty:\n        global_range = R(0, self._view.size())\n    else:\n        global_range = parsed.line_range.resolve(self._view)\n    pattern = parsed.command.pattern\n    if pattern:\n        ExGlobal.most_recent_pat = pattern\n    else:\n        pattern = ExGlobal.most_recent_pat\n    subcmd = parsed.command.subcommand\n    try:\n        matches = find_all_in_range(self._view, pattern, global_range.begin(), global_range.end())\n    except Exception as e:\n        msg = (\"Vintageous (global): %s ... in pattern '%s'\" % (str(e), pattern))\n        sublime.status_message(msg)\n        print(msg)\n        return\n    if ((not matches) or (not parsed.command.subcommand.cooperates_with_global)):\n        return\n    matches = [self._view.full_line(r.begin()) for r in matches]\n    matches = [[r.a, r.b] for r in matches]\n    self.window.run_command(subcmd.target_command, {\n        'command_line': str(subcmd),\n        'global_lines': matches,\n    })\n", "label": 0}
{"function": "\n\ndef leastsq(func, x0, args=(), Dfun=None, ftol=1e-07, xtol=1e-07, gtol=1e-07, maxfev=0, epsfcn=None, factor=100, diag=None):\n    '\\n    Minimize the sum of squares of a set of equations.\\n    Adopted from scipy.optimize.leastsq\\n\\n    ::\\n\\n        x = arg min(sum(func(y)**2,axis=0))\\n                 y\\n\\n    Parameters\\n    ----------\\n    func : callable\\n        should take at least one (possibly length N vector) argument and\\n        returns M floating point numbers.\\n    x0 : ndarray\\n        The starting estimate for the minimization.\\n    args : tuple\\n        Any extra arguments to func are placed in this tuple.\\n    Dfun : callable\\n        A function or method to compute the Jacobian of func with derivatives\\n        across the rows. If this is None, the Jacobian will be estimated.\\n    ftol : float\\n        Relative error desired in the sum of squares.\\n    xtol : float\\n        Relative error desired in the approximate solution.\\n    gtol : float\\n        Orthogonality desired between the function vector and the columns of\\n        the Jacobian.\\n    maxfev : int\\n        The maximum number of calls to the function. If zero, then 100*(N+1) is\\n        the maximum where N is the number of elements in x0.\\n    epsfcn : float\\n        A suitable step length for the forward-difference approximation of the\\n        Jacobian (for Dfun=None). If epsfcn is less than the machine precision,\\n        it is assumed that the relative errors in the functions are of the\\n        order of the machine precision.\\n    factor : float\\n        A parameter determining the initial step bound\\n        (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\\n    diag : sequence\\n        N positive entries that serve as a scale factors for the variables.\\n\\n    Returns\\n    -------\\n    x : ndarray\\n        The solution (or the result of the last iteration for an unsuccessful\\n        call).\\n    cov_x : ndarray\\n        Uses the fjac and ipvt optional outputs to construct an\\n        estimate of the jacobian around the solution.  ``None`` if a\\n        singular matrix encountered (indicates very flat curvature in\\n        some direction).  This matrix must be multiplied by the\\n        residual variance to get the covariance of the\\n        parameter estimates -- see curve_fit.\\n    infodict : dict\\n        a dictionary of optional outputs with the key s::\\n\\n            - \\'nfev\\' : the number of function calls\\n            - \\'fvec\\' : the function evaluated at the output\\n            - \\'fjac\\' : A permutation of the R matrix of a QR\\n                     factorization of the final approximate\\n                     Jacobian matrix, stored column wise.\\n                     Together with ipvt, the covariance of the\\n                     estimate can be approximated.\\n            - \\'ipvt\\' : an integer array of length N which defines\\n                     a permutation matrix, p, such that\\n                     fjac*p = q*r, where r is upper triangular\\n                     with diagonal elements of nonincreasing\\n                     magnitude. Column j of p is column ipvt(j)\\n                     of the identity matrix.\\n            - \\'qtf\\'  : the vector (transpose(q) * fvec).\\n\\n    mesg : str\\n        A string message giving information about the cause of failure.\\n    ier : int\\n        An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\\n        found.  Otherwise, the solution was not found. In either case, the\\n        optional output variable \\'mesg\\' gives more information.\\n\\n    Notes\\n    -----\\n    \"leastsq\" is a wrapper around MINPACK\\'s lmdif and lmder algorithms.\\n\\n    cov_x is a Jacobian approximation to the Hessian of the least squares\\n    objective function.\\n    This approximation assumes that the objective function is based on the\\n    difference between some observed target data (ydata) and a (non-linear)\\n    function of the parameters `f(xdata, params)` ::\\n\\n           func(params) = ydata - f(xdata, params)\\n\\n    so that the objective function is ::\\n\\n           min   sum((ydata - f(xdata, params))**2, axis=0)\\n         params\\n\\n    '\n    x0 = asarray(x0).flatten()\n    n = len(x0)\n    if (not isinstance(args, tuple)):\n        args = (args,)\n    shape = _check_func('leastsq', 'func', func, x0, args, n)\n    if (isinstance(shape, tuple) and (len(shape) > 1)):\n        shape = shape[0]\n    m = shape[0]\n    if (n > m):\n        raise TypeError(('Improper input: N=%s must not exceed M=%s' % (n, m)))\n    if (maxfev == 0):\n        maxfev = (200 * (n + 1))\n    if (epsfcn is None):\n        epsfcn = 2e-05\n    if (Dfun is None):\n        retval = _minpack._lmdif(func, x0, args, 1, ftol, xtol, gtol, maxfev, epsfcn, factor, diag)\n    else:\n        _check_func('leastsq', 'Dfun', Dfun, x0, args, n, (m, n))\n        retval = _minpack._lmder(func, Dfun, x0, args, 1, 0, ftol, xtol, gtol, maxfev, factor, diag)\n    errors = {\n        0: ['Improper input parameters.', TypeError],\n        1: [('Both actual and predicted relative reductions in the sum of squares\\n  are at most %f' % ftol), None],\n        2: [('The relative error between two consecutive iterates is at most %f' % xtol), None],\n        3: [('Both actual and predicted relative reductions in the sum of squares\\n  are at most %f and the relative error between two consecutive iterates is at \\n  most %f' % (ftol, xtol)), None],\n        4: [('The cosine of the angle between func(x) and any column of the\\n  Jacobian is at most %f in absolute value' % gtol), None],\n        5: [('Number of calls to function has reached maxfev = %d.' % maxfev), ValueError],\n        6: [('ftol=%f is too small, no further reduction in the sum of squares\\n  is possible.' % ftol), ValueError],\n        7: [('xtol=%f is too small, no further improvement in the approximate\\n  solution is possible.' % xtol), ValueError],\n        8: [('gtol=%f is too small, func(x) is orthogonal to the columns of\\n  the Jacobian to machine precision.' % gtol), ValueError],\n        'unknown': ['Unknown error.', TypeError],\n    }\n    info = retval[(- 1)]\n    mesg = errors[info][0]\n    cov_x = None\n    if (info in [1, 2, 3, 4]):\n        perm = take(eye(n), (retval[1]['ipvt'] - 1), 0)\n        r = triu(transpose(retval[1]['fjac'])[:n, :])\n        R = dot(r, perm)\n        try:\n            cov_x = inv(dot(transpose(R), R))\n        except (LinAlgError, ValueError):\n            pass\n    return (((retval[0], cov_x) + retval[1:(- 1)]) + (mesg, info))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef assignment_onvalidation(form):\n    '\\n            Validation callback for work assignments:\\n            - a worker can only be assigned once to the same job\\n\\n            @param form: the FORM\\n        '\n    db = current.db\n    s3db = current.s3db\n    table = s3db.work_assignment\n    form_vars = form.vars\n    if ('id' in form_vars):\n        record_id = form_vars.id\n    elif hasattr(form, 'record_id'):\n        record_id = form.record_id\n    else:\n        record_id = None\n    try:\n        job_id = form_vars.job_id\n    except AttributeError:\n        job_id = None\n    try:\n        person_id = form_vars.person_id\n    except AttributeError:\n        person_id = None\n    if ((job_id is None) or (person_id is None)):\n        if record_id:\n            query = ((table.id == record_id) & (table.deleted != True))\n            record = db(query).select(table.job_id, table.person_id, limitby=(0, 1)).first()\n            if record:\n                job_id = record.job_id\n                person_id = record.person_id\n        else:\n            if (job_id is None):\n                job_id = table.job_id.default\n            if (person_id is None):\n                person_id = table.person_id.default\n    if (job_id and person_id):\n        query = (((table.job_id == job_id) & (table.person_id == person_id)) & (table.deleted != True))\n        if record_id:\n            query = ((table.id != record_id) & query)\n        duplicate = db(query).select(table.id, limitby=(0, 1)).first()\n        if duplicate:\n            msg = current.T('This person is already assigned to the job')\n            form.errors['person_id'] = msg\n", "label": 1}
{"function": "\n\ndef testFromNetworkXGraph(self):\n    try:\n        import networkx\n    except ImportError as error:\n        logging.debug(error)\n        return\n    nxGraph = networkx.Graph()\n    nxGraph.graph['VListType'] = GeneralVertexList\n    nxGraph.add_edge(0, 1)\n    nxGraph.add_edge(1, 2)\n    nxGraph.add_edge(1, 3)\n    graph = self.GraphType.fromNetworkXGraph(nxGraph)\n    self.assertTrue((graph.getNumVertices() == 4))\n    self.assertTrue((graph.isUndirected() == True))\n    self.assertTrue((graph.getNumEdges() == 3))\n    self.assertTrue((graph.getEdge(0, 1) == 1))\n    self.assertTrue((graph.getEdge(1, 2) == 1))\n    self.assertTrue((graph.getEdge(1, 3) == 1))\n    nxGraph = networkx.DiGraph()\n    nxGraph.graph['VListType'] = GeneralVertexList\n    nxGraph.add_edge(0, 1)\n    nxGraph.add_edge(1, 2)\n    nxGraph.add_edge(1, 3)\n    graph = self.GraphType.fromNetworkXGraph(nxGraph)\n    self.assertTrue((graph.getNumVertices() == 4))\n    self.assertTrue((graph.isUndirected() == False))\n    self.assertTrue((graph.getNumEdges() == 3))\n    self.assertTrue((graph.getEdge(0, 1) == 1))\n    self.assertTrue((graph.getEdge(1, 2) == 1))\n    self.assertTrue((graph.getEdge(1, 3) == 1))\n    nxGraph = networkx.MultiGraph()\n    self.assertRaises(ValueError, self.GraphType.fromNetworkXGraph, nxGraph)\n    nxGraph = networkx.DiGraph()\n    nxGraph.graph['VListType'] = GeneralVertexList\n    nxGraph.add_node('a', label='abc')\n    nxGraph.add_node('b', label='i')\n    nxGraph.add_node('c', label='am')\n    nxGraph.add_node('d', label='here')\n    nxGraph.add_edge('a', 'b')\n    nxGraph.add_edge('b', 'c')\n    nxGraph.add_edge('b', 'd')\n    graph = self.GraphType.fromNetworkXGraph(nxGraph)\n    nodeDict = {\n        \n    }\n    for i in range(len(nxGraph.nodes())):\n        nodeDict[nxGraph.nodes()[i]] = i\n    self.assertTrue((graph.getNumVertices() == 4))\n    self.assertTrue((graph.isUndirected() == False))\n    self.assertTrue((graph.getNumEdges() == 3))\n    self.assertTrue((graph.getEdge(nodeDict['a'], nodeDict['b']) == 1))\n    self.assertTrue((graph.getEdge(nodeDict['b'], nodeDict['c']) == 1))\n    self.assertTrue((graph.getEdge(nodeDict['b'], nodeDict['d']) == 1))\n    self.assertTrue((graph.getVertex(0) == 'abc'))\n    self.assertTrue((graph.getVertex(1) == 'am'))\n    self.assertTrue((graph.getVertex(2) == 'i'))\n    self.assertTrue((graph.getVertex(3) == 'here'))\n    numVertices = 10\n    numFeatures = 2\n    vList = VertexList(numVertices, numFeatures)\n    vList.setVertices(numpy.random.rand(numVertices, numFeatures))\n    graph = self.GraphType(vList)\n    graph.addEdge(0, 1)\n    graph.addEdge(0, 5)\n    graph.addEdge(2, 5)\n    graph.addEdge(3, 4)\n    nxGraph = graph.toNetworkXGraph()\n    graph2 = self.GraphType.fromNetworkXGraph(nxGraph)\n    tol = (10 ** (- 6))\n    self.assertTrue((numpy.linalg.norm((graph.getVertexList().getVertices(list(range(numVertices))) - graph2.getVertexList().getVertices(list(range(numVertices))))) < tol))\n    self.assertEquals(graph.getNumEdges(), graph2.getNumEdges())\n    for i in range(numVertices):\n        for j in range(numVertices):\n            self.assertEquals(graph.getEdge(i, j), graph2.getEdge(i, j))\n    numVertices = 10\n    vList = GeneralVertexList(numVertices)\n    for i in range(numVertices):\n        vList.setVertex(i, ('s' + str(i)))\n    graph = self.GraphType(vList)\n    graph.addEdge(0, 1)\n    graph.addEdge(0, 5)\n    graph.addEdge(2, 5)\n    graph.addEdge(3, 4)\n    nxGraph = graph.toNetworkXGraph()\n    graph2 = self.GraphType.fromNetworkXGraph(nxGraph)\n    for i in range(numVertices):\n        self.assertEquals(graph.getVertex(i), graph2.getVertex(i))\n    self.assertEquals(graph.getNumEdges(), graph2.getNumEdges())\n    for i in range(numVertices):\n        for j in range(numVertices):\n            self.assertEquals(graph.getEdge(i, j), graph2.getEdge(i, j))\n", "label": 0}
{"function": "\n\ndef makeService(self, options):\n    actor = options['actor']\n    if (not actor):\n        fatal('error: no actor specified')\n        sys.exit(1)\n    try:\n        (module_path, actor_cls_name) = actor.rsplit('.', 1)\n    except ValueError:\n        fatal(('error: invalid path to actor %s' % actor))\n        sys.exit(1)\n    try:\n        mod = __import__(module_path, globals(), locals(), [actor_cls_name], (- 1))\n    except ImportError:\n        fatal(('error: could not import %s:\\n%s' % (actor, traceback.format_exc())))\n        sys.exit(1)\n    try:\n        actor_cls = getattr(mod, actor_cls_name)\n    except AttributeError:\n        fatal(('error: no such actor %s' % actor))\n        sys.exit(1)\n    kwargs = {\n        \n    }\n    if (options['params'] is not _EMPTY):\n        params = ('dict(%s)' % (options['params'],))\n        try:\n            params = eval(params)\n        except:\n            fatal('error: could not parse parameters')\n            sys.exit(1)\n        else:\n            kwargs['init_params'] = params\n    if (options['message'] is not _EMPTY):\n        initial_message = options['message']\n        try:\n            initial_message = eval(initial_message)\n        except:\n            fatal('error: could not parse initial message')\n            sys.exit(1)\n        else:\n            kwargs['initial_message'] = initial_message\n    if options['name']:\n        name = options['name']\n        if ('/' in name):\n            fatal('invalid name: names cannot contain slashes')\n            sys.exit(1)\n        else:\n            kwargs['name'] = name\n    if options['remoting']:\n        nodeid = options['remoting']\n        try:\n            _validate_nodeid(nodeid)\n        except TypeError:\n            fatal('invalid node ID')\n            sys.exit(1)\n        else:\n            kwargs['nodeid'] = nodeid\n    kwargs['keep_running'] = options['keeprunning']\n    m = MultiService()\n    actor_runner = ActorRunner(actor_cls, **kwargs)\n    actor_runner.setServiceParent(m)\n    if options['remotedebugging']:\n        port = options['remotedebuggingport']\n        try:\n            port = int(port)\n        except ValueError:\n            fatal(('Invalid port specified: %r' % port))\n            sys.exit(1)\n        username = options['remotedebuggingusername']\n        password = options['remotedebuggingpassword']\n        manhole = self.make_manhole_server(port, username, password)\n        manhole.setServiceParent(m)\n    return m\n", "label": 1}
{"function": "\n\ndef _collect(self):\n    LOG.debug(('collecting arguments/commands for %s' % self))\n    arguments = []\n    commands = []\n    arguments = list(self._meta.arguments)\n    for member in dir(self.__class__):\n        if member.startswith('_'):\n            continue\n        try:\n            func = getattr(self.__class__, member).__cement_meta__\n        except AttributeError:\n            continue\n        else:\n            func['controller'] = self\n            commands.append(func)\n    for contr in handler.list('controller'):\n        if (contr == self.__class__):\n            continue\n        contr = contr()\n        contr._setup(self.app)\n        if (contr._meta.stacked_on == self._meta.label):\n            if (contr._meta.stacked_type == 'embedded'):\n                (contr_arguments, contr_commands) = contr._collect()\n                for arg in contr_arguments:\n                    arguments.append(arg)\n                for func in contr_commands:\n                    commands.append(func)\n            elif (contr._meta.stacked_type == 'nested'):\n                metadict = {\n                    \n                }\n                metadict['label'] = re.sub('_', '-', contr._meta.label)\n                metadict['func_name'] = '_dispatch'\n                metadict['exposed'] = True\n                metadict['hide'] = contr._meta.hide\n                metadict['help'] = contr._meta.description\n                metadict['aliases'] = contr._meta.aliases\n                metadict['aliases_only'] = contr._meta.aliases_only\n                metadict['controller'] = contr\n                commands.append(metadict)\n    return (arguments, commands)\n", "label": 1}
{"function": "\n\ndef test_call_some_more():\n    from commonast import Name, Num, Starred, Keyword\n    code = 'foo(1, a, *b, c=3, **d)'\n    node = commonast.parse(code).body_nodes[0].value_node\n    assert isinstance(node, commonast.Call)\n    assert (len(node.arg_nodes) == 3)\n    assert (len(node.kwarg_nodes) == 2)\n    for (arg, cls) in zip((node.arg_nodes + node.kwarg_nodes), [Num, Name, Starred, Num, None.__class__]):\n        isinstance(arg, cls)\n    assert (node.arg_nodes[2].value_node.name == 'b')\n    assert (node.kwarg_nodes[1].name is None)\n    assert (node.kwarg_nodes[1].value_node.name == 'd')\n", "label": 0}
{"function": "\n\ndef cmp_run_conv_nnet2_classif(seed, isize, ksize, bsize, ignore_error=False, n_train=10, gpu_only=False, cpu_only=False, float_atol=1e-06, check_isfinite=True, pickle=False, verbose=0, version=(- 1)):\n    'Run the nnet2 function on 1 or 2 devices, and compares the results.\\n\\n       float_atol: None mean use the default value.\\n       check_isfinite: the debug mode option. We forward this value to debug mode.\\n                       For some parameter CrossentropyCategorical1Hot op generate inf when not optimized.\\n    '\n    if (config.mode == 'DEBUG_MODE'):\n        n_train = 1\n    orig_float32_atol = theano.tensor.basic.float32_atol\n    try:\n        if float_atol:\n            theano.tensor.basic.float32_atol = float_atol\n        if (gpu_only and cpu_only):\n            raise ValueError('Please use only one of cpu_only and gpu_only')\n        elif cpu_only:\n            use_gpu = False\n            compare = False\n        elif gpu_only:\n            use_gpu = True\n            compare = False\n        else:\n            compare = True\n        if (not compare):\n            return run_conv_nnet2_classif(use_gpu=use_gpu, seed=seed, isize=isize, ksize=ksize, bsize=bsize, n_train=n_train, check_isfinite=check_isfinite, pickle=pickle, verbose=verbose, version=version)\n        utt.seed_rng(seed)\n        (train_cpu, params_cpu, x_shape, y_shape, mode_cpu) = build_conv_nnet2_classif(use_gpu=False, isize=isize, ksize=ksize, n_batch=bsize, verbose=verbose, version=version, check_isfinite=check_isfinite)\n        utt.seed_rng(seed)\n        (train_gpu, params_gpu, x_shape_gpu, y_shape_gpu, mode_gpu) = build_conv_nnet2_classif(use_gpu=True, isize=isize, ksize=ksize, n_batch=bsize, verbose=verbose, version=version, check_isfinite=check_isfinite)\n        assert (x_shape == x_shape_gpu)\n        assert (y_shape == y_shape_gpu)\n        xval = my_rand(*x_shape)\n        yval = my_rand(*y_shape)\n        lr = theano._asarray(0.01, dtype='float32')\n        time_cpu = 0\n        time_gpu = 0\n        for i in range(n_train):\n            t0 = time.time()\n            rval_cpu = train_cpu(xval, yval, lr)[0]\n            t1 = time.time()\n            time_cpu += (t1 - t0)\n            t0 = time.time()\n            rval_gpu = train_gpu(xval, yval, lr)[0]\n            t1 = time.time()\n            time_gpu += (t1 - t0)\n            if (verbose or (not numpy.allclose(rval_cpu, rval_gpu, rtol=1e-05, atol=float_atol))):\n                print('At batch:', (i + 1))\n                print('CPU:', rval_cpu)\n                print('GPU:', rval_gpu)\n                print('abs diff:', numpy.absolute((rval_gpu - rval_cpu)))\n                print('rel diff:', numpy.absolute(((rval_gpu - rval_cpu) / rval_gpu)))\n            if (not ignore_error):\n                utt.assert_allclose(rval_cpu, rval_gpu, rtol=1e-05, atol=float_atol)\n            if (i < (n_train - 1)):\n                for (cpu_p, gpu_p) in zip(params_cpu, params_gpu):\n                    cpu_p.set_value(gpu_p.get_value(borrow=False), borrow=True)\n    finally:\n        theano.tensor.basic.float32_atol = orig_float32_atol\n    if pickle:\n        if isinstance(cpu_mode, theano.compile.ProfileMode):\n            import pickle\n            print('BEGIN CPU profile mode dump')\n            print(pickle.dumps(cpu_mode))\n            print('END CPU profile mode dump')\n        if isinstance(gpu_mode, theano.compile.ProfileMode):\n            import pickle\n            print('BEGIN GPU profile mode dump')\n            print(pickle.dumps(gpu_mode))\n            print('END GPU profile mode dump')\n", "label": 1}
{"function": "\n\ndef properties(obj, type=None, set=None):\n    '\\n    List properties for given btrfs object. The object can be path of BTRFS device,\\n    mount point, or any directories/files inside the BTRFS filesystem.\\n\\n    General options:\\n\\n    * **type**: Possible types are s[ubvol], f[ilesystem], i[node] and d[evice].\\n    * **force**: Force overwrite existing filesystem on the disk\\n    * **set**: <key=value,key1=value1...> Options for a filesystem properties.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt \\'*\\' btrfs.properties /mountpoint\\n        salt \\'*\\' btrfs.properties /dev/sda1 type=subvol set=\\'ro=false,label=\"My Storage\"\\'\\n    '\n    if (type and (type not in ['s', 'subvol', 'f', 'filesystem', 'i', 'inode', 'd', 'device'])):\n        raise CommandExecutionError('Unknown property type: \"{0}\" specified'.format(type))\n    cmd = ['btrfs']\n    cmd.append('property')\n    cmd.append(((set and 'set') or 'list'))\n    if type:\n        cmd.append('-t{0}'.format(type))\n    cmd.append(obj)\n    if set:\n        try:\n            for (key, value) in [[item.strip() for item in keyset.split('=')] for keyset in set.split(',')]:\n                cmd.append(key)\n                cmd.append(value)\n        except Exception as ex:\n            raise CommandExecutionError(ex)\n    out = __salt__['cmd.run_all'](' '.join(cmd))\n    salt.utils.fsutils._verify_run(out)\n    if (not set):\n        ret = {\n            \n        }\n        for (prop, descr) in six.iteritems(_parse_proplist(out['stdout'])):\n            ret[prop] = {\n                'description': descr,\n            }\n            value = __salt__['cmd.run_all']('btrfs property get {0} {1}'.format(obj, prop))['stdout']\n            ret[prop]['value'] = ((value and value.split('=')[(- 1)]) or 'N/A')\n        return ret\n", "label": 1}
{"function": "\n\ndef _handle(self, data):\n    self._buf_in += data\n    if self._handling:\n        return\n    self._handling = True\n    while True:\n        (before, sep, after) = self._buf_in.partition(b'\\n')\n        if (not sep):\n            break\n        try:\n            before = before.decode('utf-8', 'ignore')\n            data = json.loads(before)\n        except Exception as e:\n            msg.error('Unable to parse json: ', str_e(e))\n            msg.error('Data: ', before)\n            self._buf_in = after\n            continue\n        name = data.get('name')\n        self._buf_in = after\n        try:\n            msg.debug(('got data ' + (name or 'no name')))\n            self.emit('data', name, data)\n        except Exception as e:\n            api.send_error(('Error handling %s event.' % name), str_e(e))\n            if (name == 'room_info'):\n                editor.error_message(('Error joining workspace: %s' % str_e(e)))\n                self.stop()\n    self._handling = False\n", "label": 0}
{"function": "\n\ndef test_docopt_parser_with_tabs():\n    help_string = 'Some Tool\\n\\nUsage: tools [-t] [-i <input>...] <cmd>\\n\\nInputs:\\n\\t-i, --input <input>...\\tThe input\\n\\nOptions:\\n\\t-t\\tSome boolean\\n\\t<cmd>\\tThe command\\n    '\n    opts = Options.from_docopt(help_string)\n    assert (len(opts) == 3)\n    assert (opts['input'] is not None)\n    assert (opts['input'].nargs == '*')\n    assert (not opts['input'].required)\n    assert (opts['t'] is not None)\n    assert (opts['t'].nargs == 0)\n    assert (not opts['t'].required)\n    assert (opts['cmd'] is not None)\n    assert (opts['cmd'].nargs == 1)\n    assert opts['cmd'].required\n", "label": 1}
{"function": "\n\ndef __init__(self, RGB1, RGB2, numColors=33.0, divide=255.0, method='moreland', filename=''):\n    self.numColors = numColors\n    assert (np.mod(numColors, 2) == 1), 'For diverging colormaps odd numbers of colors are desireable!'\n    knownMethods = ['moreland', 'lab']\n    assert (method in knownMethods), 'Unknown method was specified!'\n    if (method == knownMethods[0]):\n        self.colorMap = self.generateColorMap(RGB1, RGB2, divide)\n    elif (method == knownMethods[1]):\n        self.colorMap = self.generateColorMapLab(RGB1, RGB2, divide)\n    if (filename == ''):\n        for c in self.colorMap:\n            pass\n    else:\n        with open(filename, 'w') as f:\n            for c in self.colorMap:\n                f.write('{0}, {1}, {2}\\n'.format(c[0], c[1], c[2]))\n", "label": 0}
{"function": "\n\n@app.route('/notes/<hostname>', methods=['GET', 'POST'])\n@login_required\ndef list_notes(hostname):\n    \"Retrieve a list of notes associated with a host. Or given\\n      {'user': 'username', 'note': 'some message'} post a note.\"\n    if (request.method == 'GET'):\n        try:\n            limit = request.args.get('limit', 50, type=int)\n        except ValueError:\n            abort(400)\n        notes = list(r.table('notes').filter({\n            'hostname': hostname,\n        }).order_by(r.desc('ts')).limit(limit).run(rdb.conn))\n        if notes:\n            return jsonify({\n                'notes': sorted(notes, key=(lambda k: k['ts'])),\n            })\n        else:\n            abort(404)\n    elif (request.method == 'POST'):\n        if (not request.json):\n            abort(400)\n        if ((not request.json.get('user')) or (not request.json.get('note'))):\n            abort(400)\n        if (not r.table('hosts').get_all(hostname, index='hostname').run(rdb.conn)):\n            abort(404)\n        alerting = [x['check'] for x in r.table('checks').filter({\n            'h stname': hostname,\n            'status': False,\n        }).run(rdb.conn)]\n        q = r.table('notes').insert({\n            'hostname': hostname,\n            'user': request.json.get('user'),\n            'note': request.json.get('note'),\n            'ts': time(),\n            'alerting': alerting,\n        }).run(rdb.conn)\n        if (q['inserted'] == 1):\n            return jsonify({\n                'success': True,\n            })\n        else:\n            logger.error(q)\n            abort(500)\n    else:\n        abort(400)\n", "label": 1}
{"function": "\n\ndef test_values_list_form_has_metadata(self):\n    'Test default results form has metadata.'\n    searcher = list(self.get_s().query(foo='bar').values_list('id'))\n    assert hasattr(searcher[0], '_id')\n    assert hasattr(searcher[0].es_meta, 'id')\n    assert hasattr(searcher[0].es_meta, 'score')\n    assert hasattr(searcher[0].es_meta, 'source')\n    assert hasattr(searcher[0].es_meta, 'type')\n    assert hasattr(searcher[0].es_meta, 'explanation')\n    assert hasattr(searcher[0].es_meta, 'highlight')\n", "label": 0}
{"function": "\n\ndef forms_form_browse_entries(request, slug, entries=None):\n    try:\n        form = FormsForm.objects.select_related(depth=1).get(slug=slug)\n    except:\n        return None\n    extra_context = {\n        \n    }\n    kwargs = {\n        \n    }\n    sort = ''\n    if entries:\n        kwargs['entries'] = entries\n    extra_context['filters'] = {\n        \n    }\n    extra_context['field_filters'] = []\n    form_fields = form.fields.select_related().all().order_by('label')\n    form.fields_reference = {\n        \n    }\n    hidden = request.GET.get('hidden', '')\n    if hidden:\n        hidden = hidden.split(',')\n    else:\n        hidden = []\n    for f in form_fields:\n        fl = request.GET.get(f.slug, None)\n        if fl:\n            kwargs[f.slug] = smart_str(fl)\n        if f.for_sort:\n            sort = f.slug\n        if (not f.visible_in_list):\n            hidden.append(f.slug)\n    extra_context['params'] = urllib.urlencode(kwargs)\n    kwargs['f'] = request.GET.get('f', '')\n    kwargs['q'] = request.GET.get('q', '')\n    kwargs['o'] = request.GET.get('o', sort)\n    kwargs['ot'] = request.GET.get('ot', 'asc')\n    kwargs['where'] = request.GET.get('where', '')\n    extra_context['parents'] = []\n    if kwargs['where']:\n        try:\n            bte = kwargs['where'].split(':')\n            extra_context['where_entry_id'] = entry_id = bte[1]\n            bte = bte[0].split('.')\n            extra_context['where_form_slug'] = form_slug = bte[0]\n            extra_context['where_form_field'] = form_field = bte[1]\n            while entry_id:\n                e = FormsFormEntry.objects.select_related().get(pk=entry_id, form__slug=form_slug)\n                (c, r) = forms_get_entry(e)\n                extra_context['parents'].append({\n                    'row': r[form_field],\n                    'entry': e,\n                })\n                hidden.append(r[form_field]['slug'])\n                if (not e.form.bind_to_entry):\n                    break\n                bte = e.form.bind_to_entry.split('.')\n                form_slug = bte[0]\n                form_field = bte[1]\n                entry_id = e.entry.pk\n        except Exception as e:\n            extra_context['parents'] = []\n    extra_context['parents'].reverse()\n    kwargs['hidden'] = ','.join(hidden)\n    (columns, entries) = forms_get_entries(form, **kwargs)\n    for f in form_fields:\n        if (('.' in f.reference) and f.is_a(*fields.CHOICES)):\n            fl = request.GET.get(f.slug, None)\n            choices = [('', ('--- %s ---' % f.label))]\n            for e in form.fields_reference[f.pk]:\n                choices.append((e[0], e[1]))\n            s = forms.Select(choices=choices)\n            extra_context['filters'][f.label] = s.render(f.slug, fl, None)\n        else:\n            extra_context['field_filters'].append([f.label, f.slug])\n            if fl:\n                kwargs[f.slug] = fl\n    paginator = Paginator(entries, 25)\n    page = int(request.GET.get('page', 1))\n    try:\n        rows = paginator.page(page)\n    except PageNotAnInteger:\n        rows = paginator.page(1)\n    except EmptyPage:\n        rows = paginator.page(paginator.num_pages)\n    extra_context['q'] = kwargs['q']\n    extra_context['f'] = kwargs['f']\n    extra_context['column_filter'] = kwargs['o']\n    extra_context['column_order'] = kwargs['ot']\n    extra_context['zorna_title_page'] = _('Forms')\n    extra_context['form'] = form\n    extra_context['columns'] = columns\n    extra_context['entries'] = entries\n    extra_context['rows'] = rows\n    extra_context['page'] = page\n    extra_context['paginator'] = paginator\n    extra_context['where'] = kwargs['where']\n    try:\n        r = form.bind_to_entry.split('.')\n        extra_context['bind_entry_slug'] = r[0]\n        extra_context['bind_entry_field'] = r[1]\n    except:\n        pass\n    extra_context['where'] = kwargs['where']\n    return extra_context\n", "label": 1}
{"function": "\n\ndef collect(self):\n    '\\n        Perform the bulk of the work of collectstatic.\\n\\n        Split off from handle_noargs() to facilitate testing.\\n        '\n    if self.symlink:\n        if (sys.platform == 'win32'):\n            raise CommandError(('Symlinking is not supported by this platform (%s).' % sys.platform))\n        if (not self.local):\n            raise CommandError(\"Can't symlink to a remote destination.\")\n    if self.clear:\n        self.clear_dir('')\n    handler = self._get_handler()\n    do_post_process = (self.post_process and hasattr(self.storage, 'post_process'))\n    found_files = SortedDict()\n    for finder in finders.get_finders():\n        for (path, storage) in finder.list(self.ignore_patterns):\n            if getattr(storage, 'prefix', None):\n                prefixed_path = os.path.join(storage.prefix, path)\n            else:\n                prefixed_path = path\n            if (prefixed_path not in found_files):\n                found_files[prefixed_path] = (storage, path)\n                handler(path, prefixed_path, storage)\n                if (self.progressive_post_process and do_post_process):\n                    try:\n                        self._post_process({\n                            prefixed_path: (storage, path),\n                        }, self.dry_run)\n                    except ValueError as e:\n                        message = ('%s current storage requires all files to have been collected first. Try  ecstatic.storage.CachedStaticFilesStorage' % e)\n                        raise ValueError(message)\n    if ((not self.progressive_post_process) and do_post_process):\n        self._post_process(found_files, self.dry_run)\n    return {\n        'modified': (self.copied_files + self.symlinked_files),\n        'unmodified': self.unmodified_files,\n        'post_processed': self.post_processed_files,\n    }\n", "label": 1}
{"function": "\n\ndef train(self, messageList):\n    globalClassifier = self._globalClassifier\n    localClassifier = self._localClassifier\n    trainLog = ''\n    nFolds = 5\n    msgList = map((lambda x: x[0]), messageList)\n    y = np.array(map((lambda x: (1 if (x[1] == 'pos') else 0)), messageList))\n    globalDFList = []\n    for msg in msgList:\n        v = globalClassifier.predictScore(msg)\n        globalDFList.append(v['pos'])\n    tt = tic()\n    if (sum(y) < 2):\n        trainLog += 'There are less than 2 positive data points. setting maxAlpha=0\\n'\n        maxAlpha = 0\n    else:\n        localDFList = ([float('nan')] * len(messageList))\n        folds = StratifiedKFold(y, nFolds)\n        for (trainIdx, testIdx) in folds:\n            train = [messageList[i] for i in trainIdx]\n            test = [messageList[i] for i in testIdx]\n            tmpClassifier = copy.deepcopy(localClassifier)\n            tmpClassifier.train(train)\n            curDFList = []\n            for x in test:\n                v = tmpClassifier.predictScore(x[0])\n                curDFList.append(v['pos'])\n            for i in range(len(test)):\n                localDFList[testIdx[i]] = curDFList[i]\n        assert (countTruth((lambda x: np.isnan(x)), localDFList) == 0)\n        f1List = []\n        alphaList = np.linspace(0.0, 1.0, 101)\n        for alpha in alphaList:\n            combinedDFList = [(((1 - alpha) * globalDFList[i]) + (alpha * localDFList[i])) for i in range(len(globalDFList))]\n            predy = [(1 if (v >= 0.0) else 0) for v in combinedDFList]\n            f1 = metrics.f1_score(y, predy)\n            f1List.append(f1)\n        trainLog += 'Finding alpha maximizing F1...\\n'\n        maxF1 = np.max(f1List)\n        maxIdx = np.where((np.array(f1List) == maxF1))\n        maxIdx = maxIdx[0]\n        assert (len(maxIdx) != 0)\n        candidateMaxAlpha = [alphaList[i] for i in maxIdx]\n        maxAlpha = np.median(candidateMaxAlpha)\n        trainLog += ('max(f1List) = %.3f\\n' % maxF1)\n        trainLog += ('argmax_alpha = \\n%s\\n' % str(np.array([alphaList[i] for i in maxIdx])))\n        trainLog += ('chosen alpha by median = %.3f\\n' % maxAlpha)\n        localClassifier.train(binaryLabeledMessageList)\n    elapsed = toc(tt)\n    trainLog += ('Time taken for training: %.3f (sec)\\n' % elapsed)\n    self._localClassifier = localClassifier\n    self._alpha = maxAlpha\n    self._trainLog = trainLog\n    yDeci = [self.predictScore(msg)['pos'] for msg in messageList]\n    yy = [(1 if (v == 1) else (- 1)) for v in y]\n    [A, B] = platt.SigmoidTrain(yDeci, yy)\n    plattModel = [A, B]\n    self._plattModel = plattModel\n    pass\n", "label": 1}
{"function": "\n\ndef reverse_apicontroller_action(url, status, response):\n    '\\n    Make an API call look like a direct action call by reversing the\\n    exception -> HTTP response translation that ApiController.action does\\n    '\n    try:\n        parsed = json.loads(response)\n        if parsed.get('success'):\n            return parsed['result']\n        if hasattr(parsed, 'get'):\n            err = parsed.get('error', {\n                \n            })\n        else:\n            err = {\n                \n            }\n    except (AttributeError, ValueError):\n        err = {\n            \n        }\n    if (not isinstance(err, dict)):\n        raise ServerIncompatibleError(repr([url, status, response]))\n    etype = err.get('__type')\n    emessage = err.get('message', '').split(': ', 1)[(- 1)]\n    if (etype == 'Search Query Error'):\n        raise SearchQueryError(emessage)\n    elif (etype == 'Search Error'):\n        raise SearchError(emessage)\n    elif (etype == 'Search Index Error'):\n        raise SearchIndexError(emessage)\n    elif (etype == 'Validation Error'):\n        raise ValidationError(err)\n    elif (etype == 'Not Found Error'):\n        raise NotFound(emessage)\n    elif (etype == 'Authorization Error'):\n        raise NotAuthorized(err)\n    raise CKANAPIError(repr([url, status, response]))\n", "label": 1}
{"function": "\n\ndef install_pyfile(self, node):\n    path = self.bld.get_install_path(((self.install_path + os.sep) + node.name), self.env)\n    self.bld.install_files(self.install_path, [node], self.env, self.chmod, postpone=False)\n    if (self.bld.is_install < 0):\n        info('* removing byte compiled python files')\n        for x in 'co':\n            try:\n                os.remove((path + x))\n            except OSError:\n                pass\n    if (self.bld.is_install > 0):\n        if (self.env['PYC'] or self.env['PYO']):\n            info(('* byte compiling %r' % path))\n        if self.env['PYC']:\n            program = \"\\nimport sys, py_compile\\nfor pyfile in sys.argv[1:]:\\n\\tpy_compile.compile(pyfile, pyfile + 'c')\\n\"\n            argv = [self.env['PYTHON'], '-c', program, path]\n            ret = Utils.pproc.Popen(argv).wait()\n            if ret:\n                raise Utils.WafError(('bytecode compilation failed %r' % path))\n        if self.env['PYO']:\n            program = \"\\nimport sys, py_compile\\nfor pyfile in sys.argv[1:]:\\n\\tpy_compile.compile(pyfile, pyfile + 'o')\\n\"\n            argv = [self.env['PYTHON'], self.env['PYFLAGS_OPT'], '-c', program, path]\n            ret = Utils.pproc.Popen(argv).wait()\n            if ret:\n                raise Utils.WafError(('bytecode compilation failed %r' % path))\n", "label": 1}
{"function": "\n\ndef wait_for_services(self, instances, callback=None):\n    assert self.connected\n    pending = set(instances)\n    ready = []\n    failed = []\n    while len(pending):\n        toRemove = []\n        for i in pending:\n            i.update()\n            if ('status' not in i.tags):\n                continue\n            if i.tags['status'].endswith('failed'):\n                toRemove.append(i)\n                failed.append(i)\n            elif (i.tags['status'] == 'ready'):\n                toRemove.append(i)\n                ready.append(i)\n        for i in toRemove:\n            pending.remove(i)\n        status = {\n            \n        }\n        for i in itertools.chain(pending, ready, failed):\n            if ('status' not in i.tags):\n                status_name = 'installing dependencies'\n            else:\n                status_name = i.tags['status']\n            if (status_name not in status):\n                status[status_name] = []\n            status[status_name].append(i)\n        if callback:\n            callback(status)\n        if len(pending):\n            time.sleep(1)\n    return (len(failed) == 0)\n", "label": 1}
{"function": "\n\ndef test_reduce_poly_inequalities_complex_relational():\n    assert (reduce_rational_inequalities([[Eq((x ** 2), 0)]], x, relational=True) == Eq(x, 0))\n    assert (reduce_rational_inequalities([[Le((x ** 2), 0)]], x, relational=True) == Eq(x, 0))\n    assert (reduce_rational_inequalities([[Lt((x ** 2), 0)]], x, relational=True) == False)\n    assert (reduce_rational_inequalities([[Ge((x ** 2), 0)]], x, relational=True) == And(Lt((- oo), x), Lt(x, oo)))\n    assert (reduce_rational_inequalities([[Gt((x ** 2), 0)]], x, relational=True) == And(Or(And(Lt((- oo), x), Lt(x, 0)), And(Lt(0, x), Lt(x, oo)))))\n    assert (reduce_rational_inequalities([[Ne((x ** 2), 0)]], x, relational=True) == And(Or(And(Lt((- oo), x), Lt(x, 0)), And(Lt(0, x), Lt(x, oo)))))\n    for one in (S(1), S(1.0)):\n        inf = (one * oo)\n        assert (reduce_rational_inequalities([[Eq((x ** 2), one)]], x, relational=True) == Or(Eq(x, (- one)), Eq(x, one)))\n        assert (reduce_rational_inequalities([[Le((x ** 2), one)]], x, relational=True) == And(And(Le((- one), x), Le(x, one))))\n        assert (reduce_rational_inequalities([[Lt((x ** 2), one)]], x, relational=True) == And(And(Lt((- one), x), Lt(x, one))))\n        assert (reduce_rational_inequalities([[Ge((x ** 2), one)]], x, relational=True) == And(Or(And(Le(one, x), Lt(x, inf)), And(Le(x, (- one)), Lt((- inf), x)))))\n        assert (reduce_rational_inequalities([[Gt((x ** 2), one)]], x, relational=True) == And(Or(And(Lt((- inf), x), Lt(x, (- one))), And(Lt(one, x), Lt(x, inf)))))\n        assert (reduce_rational_inequalities([[Ne((x ** 2), one)]], x, relational=True) == Or(And(Lt((- inf), x), Lt(x, (- one))), And(Lt((- one), x), Lt(x, one)), And(Lt(one, x), Lt(x, inf))))\n", "label": 1}
{"function": "\n\ndef raise_cannot_open(path):\n    \"\\n    Raise an exception saying we can't open `path`.\\n\\n    Parameters\\n    ----------\\n    path : str\\n        The path we cannot open\\n    \"\n    pieces = path.split('/')\n    for i in xrange(1, (len(pieces) + 1)):\n        so_far = '/'.join(pieces[0:i])\n        if (not os.path.exists(so_far)):\n            if (i == 1):\n                if (so_far == ''):\n                    continue\n                reraise_as(IOError((((('Cannot open ' + path) + ' (') + so_far) + ' does not exist)')))\n            parent = '/'.join(pieces[0:(i - 1)])\n            bad = pieces[(i - 1)]\n            if (not os.path.isdir(parent)):\n                reraise_as(IOError((((('Cannot open ' + path) + ' because ') + parent) + ' is not a directory.')))\n            candidates = os.listdir(parent)\n            if (len(candidates) == 0):\n                reraise_as(IOError((((('Cannot open ' + path) + ' because ') + parent) + ' is empty.')))\n            if (len(candidates) > 100):\n                reraise_as(IOError((((('Cannot open ' + path) + ' but can open ') + parent) + '.')))\n            if os.path.islink(path):\n                reraise_as(IOError((path + ' appears to be a symlink to a non-existent file')))\n            reraise_as(IOError((((((((('Cannot open ' + path) + ' but can open ') + parent) + '. Did you mean ') + match(bad, candidates)) + ' instead of ') + bad) + '?')))\n    assert False\n", "label": 0}
{"function": "\n\n@public\ndef lcm_list(seq, *gens, **args):\n    '\\n    Compute LCM of a list of polynomials.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy import lcm_list\\n    >>> from sympy.abc import x\\n\\n    >>> lcm_list([x**3 - 1, x**2 - 1, x**2 - 3*x + 2])\\n    x**5 - x**4 - 2*x**3 - x**2 + x + 2\\n\\n    '\n    seq = sympify(seq)\n\n    def try_non_polynomial_lcm(seq):\n        if ((not gens) and (not args)):\n            (domain, numbers) = construct_domain(seq)\n            if (not numbers):\n                return domain.one\n            elif domain.is_Numerical:\n                (result, numbers) = (numbers[0], numbers[1:])\n                for number in numbers:\n                    result = domain.lcm(result, number)\n                return domain.to_sympy(result)\n        return None\n    result = try_non_polynomial_lcm(seq)\n    if (result is not None):\n        return result\n    options.allowed_flags(args, ['polys'])\n    try:\n        (polys, opt) = parallel_poly_from_expr(seq, *gens, **args)\n    except PolificationFailed as exc:\n        result = try_non_polynomial_lcm(exc.exprs)\n        if (result is not None):\n            return result\n        else:\n            raise ComputationFailed('lcm_list', len(seq), exc)\n    if (not polys):\n        if (not opt.polys):\n            return S.One\n        else:\n            return Poly(1, opt=opt)\n    (result, polys) = (polys[0], polys[1:])\n    for poly in polys:\n        result = result.lcm(poly)\n    if (not opt.polys):\n        return result.as_expr()\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef create(self, ddlFile):\n    startedAt = time.time()\n    self.showStatus('Dropping prior tables')\n    for table in self.tablesInDB():\n        result = self.execute(('DROP TABLE %s' % self.dbTableName(table)), close=False, commit=False, fetch=False, action='dropping table')\n    self.showStatus('Dropping prior sequences')\n    for sequence in self.sequencesInDB():\n        result = self.execute(('DROP SEQUENCE %s' % sequence), close=False, commit=False, fetch=False, action='dropping sequence')\n    self.modelXbrl.profileStat(_('XbrlPublicDB: drop prior tables'), (time.time() - startedAt))\n    startedAt = time.time()\n    with io.open(((os.path.dirname(__file__) + os.sep) + ddlFile), 'rt', encoding='utf-8') as fh:\n        sql = fh.read().replace('%', '%%')\n    sqlstatements = []\n\n    def findstatements(start, end, laststatement):\n        for line in sql[start:end].split('\\n'):\n            (stmt, comment1, comment2) = line.partition('--')\n            laststatement += (stmt + '\\n')\n            if (';' in stmt):\n                sqlstatements.append(laststatement)\n                laststatement = ''\n        return laststatement\n    stmt = ''\n    i = 0\n    patternDollarEsc = re.compile('([$]\\\\w*[$])', (re.DOTALL + re.MULTILINE))\n    while (i < len(sql)):\n        match = patternDollarEsc.search(sql, i)\n        if (not match):\n            stmt = findstatements(i, len(sql), stmt)\n            sqlstatements.append(stmt)\n            break\n        dollarescape = match.group()\n        j = match.end()\n        stmt = findstatements(i, j, stmt)\n        i = sql.find(dollarescape, j)\n        if (i > j):\n            if (self.product == 'mysql'):\n                stmt = sql[j:i]\n                i += len(dollarescape)\n            else:\n                i += len(dollarescape)\n                stmt += sql[j:i]\n            sqlstatements.append(stmt)\n            stmt = ''\n    action = 'executing ddl in {}'.format(os.path.basename(ddlFile))\n    for (i, sql) in enumerate(sqlstatements):\n        if any(((cmd in sql) for cmd in ('CREATE TABLE', 'CREATE SEQUENCE', 'INSERT INTO', 'CREATE TYPE', 'CREATE FUNCTION', 'SET', 'CREATE INDEX', 'CREATE UNIQUE INDEX'))):\n            (statusMsg, sep, rest) = sql.strip().partition('\\n')\n            self.showStatus(statusMsg[0:50])\n            result = self.execute(sql, close=False, commit=False, fetch=False, action=action)\n            if TRACESQLFILE:\n                with io.open(TRACESQLFILE, 'a', encoding='utf-8') as fh:\n                    fh.write('\\n\\n>>> ddl {0}: \\n{1} \\n\\n>>> result: \\n{2}\\n'.format(i, sql, result))\n                    fh.write(sql)\n    self.showStatus('')\n    self.conn.commit()\n    self.modelXbrl.profileStat(_('XbrlPublicDB: create tables'), (time.time() - startedAt))\n    self.closeCursor()\n", "label": 1}
{"function": "\n\ndef audit_location_generator(devices, datadir, suffix='', mount_check=True, logger=None):\n    '\\n    Given a devices path and a data directory, yield (path, device,\\n    partition) for all files in that directory\\n\\n    :param devices: parent directory of the devices to be audited\\n    :param datadir: a directory located under self.devices. This should be\\n                    one of the DATADIR constants defined in the account,\\n                    container, and object servers.\\n    :param suffix: path name suffix required for all names returned\\n    :param mount_check: Flag to check if a mount check should be performed\\n                    on devices\\n    :param logger: a logger object\\n    '\n    device_dir = listdir(devices)\n    shuffle(device_dir)\n    for device in device_dir:\n        if (mount_check and (not ismount(os.path.join(devices, device)))):\n            if logger:\n                logger.warning(_('Skipping %s as it is not mounted'), device)\n            continue\n        datadir_path = os.path.join(devices, device, datadir)\n        try:\n            partitions = listdir(datadir_path)\n        except OSError as e:\n            if logger:\n                logger.warning('Skipping %s because %s', datadir_path, e)\n            continue\n        for partition in partitions:\n            part_path = os.path.join(datadir_path, partition)\n            try:\n                suffixes = listdir(part_path)\n            except OSError as e:\n                if (e.errno != errno.ENOTDIR):\n                    raise\n                continue\n            for asuffix in suffixes:\n                suff_path = os.path.join(part_path, asuffix)\n                try:\n                    hashes = listdir(suff_path)\n                except OSError as e:\n                    if (e.errno != errno.ENOTDIR):\n                        raise\n                    continue\n                for hsh in hashes:\n                    hash_path = os.path.join(suff_path, hsh)\n                    try:\n                        files = sorted(listdir(hash_path), reverse=True)\n                    except OSError as e:\n                        if (e.errno != errno.ENOTDIR):\n                            raise\n                        continue\n                    for fname in files:\n                        if (suffix and (not fname.endswith(suffix))):\n                            continue\n                        path = os.path.join(hash_path, fname)\n                        (yield (path, device, partition))\n", "label": 1}
{"function": "\n\ndef handle_request(self, listener_name, req, sock, addr):\n    request_start = datetime.now()\n    environ = {\n        \n    }\n    resp = None\n    try:\n        self.cfg.pre_request(self, req)\n        (resp, environ) = wsgi.create(req, sock, addr, listener_name, self.cfg)\n        environ['wsgi.multithread'] = True\n        self.nr += 1\n        if (self.alive and (self.nr >= self.max_requests)):\n            self.log.info('Autorestarting worker after current request.')\n            resp.force_close()\n            self.alive = False\n        if (not self.cfg.keepalive):\n            resp.force_close()\n        respiter = self.wsgi(environ, resp.start_response)\n        if (respiter == ALREADY_HANDLED):\n            return False\n        try:\n            if isinstance(respiter, environ['wsgi.file_wrapper']):\n                resp.write_file(respiter)\n            else:\n                for item in respiter:\n                    resp.write(item)\n            resp.close()\n            request_time = (datetime.now() - request_start)\n            self.log.access(resp, req, environ, request_time)\n        finally:\n            if hasattr(respiter, 'close'):\n                respiter.close()\n        if resp.should_close():\n            raise StopIteration()\n    except StopIteration:\n        raise\n    except EnvironmentError:\n        six.reraise(*sys.exc_info())\n    except Exception:\n        if (resp and resp.headers_sent):\n            self.log.exception('Error handling request')\n            try:\n                sock.shutdown(socket.SHUT_RDWR)\n                sock.close()\n            except EnvironmentError:\n                pass\n            raise StopIteration()\n        raise\n    finally:\n        try:\n            self.cfg.post_request(self, req, environ, resp)\n        except Exception:\n            self.log.exception('Exception in post_request hook')\n    return True\n", "label": 1}
{"function": "\n\ndef overlay_image(dstarr, dst_x, dst_y, srcarr, dst_order='RGBA', src_order='RGBA', alpha=1.0, copy=False, fill=True, flipy=False):\n    (dst_ht, dst_wd, dst_dp) = dstarr.shape\n    (src_ht, src_wd, src_dp) = srcarr.shape\n    if flipy:\n        srcarr = numpy.flipud(srcarr)\n    if (dst_y < 0):\n        dy = abs(dst_y)\n        srcarr = srcarr[dy:, :, :]\n        src_ht -= dy\n        dst_y = 0\n    if (dst_x < 0):\n        dx = abs(dst_x)\n        srcarr = srcarr[:, dx:, :]\n        src_wd -= dx\n        dst_x = 0\n    if ((src_wd <= 0) or (src_ht <= 0)):\n        return dstarr\n    ex = ((dst_y + src_ht) - dst_ht)\n    if (ex > 0):\n        srcarr = srcarr[:dst_ht, :, :]\n        src_ht -= ex\n    ex = ((dst_x + src_wd) - dst_wd)\n    if (ex > 0):\n        srcarr = srcarr[:, :dst_wd, :]\n        src_wd -= ex\n    if copy:\n        dstarr = numpy.copy(dstarr, order='C')\n    da_idx = (- 1)\n    if ('A' in dst_order):\n        da_idx = dst_order.index('A')\n    assert (da_idx == 3), ValueError('Alpha channel not in expected position in dstarr')\n    if (fill and (da_idx >= 0)):\n        dstarr[dst_y:(dst_y + src_ht), dst_x:(dst_x + src_wd), da_idx] = 255\n    if (src_dp > 3):\n        sa_idx = src_order.index('A')\n        alpha = (srcarr[0:src_ht, 0:src_wd, sa_idx] / 255.0)\n        alpha = numpy.dstack((alpha, alpha, alpha))\n    get_order = dst_order\n    if (('A' in dst_order) and (not ('A' in src_order))):\n        get_order = dst_order.replace('A', '')\n    if (get_order != src_order):\n        srcarr = reorder_image(get_order, srcarr, src_order)\n    a_arr = (alpha * srcarr[0:src_ht, 0:src_wd, 0:3]).astype(numpy.uint8)\n    b_arr = ((1.0 - alpha) * dstarr[dst_y:(dst_y + src_ht), dst_x:(dst_x + src_wd), 0:3]).astype(numpy.uint8)\n    dstarr[dst_y:(dst_y + src_ht), dst_x:(dst_x + src_wd), 0:3] = (a_arr[0:src_ht, 0:src_wd, 0:3] + b_arr[0:src_ht, 0:src_wd, 0:3])\n    return dstarr\n", "label": 1}
{"function": "\n\ndef copytree(src, dst, symlinks=False, ignore=None):\n    'Recursively copy a directory tree using copy2().\\n\\n    The destination directory must not already exist.\\n    If exception(s) occur, an Error is raised with a list of reasons.\\n\\n    If the optional symlinks flag is true, symbolic links in the\\n    source tree result in symbolic links in the destination tree; if\\n    it is false, the contents of the files pointed to by symbolic\\n    links are copied.\\n\\n    The optional ignore argument is a callable. If given, it\\n    is called with the `src` parameter, which is the directory\\n    being visited by copytree(), and `names` which is the list of\\n    `src` contents, as returned by os.listdir():\\n\\n        callable(src, names) -> ignored_names\\n\\n    Since copytree() is called recursively, the callable will be\\n    called once for each directory that is copied. It returns a\\n    list of names relative to the `src` directory that should\\n    not be copied.\\n\\n    XXX Consider this example code rather than the ultimate tool.\\n\\n    '\n    from shutil import copy2, Error, copystat\n    names = os.listdir(src)\n    if (ignore is not None):\n        ignored_names = ignore(src, names)\n    else:\n        ignored_names = set()\n    os.makedirs(dst)\n    errors = []\n    for name in names:\n        if (name in ignored_names):\n            continue\n        srcname = os.path.join(src, name)\n        dstname = os.path.join(dst, name)\n        try:\n            if (symlinks and os.path.islink(srcname)):\n                linkto = os.readlink(srcname)\n                os.symlink(linkto, dstname)\n            elif os.path.isdir(srcname):\n                copytree(srcname, dstname, symlinks, ignore)\n            else:\n                copy2(srcname, dstname)\n        except Error as err:\n            errors.extend(err.args[0])\n        except EnvironmentError as why:\n            errors.append((srcname, dstname, str(why)))\n    try:\n        copystat(src, dst)\n    except OSError as why:\n        if ((WindowsError is not None) and isinstance(why, WindowsError)):\n            pass\n        else:\n            errors.extend((src, dst, str(why)))\n    if errors:\n        raise Error(errors)\n", "label": 1}
{"function": "\n\ndef load_pkcs12(buffer, passphrase=None):\n    '\\n    Load a PKCS12 object from a buffer\\n\\n    :param buffer: The buffer the certificate is stored in\\n    :param passphrase: (Optional) The password to decrypt the PKCS12 lump\\n    :returns: The PKCS12 object\\n    '\n    passphrase = _text_to_bytes_and_warn('passphrase', passphrase)\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if (not passphrase):\n        passphrase = _ffi.NULL\n    p12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\n    if (p12 == _ffi.NULL):\n        _raise_current_error()\n    p12 = _ffi.gc(p12, _lib.PKCS12_free)\n    pkey = _ffi.new('EVP_PKEY**')\n    cert = _ffi.new('X509**')\n    cacerts = _ffi.new('Cryptography_STACK_OF_X509**')\n    parse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\n    if (not parse_result):\n        _raise_current_error()\n    cacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\n    try:\n        _raise_current_error()\n    except Error:\n        pass\n    if (pkey[0] == _ffi.NULL):\n        pykey = None\n    else:\n        pykey = PKey.__new__(PKey)\n        pykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\n    if (cert[0] == _ffi.NULL):\n        pycert = None\n        friendlyname = None\n    else:\n        pycert = X509.__new__(X509)\n        pycert._x509 = _ffi.gc(cert[0], _lib.X509_free)\n        friendlyname_length = _ffi.new('int*')\n        friendlyname_buffer = _lib.X509_alias_get0(cert[0], friendlyname_length)\n        friendlyname = _ffi.buffer(friendlyname_buffer, friendlyname_length[0])[:]\n        if (friendlyname_buffer == _ffi.NULL):\n            friendlyname = None\n    pycacerts = []\n    for i in range(_lib.sk_X509_num(cacerts)):\n        pycacert = X509.__new__(X509)\n        pycacert._x509 = _lib.sk_X509_value(cacerts, i)\n        pycacerts.append(pycacert)\n    if (not pycacerts):\n        pycacerts = None\n    pkcs12 = PKCS12.__new__(PKCS12)\n    pkcs12._pkey = pykey\n    pkcs12._cert = pycert\n    pkcs12._cacerts = pycacerts\n    pkcs12._friendlyname = friendlyname\n    return pkcs12\n", "label": 1}
{"function": "\n\ndef _run(self):\n    args = self.args\n\n    def notes(noteArgs, **kwargs):\n        gitArgs = ('notes --ref=%s %s %s' % (noteNamespace, noteArgs, args.commit))\n        return git(gitArgs, **kwargs)\n\n    def add(note):\n        notes(('add -fm \"%s\"' % note))\n    note = notes('show', errorValue='')\n    if args.delete:\n        if (not note):\n            return\n        workitem = args.workitem\n        if workitem:\n            items = note.split(',')\n            try:\n                items.remove(str(workitem))\n            except ValueError:\n                fail(('Workitem %s is not associated with %s' % (workitem, args.commit)))\n            if items:\n                add(','.join(items))\n                return\n        notes('remove')\n    elif args.workitem:\n        if note:\n            note += ','\n        add((note + str(args.workitem)))\n    elif note:\n        print(note)\n", "label": 0}
{"function": "\n\ndef test_metadata_plugin_rpmqa_failure(tmpdir):\n    initial_timestamp = datetime.now()\n    workflow = prepare()\n    workflow.prebuild_results = {\n        CpDockerfilePlugin.key: 'dockerfile-content',\n        DistgitFetchArtefactsPlugin.key: 'artefact1\\nartefact2',\n    }\n    workflow.postbuild_results = {\n        PostBuildRPMqaPlugin.key: RuntimeError(),\n    }\n    workflow.plugins_timestamps = {\n        CpDockerfilePlugin.key: initial_timestamp.isoformat(),\n        DistgitFetchArtefactsPlugin.key: (initial_timestamp + timedelta(seconds=1)).isoformat(),\n        PostBuildRPMqaPlugin.key: (initial_timestamp + timedelta(seconds=3)).isoformat(),\n    }\n    workflow.plugins_durations = {\n        CpDockerfilePlugin.key: 1.01,\n        DistgitFetchArtefactsPlugin.key: 2.02,\n        PostBuildRPMqaPlugin.key: 3.03,\n    }\n    workflow.plugins_errors = {\n        PostBuildRPMqaPlugin.key: 'foo',\n    }\n    runner = ExitPluginsRunner(None, workflow, [{\n        'name': StoreMetadataInOSv3Plugin.key,\n        'args': {\n            'url': 'http://example.com/',\n        },\n    }])\n    output = runner.run()\n    assert (StoreMetadataInOSv3Plugin.key in output)\n    labels = output[StoreMetadataInOSv3Plugin.key]['annotations']\n    assert ('dockerfile' in labels)\n    assert ('artefacts' in labels)\n    assert ('logs' in labels)\n    assert ('rpm-packages' in labels)\n    assert ('repositories' in labels)\n    assert ('commit_id' in labels)\n    assert ('base-image-id' in labels)\n    assert ('base-image-name' in labels)\n    assert ('image-id' in labels)\n    assert (len(labels['rpm-packages']) == 0)\n    assert ('plugins-metadata' in labels)\n    assert ('errors' in labels['plugins-metadata'])\n    assert ('durations' in labels['plugins-metadata'])\n    assert ('timestamps' in labels['plugins-metadata'])\n    plugins_metadata = json.loads(labels['plugins-metadata'])\n    assert ('all_rpm_packages' in plugins_metadata['errors'])\n    assert ('cp_dockerfile' in plugins_metadata['durations'])\n    assert ('distgit_fetch_artefacts' in plugins_metadata['durations'])\n    assert ('all_rpm_packages' in plugins_metadata['durations'])\n", "label": 1}
{"function": "\n\ndef __init__(self, serial_address=None):\n    super().__init__()\n    logger = logging.getLogger('AAG_cloud_sensor')\n    if (len(logger.handlers) == 0):\n        logger.setLevel(logging.DEBUG)\n        LogConsoleHandler = logging.StreamHandler()\n        LogConsoleHandler.setLevel(logging.INFO)\n        LogFormat = logging.Formatter('%(asctime)23s %(levelname)8s: %(message)s')\n        LogConsoleHandler.setFormatter(LogFormat)\n        logger.addHandler(LogConsoleHandler)\n        LogFilePath = os.path.join('/', 'var', 'panoptes', 'logs', 'PanoptesWeather')\n        if (not os.path.exists(LogFilePath)):\n            os.mkdir(LogFilePath)\n        now = dt.utcnow()\n        LogFileName = now.strftime('AAGCloudSensor.log')\n        LogFile = os.path.join(LogFilePath, LogFileName)\n        LogFileHandler = logging.handlers.TimedRotatingFileHandler(LogFile, when='midnight', interval=1, utc=True)\n        LogFileHandler.setLevel(logging.DEBUG)\n        LogFileHandler.setFormatter(LogFormat)\n        logger.addHandler(LogFileHandler)\n    self.logger = logger\n    self.cfg = load_config()['weather']['aag_cloud']\n    if (not serial_address):\n        serial_address = self.cfg.get('serial_port', '/dev/ttyUSB0')\n    if self.logger:\n        self.logger.debug('Using serial address: {}'.format(serial_address))\n    if serial_address:\n        if self.logger:\n            self.logger.info('Connecting to AAG Cloud Sensor')\n        try:\n            self.AAG = serial.Serial(serial_address, 9600, timeout=2)\n            self.logger.info('  Connected to Cloud Sensor on {}'.format(serial_address))\n        except OSError as e:\n            self.logger.error('Unable to connect to AAG Cloud Sensor')\n            self.logger.error('  {}'.format(e.errno))\n            self.logger.error('  {}'.format(e.strerror))\n            self.AAG = None\n        except:\n            self.logger.error('Unable to connect to AAG Cloud Sensor')\n            self.AAG = None\n    else:\n        self.AAG = None\n    self.last_update = None\n    self.safe = None\n    self.ambient_temp = None\n    self.sky_temp = None\n    self.wind_speed = None\n    self.internal_voltage = None\n    self.LDR_resistance = None\n    self.rain_sensor_temp = None\n    self.PWM = None\n    self.errors = None\n    self.switch = None\n    self.safe_dict = None\n    self.hibernate = 0.5\n    if ('heater' in self.cfg):\n        self.heater_cfg = self.cfg['heater']\n    else:\n        self.heater_cfg = {\n            'low_temp': 0,\n            'low_delta': 6,\n            'high_temp': 20,\n            'high_delta': 4,\n            'min_power': 10,\n            'impulse_temp': 10,\n            'impulse_duration': 60,\n            'impulse_cycle': 600,\n        }\n    self.heater_PID = PID(Kp=3.0, Ki=0.02, Kd=200.0, max_age=300, output_limits=[self.heater_cfg['min_power'], 100])\n    self.impulse_heating = None\n    self.impulse_start = None\n    self.commands = {\n        '!A': 'Get internal name',\n        '!B': 'Get firmware version',\n        '!C': 'Get values',\n        '!D': 'Get internal errors',\n        '!E': 'Get rain frequency',\n        '!F': 'Get switch status',\n        '!G': 'Set switch open',\n        '!H': 'Set switch closed',\n        'P\\\\d\\\\d\\\\d\\\\d!': 'Set PWM value',\n        '!Q': 'Get PWM value',\n        '!S': 'Get sky IR temperature',\n        '!T': 'Get sensor temperature',\n        '!z': 'Reset RS232 buffer pointers',\n        '!K': 'Get serial number',\n        'v!': 'Query if anemometer enabled',\n        'V!': 'Get wind speed',\n        'M!': 'Get electrical constants',\n        '!Pxxxx': 'Set PWM value to xxxx',\n    }\n    self.expects = {\n        '!A': '!N\\\\s+(\\\\w+)!',\n        '!B': '!V\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        '!C': '!6\\\\s+([\\\\d\\\\.\\\\-]+)!4\\\\s+([\\\\d\\\\.\\\\-]+)!5\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        '!D': '!E1\\\\s+([\\\\d\\\\.]+)!E2\\\\s+([\\\\d\\\\.]+)!E3\\\\s+([\\\\d\\\\.]+)!E4\\\\s+([\\\\d\\\\.]+)!',\n        '!E': '!R\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        '!F': '!Y\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        'P\\\\d\\\\d\\\\d\\\\d!': '!Q\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        '!Q': '!Q\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        '!S': '!1\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        '!T': '!2\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        '!K': '!K(\\\\d+)\\\\s*\\\\x00!',\n        'v!': '!v\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        'V!': '!w\\\\s+([\\\\d\\\\.\\\\-]+)!',\n        'M!': '!M(.{12})',\n    }\n    self.delays = {\n        '!E': 0.35,\n    }\n    if self.AAG:\n        result = self.query('!A')\n        if result:\n            self.name = result[0].strip()\n            if self.logger:\n                self.logger.info('  Device Name is \"{}\"'.format(self.name))\n        else:\n            self.name = ''\n            if self.logger:\n                self.logger.warning('  Failed to get Device Name')\n            sys.exit(1)\n        result = self.query('!B')\n        if result:\n            self.firmware_version = result[0].strip()\n            if self.logger:\n                self.logger.info('  Firmware Version = {}'.format(self.firmware_version))\n        else:\n            self.firmware_version = ''\n            if self.logger:\n                self.logger.warning('  Failed to get Firmware Version')\n            sys.exit(1)\n        result = self.query('!K')\n        if result:\n            self.serial_number = result[0].strip()\n            if self.logger:\n                self.logger.info('  Serial Number: {}'.format(self.serial_number))\n        else:\n            self.serial_number = ''\n            if self.logger:\n                self.logger.warning('  Failed to get Serial Number')\n            sys.exit(1)\n", "label": 1}
{"function": "\n\ndef get_common_replies(locale=settings.WIKI_DEFAULT_LANGUAGE):\n    'Returns the common replies.\\n\\n    Parses the KB article with the replies puts them in a list of dicts.\\n\\n    The KB article should have the following wiki syntax structure::\\n\\n        =Category 1=\\n\\n        ==Reply 1==\\n        Reply goes here http://example.com/kb-article\\n\\n        ==Reply 2==\\n        Another reply here\\n\\n        =Category 2=\\n        ==Reply 3==\\n        And another reply\\n\\n\\n    Which results in the following HTML::\\n\\n        <h1 id=\"w_category-1\">Category 1</h1>\\n        <h2 id=\"w_snippet-1\">Reply 1</h2>\\n        <p>Reply goes here <a href=\"http://example.com/kb-article\">\\n        http://example.com/kb-article</a>\\n        </p>\\n        <h2 id=\"w_snippet-2\">Reply 2</h2>\\n        <p>Another reply here\\n        </p>\\n        <h1 id=\"w_category-2\">Category 2</h1>\\n        <h2 id=\"w_snippet-3\">Reply 3</h2>\\n        <p>And another reply\\n        </p>\\n\\n\\n    The resulting list returned would be::\\n\\n        [{\\'title\\': \\'Category 1\\',\\n          \\'responses\\':\\n            [{\\'title\\': \\'Reply 1\\',\\n              \\'response\\': \\'Reply goes here http://example.com/kb-article\\'},\\n             {\\'title\\': \\'Reply 2\\',\\n              \\'response\\': \\'Another reply here\\'}]\\n         },\\n         {\\'title\\': \\'Category 2\\',\\n          \\'responses\\':\\n            [{\\'title\\': \\'Reply 3\\',\\n              \\'response\\': \\'And another reply\\'}]\\n         }]\\n\\n    '\n    replies = []\n    try:\n        default_doc = Document.objects.get(slug=REPLIES_DOCUMENT_SLUG, locale=settings.WIKI_DEFAULT_LANGUAGE)\n    except Document.DoesNotExist:\n        return replies\n    if (locale != default_doc.locale):\n        translated_doc = default_doc.translated_to(locale)\n        if (translated_doc and translated_doc.current_revision):\n            doc = translated_doc\n        else:\n            doc = default_doc\n    else:\n        doc = default_doc\n    pq = PyQuery(doc.html)\n    try:\n        current_node = pq('h1')[0]\n    except IndexError:\n        return replies\n    current_category = None\n    current_response = None\n    while (current_node is not None):\n        if (current_node.tag == 'h1'):\n            current_category = {\n                'title': current_node.text,\n                'responses': [],\n            }\n            replies.append(current_category)\n        elif (current_node.tag == 'h2'):\n            current_response = {\n                'title': current_node.text,\n                'response': '',\n            }\n            current_category['responses'].append(current_response)\n        elif (current_node.tag == 'p'):\n            text = current_node.text_content().strip()\n            if (text and current_response):\n                current_response['response'] = text\n        current_node = current_node.getnext()\n    return replies\n", "label": 1}
{"function": "\n\ndef testInclude(self):\n    (fd, tempPath) = tempfile.mkstemp(suffix='.cfg')\n\n    def unlinkTemp(path):\n        try:\n            os.unlink(path)\n        except:\n            pass\n    atexit.register(unlinkTemp, tempPath)\n    fp = os.fdopen(fd, 'w')\n    ((print >> fp), '[section3]\\nbaz = somevalue\\n')\n    fp.close()\n    s = ('%s\\n\\n%%include \"%s\"\\n' % (CONFIG2, tempPath))\n    os.environ['SOME_ENV_VAR'] = 'test_test_test'\n    config = Configuration()\n    config.readfp(StringIO(s))\n    unlinkTemp(tempPath)\n    assert config.has_section('section1')\n    assert config.has_section('section2')\n    assert config.has_section('section3')\n    assert (not config.has_section('foo'))\n    assert (not config.has_section('bar'))\n    assert (not config.has_section('bar2'))\n    assert config.has_option('section1', 'foo')\n    assert config.has_option('section1', 'bar')\n    assert (not config.has_option('section1', 'bar2'))\n    assert config.has_option('section2', 'foo')\n    assert config.has_option('section2', 'bar')\n    assert config.has_option('section3', 'baz')\n    assert (config.get('section1', 'foo') == 'bar')\n    assert (config.get('section1', 'bar') == 'bar')\n    assert (config.get('section2', 'foo') == 'bar')\n    assert (config.get('section2', 'bar') == os.environ['SOME_ENV_VAR'])\n    assert (config.get('section3', 'baz') == 'somevalue')\n", "label": 1}
{"function": "\n\ndef test_update_changes_with_autoflush(self):\n    User = self.classes.User\n    sess = Session()\n    (john, jack, jill, jane) = sess.query(User).order_by(User.id).all()\n    john.age = 50\n    jack.age = 37\n    sess.query(User).filter((User.age > 29)).update({\n        'age': (User.age - 10),\n    }, synchronize_session='evaluate')\n    for x in (john, jack, jill, jane):\n        assert (not sess.is_modified(x))\n    eq_([john.age, jack.age, jill.age, jane.age], [40, 27, 29, 27])\n    john.age = 25\n    assert (john in sess.dirty)\n    assert (jack not in sess.dirty)\n    assert (jill not in sess.dirty)\n    assert sess.is_modified(john)\n    assert (not sess.is_modified(jack))\n", "label": 0}
{"function": "\n\ndef _return_handler(self, ret_value, func, arguments):\n    'Check return values for errors and warnings.\\n        '\n    logger.debug('%s%s -> %r', func.__name__, _args_to_str(arguments), ret_value, extra=self._logging_extra)\n    try:\n        ret_value = constants.StatusCode(ret_value)\n    except ValueError:\n        pass\n    self._last_status = ret_value\n    session = None\n    if (func.__name__ not in ('viFindNext',)):\n        try:\n            session = arguments[0]\n        except KeyError:\n            raise Exception(('Function %r does not seem to be a valid visa function (len args %d)' % (func, len(arguments))))\n        if (func.__name__ in ('viOpenDefaultRM',)):\n            session = session._obj.value\n        if isinstance(session, integer_types):\n            self._last_status_in_session[session] = ret_value\n        elif (func.__name__ not in ('viClose', 'viGetAttribute', 'viSetAttribute', 'viStatusDesc')):\n            raise Exception(('Function %r does not seem to be a valid visa function (type args[0] %r)' % (func, type(session))))\n    if (ret_value < 0):\n        raise errors.VisaIOError(ret_value)\n    if (ret_value in self.issue_warning_on):\n        if (session and (ret_value not in self._ignore_warning_in_session[session])):\n            warnings.warn(errors.VisaIOWarning(ret_value), stacklevel=2)\n    return ret_value\n", "label": 1}
{"function": "\n\ndef _parse_waf(context, repos, record, identifier):\n    recobjs = []\n    content = util.http_request('GET', record)\n    LOGGER.debug(content)\n    try:\n        parser = etree.HTMLParser()\n        tree = etree.fromstring(content, parser)\n    except Exception as err:\n        raise Exception(('Could not parse WAF: %s' % str(err)))\n    up = urlparse(record)\n    links = []\n    LOGGER.debug('collecting links')\n    for link in tree.xpath('//a/@href'):\n        link = link.strip()\n        if (not link):\n            continue\n        if (link.find('?') != (- 1)):\n            continue\n        if (not link.endswith('.xml')):\n            LOGGER.debug('Skipping, not .xml')\n            continue\n        if ('/' in link):\n            if (link[(- 1)] == '/'):\n                continue\n            if (link[0] == '/'):\n                link = ('%s://%s%s' % (up.scheme, up.netloc, link))\n        else:\n            link = ('%s/%s' % (record, link))\n        LOGGER.debug('URL is: %s', link)\n        links.append(link)\n    LOGGER.debug('%d links found', len(links))\n    for link in links:\n        LOGGER.debug('Processing link %s', link)\n        linkcontent = util.http_request('GET', link)\n        recobj = _parse_metadata(context, repos, linkcontent)[0]\n        recobj.source = link\n        recobj.mdsource = link\n        recobjs.append(recobj)\n    return recobjs\n", "label": 0}
{"function": "\n\n@xmlrpc_func(returns='string', args=['string', 'string'])\ndef pingback_ping(source, target):\n    \"\\n    pingback.ping(sourceURI, targetURI) => 'Pingback message'\\n\\n    Notifies the server that a link has been added to sourceURI,\\n    pointing to targetURI.\\n\\n    See: http://hixie.ch/specs/pingback/pingback-1.0\\n    \"\n    try:\n        if (source == target):\n            return UNDEFINED_ERROR\n        site = Site.objects.get_current()\n        try:\n            document = ''.join(map((lambda byte_line: byte_line.decode('utf-8')), urlopen(source).readlines()))\n        except (HTTPError, URLError):\n            return SOURCE_DOES_NOT_EXIST\n        if (target not in document):\n            return SOURCE_DOES_NOT_LINK\n        (scheme, netloc, path, query, fragment) = urlsplit(target)\n        if (netloc != site.domain):\n            return TARGET_DOES_NOT_EXIST\n        try:\n            (view, args, kwargs) = resolve(path)\n        except Resolver404:\n            return TARGET_DOES_NOT_EXIST\n        try:\n            entry = Entry.published.get(slug=kwargs['slug'], publication_date__year=kwargs['year'], publication_date__month=kwargs['month'], publication_date__day=kwargs['day'])\n            if (not entry.pingbacks_are_open):\n                return TARGET_IS_NOT_PINGABLE\n        except (KeyError, Entry.DoesNotExist):\n            return TARGET_IS_NOT_PINGABLE\n        soup = BeautifulSoup(document, 'html.parser')\n        title = six.text_type(soup.find('title'))\n        title = ((title and strip_tags(title)) or _('No title'))\n        description = generate_pingback_content(soup, target, PINGBACK_CONTENT_LENGTH)\n        (pingback, created) = comments.get_model().objects.get_or_create(content_type=ContentType.objects.get_for_model(Entry), object_pk=entry.pk, user_url=source, site=site, defaults={\n            'comment': description,\n            'user_name': title,\n            'submit_date': timezone.now(),\n        })\n        if created:\n            pingback.flags.create(user=get_user_flagger(), flag=PINGBACK)\n            pingback_was_posted.send(pingback.__class__, pingback=pingback, entry=entry)\n            return ('Pingback from %s to %s registered.' % (source, target))\n        return PINGBACK_ALREADY_REGISTERED\n    except:\n        return UNDEFINED_ERROR\n", "label": 1}
{"function": "\n\ndef test_bind_arguments(self):\n    (users, Address, addresses, User) = (self.tables.users, self.classes.Address, self.tables.addresses, self.classes.User)\n    mapper(User, users)\n    mapper(Address, addresses)\n    e1 = engines.testing_engine()\n    e2 = engines.testing_engine()\n    e3 = engines.testing_engine()\n    sess = Session(e3)\n    sess.bind_mapper(User, e1)\n    sess.bind_mapper(Address, e2)\n    assert (sess.connection().engine is e3)\n    assert (sess.connection(bind=e1).engine is e1)\n    assert (sess.connection(mapper=Address, bind=e1).engine is e1)\n    assert (sess.connection(mapper=Address).engine is e2)\n    assert (sess.connection(clause=addresses.select()).engine is e2)\n    assert (sess.connection(mapper=User, clause=addresses.select()).engine is e1)\n    assert (sess.connection(mapper=User, clause=addresses.select(), bind=e2).engine is e2)\n    sess.close()\n", "label": 0}
{"function": "\n\ndef iterate_from(self, start_tok):\n    if (self._cache[0] <= start_tok < self._cache[1]):\n        for tok in self._cache[2][(start_tok - self._cache[0]):]:\n            (yield tok)\n            start_tok += 1\n    if (start_tok < self._toknum[(- 1)]):\n        block_index = (bisect.bisect_right(self._toknum, start_tok) - 1)\n        toknum = self._toknum[block_index]\n        filepos = self._filepos[block_index]\n    else:\n        block_index = (len(self._toknum) - 1)\n        toknum = self._toknum[(- 1)]\n        filepos = self._filepos[(- 1)]\n    if (self._stream is None):\n        self._open()\n    while (filepos < self._eofpos):\n        self._stream.seek(filepos)\n        self._current_toknum = toknum\n        self._current_blocknum = block_index\n        tokens = self.read_block(self._stream)\n        assert isinstance(tokens, (tuple, list, AbstractLazySequence)), ('block reader %s() should return list or tuple.' % self.read_block.__name__)\n        num_toks = len(tokens)\n        new_filepos = self._stream.tell()\n        assert (new_filepos > filepos), ('block reader %s() should consume at least 1 byte (filepos=%d)' % (self.read_block.__name__, filepos))\n        self._cache = (toknum, (toknum + num_toks), list(tokens))\n        assert (toknum <= self._toknum[(- 1)])\n        if (num_toks > 0):\n            block_index += 1\n            if (toknum == self._toknum[(- 1)]):\n                assert (new_filepos > self._filepos[(- 1)])\n                self._filepos.append(new_filepos)\n                self._toknum.append((toknum + num_toks))\n            else:\n                assert (new_filepos == self._filepos[block_index]), 'inconsistent block reader (num chars read)'\n                assert ((toknum + num_toks) == self._toknum[block_index]), 'inconsistent block reader (num tokens returned)'\n        if (new_filepos == self._eofpos):\n            self._len = (toknum + num_toks)\n        for tok in tokens[max(0, (start_tok - toknum)):]:\n            (yield tok)\n        assert (new_filepos <= self._eofpos)\n        if (new_filepos == self._eofpos):\n            break\n        toknum += num_toks\n        filepos = new_filepos\n    assert (self._len is not None)\n    self.close()\n", "label": 1}
{"function": "\n\n@contextmanager\ndef assert_produces_warning(expected_warning=Warning, filter_level='always', clear=None, check_stacklevel=True):\n    \"\\n    Context manager for running code that expects to raise (or not raise)\\n    warnings.  Checks that code raises the expected warning and only the\\n    expected warning. Pass ``False`` or ``None`` to check that it does *not*\\n    raise a warning. Defaults to ``exception.Warning``, baseclass of all\\n    Warnings. (basically a wrapper around ``warnings.catch_warnings``).\\n\\n    >>> import warnings\\n    >>> with assert_produces_warning():\\n    ...     warnings.warn(UserWarning())\\n    ...\\n    >>> with assert_produces_warning(False):\\n    ...     warnings.warn(RuntimeWarning())\\n    ...\\n    Traceback (most recent call last):\\n        ...\\n    AssertionError: Caused unexpected warning(s): ['RuntimeWarning'].\\n    >>> with assert_produces_warning(UserWarning):\\n    ...     warnings.warn(RuntimeWarning())\\n    Traceback (most recent call last):\\n        ...\\n    AssertionError: Did not see expected warning of class 'UserWarning'.\\n\\n    ..warn:: This is *not* thread-safe.\\n    \"\n    with warnings.catch_warnings(record=True) as w:\n        if (clear is not None):\n            if (not is_list_like(clear)):\n                clear = [clear]\n            for m in clear:\n                try:\n                    m.__warningregistry__.clear()\n                except:\n                    pass\n        saw_warning = False\n        warnings.simplefilter(filter_level)\n        (yield w)\n        extra_warnings = []\n        for actual_warning in w:\n            if (expected_warning and issubclass(actual_warning.category, expected_warning)):\n                saw_warning = True\n                if (check_stacklevel and issubclass(actual_warning.category, (FutureWarning, DeprecationWarning))):\n                    from inspect import getframeinfo, stack\n                    caller = getframeinfo(stack()[2][0])\n                    msg = 'Warning not set with correct stacklevel. File where warning is raised: {0} != {1}. Warning message: {2}'.format(actual_warning.filename, caller.filename, actual_warning.message)\n                    assert (actual_warning.filename == caller.filename), msg\n            else:\n                extra_warnings.append(actual_warning.category.__name__)\n        if expected_warning:\n            assert saw_warning, ('Did not see expected warning of class %r.' % expected_warning.__name__)\n        assert (not extra_warnings), ('Caused unexpected warning(s): %r.' % extra_warnings)\n", "label": 1}
{"function": "\n\ndef test_solve_biquadratic():\n    (x0, y0, x1, y1, r) = symbols('x0 y0 x1 y1 r')\n    f_1 = ((((x - 1) ** 2) + ((y - 1) ** 2)) - (r ** 2))\n    f_2 = ((((x - 2) ** 2) + ((y - 2) ** 2)) - (r ** 2))\n    assert (solve_poly_system([f_1, f_2], x, y) == [(((S(3) / 2) - (sqrt(((- 1) + (2 * (r ** 2)))) / 2)), ((S(3) / 2) + (sqrt(((- 1) + (2 * (r ** 2)))) / 2))), (((S(3) / 2) + (sqrt(((- 1) + (2 * (r ** 2)))) / 2)), ((S(3) / 2) - (sqrt(((- 1) + (2 * (r ** 2)))) / 2)))])\n    f_1 = ((((x - 1) ** 2) + ((y - 2) ** 2)) - (r ** 2))\n    f_2 = ((((x - 1) ** 2) + ((y - 1) ** 2)) - (r ** 2))\n    assert (solve_poly_system([f_1, f_2], x, y) == [((1 - (sqrt((((2 * r) - 1) * ((2 * r) + 1))) / 2)), (S(3) / 2)), ((1 + (sqrt((((2 * r) - 1) * ((2 * r) + 1))) / 2)), (S(3) / 2))])\n    query = (lambda expr: (expr.is_Pow and (expr.exp is S.Half)))\n    f_1 = ((((x - 1) ** 2) + ((y - 2) ** 2)) - (r ** 2))\n    f_2 = ((((x - x1) ** 2) + ((y - 1) ** 2)) - (r ** 2))\n    result = solve_poly_system([f_1, f_2], x, y)\n    assert ((len(result) == 2) and all(((len(r) == 2) for r in result)))\n    assert all(((r.count(query) == 1) for r in flatten(result)))\n    f_1 = ((((x - x0) ** 2) + ((y - y0) ** 2)) - (r ** 2))\n    f_2 = ((((x - x1) ** 2) + ((y - y1) ** 2)) - (r ** 2))\n    result = solve_poly_system([f_1, f_2], x, y)\n    assert ((len(result) == 2) and all(((len(r) == 2) for r in result)))\n    assert all(((len(r.find(query)) == 1) for r in flatten(result)))\n", "label": 0}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a time. Returns a\\n        Python datetime.time object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value.time()\n    if isinstance(value, datetime.time):\n        return value\n    if isinstance(value, list):\n        if (len(value) != 2):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES)):\n            return None\n        start_value = value[0]\n        end_value = value[1]\n    start_time = None\n    end_time = None\n    for format in (self.input_formats or formats.get_format('TIME_INPUT_FORMATS')):\n        try:\n            start_time = datetime.datetime(*time.strptime(start_value, format)[:6]).time()\n        except ValueError:\n            if start_time:\n                continue\n            else:\n                raise ValidationError(self.error_messages['invalid'])\n    for format in (self.input_formats or formats.get_format('TIME_INPUT_FORMATS')):\n        try:\n            end_time = datetime.datetime(*time.strptime(end_value, format)[:6]).time()\n        except ValueError:\n            if end_time:\n                continue\n            else:\n                raise ValidationError(self.error_messages['invalid'])\n    return (start_time, end_time)\n", "label": 1}
{"function": "\n\ndef load_profile(self, profile_directory):\n    list_files = os.listdir(profile_directory)\n    if (not list_files):\n        raise LangDetectException(ErrorCode.NeedLoadProfileError, ('Not found profile: ' + profile_directory))\n    (langsize, index) = (len(list_files), 0)\n    for filename in list_files:\n        if filename.startswith('.'):\n            continue\n        filename = path.join(profile_directory, filename)\n        if (not path.isfile(filename)):\n            continue\n        f = None\n        try:\n            if (sys.version_info[0] < 3):\n                f = open(filename, 'r')\n            else:\n                f = open(filename, 'r', encoding='utf-8')\n            json_data = json.load(f)\n            profile = LangProfile(**json_data)\n            self.add_profile(profile, index, langsize)\n            index += 1\n        except IOError:\n            raise LangDetectException(ErrorCode.FileLoadError, ('Cannot open \"%s\"' % filename))\n        except:\n            raise LangDetectException(ErrorCode.FormatError, ('Profile format error in \"%s\"' % filename))\n        finally:\n            if f:\n                f.close()\n", "label": 0}
{"function": "\n\ndef trace_memory_usage_with_windbg(target_cmd, init_sym, memory_limit, timeout=_common.DEFAULT_TIMEOUT, symbols=None, env=None, callback=None, callback_args=None):\n    '\\n    This can be used to help pin point the call that causes the target\\n    application to exceed the defined memory limit.  It returns a\\n    :class:`FuzzResult`.\\n\\n    init_sym is the symbol used for the initial breakpoint used to\\n    configure the run.  For example: <target>!main\\n\\n    memory_limit is the memory limit in bytes that the application\\n    should not exceed.\\n\\n    TODO: Add support for callback\\n\\n    Availability: Windows.\\n    '\n    if (callback is not None):\n        raise RuntimeError('callback support not implemented')\n    if (not isinstance(init_sym, str)):\n        raise TypeError(('init_sym must be of type str not %s' % type(init_sym).__name__))\n    if (not isinstance(memory_limit, int)):\n        raise TypeError(('memory_limit be of type int not %s' % type(memory_limit).__name__))\n    if (env is None):\n        env = dict(os.environ)\n    env['LIBC_FATAL_STDERR_'] = '1'\n    if (symbols is not None):\n        env['_NT_ALT_SYMBOL_PATH'] = symbols\n    elif target_cmd:\n        env['_NT_ALT_SYMBOL_PATH'] = os.path.abspath(os.path.dirname(target_cmd[0]))\n    dbg_script = os.path.join(_common.PATH_DBG, 'scripts', 'WinDBGMemoryLimit.py')\n    cdb_cmd = [TOOL_CDB, '-x', '-y', 'SRV*c:\\\\websymbols*http://msdl.microsoft.com/download/symbols', '-c', ('.load pykd.pyd;!py %s -l %d -s %s;q' % (dbg_script, memory_limit, init_sym))]\n    gflags_args = _common._disable_gflags(target_cmd[0])\n    with tempfile.TemporaryFile(mode='w+t') as f:\n        p = subprocess.Popen((cdb_cmd + target_cmd), stderr=f, stdout=f, env=env, stdin=subprocess.PIPE, creationflags=_common.POPEN_FLAGS)\n        timeout_time = (time.time() + timeout)\n        while (p.poll() is None):\n            if (time.time() > timeout_time):\n                with open(os.devnull, 'w') as fp:\n                    subprocess.call(['taskkill', '/pid', str(p.pid), '/f'], stdout=fp, stderr=fp)\n                p.wait()\n            time.sleep(0.5)\n        f.seek(0, os.SEEK_SET)\n        output = []\n        for line in f.readlines():\n            if (not line.strip().startswith('Breakpoint')):\n                output.append(line)\n    if (gflags_args is not None):\n        _common._set_gflags(target_cmd[0], **gflags_args)\n    output = ''.join(output)\n    has_tb = output.find('Traceback (most recent call last):')\n    if (has_tb != (- 1)):\n        raise RuntimeError(('CDB Python Failure\\n%s\\n%s' % (('-' * 80), output[has_tb:])))\n    call_stack = _common.process_exploitable_output(output)[0]\n    if (not call_stack):\n        return _common.FuzzResult(text=output)\n    return _common.FuzzResult(classification=_common.EXCESS_MEMORY_USAGE, text=output, backtrace=call_stack)\n", "label": 1}
{"function": "\n\ndef test_basic_request(graph, groups, permissions, session, standard_graph, users):\n    group_sre = groups['team-sre']\n    group_not_sre = [g for (name, g) in groups.items() if (name != 'team-sre')]\n    assert (not any([group.my_requests(status='pending').all() for group in groups.values()])), 'no group should start with pending requests'\n    group_sre.add_member(users['testuser@a.co'], users['testuser@a.co'], reason='for the lulz')\n    session.commit()\n    request_not_sre = [group.my_requests(status='pending').all() for group in group_not_sre]\n    assert (not any(request_not_sre)), 'only affected group should show pending requests'\n    request_sre = group_sre.my_requests(status='pending').all()\n    assert (len(request_sre) == 1), 'affected group should have request'\n    request = session.query(Request).filter_by(id=request_sre[0].id).scalar()\n    request.update_status(users['gary@a.co'], 'actioned', 'for being a good person')\n    session.commit()\n    assert (not any([group.my_requests(status='pending').all() for group in groups.values()])), 'no group should have requests after being actioned'\n", "label": 0}
{"function": "\n\ndef login_by_token(request):\n    ret = None\n    auth_exception = None\n    if (request.method == 'POST'):\n        form = FacebookConnectForm(request.POST)\n        if form.is_valid():\n            try:\n                provider = providers.registry.by_id(FacebookProvider.id)\n                login_options = provider.get_fb_login_options(request)\n                app = providers.registry.by_id(FacebookProvider.id).get_app(request)\n                access_token = form.cleaned_data['access_token']\n                if (login_options.get('auth_type') == 'reauthenticate'):\n                    info = requests.get((GRAPH_API_URL + '/oauth/access_token_info'), params={\n                        'client_id': app.client_id,\n                        'access_token': access_token,\n                    }).json()\n                    nonce = provider.get_nonce(request, pop=True)\n                    ok = (nonce and (nonce == info.get('auth_nonce')))\n                else:\n                    ok = True\n                if (ok and provider.get_settings().get('EXCHANGE_TOKEN')):\n                    resp = requests.get((GRAPH_API_URL + '/oauth/access_token'), params={\n                        'grant_type': 'fb_exchange_token',\n                        'client_id': app.client_id,\n                        'client_secret': app.secret,\n                        'fb_exchange_token': access_token,\n                    }).json()\n                    access_token = resp['access_token']\n                if ok:\n                    token = SocialToken(app=app, token=access_token)\n                    login = fb_complete_login(request, app, token)\n                    login.token = token\n                    login.state = SocialLogin.state_from_request(request)\n                    ret = complete_social_login(request, login)\n            except requests.RequestException as e:\n                logger.exception('Error accessing FB user profile')\n                auth_exception = e\n    if (not ret):\n        ret = render_authentication_error(request, FacebookProvider.id, exception=auth_exception)\n    return ret\n", "label": 0}
{"function": "\n\ndef client_send_osc_message(self, category, name, data):\n    \"Sends an OSC message to the client to update it\\n        Parameters:\\n        category - type of update, sw, coil, lamp, led, etc.\\n        name - the name of the object we're updating\\n        data - the data we're sending\\n        \"\n    if self.OSC_clients:\n        self.OSC_message = OSCmodule.OSCMessage(((('/' + str(category)) + '/') + name))\n        self.OSC_message.append(data)\n        for k in self.OSC_clients.items():\n            try:\n                if self.config['debug_messages']:\n                    self.log.info('Sending OSC Message to client:%s: %s', k, self.OSC_message)\n                k[1].send(self.OSC_message)\n            except OSCmodule.OSCClientError:\n                self.log.info('OSC client at address %s disconnected', k[0])\n                self.clients_to_delete.append(k)\n                break\n    for client in self.clients_to_delete:\n        if (client in self.OSC_clients):\n            del self.OSC_clients[client]\n    self.clients_to_delete = []\n    for client in self.clients_to_add:\n        self.setup_osc_client(client)\n", "label": 0}
{"function": "\n\ndef post_process_extensions(self, extensions, resp_obj, request, action_args):\n    for ext in extensions:\n        response = None\n        if inspect.isgenerator(ext):\n            try:\n                with ResourceExceptionHandler():\n                    response = ext.send(resp_obj)\n            except StopIteration:\n                continue\n            except Fault as ex:\n                response = ex\n        else:\n            try:\n                with ResourceExceptionHandler():\n                    response = ext(req=request, resp_obj=resp_obj, **action_args)\n            except exception.VersionNotFoundForAPIMethod:\n                continue\n            except Fault as ex:\n                response = ex\n        if response:\n            return response\n    return None\n", "label": 0}
{"function": "\n\ndef validate_params(method, D):\n    if (type(D['params']) == Object):\n        keys = list(method.json_arg_types.keys())\n        if (len(keys) != len(D['params'])):\n            raise InvalidParamsError(('Not enough params provided for %s' % method.json_sig))\n        for k in keys:\n            if (not (k in D['params'])):\n                print('\\n\\n\\n\\nSHITTER SHITTER', k, D, '\\n\\n\\n\\n')\n                raise InvalidParamsError(('%s is not a valid parameter for %s' % (k, method.json_sig)))\n            if (not (Any.kind(D['params'][k]) == method.json_arg_types[k])):\n                raise InvalidParamsError(('%s is not the correct type %s for %s' % (type(D['params'][k]), method.json_arg_types[k], method.json_sig)))\n    elif (type(D['params']) == Array):\n        arg_types = list(method.json_arg_types.values())\n        try:\n            for (i, arg) in enumerate(D['params']):\n                if (not (Any.kind(arg) == arg_types[i])):\n                    raise InvalidParamsError(('%s is not the correct type %s for %s' % (type(arg), arg_types[i], method.json_sig)))\n        except IndexError:\n            raise InvalidParamsError(('Too many params provided for %s' % method.json_sig))\n        else:\n            if (len(D['params']) != len(arg_types)):\n                raise InvalidParamsError(('Not enough params provided for %s' % method.json_sig))\n", "label": 1}
{"function": "\n\ndef compute(self):\n    machine = self.get_machine()\n    if (not self.has_input('command')):\n        raise ModuleError(self, 'No command specified')\n    command = self.get_input('command').strip()\n    working_directory = (self.get_input('working_directory') if self.has_input('working_directory') else '.')\n    if (not self.has_input('input_directory')):\n        raise ModuleError(self, 'No input directory specified')\n    input_directory = self.get_input('input_directory').strip()\n    additional_arguments = {\n        'processes': 1,\n        'time': (- 1),\n        'mpi': False,\n        'threads': 1,\n        'memory': (- 1),\n        'diskspace': (- 1),\n    }\n    for k in additional_arguments:\n        if self.has_input(k):\n            additional_arguments[k] = self.get_input(k)\n    use_machine(machine)\n    cdir = CreateDirectory('remote', working_directory)\n    trans = TransferFiles('remote', input_directory, working_directory, dependencies=[cdir])\n    job = PBS('remote', command, working_directory, dependencies=[trans], **additional_arguments)\n    job.run()\n    ret = job._ret\n    if ret:\n        try:\n            job_id = int(ret)\n        except ValueError:\n            end_machine()\n            raise ModuleError(self, ('Error submitting job: %s' % ret))\n    finished = job.finished()\n    job_info = job.get_job_info()\n    if job_info:\n        self.annotate({\n            'job_info': job.get_job_info(),\n        })\n    if (not finished):\n        status = job.status()\n        if job_info:\n            comment = [line for line in job_info.split('\\n') if line.startswith('comment =')]\n            if comment:\n                status += (': ' + comment[10:])\n        end_machine()\n        raise ModuleSuspended(self, ('%s' % status), handle=job)\n    get_result = TransferFiles('local', input_directory, working_directory, dependencies=[cdir])\n    get_result.run()\n    end_machine()\n    self.set_output('stdout', job.standard_output())\n    self.set_output('stderr', job.standard_error())\n    files = machine.local.send_command(('ls -l %s' % input_directory))\n    self.set_output('file_list', [f.split(' ')[(- 1)] for f in files.split('\\n')[1:]])\n", "label": 1}
{"function": "\n\ndef handleJsonMessage(self, incomingJsonMessage):\n    if (not isinstance(incomingJsonMessage, dict)):\n        raise MalformedMessageException(('Incoming message was not a dictionary: %s' % incomingJsonMessage))\n    if (not ('messageId' in incomingJsonMessage)):\n        raise MalformedMessageException(('Invalid incoming message id: %s' % incomingJsonMessage))\n    if (incomingJsonMessage['messageId'] != self.expectedMessageId):\n        raise MalformedMessageException(('Invalid incoming message id: expected %s, but got %s. %s' % (self.expectedMessageId, incomingJsonMessage['messageId'], incomingJsonMessage)))\n    try:\n        self.expectedMessageId += 1\n        if (incomingJsonMessage['messageType'] not in self.messageTypeHandlers):\n            raise MalformedMessageException('Invalid incoming messageType')\n        if (not ('objectDefinition' in incomingJsonMessage)):\n            raise MalformedMessageException('No object definition given')\n        if (incomingJsonMessage['messageType'] != 'ServerFlushObjectIdsBelow'):\n            obj = self.extractObjectDefinition(incomingJsonMessage['objectDefinition'])\n        else:\n            obj = None\n        return self.messageTypeHandlers[incomingJsonMessage['messageType']](incomingJsonMessage, obj)\n    except MalformedMessageException:\n        raise\n    except Exception as e:\n        return [unexpectedExceptionJson(incomingJsonMessage, Exceptions.wrapException(e).message)]\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize(['gevent_count', 'subpool_size', 'iterations', 'expected_clients'], [(None, None, 1, 1), (None, None, 2, 1), (None, 10, 1, 10), (None, 10, 2, 10), (None, 200, 1, 100), (None, 200, 2, 100), (4, None, 1, 4), (4, None, 2, 4), (2, 2, 1, 4), (2, 2, 2, 4)])\ndef test_redis_disconnections(gevent_count, subpool_size, iterations, expected_clients, worker):\n    \" mrq.context.connections is not the actual connections pool that the worker uses.\\n        this worker's pool is not accessible from here, since it runs in a different thread.\\n    \"\n    from mrq.context import connections\n    worker.start_deps()\n    gevent_count = (gevent_count if (gevent_count is not None) else 1)\n    get_clients = (lambda : [c for c in connections.redis.client_list() if (c.get('cmd') != 'client')])\n    assert (len(get_clients()) == 0)\n    kwargs = {\n        'flags': '--redis_max_connections 100',\n        'deps': False,\n    }\n    if gevent_count:\n        kwargs['flags'] += (' --gevent %s' % gevent_count)\n    worker.start(**kwargs)\n    for i in range(0, iterations):\n        worker.send_tasks('tests.tasks.redis.Disconnections', ([{\n            'subpool_size': subpool_size,\n        }] * gevent_count))\n    assert (len(get_clients()) == expected_clients)\n    worker.stop(deps=False)\n    assert (len(get_clients()) == 0)\n    worker.stop_deps()\n", "label": 0}
{"function": "\n\ndef _calculate_approval(self):\n    'Calculates the approval information for the review request.'\n    from reviewboard.extensions.hooks import ReviewRequestApprovalHook\n    approved = True\n    failure = None\n    if (self.shipit_count == 0):\n        approved = False\n        failure = 'The review request has not been marked \"Ship It!\"'\n    elif (self.issue_open_count > 0):\n        approved = False\n        failure = 'The review request has open issues.'\n    for hook in ReviewRequestApprovalHook.hooks:\n        try:\n            result = hook.is_approved(self, approved, failure)\n            if isinstance(result, tuple):\n                (approved, failure) = result\n            elif isinstance(result, bool):\n                approved = result\n            else:\n                raise ValueError(('%r returned an invalid value %r from is_approved' % (hook, result)))\n            if approved:\n                failure = None\n        except Exception as e:\n            extension = hook.extension\n            logging.error('Error when running ReviewRequestApprovalHook.is_approved function in extension: \"%s\": %s', extension.id, e, exc_info=1)\n    self._approval_failure = failure\n    self._approved = approved\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('page', [None, 1, 5])\ndef test_with_classifiers(self, monkeypatch, db_request, page):\n    params = MultiDict([('q', 'foo bar'), ('c', 'foo :: bar'), ('c', 'fiz :: buz')])\n    if (page is not None):\n        params['page'] = page\n    db_request.params = params\n    es_query = pretend.stub(suggest=pretend.call_recorder((lambda *a, **kw: es_query)), filter=pretend.call_recorder((lambda *a, **kw: es_query)), sort=pretend.call_recorder((lambda *a, **kw: es_query)))\n    db_request.es = pretend.stub(query=pretend.call_recorder((lambda *a, **kw: es_query)))\n    classifier1 = ClassifierFactory.create(classifier='foo :: bar')\n    classifier2 = ClassifierFactory.create(classifier='foo :: baz')\n    classifier3 = ClassifierFactory.create(classifier='fiz :: buz')\n    page_obj = pretend.stub(page_count=((page or 1) + 10))\n    page_cls = pretend.call_recorder((lambda *a, **kw: page_obj))\n    monkeypatch.setattr(views, 'ElasticsearchPage', page_cls)\n    url_maker = pretend.stub()\n    url_maker_factory = pretend.call_recorder((lambda request: url_maker))\n    monkeypatch.setattr(views, 'paginate_url_factory', url_maker_factory)\n    assert (search(db_request) == {\n        'page': page_obj,\n        'term': params.get('q', ''),\n        'order': params.get('o', ''),\n        'applied_filters': params.getall('c'),\n        'available_filters': [('fiz', [classifier3.classifier]), ('foo', [classifier1.classifier, classifier2.classifier])],\n    })\n    assert (page_cls.calls == [pretend.call(es_query, url_maker=url_maker, page=(page or 1))])\n    assert (url_maker_factory.calls == [pretend.call(db_request)])\n    assert (db_request.es.query.calls == [pretend.call('multi_match', query='foo bar', fields=['name^2', 'version', 'author', 'author_email', 'maintainer', 'maintainer_email', 'home_page', 'license', 'summary', 'description', 'keywords', 'platform', 'download_url'])])\n    assert (es_query.suggest.calls == [pretend.call(name='name_suggestion', term={\n        'field': 'name',\n    }, text='foo bar')])\n    assert (es_query.filter.calls == [pretend.call('terms', classifiers=['foo :: bar', 'fiz :: buz'])])\n", "label": 0}
{"function": "\n\ndef extract_conf_from(mod, conf=ModuleConfig(CONF_SPEC), depth=0, max_depth=2):\n    'recursively extract keys from module or object\\n    by passed config scheme\\n    '\n    for (key, default_value) in six.iteritems(conf):\n        conf[key] = _get_key_from_module(mod, key, default_value)\n    try:\n        filtered_apps = [app for app in conf['apps'] if (app not in BLACKLIST)]\n    except TypeError:\n        pass\n    except Exception as e:\n        warnings.warn(('Error %s during loading %s' % (e, conf['apps'])))\n    for app in filtered_apps:\n        try:\n            app_module = import_module(app)\n            if (app_module != mod):\n                app_module = _get_correct_module(app_module)\n                if (depth < max_depth):\n                    mod_conf = extract_conf_from(app_module, depth=(depth + 1))\n                    for (k, v) in six.iteritems(mod_conf):\n                        if (k == 'config'):\n                            continue\n                        if isinstance(v, dict):\n                            conf[k].update(v)\n                        elif isinstance(v, (list, tuple)):\n                            conf[k] = merge(conf[k], v)\n        except Exception as e:\n            pass\n    return conf\n", "label": 1}
{"function": "\n\ndef addRenderTargetSection(g):\n    s = g.report.create(Report.Section, 'Render targets')\n    configs = set()\n    for event in g.trace.events:\n        for (key, value) in event.values.items():\n            if isinstance(value, Trace.Object):\n                if (value.cls.name == 'EGLConfig'):\n                    id = value.attrs.get('config_id')\n                    if ((not id) or (id in configs)):\n                        continue\n                    configs.add(id)\n                    s2 = s.create(Report.Section, ('EGL configuration #%d' % id))\n                    t = s2.create(Report.Table, ['Property', 'Value'])\n                    for (cfgKey, cfgValue) in value.attrs.items():\n                        cfgKey = cfgKey.replace('_', ' ').capitalize()\n                        t.addRow(cfgKey, cfgValue)\n        if (event.name == 'eglCreateWindowSurface'):\n            window = event.values.get('window')\n            if (not window):\n                continue\n            s2 = s.create(Report.Section, ('Window surface 0x%x' % event.values[None].id))\n            t = s2.create(Report.Table, ['Property', 'Value'])\n            t.addRow('Width', window.attrs['width'])\n            t.addRow('Height', window.attrs['height'])\n            t.addRow('Config ID', event.values['config'].id)\n        elif (event.name == 'eglCreatePixmapSurface'):\n            pixmap = event.values.get('pixmap')\n            if (not pixmap):\n                continue\n            s2 = s.create(Report.Section, ('Pixmap surface 0x%x' % event.values[None].id))\n            t = s2.create(Report.Table, ['Property', 'Value'])\n            t.addRow('Width', pixmap.attrs['width'])\n            t.addRow('Height', pixmap.attrs['height'])\n            t.addRow('Config ID', event.values['config'].id)\n        elif (event.name == 'eglCreatePbufferSurface'):\n            attrs = event.values.get('attrib_list')\n            if (not attrs):\n                continue\n            try:\n                width = attrs[(attrs.index(g.constants.EGL_WIDTH) + 1)]\n                height = attrs[(attrs.index(g.constants.EGL_HEIGHT) + 1)]\n            except ValueError:\n                continue\n            s2 = s.create(Report.Section, ('Pbuffer surface 0x%x' % event.values[None].id))\n            t = s2.create(Report.Table, ['Property', 'Value'])\n            t.addRow('Width', width)\n            t.addRow('Height', height)\n            t.addRow('Config ID', event.values['config'].id)\n", "label": 1}
{"function": "\n\n@requires_search\ndef update_node(node, index=None, bulk=False):\n    index = (index or INDEX)\n    from website.addons.wiki.model import NodeWikiPage\n    category = get_doctype_from_node(node)\n    if (category == 'project'):\n        elastic_document_id = node._id\n        parent_id = None\n    else:\n        try:\n            elastic_document_id = node._id\n            parent_id = node.parent_id\n        except IndexError:\n            return\n    from website.files.models.osfstorage import OsfStorageFile\n    for file_ in paginated(OsfStorageFile, Q('node', 'eq', node)):\n        update_file(file_, index=index)\n    if (node.is_deleted or (not node.is_public) or node.archiving):\n        delete_doc(elastic_document_id, node)\n    else:\n        try:\n            normalized_title = six.u(node.title)\n        except TypeError:\n            normalized_title = node.title\n        normalized_title = unicodedata.normalize('NFKD', normalized_title).encode('ascii', 'ignore')\n        elastic_document = {\n            'id': elastic_document_id,\n            'contributors': [{\n                'fullname': x.fullname,\n                'url': (x.profile_url if x.is_active else None),\n            } for x in node.visible_contributors if (x is not None)],\n            'title': node.title,\n            'normalized_title': normalized_title,\n            'category': category,\n            'public': node.is_public,\n            'tags': [tag._id for tag in node.tags if tag],\n            'description': node.description,\n            'url': node.url,\n            'is_registration': node.is_registration,\n            'is_pending_registration': node.is_pending_registration,\n            'is_retracted': node.is_retracted,\n            'is_pending_retraction': node.is_pending_retraction,\n            'embargo_end_date': (node.embargo_end_date.strftime('%A, %b. %d, %Y') if node.embargo_end_date else False),\n            'is_pending_embargo': node.is_pending_embargo,\n            'registered_date': node.registered_date,\n            'wikis': {\n                \n            },\n            'parent_id': parent_id,\n            'date_created': node.date_created,\n            'license': serialize_node_license_record(node.license),\n            'primary_institution': (node.primary_institution.name if node.primary_institution else None),\n            'boost': (int((not node.is_registration)) + 1),\n        }\n        if (not node.is_retracted):\n            for wiki in [NodeWikiPage.load(x) for x in node.wiki_pages_current.values()]:\n                elastic_document['wikis'][wiki.page_name] = wiki.raw_text(node)\n        if bulk:\n            return elastic_document\n        else:\n            es.index(index=index, doc_type=category, id=elastic_document_id, body=elastic_document, refresh=True)\n", "label": 1}
{"function": "\n\ndef print_bench_results(self):\n    self.out.write('==============================\\n')\n    self.out.write(' *** BENCHMARKING RESULTS *** \\n')\n    self.out.write('==============================\\n')\n    self.out.write('\\n')\n    results = []\n    for (item, outcome) in self._memo:\n        if isinstance(item, Item):\n            best = item.benchtime\n            if (best is None):\n                tstr = '---'\n            else:\n                if (best > 0.0):\n                    order = min((- int((_floor(log10(best)) // 3))), 3)\n                else:\n                    order = 3\n                tstr = ('%.*g %s' % (precision, (best * scaling[order]), units[order]))\n            results.append([item.name, tstr, item.benchtitle])\n    wm = ([0] * len(units))\n    we = ([0] * len(units))\n    for s in results:\n        tstr = s[1]\n        (n, u) = tstr.split()\n        un = unitn[u]\n        try:\n            (m, e) = n.split('.')\n        except ValueError:\n            (m, e) = (n, '')\n        wm[un] = max(len(m), wm[un])\n        we[un] = max(len(e), we[un])\n    for s in results:\n        tstr = s[1]\n        (n, u) = tstr.split()\n        un = unitn[u]\n        try:\n            (m, e) = n.split('.')\n        except ValueError:\n            (m, e) = (n, '')\n        m = m.rjust(wm[un])\n        e = e.ljust(we[un])\n        if e.strip():\n            n = '.'.join((m, e))\n        else:\n            n = ' '.join((m, e))\n        txt = ''\n        for i in range(len(units)):\n            if (i == un):\n                txt += n\n            else:\n                txt += (' ' * ((wm[i] + we[i]) + 1))\n        s[1] = ('%s %s' % (txt, u))\n    for i in range(2):\n        w = max((len(s[i]) for s in results))\n        for s in results:\n            s[i] = s[i].ljust(w)\n    for s in results:\n        self.out.write(('%s  |  %s  |  %s\\n' % tuple(s)))\n", "label": 1}
{"function": "\n\ndef mklink(self, source, target, force=None):\n    linked = False\n    if (not os.path.exists(source)):\n        raise SymlinkError('symlink source \"{0}\" does not exist'.format(collapseuser(source)))\n    try:\n        os.symlink(source, target)\n        linked = True\n        tty.puts('symlinked {0} -> {1}'.format(tty.color(collapseuser(target), tty.MAGENTA), collapseuser(source)))\n    except OSError as e:\n        if (e.errno != errno.EEXIST):\n            raise\n        if os.path.islink(target):\n            if os.path.samefile(os.path.realpath(target), os.path.realpath(source)):\n                linked = True\n                tty.putdebug('Already linked: {0} -> {1}'.format(tty.color(collapseuser(target), tty.MAGENTA), collapseuser(source)), self.debug)\n            else:\n                fmt = 'Linked to wrong target: {0} -> {1} (instead of {2})'\n                tty.puterr(fmt.format(tty.color(target, tty.MAGENTA), os.path.realpath(collapseuser(target)), os.path.realpath(collapseuser(source))), warning=force)\n        else:\n            tty.puterr('{0} symlink target already exists at: {1}'.format(collapseuser(source), collapseuser(target)), warning=force)\n    if ((not linked) and force):\n        try:\n            osx.move_to_trash(target)\n            print(tty.progress('Moved {0} to trash').format(target))\n        except OSError as e:\n            tty.puterr('Error moving {0} to trash: {1}'.format(target, str(e)))\n            return False\n        return self.mklink(source, target, force)\n    return linked\n", "label": 0}
{"function": "\n\ndef save(self, **kwargs):\n    if (not self.content_type_id):\n        self.content_type = ContentType.objects.get_for_model(self)\n    send_signal = None\n    old_self = None\n    if self.pk:\n        try:\n            old_self = self.__class__.objects.get(pk=self.pk)\n        except Publishable.DoesNotExist:\n            pass\n    if old_self:\n        old_path = old_self.get_absolute_url()\n        new_path = self.get_absolute_url()\n        if ((old_path != new_path) and new_path and (not old_self.static)):\n            redirect = Redirect.objects.get_or_create(old_path=old_path, site=self.category.site)[0]\n            redirect.new_path = new_path\n            redirect.save(force_update=True)\n            Redirect.objects.filter(new_path=old_path).exclude(pk=redirect.pk).update(new_path=new_path)\n        if (old_self.is_published() != self.is_published()):\n            if self.is_published():\n                send_signal = content_published\n                self.announced = True\n            else:\n                send_signal = content_unpublished\n                self.announced = False\n        if ((old_self.published != self.published) and (self.published is False)):\n            send_signal = content_unpublished\n            self.announced = False\n        if ((old_self.last_updated == old_self.publish_from) and (self.last_updated == old_self.last_updated)):\n            self.last_updated = self.publish_from\n    elif self.is_published():\n        send_signal = content_published\n        self.announced = True\n    if (not self.last_updated):\n        self.last_updated = self.publish_from\n    super(Publishable, self).save(**kwargs)\n    if send_signal:\n        send_signal.send(sender=self.__class__, publishable=self)\n", "label": 1}
{"function": "\n\ndef __init__(self, **kwargs):\n    '\\n        Use keyword arguments to overwrite\\n        any of the class attributes for this instance.\\n\\n        '\n    assert all((hasattr(type(self), attr) for attr in kwargs.keys()))\n    self.__dict__.update(**kwargs)\n    if (self.stdin_encoding is None):\n        self.stdin_encoding = (getattr(self.stdin, 'encoding', None) or 'utf8')\n    if (self.stdout_encoding is None):\n        actual_stdout = self.stdout\n        if is_windows:\n            from colorama import AnsiToWin32\n            if isinstance(self.stdout, AnsiToWin32):\n                actual_stdout = self.stdout.wrapped\n        self.stdout_encoding = (getattr(actual_stdout, 'encoding', None) or 'utf8')\n", "label": 0}
{"function": "\n\ndef test_column_types(self):\n    df = self.alltypes.execute()\n    assert (df.tinyint_col.dtype.name == 'int8')\n    assert (df.smallint_col.dtype.name == 'int16')\n    assert (df.int_col.dtype.name == 'int32')\n    assert (df.bigint_col.dtype.name == 'int64')\n    assert (df.float_col.dtype.name == 'float32')\n    assert (df.double_col.dtype.name == 'float64')\n    assert pd.core.common.is_datetime64_dtype(df.timestamp_col.dtype)\n", "label": 0}
{"function": "\n\ndef test_getitem(session):\n    set_ = session.set(key('test_sortedset_getitem'), S('abc'), SortedSet)\n    assert (set_['a'] == 1)\n    assert (set_['b'] == 1)\n    assert (set_['c'] == 1)\n    with raises(KeyError):\n        set_['d']\n    with raises(TypeError):\n        set_[123]\n    set_.update(a=2.1, c=(- 2))\n    assert (set_['a'] == 3.1)\n    assert (set_['b'] == 1)\n    assert (set_['c'] == (- 1))\n    setx = session.set(key('test_sortedsetx_getitem'), S([1, 2, 3]), IntSet)\n    assert (setx[1] == 1)\n    assert (setx[2] == 1)\n    assert (setx[3] == 1)\n    with raises(KeyError):\n        setx[4]\n    with raises(TypeError):\n        setx['a']\n    setx.update({\n        1: 2.1,\n        3: (- 2),\n    })\n    assert (setx[1] == 3.1)\n    assert (setx[2] == 1)\n    assert (setx[3] == (- 1))\n", "label": 1}
{"function": "\n\ndef resolve_rosdep_key(key, os_name, os_version, ros_distro=None, ignored=None, retry=True):\n    ignored = (ignored or [])\n    ctx = create_default_installer_context()\n    try:\n        installer_key = ctx.get_default_os_installer_key(os_name)\n    except KeyError:\n        BloomGenerator.exit(\"Could not determine the installer for '{0}'\".format(os_name))\n    installer = ctx.get_installer(installer_key)\n    ros_distro = (ros_distro or DEFAULT_ROS_DISTRO)\n    view = get_view(os_name, os_version, ros_distro)\n    try:\n        return resolve_more_for_os(key, view, installer, os_name, os_version)\n    except (KeyError, ResolutionError) as exc:\n        debug(traceback.format_exc())\n        if (key in ignored):\n            return (None, None, None)\n        if isinstance(exc, KeyError):\n            error(\"Could not resolve rosdep key '{0}'\".format(key))\n            returncode = code.GENERATOR_NO_SUCH_ROSDEP_KEY\n        else:\n            error(\"Could not resolve rosdep key '{0}' for distro '{1}':\".format(key, os_version))\n            info(str(exc), use_prefix=False)\n            returncode = code.GENERATOR_NO_ROSDEP_KEY_FOR_DISTRO\n        if retry:\n            error('Try to resolve the problem with rosdep and then continue.')\n            if maybe_continue():\n                update_rosdep()\n                invalidate_view_cache()\n                return resolve_rosdep_key(key, os_name, os_version, ros_distro, ignored, retry=True)\n        BloomGenerator.exit(\"Failed to resolve rosdep key '{0}', aborting.\".format(key), returncode=returncode)\n", "label": 0}
{"function": "\n\n@login_required\ndef rule_new(request, testplan_id):\n    try:\n        testplan = TestPlan(auth_token=request.user.password).get(testplan_id)\n    except UnauthorizedException:\n        logger.warning('User unauthorized. Signing out...')\n        return signout(request)\n    except NotFoundException:\n        return render(request, '404.html')\n    except Exception as inst:\n        logger.error('Unexpected exception', exc_info=True)\n        messages.error(request, (inst.message if inst.message else 'Unexpected error'))\n        return HttpResponseRedirect(reverse('testplan_list'))\n    form = RuleForm((request.POST or None))\n    if form.is_valid():\n        try:\n            rule_id = Rule(testplan_id, auth_token=request.user.password).create(form.cleaned_data)\n            return HttpResponseRedirect(reverse('rule_details', args=(str(testplan_id), str(rule_id))))\n        except UnauthorizedException:\n            logger.warning('User unauthorized. Signing out...')\n            return signout(request)\n        except Exception as inst:\n            messages.error(request, (inst.message if inst.message else 'Unexpected error'))\n            return HttpResponseRedirect(reverse('rule_new', args=(str(testplan_id),)))\n    return render(request, 'rules/rule_new.html', {\n        'form': form,\n        'testplan': testplan,\n    })\n", "label": 0}
{"function": "\n\ndef test_update(self):\n    kc = KeyBundle([{\n        'kty': 'oct',\n        'key': 'supersecret',\n        'use': 'sig',\n    }])\n    assert (len(kc.get('oct')) == 1)\n    assert (len(kc.get('rsa')) == 0)\n    assert (kc.remote is False)\n    assert (kc.source is None)\n    kc.update()\n    assert (len(kc.get('oct')) == 1)\n    assert (len(kc.get('rsa')) == 0)\n    assert (kc.remote is False)\n    assert (kc.source is None)\n", "label": 0}
{"function": "\n\ndef __enter__(self):\n    if self.use_loopback:\n        platformsettings.setup_temporary_loopback_config()\n    if (self.init_cwnd != '0'):\n        platformsettings.set_temporary_tcp_init_cwnd(self.init_cwnd)\n    try:\n        ipfw_list = platformsettings.ipfw('list')\n        if (not ipfw_list.startswith('65535 ')):\n            logging.warn('ipfw has existing rules:\\n%s', ipfw_list)\n            self._delete_rules(ipfw_list)\n    except Exception:\n        pass\n    if ((self.up_bandwidth == '0') and (self.down_bandwidth == '0') and (self.delay_ms == '0') and (self.packet_loss_rate == '0')):\n        logging.info('Skipped shaping traffic.')\n        return\n    if (not self.ports):\n        raise TrafficShaperException('No ports on which to shape traffic.')\n    ports = ','.join((str(p) for p in self.ports))\n    half_delay_ms = (int(self.delay_ms) / 2)\n    try:\n        platformsettings.ipfw('pipe', self._UPLOAD_PIPE, 'config', 'bw', self.up_bandwidth, 'delay', half_delay_ms)\n        platformsettings.ipfw('queue', self._UPLOAD_QUEUE, 'config', 'pipe', self._UPLOAD_PIPE, 'plr', self.packet_loss_rate, 'queue', self._QUEUE_SLOTS, 'mask', 'src-port', '0xffff')\n        platformsettings.ipfw('add', self._UPLOAD_RULE, 'queue', self._UPLOAD_QUEUE, 'ip', 'from', 'any', 'to', self.host, ((self.use_loopback and 'out') or 'in'), 'dst-port', ports)\n        self.is_shaping = True\n        platformsettings.ipfw('pipe', self._DOWNLOAD_PIPE, 'config', 'bw', self.down_bandwidth, 'delay', half_delay_ms)\n        platformsettings.ipfw('queue', self._DOWNLOAD_QUEUE, 'config', 'pipe', self._DOWNLOAD_PIPE, 'plr', self.packet_loss_rate, 'queue', self._QUEUE_SLOTS, 'mask', 'dst-port', '0xffff')\n        platformsettings.ipfw('add', self._DOWNLOAD_RULE, 'queue', self._DOWNLOAD_QUEUE, 'ip', 'from', self.host, 'to', 'any', 'out', 'src-port', ports)\n        logging.info('Started shaping traffic')\n    except Exception:\n        logging.error('Unable to shape traffic.')\n        raise\n", "label": 1}
{"function": "\n\ndef test_operations():\n    per = SeqPer((1, 2), (n, 0, oo))\n    per2 = SeqPer((2, 4), (n, 0, oo))\n    form = SeqFormula((n ** 2))\n    form2 = SeqFormula((n ** 3))\n    assert (((per + form) + form2) == SeqAdd(per, form, form2))\n    assert (((per + form) - form2) == SeqAdd(per, form, (- form2)))\n    assert (((per + form) - S.EmptySequence) == SeqAdd(per, form))\n    assert (((per + per2) + form) == SeqAdd(SeqPer((3, 6), (n, 0, oo)), form))\n    assert ((S.EmptySequence - per) == (- per))\n    assert ((form + form) == SeqFormula((2 * (n ** 2))))\n    assert (((per * form) * form2) == SeqMul(per, form, form2))\n    assert ((form * form) == SeqFormula((n ** 4)))\n    assert ((form * (- form)) == SeqFormula((- (n ** 4))))\n    assert ((form * (per + form2)) == SeqMul(form, SeqAdd(per, form2)))\n    assert ((form * (per + per)) == SeqMul(form, per2))\n    assert (form.coeff_mul(m) == SeqFormula((m * (n ** 2)), (n, 0, oo)))\n    assert (per.coeff_mul(m) == SeqPer((m, (2 * m)), (n, 0, oo)))\n", "label": 1}
{"function": "\n\ndef rAssertAlmostEqual(self, a, b, rel_err=2e-15, abs_err=5e-323, msg=None):\n    'Fail if the two floating-point numbers are not almost equal.\\n\\n        Determine whether floating-point values a and b are equal to within\\n        a (small) rounding error.  The default values for rel_err and\\n        abs_err are chosen to be suitable for platforms where a float is\\n        represented by an IEEE 754 double.  They allow an error of between\\n        9 and 19 ulps.\\n        '\n    if math.isnan(a):\n        if math.isnan(b):\n            return\n        self.fail((msg or '{!r} should be nan'.format(b)))\n    if math.isinf(a):\n        if (a == b):\n            return\n        self.fail((msg or 'finite result where infinity expected: expected {!r}, got {!r}'.format(a, b)))\n    if ((not a) and (not b)):\n        if (math.copysign(1.0, a) != math.copysign(1.0, b)):\n            self.fail((msg or 'zero has wrong sign: expected {!r}, got {!r}'.format(a, b)))\n    try:\n        absolute_error = abs((b - a))\n    except OverflowError:\n        pass\n    else:\n        if (absolute_error <= max(abs_err, (rel_err * abs(a)))):\n            return\n    self.fail((msg or '{!r} and {!r} are not sufficiently close'.format(a, b)))\n", "label": 1}
{"function": "\n\ndef resolve_redirects(self, resp, req, stream=False, timeout=None, verify=True, cert=None, proxies=None):\n    'Receives a Response. Returns a generator of Responses.'\n    i = 0\n    while (('location' in resp.headers) and (resp.status_code in REDIRECT_STATI)):\n        prepared_request = req.copy()\n        resp.content\n        if (i >= self.max_redirects):\n            raise TooManyRedirects(('Exceeded %s redirects.' % self.max_redirects))\n        resp.close()\n        url = resp.headers['location']\n        method = req.method\n        if url.startswith('//'):\n            parsed_rurl = urlparse(resp.url)\n            url = ('%s:%s' % (parsed_rurl.scheme, url))\n        parsed = urlparse(url)\n        url = parsed.geturl()\n        if (not urlparse(url).netloc):\n            url = urljoin(resp.url, requote_uri(url))\n        else:\n            url = requote_uri(url)\n        prepared_request.url = url\n        if ((resp.status_code == codes.see_other) and (method != 'HEAD')):\n            method = 'GET'\n        if ((resp.status_code == codes.found) and (method != 'HEAD')):\n            method = 'GET'\n        if ((resp.status_code == codes.moved) and (method == 'POST')):\n            method = 'GET'\n        prepared_request.method = method\n        if (resp.status_code not in (codes.temporary, codes.resume)):\n            if ('Content-Length' in prepared_request.headers):\n                del prepared_request.headers['Content-Length']\n            prepared_request.body = None\n        headers = prepared_request.headers\n        try:\n            del headers['Cookie']\n        except KeyError:\n            pass\n        extract_cookies_to_jar(prepared_request._cookies, prepared_request, resp.raw)\n        prepared_request._cookies.update(self.cookies)\n        prepared_request.prepare_cookies(prepared_request._cookies)\n        resp = self.send(prepared_request, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies, allow_redirects=False)\n        extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)\n        i += 1\n        (yield resp)\n", "label": 1}
{"function": "\n\ndef test_external_proxy(request, io_loop):\n    'Test a proxy started before the Hub'\n    auth_token = 'secret!'\n    proxy_ip = '127.0.0.1'\n    proxy_port = 54321\n    app = MockHub.instance(proxy_api_ip=proxy_ip, proxy_api_port=proxy_port, proxy_auth_token=auth_token)\n\n    def fin():\n        MockHub.clear_instance()\n        app.stop()\n    request.addfinalizer(fin)\n    env = os.environ.copy()\n    env['CONFIGPROXY_AUTH_TOKEN'] = auth_token\n    cmd = (app.proxy_cmd + ['--ip', app.ip, '--port', str(app.port), '--api-ip', proxy_ip, '--api-port', str(proxy_port), '--default-target', ('http://%s:%i' % (app.hub_ip, app.hub_port))])\n    if app.subdomain_host:\n        cmd.append('--host-routing')\n    proxy = Popen(cmd, env=env)\n\n    def _cleanup_proxy():\n        if (proxy.poll() is None):\n            proxy.terminate()\n    request.addfinalizer(_cleanup_proxy)\n\n    def wait_for_proxy():\n        io_loop.run_sync((lambda : wait_for_http_server(('http://%s:%i' % (proxy_ip, proxy_port)))))\n    wait_for_proxy()\n    app.start([])\n    assert (app.proxy_process is None)\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    assert (list(routes.keys()) == ['/'])\n    name = 'river'\n    r = api_request(app, 'users', name, method='post')\n    r.raise_for_status()\n    r = api_request(app, 'users', name, 'server', method='post')\n    r.raise_for_status()\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    user_path = '/user/river'\n    if app.subdomain_host:\n        domain = urlparse(app.subdomain_host).hostname\n        user_path = (('/%s.%s' % (name, domain)) + user_path)\n    assert (sorted(routes.keys()) == ['/', user_path])\n    proxy.terminate()\n    proxy = Popen(cmd, env=env)\n    wait_for_proxy()\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    assert (list(routes.keys()) == ['/'])\n    r = api_request(app, 'proxy', method='post')\n    r.raise_for_status()\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    assert (sorted(routes.keys()) == ['/', user_path])\n    proxy.terminate()\n    new_auth_token = 'different!'\n    env['CONFIGPROXY_AUTH_TOKEN'] = new_auth_token\n    proxy_port = 55432\n    cmd = (app.proxy_cmd + ['--ip', app.ip, '--port', str(app.port), '--api-ip', app.proxy_api_ip, '--api-port', str(proxy_port), '--default-target', ('http://%s:%i' % (app.hub_ip, app.hub_port))])\n    if app.subdomain_host:\n        cmd.append('--host-routing')\n    proxy = Popen(cmd, env=env)\n    wait_for_proxy()\n    r = api_request(app, 'proxy', method='patch', data=json.dumps({\n        'port': proxy_port,\n        'protocol': 'http',\n        'ip': app.ip,\n        'auth_token': new_auth_token,\n    }))\n    r.raise_for_status()\n    assert (app.proxy.api_server.port == proxy_port)\n\n    def get_app_proxy_token():\n        q = Queue()\n        app.io_loop.add_callback((lambda : q.put(app.proxy.auth_token)))\n        return q.get(timeout=2)\n    assert (get_app_proxy_token() == new_auth_token)\n    app.proxy.auth_token = new_auth_token\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    assert (sorted(routes.keys()) == ['/', user_path])\n", "label": 1}
{"function": "\n\ndef test_fetchable_get_item():\n    call_counter = mock.Mock()\n\n    def fetch():\n        for i in range(3):\n            for i in range(5):\n                (yield i)\n            call_counter()\n    fetcher = mock.Mock(fetch=fetch)\n    fetchable = util.Fetchable(fetcher)\n    assert (fetchable[:2] == [0, 1])\n    assert (fetchable[:3] == [0, 1, 2])\n    assert (fetchable[3] == 3)\n    assert (call_counter.call_count == 0)\n    assert (fetchable[9] == 4)\n    assert (call_counter.call_count == 1)\n    assert (fetchable[2:3] == [2])\n    assert (fetchable[:] == fetchable[:])\n    assert (len(fetchable[:]) == 15)\n    assert (fetchable[:100000] == fetchable[:])\n    assert (fetchable[100000:] == [])\n    assert (fetchable[100000:10000000] == [])\n    with pytest.raises(IndexError):\n        fetchable[12312312]\n", "label": 1}
{"function": "\n\ndef copyfile(source, dest, backup_mode='', cachedir=''):\n    '\\n    Copy files from a source to a destination in an atomic way, and if\\n    specified cache the file.\\n    '\n    if (not os.path.isfile(source)):\n        raise IOError('[Errno 2] No such file or directory: {0}'.format(source))\n    if (not os.path.isdir(os.path.dirname(dest))):\n        raise IOError('[Errno 2] No such file or directory: {0}'.format(dest))\n    bname = os.path.basename(dest)\n    dname = os.path.dirname(os.path.abspath(dest))\n    tgt = salt.utils.mkstemp(prefix=bname, dir=dname)\n    shutil.copyfile(source, tgt)\n    bkroot = ''\n    if cachedir:\n        bkroot = os.path.join(cachedir, 'file_backup')\n    if ((backup_mode == 'minion') or ((backup_mode == 'both') and bkroot)):\n        if os.path.exists(dest):\n            salt.utils.backup_minion(dest, bkroot)\n    if ((backup_mode == 'master') or ((backup_mode == 'both') and bkroot)):\n        pass\n    fstat = None\n    if (not salt.utils.is_windows()):\n        try:\n            fstat = os.stat(dest)\n        except OSError:\n            pass\n    shutil.move(tgt, dest)\n    if (fstat is not None):\n        os.chown(dest, fstat.st_uid, fstat.st_gid)\n        os.chmod(dest, fstat.st_mode)\n    rcon = salt.utils.which('restorecon')\n    if rcon:\n        policy = False\n        try:\n            policy = salt.modules.selinux.getenforce()\n        except (ImportError, CommandExecutionError):\n            pass\n        if (policy == 'Enforcing'):\n            with salt.utils.fopen(os.devnull, 'w') as dev_null:\n                cmd = [rcon, dest]\n                subprocess.call(cmd, stdout=dev_null, stderr=dev_null)\n    if os.path.isfile(tgt):\n        try:\n            os.remove(tgt)\n        except Exception:\n            pass\n", "label": 1}
{"function": "\n\ndef handle(self, **options):\n    database = options['database']\n    connection = connections[database]\n    verbosity = options['verbosity']\n    interactive = options['interactive']\n    reset_sequences = options.get('reset_sequences', True)\n    allow_cascade = options.get('allow_cascade', False)\n    inhibit_post_migrate = options.get('inhibit_post_migrate', False)\n    self.style = no_style()\n    for app_config in apps.get_app_configs():\n        try:\n            import_module('.management', app_config.name)\n        except ImportError:\n            pass\n    sql_list = sql_flush(self.style, connection, only_django=True, reset_sequences=reset_sequences, allow_cascade=allow_cascade)\n    if interactive:\n        confirm = input((\"You have requested a flush of the database.\\nThis will IRREVERSIBLY DESTROY all data currently in the %r database,\\nand return each table to an empty state.\\nAre you sure you want to do this?\\n\\n    Type 'yes' to continue, or 'no' to cancel: \" % connection.settings_dict['NAME']))\n    else:\n        confirm = 'yes'\n    if (confirm == 'yes'):\n        try:\n            with transaction.atomic(using=database, savepoint=connection.features.can_rollback_ddl):\n                with connection.cursor() as cursor:\n                    for sql in sql_list:\n                        cursor.execute(sql)\n        except Exception as e:\n            new_msg = (\"Database %s couldn't be flushed. Possible reasons:\\n  * The database isn't running or isn't configured correctly.\\n  * At least one of the expected database tables doesn't exist.\\n  * The SQL was invalid.\\nHint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.\\nThe full error: %s\" % (connection.settings_dict['NAME'], e))\n            six.reraise(CommandError, CommandError(new_msg), sys.exc_info()[2])\n        if (sql_list and (not inhibit_post_migrate)):\n            emit_post_migrate_signal(verbosity, interactive, database)\n    else:\n        self.stdout.write('Flush cancelled.\\n')\n", "label": 0}
{"function": "\n\ndef testMemory(self):\n    global logger\n    res = Memory(logger).check({\n        \n    })\n    if Platform.is_linux():\n        MEM_METRICS = ['swapTotal', 'swapFree', 'swapPctFree', 'swapUsed', 'physTotal', 'physFree', 'physUsed', 'physBuffers', 'physCached', 'physUsable', 'physPctUsable', 'physShared']\n        for k in MEM_METRICS:\n            if ((k == 'swapPctFree') and (res['swapTotal'] == 0)):\n                continue\n            assert (k in res), res\n        assert (res['swapTotal'] == (res['swapFree'] + res['swapUsed']))\n        assert (res['physTotal'] == (res['physFree'] + res['physUsed']))\n    elif (sys.platform == 'darwin'):\n        for k in ('swapFree', 'swapUsed', 'physFree', 'physUsed'):\n            assert (k in res), res\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    self.login_default_admin()\n    project = self.create_project()\n    plan = self.create_plan(project, label='Foo')\n    step = self.create_step(plan=plan)\n    self.login_default_admin()\n    path = '/api/0/steps/{0}/'.format(step.id.hex)\n    resp = self.client.post(path, data={\n        'order': 1,\n        'implementation': 'changes.buildsteps.dummy.DummyBuildStep',\n        'data': '{}',\n        'build.timeout': '1',\n    })\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (data['data'] == '{}')\n    assert (data['order'] == 1)\n    assert (data['implementation'] == 'changes.buildsteps.dummy.DummyBuildStep')\n    assert (data['options'] == {\n        'build.timeout': '1',\n    })\n    db.session.expire(step)\n    step = Step.query.get(step.id)\n    assert (step.data == {\n        \n    })\n    assert (step.order == 1)\n    assert (step.implementation == 'changes.buildsteps.dummy.DummyBuildStep')\n    options = list(ItemOption.query.filter((ItemOption.item_id == step.id)))\n    assert (len(options) == 1)\n    assert (options[0].name == 'build.timeout')\n    assert (options[0].value == '1')\n", "label": 1}
{"function": "\n\ndef removeDupsAndExcl(mergeFile):\n    numberOfRules = settings['numberofrules']\n    if os.path.isfile(settings['whitelistfile']):\n        with open(settings['whitelistfile'], 'r') as ins:\n            for line in ins:\n                line = line.strip(' \\t\\n\\r')\n                if (line and (not line.startswith('#'))):\n                    settings['exclusions'].append(line)\n    if (not os.path.exists(settings['outputpath'])):\n        os.makedirs(settings['outputpath'])\n    finalFile = open(os.path.join(settings['outputpath'], 'hosts'), ('w+b' if Python3 else 'w+'))\n    mergeFile.seek(0)\n    hostnames = set()\n    hostnames.add('localhost')\n    hostnames.add('localhost.localdomain')\n    hostnames.add('local')\n    hostnames.add('broadcasthost')\n    exclusions = settings['exclusions']\n    for line in mergeFile.readlines():\n        write = 'true'\n        line = line.decode('UTF-8')\n        line = line.replace('\\t+', ' ')\n        line = (line.rstrip() + '\\n')\n        if ((line[0] == '#') or re.match('^\\\\s*$', line[0])):\n            writeData(finalFile, line)\n            continue\n        if ('::1' in line):\n            continue\n        strippedRule = stripRule(line)\n        if ((not strippedRule) or matchesExclusions(strippedRule)):\n            continue\n        (hostname, normalizedRule) = normalizeRule(strippedRule)\n        for exclude in exclusions:\n            if (exclude in line):\n                write = 'false'\n                break\n        if (normalizedRule and (hostname not in hostnames) and (write == 'true')):\n            writeData(finalFile, normalizedRule)\n            hostnames.add(hostname)\n            numberOfRules += 1\n    settings['numberofrules'] = numberOfRules\n    mergeFile.close()\n    return finalFile\n", "label": 1}
{"function": "\n\ndef ValidateSyntax(self):\n    'Validate artifact syntax.\\n\\n    This method can be used to validate individual artifacts as they are loaded,\\n    without needing all artifacts to be loaded first, as for Validate().\\n\\n    Raises:\\n      ArtifactDefinitionError: If artifact is invalid.\\n    '\n    cls_name = self.name\n    if (not self.doc):\n        raise ArtifactDefinitionError(('Artifact %s has missing doc' % cls_name))\n    for supp_os in self.supported_os:\n        if (supp_os not in self.SUPPORTED_OS_LIST):\n            raise ArtifactDefinitionError(('Artifact %s has invalid supported_os %s' % (cls_name, supp_os)))\n    for condition in self.conditions:\n        try:\n            of = objectfilter.Parser(condition).Parse()\n            of.Compile(objectfilter.BaseFilterImplementation)\n        except ConditionError as e:\n            raise ArtifactDefinitionError(('Artifact %s has invalid condition %s. %s' % (cls_name, condition, e)))\n    for label in self.labels:\n        if (label not in self.ARTIFACT_LABELS):\n            raise ArtifactDefinitionError(('Artifact %s has an invalid label %s. Please use one from ARTIFACT_LABELS.' % (cls_name, label)))\n    valid_provides = rdf_client.KnowledgeBase().GetKbFieldNames()\n    for kb_var in self.provides:\n        if (kb_var not in valid_provides):\n            raise ArtifactDefinitionError((\"Artifact %s has broken provides: '%s' not in KB fields: %s\" % (cls_name, kb_var, valid_provides)))\n    for dep in self.GetArtifactPathDependencies():\n        if (dep not in valid_provides):\n            raise ArtifactDefinitionError((\"Artifact %s has an invalid path dependency: '%s', not in KB fields: %s\" % (cls_name, dep, valid_provides)))\n    for source in self.sources:\n        try:\n            source.Validate()\n        except Error as e:\n            raise ArtifactDefinitionError(('Artifact %s has bad source. %s' % (cls_name, e)))\n", "label": 1}
{"function": "\n\ndef __init__(self, records, fuel_type, unit_name, record_type='interval', freq=None, pulse_value=None, name=None, data=None, estimated=None):\n    if (unit_name in self.UNITS):\n        self.unit_name = self.UNITS[unit_name]['target_unit']\n        multiplier = self.UNITS[unit_name]['multiplier']\n    else:\n        message = 'Unsupported unit name: \"{}\".'.format(unit_name)\n        raise ValueError(message)\n    if (fuel_type not in ['electricity', 'natural_gas', 'fuel_oil', 'propane', 'liquid_propane', 'kerosene', 'diesel', 'fuel_cell']):\n        message = 'Unsupported fuel type: \"{}\".'.format(fuel_type)\n        raise ValueError(message)\n    else:\n        self.fuel_type = fuel_type\n    self.freq = freq\n    self.pulse_value = pulse_value\n    self.name = name\n    if (data is not None):\n        if (not (records is None)):\n            message = 'Please provide either data or records, but not both.'\n            raise ValueError(message)\n        if (estimated is None):\n            message = 'Please provide the the `estimated` attribute, which contains boolean values indicating whether or not the data is estimated. Should have the same index as `data`'\n            raise ValueError(message)\n        self.data = data\n        self.estimated = estimated\n        if (freq is None):\n            self.freq_timedelta = None\n        elif (freq[(- 1)] not in ['D', 'H', 'T', 'S']):\n            message = 'Invalid frequency specification: \"{}\".'.format(freq)\n            raise ValueError(message)\n        else:\n            try:\n                dummy_start_date = datetime(1970, 1, 1, tzinfo=pytz.utc)\n                dummy_date_range = pd.date_range(dummy_start_date, periods=2, freq=freq)\n                freq_timedelta = (dummy_date_range[1] - dummy_date_range[0])\n            except ValueError:\n                message = 'Invalid frequency specification: \"{}\".'.format(freq)\n                raise ValueError(message)\n            self.freq_timedelta = freq_timedelta\n        return\n    if ('interval' == record_type):\n        if ((freq is None) or (freq[(- 1)] not in ['D', 'H', 'T', 'S'])):\n            message = 'Invalid frequency specification: \"{}\".'.format(freq)\n            raise ValueError(message)\n        else:\n            try:\n                dummy_start_date = datetime(1970, 1, 1, tzinfo=pytz.utc)\n                dummy_date_range = pd.date_range(dummy_start_date, periods=2, freq=freq)\n                freq_timedelta = (dummy_date_range[1] - dummy_date_range[0])\n            except ValueError:\n                message = 'Invalid frequency specification: \"{}\".'.format(freq)\n                raise ValueError(message)\n        self.freq_timedelta = freq_timedelta\n        (self.data, self.estimated) = self._import_interval(records)\n    elif (record_type in ['arbitrary', 'billing']):\n        self.freq_timedelta = None\n        (self.data, self.estimated) = self._import_arbitrary(records)\n    elif (record_type in ['arbitrary_start', 'billing_start']):\n        self.freq_timedelta = None\n        (self.data, self.estimated) = self._import_arbitrary_start(records)\n    elif (record_type in ['arbitrary_end', 'billing_end']):\n        self.freq_timedelta = None\n        (self.data, self.estimated) = self._import_arbitrary_end(records)\n    elif ('pulse' == record_type):\n        if ((pulse_value is None) or (pulse_value <= 0)):\n            message = 'Expected pulse_value to be a positive float, but got {} instead.'.format(pulse_value)\n            raise ValueError(message)\n        self.freq_timedelta = None\n        (self.data, self.estimated) = self._import_pulse(records)\n    else:\n        message('Invalid record_type: \"{}\".'.format(record_type))\n        raise ValueError\n    self.data *= multiplier\n", "label": 1}
{"function": "\n\ndef rs_tan(p, x, prec):\n    \"\\n    Tangent of a series.\\n\\n    Return the series expansion of the tan of ``p``, about 0.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_tan\\n    >>> R, x, y = ring('x, y', QQ)\\n    >>> rs_tan(x + x*y, x, 4)\\n    1/3*x**3*y**3 + x**3*y**2 + x**3*y + 1/3*x**3 + x*y + x\\n\\n   See Also\\n   ========\\n\\n   _tan1, tan\\n   \"\n    if rs_is_puiseux(p, x):\n        r = rs_puiseux(rs_tan, p, x, prec)\n        return r\n    R = p.ring\n    const = 0\n    c = _get_constant_term(p, x)\n    if c:\n        if (R.domain is EX):\n            c_expr = c.as_expr()\n            const = tan(c_expr)\n        elif isinstance(c, PolyElement):\n            try:\n                c_expr = c.as_expr()\n                const = R(tan(c_expr))\n            except ValueError:\n                R = R.add_gens([tan(c_expr)])\n                p = p.set_ring(R)\n                x = x.set_ring(R)\n                c = c.set_ring(R)\n                const = R(tan(c_expr))\n        else:\n            try:\n                const = R(tan(c))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        p1 = (p - c)\n        t2 = rs_tan(p1, x, prec)\n        t = rs_series_inversion((1 - (const * t2)), x, prec)\n        return rs_mul((const + t2), t, x, prec)\n    if (R.ngens == 1):\n        return _tan1(p, x, prec)\n    else:\n        return rs_fun(p, rs_tan, x, prec)\n", "label": 0}
{"function": "\n\ndef __init__(self, im):\n    data = None\n    colortable = None\n    if hasattr(im, 'toUtf8'):\n        im = unicode(im.toUtf8(), 'utf-8')\n    if Image.isStringType(im):\n        im = Image.open(im)\n    if (im.mode == '1'):\n        format = QImage.Format_Mono\n    elif (im.mode == 'L'):\n        format = QImage.Format_Indexed8\n        colortable = []\n        for i in range(256):\n            colortable.append(rgb(i, i, i))\n    elif (im.mode == 'P'):\n        format = QImage.Format_Indexed8\n        colortable = []\n        palette = im.getpalette()\n        for i in range(0, len(palette), 3):\n            colortable.append(rgb(*palette[i:(i + 3)]))\n    elif (im.mode == 'RGB'):\n        data = im.tostring('raw', 'BGRX')\n        format = QImage.Format_RGB32\n    elif (im.mode == 'RGBA'):\n        try:\n            data = im.tostring('raw', 'BGRA')\n        except SystemError:\n            (r, g, b, a) = im.split()\n            im = Image.merge('RGBA', (b, g, r, a))\n        format = QImage.Format_ARGB32\n    else:\n        raise ValueError(('unsupported image mode %r' % im.mode))\n    self.__data = (data or im.tostring())\n    QImage.__init__(self, self.__data, im.size[0], im.size[1], format)\n    if colortable:\n        self.setColorTable(colortable)\n", "label": 1}
{"function": "\n\ndef _parse_headers(self, data):\n    idx = data.find(b('\\r\\n\\r\\n'))\n    if (idx < 0):\n        return False\n    lines = [(bytes_to_str(line) + '\\r\\n') for line in data[:idx].split(b('\\r\\n'))]\n    while len(lines):\n        curr = lines.pop(0)\n        if (curr.find(':') < 0):\n            raise InvalidHeader(('invalid line %s' % curr.strip()))\n        (name, value) = curr.split(':', 1)\n        name = name.rstrip(' \\t').upper()\n        if HEADER_RE.search(name):\n            raise InvalidHeader(('invalid header name %s' % name))\n        (name, value) = (name.strip(), [value.lstrip()])\n        while (len(lines) and lines[0].startswith((' ', '\\t'))):\n            value.append(lines.pop(0))\n        value = ''.join(value).rstrip()\n        self._headers.add_header(name, value)\n        key = ('HTTP_%s' % name.upper().replace('-', '_'))\n        self._environ[key] = value\n    clen = self._headers.get('content-length')\n    te = self._headers.get('transfer-encoding', '').lower()\n    if (clen is not None):\n        try:\n            self._clen_rest = self._clen = int(clen)\n        except ValueError:\n            pass\n    else:\n        self._chunked = (te == 'chunked')\n        if (not self._chunked):\n            self._clen_rest = MAXSIZE\n    encoding = self._headers.get('content-encoding')\n    if self.decompress:\n        if (encoding == 'gzip'):\n            self.__decompress_obj = zlib.decompressobj((16 + zlib.MAX_WBITS))\n        elif (encoding == 'deflate'):\n            self.__decompress_obj = zlib.decompressobj()\n    rest = data[(idx + 4):]\n    self._buf = [rest]\n    self.__on_headers_complete = True\n    return len(rest)\n", "label": 1}
{"function": "\n\ndef cli_delete(context, path, body=None, recursive=False, yes_empty_account=False, yes_delete_account=False, until_empty=False):\n    \"\\n    Deletes the item (account, container, or object) at the path.\\n\\n    See :py:mod:`swiftly.cli.delete` for context usage information.\\n\\n    See :py:class:`CLIDelete` for more information.\\n\\n    :param context: The :py:class:`swiftly.cli.context.CLIContext` to\\n        use.\\n    :param path: The path of the item (acount, container, or object)\\n        to delete.\\n    :param body: The body to send with the DELETE request. Bodies are\\n        not normally sent with DELETE requests, but this can be\\n        useful with bulk deletes for instance.\\n    :param recursive: If True and the item is an account or\\n        container, deletes will be issued for any containing items as\\n        well. This does one pass at the deletion; so if objects revert\\n        to previous versions or if new objects otherwise arise during\\n        the process, the container(s) may not be empty once done. Set\\n        `until_empty` to True if you want multiple passes to keep trying\\n        to fully empty the containers.\\n    :param until_empty: If True and recursive is True, this will cause\\n        Swiftly to keep looping through the deletes until the containers\\n        are completely empty. Useful if you have object versioning\\n        turned on or otherwise have objects that seemingly reappear\\n        after being deleted. It could also run forever if you have\\n        something that's uploading objects at a faster rate than they\\n        are deleted.\\n    :param yes_empty_account: This must be set to True for\\n        verification when the item is an account and recursive is\\n        True.\\n    :param yes_delete_account: This must be set to True for\\n        verification when the item is an account and you really wish\\n        a delete to be issued for the account itself.\\n    \"\n    path = (path.lstrip('/') if path else '')\n    if (not path):\n        if yes_empty_account:\n            cli_empty_account(context, yes_empty_account=yes_empty_account, until_empty=until_empty)\n        if yes_delete_account:\n            with context.client_manager.with_client() as client:\n                (status, reason, headers, contents) = client.delete_account(headers=context.headers, query=context.query, cdn=context.cdn, body=body, yes_i_mean_delete_the_account=yes_delete_account)\n                if ((status // 100) != 2):\n                    if ((status == 404) and context.ignore_404):\n                        return\n                    raise ReturnCode(('deleting account: %s %s' % (status, reason)))\n    elif ('/' not in path.rstrip('/')):\n        path = path.rstrip('/')\n        if recursive:\n            cli_empty_container(context, path, until_empty=until_empty)\n        with context.client_manager.with_client() as client:\n            (status, reason, headers, contents) = client.delete_container(path, headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n            if ((status // 100) != 2):\n                if ((status == 404) and context.ignore_404):\n                    return\n                raise ReturnCode(('deleting container %r: %s %s' % (path, status, reason)))\n    else:\n        with context.client_manager.with_client() as client:\n            (status, reason, headers, contents) = client.delete_object(*path.split('/', 1), headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n            if ((status // 100) != 2):\n                if ((status == 404) and context.ignore_404):\n                    return\n                raise ReturnCode(('deleting object %r: %s %s' % (path, status, reason)))\n", "label": 1}
{"function": "\n\ndef test_get_customer_list(self):\n    CustomerFactory.create_batch(40)\n    url = reverse('customer-list')\n    response = self.client.get(url)\n    full_url = None\n    for field in response.data:\n        full_url = field.get('url', None)\n        if full_url:\n            break\n    if full_url:\n        domain = full_url.split('/')[2]\n        full_url = ((full_url.split(domain)[0] + domain) + url)\n    assert (response.status_code == status.HTTP_200_OK)\n    assert (response._headers['link'] == ('Link', (((((((('<' + full_url) + '?page=2; rel=\"next\">, ') + '<') + full_url) + '?page=1; rel=\"first\">, ') + '<') + full_url) + '?page=2; rel=\"last\">')))\n    response = self.client.get((url + '?page=2'))\n    assert (response.status_code == status.HTTP_200_OK)\n    assert (response._headers['link'] == ('Link', (((((((('<' + full_url) + '; rel=\"prev\">, ') + '<') + full_url) + '?page=1; rel=\"first\">, ') + '<') + full_url) + '?page=2; rel=\"last\">')))\n", "label": 0}
{"function": "\n\ndef test_cardinality_qk():\n    rules = {\n        'max_cardinality': 2,\n        'timeframe': datetime.timedelta(minutes=10),\n        'cardinality_field': 'foo',\n        'timestamp_field': '@timestamp',\n        'query_key': 'user',\n    }\n    rule = CardinalityRule(rules)\n    users = ['foo', 'bar', 'baz']\n    for user in users:\n        event = {\n            '@timestamp': datetime.datetime.now(),\n            'user': user,\n            'foo': ('foo' + user),\n        }\n        rule.add_data([event])\n        assert (len(rule.matches) == 0)\n    rule.garbage_collect(datetime.datetime.now())\n    values = ['faz', 'fuz', 'fiz']\n    for value in values:\n        event = {\n            '@timestamp': (datetime.datetime.now() + datetime.timedelta(minutes=5)),\n            'user': 'baz',\n            'foo': value,\n        }\n        rule.add_data([event])\n    rule.garbage_collect((datetime.datetime.now() + datetime.timedelta(minutes=5)))\n    assert (len(rule.matches) == 2)\n    assert (rule.matches[0]['user'] == 'baz')\n    assert (rule.matches[1]['user'] == 'baz')\n    assert (rule.matches[0]['foo'] == 'fuz')\n    assert (rule.matches[1]['foo'] == 'fiz')\n", "label": 0}
{"function": "\n\ndef pre_process_json(obj):\n    '\\n    Preprocess items in a dictionary or list and prepare them to be json serialized.\\n    '\n    if (type(obj) is dict):\n        new_dict = {\n            \n        }\n        for (key, value) in obj.items():\n            new_dict[key] = pre_process_json(value)\n        return new_dict\n    elif (type(obj) is list):\n        new_list = []\n        for item in obj:\n            new_list.append(pre_process_json(item))\n        return new_list\n    elif hasattr(obj, 'todict'):\n        return dict(obj.todict())\n    else:\n        try:\n            json.dumps(obj)\n        except TypeError:\n            try:\n                json.dumps(obj.__dict__)\n            except TypeError:\n                return str(obj)\n            else:\n                return obj.__dict__\n        else:\n            return obj\n", "label": 0}
{"function": "\n\ndef handle_save_embeds(sender, instance, **kwargs):\n    embedded_media_fields = FieldRegistry.get_fields(sender)\n    if (not embedded_media_fields):\n        return\n    urls = []\n    for field in instance._meta.fields:\n        if isinstance(field, TextField):\n            urls.extend(re.findall(URL_RE, getattr(instance, field.name)))\n    urls = set(urls)\n    for embedded_field in embedded_media_fields:\n        m2m = getattr(instance, embedded_field.name)\n        m2m.clear()\n        for url in urls:\n            try:\n                provider = oembed.site.provider_for_url(url)\n            except OEmbedMissingEndpoint:\n                pass\n            else:\n                if ((not embedded_field.media_type) or (provider.resource_type in embedded_field.media_type)):\n                    (media_obj, created) = AggregateMedia.objects.get_or_create(url=url)\n                    m2m.add(media_obj)\n", "label": 0}
{"function": "\n\ndef get_ordering(self, request, queryset, view):\n    '\\n        Return a tuple of strings, that may be used in an `order_by` method.\\n        '\n    ordering_filters = [filter_cls for filter_cls in getattr(view, 'filter_backends', []) if hasattr(filter_cls, 'get_ordering')]\n    if ordering_filters:\n        filter_cls = ordering_filters[0]\n        filter_instance = filter_cls()\n        ordering = filter_instance.get_ordering(request, queryset, view)\n        assert (ordering is not None), 'Using cursor pagination, but filter class {filter_cls} returned a `None` ordering.'.format(filter_cls=filter_cls.__name__)\n    else:\n        ordering = self.ordering\n        assert (ordering is not None), 'Using cursor pagination, but no ordering attribute was declared on the pagination class.'\n        assert ('__' not in ordering), 'Cursor pagination does not support double underscore lookups for orderings. Orderings should be an unchanging, unique or nearly-unique field on the model, such as \"-created\" or \"pk\".'\n    assert isinstance(ordering, (six.string_types, list, tuple)), 'Invalid ordering. Expected string or tuple, but got {type}'.format(type=type(ordering).__name__)\n    if isinstance(ordering, six.string_types):\n        return (ordering,)\n    return tuple(ordering)\n", "label": 0}
{"function": "\n\ndef _visit_goal(self, goal, context, goal_info_by_goal, target_roots_replacement):\n    if (goal in goal_info_by_goal):\n        return\n    tasktypes_by_name = OrderedDict()\n    goal_dependencies = set()\n    visited_task_types = set()\n    for task_name in reversed(goal.ordered_task_names()):\n        task_type = goal.task_type_by_name(task_name)\n        tasktypes_by_name[task_name] = task_type\n        visited_task_types.add(task_type)\n        alternate_target_roots = task_type._alternate_target_roots(context.options, context.address_mapper, context.build_graph)\n        target_roots_replacement.propose_alternates(task_type, alternate_target_roots)\n        round_manager = RoundManager(context)\n        task_type._prepare(context.options, round_manager)\n        try:\n            dependencies = round_manager.get_dependencies()\n            for producer_info in dependencies:\n                producer_goal = producer_info.goal\n                if (producer_goal == goal):\n                    if (producer_info.task_type == task_type):\n                        pass\n                    elif (producer_info.task_type in visited_task_types):\n                        ordering = '\\n\\t'.join((\"[{0}] '{1}' {2}\".format(i, tn, goal.task_type_by_name(tn).__name__) for (i, tn) in enumerate(goal.ordered_task_names())))\n                        raise self.TaskOrderError(\"TaskRegistrar '{name}' with action {consumer_task} depends on {data} from task {producer_task} which is ordered after it in the '{goal}' goal:\\n\\t{ordering}\".format(name=task_name, consumer_task=task_type.__name__, data=producer_info.product_type, producer_task=producer_info.task_type.__name__, goal=goal.name, ordering=ordering))\n                    else:\n                        pass\n                else:\n                    goal_dependencies.add(producer_goal)\n        except round_manager.MissingProductError as e:\n            raise self.MissingProductError(\"Could not satisfy data dependencies for goal '{name}' with action {action}: {error}\".format(name=task_name, action=task_type.__name__, error=e))\n    goal_info = self.GoalInfo(goal, tasktypes_by_name, goal_dependencies)\n    goal_info_by_goal[goal] = goal_info\n    for goal_dependency in goal_dependencies:\n        self._visit_goal(goal_dependency, context, goal_info_by_goal, target_roots_replacement)\n", "label": 0}
{"function": "\n\ndef put(self, key, value, cas=None, flags=None, acquire=None, release=None, token=None, dc=None):\n    \"\\n            Sets *key* to the given *value*.\\n\\n            *value* can either be None (useful for marking a key as a\\n            directory) or any string type, including binary data (e.g. a\\n            msgpack'd data structure)\\n\\n            The optional *cas* parameter is used to turn the PUT into a\\n            Check-And-Set operation. This is very useful as it allows clients\\n            to build more complex syncronization primitives on top. If the\\n            index is 0, then Consul will only put the key if it does not\\n            already exist. If the index is non-zero, then the key is only set\\n            if the index matches the ModifyIndex of that key.\\n\\n            An optional *flags* can be set. This can be used to specify an\\n            unsigned value between 0 and 2^64-1.\\n\\n            *acquire* is an optional session_id. if supplied a lock acquisition\\n            will be attempted.\\n\\n            *release* is an optional session_id. if supplied a lock release\\n            will be attempted.\\n\\n            *token* is an optional `ACL token`_ to apply to this request. If\\n            the token's policy is not allowed to write to this key an\\n            *ACLPermissionDenied* exception will be raised.\\n\\n            *dc* is the optional datacenter that you wish to communicate with.\\n            If None is provided, defaults to the agent's datacenter.\\n\\n            The return value is simply either True or False. If False is\\n            returned, then the update has not taken place.\\n            \"\n    assert (not key.startswith('/')), 'keys should not start with a forward slash'\n    assert ((value is None) or isinstance(value, (six.string_types, six.binary_type))), 'value should be None or a string / binary data'\n    params = {\n        \n    }\n    if (cas is not None):\n        params['cas'] = cas\n    if (flags is not None):\n        params['flags'] = flags\n    if acquire:\n        params['acquire'] = acquire\n    if release:\n        params['release'] = release\n    token = (token or self.agent.token)\n    if token:\n        params['token'] = token\n    dc = (dc or self.agent.dc)\n    if dc:\n        params['dc'] = dc\n    return self.agent.http.put(callback(is_json=True), ('/v1/kv/%s' % key), params=params, data=value)\n", "label": 1}
{"function": "\n\ndef encode(string, encodings=None):\n    if ((not PY2) and isinstance(string, bytes)):\n        return string\n    if (PY2 and isinstance(string, str)):\n        return string\n    if (encodings is None):\n        encodings = ['utf-8', 'latin1', 'ascii']\n    for encoding in encodings:\n        try:\n            return string.encode(encoding)\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            pass\n    return string.encode(encodings[0], errors='ignore')\n", "label": 0}
{"function": "\n\ndef template_source(request):\n    \"\\n    Return the source of a template, syntax-highlighted by Pygments if\\n    it's available.\\n    \"\n    template_name = request.GET.get('template', None)\n    if (template_name is None):\n        return HttpResponseBadRequest('\"template\" key is required')\n    final_loaders = []\n    loaders = get_template_loaders()\n    for loader in loaders:\n        if (loader is not None):\n            if hasattr(loader, 'loaders'):\n                final_loaders += loader.loaders\n            else:\n                final_loaders.append(loader)\n    for loader in final_loaders:\n        try:\n            (source, display_name) = loader.load_template_source(template_name)\n            break\n        except TemplateDoesNotExist:\n            source = ('Template Does Not Exist: %s' % (template_name,))\n    try:\n        from pygments import highlight\n        from pygments.lexers import HtmlDjangoLexer\n        from pygments.formatters import HtmlFormatter\n        source = highlight(source, HtmlDjangoLexer(), HtmlFormatter())\n        source = mark_safe(source)\n        source.pygmentized = True\n    except ImportError:\n        pass\n    return render_to_response('debug_toolbar/panels/template_source.html', {\n        'source': source,\n        'template_name': template_name,\n    })\n", "label": 0}
{"function": "\n\ndef execute_sql(self, sql, params=None, require_commit=True, named_cursor=False):\n    logger.debug((sql, params))\n    use_named_cursor = (named_cursor or (self.server_side_cursors and sql.lower().startswith('select')))\n    with self.exception_wrapper():\n        if use_named_cursor:\n            cursor = self.get_cursor(name=str(uuid.uuid1()))\n            require_commit = False\n        else:\n            cursor = self.get_cursor()\n        try:\n            cursor.execute(sql, (params or ()))\n        except Exception as exc:\n            if (self.get_autocommit() and self.autorollback):\n                self.rollback()\n            raise\n        else:\n            if (require_commit and self.get_autocommit()):\n                self.commit()\n    return cursor\n", "label": 1}
{"function": "\n\ndef fsub(ctx, x, y, **kwargs):\n    \"\\n        Subtracts the numbers *x* and *y*, giving a floating-point result,\\n        optionally using a custom precision and rounding mode.\\n\\n        See the documentation of :func:`~mpmath.fadd` for a detailed description\\n        of how to specify precision and rounding.\\n\\n        **Examples**\\n\\n        Using :func:`~mpmath.fsub` with precision and rounding control::\\n\\n            >>> from mpmath import *\\n            >>> mp.dps = 15; mp.pretty = False\\n            >>> fsub(2, 1e-20)\\n            mpf('2.0')\\n            >>> fsub(2, 1e-20, rounding='d')\\n            mpf('1.9999999999999998')\\n            >>> nprint(fsub(2, 1e-20, prec=100), 25)\\n            1.99999999999999999999\\n            >>> nprint(fsub(2, 1e-20, dps=15), 25)\\n            2.0\\n            >>> nprint(fsub(2, 1e-20, dps=25), 25)\\n            1.99999999999999999999\\n            >>> nprint(fsub(2, 1e-20, exact=True), 25)\\n            1.99999999999999999999\\n\\n        Exact subtraction avoids cancellation errors, enforcing familiar laws\\n        of numbers such as `x-y+y = x`, which don't hold in floating-point\\n        arithmetic with finite precision::\\n\\n            >>> x, y = mpf(2), mpf('1e1000')\\n            >>> print(x - y + y)\\n            0.0\\n            >>> print(fsub(x, y, prec=inf) + y)\\n            2.0\\n            >>> print(fsub(x, y, exact=True) + y)\\n            2.0\\n\\n        Exact addition can be inefficient and may be impossible to perform\\n        with large magnitude differences::\\n\\n            >>> fsub(1, '1e-100000000000000000000', prec=inf)\\n            Traceback (most recent call last):\\n              ...\\n            OverflowError: the exact result does not fit in memory\\n\\n        \"\n    (prec, rounding) = ctx._parse_prec(kwargs)\n    x = ctx.convert(x)\n    y = ctx.convert(y)\n    try:\n        if hasattr(x, '_mpf_'):\n            if hasattr(y, '_mpf_'):\n                return ctx.make_mpf(mpf_sub(x._mpf_, y._mpf_, prec, rounding))\n            if hasattr(y, '_mpc_'):\n                return ctx.make_mpc(mpc_sub((x._mpf_, fzero), y._mpc_, prec, rounding))\n        if hasattr(x, '_mpc_'):\n            if hasattr(y, '_mpf_'):\n                return ctx.make_mpc(mpc_sub_mpf(x._mpc_, y._mpf_, prec, rounding))\n            if hasattr(y, '_mpc_'):\n                return ctx.make_mpc(mpc_sub(x._mpc_, y._mpc_, prec, rounding))\n    except (ValueError, OverflowError):\n        raise OverflowError(ctx._exact_overflow_msg)\n    raise ValueError('Arguments need to be mpf or mpc compatible numbers')\n", "label": 0}
{"function": "\n\ndef generate_global_orchestrate_file(self):\n    accounts = set([host.cloud_image.account for host in self.hosts.all()])\n    orchestrate = {\n        \n    }\n    for account in accounts:\n        target = 'G@stack_id:{0} and G@cloud_account:{1}'.format(self.id, account.slug)\n        groups = {\n            \n        }\n        for component in account.formula_components.all():\n            groups.setdefault(component.order, set()).add(component.sls_path)\n        for order in sorted(groups.keys()):\n            for role in groups[order]:\n                state_title = '{0}_{1}'.format(account.slug, role)\n                orchestrate[state_title] = {\n                    'salt.state': [{\n                        'tgt': target,\n                    }, {\n                        'tgt_type': 'compound',\n                    }, {\n                        'sls': role,\n                    }],\n                }\n                depend = (order - 1)\n                while (depend >= 0):\n                    if (depend in groups.keys()):\n                        orchestrate[role]['salt.state'].append({\n                            'require': [{\n                                'salt': req,\n                            } for req in groups[depend]],\n                        })\n                        break\n                    depend -= 1\n    yaml_data = yaml.safe_dump(orchestrate, default_flow_style=False)\n    if (not self.global_orchestrate_file):\n        self.global_orchestrate_file.save('global_orchestrate.sls', ContentFile(yaml_data))\n    else:\n        with open(self.global_orchestrate_file.path, 'w') as f:\n            f.write(yaml_data)\n", "label": 0}
{"function": "\n\ndef assert_attr_equal(attr, left, right, obj='Attributes'):\n    \"checks attributes are equal. Both objects must have attribute.\\n\\n    Parameters\\n    ----------\\n    attr : str\\n        Attribute name being compared.\\n    left : object\\n    right : object\\n    obj : str, default 'Attributes'\\n        Specify object name being compared, internally used to show appropriate\\n        assertion message\\n    \"\n    left_attr = getattr(left, attr)\n    right_attr = getattr(right, attr)\n    if (left_attr is right_attr):\n        return True\n    elif (is_number(left_attr) and np.isnan(left_attr) and is_number(right_attr) and np.isnan(right_attr)):\n        return True\n    try:\n        result = (left_attr == right_attr)\n    except TypeError:\n        result = False\n    if (not isinstance(result, bool)):\n        result = result.all()\n    if result:\n        return True\n    else:\n        raise_assert_detail(obj, 'Attribute \"{0}\" are different'.format(attr), left_attr, right_attr)\n", "label": 0}
{"function": "\n\ndef config_file_opt(self, path2json):\n    if os.path.exists(path2json):\n        with codecs.open(opt.config, 'r', encoding='ascii') as f:\n            json_con = json_loads(f.read())\n        self.server = (self.server or json_con.get('server', None))\n        self.password = (self.password or json_con.get('password', None))\n        self.server_port = (self.server_port or json_con.get('server_port', None))\n        self.local_port = (self.local_port or json_con.get('local_port', None))\n        self.method = (self.method or json_con.get('method', 'table'))\n        self.timeout = (self.timeout or json_con.get('timeout', None))\n        self.debug = (self.debug or json_con.get('debug', False))\n    else:\n        logger.warning(('the json file path `%s` is not exists' % path2json))\n", "label": 0}
{"function": "\n\ndef get_content(self, context=None):\n    ' Does all context binding and pathing to get content, templated out '\n    if self.is_file:\n        path = self.content\n        if (self.is_template_path and context):\n            path = string.Template(path).safe_substitute(context.get_values())\n        data = None\n        with open(path, 'r') as f:\n            data = f.read()\n        if (self.is_template_content and context):\n            return string.Template(data).safe_substitute(context.get_values())\n        else:\n            return data\n    elif (self.is_template_content and context):\n        return safe_substitute_unicode_template(self.content, context.get_values())\n    else:\n        return self.content\n", "label": 0}
{"function": "\n\ndef __init__(self, params=None, auto_init=True):\n    'Create a FileDownloader object with the given options.'\n    if (params is None):\n        params = {\n            \n        }\n    self._ies = []\n    self._ies_instances = {\n        \n    }\n    self._pps = []\n    self._progress_hooks = []\n    self._download_retcode = 0\n    self._num_downloads = 0\n    self._screen_file = [sys.stdout, sys.stderr][params.get('logtostderr', False)]\n    self._err_file = sys.stderr\n    self.params = params\n    self.cache = Cache(self)\n    if params.get('bidi_workaround', False):\n        try:\n            import pty\n            (master, slave) = pty.openpty()\n            width = get_term_width()\n            if (width is None):\n                width_args = []\n            else:\n                width_args = ['-w', str(width)]\n            sp_kwargs = dict(stdin=subprocess.PIPE, stdout=slave, stderr=self._err_file)\n            try:\n                self._output_process = subprocess.Popen((['bidiv'] + width_args), **sp_kwargs)\n            except OSError:\n                self._output_process = subprocess.Popen((['fribidi', '-c', 'UTF-8'] + width_args), **sp_kwargs)\n            self._output_channel = os.fdopen(master, 'rb')\n        except OSError as ose:\n            if (ose.errno == 2):\n                self.report_warning('Could not find fribidi executable, ignoring --bidi-workaround . Make sure that  fribidi  is an executable file in one of the directories in your $PATH.')\n            else:\n                raise\n    if ((sys.version_info >= (3,)) and (sys.platform != 'win32') and (sys.getfilesystemencoding() in ['ascii', 'ANSI_X3.4-1968']) and (not params.get('restrictfilenames', False))):\n        self.report_warning('Assuming --restrict-filenames since file system encoding cannot encode all characters. Set the LC_ALL environment variable to fix this.')\n        self.params['restrictfilenames'] = True\n    if ('%(stitle)s' in self.params.get('outtmpl', '')):\n        self.report_warning('%(stitle)s is deprecated. Use the %(title)s and the --restrict-filenames flag(which also secures %(uploader)s et al) instead.')\n    self._setup_opener()\n    if auto_init:\n        self.print_debug_header()\n        self.add_default_info_extractors()\n    for pp_def_raw in self.params.get('postprocessors', []):\n        pp_class = get_postprocessor(pp_def_raw['key'])\n        pp_def = dict(pp_def_raw)\n        del pp_def['key']\n        pp = pp_class(self, **compat_kwargs(pp_def))\n        self.add_post_processor(pp)\n    for ph in self.params.get('progress_hooks', []):\n        self.add_progress_hook(ph)\n", "label": 1}
{"function": "\n\ndef __call__(self):\n    request = current.request\n    response = current.response\n    view = path.join(request.folder, 'modules', 'templates', THEME, 'views', 'index.html')\n    try:\n        response.view = open(view, 'rb')\n    except IOError:\n        from gluon.http import HTTP\n        raise HTTP(404, ('Unable to open Custom View: %s' % view))\n    if current.auth.is_logged_in():\n        grid = 'grid_12'\n    else:\n        grid = 'grid_8'\n    latest_projects = DIV(_id='front-latest-body', _class=('%s alpha' % grid))\n    lappend = latest_projects.append\n    db = current.db\n    s3db = current.s3db\n    table = s3db.project_project\n    table_drrpp = s3db.project_drrpp\n    query = ((table.deleted != True) & (table.approved_by != None))\n    rows = db(query).select(table.id, table.name, table_drrpp.activities, table.organisation_id, table.start_date, left=table_drrpp.on((table.id == table_drrpp.project_id)), limitby=(0, 3))\n    project_ids = [r.project_project.id for r in rows]\n    ltable = s3db.project_location\n    gtable = s3db.gis_location\n    query = ((((ltable.deleted != True) & (ltable.project_id == table.id)) & (gtable.id == ltable.location_id)) & (gtable.level == 'L0'))\n    locations = db(query).select(ltable.project_id, gtable.L0)\n    odd = True\n    for row in rows:\n        countries = [l.gis_location.L0 for l in locations if (l.project_location.project_id == row.project_project.id)]\n        location = ', '.join(countries)\n        if odd:\n            _class = ('front-latest-item odd %s alpha' % grid)\n        else:\n            _class = ('front-latest-item even %s alpha' % grid)\n        card = DIV(DIV(A(row.project_project.name, _href=URL(c='project', f='project', args=[row.project_project.id])), _class=('front-latest-title %s' % grid)), DIV(('Lead Organization: %s' % s3db.org_organisation_represent(row.project_project.organisation_id)), _class=('front-latest-desc %s' % grid)), DIV(SPAN(('Start Date: %s' % row.project_project.start_date), _class='front-latest-info-date'), SPAN(('Countries: %s' % location), _class='front-latest-info-location'), _class=('front-latest-info %s' % grid)), DIV((row.project_drrpp.activities or ''), _class=('front-latest-desc %s' % grid)), _class=_class)\n        lappend(card)\n        odd = (False if odd else True)\n    login = current.auth.login(inline=True)\n    appname = request.application\n    s3 = response.s3\n    if current.session.s3.debug:\n        s3.scripts.append(('/%s/static/themes/DRRPP/js/slides.jquery.js' % appname))\n    else:\n        s3.scripts.append(('/%s/static/themes/DRRPP/js/slides.min.jquery.js' % appname))\n    s3.jquery_ready.append(\"\\n$('#slides').slides({\\n play:8000,\\n animationStart:function(current){\\n  $('.caption').animate({\\n   bottom:-35\\n  },100);\\n },\\n animationComplete:function(current){\\n  $('.caption').animate({\\n   bottom:0\\n  },200);\\n },\\n slidesLoaded:function() {\\n  $('.caption').animate({\\n   bottom:0\\n  },200);\\n }\\n})\")\n    return dict(title='Home', form=login, latest_projects=latest_projects)\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    fake_author_id = uuid4()\n    project = self.create_project()\n    self.create_build(project)\n    path = '/api/0/authors/{0}/builds/'.format(fake_author_id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 404)\n    data = self.unserialize(resp)\n    assert (len(data) == 0)\n    author = Author(email=self.default_user.email, name='Foo Bar')\n    db.session.add(author)\n    build = self.create_build(project, author=author)\n    path = '/api/0/authors/{0}/builds/'.format(author.id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 401)\n    self.login(self.default_user)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    (username, domain) = self.default_user.email.split('@', 1)\n    author = self.create_author('{}+foo@{}'.format(username, domain))\n    self.create_build(project, author=author)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 2)\n", "label": 1}
{"function": "\n\n@mock.patch('olympia.addons.tasks.make_checksum')\n@mock.patch('olympia.addons.tasks.create_persona_preview_images')\n@mock.patch('olympia.addons.tasks.save_persona_image')\n@pytest.mark.skipif((not hasattr(Image.core, 'jpeg_encoder')), reason='Not having a jpeg encoder makes test sad')\ndef test_success(self, save_persona_image_mock, create_persona_preview_images_mock, make_checksum_mock):\n    make_checksum_mock.return_value = 'hashyourselfbeforeyoucrashyourself'\n    self.request.user = UserProfile.objects.get(pk=2519)\n    data = self.get_dict()\n    (header_url, footer_url) = self.get_img_urls()\n    img = open(get_image_path('persona-header.jpg'), 'rb')\n    r_ajax = self.client.post(header_url, {\n        'upload_image': img,\n    })\n    data.update(header_hash=json.loads(r_ajax.content)['upload_hash'])\n    img = open(get_image_path('persona-footer.jpg'), 'rb')\n    r_ajax = self.client.post(footer_url, {\n        'upload_image': img,\n    })\n    data.update(footer_hash=json.loads(r_ajax.content)['upload_hash'])\n    self.post()\n    assert self.form.is_valid(), self.form.errors\n    self.form.save()\n    addon = Addon.objects.filter(type=amo.ADDON_PERSONA).order_by('-id')[0]\n    persona = addon.persona\n    assert (unicode(addon.name) == data['name'])\n    assert (addon.slug == data['slug'])\n    self.assertSetEqual(addon.categories.values_list('id', flat=True), [self.cat.id])\n    self.assertSetEqual(addon.tags.values_list('tag_text', flat=True), data['tags'].split(', '))\n    assert (persona.persona_id == 0)\n    assert (persona.license == data['license'])\n    assert (persona.accentcolor == data['accentcolor'].lstrip('#'))\n    assert (persona.textcolor == data['textcolor'].lstrip('#'))\n    assert (persona.author == self.request.user.username)\n    assert (persona.display_username == self.request.user.name)\n    assert (not persona.dupe_persona)\n    v = addon.versions.all()\n    assert (len(v) == 1)\n    assert (v[0].version == '0')\n    dst = os.path.join(user_media_path('addons'), str(addon.id))\n    header_src = os.path.join(settings.TMP_PATH, 'persona_header', 'b4ll1n')\n    footer_src = os.path.join(settings.TMP_PATH, 'persona_footer', '5w4g')\n    assert (save_persona_image_mock.mock_calls == [mock.call(src=header_src, full_dst=os.path.join(dst, 'header.png')), mock.call(src=footer_src, full_dst=os.path.join(dst, 'footer.png'))])\n    create_persona_preview_images_mock.assert_called_with(src=header_src, full_dst=[os.path.join(dst, 'preview.png'), os.path.join(dst, 'icon.png')], set_modified_on=[addon])\n", "label": 1}
{"function": "\n\n@require_POST\n@csrf_exempt\n@cross_domain_post_response\n@host_check\ndef dislike_comment(request, comment_id):\n    try:\n        comment = Comment.objects.get(id=comment_id)\n    except Comment.DoesNotExist as e:\n        data = {\n            'placement': 'comments_container',\n            'content': str(e),\n        }\n        return HttpResponseNotFound(json.dumps(data))\n    if request.user.is_anonymous():\n        site_admin = False\n    else:\n        site_admin = bool(request.user.get_comments(comment_id))\n    if site_admin:\n        if request.user.hidden.filter(id=comment.thread.site.id):\n            return HttpResponseBadRequest((_('user is disabled on site with id %s') % comment.thread.site.id))\n    liked_comments = request.session.get('liked_comments', [])\n    disliked_comments = request.session.get('disliked_comments', [])\n    if (comment.id in liked_comments):\n        comment.undo_like(request.user)\n        remove_from_session_list(request, 'liked_comments', comment.id)\n        comment.dislike(request.user)\n        add_to_session_list(request, 'disliked_comments', comment.id)\n    elif (comment.id not in disliked_comments):\n        comment.dislike(request.user)\n        add_to_session_list(request, 'disliked_comments', comment.id)\n    comments = Comment.objects.filter(thread=comment.thread)\n    posted_comments = request.session.get('posted_comments', [])\n    site = comment.thread.site\n    resp = render(request, 'comments.html', {\n        'comments': comments,\n        'posted_comments': posted_comments,\n        'last_posted_comment_id': (posted_comments[(- 1)] if posted_comments else None),\n        'rs_customer_id': site.rs_customer_id,\n        'all_comments': request.session.get('all_comments', False),\n        'site_admin': site_admin,\n    })\n    data = {\n        'placement': 'comments_container',\n        'content': resp.content,\n    }\n    return HttpResponse(json.dumps(data))\n", "label": 0}
{"function": "\n\ndef _read_object_from_repo(self, rev=None, relpath=None, sha=None):\n    'Read an object from the git repo.\\n    This is implemented via a pipe to git cat-file --batch\\n    '\n    if sha:\n        spec = (sha + '\\n')\n    else:\n        assert (rev is not None)\n        assert (relpath is not None)\n        relpath = self._fixup_dot_relative(relpath)\n        spec = '{}:{}\\n'.format(rev, relpath)\n    self._maybe_start_cat_file_process()\n    self._cat_file_process.stdin.write(spec)\n    self._cat_file_process.stdin.flush()\n    header = None\n    while (not header):\n        header = self._cat_file_process.stdout.readline()\n        if (self._cat_file_process.poll() is not None):\n            raise self.GitDiedException(\"Git cat-file died while trying to read '{}'.\".format(spec))\n    header = header.rstrip()\n    parts = header.rsplit(SPACE, 2)\n    if (len(parts) == 2):\n        assert (parts[1] == 'missing')\n        raise self.MissingFileException(rev, relpath)\n    (_, object_type, object_len) = parts\n    blob = self._cat_file_process.stdout.read(int(object_len))\n    assert (self._cat_file_process.stdout.read(1) == '\\n')\n    assert (len(blob) == int(object_len))\n    return (object_type, blob)\n", "label": 0}
{"function": "\n\ndef _get_python_variables(python_exe, variables, imports=['import sys']):\n    'Run a python interpreter and print some variables'\n    program = list(imports)\n    program.append('')\n    for v in variables:\n        program.append(('print(repr(%s))' % v))\n    os_env = dict(os.environ)\n    try:\n        del os_env['MACOSX_DEPLOYMENT_TARGET']\n    except KeyError:\n        pass\n    proc = Utils.pproc.Popen([python_exe, '-c', '\\n'.join(program)], stdout=Utils.pproc.PIPE, env=os_env)\n    output = proc.communicate()[0].split('\\n')\n    if proc.returncode:\n        if Options.options.verbose:\n            warn(('Python program to extract python configuration variables failed:\\n%s' % '\\n'.join([('line %03i: %s' % ((lineno + 1), line)) for (lineno, line) in enumerate(program)])))\n        raise RuntimeError\n    return_values = []\n    for s in output:\n        s = s.strip()\n        if (not s):\n            continue\n        if (s == 'None'):\n            return_values.append(None)\n        elif ((s[0] == \"'\") and (s[(- 1)] == \"'\")):\n            return_values.append(s[1:(- 1)])\n        elif s[0].isdigit():\n            return_values.append(int(s))\n        else:\n            break\n    return return_values\n", "label": 1}
{"function": "\n\ndef read_results(results_fpath, lamb):\n    last = {\n        \n    }\n    classes = {\n        \n    }\n    objs = set()\n    users = set()\n    joint = defaultdict((lambda : defaultdict(int)))\n    glob = defaultdict((lambda : defaultdict(int)))\n    with open(results_fpath) as results_file:\n        results_file.readline()\n        for line in results_file:\n            spl = line.strip().split()\n            user = spl[0]\n            class_ = spl[1]\n            last_stage = spl[(- 1)].split('-')[(- 1)]\n            last[user] = last_stage\n            classes[user] = class_\n            for item in spl[2:]:\n                (obj, stage) = item.split('-')\n                joint[class_][(stage, obj)] += 1\n                glob[class_][stage] += 1\n                objs.add(obj)\n            users.add(user)\n    probs = {\n        \n    }\n    for u in last:\n        probs[u] = {\n            \n        }\n        class_ = classes[u]\n        for o in objs:\n            probs[u][o] = ((lamb + joint[class_][(last[u], o)]) / ((len(objs) * lamb) + glob[class_][last[u]]))\n    for u in probs:\n        sum_ = sum(probs[u].values())\n        for o in objs:\n            probs[u][o] = ((probs[u][o] / sum_) if (sum_ > 0) else 0)\n    sorted_probs = {\n        \n    }\n    for u in probs:\n        sorted_probs[u] = sorted([(v, o) for (o, v) in probs[u].items()], reverse=True)\n    return (sorted_probs, users, objs)\n", "label": 0}
{"function": "\n\ndef execute(self):\n    if self.get_options().skip:\n        self.context.log.info('Skipping scalastyle.')\n        return\n    targets = self.get_non_synthetic_scala_targets(self.context.targets())\n    if (not targets):\n        return\n    with self.invalidated(targets) as invalidation_check:\n        invalid_targets = [vt.target for vt in invalidation_check.invalid_vts]\n        scalastyle_config = self.validate_scalastyle_config()\n        scalastyle_verbose = self.get_options().verbose\n        scalastyle_quiet = (self.get_options().quiet or False)\n        scalastyle_excluder = self.create_file_excluder()\n        self.context.log.debug('Non synthetic scala targets to be checked:')\n        for target in invalid_targets:\n            self.context.log.debug('  {address_spec}'.format(address_spec=target.address.spec))\n        scala_sources = self.get_non_excluded_scala_sources(scalastyle_excluder, invalid_targets)\n        self.context.log.debug('Non excluded scala sources to be checked:')\n        for source in scala_sources:\n            self.context.log.debug('  {source}'.format(source=source))\n        if scala_sources:\n\n            def call(srcs):\n\n                def to_java_boolean(x):\n                    return str(x).lower()\n                cp = ScalaPlatform.global_instance().style_classpath(self.context.products)\n                scalastyle_args = ['-c', scalastyle_config, '-v', to_java_boolean(scalastyle_verbose), '-q', to_java_boolean(scalastyle_quiet)]\n                return self.runjava(classpath=cp, main=self._MAIN, jvm_options=self.get_options().jvm_options, args=(scalastyle_args + srcs))\n            result = Xargs(call).execute(scala_sources)\n            if (result != 0):\n                raise TaskError('java {entry} ... exited non-zero ({exit_code})'.format(entry=Scalastyle._MAIN, exit_code=result))\n", "label": 0}
{"function": "\n\ndef run_app(**kwargs):\n    sys_args = sys.argv\n    runner_name = os.path.basename(sys_args[0])\n    (args, command, command_args) = parse_args(sys_args[1:])\n    if (not command):\n        print(('usage: %s [--config=/path/to/settings.py] [command] [options]' % runner_name))\n        sys.exit(1)\n    default_config_path = kwargs.get('default_config_path')\n    parser = OptionParser()\n    if (command == 'init'):\n        (options, opt_args) = parser.parse_args()\n        settings_initializer = kwargs.get('settings_initializer')\n        config_path = os.path.expanduser((' '.join(opt_args[1:]) or default_config_path))\n        if os.path.exists(config_path):\n            resp = None\n            while (resp not in ('Y', 'n')):\n                resp = raw_input(('File already exists at %r, overwrite? [nY] ' % config_path))\n                if (resp == 'n'):\n                    print('Aborted!')\n                    return\n        try:\n            create_default_settings(config_path, settings_initializer)\n        except OSError as e:\n            raise e.__class__(('Unable to write default settings file to %r' % config_path))\n        print(('Configuration file created at %r' % config_path))\n        return\n    parser.add_option('--config', metavar='CONFIG')\n    (options, logan_args) = parser.parse_args(args)\n    config_path = options.config\n    configure_app(config_path=config_path, **kwargs)\n    management.execute_from_command_line(([runner_name, command] + command_args))\n    sys.exit(0)\n", "label": 0}
{"function": "\n\ndef is_concyclic(*points):\n    'Is a sequence of points concyclic?\\n\\n        Test whether or not a sequence of points are concyclic (i.e., they lie\\n        on a circle).\\n\\n        Parameters\\n        ==========\\n\\n        points : sequence of Points\\n\\n        Returns\\n        =======\\n\\n        is_concyclic : boolean\\n            True if points are concyclic, False otherwise.\\n\\n        See Also\\n        ========\\n\\n        sympy.geometry.ellipse.Circle\\n\\n        Notes\\n        =====\\n\\n        No points are not considered to be concyclic. One or two points\\n        are definitely concyclic and three points are conyclic iff they\\n        are not collinear.\\n\\n        For more than three points, create a circle from the first three\\n        points. If the circle cannot be created (i.e., they are collinear)\\n        then all of the points cannot be concyclic. If the circle is created\\n        successfully then simply check the remaining points for containment\\n        in the circle.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.geometry import Point\\n        >>> p1, p2 = Point(-1, 0), Point(1, 0)\\n        >>> p3, p4 = Point(0, 1), Point(-1, 2)\\n        >>> Point.is_concyclic(p1, p2, p3)\\n        True\\n        >>> Point.is_concyclic(p1, p2, p3, p4)\\n        False\\n\\n        '\n    if (len(points) == 0):\n        return False\n    if (len(points) <= 2):\n        return True\n    points = [Point(p) for p in points]\n    if (len(points) == 3):\n        return (not Point.is_collinear(*points))\n    try:\n        from .ellipse import Circle\n        c = Circle(points[0], points[1], points[2])\n        for point in points[3:]:\n            if (point not in c):\n                return False\n        return True\n    except GeometryError:\n        return False\n", "label": 0}
{"function": "\n\ndef _get_boot_device(node, drac_boot_devices=None):\n    client = drac_common.get_drac_client(node)\n    try:\n        boot_modes = client.list_boot_modes()\n        next_boot_modes = [mode.id for mode in boot_modes if mode.is_next]\n        if (NON_PERSISTENT_BOOT_MODE in next_boot_modes):\n            next_boot_mode = NON_PERSISTENT_BOOT_MODE\n        else:\n            next_boot_mode = next_boot_modes[0]\n        if (drac_boot_devices is None):\n            drac_boot_devices = client.list_boot_devices()\n        drac_boot_device = drac_boot_devices[next_boot_mode][0]\n        boot_device = next((key for (key, value) in _BOOT_DEVICES_MAP.items() if (value in drac_boot_device.id)))\n        return {\n            'boot_device': boot_device,\n            'persistent': (next_boot_mode == PERSISTENT_BOOT_MODE),\n        }\n    except (drac_exceptions.BaseClientException, IndexError) as exc:\n        LOG.error(_LE('DRAC driver failed to get next boot mode for node %(node_uuid)s. Reason: %(error)s.'), {\n            'node_uuid': node.uuid,\n            'error': exc,\n        })\n        raise exception.DracOperationError(error=exc)\n", "label": 0}
{"function": "\n\n@memoize_generator\ndef process(self, stack, stream):\n    for (token_type, value) in stream:\n        if ((token_type in Name) and (value.upper() == 'INCLUDE')):\n            self.detected = True\n            continue\n        elif self.detected:\n            if (token_type in Whitespace):\n                continue\n            if (token_type in String.Symbol):\n                path = join(self.dirpath, value[1:(- 1)])\n                try:\n                    f = open(path)\n                    raw_sql = f.read()\n                    f.close()\n                except IOError as err:\n                    if self.raiseexceptions:\n                        raise\n                    (yield (Comment, ('-- IOError: %s\\n' % err)))\n                else:\n                    try:\n                        filtr = IncludeStatement(self.dirpath, (self.maxRecursive - 1), self.raiseexceptions)\n                    except ValueError as err:\n                        if self.raiseexceptions:\n                            raise\n                        (yield (Comment, ('-- ValueError: %s\\n' % err)))\n                    stack = FilterStack()\n                    stack.preprocess.append(filtr)\n                    for tv in stack.run(raw_sql):\n                        (yield tv)\n                self.detected = False\n            continue\n        (yield (token_type, value))\n", "label": 1}
{"function": "\n\ndef changeActivity(old, new):\n    Base = declarative_base()\n    Session = sessionmaker(bind=op.get_bind())\n\n    class Move(Base):\n        __tablename__ = 'move'\n        id = sa.Column(sa.Integer, name='id', primary_key=True)\n        activity = sa.Column(sa.String, name='activity')\n        strokeCount = sa.Column(sa.Integer, name='stroke_count')\n\n    class Sample(Base):\n        __tablename__ = 'sample'\n        id = sa.Column(sa.Integer, name='id', primary_key=True)\n        moveId = sa.Column(sa.Integer, sa.ForeignKey(Move.id), name='move_id', nullable=False)\n        move = sa.orm.relationship(Move, backref=sa.orm.backref('samples', lazy='dynamic'))\n        events = sa.Column(sa.String, name='events')\n    session = Session()\n    for move in session.query(Move):\n        strokeCount = 0\n        for (eventData,) in session.query(Sample.events).filter((Sample.move == move), (Sample.events != None)):\n            events = json.loads(eventData)\n            if (('swimming' in events) and (events['swimming']['type'] == 'Stroke')):\n                strokeCount += 1\n        if ('swimming' in move.activity):\n            assert (strokeCount > 0)\n        if (strokeCount > 0):\n            move.strokeCount = strokeCount\n    session.commit()\n", "label": 0}
{"function": "\n\ndef request(self, **kwargs):\n    '\\n        Perform network request.\\n\\n        You can specify grab settings in ``**kwargs``.\\n        Any keyword argument will be passed to ``self.config``.\\n\\n        Returns: ``Document`` objects.\\n        '\n    self.prepare_request(**kwargs)\n    refresh_count = 0\n    while True:\n        self.log_request()\n        try:\n            self.transport.request()\n        except error.GrabError:\n            self.reset_temporary_options()\n            self.save_failed_dump()\n            raise\n        else:\n            doc = self.process_request_result()\n            if self.config['follow_location']:\n                if (doc.code in (301, 302, 303, 307, 308)):\n                    if doc.headers.get('Location'):\n                        refresh_count += 1\n                        if (refresh_count > self.config['redirect_limit']):\n                            raise error.GrabTooManyRedirectsError()\n                        else:\n                            url = doc.headers.get('Location')\n                            self.prepare_request(url=self.make_url_absolute(url), referer=None)\n                            continue\n            if self.config['follow_refresh']:\n                refresh_url = self.doc.get_meta_refresh_url()\n                if (refresh_url is not None):\n                    refresh_count += 1\n                    if (refresh_count > self.config['redirect_limit']):\n                        raise error.GrabTooManyRedirectsError()\n                    else:\n                        self.prepare_request(url=self.make_url_absolute(refresh_url), referer=None)\n                        continue\n            return doc\n", "label": 1}
{"function": "\n\ndef test_remove_key(self):\n    ks = KeyJar()\n    ks[''] = KeyBundle([{\n        'kty': 'oct',\n        'key': 'a1b2c3d4',\n        'use': 'sig',\n    }, {\n        'kty': 'oct',\n        'key': 'a1b2c3d4',\n        'use': 'ver',\n    }])\n    ks['http://www.example.org'] = [KeyBundle([{\n        'kty': 'oct',\n        'key': 'e5f6g7h8',\n        'use': 'sig',\n    }, {\n        'kty': 'oct',\n        'key': 'e5f6g7h8',\n        'use': 'ver',\n    }]), keybundle_from_local_file(RSAKEY, 'rsa', ['enc', 'dec'])]\n    ks['http://www.example.com'] = keybundle_from_local_file(RSA0, 'rsa', ['enc', 'dec'])\n    coll = ks['http://www.example.org']\n    assert (len(coll) == 2)\n    keys = ks.get_encrypt_key(key_type='RSA', owner='http://www.example.org')\n    assert (len(keys) == 1)\n    _key = keys[0]\n    ks.remove_key('http://www.example.org', 'RSA', _key)\n    coll = ks['http://www.example.org']\n    assert (len(coll) == 1)\n    keys = ks.get_encrypt_key(key_type='rsa', owner='http://www.example.org')\n    assert (len(keys) == 0)\n    keys = ks.verify_keys('http://www.example.com')\n    assert (len(keys) == 1)\n    assert (len([k for k in keys if (k.kty == 'oct')]) == 1)\n    keys = ks.decrypt_keys('http://www.example.org')\n    assert (keys == [])\n", "label": 0}
{"function": "\n\ndef new_clustered_sortind(x, k=10, row_key=None, cluster_key=None):\n    '\\n    Uses MiniBatch k-means clustering to cluster matrix into groups.\\n\\n    Each cluster of rows is then sorted by `scorefunc` -- by default, the max\\n    peak height when all rows in a cluster are averaged, or\\n    cluster.mean(axis=0).max().\\n\\n    Returns the index that will sort the rows of `x` and a list of \"breaks\".\\n    `breaks` is essentially a cumulative row count for each cluster boundary.\\n    In other words, after plotting the array you can use axhline on each\\n    \"break\" to plot the cluster boundary.\\n\\n    If `k` is a list or tuple, iteratively try each one and select the best\\n    with the lowest mean distance from cluster centers.\\n\\n    :param x: Matrix whose rows are to be clustered\\n    :param k: Number of clusters to create or a list of potential clusters; the\\n        optimum will be chosen from the list\\n    :param row_key:\\n        Optional function to act as a sort key for sorting rows within\\n        clusters.  Signature should be `scorefunc(a)` where `a` is a 1-D NumPy\\n        array.\\n    :param cluster_key:\\n        Optional function for sorting clusters.  Signature is `clusterfunc(a)`\\n        where `a` is a NumPy array containing all rows of `x` for cluster `i`.\\n        It must return a single value.\\n    '\n    try:\n        from sklearn.cluster import MiniBatchKMeans\n    except ImportError:\n        raise ImportError('please install scikits.learn for clustering.')\n    if isinstance(k, int):\n        best_k = k\n    else:\n        mean_dists = {\n            \n        }\n        for _k in k:\n            mbk = MiniBatchKMeans(init='k-means++', n_clusters=_k)\n            mbk.fit(x)\n            mean_dists[_k] = mbk.transform(x).mean()\n        best_k = sorted(mean_dists.items(), key=(lambda x: x[1]))[(- 1)][0]\n    mbk = MiniBatchKMeans(init='k-means++', n_clusters=best_k)\n    mbk.fit(x)\n    k = best_k\n    labels = mbk.labels_\n    scores = np.zeros(labels.shape, dtype=float)\n    if cluster_key:\n\n        def _cluster_key(i):\n            return cluster_key(x[(labels == i), :])\n        sorted_labels = sorted(range(k), key=_cluster_key)\n    else:\n        sorted_labels = range(k)\n    if row_key:\n\n        def _row_key(i):\n            return row_key(x[i, :])\n    final_ind = []\n    breaks = []\n    pos = 0\n    for label in sorted_labels:\n        label_inds = np.nonzero((labels == label))[0]\n        if row_key:\n            label_sort_ind = sorted(label_inds, key=_row_key)\n        else:\n            label_sort_ind = label_inds\n        for li in label_sort_ind:\n            final_ind.append(li)\n        pos += len(label_inds)\n        breaks.append(pos)\n    return (np.array(final_ind), np.array(breaks))\n", "label": 0}
{"function": "\n\ndef __init__(self, patterns_ini=None, input_format='pdf', dedup=False, library='pdfminer', output_format='csv', output_handler=None):\n    basedir = os.path.dirname(os.path.abspath(__file__))\n    if (patterns_ini is None):\n        patterns_ini = os.path.join(basedir, 'patterns.ini')\n    self.load_patterns(patterns_ini)\n    self.whitelist = WhiteList(basedir)\n    self.dedup = dedup\n    if output_handler:\n        self.handler = output_handler\n    else:\n        self.handler = output.getHandler(output_format)\n    self.ext_filter = ('*.' + input_format)\n    parser_format = ('parse_' + input_format)\n    try:\n        self.parser_func = getattr(self, parser_format)\n    except AttributeError:\n        e = ('Selected parser format is not supported: %s' % input_format)\n        raise NotImplementedError(e)\n    self.library = library\n    if (input_format == 'pdf'):\n        if (library not in IMPORTS):\n            e = ('Selected PDF parser library not found: %s' % library)\n            raise ImportError(e)\n    elif (input_format == 'html'):\n        if ('beautifulsoup' not in IMPORTS):\n            e = 'HTML parser library not found: BeautifulSoup'\n            raise ImportError(e)\n", "label": 0}
{"function": "\n\ndef fetch_metadata(self):\n    'Fetches the metadata for the table and merges it in'\n    try:\n        table = self.database.get_table(self.table_name, schema=self.schema)\n    except Exception as e:\n        flasher(str(e))\n        flasher(\"Table doesn't seem to exist in the specified database, couldn't fetch column information\", 'danger')\n        return\n    TC = TableColumn\n    M = SqlMetric\n    metrics = []\n    any_date_col = None\n    for col in table.columns:\n        try:\n            datatype = str(col.type)\n        except Exception as e:\n            datatype = 'UNKNOWN'\n        dbcol = db.session.query(TC).filter((TC.table == self)).filter((TC.column_name == col.name)).first()\n        db.session.flush()\n        if (not dbcol):\n            dbcol = TableColumn(column_name=col.name)\n            num_types = ('DOUBLE', 'FLOAT', 'INT', 'BIGINT', 'LONG')\n            date_types = ('DATE', 'TIME')\n            str_types = ('VARCHAR', 'STRING')\n            datatype = str(datatype).upper()\n            if any([(t in datatype) for t in str_types]):\n                dbcol.groupby = True\n                dbcol.filterable = True\n            elif any([(t in datatype) for t in num_types]):\n                dbcol.sum = True\n            elif any([(t in datatype) for t in date_types]):\n                dbcol.is_dttm = True\n        db.session.merge(self)\n        self.columns.append(dbcol)\n        if ((not any_date_col) and ('date' in datatype.lower())):\n            any_date_col = col.name\n        quoted = '{}'.format(column(dbcol.column_name).compile(dialect=db.engine.dialect))\n        if dbcol.sum:\n            metrics.append(M(metric_name=('sum__' + dbcol.column_name), verbose_name=('sum__' + dbcol.column_name), metric_type='sum', expression='SUM({})'.format(quoted)))\n        if dbcol.max:\n            metrics.append(M(metric_name=('max__' + dbcol.column_name), verbose_name=('max__' + dbcol.column_name), metric_type='max', expression='MAX({})'.format(quoted)))\n        if dbcol.min:\n            metrics.append(M(metric_name=('min__' + dbcol.column_name), verbose_name=('min__' + dbcol.column_name), metric_type='min', expression='MIN({})'.format(quoted)))\n        if dbcol.count_distinct:\n            metrics.append(M(metric_name=('count_distinct__' + dbcol.column_name), verbose_name=('count_distinct__' + dbcol.column_name), metric_type='count_distinct', expression='COUNT(DISTINCT {})'.format(quoted)))\n        dbcol.type = datatype\n        db.session.merge(self)\n        db.session.commit()\n    metrics.append(M(metric_name='count', verbose_name='COUNT(*)', metric_type='count', expression='COUNT(*)'))\n    for metric in metrics:\n        m = db.session.query(M).filter((M.metric_name == metric.metric_name)).filter((M.table_id == self.id)).first()\n        metric.table_id = self.id\n        if (not m):\n            db.session.add(metric)\n            db.session.commit()\n    if (not self.main_dttm_col):\n        self.main_dttm_col = any_date_col\n", "label": 1}
{"function": "\n\ndef fruchterman_reingold_layout(G, dim=2, k=None, pos=None, fixed=None, iterations=50, weight='weight', scale=1.0):\n    \"Position nodes using Fruchterman-Reingold force-directed algorithm. \\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n\\n    dim : int\\n       Dimension of layout\\n\\n    k : float (default=None)\\n       Optimal distance between nodes.  If None the distance is set to\\n       1/sqrt(n) where n is the number of nodes.  Increase this value\\n       to move nodes farther apart.\\n\\n\\n    pos : dict or None  optional (default=None)\\n       Initial positions for nodes as a dictionary with node as keys\\n       and values as a list or tuple.  If None, then nuse random initial\\n       positions.\\n\\n    fixed : list or None  optional (default=None)\\n      Nodes to keep fixed at initial position.\\n\\n    iterations : int  optional (default=50)\\n       Number of iterations of spring-force relaxation\\n\\n    weight : string or None   optional (default='weight')\\n        The edge attribute that holds the numerical value used for\\n        the edge weight.  If None, then all edge weights are 1.\\n\\n    scale : float (default=1.0)\\n        Scale factor for positions. The nodes are positioned \\n        in a box of size [0,scale] x [0,scale].  \\n\\n\\n    Returns\\n    -------\\n    dict :\\n       A dictionary of positions keyed by node\\n\\n    Examples\\n    --------\\n    >>> G=nx.path_graph(4)\\n    >>> pos=nx.spring_layout(G)\\n\\n    # The same using longer function name\\n    >>> pos=nx.fruchterman_reingold_layout(G)\\n    \"\n    try:\n        import numpy as np\n    except ImportError:\n        raise ImportError('fruchterman_reingold_layout() requires numpy: http://scipy.org/ ')\n    if (fixed is not None):\n        nfixed = dict(zip(G, range(len(G))))\n        fixed = np.asarray([nfixed[v] for v in fixed])\n    if (pos is not None):\n        pos_arr = np.asarray(np.random.random((len(G), dim)))\n        for (i, n) in enumerate(G):\n            if (n in pos):\n                pos_arr[i] = np.asarray(pos[n])\n    else:\n        pos_arr = None\n    if (len(G) == 0):\n        return {\n            \n        }\n    if (len(G) == 1):\n        return {\n            G.nodes()[0]: ((1,) * dim),\n        }\n    try:\n        if (len(G) < 500):\n            raise ValueError\n        A = nx.to_scipy_sparse_matrix(G, weight=weight, dtype='f')\n        pos = _sparse_fruchterman_reingold(A, dim, k, pos_arr, fixed, iterations)\n    except:\n        A = nx.to_numpy_matrix(G, weight=weight)\n        pos = _fruchterman_reingold(A, dim, k, pos_arr, fixed, iterations)\n    if (fixed is None):\n        pos = _rescale_layout(pos, scale=scale)\n    return dict(zip(G, pos))\n", "label": 1}
{"function": "\n\ndef check(self):\n    script = self.script\n    s = script.engine.current_scene\n    src = s.children[0]\n    g = src.children[0].children[1]\n    assert (g.glyph.glyph_source.glyph_position == 'center')\n    assert (g.glyph.glyph.vector_mode == 'use_normal')\n    assert (g.glyph.glyph.scale_factor == 0.5)\n    assert (g.actor.property.line_width == 1.0)\n    v = src.children[0].children[2]\n    glyph = v.glyph\n    gs = glyph.glyph_source\n    assert (gs.glyph_position == 'tail')\n    assert (gs.glyph_source == gs.glyph_list[1])\n    assert numpy.allclose(v.implicit_plane.normal, (0.0, 1.0, 0.0))\n    v = src.children[0].children[3]\n    glyph = v.glyph\n    gs = glyph.glyph_source\n    assert (gs.glyph_source == gs.glyph_list[2])\n    assert (gs.glyph_position == 'head')\n    assert numpy.allclose(v.implicit_plane.normal, (0.0, 1.0, 0.0))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef load_args():\n    if IronWorker.isLoaded:\n        return\n    for i in range(len(sys.argv)):\n        if (sys.argv[i] == '-id'):\n            IronWorker.arguments['task_id'] = sys.argv[(i + 1)]\n        if (sys.argv[i] == '-d'):\n            IronWorker.arguments['dir'] = sys.argv[(i + 1)]\n        if (sys.argv[i] == '-payload'):\n            IronWorker.arguments['payload_file'] = sys.argv[(i + 1)]\n        if (sys.argv[i] == '-config'):\n            IronWorker.arguments['config_file'] = sys.argv[(i + 1)]\n    if os.getenv('TASK_ID'):\n        IronWorker.arguments['task_id'] = os.getenv('TASK_ID')\n    if os.getenv('TASK_DIR'):\n        IronWorker.arguments['dir'] = os.getenv('TASK_DIR')\n    if os.getenv('PAYLOAD_FILE'):\n        IronWorker.arguments['payload_file'] = os.getenv('PAYLOAD_FILE')\n    if os.getenv('CONFIG_FILE'):\n        IronWorker.arguments['config_file'] = os.getenv('CONFIG_FILE')\n    if (('payload_file' in IronWorker.arguments) and file_exists(IronWorker.arguments['payload_file'])):\n        f = open(IronWorker.arguments['payload_file'], 'r')\n        try:\n            content = f.read()\n            f.close()\n            IronWorker.arguments['payload'] = json.loads(content)\n        except Exception as e:\n            print((\"Couldn't parse IronWorker payload into json, leaving as string. %s\" % e))\n    if (('config_file' in IronWorker.arguments) and file_exists(IronWorker.arguments['config_file'])):\n        f = open(IronWorker.arguments['config_file'])\n        try:\n            content = f.read()\n            f.close()\n            IronWorker.arguments['config'] = json.loads(content)\n        except Exception as e:\n            print((\"Couldn't parse IronWorker config into json. %s\" % e))\n    IronWorker.isLoaded = True\n", "label": 1}
{"function": "\n\ndef inputhook_wx3():\n    'Run the wx event loop by processing pending events only.\\n    \\n    This is like inputhook_wx1, but it keeps processing pending events\\n    until stdin is ready.  After processing all pending events, a call to \\n    time.sleep is inserted.  This is needed, otherwise, CPU usage is at 100%.\\n    This sleep time should be tuned though for best performance.\\n    '\n    try:\n        app = wx.GetApp()\n        if (app is not None):\n            assert wx.Thread_IsMain()\n            if (not isinstance(signal.getsignal(signal.SIGINT), collections.Callable)):\n                signal.signal(signal.SIGINT, signal.default_int_handler)\n            evtloop = wx.EventLoop()\n            ea = wx.EventLoopActivator(evtloop)\n            t = clock()\n            while (not stdin_ready()):\n                while evtloop.Pending():\n                    t = clock()\n                    evtloop.Dispatch()\n                app.ProcessIdle()\n                used_time = (clock() - t)\n                if (used_time > (5 * 60.0)):\n                    time.sleep(5.0)\n                elif (used_time > 10.0):\n                    time.sleep(1.0)\n                elif (used_time > 0.1):\n                    time.sleep(0.05)\n                else:\n                    time.sleep(0.001)\n            del ea\n    except KeyboardInterrupt:\n        pass\n    return 0\n", "label": 0}
{"function": "\n\ndef get_file_content(url, comes_from=None):\n    'Gets the content of a file; it may be a filename, file: URL, or\\n    http: URL.  Returns (location, content)'\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if ((scheme == 'file') and comes_from and comes_from.startswith('http')):\n            raise InstallationError(('Requirements file %s references URL %s, which is local' % (comes_from, url)))\n        if (scheme == 'file'):\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = ((match.group(1) + ':') + path.split('|', 1)[1])\n            path = urllib.unquote(path)\n            if path.startswith('/'):\n                path = ('/' + path.lstrip('/'))\n            url = path\n        else:\n            resp = urlopen(url)\n            return (geturl(resp), resp.read())\n    try:\n        f = open(url)\n        content = f.read()\n    except IOError:\n        e = sys.exc_info()[1]\n        raise InstallationError(('Could not open requirements file: %s' % str(e)))\n    else:\n        f.close()\n    return (url, content)\n", "label": 0}
{"function": "\n\n@frappe.whitelist()\ndef runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\n    'run controller method - old style'\n    if (not args):\n        args = (arg or '')\n    if dt:\n        if (not dn):\n            dn = dt\n        doc = frappe.get_doc(dt, dn)\n    else:\n        doc = frappe.get_doc(json.loads(docs))\n        doc._original_modified = doc.modified\n        doc.check_if_latest()\n    if (not doc.has_permission('read')):\n        frappe.msgprint(_('Not permitted'), raise_exception=True)\n    if doc:\n        try:\n            args = json.loads(args)\n        except ValueError:\n            args = args\n        (fnargs, varargs, varkw, defaults) = inspect.getargspec(getattr(doc, method))\n        if ((not fnargs) or ((len(fnargs) == 1) and (fnargs[0] == 'self'))):\n            r = doc.run_method(method)\n        elif (('args' in fnargs) or (not isinstance(args, dict))):\n            r = doc.run_method(method, args)\n        else:\n            r = doc.run_method(method, **args)\n        if r:\n            if cint(frappe.form_dict.get('as_csv')):\n                make_csv_output(r, doc.doctype)\n            else:\n                frappe.response['message'] = r\n        frappe.response.docs.append(doc)\n", "label": 1}
{"function": "\n\ndef exchange_declare(self, exchange=None, type='direct', durable=False, auto_delete=False, arguments=None, nowait=False, passive=False):\n    'Declare exchange.'\n    type = (type or 'direct')\n    exchange = (exchange or ('amq.%s' % type))\n    if passive:\n        if (exchange not in self.state.exchanges):\n            raise ChannelError('NOT_FOUND - no exchange {0!r} in vhost {1!r}'.format(exchange, (self.connection.client.virtual_host or '/')), (50, 10), 'Channel.exchange_declare', '404')\n        return\n    try:\n        prev = self.state.exchanges[exchange]\n        if (not self.typeof(exchange).equivalent(prev, exchange, type, durable, auto_delete, arguments)):\n            raise NotEquivalentError(NOT_EQUIVALENT_FMT.format(exchange, (self.connection.client.virtual_host or '/')))\n    except KeyError:\n        self.state.exchanges[exchange] = {\n            'type': type,\n            'durable': durable,\n            'auto_delete': auto_delete,\n            'arguments': (arguments or {\n                \n            }),\n            'table': [],\n        }\n", "label": 0}
{"function": "\n\ndef test_setup(tmpdir):\n    tmpdir = str(tmpdir)\n    etcdir = os.path.join(tmpdir, 'first', 'etc', 'squadron')\n    vardir = os.path.join(tmpdir, 'first', 'var', 'squadron')\n    with mock.patch('__builtin__.raw_input', side_effect=[etcdir, vardir]):\n        setup.setup(None, None)\n        assert os.path.exists(os.path.join(etcdir, 'config'))\n        assert os.path.exists(os.path.join(vardir, 'info.json'))\n    etcdir = os.path.join(tmpdir, 'second', 'etc', 'squadron')\n    vardir = os.path.join(tmpdir, 'second', 'var', 'squadron')\n    with mock.patch('__builtin__.raw_input', return_value=etcdir):\n        setup.setup(None, vardir)\n        assert os.path.exists(os.path.join(etcdir, 'config'))\n        assert os.path.exists(os.path.join(vardir, 'info.json'))\n    etcdir = os.path.join(tmpdir, 'third', 'etc', 'squadron')\n    vardir = os.path.join(tmpdir, 'third', 'var', 'squadron')\n    with mock.patch('__builtin__.raw_input', return_value=vardir):\n        setup.setup(etcdir, None)\n        assert os.path.exists(os.path.join(etcdir, 'config'))\n        assert os.path.exists(os.path.join(vardir, 'info.json'))\n    etcdir = os.path.join(tmpdir, 'fourth', 'etc', 'squadron')\n    vardir = os.path.join(tmpdir, 'fourth', 'var', 'squadron')\n    with mock.patch('__builtin__.raw_input', side_effect=Exception(\"Don't call this\")):\n        setup.setup(etcdir, vardir)\n        assert os.path.exists(os.path.join(etcdir, 'config'))\n        assert os.path.exists(os.path.join(vardir, 'info.json'))\n", "label": 0}
{"function": "\n\n@expose_action\ndef status(self):\n    'Get the status of the daemon.'\n    if (self.pidfile is None):\n        raise DaemonError('Cannot get status of daemon without PID file')\n    pid = self._read_pidfile()\n    if (pid is None):\n        self._emit_message('{prog} -- not running\\n'.format(prog=self.prog))\n        sys.exit(1)\n    proc = psutil.Process(pid)\n    data = {\n        'prog': self.prog,\n        'pid': pid,\n        'status': proc.status(),\n        'uptime': '0m',\n        'cpu': 0.0,\n        'memory': 0.0,\n    }\n    pgid = os.getpgid(pid)\n    for gproc in psutil.process_iter():\n        try:\n            if ((os.getpgid(gproc.pid) == pgid) and (gproc.pid != 0)):\n                data['cpu'] += gproc.cpu_percent(interval=0.1)\n                data['memory'] += gproc.memory_percent()\n        except (psutil.Error, OSError):\n            continue\n    try:\n        uptime_mins = int(round(((time.time() - proc.create_time()) / 60)))\n        (uptime_hours, uptime_mins) = divmod(uptime_mins, 60)\n        data['uptime'] = (str(uptime_mins) + 'm')\n        if uptime_hours:\n            (uptime_days, uptime_hours) = divmod(uptime_hours, 24)\n            data['uptime'] = ((str(uptime_hours) + 'h ') + data['uptime'])\n            if uptime_days:\n                data['uptime'] = ((str(uptime_days) + 'd ') + data['uptime'])\n    except psutil.Error:\n        pass\n    template = '{prog} -- pid: {pid}, status: {status}, uptime: {uptime}, %cpu: {cpu:.1f}, %mem: {memory:.1f}\\n'\n    self._emit_message(template.format(**data))\n", "label": 0}
{"function": "\n\ndef _match_search_query(left, right):\n    left_filter = set([value for (param_name, value) in left if ('filter' in _maybe_decode(param_name))])\n    right_filter = set([value for (param_name, value) in right if ('filter' in _maybe_decode(param_name))])\n    left_rest = set([(param_name, value) for (param_name, value) in left if ('filter' not in _maybe_decode(param_name))])\n    right_rest = set([(param_name, value) for (param_name, value) in right if ('filter' not in _maybe_decode(param_name))])\n    try:\n        log.info(simplejson.dumps({\n            'filter_differences': list(left_filter.symmetric_difference(right_filter)),\n            'rest_differences': list(left_rest.symmetric_difference(right_rest)),\n        }, encoding='utf-8'))\n    except Exception as e:\n        log.warning(e)\n    return ((left_filter == right_filter) and (left_rest == right_rest))\n", "label": 1}
{"function": "\n\ndef clean(self, value):\n    '\\n        Validates every value in the given list. A value is validated against\\n        the corresponding Field in self.fields.\\n\\n        For example, if this MultiValueField was instantiated with\\n        fields=(DateField(), TimeField()), clean() would call\\n        DateField.clean(value[0]) and TimeField.clean(value[1]).\\n        '\n    clean_data = []\n    errors = []\n    if ((not value) or isinstance(value, (list, tuple))):\n        if ((not value) or (not [v for v in value if (v not in self.empty_values)])):\n            if self.required:\n                raise ValidationError(self.error_messages['required'], code='required')\n            else:\n                return self.compress([])\n    else:\n        raise ValidationError(self.error_messages['invalid'], code='invalid')\n    for (i, field) in enumerate(self.fields):\n        try:\n            field_value = value[i]\n        except IndexError:\n            field_value = None\n        if (field_value in self.empty_values):\n            if self.require_all_fields:\n                if self.required:\n                    raise ValidationError(self.error_messages['required'], code='required')\n            elif field.required:\n                if (field.error_messages['incomplete'] not in errors):\n                    errors.append(field.error_messages['incomplete'])\n                continue\n        try:\n            clean_data.append(field.clean(field_value))\n        except ValidationError as e:\n            errors.extend((m for m in e.error_list if (m not in errors)))\n    if errors:\n        raise ValidationError(errors)\n    out = self.compress(clean_data)\n    self.validate(out)\n    self.run_validators(out)\n    return out\n", "label": 1}
{"function": "\n\ndef normal(self, size, avg=0.0, std=1.0, ndim=None, dtype=None, nstreams=None):\n    '\\n        Parameters\\n        ----------\\n        size\\n            Can be a list of integers or Theano variables (ex: the shape\\n            of another Theano Variable).\\n        dtype\\n            The output data type. If dtype is not specified, it will be\\n            inferred from the dtype of low and high, but will be at\\n            least as precise as floatX.\\n        nstreams\\n            Number of streams.\\n\\n        '\n    avg = as_tensor_variable(avg)\n    std = as_tensor_variable(std)\n    if (dtype is None):\n        dtype = scal.upcast(config.floatX, avg.dtype, std.dtype)\n    avg = cast(avg, dtype)\n    std = cast(std, dtype)\n    evened = False\n    constant = False\n    if (isinstance(size, tuple) and all([isinstance(i, (numpy.integer, int)) for i in size])):\n        constant = True\n        n_samples = numpy.prod(size, dtype='int64')\n        if ((n_samples % 2) == 1):\n            n_samples += 1\n            evened = True\n    else:\n        n_samples = (prod(size) + (prod(size) % 2))\n    flattened = self.uniform(size=(n_samples,), dtype=dtype, nstreams=nstreams)\n    if constant:\n        U1 = flattened[:(n_samples // 2)]\n        U2 = flattened[(n_samples // 2):]\n    else:\n        U1 = flattened[:(prod(flattened.shape) // 2)]\n        U2 = flattened[(prod(flattened.shape) // 2):]\n    sqrt_ln_U1 = sqrt(((- 2.0) * log(U1)))\n    first_half = (sqrt_ln_U1 * cos((numpy.array((2.0 * numpy.pi), dtype=dtype) * U2)))\n    second_half = (sqrt_ln_U1 * sin((numpy.array((2.0 * numpy.pi), dtype=dtype) * U2)))\n    normal_samples = join(0, first_half, second_half)\n    final_samples = None\n    if evened:\n        final_samples = normal_samples[:(- 1)]\n    elif constant:\n        final_samples = normal_samples\n    else:\n        final_samples = normal_samples[:prod(size)]\n    if (not size):\n        size = tensor.constant(size, dtype='int64')\n    final_samples = final_samples.reshape(size)\n    final_samples = (avg + (std * final_samples))\n    assert (final_samples.dtype == dtype)\n    return final_samples\n", "label": 1}
{"function": "\n\ndef test_ed25519py():\n    kp0 = ed25519py.crypto_sign_keypair(binary((' ' * 32)))\n    kp = ed25519py.crypto_sign_keypair()\n    signed = ed25519py.crypto_sign(binary('test'), kp.sk)\n    ed25519py.crypto_sign_open(signed, kp.vk)\n    try:\n        ed25519py.crypto_sign_open(signed, kp0.vk)\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign_keypair(binary((' ' * 33)))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign(binary(''), (binary(' ') * 31))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign_open(binary(''), (binary(' ') * 31))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n", "label": 0}
{"function": "\n\ndef whataremyips(bind_ip=None):\n    '\\n    Get \"our\" IP addresses (\"us\" being the set of services configured by\\n    one `*.conf` file). If our REST listens on a specific address, return it.\\n    Otherwise, if listen on \\'0.0.0.0\\' or \\'::\\' return all addresses, including\\n    the loopback.\\n\\n    :param str bind_ip: Optional bind_ip from a config file; may be IP address\\n                        or hostname.\\n    :returns: list of Strings of ip addresses\\n    '\n    if bind_ip:\n        try:\n            (_, _, _, _, sockaddr) = socket.getaddrinfo(bind_ip, None, 0, socket.SOCK_STREAM, 0, socket.AI_NUMERICHOST)[0]\n            if (sockaddr[0] not in ('0.0.0.0', '::')):\n                return [bind_ip]\n        except socket.gaierror:\n            pass\n    addresses = []\n    for interface in netifaces.interfaces():\n        try:\n            iface_data = netifaces.ifaddresses(interface)\n            for family in iface_data:\n                if (family not in (netifaces.AF_INET, netifaces.AF_INET6)):\n                    continue\n                for address in iface_data[family]:\n                    addr = address['addr']\n                    if (family == netifaces.AF_INET6):\n                        addr = expand_ipv6(addr.split('%')[0])\n                    addresses.append(addr)\n        except ValueError:\n            pass\n    return addresses\n", "label": 0}
{"function": "\n\ndef handle(self, fn_name, action, *args, **kwds):\n    self.parent.calls.append((self, fn_name, args, kwds))\n    if (action is None):\n        return None\n    elif (action == 'return self'):\n        return self\n    elif (action == 'return response'):\n        res = MockResponse(200, 'OK', {\n            \n        }, '')\n        return res\n    elif (action == 'return request'):\n        return Request('http://blah/')\n    elif action.startswith('error'):\n        code = action[(action.rfind(' ') + 1):]\n        try:\n            code = int(code)\n        except ValueError:\n            pass\n        res = MockResponse(200, 'OK', {\n            \n        }, '')\n        return self.parent.error('http', args[0], res, code, '', {\n            \n        })\n    elif (action == 'raise'):\n        raise urllib.error.URLError('blah')\n    assert False\n", "label": 0}
{"function": "\n\ndef ParseDepends(self, filename, must_exist=None, only_one=0):\n    '\\n        Parse a mkdep-style file for explicit dependencies.  This is\\n        completely abusable, and should be unnecessary in the \"normal\"\\n        case of proper SCons configuration, but it may help make\\n        the transition from a Make hierarchy easier for some people\\n        to swallow.  It can also be genuinely useful when using a tool\\n        that can write a .d file, but for which writing a scanner would\\n        be too complicated.\\n        '\n    filename = self.subst(filename)\n    try:\n        fp = open(filename, 'r')\n    except IOError:\n        if must_exist:\n            raise\n        return\n    lines = SCons.Util.LogicalLines(fp).readlines()\n    lines = [l for l in lines if (l[0] != '#')]\n    tdlist = []\n    for line in lines:\n        try:\n            (target, depends) = line.split(':', 1)\n        except (AttributeError, ValueError):\n            pass\n        else:\n            tdlist.append((target.split(), depends.split()))\n    if only_one:\n        targets = []\n        for td in tdlist:\n            targets.extend(td[0])\n        if (len(targets) > 1):\n            raise SCons.Errors.UserError((\"More than one dependency target found in `%s':  %s\" % (filename, targets)))\n    for (target, depends) in tdlist:\n        self.Depends(target, depends)\n", "label": 1}
{"function": "\n\ndef run_tests(test_labels, verbosity=1, interactive=True, extra_tests=[]):\n    \"\\n    worsk exactly as per normal test\\n    but only creates the test_db if it doesn't yet exist\\n    and does not destroy it when done\\n    tables are flushed and fixtures loaded between tests as per usual\\n    but if your schema has not changed then this saves significant amounts of time\\n    and speeds up the test cycle\\n\\n    Run the unit tests for all the test labels in the provided list.\\n    Labels must be of the form:\\n     - app.TestClass.test_method\\n        Run a single specific test method\\n     - app.TestClass\\n        Run all the test methods in a given class\\n     - app\\n        Search for doctests and unittests in the named application.\\n\\n    When looking for tests, the test runner will look in the models and\\n    tests modules for the application.\\n\\n    A list of 'extra' tests may also be provided; these tests\\n    will be added to the test suite.\\n\\n    Returns the number of tests that failed.\\n    \"\n    setup_test_environment()\n    settings.DEBUG = False\n    suite = unittest.TestSuite()\n    if test_labels:\n        for label in test_labels:\n            if ('.' in label):\n                suite.addTest(build_test(label))\n            else:\n                app = get_app(label)\n                suite.addTest(build_suite(app))\n    else:\n        for app in get_apps():\n            suite.addTest(build_suite(app))\n    for test in extra_tests:\n        suite.addTest(test)\n    suite = reorder_suite(suite, (TestCase,))\n    old_name = settings.DATABASES['default']['NAME']\n    from django.db.backends import creation\n    from django.db import connection, DatabaseError\n    if settings.DATABASES['default']['TEST_NAME']:\n        settings.DATABASES['default']['NAME'] = settings.DATABASES['default']['TEST_NAME']\n    else:\n        settings.DATABASES['default']['NAME'] = (creation.TEST_DATABASE_PREFIX + settings.DATABASES['default']['NAME'])\n    connection.settings_dict['DATABASE_NAME'] = settings.DATABASES['default']['NAME']\n    try:\n        if (settings.DATABASES['default']['ENGINE'] == 'sqlite3'):\n            if (not os.path.exists(settings.DATABASES['default']['NAME'])):\n                raise DatabaseError\n        cursor = connection.cursor()\n    except Exception:\n        settings.DATABASES['default']['NAME'] = old_name\n        connection.settings_dict['DATABASE_NAME'] = old_name\n        connection.creation.create_test_db(verbosity, autoclobber=True)\n    else:\n        connection.close()\n    settings.DATABASES['default']['SUPPORTS_TRANSACTIONS'] = connections_support_transactions()\n    result = unittest.TextTestRunner(verbosity=verbosity).run(suite)\n    settings.DATABASES['default']['NAME'] = old_name\n    connection.settings_dict['DATABASE_NAME'] = old_name\n    teardown_test_environment()\n    return (len(result.failures) + len(result.errors))\n", "label": 1}
{"function": "\n\ndef _select_range(self, multiselect, keep_anchor, node, idx):\n    'Selects a range between self._anchor and node or idx.\\n        If multiselect is True, it will be added to the selection, otherwise\\n        it will unselect everything before selecting the range. This is only\\n        called if self.multiselect is True.\\n        If keep anchor is False, the anchor is moved to node. This should\\n        always be True for keyboard selection.\\n        '\n    select = self.select_node\n    sister_nodes = self.get_selectable_nodes()\n    end = (len(sister_nodes) - 1)\n    last_node = self._anchor\n    last_idx = self._anchor_idx\n    if (last_node is None):\n        last_idx = end\n        last_node = sister_nodes[end]\n    elif ((last_idx > end) or (sister_nodes[last_idx] != last_node)):\n        try:\n            last_idx = self.get_index_of_node(last_node, sister_nodes)\n        except ValueError:\n            return\n    if ((idx > end) or (sister_nodes[idx] != node)):\n        try:\n            idx = self.get_index_of_node(node, sister_nodes)\n        except ValueError:\n            return\n    if (last_idx > idx):\n        (last_idx, idx) = (idx, last_idx)\n    if (not multiselect):\n        self.clear_selection()\n    for item in sister_nodes[last_idx:(idx + 1)]:\n        select(item)\n    if keep_anchor:\n        self._anchor = last_node\n        self._anchor_idx = last_idx\n    else:\n        self._anchor = node\n        self._anchor_idx = idx\n    self._last_selected_node = node\n    self._last_node_idx = idx\n", "label": 1}
{"function": "\n\ndef visit_metadata(self, metadata):\n    if (self.tables is not None):\n        tables = self.tables\n    else:\n        tables = list(metadata.tables.values())\n    try:\n        unsorted_tables = [t for t in tables if self._can_drop_table(t)]\n        collection = list(reversed(sort_tables_and_constraints(unsorted_tables, filter_fn=(lambda constraint: (False if ((not self.dialect.supports_alter) or (constraint.name is None)) else None)))))\n    except exc.CircularDependencyError as err2:\n        if (not self.dialect.supports_alter):\n            util.warn((\"Can't sort tables for DROP; an unresolvable foreign key dependency exists between tables: %s, and backend does not support ALTER.  To restore at least a partial sort, apply use_alter=True to ForeignKey and ForeignKeyConstraint objects involved in the cycle to mark these as known cycles that will be ignored.\" % ', '.join(sorted([t.fullname for t in err2.cycles]))))\n            collection = [(t, ()) for t in unsorted_tables]\n        else:\n            util.raise_from_cause(exc.CircularDependencyError(err2.args[0], err2.cycles, err2.edges, msg=(\"Can't sort tables for DROP; an unresolvable foreign key dependency exists between tables: %s.  Please ensure that the ForeignKey and ForeignKeyConstraint objects involved in the cycle have names so that they can be dropped using DROP CONSTRAINT.\" % ', '.join(sorted([t.fullname for t in err2.cycles])))))\n    seq_coll = [s for s in metadata._sequences.values() if ((s.column is None) and self._can_drop_sequence(s))]\n    event_collection = [t for (t, fks) in collection if (t is not None)]\n    metadata.dispatch.before_drop(metadata, self.connection, tables=event_collection, checkfirst=self.checkfirst, _ddl_runner=self)\n    for (table, fkcs) in collection:\n        if (table is not None):\n            self.traverse_single(table, drop_ok=True, _is_metadata_operation=True)\n        else:\n            for fkc in fkcs:\n                self.traverse_single(fkc)\n    for seq in seq_coll:\n        self.traverse_single(seq, drop_ok=True)\n    metadata.dispatch.after_drop(metadata, self.connection, tables=event_collection, checkfirst=self.checkfirst, _ddl_runner=self)\n", "label": 1}
{"function": "\n\ndef import_cmdset(path, cmdsetobj, emit_to_obj=None, no_logging=False):\n    \"\\n    This helper function is used by the cmdsethandler to load a cmdset\\n    instance from a python module, given a python_path. It's usually accessed\\n    through the cmdsethandler's add() and add_default() methods.\\n    path - This is the full path to the cmdset object on python dot-form\\n\\n    Args:\\n        path (str): The path to the command set to load.\\n        cmdsetobj (CmdSet): The database object/typeclass on which this cmdset is to be\\n            assigned (this can be also channels and exits, as well as players\\n            but there will always be such an object)\\n        emit_to_obj (Object, optional): If given, error is emitted to\\n            this object (in addition to logging)\\n        no_logging (bool, optional): Don't log/send error messages.\\n            This can be useful if import_cmdset is just used to check if\\n            this is a valid python path or not.\\n    Returns:\\n        cmdset (CmdSet): The imported command set. If an error was\\n            encountered, `commands.cmdsethandler._ErrorCmdSet` is returned\\n            for the benefit of the handler.\\n\\n    \"\n    python_paths = ([path] + [('%s.%s' % (prefix, path)) for prefix in _CMDSET_PATHS if (not path.startswith(prefix))])\n    errstring = ''\n    for python_path in python_paths:\n        if ('.' in path):\n            (modpath, classname) = python_path.rsplit('.', 1)\n        else:\n            raise ImportError((\"The path '%s' is not on the form modulepath.ClassName\" % path))\n        try:\n            cmdsetclass = _CACHED_CMDSETS.get(python_path, None)\n            if (not cmdsetclass):\n                try:\n                    module = import_module(modpath, package='evennia')\n                except ImportError:\n                    if (len(trace()) > 2):\n                        exc = sys.exc_info()\n                        raise_(exc[1], None, exc[2])\n                    else:\n                        errstring += _((\"\\n(Unsuccessfully tried '%s').\" % python_path))\n                        continue\n                try:\n                    cmdsetclass = getattr(module, classname)\n                except AttributeError:\n                    if (len(trace()) > 2):\n                        exc = sys.exc_info()\n                        raise_(exc[1], None, exc[2])\n                    else:\n                        errstring += _((\"\\n(Unsuccessfully tried '%s').\" % python_path))\n                        continue\n                _CACHED_CMDSETS[python_path] = cmdsetclass\n            if callable(cmdsetclass):\n                cmdsetclass = cmdsetclass(cmdsetobj)\n            errstring = ''\n            return cmdsetclass\n        except ImportError as e:\n            logger.log_trace()\n            errstring += _('\\nError loading cmdset {path}: \"{error}\"')\n            errstring = errstring.format(path=python_path, error=e)\n            break\n        except KeyError:\n            logger.log_trace()\n            errstring += _(\"\\nError in loading cmdset: No cmdset class '{classname}' in {path}.\")\n            errstring = errstring.format(classname=classname, path=python_path)\n            break\n        except SyntaxError as e:\n            logger.log_trace()\n            errstring += _('\\nSyntaxError encountered when loading cmdset \\'{path}\\': \"{error}\".')\n            errstring = errstring.format(path=python_path, error=e)\n            break\n        except Exception as e:\n            logger.log_trace()\n            errstring += _('\\nCompile/Run error when loading cmdset \\'{path}\\': \"{error}\".')\n            errstring = errstring.format(path=python_path, error=e)\n            break\n    if errstring:\n        errstring = errstring.strip()\n        if (not no_logging):\n            logger.log_err(errstring)\n            if (emit_to_obj and (not ServerConfig.objects.conf('server_starting_mode'))):\n                emit_to_obj.msg(errstring)\n        err_cmdset = _ErrorCmdSet()\n        err_cmdset.errmessage = (errstring + _('\\n (See log for details.)'))\n        return err_cmdset\n", "label": 1}
{"function": "\n\ndef _build_one(self, url):\n    'Get the given ``url`` from the app and write the matching file.\\n        '\n    client = self.app.test_client()\n    base_url = self.app.config['FREEZER_BASE_URL']\n    with conditional_context(self.url_for_logger, self.log_url_for):\n        with conditional_context(patch_url_for(self.app), self.app.config['FREEZER_RELATIVE_URLS']):\n            response = client.get(url, follow_redirects=True, base_url=base_url)\n    ignore_404 = self.app.config['FREEZER_IGNORE_404_NOT_FOUND']\n    if (response.status_code != 200):\n        if ((response.status_code == 404) and ignore_404):\n            warnings.warn(('Ignored %r on URL %s' % (response.status, url)), NotFoundWarning, stacklevel=3)\n        else:\n            raise ValueError(('Unexpected status %r on URL %s' % (response.status, url)))\n    destination_path = self.urlpath_to_filepath(url)\n    filename = os.path.join(self.root, *destination_path.split('/'))\n    if (not self.app.config['FREEZER_IGNORE_MIMETYPE_WARNINGS']):\n        basename = os.path.basename(filename)\n        (guessed_type, guessed_encoding) = mimetypes.guess_type(basename)\n        if (not guessed_type):\n            guessed_type = self.app.config['FREEZER_DEFAULT_MIMETYPE']\n        if (not (guessed_type == response.mimetype)):\n            warnings.warn(('Filename extension of %r (type %s) does not match Content-Type: %s' % (basename, guessed_type, response.content_type)), MimetypeMismatchWarning, stacklevel=3)\n    dirname = os.path.dirname(filename)\n    if (not os.path.isdir(dirname)):\n        os.makedirs(dirname)\n    content = response.data\n    if os.path.isfile(filename):\n        with open(filename, 'rb') as fd:\n            previous_content = fd.read()\n    else:\n        previous_content = None\n    if (content != previous_content):\n        with open(filename, 'wb') as fd:\n            fd.write(content)\n    response.close()\n    return filename\n", "label": 0}
{"function": "\n\ndef calibrate(args):\n    '\\n    %prog calibrate calibrate.JPG boxsize\\n\\n    Calibrate pixel-inch ratio and color adjustment.\\n    - `calibrate.JPG` is the photo containig a colorchecker\\n    - `boxsize` is the measured size for the boxes on printed colorchecker, in\\n      squared centimeter (cm2) units\\n    '\n    xargs = args[2:]\n    p = OptionParser(calibrate.__doc__)\n    (opts, args, iopts) = add_seeds_options(p, args)\n    if (len(args) != 2):\n        sys.exit((not p.print_help()))\n    (imagefile, boxsize) = args\n    boxsize = float(boxsize)\n    colorcheckerfile = op.join(datadir, 'colorchecker.txt')\n    colorchecker = []\n    expected = 0\n    for row in open(colorcheckerfile):\n        boxes = row.split()\n        colorchecker.append(boxes)\n        expected += len(boxes)\n    folder = op.split(imagefile)[0]\n    objects = seeds(([imagefile, '--outdir={0}'.format(folder)] + xargs))\n    nseeds = len(objects)\n    logging.debug('Found {0} boxes (expected={1})'.format(nseeds, expected))\n    assert ((expected - 4) <= nseeds <= (expected + 4)), 'Number of boxes drastically different from {0}'.format(expected)\n    boxes = [t.area for t in objects]\n    reject = reject_outliers(boxes)\n    retained_boxes = [b for (r, b) in zip(reject, boxes) if (not r)]\n    mbox = np.median(retained_boxes)\n    pixel_cm_ratio = ((mbox / boxsize) ** 0.5)\n    logging.debug('Median box size: {0} pixels. Measured box size: {1} cm2'.format(mbox, boxsize))\n    logging.debug('Pixel-cm ratio: {0}'.format(pixel_cm_ratio))\n    xs = [t.x for t in objects]\n    ys = [t.y for t in objects]\n    idx_xs = get_kmeans(xs, 6)\n    idx_ys = get_kmeans(ys, 4)\n    for (xi, yi, s) in zip(idx_xs, idx_ys, objects):\n        s.rank = (yi, xi)\n    objects.sort(key=(lambda x: x.rank))\n    colormap = []\n    for s in objects:\n        (x, y) = s.rank\n        (observed, expected) = (s.rgb, rgb_to_triplet(colorchecker[x][y]))\n        colormap.append((np.array(observed), np.array(expected)))\n    tr0 = np.eye(3).flatten()\n    ((print >> sys.stderr), 'Initial distance:', total_error(tr0, colormap))\n    tr = fmin(total_error, tr0, args=(colormap,))\n    tr.resize((3, 3))\n    ((print >> sys.stderr), 'RGB linear transform:\\n', tr)\n    calib = {\n        'PixelCMratio': pixel_cm_ratio,\n        'RGBtransform': tr.tolist(),\n    }\n    jsonfile = op.join(folder, 'calibrate.json')\n    fw = must_open(jsonfile, 'w')\n    ((print >> fw), json.dumps(calib, indent=4))\n    fw.close()\n    logging.debug('Calibration specs written to `{0}`.'.format(jsonfile))\n    return jsonfile\n", "label": 1}
{"function": "\n\ndef create_map_output_array(self, fn, array_args, axes, cartesian_product=False, name='output'):\n    '\\n    Given a function and its argument, use shape inference to figure out the\\n    result shape of the array and preallocate it.  If the result should be a\\n    scalar, just return a scalar variable.\\n    '\n    assert self.is_fn(fn), ('Expected function, got %s' % (fn,))\n    assert isinstance(array_args, (list, tuple)), ('Expected list of array args, got %s' % (array_args,))\n    axes = self.normalize_axes(array_args, axes)\n    n_indices = 0\n    for (arg, axis) in zip(array_args, axes):\n        r = self.rank(arg)\n        if (r == 0):\n            continue\n        if (axis is None):\n            if cartesian_product:\n                n_indices += self.rank(arg)\n            else:\n                n_indices = max(n_indices, self.rank(arg))\n        elif (r <= axis):\n            continue\n        elif cartesian_product:\n            n_indices += 1\n        else:\n            n_indices = max(n_indices, 1)\n    inner_args = self.get_slices(builder=self, array_arg_vars=array_args, axes=axes, index_input_vars=([zero_i64] * n_indices), cartesian_product=cartesian_product)\n    if cartesian_product:\n        extra_dims = []\n        for (array, axis) in zip(array_args, axes):\n            rank = self.rank(array)\n            if (axis is None):\n                dim = 1\n            elif (rank > axis):\n                dim = self.shape(array, axis)\n            else:\n                dim = 1\n            extra_dims.append(dim)\n        outer_shape_tuple = self.tuple(extra_dims)\n    else:\n        outer_shape_tuple = self.iter_bounds(array_args, axes)\n        if isinstance(outer_shape_tuple.type, ScalarT):\n            outer_shape_tuple = self.tuple([outer_shape_tuple])\n    return self.create_output_array(fn, inner_args, outer_shape_tuple, name)\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@register_specialize\n@gof.local_optimizer([Subtensor])\ndef local_subtensor_merge(node):\n    '\\n    Refactored optimization to deal with all cases of tensor merging.\\n    Given a subgraph of the form Subtensor(Subtensor(u)), the optimization\\n    expresses all slices in a canonical form, and then merges them together.\\n\\n    '\n    if isinstance(node.op, Subtensor):\n        u = node.inputs[0]\n        if (u.owner and isinstance(u.owner.op, Subtensor)):\n            x = u.owner.inputs[0]\n            slices1 = get_idx_list(u.owner.inputs, u.owner.op.idx_list)\n            slices2 = get_idx_list(node.inputs, node.op.idx_list)\n            try:\n                xshape = node.fgraph.shape_feature.shape_of[x]\n                ushape = node.fgraph.shape_feature.shape_of[u]\n            except AttributeError:\n                xshape = x.shape\n                ushape = u.shape\n            merged_slices = []\n            pos_2 = 0\n            pos_1 = 0\n            while ((pos_1 < len(slices1)) and (pos_2 < len(slices2))):\n                slice1 = slices1[pos_1]\n                if (type(slice1) is slice):\n                    merged_slices.append(merge_two_slices(slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]))\n                    pos_2 += 1\n                else:\n                    merged_slices.append(slice1)\n                pos_1 += 1\n            if (pos_2 < len(slices2)):\n                merged_slices += slices2[pos_2:]\n            else:\n                merged_slices += slices1[pos_1:]\n            merged_slices = make_constant(merged_slices)\n            subtens = Subtensor(merged_slices)\n            sl_ins = Subtensor.collapse(merged_slices, (lambda x: isinstance(x, T.Variable)))\n            out = subtens(x, *sl_ins)\n            copy_stack_trace([node.outputs[0], node.inputs[0]], out)\n            return [out]\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('grid_type', ALL_GRID_TYPES)\ndef test_append_two_quantities(self, grid_type):\n    g = self.grid[grid_type]\n    g['density'] = []\n    g['energy'] = []\n    g['density'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    assert g['density'][0]\n    assert g['density'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['density'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert g['energy'][0]\n    assert g['energy'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['energy'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert (g.n_dust == 1)\n", "label": 0}
{"function": "\n\ndef Time2Internaldate(date_time):\n    '\\'\"DD-Mmm-YYYY HH:MM:SS +HHMM\"\\' = Time2Internaldate(date_time)\\n\\n    Convert \\'date_time\\' to IMAP4 INTERNALDATE representation.\\n\\n    The date_time argument can be a number (int or float) representing\\n    seconds since epoch (as returned by time.time()), a 9-tuple\\n    representing local time, an instance of time.struct_time (as\\n    returned by time.localtime()), an aware datetime instance or a\\n    double-quoted string.  In the last case, it is assumed to already\\n    be in the correct format.'\n    from datetime import datetime, timezone, timedelta\n    if isinstance(date_time, (int, float)):\n        tt = time.localtime(date_time)\n    elif isinstance(date_time, tuple):\n        try:\n            gmtoff = date_time.tm_gmtoff\n        except AttributeError:\n            if time.daylight:\n                dst = date_time[8]\n                if (dst == (- 1)):\n                    dst = time.localtime(time.mktime(date_time))[8]\n                gmtoff = (- (time.timezone, time.altzone)[dst])\n            else:\n                gmtoff = (- time.timezone)\n        delta = timedelta(seconds=gmtoff)\n        dt = datetime(*date_time[:6], tzinfo=timezone(delta))\n    elif isinstance(date_time, datetime):\n        if (date_time.tzinfo is None):\n            raise ValueError('date_time must be aware')\n        dt = date_time\n    elif (isinstance(date_time, str) and ((date_time[0], date_time[(- 1)]) == ('\"', '\"'))):\n        return date_time\n    else:\n        raise ValueError('date_time not of a known type')\n    fmt = '\"%d-{}-%Y %H:%M:%S %z\"'.format(MonthNames[dt.month])\n    return dt.strftime(fmt)\n", "label": 0}
{"function": "\n\ndef _http_request(self, method, uri, headers=None, body_parts=None):\n    \"Makes an HTTP request using httplib.\\n\\n    Args:\\n      method: str example: 'GET', 'POST', 'PUT', 'DELETE', etc.\\n      uri: str or atom.http_core.Uri\\n      headers: dict of strings mapping to strings which will be sent as HTTP\\n               headers in the request.\\n      body_parts: list of strings, objects with a read method, or objects\\n                  which can be converted to strings using str. Each of these\\n                  will be sent in order as the body of the HTTP request.\\n    \"\n    if isinstance(uri, (str, unicode)):\n        uri = Uri.parse_uri(uri)\n    connection = self._get_connection(uri, headers=headers)\n    if self.debug:\n        connection.debuglevel = 1\n    if (connection.host != uri.host):\n        connection.putrequest(method, str(uri))\n    else:\n        connection.putrequest(method, uri._get_relative_path())\n    if ((uri.scheme == 'https') and (int((uri.port or 443)) == 443) and hasattr(connection, '_buffer') and isinstance(connection._buffer, list)):\n        header_line = ('Host: %s:443' % uri.host)\n        replacement_header_line = ('Host: %s' % uri.host)\n        try:\n            connection._buffer[connection._buffer.index(header_line)] = replacement_header_line\n        except ValueError:\n            pass\n    for (header_name, value) in headers.iteritems():\n        connection.putheader(header_name, value)\n    connection.endheaders()\n    if (body_parts and filter((lambda x: (x != '')), body_parts)):\n        for part in body_parts:\n            _send_data_part(part, connection)\n    return connection.getresponse()\n", "label": 1}
{"function": "\n\ndef process_m2m(self, instance, field):\n    auto_created_through_model = False\n    through = get_remote_field(field).through\n    auto_created_through_model = through._meta.auto_created\n    if auto_created_through_model:\n        return self.process_field(instance, field)\n    kwargs = {\n        \n    }\n    if (field.name in self.generate_m2m):\n        related_fks = [fk for fk in through._meta.fields if (isinstance(fk, related.ForeignKey) and (get_remote_field_to(fk) is get_remote_field_to(field)))]\n        self_fks = [fk for fk in through._meta.fields if (isinstance(fk, related.ForeignKey) and (get_remote_field_to(fk) is self.model))]\n        assert (len(related_fks) == 1)\n        assert (len(self_fks) == 1)\n        related_fk = related_fks[0]\n        self_fk = self_fks[0]\n        (min_count, max_count) = self.generate_m2m[field.name]\n        intermediary_model = generators.MultipleInstanceGenerator(AutoFixture(through, field_values={\n            self_fk.name: instance,\n            related_fk.name: generators.InstanceGenerator(autofixture.get(get_remote_field_to(field))),\n        }), min_count=min_count, max_count=max_count, **kwargs).generate()\n", "label": 1}
{"function": "\n\ndef url_for(endpoint, **values):\n    'Generates a URL to the given endpoint with the method provided.\\n\\n    Variable arguments that are unknown to the target endpoint are appended\\n    to the generated URL as query arguments.  If the value of a query argument\\n    is ``None``, the whole pair is skipped.  In case blueprints are active\\n    you can shortcut references to the same blueprint by prefixing the\\n    local endpoint with a dot (``.``).\\n\\n    This will reference the index function local to the current blueprint::\\n\\n        url_for(\\'.index\\')\\n\\n    For more information, head over to the :ref:`Quickstart <url-building>`.\\n\\n    To integrate applications, :class:`Flask` has a hook to intercept URL build\\n    errors through :attr:`Flask.url_build_error_handlers`.  The `url_for`\\n    function results in a :exc:`~werkzeug.routing.BuildError` when the current\\n    app does not have a URL for the given endpoint and values.  When it does, the\\n    :data:`~flask.current_app` calls its :attr:`~Flask.url_build_error_handlers` if\\n    it is not ``None``, which can return a string to use as the result of\\n    `url_for` (instead of `url_for`\\'s default to raise the\\n    :exc:`~werkzeug.routing.BuildError` exception) or re-raise the exception.\\n    An example::\\n\\n        def external_url_handler(error, endpoint, values):\\n            \"Looks up an external URL when `url_for` cannot build a URL.\"\\n            # This is an example of hooking the build_error_handler.\\n            # Here, lookup_url is some utility function you\\'ve built\\n            # which looks up the endpoint in some external URL registry.\\n            url = lookup_url(endpoint, **values)\\n            if url is None:\\n                # External lookup did not have a URL.\\n                # Re-raise the BuildError, in context of original traceback.\\n                exc_type, exc_value, tb = sys.exc_info()\\n                if exc_value is error:\\n                    raise exc_type, exc_value, tb\\n                else:\\n                    raise error\\n            # url_for will use this result, instead of raising BuildError.\\n            return url\\n\\n        app.url_build_error_handlers.append(external_url_handler)\\n\\n    Here, `error` is the instance of :exc:`~werkzeug.routing.BuildError`, and\\n    `endpoint` and `values` are the arguments passed into `url_for`.  Note\\n    that this is for building URLs outside the current application, and not for\\n    handling 404 NotFound errors.\\n\\n    .. versionadded:: 0.10\\n       The `_scheme` parameter was added.\\n\\n    .. versionadded:: 0.9\\n       The `_anchor` and `_method` parameters were added.\\n\\n    .. versionadded:: 0.9\\n       Calls :meth:`Flask.handle_build_error` on\\n       :exc:`~werkzeug.routing.BuildError`.\\n\\n    :param endpoint: the endpoint of the URL (name of the function)\\n    :param values: the variable arguments of the URL rule\\n    :param _external: if set to ``True``, an absolute URL is generated. Server\\n      address can be changed via ``SERVER_NAME`` configuration variable which\\n      defaults to `localhost`.\\n    :param _scheme: a string specifying the desired URL scheme. The `_external`\\n      parameter must be set to ``True`` or a :exc:`ValueError` is raised. The default\\n      behavior uses the same scheme as the current request, or\\n      ``PREFERRED_URL_SCHEME`` from the :ref:`app configuration <config>` if no\\n      request context is available. As of Werkzeug 0.10, this also can be set\\n      to an empty string to build protocol-relative URLs.\\n    :param _anchor: if provided this is added as anchor to the URL.\\n    :param _method: if provided this explicitly specifies an HTTP method.\\n    '\n    appctx = _app_ctx_stack.top\n    reqctx = _request_ctx_stack.top\n    if (appctx is None):\n        raise RuntimeError('Attempted to generate a URL without the application context being pushed. This has to be executed when application context is available.')\n    if (reqctx is not None):\n        url_adapter = reqctx.url_adapter\n        blueprint_name = request.blueprint\n        if (not reqctx.request._is_old_module):\n            if (endpoint[:1] == '.'):\n                if (blueprint_name is not None):\n                    endpoint = (blueprint_name + endpoint)\n                else:\n                    endpoint = endpoint[1:]\n        elif ('.' not in endpoint):\n            if (blueprint_name is not None):\n                endpoint = ((blueprint_name + '.') + endpoint)\n        elif endpoint.startswith('.'):\n            endpoint = endpoint[1:]\n        external = values.pop('_external', False)\n    else:\n        url_adapter = appctx.url_adapter\n        if (url_adapter is None):\n            raise RuntimeError('Application was not able to create a URL adapter for request independent URL generation. You might be able to fix this by setting the SERVER_NAME config variable.')\n        external = values.pop('_external', True)\n    anchor = values.pop('_anchor', None)\n    method = values.pop('_method', None)\n    scheme = values.pop('_scheme', None)\n    appctx.app.inject_url_defaults(endpoint, values)\n    if (scheme is not None):\n        if (not external):\n            raise ValueError('When specifying _scheme, _external must be True')\n        url_adapter.url_scheme = scheme\n    try:\n        rv = url_adapter.build(endpoint, values, method=method, force_external=external)\n    except BuildError as error:\n        values['_external'] = external\n        values['_anchor'] = anchor\n        values['_method'] = method\n        return appctx.app.handle_url_build_error(error, endpoint, values)\n    if (anchor is not None):\n        rv += ('#' + url_quote(anchor))\n    return rv\n", "label": 1}
{"function": "\n\ndef info_xy(self, data_x, data_y, settings):\n    try:\n        value = self.get_data_xy(int((data_x + 0.5)), int((data_y + 0.5)))\n    except Exception as e:\n        value = None\n    system = settings.get('wcs_coords', None)\n    format = settings.get('wcs_display', 'sexagesimal')\n    (ra_lbl, dec_lbl) = (six.unichr(945), six.unichr(948))\n    ts = time.time()\n    try:\n        if (self.wcs is None):\n            self.logger.debug('No WCS for this image')\n            ra_txt = dec_txt = 'NO WCS'\n        elif (self.wcs.coordsys == 'raw'):\n            self.logger.debug('No coordinate system determined')\n            ra_txt = dec_txt = 'NO WCS'\n        elif (self.wcs.coordsys == 'pixel'):\n            args = ([data_x, data_y] + self.revnaxis)\n            (x, y) = self.wcs.pixtosystem(args, system=system, coords='data')\n            ra_txt = ('%+.3f' % x)\n            dec_txt = ('%+.3f' % y)\n            (ra_lbl, dec_lbl) = ('X', 'Y')\n        else:\n            args = ([data_x, data_y] + self.revnaxis)\n            (lon_deg, lat_deg) = self.wcs.pixtosystem(args, system=system, coords='data')\n            if (format == 'sexagesimal'):\n                if (system in ('galactic', 'ecliptic')):\n                    (sign, deg, min, sec) = wcs.degToDms(lon_deg, isLatitude=False)\n                    ra_txt = ('+%03d:%02d:%06.3f' % (deg, min, sec))\n                else:\n                    (deg, min, sec) = wcs.degToHms(lon_deg)\n                    ra_txt = ('%02d:%02d:%06.3f' % (deg, min, sec))\n                (sign, deg, min, sec) = wcs.degToDms(lat_deg)\n                if (sign < 0):\n                    sign = '-'\n                else:\n                    sign = '+'\n                dec_txt = ('%s%02d:%02d:%06.3f' % (sign, deg, min, sec))\n            else:\n                ra_txt = ('%+10.7f' % lon_deg)\n                dec_txt = ('%+10.7f' % lat_deg)\n            if (system == 'galactic'):\n                (ra_lbl, dec_lbl) = ('l', 'b')\n            elif (system == 'ecliptic'):\n                (ra_lbl, dec_lbl) = (six.unichr(955), six.unichr(946))\n            elif (system == 'helioprojective'):\n                ra_txt = ('%+5.3f' % (lon_deg * 3600))\n                dec_txt = ('%+5.3f' % (lat_deg * 3600))\n                (ra_lbl, dec_lbl) = ('x-Solar', 'y-Solar')\n    except Exception as e:\n        self.logger.warning(('Bad coordinate conversion: %s' % str(e)))\n        ra_txt = 'BAD WCS'\n        dec_txt = 'BAD WCS'\n        try:\n            (type_, value_, tb) = sys.exc_info()\n            tb_str = ''.join(traceback.format_tb(tb))\n            self.logger.error(('Traceback:\\n%s' % tb_str))\n        except Exception:\n            tb_str = 'Traceback information unavailable.'\n            self.logger.error(tb_str)\n    te = (time.time() - ts)\n    info = Bunch.Bunch(itype='astro', data_x=data_x, data_y=data_y, x=data_x, y=data_y, ra_txt=ra_txt, dec_txt=dec_txt, ra_lbl=ra_lbl, dec_lbl=dec_lbl, value=value)\n    return info\n", "label": 1}
{"function": "\n\n@client.set_process_callback\ndef process(frames):\n    'Main callback.'\n    events = {\n        \n    }\n    buf = memoryview(audioport.get_buffer()).cast('f')\n    for (offset, data) in midiport.incoming_midi_events():\n        if (len(data) == 3):\n            (status, pitch, vel) = bytes(data)\n            status >>= 4\n            if ((status == NOTEON) and (vel > 0)):\n                events.setdefault(offset, []).append((pitch, vel))\n            elif (status in (NOTEON, NOTEOFF)):\n                events.setdefault(offset, []).append((pitch, 0))\n            else:\n                pass\n        else:\n            pass\n    for i in range(len(buf)):\n        buf[i] = 0\n        try:\n            eventlist = events[i]\n        except KeyError:\n            pass\n        else:\n            for (pitch, vel) in eventlist:\n                if (pitch not in voices):\n                    if (not vel):\n                        break\n                    voices[pitch] = Voice(pitch)\n                voices[pitch].trigger(vel)\n        for voice in voices.values():\n            voice.update()\n            if (voice.weight > 0):\n                buf[i] += (voice.weight * math.sin(((2 * math.pi) * voice.time)))\n                voice.time += voice.time_increment\n                if (voice.time >= 1):\n                    voice.time -= 1\n    dead = [k for (k, v) in voices.items() if (v.weight <= 0)]\n    for pitch in dead:\n        del voices[pitch]\n", "label": 1}
{"function": "\n\ndef test_result_generation_when_a_quarantined_test_has_two_cases():\n    jobstep = JobStep(id=uuid.uuid4(), project_id=uuid.uuid4(), job_id=uuid.uuid4())\n    fp = StringIO(SAMPLE_XUNIT_DOUBLE_CASES.replace('<testcase c', '<testcase quarantined=\"1\" c', 1).replace('<testcase c', '<testcase quarantined=\"1\" c', 1))\n    handler = XunitHandler(jobstep)\n    results = handler.get_tests(fp)\n    assert (len(results) == 2)\n    r1 = results[0]\n    assert (type(r1) is TestResult)\n    assert (r1.step == jobstep)\n    assert (r1.package is None)\n    assert (r1.name == 'test_simple.SampleTest.test_falsehood')\n    assert (r1.duration == 750.0)\n    assert (r1.result == Result.quarantined_failed)\n    assert (r1.message == 'test_simple.py:8: in test_falsehood\\n    assert False\\nE   AssertionError: assert False\\n\\ntest_simple.py:4: in tearDown\\n    1/0\\nE   ZeroDivisionError: integer division or modulo by zero')\n    assert (r1.reruns == 3)\n    r2 = results[1]\n    assert (type(r2) is TestResult)\n    assert (r2.step == jobstep)\n    assert (r2.package is None)\n    assert (r2.name == 'test_simple.SampleTest.test_truth')\n    assert (r2.duration == 1250.0)\n    assert (r2.result == Result.failed)\n    assert (r2.message == 'test_simple.py:4: in tearDown\\n    1/0\\nE   ZeroDivisionError: integer division or modulo by zero')\n    assert (r2.reruns == 0)\n", "label": 1}
{"function": "\n\n@access.user\n@loadmodel(model='item', level=AccessType.WRITE)\n@describeRoute(Description('Set geospatial fields on an item.').notes('Set geospatial fields to null to delete them.').param('id', 'The ID of the item.', paramType='path').param('body', 'A JSON object containing the geospatial fields to add.', paramType='body').errorResponse('ID was invalid.').errorResponse('Invalid JSON was passed in request body.').errorResponse('Geospatial key name was invalid.').errorResponse('Geospatial field did not contain valid GeoJSON.').errorResponse('Write access was denied for the item.', 403))\ndef setGeospatial(self, item, params):\n    \"\\n        Set geospatial data on an item.\\n\\n        :param item: item on which to set geospatial data.\\n        :type item: dict[str, unknown]\\n        :param params: parameters to the API call, unused.\\n        :type params: dict[str, unknown]\\n        :returns: filtered fields of the item with geospatial data appended to\\n                 its 'geo' field.\\n        :rtype : dict[str, unknown]\\n        :raise RestException: on malformed, forbidden, or unauthorized API call.\\n        \"\n    geospatial = self.getBodyJson()\n    for (k, v) in six.viewitems(geospatial):\n        if (('.' in k) or (k[0] == '$')):\n            raise RestException(('Geospatial key name %s must not contain a period or begin with a dollar sign.' % k))\n        if v:\n            try:\n                GeoJSON.to_instance(v, strict=True)\n            except ValueError:\n                raise RestException(('Geospatial field with key %s does not contain valid GeoJSON: %s' % (k, v)))\n    if (GEOSPATIAL_FIELD not in item):\n        item[GEOSPATIAL_FIELD] = dict()\n    item[GEOSPATIAL_FIELD].update(six.viewitems(geospatial))\n    keys = [k for (k, v) in six.viewitems(item[GEOSPATIAL_FIELD]) if (v is None)]\n    for key in keys:\n        del item[GEOSPATIAL_FIELD][key]\n    item = self.model('item').updateItem(item)\n    return self._filter(item)\n", "label": 0}
{"function": "\n\ndef __init__(self, parser, path, source_resource=None):\n    self.parser = parser\n    self.path = path\n    self.source_resource = source_resource\n    self.entities = OrderedDict()\n    self.escape_quotes_on = (('mobile/android/base' in path) and (parser is DTDParser))\n    if source_resource:\n        for (key, entity) in source_resource.entities.items():\n            self.entities[key] = copy_source_entity(entity)\n    try:\n        self.structure = parser.get_structure(read_file(path, uncomment_moz_langpack=((parser is IncParser) and (not source_resource))))\n    except IOError:\n        if source_resource:\n            return\n        else:\n            raise\n    comments = []\n    current_order = 0\n    for obj in self.structure:\n        if isinstance(obj, silme.core.entity.Entity):\n            if self.escape_quotes_on:\n                obj.value = self.unescape_quotes(obj.value)\n            entity = SilmeEntity(obj, comments, current_order)\n            self.entities[entity.key] = entity\n            current_order += 1\n            comments = []\n        elif isinstance(obj, silme.core.structure.Comment):\n            for comment in obj:\n                lines = unicode(comment).strip().split('\\n')\n                comments += [line.strip() for line in lines]\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    match = self._mapper.match(environ['PATH_INFO'])\n    if (not match):\n        return self._not_found(environ, start_response)\n    controller_name = match['controller']\n    action_name = match['action']\n    controller_action = (controller_name, action_name)\n    del match['controller']\n    del match['action']\n    environ['concurrence.application'] = self\n    request = Request(environ)\n    response = Response(content_type=self.default_content_type, charset=self.default_charset)\n    with self._scoped_request.set(request):\n        with self._scoped_response.set(response):\n            if (not (controller_action in self._filter_chain)):\n\n                def last(next, *args, **kwargs):\n                    return self.call_controller(controller_name, action_name, *args, **kwargs)\n                filter_chain = []\n                for (i, filter) in enumerate((self._filters.get(controller_action, []) + [last, None])):\n\n                    def create_next(_i, _filter):\n\n                        def next(*args, **kwargs):\n                            return _filter(filter_chain[(_i + 1)], *args, **kwargs)\n                        return next\n                    filter_chain.append(create_next(i, filter))\n                self._filter_chain[controller_action] = filter_chain\n            result = self._filter_chain[controller_action][0](**match)\n    if (type(result) == str):\n        response.body = result\n    elif (type(result) == unicode):\n        response.unicode_body = result\n    elif (result is None):\n        response.body = ''\n    elif (type(result) == type(response)):\n        response = result\n    else:\n        assert False, ('result must be None, str, unicode or response object, found: %s' % type(result))\n    return response(environ, start_response)\n", "label": 0}
{"function": "\n\ndef Buffer(self, geometries, distances, unit=None, unionResults=False, inSR=None, outSR=None, bufferSR=None):\n    'The buffer operation is performed on a geometry service resource.\\n           The result of this operation is buffer polygons at the specified\\n           distances for the input geometry array. An option is available to\\n           union buffers at each distance.'\n    if isinstance(geometries, geometry.Geometry):\n        geometries = [geometries]\n    if isinstance(distances, (list, tuple)):\n        distances = ','.join((str(distance) for distance in distances))\n    geometry_types = set([x.__geometry_type__ for x in geometries])\n    assert (len(geometry_types) == 1), 'Too many geometry types'\n    geo_json = json.dumps({\n        'geometryType': list(geometry_types)[0],\n        'geometries': [geo._json_struct_without_sr for geo in geometries],\n    })\n    if (inSR is None):\n        inSR = geometries[0].spatialReference.wkid\n    if (outSR is None):\n        outSR = geometries[0].spatialReference.wkid\n    if (bufferSR is None):\n        bufferSR = geometries[0].spatialReference.wkid\n    return self._get_subfolder('buffer', GeometryResult, {\n        'geometries': geo_json,\n        'distances': distances,\n        'unit': unit,\n        'unionResults': unionResults,\n        'inSR': inSR,\n        'outSR': outSR,\n        'bufferSR': bufferSR,\n    })\n", "label": 0}
{"function": "\n\ndef _validatePolicy(self, policy):\n    '\\n        Validate a policy JSON object.  Only a limited set of keys is\\n        supported, and each of them has a restricted data type.\\n\\n        :param policy: JSON object to validate.  This may also be a Python\\n                           dictionary as if the JSON was already decoded.\\n        :returns: a validate policy dictionary.\\n        '\n    if (not isinstance(policy, dict)):\n        try:\n            policy = json.loads(policy)\n        except ValueError:\n            raise RestException('The policy parameter must be JSON.')\n    if (not isinstance(policy, dict)):\n        raise RestException('The policy parameter must be a dictionary.')\n    validKeys = []\n    for key in dir(self):\n        if key.startswith('_validate_'):\n            validKeys.append(key.split('_validate_', 1)[1])\n    for key in list(policy):\n        if key.startswith('_'):\n            del policy[key]\n    for key in policy:\n        if (key not in validKeys):\n            raise RestException(('%s is not a valid quota policy key.  Valid keys are %s.' % (key, ', '.join(sorted(validKeys)))))\n        funcName = ('_validate_' + key)\n        policy[key] = getattr(self, funcName)(policy[key])\n    return policy\n", "label": 0}
{"function": "\n\ndef update_contenttypes(app_config, verbosity=2, interactive=True, using=DEFAULT_DB_ALIAS, **kwargs):\n    '\\n    Creates content types for models in the given app, removing any model\\n    entries that no longer have a matching model class.\\n    '\n    if (not app_config.models_module):\n        return\n    try:\n        ContentType = apps.get_model('contenttypes', 'ContentType')\n    except LookupError:\n        return\n    if (not router.allow_migrate_model(using, ContentType)):\n        return\n    ContentType.objects.clear_cache()\n    app_label = app_config.label\n    app_models = {model._meta.model_name: model for model in app_config.get_models()}\n    if (not app_models):\n        return\n    content_types = {ct.model: ct for ct in ContentType.objects.using(using).filter(app_label=app_label)}\n    to_remove = [ct for (model_name, ct) in six.iteritems(content_types) if (model_name not in app_models)]\n    cts = [ContentType(app_label=app_label, model=model_name) for (model_name, model) in six.iteritems(app_models) if (model_name not in content_types)]\n    ContentType.objects.using(using).bulk_create(cts)\n    if (verbosity >= 2):\n        for ct in cts:\n            print((\"Adding content type '%s | %s'\" % (ct.app_label, ct.model)))\n    if to_remove:\n        if interactive:\n            content_type_display = '\\n'.join((('    %s | %s' % (ct.app_label, ct.model)) for ct in to_remove))\n            ok_to_delete = input((\"The following content types are stale and need to be deleted:\\n\\n%s\\n\\nAny objects related to these content types by a foreign key will also\\nbe deleted. Are you sure you want to delete these content types?\\nIf you're unsure, answer 'no'.\\n\\n    Type 'yes' to continue, or 'no' to cancel: \" % content_type_display))\n        else:\n            ok_to_delete = False\n        if (ok_to_delete == 'yes'):\n            for ct in to_remove:\n                if (verbosity >= 2):\n                    print((\"Deleting stale content type '%s | %s'\" % (ct.app_label, ct.model)))\n                ct.delete()\n        elif (verbosity >= 2):\n            print('Stale content types remain.')\n", "label": 1}
{"function": "\n\ndef mod_data(fsclient):\n    '\\n    Generate the module arguments for the shim data\\n    '\n    sync_refs = ['modules', 'states', 'grains', 'renderers', 'returners']\n    ret = {\n        \n    }\n    envs = fsclient.envs()\n    ver_base = ''\n    for env in envs:\n        files = fsclient.file_list(env)\n        for ref in sync_refs:\n            mods_data = {\n                \n            }\n            pref = '_{0}'.format(ref)\n            for fn_ in sorted(files):\n                if fn_.startswith(pref):\n                    if fn_.endswith(('.py', '.so', '.pyx')):\n                        full = salt.utils.url.create(fn_)\n                        mod_path = fsclient.cache_file(full, env)\n                        if (not os.path.isfile(mod_path)):\n                            continue\n                        mods_data[os.path.basename(fn_)] = mod_path\n                        chunk = salt.utils.get_hash(mod_path)\n                        ver_base += chunk\n            if mods_data:\n                if (ref in ret):\n                    ret[ref].update(mods_data)\n                else:\n                    ret[ref] = mods_data\n    if (not ret):\n        return {\n            \n        }\n    ver = hashlib.sha1(ver_base).hexdigest()\n    ext_tar_path = os.path.join(fsclient.opts['cachedir'], 'ext_mods.{0}.tgz'.format(ver))\n    mods = {\n        'version': ver,\n        'file': ext_tar_path,\n    }\n    if os.path.isfile(ext_tar_path):\n        return mods\n    tfp = tarfile.open(ext_tar_path, 'w:gz')\n    verfile = os.path.join(fsclient.opts['cachedir'], 'ext_mods.ver')\n    with salt.utils.fopen(verfile, 'w+') as fp_:\n        fp_.write(ver)\n    tfp.add(verfile, 'ext_version')\n    for ref in ret:\n        for fn_ in ret[ref]:\n            tfp.add(ret[ref][fn_], os.path.join(ref, fn_))\n    tfp.close()\n    return mods\n", "label": 1}
{"function": "\n\ndef python(self, options):\n    import code\n    imported_objects = {\n        \n    }\n    try:\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter\n        readline.set_completer(rlcompleter.Completer(imported_objects).complete)\n        readline_doc = getattr(readline, '__doc__', '')\n        if ((readline_doc is not None) and ('libedit' in readline_doc)):\n            readline.parse_and_bind('bind ^I rl_complete')\n        else:\n            readline.parse_and_bind('tab:complete')\n    if (not options['no_startup']):\n        for pythonrc in (os.environ.get('PYTHONSTARTUP'), '~/.pythonrc.py'):\n            if (not pythonrc):\n                continue\n            pythonrc = os.path.expanduser(pythonrc)\n            if (not os.path.isfile(pythonrc)):\n                continue\n            try:\n                with open(pythonrc) as handle:\n                    exec(compile(handle.read(), pythonrc, 'exec'), imported_objects)\n            except NameError:\n                pass\n    code.interact(local=imported_objects)\n", "label": 0}
{"function": "\n\ndef process_handler_result(self, result, task=None):\n    '\\n        Process result received from the task handler.\\n\\n        Result could be:\\n        * None\\n        * Task instance\\n        * Data instance.\\n        '\n    if isinstance(result, Task):\n        self.add_task(result)\n    elif isinstance(result, Data):\n        handler = self.find_data_handler(result)\n        try:\n            data_result = handler(**result.storage)\n            if (data_result is None):\n                pass\n            else:\n                for something in data_result:\n                    self.process_handler_result(something, task)\n        except Exception as ex:\n            self.process_handler_error(('data_%s' % result.handler_key), ex, task)\n    elif (result is None):\n        pass\n    elif isinstance(result, Exception):\n        handler = self.find_task_handler(task)\n        handler_name = getattr(handler, '__name__', 'NONE')\n        self.process_handler_error(handler_name, result, task)\n    elif isinstance(result, dict):\n        if (result.get('type') == 'stat'):\n            for (name, count) in result['counters'].items():\n                self.stat.inc(name, count)\n            for (name, items) in result['collections'].items():\n                for item in items:\n                    self.stat.collect(name, item)\n        else:\n            raise SpiderError(('Unknown result type: %s' % result))\n    else:\n        raise SpiderError(('Unknown result type: %s' % result))\n", "label": 1}
{"function": "\n\ndef __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, eval_every=10, iterations=50, gamma_threshold=0.001):\n    \"\\n        If given, start training from the iterable `corpus` straight away. If not given,\\n        the model is left untrained (presumably because you want to call `update()` manually).\\n\\n        `num_topics` is the number of requested latent topics to be extracted from\\n        the training corpus.\\n\\n        `id2word` is a mapping from word ids (integers) to words (strings). It is\\n        used to determine the vocabulary size, as well as for debugging and topic\\n        printing.\\n\\n        `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\\n        (theta) and topic-word (lambda) distributions. Both default to a symmetric\\n        1.0/num_topics prior.\\n\\n        `alpha` can be set to an explicit array = prior of your choice. It also\\n        support special values of 'asymmetric' and 'auto': the former uses a fixed\\n        normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\\n        prior directly from your data.\\n\\n        `eta' can be a scalar for a symmetric prior over topic/word\\n        distributions, or a matrix of shape num_topics x num_words,\\n        which can be used to impose asymmetric priors over the word\\n        distribution on a per-topic basis. This may be useful if you\\n        want to seed certain topics with particular words by boosting\\n        the priors for those words.\\n\\n        Turn on `distributed` to force distributed computing (see the `web tutorial <http://radimrehurek.com/gensim/distributed.html>`_\\n        on how to set up a cluster of machines for gensim).\\n\\n        Calculate and log perplexity estimate from the latest mini-batch every\\n        `eval_every` model updates (setting this to 1 slows down training ~2x;\\n        default is 10 for better performance). Set to None to disable perplexity estimation.\\n\\n        Example:\\n\\n        >>> lda = LdaModel(corpus, num_topics=100)  # train model\\n        >>> print(lda[doc_bow]) # get topic probability distribution for a document\\n        >>> lda.update(corpus2) # update the LDA model with additional documents\\n        >>> print(lda[doc_bow])\\n\\n        >>> lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\\n\\n        \"\n    self.id2word = id2word\n    if ((corpus is None) and (self.id2word is None)):\n        raise ValueError('at least one of corpus/id2word must be specified, to establish input space dimensionality')\n    if (self.id2word is None):\n        logger.warning('no word id mapping provided; initializing from corpus, assuming identity')\n        self.id2word = utils.dict_from_corpus(corpus)\n        self.num_terms = len(self.id2word)\n    elif (len(self.id2word) > 0):\n        self.num_terms = (1 + max(self.id2word.keys()))\n    else:\n        self.num_terms = 0\n    if (self.num_terms == 0):\n        raise ValueError('cannot compute LDA over an empty collection (no terms)')\n    self.distributed = bool(distributed)\n    self.num_topics = int(num_topics)\n    self.chunksize = chunksize\n    self.decay = decay\n    self.num_updates = 0\n    self.passes = passes\n    self.update_every = update_every\n    self.eval_every = eval_every\n    self.optimize_alpha = (alpha == 'auto')\n    if ((alpha == 'symmetric') or (alpha is None)):\n        logger.info(('using symmetric alpha at %s' % (1.0 / num_topics)))\n        self.alpha = numpy.asarray([(1.0 / num_topics) for i in xrange(num_topics)])\n    elif (alpha == 'asymmetric'):\n        self.alpha = numpy.asarray([(1.0 / (i + numpy.sqrt(num_topics))) for i in xrange(num_topics)])\n        self.alpha /= self.alpha.sum()\n        logger.info(('using asymmetric alpha %s' % list(self.alpha)))\n    elif (alpha == 'auto'):\n        self.alpha = numpy.asarray([(1.0 / num_topics) for i in xrange(num_topics)])\n        logger.info(('using autotuned alpha, starting with %s' % list(self.alpha)))\n    else:\n        self.alpha = (alpha if isinstance(alpha, numpy.ndarray) else numpy.asarray(([alpha] * num_topics)))\n        if (len(self.alpha) != num_topics):\n            raise RuntimeError('invalid alpha shape (must match num_topics)')\n    if (eta is None):\n        self.eta = (1.0 / num_topics)\n    else:\n        self.eta = eta\n    self.iterations = iterations\n    self.gamma_threshold = gamma_threshold\n    if (not distributed):\n        logger.info('using serial LDA version on this node')\n        self.dispatcher = None\n        self.numworkers = 1\n    else:\n        if self.optimize_alpha:\n            raise NotImplementedError('auto-optimizing alpha not implemented in distributed LDA')\n        try:\n            import Pyro4\n            dispatcher = Pyro4.Proxy('PYRONAME:gensim.lda_dispatcher')\n            dispatcher._pyroOneway.add('exit')\n            logger.debug(('looking for dispatcher at %s' % str(dispatcher._pyroUri)))\n            dispatcher.initialize(id2word=self.id2word, num_topics=num_topics, chunksize=chunksize, alpha=alpha, eta=eta, distributed=False)\n            self.dispatcher = dispatcher\n            self.numworkers = len(dispatcher.getworkers())\n            logger.info(('using distributed version with %i workers' % self.numworkers))\n        except Exception as err:\n            logger.error(('failed to initialize distributed LDA (%s)' % err))\n            raise RuntimeError(('failed to initialize distributed LDA (%s)' % err))\n    self.state = LdaState(self.eta, (self.num_topics, self.num_terms))\n    self.state.sstats = numpy.random.gamma(100.0, (1.0 / 100.0), (self.num_topics, self.num_terms))\n    self.sync_state()\n    if (corpus is not None):\n        self.update(corpus)\n", "label": 1}
{"function": "\n\ndef update_frame(self, focus=None):\n    if self.viewer.paused:\n        thead_attr = 'thead.paused'\n    elif (not self.viewer.active):\n        thead_attr = 'thead.inactive'\n    else:\n        thead_attr = 'thead'\n    self.thead.set_attr_map({\n        None: thead_attr,\n    })\n    for (x, (__, __, __, order)) in enumerate(self.columns):\n        attr = ((thead_attr + '.sorted') if (order is self.order) else None)\n        widget = self.thead.base_widget.contents[x][0]\n        (text, __) = widget.get_text()\n        widget.set_text((attr, text))\n    if self.viewer.paused:\n        return\n    stats = self.get_stats()\n    if (stats is None):\n        return\n    title = self.title\n    time = self.time\n    if (title or time):\n        if (time is not None):\n            time_string = '{:%H:%M:%S}'.format(time)\n        if (title and time):\n            markup = [('weak', title), ' ', time_string]\n        elif title:\n            markup = title\n        else:\n            markup = time_string\n        meta_info = urwid.Text(markup, align='right')\n    else:\n        meta_info = None\n    fraction_string = '({0}/{1})'.format(fmt.format_time(self.cpu_time), fmt.format_time(self.wall_time))\n    try:\n        cpu_usage = (self.cpu_time / self.wall_time)\n    except ZeroDivisionError:\n        cpu_usage = 0.0\n    cpu_info = urwid.Text(['CPU ', fmt.markup_percent(cpu_usage, unit=True), ' ', ('weak', fraction_string)])\n    col_opts = ('weight', 1, False)\n    self.header.contents = [(w, col_opts) for w in [cpu_info, meta_info] if w]\n", "label": 1}
{"function": "\n\ndef getCovMatrix(self, x=None, z=None, mode=None):\n    self.checkInputGetCovMatrix(x, z, mode)\n    Q = self.para[0]\n    if (mode == 'self_test'):\n        (nn, D) = z.shape\n    else:\n        (nn, D) = x.shape\n    assert (Q == old_div(len(self.hyp), (1 + (2 * D))))\n    w = np.exp(self.hyp[:Q])\n    m = np.exp(np.reshape(self.hyp[Q:(Q + (Q * D))], (D, Q)))\n    v = np.exp((2 * np.reshape(self.hyp[(Q + (Q * D)):], (D, Q))))\n    if (mode == 'self_test'):\n        d2 = np.zeros((nn, 1, D))\n    elif (mode == 'train'):\n        d2 = np.zeros((nn, nn, D))\n        for j in range(D):\n            xslice = np.atleast_2d(x[:, j]).T\n            d2[:, :, j] = spdist.cdist(xslice, xslice, 'sqeuclidean')\n    elif (mode == 'cross'):\n        d2 = np.zeros((nn, z.shape[0], D))\n        for j in range(D):\n            xslice = np.atleast_2d(x[:, j]).T\n            zslice = np.atleast_2d(z[:, j]).T\n            d2[:, :, j] = spdist.cdist(xslice, zslice, 'sqeuclidean')\n    d = np.sqrt(d2)\n    k = (lambda d2v_dm: (np.exp((((- 2) * (np.pi ** 2)) * d2v_dm[0])) * np.cos(((2 * np.pi) * d2v_dm[1]))))\n    km = (lambda dm: ((((- 2) * np.pi) * np.tan(((2 * np.pi) * dm))) * dm))\n    kv = (lambda d2v: ((- d2v) * ((2 * np.pi) ** 2)))\n    A = 0.0\n    c = 1.0\n    qq = list(range(Q))\n    for q in qq:\n        C = (w[q] * c)\n        for j in range(D):\n            C = (C * k(((d2[:, :, j] * v[(j, q)]), (d[:, :, j] * m[(j, q)]))))\n            A = (A + C)\n    return A\n", "label": 0}
{"function": "\n\ndef force_text(s, encoding='utf-8', strings_only=False, errors='strict'):\n    \"\\n    Similar to smart_text, except that lazy instances are resolved to\\n    strings, rather than kept as lazy objects.\\n\\n    If strings_only is True, don't convert (some) non-string-like objects.\\n    \"\n    if isinstance(s, text_type):\n        return s\n    if (strings_only and is_protected_type(s)):\n        return s\n    try:\n        if (not isinstance(s, string_types)):\n            if hasattr(s, '__unicode__'):\n                s = s.__unicode__()\n            elif (not PY2):\n                if isinstance(s, bytes):\n                    s = text_type(s, encoding, errors)\n                else:\n                    s = text_type(s)\n            else:\n                s = text_type(bytes(s), encoding, errors)\n        else:\n            s = s.decode(encoding, errors)\n    except UnicodeDecodeError as e:\n        if (not isinstance(s, Exception)):\n            raise UnicodeDecodeError(*e.args)\n        else:\n            s = ' '.join([force_text(arg, encoding, strings_only, errors) for arg in s])\n    return s\n", "label": 1}
{"function": "\n\ndef calculate_hash(self, callback=None):\n    cookie_string = ''\n    try:\n        if self.blacklist:\n            string_with_spaces = re.sub(self.cookie_regex, '', self.request.headers['Cookie']).strip()\n            cookie_string = ''.join(string_with_spaces.split(' '))\n        else:\n            cookies_matrix = re.findall(self.cookie_regex, self.request.headers['Cookie'])\n            for cookie_tuple in cookies_matrix:\n                for item in cookie_tuple:\n                    if item:\n                        cookie_string += item.strip()\n    except KeyError:\n        pass\n    request_mod = ((self.request.method + self.request.url) + self.request.version)\n    request_mod = ((request_mod + self.request.body) + cookie_string)\n    try:\n        request_mod = (request_mod + self.request.headers['User-Agent'])\n    except KeyError:\n        pass\n    try:\n        request_mod = (request_mod + self.request.headers['Sec-Websocket-Key'])\n    except KeyError:\n        pass\n    md5_hash = hashlib.md5()\n    md5_hash.update(request_mod)\n    self.request_hash = md5_hash.hexdigest()\n    self.file_path = os.path.join(self.cache_dir, self.request_hash)\n    if callback:\n        callback(self.request_hash)\n", "label": 0}
{"function": "\n\ndef parse_code(self, selector):\n    \"\\n        Parse code against Java grammar\\n\\n        @param selector: scope selector (refer to GrammarParser's selector)\\n        \"\n    try:\n        scope = GrammarParser(sublime.decode_value(sublime.load_resource('Packages/Javatar/grammars/Java8.javatar-grammar')))\n        parse_output = scope.parse_grammar(self.view.substr(sublime.Region(0, self.view.size())))\n        status_text = ''\n        if parse_output['success']:\n            if (selector == ''):\n                nodes = scope.find_all()\n            elif (selector == '#'):\n                selections = self.view.sel()\n                nodes = scope.find_by_region([0, 0])\n                if selections:\n                    first_sel = selections[0]\n                    if first_sel.empty():\n                        nodes = scope.find_by_region([first_sel.begin(), first_sel.end()])\n                    else:\n                        nodes = scope.find_inside_region([first_sel.begin(), first_sel.end()])\n            else:\n                nodes = scope.find_by_selectors(selector)\n            if (selector != '#'):\n                status_text = 'Parsing got {} tokens'.format(len(nodes))\n            for node in nodes:\n                if (selector == '#'):\n                    if (status_text == ''):\n                        status_text += node['name']\n                    else:\n                        status_text += (' ' + node['name'])\n                else:\n                    Logger().none('#{begin}:{end} => {name}'.format_map(node))\n                    Logger().none('   => {value}'.format_map(node))\n            Logger().none('Total: {} tokens'.format(len(nodes)))\n        if (selector != '#'):\n            if ((status_text != '') and (str(parse_output['end']) == str(self.view.size()))):\n                status_text += ' in {elapse_time:.2f}s'.format(elapse_time=scope.get_elapse_time())\n            else:\n                status_text = ('Parsing failed [%s/%s] in {%.2f}s' % (parse_output['end'], self.view.size(), scope.get_elapse_time()))\n        Logger().none(('Ending: %s/%s' % (parse_output['end'], self.view.size())))\n        Logger().none('Parsing Time: {elapse_time:.2f}s'.format(elapse_time=scope.get_elapse_time()))\n        StatusManager().show_status(status_text)\n    except Exception as e:\n        ActionHistory().add_action('javatar.commands.utils.parse_code', 'Error while parsing', e)\n", "label": 1}
{"function": "\n\ndef check_main_evennia_dependencies():\n    '\\n    Checks and imports the Evennia dependencies. This must be done\\n    already before the paths are set up.\\n\\n    Returns:\\n        not_error (bool): True if no dependency error was found.\\n\\n    '\n    error = False\n    pversion = '.'.join((str(num) for num in sys.version_info if (type(num) == int)))\n    if (pversion < PYTHON_MIN):\n        print(ERROR_PYTHON_VERSION.format(pversion=pversion, python_min=PYTHON_MIN))\n        error = True\n    try:\n        import twisted\n        tversion = twisted.version.short()\n        if (tversion < TWISTED_MIN):\n            print(ERROR_TWISTED_VERSION.format(tversion=tversion, twisted_min=TWISTED_MIN))\n            error = True\n    except ImportError:\n        print(ERROR_NOTWISTED)\n        error = True\n    try:\n        dversion = '.'.join((str(num) for num in django.VERSION if (type(num) == int)))\n        dversion_main = '.'.join(dversion.split('.')[:2])\n        if (dversion < DJANGO_MIN):\n            print(ERROR_DJANGO_MIN.format(dversion=dversion_main, django_min=DJANGO_MIN))\n            error = True\n        elif (DJANGO_MIN <= dversion < DJANGO_REC):\n            print(NOTE_DJANGO_MIN.format(dversion=dversion_main, django_rec=DJANGO_REC))\n        elif (DJANGO_REC < dversion_main):\n            print(NOTE_DJANGO_NEW.format(dversion=dversion_main, django_rec=DJANGO_REC))\n    except ImportError:\n        print(ERROR_NODJANGO)\n        error = True\n    if error:\n        sys.exit()\n    return (not error)\n", "label": 1}
{"function": "\n\ndef check_environ(environ):\n    assert isinstance(environ, dict), ('Environment is not of the right type: %r (environment: %r)' % (type(environ), environ))\n    for key in ['REQUEST_METHOD', 'SERVER_NAME', 'SERVER_PORT', 'wsgi.version', 'wsgi.input', 'wsgi.errors', 'wsgi.multithread', 'wsgi.multiprocess', 'wsgi.run_once']:\n        assert (key in environ), ('Environment missing required key: %r' % key)\n    for key in ['HTTP_CONTENT_TYPE', 'HTTP_CONTENT_LENGTH']:\n        assert (key not in environ), ('Environment should not have the key: %s (use %s instead)' % (key, key[5:]))\n    if ('QUERY_STRING' not in environ):\n        warnings.warn('QUERY_STRING is not in the WSGI environment; the cgi module will use sys.argv when this variable is missing, so application errors are more likely', WSGIWarning)\n    for key in environ.keys():\n        if ('.' in key):\n            continue\n        assert isinstance(environ[key], str), ('Environmental variable %s is not a string: %r (value: %r)' % (key, type(environ[key]), environ[key]))\n    assert isinstance(environ['wsgi.version'], tuple), ('wsgi.version should be a tuple (%r)' % environ['wsgi.version'])\n    assert (environ['wsgi.url_scheme'] in ('http', 'https')), ('wsgi.url_scheme unknown: %r' % environ['wsgi.url_scheme'])\n    check_input(environ['wsgi.input'])\n    check_errors(environ['wsgi.errors'])\n    if (environ['REQUEST_METHOD'] not in ('GET', 'HEAD', 'POST', 'OPTIONS', 'PUT', 'DELETE', 'TRACE')):\n        warnings.warn(('Unknown REQUEST_METHOD: %r' % environ['REQUEST_METHOD']), WSGIWarning)\n    assert ((not environ.get('SCRIPT_NAME')) or environ['SCRIPT_NAME'].startswith('/')), (\"SCRIPT_NAME doesn't start with /: %r\" % environ['SCRIPT_NAME'])\n    assert ((not environ.get('PATH_INFO')) or environ['PATH_INFO'].startswith('/')), (\"PATH_INFO doesn't start with /: %r\" % environ['PATH_INFO'])\n    if environ.get('CONTENT_LENGTH'):\n        assert (int(environ['CONTENT_LENGTH']) >= 0), ('Invalid CONTENT_LENGTH: %r' % environ['CONTENT_LENGTH'])\n    if (not environ.get('SCRIPT_NAME')):\n        assert ('PATH_INFO' in environ), \"One of SCRIPT_NAME or PATH_INFO are required (PATH_INFO should at least be '/' if SCRIPT_NAME is empty)\"\n    assert (environ.get('SCRIPT_NAME') != '/'), \"SCRIPT_NAME cannot be '/'; it should instead be '', and PATH_INFO should be '/'\"\n", "label": 1}
{"function": "\n\n@tornado.gen.coroutine\ndef _stream_return(self):\n    while ((not self._closing) and ((not self._connecting_future.done()) or (self._connecting_future.result() is not True))):\n        (yield self._connecting_future)\n    unpacker = msgpack.Unpacker()\n    while (not self._closing):\n        try:\n            self._read_until_future = self._stream.read_bytes(4096, partial=True)\n            wire_bytes = (yield self._read_until_future)\n            unpacker.feed(wire_bytes)\n            for framed_msg in unpacker:\n                if six.PY3:\n                    framed_msg = salt.transport.frame.decode_embedded_strs(framed_msg)\n                header = framed_msg['head']\n                body = framed_msg['body']\n                message_id = header.get('mid')\n                if (message_id in self.send_future_map):\n                    self.send_future_map.pop(message_id).set_result(body)\n                    self.remove_message_timeout(message_id)\n                elif (self._on_recv is not None):\n                    self.io_loop.spawn_callback(self._on_recv, header, body)\n                else:\n                    log.error('Got response for message_id {0} that we are not tracking'.format(message_id))\n        except tornado.iostream.StreamClosedError as e:\n            log.debug('tcp stream to {0}:{1} closed, unable to recv'.format(self.host, self.port))\n            for future in six.itervalues(self.send_future_map):\n                future.set_exception(e)\n            self.send_future_map = {\n                \n            }\n            if self._closing:\n                return\n            if self.disconnect_callback:\n                self.disconnect_callback()\n            if self._connecting_future.done():\n                self._connecting_future = self.connect()\n            (yield self._connecting_future)\n        except Exception as e:\n            log.error('Exception parsing response', exc_info=True)\n            for future in six.itervalues(self.send_future_map):\n                future.set_exception(e)\n            self.send_future_map = {\n                \n            }\n            if self._closing:\n                return\n            if self.disconnect_callback:\n                self.disconnect_callback()\n            if self._connecting_future.done():\n                self._connecting_future = self.connect()\n            (yield self._connecting_future)\n", "label": 1}
{"function": "\n\ndef initial_validation(request, prefix):\n    \"\\n    Returns the related model instance and post data to use in the\\n    comment/rating views below.\\n\\n    Both comments and ratings have a ``prefix_ACCOUNT_REQUIRED``\\n    setting. If this is ``True`` and the user is unauthenticated, we\\n    store their post data in their session, and redirect to login with\\n    the view's url (also defined by the prefix arg) as the ``next``\\n    param. We can then check the session data once they log in,\\n    and complete the action authenticated.\\n\\n    On successful post, we pass the related object and post data back,\\n    which may have come from the session, for each of the comments and\\n    ratings view functions to deal with as needed.\\n    \"\n    post_data = request.POST\n    login_required_setting_name = (prefix.upper() + 'S_ACCOUNT_REQUIRED')\n    posted_session_key = ('unauthenticated_' + prefix)\n    redirect_url = ''\n    if getattr(settings, login_required_setting_name, False):\n        if (not request.user.is_authenticated()):\n            request.session[posted_session_key] = request.POST\n            error(request, _('You must be logged in. Please log in or sign up to complete this action.'))\n            redirect_url = ('%s?next=%s' % (settings.LOGIN_URL, reverse(prefix)))\n        elif (posted_session_key in request.session):\n            post_data = request.session.pop(posted_session_key)\n    if (not redirect_url):\n        model_data = post_data.get('content_type', '').split('.', 1)\n        if (len(model_data) != 2):\n            return HttpResponseBadRequest()\n        try:\n            model = apps.get_model(*model_data)\n            obj = model.objects.get(id=post_data.get('object_pk', None))\n        except (TypeError, ObjectDoesNotExist, LookupError):\n            redirect_url = '/'\n    if redirect_url:\n        if request.is_ajax():\n            return HttpResponse(dumps({\n                'location': redirect_url,\n            }))\n        else:\n            return redirect(redirect_url)\n    return (obj, post_data)\n", "label": 0}
{"function": "\n\ndef ssl_options_to_context(ssl_options):\n    'Try to convert an ``ssl_options`` dictionary to an\\n    `~ssl.SSLContext` object.\\n\\n    The ``ssl_options`` dictionary contains keywords to be passed to\\n    `ssl.wrap_socket`.  In Python 3.2+, `ssl.SSLContext` objects can\\n    be used instead.  This function converts the dict form to its\\n    `~ssl.SSLContext` equivalent, and may be used when a component which\\n    accepts both forms needs to upgrade to the `~ssl.SSLContext` version\\n    to use features like SNI or NPN.\\n    '\n    if isinstance(ssl_options, dict):\n        assert all(((k in _SSL_CONTEXT_KEYWORDS) for k in ssl_options)), ssl_options\n    if ((not hasattr(ssl, 'SSLContext')) or isinstance(ssl_options, ssl.SSLContext)):\n        return ssl_options\n    context = ssl.SSLContext(ssl_options.get('ssl_version', ssl.PROTOCOL_SSLv23))\n    if ('certfile' in ssl_options):\n        context.load_cert_chain(ssl_options['certfile'], ssl_options.get('keyfile', None))\n    if ('cert_reqs' in ssl_options):\n        context.verify_mode = ssl_options['cert_reqs']\n    if ('ca_certs' in ssl_options):\n        context.load_verify_locations(ssl_options['ca_certs'])\n    if ('ciphers' in ssl_options):\n        context.set_ciphers(ssl_options['ciphers'])\n    if hasattr(ssl, 'OP_NO_COMPRESSION'):\n        context.options |= ssl.OP_NO_COMPRESSION\n    return context\n", "label": 0}
{"function": "\n\ndef __init__(self, global_conf, script, path=None, include_os_environ=True, query_string=None):\n    if global_conf:\n        raise NotImplemented('global_conf is no longer supported for CGIApplication (use make_cgi_application); please pass None instead')\n    self.script_filename = script\n    if (path is None):\n        path = os.environ.get('PATH', '').split(':')\n    self.path = path\n    if ('?' in script):\n        assert (query_string is None), (\"You cannot have '?' in your script name (%r) and also give a query_string (%r)\" % (script, query_string))\n        (script, query_string) = script.split('?', 1)\n    if (os.path.abspath(script) != script):\n        for path_dir in self.path:\n            if os.path.exists(os.path.join(path_dir, script)):\n                self.script = os.path.join(path_dir, script)\n                break\n        else:\n            raise CGIError(('Script %r not found in path %r' % (script, self.path)))\n    else:\n        self.script = script\n    self.include_os_environ = include_os_environ\n    self.query_string = query_string\n", "label": 0}
{"function": "\n\ndef fromstring(stringrep, fieldsep='\\t'):\n    'Parse the string representation of a dataset and return a Dataset instance.\\n    \\n    See the documentation for fromfile() for information about file format.\\n    \\n    '\n\n    def dataitem(item, v):\n        item = item.strip()\n        intervention = False\n        missing = False\n        if (item[0] == '!'):\n            intervention = True\n            item = item[1:]\n        elif (item[(- 1)] == '!'):\n            intervention = True\n            item = item[:(- 1)]\n        if ((item[0] in ('x', 'X')) or (item[(- 1)] in ('x', 'X'))):\n            missing = True\n            item = ('0' if (not isinstance(v, ClassVariable)) else v.labels[0])\n        val = item\n        if isinstance(v, ClassVariable):\n            try:\n                val = v.label2int[val]\n            except KeyError:\n                raise ClassVariableError()\n        elif isinstance(v, DiscreteVariable):\n            try:\n                val = int(val)\n            except ValueError:\n                msg = ('Invalid value for discrete variable %s: %s' % (v.name, val))\n                raise ParsingError(msg)\n        elif isinstance(v, ContinuousVariable):\n            try:\n                val = float(val)\n            except ValueError:\n                msg = ('Invalid value for continuous variable %s: %s' % (v.name, val))\n                raise ParsingError(msg)\n        elif ('.' in val):\n            try:\n                val = float(val)\n            except:\n                msg = ('Cannot convert value %s to a float.' % val)\n                raise ParsingError(msg)\n        else:\n            try:\n                val = int(val)\n            except:\n                msg = ('Cannot convert value %s to an int.' % val)\n                raise ParsingError(msg)\n        return (val, missing, intervention)\n    dtype_re = re.compile('([\\\\w\\\\d_-]+)[\\\\(]*([\\\\w\\\\d\\\\s,]*)[\\\\)]*')\n\n    def variable(v):\n        v = v.strip('\"')\n        parts = v.split(',', 1)\n        if (len(parts) is 2):\n            (name, dtype) = parts\n            match = dtype_re.match(dtype)\n            if (not match):\n                raise ParsingError(('Error parsing variable header: %s' % v))\n            (dtype_name, dtype_param) = match.groups()\n            dtype_name = dtype_name.lower()\n        else:\n            name = parts[0]\n            (dtype_name, dtype_param) = (None, None)\n        vartypes = {\n            None: Variable,\n            'continuous': ContinuousVariable,\n            'discrete': DiscreteVariable,\n            'class': ClassVariable,\n        }\n        return vartypes[dtype_name](name, dtype_param)\n    lines = (l.strip() for l in stringrep.splitlines() if l)\n    lines = (l for l in lines if (not l.startswith('#')))\n    variables = lines.next().split(fieldsep)\n    variables = N.array([variable(v) for v in variables])\n    d = [[c for c in row.split(fieldsep)] for row in lines]\n    samplenames = (True if (len(d[0]) == (len(variables) + 1)) else False)\n    samples = None\n    if samplenames:\n        samples = N.array([Sample(row[0]) for row in d])\n        d = [row[1:] for row in d]\n    d = N.array([[dataitem(c, v) for (c, v) in zip(row, variables)] for row in d])\n    (obs, missing, interventions) = d.transpose(2, 0, 1)\n    dtype = ('int' if (obs.dtype.kind is 'i') else obs.dtype)\n    d = Dataset(obs.astype(dtype), missing.astype(bool), interventions.astype(bool), variables, samples)\n    d.check_arities()\n    return d\n", "label": 1}
{"function": "\n\ndef fetch_events(query):\n    queue_id = query['queue_id']\n    dont_block = query['dont_block']\n    last_event_id = query['last_event_id']\n    user_profile_id = query['user_profile_id']\n    new_queue_data = query.get('new_queue_data')\n    user_profile_email = query['user_profile_email']\n    client_type_name = query['client_type_name']\n    handler_id = query['handler_id']\n    try:\n        was_connected = False\n        orig_queue_id = queue_id\n        extra_log_data = ''\n        if (queue_id is None):\n            if dont_block:\n                client = allocate_client_descriptor(new_queue_data)\n                queue_id = client.event_queue.id\n            else:\n                raise JsonableError(\"Missing 'queue_id' argument\")\n        else:\n            if (last_event_id is None):\n                raise JsonableError(\"Missing 'last_event_id' argument\")\n            client = get_client_descriptor(queue_id)\n            if (client is None):\n                raise JsonableError(('Bad event queue id: %s' % (queue_id,)))\n            if (user_profile_id != client.user_profile_id):\n                raise JsonableError('You are not authorized to get events from this queue')\n            client.event_queue.prune(last_event_id)\n            was_connected = client.finish_current_handler()\n        if ((not client.event_queue.empty()) or dont_block):\n            response = dict(events=client.event_queue.contents(), handler_id=handler_id)\n            if (orig_queue_id is None):\n                response['queue_id'] = queue_id\n            extra_log_data = ('[%s/%s]' % (queue_id, len(response['events'])))\n            if was_connected:\n                extra_log_data += ' [was connected]'\n            return dict(type='response', response=response, extra_log_data=extra_log_data)\n        if was_connected:\n            logging.info(('Disconnected handler for queue %s (%s/%s)' % (queue_id, user_profile_email, client_type_name)))\n    except JsonableError as e:\n        if (hasattr(e, 'to_json_error_msg') and callable(e.to_json_error_msg)):\n            return dict(type='error', handler_id=handler_id, message=e.to_json_error_msg())\n        raise e\n    client.connect_handler(handler_id, client_type_name)\n    return dict(type='async')\n", "label": 1}
{"function": "\n\n@classmethod\ndef build_network_xml(cls, network_name, bridge_name, addresses=None, forward=None, ip_network_address=None, ip_network_prefixlen=None, stp=True, has_pxe_server=False, has_dhcp_server=False, dhcp_range_start=None, dhcp_range_end=None, tftp_root_dir=None):\n    'Generate network XML\\n\\n        :type network: Network\\n            :rtype : String\\n        '\n    if (addresses is None):\n        addresses = []\n    network_xml = XMLGenerator('network')\n    network_xml.name(cls._crop_name(network_name))\n    network_xml.bridge(name=bridge_name, stp=('on' if stp else 'off'), delay='0')\n    if forward:\n        network_xml.forward(mode=forward)\n    if (ip_network_address is None):\n        return str(network_xml)\n    with network_xml.ip(address=ip_network_address, prefix=ip_network_prefixlen):\n        if (has_pxe_server and tftp_root_dir):\n            network_xml.tftp(root=tftp_root_dir)\n        if has_dhcp_server:\n            with network_xml.dhcp:\n                network_xml.range(start=dhcp_range_start, end=dhcp_range_end)\n                for address in addresses:\n                    network_xml.host(mac=address['mac'], ip=address['ip'], name=address['name'])\n                if has_pxe_server:\n                    network_xml.bootp(file='pxelinux.0')\n    return str(network_xml)\n", "label": 0}
{"function": "\n\ndef cascade_visibility_down(element, visibility_mode):\n    'Sets visibility for all descendents of an element. (cascades down).'\n    links = [rel.get_accessor_name() for rel in element._meta.get_all_related_objects()]\n    for link in links:\n        objects = getattr(element, link).all()\n        for object in objects:\n            try:\n                if (visibility_mode == 'private'):\n                    if object.public:\n                        object.public = False\n                        object.save()\n                elif (visibility_mode == 'public'):\n                    if (not object.public):\n                        object.public = True\n                        object.save()\n            except Exception as e:\n                pass\n            if object._meta.get_all_related_objects():\n                cascade_visibility_down(object, visibility_mode)\n", "label": 0}
{"function": "\n\ndef add_ace(path, objectType, user, permission, acetype, propagation):\n    \"\\n    add an ace to an object\\n\\n    path:  path to the object (i.e. c:\\\\\\\\temp\\\\\\\\file, HKEY_LOCAL_MACHINE\\\\\\\\SOFTWARE\\\\\\\\KEY, etc)\\n    user: user to add\\n    permission:  permissions for the user\\n    acetype:  either allow/deny for each user/permission (ALLOW, DENY)\\n    propagation: how the ACE applies to children for Registry Keys and Directories(KEY, KEY&SUBKEYS, SUBKEYS)\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        allow domain\\\\fakeuser full control on HKLM\\\\\\\\SOFTWARE\\\\\\\\somekey, propagate to this key and subkeys\\n            salt 'myminion' win_dacl.add_ace 'HKEY_LOCAL_MACHINE\\\\\\\\SOFTWARE\\\\\\\\somekey' 'Registry' 'domain\\\\fakeuser' 'FULLCONTROL' 'ALLOW' 'KEY&SUBKEYS'\\n    \"\n    ret = {\n        'result': None,\n        'changes': {\n            \n        },\n        'comment': '',\n    }\n    if (path and user and permission and acetype and propagation):\n        if (objectType.upper() == 'FILE'):\n            propagation = 'FILE'\n        dc = daclConstants()\n        objectTypeBit = dc.getObjectTypeBit(objectType)\n        path = dc.processPath(path, objectTypeBit)\n        user = user.strip()\n        permission = permission.strip().upper()\n        acetype = acetype.strip().upper()\n        propagation = propagation.strip().upper()\n        sidRet = _getUserSid(user)\n        if (not sidRet['result']):\n            return sidRet\n        permissionbit = dc.getPermissionBit(objectTypeBit, permission)\n        acetypebit = dc.getAceTypeBit(acetype)\n        propagationbit = dc.getPropagationBit(objectTypeBit, propagation)\n        dacl = _get_dacl(path, objectTypeBit)\n        if dacl:\n            acesAdded = []\n            try:\n                if (acetypebit == 0):\n                    dacl.AddAccessAllowedAceEx(win32security.ACL_REVISION, propagationbit, permissionbit, sidRet['sid'])\n                elif (acetypebit == 1):\n                    dacl.AddAccessDeniedAceEx(win32security.ACL_REVISION, propagationbit, permissionbit, sidRet['sid'])\n                win32security.SetNamedSecurityInfo(path, objectTypeBit, win32security.DACL_SECURITY_INFORMATION, None, None, dacl, None)\n                acesAdded.append('{0} {1} {2} on {3}'.format(user, dc.getAceTypeText(acetype), dc.getPermissionText(objectTypeBit, permission), dc.getPropagationText(objectTypeBit, propagation)))\n                ret['result'] = True\n            except Exception as e:\n                ret['comment'] = 'An error occurred attempting to add the ace.  The error was {0}'.format(e)\n                ret['result'] = False\n                return ret\n            if acesAdded:\n                ret['changes']['Added ACEs'] = acesAdded\n        else:\n            ret['comment'] = 'Unable to obtain the DACL of {0}'.format(path)\n    else:\n        ret['comment'] = 'An empty value was specified for a required item.'\n        ret['result'] = False\n    return ret\n", "label": 1}
{"function": "\n\ndef gather_candidates(self, context):\n    line = self.vim.current.window.cursor[0]\n    column = context['complete_position']\n    buf = self.vim.current.buffer\n    offset = ((self.vim.call('line2byte', line) + charpos2bytepos(self.vim, context['input'][:column], column)) - 1)\n    source = '\\n'.join(buf).encode()\n    process = subprocess.Popen([self.GoCodeBinary(), '-f=json', 'autocomplete', buf.name, str(offset)], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True)\n    process.stdin.write(source)\n    (stdout_data, stderr_data) = process.communicate()\n    result = loads(stdout_data.decode())\n    try:\n        if (result[1][0]['class'] == 'PANIC'):\n            error(self.vim, 'gocode panicked')\n            return []\n        if self.sort_class:\n            class_dict = {\n                'package': [],\n                'func': [],\n                'type': [],\n                'var': [],\n                'const': [],\n            }\n        out = []\n        sep = ' '\n        for complete in result[1]:\n            word = complete['name']\n            info = complete['type']\n            _class = complete['class']\n            abbr = str(((word + sep) + info)).replace(' func', '', 1)\n            kind = _class\n            if ((_class == 'package') and self.package_dot):\n                word += '.'\n            candidates = dict(word=word, abbr=abbr, kind=kind, info=info, menu=self.mark, dup=1)\n            if ((not self.sort_class) or (_class == 'import')):\n                out.append(candidates)\n            else:\n                class_dict[_class].append(candidates)\n        if self.sort_class:\n            for c in self.sort_class:\n                for x in class_dict[c]:\n                    out.append(x)\n        return out\n    except Exception:\n        return []\n", "label": 1}
{"function": "\n\n@staticmethod\ndef validateInfo(doc):\n    '\\n        Makes sure the root field is a valid absolute path and is writeable.\\n        '\n    if ('prefix' not in doc):\n        doc['prefix'] = ''\n    doc['prefix'] = doc['prefix'].strip('/')\n    if (not doc.get('bucket')):\n        raise ValidationException('Bucket must not be empty.', 'bucket')\n    if (not doc.get('readOnly')):\n        if (not doc.get('secret')):\n            raise ValidationException('Secret key must not be empty.', 'secret')\n        if (not doc.get('accessKeyId')):\n            raise ValidationException('Access key ID must not be empty.', 'accessKeyId')\n    if ('service' not in doc):\n        doc['service'] = ''\n    if (doc['service'] != ''):\n        service = re.match('^((https?)://)?([^:/]+)(:([0-9]+))?$', doc['service'])\n        if (not service):\n            raise ValidationException('The service must of the form [http[s]://](host domain)[:(port)].', 'service')\n    doc['botoConnect'] = makeBotoConnectParams(doc['accessKeyId'], doc['secret'], doc['service'])\n    conn = botoConnectS3(doc['botoConnect'])\n    if doc.get('readOnly'):\n        try:\n            conn.get_bucket(bucket_name=doc['bucket'], validate=True)\n        except Exception:\n            logger.exception('S3 assetstore validation exception')\n            raise ValidationException(('Unable to connect to bucket \"%s\".' % doc['bucket']), 'bucket')\n    else:\n        try:\n            bucket = conn.get_bucket(bucket_name=doc['bucket'], validate=True)\n            testKey = boto.s3.key.Key(bucket=bucket, name='/'.join(filter(None, (doc['prefix'], 'test'))))\n            testKey.set_contents_from_string('')\n        except Exception:\n            logger.exception('S3 assetstore validation exception')\n            raise ValidationException(('Unable to write into bucket \"%s\".' % doc['bucket']), 'bucket')\n    return doc\n", "label": 1}
{"function": "\n\n@register.filter\ndef thumbnail(url, size='200x200'):\n    '\\n    Given a URL (local or remote) to an image, creates a thumbnailed version of the image, saving\\n    it locally and then returning the URL to the new, smaller version. If the argument passed is a\\n    single integer, like \"200\", will output a version of the image no larger than 200px wide. If the\\n    argument passed is two integers, like, \"200x300\", will output a cropped version of the image that\\n    is exactly 200px wide by 300px tall.\\n\\n    Examples:\\n\\n    {{ story.leadphoto.url|thumbnail:\"200\" }}\\n    {{ story.leadphoto.url|thumbnail:\"300x150\" }}\\n\\n    '\n    import os\n    if url.startswith(settings.MEDIA_URL):\n        url = url[len(settings.MEDIA_URL):]\n    original_path = (settings.MEDIA_ROOT + url)\n    try:\n        (basename, format) = original_path.rsplit('.', 1)\n    except ValueError:\n        return os.path.join(settings.MEDIA_URL, url)\n    thumbnail = ((((basename + '_t') + size) + '.') + format)\n    thumbnail_url = ('%s%s' % (settings.MEDIA_URL, thumbnail[len(settings.MEDIA_ROOT):]))\n    if (not os.path.exists(thumbnail)):\n        import Image\n        try:\n            image = Image.open(original_path)\n        except IOError:\n            return os.path.join(settings.MEDIA_URL, url)\n        original_image = image.copy()\n        original_width = original_image.size[0]\n        original_height = original_image.size[1]\n        try:\n            (desired_width, desired_height) = [int(x) for x in size.split('x')]\n            new_size = (desired_width, desired_height)\n            crop = True\n        except ValueError:\n            if (size[0] == 'x'):\n                desired_height = int(size[1:])\n                new_size = (original_width, desired_height)\n                crop = False\n            else:\n                desired_width = int(size)\n                new_size = (desired_width, original_height)\n                crop = False\n        if crop:\n            if ((original_height / (original_width / float(desired_width))) < desired_height):\n                image.thumbnail((original_width, desired_height), Image.ANTIALIAS)\n            else:\n                image.thumbnail((desired_width, original_height), Image.ANTIALIAS)\n            if ((image.size[0] >= desired_width) and (image.size[1] >= desired_height)):\n                left = ((image.size[0] - desired_width) / 2)\n                top = ((image.size[1] - desired_height) / 2)\n                right = (left + desired_width)\n                bottom = (top + desired_height)\n                cropped_image = image.crop((left, top, right, bottom))\n                image = cropped_image\n        else:\n            image.thumbnail(new_size, Image.ANTIALIAS)\n        try:\n            image.save(thumbnail, image.format, quality=85)\n        except KeyError:\n            return ''\n    return thumbnail_url\n", "label": 1}
{"function": "\n\ndef process_request_result(self, prepare_response_func=None):\n    '\\n        Process result of real request performed via transport extension.\\n        '\n    now = datetime.utcnow()\n    if self.config['debug_post']:\n        post = (self.config['post'] or self.config['multipart_post'])\n        if isinstance(post, dict):\n            post = list(post.items())\n        if post:\n            if isinstance(post, six.string_types):\n                post = (post[:self.config['debug_post_limit']] + '...')\n            else:\n                items = normalize_http_values(post, charset='utf-8')\n                new_items = []\n                for (key, value) in items:\n                    if (len(value) > self.config['debug_post_limit']):\n                        value = (value[:self.config['debug_post_limit']] + '...')\n                    else:\n                        value = value\n                    new_items.append((key, value))\n                post = '\\n'.join((('%-25s: %s' % x) for x in new_items))\n        if post:\n            logger_network.debug(('[%02d] POST request:\\n%s\\n' % (self.request_counter, post)))\n    self.reset_temporary_options()\n    if prepare_response_func:\n        self.doc = prepare_response_func(self.transport, self)\n    else:\n        self.doc = self.transport.prepare_response(self)\n    if (self.doc.grab is None):\n        self.doc.grab = weakref.proxy(self)\n    if self.config['reuse_cookies']:\n        self.cookies.update(self.doc.cookies)\n    self.doc.timestamp = now\n    self.config['charset'] = self.doc.charset\n    if self.config['log_file']:\n        with open(self.config['log_file'], 'wb') as out:\n            out.write(self.doc.body)\n    if self.config['cookiefile']:\n        self.cookies.save_to_file(self.config['cookiefile'])\n    if self.config['reuse_referer']:\n        self.config['referer'] = self.doc.url\n    self.copy_request_data()\n    if self.config['log_dir']:\n        self.save_dumps()\n    return self.doc\n", "label": 1}
{"function": "\n\ndef ring_edges(inner_radius, width, spacing=0, num_rings=None):\n    ' Calculate the inner and outer radius of a set of rings.\\n\\n    The number of rings, their widths, and any spacing between rings can be\\n    specified. They can be uniform or varied.\\n\\n    Parameters\\n    ----------\\n    inner_radius : float\\n        inner radius of the inner-most ring\\n\\n    width : float or list of floats\\n        ring thickness\\n        If a float, all rings will have the same thickness.\\n\\n    spacing : float or list of floats, optional\\n        margin between rings, 0 by default\\n        If a float, all rings will have the same spacing. If a list,\\n        the length of the list must be one less than the number of\\n        rings.\\n\\n    num_rings : int, optional\\n        number of rings\\n        Required if width and spacing are not lists and number\\n        cannot thereby be inferred. If it is given and can also be\\n        inferred, input is checked for consistency.\\n\\n    Returns\\n    -------\\n    edges : array\\n        inner and outer radius for each ring\\n\\n    Example\\n    -------\\n    # Make two rings starting at r=1px, each 5px wide\\n    >>> ring_edges(inner_radius=1, width=5, num_rings=2)\\n    [(1, 6), (6, 11)]\\n    # Make three rings of different widths and spacings.\\n    # Since the width and spacings are given individually, the number of\\n    # rings here is simply inferred.\\n    >>> ring_edges(inner_radius=1, width=(5, 4, 3), spacing=(1, 2))\\n    [(1, 6), (7, 11), (13, 16)]\\n    '\n    width_is_list = isinstance(width, collections.Iterable)\n    spacing_is_list = isinstance(spacing, collections.Iterable)\n    if (width_is_list and spacing_is_list):\n        if (len(width) != (len(spacing) - 1)):\n            raise ValueError('List of spacings must be one less than list of widths.')\n    if (num_rings is None):\n        try:\n            num_rings = len(width)\n        except TypeError:\n            try:\n                num_rings = (len(spacing) + 1)\n            except TypeError:\n                raise ValueError('Since width and spacing are constant, num_rings cannot be inferred and must be specified.')\n    else:\n        if width_is_list:\n            if (num_rings != len(width)):\n                raise ValueError('num_rings does not match width list')\n        if spacing_is_list:\n            if ((num_rings - 1) != len(spacing)):\n                raise ValueError('num_rings does not match spacing list')\n    if (not width_is_list):\n        width = (np.ones(num_rings) * width)\n    if (not spacing_is_list):\n        spacing = (np.ones((num_rings - 1)) * spacing)\n    all_spacings = np.insert(spacing, 0, inner_radius)\n    steps = np.array([all_spacings, width]).T.ravel()\n    edges = np.cumsum(steps).reshape((- 1), 2)\n    return edges\n", "label": 1}
{"function": "\n\ndef request(self, method, url, data=None, headers=None):\n    if (data and isinstance(data, bytes)):\n        data = data.decode()\n    if (data and (not isinstance(data, basestring))):\n        data = urlencode(data)\n    if (data is not None):\n        data = data.encode('utf8')\n    if (data and (not headers.get('Content-Type', None))):\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    request_headers = self.headers.copy()\n    if headers:\n        for (k, v) in headers.items():\n            if (v is None):\n                del request_headers[k]\n            else:\n                request_headers[k] = v\n    try:\n        func = getattr(requests, method.lower())\n    except AttributeError:\n        raise Exception(\"HTTP method '{}' is not supported\".format(method))\n    response = func(url, data=data, headers=request_headers)\n    if (response.status_code > 399):\n        raise HTTPError(response.status_code, '{}: {}'.format(response.status_code, response.content))\n    return response\n", "label": 1}
{"function": "\n\ndef code_grant_type(self, areq):\n    try:\n        _info = self.sdb[areq['code']]\n    except KeyError:\n        err = TokenErrorResponse(error='invalid_grant', error_description='Unknown access grant')\n        return Response(err.to_json(), content='application/json', status='401 Unauthorized')\n    authzreq = json.loads(_info['authzreq'])\n    if ('code_verifier' in areq):\n        try:\n            _method = authzreq['code_challenge_method']\n        except KeyError:\n            _method = 'S256'\n        resp = self.verify_code_challenge(areq['code_verifier'], authzreq['code_challenge'], _method)\n        if resp:\n            return resp\n    if ('state' in areq):\n        if (self.sdb[areq['code']]['state'] != areq['state']):\n            err = TokenErrorResponse(error='unauthorized_client')\n            return Unauthorized(err.to_json(), content='application/json')\n    resp = self.token_scope_check(areq, _info)\n    if resp:\n        return resp\n    if ('redirect_uri' in _info):\n        assert (areq['redirect_uri'] == _info['redirect_uri'])\n    issue_refresh = False\n    if (('scope' in authzreq) and ('offline_access' in authzreq['scope'])):\n        if (authzreq['response_type'] == 'code'):\n            issue_refresh = True\n    try:\n        _tinfo = self.sdb.upgrade_to_token(areq['code'], issue_refresh=issue_refresh)\n    except AccessCodeUsed:\n        err = TokenErrorResponse(error='invalid_grant', error_description='Access grant used')\n        return Response(err.to_json(), content='application/json', status='401 Unauthorized')\n    logger.debug(('_tinfo: %s' % _tinfo))\n    atr = AccessTokenResponse(**by_schema(AccessTokenResponse, **_tinfo))\n    logger.debug(('AccessTokenResponse: %s' % atr))\n    return Response(atr.to_json(), content='application/json')\n", "label": 1}
{"function": "\n\ndef parse_for(tokens, name, context):\n    (first, pos) = tokens[0]\n    tokens = tokens[1:]\n    context = (('for',) + context)\n    content = []\n    assert first.startswith('for ')\n    if first.endswith(':'):\n        first = first[:(- 1)]\n    first = first[3:].strip()\n    match = in_re.search(first)\n    if (not match):\n        raise TemplateError(('Bad for (no \"in\") in %r' % first), position=pos, name=name)\n    vars = first[:match.start()]\n    if ('(' in vars):\n        raise TemplateError(('You cannot have () in the variable section of a for loop (%r)' % vars), position=pos, name=name)\n    vars = tuple([v.strip() for v in first[:match.start()].split(',') if v.strip()])\n    expr = first[match.end():]\n    while 1:\n        if (not tokens):\n            raise TemplateError('No {{endfor}}', position=pos, name=name)\n        if (isinstance(tokens[0], tuple) and (tokens[0][0] == 'endfor')):\n            return (('for', pos, vars, expr, content), tokens[1:])\n        (next_chunk, tokens) = parse_expr(tokens, name, context)\n        content.append(next_chunk)\n", "label": 1}
{"function": "\n\n@click.command('upgrade', short_help='Upgrade PlatformIO to the latest version')\ndef cli():\n    last = get_latest_version()\n    if (__version__ == last):\n        return click.secho((\"You're up-to-date!\\nPlatformIO %s is currently the newest version available.\" % __version__), fg='green')\n    else:\n        click.secho('Please wait while upgrading PlatformIO ...', fg='yellow')\n        to_develop = False\n        try:\n            from pkg_resources import parse_version\n            to_develop = (parse_version(last) < parse_version(__version__))\n        except ImportError:\n            pass\n        cmds = (['pip', 'install', '--upgrade', ('https://github.com/platformio/platformio/archive/develop.zip' if to_develop else 'platformio')], ['platformio', '--version'])\n        cmd = None\n        r = None\n        try:\n            for cmd in cmds:\n                cmd = ([os.path.normpath(sys.executable), '-m'] + cmd)\n                r = None\n                r = util.exec_command(cmd)\n                if ((r['returncode'] != 0) and (cmd[2] == 'pip')):\n                    cmd.insert(3, '--no-cache-dir')\n                    r = util.exec_command(cmd)\n                assert (r['returncode'] == 0)\n            assert ('version' in r['out'])\n            actual_version = r['out'].strip().split('version', 1)[1].strip()\n            click.secho(('PlatformIO has been successfully upgraded to %s' % actual_version), fg='green')\n            click.echo('Release notes: ', nl=False)\n            click.secho('http://docs.platformio.org/en/latest/history.html', fg='cyan')\n        except Exception as e:\n            if (not r):\n                raise exception.UpgradeError('\\n'.join([str(cmd), str(e)]))\n            permission_errors = ('permission denied', 'not permitted')\n            if (any([(m in r['err'].lower()) for m in permission_errors]) and ('windows' not in util.get_systype())):\n                click.secho(\"\\n-----------------\\nPermission denied\\n-----------------\\nYou need the `sudo` permission to install Python packages. Try\\n\\n> sudo pip install -U platformio\\n\\nWARNING! Don't use `sudo` for the rest PlatformIO commands.\\n\", fg='yellow', err=True)\n                raise exception.ReturnErrorCode()\n            else:\n                raise exception.UpgradeError('\\n'.join([str(cmd), r['out'], r['err']]))\n", "label": 1}
{"function": "\n\ndef __init__(self, job_name=None, jenkins_url=None, jenkins_diff_url=None, auth_keyname=None, verify=True, cluster=None, debug_config=None, cpus=4, memory=(8 * 1024)):\n    '\\n        The JenkinsBuildStep constructor here, which is used as a base\\n        for all Jenkins builds, only accepts parameters which are used\\n        to determine where and how to schedule the builds and does not\\n        deal with any of the logic regarding what to actually schedule\\n        on those jenkins masters.\\n\\n        jenkins_url and jenkins_diff_url are either a single url or a list\\n        of possible jenkins masters for use of queueing. We don\\'t worry\\n        about which slaves to schedule them on at this level.\\n\\n        Args:\\n            auth_keyname: A key in the Changes config file that specifies the\\n                auth parameter to pass to the requests library. This could be\\n                a (username, password) tuple, in which case the requests library\\n                uses HTTPBasicAuth. For details, see http://docs.python-requests.org/en/latest/user/authentication/#basic-authentication\\n            verify (str or bool): The verify parameter to pass to the requests\\n                library for verifying SSL certificates. For details,\\n                see http://docs.python-requests.org/en/latest/user/advanced/#ssl-cert-verification\\n            cpus (int): Number of CPUs this buildstep requires to run.\\n            memory (int): Memory required for this buildstep in megabytes.\\n            cluster (Optional[str]): The Jenkins label to apply to be used to restrict the build to a subset of\\n                slaves where the master supports it.\\n            debug_config: A dictionary of debug config options. These are passed through\\n                to changes-client. There is also an infra_failures option, which takes a\\n                dictionary used to force infrastructure failures in builds. The keys of\\n                this dictionary refer to the phase (either \\'primary\\' or \\'expanded\\' if\\n                applicable), and the values are the probabilities with which\\n                a JobStep in that phase will fail.\\n                An example: \"debug_config\": {\"infra_failures\": {\"primary\": 0.5}}\\n                This will then cause an infra failure in the primary JobStep with\\n                probability 0.5.\\n        '\n    if (job_name is None):\n        raise ValueError('Missing required config: need job_name.')\n    if any(((int_field and (type(int_field) != int)) for int_field in (cpus, memory))):\n        raise ValueError('cpus and memory fields must be JSON ints')\n    if (not isinstance(jenkins_url, (list, tuple))):\n        if jenkins_url:\n            jenkins_url = [jenkins_url]\n        else:\n            jenkins_url = []\n    if (not isinstance(jenkins_diff_url, (list, tuple))):\n        if jenkins_diff_url:\n            jenkins_diff_url = [jenkins_diff_url]\n        else:\n            jenkins_diff_url = []\n    self.job_name = job_name\n    self.jenkins_urls = jenkins_url\n    self.jenkins_diff_urls = jenkins_diff_url\n    self.auth_keyname = auth_keyname\n    self.verify = verify\n    self.cluster = cluster\n    self.debug_config = (debug_config or {\n        \n    })\n    self._resources = {\n        \n    }\n    if cpus:\n        self._resources['cpus'] = cpus\n    if memory:\n        self._resources['memory'] = memory\n", "label": 1}
{"function": "\n\ndef test_equivalence():\n    preferences = DummyPrefs()\n    query = Query('')\n    query['sum_by'] = {\n        'core': [''],\n    }\n    targets = {\n        'servers.host.cpu.cpu0.irq': {\n            'id': 'servers.host.cpu.cpu0.irq',\n            'tags': {\n                'core': 'cpu0',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'irq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.cpu0.softirq': {\n            'id': 'servers.host.cpu.cpu0.softirq',\n            'tags': {\n                'core': 'cpu0',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'softirq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.cpu2.irq': {\n            'id': 'servers.host.cpu.cpu2.irq',\n            'tags': {\n                'core': 'cpu2',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'irq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.cpu2.softirq': {\n            'id': 'servers.host.cpu.cpu2.softirq',\n            'tags': {\n                'core': 'cpu2',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'softirq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.total.irq': {\n            'id': 'servers.host.cpu.total.irq',\n            'tags': {\n                'core': '_sum_',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'irq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.total.softirq': {\n            'id': 'servers.host.cpu.total.softirq',\n            'tags': {\n                'core': '_sum_',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'softirq',\n                'unit': 'cpu_state',\n            },\n        },\n    }\n    (graphs, _query) = g.build_from_targets(targets, query, preferences)\n    assert (len(graphs) == 1)\n    (_, graph) = graphs.popitem()\n    assert (len(graph['targets']) == 2)\n    ids = [t['id'] for t in graph['targets']]\n    assert (ids == ['servers.host.cpu.total.irq', 'servers.host.cpu.total.softirq'])\n    query = Query('core:(_sum_|cpu0|cpu2) sum by core')\n    (graphs, _query) = g.build_from_targets(targets, query, preferences)\n    assert (len(graphs) == 1)\n    (_, graph) = graphs.popitem()\n    assert (len(graph['targets']) == 2)\n    ids = [t['id'] for t in graph['targets']]\n    assert (ids == [['servers.host.cpu.cpu0.softirq', 'servers.host.cpu.cpu2.softirq'], ['servers.host.cpu.cpu0.irq', 'servers.host.cpu.cpu2.irq']])\n", "label": 0}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--project-list', default=governance.PROJECTS_LIST, help='a URL pointing to a projects.yaml file, defaults to %(default)s')\n    parser.add_argument('--releases-repo', default=os.path.expanduser('~/repos/openstack/releases'), help='path to local copy of the releases repository')\n    parser.add_argument('--format', '-f', choices=['csv', 'etherpad'], default='csv')\n    parser.add_argument('series', help='the series name')\n    args = parser.parse_args()\n    latest_versions = {\n        \n    }\n    pat = os.path.join(args.releases_repo, 'deliverables', args.series, '*.yaml')\n    for fn in glob.glob(pat):\n        with open(fn, 'r') as f:\n            y = yaml.safe_load(f.read())\n        deliverable = os.path.basename(fn)[:(- 5)]\n        v = y['releases'][(- 1)]['version']\n        latest_versions[deliverable] = v\n    team_data = governance.get_team_data()\n    teams = {n.lower(): governance.Team(n, i) for (n, i) in team_data.items()}\n    deliverables_by_model = {\n        MILESTONE: {\n            'managed': {\n                \n            },\n            'unmanaged': {\n                \n            },\n        },\n        INTERMEDIARY: {\n            'managed': {\n                \n            },\n            'unmanaged': {\n                \n            },\n        },\n    }\n    for t in teams.values():\n        for (dn, di) in t.deliverables.items():\n            for model in deliverables_by_model.keys():\n                if (model in di.tags):\n                    if ('release:managed' in di.tags):\n                        managed = 'managed'\n                    else:\n                        managed = 'unmanaged'\n                    dbm_team = deliverables_by_model[model][managed].setdefault(di.team.name.lower(), [])\n                    dbm_team.append(di)\n                    break\n    if (args.format == 'csv'):\n        writer = csv.writer(sys.stdout)\n        writer.writerow(('Managed', 'Release Model', 'Team', 'PTL Nick', 'PTL Email', 'IRC Channel', 'Deliverable Type', 'Deliverable Name', 'Latest Version'))\n        for managed in ['managed', 'unmanaged']:\n            for model in [MILESTONE, INTERMEDIARY]:\n                short_model = model.rpartition('-')[(- 1)]\n                dbm_teams = sorted(deliverables_by_model[model][managed].items())\n                for (team_name, team_deliverables) in dbm_teams:\n                    team = teams[team_name]\n                    for d in sorted(team_deliverables, key=(lambda d: d.name)):\n                        writer.writerow((managed, short_model, team.name.lower(), team.data['ptl']['irc'], team.data['ptl']['email'], team.data.get('irc-channel'), d.type, d.name, latest_versions.get(d.name, 'not found')))\n    else:\n        for managed in ['managed', 'unmanaged']:\n            print('{}\\n'.format(managed))\n            for model in [MILESTONE, INTERMEDIARY]:\n                print('  * {}\\n'.format(model))\n                dbm_teams = sorted(deliverables_by_model[model][managed].items())\n                for (team_name, team_deliverables) in dbm_teams:\n                    team = teams[team_name]\n                    print('    * {}'.format(team_name))\n                    print('      * PTL: {} - {}'.format(team.data['ptl']['irc'], team.data['ptl']['email']))\n                    print('      * IRC: {}'.format(team.data.get('irc-channel', '')))\n                    print('      * Deliverables')\n                    for d in sorted(team_deliverables, key=(lambda d: d.name)):\n                        v = latest_versions.get(d.name, 'not found')\n                        print('        * {d.name} ({d.type}) [{v}]'.format(d=d, v=v))\n                    print()\n", "label": 1}
{"function": "\n\ndef make_temp_dir(filename, signal=False):\n    'Creates a temporary folder and returns the joined filename'\n    try:\n        if (((testParams['user_tempdir'] is not None) and (testParams['user_tempdir'] != '')) and (testParams['actual_tempdir'] == '')):\n            testParams['actual_tempdir'] = testParams['user_tempdir']\n        if (not os.path.isdir(testParams['actual_tempdir'])):\n            os.makedirs(testParams['actual_tempdir'])\n        return os.path.join(testParams['actual_tempdir'], filename)\n    except OSError as exc:\n        actual_tempdir = os.path.join(tempfile.gettempdir(), testParams['tempdir'])\n        if signal:\n            errwrite(('I used `tempfile.gettempdir()` to create the temporary folder `%s`.' % actual_tempdir))\n        testParams['actual_tempdir'] = actual_tempdir\n        if (not os.path.isdir(testParams['actual_tempdir'])):\n            try:\n                os.makedirs(testParams['actual_tempdir'])\n            except Exception:\n                pass\n        return os.path.join(actual_tempdir, filename)\n    except:\n        get_root_logger().error('Could not create a directory.')\n        raise\n", "label": 0}
{"function": "\n\ndef run(self):\n    'Starts or resumes the generator, running until it reaches a\\n        yield point that is not ready.\\n        '\n    if (self.running or self.finished):\n        return\n    try:\n        self.running = True\n        while True:\n            future = self.future\n            if (not future.done()):\n                return\n            self.future = None\n            try:\n                orig_stack_contexts = stack_context._state.contexts\n                exc_info = None\n                try:\n                    value = future.result()\n                except Exception:\n                    self.had_exception = True\n                    exc_info = sys.exc_info()\n                if (exc_info is not None):\n                    yielded = self.gen.throw(*exc_info)\n                    exc_info = None\n                else:\n                    yielded = self.gen.send(value)\n                if (stack_context._state.contexts is not orig_stack_contexts):\n                    self.gen.throw(stack_context.StackContextInconsistentError('stack_context inconsistency (probably caused by yield within a \"with StackContext\" block)'))\n            except (StopIteration, Return) as e:\n                self.finished = True\n                self.future = _null_future\n                if (self.pending_callbacks and (not self.had_exception)):\n                    raise LeakedCallbackError(('finished without waiting for callbacks %r' % self.pending_callbacks))\n                self.result_future.set_result(_value_from_stopiteration(e))\n                self.result_future = None\n                self._deactivate_stack_context()\n                return\n            except Exception:\n                self.finished = True\n                self.future = _null_future\n                self.result_future.set_exc_info(sys.exc_info())\n                self.result_future = None\n                self._deactivate_stack_context()\n                return\n            if (not self.handle_yield(yielded)):\n                return\n    finally:\n        self.running = False\n", "label": 1}
{"function": "\n\ndef parse_innotop_mode_c(self):\n    with open(self.infile, 'r') as infh:\n        headerline = infh.readline()\n        columns = headerline.split()[2:]\n        outfilehandlers = {\n            \n        }\n        for line in infh:\n            l = line.strip().split(' ', 1)\n            if (len(l) <= 1):\n                continue\n            ts = l[0].strip().replace('T', ' ')\n            try:\n                nameval = l[1].strip().split('\\t', 1)\n            except IndexError:\n                logger.warn('Badly formatted line: %s', line)\n                logger.warn('Expected tab separated values')\n                continue\n            command = nameval[0]\n            if (command not in outfilehandlers):\n                if (len(outfilehandlers) > self.C_MAX_COMMANDS):\n                    continue\n                outfilehandlers[command] = {\n                    \n                }\n            words = nameval[1].split('\\t')\n            for i in range(len(words)):\n                if (self.options and (columns[i] not in self.options)):\n                    continue\n                if (columns[i] not in outfilehandlers[command]):\n                    outfilehandlers[command][columns[i]] = open(self.get_csv_C(command, columns[i]), 'w')\n                    self.csv_files.append(self.get_csv_C(command, columns[i]))\n                ts = naarad.utils.reconcile_timezones(ts, self.timezone, self.graph_timezone)\n                outfilehandlers[command][columns[i]].write((ts + ','))\n                outfilehandlers[command][columns[i]].write(words[i])\n                outfilehandlers[command][columns[i]].write('\\n')\n        for command in outfilehandlers:\n            for column in outfilehandlers[command]:\n                outfilehandlers[command][column].close()\n    return True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _generate_syntax_file_map():\n    '\\n        Generate a map of all file types to their syntax files.\\n        '\n    syntax_file_map = {\n        \n    }\n    packages_path = sublime.packages_path()\n    packages = [f for f in os.listdir(packages_path) if os.path.isdir(os.path.join(packages_path, f))]\n    for package in packages:\n        package_dir = os.path.join(packages_path, package)\n        syntax_files = [os.path.join(package_dir, f) for f in os.listdir(package_dir) if f.endswith('.tmLanguage')]\n        for syntax_file in syntax_files:\n            try:\n                plist = plistlib.readPlist(syntax_file)\n                if plist:\n                    for file_type in plist['fileTypes']:\n                        syntax_file_map[file_type.lower()] = syntax_file\n            except expat.ExpatError:\n                logger.warn((\"could not parse '%s'\" % syntax_file))\n            except KeyError:\n                pass\n    if hasattr(sublime, 'find_resources'):\n        syntax_files = sublime.find_resources('*.tmLanguage')\n        for syntax_file in syntax_files:\n            try:\n                plist = plistlib.readPlistFromBytes(bytearray(sublime.load_resource(syntax_file), 'utf-8'))\n                if plist:\n                    for file_type in plist['fileTypes']:\n                        syntax_file_map[file_type.lower()] = syntax_file\n            except expat.ExpatError:\n                logger.warn((\"could not parse '%s'\" % syntax_file))\n            except KeyError:\n                pass\n    return syntax_file_map\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kwargs):\n    '\\n        From the Py3 bytes docstring:\\n\\n        bytes(iterable_of_ints) -> bytes\\n        bytes(string, encoding[, errors]) -> bytes\\n        bytes(bytes_or_buffer) -> immutable copy of bytes_or_buffer\\n        bytes(int) -> bytes object of size given by the parameter initialized with null bytes\\n        bytes() -> empty bytes object\\n        \\n        Construct an immutable array of bytes from:\\n          - an iterable yielding integers in range(256)\\n          - a text string encoded using the specified encoding\\n          - any object implementing the buffer API.\\n          - an integer\\n        '\n    encoding = None\n    errors = None\n    if (len(args) == 0):\n        return super(newbytes, cls).__new__(cls)\n    elif (len(args) >= 2):\n        args = list(args)\n        if (len(args) == 3):\n            errors = args.pop()\n        encoding = args.pop()\n    if (type(args[0]) == newbytes):\n        return args[0]\n    elif isinstance(args[0], _builtin_bytes):\n        value = args[0]\n    elif isinstance(args[0], unicode):\n        try:\n            if ('encoding' in kwargs):\n                assert (encoding is None)\n                encoding = kwargs['encoding']\n            if ('errors' in kwargs):\n                assert (errors is None)\n                errors = kwargs['errors']\n        except AssertionError:\n            raise TypeError('Argument given by name and position')\n        if (encoding is None):\n            raise TypeError('unicode string argument without an encoding')\n        newargs = [encoding]\n        if (errors is not None):\n            newargs.append(errors)\n        value = args[0].encode(*newargs)\n    elif isinstance(args[0], Iterable):\n        if (len(args[0]) == 0):\n            value = b''\n        else:\n            try:\n                values = [chr(x) for x in args[0]]\n                value = b''.join(values)\n            except:\n                raise ValueError('bytes must be in range(0, 256)')\n    elif isinstance(args[0], Integral):\n        if (args[0] < 0):\n            raise ValueError('negative count')\n        value = (b'\\x00' * args[0])\n    else:\n        value = args[0]\n    return super(newbytes, cls).__new__(cls, value)\n", "label": 1}
{"function": "\n\ndef broadcast_like(value, template, fgraph, dtype=None):\n    '\\n    Return a Variable with the same shape and dtype as the template,\\n    filled by broadcasting value through it. `value` will be cast as\\n    necessary.\\n\\n    '\n    value = T.as_tensor_variable(value)\n    if (value.type == template.type):\n        return value\n    if (template not in fgraph.variables):\n        raise NotImplementedError('broadcast_like currently requires the template Variable to be in the fgraph already')\n    if hasattr(fgraph, 'shape_feature'):\n        new_shape = fgraph.shape_feature.shape_of[template]\n    else:\n        new_shape = template.shape\n    if (dtype is None):\n        dtype = template.dtype\n    rval = T.alloc(T.cast(value, dtype), *new_shape)\n    if (rval.broadcastable != template.broadcastable):\n        rval = T.unbroadcast(rval, *[i for i in xrange(rval.ndim) if (rval.broadcastable[i] and (not template.broadcastable[i]))])\n    assert (rval.type.dtype == dtype)\n    if (rval.type.broadcastable != template.broadcastable):\n        raise AssertionError(((('rval.type.broadcastable is ' + str(rval.type.broadcastable)) + ' but template.broadcastable is') + str(template.broadcastable)))\n    return rval\n", "label": 1}
{"function": "\n\ndef as_sql(self, qn, connection):\n    \"\\n        Returns the SQL version of the where clause and the value to be\\n        substituted in. Returns '', [] if this node matches everything,\\n        None, [] if this node is empty, and raises EmptyResultSet if this\\n        node can't match anything.\\n        \"\n    soql_trans = qn.query_topology()\n    result = []\n    result_params = []\n    (everything_childs, nothing_childs) = (0, 0)\n    non_empty_childs = len(self.children)\n    for child in self.children:\n        try:\n            if hasattr(child, 'as_sql'):\n                (sql, params) = qn.compile(child)\n            else:\n                (sql, params) = self.make_atom(child, qn, connection)\n        except EmptyResultSet:\n            nothing_childs += 1\n        else:\n            if sql:\n                x_match = re.match('(\\\\w+)\\\\.(.*)', sql)\n                if x_match:\n                    (x_table, x_field) = x_match.groups()\n                    sql = ('%s.%s' % (soql_trans[x_table], x_field))\n                result.append(sql)\n                result_params.extend(params)\n            else:\n                if (sql is None):\n                    non_empty_childs -= 1\n                    continue\n                everything_childs += 1\n        if (self.connector == AND):\n            (full_needed, empty_needed) = (non_empty_childs, 1)\n        else:\n            (full_needed, empty_needed) = (1, non_empty_childs)\n        if ((empty_needed - nothing_childs) <= 0):\n            if self.negated:\n                return ('', [])\n            else:\n                raise EmptyResultSet\n        if ((full_needed - everything_childs) <= 0):\n            if self.negated:\n                raise EmptyResultSet\n            else:\n                return ('', [])\n    if (non_empty_childs == 0):\n        return (None, [])\n    conn = (' %s ' % self.connector)\n    sql_string = conn.join(result)\n    if sql_string:\n        if self.negated:\n            sql_string = ('(NOT (%s))' % sql_string)\n        elif (len(result) > 1):\n            sql_string = ('(%s)' % sql_string)\n    return (sql_string, result_params)\n", "label": 1}
{"function": "\n\ndef do_ssh(args, env):\n    'The \"ravello ssh\" command.'\n    with env.let(quiet=True):\n        login.default_login()\n        keypair.default_keypair()\n    if manifest.manifest_exists():\n        with env.let(quiet=True):\n            manif = manifest.default_manifest()\n    else:\n        manif = None\n    parts = args.application.split(':')\n    if ((len(parts) in (1, 2)) and (manif is None)):\n        error.raise_error('No manifest found ({0}).\\nPlease specify the fully qualified app name.\\nUse `ravtest ps --all` for a list.', manifest.manifest_name())\n    if (len(parts) in (1, 2)):\n        project = manif['project']['name']\n        console.info('Project name is `{0}`.', project)\n        defname = parts[0]\n        instance = (parts[1] if (len(parts) == 2) else None)\n    elif (len(parts) == 3):\n        (project, defname, instance) = parts\n    else:\n        error.raise_error('Illegal application name: `{0}`.', appname)\n    apps = cache.find_applications(project, defname, instance)\n    if (len(apps) == 0):\n        error.raise_error('No instances of application `{0}` exist.', defname)\n    elif (len(apps) > 1):\n        error.raise_error('Multiple instances of `{0}` exist.\\nUse `ravtest ps` to list the instances and then\\nspecify the application with its instance id.', defname)\n    app = cache.get_application(apps[0]['id'])\n    appname = app['name']\n    (_, _, instance) = appname.split(':')\n    vmname = args.vm\n    vm = application.get_vm(app, vmname)\n    if (vm is None):\n        error.raise_error('Application `{0}:{1}` has no VM named `{2}`.\\nUse `ravtest ps --full` to see a list of VMs.', defname, instance, vmname)\n    console.info('Connecting to VM `{0}` of application `{1}:{2}`...', vmname, defname, instance)\n    state = application.get_application_state(app)\n    if (state not in ('PUBLISHING', 'STARTING', 'STOPPED', 'STARTED')):\n        error.raise_error('VM `{0}` is in an unknown state.', vmname)\n    userdata = vm.get('customVmConfigurationData', {\n        \n    })\n    vmkey = userdata.get('keypair', {\n        \n    })\n    if (vmkey.get('id') != env.public_key['id']):\n        error.raise_error('VM uses unknown public key `{0}`.', vmkey.get('name'))\n    application.start_application(app)\n    application.wait_for_application(app, [vmname])\n    host = 'ravello@{0}'.format(vm['dynamicMetadata']['externalIp'])\n    command = '~/bin/run {0}'.format(args.testid)\n    openssh = util.find_openssh()\n    interactive = os.isatty(sys.stdin.fileno())\n    if (interactive and openssh):\n        if (not sys.platform.startswith('win')):\n            argv = ['ssh', '-i', env.private_key_file, '-o', 'UserKnownHostsFile=/dev/null', '-o', 'StrictHostKeyChecking=no', '-o', 'LogLevel=quiet', '-t', host, command]\n            console.debug('Starting {0}', ' '.join(argv))\n            os.execve(openssh, argv, os.environ)\n        else:\n            command = [openssh, '-i', env.private_key_file, '-o', 'UserKnownHostsFile=NUL', '-o', 'StrictHostKeyChecking=no', '-o', 'LogLevel=quiet', '-t', host, command]\n            ssh = subprocess.Popen(command)\n            ret = ssh.wait()\n            error.exit(ret)\n    console.info(textwrap.dedent('            Warning: no local openssh installation found.\\n            Falling back to Fabric/Paramiko for an interactive shell.\\n            However, please note:\\n\\n            * CTRL-C and terminal resize signals may not work.\\n            * Output of programs that repaint the screen may\\n              be garbled (e.g. progress bars).\\n            '))\n    fab.env.host_string = host\n    fab.env.key_filename = env.private_key_file\n    fab.env.disable_known_hosts = True\n    fab.env.remote_interrupt = True\n    fab.env.output_prefix = None\n    fabric.state.output.running = None\n    fabric.state.output.status = None\n    ret = fab.run(command, warn_only=True)\n    return ret.return_code\n", "label": 1}
{"function": "\n\n@extensions.expected_errors((400, 403, 404))\ndef create(self, req, body):\n    context = _authorize_context(req)\n    sg_rule = self._from_body(body, 'security_group_rule')\n    try:\n        parent_group_id = self.security_group_api.validate_id(sg_rule.get('parent_group_id'))\n        security_group = self.security_group_api.get(context, None, parent_group_id, map_exception=True)\n        new_rule = self._rule_args_to_dict(context, to_port=sg_rule.get('to_port'), from_port=sg_rule.get('from_port'), ip_protocol=sg_rule.get('ip_protocol'), cidr=sg_rule.get('cidr'), group_id=sg_rule.get('group_id'))\n    except (exception.Invalid, exception.InvalidCidr) as exp:\n        raise exc.HTTPBadRequest(explanation=exp.format_message())\n    except exception.SecurityGroupNotFound as exp:\n        raise exc.HTTPNotFound(explanation=exp.format_message())\n    if (new_rule is None):\n        msg = _('Not enough parameters to build a valid rule.')\n        raise exc.HTTPBadRequest(explanation=msg)\n    new_rule['parent_group_id'] = security_group['id']\n    if ('cidr' in new_rule):\n        (net, prefixlen) = netutils.get_net_and_prefixlen(new_rule['cidr'])\n        if ((net not in ('0.0.0.0', '::')) and (prefixlen == '0')):\n            msg = (_('Bad prefix for network in cidr %s') % new_rule['cidr'])\n            raise exc.HTTPBadRequest(explanation=msg)\n    group_rule_data = None\n    try:\n        if sg_rule.get('group_id'):\n            source_group = self.security_group_api.get(context, id=sg_rule['group_id'])\n            group_rule_data = {\n                'name': source_group.get('name'),\n                'tenant_id': source_group.get('project_id'),\n            }\n        security_group_rule = self.security_group_api.create_security_group_rule(context, security_group, new_rule)\n    except exception.Invalid as exp:\n        raise exc.HTTPBadRequest(explanation=exp.format_message())\n    except exception.SecurityGroupNotFound as exp:\n        raise exc.HTTPNotFound(explanation=exp.format_message())\n    except exception.SecurityGroupLimitExceeded as exp:\n        raise exc.HTTPForbidden(explanation=exp.format_message())\n    formatted_rule = self._format_security_group_rule(context, security_group_rule, group_rule_data)\n    return {\n        'security_group_rule': formatted_rule,\n    }\n", "label": 1}
{"function": "\n\ndef render_pstate(self, sls, saltenv, mods, defaults=None):\n    '\\n        Collect a single pillar sls file and render it\\n        '\n    if (defaults is None):\n        defaults = {\n            \n        }\n    err = ''\n    errors = []\n    fn_ = self.client.get_state(sls, saltenv).get('dest', False)\n    if (not fn_):\n        if (sls in self.ignored_pillars.get(saltenv, [])):\n            log.debug(\"Skipping ignored and missing SLS '{0}' in environment '{1}'\".format(sls, saltenv))\n            return (None, mods, errors)\n        elif self.opts['pillar_roots'].get(saltenv):\n            msg = \"Specified SLS '{0}' in environment '{1}' is not available on the salt master\".format(sls, saltenv)\n            log.error(msg)\n            errors.append(msg)\n        else:\n            msg = \"Specified SLS '{0}' in environment '{1}' was not found. \".format(sls, saltenv)\n            if (self.opts.get('__git_pillar', False) is True):\n                msg += 'This is likely caused by a git_pillar top file containing an environment other than the one for the branch in which it resides. Each git_pillar branch/tag must have its own top file.'\n            else:\n                msg += \"This could be because SLS '{0}' is in an environment other than '{1}', but '{1}' is included in that environment's Pillar top file. It could also be due to environment '{1}' not being defined in 'pillar_roots'.\".format(sls, saltenv)\n            log.debug(msg)\n            return (None, mods, errors)\n    state = None\n    try:\n        state = compile_template(fn_, self.rend, self.opts['renderer'], saltenv, sls, _pillar_rend=True, **defaults)\n    except Exception as exc:\n        msg = \"Rendering SLS '{0}' failed, render error:\\n{1}\".format(sls, exc)\n        log.critical(msg)\n        if self.opts.get('pillar_safe_render_error', True):\n            errors.append(\"Rendering SLS '{0}' failed. Please see master log for details.\".format(sls))\n        else:\n            errors.append(msg)\n    mods.add(sls)\n    nstate = None\n    if state:\n        if (not isinstance(state, dict)):\n            msg = \"SLS '{0}' does not render to a dictionary\".format(sls)\n            log.error(msg)\n            errors.append(msg)\n        elif ('include' in state):\n            if (not isinstance(state['include'], list)):\n                msg = \"Include Declaration in SLS '{0}' is not formed as a list\".format(sls)\n                log.error(msg)\n                errors.append(msg)\n            else:\n                for sub_sls in state.pop('include'):\n                    if isinstance(sub_sls, dict):\n                        (sub_sls, v) = next(six.iteritems(sub_sls))\n                        defaults = v.get('defaults', {\n                            \n                        })\n                        key = v.get('key', None)\n                    else:\n                        key = None\n                    if (sub_sls not in mods):\n                        (nstate, mods, err) = self.render_pstate(sub_sls, saltenv, mods, defaults)\n                        if nstate:\n                            if key:\n                                nstate = {\n                                    key: nstate,\n                                }\n                            state = merge(state, nstate, self.merge_strategy, self.opts.get('renderer', 'yaml'), self.opts.get('pillar_merge_lists', False))\n                        if err:\n                            errors += err\n    return (state, mods, errors)\n", "label": 1}
{"function": "\n\ndef add_source_to_zipfile(zip_out, result):\n    ' Add a LocalProcessedResult to zipfile via expand_and_add_csv_to_zipfile().\\n    \\n        Use result code_version to determine whether to expand; 3+ will do it.\\n    '\n    (_, ext) = splitext(result.filename)\n    try:\n        number = [int(n) for n in result.code_version.split('.')]\n    except Exception as e:\n        _L.info('Skipping street name expansion for {} (error {})'.format(result.source_base, e))\n        do_expand = False\n    else:\n        if (number >= [2, 13]):\n            do_expand = True\n        else:\n            _L.info('Skipping street name expansion for {} (version {})'.format(result.source_base, result.code_version))\n            do_expand = False\n    if do_expand:\n        is_us = (is_us_northeast(result) or is_us_midwest(result) or is_us_south(result) or is_us_west(result))\n        if (not is_us):\n            _L.info('Skipping street name expansion for {} (not U.S.)'.format(result.source_base))\n            do_expand = False\n    if (ext == '.csv'):\n        with open(result.filename) as file:\n            expand_and_add_csv_to_zipfile(zip_out, (result.source_base + ext), file, do_expand)\n    elif (ext == '.zip'):\n        zip_in = ZipFile(result.filename, 'r')\n        for zipinfo in zip_in.infolist():\n            if (zipinfo.filename == 'README.txt'):\n                continue\n            elif (splitext(zipinfo.filename)[1] == '.csv'):\n                zipped_file = zip_in.open(zipinfo.filename)\n                expand_and_add_csv_to_zipfile(zip_out, zipinfo.filename, zipped_file, do_expand)\n            else:\n                zip_out.writestr(zipinfo, zip_in.read(zipinfo.filename))\n        zip_in.close()\n", "label": 1}
{"function": "\n\ndef commit(self):\n    self.log('in commit')\n    for p in [c for c in self.block_candidates.values() if (c.block.prevhash == self.head.hash)]:\n        assert isinstance(p, BlockProposal)\n        ls = self.heights[p.height].last_quorum_lockset\n        if (ls and (ls.has_quorum == p.blockhash)):\n            self.store_proposal(p)\n            self.store_last_committing_lockset(ls)\n            success = self.chainservice.commit_block(p.block)\n            assert success\n            if success:\n                self.log('commited', p=p, hash=phx(p.blockhash))\n                assert (self.head == p.block)\n                self.commit()\n                return True\n            else:\n                self.log('could not commit', p=p)\n        else:\n            self.log('no quorum for', p=p)\n            if ls:\n                self.log('votes', votes=ls.votes)\n", "label": 1}
{"function": "\n\ndef makedirs_count(path, count=0):\n    \"\\n    Same as os.makedirs() except that this method returns the number of\\n    new directories that had to be created.\\n\\n    Also, this does not raise an error if target directory already exists.\\n    This behaviour is similar to Python 3.x's os.makedirs() called with\\n    exist_ok=True. Also similar to swift.common.utils.mkdirs()\\n\\n    https://hg.python.org/cpython/file/v3.4.2/Lib/os.py#l212\\n    \"\n    (head, tail) = os.path.split(path)\n    if (not tail):\n        (head, tail) = os.path.split(head)\n    if (head and tail and (not os.path.exists(head))):\n        count = makedirs_count(head, count)\n        if (tail == os.path.curdir):\n            return\n    try:\n        os.mkdir(path)\n    except OSError as e:\n        if ((e.errno != errno.EEXIST) or (not os.path.isdir(path))):\n            raise\n    else:\n        count += 1\n    return count\n", "label": 0}
{"function": "\n\ndef _configure(self, qtile, bar):\n    base._Widget._configure(self, qtile, bar)\n    if (not self.filename):\n        raise ValueError('Filename not set!')\n    self.filename = os.path.expanduser(self.filename)\n    try:\n        self.image = cairocffi.ImageSurface.create_from_png(self.filename)\n    except MemoryError:\n        raise ValueError((\"The image '%s' doesn't seem to be a valid PNG\" % self.filename))\n    self.pattern = cairocffi.SurfacePattern(self.image)\n    self.image_width = self.image.get_width()\n    self.image_height = self.image.get_height()\n    if self.scale:\n        if self.bar.horizontal:\n            new_height = (self.bar.height - (self.margin_y * 2))\n            if (new_height and (self.image_height != new_height)):\n                scaler = cairocffi.Matrix()\n                sp = (self.image_height / new_height)\n                self.image_height = new_height\n                self.image_width = int((self.image_width / sp))\n                scaler.scale(sp, sp)\n                self.pattern.set_matrix(scaler)\n        else:\n            new_width = (self.bar.width - (self.margin_x * 2))\n            if (new_width and (self.image_width != new_width)):\n                scaler = cairocffi.Matrix()\n                sp = (self.image_width / new_width)\n                self.image_width = new_width\n                self.image_height = int((self.image_height / sp))\n                scaler.scale(sp, sp)\n                self.pattern.set_matrix(scaler)\n", "label": 0}
{"function": "\n\ndef run(self):\n    ' Start polling the socket. '\n    self.heartbeat_controller.reset()\n    self._force_recon = False\n    while (not self.stop.is_set()):\n        try:\n            if (not self._check_connection()):\n                continue\n        except ChromecastConnectionError:\n            break\n        (can_read, _, _) = select.select([self.socket], [], [], POLL_TIME)\n        message = data = None\n        if ((self.socket in can_read) and (not self._force_recon)):\n            try:\n                message = self._read_message()\n            except InterruptLoop as exc:\n                if self.stop.is_set():\n                    self.logger.info('Stopped while reading message, disconnecting.')\n                    break\n                else:\n                    self.logger.exception('Interruption caught without being stopped %s', exc)\n                    break\n            except ssl.SSLError as exc:\n                if (exc.errno == ssl.SSL_ERROR_EOF):\n                    if self.stop.is_set():\n                        break\n                raise\n            except socket.error:\n                self._force_recon = True\n                self.logger.info('Error reading from socket.')\n            else:\n                data = _json_from_message(message)\n        if (not message):\n            continue\n        if self.stop.is_set():\n            break\n        self._route_message(message, data)\n        if (REQUEST_ID in data):\n            callback = self._request_callbacks.pop(data[REQUEST_ID], None)\n            if (callback is not None):\n                event = callback['event']\n                callback['response'] = data\n                event.set()\n    self._cleanup()\n", "label": 1}
{"function": "\n\ndef put(self, id):\n    '**Update a Quest**\\n\\n        **Example Request:**\\n\\n        .. sourcecode:: http\\n\\n             PUT /api/v1/quest/1 HTTP/1.1\\n             Host: localhost\\n             Content-Type: application/json\\n\\n             {\\n                 \"description\": \"New desc\",\\n                 \"creator\": \"tammy\"\\n             }\\n\\n        **Example response:**\\n\\n        .. sourcecode:: http\\n\\n            HTTP/1.1 200 OK\\n            Content-Type: application/json\\n\\n            {\\n                \"status\": \"ok\",\\n                \"id\": 1,\\n                \"href\": \"/api/v1/quests/1\",\\n                \"creator\": \"tammy\",\\n                \"embarkTime\": timestamp,\\n                \"targetTime\": timestamp,\\n                \"completionTime\": timestamp,\\n                \"description\": \"New desc\",\\n                \"labors\": [],\\n            }\\n\\n        :param id: id of the Quest that should be updated.\\n        :type id: string\\n\\n        :reqjson string description: the new description of the Quest\\n        :reqjson string creator: The new username of the creator (owner)\\n        :regjson timestamp targetTime: Set a new targetTime\\n\\n        :reqheader Content-Type: The server expects a json body specified with\\n                                 this header.\\n\\n        :statuscode 200: The request was successful.\\n        :statuscode 400: The request was malformed.\\n        :statuscode 401: The request was made without being logged in.\\n        :statuscode 403: The request was made with insufficient permissions.\\n        :statuscode 404: The Quest was not found.\\n        :statuscode 409: There was a conflict with another resource.\\n        '\n    log.info('QUEST: Updating {}'.format(id))\n    quest = self.session.query(Quest).filter_by(id=id).scalar()\n    if (not quest):\n        raise exc.NotFound('No such Quest {} found'.format(id))\n    new_desc = None\n    new_creator = None\n    new_target_time = None\n    try:\n        if ('description' in self.jbody):\n            new_desc = self.jbody['description']\n        if ('creator' in self.jbody):\n            new_creator = self.jbody['creator']\n            if (not EMAIL_REGEX.match(new_creator)):\n                new_creator += ('@' + self.domain)\n        if ('targetTime' in self.jbody):\n            new_target_time = parser.parse(self.jbody['targetTime'])\n            new_target_time = new_target_time.replace(tzinfo=None)\n            if (new_target_time <= datetime.utcnow()):\n                raise exc.BadRequest('Quest target date must be in future')\n    except KeyError as err:\n        raise exc.BadRequest('Missing Required Argument: {}'.format(err.message))\n    try:\n        if new_desc:\n            quest = quest.update(description=new_desc)\n        if (new_creator is not None):\n            quest = quest.update(creator=new_creator)\n        if new_target_time:\n            quest = quest.update(target_time=new_target_time)\n    except IntegrityError as err:\n        raise exc.Conflict(str(err.orig))\n    json = quest.to_dict(self.href_prefix)\n    self.success(json)\n    log.info('QUEST [{}]: Updated: {} {} {}'.format(id, ('new description {}'.format(new_desc) if new_desc else ''), ('new creator {}'.format(new_creator) if new_creator else ''), ('new target time {}'.format(new_target_time) if new_target_time else '')))\n", "label": 1}
{"function": "\n\ndef main(prettypath, verify=False):\n    for pins in range(2, 9):\n        for generator in (top_pth_fp, side_pth_fp, top_smd_fp, side_smd_fp):\n            (name, fp) = generator(pins)\n            path = os.path.join(prettypath, (name + '.kicad_mod'))\n            if verify:\n                print('Verifying', path)\n            if os.path.isfile(path):\n                with open(path) as f:\n                    old = f.read()\n                old = [n for n in sexp_parse(old) if (n[0] != 'tedit')]\n                new = [n for n in sexp_parse(fp) if (n[0] != 'tedit')]\n                if (new == old):\n                    continue\n            if verify:\n                return False\n            else:\n                with open(path, 'w') as f:\n                    f.write(fp)\n    if verify:\n        return True\n", "label": 1}
{"function": "\n\ndef test_read_job3():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test.test', 'test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef run(self, server, file_id, size, abstract_path, buffer_size=constants.DEFAULT_BUFFER_SIZE):\n    response = None\n    client = None\n    buf = []\n    more_coming = True\n    have_next = False\n    failure = None\n    received = 0\n    chunks_received = 0\n    last_chunk_id = None\n    (server << ('request', file_id))\n    while True:\n        try:\n            msg = self.get(OR(('chunk', ANY, ANY, ANY), 'next', ('terminated', ANY)), timeout=10)\n        except Empty:\n            (client << ('failure', 'timeout'))\n            break\n        if (('terminated', ANY) == msg):\n            (_, who) = msg\n            if (who == client):\n                break\n            else:\n                assert (who == response)\n                if more_coming:\n                    failure = 'response-died'\n        elif (msg == 'next'):\n            assert (buf or more_coming), 'should have stopped already'\n            client = self.sender\n            if failure:\n                (client << ('failure', failure))\n                break\n            if buf:\n                chunk = buf.pop(0)\n                (client << ('chunk', chunk, (bool(buf) or more_coming)))\n                if ((not more_coming) and (not buf)):\n                    (client << 'stop')\n                    break\n            else:\n                have_next = True\n        else:\n            (_, chunk, more_coming, chunk_id) = msg\n            received += len(chunk)\n            chunks_received += 1\n            if (not ((last_chunk_id is None) or (chunk_id == (last_chunk_id + 1)))):\n                failure = 'inconsistent'\n                continue\n            last_chunk_id = chunk_id\n            if (not response):\n                response = self.sender\n                self.watch(response)\n            else:\n                (response << ('received', received))\n            if have_next:\n                assert (not buf)\n                have_next = False\n                (client << ('chunk', chunk, more_coming))\n                if (not more_coming):\n                    (client << 'stop')\n                    break\n            else:\n                buf.append(chunk)\n", "label": 1}
{"function": "\n\ndef iterlookupjoin(left, right, lkey, rkey, missing=None, lprefix=None, rprefix=None):\n    lit = iter(left)\n    rit = iter(right)\n    lhdr = next(lit)\n    rhdr = next(rit)\n    lkind = asindices(lhdr, lkey)\n    rkind = asindices(rhdr, rkey)\n    lgetk = operator.itemgetter(*lkind)\n    rgetk = operator.itemgetter(*rkind)\n    rvind = [i for i in range(len(rhdr)) if (i not in rkind)]\n    rgetv = rowgetter(*rvind)\n    if (lprefix is None):\n        outhdr = list(lhdr)\n    else:\n        outhdr = [(text_type(lprefix) + text_type(f)) for f in lhdr]\n    if (rprefix is None):\n        outhdr.extend(rgetv(rhdr))\n    else:\n        outhdr.extend([(text_type(rprefix) + text_type(f)) for f in rgetv(rhdr)])\n    (yield tuple(outhdr))\n\n    def joinrows(_lrowgrp, _rrowgrp):\n        if (_rrowgrp is None):\n            for lrow in _lrowgrp:\n                outrow = list(lrow)\n                outrow.extend(([missing] * len(rvind)))\n                (yield tuple(outrow))\n        else:\n            rrow = next(iter(_rrowgrp))\n            for lrow in _lrowgrp:\n                outrow = list(lrow)\n                outrow.extend(rgetv(rrow))\n                (yield tuple(outrow))\n    lgit = itertools.groupby(lit, key=lgetk)\n    rgit = itertools.groupby(rit, key=rgetk)\n    lrowgrp = []\n    (lkval, rkval) = (None, None)\n    try:\n        (lkval, lrowgrp) = next(lgit)\n        (rkval, rrowgrp) = next(rgit)\n        while True:\n            if (lkval < rkval):\n                for row in joinrows(lrowgrp, None):\n                    (yield tuple(row))\n                (lkval, lrowgrp) = next(lgit)\n            elif (lkval > rkval):\n                (rkval, rrowgrp) = next(rgit)\n            else:\n                for row in joinrows(lrowgrp, rrowgrp):\n                    (yield tuple(row))\n                (lkval, lrowgrp) = next(lgit)\n                (rkval, rrowgrp) = next(rgit)\n    except StopIteration:\n        pass\n    if (lkval > rkval):\n        for row in joinrows(lrowgrp, None):\n            (yield tuple(row))\n    for (lkval, lrowgrp) in lgit:\n        for row in joinrows(lrowgrp, None):\n            (yield tuple(row))\n", "label": 1}
{"function": "\n\ndef handle_noargs(self, **options):\n    if ((not options['watch']) and (not options['initial_scan'])):\n        sys.exit('--no-initial-scan option should be used with --watch.')\n    scanned_dirs = get_scanned_dirs()\n    verbosity = int(options['verbosity'])\n    compilers = utils.get_compilers().values()\n    if ((not options['watch']) or options['initial_scan']):\n        for scanned_dir in scanned_dirs:\n            for (dirname, dirnames, filenames) in os.walk(scanned_dir):\n                for filename in filenames:\n                    path = os.path.join(dirname, filename)[len(scanned_dir):]\n                    if path.startswith('/'):\n                        path = path[1:]\n                    for compiler in compilers:\n                        if compiler.is_supported(path):\n                            try:\n                                compiler.handle_changed_file(path, verbosity=options['verbosity'])\n                            except (exceptions.StaticCompilationError, ValueError) as e:\n                                print(e)\n                            break\n    if options['watch']:\n        from static_precompiler.watch import watch_dirs\n        watch_dirs(scanned_dirs, verbosity)\n", "label": 1}
{"function": "\n\ndef __init__(self, v, aspectValues=None):\n    global Aspect\n    if (Aspect is None):\n        from arelle.ModelFormulaObject import Aspect\n    self.modelXbrl = v.modelXbrl\n    if (aspectValues is None):\n        aspectValues = {\n            \n        }\n    self.aspectEntryObjectId = aspectValues.get('aspectEntryObjectId', None)\n    if (Aspect.CONCEPT in aspectValues):\n        qname = aspectValues[Aspect.CONCEPT]\n        self.qname = qname\n        self.concept = v.modelXbrl.qnameConcepts.get(qname)\n        self.isItem = ((self.concept is not None) and self.concept.isItem)\n        self.isTuple = ((self.concept is not None) and self.concept.isTuple)\n    else:\n        self.qname = None\n        self.concept = None\n        self.isItem = False\n        self.isTuple = False\n    if (Aspect.LOCATION in aspectValues):\n        self.parent = aspectValues[Aspect.LOCATION]\n        try:\n            self.isTuple = self.parent.isTuple\n        except AttributeError:\n            self.isTuple = False\n    else:\n        self.parent = v.modelXbrl.modelDocument.xmlRootElement\n    self.isNumeric = ((self.concept is not None) and self.concept.isNumeric)\n    self.context = ContextPrototype(v, aspectValues)\n    if (Aspect.UNIT in aspectValues):\n        self.unit = UnitPrototype(v, aspectValues)\n    else:\n        self.unit = None\n    self.factObjectId = None\n", "label": 0}
{"function": "\n\n@classmethod\ndef get_distribution(cls, ctx, name=None, recipes=[], allow_download=True, force_build=False, allow_build=True, extra_dist_dirs=[], require_perfect_match=False):\n    \"Takes information about the distribution, and decides what kind of\\n        distribution it will be.\\n\\n        If parameters conflict (e.g. a dist with that name already\\n        exists, but doesn't have the right set of recipes),\\n        an error is thrown.\\n\\n        Parameters\\n        ----------\\n        name : str\\n            The name of the distribution. If a dist with this name already '\\n            exists, it will be used.\\n        recipes : list\\n            The recipes that the distribution must contain.\\n        allow_download : bool\\n            Whether binary dists may be downloaded.\\n        allow_build : bool\\n            Whether the distribution may be built from scratch if necessary.\\n            This is always False on e.g. Windows.\\n        force_download: bool\\n            If True, only downloaded dists are considered.\\n        force_build : bool\\n            If True, the dist is forced to be built locally.\\n        extra_dist_dirs : list\\n            Any extra directories in which to search for dists.\\n        require_perfect_match : bool\\n            If True, will only match distributions with precisely the\\n            correct set of recipes.\\n        \"\n    existing_dists = Distribution.get_distributions(ctx)\n    needs_build = True\n    possible_dists = existing_dists\n    if ((name is not None) and name):\n        possible_dists = [d for d in possible_dists if (d.name == name)]\n    _possible_dists = []\n    for dist in possible_dists:\n        for recipe in recipes:\n            if (recipe not in dist.recipes):\n                break\n        else:\n            _possible_dists.append(dist)\n    possible_dists = _possible_dists\n    if possible_dists:\n        info('Of the existing distributions, the following meet the given requirements:')\n        pretty_log_dists(possible_dists)\n    else:\n        info('No existing dists meet the given requirements!')\n    for dist in possible_dists:\n        if force_build:\n            continue\n        if ((set(dist.recipes) == set(recipes)) or (set(recipes).issubset(set(dist.recipes)) and (not require_perfect_match))):\n            info_notify('{} has compatible recipes, using this one'.format(dist.name))\n            return dist\n    assert (len(possible_dists) < 2)\n    if ((not name) and possible_dists):\n        info('Asked for dist with name {} with recipes ({}), but a dist with this name already exists and has incompatible recipes ({})'.format(name, ', '.join(recipes), ', '.join(possible_dists[0].recipes)))\n        info('No compatible dist found, so exiting.')\n        exit(1)\n    dist = Distribution(ctx)\n    dist.needs_build = True\n    if (not name):\n        filen = 'unnamed_dist_{}'\n        i = 1\n        while exists(join(ctx.dist_dir, filen.format(i))):\n            i += 1\n        name = filen.format(i)\n    dist.name = name\n    dist.dist_dir = join(ctx.dist_dir, dist.name)\n    dist.recipes = recipes\n    return dist\n", "label": 1}
{"function": "\n\ndef acquire(self, timeout=None):\n    timeout = (((timeout is not None) and timeout) or self.timeout)\n    end_time = time.time()\n    if ((timeout is not None) and (timeout > 0)):\n        end_time += timeout\n    while True:\n        try:\n            os.symlink(self.unique_name, self.lock_file)\n        except OSError:\n            if self.i_am_locking():\n                return\n            else:\n                if ((timeout is not None) and (time.time() > end_time)):\n                    if (timeout > 0):\n                        raise LockTimeout(('Timeout waiting to acquire lock for %s' % self.path))\n                    else:\n                        raise AlreadyLocked(('%s is already locked' % self.path))\n                time.sleep(((timeout / 10) if (timeout is not None) else 0.1))\n        else:\n            return\n", "label": 1}
{"function": "\n\ndef staggered_retries(run, *a, **kw):\n    '\\n    A version of spawn that will block will it is done\\n    running the function, and which will call the function\\n    repeatedly as time progresses through the timeouts list.\\n\\n    Best used for idempotent network calls (e.g. HTTP GETs).\\n\\n    e.g.::\\n\\n        user_data = async.staggered_retries(get_data, max_results,\\n                                            latent_data_ok, public_credential_load,\\n                                            timeouts_secs=[0.1, 0.5, 1, 2])\\n\\n    returns None on timeout.\\n    '\n    ctx = context.get_context()\n    ready = gevent.event.Event()\n    ready.clear()\n\n    def callback(source):\n        if source.successful():\n            ready.set()\n    if ('timeouts_secs' in kw):\n        timeouts_secs = kw.pop('timeouts_secs')\n    else:\n        timeouts_secs = [0.05, 0.1, 0.15, 0.2]\n    if (timeouts_secs[0] > 0):\n        timeouts_secs.insert(0, 0)\n    gs = gevent.spawn(run, *a, **kw)\n    gs.link_value(callback)\n    running = [gs]\n    for i in range(1, len(timeouts_secs)):\n        this_timeout = (timeouts_secs[i] - timeouts_secs[(i - 1)])\n        if ctx.dev:\n            this_timeout = (this_timeout * 5.0)\n        ml.ld2('Using timeout {0}', this_timeout)\n        try:\n            with gevent.Timeout(this_timeout):\n                ready.wait()\n                break\n        except gevent.Timeout:\n            ml.ld2('Timed out!')\n            log_rec = ctx.log.critical('ASYNC.STAGGER', run.__name__)\n            log_rec.failure('timed out after {timeout}', timeout=this_timeout)\n            gs = gevent.spawn(run, *a, **kw)\n            gs.link_value(callback)\n            running.append(gs)\n    vals = [l.value for l in running if l.successful()]\n    for g in running:\n        g.kill()\n    if vals:\n        return vals[0]\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef tearDown(self):\n    'Runs after each test method to tear down test environment.'\n    try:\n        self.mox.UnsetStubs()\n        self.stubs.UnsetAll()\n        self.stubs.SmartUnsetAll()\n        self.mox.VerifyAll()\n        super(TestCase, self).tearDown()\n    finally:\n        if FLAGS.fake_rabbit:\n            fakerabbit.reset_all()\n        if (FLAGS.connection_type == 'fake'):\n            if hasattr(fake.FakeConnection, '_instance'):\n                del fake.FakeConnection._instance\n        if (FLAGS.image_service == 'nova.image.fake.FakeImageService'):\n            nova.image.fake.FakeImageService_reset()\n        self.reset_flags()\n        for x in self.injected:\n            try:\n                x.stop()\n            except AssertionError:\n                pass\n        for x in self._services:\n            try:\n                x.kill()\n            except Exception:\n                pass\n", "label": 0}
{"function": "\n\n@property\ndef previous_time(self):\n    now = timezone.now()\n    recurring_end = occurring_end = None\n    try:\n        occurring_rule = self.occurring_rule\n    except OccurringRule.DoesNotExist:\n        pass\n    else:\n        if (occurring_rule and (occurring_rule.dt_end < now)):\n            occurring_end = (occurring_rule.dt_end, occurring_rule)\n    rrules = self.recurring_rules.filter(begin__lt=now)\n    recurring_ends = [(rule.dt_end, rule) for rule in rrules if (rule.dt_end is not None)]\n    recurring_ends.sort(key=itemgetter(0), reverse=True)\n    try:\n        recurring_end = recurring_ends[0]\n    except IndexError:\n        pass\n    ends = [i for i in (recurring_end, occurring_end) if (i is not None)]\n    ends.sort(key=itemgetter(0), reverse=True)\n    try:\n        return ends[0][1]\n    except IndexError:\n        return None\n", "label": 1}
{"function": "\n\ndef get_substream_rstates(self, n_streams, dtype, inc_rstate=True):\n    '\\n        Initialize a matrix in which each row is a MRG stream state,\\n        and they are spaced by 2**72 samples.\\n\\n        '\n    assert isinstance(dtype, str)\n    assert (n_streams < (2 ** 72))\n    assert (n_streams > 0)\n    rval = numpy.zeros((n_streams, 6), dtype='int32')\n    rval[0] = self.rstate\n    if (multMatVect.dot_modulo is None):\n        multMatVect(rval[0], A1p72, M1, A2p72, M2)\n    f = multMatVect.dot_modulo\n    f.input_storage[0].storage[0] = A1p72\n    f.input_storage[2].storage[0] = M1\n    f.input_storage[3].storage[0] = A2p72\n    f.input_storage[5].storage[0] = M2\n    for i in xrange(1, n_streams):\n        v = rval[(i - 1)]\n        f.input_storage[1].storage[0] = v[:3]\n        f.input_storage[4].storage[0] = v[3:]\n        f.fn()\n        rval[i] = f.output_storage[0].storage[0]\n    if inc_rstate:\n        self.inc_rstate()\n    if (self.use_cuda and (dtype == 'float32')):\n        rval = rval.flatten()\n        tmp_float_buf = numpy.frombuffer(rval.data, dtype='float32')\n        assert (tmp_float_buf.shape == rval.shape)\n        assert (tmp_float_buf.view('int32') == rval).all()\n        rval = tmp_float_buf\n    return rval\n", "label": 1}
{"function": "\n\ndef apply(self, tree):\n    '\\n        Applies the configured optimizations to the given node tree. Modifies the tree in-place\\n        to be sure to have a deep copy if you need the original one. It raises an error instance\\n        whenever any optimization could not be applied to the given tree.\\n        '\n    enabled = self.__optimizations\n    if ('wrap' in enabled):\n        try:\n            ClosureWrapper.optimize(tree)\n        except CryptPrivates.Error as err:\n            raise Error(err)\n    if ('declarations' in enabled):\n        try:\n            CombineDeclarations.optimize(tree)\n        except CombineDeclarations.Error as err:\n            raise Error(err)\n    if ('blocks' in enabled):\n        try:\n            BlockReducer.optimize(tree)\n        except BlockReducer.Error as err:\n            raise Error(err)\n    if ('variables' in enabled):\n        try:\n            LocalVariables.optimize(tree)\n        except LocalVariables.Error as err:\n            raise Error(err)\n    if ('privates' in enabled):\n        try:\n            CryptPrivates.optimize(tree, tree.fileId)\n        except CryptPrivates.Error as err:\n            raise Error(err)\n", "label": 1}
{"function": "\n\ndef edit_article(article, directory, editor, repo, default_commit_msg, extension, test, editorflags):\n    'edit an article within your docs'\n    a = add_fileextension(article, extension)\n    a = os.path.join(directory, a)\n    d = os.path.dirname(a)\n    if (not os.path.isdir(d)):\n        try:\n            os.makedirs(d)\n        except OSError:\n            print(('Error: Creation of path %s is not possible' % d))\n            return False\n    if (test is False):\n        try:\n            if (editorflags is False):\n                subprocess.call([editor, a])\n            else:\n                subprocess.call([editor, editorflags, a])\n        except OSError:\n            print((\"Error: '%s' No such file or directory\" % editor))\n            return False\n    else:\n        try:\n            fp = open(a, 'ab+')\n            content = 'TEST CHANGE'\n            fp.write(content)\n            fp.close()\n        except OSError:\n            print((\"Error: '%s' No such file or directory\" % editor))\n    try:\n        repo.git.add(a)\n        if repo.is_dirty():\n            if (test is False):\n                try:\n                    msg = raw_input('Commit message: ')\n                    if (not msg):\n                        msg = default_commit_msg\n                except OSError:\n                    print('Error: Could not create commit')\n            else:\n                msg = default_commit_msg\n                print('automatic change done')\n            try:\n                repo.git.commit(m=msg)\n            except OSError:\n                print('Error: Could not create commit')\n        else:\n            print('Nothing to commit')\n    except (OSError, git.exc.GitCommandError) as e:\n        print('Error: Could not create commit')\n", "label": 1}
{"function": "\n\ndef slurp(self):\n    \"\\n        :returns: item_list - list of ELB's.\\n        :returns: exception_map - A dict where the keys are a tuple containing the\\n            location of the exception and the value is the actual exception\\n\\n        \"\n    self.prep_for_slurp()\n    from security_monkey.common.sts_connect import connect\n    item_list = []\n    exception_map = {\n        \n    }\n    for account in self.accounts:\n        self._setup_botocore(account)\n        for region in regions():\n            app.logger.debug('Checking {}/{}/{}'.format(self.index, account, region.name))\n            elb_conn = connect(account, 'ec2.elb', region=region.name)\n            botocore_client = self.botocore_session.create_client('elb', region_name=region.name)\n            botocore_operation = botocore_client.describe_load_balancer_policies\n            try:\n                all_elbs = []\n                marker = None\n                while True:\n                    response = self.wrap_aws_rate_limited_call(elb_conn.get_all_load_balancers, marker=marker)\n                    all_elbs.extend(response)\n                    if response.next_marker:\n                        marker = response.next_marker\n                    else:\n                        break\n            except Exception as e:\n                if (region.name not in TROUBLE_REGIONS):\n                    exc = BotoConnectionIssue(str(e), self.index, account, region.name)\n                    self.slurp_exception((self.index, account, region.name), exc, exception_map)\n                continue\n            app.logger.debug('Found {} {}'.format(len(all_elbs), self.i_am_plural))\n            for elb in all_elbs:\n                if self.check_ignore_list(elb.name):\n                    continue\n                try:\n                    elb_map = {\n                        \n                    }\n                    elb_map['availability_zones'] = list(elb.availability_zones)\n                    elb_map['canonical_hosted_zone_name'] = elb.canonical_hosted_zone_name\n                    elb_map['canonical_hosted_zone_name_id'] = elb.canonical_hosted_zone_name_id\n                    elb_map['dns_name'] = elb.dns_name\n                    elb_map['health_check'] = {\n                        'target': elb.health_check.target,\n                        'interval': elb.health_check.interval,\n                    }\n                    elb_map['is_cross_zone_load_balancing'] = self.wrap_aws_rate_limited_call(elb.is_cross_zone_load_balancing)\n                    elb_map['scheme'] = elb.scheme\n                    elb_map['security_groups'] = list(elb.security_groups)\n                    elb_map['source_security_group'] = elb.source_security_group.name\n                    elb_map['subnets'] = list(elb.subnets)\n                    elb_map['vpc_id'] = elb.vpc_id\n                    elb_map['is_logging'] = self.wrap_aws_rate_limited_call((lambda : elb.get_attributes().access_log.enabled))\n                    backends = []\n                    for be in elb.backends:\n                        backend = {\n                            \n                        }\n                        backend['instance_port'] = be.instance_port\n                        policies = []\n                        for bepol in be.policies:\n                            policies.append(bepol.policy_name)\n                        backend['policies'] = policies\n                        backends.append(backend)\n                    elb_map['backends'] = backends\n                    elb_policies = self._get_listener_policies(botocore_operation, elb)\n                    listeners = []\n                    for li in elb.listeners:\n                        listener = {\n                            'load_balancer_port': li.load_balancer_port,\n                            'instance_port': li.instance_port,\n                            'protocol': li.protocol,\n                            'instance_protocol': li.instance_protocol,\n                            'ssl_certificate_id': li.ssl_certificate_id,\n                            'policies': [elb_policies.get(policy_name, {\n                                'name': policy_name,\n                            }) for policy_name in li.policy_names],\n                        }\n                        listeners.append(listener)\n                    elb_map['listeners'] = listeners\n                    policies = {\n                        \n                    }\n                    app_cookie_stickiness_policies = []\n                    for policy in elb.policies.app_cookie_stickiness_policies:\n                        app_cookie_stickiness_policy = {\n                            \n                        }\n                        app_cookie_stickiness_policy['policy_name'] = policy.policy_name\n                        app_cookie_stickiness_policy['cookie_name'] = policy.cookie_name\n                        app_cookie_stickiness_policies.append(app_cookie_stickiness_policy)\n                    policies['app_cookie_stickiness_policies'] = app_cookie_stickiness_policies\n                    lb_cookie_stickiness_policies = []\n                    for policy in elb.policies.lb_cookie_stickiness_policies:\n                        lb_cookie_stickiness_policy = {\n                            \n                        }\n                        lb_cookie_stickiness_policy['policy_name'] = policy.policy_name\n                        lb_cookie_stickiness_policy['cookie_expiration_period'] = policy.cookie_expiration_period\n                        lb_cookie_stickiness_policies.append(lb_cookie_stickiness_policy)\n                    policies['lb_cookie_stickiness_policies'] = lb_cookie_stickiness_policies\n                    policies['other_policies'] = []\n                    for opol in elb.policies.other_policies:\n                        policies['other_policies'].append(opol.policy_name)\n                    elb_map['policies'] = policies\n                    item = ELBItem(region=region.name, account=account, name=elb.name, config=elb_map)\n                    item_list.append(item)\n                except Exception as e:\n                    self.slurp_exception((self.index, account, region.name, elb.name), e, exception_map)\n                    continue\n    return (item_list, exception_map)\n", "label": 1}
{"function": "\n\ndef client_request(self, client, method, url, **kwargs):\n    \"Send an http request using `client`'s endpoint and specified `url`.\\n\\n        If request was rejected as unauthorized (possibly because the token is\\n        expired), issue one authorization attempt and send the request once\\n        again.\\n\\n        :param client: instance of BaseClient descendant\\n        :param method: method of HTTP request\\n        :param url: URL of HTTP request\\n        :param kwargs: any other parameter that can be passed to\\n            `HTTPClient.request`\\n        \"\n    filter_args = {\n        'endpoint_type': (client.endpoint_type or self.endpoint_type),\n        'service_type': client.service_type,\n    }\n    (token, endpoint) = (self.cached_token, client.cached_endpoint)\n    just_authenticated = False\n    if (not (token and endpoint)):\n        try:\n            (token, endpoint) = self.auth_plugin.token_and_endpoint(**filter_args)\n        except exceptions.EndpointException:\n            pass\n        if (not (token and endpoint)):\n            self.authenticate()\n            just_authenticated = True\n            (token, endpoint) = self.auth_plugin.token_and_endpoint(**filter_args)\n            if (not (token and endpoint)):\n                raise exceptions.AuthorizationFailure(_('Cannot find endpoint or token for request'))\n    old_token_endpoint = (token, endpoint)\n    kwargs.setdefault('headers', {\n        \n    })['X-Auth-Token'] = token\n    self.cached_token = token\n    client.cached_endpoint = endpoint\n    try:\n        return self.request(method, self.concat_url(endpoint, url), **kwargs)\n    except exceptions.ConnectionRefused as refused_ex:\n        print(('ERROR: Cannot connect to %s.  Make sure it ' + ('is valid: %s.' % endpoint)))\n        raise refused_ex\n    except exceptions.EndpointException as endpoint_ex:\n        print((('ERROR: Service catalog endpoint is invalid. ' + 'Please check your ') + ('endpoint is valid: %s.' % endpoint)))\n        raise endpoint_ex\n    except requests.ConnectionError as conn_err:\n        print((('ERROR: Unable to connect to endpoint.  ' + 'Make sure that ') + ('your endpoint (%s) is valid.' % endpoint)))\n        raise conn_err\n    except exceptions.Unauthorized as unauth_ex:\n        if just_authenticated:\n            raise\n        self.cached_token = None\n        client.cached_endpoint = None\n        self.authenticate()\n        try:\n            (token, endpoint) = self.auth_plugin.token_and_endpoint(**filter_args)\n        except exceptions.EndpointException:\n            raise unauth_ex\n        if ((not (token and endpoint)) or (old_token_endpoint == (token, endpoint))):\n            raise unauth_ex\n        self.cached_token = token\n        client.cached_endpoint = endpoint\n        kwargs['headers']['X-Auth-Token'] = token\n        return self.request(method, self.concat_url(endpoint, url), **kwargs)\n", "label": 1}
{"function": "\n\ndef assertFormErrors(self, response, count=0, message=None, context_name='form'):\n    \"\\n        Asserts that the response does contain a form in it's\\n        context, and that form has errors, if count were given,\\n        it must match the exact numbers of errors\\n        \"\n    context = getattr(response, 'context', {\n        \n    })\n    assert (context and (context_name in context)), 'The response did not contain a form.'\n    errors = response.context[context_name]._errors\n    if count:\n        assert (len(errors) == count), ('%d errors were found on the form, %d expected' % (len(errors), count))\n        if (message and (message not in unicode(errors))):\n            self.fail(('Expected message not found, instead found: %s' % [('%s: %s' % (key, [e for e in field_errors])) for (key, field_errors) in errors.items()]))\n    else:\n        assert (len(errors) > 0), 'No errors were found on the form'\n", "label": 0}
{"function": "\n\ndef discover_modules(self):\n    \" Return module sequence discovered from ``self.package_name``\\n\\n\\n        Parameters\\n        ----------\\n        None\\n\\n        Returns\\n        -------\\n        mods : sequence\\n            Sequence of module names within ``self.package_name``\\n\\n        Examples\\n        --------\\n        >>> dw = ApiDocWriter('sphinx')\\n        >>> mods = dw.discover_modules()\\n        >>> 'sphinx.util' in mods\\n        True\\n        >>> dw.skip_patterns.append('\\\\.util$')\\n        >>> 'sphinx.util' in dw.discover_modules()\\n        False\\n        >>>\\n        \"\n    modules = [self.package_name]\n    for (dirpath, dirnames, filenames) in os.walk(self.root_path, topdown=False):\n        root_uri = self._path2uri(os.path.join(self.root_path, dirpath))\n        filenames = [f for f in filenames if f.endswith('.py')]\n        uris = [u.strip('.py') for u in (dirnames + filenames)]\n        for uri in uris:\n            package_uri = '.'.join((root_uri, uri))\n            if (self._uri2path(package_uri) and self._survives_exclude(package_uri)):\n                try:\n                    mod = __import__(package_uri, fromlist=['cesium'])\n                    mod.__all__\n                    modules.append(package_uri)\n                except (ImportError, AttributeError):\n                    pass\n    return sorted(modules)\n", "label": 0}
{"function": "\n\ndef test_database_url_with_options(self):\n    os.environ['DATABASE_URL'] = 'postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn?sslrootcert=rds-combined-ca-bundle.pem&sslmode=verify-full'\n    url = dj_database_url.config()\n    assert (url['ENGINE'] == 'django.db.backends.postgresql_psycopg2')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 5431)\n    assert (url['OPTIONS'] == {\n        'sslrootcert': 'rds-combined-ca-bundle.pem',\n        'sslmode': 'verify-full',\n    })\n    os.environ['DATABASE_URL'] = 'postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn?'\n    url = dj_database_url.config()\n    assert ('OPTIONS' not in url)\n", "label": 0}
{"function": "\n\ndef getEncodableAttributes(self, obj, codec=None):\n    attrs = pyamf.ClassAlias.getEncodableAttributes(self, obj, codec=codec)\n    gae_objects = (getGAEObjects(codec.context) if codec else None)\n    if (self.reference_properties and gae_objects):\n        for (name, prop) in self.reference_properties.iteritems():\n            klass = prop.reference_class\n            key = prop.get_value_for_datastore(obj)\n            if (not key):\n                continue\n            try:\n                attrs[name] = gae_objects.getClassKey(klass, key)\n            except KeyError:\n                ref_obj = getattr(obj, name)\n                gae_objects.addClassKey(klass, key, ref_obj)\n                attrs[name] = ref_obj\n    for k in attrs.keys()[:]:\n        if k.startswith('_'):\n            del attrs[k]\n    for attr in obj.dynamic_properties():\n        attrs[attr] = getattr(obj, attr)\n    if (not self.no_key_attr):\n        attrs[self.KEY_ATTR] = (str(obj.key()) if obj.is_saved() else None)\n    return attrs\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_hdf(node):\n    data = json.loads(node[0])\n    desc = node[1]\n    try:\n        cmap = json.loads(node[2])\n    except:\n        cmap = node[2]\n    vmin = json.loads(node[3])\n    vmax = json.loads(node[4])\n    state = json.loads(node[5])\n    attrs = json.loads(node[6])\n    try:\n        xfmname = json.loads(node[7])\n    except ValueError:\n        xfmname = None\n    if (not isinstance(vmin, list)):\n        vmin = [vmin]\n    if (not isinstance(vmax, list)):\n        vmax = [vmax]\n    if (not isinstance(cmap, list)):\n        cmap = [cmap]\n    if (len(data) == 1):\n        xfm = (None if (xfmname is None) else xfmname[0])\n        return _from_hdf_view(node.file, data[0], xfmname=xfm, cmap=cmap[0], description=desc, vmin=vmin[0], vmax=vmax[0], state=state, **attrs)\n    else:\n        views = [_from_hdf_view(node.file, d, xfmname=x) for (d, x) in zip(data, xfname)]\n        raise NotImplementedError\n", "label": 0}
{"function": "\n\ndef to_field_allowed(self, request, to_field):\n    '\\n        Returns True if the model associated with this admin should be\\n        allowed to be referenced by the specified field.\\n        '\n    opts = self.model._meta\n    try:\n        field = opts.get_field(to_field)\n    except FieldDoesNotExist:\n        return False\n    if field.primary_key:\n        return True\n    for many_to_many in opts.many_to_many:\n        if (many_to_many.m2m_target_field_name() == to_field):\n            return True\n    registered_models = set()\n    for (model, admin) in self.admin_site._registry.items():\n        registered_models.add(model)\n        for inline in admin.inlines:\n            registered_models.add(inline.model)\n    related_objects = (f for f in opts.get_fields(include_hidden=True) if (f.auto_created and (not f.concrete)))\n    for related_object in related_objects:\n        related_model = related_object.related_model\n        remote_field = related_object.field.remote_field\n        if (any((issubclass(model, related_model) for model in registered_models)) and hasattr(remote_field, 'get_related_field') and (remote_field.get_related_field() == field)):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef wait_for_schema_agreement(self, connection=None, preloaded_results=None, wait_time=None):\n    total_timeout = (wait_time if (wait_time is not None) else self._cluster.max_schema_agreement_wait)\n    if (total_timeout <= 0):\n        return True\n    with self._schema_agreement_lock:\n        if self._is_shutdown:\n            return\n        if (not connection):\n            connection = self._connection\n        if preloaded_results:\n            log.debug('[control connection] Attempting to use preloaded results for schema agreement')\n            peers_result = preloaded_results[0]\n            local_result = preloaded_results[1]\n            schema_mismatches = self._get_schema_mismatches(peers_result, local_result, connection.host)\n            if (schema_mismatches is None):\n                return True\n        log.debug('[control connection] Waiting for schema agreement')\n        start = self._time.time()\n        elapsed = 0\n        cl = ConsistencyLevel.ONE\n        schema_mismatches = None\n        while (elapsed < total_timeout):\n            peers_query = QueryMessage(query=self._SELECT_SCHEMA_PEERS, consistency_level=cl)\n            local_query = QueryMessage(query=self._SELECT_SCHEMA_LOCAL, consistency_level=cl)\n            try:\n                timeout = min(self._timeout, (total_timeout - elapsed))\n                (peers_result, local_result) = connection.wait_for_responses(peers_query, local_query, timeout=timeout)\n            except OperationTimedOut as timeout:\n                log.debug('[control connection] Timed out waiting for response during schema agreement check: %s', timeout)\n                elapsed = (self._time.time() - start)\n                continue\n            except ConnectionShutdown:\n                if self._is_shutdown:\n                    log.debug('[control connection] Aborting wait for schema match due to shutdown')\n                    return None\n                else:\n                    raise\n            schema_mismatches = self._get_schema_mismatches(peers_result, local_result, connection.host)\n            if (schema_mismatches is None):\n                return True\n            log.debug('[control connection] Schemas mismatched, trying again')\n            self._time.sleep(0.2)\n            elapsed = (self._time.time() - start)\n        log.warning('Node %s is reporting a schema disagreement: %s', connection.host, schema_mismatches)\n        return False\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    cleaver = Cleaver(environ, self._identity, self._backend, count_humans_only=self.count_humans_only)\n    environ[self.environ_key] = cleaver\n    if self.allow_override:\n        self._handle_variant_overrides(environ)\n    if (self.count_humans_only and (environ.get('REQUEST_METHOD', '') == 'POST') and (self.human_callback_token in environ.get('PATH_INFO', ''))):\n        (fp, length) = SplitMiddleware._copy_body_to_tempfile(environ)\n        environ.setdefault('CONTENT_LENGTH', length)\n        fs = cgi.FieldStorage(fp=fp, environ=environ, keep_blank_values=True)\n        try:\n            try:\n                x = int(fs.getlist('x')[0])\n            except (IndexError, ValueError):\n                x = 0\n            try:\n                y = int(fs.getlist('y')[0])\n            except (IndexError, ValueError):\n                y = 0\n            try:\n                z = int(fs.getlist('z')[0])\n            except (IndexError, ValueError):\n                z = 0\n            if (x and y and z and ((x + y) == z)):\n                self._backend.mark_human(cleaver.identity)\n                for e in self._backend.all_experiments():\n                    variant = self._backend.get_variant(cleaver.identity, e.name)\n                    if variant:\n                        self._backend.mark_participant(e.name, variant)\n                start_response('204 No Content', [('Content-Type', 'text/plain')])\n                return []\n        except (KeyError, ValueError):\n            pass\n        start_response('401 Unauthorized', [('Content-Type', 'text/plain')])\n        return []\n    return self.app(environ, start_response)\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_2d_freq():\n    ' reading/writing of 2D RNMRTK frequency domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_2d', 'freq_2d.sec'))\n    assert (data.shape == (2048, 4096))\n    assert (np.abs((data[(0, 1)] - (- 0.19))) <= 0.01)\n    assert (np.abs((data[(10, 18)] - 0.88)) <= 0.01)\n    assert (dic['sw'][1] == 50000.0)\n    assert (dic['sf'][1] == 125.69)\n    assert (dic['ppm'][1] == 55.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 50.65)\n    assert (dic['ppm'][0] == 120.0)\n    write_readback(dic, data)\n", "label": 0}
{"function": "\n\n@pytest.mark.skipif((sys.platform == 'darwin'), reason='Microsecond granularity does not work on Mac OS X')\ndef test_local_with_microsecond_auto_version(tmpdir):\n    after_dt = (datetime.now() - timedelta(seconds=1))\n    component_dir = tmpdir.mkdir('component')\n    bower_json_file = component_dir.join('bower.json')\n    bower_json_file.write(json.dumps({\n        'name': 'component',\n        'version': '2.1',\n        'main': 'main.js',\n    }))\n    main_js_file = component_dir.join('main.js')\n    main_js_file.write('/* this is main.js */')\n    bower = bowerstatic.Bower(autoversion=filesystem_microsecond_autoversion)\n    components = bower.components('components', os.path.join(os.path.dirname(__file__), 'bower_components'))\n    local = bower.local_components('local', components)\n    local.component(component_dir.strpath, version=None)\n\n    def wsgi(environ, start_response):\n        start_response('200 OK', [('Content-Type', 'text/html;charset=UTF-8')])\n        include = local.includer(environ)\n        include('component/main.js')\n        return [b'<html><head></head><body>Hello!</body></html>']\n    wrapped = bower.wrap(wsgi)\n    c = Client(wrapped)\n    response = c.get('/')\n    before_dt = datetime.now()\n\n    def get_url_dt(response):\n        s = compat.text_type(response.body, 'UTF-8')\n        start = (s.find('src=\"') + len('src=\"'))\n        end = s.find('\"', start)\n        url = s[start:end]\n        parts = url.split('/')\n        url_dt_str = parts[4]\n        url_dt = datetime.strptime(url_dt_str, '%Y-%m-%dT%H:%M:%S.%f')\n        return (url_dt_str, url_dt)\n    (url_dt_str, url_dt) = get_url_dt(response)\n    assert (url_dt >= after_dt)\n    assert (url_dt <= before_dt)\n    response = c.get(('/bowerstatic/local/component/%s/main.js' % url_dt_str))\n    assert (response.body == b'/* this is main.js */')\n    after_dt = (datetime.now() - timedelta(seconds=1))\n    main_js_file.write('/* this is main.js, modified */')\n    response = c.get('/')\n    before_dt = datetime.now()\n    (original_url_dt_str, original_url_dt) = (url_dt_str, url_dt)\n    (url_dt_str, url_dt) = get_url_dt(response)\n    assert (original_url_dt_str != url_dt_str)\n    assert (url_dt >= after_dt)\n    assert (url_dt <= before_dt)\n    assert (url_dt > original_url_dt)\n    c.get(('/bowerstatic/local/component/%s/main.js' % original_url_dt_str), status=404)\n    response = c.get(('/bowerstatic/local/component/%s/main.js' % url_dt_str))\n    assert (response.body == b'/* this is main.js, modified */')\n", "label": 0}
{"function": "\n\ndef run(fqdns, ip_addresses, dns, check, timer, logger):\n    '\\n    This is the \"main loop\". It will repeatedly check that the machines\\n    pointed by the DNS records are fine.\\n    '\n    if isinstance(fqdns, basestring):\n        fqdns = [fqdns]\n    if isinstance(ip_addresses, basestring):\n        ip_addresses = [ip_addresses]\n    logger.info('[INFO] Python-dns-failover starting')\n    while True:\n        now = time.time()\n        nextCheck = timer.getNextCheckTime()\n        if (now > nextCheck):\n            logger.warning('[WARNING] We are late by {0} seconds'.format((now - nextCheck)))\n        while (time.time() < nextCheck):\n            wait = (nextCheck - time.time())\n            logger.info('[INFO] Waiting {0} seconds before next round of checks'.format(wait))\n            time.sleep(wait)\n        for fqdn in fqdns:\n            logger.debug('[DEBUG] Getting DNS records for {0}'.format(fqdn))\n            try:\n                records = dns.get_a_records(fqdn)\n            except Exception as e:\n                logger.error('[ERROR] Error while retrieving DNS records for {0}: {1}'.format(fqdn, e))\n                continue\n            else:\n                logger.debug('[DEBUG] DNS current records: {0}'.format(records))\n            logger.debug('[DEBUG] Checking servers')\n            for ip_addr in ip_addresses:\n                if (not retryBoundedCheck(ip_addr, check, timer, logger)):\n                    if (ip_addr in records):\n                        if (len(records) < 2):\n                            logger.error('[ERROR] Server {0} seems dead and it is on a DNS record for {1}, but it will not be removed since it is the last server.'.format(ip_addr, fqdn))\n                        else:\n                            logger.error('[ERROR] Server {0} seems dead and it is on a DNS record for {1}. Removing it.'.format(ip_addr, fqdn))\n                            try:\n                                n_deleted = dns.delete_a_record(fqdn, ip_addr)\n                            except Exception as e:\n                                logger.error('[ERROR] Error while deleting record {0} for {1}: {2}'.format(ip_addr, fqdn, e))\n                            else:\n                                logger.info('[INFO] Records targeting {0} were removed from the DNS ({1}).'.format(ip_addr, n_deleted))\n                                break\n                    else:\n                        logger.info('[INFO] Server {0} continues dead.'.format(ip_addr))\n                elif (ip_addr in records):\n                    logger.info('[INFO] Server {0} seems alive and it is on the DNS'.format(ip_addr))\n                else:\n                    logger.warning('[WARNING] Server {0} seems alive and it is not on a DNS record for {1}. Adding it.'.format(ip_addr, fqdn))\n                    try:\n                        dns.add_a_record(fqdn, ip_addr)\n                    except Exception as e:\n                        logger.error('[ERROR] Error while adding record {0} for {1}: {2}'.format(ip_addr, fqdn, e))\n                    else:\n                        logger.info('[INFO] Record targeting {0} was added to the DNS.'.format(ip_addr))\n", "label": 1}
{"function": "\n\ndef test_m2o_lazy_loader_on_persistent(self):\n    'Compare the behaviors from the lazyloader using\\n        the \"committed\" state in all cases, vs. the lazyloader\\n        using the \"current\" state in all cases except during flush.\\n\\n        '\n    for loadfk in (True, False):\n        for loadrel in (True, False):\n            for autoflush in (True, False):\n                for manualflush in (True, False):\n                    for fake_autoexpire in (True, False):\n                        sess.autoflush = autoflush\n                        if loadfk:\n                            c1.parent_id\n                        if loadrel:\n                            c1.parent\n                        c1.parent_id = p2.id\n                        if manualflush:\n                            sess.flush()\n                        if fake_autoexpire:\n                            sess.expire(c1, ['parent'])\n                        if (loadrel and (not fake_autoexpire)):\n                            assert (c1.parent is p1)\n                        else:\n                            assert (c1.parent is p2)\n                        sess.rollback()\n", "label": 1}
{"function": "\n\ndef test_epsilon_rounding():\n    a = fb((('0.101' + ('0' * 200)) + '1'))\n    b = fb('1.10101')\n    c = mpf_mul(a, b, 250, round_floor)\n    assert (mpf_div(c, b, bitcount(a[1]), round_floor) == a)\n    assert (mpf_div(c, b, 2, round_down) == fb('0.10'))\n    assert (mpf_div(c, b, 3, round_down) == fb('0.101'))\n    assert (mpf_div(c, b, 2, round_up) == fb('0.11'))\n    assert (mpf_div(c, b, 3, round_up) == fb('0.110'))\n    assert (mpf_div(c, b, 2, round_floor) == fb('0.10'))\n    assert (mpf_div(c, b, 3, round_floor) == fb('0.101'))\n    assert (mpf_div(c, b, 2, round_ceiling) == fb('0.11'))\n    assert (mpf_div(c, b, 3, round_ceiling) == fb('0.110'))\n    a = fb((('-0.101' + ('0' * 200)) + '1'))\n    b = fb('1.10101')\n    c = mpf_mul(a, b, 250, round_floor)\n    assert (mpf_div(c, b, bitcount(a[1]), round_floor) == a)\n    assert (mpf_div(c, b, 2, round_down) == fb('-0.10'))\n    assert (mpf_div(c, b, 3, round_up) == fb('-0.110'))\n    assert (mpf_div(c, b, 2, round_floor) == fb('-0.11'))\n    assert (mpf_div(c, b, 3, round_floor) == fb('-0.110'))\n    assert (mpf_div(c, b, 2, round_ceiling) == fb('-0.10'))\n    assert (mpf_div(c, b, 3, round_ceiling) == fb('-0.101'))\n", "label": 1}
{"function": "\n\ndef transform(self, node, get_nodes=True):\n    '\\n        Checks if the graph from node corresponds to in_pattern. If it does,\\n        constructs out_pattern and performs the replacement.\\n\\n        '\n    if (get_nodes and (self.get_nodes is not None)):\n        for real_node in self.get_nodes(node):\n            if (real_node == 'output'):\n                continue\n            ret = self.transform(real_node, get_nodes=False)\n            if ((ret is not False) and (ret is not None)):\n                assert (len(real_node.outputs) == len(ret))\n                if self.values_eq_approx:\n                    ret.tag.values_eq_approx = self.values_eq_approx\n                return dict(izip(real_node.outputs, ret))\n    if (node.op != self.op):\n        return False\n\n    def match(pattern, expr, u, allow_multiple_clients=False, pdb=False):\n\n        def retry_with_equiv():\n            if (not self.skip_identities_fn):\n                return False\n            expr_equiv = self.skip_identities_fn(expr)\n            if (expr_equiv is None):\n                return False\n            return match(pattern, expr_equiv, u, allow_multiple_clients=allow_multiple_clients)\n        if isinstance(pattern, (list, tuple)):\n            if (expr.owner is None):\n                return False\n            if ((not (expr.owner.op == pattern[0])) or ((not allow_multiple_clients) and (len(expr.clients) > 1))):\n                return retry_with_equiv()\n            if ((len(pattern) - 1) != len(expr.owner.inputs)):\n                return retry_with_equiv()\n            for (p, v) in zip(pattern[1:], expr.owner.inputs):\n                u = match(p, v, u, self.allow_multiple_clients)\n                if (not u):\n                    return False\n        elif isinstance(pattern, dict):\n            try:\n                real_pattern = pattern['pattern']\n            except KeyError:\n                raise KeyError((\"Malformed pattern: %s (expected key 'pattern')\" % pattern))\n            constraint = pattern.get('constraint', (lambda expr: True))\n            if constraint(expr):\n                return match(real_pattern, expr, u, pattern.get('allow_multiple_clients', allow_multiple_clients))\n            else:\n                return retry_with_equiv()\n        elif isinstance(pattern, string_types):\n            v = unify.Var(pattern)\n            if ((u[v] is not v) and (u[v] is not expr)):\n                return retry_with_equiv()\n            else:\n                u = u.merge(expr, v)\n        elif (isinstance(pattern, (int, float)) and isinstance(expr, graph.Constant)):\n            if numpy.all((theano.tensor.constant(pattern).value == expr.value)):\n                return u\n            else:\n                return retry_with_equiv()\n        elif (isinstance(pattern, graph.Constant) and isinstance(expr, graph.Constant) and pattern.equals(expr)):\n            return u\n        else:\n            return retry_with_equiv()\n        if pdb:\n            import pdb\n            pdb.set_trace()\n        return u\n    u = match(self.in_pattern, node.out, unify.Unification(), True, self.pdb)\n    if u:\n\n        def build(pattern, u):\n            if isinstance(pattern, (list, tuple)):\n                args = [build(p, u) for p in pattern[1:]]\n                return pattern[0](*args)\n            elif isinstance(pattern, string_types):\n                return u[unify.Var(pattern)]\n            elif isinstance(pattern, (int, float)):\n                return pattern\n            else:\n                return pattern.clone()\n        p = self.out_pattern\n        ret = build(p, u)\n        if self.values_eq_approx:\n            ret.tag.values_eq_approx = self.values_eq_approx\n        return [ret]\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef _get_svn_url_rev(self, location):\n    from pip.exceptions import InstallationError\n    with open(os.path.join(location, self.dirname, 'entries')) as f:\n        data = f.read()\n    if (data.startswith('8') or data.startswith('9') or data.startswith('10')):\n        data = list(map(str.splitlines, data.split('\\n\\x0c\\n')))\n        del data[0][0]\n        url = data[0][3]\n        revs = ([int(d[9]) for d in data if ((len(d) > 9) and d[9])] + [0])\n    elif data.startswith('<?xml'):\n        match = _svn_xml_url_re.search(data)\n        if (not match):\n            raise ValueError(('Badly formatted data: %r' % data))\n        url = match.group(1)\n        revs = ([int(m.group(1)) for m in _svn_rev_re.finditer(data)] + [0])\n    else:\n        try:\n            xml = self.run_command(['info', '--xml', location], show_stdout=False)\n            url = _svn_info_xml_url_re.search(xml).group(1)\n            revs = [int(m.group(1)) for m in _svn_info_xml_rev_re.finditer(xml)]\n        except InstallationError:\n            (url, revs) = (None, [])\n    if revs:\n        rev = max(revs)\n    else:\n        rev = 0\n    return (url, rev)\n", "label": 1}
{"function": "\n\ndef div(self, other):\n    'Return ``self`` and ``other`` with ``gcd`` removed from each.\\n        This is optimized for the case when there are many factors in common.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.core.exprtools import Factors\\n        >>> from sympy.abc import x, y, z\\n        >>> from sympy import S\\n\\n        >>> a = Factors((x*y**2).as_powers_dict())\\n        >>> a.div(a)\\n        (Factors({}), Factors({}))\\n        >>> a.div(x*z)\\n        (Factors({y: 2}), Factors({z: 1}))\\n\\n        The ``/`` operator only gives ``quo``:\\n\\n        >>> a/x\\n        Factors({y: 2})\\n\\n        Factors treats its factors as though they are all in the numerator, so\\n        if you violate this assumption the results will be correct but will\\n        not strictly correspond to the numerator and denominator of the ratio:\\n\\n        >>> a.div(x/z)\\n        (Factors({y: 2}), Factors({z: -1}))\\n\\n        Factors is also naive about bases: it does not attempt any denesting\\n        of Rational-base terms, for example the following does not become\\n        2**(2*x)/2.\\n\\n        >>> Factors(2**(2*x + 2)).div(S(8))\\n        (Factors({2: 2*x + 2}), Factors({8: 1}))\\n\\n        factor_terms can clean up such Rational-bases powers:\\n\\n        >>> from sympy.core.exprtools import factor_terms\\n        >>> n, d = Factors(2**(2*x + 2)).div(S(8))\\n        >>> n.as_expr()/d.as_expr()\\n        2**(2*x + 2)/8\\n        >>> factor_terms(_)\\n        2**(2*x)/2\\n\\n        '\n    (quo, rem) = (dict(self.factors), {\n        \n    })\n    if (not isinstance(other, Factors)):\n        other = Factors(other)\n        if other.is_zero:\n            raise ZeroDivisionError\n        if self.is_zero:\n            return (Factors(S.Zero), Factors())\n    for (factor, exp) in other.factors.items():\n        if (factor in quo):\n            d = (quo[factor] - exp)\n            if _isnumber(d):\n                if (d <= 0):\n                    del quo[factor]\n                if (d >= 0):\n                    if d:\n                        quo[factor] = d\n                    continue\n                exp = (- d)\n            else:\n                r = quo[factor].extract_additively(exp)\n                if (r is not None):\n                    if r:\n                        quo[factor] = r\n                    else:\n                        del quo[factor]\n                else:\n                    other_exp = exp\n                    (sc, sa) = quo[factor].as_coeff_Add()\n                    if sc:\n                        (oc, oa) = other_exp.as_coeff_Add()\n                        diff = (sc - oc)\n                        if (diff > 0):\n                            quo[factor] -= oc\n                            other_exp = oa\n                        elif (diff < 0):\n                            quo[factor] -= sc\n                            other_exp = (oa - diff)\n                        else:\n                            quo[factor] = sa\n                            other_exp = oa\n                    if other_exp:\n                        rem[factor] = other_exp\n                    else:\n                        assert (factor not in rem)\n                continue\n        rem[factor] = exp\n    return (Factors(quo), Factors(rem))\n", "label": 1}
{"function": "\n\ndef pop_chunk(self, chunk_max_size):\n    'Pops a chunk of the given max size.\\n\\n        Optimized to avoid too much string copies.\\n\\n        Args:\\n            chunk_max_size (int): max size of the returned chunk.\\n\\n        Returns:\\n            string (bytes) with a size <= chunk_max_size.\\n        '\n    if (self._total_length < chunk_max_size):\n        res = self._tobytes()\n        self.clear()\n        return res\n    first_iteration = True\n    while True:\n        try:\n            data = self._deque.popleft()\n            data_length = len(data)\n            self._total_length -= data_length\n            if first_iteration:\n                if (data_length == chunk_max_size):\n                    return data\n                elif (data_length > chunk_max_size):\n                    view = self._get_pointer_or_memoryview(data, data_length)\n                    self.appendleft(view[chunk_max_size:])\n                    return view[:chunk_max_size]\n                else:\n                    chunk_write_buffer = WriteBuffer()\n            elif ((chunk_write_buffer._total_length + data_length) > chunk_max_size):\n                view = self._get_pointer_or_memoryview(data, data_length)\n                limit = ((chunk_max_size - chunk_write_buffer._total_length) - data_length)\n                self.appendleft(view[limit:])\n                data = view[:limit]\n            chunk_write_buffer.append(data)\n            if (chunk_write_buffer._total_length >= chunk_max_size):\n                break\n        except IndexError:\n            self._has_view = False\n            break\n        first_iteration = False\n    return chunk_write_buffer._tobytes()\n", "label": 0}
{"function": "\n\ndef generate(self, outdir):\n    if os.path.exists(outdir):\n        if os.path.isdir(outdir):\n            for entry in os.listdir(outdir):\n                path = os.path.join(outdir, entry)\n                if os.path.isdir(path):\n                    shutil.rmtree(path)\n                else:\n                    os.remove(path)\n        else:\n            raise ValueError(('%r is not a directory' % outdir))\n    try:\n        os.mkdir(outdir)\n    except OSError as err:\n        if (err.errno != errno.EEXIST):\n            raise\n    for filespec in self.files:\n        path = os.path.join(outdir, filespec.realpath(self.index_file))\n        if (not os.path.exists(os.path.dirname(path))):\n            os.makedirs(os.path.dirname(path))\n        with open(path, 'wb') as fobj:\n            data = self.view(filespec, mode='generating')\n            fobj.write(data)\n", "label": 0}
{"function": "\n\ndef test_proxy_snake_dict():\n    my_data = {\n        'one': 1,\n        'two': 2,\n        'none': None,\n        'threeOrFor': 3,\n        'inside': {\n            'otherCamelCase': 3,\n        },\n    }\n    p = ProxySnakeDict(my_data)\n    assert ('one' in p)\n    assert ('two' in p)\n    assert ('threeOrFor' in p)\n    assert ('none' in p)\n    assert (len(p) == len(my_data))\n    assert (p['none'] is None)\n    assert (p.get('none') is None)\n    assert (p.get('none_existent') is None)\n    assert ('three_or_for' in p)\n    assert (p.get('three_or_for') == 3)\n    assert ('inside' in p)\n    assert ('other_camel_case' in p['inside'])\n    assert (sorted(p.items()) == sorted(list([('inside', ProxySnakeDict({\n        'other_camel_case': 3,\n    })), ('none', None), ('three_or_for', 3), ('two', 2), ('one', 1)])))\n", "label": 1}
{"function": "\n\ndef _parallel_poly_from_expr(exprs, opt):\n    'Construct polynomials from expressions. '\n    from sympy.functions.elementary.piecewise import Piecewise\n    if (len(exprs) == 2):\n        (f, g) = exprs\n        if (isinstance(f, Poly) and isinstance(g, Poly)):\n            f = f.__class__._from_poly(f, opt)\n            g = g.__class__._from_poly(g, opt)\n            (f, g) = f.unify(g)\n            opt.gens = f.gens\n            opt.domain = f.domain\n            if (opt.polys is None):\n                opt.polys = True\n            return ([f, g], opt)\n    (origs, exprs) = (list(exprs), [])\n    (_exprs, _polys) = ([], [])\n    failed = False\n    for (i, expr) in enumerate(origs):\n        expr = sympify(expr)\n        if isinstance(expr, Basic):\n            if expr.is_Poly:\n                _polys.append(i)\n            else:\n                _exprs.append(i)\n                if opt.expand:\n                    expr = expr.expand()\n        else:\n            failed = True\n        exprs.append(expr)\n    if failed:\n        raise PolificationFailed(opt, origs, exprs, True)\n    if _polys:\n        for i in _polys:\n            exprs[i] = exprs[i].as_expr()\n    try:\n        (reps, opt) = _parallel_dict_from_expr(exprs, opt)\n    except GeneratorsNeeded:\n        raise PolificationFailed(opt, origs, exprs, True)\n    for k in opt.gens:\n        if isinstance(k, Piecewise):\n            raise PolynomialError('Piecewise generators do not make sense')\n    (coeffs_list, lengths) = ([], [])\n    all_monoms = []\n    all_coeffs = []\n    for rep in reps:\n        (monoms, coeffs) = list(zip(*list(rep.items())))\n        coeffs_list.extend(coeffs)\n        all_monoms.append(monoms)\n        lengths.append(len(coeffs))\n    domain = opt.domain\n    if (domain is None):\n        (opt.domain, coeffs_list) = construct_domain(coeffs_list, opt=opt)\n    else:\n        coeffs_list = list(map(domain.from_sympy, coeffs_list))\n    for k in lengths:\n        all_coeffs.append(coeffs_list[:k])\n        coeffs_list = coeffs_list[k:]\n    polys = []\n    for (monoms, coeffs) in zip(all_monoms, all_coeffs):\n        rep = dict(list(zip(monoms, coeffs)))\n        poly = Poly._from_dict(rep, opt)\n        polys.append(poly)\n    if (opt.polys is None):\n        opt.polys = bool(_polys)\n    return (polys, opt)\n", "label": 1}
{"function": "\n\ndef _downloadVariable(varname, dbname, dts, bbox):\n    'Downloads the PRISM data products for a specific variable and a set of\\n    dates *dt*. *varname* can be ppt, tmax or tmin.'\n    url = 'prism.oregonstate.edu'\n    ftp = FTP(url)\n    ftp.login()\n    ftp.cwd('daily/{0}'.format(varname))\n    outpath = tempfile.mkdtemp()\n    years = list(set([t.year for t in dts]))\n    for yr in years:\n        ftp.cwd('{0}'.format(yr))\n        filenames = [f for f in ftp.nlst() if ((datetime.strptime(f.split('_')[(- 2)], '%Y%m%d') >= dts[0]) and (datetime.strptime(f.split('_')[(- 2)], '%Y%m%d') <= dts[1]))]\n        for fname in filenames:\n            dt = datetime.strptime(fname.split('_')[(- 2)], '%Y%m%d')\n            with open('{0}/{1}'.format(outpath, fname), 'wb') as f:\n                ftp.retrbinary('RETR {0}'.format(fname), f.write)\n            if fname.endswith('zip'):\n                fz = zipfile.ZipFile('{0}/{1}'.format(outpath, fname))\n                lfilename = filter((lambda s: s.endswith('bil')), fz.namelist())[0]\n                fz.extractall(outpath)\n            else:\n                lfilename = fname\n            tfilename = lfilename.replace('.bil', '.tif')\n            if (bbox is not None):\n                subprocess.call(['gdal_translate', '-projwin', '{0}'.format(bbox[0]), '{0}'.format(bbox[3]), '{0}'.format(bbox[2]), '{0}'.format(bbox[1]), '{0}/{1}'.format(outpath, lfilename), '{0}/{1}'.format(outpath, tfilename)])\n                dbio.ingest(dbname, '{0}/{1}'.format(outpath, tfilename), dt, table[varname], False)\n            else:\n                dbio.ingest(dbname, '{0}/{1}'.format(outpath, lfilename), dt, table[varname], False)\n        ftp.cwd('..')\n", "label": 0}
{"function": "\n\ndef main():\n    a = A()\n    b = unknown(a)\n    assert (b.x == 1)\n    assert (b.y == 2)\n    assert (b.z == 'z')\n    assert (b.w == 'w')\n    c = B()\n    d = unknown(c)\n    assert (d.x == 1)\n    assert (d.y == 2)\n    assert (d.z == 'z')\n    assert (d.w == 'XXX')\n", "label": 0}
{"function": "\n\ndef main(argv):\n    if (len(argv) != 7):\n        prog = basename(argv[0])\n        print(('Syntax: %s <path> <redirect> <max_file_size> <max_file_count> <seconds> <key>' % prog))\n        print()\n        print('Where:')\n        print('  <path>            The prefix to use for form uploaded')\n        print('                    objects. For example:')\n        print('                    /v1/account/container/object_prefix_ would')\n        print('                    ensure all form uploads have that path')\n        print('                    prepended to the browser-given file name.')\n        print('  <redirect>        The URL to redirect the browser to after')\n        print('                    the uploads have completed.')\n        print('  <max_file_size>   The maximum file size per file uploaded.')\n        print('  <max_file_count>  The maximum number of uploaded files')\n        print('                    allowed.')\n        print('  <seconds>         The number of seconds from now to allow')\n        print('                    the form post to begin.')\n        print('  <key>             The X-Account-Meta-Temp-URL-Key for the')\n        print('                    account.')\n        print()\n        print('Example output:')\n        print('    Expires: 1323842228')\n        print('  Signature: 18de97e47345a82c4dbfb3b06a640dbb')\n        print()\n        print('Sample form:')\n        print()\n        print('NOTE: the <form> tag\\'s \"action\" attribute does not contain the Swift cluster\\'s hostname.')\n        print('You should manually add it before using the form.')\n        print()\n        print('<form action=\"/v1/a/c/o\" method=\"POST\" enctype=\"multipart/form-data\">')\n        print('  <input type=\"hidden\" name=\"max_file_size\" value=\"123\" />')\n        print('  ... more HTML ...')\n        print('  <input type=\"submit\" />')\n        print('</form>')\n        return 1\n    (path, redirect, max_file_size, max_file_count, seconds, key) = argv[1:]\n    try:\n        max_file_size = int(max_file_size)\n    except ValueError:\n        max_file_size = (- 1)\n    if (max_file_size < 0):\n        print('Please use a <max_file_size> value greater than or equal to 0.')\n        return 1\n    try:\n        max_file_count = int(max_file_count)\n    except ValueError:\n        max_file_count = 0\n    if (max_file_count < 1):\n        print('Please use a positive <max_file_count> value.')\n        return 1\n    try:\n        expires = int((time() + int(seconds)))\n    except ValueError:\n        expires = 0\n    if (expires < 1):\n        print('Please use a positive <seconds> value.')\n        return 1\n    parts = path.split('/', 4)\n    if ((len(parts) < 4) or parts[0] or (parts[1] != 'v1') or (not parts[2]) or (not parts[3])):\n        print('<path> must point to a container at least.')\n        print('For example: /v1/account/container')\n        print('         Or: /v1/account/container/object_prefix')\n        return 1\n    sig = hmac.new(key, ('%s\\n%s\\n%s\\n%s\\n%s' % (path, redirect, max_file_size, max_file_count, expires)), sha1).hexdigest()\n    print('  Expires:', expires)\n    print('Signature:', sig)\n    print('')\n    print('Sample form:\\n')\n    print('NOTE: the <form> tag\\'s \"action\" attribute does not contain the Swift cluster\\'s hostname.')\n    print('You should manually add it before using the form.\\n')\n    print(('<form action=\"%s\" method=\"POST\" enctype=\"multipart/form-data\">' % path))\n    if redirect:\n        print(('  <input type=\"hidden\" name=\"redirect\" value=\"%s\" />' % redirect))\n    print(('  <input type=\"hidden\" name=\"max_file_size\" value=\"%d\" />' % max_file_size))\n    print(('  <input type=\"hidden\" name=\"max_file_count\" value=\"%d\" />' % max_file_count))\n    print(('  <input type=\"hidden\" name=\"expires\" value=\"%d\" />' % expires))\n    print(('  <input type=\"hidden\" name=\"signature\" value=\"%s\" />' % sig))\n    print(('  <!-- This signature allows for at most %d files, -->' % max_file_count))\n    print('  <!-- but it may also have any smaller number. -->')\n    print('  <!-- Remove file inputs as needed. -->')\n    for i in range(max_file_count):\n        print(('  <input type=\"file\" name=\"file%d\" />' % i))\n        print('  <br />')\n    print('  <input type=\"submit\" />')\n    print('</form>')\n    return 0\n", "label": 1}
{"function": "\n\n@gof.local_optimizer([T.pow])\ndef local_pow_specialize(node):\n    if (node.op == T.pow):\n        odtype = node.outputs[0].dtype\n        xsym = node.inputs[0]\n        ysym = node.inputs[1]\n        y = local_mul_canonizer.get_constant(ysym)\n        if ((y is not None) and encompasses_broadcastable(xsym.type.broadcastable, ysym.type.broadcastable)):\n            rval = None\n            if N.all((y == 2)):\n                rval = [T.sqr(xsym)]\n            if N.all((y == 1)):\n                rval = [xsym]\n            if N.all((y == 0)):\n                rval = [T.fill(xsym, numpy.asarray(1, dtype=odtype))]\n            if N.all((y == 0.5)):\n                rval = [T.sqrt(xsym)]\n            if N.all((y == (- 0.5))):\n                rval = [T.inv(T.sqrt(xsym))]\n            if N.all((y == (- 1))):\n                rval = [T.inv(xsym)]\n            if N.all((y == (- 2))):\n                rval = [T.inv(T.sqr(xsym))]\n            if rval:\n                rval[0] = T.cast(rval[0], odtype)\n                assert (rval[0].type == node.outputs[0].type), (rval, node.outputs)\n                return rval\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef reconnect(self, wait=False, reset=True):\n    self._attempt += 1\n    self.nextReconnectTime = None\n    if self.connected:\n        drivers.log.reconnect(self.irc.network)\n        if (self in self._instances):\n            self._instances.remove(self)\n        try:\n            self.conn.shutdown(socket.SHUT_RDWR)\n        except:\n            pass\n        self.conn.close()\n        self.connected = False\n    if reset:\n        drivers.log.debug('Resetting %s.', self.irc)\n        self.irc.reset()\n    else:\n        drivers.log.debug('Not resetting %s.', self.irc)\n    if wait:\n        self.scheduleReconnect()\n        return\n    self.server = self._getNextServer()\n    network_config = getattr(conf.supybot.networks, self.irc.network)\n    socks_proxy = network_config.socksproxy()\n    try:\n        if socks_proxy:\n            import socks\n    except ImportError:\n        log.error('Cannot use socks proxy (SocksiPy not installed), using direct connection instead.')\n        socks_proxy = ''\n    if socks_proxy:\n        address = self.server[0]\n    else:\n        try:\n            address = utils.net.getAddressFromHostname(self.server[0], attempt=self._attempt)\n        except (socket.gaierror, socket.error) as e:\n            drivers.log.connectError(self.currentServer, e)\n            self.scheduleReconnect()\n            return\n    port = self.server[1]\n    drivers.log.connect(self.currentServer)\n    try:\n        self.conn = utils.net.getSocket(address, port=port, socks_proxy=socks_proxy, vhost=conf.supybot.protocols.irc.vhost(), vhostv6=conf.supybot.protocols.irc.vhostv6())\n    except socket.error as e:\n        drivers.log.connectError(self.currentServer, e)\n        self.scheduleReconnect()\n        return\n    self.conn.settimeout(max(10, (conf.supybot.drivers.poll() * 10)))\n    try:\n        self.conn.connect((address, port))\n        if network_config.ssl():\n            self.starttls()\n        elif (not network_config.requireStarttls()):\n            drivers.log.warning(('Connection to network %s does not use SSL/TLS, which makes it vulnerable to man-in-the-middle attacks and passive eavesdropping. You should consider upgrading your connection to SSL/TLS <http://doc.supybot.aperio.fr/en/latest/use/faq.html#how-to-make-a-connection-secure>' % self.irc.network))\n\n        def setTimeout():\n            self.conn.settimeout(conf.supybot.drivers.poll())\n        conf.supybot.drivers.poll.addCallback(setTimeout)\n        setTimeout()\n        self.connected = True\n        self.resetDelay()\n    except socket.error as e:\n        if (e.args[0] == 115):\n            now = time.time()\n            when = (now + 60)\n            whenS = log.timestamp(when)\n            drivers.log.debug('Connection in progress, scheduling connectedness check for %s', whenS)\n            self.writeCheckTime = when\n        else:\n            drivers.log.connectError(self.currentServer, e)\n            self.scheduleReconnect()\n        return\n    self._instances.append(self)\n", "label": 1}
{"function": "\n\ndef mrg_next_value(rstate, new_rstate):\n    (x11, x12, x13, x21, x22, x23) = rstate\n    assert (type(x11) == numpy.int32)\n    (i0, i7, i9, i15, i16, i22, i24) = np_int32_vals\n    y1 = (((((x12 & MASK12) << i22) + (x12 >> i9)) + ((x13 & MASK13) << i7)) + (x13 >> i24))\n    assert (type(y1) == numpy.int32)\n    if ((y1 < 0) or (y1 >= M1)):\n        y1 -= M1\n    y1 += x13\n    if ((y1 < 0) or (y1 >= M1)):\n        y1 -= M1\n    x13 = x12\n    x12 = x11\n    x11 = y1\n    y1 = (((x21 & MASK2) << i15) + (MULT2 * (x21 >> i16)))\n    assert (type(y1) == numpy.int32)\n    if ((y1 < 0) or (y1 >= M2)):\n        y1 -= M2\n    y2 = (((x23 & MASK2) << i15) + (MULT2 * (x23 >> i16)))\n    assert (type(y2) == numpy.int32)\n    if ((y2 < 0) or (y2 >= M2)):\n        y2 -= M2\n    y2 += x23\n    if ((y2 < 0) or (y2 >= M2)):\n        y2 -= M2\n    y2 += y1\n    if ((y2 < 0) or (y2 >= M2)):\n        y2 -= M2\n    x23 = x22\n    x22 = x21\n    x21 = y2\n    new_rstate[...] = [x11, x12, x13, x21, x22, x23]\n    assert (new_rstate.dtype == numpy.int32)\n    if (x11 <= x21):\n        return (((x11 - x21) + M1) * NORM)\n    else:\n        return ((x11 - x21) * NORM)\n", "label": 1}
{"function": "\n\ndef __init__(self, elem, nmSpc):\n    try:\n        self.name = elem.find(((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('IndividualName'))).text\n    except AttributeError:\n        self.name = None\n    try:\n        self.organization = elem.find(nmSpc.OWS('ProviderName')).text\n    except AttributeError:\n        self.organization = None\n    try:\n        self.address = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('DeliveryPoint'))).text\n    except AttributeError:\n        self.address = None\n    try:\n        self.city = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('City'))).text\n    except AttributeError:\n        self.city = None\n    try:\n        self.region = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('AdministrativeArea'))).text\n    except AttributeError:\n        self.region = None\n    try:\n        self.postcode = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('PostalCode'))).text\n    except AttributeError:\n        self.postcode = None\n    try:\n        self.country = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('Country'))).text\n    except AttributeError:\n        self.country = None\n    try:\n        self.email = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('ElectronicMailAddress'))).text\n    except AttributeError:\n        self.email = None\n", "label": 0}
{"function": "\n\ndef process_filing_body(filingnum, fp=None, logger=None):\n    if (not fp):\n        fp = form_parser()\n    if (not logger):\n        logger = fec_logger()\n    msg = ('process_filing_body: Starting # %s' % filingnum)\n    logger.info(msg)\n    connection = get_connection()\n    cursor = connection.cursor()\n    cmd = ('select fec_id, is_superceded, data_is_processed from fec_alerts_new_filing where filing_number=%s' % filingnum)\n    cursor.execute(cmd)\n    cd = CSV_dumper(connection)\n    result = cursor.fetchone()\n    if (not result):\n        msg = (\"process_filing_body: Couldn't find a new_filing for filing %s\" % filingnum)\n        logger.error(msg)\n        raise FilingHeaderDoesNotExist(msg)\n    header_id = 1\n    is_amended = result[1]\n    is_already_processed = result[2]\n    if is_already_processed:\n        msg = 'process_filing_body: This filing has already been entered.'\n        logger.error(msg)\n        raise FilingHeaderAlreadyProcessed(msg)\n    f1 = filing(filingnum)\n    form = f1.get_form_type()\n    version = f1.get_version()\n    filer_id = f1.get_filer_id()\n    if (not fp.is_allowed_form(form)):\n        if verbose:\n            msg = ('process_filing_body: Not a parseable form: %s - %s' % (form, filingnum))\n            logger.error(msg)\n        return None\n    linenum = 0\n    while True:\n        linenum += 1\n        row = f1.get_body_row()\n        if (not row):\n            break\n        try:\n            linedict = fp.parse_form_line(row, version)\n            process_body_row(linedict, filingnum, header_id, is_amended, cd, filer_id)\n        except ParserMissingError:\n            msg = ('process_filing_body: Unknown line type in filing %s line %s: type=%s Skipping.' % (filingnum, linenum, row[0]))\n            logger.warn(msg)\n            continue\n    cd.commit_all()\n    cd.close()\n    counter = cd.get_counter()\n    total_rows = 0\n    for i in counter:\n        total_rows += counter[i]\n    msg = ('process_filing_body: Filing # %s Total rows: %s Tally is: %s' % (filingnum, total_rows, counter))\n    logger.info(msg)\n    header_data = dict_to_hstore(counter)\n    cmd = (\"update fec_alerts_new_filing set lines_present='%s'::hstore where filing_number=%s\" % (header_data, filingnum))\n    cursor.execute(cmd)\n    cmd = ('update fec_alerts_new_filing set data_is_processed = True where filing_number=%s' % filingnum)\n    cursor.execute(cmd)\n    cmd = (\"update summary_data_committee_overlay set is_dirty=True where fec_id='%s'\" % filer_id)\n    cursor.execute(cmd)\n", "label": 1}
{"function": "\n\n@contextlib.contextmanager\ndef wait_lock(path, lock_fn=None, timeout=5, sleep=0.1, time_start=None):\n    '\\n    Obtain a write lock. If one exists, wait for it to release first\\n    '\n    if (not isinstance(path, six.string_types)):\n        raise FileLockError('path must be a string')\n    if (lock_fn is None):\n        lock_fn = (path + '.w')\n    if (time_start is None):\n        time_start = time.time()\n    obtained_lock = False\n\n    def _raise_error(msg, race=False):\n        '\\n        Raise a FileLockError\\n        '\n        raise FileLockError(msg, time_start=time_start)\n    try:\n        if (os.path.exists(lock_fn) and (not os.path.isfile(lock_fn))):\n            _raise_error('lock_fn {0} exists and is not a file'.format(lock_fn))\n        open_flags = ((os.O_CREAT | os.O_EXCL) | os.O_WRONLY)\n        while ((time.time() - time_start) < timeout):\n            try:\n                fh_ = os.open(lock_fn, open_flags)\n            except (IOError, OSError) as exc:\n                if (exc.errno != errno.EEXIST):\n                    _raise_error('Error {0} encountered obtaining file lock {1}: {2}'.format(exc.errno, lock_fn, exc.strerror))\n                log.trace('Lock file %s exists, sleeping %f seconds', lock_fn, sleep)\n                time.sleep(sleep)\n            else:\n                with os.fdopen(fh_, 'w'):\n                    pass\n                log.trace('Write lock %s obtained', lock_fn)\n                obtained_lock = True\n                (yield)\n                break\n        else:\n            _raise_error('Timeout of {0} seconds exceeded waiting for lock_fn {1} to be released'.format(timeout, lock_fn))\n    except FileLockError:\n        raise\n    except Exception as exc:\n        _raise_error('Error encountered obtaining file lock {0}: {1}'.format(lock_fn, exc))\n    finally:\n        if obtained_lock:\n            os.remove(lock_fn)\n            log.trace('Write lock for %s (%s) released', path, lock_fn)\n", "label": 1}
{"function": "\n\ndef test_promotes(self):\n    p = Problem(root=Group())\n    p.root.add('comp', self.comp)\n    self.comp.add_param('xxyyzz', 0.0)\n    self.comp.add_param('foobar', 0.0)\n    self.comp.add_output('a:bcd:efg', (- 1))\n    self.comp.add_output('x_y_z', np.zeros(10))\n    self.comp._promotes = ('*',)\n    p.setup(check=False)\n    for name in self.comp._init_params_dict:\n        self.assertTrue(self.comp._promoted(name))\n    for name in self.comp._init_unknowns_dict:\n        self.assertTrue(self.comp._promoted(name))\n    self.assertFalse(self.comp._promoted('blah'))\n    self.comp._promotes = ('x*',)\n    p.setup(check=False)\n    for name in self.comp._init_params_dict:\n        if name.startswith('x'):\n            self.assertTrue(self.comp._promoted(name))\n        else:\n            self.assertFalse(self.comp._promoted(name))\n    for name in self.comp._init_unknowns_dict:\n        if name.startswith('x'):\n            self.assertTrue(self.comp._promoted(name))\n        else:\n            self.assertFalse(self.comp._promoted(name))\n    self.comp._promotes = ('*:efg',)\n    p.setup(check=False)\n    for name in self.comp._init_params_dict:\n        if name.endswith(':efg'):\n            self.assertTrue(self.comp._promoted(name))\n        else:\n            self.assertFalse(self.comp._promoted(name))\n    for name in self.comp._init_unknowns_dict:\n        if name.endswith(':efg'):\n            self.assertTrue(self.comp._promoted(name))\n        else:\n            self.assertFalse(self.comp._promoted(name))\n    try:\n        self.comp._promotes = '*'\n        self.comp._promoted('xxyyzz')\n    except Exception as err:\n        self.assertEqual(text_type(err), \"'comp' promotes must be specified as a list, tuple or other iterator of strings, but '*' was specified\")\n", "label": 1}
{"function": "\n\ndef fit_inside(image, shape):\n    '\\n    Scales image down to fit inside shape preserves proportions of image\\n\\n    Parameters\\n    ----------\\n    image : WRITEME\\n    shape : WRITEME\\n\\n    Returns\\n    -------\\n    WRITEME\\n    '\n    assert (len(image.shape) == 3)\n    assert (len(shape) == 2)\n    if ((image.shape[0] <= shape[0]) and (image.shape[1] <= shape[1])):\n        return image.copy()\n    row_ratio = (float(image.shape[0]) / float(shape[0]))\n    col_ratio = (float(image.shape[1]) / float(shape[1]))\n    if (row_ratio > col_ratio):\n        target_shape = [shape[0], min((image.shape[1] / row_ratio), shape[1])]\n    else:\n        target_shape = [min((image.shape[0] / col_ratio), shape[0]), shape[1]]\n    assert (target_shape[0] <= shape[0])\n    assert (target_shape[1] <= shape[1])\n    assert ((target_shape[0] == shape[0]) or (target_shape[1] == shape[1]))\n    rval = rescale(image, target_shape)\n    return rval\n", "label": 0}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_time():\n    ' reading/writing of 3D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_3d', 'time_3d.sec'))\n    assert (data.shape == (128, 88, 1250))\n    assert (np.abs((data[(0, 1, 2)].real - 7.98)) <= 0.01)\n    assert (np.abs((data[(0, 1, 2)].imag - 33.82)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].real - (- 9.36))) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].imag - (- 7.75))) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef process_response(self, request, response):\n    if (not self.is_cms_request(request)):\n        return response\n    from django.utils.cache import add_never_cache_headers\n    if ((hasattr(request, 'toolbar') and request.toolbar.edit_mode) or (not all((ph.cache_placeholder for (ph, __) in getattr(request, 'placeholders', {\n        \n    }).values())))):\n        add_never_cache_headers(response)\n    if (hasattr(request, 'user') and request.user.is_staff and (response.status_code != 500)):\n        try:\n            pk = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE)).only('pk').order_by('-pk')[0].pk\n            if (hasattr(request, 'cms_latest_entry') and (request.cms_latest_entry != pk)):\n                log = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE))[0]\n                request.session['cms_log_latest'] = log.pk\n        except IndexError:\n            pass\n    return response\n", "label": 1}
{"function": "\n\ndef testParse(self):\n    s = 'foo = pkg_resources.tests.test_resources:TestEntryPoints [x]'\n    ep = EntryPoint.parse(s, self.dist)\n    self.assertfields(ep)\n    ep = EntryPoint.parse('bar baz=  spammity[PING]')\n    assert (ep.name == 'bar baz')\n    assert (ep.module_name == 'spammity')\n    assert (ep.attrs == ())\n    assert (ep.extras == ('ping',))\n    ep = EntryPoint.parse(' fizzly =  wocka:foo')\n    assert (ep.name == 'fizzly')\n    assert (ep.module_name == 'wocka')\n    assert (ep.attrs == ('foo',))\n    assert (ep.extras == ())\n    spec = 'html+mako = mako.ext.pygmentplugin:MakoHtmlLexer'\n    ep = EntryPoint.parse(spec)\n    assert (ep.name == 'html+mako')\n", "label": 0}
{"function": "\n\ndef _parse_network_settings(opts, current):\n    '\\n    Filters given options and outputs valid settings for\\n    the global network settings file.\\n    '\n    opts = dict(((k.lower(), v) for (k, v) in six.iteritems(opts)))\n    current = dict(((k.lower(), v) for (k, v) in six.iteritems(current)))\n    result = {\n        \n    }\n    valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n    if ('enabled' not in opts):\n        try:\n            opts['networking'] = current['networking']\n            _log_default_network('networking', current['networking'])\n        except ValueError:\n            _raise_error_network('networking', valid)\n    else:\n        opts['networking'] = opts['enabled']\n    if (opts['networking'] in valid):\n        if (opts['networking'] in _CONFIG_TRUE):\n            result['networking'] = 'yes'\n        elif (opts['networking'] in _CONFIG_FALSE):\n            result['networking'] = 'no'\n    else:\n        _raise_error_network('networking', valid)\n    if ('hostname' not in opts):\n        try:\n            opts['hostname'] = current['hostname']\n            _log_default_network('hostname', current['hostname'])\n        except Exception:\n            _raise_error_network('hostname', ['server1.example.com'])\n    if opts['hostname']:\n        result['hostname'] = opts['hostname']\n    else:\n        _raise_error_network('hostname', ['server1.example.com'])\n    if ('nozeroconf' in opts):\n        if (opts['nozeroconf'] in valid):\n            if (opts['nozeroconf'] in _CONFIG_TRUE):\n                result['nozeroconf'] = 'true'\n            elif (opts['nozeroconf'] in _CONFIG_FALSE):\n                result['nozeroconf'] = 'false'\n        else:\n            _raise_error_network('nozeroconf', valid)\n    for opt in opts:\n        if (opt not in ['networking', 'hostname', 'nozeroconf']):\n            result[opt] = opts[opt]\n    return result\n", "label": 1}
{"function": "\n\n@register.simple_tag(takes_context=True)\ndef wagtailuserbar(context, position='bottom-right'):\n    try:\n        request = context['request']\n    except KeyError:\n        return ''\n    if (not request.user.has_perm('wagtailadmin.access_admin')):\n        return ''\n    page = get_page_instance(context)\n    if (page is None):\n        return ''\n    if (page.pk is None):\n        return ''\n    try:\n        revision_id = request.revision_id\n    except AttributeError:\n        revision_id = None\n    if (revision_id is None):\n        items = [AdminItem(), ExplorePageItem(Page.objects.get(id=page.id)), EditPageItem(Page.objects.get(id=page.id)), AddPageItem(Page.objects.get(id=page.id))]\n    else:\n        items = [AdminItem(), ExplorePageItem(PageRevision.objects.get(id=revision_id).page), EditPageItem(PageRevision.objects.get(id=revision_id).page), AddPageItem(PageRevision.objects.get(id=revision_id).page), ApproveModerationEditPageItem(PageRevision.objects.get(id=revision_id)), RejectModerationEditPageItem(PageRevision.objects.get(id=revision_id))]\n    for fn in hooks.get_hooks('construct_wagtail_userbar'):\n        fn(request, items)\n    rendered_items = [item.render(request) for item in items]\n    rendered_items = [item for item in rendered_items if item]\n    return render_to_string('wagtailadmin/userbar/base.html', {\n        'request': request,\n        'items': rendered_items,\n        'position': position,\n        'page': page,\n        'revision_id': revision_id,\n    })\n", "label": 1}
{"function": "\n\ndef copy_tree(src, dst, symlinks=False, ignore=None):\n    'The ``copy_tree`` function is an exact copy of the built-in\\n    :func:`shutil.copytree`, with one key difference: it will not\\n    raise an exception if part of the tree already exists. It achieves\\n    this by using :func:`mkdir_p`.\\n\\n    Args:\\n        src (str): Path of the source directory to copy.\\n        dst (str): Destination path. Existing directories accepted.\\n        symlinks (bool): If ``True``, copy symlinks rather than their\\n            contents.\\n        ignore (callable): A callable that takes a path and directory\\n            listing, returning the files within the listing to be ignored.\\n\\n    For more details, check out :func:`shutil.copytree` and\\n    :func:`shutil.copy2`.\\n\\n    '\n    names = os.listdir(src)\n    if (ignore is not None):\n        ignored_names = ignore(src, names)\n    else:\n        ignored_names = set()\n    mkdir_p(dst)\n    errors = []\n    for name in names:\n        if (name in ignored_names):\n            continue\n        srcname = os.path.join(src, name)\n        dstname = os.path.join(dst, name)\n        try:\n            if (symlinks and os.path.islink(srcname)):\n                linkto = os.readlink(srcname)\n                os.symlink(linkto, dstname)\n            elif os.path.isdir(srcname):\n                copytree(srcname, dstname, symlinks, ignore)\n            else:\n                copy2(srcname, dstname)\n        except Error as e:\n            errors.extend(e.args[0])\n        except EnvironmentError as why:\n            errors.append((srcname, dstname, str(why)))\n    try:\n        copystat(src, dst)\n    except OSError as why:\n        if ((WindowsError is not None) and isinstance(why, WindowsError)):\n            pass\n        else:\n            errors.append((src, dst, str(why)))\n    if errors:\n        raise Error(errors)\n", "label": 1}
{"function": "\n\ndef _real_extract(self, url):\n    mobj = re.match(self._VALID_URL, url)\n    video_id = mobj.group('id')\n    if mobj.group('config'):\n        config_url = (url + '&form=json')\n        config_url = config_url.replace('swf/', 'config/')\n        config_url = config_url.replace('onsite/', 'onsite/config/')\n        config = self._download_json(config_url, video_id, 'Downloading config')\n        smil_url = (config['releaseUrl'] + '&format=SMIL&formats=MPEG4&manifest=f4m')\n    else:\n        smil_url = 'http://link.theplatform.com/s/dJ5BDC/{0}/meta.smil?format=smil&mbr=true'.format(video_id)\n    meta = self._download_xml(smil_url, video_id)\n    try:\n        error_msg = next((n.attrib['abstract'] for n in meta.findall(_x('.//smil:ref')) if (n.attrib.get('title') == 'Geographic Restriction')))\n    except StopIteration:\n        pass\n    else:\n        raise ExtractorError(error_msg, expected=True)\n    info_url = 'http://link.theplatform.com/s/dJ5BDC/{0}?format=preview'.format(video_id)\n    info_json = self._download_webpage(info_url, video_id)\n    info = json.loads(info_json)\n    subtitles = {\n        \n    }\n    captions = info.get('captions')\n    if isinstance(captions, list):\n        for caption in captions:\n            (lang, src) = (caption.get('lang'), caption.get('src'))\n            if (lang and src):\n                subtitles[lang] = src\n    if self._downloader.params.get('listsubtitles', False):\n        self._list_available_subtitles(video_id, subtitles)\n        return\n    subtitles = self.extract_subtitles(video_id, subtitles)\n    head = meta.find(_x('smil:head'))\n    body = meta.find(_x('smil:body'))\n    f4m_node = body.find(_x('smil:seq//smil:video'))\n    if ((f4m_node is not None) and ('.f4m' in f4m_node.attrib['src'])):\n        f4m_url = f4m_node.attrib['src']\n        if ('manifest.f4m?' not in f4m_url):\n            f4m_url += '?'\n        f4m_url += '&g=UXWGVKRWHFSP&hdcore=3.0.3'\n        formats = self._extract_f4m_formats(f4m_url, video_id)\n    else:\n        formats = []\n        switch = body.find(_x('smil:switch'))\n        if (switch is not None):\n            base_url = head.find(_x('smil:meta')).attrib['base']\n            for f in switch.findall(_x('smil:video')):\n                attr = f.attrib\n                width = int(attr['width'])\n                height = int(attr['height'])\n                vbr = (int(attr['system-bitrate']) // 1000)\n                format_id = ('%dx%d_%dk' % (width, height, vbr))\n                formats.append({\n                    'format_id': format_id,\n                    'url': base_url,\n                    'play_path': ('mp4:' + attr['src']),\n                    'ext': 'flv',\n                    'width': width,\n                    'height': height,\n                    'vbr': vbr,\n                })\n        else:\n            switch = body.find(_x('smil:seq//smil:switch'))\n            for f in switch.findall(_x('smil:video')):\n                attr = f.attrib\n                vbr = (int(attr['system-bitrate']) // 1000)\n                ext = determine_ext(attr['src'])\n                if (ext == 'once'):\n                    ext = 'mp4'\n                formats.append({\n                    'format_id': compat_str(vbr),\n                    'url': attr['src'],\n                    'vbr': vbr,\n                    'ext': ext,\n                })\n        self._sort_formats(formats)\n    return {\n        'id': video_id,\n        'title': info['title'],\n        'subtitles': subtitles,\n        'formats': formats,\n        'description': info['description'],\n        'thumbnail': info['defaultThumbnailUrl'],\n        'duration': (info['duration'] // 1000),\n    }\n", "label": 1}
{"function": "\n\ndef route_url(route_name, request, *elements, **kw):\n    try:\n        reg = request.registry\n    except AttributeError:\n        reg = get_current_registry()\n    mapper = reg.getUtility(IRoutesMapper)\n    route = mapper.routes.get(route_name)\n    if (route and ('custom_url_generator' in route.__dict__)):\n        (route_name, request, elements, kw) = route.custom_url_generator(route_name, request, *elements, **kw)\n    anchor = ''\n    qs = ''\n    app_url = None\n    if ('_query' in kw):\n        qs = ('?' + urlencode(kw.pop('_query'), doseq=True))\n    if ('_anchor' in kw):\n        anchor = kw.pop('_anchor')\n        if isinstance(anchor, unicode):\n            anchor = anchor.encode('utf-8')\n        anchor = ('#' + anchor)\n    if ('_app_url' in kw):\n        app_url = kw.pop('_app_url')\n    path = mapper.generate(route_name, kw)\n    if elements:\n        suffix = _join_elements(elements)\n        if (not path.endswith('/')):\n            suffix = ('/' + suffix)\n    else:\n        suffix = ''\n    if (app_url is None):\n        app_url = request.application_url\n    return ((((app_url + path) + suffix) + qs) + anchor)\n", "label": 1}
{"function": "\n\ndef _update_branch(repo, branch, is_active=False):\n    'Update a single branch.'\n    print(INDENT2, 'Updating', (BOLD + branch.name), end=': ')\n    upstream = branch.tracking_branch()\n    if (not upstream):\n        print((YELLOW + 'skipped:'), 'no upstream is tracked.')\n        return\n    try:\n        branch.commit\n    except ValueError:\n        print((YELLOW + 'skipped:'), 'branch has no revisions.')\n        return\n    try:\n        upstream.commit\n    except ValueError:\n        print((YELLOW + 'skipped:'), 'upstream does not exist.')\n        return\n    base = repo.git.merge_base(branch.commit, upstream.commit)\n    if (repo.commit(base) == upstream.commit):\n        print((BLUE + 'up to date'), end='.\\n')\n        return\n    if is_active:\n        try:\n            repo.git.merge(upstream.name, ff_only=True)\n            print((GREEN + 'done'), end='.\\n')\n        except exc.GitCommandError as err:\n            msg = err.stderr\n            if (('local changes' in msg) and ('would be overwritten' in msg)):\n                print((YELLOW + 'skipped:'), 'uncommitted changes.')\n            else:\n                print((YELLOW + 'skipped:'), 'not possible to fast-forward.')\n    else:\n        status = repo.git.merge_base(branch.commit, upstream.commit, is_ancestor=True, with_extended_output=True, with_exceptions=False)[0]\n        if (status != 0):\n            print((YELLOW + 'skipped:'), 'not possible to fast-forward.')\n        else:\n            repo.git.branch(branch.name, upstream.name, force=True)\n            print((GREEN + 'done'), end='.\\n')\n", "label": 0}
{"function": "\n\ndef unshorten(self, uri, type=None, timeout=10):\n    domain = urlsplit(uri).netloc\n    self._timeout = timeout\n    if (re.search(self._adfly_regex, domain, re.IGNORECASE) or (type == 'adfly')):\n        return self._unshorten_adfly(uri)\n    if (re.search(self._adfocus_regex, domain, re.IGNORECASE) or (type == 'adfocus')):\n        return self._unshorten_adfocus(uri)\n    if (re.search(self._linkbucks_regex, domain, re.IGNORECASE) or (type == 'linkbucks')):\n        if linkbucks_support:\n            return self._unshorten_linkbucks(uri)\n        else:\n            return (uri, 'linkbucks.com not supported. Install selenium package to add support.')\n    if (re.search(self._lnxlu_regex, domain, re.IGNORECASE) or (type == 'lnxlu')):\n        return self._unshorten_lnxlu(uri)\n    if re.search(self._shst_regex, domain, re.IGNORECASE):\n        return self._unshorten_shst(uri)\n    try:\n        if (domain == 't.co'):\n            r = requests.get(uri, timeout=self._timeout)\n            return (r.url, r.status_code)\n        if (domain == 'p.ost.im'):\n            r = requests.get(uri, headers=self._headers, timeout=self._timeout)\n            uri = re.findall('.*url\\\\=(.*?)\\\\\"\\\\.*', r.text)[0]\n            return (uri, 200)\n        r = requests.head(uri, headers=self._headers, timeout=self._timeout)\n        while True:\n            if ('location' in r.headers):\n                r = requests.head(r.headers['location'])\n                uri = r.url\n            else:\n                return (r.url, r.status_code)\n    except Exception as e:\n        return (uri, str(e))\n", "label": 1}
{"function": "\n\ndef leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None):\n    '\\n    Minimize the sum of squares of a set of equations.\\n\\n    ::\\n\\n        x = arg min(sum(func(y)**2,axis=0))\\n                 y\\n\\n    Parameters\\n    ----------\\n    func : callable\\n        should take at least one (possibly length N vector) argument and\\n        returns M floating point numbers. It must not return NaNs or\\n        fitting might fail.\\n    x0 : ndarray\\n        The starting estimate for the minimization.\\n    args : tuple, optional\\n        Any extra arguments to func are placed in this tuple.\\n    Dfun : callable, optional\\n        A function or method to compute the Jacobian of func with derivatives\\n        across the rows. If this is None, the Jacobian will be estimated.\\n    full_output : bool, optional\\n        non-zero to return all optional outputs.\\n    col_deriv : bool, optional\\n        non-zero to specify that the Jacobian function computes derivatives\\n        down the columns (faster, because there is no transpose operation).\\n    ftol : float, optional\\n        Relative error desired in the sum of squares.\\n    xtol : float, optional\\n        Relative error desired in the approximate solution.\\n    gtol : float, optional\\n        Orthogonality desired between the function vector and the columns of\\n        the Jacobian.\\n    maxfev : int, optional\\n        The maximum number of calls to the function. If `Dfun` is provided\\n        then the default `maxfev` is 100*(N+1) where N is the number of elements\\n        in x0, otherwise the default `maxfev` is 200*(N+1).\\n    epsfcn : float, optional\\n        A variable used in determining a suitable step length for the forward-\\n        difference approximation of the Jacobian (for Dfun=None).\\n        Normally the actual step length will be sqrt(epsfcn)*x\\n        If epsfcn is less than the machine precision, it is assumed that the\\n        relative errors are of the order of the machine precision.\\n    factor : float, optional\\n        A parameter determining the initial step bound\\n        (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\\n    diag : sequence, optional\\n        N positive entries that serve as a scale factors for the variables.\\n\\n    Returns\\n    -------\\n    x : ndarray\\n        The solution (or the result of the last iteration for an unsuccessful\\n        call).\\n    cov_x : ndarray\\n        Uses the fjac and ipvt optional outputs to construct an\\n        estimate of the jacobian around the solution. None if a\\n        singular matrix encountered (indicates very flat curvature in\\n        some direction).  This matrix must be multiplied by the\\n        residual variance to get the covariance of the\\n        parameter estimates -- see curve_fit.\\n    infodict : dict\\n        a dictionary of optional outputs with the key s:\\n\\n        ``nfev``\\n            The number of function calls\\n        ``fvec``\\n            The function evaluated at the output\\n        ``fjac``\\n            A permutation of the R matrix of a QR\\n            factorization of the final approximate\\n            Jacobian matrix, stored column wise.\\n            Together with ipvt, the covariance of the\\n            estimate can be approximated.\\n        ``ipvt``\\n            An integer array of length N which defines\\n            a permutation matrix, p, such that\\n            fjac*p = q*r, where r is upper triangular\\n            with diagonal elements of nonincreasing\\n            magnitude. Column j of p is column ipvt(j)\\n            of the identity matrix.\\n        ``qtf``\\n            The vector (transpose(q) * fvec).\\n\\n    mesg : str\\n        A string message giving information about the cause of failure.\\n    ier : int\\n        An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\\n        found.  Otherwise, the solution was not found. In either case, the\\n        optional output variable \\'mesg\\' gives more information.\\n\\n    Notes\\n    -----\\n    \"leastsq\" is a wrapper around MINPACK\\'s lmdif and lmder algorithms.\\n\\n    cov_x is a Jacobian approximation to the Hessian of the least squares\\n    objective function.\\n    This approximation assumes that the objective function is based on the\\n    difference between some observed target data (ydata) and a (non-linear)\\n    function of the parameters `f(xdata, params)` ::\\n\\n           func(params) = ydata - f(xdata, params)\\n\\n    so that the objective function is ::\\n\\n           min   sum((ydata - f(xdata, params))**2, axis=0)\\n         params\\n\\n    '\n    x0 = asarray(x0).flatten()\n    n = len(x0)\n    if (not isinstance(args, tuple)):\n        args = (args,)\n    (shape, dtype) = _check_func('leastsq', 'func', func, x0, args, n)\n    m = shape[0]\n    if (n > m):\n        raise TypeError(('Improper input: N=%s must not exceed M=%s' % (n, m)))\n    if (epsfcn is None):\n        epsfcn = finfo(dtype).eps\n    if (Dfun is None):\n        if (maxfev == 0):\n            maxfev = (200 * (n + 1))\n        retval = _minpack._lmdif(func, x0, args, full_output, ftol, xtol, gtol, maxfev, epsfcn, factor, diag)\n    else:\n        if col_deriv:\n            _check_func('leastsq', 'Dfun', Dfun, x0, args, n, (n, m))\n        else:\n            _check_func('leastsq', 'Dfun', Dfun, x0, args, n, (m, n))\n        if (maxfev == 0):\n            maxfev = (100 * (n + 1))\n        retval = _minpack._lmder(func, Dfun, x0, args, full_output, col_deriv, ftol, xtol, gtol, maxfev, factor, diag)\n    errors = {\n        0: ['Improper input parameters.', TypeError],\n        1: [('Both actual and predicted relative reductions in the sum of squares\\n  are at most %f' % ftol), None],\n        2: [('The relative error between two consecutive iterates is at most %f' % xtol), None],\n        3: [('Both actual and predicted relative reductions in the sum of squares\\n  are at most %f and the relative error between two consecutive iterates is at \\n  most %f' % (ftol, xtol)), None],\n        4: [('The cosine of the angle between func(x) and any column of the\\n  Jacobian is at most %f in absolute value' % gtol), None],\n        5: [('Number of calls to function has reached maxfev = %d.' % maxfev), ValueError],\n        6: [('ftol=%f is too small, no further reduction in the sum of squares\\n  is possible.' % ftol), ValueError],\n        7: [('xtol=%f is too small, no further improvement in the approximate\\n  solution is possible.' % xtol), ValueError],\n        8: [('gtol=%f is too small, func(x) is orthogonal to the columns of\\n  the Jacobian to machine precision.' % gtol), ValueError],\n        'unknown': ['Unknown error.', TypeError],\n    }\n    info = retval[(- 1)]\n    if ((info not in [1, 2, 3, 4]) and (not full_output)):\n        if (info in [5, 6, 7, 8]):\n            warnings.warn(errors[info][0], RuntimeWarning)\n        else:\n            try:\n                raise errors[info][1](errors[info][0])\n            except KeyError:\n                raise errors['unknown'][1](errors['unknown'][0])\n    mesg = errors[info][0]\n    if full_output:\n        cov_x = None\n        if (info in [1, 2, 3, 4]):\n            from numpy.dual import inv\n            from numpy.linalg import LinAlgError\n            perm = take(eye(n), (retval[1]['ipvt'] - 1), 0)\n            r = triu(transpose(retval[1]['fjac'])[:n, :])\n            R = dot(r, perm)\n            try:\n                cov_x = inv(dot(transpose(R), R))\n            except (LinAlgError, ValueError):\n                pass\n        return (((retval[0], cov_x) + retval[1:(- 1)]) + (mesg, info))\n    else:\n        return (retval[0], info)\n", "label": 1}
{"function": "\n\n@requires_IEEE_754\ndef test_specific_values(self):\n\n    def rect_complex(z):\n        'Wrapped version of rect that accepts a complex number instead of\\n            two float arguments.'\n        return cmath.rect(z.real, z.imag)\n\n    def polar_complex(z):\n        'Wrapped version of polar that returns a complex number instead of\\n            two floats.'\n        return complex(*polar(z))\n    for (id, fn, ar, ai, er, ei, flags) in parse_testfile(test_file):\n        arg = complex(ar, ai)\n        expected = complex(er, ei)\n        if (fn == 'rect'):\n            function = rect_complex\n        elif (fn == 'polar'):\n            function = polar_complex\n        else:\n            function = getattr(cmath, fn)\n        if (('divide-by-zero' in flags) or ('invalid' in flags)):\n            try:\n                actual = function(arg)\n            except ValueError:\n                continue\n            else:\n                self.fail('ValueError not raised in test {}: {}(complex({!r}, {!r}))'.format(id, fn, ar, ai))\n        if ('overflow' in flags):\n            try:\n                actual = function(arg)\n            except OverflowError:\n                continue\n            else:\n                self.fail('OverflowError not raised in test {}: {}(complex({!r}, {!r}))'.format(id, fn, ar, ai))\n        actual = function(arg)\n        if ('ignore-real-sign' in flags):\n            actual = complex(abs(actual.real), actual.imag)\n            expected = complex(abs(expected.real), expected.imag)\n        if ('ignore-imag-sign' in flags):\n            actual = complex(actual.real, abs(actual.imag))\n            expected = complex(expected.real, abs(expected.imag))\n        if (fn in ('log', 'log10')):\n            real_abs_err = 2e-15\n        else:\n            real_abs_err = 5e-323\n        error_message = '{}: {}(complex({!r}, {!r}))\\nExpected: complex({!r}, {!r})\\nReceived: complex({!r}, {!r})\\nReceived value insufficiently close to expected value.'.format(id, fn, ar, ai, expected.real, expected.imag, actual.real, actual.imag)\n        self.rAssertAlmostEqual(expected.real, actual.real, abs_err=real_abs_err, msg=error_message)\n        self.rAssertAlmostEqual(expected.imag, actual.imag, msg=error_message)\n", "label": 1}
{"function": "\n\ndef finish_args(environ, args):\n    'Do some final transformations\\n\\n\\tTransforms ``*_override`` arguments into dictionaries, adding overrides from \\n\\tenvironment variables. Transforms ``renderer_arg`` argument into dictionary \\n\\tas well, but only if it is true.\\n\\n\\t:param dict environ:\\n\\t\\tEnvironment from which additional overrides should be taken from.\\n\\t:param args:\\n\\t\\tArguments object returned by \\n\\t\\t:py:meth:`argparse.ArgumentParser.parse_args`. Will be modified \\n\\t\\tin-place.\\n\\n\\t:return: Object received as second (``args``) argument.\\n\\t'\n    args.config_override = mergeargs(chain(parse_override_var(environ.get('POWERLINE_CONFIG_OVERRIDES', '')), (parsedotval(v) for v in (args.config_override or ()))))\n    args.theme_override = mergeargs(chain(parse_override_var(environ.get('POWERLINE_THEME_OVERRIDES', '')), (parsedotval(v) for v in (args.theme_override or ()))))\n    if args.renderer_arg:\n        args.renderer_arg = mergeargs((parsedotval(v) for v in args.renderer_arg), remove=True)\n        if ('pane_id' in args.renderer_arg):\n            if isinstance(args.renderer_arg['pane_id'], (bytes, unicode)):\n                try:\n                    args.renderer_arg['pane_id'] = int(args.renderer_arg['pane_id'].lstrip(' %'))\n                except ValueError:\n                    pass\n            if ('client_id' not in args.renderer_arg):\n                args.renderer_arg['client_id'] = args.renderer_arg['pane_id']\n    args.config_path = ([path for path in environ.get('POWERLINE_CONFIG_PATHS', '').split(':') if path] + (args.config_path or []))\n    args.side = args.side[0]\n    return args\n", "label": 1}
{"function": "\n\ndef run(self):\n    res = None\n    os.system('clear')\n    while True:\n        try:\n            if (not self.server_version):\n                self.server_version = self.client.perform_request('version')\n                if (self.server_version.capabilities and ('async' in self.server_version.capabilities)):\n                    self.block = False\n                elif self.supports_blocking:\n                    self.block = True\n                else:\n                    raise BlockingNotSupportedError('Debugger requires blocking mode')\n            self.render()\n            if (not self.block):\n                done = False\n                while (not done):\n                    res = self.client.perform_request('version', block=True)\n                    if res.is_success:\n                        done = True\n        except ConnectionError as e:\n            try:\n                msg = e.message.args[1].strerror\n            except:\n                try:\n                    msg = e.message.args[0]\n                except:\n                    msg = str(e)\n            traceback.print_exc()\n            self.do_render(error='Error: {}'.format(msg))\n            self.server_version = None\n            time.sleep(1)\n", "label": 1}
{"function": "\n\ndef _get_path(path, key, name):\n    'Helper to get a dataset path'\n    if (path is None):\n        def_path = op.realpath(op.join(op.dirname(__file__), '..', '..', 'examples'))\n        if (get_config(key) is None):\n            key = 'MNE_DATA'\n        path = get_config(key, def_path)\n        if ((not op.exists(path)) or (not os.access(path, os.W_OK))):\n            try:\n                os.mkdir(path)\n            except OSError:\n                try:\n                    logger.info(('Checking for %s data in \"~/mne_data\"...' % name))\n                    path = op.join(op.expanduser('~'), 'mne_data')\n                    if (not op.exists(path)):\n                        logger.info(\"Trying to create '~/mne_data' in home directory\")\n                        os.mkdir(path)\n                except OSError:\n                    raise OSError((\"User does not have write permissions at '%s', try giving the path as an argument to data_path() where user has write permissions, for ex:data_path('/home/xyz/me2/')\" % path))\n    if (not isinstance(path, string_types)):\n        raise ValueError('path must be a string or None')\n    return path\n", "label": 0}
{"function": "\n\ndef create_client(self, userid='', **kwargs):\n    '\\n        Do an instantiation of a client instance\\n\\n        :param userid: An identifier of the user\\n        :param: Keyword arguments\\n            Keys are [\"srv_discovery_url\", \"client_info\", \"client_registration\",\\n            \"provider_info\"]\\n        :return: client instance\\n        '\n    _key_set = set(list(kwargs.keys()))\n    try:\n        _verify_ssl = kwargs['verify_ssl']\n    except KeyError:\n        _verify_ssl = self.verify_ssl\n    else:\n        _key_set.discard('verify_ssl')\n    client = self.client_cls(client_authn_method=CLIENT_AUTHN_METHOD, behaviour=kwargs['behaviour'], verify_ssl=_verify_ssl)\n    try:\n        client.userinfo_request_method = kwargs['userinfo_request_method']\n    except KeyError:\n        pass\n    else:\n        _key_set.discard('userinfo_request_method')\n    _key_set.discard('behaviour')\n    for param in ['allow']:\n        try:\n            setattr(client, param, kwargs[param])\n        except KeyError:\n            pass\n        else:\n            _key_set.discard(param)\n    if (_key_set == {'client_info'}):\n        if (not userid):\n            raise MissingAttribute('Missing userid specification')\n        issuer = client.wf.discovery_query(userid)\n        pcr = client.provider_config(issuer)\n        client.register(client.provider_info['registration_endpoint'], **kwargs['client_info'])\n        self.get_path(kwargs['client_info']['redirect_uris'], issuer)\n    elif (_key_set == set(['client_info', 'srv_discovery_url'])):\n        _ = client.provider_config(kwargs['srv_discovery_url'])\n        _ = client.register(client.provider_info['registration_endpoint'], **kwargs['client_info'])\n        self.get_path(kwargs['client_info']['redirect_uris'], kwargs['srv_discovery_url'])\n    elif (_key_set == set(['provider_info', 'client_info'])):\n        client.handle_provider_config(ProviderConfigurationResponse(**kwargs['provider_info']), kwargs['provider_info']['issuer'])\n        _ = client.register(client.provider_info['registration_endpoint'], **kwargs['client_info'])\n        self.get_path(kwargs['client_info']['redirect_uris'], kwargs['provider_info']['issuer'])\n    elif (_key_set == set(['provider_info', 'client_registration'])):\n        client.handle_provider_config(ProviderConfigurationResponse(**kwargs['provider_info']), kwargs['provider_info']['issuer'])\n        client.store_registration_info(RegistrationResponse(**kwargs['client_registration']))\n        self.get_path(kwargs['client_info']['redirect_uris'], kwargs['provider_info']['issuer'])\n    elif (_key_set == set(['srv_discovery_url', 'client_registration'])):\n        _ = client.provider_config(kwargs['srv_discovery_url'])\n        client.store_registration_info(RegistrationResponse(**kwargs['client_registration']))\n        self.get_path(kwargs['client_registration']['redirect_uris'], kwargs['srv_discovery_url'])\n    else:\n        raise Exception('Configuration error ?')\n    return client\n", "label": 1}
{"function": "\n\ndef changes(self, timeout_ms=0):\n    'Return paths for changed files and directories.\\n\\n    start() must be called before this method.\\n\\n    Args:\\n      timeout_ms: a timeout in milliseconds on which this watcher will block\\n                  waiting for a change. It allows for external polling threads\\n                  to react immediately on a change instead of waiting for\\n                  a random polling delay.\\n\\n    Returns:\\n      A set of strings representing file and directory paths that have changed\\n      since the last call to get_changed_paths.\\n    '\n    paths = set()\n    while True:\n        with self._inotify_fd_lock:\n            if (self._inotify_fd < 0):\n                return set()\n            if (not self._inotify_poll.poll((_AGGREGATE_CHANGES_MS_APART if paths else timeout_ms))):\n                break\n            self._inotify_events += os.read(self._inotify_fd, 1024)\n            while (len(self._inotify_events) > _INOTIFY_EVENT_SIZE):\n                (wd, mask, cookie, length) = _INOTIFY_EVENT.unpack(self._inotify_events[:_INOTIFY_EVENT_SIZE])\n                if (len(self._inotify_events) < (_INOTIFY_EVENT_SIZE + length)):\n                    break\n                name = self._inotify_events[_INOTIFY_EVENT_SIZE:(_INOTIFY_EVENT_SIZE + length)]\n                name = name.rstrip('\\x00')\n                logging.debug('wd=%s, mask=%s, cookie=%s, length=%s, name=%r', wd, _bit_str(mask, _ATTRIBUTE_MASK_NAMES), cookie, length, name)\n                self._inotify_events = self._inotify_events[(_INOTIFY_EVENT_SIZE + length):]\n                if (mask & IN_IGNORED):\n                    continue\n                try:\n                    directory = self._watch_to_directory[wd]\n                except KeyError:\n                    logging.debug('Watch deleted for watch descriptor=%d', wd)\n                    continue\n                path = os.path.join(directory, name)\n                if (os.path.isdir(path) or (path in self._directory_to_watch_descriptor)):\n                    if (mask & IN_DELETE):\n                        self._remove_watch_for_path(path)\n                    elif (mask & IN_MOVED_FROM):\n                        self._remove_watch_for_path(path)\n                    elif (mask & IN_CREATE):\n                        self._add_watch_for_path(path)\n                    elif (mask & IN_MOVED_TO):\n                        self._add_watch_for_path(path)\n                if ((path not in paths) and (not watcher_common.ignore_file(path))):\n                    paths.add(path)\n    return paths\n", "label": 1}
{"function": "\n\ndef main():\n    global remote_host, remote_port, remote_ssl\n    msg.LOG_LEVEL = msg.LOG_LEVELS['ERROR']\n    usage = 'Figure it out :P'\n    parser = optparse.OptionParser(usage=usage)\n    parser.add_option('--url', dest='url', default=None)\n    parser.add_option('--data', dest='data', default=None)\n    parser.add_option('--method', dest='method', default=None)\n    parser.add_option('--host', dest='host', default=None)\n    parser.add_option('--port', dest='port', default=None)\n    parser.add_option('--ssl', dest='ssl', default=None)\n    (options, args) = parser.parse_args()\n    if options.url:\n        data = None\n        err = False\n        if options.data:\n            data = json.loads(options.data)\n        try:\n            r = api.hit_url(options.host, options.url, data, options.method)\n        except HTTPError as e:\n            r = e\n        except URLError as e:\n            r = e\n            err = True\n        try:\n            print(r.code)\n        except Exception:\n            err = True\n        if err:\n            print(r.reason)\n        else:\n            print(r.read().decode('utf-8'))\n        sys.exit(err)\n    if (not options.host):\n        sys.exit(1)\n    remote_host = options.host\n    remote_port = (int(options.port) or remote_port)\n    remote_ssl = (bool(options.ssl) or remote_ssl)\n    proxy = Server()\n    (_, port) = reactor.reactor.listen(proxy, port=int(G.PROXY_PORT))\n\n    def on_ready():\n        print(('Now listening on <%s>' % port))\n        sys.stdout.flush()\n    utils.set_timeout(on_ready, 100)\n    try:\n        reactor.reactor.block()\n    except KeyboardInterrupt:\n        print('ciao')\n", "label": 1}
{"function": "\n\n@permission_required('core.manage_shop')\ndef manage_related_products_inline(request, product_id, as_string=False, template_name='manage/product/related_products_inline.html'):\n    'View which shows all related products for the product with the passed id.\\n    '\n    product = Product.objects.get(pk=product_id)\n    related_products = product.related_products.all()\n    related_products_ids = [p.id for p in related_products]\n    r = (request.POST if (request.method == 'POST') else request.GET)\n    s = request.session\n    if (r.get('keep-filters') or r.get('page')):\n        page = r.get('page', s.get('related_products', 1))\n        filter_ = r.get('filter', s.get('filter'))\n        category_filter = r.get('related_products_category_filter', s.get('related_products_category_filter'))\n    else:\n        page = r.get('page', 1)\n        filter_ = r.get('filter')\n        category_filter = r.get('related_products_category_filter')\n    s['related_products_page'] = page\n    s['filter'] = filter_\n    s['related_products_category_filter'] = category_filter\n    try:\n        s['related-products-amount'] = int(r.get('related-products-amount', s.get('related-products-amount')))\n    except TypeError:\n        s['related-products-amount'] = 25\n    filters = Q()\n    if filter_:\n        filters &= (Q(name__icontains=filter_) | Q(sku__icontains=filter_))\n        filters |= ((Q(sub_type=VARIANT) & Q(active_sku=False)) & Q(parent__sku__icontains=filter_))\n        filters |= ((Q(sub_type=VARIANT) & Q(active_name=False)) & Q(parent__name__icontains=filter_))\n    if category_filter:\n        if (category_filter == 'None'):\n            filters &= Q(categories=None)\n        elif (category_filter == 'All'):\n            pass\n        else:\n            category = lfs_get_object_or_404(Category, pk=category_filter)\n            categories = [category]\n            categories.extend(category.get_all_children())\n            filters &= Q(categories__in=categories)\n    products = Product.objects.filter(filters).exclude(pk__in=related_products_ids).exclude(pk=product.pk)\n    paginator = Paginator(products, s['related-products-amount'])\n    total = products.count()\n    try:\n        page = paginator.page(page)\n    except EmptyPage:\n        page = 0\n    result = render_to_string(template_name, RequestContext(request, {\n        'product': product,\n        'related_products': related_products,\n        'total': total,\n        'page': page,\n        'paginator': paginator,\n        'filter': filter_,\n    }))\n    if as_string:\n        return result\n    else:\n        return HttpResponse(json.dumps({\n            'html': [['#related-products-inline', result]],\n        }), content_type='application/json')\n", "label": 1}
{"function": "\n\ndef normal(self, other):\n    'Return ``self`` and ``other`` with ``gcd`` removed from each.\\n        The only differences between this and method ``div`` is that this\\n        is 1) optimized for the case when there are few factors in common and\\n        2) this does not raise an error if ``other`` is zero.\\n\\n        See Also\\n        ========\\n        div\\n\\n        '\n    if (not isinstance(other, Factors)):\n        other = Factors(other)\n        if other.is_zero:\n            return (Factors(), Factors(S.Zero))\n        if self.is_zero:\n            return (Factors(S.Zero), Factors())\n    self_factors = dict(self.factors)\n    other_factors = dict(other.factors)\n    for (factor, self_exp) in self.factors.items():\n        try:\n            other_exp = other.factors[factor]\n        except KeyError:\n            continue\n        exp = (self_exp - other_exp)\n        if (not exp):\n            del self_factors[factor]\n            del other_factors[factor]\n        elif _isnumber(exp):\n            if (exp > 0):\n                self_factors[factor] = exp\n                del other_factors[factor]\n            else:\n                del self_factors[factor]\n                other_factors[factor] = (- exp)\n        else:\n            r = self_exp.extract_additively(other_exp)\n            if (r is not None):\n                if r:\n                    self_factors[factor] = r\n                    del other_factors[factor]\n                else:\n                    del self_factors[factor]\n                    del other_factors[factor]\n            else:\n                (sc, sa) = self_exp.as_coeff_Add()\n                if sc:\n                    (oc, oa) = other_exp.as_coeff_Add()\n                    diff = (sc - oc)\n                    if (diff > 0):\n                        self_factors[factor] -= oc\n                        other_exp = oa\n                    elif (diff < 0):\n                        self_factors[factor] -= sc\n                        other_factors[factor] -= sc\n                        other_exp = (oa - diff)\n                    else:\n                        self_factors[factor] = sa\n                        other_exp = oa\n                if other_exp:\n                    other_factors[factor] = other_exp\n                else:\n                    del other_factors[factor]\n    return (Factors(self_factors), Factors(other_factors))\n", "label": 1}
{"function": "\n\ndef test_get_browsers():\n    browsers = utils.get_browsers()\n    browser_list = []\n    for browser in browsers:\n        if (browser[0] == 'Internet Explorer'):\n            assert (int(float(browser[1])) < 20)\n        browser_list.append(browser[0])\n    assert (len(browser_list) == 5)\n    assert ('Firefox' in browser_list)\n    assert ('Opera' in browser_list)\n    assert ('Chrome' in browser_list)\n    assert ('Internet Explorer' in browser_list)\n    assert ('Safari' in browser_list)\n    global browser_list\n", "label": 0}
{"function": "\n\ndef setup(self, model, dataset):\n    '\\n        Allows the training algorithm to do some preliminary configuration\\n        *before* we actually start training the model. The dataset is provided\\n        in case other derived training algorithms need to modify model based on\\n        the dataset.\\n\\n        Parameters\\n        ----------\\n        model : object\\n            A Python object representing the model to train. Loosely\\n            implementing the interface of models.model.Model.\\n        dataset : pylearn2.datasets.dataset.Dataset\\n            Dataset object used to draw training data\\n        '\n    self.model = model\n    if (self.cost is None):\n        self.cost = model.get_default_cost()\n    try:\n        if self.cost.is_stochastic():\n            raise TypeError('BGD is not compatible with stochastic costs.')\n    except NotImplementedError:\n        warnings.warn('BGD is not compatible with stochastic costs and cannot determine whether the current cost is stochastic.')\n    if (self.batch_size is None):\n        self.batch_size = model.force_batch_size\n    else:\n        batch_size = self.batch_size\n        if self.set_batch_size:\n            model.set_batch_size(batch_size)\n        elif hasattr(model, 'force_batch_size'):\n            if (not ((model.force_batch_size is None) or (model.force_batch_size <= 0) or (batch_size == model.force_batch_size))):\n                raise ValueError(('batch_size is %d but ' + ('model.force_batch_size is %d' % (batch_size, model.force_batch_size))))\n    self.monitor = Monitor.get_monitor(model)\n    self.monitor.set_theano_function_mode(self.theano_function_mode)\n    data_specs = self.cost.get_data_specs(model)\n    mapping = DataSpecsMapping(data_specs)\n    space_tuple = mapping.flatten(data_specs[0], return_tuple=True)\n    source_tuple = mapping.flatten(data_specs[1], return_tuple=True)\n    theano_args = []\n    for (space, source) in safe_zip(space_tuple, source_tuple):\n        name = ('BGD_[%s]' % source)\n        arg = space.make_theano_batch(name=name)\n        theano_args.append(arg)\n    theano_args = tuple(theano_args)\n    nested_args = mapping.nest(theano_args)\n    fixed_var_descr = self.cost.get_fixed_var_descr(model, nested_args)\n    self.on_load_batch = fixed_var_descr.on_load_batch\n    cost_value = self.cost.expr(model, nested_args, **fixed_var_descr.fixed_vars)\n    (grads, grad_updates) = self.cost.get_gradients(model, nested_args, **fixed_var_descr.fixed_vars)\n    assert isinstance(grads, OrderedDict)\n    assert isinstance(grad_updates, OrderedDict)\n    if (cost_value is None):\n        raise ValueError(((('BGD is incompatible with ' + str(self.cost)) + ' because it is intractable, but BGD uses the ') + 'cost function value to do line searches.'))\n\n    def capture(f, mapping=mapping):\n        new_f = (lambda *args: f(mapping.flatten(args, return_tuple=True)))\n        return new_f\n    obj_prereqs = [capture(f) for f in fixed_var_descr.on_load_batch]\n    if (self.monitoring_dataset is not None):\n        if ((self.monitoring_batch_size is None) and (self.monitoring_batches is None)):\n            self.monitoring_batch_size = self.batch_size\n            self.monitoring_batches = self.batches_per_iter\n        self.monitor.setup(dataset=self.monitoring_dataset, cost=self.cost, batch_size=self.monitoring_batch_size, num_batches=self.monitoring_batches, obj_prereqs=obj_prereqs, cost_monitoring_args=fixed_var_descr.fixed_vars)\n    params = model.get_params()\n    self.optimizer = BatchGradientDescent(objective=cost_value, gradients=grads, gradient_updates=grad_updates, params=params, param_constrainers=[model.modify_updates], lr_scalers=model.get_lr_scalers(), inputs=theano_args, verbose=self.verbose_optimization, max_iter=self.updates_per_batch, reset_alpha=self.reset_alpha, conjugate=self.conjugate, reset_conjugate=self.reset_conjugate, min_init_alpha=self.min_init_alpha, line_search_mode=self.line_search_mode, theano_function_mode=self.theano_function_mode, init_alpha=self.init_alpha)\n    if (self.monitoring_dataset is not None):\n        self.monitor.add_channel(name='ave_step_size', ipt=None, val=self.optimizer.ave_step_size, data_specs=(NullSpace(), ''), dataset=first_value(self.monitoring_dataset))\n        self.monitor.add_channel(name='ave_grad_size', ipt=None, val=self.optimizer.ave_grad_size, data_specs=(NullSpace(), ''), dataset=first_value(self.monitoring_dataset))\n        self.monitor.add_channel(name='ave_grad_mult', ipt=None, val=self.optimizer.ave_grad_mult, data_specs=(NullSpace(), ''), dataset=first_value(self.monitoring_dataset))\n    self.first = True\n    self.bSetup = True\n", "label": 1}
{"function": "\n\ndef test_has_multiple():\n    f = (((x ** 2) * y) + sin(((2 ** t) + log(z))))\n    assert f.has(x)\n    assert f.has(y)\n    assert f.has(z)\n    assert f.has(t)\n    assert (not f.has(u))\n    assert f.has(x, y, z, t)\n    assert f.has(x, y, z, t, u)\n    i = Integer(4400)\n    assert (not i.has(x))\n    assert (i * (x ** i)).has(x)\n    assert (not (i * (y ** i)).has(x))\n    assert (i * (y ** i)).has(x, y)\n    assert (not (i * (y ** i)).has(x, z))\n", "label": 1}
{"function": "\n\ndef __boot():\n    import sys\n    import os\n    PYTHONPATH = os.environ.get('PYTHONPATH')\n    if ((PYTHONPATH is None) or ((sys.platform == 'win32') and (not PYTHONPATH))):\n        PYTHONPATH = []\n    else:\n        PYTHONPATH = PYTHONPATH.split(os.pathsep)\n    pic = getattr(sys, 'path_importer_cache', {\n        \n    })\n    stdpath = sys.path[len(PYTHONPATH):]\n    mydir = os.path.dirname(__file__)\n    for item in stdpath:\n        if ((item == mydir) or (not item)):\n            continue\n        importer = pic.get(item)\n        if (importer is not None):\n            loader = importer.find_module('site')\n            if (loader is not None):\n                loader.load_module('site')\n                break\n        else:\n            try:\n                import imp\n                (stream, path, descr) = imp.find_module('site', [item])\n            except ImportError:\n                continue\n            if (stream is None):\n                continue\n            try:\n                imp.load_module('site', stream, path, descr)\n            finally:\n                stream.close()\n            break\n    else:\n        raise ImportError(\"Couldn't find the real 'site' module\")\n    known_paths = dict([(makepath(item)[1], 1) for item in sys.path])\n    oldpos = getattr(sys, '__egginsert', 0)\n    sys.__egginsert = 0\n    for item in PYTHONPATH:\n        addsitedir(item)\n    sys.__egginsert += oldpos\n    (d, nd) = makepath(stdpath[0])\n    insert_at = None\n    new_path = []\n    for item in sys.path:\n        (p, np) = makepath(item)\n        if ((np == nd) and (insert_at is None)):\n            insert_at = len(new_path)\n        if ((np in known_paths) or (insert_at is None)):\n            new_path.append(item)\n        else:\n            new_path.insert(insert_at, item)\n            insert_at += 1\n    sys.path[:] = new_path\n", "label": 1}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01, gamma=10, cof=4000):\n    samplerate = self.audio.samplerate\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for y in super(Y3, self).walk(N, freq_base, freq_max, start, end, each=False, combine=False, gamma=gamma, cof=cof):\n        y[..., 1:] = np.diff(y, 1, 1)\n        if (not combine):\n            if each:\n                for v in y:\n                    (yield v)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\n@pytest.mark.slow\n@pytest.inlineCallbacks\ndef test_late_start(self):\n    self.q = Queue.Queue()\n\n    def cb(key, value):\n        self.q.put({\n            'key': key,\n            'value': value,\n        })\n\n    def started_cb(started):\n        self.q.put(started)\n    self.storage = storage.Storage()\n    self.storage.set('test', '1', 1, None)\n    self.storage.set('test', '2', 2, None)\n    self.storage.set('test', '3', 3, None)\n    assert ('test1' in self.storage.localstore)\n    assert ('test2' in self.storage.localstore)\n    assert ('test3' in self.storage.localstore)\n    (yield threads.defer_to_thread(self.storage.start, CalvinCB(started_cb)))\n    (yield threads.defer_to_thread(time.sleep, 2))\n    value = self.q.get(timeout=0.2)\n    assert value\n    assert ('test1' not in self.storage.localstore)\n    assert ('test2' not in self.storage.localstore)\n    assert ('test3' not in self.storage.localstore)\n    (yield threads.defer_to_thread(self.storage.get, 'test', '3', CalvinCB(func=cb)))\n    value = self.q.get(timeout=0.2)\n    assert (value['value'] == 3)\n    (yield threads.defer_to_thread(self.storage.stop))\n", "label": 0}
{"function": "\n\ndef generate_service(directory, name=None, url=None, plural=None):\n    if (not name):\n        name = raw_input('Service Name: ')\n    if (not url):\n        url = raw_input('Endpoint URL: ')\n    if (not plural):\n        plural = raw_input('Plural Form: ')\n    endpoints = []\n    service = os.path.join('app', 'services', (name.lower() + 'Service.js'))\n    with open(os.path.join(directory, 'assets', service), 'w') as f:\n        f.write(('app.service(\\'%sService\\', [\"$http\", \"$q\", function ($http, $q) {\\n    \\'use strict\\';\\n    return {\\n' % name.title()))\n        for method in _endpoints:\n            title = name.title()\n            if method['data']:\n                dataline = ',\\n                data: data'\n            else:\n                dataline = ''\n            if method['ext']:\n                extensions = method['ext']\n            else:\n                extensions = ''\n            if method['args']:\n                arguments = method['args']\n            else:\n                arguments = ''\n            if method['plural']:\n                title = plural.title()\n            serv = _template.substitute(name=method['name'], title=title, args=arguments, method=method['method'], url=url, ext=extensions, dataline=dataline)\n            endpoints.append(serv)\n        f.write(',\\n'.join(endpoints))\n        f.write('\\n\\n    };\\n}]);')\n", "label": 0}
{"function": "\n\ndef __init__(self, user, game, transaction_items):\n    self.user = user\n    self.game = game\n    self.id = create_id()\n    self.items = transaction_items\n    total = 0\n    game_store_items = StoreList.get(game)\n    resource_keys = set()\n    if (len(transaction_items) == 0):\n        raise StoreError('Basket is empty')\n    for (item_key, item) in transaction_items.items():\n        try:\n            basket_amount = int(item['amount'])\n            basket_price = int(item['price'])\n        except (ValueError, KeyError, TypeError):\n            raise StoreError(('Item \"%s\" amount and price must be integers' % item_key))\n        if (basket_amount == 0):\n            continue\n        elif (basket_amount < 0):\n            raise StoreError(('Item \"%s\" amount must be non-negative' % item_key))\n        game_offering = game_store_items.get_offering(item_key)\n        minor_price = game_offering.get_price().get_minor_amount()\n        if (basket_price != minor_price):\n            raise StoreError(('Item \"%s\" price does not match' % item_key))\n        offering_output = game_offering.output\n        try:\n            basket_item_output = item['output']\n            if (not isinstance(basket_item_output, dict)):\n                raise ValueError()\n            if (len(offering_output) != len(basket_item_output)):\n                raise ValueError()\n            for (resource_key, amount) in offering_output.items():\n                if (amount != basket_item_output[resource_key]):\n                    raise ValueError()\n                resource_keys.add(resource_key)\n        except (KeyError, ValueError):\n            raise StoreError(('Offering \"%s\" output resources do not match' % item_key))\n        self.items[item_key] = {\n            'price': basket_price,\n            'amount': basket_amount,\n        }\n        total += (minor_price * basket_amount)\n    for key in resource_keys:\n        resource = game_store_items.get_resource(key)\n        if ((resource is not None) and (resource.type == 'own')):\n            user_store_items = game_store_items.get_store_user(user)\n            if (user_store_items is not None):\n                try:\n                    user_item = user_store_items.get_item(key)\n                    if (user_item['amount'] > 0):\n                        raise StoreError('Basket contains an ownable item you already own')\n                except KeyError:\n                    pass\n    self.total = total\n    self.completed = False\n    self.completed_time = None\n    UserTransactionsList.get(user).add_transaction(self.id, self)\n", "label": 1}
{"function": "\n\ndef cmp_observation(x, y):\n    \"\\n    for a given COBservation, do the following.\\n    1: If it's an addendum/reconciliation trumps all\\n    2: If it's direct, and other is not direct, the direct wins\\n    3: If both direct, do by earliest date\\n    4: If neither direct, do by earliest encounter_date regardless of method.\\n\\n    < -1\\n    = 0\\n    > 1\\n\\n    Assumes that x and y have the same observation date (cell in the DOT json array)\\n    Encounter date is the date in which the date cell is observed.\\n    \"\n    assert (x.observed_date.date() == y.observed_date.date())\n    x_is_reconciliation = getattr(x, 'is_reconciliation', None)\n    y_is_reconciliation = getattr(y, 'is_reconciliation', None)\n    if (x_is_reconciliation and y_is_reconciliation):\n        return cmp(y.submitted_date, x.submitted_date)\n    elif (x_is_reconciliation and (not y_is_reconciliation)):\n        return 1\n    elif ((not x_is_reconciliation) and y_is_reconciliation):\n        return (- 1)\n    elif ((x.method == DOT_OBSERVATION_DIRECT) and (y.method == DOT_OBSERVATION_DIRECT)):\n        return cmp(y.encounter_date, x.encounter_date)\n    elif ((x.method == DOT_OBSERVATION_DIRECT) and (y.method != DOT_OBSERVATION_DIRECT)):\n        return 1\n    elif ((x.method != DOT_OBSERVATION_DIRECT) and (y.method == DOT_OBSERVATION_DIRECT)):\n        return (- 1)\n    elif (((x.adherence, x.method) == DOT_UNCHECKED_CELL) and ((y.adherence, y.method) != DOT_UNCHECKED_CELL)):\n        return (- 1)\n    elif (((x.adherence, x.method) != DOT_UNCHECKED_CELL) and ((y.adherence, y.method) == DOT_UNCHECKED_CELL)):\n        return 1\n    else:\n        return cmp(y.encounter_date, x.encounter_date)\n", "label": 1}
{"function": "\n\ndef _from_hdf_view(h5, data, xfmname=None, vmin=None, vmax=None, **kwargs):\n    try:\n        basestring\n        strcls = (unicode, str)\n    except NameError:\n        strcls = str\n    if isinstance(data, strcls):\n        return _from_hdf_data(h5, data, xfmname=xfmname, vmin=vmin, vmax=vmax, **kwargs)\n    if (len(data) == 2):\n        dim1 = _from_hdf_data(h5, data[0], xfmname=xfmname[0])\n        dim2 = _from_hdf_data(h5, data[1], xfmname=xfmname[1])\n        cls = (Vertex2D if isinstance(dim1, Vertex) else Volume2D)\n        return cls(dim1, dim2, vmin=vmin[0], vmin2=vmin[1], vmax=vmax[0], vmax2=vmax[1], **kwargs)\n    elif (len(data) == 4):\n        (red, green, blue) = [_from_hdf_data(h5, d, xfmname=xfmname) for d in data[:3]]\n        alpha = None\n        if (data[3] is not None):\n            alpha = _from_hdf_data(h5, data[3], xfmname=xfmname)\n        cls = (VertexRGB if isinstance(red, Vertex) else VolumeRGB)\n        return cls(red, green, blue, alpha=alpha, **kwargs)\n    else:\n        raise ValueError('Invalid Dataview specification')\n", "label": 0}
{"function": "\n\ndef run(self, cmd, code):\n    'Run the module checker or executable on code and return the output.'\n    if (self.module is not None):\n        use_module = False\n        if (not self.check_version):\n            use_module = True\n        else:\n            settings = self.get_view_settings()\n            version = settings.get('@python')\n            if (version is None):\n                use_module = ((cmd is None) or (cmd[0] == '<builtin>'))\n            else:\n                version = util.find_python(version=version, module=self.module)\n                use_module = (version[0] == '<builtin>')\n        if use_module:\n            if persist.debug_mode():\n                persist.printf('{}: {} <builtin>'.format(self.name, os.path.basename((self.filename or '<unsaved>'))))\n            try:\n                errors = self.check(code, os.path.basename((self.filename or '<unsaved>')))\n            except Exception as err:\n                persist.printf('ERROR: exception in {}.check: {}'.format(self.name, str(err)))\n                errors = ''\n            if isinstance(errors, (tuple, list)):\n                return '\\n'.join([str(e) for e in errors])\n            else:\n                return errors\n        else:\n            cmd = self._cmd\n    else:\n        cmd = (self.cmd or self._cmd)\n    cmd = self.build_cmd(cmd=cmd)\n    if cmd:\n        return super().run(cmd, code)\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    '\\n        If we should show the toolbar for this request, put it on\\n        request.toolbar. Then call the request_hook on the toolbar.\\n        '\n    if (not self.is_cms_request(request)):\n        return\n    edit_on = get_cms_setting('CMS_TOOLBAR_URL__EDIT_ON')\n    edit_off = get_cms_setting('CMS_TOOLBAR_URL__EDIT_OFF')\n    build = get_cms_setting('CMS_TOOLBAR_URL__BUILD')\n    disable = get_cms_setting('CMS_TOOLBAR_URL__DISABLE')\n    anonymous_on = get_cms_setting('TOOLBAR_ANONYMOUS_ON')\n    if (disable in request.GET):\n        request.session['cms_toolbar_disabled'] = True\n    if (edit_on in request.GET):\n        request.session['cms_toolbar_disabled'] = False\n    if ((not request.session.get('cms_toolbar_disabled', False)) and (request.user.is_staff or (anonymous_on and request.user.is_anonymous()))):\n        if ((edit_on in request.GET) and (not request.session.get('cms_edit', False))):\n            if (not request.session.get('cms_edit', False)):\n                menu_pool.clear()\n            request.session['cms_edit'] = True\n            if request.session.get('cms_build', False):\n                request.session['cms_build'] = False\n        if ((edit_off in request.GET) and request.session.get('cms_edit', True)):\n            if request.session.get('cms_edit', True):\n                menu_pool.clear()\n            request.session['cms_edit'] = False\n            if request.session.get('cms_build', False):\n                request.session['cms_build'] = False\n        if ((build in request.GET) and (not request.session.get('cms_build', False))):\n            request.session['cms_build'] = True\n    else:\n        request.session['cms_build'] = False\n        request.session['cms_edit'] = False\n    if request.user.is_staff:\n        try:\n            request.cms_latest_entry = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE)).only('pk').order_by('-pk')[0].pk\n        except IndexError:\n            request.cms_latest_entry = (- 1)\n    request.toolbar = CMSToolbar(request)\n", "label": 1}
{"function": "\n\ndef test_build_install(self):\n    fdir = self._create_formula_files(_F1)\n    self.client.run(['build', fdir])\n    pkgpath = self.ui._status[(- 1)].split()[(- 1)]\n    assert os.path.exists(pkgpath)\n    self.client.run(['local', 'install', pkgpath])\n    for (path, contents) in _F1['contents']:\n        path = os.path.join(__opts__['file_roots']['base'][0], _F1['definition']['name'], path)\n        assert os.path.exists(path)\n        assert (open(path, 'r').read() == contents)\n    self.client.run(['info', _F1['definition']['name']])\n    lines = self.ui._status[(- 1)].split('\\n')\n    for (key, line) in (('name', 'Name: {0}'), ('version', 'Version: {0}'), ('release', 'Release: {0}'), ('summary', 'Summary: {0}')):\n        assert (line.format(_F1['definition'][key]) in lines)\n    self.ui._error = []\n    self.client.run(['local', 'install', pkgpath])\n    assert (len(self.ui._error) > 0)\n    __opts__['force'] = True\n    self.ui._error = []\n    self.client.run(['local', 'install', pkgpath])\n    assert (len(self.ui._error) == 0)\n    __opts__['force'] = False\n", "label": 0}
{"function": "\n\ndef test_file():\n    nmeafile = pynmea2.NMEAFile(StringIO(TEST_DATA))\n    nmea_strings = nmeafile.read()\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    del nmeafile\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [_f.readline() for i in range(10)]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [s for s in _f]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [_f.next() for i in range(10)]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n", "label": 1}
{"function": "\n\n@tornado.gen.coroutine\ndef disbatch(self):\n    '\\n        Disbatch all lowstates to the appropriate clients\\n        '\n    ret = []\n    for low in self.lowstate:\n        if (not self._verify_client(low)):\n            return\n        if ((self.token is not None) and ('token' not in low)):\n            low['token'] = self.token\n        if (not (('token' in low) or (('username' in low) and ('password' in low) and ('eauth' in low)))):\n            ret.append('Failed to authenticate')\n            break\n        try:\n            chunk_ret = (yield getattr(self, '_disbatch_{0}'.format(low['client']))(low))\n            ret.append(chunk_ret)\n        except EauthAuthenticationError as exc:\n            ret.append('Failed to authenticate')\n            break\n        except Exception as ex:\n            ret.append('Unexpected exception while handling request: {0}'.format(ex))\n            logger.error('Unexpected exception while handling request:', exc_info=True)\n    self.write(self.serialize({\n        'return': ret,\n    }))\n    self.finish()\n", "label": 1}
{"function": "\n\ndef super_len(o):\n    total_length = 0\n    current_position = 0\n    if hasattr(o, '__len__'):\n        total_length = len(o)\n    elif hasattr(o, 'len'):\n        total_length = o.len\n    elif hasattr(o, 'getvalue'):\n        total_length = len(o.getvalue())\n    elif hasattr(o, 'fileno'):\n        try:\n            fileno = o.fileno()\n        except io.UnsupportedOperation:\n            pass\n        else:\n            total_length = os.fstat(fileno).st_size\n            if ('b' not in o.mode):\n                warnings.warn(\"Requests has determined the content-length for this request using the binary size of the file: however, the file has been opened in text mode (i.e. without the 'b' flag in the mode). This may lead to an incorrect content-length. In Requests 3.0, support will be removed for files in text mode.\", FileModeWarning)\n    if hasattr(o, 'tell'):\n        try:\n            current_position = o.tell()\n        except (OSError, IOError):\n            current_position = total_length\n    return max(0, (total_length - current_position))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef build(op, a, b, out=None, **kwargs):\n    '\\n        Build OpTreeNode.\\n\\n        Arguments:\\n            a (OpTreeNode, Tensor, numeric): left-hand side operand.\\n            b (OpTreeNode, Tensor, numeric): right-hand side operand.\\n            out (Tensor, optional): where the result will be stored. If out is\\n                                    not None, the op-tree will be executed.\\n            kwargs: optional argument such as axis of the reducion.\\n        '\n    for arg in (a, b):\n        if (not isinstance(arg, (int, float, Tensor, OpTreeNode, type(None)))):\n            return NotImplemented\n    out_shape = [1, 1]\n    if isinstance(a, (OpTreeNode, Tensor)):\n        a_shape = a.shape\n    elif isinstance(a, (float, int)):\n        a_shape = [1, 1]\n    else:\n        a_shape = [0, 0]\n    if isinstance(b, (OpTreeNode, Tensor)):\n        b_shape = b.shape\n    elif isinstance(b, (float, int)):\n        b_shape = [1, 1]\n    else:\n        b_shape = [0, 0]\n    if (len(a_shape) == 1):\n        a_shape = (a_shape + (1,))\n    if (len(b_shape) == 1):\n        b_shape = (b_shape + (1,))\n    if (op in OpCollection.ew_ops):\n        for i in range(2):\n            out_shape[i] = max(a_shape[i], b_shape[i])\n    elif (op in OpCollection.reduction_ops):\n        if ('axis' in kwargs):\n            out_shape = list(a_shape)\n            out_shape[kwargs['axis']] = 1\n        else:\n            pass\n    elif (op == 'assign'):\n        out_shape = a_shape\n    elif (op == 'dot'):\n        assert ((len(a_shape) == len(b_shape)) and (len(b_shape) == 2) and (a_shape[1] == b_shape[0]))\n        out_shape = (a_shape[0], b_shape[1])\n    elif (op == 'transpose'):\n        assert (b is None)\n        out_shape = tuple(reversed(a_shape))\n    else:\n        raise TypeError(('%s is not a valid operation' % op))\n    out_shape = tuple(out_shape)\n    op_dict = {\n        'op': op,\n        'shape': out_shape,\n    }\n    op_dict.update(kwargs)\n    node = OpTreeNode(op_dict, a, b)\n    if (op == 'assign'):\n        return node.execute()\n    if (out is not None):\n        return OpTreeNode({\n            'op': 'assign',\n        }, out, node).execute()\n    return node\n", "label": 1}
{"function": "\n\ndef stack_files(self, timeslice_info_list, stack_dataset_path, band1_vrt_path=None, overwrite=False):\n    if (os.path.exists(stack_dataset_path) and (not overwrite)):\n        logger.debug('Stack VRT file %s already exists', stack_dataset_path)\n        return\n    band_no = timeslice_info_list[0]['tile_layer']\n    build_vrt = True\n    if (band_no == 1):\n        intermediate_path = stack_dataset_path\n    elif band1_vrt_path:\n        intermediate_path = band1_vrt_path\n        build_vrt = False\n    else:\n        intermediate_path = re.sub('\\\\.vrt$', '.tmp', stack_dataset_path)\n    file_list_path = re.sub('\\\\.vrt$', '.txt', stack_dataset_path)\n    if build_vrt:\n        logger.info('Creating %d layer stack VRT file %s', len(timeslice_info_list), stack_dataset_path)\n        list_file = open(file_list_path, 'w')\n        list_file.write('\\n'.join([timeslice_info['tile_pathname'] for timeslice_info in timeslice_info_list]))\n        list_file.close()\n        del list_file\n        command_string = 'gdalbuildvrt'\n        if (not self.debug):\n            command_string += ' -q'\n        command_string += (' -separate -input_file_list %s -overwrite %s' % (file_list_path, intermediate_path))\n        if (not self.debug):\n            command_string += ('\\nrm %s' % file_list_path)\n    else:\n        command_string = ''\n    if (band_no > 1):\n        if command_string:\n            command_string += '\\n'\n        command_string += ('cat %s | sed s/\\\\<SourceBand\\\\>1\\\\<\\\\\\\\/SourceBand\\\\>/\\\\<SourceBand\\\\>%d\\\\<\\\\\\\\/SourceBand\\\\>/g > %s' % (intermediate_path, band_no, stack_dataset_path))\n        if build_vrt:\n            if (not self.debug):\n                command_string += ('\\nrm %s' % intermediate_path)\n        else:\n            logger.info('Creating %d layer stack VRT file %s from %s', len(timeslice_info_list), stack_dataset_path, intermediate_path)\n    logger.debug('command_string = %s', command_string)\n    result = execute(command_string=command_string)\n    if result['stdout']:\n        log_multiline(logger.info, result['stdout'], ('stdout from ' + command_string), '\\t')\n    if result['stderr']:\n        log_multiline(logger.debug, result['stderr'], ('stderr from ' + command_string), '\\t')\n    if result['returncode']:\n        raise Exception('%s failed', command_string)\n    temporal_stack_dataset = gdal.Open(stack_dataset_path)\n    assert temporal_stack_dataset, ('Unable to open VRT %s' % stack_dataset_path)\n    for band_index in range(len(timeslice_info_list)):\n        band = temporal_stack_dataset.GetRasterBand((band_index + 1))\n        metadata_dict = dict(timeslice_info_list[band_index])\n        for key in metadata_dict.keys():\n            metadata_dict[key] = str(metadata_dict[key])\n        band.SetMetadata(metadata_dict)\n        log_multiline(logger.debug, band.GetMetadata(), 'band.GetMetadata()', '\\t')\n        nodata_value = timeslice_info_list[band_index]['nodata_value']\n        if (nodata_value is not None):\n            logger.debug('nodata_value = %s', nodata_value)\n            band.SetNoDataValue(nodata_value)\n    temporal_stack_dataset.FlushCache()\n", "label": 1}
{"function": "\n\n@dispatch(_strtypes)\ndef discover(s):\n    if (not s):\n        return null\n    for f in string_coercions:\n        try:\n            return discover(f(s))\n        except (ValueError, KeyError):\n            pass\n    if (s.isalpha() or s.isspace()):\n        return string\n    try:\n        d = dateparse(s)\n    except (ValueError, OverflowError):\n        pass\n    else:\n        return (date_ if is_zero_time(d.time()) else datetime_)\n    return string\n", "label": 0}
{"function": "\n\ndef post(self):\n    form = AuditCreateForm(self.request.arguments)\n    if (not form.validate()):\n        return self.render('audit-create.html', form=form, alerts=self.get_form_alerts(form.errors))\n    user = self.get_current_user()\n    if (not user.has_permission(AUDIT_MANAGER)):\n        return self.forbidden()\n    open_audits = self.session.query(Audit).filter((Audit.complete == False)).all()\n    if open_audits:\n        raise Exception('Sorry, there are audits in progress.')\n    ends_at = datetime.strptime(form.data['ends_at'], '%m/%d/%Y')\n    audited_groups = []\n    for groupname in self.graph.groups:\n        if (not self.graph.get_group_details(groupname)['audited']):\n            continue\n        group = Group.get(self.session, name=groupname)\n        audit = Audit(group_id=group.id, ends_at=ends_at)\n        try:\n            audit.add(self.session)\n            self.session.flush()\n        except IntegrityError:\n            self.session.rollback()\n            raise Exception('Failed to start the audit. Please try again.')\n        audited_groups.append(group)\n        group.audit_id = audit.id\n        for member in group.my_members().values():\n            auditmember = AuditMember(audit_id=audit.id, edge_id=member.edge_id)\n            try:\n                auditmember.add(self.session)\n            except IntegrityError:\n                self.session.rollback()\n                raise Exception('Failed to start the audit. Please try again.')\n    self.session.commit()\n    AuditLog.log(self.session, self.current_user.id, 'start_audit', 'Started global audit.', category=AuditLogCategory.audit)\n    schedule_times = []\n    not_before = (datetime.utcnow() + timedelta(1))\n    for days_prior in (28, 21, 14, 7, 3, 1):\n        email_time = (ends_at - timedelta(days_prior))\n        email_time.replace(hour=17, minute=0, second=0)\n        if (email_time > not_before):\n            schedule_times.append((days_prior, email_time))\n    for group in audited_groups:\n        mail_to = [member.name for member in group.my_users() if (GROUP_EDGE_ROLES[member.role] in ('owner', 'np-owner'))]\n        send_email(self.session, mail_to, 'Group Audit: {}'.format(group.name), 'audit_notice', settings, {\n            'group': group.name,\n            'ends_at': ends_at,\n        })\n        for (days_prior, email_time) in schedule_times:\n            send_async_email(self.session, mail_to, 'Group Audit: {} - {} day(s) left'.format(group.name, days_prior), 'audit_notice_reminder', settings, {\n                'group': group.name,\n                'ends_at': ends_at,\n                'days_left': days_prior,\n            }, email_time, async_key='audit-{}'.format(group.id))\n    return self.redirect('/audits')\n", "label": 1}
{"function": "\n\ndef run_as_initiator(self, terminate=(lambda : False)):\n    recv_timeout = (0.001 * (self.cfg['recv-lto'] + 10))\n    symm = 0\n    try:\n        pdu = self.collect(delay=0.01)\n        while (not terminate()):\n            if (pdu is None):\n                pdu = Symmetry()\n            pdu = self.exchange(pdu, recv_timeout)\n            if (pdu is None):\n                return self.terminate(reason='link disruption')\n            if (pdu == Disconnect(0, 0)):\n                return self.terminate(reason='remote choice')\n            symm = ((symm + 1) if (type(pdu) == Symmetry) else 0)\n            self.dispatch(pdu)\n            pdu = self.collect(delay=0.001)\n            if ((pdu is None) and (symm >= 10)):\n                pdu = self.collect(delay=0.05)\n        else:\n            self.terminate(reason='local choice')\n    except KeyboardInterrupt:\n        print\n        self.terminate(reason='local choice')\n        raise KeyboardInterrupt\n    except IOError:\n        self.terminate(reason='input/output error')\n        raise SystemExit\n    finally:\n        log.debug('llc run loop terminated on initiator')\n", "label": 1}
{"function": "\n\ndef test_unitroots():\n    assert (unitroots(1) == [1])\n    assert (unitroots(2) == [1, (- 1)])\n    (a, b, c) = unitroots(3)\n    assert (a == 1)\n    assert b.ae(((- 0.5) + 0.8660254037844386j))\n    assert c.ae(((- 0.5) - 0.8660254037844386j))\n    assert (unitroots(1, primitive=True) == [1])\n    assert (unitroots(2, primitive=True) == [(- 1)])\n    assert (unitroots(3, primitive=True) == unitroots(3)[1:])\n    assert (unitroots(4, primitive=True) == [j, (- j)])\n    assert (len(unitroots(17, primitive=True)) == 16)\n    assert (len(unitroots(16, primitive=True)) == 8)\n", "label": 1}
{"function": "\n\ndef fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True):\n    \"Load the filenames and data from the 20 newsgroups dataset.\\n\\n    Read more in the :ref:`User Guide <20newsgroups>`.\\n\\n    Parameters\\n    ----------\\n    subset: 'train' or 'test', 'all', optional\\n        Select the dataset to load: 'train' for the training set, 'test'\\n        for the test set, 'all' for both, with shuffled ordering.\\n\\n    data_home: optional, default: None\\n        Specify a download and cache folder for the datasets. If None,\\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\\n\\n    categories: None or collection of string or unicode\\n        If None (default), load all the categories.\\n        If not None, list of category names to load (other categories\\n        ignored).\\n\\n    shuffle: bool, optional\\n        Whether or not to shuffle the data: might be important for models that\\n        make the assumption that the samples are independent and identically\\n        distributed (i.i.d.), such as stochastic gradient descent.\\n\\n    random_state: numpy random number generator or seed integer\\n        Used to shuffle the dataset.\\n\\n    download_if_missing: optional, True by default\\n        If False, raise an IOError if the data is not locally available\\n        instead of trying to download the data from the source site.\\n\\n    remove: tuple\\n        May contain any subset of ('headers', 'footers', 'quotes'). Each of\\n        these are kinds of text that will be detected and removed from the\\n        newsgroup posts, preventing classifiers from overfitting on\\n        metadata.\\n\\n        'headers' removes newsgroup headers, 'footers' removes blocks at the\\n        ends of posts that look like signatures, and 'quotes' removes lines\\n        that appear to be quoting another post.\\n\\n        'headers' follows an exact standard; the other filters are not always\\n        correct.\\n    \"\n    data_home = get_data_home(data_home=data_home)\n    cache_path = _pkl_filepath(data_home, CACHE_NAME)\n    twenty_home = os.path.join(data_home, '20news_home')\n    cache = None\n    if os.path.exists(cache_path):\n        try:\n            with open(cache_path, 'rb') as f:\n                compressed_content = f.read()\n            uncompressed_content = codecs.decode(compressed_content, 'zlib_codec')\n            cache = pickle.loads(uncompressed_content)\n        except Exception as e:\n            print((80 * '_'))\n            print('Cache loading failed')\n            print((80 * '_'))\n            print(e)\n    if (cache is None):\n        if download_if_missing:\n            print('Downloading 20news dataset. This may take a few minutes.')\n            cache = download_20newsgroups(target_dir=twenty_home, cache_path=cache_path)\n        else:\n            raise IOError('20Newsgroups dataset not found')\n    if (subset in ('train', 'test')):\n        data = cache[subset]\n    elif (subset == 'all'):\n        data_lst = list()\n        target = list()\n        filenames = list()\n        for subset in ('train', 'test'):\n            data = cache[subset]\n            data_lst.extend(data.data)\n            target.extend(data.target)\n            filenames.extend(data.filenames)\n        data.data = data_lst\n        data.target = np.array(target)\n        data.filenames = np.array(filenames)\n    else:\n        raise ValueError((\"subset can only be 'train', 'test' or 'all', got '%s'\" % subset))\n    data.description = 'the 20 newsgroups by date dataset'\n    if ('headers' in remove):\n        data.data = [strip_newsgroup_header(text) for text in data.data]\n    if ('footers' in remove):\n        data.data = [strip_newsgroup_footer(text) for text in data.data]\n    if ('quotes' in remove):\n        data.data = [strip_newsgroup_quoting(text) for text in data.data]\n    if (categories is not None):\n        labels = [(data.target_names.index(cat), cat) for cat in categories]\n        labels.sort()\n        (labels, categories) = zip(*labels)\n        mask = np.in1d(data.target, labels)\n        data.filenames = data.filenames[mask]\n        data.target = data.target[mask]\n        data.target = np.searchsorted(labels, data.target)\n        data.target_names = list(categories)\n        data_lst = np.array(data.data, dtype=object)\n        data_lst = data_lst[mask]\n        data.data = data_lst.tolist()\n    if shuffle:\n        random_state = check_random_state(random_state)\n        indices = np.arange(data.target.shape[0])\n        random_state.shuffle(indices)\n        data.filenames = data.filenames[indices]\n        data.target = data.target[indices]\n        data_lst = np.array(data.data, dtype=object)\n        data_lst = data_lst[indices]\n        data.data = data_lst.tolist()\n    return data\n", "label": 1}
{"function": "\n\n@modify_site\ndef create_item(self, path):\n    'Creates a new Location object for the site.\\n\\n        The location path should be canonical and should not contain\\n        parts that are not used for access control (query, fragment,\\n        parameters). Location should not contain non-ascii characters.\\n\\n        Raises:\\n            ValidationError if the path is invalid or if a site\\n            already has a location with such path.\\n            LimitExceeded if the site defines a maximum number of\\n            locations and adding a new one would exceed this number.\\n        '\n    locations_limit = self.site.locations_limit\n    if ((locations_limit is not None) and (self.count() >= locations_limit)):\n        raise LimitExceeded('Locations limit exceeded')\n    if (not url_utils.is_canonical(path)):\n        raise ValidationError('Path should be absolute and normalized (starting with / without /../ or /./ or //).')\n    if (len(path) > self.PATH_LEN_LIMIT):\n        raise ValidationError('Path too long')\n    if url_utils.contains_fragment(path):\n        raise ValidationError(\"Path should not contain fragment ('#' part).\")\n    if url_utils.contains_query(path):\n        raise ValidationError(\"Path should not contain query ('?' part).\")\n    if url_utils.contains_params(path):\n        raise ValidationError(\"Path should not contain parameters (';' part).\")\n    try:\n        path.encode('ascii')\n    except UnicodeError:\n        raise ValidationError('Path should contain only ascii characters.')\n    if (self.get_unique((lambda item: (item.path == path))) is not None):\n        raise ValidationError('Location already exists.')\n    return self._do_create_item(path=path)\n", "label": 0}
{"function": "\n\ndef save_hashes(self, hashes):\n    try:\n        hashes_folder = join(self.cache_dir, self._cached_hash_folder)\n        try:\n            makedirs(hashes_folder)\n        except OSError as e:\n            if (e.errno != EEXIST):\n                LOG.error(str(e))\n                return\n        for file_path in iglob(join(hashes_folder, '*.json')):\n            try:\n                file_obj = open(file_path, 'rb')\n                hashes_meta = json_load(file_obj)\n                file_obj.close()\n                hashes_host = hashes_meta['host']\n                if (hashes_host == self.hub_pool.host):\n                    hashes.difference_update(hashes_meta['hashes'])\n            except (IOError, TypeError, ValueError, KeyError, AttributeError):\n                pass\n        if hashes:\n            try:\n                file_path = join(hashes_folder, ('%d.json' % long(time())))\n                file_obj = open(file_path, 'wb')\n                hashes_meta = {\n                    'version': 2,\n                    'host': self.hub_pool.host,\n                    'hashes': list(hashes),\n                }\n                json_dump(hashes_meta, file_obj, separators=(',', ':'))\n                file_obj.close()\n            except IOError:\n                pass\n    except Exception as e:\n        LOG.error(str(e))\n", "label": 0}
{"function": "\n\ndef _load_imgs(file_paths, slice_, color, resize):\n    'Internally used to load images'\n    try:\n        try:\n            from scipy.misc import imread\n        except ImportError:\n            from scipy.misc.pilutil import imread\n        from scipy.misc import imresize\n    except ImportError:\n        raise ImportError('The Python Imaging Library (PIL) is required to load data from jpeg files')\n    default_slice = (slice(0, 250), slice(0, 250))\n    if (slice_ is None):\n        slice_ = default_slice\n    else:\n        slice_ = tuple(((s or ds) for (s, ds) in zip(slice_, default_slice)))\n    (h_slice, w_slice) = slice_\n    h = ((h_slice.stop - h_slice.start) // (h_slice.step or 1))\n    w = ((w_slice.stop - w_slice.start) // (w_slice.step or 1))\n    if (resize is not None):\n        resize = float(resize)\n        h = int((resize * h))\n        w = int((resize * w))\n    n_faces = len(file_paths)\n    if (not color):\n        faces = np.zeros((n_faces, h, w), dtype=np.float32)\n    else:\n        faces = np.zeros((n_faces, h, w, 3), dtype=np.float32)\n    for (i, file_path) in enumerate(file_paths):\n        if ((i % 1000) == 0):\n            logger.info('Loading face #%05d / %05d', (i + 1), n_faces)\n        img = imread(file_path)\n        if (img.ndim is 0):\n            raise RuntimeError(('Failed to read the image file %s, Please make sure that libjpeg is installed' % file_path))\n        face = np.asarray(img[slice_], dtype=np.float32)\n        face /= 255.0\n        if (resize is not None):\n            face = imresize(face, resize)\n        if (not color):\n            face = face.mean(axis=2)\n        faces[(i, ...)] = face\n    return faces\n", "label": 1}
{"function": "\n\ndef range(self, range_string, row=0, column=0):\n    'Returns a 2D array of cells, with optional row and column offsets.\\n\\n        :param range_string: cell range string or `named range` name\\n        :type range_string: string\\n\\n        :param row: number of rows to offset\\n        :type row: int\\n\\n        :param column: number of columns to offset\\n        :type column: int\\n\\n        :rtype: tuples of tuples of :class:`openpyxl.cell.Cell`\\n\\n        '\n    if (':' in range_string):\n        result = []\n        (min_range, max_range) = range_string.split(':')\n        (min_col, min_row) = coordinate_from_string(min_range)\n        (max_col, max_row) = coordinate_from_string(max_range)\n        if column:\n            min_col = get_column_letter((column_index_from_string(min_col) + column))\n            max_col = get_column_letter((column_index_from_string(max_col) + column))\n        min_col = column_index_from_string(min_col)\n        max_col = column_index_from_string(max_col)\n        cache_cols = {\n            \n        }\n        for col in xrange(min_col, (max_col + 1)):\n            cache_cols[col] = get_column_letter(col)\n        rows = xrange((min_row + row), ((max_row + row) + 1))\n        cols = xrange(min_col, (max_col + 1))\n        for row in rows:\n            new_row = []\n            for col in cols:\n                new_row.append(self.cell(('%s%s' % (cache_cols[col], row))))\n            result.append(tuple(new_row))\n        return tuple(result)\n    else:\n        try:\n            return self.cell(coordinate=range_string, row=row, column=column)\n        except CellCoordinatesException:\n            pass\n        named_range = self._parent.get_named_range(range_string)\n        if (named_range is None):\n            msg = ('%s is not a valid range name' % range_string)\n            raise NamedRangeException(msg)\n        result = []\n        for destination in named_range.destinations:\n            (worksheet, cells_range) = destination\n            if (worksheet is not self):\n                msg = ('Range %s is not defined on worksheet %s' % (cells_range, self.title))\n                raise NamedRangeException(msg)\n            content = self.range(cells_range)\n            if isinstance(content, tuple):\n                for cells in content:\n                    result.extend(cells)\n            else:\n                result.append(content)\n        if (len(result) == 1):\n            return result[0]\n        else:\n            return tuple(result)\n", "label": 1}
{"function": "\n\n@click.command()\n@click.option('--apikey', default=load_config_key, help='API key to use')\n@click.option('--list', 'listcodes', is_flag=True, help='List all valid team code/team name pairs')\n@click.option('--live', is_flag=True, help='Shows live scores from various leagues')\n@click.option('--use12hour', is_flag=True, default=False, help='Displays the time using 12 hour format instead of 24 (default).')\n@click.option('--standings', is_flag=True, help='Standings for a particular league')\n@click.option('--league', '-league', type=click.Choice(LEAGUE_IDS.keys()), help='Choose the league whose fixtures you want to see. See league codes listed in README.')\n@click.option('--players', is_flag=True, help='Shows players for a particular team')\n@click.option('--team', type=click.Choice(TEAM_NAMES.keys()), help='Choose the team whose fixtures you want to see. See team codes listed in README.')\n@click.option('--lookup', is_flag=True, help='Get team name from team code when used with --team command.')\n@click.option('--time', default=6, help='The number of days in the past for which you want to see the scores')\n@click.option('--upcoming', is_flag=True, default=False, help='Displays upcoming games when used with --time command.')\n@click.option('--stdout', 'output_format', flag_value='stdout', default=True, help='Print to stdout')\n@click.option('--csv', 'output_format', flag_value='csv', help='Output in CSV format')\n@click.option('--json', 'output_format', flag_value='json', help='Output in JSON format')\n@click.option('-o', '--output-file', default=None, help='Save output to a file (only if csv or json option is provided)')\ndef main(league, time, standings, team, live, use12hour, players, output_format, output_file, upcoming, lookup, listcodes, apikey):\n    'A CLI for live and past football scores from various football leagues'\n    global headers\n    headers = {\n        'X-Auth-Token': apikey,\n    }\n    try:\n        if ((output_format == 'stdout') and output_file):\n            raise IncorrectParametersException('Printing output to stdout and saving to a file are mutually exclusive')\n        writer = get_writer(output_format, output_file)\n        if listcodes:\n            list_team_codes()\n            return\n        if live:\n            get_live_scores(writer, use12hour)\n            return\n        if standings:\n            if (not league):\n                raise IncorrectParametersException('Please specify a league. Example --standings --league=EPL')\n            get_standings(league, writer)\n            return\n        if team:\n            if lookup:\n                map_team_id(team)\n                return\n            if players:\n                get_team_players(team, writer)\n                return\n            else:\n                get_team_scores(team, time, writer, upcoming, use12hour)\n                return\n        get_league_scores(league, time, writer, upcoming, use12hour)\n    except IncorrectParametersException as e:\n        click.secho(e.message, fg='red', bold=True)\n", "label": 1}
{"function": "\n\ndef parallel_map(function, *args, **kwargs):\n    \"Wrapper around IPython's map_sync() that defaults to map().\\n\\n    This might use IPython's parallel map_sync(), or the standard map()\\n    function if IPython cannot be used.\\n\\n    If the 'ask' keyword argument is true, the user will be prompted to start\\n    IPython engines, but the function will still default to map() if the user\\n    cancels.\\n    If the 'ipython' keyword argument is True, the function will return an\\n    additional boolean indicating whether this was computed through IPython\\n    (True) or with the default map() function (False).\\n    \"\n    say_ipython = kwargs.pop('ipython', False)\n    ask = kwargs.pop('ask', False)\n    if kwargs:\n        raise TypeError('map() got unexpected keyword arguments')\n    try:\n        import IPython.parallel\n    except ImportError:\n        (result, ipython) = (map(function, *args), False)\n    else:\n        from engine_manager import EngineManager\n        c = EngineManager.ensure_controller(connect_only=(not ask))\n        if ((c is not None) and (not c.ids)):\n            EngineManager.start_engines(prompt='A module is performing a parallelizable operation, however no IPython engines are running. Do you want to start some?')\n        if ((c is None) or (not c.ids)):\n            (result, ipython) = (map(function, *args), False)\n        else:\n            ldview = c.load_balanced_view()\n            (result, ipython) = (ldview.map_sync(function, *args), True)\n    if say_ipython:\n        return (result, ipython)\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef populate_functions(self, schema, filter_func):\n    'Returns a list of function names\\n\\n        filter_func is a function that accepts a FunctionMetadata namedtuple\\n        and returns a boolean indicating whether that function should be\\n        kept or discarded\\n        '\n    metadata = self.dbmetadata['functions']\n    if schema:\n        try:\n            return [func for (func, metas) in metadata[schema].items() for meta in metas if filter_func(meta)]\n        except KeyError:\n            return []\n    else:\n        return [func for schema in self.search_path for (func, metas) in metadata[schema].items() for meta in metas if filter_func(meta)]\n", "label": 0}
{"function": "\n\ndef cached_request(self, request):\n    '\\n        Return a cached response if it exists in the cache, otherwise\\n        return False.\\n        '\n    cache_url = self.cache_url(request.url)\n    cc = self.parse_cache_control(request.headers)\n    no_cache = (True if ('no-cache' in cc) else False)\n    if (('max-age' in cc) and (cc['max-age'] == 0)):\n        no_cache = True\n    if no_cache:\n        return False\n    resp = self.serializer.loads(request, self.cache.get(cache_url))\n    if (not resp):\n        return False\n    if (resp.status == 301):\n        return resp\n    headers = CaseInsensitiveDict(resp.headers)\n    if ((not headers) or ('date' not in headers)):\n        if ('etag' not in headers):\n            self.cache.delete(cache_url)\n        return False\n    now = time.time()\n    date = calendar.timegm(parsedate_tz(headers['date']))\n    current_age = max(0, (now - date))\n    resp_cc = self.parse_cache_control(headers)\n    freshness_lifetime = 0\n    if (('max-age' in resp_cc) and resp_cc['max-age'].isdigit()):\n        freshness_lifetime = int(resp_cc['max-age'])\n    elif ('expires' in headers):\n        expires = parsedate_tz(headers['expires'])\n        if (expires is not None):\n            expire_time = (calendar.timegm(expires) - date)\n            freshness_lifetime = max(0, expire_time)\n    if ('max-age' in cc):\n        try:\n            freshness_lifetime = int(cc['max-age'])\n        except ValueError:\n            freshness_lifetime = 0\n    if ('min-fresh' in cc):\n        try:\n            min_fresh = int(cc['min-fresh'])\n        except ValueError:\n            min_fresh = 0\n        current_age += min_fresh\n    fresh = (freshness_lifetime > current_age)\n    if fresh:\n        return resp\n    if ('etag' not in headers):\n        self.cache.delete(cache_url)\n    return False\n", "label": 1}
{"function": "\n\ndef _process_results(self):\n    for i in range(MAX_LOOP_ITERATIONS):\n        try:\n            (status, msg, sc_name, instance) = self.resultsq.get_nowait()\n        except Empty:\n            break\n        if (status == FAILURE):\n            self.nb_failures += 1\n            if (self.nb_failures >= (self.pool_size - 1)):\n                self.nb_failures = 0\n                self.restart_pool()\n            continue\n        self.report_as_service_check(sc_name, status, instance, msg)\n        skip_event = _is_affirmative(instance.get('skip_event', False))\n        instance_name = instance['name']\n        if (not skip_event):\n            self.warning('Using events for service checks is deprecated in favor of monitors and will be removed in future versions of the Datadog Agent.')\n            event = None\n            if (instance_name not in self.statuses):\n                self.statuses[instance_name] = defaultdict(list)\n            self.statuses[instance_name][sc_name].append(status)\n            window = int(instance.get('window', 1))\n            if (window > 256):\n                self.log.warning('Maximum window size (256) exceeded, defaulting it to 256')\n                window = 256\n            threshold = instance.get('threshold', 1)\n            if (len(self.statuses[instance_name][sc_name]) > window):\n                self.statuses[instance_name][sc_name].pop(0)\n            nb_failures = self.statuses[instance_name][sc_name].count(Status.DOWN)\n            if (nb_failures >= threshold):\n                if (self.notified.get((instance_name, sc_name), Status.UP) != Status.DOWN):\n                    event = self._create_status_event(sc_name, status, msg, instance)\n                    self.notified[(instance_name, sc_name)] = Status.DOWN\n            elif (self.notified.get((instance_name, sc_name), Status.UP) != Status.UP):\n                event = self._create_status_event(sc_name, status, msg, instance)\n                self.notified[(instance_name, sc_name)] = Status.UP\n            if (event is not None):\n                self.events.append(event)\n        if (instance_name in self.jobs_status):\n            del self.jobs_status[instance_name]\n", "label": 1}
{"function": "\n\ndef _execute_transaction(self, connection, commands, raise_on_error):\n    cmds = chain([(('MULTI',), {\n        \n    })], commands, [(('EXEC',), {\n        \n    })])\n    all_cmds = connection.pack_commands([args for (args, _) in cmds])\n    connection.send_packed_command(all_cmds)\n    errors = []\n    try:\n        self.parse_response(connection, '_')\n    except ResponseError:\n        errors.append((0, sys.exc_info()[1]))\n    for (i, command) in enumerate(commands):\n        try:\n            self.parse_response(connection, '_')\n        except ResponseError:\n            ex = sys.exc_info()[1]\n            self.annotate_exception(ex, (i + 1), command[0])\n            errors.append((i, ex))\n    try:\n        response = self.parse_response(connection, '_')\n    except ExecAbortError:\n        if self.explicit_transaction:\n            self.immediate_execute_command('DISCARD')\n        if errors:\n            raise errors[0][1]\n        raise sys.exc_info()[1]\n    if (response is None):\n        raise WatchError('Watched variable changed.')\n    for (i, e) in errors:\n        response.insert(i, e)\n    if (len(response) != len(commands)):\n        self.connection.disconnect()\n        raise ResponseError('Wrong number of response items from pipeline execution')\n    if raise_on_error:\n        self.raise_first_error(commands, response)\n    data = []\n    for (r, cmd) in izip(response, commands):\n        if (not isinstance(r, Exception)):\n            (args, options) = cmd\n            command_name = args[0]\n            if (command_name in self.response_callbacks):\n                r = self.response_callbacks[command_name](r, **options)\n        data.append(r)\n    return data\n", "label": 1}
{"function": "\n\ndef test_1d_filter():\n    'Test our private overlap-add filtering function'\n    for n_signal in (1, 2, 5, 10, 20, 40, 100, 200, 400, 1000, 2000):\n        x = rng.randn(n_signal)\n        for n_filter in (2, 5, 10, 20, 40, 100, 200, 400, 1000, 2000):\n            if (n_filter > n_signal):\n                continue\n            for filter_type in ('identity', 'random'):\n                if (filter_type == 'random'):\n                    h = rng.randn(n_filter)\n                else:\n                    h = np.concatenate([[1.0], np.zeros((n_filter - 1))])\n                n_pad = max(min(n_filter, (n_signal - 1)), 0)\n                x_pad = _smart_pad(x, np.array([n_pad, n_pad]))\n                for zero_phase in (True, False):\n                    if zero_phase:\n                        x_expected = np.convolve(x_pad, h)[::(- 1)]\n                        x_expected = np.convolve(x_expected, h)[::(- 1)]\n                        x_expected = x_expected[(len(h) - 1):(- (len(h) - 1))]\n                    else:\n                        x_expected = np.convolve(x_pad, h)\n                        x_expected = x_expected[:(- (len(h) - 1))]\n                    if (n_pad > 0):\n                        x_expected = x_expected[n_pad:(- n_pad)]\n                    if (filter_type == 'identity'):\n                        assert_allclose(x_expected, x)\n                    for n_fft in (None, 32, 128, 129, 1023, 1024, 1025, 2048):\n                        x_copy = x[np.newaxis, :].copy()\n                        if ((n_fft is not None) and (n_fft < ((2 * n_filter) - 1)) and zero_phase):\n                            assert_raises(ValueError, _overlap_add_filter, x_copy, h, n_fft, zero_phase)\n                        elif ((n_fft is not None) and (n_fft < n_filter) and (not zero_phase)):\n                            assert_raises(ValueError, _overlap_add_filter, x_copy, h, n_fft, zero_phase)\n                        else:\n                            with warnings.catch_warnings(record=True):\n                                x_filtered = _overlap_add_filter(x_copy, h, n_fft, zero_phase)[0]\n                            assert_allclose(x_expected, x_filtered)\n", "label": 1}
{"function": "\n\ndef has_header(self, sample):\n    rdr = reader(StringIO(sample), self.sniff(sample))\n    header = next(rdr)\n    columns = len(header)\n    columnTypes = {\n        \n    }\n    for i in range(columns):\n        columnTypes[i] = None\n    checked = 0\n    for row in rdr:\n        if (checked > 20):\n            break\n        checked += 1\n        if (len(row) != columns):\n            continue\n        for col in list(columnTypes.keys()):\n            for thisType in [int, float, complex]:\n                try:\n                    thisType(row[col])\n                    break\n                except (ValueError, OverflowError):\n                    pass\n            else:\n                thisType = len(row[col])\n            if (thisType != columnTypes[col]):\n                if (columnTypes[col] is None):\n                    columnTypes[col] = thisType\n                else:\n                    del columnTypes[col]\n    hasHeader = 0\n    for (col, colType) in columnTypes.items():\n        if (type(colType) == type(0)):\n            if (len(header[col]) != colType):\n                hasHeader += 1\n            else:\n                hasHeader -= 1\n        else:\n            try:\n                colType(header[col])\n            except (ValueError, TypeError):\n                hasHeader += 1\n            else:\n                hasHeader -= 1\n    return (hasHeader > 0)\n", "label": 1}
{"function": "\n\ndef unpack(ext, source, dest_path):\n    '\\n    Unpack the archive |source| to |dest_path|.\\n    Note: |source| can be a file handle or a path.\\n    |ext| contains the extension of the archive.\\n    '\n    if (ext != '.zip'):\n        close_source = False\n        try:\n            if isinstance(source, basestring):\n                source = open(source, 'rb')\n                close_source = True\n            if ((ext == '.tar.gz') or (ext == '.tgz')):\n                un_tar_directory(source, dest_path, 'gz')\n            elif (ext == '.tar.bz2'):\n                un_tar_directory(source, dest_path, 'bz2')\n            elif (ext == '.gz'):\n                with open(dest_path, 'wb') as f:\n                    shutil.copyfileobj(un_gzip_stream(source), f)\n            else:\n                raise UsageError('Not an archive.')\n        except (tarfile.TarError, IOError):\n            raise UsageError('Invalid archive upload.')\n        finally:\n            if close_source:\n                source.close()\n    else:\n        delete_source = False\n        try:\n            if (not isinstance(source, basestring)):\n                temp_path = (dest_path + '.zip')\n                with open(temp_path, 'wb') as f:\n                    shutil.copyfileobj(source, f)\n                source = temp_path\n                delete_source = True\n            exitcode = subprocess.call(['unzip', '-q', source, '-d', dest_path])\n            if (exitcode != 0):\n                raise UsageError('Invalid archive upload.')\n        finally:\n            if delete_source:\n                path_util.remove(source)\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, x, k):\n    x = sympify(x)\n    k = sympify(k)\n    if ((x is S.NaN) or (k is S.NaN)):\n        return S.NaN\n    elif (k.is_integer and (x == k)):\n        return factorial(x)\n    elif k.is_Integer:\n        if (k is S.Zero):\n            return S.One\n        elif k.is_positive:\n            if (x is S.Infinity):\n                return S.Infinity\n            elif (x is S.NegativeInfinity):\n                if k.is_odd:\n                    return S.NegativeInfinity\n                else:\n                    return S.Infinity\n            else:\n                try:\n                    (F, opt) = poly_from_expr(x)\n                except PolificationFailed:\n                    return reduce((lambda r, i: (r * (x - i))), range(0, int(k)), 1)\n                if ((len(opt.gens) > 1) or (F.degree() <= 1)):\n                    return reduce((lambda r, i: (r * (x - i))), range(0, int(k)), 1)\n                else:\n                    v = opt.gens[0]\n                    return reduce((lambda r, i: (r * F.subs(v, (v - i)).expand())), range(0, int(k)), 1)\n        elif (x is S.Infinity):\n            return S.Infinity\n        elif (x is S.NegativeInfinity):\n            return S.Infinity\n        else:\n            try:\n                (F, opt) = poly_from_expr(x)\n            except PolificationFailed:\n                return (1 / reduce((lambda r, i: (r * (x + i))), range(1, (abs(int(k)) + 1)), 1))\n            if ((len(opt.gens) > 1) or (F.degree() <= 1)):\n                return (1 / reduce((lambda r, i: (r * (x + i))), range(1, (abs(int(k)) + 1)), 1))\n            else:\n                v = opt.gens[0]\n                return (1 / reduce((lambda r, i: (r * F.subs(v, (v + i)).expand())), range(1, (abs(int(k)) + 1)), 1))\n", "label": 1}
{"function": "\n\ndef finish(self):\n    self.done = True\n    if (self.has_trailers and hasattr(self.fp, 'read_trailer_lines')):\n        self.trailers = {\n            \n        }\n        try:\n            for line in self.fp.read_trailer_lines():\n                if (line[0] in ntob(' \\t')):\n                    v = line.strip()\n                else:\n                    try:\n                        (k, v) = line.split(ntob(':'), 1)\n                    except ValueError:\n                        raise ValueError('Illegal header line.')\n                    k = k.strip().title()\n                    v = v.strip()\n                if (k in comma_separated_headers):\n                    existing = self.trailers.get(envname)\n                    if existing:\n                        v = ntob(', ').join((existing, v))\n                self.trailers[k] = v\n        except Exception:\n            e = sys.exc_info()[1]\n            if (e.__class__.__name__ == 'MaxSizeExceeded'):\n                raise cherrypy.HTTPError(413, ('Maximum request length: %r' % e.args[1]))\n            else:\n                raise\n", "label": 0}
{"function": "\n\ndef rotate_clip(data_np, theta_deg, rotctr_x=None, rotctr_y=None, out=None):\n    '\\n    Rotate numpy array `data_np` by `theta_deg` around rotation center\\n    (rotctr_x, rotctr_y).  If the rotation center is omitted it defaults\\n    to the center of the array.\\n\\n    No adjustment is done to the data array beforehand, so the result will\\n    be clipped according to the size of the array (the output array will be\\n    the same size as the input array).\\n    '\n    if (math.fmod(theta_deg, 360.0) == 0.0):\n        return data_np\n    (ht, wd) = data_np.shape[:2]\n    if (rotctr_x is None):\n        rotctr_x = (wd // 2)\n    if (rotctr_y is None):\n        rotctr_y = (ht // 2)\n    if have_opencv:\n        M = cv2.getRotationMatrix2D((rotctr_y, rotctr_x), theta_deg, 1)\n        if (out is not None):\n            out[:, :, ...] = cv2.warpAffine(data_np, M, (wd, ht))\n            newdata = out\n        else:\n            newdata = cv2.warpAffine(data_np, M, (wd, ht))\n            (new_ht, new_wd) = newdata.shape[:2]\n            assert ((wd == new_wd) and (ht == new_ht)), Exception(('rotated cutout is %dx%d original=%dx%d' % (new_wd, new_ht, wd, ht)))\n    else:\n        (yi, xi) = numpy.mgrid[0:ht, 0:wd]\n        xi -= rotctr_x\n        yi -= rotctr_y\n        cos_t = numpy.cos(numpy.radians(theta_deg))\n        sin_t = numpy.sin(numpy.radians(theta_deg))\n        if have_numexpr:\n            ap = ne.evaluate('(xi * cos_t) - (yi * sin_t) + rotctr_x')\n            bp = ne.evaluate('(xi * sin_t) + (yi * cos_t) + rotctr_y')\n        else:\n            ap = (((xi * cos_t) - (yi * sin_t)) + rotctr_x)\n            bp = (((xi * sin_t) + (yi * cos_t)) + rotctr_y)\n        numpy.rint(ap, out=ap)\n        ap = ap.astype('int')\n        ap.clip(0, (wd - 1), out=ap)\n        numpy.rint(bp, out=bp)\n        bp = bp.astype('int')\n        bp.clip(0, (ht - 1), out=bp)\n        if (out is not None):\n            out[:, :, ...] = data_np[(bp, ap)]\n            newdata = out\n        else:\n            newdata = data_np[(bp, ap)]\n            (new_ht, new_wd) = newdata.shape[:2]\n            assert ((wd == new_wd) and (ht == new_ht)), Exception(('rotated cutout is %dx%d original=%dx%d' % (new_wd, new_ht, wd, ht)))\n    return newdata\n", "label": 0}
{"function": "\n\ndef _import_arbitrary(self, records):\n    if (records == []):\n        return (pd.Series([]), pd.Series([]))\n    try:\n        sorted_records = sorted(records, key=(lambda x: x['start']))\n    except KeyError:\n        message = 'Records must all have a \"start\" key and an \"end\" key.'\n        raise ValueError(message)\n    start_datetimes = []\n    values = []\n    estimateds = []\n    previous_end_datetime = None\n    for record in sorted_records:\n        start = record.get('start')\n        end = record.get('end')\n        value = record.get('value')\n        estimated = record.get('estimated')\n        if ((start is None) or (end is None)):\n            message = 'Records must all have a \"start\" key and an \"end\" key.'\n            raise ValueError(message)\n        elif (start >= end):\n            message = 'Record start must be earlier than end:{} >= {}.'.format(start, end)\n            raise ValueError(message)\n        if ((previous_end_datetime is None) or (start == previous_end_datetime)):\n            start_datetimes.append(start)\n            values.append(value)\n            previous_end_datetime = end\n            estimateds.append(bool(estimated))\n        elif (start < previous_end_datetime):\n            message = 'Skipping overlapping record: start ({}) < previous end ({})'.format(start, previous_end_datetime)\n            warn(message)\n        else:\n            start_datetimes.append(previous_end_datetime)\n            values.append(np.nan)\n            estimateds.append(False)\n            start_datetimes.append(start)\n            values.append(value)\n            previous_end_datetime = end\n            estimateds.append(bool(estimated))\n    start_datetimes.append(previous_end_datetime)\n    values.append(np.nan)\n    estimateds.append(False)\n    dt_index = pd.DatetimeIndex(start_datetimes)\n    data = pd.Series(values, index=dt_index)\n    estimated = pd.Series(estimateds, index=dt_index)\n    return (data, estimated)\n", "label": 0}
{"function": "\n\ndef read_in_chunks(iterator, chunk_size=None, fill_size=False, yield_empty=False):\n    '\\n    Return a generator which yields data in chunks.\\n\\n    :param iterator: An object which implements an iterator interface\\n                     or a File like object with read method.\\n    :type iterator: :class:`object` which implements iterator interface.\\n\\n    :param chunk_size: Optional chunk size (defaults to CHUNK_SIZE)\\n    :type chunk_size: ``int``\\n\\n    :param fill_size: If True, make sure chunks are exactly chunk_size in\\n                      length (except for last chunk).\\n    :type fill_size: ``bool``\\n\\n    :param yield_empty: If true and iterator returned no data, yield empty\\n                        bytes object before raising StopIteration.\\n    :type yield_empty: ``bool``\\n\\n    TODO: At some point in the future we could use byte arrays here if version\\n    >= Python 3. This should speed things up a bit and reduce memory usage.\\n    '\n    chunk_size = (chunk_size or CHUNK_SIZE)\n    if isinstance(iterator, (file, httplib.HTTPResponse)):\n        get_data = iterator.read\n        args = (chunk_size,)\n    else:\n        get_data = next\n        args = (iterator,)\n    data = b('')\n    empty = False\n    while ((not empty) or (len(data) > 0)):\n        if (not empty):\n            try:\n                chunk = b(get_data(*args))\n                if (len(chunk) > 0):\n                    data += chunk\n                else:\n                    empty = True\n            except StopIteration:\n                empty = True\n        if (len(data) == 0):\n            if (empty and yield_empty):\n                (yield b(''))\n            raise StopIteration\n        if fill_size:\n            if (empty or (len(data) >= chunk_size)):\n                (yield data[:chunk_size])\n                data = data[chunk_size:]\n        else:\n            (yield data)\n            data = b('')\n", "label": 1}
{"function": "\n\ndef run(self):\n    if (getattr(self.distribution, 'salt_download_windows_dlls', None) is None):\n        print(\"This command is not meant to be called on it's own\")\n        exit(1)\n    import platform\n    from pip.utils.logging import indent_log\n    (platform_bits, _) = platform.architecture()\n    url = 'https://repo.saltstack.com/windows/dependencies/{bits}/{fname}32.dll'\n    dest = os.path.join(os.path.dirname(sys.executable), '{fname}32.dll')\n    with indent_log():\n        for fname in ('libeay', 'ssleay'):\n            furl = url.format(bits=platform_bits[:2], fname=fname)\n            fdest = dest.format(fname=fname)\n            if (not os.path.exists(fdest)):\n                log.info('Downloading {0}32.dll to {1} from {2}'.format(fname, fdest, furl))\n                try:\n                    import requests\n                    from contextlib import closing\n                    with closing(requests.get(furl, stream=True)) as req:\n                        if (req.status_code == 200):\n                            with open(fdest, 'wb') as wfh:\n                                for chunk in req.iter_content(chunk_size=4096):\n                                    if chunk:\n                                        wfh.write(chunk)\n                                        wfh.flush()\n                        else:\n                            log.error('Failed to download {0}32.dll to {1} from {2}'.format(fname, fdest, furl))\n                except ImportError:\n                    req = urlopen(furl)\n                    if (req.getcode() == 200):\n                        with open(fdest, 'wb') as wfh:\n                            if IS_PY3:\n                                while True:\n                                    chunk = req.read(4096)\n                                    if (len(chunk) == 0):\n                                        break\n                                    wfh.write(chunk)\n                                    wfh.flush()\n                            else:\n                                while True:\n                                    for chunk in req.read(4096):\n                                        if (not chunk):\n                                            break\n                                        wfh.write(chunk)\n                                        wfh.flush()\n                    else:\n                        log.error('Failed to download {0}32.dll to {1} from {2}'.format(fname, fdest, furl))\n", "label": 1}
{"function": "\n\ndef main():\n    '\\n    Start main program: Parse user arguments and take action\\n    '\n    parser = argparse.ArgumentParser(description='mx: Orchestrate tmux sessions and git projects')\n    parser.add_argument('action', type=str, nargs='?', default='start', choices=(WORKSPACE_COMMANDS + GIT_COMMANDS), help='an action for %(prog)s (default: %(default)s)')\n    parser.add_argument('session', type=str, nargs='?', help=\"session for %(prog)s to load (default: current directory's .mx.yml)\")\n    parser.add_argument('-c', '--config', type=str, default='.mx.yml', help='workspace yml config file (default: %(default)s)')\n    parser.add_argument('-v', action='version', version='%(prog)s {}'.format(__version__))\n    args = parser.parse_args()\n    cache_dir = os.environ.get('XDG_CACHE_HOME', os.path.join(os.environ.get('HOME'), '.cache'))\n    pool_dir = os.path.join(cache_dir, 'mx')\n    if args.session:\n        cfg_path = os.path.join(pool_dir, '{}.yml'.format(args.session))\n    else:\n        cfg_path = os.path.realpath(args.config)\n    log = Logger()\n    if (not os.path.isfile(cfg_path)):\n        if (args.action == 'init'):\n            schema = Workspace.initialize(os.getcwd())\n            with open(cfg_path, 'w') as cfg_file:\n                cfg_file.write(yaml.safe_dump(schema, default_flow_style=False))\n            args.action = 'status'\n        else:\n            log.echo('[red]ERROR: [reset]Unable to find [white]{}'.format(cfg_path))\n            sys.exit(2)\n    try:\n        with open(cfg_path, 'r') as stream:\n            config = yaml.load(stream)\n        run(config, args.action)\n        if (not os.path.isdir(pool_dir)):\n            os.makedirs(pool_dir)\n        link = os.path.join(pool_dir, '{}.yml'.format(config.get('name')))\n        if (not os.path.isfile(link)):\n            os.symlink(cfg_path, link)\n    except (WorkspaceException, TmuxException) as e:\n        if (hasattr(e, '__context__') and e.__context__):\n            log.echo(' -> {}'.format(e.__context__))\n        if hasattr(e, 'errors'):\n            log.echo('[red]{}: [reset]{}'.format(e.message, e.errors))\n        else:\n            log.echo('[red]Raw error: [reset]{}'.format(str(e)))\n        sys.exit(3)\n", "label": 0}
{"function": "\n\ndef refresh(self):\n    if self.subscribed:\n        self.title = 'Subscribed projects'\n        if self.unreviewed:\n            self.title += ' with unreviewed changes'\n    else:\n        self.title = 'All projects'\n    self.app.status.update(title=self.title)\n    with self.app.db.getSession() as session:\n        i = 0\n        for project in session.getProjects(topicless=True, subscribed=self.subscribed, unreviewed=self.unreviewed):\n            i = self._projectRow(i, project, None)\n        for topic in session.getTopics():\n            i = self._topicRow(i, topic)\n            topic_unreviewed = 0\n            topic_open = 0\n            for project in topic.projects:\n                cache = self.app.project_cache.get(project)\n                topic_unreviewed += cache['unreviewed_changes']\n                topic_open += cache['open_changes']\n                if self.subscribed:\n                    if (not project.subscribed):\n                        continue\n                    if (self.unreviewed and (not cache['unreviewed_changes'])):\n                        continue\n                if (topic.key in self.open_topics):\n                    i = self._projectRow(i, project, topic)\n            topic_row = self.topic_rows.get(topic.key)\n            topic_row.update(topic, topic_unreviewed, topic_open)\n    while (i < len(self.listbox.body)):\n        current_row = self.listbox.body[i]\n        self._deleteRow(current_row)\n", "label": 1}
{"function": "\n\ndef call_ck(i):\n    '\\n    Input:  {\\n              Input for CK\\n            }\\n\\n    Output: {\\n              return       - return code =  0, if successful\\n                                         >  0, if error\\n              (error)      - error text if return > 0\\n\\n              (stdout)     - stdout, if available\\n              (stderr)     - stderr, if available\\n              (std)        - stdout+stderr\\n            }\\n    '\n    import subprocess\n    action = i.get('action', '')\n    if (action == ''):\n        return {\n            'return': 1,\n            'error': 'action is not defined',\n        }\n    (fd, fn) = tempfile.mkstemp(suffix='.tmp', prefix='ck-')\n    os.close(fd)\n    dc = i.get('detach_console', '')\n    if (dc == 'yes'):\n        i['out'] = 'con'\n    rr = {\n        'return': 0,\n    }\n    rr['stdout'] = ''\n    rr['stderr'] = ''\n    rx = ck.save_json_to_file({\n        'json_file': fn,\n        'dict': i,\n    })\n    if (rx['return'] > 0):\n        return rx\n    cmd = ((('ck ' + action) + ' @') + fn)\n    if (dc == 'yes'):\n        rx = ck.get_os_ck({\n            \n        })\n        if (rx['return'] > 0):\n            return rx\n        plat = rx['platform']\n        dci = ck.cfg.get('detached_console', {\n            \n        }).get(plat, {\n            \n        })\n        dcmd = dci.get('cmd', '')\n        if (dcmd == ''):\n            return {\n                'return': 1,\n                'error': 'detached console is requested but cmd is not defined in kernel configuration',\n            }\n        dcmd = dcmd.replace('$#cmd#$', cmd)\n        if (dci.get('use_create_new_console_flag', '') == 'yes'):\n            process = subprocess.Popen(dcmd, stdin=None, stdout=None, stderr=None, shell=True, close_fds=True, creationflags=subprocess.CREATE_NEW_CONSOLE)\n        else:\n            try:\n                pid = os.fork()\n            except OSError as e:\n                return {\n                    'return': 1,\n                    'error': (('forking detached console failed (' + format(e)) + ')'),\n                }\n            if (pid == 0):\n                os.setsid()\n                pid = os.fork()\n                if (pid != 0):\n                    os._exit(0)\n                try:\n                    maxfd = os.sysconf('SC_OPEN_MAX')\n                except (AttributeError, ValueError):\n                    maxfd = 1024\n                for fd in range(maxfd):\n                    try:\n                        os.close(fd)\n                    except OSError:\n                        pass\n                os.open('/dev/null', os.O_RDWR)\n                os.dup2(0, 1)\n                os.dup2(0, 2)\n                process = os.system(dcmd)\n                os._exit(0)\n        stdout = ck.cfg.get('detached_console_html', 'Console was detached ...')\n        stderr = ''\n    else:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n        (stdout, stderr) = process.communicate()\n    try:\n        stdout = stdout.decode('utf8')\n    except Exception as e:\n        pass\n    try:\n        stderr = stderr.decode('utf8')\n    except Exception as e:\n        pass\n    rr['std'] = (stdout + stderr)\n    rr['stdout'] = stdout\n    rr['stderr'] = stderr\n    return rr\n", "label": 1}
{"function": "\n\ndef syncjob(self, response, interval=0.5):\n    'Block until job completes and return result\\n\\n        response: XML response tag from firewall when job is created\\n\\n        :returns True if job completed successfully, False if not\\n        '\n    if (interval is not None):\n        try:\n            interval = float(interval)\n            if (interval < 0):\n                raise ValueError\n        except ValueError:\n            raise err.PanDeviceError(('Invalid interval: %s' % interval))\n    job = response.find('./result/job')\n    if (job is None):\n        return False\n    job = job.text\n    self._logger.debug('Syncing job: %s', job)\n    cmd = ('show jobs id \"%s\"' % job)\n    start_time = time.time()\n    while True:\n        try:\n            self.xapi.op(cmd=cmd, cmd_xml=True)\n        except pan.xapi.PanXapiError as msg:\n            raise pan.xapi.PanXapiError(('commit %s: %s' % (cmd, msg)))\n        path = './result/job/status'\n        status = self.xapi.element_root.find(path)\n        if (status is None):\n            raise pan.xapi.PanXapiError(('No status element in ' + (\"'%s' response\" % cmd)))\n        if (status.text == 'FIN'):\n            pconf = PanConfig(self.xapi.element_result)\n            response = pconf.python()\n            job = response['result']\n            if (job is None):\n                return\n            job = job['job']\n            success = (True if (job['result'] == 'OK') else False)\n            messages = job['details']['line']\n            if issubclass(messages.__class__, basestring):\n                messages = [messages]\n            result = {\n                'success': success,\n                'result': job['result'],\n                'jobid': job['id'],\n                'user': job['user'],\n                'warnings': job['warnings'],\n                'starttime': job['tenq'],\n                'endtime': job['tfin'],\n                'messages': messages,\n            }\n            return result\n        self._logger.debug('Job %s status %s', job, status.text)\n        if ((self.timeout is not None) and (self.timeout != 0) and (time.time() > (start_time + self.timeout))):\n            raise pan.xapi.PanXapiError(('Timeout waiting for ' + ('job %s completion' % job)))\n        self._logger.debug('Sleep %.2f seconds', interval)\n        time.sleep(interval)\n", "label": 1}
{"function": "\n\ndef test_json_conversion():\n    from commonast import Node, Assign, Name, BinOp, Bytes, Num\n    roota = Assign([Name('foo')], BinOp('Add', Name('a'), Num(3)))\n    rootb = Assign([Name('foo')], BinOp('Add', None, Num(3.2)))\n    rootc = Assign([Name('foo')], BinOp('Add', Bytes(b'xx'), Num(4j)))\n    for node1 in (roota, rootb, rootc):\n        js = node1.tojson()\n        node2 = Node.fromjson(js)\n        assert (js.count('BinOp') == 1)\n        assert (js.count('Num') == 1)\n        assert (node2.target_nodes[0].name == node1.target_nodes[0].name)\n        assert (node2.value_node.op == node1.value_node.op)\n        assert (node2.value_node.left_node == node1.value_node.left_node)\n        assert (node2.value_node.right_node.value == node1.value_node.right_node.value)\n        (node1 == node2)\n    assert (roota != rootb)\n    assert (roota != rootc)\n    with raises(ValueError):\n        (roota == 5)\n    assert (str(roota) == roota.tojson())\n    assert (len(repr(roota)) < 80)\n", "label": 1}
{"function": "\n\ndef encode(self, command, source, dest, pretend=False):\n    'Encode `source` to `dest` using command template `command`.\\n\\n        Raises `subprocess.CalledProcessError` if the command exited with a\\n        non-zero status code.\\n        '\n    assert isinstance(command, bytes)\n    assert isinstance(source, bytes)\n    assert isinstance(dest, bytes)\n    quiet = self.config['quiet'].get(bool)\n    if ((not quiet) and (not pretend)):\n        self._log.info('Encoding {0}', util.displayable_path(source))\n    args = shlex.split(command)\n    for (i, arg) in enumerate(args):\n        args[i] = Template(arg).safe_substitute({\n            b'source': source,\n            b'dest': dest,\n        })\n    if pretend:\n        self._log.info(' '.join(ui.decargs(args)))\n        return\n    try:\n        util.command_output(args)\n    except subprocess.CalledProcessError as exc:\n        self._log.info('Encoding {0} failed. Cleaning up...', util.displayable_path(source))\n        self._log.debug('Command {0} exited with status {1}', exc.cmd.decode('utf8', 'ignore'), exc.returncode)\n        util.remove(dest)\n        util.prune_dirs(os.path.dirname(dest))\n        raise\n    except OSError as exc:\n        raise ui.UserError(\"convert: could invoke '{0}': {1}\".format(' '.join(args), exc))\n    if ((not quiet) and (not pretend)):\n        self._log.info('Finished encoding {0}', util.displayable_path(source))\n", "label": 1}
{"function": "\n\ndef _repr(self, value, pos):\n    try:\n        if (value is None):\n            return ''\n        if self._unicode:\n            try:\n                value = str(value)\n                if (not is_unicode(value)):\n                    value = value.decode('utf-8')\n            except UnicodeDecodeError:\n                value = bytes(value)\n        else:\n            if (not isinstance(value, basestring_)):\n                value = coerce_text(value)\n            if (is_unicode(value) and self.default_encoding):\n                value = value.encode(self.default_encoding)\n    except:\n        exc_info = sys.exc_info()\n        e = exc_info[1]\n        e.args = (self._add_line_info(e.args[0], pos),)\n        raise e\n    else:\n        if (self._unicode and isinstance(value, bytes)):\n            if (not self.default_encoding):\n                raise UnicodeDecodeError(('Cannot decode bytes value %r into unicode (no default_encoding provided)' % value))\n            try:\n                value = value.decode(self.default_encoding)\n            except UnicodeDecodeError as e:\n                raise UnicodeDecodeError(e.encoding, e.object, e.start, e.end, (e.reason + (' in string %r' % value)))\n        elif ((not self._unicode) and is_unicode(value)):\n            if (not self.default_encoding):\n                raise UnicodeEncodeError(('Cannot encode unicode value %r into bytes (no default_encoding provided)' % value))\n            value = value.encode(self.default_encoding)\n        return value\n", "label": 1}
{"function": "\n\ndef test_source_packages():\n    for ext in ('.tar.gz', '.tar', '.tgz', '.zip', '.tar.bz2'):\n        sl = SourcePackage(('a_p_r-3.1.3' + ext))\n        assert (sl._name == 'a_p_r')\n        assert (sl.name == 'a-p-r')\n        assert (sl.raw_version == '3.1.3')\n        assert (sl.version == parse_version(sl.raw_version))\n        for req in ('a_p_r', 'a_p_r>2', 'a_p_r>3', 'a_p_r>=3.1.3', 'a_p_r==3.1.3', 'a_p_r>3,<3.5'):\n            assert sl.satisfies(req)\n            assert sl.satisfies(Requirement.parse(req))\n        for req in ('foo', 'a_p_r==4.0.0', 'a_p_r>4.0.0', 'a_p_r>3.0.0,<3.0.3', 'a==3.1.3'):\n            assert (not sl.satisfies(req))\n    sl = SourcePackage('python-dateutil-1.5.tar.gz')\n    assert (sl.name == 'python-dateutil')\n    assert (sl.raw_version == '1.5')\n", "label": 1}
{"function": "\n\ndef newuser():\n    'Authenticate a new user for the first time.\\n\\n    Creates a file `.pingmeconfig` inside home folder.\\n\\n    Parameters\\n    ----------\\n    email : string\\n        Email address of the user (unverified)\\n    password : string\\n        Password of the user (Stored using hashlib)\\n    phone_numer = string\\n        Phone number including country code (India, by default)\\n    save_password = bool\\n        Prompt for password for each ping\\n\\n    '\n    EMAIL_REGEX = re.compile(\"^\\\\w+([-+.']\\\\w+)*@\\\\w+([-.]\\\\w+)*\\\\.\\\\w+([-.]\\\\w+)*$\")\n    sys.stdout.write('Email : ')\n    email = sys.stdin.readline()\n    while (not EMAIL_REGEX.match(email)):\n        sys.stderr.write('Wrong email address. Try again.\\n')\n        sys.stdout.write('Email : ')\n        email = sys.stdin.readline()\n    email = email.rstrip()\n    password = hashlib.md5(getpass.getpass().rstrip()).hexdigest()\n    while (password == 'd41d8cd98f00b204e9800998ecf8427e'):\n        sys.stderr.write('Invalid password. Try Again.\\n')\n        password = hashlib.md5(getpass.getpass().rstrip()).hexdigest()\n    repass = hashlib.md5(getpass.getpass('Re-enter : ').rstrip()).hexdigest()\n    while (password != repass):\n        sys.stderr.write('Password match failed. Try again.\\n')\n        password = hashlib.md5(getpass.getpass().rstrip()).hexdigest()\n        repass = hashlib.md5(getpass.getpass(('Re-enter : ' + '')).rstrip()).hexdigest()\n    while True:\n        try:\n            sys.stdout.write('Phone number : ')\n            read_number = sys.stdin.readline()\n            read_number = phonenumbers.parse(read_number, 'IN')\n            while (not phonenumbers.is_valid_number(read_number)):\n                sys.stderr.write('Phone number is invalid. Try again.\\n')\n                sys.stdout.write('Phone number : ')\n                read_number = sys.stdin.readline()\n                read_number = phonenumbers.parse(read_number, 'IN')\n            break\n        except Exception as e:\n            print(e)\n    number = str(read_number.national_number).rstrip()\n    country_code = str(read_number.country_code)\n    country_name = countrylist.code_to_country[('+' + country_code)]\n    save_password = 'YES'\n    sys.stdout.write('Prompt for password ? (y/N) : ')\n    opt = sys.stdin.readline()\n    if (opt.strip() == 'y'):\n        save_password = 'NO'\n    target = 'http://ping-me.himanshumishra.in/config/'\n    credentials = {\n        'email': email,\n        'password': password,\n        'phone': number,\n        'join_date': datetime.date.today(),\n        'os': sys.platform,\n        'country_code': country_code,\n        'country_name': country_name,\n        'phone_os': 'Unknown',\n    }\n    r = requests.post(target, data=credentials)\n    if (r.reason == 'OK'):\n        if (ast.literal_eval(r.text)['success'] == 'True'):\n            config_file = open((home + '/.pingmeconfig'), 'w+')\n            config_file.write((('[email]\\n\\t' + email) + '\\n'))\n            config_file.write((('[password]\\n\\t' + password) + '\\n'))\n            config_file.write((((((('[phone]\\n\\t' + country_code) + ' ') + number) + ' ') + country_name) + '\\n'))\n            config_file.write(((('[preference]\\n\\t' + 'SAVE_PASSWORD = ') + save_password) + '\\n'))\n            config_file.close()\n        else:\n            sys.stderr.write((('\\nERROR : ' + ast.literal_eval(r.text)['reason']) + '\\n'))\n    else:\n        sys.stderr.write('\\nERROR : Problem on the server. Contact sysadmin\\n')\n", "label": 0}
{"function": "\n\ndef test_read_git_configs(tmpdir, test_git):\n    tmpdir.mkdir('foo')\n    tmpdir.mkdir('bar')\n    wt = qisys.worktree.WorkTree(tmpdir.strpath)\n    foo_proj = wt.add_project('foo')\n    bar_proj = wt.add_project('bar')\n    git = test_git(foo_proj.path)\n    git.initialize()\n    git = test_git(bar_proj.path)\n    git.initialize(branch='next')\n    tmpdir.join('.qi').join('git.xml').write(' <qigit>\\n <project src=\"foo\" >\\n    <branch name=\"master\" tracks=\"origin\" />\\n </project>\\n <project src=\"bar\" >\\n    <branch name=\"next\" tracks=\"origin\" />\\n    <remote name=\"origin\" url=\"git@srv:bar.git\" />\\n    <remote name=\"gerrit\" url=\"john@gerrit:bar.git\" review=\"true\"/>\\n </project>\\n</qigit>\\n')\n    git_wt = qisrc.worktree.GitWorkTree(wt)\n    git_projects = git_wt.git_projects\n    assert (len(git_projects) == 2)\n    foo = git_wt.get_git_project('foo')\n    assert (foo.src == 'foo')\n    assert (len(foo.branches) == 1)\n    assert (foo.branches[0].name == 'master')\n    assert (foo.branches[0].tracks == 'origin')\n    assert (not foo.remotes)\n    bar = git_wt.get_git_project('bar')\n    assert (bar.src == 'bar')\n    assert (len(bar.branches) == 1)\n    assert (len(bar.remotes) == 2)\n    origin = bar.remotes[0]\n    assert (origin.name == 'origin')\n    assert (origin.url == 'git@srv:bar.git')\n    gerrit = bar.remotes[1]\n    assert (gerrit.name == 'gerrit')\n    assert (origin.url == 'git@srv:bar.git')\n", "label": 1}
{"function": "\n\ndef submit_project_or_release(user, post_data, files):\n    'Registers/updates a project or release'\n    try:\n        project = Project.objects.get(name=post_data['name'])\n        if (project.owner != user):\n            return HttpResponseForbidden('That project is owned by someone else!')\n    except Project.DoesNotExist:\n        project = None\n    project_form = ProjectForm(post_data, instance=project)\n    if project_form.is_valid():\n        project = project_form.save(commit=False)\n        project.owner = user\n        project.save()\n        for c in post_data.getlist('classifiers'):\n            (classifier, created) = Classifier.objects.get_or_create(name=c)\n            project.classifiers.add(classifier)\n        if files:\n            allow_overwrite = getattr(settings, 'DJANGOPYPI_ALLOW_VERSION_OVERWRITE', False)\n            try:\n                release = Release.objects.get(version=post_data['version'], project=project, distribution=((UPLOAD_TO + '/') + files['distribution']._name))\n                if (not allow_overwrite):\n                    return HttpResponseForbidden((ALREADY_EXISTS_FMT % (release.filename, release)))\n            except Release.DoesNotExist:\n                release = None\n            release_form = ReleaseForm(post_data, files, instance=release)\n            if release_form.is_valid():\n                if (release and os.path.exists(release.distribution.path)):\n                    os.remove(release.distribution.path)\n                release = release_form.save(commit=False)\n                release.project = project\n                release.save()\n            else:\n                return HttpResponseBadRequest(('ERRORS: %s' % release_form.errors))\n    else:\n        return HttpResponseBadRequest(('ERRORS: %s' % project_form.errors))\n    return HttpResponse()\n", "label": 1}
{"function": "\n\ndef send_ready(self, conn, count):\n    if (self.state == CLOSED):\n        self.logger.debug(('[%s] cannot send RDY (in state CLOSED)' % conn))\n        return\n    if (self.state == BACKOFF):\n        self.logger.debug(('[%s] cannot send RDY (in state BACKOFF)' % conn))\n        return\n    if ((self.state == THROTTLED) and self.total_in_flight_or_ready):\n        msg = '[%s] cannot send RDY (THROTTLED and %d in flight or ready)'\n        self.logger.debug((msg % (conn, self.total_in_flight_or_ready)))\n        return\n    if (not conn.is_connected):\n        self.logger.debug(('[%s] cannot send RDY (connection closed)' % conn))\n        return\n    total = ((self.total_ready_count - conn.ready_count) + count)\n    if (total > self.max_in_flight):\n        if (not (conn.ready_count or conn.in_flight)):\n            self.logger.debug(('[%s] sending later' % conn))\n            gevent.spawn_later(5, self.send_ready, conn, count)\n        return\n    self.logger.debug(('[%s] sending RDY %d' % (conn, count)))\n    try:\n        conn.ready(count)\n    except NSQSocketError as error:\n        self.logger.warn(('[%s] RDY %d failed (%r)' % (conn, count, error)))\n", "label": 0}
{"function": "\n\ndef test_rolling():\n    time = MockedTime()\n    percentile = RollingPercentile(time, 60000, 12, 1000, True)\n    percentile.add_value(1000)\n    percentile.add_value(1000)\n    percentile.add_value(1000)\n    percentile.add_value(2000)\n    assert (percentile.buckets.size == 1)\n    assert (percentile.percentile(50) == 0)\n    time.increment(6000)\n    assert (percentile.buckets.size == 1)\n    assert (percentile.percentile(50) == 1000)\n    assert (percentile.buckets.size == 2)\n    percentile.add_value(1000)\n    percentile.add_value(500)\n    assert (percentile.buckets.size == 2)\n    percentile.add_value(200)\n    percentile.add_value(200)\n    percentile.add_value(1600)\n    percentile.add_value(200)\n    percentile.add_value(1600)\n    percentile.add_value(1600)\n    assert (percentile.percentile(50) == 1000)\n    time.increment(6000)\n    snapshot = PercentileSnapshot(1000, 1000, 1000, 2000, 1000, 500, 200, 200, 1600, 200, 1600, 1600)\n    assert (snapshot.percentile(0.15) == percentile.percentile(0.15))\n    assert (snapshot.percentile(0.5) == percentile.percentile(0.5))\n    assert (snapshot.percentile(0.9) == percentile.percentile(0.9))\n    assert (snapshot.percentile(0.995) == percentile.percentile(0.995))\n    assert (snapshot.mean() == 991)\n", "label": 1}
{"function": "\n\ndef test_get_connections_all(self):\n    tcp_template = \"import socket;s = socket.socket($family, socket.SOCK_STREAM);s.bind(('$addr', 0));s.listen(1);conn, addr = s.accept();\"\n    udp_template = \"import socket, time;s = socket.socket($family, socket.SOCK_DGRAM);s.bind(('$addr', 0));time.sleep(100);\"\n    from string import Template\n    tcp4_template = Template(tcp_template).substitute(family=socket.AF_INET, addr='127.0.0.1')\n    udp4_template = Template(udp_template).substitute(family=socket.AF_INET, addr='127.0.0.1')\n    tcp6_template = Template(tcp_template).substitute(family=socket.AF_INET6, addr='::1')\n    udp6_template = Template(udp_template).substitute(family=socket.AF_INET6, addr='::1')\n    tcp4_proc = get_test_subprocess([PYTHON, '-c', tcp4_template])\n    udp4_proc = get_test_subprocess([PYTHON, '-c', udp4_template])\n    if supports_ipv6():\n        tcp6_proc = get_test_subprocess([PYTHON, '-c', tcp6_template])\n        udp6_proc = get_test_subprocess([PYTHON, '-c', udp6_template])\n    else:\n        tcp6_proc = None\n        udp6_proc = None\n    all_kinds = ('all', 'inet', 'inet4', 'inet6', 'tcp', 'tcp4', 'tcp6', 'udp', 'udp4', 'udp6')\n    for p in psutil.Process(os.getpid()).get_children():\n        for conn in p.get_connections():\n            if (p.pid == tcp4_proc.pid):\n                self.assertEqual(conn.family, socket.AF_INET)\n                self.assertEqual(conn.type, socket.SOCK_STREAM)\n                self.assertEqual(conn.local_address[0], '127.0.0.1')\n                self.assertEqual(conn.remote_address, ())\n                self.assertEqual(conn.status, 'LISTEN')\n                for kind in all_kinds:\n                    cons = p.get_connections(kind=kind)\n                    if (kind in ('all', 'inet', 'inet4', 'tcp', 'tcp4')):\n                        assert (cons != []), cons\n                    else:\n                        self.assertEqual(cons, [], cons)\n            elif (p.pid == udp4_proc.pid):\n                self.assertEqual(conn.family, socket.AF_INET)\n                self.assertEqual(conn.type, socket.SOCK_DGRAM)\n                self.assertEqual(conn.local_address[0], '127.0.0.1')\n                self.assertEqual(conn.remote_address, ())\n                self.assertEqual(conn.status, '')\n                for kind in all_kinds:\n                    cons = p.get_connections(kind=kind)\n                    if (kind in ('all', 'inet', 'inet4', 'udp', 'udp4')):\n                        assert (cons != []), cons\n                    else:\n                        self.assertEqual(cons, [], cons)\n            elif (p.pid == getattr(tcp6_proc, 'pid', None)):\n                self.assertEqual(conn.family, socket.AF_INET6)\n                self.assertEqual(conn.type, socket.SOCK_STREAM)\n                self.assertIn(conn.local_address[0], ('::', '::1'))\n                self.assertEqual(conn.remote_address, ())\n                self.assertEqual(conn.status, 'LISTEN')\n                for kind in all_kinds:\n                    cons = p.get_connections(kind=kind)\n                    if (kind in ('all', 'inet', 'inet6', 'tcp', 'tcp6')):\n                        assert (cons != []), cons\n                    else:\n                        self.assertEqual(cons, [], cons)\n            elif (p.pid == getattr(udp6_proc, 'pid', None)):\n                self.assertEqual(conn.family, socket.AF_INET6)\n                self.assertEqual(conn.type, socket.SOCK_DGRAM)\n                self.assertIn(conn.local_address[0], ('::', '::1'))\n                self.assertEqual(conn.remote_address, ())\n                self.assertEqual(conn.status, '')\n                for kind in all_kinds:\n                    cons = p.get_connections(kind=kind)\n                    if (kind in ('all', 'inet', 'inet6', 'udp', 'udp6')):\n                        assert (cons != []), cons\n                    else:\n                        self.assertEqual(cons, [], cons)\n", "label": 1}
{"function": "\n\ndef embed_code_links(app, exception):\n    'Embed hyperlinks to documentation into example code'\n    if (exception is not None):\n        return\n    print('Embedding documentation hyperlinks in examples..')\n    if (app.builder.name == 'latex'):\n        return\n    doc_resolvers = {\n        \n    }\n    doc_resolvers['sklearn'] = SphinxDocLinkResolver(app.builder.outdir, relative=True)\n    resolver_urls = {\n        'matplotlib': 'http://matplotlib.org',\n        'numpy': 'http://docs.scipy.org/doc/numpy-1.6.0',\n        'scipy': 'http://docs.scipy.org/doc/scipy-0.11.0/reference',\n    }\n    for (this_module, url) in resolver_urls.items():\n        try:\n            doc_resolvers[this_module] = SphinxDocLinkResolver(url)\n        except HTTPError as e:\n            print('The following HTTP Error has occurred:\\n')\n            print(e.code)\n        except URLError as e:\n            print('\\n...\\nWarning: Embedding the documentation hyperlinks requires internet access.\\nPlease check your network connection.\\nUnable to continue embedding `{0}` links due to a URL Error:\\n'.format(this_module))\n            print(e.args)\n    example_dir = os.path.join(app.builder.srcdir, 'auto_examples')\n    html_example_dir = os.path.abspath(os.path.join(app.builder.outdir, 'auto_examples'))\n    link_pattern = '<a href=\"%s\">%s</a>'\n    orig_pattern = '<span class=\"n\">%s</span>'\n    period = '<span class=\"o\">.</span>'\n    for (dirpath, _, filenames) in os.walk(html_example_dir):\n        for fname in filenames:\n            print(('\\tprocessing: %s' % fname))\n            full_fname = os.path.join(html_example_dir, dirpath, fname)\n            subpath = dirpath[(len(html_example_dir) + 1):]\n            pickle_fname = os.path.join(example_dir, subpath, (fname[:(- 5)] + '_codeobj.pickle'))\n            if os.path.exists(pickle_fname):\n                with open(pickle_fname, 'rb') as fid:\n                    example_code_obj = pickle.load(fid)\n                fid.close()\n                str_repl = {\n                    \n                }\n                for (name, cobj) in example_code_obj.items():\n                    this_module = cobj['module'].split('.')[0]\n                    if (this_module not in doc_resolvers):\n                        continue\n                    try:\n                        link = doc_resolvers[this_module].resolve(cobj, full_fname)\n                    except (HTTPError, URLError) as e:\n                        print('The following error has occurred:\\n')\n                        print(repr(e))\n                        continue\n                    if (link is not None):\n                        parts = name.split('.')\n                        name_html = period.join(((orig_pattern % part) for part in parts))\n                        str_repl[name_html] = (link_pattern % (link, name_html))\n                names = sorted(str_repl, key=len, reverse=True)\n                expr = re.compile(('(?<!\\\\.)\\\\b' + '|'.join((re.escape(name) for name in names))))\n\n                def substitute_link(match):\n                    return str_repl[match.group()]\n                if (len(str_repl) > 0):\n                    with open(full_fname, 'rb') as fid:\n                        lines_in = fid.readlines()\n                    with open(full_fname, 'wb') as fid:\n                        for line in lines_in:\n                            line = line.decode('utf-8')\n                            line = expr.sub(substitute_link, line)\n                            fid.write(line.encode('utf-8'))\n    print('[done]')\n", "label": 1}
{"function": "\n\ndef applicationDataReceived(self, bytes):\n    '\\n        '\n    if (not self.interacting):\n        self.transport.write(bytes)\n        if (bytes in ('\\r', '\\n')):\n            self.transport.write('\\n')\n            pieces = self.cmdbuf.split(' ', 1)\n            self.cmdbuf = ''\n            (cmd, args) = (pieces[0], '')\n            if (len(pieces) > 1):\n                args = pieces[1]\n            try:\n                func = getattr(self, ('cmd_' + cmd))\n            except AttributeError:\n                self.transport.write('** Unknown command.\\r\\n')\n                return\n            func(args)\n        else:\n            self.cmdbuf += bytes\n    else:\n        for c in bytes:\n            if (ord(c) == 27):\n                self.interacting.terminal.delInteractor(self)\n                self.interacting = None\n                self.transport.write('\\r\\n** Interactive session closed.\\r\\n')\n                return\n        if (not self.readonly):\n            if (type(bytes) == type('')):\n                ttylog.ttylog_write(self.interacting.terminal.ttylog_file, len(bytes), ttylog.TYPE_INTERACT, time.time(), bytes)\n            for c in bytes:\n                recvline.HistoricRecvLine.keystrokeReceived(self.interacting, c, None)\n", "label": 0}
{"function": "\n\ndef create_version(obj, session, deleted=False):\n    obj_mapper = object_mapper(obj)\n    history_mapper = obj.__history_mapper__\n    history_cls = history_mapper.class_\n    obj_state = attributes.instance_state(obj)\n    attr = {\n        \n    }\n    obj_changed = False\n    for (om, hm) in zip(obj_mapper.iterate_to_root(), history_mapper.iterate_to_root()):\n        if hm.single:\n            continue\n        for hist_col in hm.local_table.c:\n            if _is_versioning_col(hist_col):\n                continue\n            obj_col = om.local_table.c[hist_col.key]\n            try:\n                prop = obj_mapper.get_property_by_column(obj_col)\n            except UnmappedColumnError:\n                continue\n            if (prop.key not in obj_state.dict):\n                getattr(obj, prop.key)\n            (a, u, d) = attributes.get_history(obj, prop.key)\n            if d:\n                attr[prop.key] = d[0]\n                obj_changed = True\n            elif u:\n                attr[prop.key] = u[0]\n            elif a:\n                attr[prop.key] = a[0]\n                obj_changed = True\n    if (not obj_changed):\n        for prop in obj_mapper.iterate_properties:\n            if (isinstance(prop, RelationshipProperty) and attributes.get_history(obj, prop.key, passive=attributes.PASSIVE_NO_INITIALIZE).has_changes()):\n                for p in prop.local_columns:\n                    if p.foreign_keys:\n                        obj_changed = True\n                        break\n                if (obj_changed is True):\n                    break\n    if ((not obj_changed) and (not deleted)):\n        return\n    attr['version'] = obj.version\n    hist = history_cls()\n    for (key, value) in attr.items():\n        setattr(hist, key, value)\n    session.add(hist)\n    obj.version += 1\n", "label": 1}
{"function": "\n\ndef _evaluate_condition(self, conditions, trait, at_init=False):\n    \" Evaluates a list of (eval, editor) pairs and sets a specified trait\\n        on each editor to reflect the Boolean value of the expression.\\n\\n        1) All conditions are evaluated\\n        2) The elements whose condition evaluates to False are updated\\n        3) The elements whose condition evaluates to True are updated\\n\\n        E.g., we first make invisible all elements for which 'visible_when'\\n        evaluates to False, and then we make visible the ones\\n        for which 'visible_when' is True. This avoids mutually exclusive\\n        elements to be visible at the same time, and thus making a dialog\\n        unnecessarily large.\\n\\n        The state of an editor is updated only when it changes, unless\\n        at_init is set to True.\\n\\n        Parameters\\n        ----------\\n        conditions : list of (str, Editor) tuple\\n            A list of tuples, each formed by 1) a string that contains a\\n            condition that evaluates to either True or False, and\\n            2) the editor whose state depends on the condition\\n\\n        trait : str\\n            The trait that is set by the condition.\\n            Either 'visible, 'enabled', or 'checked'.\\n\\n        at_init : bool\\n            If False, the state of an editor is set only when it changes\\n            (e.g., a visible element would not be updated to visible=True\\n            again). If True, the state is always updated (used at\\n            initialization).\\n        \"\n    context = self._get_context(self.context)\n    activate = []\n    deactivate = []\n    for (when, editor) in conditions:\n        try:\n            cond_value = eval(when, globals(), context)\n            editor_state = getattr(editor, trait)\n            if (cond_value and (at_init or (not editor_state))):\n                activate.append(editor)\n            if ((not cond_value) and (at_init or editor_state)):\n                deactivate.append(editor)\n        except Exception:\n            from traitsui.api import raise_to_debug\n            raise_to_debug()\n    for editor in deactivate:\n        setattr(editor, trait, False)\n    for editor in activate:\n        setattr(editor, trait, True)\n", "label": 1}
{"function": "\n\n@access.public\n@describeRoute(Description('Search for items that are entirely within either a GeoJSON polygon or a circular region.').param('field', 'Name of field containing GeoJSON on which to search.', required=True).param('geometry', 'Search query condition as a GeoJSON polygon.', required=False).param('center', 'Center of search radius as a GeoJSON point.', required=False).param('radius', 'Search radius in meters.', required=False, dataType='number').param('limit', 'Result set size limit (default=50).', required=False, dataType='integer').param('offset', 'Offset into result set (default=0).', required=False, dataType='integer').errorResponse().notes(\"Either parameter 'geometry' or both parameters 'center'  and 'radius' are required.\"))\ndef within(self, params):\n    \"\\n        Search for items that are entirely within either a GeoJSON polygon or a\\n        circular region. Either parameter 'geometry' or both parameters 'center'\\n        and 'radius' are required.\\n\\n        :param params: parameters to the API call, including 'field' and either\\n                       'geometry' or both 'center' and 'radius'.\\n        :type params: dict[str, unknown]\\n        :returns: filtered fields of the matching items with geospatial data\\n                 appended to the 'geo' field of each item.\\n        :rtype: list[dict[str, unknown]]\\n        :raise RestException: on malformed API call.\\n        \"\n    self.requireParams(('field',), params)\n    if ('geometry' in params):\n        try:\n            geometry = bson.json_util.loads(params['geometry'])\n            GeoJSON.to_instance(geometry, strict=True)\n            if (geometry['type'] != 'Polygon'):\n                raise ValueError\n        except (TypeError, ValueError):\n            raise RestException(\"Invalid GeoJSON passed as 'geometry' parameter.\")\n        condition = {\n            '$geometry': geometry,\n        }\n    elif (('center' in params) and ('radius' in params)):\n        try:\n            radius = (float(params['radius']) / self._RADIUS_OF_EARTH)\n            if (radius < 0.0):\n                raise ValueError\n        except ValueError:\n            raise RestException(\"Parameter 'radius' must be a number.\")\n        try:\n            center = bson.json_util.loads(params['center'])\n            GeoJSON.to_instance(center, strict=True)\n            if (center['type'] != 'Point'):\n                raise ValueError\n        except (TypeError, ValueError):\n            raise RestException(\"Invalid GeoJSON passed as 'center' parameter.\")\n        condition = {\n            '$centerSphere': [center['coordinates'], radius],\n        }\n    else:\n        raise RestException(\"Either parameter 'geometry' or both parameters 'center' and 'radius' are required.\")\n    if (params['field'][:3] == ('%s.' % GEOSPATIAL_FIELD)):\n        field = params['field'].strip()\n    else:\n        field = ('%s.%s' % (GEOSPATIAL_FIELD, params['field'].strip()))\n    (limit, offset, sort) = self.getPagingParameters(params, 'lowerName')\n    query = {\n        field: {\n            '$geoWithin': condition,\n        },\n    }\n    return self._find(query, limit, offset, sort)\n", "label": 1}
{"function": "\n\ndef add_homology(self, seq, search, id=None, idFormat='%s_%d', autoIncrement=False, maxAnnot=999999, maxLoss=None, sliceInfo=None, **kwargs):\n    'find homology in our seq db and add as annotations'\n    try:\n        if (self.sliceAttrDict['id'] != 0):\n            raise KeyError\n    except KeyError:\n        sliceAttrDict['id'] = 0\n        sliceAttrDict['start'] = 1\n        sliceAttrDict['stop'] = 2\n    if autoIncrement:\n        id = len(self.sliceDB)\n    elif (id is None):\n        id = seq.id\n    if isinstance(search, str):\n        search = getattr(self.seqDB, search)\n    if isinstance(seq, str):\n        seq = Sequence(seq, str(id))\n    al = search(seq, **kwargs)\n    if (maxLoss is not None):\n        kwargs['minAlignSize'] = (len(seq) - maxLoss)\n    hits = al[seq].keys(**kwargs)\n    if (len(hits) > maxAnnot):\n        raise ValueError(('too many hits for %s: %d' % (id, len(hits))))\n    out = []\n    i = 0\n    k = id\n    for ival in hits:\n        if (len(hits) > 1):\n            if autoIncrement:\n                k = len(self.sliceDB)\n            else:\n                k = (idFormat % (id, i))\n            i += 1\n        if (sliceInfo is not None):\n            a = self.new_annotation(k, ((ival.id, ival.start, ival.stop) + sliceInfo))\n        else:\n            a = self.new_annotation(k, (ival.id, ival.start, ival.stop))\n        out.append(a)\n    return out\n", "label": 1}
{"function": "\n\ndef _load_from_csv(self, reader, entity_type, source):\n    j = 0\n    for (i, line) in enumerate(reader):\n        (postcode_abbrev, (easting, northing)) = (line[0], line[10:12])\n        postcode_abbrev = postcode_abbrev.replace(' ', '')\n        if re.match('[A-Z][0-9]{2}[A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:2], postcode_abbrev[2:]))\n        elif re.match('[A-Z][0-9]{3}[A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:3], postcode_abbrev[3:]))\n        elif re.match('[A-Z]{2}[0-9]{2}[A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:3], postcode_abbrev[3:]))\n        elif re.match('[A-Z]{2}[0-9]{3}[A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:4], postcode_abbrev[4:]))\n        elif re.match('[A-Z][0-9][A-Z][0-9][A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:3], postcode_abbrev[3:]))\n        elif re.match('[A-Z]{2}[0-9][A-Z][0-9][A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:4], postcode_abbrev[4:]))\n        else:\n            postcode = postcode_abbrev\n        try:\n            (easting, northing) = (int(easting), int(northing))\n        except ValueError:\n            continue\n        j += 1\n        try:\n            entity = Entity.objects.get(source=source, _identifiers__scheme='postcode', _identifiers__value=postcode_abbrev)\n        except Entity.DoesNotExist:\n            entity = Entity(source=source)\n        entity.location = Point(easting, northing, srid=27700)\n        entity.geometry = entity.location\n        entity.primary_type = entity_type\n        identifiers = {\n            'postcode': postcode_abbrev,\n            'postcode-canonical': postcode,\n        }\n        entity.save(identifiers=identifiers)\n        set_name_in_language(entity, 'en', title=postcode)\n        entity.all_types.add(entity_type)\n        entity.update_all_types_completion()\n", "label": 0}
{"function": "\n\ndef pick_url(options, allow_custom_input=False):\n    'Allows the user to pick one of the options or enter a new locaiton'\n    term = get_term()\n    puts('Please choose one of the following options: ')\n    index = 1\n    enter_custom_choice = 1\n    default_choice = 1\n    for option in options:\n        if (option.priority < 1):\n            puts((' (%d) %s ' % (index, term.red(option.url))))\n            if (default_choice == index):\n                default_choice += 1\n        else:\n            to_print = (' (%d) %s ' % (index, option.url))\n            if (index == default_choice):\n                to_print = term.green(to_print)\n            puts(to_print)\n        index += 1\n    enter_custom_choice = (len(options) + 1)\n    if allow_custom_input:\n        puts((' (%d) enter a new location' % index))\n\n    def option_to_str(i):\n        if (not (i == default_choice)):\n            return str(i)\n        else:\n            return (str(i) + '=default')\n    if allow_custom_input:\n        choices = map(option_to_str, range(1, (index + 1)))\n    else:\n        choices = map(option_to_str, range(1, index))\n    while True:\n        url = None\n        choice = raw_input(('Make your choice (%s): ' % (','.join(choices),)))\n        selected = None\n        if (choice == ''):\n            selected = default_choice\n        elif (choice in choices):\n            selected = choice\n            try:\n                selected = int(choice)\n            except ValueError:\n                selected = None\n        elif (choice == str(default_choice)):\n            selected = default_choice\n        if selected:\n            if (allow_custom_input and (selected == enter_custom_choice)):\n                url = enter_url()\n            else:\n                url = options[(selected - 1)].url\n            if url:\n                return url\n", "label": 1}
{"function": "\n\ndef setvals(grains, destructive=False):\n    '\\n    Set new grains values in the grains config file\\n\\n    :param Destructive: If an operation results in a key being removed, delete the key, too. Defaults to False.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt \\'*\\' grains.setvals \"{\\'key1\\': \\'val1\\', \\'key2\\': \\'val2\\'}\"\\n    '\n    new_grains = grains\n    if (not isinstance(new_grains, collections.Mapping)):\n        raise SaltException('setvals grains must be a dictionary.')\n    grains = {\n        \n    }\n    if os.path.isfile(__opts__['conf_file']):\n        gfn = os.path.join(os.path.dirname(__opts__['conf_file']), 'grains')\n    elif os.path.isdir(__opts__['conf_file']):\n        gfn = os.path.join(__opts__['conf_file'], 'grains')\n    else:\n        gfn = os.path.join(os.path.dirname(__opts__['conf_file']), 'grains')\n    if os.path.isfile(gfn):\n        with salt.utils.fopen(gfn, 'rb') as fp_:\n            try:\n                grains = yaml.safe_load(fp_.read())\n            except yaml.YAMLError as exc:\n                return 'Unable to read existing grains file: {0}'.format(exc)\n        if (not isinstance(grains, dict)):\n            grains = {\n                \n            }\n    for (key, val) in six.iteritems(new_grains):\n        if ((val is None) and (destructive is True)):\n            if (key in grains):\n                del grains[key]\n            if (key in __grains__):\n                del __grains__[key]\n        else:\n            grains[key] = val\n            __grains__[key] = val\n    try:\n        yaml_reps = copy.deepcopy(yaml.representer.SafeRepresenter.yaml_representers)\n        yaml_multi_reps = copy.deepcopy(yaml.representer.SafeRepresenter.yaml_multi_representers)\n    except (TypeError, NameError):\n        yaml_reps = salt.utils.compat.deepcopy_bound(yaml.representer.SafeRepresenter.yaml_representers)\n        yaml_multi_reps = salt.utils.compat.deepcopy_bound(yaml.representer.SafeRepresenter.yaml_multi_representers)\n    yaml.representer.SafeRepresenter.add_representer(collections.defaultdict, yaml.representer.SafeRepresenter.represent_dict)\n    yaml.representer.SafeRepresenter.add_representer(OrderedDict, yaml.representer.SafeRepresenter.represent_dict)\n    cstr = yaml.safe_dump(grains, default_flow_style=False)\n    yaml.representer.SafeRepresenter.yaml_representers = yaml_reps\n    yaml.representer.SafeRepresenter.yaml_multi_representers = yaml_multi_reps\n    try:\n        with salt.utils.fopen(gfn, 'w+') as fp_:\n            fp_.write(cstr)\n    except (IOError, OSError):\n        msg = 'Unable to write to grains file at {0}. Check permissions.'\n        log.error(msg.format(gfn))\n    fn_ = os.path.join(__opts__['cachedir'], 'module_refresh')\n    try:\n        with salt.utils.flopen(fn_, 'w+') as fp_:\n            fp_.write('')\n    except (IOError, OSError):\n        msg = 'Unable to write to cache file {0}. Check permissions.'\n        log.error(msg.format(fn_))\n    if (not __opts__.get('local', False)):\n        __salt__['saltutil.sync_grains']()\n    return new_grains\n", "label": 1}
{"function": "\n\ndef parse_command_line(args=None):\n    if (args is None):\n        args = sys.argv[1:]\n    usage = __doc__.format(cmd=os.path.basename(sys.argv[0]))\n    options = docopt(usage, args)\n    for opt_name in ('author', 'email', 'description'):\n        options[opt_name] = options.pop(('--' + opt_name))\n    options['command'] = options.pop('<command>')\n    options['project_name'] = options.pop('--project-name')\n    if (not RE_COMMAND_NAME.match(options['command'])):\n        die('command name must match regular expression {!r}', RE_COMMAND_NAME.pattern)\n    for (info, question, default_value) in (('author', 'Your name:  ', None), ('email', 'Your email: ', None), ('description', 'Description: ', None)):\n        if options.get(info, None):\n            continue\n        try:\n            options[info] = ask(question)\n        except EOFError:\n            pass\n        if (not options[info]):\n            if (default_value is None):\n                options[info] = ''\n            else:\n                options[info] = default_value\n    if (options['project_name'] is None):\n        default = 'OpenLMI {command} Script'.format(command=options['command'].capitalize())\n        try:\n            options['project_name'] = ask('Project name [{default}]: '.format(default=default))\n        except EOFError:\n            pass\n        if (not options['project_name']):\n            options['project_name'] = default\n    options = {k: (v.decode('utf-8') if isinstance(v, str) else v) for (k, v) in options.items()}\n    return options\n", "label": 1}
{"function": "\n\ndef add_srs_entry(srs, auth_name='EPSG', auth_srid=None, ref_sys_name=None, database=None):\n    '\\n    This function takes a GDAL SpatialReference system and adds its information\\n    to the `spatial_ref_sys` table of the spatial backend.  Doing this enables\\n    database-level spatial transformations for the backend.  Thus, this utility\\n    is useful for adding spatial reference systems not included by default with\\n    the backend -- for example, the so-called \"Google Maps Mercator Projection\"\\n    is excluded in PostGIS 1.3 and below, and the following adds it to the\\n    `spatial_ref_sys` table:\\n\\n    >>> from django.contrib.gis.utils import add_srs_entry\\n    >>> add_srs_entry(900913)\\n\\n    Keyword Arguments:\\n     auth_name:\\n       This keyword may be customized with the value of the `auth_name` field.\\n       Defaults to \\'EPSG\\'.\\n\\n     auth_srid:\\n       This keyword may be customized with the value of the `auth_srid` field.\\n       Defaults to the SRID determined by GDAL.\\n\\n     ref_sys_name:\\n       For SpatiaLite users only, sets the value of the `ref_sys_name` field.\\n       Defaults to the name determined by GDAL.\\n\\n     database:\\n      The name of the database connection to use; the default is the value\\n      of `django.db.DEFAULT_DB_ALIAS` (at the time of this writing, it\\'s value\\n      is \\'default\\').\\n    '\n    from django.db import connections, DEFAULT_DB_ALIAS\n    if (not database):\n        database = DEFAULT_DB_ALIAS\n    connection = connections[database]\n    if (not hasattr(connection.ops, 'spatial_version')):\n        raise Exception('The `add_srs_entry` utility only works with spatial backends.')\n    if (connection.ops.oracle or connection.ops.mysql):\n        raise Exception('This utility does not support the Oracle or MySQL spatial backends.')\n    SpatialRefSys = connection.ops.spatial_ref_sys()\n    if (not isinstance(srs, SpatialReference)):\n        srs = SpatialReference(srs)\n    if (srs.srid is None):\n        raise Exception('Spatial reference requires an SRID to be compatible with the spatial backend.')\n    kwargs = {\n        'srid': srs.srid,\n        'auth_name': auth_name,\n        'auth_srid': (auth_srid or srs.srid),\n        'proj4text': srs.proj4,\n    }\n    if connection.ops.postgis:\n        kwargs['srtext'] = srs.wkt\n    if connection.ops.spatialite:\n        kwargs['ref_sys_name'] = (ref_sys_name or srs.name)\n    try:\n        sr = SpatialRefSys.objects.using(database).get(srid=srs.srid)\n    except SpatialRefSys.DoesNotExist:\n        sr = SpatialRefSys.objects.using(database).create(**kwargs)\n", "label": 1}
{"function": "\n\ndef interactive_running(args):\n    \"\\n    This function provides an interactive environment for running the system.\\n    It receives text from the standard input, tokenizes it, and calls the function\\n    given as a parameter to produce an answer.\\n    \\n    :param task: 'pos', 'srl' or 'dependency'\\n    :param use_tokenizer: whether to use built-in tokenizer\\n    \"\n    use_tokenizer = (not args.disable_tokenizer)\n    task_lower = args.task.lower()\n    if (task_lower == 'pos'):\n        tagger = nlpnet.taggers.POSTagger(language=args.lang)\n    elif (task_lower == 'srl'):\n        tagger = nlpnet.taggers.SRLTagger(language=args.lang)\n    elif (task_lower == 'dependency'):\n        tagger = nlpnet.taggers.DependencyParser(language=args.lang)\n    else:\n        raise ValueError(('Unknown task: %s' % args.task))\n    while True:\n        try:\n            text = raw_input()\n        except KeyboardInterrupt:\n            break\n        except EOFError:\n            break\n        if (type(text) is not unicode):\n            text = unicode(text, 'utf-8')\n        if use_tokenizer:\n            result = tagger.tag(text)\n        else:\n            tokens = text.split()\n            if (task_lower != 'dependency'):\n                result = [tagger.tag_tokens(tokens, True)]\n            else:\n                result = [tagger.tag_tokens(tokens)]\n        _print_tagged(result, task_lower)\n", "label": 0}
{"function": "\n\n@app.route('/v1/services/<id>', methods=['PUT'])\n@authnz.require_auth\n@authnz.require_csrf_token\ndef map_service_credentials(id):\n    data = request.get_json()\n    try:\n        _service = Service.get(id)\n        if (_service.data_type != 'service'):\n            msg = 'id provided is not a service.'\n            return (jsonify({\n                'error': msg,\n            }), 400)\n        revision = (_service.revision + 1)\n        _service_credential_ids = _service.credentials\n    except Service.DoesNotExist:\n        revision = 1\n        _service_credential_ids = []\n    if data.get('credentials'):\n        conflicts = _pair_key_conflicts_for_credentials(copy.deepcopy(data['credentials']))\n        if conflicts:\n            ret = {\n                'error': 'Conflicting key pairs in mapped service.',\n                'conflicts': conflicts,\n            }\n            return (jsonify(ret), 400)\n    if (revision == 1):\n        try:\n            keymanager.ensure_grants(id)\n        except keymanager.ServiceCreateGrantError:\n            msg = 'Failed to add grants for {0}.'.format(id)\n            logging.error(msg)\n    try:\n        Service(id='{0}-{1}'.format(id, revision), data_type='archive-service', credentials=data.get('credentials'), enabled=data.get('enabled'), revision=revision, modified_by=authnz.get_logged_in_user_email()).save(id__null=True)\n    except PutError as e:\n        logging.error(e)\n        return (jsonify({\n            'error': 'Failed to add service to archive.',\n        }), 500)\n    try:\n        service = Service(id=id, data_type='service', credentials=data['credentials'], enabled=data.get('enabled'), revision=revision, modified_by=authnz.get_logged_in_user_email())\n        service.save()\n    except PutError as e:\n        logging.error(e)\n        return (jsonify({\n            'error': 'Failed to update active service.',\n        }), 500)\n    added = list((set(service.credentials) - set(_service_credential_ids)))\n    removed = list((set(_service_credential_ids) - set(service.credentials)))\n    msg = 'Added credentials: {0}; Removed credentials {1}; Revision {2}'\n    msg = msg.format(added, removed, service.revision)\n    graphite.send_event([id], msg)\n    try:\n        credentials = _get_credentials(service.credentials)\n    except KeyError:\n        return (jsonify({\n            'error': 'Decryption error.',\n        }), 500)\n    return jsonify({\n        'id': service.id,\n        'credentials': credentials,\n        'revision': service.revision,\n        'enabled': service.enabled,\n        'modified_date': service.modified_date,\n        'modified_by': service.modified_by,\n    })\n", "label": 0}
{"function": "\n\ndef getStandardLibraryPaths():\n    ' Get the standard library paths.\\n\\n    '\n    if (not hasattr(getStandardLibraryPaths, 'result')):\n        os_filename = os.__file__\n        if os_filename.endswith('.pyc'):\n            os_filename = os_filename[:(- 1)]\n        os_path = Utils.normcase(Utils.dirname(os_filename))\n        stdlib_paths = set([os_path])\n        if Utils.isLink(os_filename):\n            os_filename = Utils.readLink(os_filename)\n            stdlib_paths.add(Utils.normcase(Utils.dirname(os_filename)))\n        orig_prefix_filename = Utils.joinpath(os_path, 'orig-prefix.txt')\n        if Utils.isFile(orig_prefix_filename):\n            search = os_path\n            lib_part = ''\n            while (os.path.splitdrive(search)[1] not in (os.path.sep, '')):\n                if (Utils.isFile(Utils.joinpath(search, 'bin/activate')) or Utils.isFile(Utils.joinpath(search, 'scripts/activate'))):\n                    break\n                lib_part = Utils.joinpath(Utils.basename(search), lib_part)\n                search = Utils.dirname(search)\n            assert (search and lib_part)\n            stdlib_paths.add(Utils.normcase(Utils.joinpath(open(orig_prefix_filename).read(), lib_part)))\n        python_link_filename = Utils.joinpath(os_path, '..', '.Python')\n        if Utils.isLink(python_link_filename):\n            stdlib_paths.add(Utils.normcase(Utils.joinpath(Utils.readLink(python_link_filename), 'lib')))\n        for stdlib_path in set(stdlib_paths):\n            candidate = Utils.joinpath(stdlib_path, 'lib-tk')\n            if Utils.isDir(candidate):\n                stdlib_paths.add(candidate)\n        getStandardLibraryPaths.result = [Utils.normcase(stdlib_path) for stdlib_path in stdlib_paths]\n    return getStandardLibraryPaths.result\n", "label": 1}
{"function": "\n\n@handle_response_format\n@treeio_login_required\n@_process_mass_form\ndef stream_view(request, stream_id, response_format='html'):\n    'Stream view page'\n    user = request.user.profile\n    stream = get_object_or_404(MessageStream, pk=stream_id)\n    if (not request.user.profile.has_permission(stream)):\n        return user_denied(request, message=\"You don't have access to this Stream\", response_format=response_format)\n    if request.user.profile.has_permission(stream, mode='x'):\n        if request.POST:\n            message = Message()\n            message.author = user.get_contact()\n            if (not message.author):\n                return user_denied(request, message=\"You can't send message without a Contact Card assigned to you.\", response_format=response_format)\n            form = MessageForm(request.user.profile, None, None, request.POST, instance=message)\n            if form.is_valid():\n                message = form.save()\n                message.recipients.add(user.get_contact())\n                message.set_user_from_request(request)\n                message.read_by.add(user)\n                try:\n                    if (('multicomplete_recipients' in request.POST) and request.POST['multicomplete_recipients']):\n                        try:\n                            conf = ModuleSetting.get_for_module('treeio.messaging', 'default_contact_type')[0]\n                            default_contact_type = ContactType.objects.get(pk=long(conf.value))\n                        except Exception:\n                            default_contact_type = None\n                        emails = request.POST['multicomplete_recipients'].split(',')\n                        for email in emails:\n                            emailstr = unicode(email).strip()\n                            if re.match('[a-zA-Z0-9+_\\\\-\\\\.]+@[0-9a-zA-Z][.-0-9a-zA-Z]*.[a-zA-Z]+', emailstr):\n                                (contact, created) = Contact.get_or_create_by_email(emailstr, contact_type=default_contact_type)\n                                message.recipients.add(contact)\n                                if created:\n                                    contact.set_user_from_request(request)\n                except:\n                    pass\n                message.send_email()\n                return HttpResponseRedirect(reverse('messaging_stream_view', args=[stream.id]))\n        else:\n            form = MessageForm(request.user.profile, stream_id)\n    else:\n        form = None\n    objects = Object.filter_by_request(request, Message.objects.filter(reply_to__isnull=True, stream=stream).order_by('-date_created'))\n    context = _get_default_context(request)\n    context.update({\n        'messages': objects,\n        'form': form,\n        'stream': stream,\n    })\n    return render_to_response('messaging/stream_view', context, context_instance=RequestContext(request), response_format=response_format)\n", "label": 1}
{"function": "\n\ndef find_resource(filename_or_handle, res_type, res_id, res_lang=None):\n    \"Locate a resource inside the given file or module handle.\\n\\n    This function returns a tuple (start,end) giving the location of the\\n    specified resource inside the given module.\\n\\n    Currently this relies on the kernel32.LockResource function returning\\n    a pointer based at the module handle; ideally we'd do our own parsing.\\n    \"\n    tdir = None\n    free_library = False\n    try:\n        if (res_lang is None):\n            res_lang = _DEFAULT_RESLANG\n        if isinstance(filename_or_handle, basestring):\n            filename = filename_or_handle\n            if (not isinstance(filename, unicode)):\n                filename = filename.decode(sys.getfilesystemencoding())\n            for nm in get_loaded_modules():\n                if (os.path.abspath(filename) == nm):\n                    ext = filename[filename.rfind('.'):]\n                    tdir = tempfile.mkdtemp()\n                    with open(filename, 'rb') as inF:\n                        filename = os.path.join(tdir, ('tempmodule' + ext))\n                        with open(filename, 'wb') as outF:\n                            outF.write(inF.read())\n                    break\n            l_handle = k32.LoadLibraryExW(filename, None, LOAD_LIBRARY_AS_DATAFILE)\n            if (not l_handle):\n                raise ctypes.WinError()\n            free_library = True\n        else:\n            l_handle = filename_or_handle\n        r_handle = k32.FindResourceExW(l_handle, res_type, res_id, res_lang)\n        if (not r_handle):\n            raise ctypes.WinError()\n        r_size = k32.SizeofResource(l_handle, r_handle)\n        if (not r_size):\n            raise ctypes.WinError()\n        r_info = k32.LoadResource(l_handle, r_handle)\n        if (not r_info):\n            raise ctypes.WinError()\n        r_ptr = k32.LockResource(r_info)\n        if (not r_ptr):\n            raise ctypes.WinError()\n        return (((r_ptr - l_handle) + 1), (((r_ptr - l_handle) + r_size) + 1))\n    finally:\n        if free_library:\n            k32.FreeLibrary(l_handle)\n        if (tdir is not None):\n            for nm in os.listdir(tdir):\n                os.unlink(os.path.join(tdir, nm))\n            os.rmdir(tdir)\n", "label": 1}
{"function": "\n\ndef _ExtractScore(stdout, vm, keep_partial_results):\n    'Extracts the SPEC(int|fp) score from stdout.\\n\\n  Args:\\n    stdout: stdout from running RemoteCommand.\\n    vm: The vm instance where SPEC CPU2006 was run.\\n    keep_partial_results: A boolean indicating whether partial results should\\n        be extracted in the event that not all benchmarks were successfully\\n        run. See the \"runspec_keep_partial_results\" flag for more info.\\n\\n  Sample input for SPECint:\\n      ...\\n      ...\\n      =============================================\\n      400.perlbench    9770        417       23.4 *\\n      401.bzip2        9650        565       17.1 *\\n      403.gcc          8050        364       22.1 *\\n      429.mcf          9120        364       25.1 *\\n      445.gobmk       10490        499       21.0 *\\n      456.hmmer        9330        491       19.0 *\\n      458.sjeng       12100        588       20.6 *\\n      462.libquantum  20720        468       44.2 *\\n      464.h264ref     22130        700       31.6 *\\n      471.omnetpp      6250        349       17.9 *\\n      473.astar        7020        482       14.6 *\\n      483.xalancbmk    6900        248       27.8 *\\n       Est. SPECint(R)_base2006              22.7\\n\\n  Sample input for SPECfp:\\n      ...\\n      ...\\n      =============================================\\n      410.bwaves      13590        717      19.0  *\\n      416.gamess      19580        923      21.2  *\\n      433.milc         9180        480      19.1  *\\n      434.zeusmp       9100        600      15.2  *\\n      435.gromacs      7140        605      11.8  *\\n      436.cactusADM   11950       1289       9.27 *\\n      437.leslie3d     9400        859      10.9  *\\n      444.namd         8020        504      15.9  *\\n      447.dealII      11440        409      28.0  *\\n      450.soplex       8340        272      30.6  *\\n      453.povray       5320        231      23.0  *\\n      454.calculix     8250        993       8.31 *\\n      459.GemsFDTD    10610        775      13.7  *\\n      465.tonto        9840        565      17.4  *\\n      470.lbm         13740        365      37.7  *\\n      481.wrf         11170        788      14.2  *\\n      482.sphinx3     19490        668      29.2  *\\n       Est. SPECfp(R)_base2006              17.5\\n\\n  Returns:\\n      A list of sample.Sample objects.\\n  '\n    results = []\n    re_begin_section = re.compile('^={1,}')\n    re_end_section = re.compile('Est. (SPEC.*_base2006)\\\\s*(\\\\S*)')\n    result_section = []\n    in_result_section = False\n    for line in stdout.splitlines():\n        if in_result_section:\n            result_section.append(line)\n        match = re.search(re_begin_section, line)\n        if match:\n            assert (not in_result_section)\n            in_result_section = True\n            continue\n        match = re.search(re_end_section, line)\n        if match:\n            assert in_result_section\n            spec_name = str(match.group(1))\n            try:\n                spec_score = float(match.group(2))\n            except ValueError:\n                spec_score = None\n            in_result_section = False\n            result_section.pop()\n    metadata = {\n        'num_cpus': vm.num_cpus,\n    }\n    metadata.update(vm.GetMachineTypeDict())\n    missing_results = []\n    for benchmark in result_section:\n        if ('NR' in benchmark):\n            logging.warning('SPEC CPU2006 missing result: %s', benchmark)\n            missing_results.append(str(benchmark.split()[0]))\n            continue\n        (name, _, _, score, _) = benchmark.split()\n        results.append(sample.Sample(str(name), float(score), '', metadata))\n    if (spec_score is None):\n        missing_results.append(spec_name)\n    if missing_results:\n        if keep_partial_results:\n            metadata['partial'] = 'true'\n            metadata['missing_results'] = ','.join(missing_results)\n        else:\n            raise errors.Benchmarks.RunError(('speccpu2006: results missing, see log: ' + ','.join(missing_results)))\n    if (spec_score is not None):\n        results.append(sample.Sample(spec_name, spec_score, '', metadata))\n    return results\n", "label": 1}
{"function": "\n\ndef make_axis(self, ax, location, props):\n    'Given a mpl axes instance, returns a Bokeh LinearAxis object.'\n    tf = props['tickformat']\n    tv = props['tickvalues']\n    if (tf and any((isinstance(x, string_types) for x in tf))):\n        laxis = CategoricalAxis(axis_label=ax.get_label_text())\n        assert (np.min(tv) >= 0), 'Assuming categorical axis have positive-integer dump tick values'\n        offset = (np.min(tv) - 1)\n        rng = FactorRange(factors=[str(x) for x in tf], offset=offset)\n        if (location in ['above', 'below']):\n            self.plot.x_range = rng\n        else:\n            self.plot.y_range = rng\n    elif (props['scale'] == 'linear'):\n        laxis = LinearAxis(axis_label=ax.get_label_text())\n    elif (props['scale'] == 'date'):\n        laxis = DatetimeAxis(axis_label=ax.get_label_text())\n    self.plot.add_layout(laxis, location)\n    label = ax.get_label()\n    self.text_props(label, laxis, prefix='axis_label_')\n    if (props['nticks'] == 0):\n        laxis.major_tick_line_color = None\n        laxis.minor_tick_line_color = None\n        laxis.major_label_text_color = None\n    ticklabels = ax.get_ticklabels()\n    if ticklabels:\n        self.text_props(ticklabels[0], laxis, prefix='major_label_')\n    if self.xkcd:\n        laxis.axis_line_width = 3\n        laxis.axis_label_text_font = 'Comic Sans MS, Textile, cursive'\n        laxis.axis_label_text_font_style = 'bold'\n        laxis.axis_label_text_color = 'black'\n        laxis.major_label_text_font = 'Comic Sans MS, Textile, cursive'\n        laxis.major_label_text_font_style = 'bold'\n        laxis.major_label_text_color = 'black'\n    return laxis\n", "label": 1}
{"function": "\n\ndef test_marginal(self, huang_darwiche_jt, huang_darwiche_dag):\n    bbn_nodes = dict([(node.name, node) for node in huang_darwiche_dag.nodes])\n    assignments = huang_darwiche_jt.assign_clusters(huang_darwiche_dag)\n    huang_darwiche_jt.initialize_potentials(assignments, huang_darwiche_dag)\n    huang_darwiche_jt.propagate()\n    p_A = huang_darwiche_jt.marginal(bbn_nodes['f_a'])\n    assert (r3(p_A[(('a', True),)]) == 0.5)\n    assert (r3(p_A[(('a', False),)]) == 0.5)\n    p_D = huang_darwiche_jt.marginal(bbn_nodes['f_d'])\n    assert (r3(p_D[(('d', True),)]) == 0.68)\n    assert (r3(p_D[(('d', False),)]) == 0.32)\n    '\\n        +------+-------+----------+\\n        | Node | Value | Marginal |\\n        +------+-------+----------+\\n        | a    | False | 0.500000 |\\n        | a    | True  | 0.500000 |\\n        | b    | False | 0.550000 |\\n        | b    | True  | 0.450000 |\\n        | c    | False | 0.550000 |\\n        | c    | True  | 0.450000 |\\n        | d    | False | 0.320000 |\\n        | d    | True  | 0.680000 |\\n        | e    | False | 0.535000 |\\n        | e    | True  | 0.465000 |\\n        | f    | False | 0.823694 |\\n        | f    | True  | 0.176306 |\\n        | g    | False | 0.585000 |\\n        | g    | True  | 0.415000 |\\n        | h    | False | 0.176900 |\\n        | h    | True  | 0.823100 |\\n        +------+-------+----------+\\n        '\n    p_B = huang_darwiche_jt.marginal(bbn_nodes['f_b'])\n    assert (r3(p_B[(('b', True),)]) == 0.45)\n    assert (r3(p_B[(('b', False),)]) == 0.55)\n    p_C = huang_darwiche_jt.marginal(bbn_nodes['f_c'])\n    assert (r3(p_C[(('c', True),)]) == 0.45)\n    assert (r3(p_C[(('c', False),)]) == 0.55)\n    p_E = huang_darwiche_jt.marginal(bbn_nodes['f_e'])\n    assert (r3(p_E[(('e', True),)]) == 0.465)\n    assert (r3(p_E[(('e', False),)]) == 0.535)\n    p_F = huang_darwiche_jt.marginal(bbn_nodes['f_f'])\n    assert (r3(p_F[(('f', True),)]) == 0.176)\n    assert (r3(p_F[(('f', False),)]) == 0.824)\n    p_G = huang_darwiche_jt.marginal(bbn_nodes['f_g'])\n    assert (r3(p_G[(('g', True),)]) == 0.415)\n    assert (r3(p_G[(('g', False),)]) == 0.585)\n    p_H = huang_darwiche_jt.marginal(bbn_nodes['f_h'])\n    assert (r3(p_H[(('h', True),)]) == 0.823)\n    assert (r3(p_H[(('h', False),)]) == 0.177)\n", "label": 1}
{"function": "\n\ndef upload_files(self, ultra):\n    hashes = self.load_hashes(self.hub_project)\n    (files_scanned, files_to_upload) = self.scan_files(hashes, ultra)\n    if self.stopped:\n        return False\n    num_files = self.num_files\n    if (num_files <= 0):\n        return True\n    boundary = gen_boundary()\n    local_deploy = (self.hub_pool.host in ['127.0.0.1', '0.0.0.0', 'localhost'])\n    try:\n        if local_deploy:\n            params = {\n                'files.path': self.get_meta_data_path(),\n                'encoding': 'gzip',\n                'project': self.hub_project,\n                'version': self.hub_version,\n                'versiontitle': self.hub_versiontitle,\n                'pluginmain': self.plugin_main,\n                'canvasmain': self.canvas_main,\n                'flashmain': self.flash_main,\n                'mappingtable': self.mapping_table,\n                'engineversion': self.engine_version,\n                'ismultiplayer': self.is_multiplayer,\n                'aspectratio': self.aspect_ratio,\n                'numfiles': str(num_files),\n                'numbytes': str(self.num_bytes),\n                'localversion': __version__,\n            }\n            r = self.hub_pool.request('POST', '/dynamic/upload/begin', fields=params, headers={\n                'Cookie': self.hub_cookie,\n            }, timeout=self.hub_timeout)\n        else:\n            r = self.post('/dynamic/upload/begin', [MultipartParam('files', filename='files.json', filetype='application/json; charset=utf-8', fileobj=open(self.get_meta_data_path(), 'rb')), ('encoding', 'gzip'), ('project', self.hub_project), ('version', self.hub_version), ('versiontitle', self.hub_versiontitle), ('pluginmain', self.plugin_main), ('canvasmain', self.canvas_main), ('flashmain', self.flash_main), ('mappingtable', self.mapping_table), ('engineversion', self.engine_version), ('ismultiplayer', self.is_multiplayer), ('aspectratio', self.aspect_ratio), ('numfiles', num_files), ('numbytes', self.num_bytes), ('localversion', __version__)], boundary)\n    except IOError:\n        self.stop(('Error opening file \"%s\".' % self.get_meta_data_path()))\n        return False\n    except (HTTPError, SSLError) as e:\n        self.stop(('Error starting upload: \"%s\".' % e))\n        return False\n    if (r.status == 504):\n        self.stop('Hub timed out.')\n        return False\n    if ((r.headers.get('content-type', '') == 'application/json; charset=utf-8') and (r.data != '')):\n        try:\n            answer = json_loads(r.data)\n        except JSONDecodeError as e:\n            LOG.error(e)\n            answer = {\n                \n            }\n    else:\n        answer = {\n            \n        }\n    if (r.status != 200):\n        msg = answer.get('msg', False)\n        if msg:\n            self.stop(msg)\n        else:\n            self.stop(('Error starting upload: \"%s\".' % r.reason))\n        return False\n    hub_session = answer.get('session', None)\n    if ((not answer.get('ok', False)) or (not hub_session)):\n        self.stop('Unsupported response format from Hub.')\n        return False\n    self.hub_session = hub_session\n    get_cached_file_name = _get_cached_file_name\n    for (file_name, file_size, file_hash) in files_scanned:\n        hashes.add(get_cached_file_name(file_name, file_hash, file_size))\n        self.uploaded_bytes += file_size\n        self.uploaded_files += 1\n    if (self.uploaded_files >= num_files):\n        self.save_hashes(hashes)\n        return True\n    uploaded_queue = Queue()\n    self.start_upload_workers(files_to_upload, uploaded_queue, boundary, local_deploy)\n    while True:\n        item = uploaded_queue.get()\n        if ((item is None) or self.stopped):\n            break\n        (file_name, file_size, file_hash) = item\n        hashes.add(get_cached_file_name(file_name, file_hash, file_size))\n        self.uploaded_bytes += file_size\n        self.uploaded_files += 1\n        if (self.uploaded_files >= num_files):\n            self.save_hashes(hashes)\n            return True\n        item = None\n    self.save_hashes(hashes)\n    return False\n", "label": 1}
{"function": "\n\ndef core_select_lib(category, llist, create_instance=False, base='kivy.core', basemodule=None):\n    if ('KIVY_DOC' in os.environ):\n        return\n    category = category.lower()\n    basemodule = (basemodule or category)\n    libs_ignored = []\n    errs = []\n    for (option, modulename, classname) in llist:\n        try:\n            try:\n                if (option not in kivy.kivy_options[category]):\n                    libs_ignored.append(modulename)\n                    Logger.debug('{0}: Provider <{1}> ignored by config'.format(category.capitalize(), option))\n                    continue\n            except KeyError:\n                pass\n            mod = __import__(name='{2}.{0}.{1}'.format(basemodule, modulename, base), globals=globals(), locals=locals(), fromlist=[modulename], level=0)\n            cls = mod.__getattribute__(classname)\n            Logger.info('{0}: Provider: {1}{2}'.format(category.capitalize(), option, ('({0} ignored)'.format(libs_ignored) if libs_ignored else '')))\n            if create_instance:\n                cls = cls()\n            return cls\n        except ImportError as e:\n            errs.append((option, e, sys.exc_info()[2]))\n            libs_ignored.append(modulename)\n            Logger.debug('{0}: Ignored <{1}> (import error)'.format(category.capitalize(), option))\n            Logger.trace('', exc_info=e)\n        except CoreCriticalException as e:\n            errs.append((option, e, sys.exc_info()[2]))\n            Logger.error('{0}: Unable to use {1}'.format(category.capitalize(), option))\n            Logger.error('{0}: The module raised an important error: {1!r}'.format(category.capitalize(), e.message))\n            raise\n        except Exception as e:\n            errs.append((option, e, sys.exc_info()[2]))\n            libs_ignored.append(modulename)\n            Logger.trace('{0}: Unable to use {1}'.format(category.capitalize(), option, category))\n            Logger.trace('', exc_info=e)\n    err = '\\n'.join(['{} - {}: {}\\n{}'.format(opt, e.__class__.__name__, e, ''.join(traceback.format_tb(tb))) for (opt, e, tb) in errs])\n    Logger.critical('{0}: Unable to find any valuable {0} provider at all!\\n{1}'.format(category.capitalize(), err))\n", "label": 1}
{"function": "\n\ndef get_current_obs(self):\n    (robot_x, robot_y) = self.wrapped_env.get_body_com('torso')[:2]\n    ori = self.wrapped_env.model.data.qpos[self.__class__.ORI_IND]\n    structure = self.__class__.MAZE_STRUCTURE\n    size_scaling = self.__class__.MAZE_SIZE_SCALING\n    segments = []\n    for i in range(len(structure)):\n        for j in range(len(structure[0])):\n            if ((structure[i][j] == 1) or (structure[i][j] == 'g')):\n                cx = ((j * size_scaling) - self._init_torso_x)\n                cy = ((i * size_scaling) - self._init_torso_y)\n                x1 = (cx - (0.5 * size_scaling))\n                x2 = (cx + (0.5 * size_scaling))\n                y1 = (cy - (0.5 * size_scaling))\n                y2 = (cy + (0.5 * size_scaling))\n                struct_segments = [((x1, y1), (x2, y1)), ((x2, y1), (x2, y2)), ((x2, y2), (x1, y2)), ((x1, y2), (x1, y1))]\n                for seg in struct_segments:\n                    segments.append(dict(segment=seg, type=structure[i][j]))\n    wall_readings = np.zeros(self._n_bins)\n    goal_readings = np.zeros(self._n_bins)\n    for ray_idx in xrange(self._n_bins):\n        ray_ori = ((ori - (self._sensor_span * 0.5)) + (((1.0 * ((2 * ray_idx) + 1)) / (2 * self._n_bins)) * self._sensor_span))\n        ray_segments = []\n        for seg in segments:\n            p = ray_segment_intersect(ray=((robot_x, robot_y), ray_ori), segment=seg['segment'])\n            if (p is not None):\n                ray_segments.append(dict(segment=seg['segment'], type=seg['type'], ray_ori=ray_ori, distance=point_distance(p, (robot_x, robot_y))))\n        if (len(ray_segments) > 0):\n            first_seg = sorted(ray_segments, key=(lambda x: x['distance']))[0]\n            if (first_seg['type'] == 1):\n                if (first_seg['distance'] <= self._sensor_range):\n                    wall_readings[ray_idx] = ((self._sensor_range - first_seg['distance']) / self._sensor_range)\n            elif (first_seg['type'] == 'g'):\n                if (first_seg['distance'] <= self._sensor_range):\n                    goal_readings[ray_idx] = ((self._sensor_range - first_seg['distance']) / self._sensor_range)\n            else:\n                assert False\n    obs = np.concatenate([self.wrapped_env.get_current_obs(), wall_readings, goal_readings])\n    return obs\n", "label": 1}
{"function": "\n\n@classmethod\ndef parse(cls, value):\n    '\\n        Parse this from a header value\\n        '\n    results = []\n    weak_results = []\n    while value:\n        if value.lower().startswith('w/'):\n            weak = True\n            value = value[2:]\n        else:\n            weak = False\n        if value.startswith('\"'):\n            try:\n                (etag, rest) = value[1:].split('\"', 1)\n            except ValueError:\n                etag = value.strip(' \",')\n                rest = ''\n            else:\n                rest = rest.strip(', ')\n        elif (',' in value):\n            (etag, rest) = value.split(',', 1)\n            rest = rest.strip()\n        else:\n            etag = value\n            rest = ''\n        if (etag == '*'):\n            return AnyETag\n        if etag:\n            if weak:\n                weak_results.append(etag)\n            else:\n                results.append(etag)\n        value = rest\n    return cls(results, weak_results)\n", "label": 0}
{"function": "\n\ndef main():\n    o = ord('x')\n    assert (o == 120)\n    n = float('1.1')\n    assert (n == 1.1)\n    n = float('NaN')\n    print(n)\n    assert (isNaN(n) == True)\n    r = round(1.1234, 2)\n    print(r)\n    assert (str(r) == '1.12')\n    x = chr(120)\n    print(x)\n    assert (x == 'x')\n    r = round(100.001, 2)\n    assert (r == 100)\n    i = int(100.1)\n    assert (i == 100)\n    r = round(5.49)\n    assert (r == 5)\n    r = round(5.49, 1)\n    assert (r == 5.5)\n", "label": 0}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('--exclude-logs', action='store_true', default=False, help=\"Don't include logs in the generated tarball\")\n    parser.add_argument('--exclude-configs', action='store_true', default=False, help=\"Don't include configs in the generated tarball\")\n    parser.add_argument('--exclude-content', action='store_true', default=False, help=\"Don't include content packs in the generated tarball\")\n    parser.add_argument('--exclude-system-info', action='store_true', default=False, help=\"Don't include system information in the generated tarball\")\n    parser.add_argument('--exclude-shell-commands', action='store_true', default=False, help=\"Don't include shell commands output in the generated tarball\")\n    parser.add_argument('--yes', action='store_true', default=False, help='Run in non-interactive mode and answer \"yes\" to all the questions')\n    parser.add_argument('--review', action='store_true', default=False, help=\"Generate the tarball, but don't encrypt and upload it\")\n    parser.add_argument('--debug', action='store_true', default=False, help='Enable debug mode')\n    parser.add_argument('--config', action='store', default=None, help='Get required configurations from config file')\n    parser.add_argument('--output', action='store', default=None, help='Specify output file path')\n    parser.add_argument('--existing-file', action='store', default=None, help='Specify an existing file to operate on')\n    args = parser.parse_args()\n    setup_logging()\n    abort = True\n    for arg_name in ARG_NAMES:\n        abort &= getattr(args, arg_name, False)\n    if abort:\n        print('Generated tarball would be empty. Aborting.')\n        sys.exit(2)\n    if args.config:\n        try:\n            with open(args.config, 'r') as yaml_file:\n                config_file = yaml.safe_load(yaml_file)\n        except Exception as e:\n            LOG.error(('Failed to parse config file: %s' % e))\n            sys.exit(1)\n        if (not isinstance(config_file, dict)):\n            LOG.error('Unrecognized config file format')\n            sys.exit(1)\n    else:\n        config_file = {\n            \n        }\n    company_name = config_file.get('company_name', COMPANY_NAME)\n    encrypt = True\n    upload = True\n    if args.review:\n        encrypt = False\n        upload = False\n    if encrypt:\n        if (not GPG_INSTALLED):\n            msg = '\"gpg\" binary not found, can\\'t proceed. Make sure \"gpg\" is installed and available in PATH.'\n            raise ValueError(msg)\n    if ((not args.yes) and (not args.existing_file) and upload):\n        submitted_content = [name.replace('exclude_', '') for name in ARG_NAMES if (not getattr(args, name, False))]\n        submitted_content = ', '.join(submitted_content)\n        print(('This will submit the following information to %s: %s' % (company_name, submitted_content)))\n        value = six.moves.input('Are you sure you want to proceed? [y/n] ')\n        if (value.strip().lower() not in ['y', 'yes']):\n            print('Aborting')\n            sys.exit(1)\n    user_info = {\n        \n    }\n    if ((not args.yes) and (not args.existing_file)):\n        print('If you want us to get back to you via email, you can provide additional context such as your name, email and an optional comment')\n        value = six.moves.input('Would you like to provide additional context? [y/n] ')\n        if (value.strip().lower() in ['y', 'yes']):\n            user_info['name'] = six.moves.input('Name: ')\n            user_info['email'] = six.moves.input('Email: ')\n            user_info['comment'] = six.moves.input('Comment: ')\n    debug_collector = DebugInfoCollector(include_logs=(not args.exclude_logs), include_configs=(not args.exclude_configs), include_content=(not args.exclude_content), include_system_info=(not args.exclude_system_info), include_shell_commands=(not args.exclude_shell_commands), user_info=user_info, debug=args.debug, config_file=config_file, output_path=args.output)\n    debug_collector.run(encrypt=encrypt, upload=upload, existing_file=args.existing_file)\n", "label": 1}
{"function": "\n\ndef manufacturer_products(request, slug, start=1, template_name='lfs/manufacturers/products.html'):\n    'Displays the products of the manufacturer with passed slug.\\n    '\n    last_manufacturer = request.session.get('last_manufacturer')\n    if ((last_manufacturer is None) or (last_manufacturer.slug != slug)):\n        if ('product-filter' in request.session):\n            del request.session['product-filter']\n        if ('price-filter' in request.session):\n            del request.session['price-filter']\n    try:\n        default_sorting = settings.LFS_PRODUCTS_SORTING\n    except AttributeError:\n        default_sorting = 'price'\n    sorting = request.session.get('sorting', default_sorting)\n    product_filter = request.session.get('product-filter', {\n        \n    })\n    product_filter = product_filter.items()\n    cache_key = ('%s-manufacturer-products-%s' % (settings.CACHE_MIDDLEWARE_KEY_PREFIX, slug))\n    sub_cache_key = ('%s-start-%s-sorting-%s' % (settings.CACHE_MIDDLEWARE_KEY_PREFIX, start, sorting))\n    filter_key = [('%s-%s' % (i[0], i[1])) for i in product_filter]\n    if filter_key:\n        sub_cache_key += ('-%s' % '-'.join(filter_key))\n    price_filter = request.session.get('price-filter')\n    if price_filter:\n        sub_cache_key += ('-%s-%s' % (price_filter['min'], price_filter['max']))\n    temp = cache.get(cache_key)\n    if (temp is not None):\n        try:\n            return temp[sub_cache_key]\n        except KeyError:\n            pass\n    else:\n        temp = dict()\n    manufacturer = lfs_get_object_or_404(Manufacturer, slug=slug)\n    try:\n        start = int(start)\n    except (ValueError, TypeError):\n        start = 1\n    format_info = manufacturer.get_format_info()\n    amount_of_rows = format_info['product_rows']\n    amount_of_cols = format_info['product_cols']\n    amount = (amount_of_rows * amount_of_cols)\n    all_products = manufacturer.get_filtered_products(product_filter, price_filter, sorting)\n    paginator = Paginator(all_products, amount)\n    try:\n        current_page = paginator.page(start)\n    except (EmptyPage, InvalidPage):\n        current_page = paginator.page(paginator.num_pages)\n    row = []\n    products = []\n    for (i, product) in enumerate(current_page.object_list):\n        if product.is_product_with_variants():\n            default_variant = product.get_default_variant()\n            if default_variant:\n                product = default_variant\n        image = None\n        product_image = product.get_image()\n        if product_image:\n            image = product_image.image\n        row.append({\n            'obj': product,\n            'slug': product.slug,\n            'name': product.get_name(),\n            'image': image,\n            'price_unit': product.price_unit,\n            'price_includes_tax': product.price_includes_tax(request),\n        })\n        if (((i + 1) % amount_of_cols) == 0):\n            products.append(row)\n            row = []\n    if (len(row) > 0):\n        products.append(row)\n    amount_of_products = all_products.count()\n    pagination_data = lfs_pagination(request, current_page, url=manufacturer.get_absolute_url())\n    pagination_data['total_text'] = (ungettext('%(count)d product', '%(count)d products', amount_of_products) % {\n        'count': amount_of_products,\n    })\n    result = render_to_string(template_name, RequestContext(request, {\n        'manufacturer': manufacturer,\n        'products': products,\n        'amount_of_products': amount_of_products,\n        'pagination': pagination_data,\n        'all_products': all_products,\n    }))\n    temp[sub_cache_key] = result\n    cache.set(cache_key, temp)\n    return result\n", "label": 1}
{"function": "\n\ndef compress(self, node):\n    type = node.type\n    result = None\n    if (type in self.__simple):\n        result = type\n    elif (type in self.__prefixes):\n        if getattr(node, 'postfix', False):\n            result = (self.compress(node[0]) + self.__prefixes[node.type])\n        else:\n            result = (self.__prefixes[node.type] + self.compress(node[0]))\n    elif (type in self.__dividers):\n        first = self.compress(node[0])\n        second = self.compress(node[1])\n        divider = self.__dividers[node.type]\n        if (node.type not in ('plus', 'minus')):\n            result = ('%s%s%s' % (first, divider, second))\n        else:\n            result = first\n            if first.endswith(divider):\n                result += ' '\n            result += divider\n            if second.startswith(divider):\n                result += ' '\n            result += second\n    else:\n        try:\n            result = getattr(self, ('type_%s' % type))(node)\n        except KeyError:\n            print((\"Compressor does not support type '%s' from line %s in file %s\" % (type, node.line, node.getFileName())))\n            sys.exit(1)\n    if getattr(node, 'parenthesized', None):\n        return ('(%s)' % result)\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef _query(self, session, keyspace, count=12, consistency_level=ConsistencyLevel.ONE, use_prepared=False):\n    if use_prepared:\n        query_string = ('SELECT * FROM %s.cf WHERE k = ?' % keyspace)\n        if ((not self.prepared) or (self.prepared.query_string != query_string)):\n            self.prepared = session.prepare(query_string)\n            self.prepared.consistency_level = consistency_level\n        for i in range(count):\n            tries = 0\n            while True:\n                if (tries > 100):\n                    raise RuntimeError('Failed to execute query after 100 attempts: {0}'.format(self.prepared))\n                try:\n                    self.coordinator_stats.add_coordinator(session.execute_async(self.prepared.bind((0,))))\n                    break\n                except (OperationTimedOut, ReadTimeout, ReadFailure):\n                    (ex_type, ex, tb) = sys.exc_info()\n                    log.warn('{0}: {1} Backtrace: {2}'.format(ex_type.__name__, ex, traceback.extract_tb(tb)))\n                    del tb\n                    tries += 1\n    else:\n        routing_key = struct.pack('>i', 0)\n        for i in range(count):\n            ss = SimpleStatement(('SELECT * FROM %s.cf WHERE k = 0' % keyspace), consistency_level=consistency_level, routing_key=routing_key)\n            tries = 0\n            while True:\n                if (tries > 100):\n                    raise RuntimeError('Failed to execute query after 100 attempts: {0}'.format(ss))\n                try:\n                    self.coordinator_stats.add_coordinator(session.execute_async(ss))\n                    break\n                except (OperationTimedOut, ReadTimeout, ReadFailure):\n                    (ex_type, ex, tb) = sys.exc_info()\n                    log.warn('{0}: {1} Backtrace: {2}'.format(ex_type.__name__, ex, traceback.extract_tb(tb)))\n                    del tb\n                    tries += 1\n", "label": 1}
{"function": "\n\ndef set_etcd_facts_if_unset(facts):\n    '\\n    If using embedded etcd, loads the data directory from master-config.yaml.\\n\\n    If using standalone etcd, loads ETCD_DATA_DIR from etcd.conf.\\n\\n    If anything goes wrong parsing these, the fact will not be set.\\n    '\n    if (('master' in facts) and facts['master']['embedded_etcd']):\n        etcd_facts = (facts['etcd'] if ('etcd' in facts) else dict())\n        if ('etcd_data_dir' not in etcd_facts):\n            try:\n                master_cfg_path = os.path.join(facts['common']['config_base'], 'master/master-config.yaml')\n                master_cfg_f = open(master_cfg_path, 'r')\n                config = yaml.safe_load(master_cfg_f.read())\n                master_cfg_f.close()\n                etcd_facts['etcd_data_dir'] = config['etcdConfig']['storageDirectory']\n                facts['etcd'] = etcd_facts\n            except Exception:\n                pass\n    else:\n        etcd_facts = (facts['etcd'] if ('etcd' in facts) else dict())\n        try:\n            ini_str = ('[root]\\n' + open('/etc/etcd/etcd.conf', 'r').read())\n            ini_fp = StringIO.StringIO(ini_str)\n            config = ConfigParser.RawConfigParser()\n            config.readfp(ini_fp)\n            etcd_data_dir = config.get('root', 'ETCD_DATA_DIR')\n            if (etcd_data_dir.startswith('\"') and etcd_data_dir.endswith('\"')):\n                etcd_data_dir = etcd_data_dir[1:(- 1)]\n            etcd_facts['etcd_data_dir'] = etcd_data_dir\n            facts['etcd'] = etcd_facts\n        except Exception:\n            pass\n    return facts\n", "label": 0}
{"function": "\n\ndef post(self, slug, filename):\n    '\\n        Saves given contents to file to game folder.\\n        '\n    game = get_game_by_slug(slug)\n    if (not game):\n        self.set_status(404)\n        return self.finish({\n            'ok': False,\n            'msg': ('Game does not exist: %s' % slug),\n        })\n    if (not filename):\n        self.set_status(400)\n        return self.finish({\n            'ok': False,\n            'msg': 'Missing filename',\n        })\n    if ('..' in filename):\n        self.set_status(403)\n        return self.finish({\n            'ok': False,\n            'msg': 'Cannot write outside game folder',\n        })\n    content_type = self.request.headers.get('Content-Type', '')\n    if (content_type and ('application/x-www-form-urlencoded' in content_type)):\n        content = self.get_argument('content')\n        binary = False\n    else:\n        content = self.request.body\n        binary = True\n    self.request.body = None\n    self.request.arguments = None\n    file_path = path_join(get_absolute_path(game.get_path()), normpath(filename))\n    file_dir = dirname(file_path)\n    if (not create_dir(file_dir)):\n        LOG.error('Failed to create directory at \"%s\"', file_dir)\n        self.set_status(500)\n        return self.finish({\n            'ok': False,\n            'msg': 'Failed to create directory',\n        })\n    if content:\n        if (not binary):\n            try:\n                content = content.encode('utf-8')\n            except UnicodeEncodeError as e:\n                LOG.error('Failed to encode file contents: %s', str(e))\n                self.set_status(500)\n                return self.finish({\n                    'ok': False,\n                    'msg': 'Failed to encode file contents',\n                })\n        LOG.info('Writing file at \"%s\" (%d bytes)', file_path, len(content))\n    else:\n        LOG.info('Writing empty file at \"%s\"', file_path)\n    try:\n        file_obj = open(file_path, 'wb')\n        try:\n            file_obj.write(content)\n        finally:\n            file_obj.close()\n    except IOError as e:\n        LOG.error('Failed to write file at \"%s\": %s', file_path, str(e))\n        self.set_status(500)\n        return self.finish({\n            'ok': False,\n            'msg': 'Failed to write file',\n        })\n    return self.finish({\n        'ok': True,\n    })\n", "label": 1}
{"function": "\n\ndef rebalance(self):\n    \"Figure out which brokers and partitions we should be consuming from,\\n        based on the latest information about the other consumers and brokers\\n        that are present.\\n\\n        We registered for notifications from ZooKeeper whenever a broker or \\n        consumer enters or leaves the pool. But we usually only rebalance right\\n        before we're about to take an action like fetching.\\n\\n        The rebalancing algorithm is slightly different from that described in\\n        the design doc (mostly in the sense that the design doc algorithm will\\n        leave partitions unassigned if there's an uneven distributions). The \\n        idea is that we split the partitions as evently as possible, and if\\n        some consumers need to have more partitions than others, the extra \\n        partitions always go to the earlier consumers in the list. So you could\\n        have a distribution like 4-4-4-4 or 5-5-4-4, but never 4-4-4-5.\\n\\n        Rebalancing has special consequences if the Consumer is doing manual \\n        commits (autocommit=False):\\n\\n        1. This Consumer will keep using the in memory offset state for all \\n           BrokerPartitions that it was already following before the rebalance.\\n        2. The offset state for any new BrokerPartitions that this Consumer is\\n           responsible for after the rebalance will be read from ZooKeeper.\\n        3. For those BrokerPartitions that this Consumer was reading but is no\\n           longer responsible for after the rebalance, the offset state is \\n           simply discarded. It is not persisted to ZooKeeper.\\n        \\n        So there is no guarantee of single delivery in this circumstance. If \\n        BrokerPartition 1-0 shifts ownership from Consumer A to Consumer B in \\n        the rebalance, Consumer B will pick up from the last manual commit of \\n        Consumer A -- *not* the offset that Consumer A was at when the rebalance\\n        was triggered.\\n        \"\n    log.info(('Rebalance triggered for Consumer {0}, broker partitions ' + 'before rebalance: {1}').format(self.id, unicode(self)))\n    if (not self._rebalance_enabled):\n        log.info('Rebalancing disabled -- ignoring rebalance request')\n        return\n    all_topic_consumers = self._zk_util.consumer_ids_for(self.topic, self.consumer_group)\n    all_broker_partitions = self._zk_util.broker_partitions_for(self.topic)\n    try:\n        my_index = all_topic_consumers.index(self.id)\n    except ValueError:\n        msg_tmpl = ('This consumer ({0}) not found list of consumers ' + 'for this topic {1}: {2}')\n        raise ConsumerEntryNotFoundError(msg_tmpl.format(self.id, self.topic, all_topic_consumers))\n    bp_per_consumer = (len(all_broker_partitions) / len(all_topic_consumers))\n    consumers_with_extra = range((len(all_broker_partitions) % len(all_topic_consumers)))\n    num_parts = (bp_per_consumer + (1 if (my_index in consumers_with_extra) else 0))\n    if (my_index == 0):\n        start = 0\n    elif ((my_index - 1) in consumers_with_extra):\n        start = (my_index * (bp_per_consumer + 1))\n    else:\n        start = ((len(consumers_with_extra) * (bp_per_consumer + 1)) + ((my_index - len(consumers_with_extra)) * bp_per_consumer))\n    self._broker_partitions = all_broker_partitions[start:(start + num_parts)]\n    for bp in self._bps_to_next_offsets.keys():\n        if (bp not in self._broker_partitions):\n            del self._bps_to_next_offsets[bp]\n    for bp in self._broker_partitions:\n        self._bps_to_next_offsets.setdefault(bp, None)\n    broker_conn_info = frozenset(((bp.broker_id, bp.host, bp.port) for bp in self._broker_partitions))\n    self._connections = dict(((broker_id, Kafka(host, port)) for (broker_id, host, port) in broker_conn_info))\n    self._register_callbacks()\n    if self._all_callbacks_registered():\n        self._needs_rebalance = False\n    log.info('Rebalance finished for Consumer {0}: {1}'.format(self.id, unicode(self)))\n", "label": 1}
{"function": "\n\ndef perform_job(self, job):\n    ' Wraps a job.perform() call with timeout logic and exception handlers.\\n\\n            This is the first call happening inside the greenlet.\\n        '\n    if self.config['trace_memory']:\n        job.trace_memory_start()\n    set_current_job(job)\n    gevent_timeout = None\n    if job.timeout:\n        gevent_timeout = gevent.Timeout(job.timeout, TimeoutInterrupt(('Job exceeded maximum timeout value in greenlet (%d seconds).' % job.timeout)))\n        gevent_timeout.start()\n    try:\n        job.perform()\n    except RetryInterrupt:\n        self.log.error('Caught retry')\n        job.save_retry(sys.exc_info()[1])\n    except MaxRetriesInterrupt:\n        self.log.error('Max retries reached')\n        job._save_status('maxretries', exception=True)\n    except AbortInterrupt:\n        self.log.error('Caught abort')\n        job.save_abort()\n    except TimeoutInterrupt:\n        self.log.error(('Job timeouted after %s seconds' % job.timeout))\n        job._save_status('timeout', exception=True)\n    except JobInterrupt:\n        self.log.error('Job interrupted')\n        job._save_status('interrupt', exception=True)\n    except Exception:\n        self.log.error('Job failed')\n        job._save_status('failed', exception=True)\n    finally:\n        if gevent_timeout:\n            gevent_timeout.cancel()\n        set_current_job(None)\n        self.done_jobs += 1\n        if self.config['trace_memory']:\n            job.trace_memory_stop()\n", "label": 1}
{"function": "\n\n@defun\ndef ellipfun(ctx, kind, u=None, m=None, q=None, k=None, tau=None):\n    try:\n        S = jacobi_spec[kind]\n    except KeyError:\n        raise ValueError(\"First argument must be a two-character string containing 's', 'c', 'd' or 'n', e.g.: 'sn'\")\n    if (u is None):\n\n        def f(*args, **kwargs):\n            return ctx.ellipfun(kind, *args, **kwargs)\n        f.__name__ = kind\n        return f\n    prec = ctx.prec\n    try:\n        ctx.prec += 10\n        u = ctx.convert(u)\n        q = ctx.qfrom(m=m, q=q, k=k, tau=tau)\n        if (S is None):\n            v = (ctx.one + ((0 * q) * u))\n        elif (q == ctx.zero):\n            if (S[4] == '1'):\n                v = ctx.one\n            else:\n                v = getattr(ctx, S[4])(u)\n            v += ((0 * q) * u)\n        elif (q == ctx.one):\n            if (S[5] == '1'):\n                v = ctx.one\n            else:\n                v = getattr(ctx, S[5])(u)\n            v += ((0 * q) * u)\n        else:\n            t = (u / (ctx.jtheta(3, 0, q) ** 2))\n            v = ctx.one\n            for a in S[0]:\n                v *= ctx.jtheta(a, 0, q)\n            for b in S[1]:\n                v /= ctx.jtheta(b, 0, q)\n            for c in S[2]:\n                v *= ctx.jtheta(c, t, q)\n            for d in S[3]:\n                v /= ctx.jtheta(d, t, q)\n    finally:\n        ctx.prec = prec\n    return (+ v)\n", "label": 1}
{"function": "\n\ndef indexCube(nodes, gridSize, n=None):\n    \"\\n    Returns the index of nodes on the mesh.\\n\\n\\n    Input:\\n       nodes     - string of which nodes to return. e.g. 'ABCD'\\n       gridSize  - size of the nodal grid\\n       n         - number of nodes each i,j,k direction: [ni,nj,nk]\\n\\n\\n    Output:\\n       index  - index in the order asked e.g. 'ABCD' --> (A,B,C,D)\\n\\n    TWO DIMENSIONS::\\n\\n      node(i,j)          node(i,j+1)\\n           A -------------- B\\n           |                |\\n           |    cell(i,j)   |\\n           |        I       |\\n           |                |\\n          D -------------- C\\n      node(i+1,j)        node(i+1,j+1)\\n\\n\\n    THREE DIMENSIONS::\\n\\n            node(i,j,k+1)       node(i,j+1,k+1)\\n                E --------------- F\\n               /|               / |\\n              / |              /  |\\n             /  |             /   |\\n      node(i,j,k)         node(i,j+1,k)\\n           A -------------- B     |\\n           |    H ----------|---- G\\n           |   /cell(i,j)   |   /\\n           |  /     I       |  /\\n           | /              | /\\n           D -------------- C\\n      node(i+1,j,k)      node(i+1,j+1,k)\\n\\n    \"\n    assert (type(nodes) == str), \"Nodes must be a str variable: e.g. 'ABCD'\"\n    assert isinstance(gridSize, np.ndarray), 'Number of nodes must be an ndarray'\n    nodes = nodes.upper()\n    possibleNodes = ('ABCD' if (gridSize.size == 2) else 'ABCDEFGH')\n    for node in nodes:\n        assert (node in possibleNodes), (\"Nodes must be chosen from: '%s'\" % possibleNodes)\n    dim = gridSize.size\n    if (n is None):\n        n = (gridSize - 1)\n    if (dim == 2):\n        ij = ndgrid(np.arange(n[0]), np.arange(n[1]))\n        (i, j) = (ij[:, 0], ij[:, 1])\n    elif (dim == 3):\n        ijk = ndgrid(np.arange(n[0]), np.arange(n[1]), np.arange(n[2]))\n        (i, j, k) = (ijk[:, 0], ijk[:, 1], ijk[:, 2])\n    else:\n        raise Exception('Only 2 and 3 dimensions supported.')\n    nodeMap = {\n        'A': [0, 0, 0],\n        'B': [0, 1, 0],\n        'C': [1, 1, 0],\n        'D': [1, 0, 0],\n        'E': [0, 0, 1],\n        'F': [0, 1, 1],\n        'G': [1, 1, 1],\n        'H': [1, 0, 1],\n    }\n    out = ()\n    for node in nodes:\n        shift = nodeMap[node]\n        if (dim == 2):\n            out += (sub2ind(gridSize, np.c_[((i + shift[0]), (j + shift[1]))]).flatten(),)\n        elif (dim == 3):\n            out += (sub2ind(gridSize, np.c_[((i + shift[0]), (j + shift[1]), (k + shift[2]))]).flatten(),)\n    return out\n", "label": 1}
{"function": "\n\ndef _delete_node(self, nid):\n    ' Deletes a specified tree node and all its children.\\n        '\n    for cnid in self._nodes_for(nid):\n        self._delete_node(cnid)\n    pnid = nid.parent()\n    if ((pnid is not None) and (getattr(pnid, '_dummy', None) is nid)):\n        pnid.removeChild(nid)\n        del pnid._dummy\n        return\n    try:\n        (expanded, node, object) = self._get_node_data(nid)\n    except AttributeError:\n        pass\n    else:\n        id_object = id(object)\n        object_info = self._map[id_object]\n        for (i, info) in enumerate(object_info):\n            if (id(nid) == id(info[1])):\n                del object_info[i]\n                break\n        if (len(object_info) == 0):\n            self._remove_listeners(node, object)\n            del self._map[id_object]\n    if (pnid is None):\n        self._tree.takeTopLevelItem(self._tree.indexOfTopLevelItem(nid))\n    else:\n        pnid.removeChild(nid)\n    if ((self._editor is not None) and (id(nid) == id(self._editor._editor_nid))):\n        self._clear_editor()\n", "label": 1}
{"function": "\n\ndef begin_site(self):\n    for node in self.site.content.walk():\n        for resource in node.resources:\n            created = None\n            modified = None\n            try:\n                created = resource.meta.created\n                modified = resource.meta.modified\n            except AttributeError:\n                pass\n            if ((created != self.vcs_name) and (modified != self.vcs_name)):\n                continue\n            (date_created, date_modified) = self.get_dates(resource)\n            if (created == 'git'):\n                created = (date_created or datetime.utcfromtimestamp(os.path.getctime(resource.path)))\n                created = created.replace(tzinfo=None)\n                resource.meta.created = created\n            if (modified == 'git'):\n                modified = (date_modified or resource.source.last_modified)\n                modified = modified.replace(tzinfo=None)\n                resource.meta.modified = modified\n", "label": 0}
{"function": "\n\ndef test_repeating(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, repeat=True, _time_function=time)\n    time.time = 7\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 0)\n    assert (timer.sleep_time() == 3)\n    time.time = 34\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 6)\n    time.time = 40\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 2)\n    assert (timer.sleep_time() == 10)\n", "label": 0}
{"function": "\n\ndef user(self, login, password, firstName=None, lastName=None, email=None, admin=False):\n    if (self.module.params['state'] == 'present'):\n        for (var_name, var) in [('firstName', firstName), ('lastName', lastName), ('email', email)]:\n            if (var is None):\n                self.fail(\"{} must be set if state is 'present'\".format(var_name))\n        try:\n            ret = self.authenticate(username=login, password=password)\n            me = self.get('user/me')\n            updateable = ['firstName', 'lastName', 'email', 'admin']\n            passed_in = [firstName, lastName, email, admin]\n            if (set([(k, v) for (k, v) in me.items() if (k in updateable)]) ^ set(zip(updateable, passed_in))):\n                self.put('user/{}'.format(me['_id']), parameters={\n                    'login': login,\n                    'firstName': firstName,\n                    'lastName': lastName,\n                    'password': password,\n                    'email': email,\n                    'admin': ('true' if admin else 'false'),\n                })\n                self.changed = True\n        except AuthenticationError:\n            ret = self.post('user', parameters={\n                'login': login,\n                'firstName': firstName,\n                'lastName': lastName,\n                'password': password,\n                'email': email,\n                'admin': ('true' if admin else 'false'),\n            })\n            self.changed = True\n    elif (self.module.params['state'] == 'absent'):\n        ret = []\n        try:\n            ret = self.authenticate(username=login, password=password)\n            me = self.get('user/me')\n            self.delete('user/{}'.format(me['_id']))\n            self.changed = True\n        except AuthenticationError:\n            ret = []\n    return ret\n", "label": 1}
{"function": "\n\ndef update_contenttypes(app, created_models, verbosity=2, db=DEFAULT_DB_ALIAS, **kwargs):\n    '\\n    Creates content types for models in the given app, removing any model\\n    entries that no longer have a matching model class.\\n    '\n    try:\n        get_model('contenttypes', 'ContentType')\n    except UnavailableApp:\n        return\n    if (not router.allow_syncdb(db, ContentType)):\n        return\n    ContentType.objects.clear_cache()\n    app_models = get_models(app)\n    if (not app_models):\n        return\n    app_label = app_models[0]._meta.app_label\n    app_models = dict(((model._meta.model_name, model) for model in app_models))\n    content_types = dict(((ct.model, ct) for ct in ContentType.objects.using(db).filter(app_label=app_label)))\n    to_remove = [ct for (model_name, ct) in six.iteritems(content_types) if (model_name not in app_models)]\n    cts = [ContentType(name=smart_text(model._meta.verbose_name_raw), app_label=app_label, model=model_name) for (model_name, model) in six.iteritems(app_models) if (model_name not in content_types)]\n    ContentType.objects.using(db).bulk_create(cts)\n    if (verbosity >= 2):\n        for ct in cts:\n            print((\"Adding content type '%s | %s'\" % (ct.app_label, ct.model)))\n    if to_remove:\n        if kwargs.get('interactive', False):\n            content_type_display = '\\n'.join([('    %s | %s' % (ct.app_label, ct.model)) for ct in to_remove])\n            ok_to_delete = input((\"The following content types are stale and need to be deleted:\\n\\n%s\\n\\nAny objects related to these content types by a foreign key will also\\nbe deleted. Are you sure you want to delete these content types?\\nIf you're unsure, answer 'no'.\\n\\n    Type 'yes' to continue, or 'no' to cancel: \" % content_type_display))\n        else:\n            ok_to_delete = False\n        if (ok_to_delete == 'yes'):\n            for ct in to_remove:\n                if (verbosity >= 2):\n                    print((\"Deleting stale content type '%s | %s'\" % (ct.app_label, ct.model)))\n                ct.delete()\n        elif (verbosity >= 2):\n            print('Stale content types remain.')\n", "label": 1}
{"function": "\n\ndef read(self, size=1):\n    '        Read size bytes from the serial port. If a timeout is set it may\\n        return less characters as requested. With no timeout it will block\\n        until the requested number of bytes is read.\\n        '\n    if (not self.is_open):\n        raise portNotOpenError\n    if ((self._timeout is not None) and (self._timeout != 0)):\n        timeout = (time.time() + self._timeout)\n    else:\n        timeout = None\n    data = bytearray()\n    while ((size > 0) and self.is_open):\n        try:\n            b = self.queue.get(timeout=self._timeout)\n        except queue.Empty:\n            if (self._timeout == 0):\n                break\n        else:\n            if (data is not None):\n                data += b\n                size -= 1\n            else:\n                break\n        if (timeout and (time.time() > timeout)):\n            if self.logger:\n                self.logger.info('read timeout')\n            break\n    return bytes(data)\n", "label": 1}
{"function": "\n\ndef test_SeqAdd():\n    per = SeqPer((1, 2, 3), (n, 0, oo))\n    form = SeqFormula((n ** 2))\n    per_bou = SeqPer((1, 2), (n, 1, 5))\n    form_bou = SeqFormula((n ** 2), (6, 10))\n    form_bou2 = SeqFormula((n ** 2), (1, 5))\n    assert (SeqAdd() == S.EmptySequence)\n    assert (SeqAdd(S.EmptySequence) == S.EmptySequence)\n    assert (SeqAdd(per) == per)\n    assert (SeqAdd(per, S.EmptySequence) == per)\n    assert (SeqAdd(per_bou, form_bou) == S.EmptySequence)\n    s = SeqAdd(per_bou, form_bou2, evaluate=False)\n    assert (s.args == (form_bou2, per_bou))\n    assert (s[:] == [2, 6, 10, 18, 26])\n    assert (list(s) == [2, 6, 10, 18, 26])\n    assert isinstance(SeqAdd(per, per_bou, evaluate=False), SeqAdd)\n    s1 = SeqAdd(per, per_bou)\n    assert isinstance(s1, SeqPer)\n    assert (s1 == SeqPer((2, 4, 4, 3, 3, 5), (n, 1, 5)))\n    s2 = SeqAdd(form, form_bou)\n    assert isinstance(s2, SeqFormula)\n    assert (s2 == SeqFormula((2 * (n ** 2)), (6, 10)))\n    assert (SeqAdd(form, form_bou, per) == SeqAdd(per, SeqFormula((2 * (n ** 2)), (6, 10))))\n    assert (SeqAdd(form, SeqAdd(form_bou, per)) == SeqAdd(per, SeqFormula((2 * (n ** 2)), (6, 10))))\n    assert (SeqAdd(per, SeqAdd(form, form_bou), evaluate=False) == SeqAdd(per, SeqFormula((2 * (n ** 2)), (6, 10))))\n    assert (SeqAdd(SeqPer((1, 2), (n, 0, oo)), SeqPer((1, 2), (m, 0, oo))) == SeqPer((2, 4), (n, 0, oo)))\n", "label": 1}
{"function": "\n\ndef test_goaa_happy_path(self):\n    (parser, opts, files) = self._run_check(['../gristle_file_converter.py', 'census.csv', '-d', ',', '-D', '|'], 'pass')\n    assert (opts.output is None)\n    assert (opts.recdelimiter is None)\n    assert (opts.delimiter == ',')\n    assert (opts.out_delimiter == '|')\n    assert (opts.recdelimiter is None)\n    assert (opts.out_recdelimiter is None)\n    assert (opts.quoting is False)\n    assert (opts.out_quoting is False)\n    assert (opts.quotechar == '\"')\n    assert (opts.hasheader is False)\n    assert (opts.out_hasheader is False)\n    assert (opts.stripfields is False)\n    assert (opts.verbose is True)\n    self._run_check(['../gristle_file_converter.py', 'census4.csv', '-D', '|'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census5.csv', '-d', ',', '-D', '|'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-d', ',', '-D', '|', '--hasheader', '--outhasheader'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-D', '|', '-r', '-R', '-q', '-Q', '--quotechar', '^'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-D', '|', '--stripfields'], 'pass')\n", "label": 1}
{"function": "\n\ndef assert_warns_message(warning_class, message, func, *args, **kw):\n    'Test that a certain warning occurs and with a certain message.\\n\\n    Parameters\\n    ----------\\n    warning_class : the warning class\\n        The class to test for, e.g. UserWarning.\\n\\n    message : str | callable\\n        The entire message or a substring to  test for. If callable,\\n        it takes a string as argument and will trigger an assertion error\\n        if it returns `False`.\\n\\n    func : callable\\n        Calable object to trigger warnings.\\n\\n    *args : the positional arguments to `func`.\\n\\n    **kw : the keyword arguments to `func`.\\n\\n    Returns\\n    -------\\n\\n    result : the return value of `func`\\n\\n    '\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            warnings.simplefilter('ignore', np.VisibleDeprecationWarning)\n        result = func(*args, **kw)\n        if (not (len(w) > 0)):\n            raise AssertionError(('No warning raised when calling %s' % func.__name__))\n        found = [issubclass(warning.category, warning_class) for warning in w]\n        if (not any(found)):\n            raise AssertionError(('No warning raised for %s with class %s' % (func.__name__, warning_class)))\n        message_found = False\n        for index in [i for (i, x) in enumerate(found) if x]:\n            msg = w[index].message\n            msg = str((msg.args[0] if hasattr(msg, 'args') else msg))\n            if callable(message):\n                check_in_message = message\n            else:\n                check_in_message = (lambda msg: (message in msg))\n            if check_in_message(msg):\n                message_found = True\n                break\n        if (not message_found):\n            raise AssertionError((\"Did not receive the message you expected ('%s') for <%s>, got: '%s'\" % (message, func.__name__, msg)))\n    return result\n", "label": 1}
{"function": "\n\ndef ensure_domain(name, purgeable=False, inherit_soa=False, force=False):\n    \"This function will take ``domain_name`` and make sure that a domain with\\n    that name exists. If this function creates a domain it will set the\\n    domain's purgeable flag to the value of the named arguement ``purgeable``.\\n    See the doc page about Labels and Domains for more information about this\\n    function\"\n    try:\n        domain = Domain.objects.get(name=name)\n        return domain\n    except ObjectDoesNotExist:\n        pass\n    parts = list(reversed(name.split('.')))\n    if (not force):\n        domain_name = ''\n        leaf_domain = None\n        for i in range(len(parts)):\n            domain_name = ((parts[i] + '.') + domain_name)\n            domain_name = domain_name.strip('.')\n            try:\n                tmp_domain = Domain.objects.get(name=domain_name)\n                leaf_domain = tmp_domain\n            except ObjectDoesNotExist:\n                continue\n        if (not leaf_domain):\n            raise ValidationError('Creating this record would cause the creation of a new TLD. Please contact http://www.icann.org/ for more information.')\n        if leaf_domain.delegated:\n            raise ValidationError('Creating this record would cause the creation of a domain that would be a child of a delegated domain.')\n        if (not leaf_domain.soa):\n            raise ValidationError('Creating this record would cause the creation of a domain that would not be in an existing DNS zone.')\n    domain_name = ''\n    for i in range(len(parts)):\n        domain_name = ((parts[i] + '.') + domain_name)\n        domain_name = domain_name.strip('.')\n        clobber_objects = get_clobbered(domain_name)\n        (domain, created) = Domain.objects.get_or_create(name=domain_name)\n        if (purgeable and created):\n            domain.purgeable = True\n            domain.save()\n        if (inherit_soa and created and domain.master_domain and (domain.master_domain.soa is not None)):\n            domain.soa = domain.master_domain.soa\n            domain.save()\n        for (object_, views) in clobber_objects:\n            try:\n                object_.domain = domain\n                object_.clean()\n                object_.save()\n                for view_name in views:\n                    view = View.objects.get(name=view_name)\n                    object_.views.add(view)\n                    object_.save()\n            except ValidationError:\n                pdb.set_trace()\n                pass\n    return domain\n", "label": 1}
{"function": "\n\ndef dataReceived(self, data):\n    chunk = StringIO()\n    for char in data:\n        if self.gotIAC:\n            if self.iacByte:\n                if (self.iacByte == SB):\n                    if (char == SE):\n                        self.iacSBchunk(chunk.getvalue())\n                        chunk = StringIO()\n                        del self.iacByte\n                        del self.gotIAC\n                    else:\n                        chunk.write(char)\n                else:\n                    try:\n                        getattr(self, ('iac_%s' % iacBytes[self.iacByte]))(char)\n                    except KeyError:\n                        pass\n                    del self.iacByte\n                    del self.gotIAC\n            else:\n                self.iacByte = char\n        elif (char == IAC):\n            c = chunk.getvalue()\n            if c:\n                why = self.processChunk(c)\n                if why:\n                    return why\n                chunk = StringIO()\n            self.gotIAC = 1\n        else:\n            chunk.write(char)\n    c = chunk.getvalue()\n    if c:\n        why = self.processChunk(c)\n        if why:\n            return why\n", "label": 1}
{"function": "\n\ndef _linkcode_resolve(domain, info, package, url_fmt, revision):\n    \"Determine a link to online source for a class/method/function\\n\\n    This is called by sphinx.ext.linkcode\\n\\n    An example with a long-untouched module that everyone has\\n    >>> _linkcode_resolve('py', {'module': 'tty',\\n    ...                          'fullname': 'setraw'},\\n    ...                   package='tty',\\n    ...                   url_fmt='http://hg.python.org/cpython/file/'\\n    ...                           '{revision}/Lib/{package}/{path}#L{lineno}',\\n    ...                   revision='xxxx')\\n    'http://hg.python.org/cpython/file/xxxx/Lib/tty/tty.py#L18'\\n    \"\n    if (revision is None):\n        return\n    if (domain not in ('py', 'pyx')):\n        return\n    if ((not info.get('module')) or (not info.get('fullname'))):\n        return\n    class_name = info['fullname'].split('.')[0]\n    if (type(class_name) != str):\n        class_name = class_name.encode('utf-8')\n    module = __import__(info['module'], fromlist=[class_name])\n    obj = attrgetter(info['fullname'])(module)\n    try:\n        fn = inspect.getsourcefile(obj)\n    except Exception:\n        fn = None\n    if (not fn):\n        try:\n            fn = inspect.getsourcefile(sys.modules[obj.__module__])\n        except Exception:\n            fn = None\n    if (not fn):\n        return\n    fn = os.path.relpath(fn, start=os.path.dirname(__import__(package).__file__))\n    try:\n        lineno = inspect.getsourcelines(obj)[1]\n    except Exception:\n        lineno = ''\n    return url_fmt.format(revision=revision, package=package, path=fn, lineno=lineno)\n", "label": 1}
{"function": "\n\ndef dispatch(self, pdu):\n    if isinstance(pdu, Symmetry):\n        return\n    if isinstance(pdu, AggregatedFrame):\n        if ((pdu.dsap == 0) and (pdu.ssap == 0)):\n            [log.debug(('     ' + str(p))) for p in pdu]\n            [self.dispatch(p) for p in pdu]\n        return\n    if (isinstance(pdu, Connect) and (pdu.dsap == 1)):\n        addr = self.snl.get(pdu.sn)\n        if ((not addr) or (self.sap[addr] is None)):\n            log.debug(\"no service named '{0}'\".format(pdu.sn))\n            pdu = DisconnectedMode(pdu.ssap, 1, reason=2)\n            self.sap[1].dmpdu.append(pdu)\n            return\n        pdu = Connect(dsap=addr, ssap=pdu.ssap, rw=pdu.rw, miu=pdu.miu)\n    with self.lock:\n        sap = self.sap[pdu.dsap]\n        if sap:\n            sap.enqueue(pdu)\n            return\n    log.debug('discard PDU {0}'.format(str(pdu)))\n    return\n", "label": 1}
{"function": "\n\ndef manage(commands, argv=None, delim=':'):\n    '\\n    Parses argv and runs neccessary command. Is to be used in manage.py file.\\n\\n    Accept a dict with digest name as keys and instances of\\n    :class:`Cli<iktomi.management.commands.Cli>`\\n    objects as values.\\n\\n    The format of command is the following::\\n\\n        ./manage.py digest_name:command_name[ arg1[ arg2[...]]][ --key1=kwarg1[...]]\\n\\n    where command_name is a part of digest instance method name, args and kwargs\\n    are passed to the method. For details, see\\n    :class:`Cli<iktomi.management.commands.Cli>` docs.\\n    '\n\n    def perform_auto_complete(commands):\n        from lazy import LazyCli\n        cwords = os.environ['COMP_WORDS'].split()[1:]\n        cword = int(os.environ['COMP_CWORD'])\n        try:\n            curr = cwords[(cword - 1)]\n        except IndexError:\n            curr = ''\n        suggest = []\n        if ((len(cwords) > 1) and (cwords[0] in commands.keys())):\n            value = commands[cwords[0]]\n            if isinstance(value, LazyCli):\n                value = value.get_digest()\n            for (cmd_name, _) in value.get_funcs():\n                cmd_name = cmd_name[8:]\n                suggest.append(cmd_name)\n            if (curr == ':'):\n                curr = ''\n        else:\n            suggest += (commands.keys() + [(x + ':') for x in commands.keys()])\n        suggest.sort()\n        output = ' '.join(filter((lambda x: x.startswith(curr)), suggest))\n        sys.stdout.write(output)\n    auto_complete = (('IKTOMI_AUTO_COMPLETE' in os.environ) or ('DJANGO_AUTO_COMPLETE' in os.environ))\n    if auto_complete:\n        perform_auto_complete(commands)\n        sys.exit(0)\n    argv = (sys.argv if (argv is None) else argv)\n    if (len(argv) > 1):\n        cmd_name = argv[1]\n        raw_args = argv[2:]\n        (args, kwargs) = ([], {\n            \n        })\n        for item in raw_args:\n            if item.startswith('--'):\n                splited = item[2:].split('=', 1)\n                if (len(splited) == 2):\n                    (k, v) = splited\n                elif (len(splited) == 1):\n                    (k, v) = (splited[0], True)\n                kwargs[k] = v\n            else:\n                args.append(item)\n        if (delim in cmd_name):\n            (digest_name, command) = cmd_name.split(delim)\n        else:\n            digest_name = cmd_name\n            command = None\n        try:\n            digest = commands[digest_name]\n        except KeyError:\n            _command_list(commands)\n            sys.exit('ERROR: Command \"{}\" not found'.format(digest_name))\n        try:\n            if (command is None):\n                if isinstance(digest, Cli):\n                    help_ = digest.description(argv[0], digest_name)\n                    sys.stdout.write(help_)\n                    sys.exit('ERROR: \"{}\" command digest requires command name'.format(digest_name))\n                digest(*args, **kwargs)\n            else:\n                digest(command, *args, **kwargs)\n        except CommandNotFound:\n            help_ = digest.description(argv[0], digest_name)\n            sys.stdout.write(help_)\n            sys.exit('ERROR: Command \"{}:{}\" not found'.format(digest_name, command))\n    else:\n        _command_list(commands)\n        sys.exit('Please provide any command')\n", "label": 1}
{"function": "\n\ndef as_path(tokeniser):\n    as_seq = []\n    as_set = []\n    value = tokeniser()\n    inset = False\n    try:\n        if (value == '['):\n            while True:\n                value = tokeniser()\n                if (value == ','):\n                    continue\n                if (value in ('(', '[')):\n                    inset = True\n                    while True:\n                        value = tokeniser()\n                        if (value == ')'):\n                            break\n                        as_set.append(ASN.from_string(value))\n                if (value == ')'):\n                    inset = False\n                    continue\n                if (value == ']'):\n                    if inset:\n                        inset = False\n                        continue\n                    break\n                as_seq.append(ASN.from_string(value))\n        else:\n            as_seq.append(ASN.from_string(value))\n    except ValueError:\n        raise ValueError('could not parse as-path')\n    return ASPath(as_seq, as_set)\n", "label": 1}
{"function": "\n\ndef _get_or_head_response(self, req, node_iter, partition, policy):\n    req.headers.setdefault('X-Backend-Etag-Is-At', 'X-Object-Sysmeta-Ec-Etag')\n    if (req.method == 'HEAD'):\n        concurrency = (policy.ec_ndata if self.app.concurrent_gets else 1)\n        resp = self.GETorHEAD_base(req, _('Object'), node_iter, partition, req.swift_entity_path, concurrency)\n    else:\n        orig_range = None\n        range_specs = []\n        if req.range:\n            orig_range = req.range\n            range_specs = self._convert_range(req, policy)\n        safe_iter = GreenthreadSafeIterator(node_iter)\n        with ContextPool(policy.ec_ndata) as pool:\n            pile = GreenAsyncPile(pool)\n            for _junk in range(policy.ec_ndata):\n                pile.spawn(self._fragment_GET_request, req, safe_iter, partition, policy)\n            bad_gets = []\n            etag_buckets = collections.defaultdict(list)\n            best_etag = None\n            for (get, parts_iter) in pile:\n                if is_success(get.last_status):\n                    etag = HeaderKeyDict(get.last_headers)['X-Object-Sysmeta-Ec-Etag']\n                    etag_buckets[etag].append((get, parts_iter))\n                    if ((etag != best_etag) and (len(etag_buckets[etag]) > len(etag_buckets[best_etag]))):\n                        best_etag = etag\n                else:\n                    bad_gets.append((get, parts_iter))\n                matching_response_count = max(len(etag_buckets[best_etag]), len(bad_gets))\n                if (((policy.ec_ndata - matching_response_count) > pile._pending) and (node_iter.nodes_left > 0)):\n                    pile.spawn(self._fragment_GET_request, req, safe_iter, partition, policy)\n        req.range = orig_range\n        if (len(etag_buckets[best_etag]) >= policy.ec_ndata):\n            resp_headers = HeaderKeyDict(etag_buckets[best_etag][0][0].source_headers[(- 1)])\n            resp_headers.pop('Content-Range', None)\n            eccl = resp_headers.get('X-Object-Sysmeta-Ec-Content-Length')\n            obj_length = (int(eccl) if (eccl is not None) else None)\n            fa_length = int(resp_headers['Content-Length'])\n            app_iter = ECAppIter(req.swift_entity_path, policy, [iterator for (getter, iterator) in etag_buckets[best_etag]], range_specs, fa_length, obj_length, self.app.logger)\n            resp = Response(request=req, headers=resp_headers, conditional_response=True, app_iter=app_iter)\n            try:\n                app_iter.kickoff(req, resp)\n            except HTTPException as err_resp:\n                resp = err_resp\n        else:\n            statuses = []\n            reasons = []\n            bodies = []\n            headers = []\n            for (getter, body_parts_iter) in bad_gets:\n                statuses.extend(getter.statuses)\n                reasons.extend(getter.reasons)\n                bodies.extend(getter.bodies)\n                headers.extend(getter.source_headers)\n            resp = self.best_response(req, statuses, reasons, bodies, 'Object', headers=headers)\n    self._fix_response(resp)\n    return resp\n", "label": 1}
{"function": "\n\ndef create(name, launch_config_name, availability_zones, min_size, max_size, desired_capacity=None, load_balancers=None, default_cooldown=None, health_check_type=None, health_check_period=None, placement_group=None, vpc_zone_identifier=None, tags=None, termination_policies=None, suspended_processes=None, scaling_policies=None, scheduled_actions=None, region=None, notification_arn=None, notification_types=None, key=None, keyid=None, profile=None):\n    '\\n    Create an autoscale group.\\n\\n    CLI example::\\n\\n        salt myminion boto_asg.create myasg mylc \\'[\"us-east-1a\", \"us-east-1e\"]\\' 1 10 load_balancers=\\'[\"myelb\", \"myelb2\"]\\' tags=\\'[{\"key\": \"Name\", value=\"myasg\", \"propagate_at_launch\": True}]\\'\\n    '\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    if isinstance(availability_zones, six.string_types):\n        availability_zones = json.loads(availability_zones)\n    if isinstance(load_balancers, six.string_types):\n        load_balancers = json.loads(load_balancers)\n    if isinstance(vpc_zone_identifier, six.string_types):\n        vpc_zone_identifier = json.loads(vpc_zone_identifier)\n    if isinstance(tags, six.string_types):\n        tags = json.loads(tags)\n    _tags = []\n    if tags:\n        for tag in tags:\n            try:\n                key = tag.get('key')\n            except KeyError:\n                log.error('Tag missing key.')\n                return False\n            try:\n                value = tag.get('value')\n            except KeyError:\n                log.error('Tag missing value.')\n                return False\n            propagate_at_launch = tag.get('propagate_at_launch', False)\n            _tag = autoscale.Tag(key=key, value=value, resource_id=name, propagate_at_launch=propagate_at_launch)\n            _tags.append(_tag)\n    if isinstance(termination_policies, six.string_types):\n        termination_policies = json.loads(termination_policies)\n    if isinstance(suspended_processes, six.string_types):\n        suspended_processes = json.loads(suspended_processes)\n    if isinstance(scheduled_actions, six.string_types):\n        scheduled_actions = json.loads(scheduled_actions)\n    try:\n        _asg = autoscale.AutoScalingGroup(name=name, launch_config=launch_config_name, availability_zones=availability_zones, min_size=min_size, max_size=max_size, desired_capacity=desired_capacity, load_balancers=load_balancers, default_cooldown=default_cooldown, health_check_type=health_check_type, health_check_period=health_check_period, placement_group=placement_group, tags=_tags, vpc_zone_identifier=vpc_zone_identifier, termination_policies=termination_policies, suspended_processes=suspended_processes)\n        conn.create_auto_scaling_group(_asg)\n        _create_scaling_policies(conn, name, scaling_policies)\n        _create_scheduled_actions(conn, name, scheduled_actions)\n        if (notification_arn and notification_types):\n            conn.put_notification_configuration(_asg, notification_arn, notification_types)\n        log.info('Created ASG {0}'.format(name))\n        return True\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        msg = 'Failed to create ASG {0}'.format(name)\n        log.error(msg)\n        return False\n", "label": 1}
{"function": "\n\ndef plot_bc(self, ftype=None, package=None, kper=0, color=None, plotAll=False, **kwargs):\n    \"\\n        Plot boundary conditions locations for a specific boundary\\n        type from a flopy model\\n\\n        Parameters\\n        ----------\\n        ftype : string\\n            Package name string ('WEL', 'GHB', etc.). (Default is None)\\n        package : flopy.modflow.Modflow package class instance\\n            flopy package class instance. (Default is None)\\n        kper : int\\n            Stress period to plot\\n        color : string\\n            matplotlib color string. (Default is None)\\n        plotAll : bool\\n            Boolean used to specify that boundary condition locations for all\\n            layers will be plotted on the current ModelMap layer.\\n            (Default is False)\\n        **kwargs : dictionary\\n            keyword arguments passed to matplotlib.collections.PatchCollection\\n\\n        Returns\\n        -------\\n        quadmesh : matplotlib.collections.QuadMesh\\n\\n        \"\n    if ('ax' in kwargs):\n        ax = kwargs.pop('ax')\n    else:\n        ax = self.ax\n    if (package is not None):\n        p = package\n    elif (self.model is not None):\n        if (ftype is None):\n            raise Exception('ftype not specified')\n        p = self.model.get_package(ftype)\n    else:\n        raise Exception('Cannot find package to plot')\n    try:\n        mflist = p.stress_period_data[kper]\n    except Exception as e:\n        raise Exception(('Not a list-style boundary package:' + str(e)))\n    if (mflist is None):\n        return None\n    nlay = self.model.nlay\n    plotarray = np.zeros((nlay, self.sr.nrow, self.sr.ncol), dtype=np.int)\n    if plotAll:\n        idx = [mflist['i'], mflist['j']]\n        pa = np.zeros((self.sr.nrow, self.sr.ncol), dtype=np.int)\n        pa[idx] = 1\n        for k in range(nlay):\n            plotarray[k, :, :] = pa.copy()\n    else:\n        idx = [mflist['k'], mflist['i'], mflist['j']]\n        plotarray[idx] = 1\n    plotarray = np.ma.masked_equal(plotarray, 0)\n    if (color is None):\n        if (ftype in bc_color_dict):\n            c = bc_color_dict[ftype]\n        else:\n            c = bc_color_dict['default']\n    else:\n        c = color\n    cmap = matplotlib.colors.ListedColormap(['0', c])\n    bounds = [0, 1, 2]\n    norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n    quadmesh = self.plot_array(plotarray, cmap=cmap, norm=norm, **kwargs)\n    return quadmesh\n", "label": 1}
{"function": "\n\ndef run(self, sync):\n    app = sync.app\n    with app.db.getSession() as session:\n        message = session.getMessage(self.message_key)\n        if (message is None):\n            self.log.debug(('Message %s has already been uploaded' % self.message_key))\n            return\n        change = message.revision.change\n    if (not change.held):\n        self.log.debug(('Syncing %s to find out if it should be held' % (change.id,)))\n        t = SyncChangeTask(change.id)\n        t.run(sync)\n        self.results += t.results\n    submit = False\n    change_id = None\n    with app.db.getSession() as session:\n        message = session.getMessage(self.message_key)\n        revision = message.revision\n        change = message.revision.change\n        if change.held:\n            self.log.debug(('Not uploading review to %s because it is held' % (change.id,)))\n            return\n        change_id = change.id\n        current_revision = change.revisions[(- 1)]\n        if (change.pending_status and (change.status == 'SUBMITTED')):\n            submit = True\n        data = dict(message=message.message, strict_labels=False)\n        if (revision == current_revision):\n            data['labels'] = {\n                \n            }\n            for approval in change.draft_approvals:\n                data['labels'][approval.category] = approval.value\n                session.delete(approval)\n        comments = {\n            \n        }\n        for file in revision.files:\n            if file.draft_comments:\n                comment_list = []\n                for comment in file.draft_comments:\n                    d = dict(line=comment.line, message=comment.message)\n                    if comment.parent:\n                        d['side'] = 'PARENT'\n                    comment_list.append(d)\n                    session.delete(comment)\n                comments[file.path] = comment_list\n        if comments:\n            data['comments'] = comments\n        session.delete(message)\n        sync.post(('changes/%s/revisions/%s/review' % (change.id, revision.commit)), data)\n    if submit:\n        with app.db.getSession() as session:\n            change = session.getChangeByID(change_id)\n            change.pending_status = False\n            change.pending_status_message = None\n            sync.post(('changes/%s/submit' % (change_id,)), {\n                \n            })\n    sync.submitTask(SyncChangeTask(change_id, priority=self.priority))\n", "label": 1}
{"function": "\n\ndef client_info_endpoint(self, method='GET', **kwargs):\n    '\\n        Operations on this endpoint are switched through the use of different\\n        HTTP methods\\n\\n        :param method: HTTP method used for the request\\n        :param kwargs: keyword arguments\\n        :return: A Response instance\\n        '\n    _query = parse_qs(kwargs['query'])\n    try:\n        _id = _query['client_id'][0]\n    except KeyError:\n        return BadRequest('Missing query component')\n    try:\n        assert (_id in self.cdb)\n    except AssertionError:\n        return Unauthorized()\n    try:\n        self.verify_client(kwargs['environ'], kwargs['request'], 'bearer_header', client_id=_id)\n    except (AuthnFailure, UnknownAssertionType):\n        return Unauthorized()\n    if (method == 'GET'):\n        return self.client_info(_id)\n    elif (method == 'PUT'):\n        try:\n            _request = ClientUpdateRequest().from_json(kwargs['request'])\n        except ValueError as err:\n            return BadRequest(str(err))\n        try:\n            _request.verify()\n        except InvalidRedirectUri as err:\n            msg = ClientRegistrationError(error='invalid_redirect_uri', error_description=('%s' % err))\n            return BadRequest(msg.to_json(), content='application/json')\n        except (MissingPage, VerificationError) as err:\n            msg = ClientRegistrationError(error='invalid_client_metadata', error_description=('%s' % err))\n            return BadRequest(msg.to_json(), content='application/json')\n        try:\n            self.client_info_update(_id, _request)\n            return self.client_info(_id)\n        except ModificationForbidden:\n            return Forbidden()\n    elif (method == 'DELETE'):\n        try:\n            del self.cdb[_id]\n        except KeyError:\n            return Unauthorized()\n        else:\n            return NoContent()\n", "label": 1}
{"function": "\n\ndef duration(text, loader_context):\n    cformat = loader_context.get('duration')\n    text_int = None\n    try:\n        text_int = int(text)\n    except ValueError:\n        pass\n    if (cformat == '%H:%M'):\n        if text_int:\n            text += ':00'\n    if (cformat == '%M'):\n        text = _breakdown_time_unit_overlap(text, 60)\n        cformat = '%H:%M'\n    if (cformat == '%M:%S'):\n        if text_int:\n            text += ':00'\n        text = _breakdown_time_unit_overlap(text, 60)\n        cformat = '%H:%M:%S'\n    if (cformat == '%S'):\n        if text_int:\n            if (text_int >= 3600):\n                hours_str = (str((text_int // 3600)) + ':')\n                secs_under_hour_str = str((text_int % 3600))\n                text = (hours_str + _breakdown_time_unit_overlap(secs_under_hour_str, 60))\n                cformat = '%H:%M:%S'\n            else:\n                text = _breakdown_time_unit_overlap(text, 60)\n                cformat = '%M:%S'\n    try:\n        duration = datetime.datetime.strptime(text, cformat)\n    except ValueError:\n        loader_context.get('spider').log('Duration could not be parsed (\"{t}\", Format string: \"{f}\")!'.format(t=text, f=cformat), logging.ERROR)\n        return None\n    return duration.strftime('%H:%M:%S')\n", "label": 1}
{"function": "\n\ndef activate(self, mac):\n    assert (type(mac) in (nfc.dep.Initiator, nfc.dep.Target))\n    self.mac = None\n    miu = self.cfg['recv-miu']\n    lto = self.cfg['send-lto']\n    wks = (1 + sum(sorted([(1 << sap) for sap in self.snl.values() if (sap < 15)])))\n    pax = ParameterExchange(version=(1, 1), miu=miu, lto=lto, wks=wks)\n    if (type(mac) == nfc.dep.Initiator):\n        gb = mac.activate(gbi=('Ffm' + pax.to_string()[2:]))\n        self.run = self.run_as_initiator\n        role = 'Initiator'\n    if (type(mac) == nfc.dep.Target):\n        gb = mac.activate(gbt=('Ffm' + pax.to_string()[2:]), wt=9)\n        self.run = self.run_as_target\n        role = 'Target'\n    if ((gb is not None) and gb.startswith('Ffm') and (len(gb) >= 6)):\n        info = ['LLCP Link established as NFC-DEP {0}'.format(role)]\n        info.append('Local LLCP Settings')\n        info.append('  LLCP Version: {0[0]}.{0[1]}'.format(pax.version))\n        info.append('  Link Timeout: {0} ms'.format(pax.lto))\n        info.append('  Max Inf Unit: {0} octet'.format(pax.miu))\n        info.append('  Service List: {0:016b}'.format(pax.wks))\n        pax = ProtocolDataUnit.from_string(('\\x00@' + str(gb[3:])))\n        info.append('Remote LLCP Settings')\n        info.append('  LLCP Version: {0[0]}.{0[1]}'.format(pax.version))\n        info.append('  Link Timeout: {0} ms'.format(pax.lto))\n        info.append('  Max Inf Unit: {0} octet'.format(pax.miu))\n        info.append('  Service List: {0:016b}'.format(pax.wks))\n        log.info('\\n'.join(info))\n        self.cfg['rcvd-ver'] = pax.version\n        self.cfg['send-miu'] = pax.miu\n        self.cfg['recv-lto'] = pax.lto\n        self.cfg['send-wks'] = pax.wks\n        self.cfg['send-lsc'] = pax.lsc\n        log.debug('llc cfg {0}'.format(self.cfg))\n        if ((type(mac) == nfc.dep.Initiator) and (mac.rwt is not None)):\n            max_rwt = ((4096 / 13560000.0) * (2 ** 10))\n            if (mac.rwt > max_rwt):\n                log.warning('NFC-DEP RWT {0:.3f} exceeds max {1:.3f} sec'.format(mac.rwt, max_rwt))\n        self.mac = mac\n    return bool(self.mac)\n", "label": 1}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('value')\n    group = parser.add_mutually_exclusive_group(required=False)\n    group.add_argument('-u', '--url', default=False, action='store_true')\n    group.add_argument('-f', '--file', default=False, action='store_true')\n    args = parser.parse_args()\n    if ((not args.url) and (not args.file)):\n        args.file = os.path.exists(args.value)\n        args.url = (not args.file)\n    if args.url:\n        if args.value.startswith('magnet:?'):\n            if (args.value.find('xt=urn:btih') > 0):\n                client.add_torrent_url(args.value)\n                return\n        else:\n            args.value = utils.httpize(args.value)\n            try:\n                torrent = utils.get(args.value, utf8=False)\n            except utils.HTTPError:\n                error(('invalid url: %s' % args.value))\n            client.add_torrent_url(args.value)\n    elif args.file:\n        if (not os.path.exists(args.value)):\n            error(('no such file: %s' % args.value))\n        try:\n            f = open(args.value, 'rb')\n            torrent = f.read()\n            f.close()\n        except:\n            error(('reading file: %s' % args.value))\n        client.add_torrent_file(args.value)\n    added = None\n    try:\n        decoded = bdecode(torrent)\n        encoded = bencode(decoded[b'info'])\n    except:\n        error('invalid torrent file')\n    h = hashlib.sha1(encoded).hexdigest().upper()\n    while (not added):\n        l = client.list_torrents()\n        for t in l:\n            if (t['hash'] == h):\n                added = t\n                break\n        time.sleep(1)\n    print(encoder.encode([added]))\n", "label": 1}
{"function": "\n\ndef match(self, interp, block, typemap, calltypes):\n    '\\n        Look for potential macros for expand and store their expansions.\\n        '\n    self.block = block\n    self.rewrites = rewrites = {\n        \n    }\n    for inst in block.body:\n        if isinstance(inst, ir.Assign):\n            rhs = inst.value\n            if (isinstance(rhs, ir.Expr) and (rhs.op == 'call') and isinstance(rhs.func, ir.Var)):\n                try:\n                    const = interp.infer_constant(rhs.func)\n                except errors.ConstantInferenceError:\n                    continue\n                if isinstance(const, Macro):\n                    assert const.callable\n                    new_expr = self._expand_callable_macro(interp, rhs, const, rhs.loc)\n                    rewrites[rhs] = new_expr\n            elif (isinstance(rhs, ir.Expr) and (rhs.op == 'getattr')):\n                try:\n                    const = interp.infer_constant(inst.target)\n                except errors.ConstantInferenceError:\n                    continue\n                if (isinstance(const, Macro) and (not const.callable)):\n                    new_expr = self._expand_non_callable_macro(const, rhs.loc)\n                    rewrites[rhs] = new_expr\n    return (len(rewrites) > 0)\n", "label": 1}
{"function": "\n\ndef compute(self):\n    experiment = self.get_input('experiment')\n    if configuration.check('reprounzip_python'):\n        python = configuration.reprounzip_python\n    else:\n        python = 'python'\n    environ = dict(os.environ)\n    removed = []\n    for bad_var in ('PYTHONPATH', 'PYTHONHOME'):\n        if (bad_var in os.environ):\n            removed.append(bad_var)\n            del environ[bad_var]\n    if removed:\n        warnings.warn(('Removing variables from environment before calling reprounzip: %s' % ' '.join(removed)))\n    if ((platform.system() == 'Darwin') and ('/usr/local/bin' not in environ['PATH'].split(os.pathsep))):\n        environ['PATH'] += (os.pathsep + '/usr/local/bin')\n    stdout = self.interpreter.filePool.create_file(prefix='vt_rpz_stdout_', suffix='.txt')\n    stderr = self.interpreter.filePool.create_file(prefix='vt_rpz_stderr_', suffix='.txt')\n    args = [python, '-m', 'reprounzip.plugins.vistrails', REPROUNZIP_VISTRAILS_INTERFACE_VERSION, experiment.unpacker, experiment.path, ('%d' % self.get_input('run_number'))]\n    for name in self.input_ports_order:\n        if self.has_input(name):\n            args.append('--input-file')\n            args.append(('%s:%s' % (name, self.get_input(name).name)))\n    output_ports = []\n    for name in self.output_ports_order:\n        f = self.interpreter.filePool.create_file(prefix='vt_rpz_out_')\n        args.append('--output-file')\n        args.append(('%s:%s' % (name, f.name)))\n        output_ports.append((name, f))\n    with open(stdout.name, 'wb') as stdout_fp:\n        with open(stderr.name, 'wb') as stderr_fp:\n            proc = subprocess.Popen(args, stdout=stdout_fp, stderr=stderr_fp, env=environ)\n            proc.wait()\n    with open(stderr.name, 'rb') as stderr_fp:\n        while True:\n            chunk = stderr_fp.read(4096)\n            if (not chunk):\n                break\n            sys.stderr.write(chunk)\n    if (proc.returncode != 0):\n        raise ModuleError(self, ('Plugin returned with code %d' % proc.returncode))\n    for (name, file) in output_ports:\n        self.set_output(name, file)\n    self.set_output('experiment', experiment)\n    self.set_output('stdout', stdout)\n    self.set_output('stderr', stderr)\n", "label": 1}
{"function": "\n\ndef load_extra_state(self, profile):\n    'Load extra state for the datagrid.'\n    group_name = self.request.GET.get('group', '')\n    view = self.request.GET.get('view', self.default_view)\n    user = self.request.user\n    if (view == 'outgoing'):\n        self.queryset = ReviewRequest.objects.from_user(user, user, local_site=self.local_site)\n        self.title = _('All Outgoing Review Requests')\n    elif (view == 'mine'):\n        self.queryset = ReviewRequest.objects.from_user(user, user, None, local_site=self.local_site)\n        self.title = _('All My Review Requests')\n    elif (view == 'to-me'):\n        self.queryset = ReviewRequest.objects.to_user_directly(user, user, local_site=self.local_site)\n        self.title = _('Incoming Review Requests to Me')\n    elif (view in ('to-group', 'to-watched-group')):\n        if group_name:\n            try:\n                group = Group.objects.get(name=group_name, local_site=self.local_site)\n                if (not group.is_accessible_by(user)):\n                    raise Http404\n            except Group.DoesNotExist:\n                raise Http404\n            self.queryset = ReviewRequest.objects.to_group(group_name, self.local_site, user)\n            self.title = (_('Incoming Review Requests to %s') % group_name)\n        else:\n            self.queryset = ReviewRequest.objects.to_user_groups(user, user, local_site=self.local_site)\n            self.title = _('All Incoming Review Requests to My Groups')\n    elif (view == 'starred'):\n        self.queryset = self.profile.starred_review_requests.public(user=user, local_site=self.local_site, status=None)\n        self.title = _('Starred Review Requests')\n    elif (view == 'incoming'):\n        self.queryset = ReviewRequest.objects.to_user(user, user, local_site=self.local_site)\n        self.title = _('All Incoming Review Requests')\n    else:\n        raise Http404\n    if (profile and ('show_archived' in profile.extra_data)):\n        self.show_archived = profile.extra_data['show_archived']\n    try:\n        show = self.request.GET.get('show-archived', self.show_archived)\n        self.show_archived = (int(show) != 0)\n    except ValueError:\n        pass\n    if (not self.show_archived):\n        hidden_q = ReviewRequestVisit.objects.filter(user=user).exclude(visibility=ReviewRequestVisit.VISIBLE)\n        hidden_q = hidden_q.values_list('review_request_id', flat=True)\n        self.queryset = self.queryset.exclude(pk__in=hidden_q)\n    if (profile and (self.show_archived != profile.extra_data.get('show_archived'))):\n        profile.extra_data['show_archived'] = self.show_archived\n        profile_changed = True\n    else:\n        profile_changed = False\n    self.extra_js_model_data['show_archived'] = self.show_archived\n    parent_profile_changed = super(DashboardDataGrid, self).load_extra_state(profile, allow_hide_closed=False)\n    return (profile_changed or parent_profile_changed)\n", "label": 1}
{"function": "\n\ndef test_create_alt(scratch_tree, scratch_pad):\n    sess = scratch_tree.edit('/', alt='de')\n    assert (sess.id == '')\n    assert (sess.path == '/')\n    assert (sess.record is not None)\n    assert (sess['_model'] == 'page')\n    assert (sess['title'] == 'Index')\n    assert (sess['body'] == 'Hello World!')\n    sess['body'] = 'Hallo Welt!'\n    sess.commit()\n    assert sess.closed\n    with open(sess.get_fs_path(alt='de')) as f:\n        assert (f.read().splitlines() == ['body: Hallo Welt!'])\n    scratch_pad.cache.flush()\n    item = scratch_pad.get('/', alt='de')\n    assert (item['_slug'] == '')\n    assert (item['title'] == 'Index')\n    assert (item['body'].source == 'Hallo Welt!')\n    assert (item['_model'] == 'page')\n", "label": 1}
{"function": "\n\ndef __init__(self, device, window):\n    'Create an opened instance of a device on the given window.\\n\\n        :Parameters:\\n            `device` : XInputDevice\\n                Device to open\\n            `window` : Window\\n                Window to open device on\\n\\n        '\n    assert (device._x_display == window._x_display)\n    assert device._open_device\n    self.device = device\n    self.window = window\n    self._events = []\n    try:\n        dispatcher = window.__xinput_window_event_dispatcher\n    except AttributeError:\n        dispatcher = window.__xinput_window_event_dispatcher = XInputWindowEventDispatcher()\n    dispatcher.add_instance(self)\n    device = device._open_device.contents\n    if (not device.num_classes):\n        return\n    for i in range(device.num_classes):\n        class_info = device.classes[i]\n        if (class_info.input_class == xi.KeyClass):\n            self._add(class_info, xi._deviceKeyPress, dispatcher._event_xinput_key_press)\n            self._add(class_info, xi._deviceKeyRelease, dispatcher._event_xinput_key_release)\n        elif (class_info.input_class == xi.ButtonClass):\n            self._add(class_info, xi._deviceButtonPress, dispatcher._event_xinput_button_press)\n            self._add(class_info, xi._deviceButtonRelease, dispatcher._event_xinput_button_release)\n        elif (class_info.input_class == xi.ValuatorClass):\n            self._add(class_info, xi._deviceMotionNotify, dispatcher._event_xinput_motion)\n        elif (class_info.input_class == xi.ProximityClass):\n            self._add(class_info, xi._proximityIn, dispatcher._event_xinput_proximity_in)\n            self._add(class_info, xi._proximityOut, dispatcher._event_xinput_proximity_out)\n        elif (class_info.input_class == xi.FeedbackClass):\n            pass\n        elif (class_info.input_class == xi.FocusClass):\n            pass\n        elif (class_info.input_class == xi.OtherClass):\n            pass\n    array = (xi.XEventClass * len(self._events))(*self._events)\n    xi.XSelectExtensionEvent(window._x_display, window._window, array, len(array))\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    if options.get('param'):\n        for item in options['param']:\n            if ('=' in item):\n                (key, val) = item.split('=')\n            else:\n                (key, val) = (item, True)\n            options[key] = val\n    if (not args):\n        args = []\n    for app in settings.INSTALLED_APPS:\n        if (args and (app not in args)):\n            continue\n        try:\n            app_path = import_module(app).__path__\n        except AttributeError:\n            continue\n        try:\n            imp.find_module('sampledata', app_path)\n        except ImportError:\n            continue\n        module = import_module(('%s.sampledata' % app))\n        if hasattr(module, 'generate_sampledata'):\n            self.stdout.write(('Generating sample data from %s...\\n' % app))\n            module.generate_sampledata(options)\n    self.stdout.write('Done!\\n')\n", "label": 1}
{"function": "\n\ndef parse_editable(editable_req, default_vcs=None):\n    'Parses an editable requirement into:\\n        - a requirement name\\n        - an URL\\n        - extras\\n        - editable options\\n    Accepted requirements:\\n        svn+http://blahblah@rev#egg=Foobar[baz]&subdirectory=version_subdir\\n        .[some_extra]\\n    '\n    url = editable_req\n    extras = None\n    m = re.match('^(.+)(\\\\[[^\\\\]]+\\\\])$', url)\n    if m:\n        url_no_extras = m.group(1)\n        extras = m.group(2)\n    else:\n        url_no_extras = url\n    if os.path.isdir(url_no_extras):\n        if (not os.path.exists(os.path.join(url_no_extras, 'setup.py'))):\n            raise InstallationError((\"Directory %r is not installable. File 'setup.py' not found.\" % url_no_extras))\n        url_no_extras = path_to_url(url_no_extras)\n    if url_no_extras.lower().startswith('file:'):\n        if extras:\n            return (None, url_no_extras, pkg_resources.Requirement.parse(('__placeholder__' + extras)).extras, {\n                \n            })\n        else:\n            return (None, url_no_extras, None, {\n                \n            })\n    for version_control in vcs:\n        if url.lower().startswith(('%s:' % version_control)):\n            url = ('%s+%s' % (version_control, url))\n            break\n    if ('+' not in url):\n        if default_vcs:\n            url = ((default_vcs + '+') + url)\n        else:\n            raise InstallationError(('%s should either be a path to a local project or a VCS url beginning with svn+, git+, hg+, or bzr+' % editable_req))\n    vc_type = url.split('+', 1)[0].lower()\n    if (not vcs.get_backend(vc_type)):\n        error_message = ((('For --editable=%s only ' % editable_req) + ', '.join([(backend.name + '+URL') for backend in vcs.backends])) + ' is currently supported')\n        raise InstallationError(error_message)\n    try:\n        options = _build_editable_options(editable_req)\n    except Exception as exc:\n        raise InstallationError(('--editable=%s error in editable options:%s' % (editable_req, exc)))\n    if ((not options) or ('egg' not in options)):\n        req = _build_req_from_url(editable_req)\n        if (not req):\n            raise InstallationError(('--editable=%s is not the right format; it must have #egg=Package' % editable_req))\n    else:\n        req = options['egg']\n    package = _strip_postfix(req)\n    return (package, url, None, options)\n", "label": 1}
{"function": "\n\ndef parse_docstring(self, s, pnames=None):\n    free_text = []\n    header = []\n    label = None\n    last_argname = None\n    for p in split_docstring(s):\n        argdoc = self.argdoc_re.match(p)\n        if argdoc:\n            (argname, text) = argdoc.groups()\n            if free_text:\n                if free_text[(- 1)].endswith(':'):\n                    label = free_text.pop()\n                if last_argname:\n                    if ((pnames is None) or (last_argname in pnames)):\n                        self.after[last_argname] = free_text\n                else:\n                    header.extend(free_text)\n                free_text = []\n            last_argname = argname\n            try:\n                default_label = self.get_param_type(self.signature.parameters[argname])\n            except KeyError:\n                continue\n            if ((pnames is not None) and (argname not in pnames)):\n                continue\n            if (default_label != LABEL_POS):\n                try:\n                    (param, _) = self.sections[default_label].pop(argname)\n                except KeyError:\n                    continue\n                label_ = (label or default_label)\n                if (label_ not in self.sections):\n                    self.sections[label_] = util.OrderedDict()\n            else:\n                try:\n                    (param, _) = self.sections[default_label][argname]\n                except KeyError:\n                    continue\n                label_ = default_label\n            self.sections[label_][argname] = (param, text)\n        else:\n            free_text.append(p)\n    if (not last_argname):\n        header = free_text\n        footer = []\n    else:\n        footer = free_text\n    return (lines_to_paragraphs(header), lines_to_paragraphs(footer))\n", "label": 1}
{"function": "\n\ndef test_easy_file(self):\n    cmd = ('%s %s -r 10 -o %s' % (fq_pgm, self.in_fqfn, self.out_fqfn))\n    r = envoy.run(cmd)\n    p_recs = []\n    for rec in fileinput.input(self.out_fqfn):\n        p_recs.append(rec)\n    fileinput.close()\n    assert (len(p_recs) == 4)\n    assert p_recs[0].strip().startswith('field_0')\n    assert p_recs[0].strip().endswith('10')\n    assert p_recs[1].strip().startswith('field_1')\n    assert p_recs[1].strip().endswith('A')\n    assert p_recs[2].strip().startswith('field_2')\n    assert p_recs[2].strip().endswith('B')\n    assert p_recs[3].strip().startswith('field_3')\n    assert p_recs[3].strip().endswith('C')\n", "label": 1}
{"function": "\n\ndef get(self, author_id):\n    authors = Author.find(author_id, get_current_user())\n    if ((not authors) and (author_id == 'me')):\n        return ('Either there is no current user or you are not in the\\n              author table', 401)\n    elif (not authors):\n        return ('author not found', 404)\n    try:\n        author_email = authors[0].email\n        request = PhabricatorClient()\n        request.connect()\n        user_info = request.call('user.query', {\n            'emails': [author_email],\n        })\n        if (not user_info):\n            return (('phabricator: %s not found' % author_email), 404)\n        author_phid = user_info[0]['phid']\n        diff_info = request.call('differential.query', {\n            'authors': [author_phid],\n            'status': 'status-open',\n        })\n        diff_info.sort(key=(lambda k: ((- 1) * int(k['dateModified']))))\n    except requests.exceptions.ConnectionError:\n        return ('Unable to connect to Phabricator', 503)\n    if (not diff_info):\n        return self.respond([])\n    rows = list(db.session.query(PhabricatorDiff, Build).join(Build, (Build.source_id == PhabricatorDiff.source_id)).filter(PhabricatorDiff.revision_id.in_([d['id'] for d in diff_info])))\n    serialized_builds = zip(self.serialize([row.Build for row in rows]), [row.PhabricatorDiff for row in rows])\n    builds_map = defaultdict(list)\n    for (build, phabricator_diff) in serialized_builds:\n        builds_map[str(phabricator_diff.revision_id)].append(build)\n    for d in diff_info:\n        d['builds'] = builds_map[str(d['id'])]\n    return self.respond(diff_info)\n", "label": 1}
{"function": "\n\ndef get_value(self, context, *tag_args, **tag_kwargs):\n    request = self.get_request(context)\n    output = None\n    (slot,) = tag_args\n    template_name = (tag_kwargs.get('template') or None)\n    cachable = is_true(tag_kwargs.get('cachable', (not bool(template_name))))\n    if (template_name and cachable and (not extract_literal(self.kwargs['template']))):\n        raise TemplateSyntaxError(\"{0} tag does not allow 'cachable' for variable template names!\".format(self.tag_name))\n    try_cache = (appsettings.FLUENT_CONTENTS_CACHE_OUTPUT and appsettings.FLUENT_CONTENTS_CACHE_PLACEHOLDER_OUTPUT and cachable)\n    if isinstance(slot, SharedContent):\n        sharedcontent = slot\n        if try_cache:\n            cache_key = get_shared_content_cache_key(sharedcontent)\n            output = cache.get(cache_key)\n    else:\n        site = Site.objects.get_current()\n        if try_cache:\n            cache_key_ptr = get_shared_content_cache_key_ptr(int(site.pk), slot, language_code=get_language())\n            cache_key = cache.get(cache_key_ptr)\n            if (cache_key is not None):\n                output = cache.get(cache_key)\n        if (output is None):\n            try:\n                sharedcontent = SharedContent.objects.parent_site(site).get(slug=slot)\n            except SharedContent.DoesNotExist:\n                return \"<!-- shared content '{0}' does not yet exist -->\".format(slot)\n            if (try_cache and (not cache_key)):\n                cache.set(cache_key_ptr, get_shared_content_cache_key(sharedcontent))\n    if (output is None):\n        output = self.render_shared_content(request, sharedcontent, template_name, cachable=cachable)\n    rendering.register_frontend_media(request, output.media)\n    return output.html\n", "label": 1}
{"function": "\n\ndef _wrap_result(self, result, use_codes=True, name=None, expand=None):\n    from pandas.core.index import Index, MultiIndex\n    if (use_codes and self._is_categorical):\n        result = take_1d(result, self._orig.cat.codes)\n    if ((not hasattr(result, 'ndim')) or (not hasattr(result, 'dtype'))):\n        return result\n    assert (result.ndim < 3)\n    if (expand is None):\n        expand = (False if (result.ndim == 1) else True)\n    elif ((expand is True) and (not isinstance(self._orig, Index))):\n\n        def cons_row(x):\n            if is_list_like(x):\n                return x\n            else:\n                return [x]\n        result = [cons_row(x) for x in result]\n    if (not isinstance(expand, bool)):\n        raise ValueError('expand must be True or False')\n    if (expand is False):\n        if (name is None):\n            name = getattr(result, 'name', None)\n        if (name is None):\n            name = self._orig.name\n    if isinstance(self._orig, Index):\n        if is_bool_dtype(result):\n            return result\n        if expand:\n            result = list(result)\n            return MultiIndex.from_tuples(result, names=name)\n        else:\n            return Index(result, name=name)\n    else:\n        index = self._orig.index\n        if expand:\n            cons = self._orig._constructor_expanddim\n            return cons(result, columns=name, index=index)\n        else:\n            cons = self._orig._constructor\n            return cons(result, name=name, index=index)\n", "label": 1}
{"function": "\n\ndef fetch_info(log, items, write):\n    'Get data from AcousticBrainz for the items.\\n    '\n\n    def get_value(*map_path):\n        try:\n            return reduce(operator.getitem, map_path, data)\n        except KeyError:\n            log.debug('Invalid Path: {}', map_path)\n    for item in items:\n        if item.mb_trackid:\n            log.info('getting data for: {}', item)\n            urls = [generate_url(item.mb_trackid, path) for path in LEVELS]\n            log.debug('fetching URLs: {}', ' '.join(urls))\n            try:\n                res = [requests.get(url) for url in urls]\n            except requests.RequestException as exc:\n                log.info('request error: {}', exc)\n                continue\n            if any(((r.status_code == 404) for r in res)):\n                log.info('recording ID {} not found', item.mb_trackid)\n                continue\n            try:\n                data = res[0].json()\n                data.update(res[1].json())\n            except ValueError:\n                log.debug('Invalid Response: {} & {}', [r.text for r in res])\n            item.danceable = get_value('highlevel', 'danceability', 'all', 'danceable')\n            item.gender = get_value('highlevel', 'gender', 'value')\n            item.genre_rosamerica = get_value('highlevel', 'genre_rosamerica', 'value')\n            item.mood_acoustic = get_value('highlevel', 'mood_acoustic', 'all', 'acoustic')\n            item.mood_aggressive = get_value('highlevel', 'mood_aggressive', 'all', 'aggressive')\n            item.mood_electronic = get_value('highlevel', 'mood_electronic', 'all', 'electronic')\n            item.mood_happy = get_value('highlevel', 'mood_happy', 'all', 'happy')\n            item.mood_party = get_value('highlevel', 'mood_party', 'all', 'party')\n            item.mood_relaxed = get_value('highlevel', 'mood_relaxed', 'all', 'relaxed')\n            item.mood_sad = get_value('highlevel', 'mood_sad', 'all', 'sad')\n            item.rhythm = get_value('highlevel', 'ismir04_rhythm', 'value')\n            item.tonal = get_value('highlevel', 'tonal_atonal', 'all', 'tonal')\n            item.voice_instrumental = get_value('highlevel', 'voice_instrumental', 'value')\n            item.average_loudness = get_value('lowlevel', 'average_loudness')\n            item.chords_changes_rate = get_value('tonal', 'chords_changes_rate')\n            item.chords_key = get_value('tonal', 'chords_key')\n            item.chords_number_rate = get_value('tonal', 'chords_number_rate')\n            item.chords_scale = get_value('tonal', 'chords_scale')\n            item.initial_key = '{} {}'.format(get_value('tonal', 'key_key'), get_value('tonal', 'key_scale'))\n            item.key_strength = get_value('tonal', 'key_strength')\n            item.store()\n            if write:\n                item.try_write()\n", "label": 1}
{"function": "\n\ndef log(self, sender, message, channel):\n    sender = sender[:10]\n    self.word_table.setdefault(sender, {\n        \n    })\n    if message.startswith('/'):\n        return\n    try:\n        say_something = (self.is_ping(message) or ((sender != self.conn.nick) and (random.random() < self.chattiness)))\n    except AttributeError:\n        say_something = False\n    messages = []\n    seed_key = None\n    if self.is_ping(message):\n        message = self.fix_ping(message)\n    for words in self.split_message(self.sanitize_message(message)):\n        key = tuple(words[:(- 1)])\n        if (key in self.word_table):\n            self.word_table[sender][key].append(words[(- 1)])\n        else:\n            self.word_table[sender][key] = [words[(- 1)]]\n        if ((self.stop_word not in key) and say_something):\n            for person in self.word_table:\n                if (person == sender):\n                    continue\n                if (key in self.word_table[person]):\n                    generated = self.generate_message(person, seed_key=key)\n                    if generated:\n                        messages.append((person, generated))\n    if len(messages):\n        (self.last, message) = random.choice(messages)\n        return message\n", "label": 1}
{"function": "\n\ndef run(self, edit):\n\n    def update_ordered_list(lines):\n        new_lines = []\n        next_num = None\n        kind = (lambda a: a)\n        for line in lines:\n            match = ORDER_LIST_PATTERN.match(line)\n            if (not match):\n                new_lines.append(line)\n                continue\n            new_line = (((match.group(1) + (kind(next_num) or match.group(2))) + match.group(3)) + match.group(4))\n            new_lines.append(new_line)\n            if (not next_num):\n                try:\n                    next_num = int(match.group(2))\n                    kind = str\n                except ValueError:\n                    next_num = ord(match.group(2))\n                    kind = chr\n            next_num += 1\n        return new_lines\n\n    def update_roman_list(lines):\n        new_lines = []\n        next_num = None\n        kind = (lambda a: a)\n        for line in lines:\n            match = ROMAN_PATTERN.match(line)\n            if (not match):\n                new_lines.append(line)\n                continue\n            new_line = (((match.group(1) + (kind(next_num) or match.group(2))) + match.group(3)) + match.group(4))\n            new_lines.append(new_line)\n            if (not next_num):\n                actual = match.group(2)\n                next_num = from_roman(actual.upper())\n                if (actual == actual.lower()):\n                    kind = (lambda a: to_roman(a).lower())\n                else:\n                    kind = to_roman\n            next_num += 1\n        return new_lines\n    for region in self.view.sel():\n        line_region = self.view.line(region)\n        before_point_region = sublime.Region(line_region.a, region.a)\n        before_point_content = self.view.substr(before_point_region)\n        folded = False\n        for i in self.view.folded_regions():\n            if i.contains(before_point_region):\n                self.view.insert(edit, region.a, '\\n')\n                folded = True\n        if folded:\n            break\n        match = EMPTY_LIST_PATTERN.match(before_point_content)\n        if match:\n            insert_text = ((match.group(1) + re.sub('\\\\S', ' ', str(match.group(2)))) + match.group(3))\n            self.view.erase(edit, before_point_region)\n            self.view.insert(edit, line_region.a, insert_text)\n            break\n        match = ROMAN_PATTERN.match(before_point_content)\n        if match:\n            actual = match.group(2)\n            next_num = to_roman((from_roman(actual.upper()) + 1))\n            if (actual == actual.lower()):\n                next_num = next_num.lower()\n            insert_text = ((match.group(1) + next_num) + match.group(3))\n            self.view.insert(edit, region.a, ('\\n' + insert_text))\n            pos = self.view.sel()[0].a\n            (region, lines, indent) = self.get_block_bounds()\n            new_list = update_roman_list(lines)\n            self.view.replace(edit, region, ('\\n'.join(new_list) + '\\n'))\n            self.view.sel().clear()\n            self.view.sel().add(sublime.Region(pos, pos))\n            self.view.show(pos)\n            break\n        match = ORDER_LIST_PATTERN.match(before_point_content)\n        if match:\n            try:\n                next_num = str((int(match.group(2)) + 1))\n            except ValueError:\n                next_num = chr((ord(match.group(2)) + 1))\n            insert_text = ((match.group(1) + next_num) + match.group(3))\n            self.view.insert(edit, region.a, ('\\n' + insert_text))\n            pos = self.view.sel()[0].a\n            (region, lines, indent) = self.get_block_bounds()\n            new_list = update_ordered_list(lines)\n            self.view.replace(edit, region, ('\\n'.join(new_list) + '\\n'))\n            self.view.sel().clear()\n            self.view.sel().add(sublime.Region(pos, pos))\n            self.view.show(pos)\n            break\n        match = UNORDER_LIST_PATTERN.match(before_point_content)\n        if match:\n            insert_text = (match.group(1) + match.group(2))\n            self.view.insert(edit, region.a, ('\\n' + insert_text))\n            break\n        match = NONLIST_PATTERN.match(before_point_content)\n        if match:\n            insert_text = (match.group(1) + match.group(2))\n            self.view.insert(edit, region.a, ('\\n' + insert_text))\n            break\n        self.view.insert(edit, region.a, ('\\n' + re.sub('\\\\S+\\\\s*', '', before_point_content)))\n    self.adjust_view()\n", "label": 1}
{"function": "\n\ndef test_unix_domain_adapter_monkeypatch():\n    with UnixSocketServerThread() as usock_thread:\n        with requests_unixsocket.monkeypatch('http+unix://'):\n            urlencoded_usock = requests.compat.quote_plus(usock_thread.usock)\n            url = ('http+unix://%s/path/to/page' % urlencoded_usock)\n            for method in ['get', 'post', 'head', 'patch', 'put', 'delete', 'options']:\n                logger.debug('Calling session.%s(%r) ...', method, url)\n                r = getattr(requests, method)(url)\n                logger.debug('Received response: %r with text: %r and headers: %r', r, r.text, r.headers)\n                assert (r.status_code == 200)\n                assert (r.headers['server'] == 'waitress')\n                assert (r.headers['X-Transport'] == 'unix domain socket')\n                assert (r.headers['X-Requested-Path'] == '/path/to/page')\n                assert (r.headers['X-Socket-Path'] == usock_thread.usock)\n                assert isinstance(r.connection, requests_unixsocket.UnixAdapter)\n                assert (r.url == url)\n                if (method == 'head'):\n                    assert (r.text == '')\n                else:\n                    assert (r.text == 'Hello world!')\n    for method in ['get', 'post', 'head', 'patch', 'put', 'delete', 'options']:\n        with pytest.raises(requests.exceptions.InvalidSchema):\n            getattr(requests, method)(url)\n", "label": 1}
{"function": "\n\ndef _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):\n    '\\n        Returns context_lines before and after lineno from file.\\n        Returns (pre_context_lineno, pre_context, context_line, post_context).\\n        '\n    source = None\n    if ((loader is not None) and hasattr(loader, 'get_source')):\n        try:\n            source = loader.get_source(module_name)\n        except ImportError:\n            pass\n        if (source is not None):\n            source = source.splitlines()\n    if (source is None):\n        try:\n            with open(filename, 'rb') as fp:\n                source = fp.read().splitlines()\n        except (OSError, IOError):\n            pass\n    if (source is None):\n        return (None, [], None, [])\n    if isinstance(source[0], six.binary_type):\n        encoding = 'ascii'\n        for line in source[:2]:\n            match = re.search(b'coding[:=]\\\\s*([-\\\\w.]+)', line)\n            if match:\n                encoding = match.group(1).decode('ascii')\n                break\n        source = [six.text_type(sline, encoding, 'replace') for sline in source]\n    lower_bound = max(0, (lineno - context_lines))\n    upper_bound = (lineno + context_lines)\n    pre_context = source[lower_bound:lineno]\n    context_line = source[lineno]\n    post_context = source[(lineno + 1):upper_bound]\n    return (lower_bound, pre_context, context_line, post_context)\n", "label": 1}
{"function": "\n\ndef __negotiatesocks5(self, destaddr, destport):\n    '__negotiatesocks5(self,destaddr,destport)\\n        Negotiates a connection through a SOCKS5 server.\\n        '\n    if ((self.__proxy[4] != None) and (self.__proxy[5] != None)):\n        self.sendall(struct.pack('BBBB', 5, 2, 0, 2))\n    else:\n        self.sendall(struct.pack('BBB', 5, 1, 0))\n    chosenauth = self.__recvall(2)\n    if (chosenauth[0:1] != chr(5).encode()):\n        self.close()\n        raise GeneralProxyError((1, _generalerrors[1]))\n    if (chosenauth[1:2] == chr(0).encode()):\n        pass\n    elif (chosenauth[1:2] == chr(2).encode()):\n        self.sendall(((((chr(1).encode() + chr(len(self.__proxy[4]))) + self.__proxy[4]) + chr(len(self.__proxy[5]))) + self.__proxy[5]))\n        authstat = self.__recvall(2)\n        if (authstat[0:1] != chr(1).encode()):\n            self.close()\n            raise GeneralProxyError((1, _generalerrors[1]))\n        if (authstat[1:2] != chr(0).encode()):\n            self.close()\n            raise Socks5AuthError((3, _socks5autherrors[3]))\n    else:\n        self.close()\n        if (chosenauth[1] == chr(255).encode()):\n            raise Socks5AuthError((2, _socks5autherrors[2]))\n        else:\n            raise GeneralProxyError((1, _generalerrors[1]))\n    req = struct.pack('BBB', 5, 1, 0)\n    try:\n        ipaddr = socket.inet_aton(destaddr)\n        req = ((req + chr(1).encode()) + ipaddr)\n    except socket.error:\n        if self.__proxy[3]:\n            ipaddr = None\n            req = (((req + chr(3).encode()) + chr(len(destaddr)).encode()) + destaddr)\n        else:\n            ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))\n            req = ((req + chr(1).encode()) + ipaddr)\n    req = (req + struct.pack('>H', destport))\n    self.sendall(req)\n    resp = self.__recvall(4)\n    if (resp[0:1] != chr(5).encode()):\n        self.close()\n        raise GeneralProxyError((1, _generalerrors[1]))\n    elif (resp[1:2] != chr(0).encode()):\n        self.close()\n        if (ord(resp[1:2]) <= 8):\n            raise Socks5Error((ord(resp[1:2]), _socks5errors[ord(resp[1:2])]))\n        else:\n            raise Socks5Error((9, _socks5errors[9]))\n    elif (resp[3:4] == chr(1).encode()):\n        boundaddr = self.__recvall(4)\n    elif (resp[3:4] == chr(3).encode()):\n        resp = (resp + self.recv(1))\n        boundaddr = self.__recvall(ord(resp[4:5]))\n    else:\n        self.close()\n        raise GeneralProxyError((1, _generalerrors[1]))\n    boundport = struct.unpack('>H', self.__recvall(2))[0]\n    self.__proxysockname = (boundaddr, boundport)\n    if (ipaddr != None):\n        self.__proxypeername = (socket.inet_ntoa(ipaddr), destport)\n    else:\n        self.__proxypeername = (destaddr, destport)\n", "label": 1}
{"function": "\n\ndef test_base():\n\n    class ExampleWorker(BaseWorker):\n        qinput = Queue('test_queue')\n        qoutput = Queue('test_output_queue')\n\n        def run(self, job):\n            for x in range(3):\n                (yield (str(job) * (x + 1)))\n    w = ExampleWorker()\n    qi = w.qinput\n    qo = w.qoutput\n    qi.clear()\n    qo.clear()\n    test_jobs = ['foo', 'bar', 'baz']\n    for x in test_jobs:\n        qi.send(x)\n    jobs_run = []\n    for job in w:\n        if (job is None):\n            break\n        jobs_run.append(job)\n    assert (jobs_run == test_jobs)\n    assert (len(qo) == (3 * len(test_jobs)))\n    assert ('foo' in qo)\n    assert ('foofoo' in qo)\n    assert ('bar' in qo)\n    assert ('bazbazbaz' in qo)\n    assert (len(qo) == 0)\n", "label": 1}
{"function": "\n\ndef test_update(self):\n    instance = RegularModel.objects.create(str_field='str', url_field='http://qwe.qw/', email_field='qwe@qwe.qw', int_field=42, long_field=9223372036854775807, float_field=4.2e-09, boolean_field=True, nullboolean_field=None, date_field=datetime(2015, 11, 14, 6, 13, 14, 123000), complexdate_field=datetime(2015, 11, 14, 6, 13, 14, 123456), uuid_field=UUID('36195645-d9d8-4c86-bd88-29143cdb7ad4'), id_field=ObjectId('56467a4ba21aab16872f5867'), decimal_field=(Decimal(1) / Decimal(3)))\n\n    class TestSerializer(DocumentSerializer):\n\n        class Meta():\n            model = RegularModel\n    data = {\n        'str_field': 'str1',\n        'url_field': 'http://qwe1.qw/',\n        'email_field': 'qwe1@qwe.qw',\n        'int_field': 41,\n        'long_field': 9223372036854775801,\n        'float_field': 4.1e-09,\n        'boolean_field': False,\n        'nullboolean_field': True,\n        'date_field': '2015-11-14T06:13:14.121000',\n        'complexdate_field': '2015-11-14T06:13:14.123451',\n        'uuid_field': '36195645-d9d8-4c86-bd88-29143cdb7ad1',\n        'id_field': '56467a4ba21aab16872f5861',\n        'decimal_field': '0.31',\n    }\n    serializer = TestSerializer(instance, data=data)\n    assert serializer.is_valid(), serializer.errors\n    instance = serializer.save()\n    assert (instance.str_field == 'str1')\n    assert (instance.url_field == 'http://qwe1.qw/')\n    assert (instance.email_field == 'qwe1@qwe.qw')\n    assert (instance.int_field == 41)\n    assert (instance.long_field == 9223372036854775801)\n    assert (instance.float_field == 4.1e-09)\n    assert (not instance.boolean_field)\n    assert instance.nullboolean_field\n    assert (instance.date_field == datetime(2015, 11, 14, 6, 13, 14, 121000))\n    assert (instance.complexdate_field == datetime(2015, 11, 14, 6, 13, 14, 123451))\n    assert (instance.uuid_field == UUID('36195645-d9d8-4c86-bd88-29143cdb7ad1'))\n    assert (instance.id_field == ObjectId('56467a4ba21aab16872f5861'))\n    assert (instance.decimal_field == Decimal('0.31'))\n    expected = {\n        'id': str(instance.id),\n        'str_field': 'str1',\n        'url_field': 'http://qwe1.qw/',\n        'email_field': 'qwe1@qwe.qw',\n        'int_field': 41,\n        'long_field': 9223372036854775801,\n        'float_field': 4.1e-09,\n        'boolean_field': False,\n        'nullboolean_field': True,\n        'date_field': '2015-11-14T06:13:14.121000',\n        'complexdate_field': '2015-11-14T06:13:14.123451',\n        'uuid_field': '36195645-d9d8-4c86-bd88-29143cdb7ad1',\n        'id_field': '56467a4ba21aab16872f5861',\n        'decimal_field': '0.31',\n        'custom_field': None,\n    }\n    assert (serializer.data == expected)\n", "label": 1}
{"function": "\n\ndef login(self, request):\n    '\\n        Displays the login form for the given HttpRequest.\\n        '\n    from django.contrib.auth.models import User\n    if (not request.POST.has_key(LOGIN_FORM_KEY)):\n        if request.POST:\n            message = _('Please log in again, because your session has expired.')\n        else:\n            message = ''\n        return self.display_login_form(request, message)\n    if (not request.session.test_cookie_worked()):\n        message = _(\"Looks like your browser isn't configured to accept cookies. Please enable cookies, reload this page, and try again.\")\n        return self.display_login_form(request, message)\n    else:\n        request.session.delete_test_cookie()\n    username = request.POST.get('username', None)\n    password = request.POST.get('password', None)\n    user = authenticate(username=username, password=password)\n    if (user is None):\n        message = ERROR_MESSAGE\n        if ((username is not None) and ('@' in username)):\n            try:\n                user = User.objects.get(email=username)\n            except (User.DoesNotExist, User.MultipleObjectsReturned):\n                pass\n            else:\n                if user.check_password(password):\n                    message = (_(\"Your e-mail address is not your username. Try '%s' instead.\") % user.username)\n        return self.display_login_form(request, message)\n    elif (user.is_active and user.is_staff):\n        login(request, user)\n        return http.HttpResponseRedirect(request.get_full_path())\n    else:\n        return self.display_login_form(request, ERROR_MESSAGE)\n", "label": 1}
{"function": "\n\ndef run_typed_fn(fn, args, backend=None):\n    actual_types = tuple((type_conv.typeof(arg) for arg in args))\n    expected_types = fn.input_types\n    assert (actual_types == expected_types), ('Arg type mismatch, expected %s but got %s' % (expected_types, actual_types))\n    if (backend is None):\n        backend = config.backend\n    if (backend == 'c'):\n        return c_backend.run(fn, args)\n    elif (backend == 'openmp'):\n        return openmp_backend.run(fn, args)\n    elif (backend == 'cuda'):\n        from .. import cuda_backend\n        return cuda_backend.run(fn, args)\n    elif (backend == 'llvm'):\n        from ..llvm_backend.llvm_context import global_context\n        from ..llvm_backend import generic_value_to_python\n        from ..llvm_backend import ctypes_to_generic_value, compile_fn\n        lowered_fn = pipeline.lowering.apply(fn)\n        llvm_fn = compile_fn(lowered_fn).llvm_fn\n        ctypes_inputs = [t.from_python(v) for (v, t) in zip(args, expected_types)]\n        gv_inputs = [ctypes_to_generic_value(cv, t) for (cv, t) in zip(ctypes_inputs, expected_types)]\n        exec_engine = global_context.exec_engine\n        gv_return = exec_engine.run_function(llvm_fn, gv_inputs)\n        return generic_value_to_python(gv_return, fn.return_type)\n    elif (backend == 'interp'):\n        from .. import interp\n        fn = pipeline.loopify(fn)\n        return interp.eval_fn(fn, args)\n    else:\n        assert False, ('Unknown backend %s' % backend)\n", "label": 1}
{"function": "\n\ndef _save_object(self, response, obj, destination_path, overwrite_existing=False, delete_on_failure=True, chunk_size=None):\n    '\\n        Save object to the provided path.\\n\\n        :param response: RawResponse instance.\\n        :type response: :class:`RawResponse`\\n\\n        :param obj: Object instance.\\n        :type obj: :class:`Object`\\n\\n        :param destination_path: Destination directory.\\n        :type destination_path: ``str``\\n\\n        :param delete_on_failure: True to delete partially downloaded object if\\n                                  the download fails.\\n        :type delete_on_failure: ``bool``\\n\\n        :param overwrite_existing: True to overwrite a local path if it already\\n                                   exists.\\n        :type overwrite_existing: ``bool``\\n\\n        :param chunk_size: Optional chunk size\\n            (defaults to ``libcloud.storage.base.CHUNK_SIZE``, 8kb)\\n        :type chunk_size: ``int``\\n\\n        :return: ``True`` on success, ``False`` otherwise.\\n        :rtype: ``bool``\\n        '\n    chunk_size = (chunk_size or CHUNK_SIZE)\n    base_name = os.path.basename(destination_path)\n    if ((not base_name) and (not os.path.exists(destination_path))):\n        raise LibcloudError(value=('Path %s does not exist' % destination_path), driver=self)\n    if (not base_name):\n        file_path = pjoin(destination_path, obj.name)\n    else:\n        file_path = destination_path\n    if (os.path.exists(file_path) and (not overwrite_existing)):\n        raise LibcloudError(value=(('File %s already exists, but ' % file_path) + 'overwrite_existing=False'), driver=self)\n    stream = libcloud.utils.files.read_in_chunks(response, chunk_size)\n    try:\n        data_read = next(stream)\n    except StopIteration:\n        return False\n    bytes_transferred = 0\n    with open(file_path, 'wb') as file_handle:\n        while (len(data_read) > 0):\n            file_handle.write(b(data_read))\n            bytes_transferred += len(data_read)\n            try:\n                data_read = next(stream)\n            except StopIteration:\n                data_read = ''\n    if (int(obj.size) != int(bytes_transferred)):\n        if delete_on_failure:\n            try:\n                os.unlink(file_path)\n            except Exception:\n                pass\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_raise_statement_regular_expression():\n    candidates_ok = [\"some text # raise Exception, 'text'\", \"raise ValueError('text') # raise Exception, 'text'\", \"raise ValueError('text')\", 'raise ValueError', \"raise ValueError('text')\", \"raise ValueError('text') #,\", '\\'\"\"\"This function will raise ValueError, except when it doesn\\'t\"\"\"', \"raise (ValueError('text')\"]\n    str_candidates_fail = [\"raise 'exception'\", \"raise 'Exception'\", 'raise \"exception\"', 'raise \"Exception\"', \"raise 'ValueError'\"]\n    gen_candidates_fail = [\"raise Exception('text') # raise Exception, 'text'\", \"raise Exception('text')\", 'raise Exception', \"raise Exception('text')\", \"raise Exception('text') #,\", \"raise Exception, 'text'\", \"raise Exception, 'text' # raise Exception('text')\", \"raise Exception, 'text' # raise Exception, 'text'\", \">>> raise Exception, 'text'\", \">>> raise Exception, 'text' # raise Exception('text')\", \">>> raise Exception, 'text' # raise Exception, 'text'\"]\n    old_candidates_fail = [\"raise Exception, 'text'\", \"raise Exception, 'text' # raise Exception('text')\", \"raise Exception, 'text' # raise Exception, 'text'\", \">>> raise Exception, 'text'\", \">>> raise Exception, 'text' # raise Exception('text')\", \">>> raise Exception, 'text' # raise Exception, 'text'\", \"raise ValueError, 'text'\", \"raise ValueError, 'text' # raise Exception('text')\", \"raise ValueError, 'text' # raise Exception, 'text'\", \">>> raise ValueError, 'text'\", \">>> raise ValueError, 'text' # raise Exception('text')\", \">>> raise ValueError, 'text' # raise Exception, 'text'\", 'raise(ValueError,', 'raise (ValueError,', 'raise( ValueError,', 'raise ( ValueError,', 'raise(ValueError ,', 'raise (ValueError ,', 'raise( ValueError ,', 'raise ( ValueError ,']\n    for c in candidates_ok:\n        assert (str_raise_re.search(_with_space(c)) is None), c\n        assert (gen_raise_re.search(_with_space(c)) is None), c\n        assert (old_raise_re.search(_with_space(c)) is None), c\n    for c in str_candidates_fail:\n        assert (str_raise_re.search(_with_space(c)) is not None), c\n    for c in gen_candidates_fail:\n        assert (gen_raise_re.search(_with_space(c)) is not None), c\n    for c in old_candidates_fail:\n        assert (old_raise_re.search(_with_space(c)) is not None), c\n", "label": 1}
{"function": "\n\ndef snapshot(urls=['stdout://'], namespace=misc.get_host_ipaddr(), features=defaults.DEFAULT_FEATURES_TO_CRAWL, options=defaults.DEFAULT_CRAWL_OPTIONS, since='BOOT', frequency=(- 1), crawlmode=Modes.INVM, inputfile='Undefined', format='csv', overwrite=False):\n    \"Entrypoint for crawler functionality.\\n\\n    This is the function executed by long running crawler processes. It just\\n    loops sleeping for `frequency` seconds at each crawl interval.  During each\\n    interval, it collects the features listed in `features`, and sends them to\\n    the outputs listed in `urls`.\\n\\n    :param urls: The url used as the output of the snapshot.\\n    :param namespace: This a pointer to a specific system (e.g. IP for INVM).\\n    :param features: List of features to crawl.\\n    :param options: Tree of options with details like what config files.\\n    :param since: Calculate deltas or not. XXX needs some work.\\n    :param frequency: Sleep duration between iterations. -1 means just one run.\\n    :param crawlmode: What's the system we want to crawl.\\n    :param inputfile: Applies to mode.FILE. The frame emitted is this file.\\n    :param format: The format of the frame, defaults to csv.\\n    \"\n    global should_exit\n    saved_args = locals()\n    logger.debug(('snapshot args: %s' % saved_args))\n    assert ('metadata' in options)\n    environment = options.get('environment', defaults.DEFAULT_ENVIRONMENT)\n    (since_timestamp, last_snapshot_time) = get_initial_since_values(since)\n    snapshot_num = 0\n    PR_SET_PDEATHSIG = 1\n    libc.prctl(PR_SET_PDEATHSIG, signal.SIGHUP)\n    signal.signal(signal.SIGHUP, signal_handler_exit)\n    if (crawlmode == Modes.OUTCONTAINER):\n        containers = get_filtered_list_of_containers(options, namespace)\n    while True:\n        snapshot_time = int(time.time())\n        if (crawlmode == Modes.OUTCONTAINER):\n            curr_containers = get_filtered_list_of_containers(options, namespace)\n            deleted = [c for c in containers if (c not in curr_containers)]\n            containers = curr_containers\n            for container in deleted:\n                if options.get('link_container_log_files', False):\n                    container.unlink_logfiles(options)\n            logger.debug(('Crawling %d containers' % len(containers)))\n            for container in containers:\n                logger.info(('Crawling container %s %s %s' % (container.pid, container.short_id, container.namespace)))\n                if options.get('link_container_log_files', False):\n                    container.link_logfiles(options=options)\n                snapshot_container(urls=urls, snapshot_num=snapshot_num, features=features, options=options, format=format, inputfile=inputfile, container=container, since=since, since_timestamp=since_timestamp, overwrite=overwrite)\n        elif (crawlmode in (Modes.INVM, Modes.MOUNTPOINT, Modes.DEVICE, Modes.FILE, Modes.ISCSI)):\n            snapshot_generic(crawlmode=crawlmode, urls=urls, snapshot_num=snapshot_num, features=features, options=options, format=format, inputfile=inputfile, namespace=namespace, since=since, since_timestamp=since_timestamp, overwrite=overwrite)\n        else:\n            raise RuntimeError('Unknown Mode')\n        if ((frequency < 0) or should_exit):\n            logger.info('Bye')\n            break\n        if (since == 'LASTSNAPSHOT'):\n            since_timestamp = snapshot_time\n        time.sleep(frequency)\n        snapshot_num += 1\n", "label": 1}
{"function": "\n\n@dispatch((tuple, list, set, frozenset))\ndef discover(seq):\n    if (not seq):\n        return (var * string)\n    unite = do_one([unite_identical, unite_base, unite_merge_dimensions])\n    if (all((isinstance(item, (tuple, list)) for item in seq)) and (len(set(map(len, seq))) == 1)):\n        columns = list(zip(*seq))\n        try:\n            types = [unite([discover(data) for data in column]).subshape[0] for column in columns]\n            unite = do_one([unite_identical, unite_merge_dimensions, Tuple])\n            return (len(seq) * unite(types))\n        except AttributeError:\n            pass\n    if all((isinstance(item, dict) for item in seq)):\n        keys = sorted(set.union(*(set(d) for d in seq)))\n        columns = [[item.get(key) for item in seq] for key in keys]\n        try:\n            types = [unite([discover(data) for data in column]).subshape[0] for column in columns]\n            return (len(seq) * Record(list(zip(keys, types))))\n        except AttributeError:\n            pass\n    types = list(map(discover, seq))\n    return do_one([unite_identical, unite_merge_dimensions, Tuple])(types)\n", "label": 1}
{"function": "\n\ndef __init__(self, device, args):\n    super(HIDInputMotionEventProvider, self).__init__(device, args)\n    global Window, Keyboard\n    if (Window is None):\n        from kivy.core.window import Window\n    if (Keyboard is None):\n        from kivy.core.window import Keyboard\n    self.input_fn = None\n    self.default_ranges = dict()\n    args = args.split(',')\n    if (not args):\n        Logger.error('HIDInput: Filename missing in configuration')\n        Logger.error('HIDInput: Use /dev/input/event0 for example')\n        return None\n    self.input_fn = args[0]\n    Logger.info(('HIDInput: Read event from <%s>' % self.input_fn))\n    for arg in args[1:]:\n        if (arg == ''):\n            continue\n        arg = arg.split('=')\n        if (len(arg) != 2):\n            Logger.error(('HIDInput: invalid parameter %s, not in key=value format.' % arg))\n            continue\n        (key, value) = arg\n        if (key not in HIDInputMotionEventProvider.options):\n            Logger.error(('HIDInput: unknown %s option' % key))\n            continue\n        try:\n            self.default_ranges[key] = int(value)\n        except ValueError:\n            err = ('HIDInput: invalid value \"%s\" for \"%s\"' % (key, value))\n            Logger.error(err)\n            continue\n        Logger.info(('HIDInput: Set custom %s to %d' % (key, int(value))))\n    if ('rotation' not in self.default_ranges):\n        self.default_ranges['rotation'] = 0\n    elif (self.default_ranges['rotation'] not in (0, 90, 180, 270)):\n        Logger.error('HIDInput: invalid rotation value ({})'.format(self.default_ranges['rotation']))\n        self.default_ranges['rotation'] = 0\n", "label": 1}
{"function": "\n\ndef load_module(self, name):\n    if (not name.startswith(self._vendor_pkg)):\n        raise ImportError((\"Cannot import %s, must be a subpackage of '%s'.\" % (name, self._vendor_name)))\n    if (not ((name == self._vendor_name) or any((name.startswith(pkg) for pkg in self._vendor_pkgs)))):\n        raise ImportError(('Cannot import %s, must be one of %s.' % (name, self._vendor_pkgs)))\n    if (name in sys.modules):\n        return sys.modules[name]\n    try:\n        real_meta_path = sys.meta_path[:]\n        try:\n            sys.meta_path = [m for m in sys.meta_path if (not isinstance(m, VendorAlias))]\n            __import__(name)\n            module = sys.modules[name]\n        finally:\n            for m in sys.meta_path:\n                if (m not in real_meta_path):\n                    real_meta_path.append(m)\n            sys.meta_path = real_meta_path\n    except ImportError:\n        real_name = name[len(self._vendor_pkg):]\n        try:\n            __import__(real_name)\n            module = sys.modules[real_name]\n        except ImportError:\n            raise ImportError((\"No module named '%s'\" % (name,)))\n    sys.modules[name] = module\n    return module\n", "label": 1}
{"function": "\n\ndef test_genetate_with_more_than_one_resource(self):\n    api = APISpecification(version='v1', base_url='http://api.globo.com')\n    api.add_resource(Resource('dogs', paths=[Path('/dogs', methods=[Method('PUT')])]))\n    api.add_resource(Resource('cats', paths=[Path('/cats', methods=[Method('PUT')])]))\n    result = self.gen(api)\n    doc = ElementTree.fromstring(result)\n    assert doc.tag.endswith('application')\n    resource = doc.getchildren()[0]\n    resources = resource.getchildren()\n    assert (len(resources) == 2)\n    assert resources[0].tag.endswith('resource')\n    assert (resources[0].get('path') == '/dogs')\n    method = resources[0].getchildren()[0]\n    assert method.tag.endswith('method')\n    assert (method.get('name') == 'PUT')\n    assert resources[1].tag.endswith('resource')\n    assert (resources[1].get('path') == '/cats')\n    method = resources[1].getchildren()[0]\n    assert method.tag.endswith('method')\n    assert (method.get('name') == 'PUT')\n", "label": 1}
{"function": "\n\ndef get_refs(genome_build, aligner, config):\n    'Retrieve reference genome data from a standard bcbio directory structure.\\n    '\n    ref_collection = tz.get_in(['arvados', 'reference'], config)\n    if (not ref_collection):\n        raise ValueError('Could not find reference collection in bcbio_system YAML for arvados.')\n    cfiles = collection_files(ref_collection, config['arvados'])\n    ref_prefix = None\n    for prefix in ['./%s', './genomes/%s']:\n        cur_prefix = (prefix % genome_build)\n        if any((x.startswith(cur_prefix) for x in cfiles)):\n            ref_prefix = cur_prefix\n            break\n    assert ref_prefix, ('Did not find genome files for %s:\\n%s' % (genome_build, pprint.pformat(cfiles)))\n    out = {\n        \n    }\n    base_targets = (('/%s.fa' % genome_build), '/mainIndex')\n    for dirname in ['seq', 'rtg', aligner]:\n        key = {\n            'seq': 'fasta',\n        }.get(dirname, dirname)\n        cur_files = [x for x in cfiles if x.startswith(('%s/%s/' % (ref_prefix, dirname)))]\n        cur_files = [('keep:%s' % os.path.normpath(os.path.join(ref_collection, x))) for x in cur_files]\n        base_files = [x for x in cur_files if x.endswith(base_targets)]\n        if (len(base_files) > 0):\n            assert (len(base_files) == 1), base_files\n            base_file = base_files[0]\n            del cur_files[cur_files.index(base_file)]\n            out[key] = {\n                'base': base_file,\n                'indexes': cur_files,\n            }\n        else:\n            out[key] = {\n                'indexes': cur_files,\n            }\n    return out\n", "label": 1}
{"function": "\n\ndef _get_and_update_article_content(self, afile, istxt=False):\n    (html, meta, txt, medias) = self._get_article_content(afile)\n    if (not html):\n        return (None, None, None, None)\n    attach = 0\n    if (not meta.attachments):\n        attach = 1\n    elif (meta.attachments[0] == '$ATTACHMENTS'):\n        attach = 2\n    elif (len(meta.attachments) > 0):\n        attach = 3\n    if (medias and (attach > 0)):\n        try:\n            (urls, attachids) = self._update_medias(medias)\n        except OSError as e:\n            slog.error(e)\n            return (None, None, None, None)\n        idstxt = ','.join(attachids)\n        if (attach == 1):\n            txt = ('Attachments: %s\\n%s' % (idstxt, txt))\n            slog.info(('Fill attachments: %s.' % idstxt))\n        elif (attach == 2):\n            txt = Template(txt).safe_substitute(ATTACHMENTS=idstxt)\n            slog.info(('Fill attachments: %s.' % idstxt))\n        elif (attach == 3):\n            slog.info(('Add new attachments: %s.' % idstxt))\n            meta.attachments += attachids\n            txt = re.sub('^Attachments: .*$', ('Attachments: ' + ','.join(meta.attachments)), txt, 0, re.M)\n        i = 0\n        for (path, name) in medias:\n            txt = txt.replace(path, urls[i])\n            i = (i + 1)\n        write_file(afile, txt, newline='\\n')\n        medias = self._get_medias(txt)\n        if medias:\n            slog.error('Medias in the article are maybe wrong!')\n            return (None, None, None, None)\n        (outputdir, baseurl, namepre) = self._get_output_arg(afile)\n        (html, md, txt) = wpcmd.md.convert(txt, outputdir, baseurl, namepre)\n        meta = self._get_article_metadata(md.metadata)\n    return (html, meta, txt, None)\n", "label": 1}
{"function": "\n\ndef code_match(code, select, ignore):\n    if ignore:\n        assert (not isinstance(ignore, unicode))\n        for ignored_code in [c.strip() for c in ignore]:\n            if mutual_startswith(code.lower(), ignored_code.lower()):\n                return False\n    if select:\n        assert (not isinstance(select, unicode))\n        for selected_code in [c.strip() for c in select]:\n            if mutual_startswith(code.lower(), selected_code.lower()):\n                return True\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef MoveFiles(rebalance, is_master):\n    'Commit the received files into the database.'\n    loc = data_store.DB.Location()\n    if (not os.path.exists(loc)):\n        return False\n    if (not os.path.isdir(loc)):\n        return False\n    tempdir = _CreateDirectory(loc, rebalance.id)\n    remove_file = _FileWithRemoveList(loc, rebalance)\n    to_remove = []\n    if os.path.exists(remove_file):\n        to_remove = [line.decode('utf8').rstrip('\\n') for line in open(remove_file, 'r')]\n    for fname in to_remove:\n        if (not fname.startswith(loc)):\n            logging.warning('Wrong file to remove: %s', fname)\n            continue\n        if (not os.path.exists(fname)):\n            logging.warning('File does not exist: %s', fname)\n            continue\n        if (not os.path.isfile(fname)):\n            logging.warning('Not a file: %s', fname)\n            continue\n        os.unlink(fname)\n        logging.info('Removing file %s', fname)\n    try:\n        os.unlink(remove_file)\n    except OSError:\n        pass\n    try:\n        _RecMoveFiles(tempdir, loc, '')\n    except OSError:\n        return False\n    if (not is_master):\n        if tempdir.startswith(loc):\n            shutil.rmtree(tempdir)\n    return True\n", "label": 1}
{"function": "\n\ndef run_parser(self):\n    '\\n        Main work cycle of spider process working in parser-mode.\\n        '\n    self.is_parser_idle.clear()\n    if self.parser_mode:\n        self.stat = Stat(logging_period=None)\n    self.prepare_parser()\n    process_request_count = 0\n    try:\n        recent_task_time = time.time()\n        while True:\n            try:\n                result = self.network_result_queue.get(block=False)\n            except queue.Empty:\n                self.is_parser_idle.set()\n                time.sleep(0.1)\n                self.is_parser_idle.clear()\n                logger_verbose.debug('Network result queue is empty')\n                if self.shutdown_event.is_set():\n                    logger_verbose.debug('Got shutdown event')\n                    return\n            else:\n                process_request_count += 1\n                recent_task_time = time.time()\n                if self.parser_mode:\n                    self.stat.reset()\n                try:\n                    handler = self.find_task_handler(result['task'])\n                except NoTaskHandler as ex:\n                    ex.tb = format_exc()\n                    self.parser_result_queue.put((ex, result['task']))\n                    self.stat.inc('parser:handler-not-found')\n                else:\n                    self.process_network_result_with_handler(result, handler)\n                    self.stat.inc('parser:handler-processed')\n                finally:\n                    if self.parser_mode:\n                        data = {\n                            'type': 'stat',\n                            'counters': self.stat.counters,\n                            'collections': self.stat.collections,\n                        }\n                        self.parser_result_queue.put((data, result['task']))\n                    if self.parser_mode:\n                        if self.parser_requests_per_process:\n                            if (process_request_count >= self.parser_requests_per_process):\n                                break\n    except Exception as ex:\n        logging.error('', exc_info=ex)\n        raise\n", "label": 1}
{"function": "\n\ndef do_attribute_consuming_service(conf, spsso):\n    service_description = service_name = None\n    requested_attributes = []\n    acs = conf.attribute_converters\n    req = conf.getattr('required_attributes', 'sp')\n    if req:\n        requested_attributes.extend(do_requested_attribute(req, acs, is_required='true'))\n    opt = conf.getattr('optional_attributes', 'sp')\n    if opt:\n        requested_attributes.extend(do_requested_attribute(opt, acs))\n    try:\n        if conf.description:\n            try:\n                (text, lang) = conf.description\n            except ValueError:\n                text = conf.description\n                lang = 'en'\n            service_description = [md.ServiceDescription(text=text, lang=lang)]\n    except KeyError:\n        pass\n    try:\n        if conf.name:\n            try:\n                (text, lang) = conf.name\n            except ValueError:\n                text = conf.name\n                lang = 'en'\n            service_name = [md.ServiceName(text=text, lang=lang)]\n    except KeyError:\n        pass\n    if requested_attributes:\n        if (not service_name):\n            service_name = [md.ServiceName(text='', lang='en')]\n        ac_serv = md.AttributeConsumingService(index='1', service_name=service_name, requested_attribute=requested_attributes)\n        if service_description:\n            ac_serv.service_description = service_description\n        spsso.attribute_consuming_service = [ac_serv]\n", "label": 1}
{"function": "\n\ndef dump_video(self, filename, meta_data):\n    '\\n        Dump a video from videolectures.net\\n        '\n    if os.path.exists(filename):\n        if ((self.opts.overwrite is True) and (self.opts.resume is False)):\n            os.remove(filename)\n        elif ((self.opts.overwrite is False) and (self.opts.resume is False)):\n            self.error(DownloadError, ('ERROR: file already exists. ' + 'remove it or use `overwrite`'))\n    try:\n        stdout = open(os.path.devnull, 'w')\n        subprocess.call(['rtmpdump', '-h'], stdout=stdout, stderr=subprocess.STDOUT)\n    except (OSError, IOError):\n        print('ERROR: rtmpdump could not be run. check the binary path.')\n        sys.exit(1)\n    finally:\n        stdout.close()\n    basic_args = ['rtmpdump', '-q', '-r', meta_data['streamer'], '-y', meta_data['source'], '-a', 'vod', '-o', filename]\n    if (self.opts.resume is True):\n        basic_args.append('-e')\n    retval = subprocess.Popen(basic_args)\n    while True:\n        if (retval.poll() is not None):\n            break\n        if (not os.path.exists(filename)):\n            continue\n        previous_size = os.path.getsize(filename)\n        display_size = _convert_display_size(previous_size)\n        self._to_stdout(self._CLEAR_STDOUT, skip_eol=True)\n        self._to_stdout('\\r[rtmpdump] {0}'.format(display_size), skip_eol=True)\n        time.sleep(2.0)\n    if (retval.wait() == 0):\n        current_size = os.path.getsize(filename)\n        display_size = _convert_display_size(current_size)\n        self._to_stdout(self._CLEAR_STDOUT, skip_eol=True)\n        self._to_stdout('\\r[rtmpdump] {0}'.format(display_size))\n        return True\n    else:\n        self.error(DownloadError, ('ERROR: download may be incomplete.' + ' rtmpdump exited with code 1 or 2'))\n        return False\n", "label": 1}
{"function": "\n\ndef expand_to_line(string, startIndex, endIndex):\n    linebreakRe = re.compile('\\\\n')\n    spacesAndTabsRe = re.compile('([ \\\\t]+)')\n    searchIndex = (startIndex - 1)\n    while True:\n        if (searchIndex < 0):\n            newStartIndex = (searchIndex + 1)\n            break\n        char = string[searchIndex:(searchIndex + 1)]\n        if linebreakRe.match(char):\n            newStartIndex = (searchIndex + 1)\n            break\n        else:\n            searchIndex -= 1\n    searchIndex = endIndex\n    while True:\n        if (searchIndex > (len(string) - 1)):\n            newEndIndex = searchIndex\n            break\n        char = string[searchIndex:(searchIndex + 1)]\n        if linebreakRe.match(char):\n            newEndIndex = searchIndex\n            break\n        else:\n            searchIndex += 1\n    s = string[newStartIndex:newEndIndex]\n    r = spacesAndTabsRe.match(s)\n    if (r and (r.end() <= startIndex)):\n        newStartIndex = (newStartIndex + r.end())\n    try:\n        if ((startIndex == newStartIndex) and (endIndex == newEndIndex)):\n            return None\n        else:\n            return utils.create_return_obj(newStartIndex, newEndIndex, string, 'line')\n    except NameError:\n        return None\n", "label": 1}
{"function": "\n\ndef _remotes_on(port, which_end):\n    '\\n    Return a set of ip addrs active tcp connections\\n    '\n    port = int(port)\n    ret = set()\n    proc_available = False\n    for statf in ['/proc/net/tcp', '/proc/net/tcp6']:\n        if os.path.isfile(statf):\n            proc_available = True\n            with salt.utils.fopen(statf, 'rb') as fp_:\n                for line in fp_:\n                    if line.strip().startswith('sl'):\n                        continue\n                    iret = _parse_tcp_line(line)\n                    sl = next(iter(iret))\n                    if (iret[sl][which_end] == port):\n                        ret.add(iret[sl]['remote_addr'])\n    if (not proc_available):\n        if salt.utils.is_sunos():\n            return _sunos_remotes_on(port, which_end)\n        if salt.utils.is_freebsd():\n            return _freebsd_remotes_on(port, which_end)\n        if salt.utils.is_netbsd():\n            return _netbsd_remotes_on(port, which_end)\n        if salt.utils.is_openbsd():\n            return _openbsd_remotes_on(port, which_end)\n        if salt.utils.is_windows():\n            return _windows_remotes_on(port, which_end)\n        return _linux_remotes_on(port, which_end)\n    return ret\n", "label": 1}
{"function": "\n\ndef validate(self, image_shape, filter_shape, border_mode='valid', subsample=(1, 1), input=None, filters=None, verify_grad=True, non_contiguous=False):\n    '\\n        :param image_shape: The constant shape info passed to corrMM.\\n        :param filter_shape: The constant shape info passed to corrMM.\\n        '\n    N_image_shape = [T.get_scalar_constant_value(T.as_tensor_variable(x)) for x in image_shape]\n    N_filter_shape = [T.get_scalar_constant_value(T.as_tensor_variable(x)) for x in filter_shape]\n    if (input is None):\n        input = self.input\n    if (filters is None):\n        filters = self.filters\n\n    def sym_CorrMM(input, filters):\n        input.name = 'input'\n        filters.name = 'filters'\n        rval = corr.CorrMM(border_mode, subsample)(input, filters)\n        rval.name = 'corr_output'\n        return rval\n    output = sym_CorrMM(input, filters)\n    output.name = ('CorrMM()(%s,%s)' % (input.name, filters.name))\n    theano_corr = theano.function([input, filters], output, mode=self.mode)\n    image_data = numpy.random.random(N_image_shape).astype(self.dtype)\n    filter_data = numpy.random.random(N_filter_shape).astype(self.dtype)\n    if non_contiguous:\n        image_data = numpy.transpose(image_data, axes=(0, 1, 3, 2))\n        image_data = image_data.copy()\n        image_data = numpy.transpose(image_data, axes=(0, 1, 3, 2))\n        filter_data = numpy.transpose(filter_data, axes=(0, 1, 3, 2))\n        filter_data = filter_data.copy()\n        filter_data = numpy.transpose(filter_data, axes=(0, 1, 3, 2))\n        assert (not image_data.flags['CONTIGUOUS'])\n        assert (not filter_data.flags['CONTIGUOUS'])\n    theano_output = theano_corr(image_data, filter_data)\n    filter_data_corr = numpy.array(filter_data[:, :, ::(- 1), ::(- 1)], copy=True, order='C')\n    orig_image_data = image_data\n    img_shape2d = numpy.array(N_image_shape[(- 2):])\n    fil_shape2d = numpy.array(N_filter_shape[(- 2):])\n    subsample2d = numpy.array(subsample)\n    if (border_mode == 'full'):\n        padHW = (fil_shape2d - 1)\n    elif (border_mode == 'valid'):\n        padHW = numpy.array([0, 0])\n    elif (border_mode == 'half'):\n        padHW = numpy.floor((fil_shape2d / 2)).astype('int32')\n    elif isinstance(border_mode, tuple):\n        padHW = numpy.array(border_mode)\n    elif isinstance(border_mode, int):\n        padHW = numpy.array([border_mode, border_mode])\n    else:\n        raise NotImplementedError('Unsupported border_mode {}'.format(border_mode))\n    out_shape2d = (numpy.floor((((img_shape2d + (2 * padHW)) - fil_shape2d) / subsample2d)) + 1)\n    out_shape2d = out_shape2d.astype('int32')\n    out_shape = ((N_image_shape[0], N_filter_shape[0]) + tuple(out_shape2d))\n    ref_output = numpy.zeros(out_shape)\n    ref_output.fill(0)\n    image_data2 = numpy.zeros((N_image_shape[0], N_image_shape[1], (N_image_shape[2] + (2 * padHW[0])), (N_image_shape[3] + (2 * padHW[1]))))\n    image_data2[:, :, padHW[0]:(padHW[0] + N_image_shape[2]), padHW[1]:(padHW[1] + N_image_shape[3])] = image_data\n    image_data = image_data2\n    N_image_shape = image_data.shape\n    for bb in range(N_image_shape[0]):\n        for nn in range(N_filter_shape[0]):\n            for im0 in range(N_image_shape[1]):\n                filter2d = filter_data_corr[nn, im0, :, :]\n                image2d = image_data[bb, im0, :, :]\n                for row in range(ref_output.shape[2]):\n                    irow = (row * subsample[0])\n                    for col in range(ref_output.shape[3]):\n                        icol = (col * subsample[1])\n                        ref_output[(bb, nn, row, col)] += (image2d[irow:(irow + N_filter_shape[2]), icol:(icol + N_filter_shape[3])] * filter2d[::(- 1), ::(- 1)]).sum()\n    self.assertTrue(_allclose(theano_output, ref_output))\n    if verify_grad:\n        utt.verify_grad(sym_CorrMM, [orig_image_data, filter_data])\n", "label": 1}
{"function": "\n\ndef run(self, config):\n    'Runs the stage.\\n\\n    Args:\\n      config: Specifies the source object(s) and sinks.\\n    Yields:\\n      If necessary, a pipeline future.\\n    '\n    storage = s3.S3(config=config.get('s3Credentials'))\n    s3_objects = []\n    if ('object' in config):\n        s3_objects.append(config['object'])\n    if ('objects' in config):\n        objects = config['objects']\n        for s3_obj in storage.ListBucket(objects['bucket'], objects.get('prefix')):\n            s3_objects.append(s3.S3.MakeUrl(objects['bucket'], s3_obj))\n    s3_objects = zip(s3_objects, config['sinks'])\n    (s3_obj, gcs_obj) = s3_objects.pop()\n    sub_stages = []\n    if ('objects' in config):\n        config.pop('objects')\n    for (next_s3_obj, next_gcs_obj) in s3_objects:\n        cfg = copy.deepcopy(config)\n        cfg['object'] = next_s3_obj\n        cfg['sinks'] = [next_gcs_obj]\n        s = (yield S3Input(cfg))\n        sub_stages.append(s)\n    start = config.get('start')\n    if (not start):\n        start = 0\n        config['start'] = 0\n    length = config.get('length')\n    if (not length):\n        length = storage.StatObject(s3_obj)['size']\n        config['length'] = length\n    if ('shardSize' not in config):\n        config['shardSize'] = self.REQUEST_CHUNK_SIZE\n    (shards, compositors) = self.ShardStage(config)\n    if (shards and compositors):\n        with pipeline.After(*[(yield shard) for shard in shards]):\n            _ = [(yield compositor) for compositor in compositors]\n    else:\n        handler = _S3ReadBufferHandler(s3_obj, gcs_obj, config.get('shardPrefix'))\n        storage.ReadObject(url=s3_obj, handler=handler.Handle, start=start, length=length)\n        comp_stage = common.Ignore()\n        if handler.chunk_urls:\n            comp_config = {\n                'contentType': handler.content_type,\n                'sources': handler.chunk_urls,\n                'sinks': [gcs_obj],\n            }\n            comp_stage = (yield gcscompositor.GcsCompositor(comp_config))\n            sub_stages.append(comp_stage)\n        (yield common.Append(*sub_stages))\n", "label": 1}
{"function": "\n\ndef __init__(self, settings_module):\n    for setting in dir(global_settings):\n        if (setting == setting.upper()):\n            setattr(self, setting, getattr(global_settings, setting))\n    self.SETTINGS_MODULE = settings_module\n    try:\n        mod = importlib.import_module(self.SETTINGS_MODULE)\n    except ImportError as e:\n        raise ImportError((\"Could not import settings '%s' (Is it on sys.path?): %s\" % (self.SETTINGS_MODULE, e)))\n    tuple_settings = ('INSTALLED_APPS', 'TEMPLATE_DIRS')\n    for setting in dir(mod):\n        if (setting == setting.upper()):\n            setting_value = getattr(mod, setting)\n            if ((setting in tuple_settings) and isinstance(setting_value, six.string_types)):\n                warnings.warn(('The %s setting must be a tuple. Please fix your settings, as auto-correction is now deprecated.' % setting), PendingDeprecationWarning)\n                setting_value = (setting_value,)\n            setattr(self, setting, setting_value)\n    if (not self.SECRET_KEY):\n        raise ImproperlyConfigured('The SECRET_KEY setting must not be empty.')\n    if (hasattr(time, 'tzset') and self.TIME_ZONE):\n        zoneinfo_root = '/usr/share/zoneinfo'\n        if (os.path.exists(zoneinfo_root) and (not os.path.exists(os.path.join(zoneinfo_root, *self.TIME_ZONE.split('/'))))):\n            raise ValueError(('Incorrect timezone setting: %s' % self.TIME_ZONE))\n        os.environ['TZ'] = self.TIME_ZONE\n        time.tzset()\n", "label": 1}
{"function": "\n\ndef _validate_secure_origin(self, logger, location):\n    parsed = urllib_parse.urlparse(str(location))\n    origin = (parsed.scheme, parsed.hostname, parsed.port)\n    for secure_origin in (SECURE_ORIGINS + self.secure_origins):\n        if ((origin[0] != secure_origin[0]) and (secure_origin[0] != '*')):\n            continue\n        try:\n            addr = ipaddress.ip_address((origin[1] if (isinstance(origin[1], six.text_type) or (origin[1] is None)) else origin[1].decode('utf8')))\n            network = ipaddress.ip_network((secure_origin[1] if isinstance(secure_origin[1], six.text_type) else secure_origin[1].decode('utf8')))\n        except ValueError:\n            if ((origin[1] != secure_origin[1]) and (secure_origin[1] != '*')):\n                continue\n        else:\n            if (addr not in network):\n                continue\n        if ((origin[2] != secure_origin[2]) and (secure_origin[2] != '*') and (secure_origin[2] is not None)):\n            continue\n        return True\n    logger.warning(\"The repository located at %s is not a trusted or secure host and is being ignored. If this repository is available via HTTPS it is recommended to use HTTPS instead, otherwise you may silence this warning and allow it anyways with '--trusted-host %s'.\", parsed.hostname, parsed.hostname)\n    return False\n", "label": 1}
{"function": "\n\ndef parse_body_arguments(content_type, body, arguments, files, headers=None):\n    'Parses a form request body.\\n\\n    Supports ``application/x-www-form-urlencoded`` and\\n    ``multipart/form-data``.  The ``content_type`` parameter should be\\n    a string and ``body`` should be a byte string.  The ``arguments``\\n    and ``files`` parameters are dictionaries that will be updated\\n    with the parsed contents.\\n    '\n    if (headers and ('Content-Encoding' in headers)):\n        gen_log.warning('Unsupported Content-Encoding: %s', headers['Content-Encoding'])\n        return\n    if content_type.startswith('application/x-www-form-urlencoded'):\n        try:\n            uri_arguments = parse_qs_bytes(native_str(body), keep_blank_values=True)\n        except Exception as e:\n            gen_log.warning('Invalid x-www-form-urlencoded body: %s', e)\n            uri_arguments = {\n                \n            }\n        for (name, values) in uri_arguments.items():\n            if values:\n                arguments.setdefault(name, []).extend(values)\n    elif content_type.startswith('multipart/form-data'):\n        fields = content_type.split(';')\n        for field in fields:\n            (k, sep, v) = field.strip().partition('=')\n            if ((k == 'boundary') and v):\n                parse_multipart_form_data(utf8(v), body, arguments, files)\n                break\n        else:\n            gen_log.warning('Invalid multipart/form-data')\n", "label": 1}
{"function": "\n\n@require_POST\n@anonymous_csrf\ndef watch_question(request, question_id):\n    'Start watching a question for replies or solution.'\n    question = get_object_or_404(Question, pk=question_id, is_spam=False)\n    form = WatchQuestionForm(request.user, request.POST)\n    msg = None\n    if form.is_valid():\n        user_or_email = (request.user if request.user.is_authenticated() else form.cleaned_data['email'])\n        try:\n            if (form.cleaned_data['event_type'] == 'reply'):\n                QuestionReplyEvent.notify(user_or_email, question)\n            else:\n                QuestionSolvedEvent.notify(user_or_email, question)\n            statsd.incr('questions.watches.new')\n        except ActivationRequestFailed:\n            msg = _('Could not send a message to that email address.')\n    if request.is_ajax():\n        if form.is_valid():\n            msg = (msg or (_('You will be notified of updates by email.') if request.user.is_authenticated() else _('You should receive an email shortly to confirm your subscription.')))\n            return HttpResponse(json.dumps({\n                'message': msg,\n            }))\n        if request.POST.get('from_vote'):\n            tmpl = 'questions/includes/question_vote_thanks.html'\n        else:\n            tmpl = 'questions/includes/email_subscribe.html'\n        html = render_to_string(tmpl, context={\n            'question': question,\n            'watch_form': form,\n        }, request=request)\n        return HttpResponse(json.dumps({\n            'html': html,\n        }))\n    if msg:\n        messages.add_message(request, messages.ERROR, msg)\n    return HttpResponseRedirect(question.get_absolute_url())\n", "label": 1}
{"function": "\n\ndef Run(self):\n    'The main run method of the client.\\n\\n    This method does not normally return. Only if there have been more than\\n    Client.connection_error_limit failures, the method returns and allows the\\n    client to exit.\\n    '\n    while True:\n        if (self.http_manager.consecutive_connection_errors > config_lib.CONFIG['Client.connection_error_limit']):\n            return\n        self.client_worker.SendNannyMessage()\n        now = time.time()\n        if (now > (self.last_foreman_check + config_lib.CONFIG['Client.foreman_check_frequency'])):\n            try:\n                self.client_worker.SendReply(rdf_protodict.DataBlob(), session_id=rdfvalue.FlowSessionID(flow_name='Foreman'), priority=rdf_flows.GrrMessage.Priority.LOW_PRIORITY, require_fastpoll=False, blocking=False)\n                self.last_foreman_check = now\n            except Queue.Full:\n                pass\n        try:\n            self.RunOnce()\n        except Exception:\n            logging.warn('Uncaught exception caught: %s', traceback.format_exc())\n            if flags.FLAGS.debug:\n                pdb.post_mortem()\n        if (self.client_worker.MemoryExceeded() and (not self.client_worker.IsActive()) and (self.client_worker.InQueueSize() == 0) and (self.client_worker.OutQueueSize() == 0)):\n            logging.warning('Memory exceeded - exiting.')\n            self.client_worker.SendClientAlert('Memory limit exceeded, exiting.')\n            self.client_worker.MemoryExceeded = (lambda : True)\n            self.RunOnce()\n            sys.exit((- 1))\n        self.timer.Wait()\n", "label": 1}
{"function": "\n\ndef permits(self, context, principals, permission):\n    'Returns True or False depending if the token with the specified\\n        principals has access to the given permission.\\n        '\n    principals = set(principals)\n    permissions_required = VIEWS_PERMISSIONS_REQUIRED[permission]\n    current_permissions = set()\n    if principals.intersection(self.model_creators):\n        current_permissions.add('create_model')\n    if principals.intersection(self.token_creators):\n        current_permissions.add('create_token')\n    if principals.intersection(self.token_managers):\n        current_permissions.add('manage_token')\n    model_id = context.model_id\n    if (model_id is not None):\n        try:\n            model_permissions = context.db.get_model_permissions(model_id)\n        except backend_exceptions.ModelNotFound:\n            model_permissions = {\n                \n            }\n            if (permission != 'post_model'):\n                return True\n        finally:\n            for (perm_name, credentials_ids) in iteritems(model_permissions):\n                if principals.intersection(credentials_ids):\n                    current_permissions.add(perm_name)\n    record_id = context.record_id\n    if (record_id is not None):\n        try:\n            authors = context.db.get_record_authors(model_id, record_id)\n        except backend_exceptions.RecordNotFound:\n            authors = []\n        finally:\n            if (not principals.intersection(authors)):\n                current_permissions -= AUTHORS_PERMISSIONS\n    logger.debug('Current permissions: %s', current_permissions)\n    context.request.permissions = current_permissions\n    context.request.principals = principals\n    return permissions_required.matches(current_permissions)\n", "label": 1}
{"function": "\n\ndef acquire(self, blocking=True, timeout=None):\n    \"\\n        Acquire the lock. By defaults blocks and waits forever.\\n\\n        :param blocking: Block until lock is obtained or return immediately.\\n        :type blocking: bool\\n        :param timeout: Don't wait forever to acquire the lock.\\n        :type timeout: float or None\\n\\n        :returns: Was the lock acquired?\\n        :rtype: bool\\n\\n        :raises: :exc:`~kazoo.exceptions.LockTimeout` if the lock\\n                 wasn't acquired within `timeout` seconds.\\n\\n        .. versionadded:: 1.1\\n            The timeout option.\\n        \"\n\n    def _acquire_lock():\n        got_it = self._lock.acquire(False)\n        if (not got_it):\n            raise ForceRetryError()\n        return True\n    retry = self._retry.copy()\n    retry.deadline = timeout\n    locked = self._lock.acquire(False)\n    if ((not locked) and (not blocking)):\n        return False\n    if (not locked):\n        try:\n            locked = retry(_acquire_lock)\n        except RetryFailedError:\n            return False\n    already_acquired = self.is_acquired\n    try:\n        gotten = False\n        try:\n            gotten = retry(self._inner_acquire, blocking=blocking, timeout=timeout)\n        except RetryFailedError:\n            pass\n        except KazooException:\n            exc_info = sys.exc_info()\n            if (not already_acquired):\n                self._best_effort_cleanup()\n                self.cancelled = False\n            six.reraise(exc_info[0], exc_info[1], exc_info[2])\n        if gotten:\n            self.is_acquired = gotten\n        if ((not gotten) and (not already_acquired)):\n            self._best_effort_cleanup()\n        return gotten\n    finally:\n        self._lock.release()\n", "label": 1}
{"function": "\n\ndef getframeinfo(frame, context=1):\n    '\\n    Get information about a frame or traceback object.\\n\\n    A tuple of five things is returned: the filename, the line number of\\n    the current line, the function name, a list of lines of context from\\n    the source code, and the index of the current line within that list.\\n    The optional second argument specifies the number of lines of context\\n    to return, which are centered around the current line.\\n\\n    This originally comes from ``inspect`` but is modified to handle issues\\n    with ``findsource()``.\\n    '\n    if inspect.istraceback(frame):\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n    if (not inspect.isframe(frame)):\n        raise TypeError('arg is not a frame or traceback object')\n    filename = (inspect.getsourcefile(frame) or inspect.getfile(frame))\n    if (context > 0):\n        start = ((lineno - 1) - (context // 2))\n        try:\n            (lines, lnum) = inspect.findsource(frame)\n        except Exception:\n            first_lines = lines = index = None\n        else:\n            start = max(start, 1)\n            start = max(0, min(start, (len(lines) - context)))\n            first_lines = lines[:2]\n            lines = lines[start:(start + context)]\n            index = ((lineno - 1) - start)\n    else:\n        first_lines = lines = index = None\n    if (first_lines and isinstance(first_lines[0], bytes)):\n        encoding = 'ascii'\n        for line in first_lines[:2]:\n            match = re.search(b'coding[:=]\\\\s*([-\\\\w.]+)', line)\n            if match:\n                encoding = match.group(1).decode('ascii')\n                break\n        lines = [line.decode(encoding, 'replace') for line in lines]\n    if hasattr(inspect, 'Traceback'):\n        return inspect.Traceback(filename, lineno, frame.f_code.co_name, lines, index)\n    else:\n        return (filename, lineno, frame.f_code.co_name, lines, index)\n", "label": 1}
{"function": "\n\ndef set_dns(name, dnsservers=None, searchdomains=None, path=None):\n    '\\n    .. versionchanged:: 2015.5.0\\n        The ``dnsservers`` and ``searchdomains`` parameters can now be passed\\n        as a comma-separated list.\\n\\n    Update /etc/resolv.confo\\n\\n    path\\n\\n        path to the container parent\\n        default: /var/lib/lxc (system default)\\n\\n        .. versionadded:: 2015.8.0\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt myminion lxc.set_dns ubuntu \"[\\'8.8.8.8\\', \\'4.4.4.4\\']\"\\n\\n    '\n    if (dnsservers is None):\n        dnsservers = ['8.8.8.8', '4.4.4.4']\n    elif (not isinstance(dnsservers, list)):\n        try:\n            dnsservers = dnsservers.split(',')\n        except AttributeError:\n            raise SaltInvocationError(\"Invalid input for 'dnsservers' parameter\")\n    if (searchdomains is None):\n        searchdomains = []\n    elif (not isinstance(searchdomains, list)):\n        try:\n            searchdomains = searchdomains.split(',')\n        except AttributeError:\n            raise SaltInvocationError(\"Invalid input for 'searchdomains' parameter\")\n    dns = ['nameserver {0}'.format(x) for x in dnsservers]\n    dns.extend(['search {0}'.format(x) for x in searchdomains])\n    dns = ('\\n'.join(dns) + '\\n')\n    rstr = __salt__['test.rand_str']()\n    script = '/sbin/{0}_dns.sh'.format(rstr)\n    DNS_SCRIPT = '\\n'.join(['#!/usr/bin/env bash', 'if [ -h /etc/resolv.conf ];then', ' if [ \"x$(readlink /etc/resolv.conf)\" = \"x../run/resolvconf/resolv.conf\" ];then', '  if [ ! -d /run/resolvconf/ ];then', '   mkdir -p /run/resolvconf', '  fi', '  cat > /etc/resolvconf/resolv.conf.d/head <<EOF', dns, 'EOF', '', ' fi', 'fi', 'cat > /etc/resolv.conf <<EOF', dns, 'EOF', ''])\n    result = run_all(name, 'tee {0}'.format(script), path=path, stdin=DNS_SCRIPT, python_shell=True)\n    if (result['retcode'] == 0):\n        result = run_all(name, 'sh -c \"chmod +x {0};{0}\"'.format(script), path=path, python_shell=True)\n    run_all(name, 'sh -c \\'if [ -f \"{0}\" ];then rm -f \"{0}\";fi\\''.format(script), path=path, python_shell=True)\n    if (result['retcode'] != 0):\n        error = \"Unable to write to /etc/resolv.conf in container '{0}'\".format(name)\n        if result['stderr']:\n            error += ': {0}'.format(result['stderr'])\n        raise CommandExecutionError(error)\n    return True\n", "label": 1}
{"function": "\n\ndef parse_one_cond(tokens, name, context):\n    ((first, pos), tokens) = (tokens[0], tokens[1:])\n    content = []\n    if first.endswith(':'):\n        first = first[:(- 1)]\n    if first.startswith('if '):\n        part = ('if', pos, first[3:].lstrip(), content)\n    elif first.startswith('elif '):\n        part = ('elif', pos, first[5:].lstrip(), content)\n    elif (first == 'else'):\n        part = ('else', pos, None, content)\n    else:\n        assert 0, ('Unexpected token %r at %s' % (first, pos))\n    while 1:\n        if (not tokens):\n            raise TemplateError('No {{endif}}', position=pos, name=name)\n        if (isinstance(tokens[0], tuple) and ((tokens[0][0] == 'endif') or tokens[0][0].startswith('elif ') or (tokens[0][0] == 'else'))):\n            return (part, tokens)\n        (next_chunk, tokens) = parse_expr(tokens, name, context)\n        content.append(next_chunk)\n", "label": 1}
{"function": "\n\ndef raw_interactive(application, path='', raise_on_wsgi_error=False, **environ):\n    '\\n    Runs the application in a fake environment.\\n    '\n    assert ('path_info' not in environ), 'argument list changed'\n    if raise_on_wsgi_error:\n        errors = ErrorRaiser()\n    else:\n        errors = six.BytesIO()\n    basic_environ = {\n        'REQUEST_METHOD': 'GET',\n        'SCRIPT_NAME': '',\n        'PATH_INFO': '',\n        'SERVER_NAME': 'localhost',\n        'SERVER_PORT': '80',\n        'SERVER_PROTOCOL': 'HTTP/1.0',\n        'wsgi.version': (1, 0),\n        'wsgi.url_scheme': 'http',\n        'wsgi.input': six.BytesIO(),\n        'wsgi.errors': errors,\n        'wsgi.multithread': False,\n        'wsgi.multiprocess': False,\n        'wsgi.run_once': False,\n    }\n    if path:\n        (_, _, path_info, query, fragment) = urlsplit(str(path))\n        path_info = unquote(path_info)\n        (path_info, query) = (str(path_info), str(query))\n        basic_environ['PATH_INFO'] = path_info\n        if query:\n            basic_environ['QUERY_STRING'] = query\n    for (name, value) in environ.items():\n        name = name.replace('__', '.')\n        basic_environ[name] = value\n    if (('SERVER_NAME' in basic_environ) and ('HTTP_HOST' not in basic_environ)):\n        basic_environ['HTTP_HOST'] = basic_environ['SERVER_NAME']\n    istream = basic_environ['wsgi.input']\n    if isinstance(istream, bytes):\n        basic_environ['wsgi.input'] = six.BytesIO(istream)\n        basic_environ['CONTENT_LENGTH'] = len(istream)\n    data = {\n        \n    }\n    output = []\n    headers_set = []\n    headers_sent = []\n\n    def start_response(status, headers, exc_info=None):\n        if exc_info:\n            try:\n                if headers_sent:\n                    six.reraise(exc_info[0], exc_info[1], exc_info[2])\n            finally:\n                exc_info = None\n        elif headers_set:\n            raise AssertionError('Headers already set and no exc_info!')\n        headers_set.append(True)\n        data['status'] = status\n        data['headers'] = headers\n        return output.append\n    app_iter = application(basic_environ, start_response)\n    try:\n        try:\n            for s in app_iter:\n                if (not isinstance(s, six.binary_type)):\n                    raise ValueError(('The app_iter response can only contain bytes (not unicode); got: %r' % s))\n                headers_sent.append(True)\n                if (not headers_set):\n                    raise AssertionError('Content sent w/o headers!')\n                output.append(s)\n        except TypeError as e:\n            e.args = (((e.args[0] + (' iterable: %r' % app_iter)),) + e.args[1:])\n            raise\n    finally:\n        if hasattr(app_iter, 'close'):\n            app_iter.close()\n    return (data['status'], data['headers'], b''.join(output), errors.getvalue())\n", "label": 1}
{"function": "\n\ndef _sparse_fruchterman_reingold(A, dim=2, k=None, pos=None, fixed=None, iterations=50):\n    try:\n        import numpy as np\n    except ImportError:\n        raise ImportError('_sparse_fruchterman_reingold() requires numpy: http://scipy.org/ ')\n    try:\n        (nnodes, _) = A.shape\n    except AttributeError:\n        raise nx.NetworkXError('fruchterman_reingold() takes an adjacency matrix as input')\n    try:\n        from scipy.sparse import spdiags, coo_matrix\n    except ImportError:\n        raise ImportError('_sparse_fruchterman_reingold() scipy numpy: http://scipy.org/ ')\n    try:\n        A = A.tolil()\n    except:\n        A = coo_matrix(A).tolil()\n    if (pos == None):\n        pos = np.asarray(np.random.random((nnodes, dim)), dtype=A.dtype)\n    else:\n        pos = pos.astype(A.dtype)\n    if (fixed == None):\n        fixed = []\n    if (k is None):\n        k = np.sqrt((1.0 / nnodes))\n    t = 0.1\n    dt = (t / float((iterations + 1)))\n    displacement = np.zeros((dim, nnodes))\n    for iteration in range(iterations):\n        displacement *= 0\n        for i in range(A.shape[0]):\n            if (i in fixed):\n                continue\n            delta = (pos[i] - pos).T\n            distance = np.sqrt((delta ** 2).sum(axis=0))\n            distance = np.where((distance < 0.01), 0.01, distance)\n            Ai = np.asarray(A.getrowview(i).toarray())\n            displacement[:, i] += (delta * (((k * k) / (distance ** 2)) - ((Ai * distance) / k))).sum(axis=1)\n        length = np.sqrt((displacement ** 2).sum(axis=0))\n        length = np.where((length < 0.01), 0.1, length)\n        pos += ((displacement * t) / length).T\n        t -= dt\n        pos = _rescale_layout(pos)\n    return pos\n", "label": 1}
{"function": "\n\ndef _create_port(self, port_client, instance, network_id, port_req_body, fixed_ip=None, security_group_ids=None, available_macs=None, dhcp_opts=None):\n    'Attempts to create a port for the instance on the given network.\\n\\n        :param port_client: The client to use to create the port.\\n        :param instance: Create the port for the given instance.\\n        :param network_id: Create the port on the given network.\\n        :param port_req_body: Pre-populated port request. Should have the\\n            device_id, device_owner, and any required neutron extension values.\\n        :param fixed_ip: Optional fixed IP to use from the given network.\\n        :param security_group_ids: Optional list of security group IDs to\\n            apply to the port.\\n        :param available_macs: Optional set of available MAC addresses,\\n            from which one will be used at random.\\n        :param dhcp_opts: Optional DHCP options.\\n        :returns: ID of the created port.\\n        :raises PortLimitExceeded: If neutron fails with an OverQuota error.\\n        :raises NoMoreFixedIps: If neutron fails with\\n            IpAddressGenerationFailure error.\\n        :raises: PortBindingFailed: If port binding failed.\\n        '\n    try:\n        if fixed_ip:\n            port_req_body['port']['fixed_ips'] = [{\n                'ip_address': str(fixed_ip),\n            }]\n        port_req_body['port']['network_id'] = network_id\n        port_req_body['port']['admin_state_up'] = True\n        port_req_body['port']['tenant_id'] = instance.project_id\n        if security_group_ids:\n            port_req_body['port']['security_groups'] = security_group_ids\n        if (available_macs is not None):\n            if (not available_macs):\n                raise exception.PortNotFree(instance=instance.uuid)\n            mac_address = available_macs.pop()\n            port_req_body['port']['mac_address'] = mac_address\n        if (dhcp_opts is not None):\n            port_req_body['port']['extra_dhcp_opts'] = dhcp_opts\n        port = port_client.create_port(port_req_body)\n        port_id = port['port']['id']\n        if (port['port'].get('binding:vif_type') == network_model.VIF_TYPE_BINDING_FAILED):\n            port_client.delete_port(port_id)\n            raise exception.PortBindingFailed(port_id=port_id)\n        LOG.debug('Successfully created port: %s', port_id, instance=instance)\n        return port_id\n    except neutron_client_exc.InvalidIpForNetworkClient:\n        LOG.warning(_LW('Neutron error: %(ip)s is not a valid IP address for network %(network_id)s.'), {\n            'ip': fixed_ip,\n            'network_id': network_id,\n        }, instance=instance)\n        msg = (_('Fixed IP %(ip)s is not a valid ip address for network %(network_id)s.') % {\n            'ip': fixed_ip,\n            'network_id': network_id,\n        })\n        raise exception.InvalidInput(reason=msg)\n    except neutron_client_exc.IpAddressInUseClient:\n        LOG.warning(_LW('Neutron error: Fixed IP %s is already in use.'), fixed_ip, instance=instance)\n        msg = (_('Fixed IP %s is already in use.') % fixed_ip)\n        raise exception.FixedIpAlreadyInUse(message=msg)\n    except neutron_client_exc.OverQuotaClient:\n        LOG.warning(_LW('Neutron error: Port quota exceeded in tenant: %s'), port_req_body['port']['tenant_id'], instance=instance)\n        raise exception.PortLimitExceeded()\n    except neutron_client_exc.IpAddressGenerationFailureClient:\n        LOG.warning(_LW('Neutron error: No more fixed IPs in network: %s'), network_id, instance=instance)\n        raise exception.NoMoreFixedIps(net=network_id)\n    except neutron_client_exc.MacAddressInUseClient:\n        LOG.warning(_LW('Neutron error: MAC address %(mac)s is already in use on network %(network)s.'), {\n            'mac': mac_address,\n            'network': network_id,\n        }, instance=instance)\n        raise exception.PortInUse(port_id=mac_address)\n    except neutron_client_exc.NeutronClientException:\n        with excutils.save_and_reraise_exception():\n            LOG.exception(_LE('Neutron error creating port on network %s'), network_id, instance=instance)\n", "label": 1}
{"function": "\n\n@execute_count(3)\ndef test_iteration(self):\n    ' Tests that iterating over a query set pulls back all of the expected results '\n    q = TestModel.objects(test_id=0)\n    compare_set = set([(0, 5), (1, 10), (2, 15), (3, 20)])\n    for t in q:\n        val = (t.attempt_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n    q = TestModel.objects(attempt_id=3).allow_filtering()\n    assert (len(q) == 3)\n    compare_set = set([(0, 20), (1, 20), (2, 75)])\n    for t in q:\n        val = (t.test_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n    q = TestModel.objects((TestModel.attempt_id == 3)).allow_filtering()\n    assert (len(q) == 3)\n    compare_set = set([(0, 20), (1, 20), (2, 75)])\n    for t in q:\n        val = (t.test_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n", "label": 1}
{"function": "\n\ndef run(self, edit=None):\n    if self.view.is_dirty():\n        return status('Save the file first.')\n    self.preview_opened = False\n    in_file = self.view.file_name()\n    in_tuple = file_path_tuple(in_file)\n    ext = '.tmTheme'\n    with OutputPanel(self.view.window(), 'csscheme') as out:\n        conv = tuple((c for c in converters.all if c.valid_file(in_file)))\n        if (len(conv) > 1):\n            out.write_line('Found multiple contenders for conversion.\\nIf this happened to you, please tell the developer (me) to add code for this case. Thanks.')\n            return\n        elif (not conv):\n            out.write_line((\"Couldn't match extension against a known converter.\\nKnown extensions are: %s\" % ', '.join((('.' + c.ext) for c in converters.all))))\n            return\n        conv = conv[0]\n        out.set_path(in_tuple.path)\n        executables = settings().get('executables', {\n            \n        })\n        text = conv.convert(out, in_file, executables)\n        if (not text):\n            return\n        self.previewed = (not settings().get('preview_compiled_css'))\n\n        def preview_compiled_css():\n            if (not self.previewed):\n                self.preview_compiled_css(text, conv, in_tuple.base_name)\n                self.previewed = True\n        stylesheet = CSSchemeParser().parse_stylesheet(text)\n        if stylesheet.errors:\n            conv.report_parse_errors(out, in_file, text, stylesheet.errors)\n            preview_compiled_css()\n            return\n        elif (not stylesheet.rules):\n            out.write_line('No CSS data was found')\n            return\n        for (i, r) in enumerate(stylesheet.rules):\n            if ((not r.at_keyword) or (r.at_keyword.strip('@') != 'hidden')):\n                continue\n            if (strvalue(r.value) == 'true'):\n                ext = '.hidden-tmTheme'\n                del stylesheet.rules[i]\n                break\n            else:\n                e = DumpError(r, \"Unrecognized value for 'hidden' at-rule, expected 'true'\")\n                conv.report_dump_error(out, in_file, text, e)\n                preview_compiled_css()\n                return\n        out_file = (in_tuple.no_ext + ext)\n        try:\n            CSSchemeDumper().dump_stylesheet_file(out_file, stylesheet)\n        except DumpError as e:\n            conv.report_dump_error(out, in_file, text, e)\n            if DEBUG:\n                import traceback\n                traceback.print_exc()\n            preview_compiled_css()\n            return\n    status('Build successful')\n    if settings().get('open_after_build'):\n        self.view.window().open_file(out_file)\n", "label": 1}
{"function": "\n\ndef __init__(self, num=None, m=None, szx=None, rawbytes=[]):\n    if rawbytes:\n        assert (num == None)\n        assert (m == None)\n        assert (szx == None)\n    else:\n        assert (num != None)\n        assert (m != None)\n        assert (szx != None)\n    coapOption.__init__(self, d.OPTION_NUM_BLOCK2)\n    if num:\n        self.num = num\n        self.m = m\n        self.szx = szx\n    elif (len(rawbytes) == 1):\n        self.num = ((rawbytes[0] >> 4) & 15)\n        self.m = ((rawbytes[0] >> 3) & 1)\n        self.szx = ((rawbytes[0] >> 0) & 7)\n    elif (len(rawbytes) == 2):\n        self.num = ((rawbytes[0] << 8) | ((rawbytes[1] >> 4) & 15))\n        self.m = ((rawbytes[1] >> 3) & 1)\n        self.szx = ((rawbytes[1] >> 0) & 7)\n    elif (len(rawbytes) == 3):\n        self.num = (((rawbytes[0] << 16) | (rawbytes[1] << 8)) | ((rawbytes[2] >> 4) & 15))\n        self.m = ((rawbytes[2] >> 3) & 1)\n        self.szx = ((rawbytes[2] >> 0) & 7)\n    else:\n        raise ValueError('unexpected Block2 len={0}'.format(len(rawbytes)))\n", "label": 1}
{"function": "\n\ndef test_pop_set_t(session):\n    session2 = get_session()\n    keyid = key('test_sortedset_pop_set_t')\n    set_ = session.set(keyid, {\n        'h': 1,\n        'o': 2,\n        'n': 3,\n        'g': 4.5,\n    }, SortedSet)\n    set2 = session2.get(keyid, SortedSet)\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop()\n        assert (popped == 'h')\n        assert (dict(set2) == {\n            'h': 1,\n            'o': 2,\n            'n': 3,\n            'g': 4.5,\n        })\n        with raises(CommitError):\n            len(set_)\n    assert (dict(set_) == dict(set2) == {\n        'o': 2,\n        'n': 3,\n        'g': 4.5,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop(desc=True)\n        assert (popped == 'g')\n        assert (dict(set2) == {\n            'o': 2,\n            'n': 3,\n            'g': 4.5,\n        })\n        with raises(CommitError):\n            len(set_)\n    assert (dict(set_) == dict(set2) == {\n        'o': 2,\n        'n': 3,\n        'g': 3.5,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop(score=0.5)\n        assert (popped == 'o')\n        assert (dict(set2) == {\n            'o': 2,\n            'n': 3,\n            'g': 3.5,\n        })\n    assert (dict(set_) == dict(set2) == {\n        'o': 1.5,\n        'n': 3,\n        'g': 3.5,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop(remove=0.5)\n        assert (popped == 'o')\n        assert (dict(set2) == {\n            'o': 1.5,\n            'n': 3,\n            'g': 3.5,\n        })\n    assert (dict(set_) == dict(set2) == {\n        'n': 3,\n        'g': 3.5,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.pop(remove=None)\n        assert (popped == 'n')\n        assert (dict(set2) == {\n            'n': 3,\n            'g': 3.5,\n        })\n    assert (dict(set_) == dict(set2) == {\n        'g': 3.5,\n    })\n    set_.clear()\n    with Transaction(session, [keyid]):\n        with raises(KeyError):\n            set_.pop()\n", "label": 1}
{"function": "\n\n@require_POST\ndef handle_bounce(request):\n    '\\n    Handle a bounced email via an SNS webhook.\\n\\n    Parse the bounced message and send the appropriate signal.\\n    For bounce messages the bounce_received signal is called.\\n    For complaint messages the complaint_received signal is called.\\n    See: http://docs.aws.amazon.com/sns/latest/gsg/json-formats.html#http-subscription-confirmation-json\\n    See: http://docs.amazonwebservices.com/ses/latest/DeveloperGuide/NotificationsViaSNS.html\\n    \\n    In addition to email bounce requests this endpoint also supports the SNS\\n    subscription confirmation request. This request is sent to the SNS\\n    subscription endpoint when the subscription is registered.\\n    See: http://docs.aws.amazon.com/sns/latest/gsg/Subscribe.html\\n\\n    For the format of the SNS subscription confirmation request see this URL:\\n    http://docs.aws.amazon.com/sns/latest/gsg/json-formats.html#http-subscription-confirmation-json\\n    \\n    SNS message signatures are verified by default. This funcionality can\\n    be disabled by setting AWS_SES_VERIFY_BOUNCE_SIGNATURES to False.\\n    However, this is not recommended.\\n    See: http://docs.amazonwebservices.com/sns/latest/gsg/SendMessageToHttp.verify.signature.html\\n    '\n    if hasattr(request, 'body'):\n        raw_json = request.body\n    else:\n        raw_json = request.raw_post_data\n    try:\n        notification = json.loads(raw_json)\n    except ValueError as e:\n        logger.warning('Recieved bounce with bad JSON: \"%s\"', e)\n        return HttpResponseBadRequest()\n    if (settings.VERIFY_BOUNCE_SIGNATURES and (not utils.verify_bounce_message(notification))):\n        logger.info('Recieved unverified notification: Type: %s', notification.get('Type'), extra={\n            'notification': notification,\n        })\n        return HttpResponse()\n    if (notification.get('Type') in ('SubscriptionConfirmation', 'UnsubscribeConfirmation')):\n        logger.info('Recieved subscription confirmation: TopicArn: %s', notification.get('TopicArn'), extra={\n            'notification': notification,\n        })\n        subscribe_url = notification.get('SubscribeURL')\n        try:\n            urlopen(subscribe_url).read()\n        except URLError as e:\n            logger.error('Could not confirm subscription: \"%s\"', e, extra={\n                'notification': notification,\n            }, exc_info=True)\n    elif (notification.get('Type') == 'Notification'):\n        try:\n            message = json.loads(notification['Message'])\n        except ValueError as e:\n            logger.warning('Recieved bounce with bad JSON: \"%s\"', e, extra={\n                'notification': notification,\n            })\n        else:\n            mail_obj = message.get('mail')\n            notification_type = message.get('notificationType')\n            if (notification_type == 'Bounce'):\n                bounce_obj = message.get('bounce', {\n                    \n                })\n                feedback_id = bounce_obj.get('feedbackId')\n                bounce_type = bounce_obj.get('bounceType')\n                bounce_subtype = bounce_obj.get('bounceSubType')\n                logger.info('Recieved bounce notification: feedbackId: %s, bounceType: %s, bounceSubType: %s', feedback_id, bounce_type, bounce_subtype, extra={\n                    'notification': notification,\n                })\n                signals.bounce_received.send(sender=handle_bounce, mail_obj=mail_obj, bounce_obj=bounce_obj, raw_message=raw_json)\n            elif (notification_type == 'Complaint'):\n                complaint_obj = message.get('complaint', {\n                    \n                })\n                feedback_id = complaint_obj.get('feedbackId')\n                feedback_type = complaint_obj.get('complaintFeedbackType')\n                logger.info('Recieved complaint notification: feedbackId: %s, feedbackType: %s', feedback_id, feedback_type, extra={\n                    'notification': notification,\n                })\n                signals.complaint_received.send(sender=handle_bounce, mail_obj=mail_obj, complaint_obj=complaint_obj, raw_message=raw_json)\n            else:\n                logger.warning('Recieved unknown notification', extra={\n                    'notification': notification,\n                })\n    else:\n        logger.info('Recieved unknown notification type: %s', notification.get('Type'), extra={\n            'notification': notification,\n        })\n    return HttpResponse()\n", "label": 1}
{"function": "\n\ndef run_cli(self):\n    '\\n        Run the main loop\\n        '\n    print('Version:', __version__)\n    print('Home: http://wharfee.com')\n    history = FileHistory(os.path.expanduser('~/.wharfee-history'))\n    toolbar_handler = create_toolbar_handler(self.get_long_options, self.get_fuzzy_match)\n    layout = create_prompt_layout(message='wharfee> ', lexer=CommandLexer, get_bottom_toolbar_tokens=toolbar_handler, extra_input_processors=[ConditionalProcessor(processor=HighlightMatchingBracketProcessor(chars='[](){}'), filter=(HasFocus(DEFAULT_BUFFER) & (~ IsDone())))])\n    cli_buffer = Buffer(history=history, completer=self.completer, complete_while_typing=Always(), accept_action=AcceptAction.RETURN_DOCUMENT)\n    manager = get_key_manager(self.set_long_options, self.get_long_options, self.set_fuzzy_match, self.get_fuzzy_match)\n    application = Application(style=style_factory(self.theme), layout=layout, buffer=cli_buffer, key_bindings_registry=manager.registry, on_exit=AbortAction.RAISE_EXCEPTION, on_abort=AbortAction.RETRY, ignore_case=True)\n    eventloop = create_eventloop()\n    self.dcli = CommandLineInterface(application=application, eventloop=eventloop)\n    while True:\n        try:\n            document = self.dcli.run()\n            self.handler.handle_input(document.text)\n            if isinstance(self.handler.output, GeneratorType):\n                output_stream(self.handler.command, self.handler.output, self.handler.logs)\n            elif (self.handler.output is not None):\n                lines = format_data(self.handler.command, self.handler.output)\n                click.echo_via_pager('\\n'.join(lines))\n            if self.handler.after:\n                for line in self.handler.after():\n                    click.echo(line)\n            if self.handler.exception:\n                self.logger.warning('An error was handled: %r', self.handler.exception)\n            self.refresh_completions()\n        except OptionError as ex:\n            self.logger.debug('Error: %r.', ex)\n            self.logger.error('traceback: %r', traceback.format_exc())\n            click.secho(ex.msg, fg='red')\n        except KeyboardInterrupt:\n            if self.handler.after:\n                click.echo('')\n                for line in self.handler.after():\n                    click.echo(line)\n            self.refresh_completions()\n        except DockerPermissionException as ex:\n            self.logger.debug('Permission exception: %r.', ex)\n            self.logger.error('traceback: %r', traceback.format_exc())\n            click.secho(ex.message, fg='red')\n        except EOFError:\n            break\n        except Exception as ex:\n            self.logger.debug('Exception: %r.', ex)\n            self.logger.error('traceback: %r', traceback.format_exc())\n            click.secho('{0}'.format(ex), fg='red')\n            break\n    self.revert_less_opts()\n    self.write_config_file()\n    print('Goodbye!')\n", "label": 1}
{"function": "\n\ndef seek(self, offset, whence=0):\n    assert (whence in [0, 1, 2])\n    if (whence == 2):\n        if (offset < 0):\n            raise ValueError('negative seek offset')\n        to_read = None\n    else:\n        if (whence == 0):\n            if (offset < 0):\n                raise ValueError('negative seek offset')\n            dest = offset\n        else:\n            pos = self.__pos\n            if (pos < offset):\n                raise ValueError('seek to before start of file')\n            dest = (pos + offset)\n        end = len_of_seekable(self.__cache)\n        to_read = (dest - end)\n        if (to_read < 0):\n            to_read = 0\n    if (to_read != 0):\n        self.__cache.seek(0, 2)\n        if (to_read is None):\n            assert (whence == 2)\n            self.__cache.write(self.wrapped.read())\n            self.read_complete = True\n            self.__pos = (self.__cache.tell() - offset)\n        else:\n            data = self.wrapped.read(to_read)\n            if (not data):\n                self.read_complete = True\n            else:\n                self.__cache.write(data)\n            self.__pos = dest\n    else:\n        self.__pos = dest\n", "label": 1}
{"function": "\n\ndef test_popitem_t(session):\n    session2 = get_session()\n    keyid = key('test_sortedset_popitem_t')\n    set_ = session.set(keyid, {\n        'h': 1,\n        'o': 2,\n        'n': 3,\n        'g': 4.5,\n    }, SortedSet)\n    set2 = session2.get(keyid, SortedSet)\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.popitem()\n        assert (popped == ('h', 1))\n        assert (dict(set2) == {\n            'h': 1,\n            'o': 2,\n            'n': 3,\n            'g': 4.5,\n        })\n        with raises(CommitError):\n            len(set_)\n    assert (dict(set_) == dict(set2) == {\n        'o': 2,\n        'n': 3,\n        'g': 4.5,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.popitem(desc=True)\n        assert (popped == ('g', 4.5))\n        assert (dict(set2) == {\n            'o': 2,\n            'n': 3,\n            'g': 4.5,\n        })\n        with raises(CommitError):\n            len(set_)\n    assert (dict(set_) == dict(set2) == {\n        'o': 2,\n        'n': 3,\n        'g': 3.5,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.popitem(score=0.5)\n        assert (popped == ('o', 2))\n        assert (dict(set2) == {\n            'o': 2,\n            'n': 3,\n            'g': 3.5,\n        })\n    assert (dict(set_) == dict(set2) == {\n        'o': 1.5,\n        'n': 3,\n        'g': 3.5,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.popitem(remove=0.5)\n        assert (popped == ('o', 1.5))\n        assert (dict(set2) == {\n            'o': 1.5,\n            'n': 3,\n            'g': 3.5,\n        })\n    assert (dict(set_) == dict(set2) == {\n        'n': 3,\n        'g': 3.5,\n    })\n    with Transaction(session, [keyid]):\n        len(set_)\n        popped = set_.popitem(remove=None)\n        assert (popped == ('n', 3))\n        assert (dict(set2) == {\n            'n': 3,\n            'g': 3.5,\n        })\n    assert (dict(set_) == dict(set2) == {\n        'g': 3.5,\n    })\n    set_.clear()\n    with Transaction(session, [keyid]):\n        with raises(KeyError):\n            set_.popitem()\n", "label": 1}
{"function": "\n\ndef slowloris(self, host, num, timeout, port=None):\n    port = (port or 80)\n    timeout = int(timeout)\n    conns = [Conn(host, int(port), 5) for i in range(int(num))]\n    failed = 0\n    packets = 0\n    while (not self.stop_flag.is_set()):\n        for conn in conns:\n            if self.stop_flag.is_set():\n                break\n            if (not conn.connected):\n                if conn.connect():\n                    packets += 3\n            if conn.connected:\n                query = ('?%d' % random.randint(1, 9999999999999))\n                payload = (self.primary_payload % (query, conn.host))\n                try:\n                    conn.send(payload)\n                    packets += 1\n                except socket.error:\n                    pass\n            else:\n                pass\n        for conn in conns:\n            if self.stop_flag.is_set():\n                break\n            if conn.connected:\n                try:\n                    conn.send('X-a: b\\r\\n')\n                    packets += 1\n                except socket.error:\n                    pass\n        gevent.sleep(timeout)\n    return ('%s failed, %s packets sent' % (failed, packets))\n", "label": 1}
{"function": "\n\ndef _get_cgroup_measurements(self, cgroups, ru_child, result):\n    '\\n        This method calculates the exact results for time and memory measurements.\\n        It is not important to call this method as soon as possible after the run.\\n        '\n    logging.debug('Getting cgroup measurements.')\n    cputime_wait = ((ru_child.ru_utime + ru_child.ru_stime) if ru_child else 0)\n    cputime_cgroups = None\n    if (CPUACCT in cgroups):\n        tmp = cgroups.read_cputime()\n        tmp2 = None\n        while (tmp != tmp2):\n            time.sleep(0.1)\n            tmp2 = tmp\n            tmp = cgroups.read_cputime()\n        cputime_cgroups = tmp\n        if ((cputime_wait > 0.5) and ((cputime_wait * 0.95) > cputime_cgroups)):\n            logging.warning('Cputime measured by wait was %s, cputime measured by cgroup was only %s, perhaps measurement is flawed.', cputime_wait, cputime_cgroups)\n            result['cputime'] = cputime_wait\n        else:\n            result['cputime'] = cputime_cgroups\n        for (core, coretime) in enumerate(cgroups.get_value(CPUACCT, 'usage_percpu').split(' ')):\n            try:\n                coretime = int(coretime)\n                if (coretime != 0):\n                    result[('cputime-cpu' + str(core))] = (coretime / 1000000000)\n            except (OSError, ValueError) as e:\n                logging.debug('Could not read CPU time for core %s from kernel: %s', core, e)\n    if (MEMORY in cgroups):\n        memUsageFile = 'memsw.max_usage_in_bytes'\n        if (not cgroups.has_value(MEMORY, memUsageFile)):\n            memUsageFile = 'max_usage_in_bytes'\n        if (not cgroups.has_value(MEMORY, memUsageFile)):\n            logging.warning('Memory-usage is not available due to missing files.')\n        else:\n            try:\n                result['memory'] = int(cgroups.get_value(MEMORY, memUsageFile))\n            except IOError as e:\n                if (e.errno == errno.ENOTSUP):\n                    logging.critical('Kernel does not track swap memory usage, cannot measure memory usage. Please set swapaccount=1 on your kernel command line.')\n                else:\n                    raise e\n    logging.debug('Resource usage of run: walltime=%s, cputime=%s, cgroup-cputime=%s, memory=%s', result['walltime'], cputime_wait, cputime_cgroups, result.get('memory', None))\n", "label": 1}
{"function": "\n\ndef data_get(target, key, default=None):\n    '\\n    Get an item from a list, a dict or an object using \"dot\" notation.\\n\\n    :param target: The target element\\n    :type target: list or dict or object\\n\\n    :param key: The key to get\\n    :type key: string or list\\n\\n    :param default: The default value\\n    :type default: mixed\\n\\n    :rtype: mixed\\n    '\n    from ..support import Collection\n    if (key is None):\n        return target\n    if (not isinstance(key, list)):\n        key = key.split('.')\n    for segment in key:\n        if isinstance(target, (list, tuple)):\n            try:\n                target = target[segment]\n            except IndexError:\n                return value(default)\n        elif isinstance(target, dict):\n            try:\n                target = target[segment]\n            except IndexError:\n                return value(default)\n        elif isinstance(target, Collection):\n            try:\n                target = target[segment]\n            except IndexError:\n                return value(default)\n        else:\n            try:\n                target = getattr(target, segment)\n            except AttributeError:\n                return value(default)\n    return target\n", "label": 1}
{"function": "\n\ndef transform_model_class(cls):\n    if cls.is_subtype_of('django.db.models.base.Model'):\n        core_exceptions = MANAGER.ast_from_module_name('django.core.exceptions')\n        DoesNotExist = Class('DoesNotExist', None)\n        DoesNotExist.bases = core_exceptions.lookup('ObjectDoesNotExist')[1]\n        cls.locals['DoesNotExist'] = [DoesNotExist]\n        MultipleObjectsReturned = Class('MultipleObjectsReturned', None)\n        MultipleObjectsReturned.bases = core_exceptions.lookup('MultipleObjectsReturned')[1]\n        cls.locals['MultipleObjectsReturned'] = [MultipleObjectsReturned]\n        if ('objects' not in cls.locals):\n            try:\n                Manager = MANAGER.ast_from_module_name('django.db.models.manager').lookup('Manager')[1][0]\n                QuerySet = MANAGER.ast_from_module_name('django.db.models.query').lookup('QuerySet')[1][0]\n            except IndexError:\n                pass\n            else:\n                if isinstance(Manager.body[0], Pass):\n                    for (func_name, func_list) in QuerySet.locals.items():\n                        if ((not func_name.startswith('_')) and (func_name not in Manager.locals)):\n                            func = func_list[0]\n                            if (isinstance(func, Function) and ('queryset_only' not in func.instance_attrs)):\n                                f = Function(func_name, None)\n                                f.args = Arguments()\n                                Manager.locals[func_name] = [f]\n                cls.locals['objects'] = [Instance(Manager)]\n        if ('id' not in cls.locals):\n            try:\n                AutoField = MANAGER.ast_from_module_name('django.db.models.fields').lookup('AutoField')[1][0]\n            except IndexError:\n                pass\n            else:\n                cls.locals['id'] = [Instance(AutoField)]\n", "label": 1}
{"function": "\n\n@property\ndef next_time(self):\n    '\\n        Return the OccurringRule or RecurringRule with the closest `dt_start` from now.\\n        '\n    now = timezone.now()\n    recurring_start = occurring_start = None\n    try:\n        occurring_rule = self.occurring_rule\n    except OccurringRule.DoesNotExist:\n        pass\n    else:\n        if (occurring_rule and (occurring_rule.dt_start > now)):\n            occurring_start = (occurring_rule.dt_start, occurring_rule)\n    rrules = self.recurring_rules.filter(finish__gt=now)\n    recurring_starts = [(rule.dt_start, rule) for rule in rrules if (rule.dt_start is not None)]\n    recurring_starts.sort(key=itemgetter(0))\n    try:\n        recurring_start = recurring_starts[0]\n    except IndexError:\n        pass\n    starts = [i for i in (recurring_start, occurring_start) if (i is not None)]\n    starts.sort(key=itemgetter(0))\n    try:\n        return starts[0][1]\n    except IndexError:\n        return None\n", "label": 1}
{"function": "\n\ndef train(self, binaryLabeledMessageList):\n    globalClassifier = self._globalClassifier\n    localClassifier = self._localClassifier\n    trainLog = ''\n    nFolds = 5\n    msgList = map((lambda x: x[0]), binaryLabeledMessageList)\n    y = np.array(map((lambda x: (1 if (x[1] == 'pos') else 0)), binaryLabeledMessageList))\n    globalDFList = []\n    for msg in msgList:\n        v = globalClassifier.predictScore(msg)\n        globalDFList.append(v['pos'])\n    tt = tic()\n    metaClassifier = None\n    if (sum(y) < 2):\n        trainLog += 'There are less than 2 positive data points. using globalClassifier as the classifier\\n'\n        localClassifier = None\n    else:\n        localDFList = ([float('nan')] * len(binaryLabeledMessageList))\n        folds = StratifiedKFold(y, nFolds)\n        for (trainIdx, testIdx) in folds:\n            train = [binaryLabeledMessageList[i] for i in trainIdx]\n            test = [binaryLabeledMessageList[i] for i in testIdx]\n            tmpClassifier = copy.deepcopy(localClassifier)\n            tmpClassifier.train(train)\n            curDFList = []\n            for x in test:\n                v = tmpClassifier.predictScore(x[0])\n                curDFList.append(v['pos'])\n            for i in range(len(test)):\n                localDFList[testIdx[i]] = curDFList[i]\n        assert (countTruth((lambda x: np.isnan(x)), localDFList) == 0)\n        X = np.hstack([np.matrix(globalDFList).T, np.matrix(localDFList).T])\n        metaClassifier = LinearSVC(loss='l2', penalty='l1', dual=False, tol=1e-06, random_state=0)\n        metaClassifier.fit(X, y)\n        trainLog += 'Trained a meta classifier:\\n'\n        coef = metaClassifier.coef_[0]\n        trainLog += ('  coef for (global, local): (%.6f, %.6f)\\n' % (coef[0], coef[1]))\n        trainLog += ('  intercept: %.6f\\n' % metaClassifier.intercept_)\n        localClassifier.train(binaryLabeledMessageList)\n        trainLog += 'Trained a meta classifier'\n    elapsed = toc(tt)\n    trainLog += ('Time taken for training: %.3f (sec)\\n' % elapsed)\n    self._metaClassifier = metaClassifier\n    self._trainLog = trainLog\n    yDeci = [self.predictScore(msg)['pos'] for msg in binaryLabeledMessageList]\n    yy = [(1 if (v == 1) else (- 1)) for v in y]\n    [A, B] = platt.SigmoidTrain(yDeci, yy)\n    plattModel = [A, B]\n    self._plattModel = plattModel\n    pass\n", "label": 1}
{"function": "\n\ndef test_init(self, dataFrame):\n    filterString = 'Foo < 10'\n    datasearch = DataSearch('Test', filterString)\n    assert (datasearch._filterString == filterString)\n    assert isinstance(datasearch._dataFrame, pandas.DataFrame)\n    assert (datasearch.name == 'Test')\n    datasearch = DataSearch('Test2')\n    assert (datasearch._filterString == '')\n    assert isinstance(datasearch._dataFrame, pandas.DataFrame)\n    assert (datasearch.name == 'Test2')\n    datasearch = DataSearch('Test3', dataFrame=dataFrame)\n    assert (datasearch._filterString == '')\n    assert isinstance(datasearch._dataFrame, pandas.DataFrame)\n    assert (datasearch.name == 'Test3')\n    assert (len(datasearch._dataFrame.index) == 3)\n", "label": 1}
{"function": "\n\ndef check_char_lookup(self, lookup):\n    lname = ('field__' + lookup)\n    mymodel = CharSetModel.objects.create(field={'mouldy', 'rotten'})\n    mouldy = CharSetModel.objects.filter(**{\n        lname: 'mouldy',\n    })\n    assert (mouldy.count() == 1)\n    assert (mouldy[0] == mymodel)\n    rotten = CharSetModel.objects.filter(**{\n        lname: 'rotten',\n    })\n    assert (rotten.count() == 1)\n    assert (rotten[0] == mymodel)\n    clean = CharSetModel.objects.filter(**{\n        lname: 'clean',\n    })\n    assert (clean.count() == 0)\n    with pytest.raises(ValueError):\n        list(CharSetModel.objects.filter(**{\n            lname: {'a', 'b'},\n        }))\n    both = CharSetModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) & Q(**{\n        lname: 'rotten',\n    })))\n    assert (both.count() == 1)\n    assert (both[0] == mymodel)\n    either = CharSetModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) | Q(**{\n        lname: 'clean',\n    })))\n    assert (either.count() == 1)\n    not_clean = CharSetModel.objects.exclude(**{\n        lname: 'clean',\n    })\n    assert (not_clean.count() == 1)\n    not_mouldy = CharSetModel.objects.exclude(**{\n        lname: 'mouldy',\n    })\n    assert (not_mouldy.count() == 0)\n", "label": 1}
{"function": "\n\ndef test_private_project(self):\n    cache_id = 'qwerty'\n    cached_image = 'kozmic-cache/{}'.format(cache_id)\n    try:\n        for image_data in docker.images(cached_image):\n            for repo_tag in image_data['RepoTags']:\n                if repo_tag.startswith(cached_image):\n                    docker.remove_image(image_data['Id'])\n                    break\n    except:\n        pass\n    assert (not docker.images(cached_image))\n    build = factories.BuildFactory.create(project=self.project, gh_commit_sha=self.prev_head_sha)\n    hook_call = factories.HookCallFactory.create(hook=self.hook, build=build)\n    job = self._do_job(hook_call)\n    assert (job.return_code == 0)\n    assert (job.stdout == 'Pulling \"{}\" Docker image...\\ninstalled!\\nit works\\nYEAH\\n'.format(self.hook.docker_image))\n    assert docker.images(cached_image)\n    build = factories.BuildFactory.create(project=self.project, gh_commit_sha=self.head_sha)\n    hook_call = factories.HookCallFactory.create(hook=self.hook, build=build)\n    job = self._do_job(hook_call)\n    assert (job.return_code == 0)\n    assert (job.stdout == 'Pulling \"{}\" Docker image...\\nSkipping install script as tracked files did not change...\\nit works\\nYEAH\\n'.format(self.hook.docker_image))\n", "label": 1}
{"function": "\n\ndef _traverse_repos(self, callback, repo_name=None):\n    '\\n        Traverse through all repo files and apply the functionality provided in\\n        the callback to them\\n        '\n    repo_files = []\n    if os.path.exists(self.opts['spm_repos_config']):\n        repo_files.append(self.opts['spm_repos_config'])\n    for (dirpath, dirnames, filenames) in os.walk('{0}.d'.format(self.opts['spm_repos_config'])):\n        for repo_file in filenames:\n            if (not repo_file.endswith('.repo')):\n                continue\n            repo_files.append(repo_file)\n    if (not os.path.exists(self.opts['spm_cache_dir'])):\n        os.makedirs(self.opts['spm_cache_dir'])\n    for repo_file in repo_files:\n        repo_path = '{0}.d/{1}'.format(self.opts['spm_repos_config'], repo_file)\n        with salt.utils.fopen(repo_path) as rph:\n            repo_data = yaml.safe_load(rph)\n            for repo in repo_data:\n                if (repo_data[repo].get('enabled', True) is False):\n                    continue\n                if ((repo_name is not None) and (repo != repo_name)):\n                    continue\n                callback(repo, repo_data[repo])\n", "label": 1}
{"function": "\n\ndef process_packets(self, transaction_id=None, invoked_method=None, timeout=None):\n    start = time()\n    while (self.connected and (transaction_id not in self._invoke_results)):\n        if (timeout and ((time() - start) >= timeout)):\n            raise RTMPTimeoutError('Timeout')\n        packet = self.read_packet()\n        if (packet.type == PACKET_TYPE_INVOKE):\n            try:\n                decoded = decode_amf(packet.body)\n            except IOError:\n                continue\n            try:\n                (method, transaction_id_, obj) = decoded[:3]\n                args = decoded[3:]\n            except ValueError:\n                continue\n            if (method == '_result'):\n                if (len(args) > 0):\n                    result = args[0]\n                else:\n                    result = None\n                self._invoke_results[transaction_id_] = result\n            else:\n                handler = self._invoke_handlers.get(method)\n                if handler:\n                    res = handler(*args)\n                    if (res is not None):\n                        self.call('_result', res, transaction_id=transaction_id_)\n                if (method == invoked_method):\n                    self._invoke_args[invoked_method] = args\n                    break\n            if (transaction_id_ == 1.0):\n                self._connect_result = packet\n            else:\n                self.handle_packet(packet)\n        else:\n            self.handle_packet(packet)\n    if transaction_id:\n        result = self._invoke_results.pop(transaction_id, None)\n        return result\n    if invoked_method:\n        args = self._invoke_args.pop(invoked_method, None)\n        return args\n", "label": 1}
{"function": "\n\ndef _search_suggestions(request, text, locale, product_slugs):\n    'Return an iterable of the most relevant wiki pages and questions.\\n\\n    :arg text: full text to search on\\n    :arg locale: locale to limit to\\n    :arg product_slugs: list of product slugs to filter articles on\\n        ([\"desktop\", \"mobile\", ...])\\n\\n    Items are dicts of::\\n\\n        {\\n            \\'type\\':\\n            \\'search_summary\\':\\n            \\'title\\':\\n            \\'url\\':\\n            \\'object\\':\\n        }\\n\\n    :returns: up to 3 wiki pages, then up to 3 questions.\\n\\n    '\n    question_s = QuestionMappingType.search()\n    wiki_s = DocumentMappingType.search()\n    WIKI_RESULTS = QUESTIONS_RESULTS = 3\n    default_categories = settings.SEARCH_DEFAULT_CATEGORIES\n    if product_slugs:\n        wiki_s = wiki_s.filter(product__in=product_slugs)\n        question_s = question_s.filter(product__in=product_slugs)\n    results = []\n    try:\n        query = dict(((('%s__match' % field), text) for field in DocumentMappingType.get_query_fields()))\n        query.update(dict(((('%s__match_phrase' % field), text) for field in DocumentMappingType.get_query_fields())))\n        query = es_query_with_analyzer(query, locale)\n        filter = F()\n        filter |= F(document_locale=locale)\n        filter |= F(document_locale=settings.WIKI_DEFAULT_LANGUAGE)\n        filter &= F(document_category__in=default_categories)\n        filter &= F(document_is_archived=False)\n        raw_results = wiki_s.filter(filter).query(or_=query).values_list('id')[:WIKI_RESULTS]\n        raw_results = [result[0][0] for result in raw_results]\n        for id_ in raw_results:\n            try:\n                doc = Document.objects.select_related('current_revision').get(pk=id_)\n                results.append({\n                    'search_summary': clean_excerpt(doc.current_revision.summary),\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'document',\n                    'object': doc,\n                })\n            except Document.DoesNotExist:\n                pass\n        query = dict(((('%s__match' % field), text) for field in QuestionMappingType.get_query_fields()))\n        query.update(dict(((('%s__match_phrase' % field), text) for field in QuestionMappingType.get_query_fields())))\n        question_filter = F(question_locale=locale)\n        question_filter |= F(question_locale=settings.WIKI_DEFAULT_LANGUAGE)\n        question_filter &= F(question_is_archived=False)\n        raw_results = question_s.query(or_=query).filter(question_filter).values_list('id')[:QUESTIONS_RESULTS]\n        raw_results = [result[0][0] for result in raw_results]\n        for id_ in raw_results:\n            try:\n                q = Question.objects.get(pk=id_)\n                results.append({\n                    'search_summary': clean_excerpt(q.content[0:500]),\n                    'url': q.get_absolute_url(),\n                    'title': q.title,\n                    'type': 'question',\n                    'object': q,\n                    'last_updated': q.updated,\n                    'is_solved': q.is_solved,\n                    'num_answers': q.num_answers,\n                    'num_votes': q.num_votes,\n                    'num_votes_past_week': q.num_votes_past_week,\n                })\n            except Question.DoesNotExist:\n                pass\n    except ES_EXCEPTIONS as exc:\n        statsd.incr('questions.suggestions.eserror')\n        log.debug(exc)\n    return results\n", "label": 1}
{"function": "\n\n@requires_search\ndef update_user(user, index=None):\n    index = (index or INDEX)\n    if (not user.is_active):\n        try:\n            es.delete(index=index, doc_type='user', id=user._id, refresh=True, ignore=[404])\n        except NotFoundError:\n            pass\n        return\n    names = dict(fullname=user.fullname, given_name=user.given_name, family_name=user.family_name, middle_names=user.middle_names, suffix=user.suffix)\n    normalized_names = {\n        \n    }\n    for (key, val) in names.items():\n        if (val is not None):\n            try:\n                val = six.u(val)\n            except TypeError:\n                pass\n            normalized_names[key] = unicodedata.normalize('NFKD', val).encode('ascii', 'ignore')\n    user_doc = {\n        'id': user._id,\n        'user': user.fullname,\n        'normalized_user': normalized_names['fullname'],\n        'normalized_names': normalized_names,\n        'names': names,\n        'job': (user.jobs[0]['institution'] if user.jobs else ''),\n        'job_title': (user.jobs[0]['title'] if user.jobs else ''),\n        'all_jobs': [job['institution'] for job in user.jobs[1:]],\n        'school': (user.schools[0]['institution'] if user.schools else ''),\n        'all_schools': [school['institution'] for school in user.schools],\n        'category': 'user',\n        'degree': (user.schools[0]['degree'] if user.schools else ''),\n        'social': user.social_links,\n        'boost': 2,\n    }\n    es.index(index=index, doc_type='user', body=user_doc, id=user._id, refresh=True)\n", "label": 1}
{"function": "\n\n@execute_count(8)\ndef test_success_case(self):\n    ' Test that the min and max time uuid functions work as expected '\n    pk = uuid4()\n    TimeUUIDQueryModel.create(partition=pk, time=uuid1(), data='1')\n    time.sleep(0.2)\n    TimeUUIDQueryModel.create(partition=pk, time=uuid1(), data='2')\n    time.sleep(0.2)\n    midpoint = datetime.utcnow()\n    time.sleep(0.2)\n    TimeUUIDQueryModel.create(partition=pk, time=uuid1(), data='3')\n    time.sleep(0.2)\n    TimeUUIDQueryModel.create(partition=pk, time=uuid1(), data='4')\n    time.sleep(0.2)\n    q = TimeUUIDQueryModel.filter(partition=pk, time__lte=functions.MaxTimeUUID(midpoint))\n    q = [d for d in q]\n    self.assertEqual(len(q), 2, msg=('Got: %s' % q))\n    datas = [d.data for d in q]\n    assert ('1' in datas)\n    assert ('2' in datas)\n    q = TimeUUIDQueryModel.filter(partition=pk, time__gte=functions.MinTimeUUID(midpoint))\n    assert (len(q) == 2)\n    datas = [d.data for d in q]\n    assert ('3' in datas)\n    assert ('4' in datas)\n    q = TimeUUIDQueryModel.filter((TimeUUIDQueryModel.partition == pk), (TimeUUIDQueryModel.time <= functions.MaxTimeUUID(midpoint)))\n    q = [d for d in q]\n    assert (len(q) == 2)\n    datas = [d.data for d in q]\n    assert ('1' in datas)\n    assert ('2' in datas)\n    q = TimeUUIDQueryModel.filter((TimeUUIDQueryModel.partition == pk), (TimeUUIDQueryModel.time >= functions.MinTimeUUID(midpoint)))\n    assert (len(q) == 2)\n    datas = [d.data for d in q]\n    assert ('3' in datas)\n    assert ('4' in datas)\n", "label": 1}
{"function": "\n\ndef test_index():\n    slt = SortedList(range(100), load=17)\n    for val in range(100):\n        assert (val == slt.index(val))\n    assert (slt.index(99, 0, 1000) == 99)\n    slt = SortedList((0 for rpt in range(100)), load=17)\n    for start in range(100):\n        for stop in range(start, 100):\n            assert (slt.index(0, start, (stop + 1)) == start)\n    for start in range(100):\n        assert (slt.index(0, (- (100 - start))) == start)\n    assert (slt.index(0, (- 1000)) == 0)\n", "label": 1}
{"function": "\n\ndef process_node(self, fgraph, node, lopt=None):\n    \"\\n        This function will use `lopt` to `transform` the `node`. The\\n        `transform` method will return either False or a list of Variables\\n        that are intended to replace `node.outputs`.\\n\\n        If the fgraph accepts the replacement, then the optimization is\\n        successful, and this function returns True.\\n\\n        If there are no replacement candidates or the fgraph rejects the\\n        replacements, this function returns False.\\n\\n        Parameters\\n        ----------\\n        fgraph\\n            A FunctionGraph.\\n        node\\n            An Apply instance in `fgraph`\\n        lopt\\n            A LocalOptimizer instance that may have a better idea for\\n            how to compute node's outputs.\\n\\n        Returns\\n        -------\\n        bool\\n            True iff the `node`'s outputs were replaced in the `fgraph`.\\n\\n        \"\n    lopt = (lopt or self.local_opt)\n    try:\n        replacements = lopt.transform(node)\n    except Exception as e:\n        if (self.failure_callback is not None):\n            self.failure_callback(e, self, [(x, None) for x in node.outputs], lopt, node)\n            return False\n        else:\n            raise\n    if ((replacements is False) or (replacements is None)):\n        return False\n    old_vars = node.outputs\n    if isinstance(replacements, dict):\n        old_vars = list(replacements.keys())\n        replacements = list(replacements.values())\n    elif (not isinstance(replacements, (tuple, list))):\n        raise TypeError(('Optimizer %s gave wrong type of replacement. Expected list or tuple. Got %s' % (lopt, replacements)))\n    if (len(old_vars) != len(replacements)):\n        raise ValueError(('Optimizer %s gave wrong number of replacements' % lopt))\n    for (r, rnew) in zip(old_vars, replacements):\n        if ((rnew is None) and (len(r.clients) > 0)):\n            raise ValueError('A local optimizer tried to remove a Variable that is used')\n    repl_pairs = [(r, rnew) for (r, rnew) in zip(old_vars, replacements) if ((rnew is not r) and (rnew is not None))]\n    if (len(repl_pairs) == 0):\n        return False\n    try:\n        fgraph.replace_all_validate(repl_pairs, reason=lopt)\n        return True\n    except Exception as e:\n        if (self.failure_callback is not None):\n            self.failure_callback(e, self, repl_pairs, lopt, node)\n            return False\n        else:\n            raise\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    startLoc = loc\n    instrlen = len(instring)\n    expr = self.expr\n    failParse = False\n    while (loc <= instrlen):\n        try:\n            if self.failOn:\n                try:\n                    self.failOn.tryParse(instring, loc)\n                except ParseBaseException:\n                    pass\n                else:\n                    failParse = True\n                    raise ParseException(instring, loc, ('Found expression ' + str(self.failOn)))\n                failParse = False\n            if (self.ignoreExpr is not None):\n                while 1:\n                    try:\n                        loc = self.ignoreExpr.tryParse(instring, loc)\n                    except ParseBaseException:\n                        break\n            expr._parse(instring, loc, doActions=False, callPreParse=False)\n            skipText = instring[startLoc:loc]\n            if self.includeMatch:\n                (loc, mat) = expr._parse(instring, loc, doActions, callPreParse=False)\n                if mat:\n                    skipRes = ParseResults(skipText)\n                    skipRes += mat\n                    return (loc, [skipRes])\n                else:\n                    return (loc, [skipText])\n            else:\n                return (loc, [skipText])\n        except (ParseException, IndexError):\n            if failParse:\n                raise\n            else:\n                loc += 1\n    raise ParseException(instring, loc, self.errmsg, self)\n", "label": 1}
{"function": "\n\ndef make_app(root, **kw):\n    '\\n    Utility for creating the Pecan application object.  This function should\\n    generally be called from the ``setup_app`` function in your project\\'s\\n    ``app.py`` file.\\n\\n    :param root: A string representing a root controller object (e.g.,\\n                 \"myapp.controller.root.RootController\")\\n    :param static_root: The relative path to a directory containing static\\n                        files.  Serving static files is only enabled when\\n                        debug mode is set.\\n    :param debug: A flag to enable debug mode.  This enables the debug\\n                  middleware and serving static files.\\n    :param wrap_app: A function or middleware class to wrap the Pecan app.\\n                     This must either be a wsgi middleware class or a\\n                     function that returns a wsgi application. This wrapper\\n                     is applied first before wrapping the application in\\n                     other middlewares such as Pecan\\'s debug middleware.\\n                     This should be used if you want to use middleware to\\n                     perform authentication or intercept all requests before\\n                     they are routed to the root controller.\\n    :param logging: A dictionary used to configure logging.  This uses\\n                    ``logging.config.dictConfig``.\\n\\n    All other keyword arguments are passed in to the Pecan app constructor.\\n\\n    :returns: a ``Pecan`` object.\\n    '\n    logging = kw.get('logging', {\n        \n    })\n    debug = kw.get('debug', False)\n    if logging:\n        if debug:\n            try:\n                from logging import captureWarnings\n                captureWarnings(True)\n                warnings.simplefilter('default', DeprecationWarning)\n            except ImportError:\n                pass\n        if isinstance(logging, Config):\n            logging = logging.to_dict()\n        if ('version' not in logging):\n            logging['version'] = 1\n        load_logging_config(logging)\n    app = Pecan(root, **kw)\n    wrap_app = kw.get('wrap_app', None)\n    if wrap_app:\n        app = wrap_app(app)\n    errors = kw.get('errors', getattr(conf.app, 'errors', {\n        \n    }))\n    if errors:\n        app = middleware.errordocument.ErrorDocumentMiddleware(app, errors)\n    app = middleware.recursive.RecursiveMiddleware(app)\n    static_root = kw.get('static_root', None)\n    if debug:\n        debug_kwargs = getattr(conf, 'debug', {\n            \n        })\n        debug_kwargs.setdefault('context_injectors', []).append((lambda environ: {\n            'request': environ.get('pecan.locals', {\n                \n            }).get('request'),\n        }))\n        app = DebugMiddleware(app, **debug_kwargs)\n        if static_root:\n            app = middleware.static.StaticFileMiddleware(app, static_root)\n    elif static_root:\n        warnings.warn('`static_root` is only used when `debug` is True, ignoring', RuntimeWarning)\n    if hasattr(conf, 'requestviewer'):\n        warnings.warn(''.join(['`pecan.conf.requestviewer` is deprecated.  To apply the ', '`RequestViewerHook` to your application, add it to ', \"`pecan.conf.app.hooks` or manually in your project's `app.py` \", 'file.']), DeprecationWarning)\n    return app\n", "label": 1}
{"function": "\n\ndef save(self, locale):\n    '\\n        Load the source resource, modify it with changes made to this\\n        Resource instance, and save it over the locale-specific\\n        resource.\\n        '\n    if (not self.source_resource):\n        raise SyncError('Cannot save l20n resource {0}: No source resource given.'.format(self.path))\n    with codecs.open(self.source_resource.path, 'r', 'utf-8') as resource:\n        new_structure = L20nParser().parse(resource.read())\n\n    def attr_key(entity, attr):\n        'Returns key for the attributes of entity.'\n        return '.'.join([entity.id.name, attr.id.name])\n    to_remove = []\n    for res_entity in new_structure.body:\n        if (res_entity.type == 'Entity'):\n            entity_id = res_entity.id.name\n            translations = self.entities[entity_id].strings\n            comments = self.entities[entity_id].comments\n            if (not translations):\n                entity_idx = new_structure.body.index(res_entity)\n                for comment_entity in new_structure.body[(entity_idx - len(comments)):entity_idx]:\n                    to_remove.append((new_structure.body, comment_entity))\n                has_attributes = any([bool(self.entities[attr_key(res_entity, a)].strings) for a in res_entity.attrs])\n                if has_attributes:\n                    res_entity.value = None\n                else:\n                    to_remove.append((new_structure.body, res_entity))\n                    continue\n            if (res_entity.value and (res_entity.value.type == 'String')):\n                res_entity.value.content[0] = translations[None]\n            elif (res_entity.value and (res_entity.value.type == 'Hash')):\n                res_entity.value.items = [ast.HashItem(ast.Identifier(locale.get_relative_cldr_plural(plural_relative_id)), ast.String([value], value), False) for (plural_relative_id, value) in translations.items()]\n            for attr in res_entity.attrs:\n                key = attr_key(res_entity, attr)\n                attr_translations = self.entities[key].strings\n                if (not attr_translations):\n                    to_remove.append((res_entity.attrs, attr))\n                    continue\n                attr.value.content[0] = attr_translations[None]\n    for (parent, entity) in to_remove:\n        parent.remove(entity)\n    try:\n        os.makedirs(os.path.dirname(self.path))\n    except OSError:\n        pass\n    with codecs.open(self.path, 'w+', 'utf-8') as f:\n        f.write(Serializer().serialize(new_structure))\n        log.debug('Saved file: %s', self.path)\n", "label": 1}
{"function": "\n\ndef wait(self, timeout=None):\n    \"Must be used with 'yield' as 'yield cv.wait()'.\\n        \"\n    coro = AsynCoro.cur_coro()\n    if (self._owner != coro):\n        raise RuntimeError(('\"%s\"/%s: invalid lock release - owned by \"%s\"/%s' % (coro._name, coro._id, self._owner._name, self._owner._id)))\n    assert (self._depth > 0)\n    depth = self._depth\n    self._depth = 0\n    self._owner = None\n    if self._waitlist:\n        wake = self._waitlist.pop(0)\n        wake._proceed_(True)\n    self._notifylist.append(coro)\n    start = _time()\n    if ((yield coro._await_(timeout)) is None):\n        try:\n            self._notifylist.remove(coro)\n        except ValueError:\n            pass\n        raise StopIteration(False)\n    while (self._owner is not None):\n        self._waitlist.insert(0, coro)\n        if (timeout is not None):\n            timeout -= (_time() - start)\n            if (timeout <= 0):\n                raise StopIteration(False)\n            start = _time()\n        if ((yield coro._await_(timeout)) is None):\n            try:\n                self._waitlist.remove(coro)\n            except ValueError:\n                pass\n            raise StopIteration(False)\n    assert (self._depth == 0)\n    self._owner = coro\n    self._depth = depth\n    raise StopIteration(True)\n", "label": 1}
{"function": "\n\ndef convert_to_dict(obj, ident=0, limit_ident=6):\n    ident += 1\n    if (type(obj) in primitive):\n        return obj\n    if (isinstance(obj, inspect.types.InstanceType) or (type(obj) not in (list, tuple, dict))):\n        if (ident <= limit_ident):\n            try:\n                obj = obj.convert_to_dict()\n            except AttributeError:\n                try:\n                    t = obj.__dict__\n                    t['_type_class'] = str(obj.__class__)\n                    obj = t\n                except AttributeError:\n                    return str((obj.__class__ if hasattr(obj, '__class__') else type(obj)))\n        else:\n            return str((obj.__class__ if hasattr(obj, '__class__') else type(obj)))\n    if (type(obj) is dict):\n        res = {\n            \n        }\n        for item in obj:\n            if (ident <= limit_ident):\n                res[item] = convert_to_dict(obj[item], ident)\n            else:\n                res[item] = str(obj[item])\n        return res\n    if (type(obj) in (list, tuple)):\n        res = []\n        for item in obj:\n            if (ident <= limit_ident):\n                res.append(convert_to_dict(item, ident))\n            else:\n                res.append(str(item))\n        return (res if (type(obj) is list) else tuple(res))\n", "label": 1}
{"function": "\n\ndef task_present(name, tick_script, task_type='stream', database=None, retention_policy='default', enable=True):\n    \"\\n    Ensure that a task is present and up-to-date in Kapacitor.\\n\\n    name\\n        Name of the task.\\n\\n    tick_script\\n        Path to the TICK script for the task. Can be a salt:// source.\\n\\n    task_type\\n        Task type. Defaults to 'stream'\\n\\n    database\\n        Which database to fetch data from. Defaults to None, which will use the\\n        default database in InfluxDB.\\n\\n    retention_policy\\n        Which retention policy to fetch data from. Defaults to 'default'.\\n\\n    enable\\n        Whether to enable the task or not. Defaults to True.\\n    \"\n    comments = []\n    changes = []\n    ret = {\n        'name': name,\n        'changes': {\n            \n        },\n        'result': True,\n        'comment': '',\n    }\n    task = __salt__['kapacitor.get_task'](name)\n    old_script = (task['TICKscript'] if task else '')\n    if tick_script.startswith('salt://'):\n        script_path = __salt__['cp.cache_file'](tick_script, __env__)\n    else:\n        script_path = tick_script\n    with salt.utils.fopen(script_path, 'r') as file:\n        new_script = file.read()\n    if (old_script == new_script):\n        comments.append('Task script is already up-to-date')\n    else:\n        if __opts__['test']:\n            ret['result'] = None\n            comments.append('Task would have been updated')\n        else:\n            result = __salt__['kapacitor.define_task'](name, script_path, task_type=task_type, database=database, retention_policy=retention_policy)\n            ret['result'] = result['success']\n            if (not ret['result']):\n                comments.append('Could not define task')\n                if result.get('stderr'):\n                    comments.append(result['stderr'])\n                ret['comment'] = '\\n'.join(comments)\n                return ret\n        ret['changes']['TICKscript diff'] = '\\n'.join(difflib.unified_diff(old_script.splitlines(), new_script.splitlines()))\n        comments.append('Task script updated')\n    if enable:\n        if (task and task['Enabled']):\n            comments.append('Task is already enabled')\n        else:\n            if __opts__['test']:\n                ret['result'] = None\n                comments.append('Task would have been enabled')\n            else:\n                result = __salt__['kapacitor.enable_task'](name)\n                ret['result'] = result['success']\n                if (not ret['result']):\n                    comments.append('Could not enable task')\n                    if result.get('stderr'):\n                        comments.append(result['stderr'])\n                    ret['comment'] = '\\n'.join(comments)\n                    return ret\n                comments.append('Task was enabled')\n            ret['changes']['enabled'] = {\n                'old': False,\n                'new': True,\n            }\n    elif (task and (not task['Enabled'])):\n        comments.append('Task is already disabled')\n    else:\n        if __opts__['test']:\n            ret['result'] = None\n            comments.append('Task would have been disabled')\n        else:\n            result = __salt__['kapacitor.disable_task'](name)\n            ret['result'] = result['success']\n            if (not ret['result']):\n                comments.append('Could not disable task')\n                if result.get('stderr'):\n                    comments.append(result['stderr'])\n                ret['comment'] = '\\n'.join(comments)\n                return ret\n            comments.append('Task was disabled')\n        ret['changes']['enabled'] = {\n            'old': True,\n            'new': False,\n        }\n    ret['comment'] = '\\n'.join(comments)\n    return ret\n", "label": 1}
{"function": "\n\ndef print_db_info_metadata(db_type, info, metadata):\n    '\\n    print out data base info/metadata based on its type\\n\\n    :param db_type: database type, account or container\\n    :param info: dict of data base info\\n    :param metadata: dict of data base metadata\\n    '\n    if (info is None):\n        raise ValueError('DB info is None')\n    if (db_type not in ['container', 'account']):\n        raise ValueError('Wrong DB type')\n    try:\n        account = info['account']\n        container = None\n        if (db_type == 'container'):\n            container = info['container']\n            path = ('/%s/%s' % (account, container))\n        else:\n            path = ('/%s' % account)\n        print(('Path: %s' % path))\n        print(('  Account: %s' % account))\n        if (db_type == 'container'):\n            print(('  Container: %s' % container))\n        path_hash = hash_path(account, container)\n        if (db_type == 'container'):\n            print(('  Container Hash: %s' % path_hash))\n        else:\n            print(('  Account Hash: %s' % path_hash))\n        print('Metadata:')\n        print(('  Created at: %s (%s)' % (Timestamp(info['created_at']).isoformat, info['created_at'])))\n        print(('  Put Timestamp: %s (%s)' % (Timestamp(info['put_timestamp']).isoformat, info['put_timestamp'])))\n        print(('  Delete Timestamp: %s (%s)' % (Timestamp(info['delete_timestamp']).isoformat, info['delete_timestamp'])))\n        print(('  Status Timestamp: %s (%s)' % (Timestamp(info['status_changed_at']).isoformat, info['status_changed_at'])))\n        if (db_type == 'account'):\n            print(('  Container Count: %s' % info['container_count']))\n        print(('  Object Count: %s' % info['object_count']))\n        print(('  Bytes Used: %s' % info['bytes_used']))\n        if (db_type == 'container'):\n            try:\n                policy_name = POLICIES[info['storage_policy_index']].name\n            except KeyError:\n                policy_name = 'Unknown'\n            print(('  Storage Policy: %s (%s)' % (policy_name, info['storage_policy_index'])))\n            print(('  Reported Put Timestamp: %s (%s)' % (Timestamp(info['reported_put_timestamp']).isoformat, info['reported_put_timestamp'])))\n            print(('  Reported Delete Timestamp: %s (%s)' % (Timestamp(info['reported_delete_timestamp']).isoformat, info['reported_delete_timestamp'])))\n            print(('  Reported Object Count: %s' % info['reported_object_count']))\n            print(('  Reported Bytes Used: %s' % info['reported_bytes_used']))\n        print(('  Chexor: %s' % info['hash']))\n        print(('  UUID: %s' % info['id']))\n    except KeyError as e:\n        raise ValueError(('Info is incomplete: %s' % e))\n    meta_prefix = (('x_' + db_type) + '_')\n    for (key, value) in info.items():\n        if key.lower().startswith(meta_prefix):\n            title = key.replace('_', '-').title()\n            print(('  %s: %s' % (title, value)))\n    user_metadata = {\n        \n    }\n    sys_metadata = {\n        \n    }\n    for (key, (value, timestamp)) in metadata.items():\n        if is_user_meta(db_type, key):\n            user_metadata[strip_user_meta_prefix(db_type, key)] = value\n        elif is_sys_meta(db_type, key):\n            sys_metadata[strip_sys_meta_prefix(db_type, key)] = value\n        else:\n            title = key.replace('_', '-').title()\n            print(('  %s: %s' % (title, value)))\n    if sys_metadata:\n        print(('  System Metadata: %s' % sys_metadata))\n    else:\n        print('No system metadata found in db file')\n    if user_metadata:\n        print(('  User Metadata: %s' % user_metadata))\n    else:\n        print('No user metadata found in db file')\n", "label": 1}
{"function": "\n\ndef do_idpsso_descriptor(conf, cert=None):\n    idpsso = md.IDPSSODescriptor()\n    idpsso.protocol_support_enumeration = samlp.NAMESPACE\n    endps = conf.getattr('endpoints', 'idp')\n    if endps:\n        for (endpoint, instlist) in do_endpoints(endps, ENDPOINTS['idp']).items():\n            setattr(idpsso, endpoint, instlist)\n    _do_nameid_format(idpsso, conf, 'idp')\n    scopes = conf.getattr('scope', 'idp')\n    if scopes:\n        if (idpsso.extensions is None):\n            idpsso.extensions = md.Extensions()\n        for scope in scopes:\n            mdscope = shibmd.Scope()\n            mdscope.text = scope\n            mdscope.regexp = 'false'\n            idpsso.extensions.add_extension_element(mdscope)\n    ui_info = conf.getattr('ui_info', 'idp')\n    if ui_info:\n        if (idpsso.extensions is None):\n            idpsso.extensions = md.Extensions()\n        idpsso.extensions.add_extension_element(do_uiinfo(ui_info))\n    if cert:\n        idpsso.key_descriptor = do_key_descriptor(cert)\n    for key in ['want_authn_requests_signed']:\n        try:\n            val = conf.getattr(key, 'idp')\n            if (val is None):\n                setattr(idpsso, key, DEFAULT[key])\n            else:\n                setattr(idpsso, key, ('%s' % val).lower())\n        except KeyError:\n            setattr(idpsso, key, DEFAULTS[key])\n    return idpsso\n", "label": 1}
{"function": "\n\ndef _Dynamic_Put(self, put_request, put_response):\n    if put_request.has_transaction():\n        entities = put_request.entity_list()\n        requires_id = (lambda x: ((x.id() == 0) and (not x.has_name())))\n        new_ents = [e for e in entities if requires_id(e.key().path().element_list()[(- 1)])]\n        id_request = datastore_pb.PutRequest()\n        txid = put_request.transaction().handle()\n        txdata = self.__transactions[txid]\n        assert (txdata.thread_id == thread.get_ident()), 'Transactions are single-threaded.'\n        if new_ents:\n            for ent in new_ents:\n                e = id_request.add_entity()\n                e.mutable_key().CopyFrom(ent.key())\n                e.mutable_entity_group()\n            id_response = datastore_pb.PutResponse()\n            if txdata.is_xg:\n                rpc_name = 'GetIDsXG'\n            else:\n                rpc_name = 'GetIDs'\n            super(RemoteDatastoreStub, self).MakeSyncCall('remote_datastore', rpc_name, id_request, id_response)\n            assert (id_request.entity_size() == id_response.key_size())\n            for (key, ent) in zip(id_response.key_list(), new_ents):\n                ent.mutable_key().CopyFrom(key)\n                ent.mutable_entity_group().add_element().CopyFrom(key.path().element(0))\n        for entity in entities:\n            txdata.entities[entity.key().Encode()] = (entity.key(), entity)\n            put_response.add_key().CopyFrom(entity.key())\n    else:\n        super(RemoteDatastoreStub, self).MakeSyncCall('datastore_v3', 'Put', put_request, put_response)\n", "label": 1}
{"function": "\n\ndef load_library(libname, debug_lib=True):\n    s = platform.system()\n    arch = str((ctypes.sizeof(ctypes.c_voidp) * 8))\n    path = os.path.dirname(os.path.abspath(__file__))\n    try:\n        if (hasattr(sys, 'frozen') or hasattr(sys, 'importers') or (hasattr(imp, 'is_frozen') and imp.is_forzen('__main__'))):\n            if ('site-packages.zip' in __file__):\n                path = os.path.join(os.path.dirname(os.getcwd()), 'Frameworks')\n            else:\n                path = os.path.dirname(os.path.abspath(sys.executable))\n    except:\n        pass\n    if (arch == '64'):\n        arch_param = '64'\n    else:\n        arch_param = ''\n    if (s in ('Linux', 'FreeBSD')):\n        libfn = ('lib%s%s.so' % (libname, arch_param))\n    elif (s in ('Windows', 'Microsoft')):\n        if ((arch == '32') and debug_lib):\n            print('\\nWARNING! \\nThere are known blocker bugs in 64-bit pymunk on Windows.\\nUse at your own risk. \\n')\n        libfn = ('%s%s.dll' % (libname, arch_param))\n    elif (s == 'Darwin'):\n        libfn = ('lib%s.dylib' % libname)\n    else:\n        libfn = ('lib%s.so' % libname)\n    libfn = os.path.join(path, libfn)\n    if debug_lib:\n        print(('Loading chipmunk for %s (%sbit) [%s]' % (s, arch, libfn)))\n    try:\n        lib = platform_specific_functions()['library_loader'].LoadLibrary(libfn)\n    except OSError:\n        print(\"\\nFailed to load pymunk library.\\n\\nThis error usually means that you don't have a compiled version of chipmunk in \\nthe correct spot where pymunk can find it. pymunk does not include precompiled \\nchipmunk library files for all platforms. \\n\\nThe good news is that it is usually enough (at least on *nix and OS X) to \\nsimply run the compile command first before installing and then retry again:\\n\\nYou compile chipmunk with\\n> python setup.py build_chipmunk\\nand then continue as usual with \\n> python setup.py install\\n> cd examples\\n> python basic_test.py\\n\\n(for complete instructions please see the readme file)\\n\\nIf it still doesnt work, please report as a bug on the issue tracker at \\nhttps://github.com/viblo/pymunk/issues\\nRemember to include information about your OS, which version of python you use \\nand the version of pymunk you tried to run. A description of what you did to \\ntrigger the error is also good. Please include the exception traceback if any \\n(usually found below this message).\\n\")\n        raise\n    return lib\n", "label": 1}
{"function": "\n\ndef create_container(self, conf, detach, tty):\n    'Create a single container'\n    name = conf.name\n    image_name = conf.image_name\n    container_name = conf.container_name\n    with conf.assumed_role():\n        env = dict((e.pair for e in conf.env))\n    ports = [port.host_port for port in conf.ports]\n    links = [link.pair for link in conf.links]\n    binds = conf.volumes.binds\n    command = conf.formatted_command\n    volume_names = conf.volumes.volume_names\n    volumes_from = list(conf.volumes.share_with_names)\n    port_bindings = dict([port.pair for port in conf.ports])\n    no_tty_option = conf.no_tty_option\n    uncreated = []\n    for name in binds:\n        if (not os.path.exists(name)):\n            log.info('Making volume for mounting\\tvolume=%s', name)\n            try:\n                os.makedirs(name)\n            except OSError as error:\n                uncreated.append((name, error))\n    if uncreated:\n        raise BadOption('Failed to create some volumes on the host', uncreated=uncreated)\n    log.info('Creating container from %s\\timage=%s\\tcontainer_name=%s\\ttty=%s', image_name, name, container_name, tty)\n    if binds:\n        log.info('\\tUsing volumes\\tvolumes=%s', volume_names)\n    if env:\n        log.info('\\tUsing environment\\tenv=%s', sorted(env.keys()))\n    if links:\n        log.info('\\tLinks: %s', links)\n    if ports:\n        log.info('\\tUsing ports\\tports=%s', ports)\n    if port_bindings:\n        log.info('\\tPort bindings: %s', ports)\n    if volumes_from:\n        log.info('\\tVolumes from: %s', volumes_from)\n    host_config = conf.harpoon.docker_context.create_host_config(links=links, binds=binds, volumes_from=volumes_from, port_bindings=port_bindings, devices=conf.devices, lxc_conf=conf.lxc_conf, privileged=conf.privileged, restart_policy=conf.restart_policy, dns=conf.network.dns, dns_search=conf.network.dns_search, extra_hosts=conf.network.extra_hosts, network_mode=conf.network.network_mode, publish_all_ports=conf.network.publish_all_ports, cap_add=conf.cpu.cap_add, cap_drop=conf.cpu.cap_drop, mem_limit=conf.cpu.mem_limit, memswap_limit=conf.cpu.memswap_limit, ulimits=conf.ulimits, read_only=conf.read_only_rootfs, log_config=conf.log_config, security_opt=conf.security_opt, **conf.other_options.host_config)\n    container_id = conf.harpoon.docker_context.create_container(image_name, name=container_name, detach=detach, command=command, volumes=volume_names, environment=env, tty=(False if no_tty_option else tty), user=conf.user, ports=ports, stdin_open=tty, dns=conf.network.dns, hostname=conf.network.hostname, domainname=conf.network.domainname, network_disabled=conf.network.disabled, cpuset=conf.cpu.cpuset, cpu_shares=conf.cpu.cpu_shares, host_config=host_config, **conf.other_options.create)\n    if isinstance(container_id, dict):\n        if ('errorDetail' in container_id):\n            raise BadImage('Failed to create container', image=name, error=container_id['errorDetail'])\n        container_id = container_id['Id']\n    return container_id\n", "label": 1}
{"function": "\n\ndef plot_covariance_ellipse(mean, cov=None, variance=1.0, std=None, ellipse=None, title=None, axis_equal=True, show_semiaxis=False, facecolor=None, edgecolor=None, fc='none', ec='#004080', alpha=1.0, xlim=None, ylim=None, ls='solid'):\n    ' plots the covariance ellipse where\\n\\n    mean is a (x,y) tuple for the mean of the covariance (center of ellipse)\\n\\n    cov is a 2x2 covariance matrix.\\n\\n    `variance` is the normal sigma^2 that we want to plot. If list-like,\\n    ellipses for all ellipses will be ploted. E.g. [1,2] will plot the\\n    sigma^2 = 1 and sigma^2 = 2 ellipses. Alternatively, use std for the\\n    standard deviation, in which case `variance` will be ignored.\\n\\n    ellipse is a (angle,width,height) tuple containing the angle in radians,\\n    and width and height radii.\\n\\n    You may provide either cov or ellipse, but not both.\\n\\n    plt.show() is not called, allowing you to plot multiple things on the\\n    same figure.\\n    '\n    assert ((cov is None) or (ellipse is None))\n    assert (not ((cov is None) and (ellipse is None)))\n    if (facecolor is None):\n        facecolor = fc\n    if (edgecolor is None):\n        edgecolor = ec\n    if (cov is not None):\n        ellipse = covariance_ellipse(cov)\n    if axis_equal:\n        plt.axis('equal')\n    if (title is not None):\n        plt.title(title)\n    compute_std = False\n    if (std is None):\n        std = variance\n        compute_std = True\n    if np.isscalar(std):\n        std = [std]\n    if compute_std:\n        std = np.sqrt(np.asarray(std))\n    ax = plt.gca()\n    angle = np.degrees(ellipse[0])\n    width = (ellipse[1] * 2.0)\n    height = (ellipse[2] * 2.0)\n    for sd in std:\n        e = Ellipse(xy=mean, width=(sd * width), height=(sd * height), angle=angle, facecolor=facecolor, edgecolor=edgecolor, alpha=alpha, lw=2, ls=ls)\n        ax.add_patch(e)\n    (x, y) = mean\n    plt.scatter(x, y, marker='+', color=edgecolor)\n    if (xlim is not None):\n        ax.set_xlim(xlim)\n    if (ylim is not None):\n        ax.set_ylim(ylim)\n    if show_semiaxis:\n        a = ellipse[0]\n        (h, w) = ((height / 4), (width / 4))\n        plt.plot([x, (x + (h * cos((a + (np.pi / 2)))))], [y, (y + (h * sin((a + (np.pi / 2)))))])\n        plt.plot([x, (x + (w * cos(a)))], [y, (y + (w * sin(a)))])\n", "label": 1}
{"function": "\n\ndef test_manage_session2():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test', 'test1'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == True)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == True)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == True)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == True)\n        assert (key.can_manage('test1.test') == True)\n", "label": 1}
{"function": "\n\ndef conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th', image_shape=None, filter_shape=None):\n    '\\n    border_mode: string, \"same\" or \"valid\".\\n    '\n    if (dim_ordering not in {'th', 'tf'}):\n        raise Exception(('Unknown dim_ordering ' + str(dim_ordering)))\n    if (dim_ordering == 'tf'):\n        x = x.dimshuffle((0, 3, 1, 2))\n        kernel = kernel.dimshuffle((3, 2, 0, 1))\n        if image_shape:\n            image_shape = (image_shape[0], image_shape[3], image_shape[1], image_shape[2])\n        if filter_shape:\n            filter_shape = (filter_shape[3], filter_shape[2], filter_shape[0], filter_shape[1])\n    if (border_mode == 'same'):\n        th_border_mode = 'half'\n        np_kernel = kernel.eval()\n    elif (border_mode == 'valid'):\n        th_border_mode = 'valid'\n    else:\n        raise Exception(('Border mode not supported: ' + str(border_mode)))\n\n    def int_or_none(value):\n        try:\n            return int(value)\n        except TypeError:\n            return None\n    if (image_shape is not None):\n        image_shape = tuple((int_or_none(v) for v in image_shape))\n    if (filter_shape is not None):\n        filter_shape = tuple((int_or_none(v) for v in filter_shape))\n    conv_out = T.nnet.conv2d(x, kernel, border_mode=th_border_mode, subsample=strides, input_shape=image_shape, filter_shape=filter_shape)\n    if (border_mode == 'same'):\n        if ((np_kernel.shape[2] % 2) == 0):\n            conv_out = conv_out[:, :, :(((x.shape[2] + strides[0]) - 1) // strides[0]), :]\n        if ((np_kernel.shape[3] % 2) == 0):\n            conv_out = conv_out[:, :, :, :(((x.shape[3] + strides[1]) - 1) // strides[1])]\n    if (dim_ordering == 'tf'):\n        conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n    return conv_out\n", "label": 1}
{"function": "\n\ndef _thread_run(self, **kwargs):\n    input_fn = kwargs.get('input_fn')\n    queue = kwargs.get('queue')\n    device = kwargs.get('device')\n    drs = kwargs.get('default_ranges').get\n    touches = {\n        \n    }\n    touches_sent = []\n    point = {\n        \n    }\n    l_points = []\n    range_min_position_x = 0\n    range_max_position_x = 2048\n    range_min_position_y = 0\n    range_max_position_y = 2048\n    range_min_pressure = 0\n    range_max_pressure = 255\n    range_min_abs_x = 0\n    range_max_abs_x = 255\n    range_min_abs_y = 0\n    range_max_abs_y = 255\n    range_min_abs_pressure = 0\n    range_max_abs_pressure = 255\n    invert_x = int(bool(drs('invert_x', 0)))\n    invert_y = int(bool(drs('invert_y', 1)))\n    rotation = drs('rotation', 0)\n\n    def assign_coord(point, value, invert, coords):\n        (cx, cy) = coords\n        if invert:\n            value = (1.0 - value)\n        if (rotation == 0):\n            point[cx] = value\n        elif (rotation == 90):\n            point[cy] = value\n        elif (rotation == 180):\n            point[cx] = (1.0 - value)\n        elif (rotation == 270):\n            point[cy] = (1.0 - value)\n\n    def assign_rel_coord(point, value, invert, coords):\n        (cx, cy) = coords\n        if invert:\n            value = ((- 1) * value)\n        if (rotation == 0):\n            point[cx] += value\n        elif (rotation == 90):\n            point[cy] += value\n        elif (rotation == 180):\n            point[cx] += (- value)\n        elif (rotation == 270):\n            point[cy] += (- value)\n\n    def process_as_multitouch(tv_sec, tv_usec, ev_type, ev_code, ev_value):\n        if (ev_type == EV_SYN):\n            if (ev_code == SYN_MT_REPORT):\n                if ('id' not in point):\n                    return\n                l_points.append(point.copy())\n            elif (ev_code == SYN_REPORT):\n                process(l_points)\n                del l_points[:]\n        elif ((ev_type == EV_MSC) and (ev_code in (MSC_RAW, MSC_SCAN))):\n            pass\n        elif (ev_code == ABS_MT_TRACKING_ID):\n            point.clear()\n            point['id'] = ev_value\n        elif (ev_code == ABS_MT_POSITION_X):\n            val = normalize(ev_value, range_min_position_x, range_max_position_x)\n            assign_coord(point, val, invert_x, 'xy')\n        elif (ev_code == ABS_MT_POSITION_Y):\n            val = (1.0 - normalize(ev_value, range_min_position_y, range_max_position_y))\n            assign_coord(point, val, invert_y, 'yx')\n        elif (ev_code == ABS_MT_ORIENTATION):\n            point['orientation'] = ev_value\n        elif (ev_code == ABS_MT_BLOB_ID):\n            point['blobid'] = ev_value\n        elif (ev_code == ABS_MT_PRESSURE):\n            point['pressure'] = normalize(ev_value, range_min_pressure, range_max_pressure)\n        elif (ev_code == ABS_MT_TOUCH_MAJOR):\n            point['size_w'] = ev_value\n        elif (ev_code == ABS_MT_TOUCH_MINOR):\n            point['size_h'] = ev_value\n\n    def process_as_mouse_or_keyboard(tv_sec, tv_usec, ev_type, ev_code, ev_value):\n        if (ev_type == EV_SYN):\n            if (ev_code == SYN_REPORT):\n                process([point])\n        elif (ev_type == EV_REL):\n            if (ev_code == 0):\n                assign_rel_coord(point, min(1.0, max((- 1.0), (ev_value / 1000.0))), invert_x, 'xy')\n            elif (ev_code == 1):\n                assign_rel_coord(point, min(1.0, max((- 1.0), (ev_value / 1000.0))), invert_y, 'yx')\n        elif (ev_code == ABS_X):\n            val = normalize(ev_value, range_min_abs_x, range_max_abs_x)\n            assign_coord(point, val, invert_x, 'xy')\n        elif (ev_code == ABS_Y):\n            val = (1.0 - normalize(ev_value, range_min_abs_y, range_max_abs_y))\n            assign_coord(point, val, invert_y, 'yx')\n        elif ((ev_code == ABS_PRESSURE) and (ev_type != EV_KEY)):\n            point['pressure'] = normalize(ev_value, range_min_abs_pressure, range_max_abs_pressure)\n        elif (ev_type == EV_KEY):\n            buttons = {\n                272: 'left',\n                273: 'right',\n                274: 'middle',\n                275: 'side',\n                276: 'extra',\n                277: 'forward',\n                278: 'back',\n                279: 'task',\n                330: 'touch',\n                320: 'pen',\n            }\n            if (ev_code in buttons.keys()):\n                if ev_value:\n                    if ('button' not in point):\n                        point['button'] = buttons[ev_code]\n                        point['id'] += 1\n                        if ('_avoid' in point):\n                            del point['_avoid']\n                elif ('button' in point):\n                    if (point['button'] == buttons[ev_code]):\n                        del point['button']\n                        point['id'] += 1\n                        point['_avoid'] = True\n            else:\n                if (ev_value == 1):\n                    l = keyboard_keys[ev_code][((- 1) if ('shift' in Window._modifiers) else 0)]\n                    if ((l == 'shift') or (l == 'alt')):\n                        Window._modifiers.append(l)\n                    Window.dispatch('on_key_down', Keyboard.keycodes[l.lower()], ev_code, keys_str.get(l, l), Window._modifiers)\n                if (ev_value == 0):\n                    l = keyboard_keys[ev_code][((- 1) if ('shift' in Window._modifiers) else 0)]\n                    Window.dispatch('on_key_up', Keyboard.keycodes[l.lower()], ev_code, keys_str.get(l, l), Window._modifiers)\n                    if (l == 'shift'):\n                        Window._modifiers.remove('shift')\n\n    def process(points):\n        if (not is_multitouch):\n            Window.mouse_pos = ((points[0]['x'] * Window.width), (points[0]['y'] * Window.height))\n        actives = [args['id'] for args in points if (('id' in args) and (not ('_avoid' in args)))]\n        for args in points:\n            tid = args['id']\n            try:\n                touch = touches[tid]\n                if ((touch.sx == args['x']) and (touch.sy == args['y'])):\n                    continue\n                touch.move(args)\n                if (tid not in touches_sent):\n                    queue.append(('begin', touch))\n                    touches_sent.append(tid)\n                queue.append(('update', touch))\n            except KeyError:\n                if ('_avoid' not in args):\n                    touch = HIDMotionEvent(device, tid, args)\n                    touches[touch.id] = touch\n                    if (tid not in touches_sent):\n                        queue.append(('begin', touch))\n                        touches_sent.append(tid)\n        for tid in list(touches.keys())[:]:\n            if (tid not in actives):\n                touch = touches[tid]\n                if (tid in touches_sent):\n                    touch.update_time_end()\n                    queue.append(('end', touch))\n                    touches_sent.remove(tid)\n                del touches[tid]\n\n    def normalize(value, vmin, vmax):\n        return ((value - vmin) / float((vmax - vmin)))\n    fd = open(input_fn, 'rb')\n    device_name = str(fcntl.ioctl(fd, (EVIOCGNAME + (256 << 16)), (' ' * 256))).split('\\x00')[0]\n    Logger.info(('HIDMotionEvent: using <%s>' % device_name))\n    bit = fcntl.ioctl(fd, (EVIOCGBIT + (EV_MAX << 16)), (' ' * sz_l))\n    (bit,) = struct.unpack('Q', bit)\n    is_multitouch = False\n    for x in range(EV_MAX):\n        if (x != EV_ABS):\n            continue\n        if ((bit & (1 << x)) == 0):\n            continue\n        sbit = fcntl.ioctl(fd, ((EVIOCGBIT + x) + (KEY_MAX << 16)), (' ' * sz_l))\n        (sbit,) = struct.unpack('Q', sbit)\n        for y in range(KEY_MAX):\n            if ((sbit & (1 << y)) == 0):\n                continue\n            absinfo = fcntl.ioctl(fd, ((EVIOCGABS + y) + (struct_input_absinfo_sz << 16)), (' ' * struct_input_absinfo_sz))\n            (abs_value, abs_min, abs_max, abs_fuzz, abs_flat, abs_res) = struct.unpack('iiiiii', absinfo)\n            if (y == ABS_MT_POSITION_X):\n                is_multitouch = True\n                range_min_position_x = drs('min_position_x', abs_min)\n                range_max_position_x = drs('max_position_x', abs_max)\n                Logger.info(('HIDMotionEvent: ' + ('<%s> range position X is %d - %d' % (device_name, abs_min, abs_max))))\n            elif (y == ABS_MT_POSITION_Y):\n                is_multitouch = True\n                range_min_position_y = drs('min_position_y', abs_min)\n                range_max_position_y = drs('max_position_y', abs_max)\n                Logger.info(('HIDMotionEvent: ' + ('<%s> range position Y is %d - %d' % (device_name, abs_min, abs_max))))\n            elif (y == ABS_MT_PRESSURE):\n                range_min_pressure = drs('min_pressure', abs_min)\n                range_max_pressure = drs('max_pressure', abs_max)\n                Logger.info(('HIDMotionEvent: ' + ('<%s> range pressure is %d - %d' % (device_name, abs_min, abs_max))))\n            elif (y == ABS_X):\n                range_min_abs_x = drs('min_abs_x', abs_min)\n                range_max_abs_x = drs('max_abs_x', abs_max)\n                Logger.info(('HIDMotionEvent: ' + ('<%s> range ABS X position is %d - %d' % (device_name, abs_min, abs_max))))\n            elif (y == ABS_Y):\n                range_min_abs_y = drs('min_abs_y', abs_min)\n                range_max_abs_y = drs('max_abs_y', abs_max)\n                Logger.info(('HIDMotionEvent: ' + ('<%s> range ABS Y position is %d - %d' % (device_name, abs_min, abs_max))))\n            elif (y == ABS_PRESSURE):\n                range_min_abs_pressure = drs('min_abs_pressure', abs_min)\n                range_max_abs_pressure = drs('max_abs_pressure', abs_max)\n                Logger.info(('HIDMotionEvent: ' + ('<%s> range ABS pressure is %d - %d' % (device_name, abs_min, abs_max))))\n    if (not is_multitouch):\n        point = {\n            'x': 0.5,\n            'y': 0.5,\n            'id': 0,\n            '_avoid': True,\n        }\n    while fd:\n        data = fd.read(struct_input_event_sz)\n        if (len(data) < struct_input_event_sz):\n            break\n        for i in range(int((len(data) / struct_input_event_sz))):\n            ev = data[(i * struct_input_event_sz):]\n            infos = struct.unpack('LLHHi', ev[:struct_input_event_sz])\n            if is_multitouch:\n                process_as_multitouch(*infos)\n            else:\n                process_as_mouse_or_keyboard(*infos)\n", "label": 1}
{"function": "\n\ndef untar_file(filename, location):\n    'Untar the file (tar file located at filename) to the destination location'\n    if (not os.path.exists(location)):\n        os.makedirs(location)\n    if (filename.lower().endswith('.gz') or filename.lower().endswith('.tgz')):\n        mode = 'r:gz'\n    elif (filename.lower().endswith('.bz2') or filename.lower().endswith('.tbz')):\n        mode = 'r:bz2'\n    elif filename.lower().endswith('.tar'):\n        mode = 'r'\n    else:\n        logger.warn(('Cannot determine compression type for file %s' % filename))\n        mode = 'r:*'\n    tar = tarfile.open(filename, mode)\n    try:\n        leading = has_leading_dir([member.name for member in tar.getmembers() if (member.name != 'pax_global_header')])\n        for member in tar.getmembers():\n            fn = member.name\n            if (fn == 'pax_global_header'):\n                continue\n            if leading:\n                fn = split_leading_dir(fn)[1]\n            path = os.path.join(location, fn)\n            if member.isdir():\n                if (not os.path.exists(path)):\n                    os.makedirs(path)\n            else:\n                try:\n                    fp = tar.extractfile(member)\n                except (KeyError, AttributeError):\n                    e = sys.exc_info()[1]\n                    logger.warn(('In the tar file %s the member %s is invalid: %s' % (filename, member.name, e)))\n                    continue\n                if (not os.path.exists(os.path.dirname(path))):\n                    os.makedirs(os.path.dirname(path))\n                destfp = open(path, 'wb')\n                try:\n                    shutil.copyfileobj(fp, destfp)\n                finally:\n                    destfp.close()\n                fp.close()\n    finally:\n        tar.close()\n", "label": 1}
{"function": "\n\ndef getSource(self, url, form_data, referer, xml=False, mobile=False):\n    url = self.fixurl(url)\n    if (not referer):\n        referer = url\n    else:\n        referer = self.fixurl(referer)\n    headers = {\n        'Referer': referer,\n    }\n    if mobile:\n        self.s.headers.update({\n            'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B143 Safari/601.1',\n        })\n    if xml:\n        headers['X-Requested-With'] = 'XMLHttpRequest'\n    if ('dinozap.info' in urlparse.urlsplit(url).netloc):\n        headers['X-Forwarded-For'] = '178.162.222.111'\n    if ('playerhd2.pw' in urlparse.urlsplit(url).netloc):\n        headers['X-Forwarded-For'] = '178.162.222.121'\n    if ('playerapp1.pw' in urlparse.urlsplit(url).netloc):\n        headers['X-Forwarded-For'] = '178.162.222.122'\n    if ('finecast.tv' in urlparse.urlsplit(url).netloc):\n        self.s.headers.update({\n            'Cookie': 'PHPSESSID=d08b73a2b7e0945b3b1bb700f01f7d72',\n        })\n    if form_data:\n        if ('uagent' in form_data[0]):\n            form_data[0] = ('uagent', urllib.quote(self.s.headers['User-Agent']))\n        if ('123456789' in form_data[0]):\n            import random\n            cotok = str(random.randrange(100000000, 999999999))\n            form_data[0] = ('token', cotok)\n            r = self.s.post(url, headers=headers, data=form_data, timeout=20, cookies={\n                'token': cotok,\n            })\n        else:\n            r = self.s.post(url, headers=headers, data=form_data, timeout=20)\n        response = r.text\n    else:\n        try:\n            r = self.s.get(url, headers=headers, timeout=20)\n            response = r.text\n        except requests.exceptions.MissingSchema:\n            response = 'pass'\n    print('>>>>>>>>>>>>> LEN <<<<<<<<<', len(response))\n    if self.cookie_file:\n        self.save_cookies_lwp(self.s.cookies, self.cookie_file)\n    return HTMLParser().unescape(response)\n", "label": 1}
{"function": "\n\ndef post_files(self, files, start, end, uploaded_queue_put, boundary, local_deploy):\n    hub_session = self.hub_session\n    hub_cookie = self.hub_cookie\n    hub_pool = self.hub_pool\n    while (start < end):\n        if self.stopped:\n            uploaded_queue_put(None)\n            break\n        item = files[start]\n        start += 1\n        (deploy_path, relative_path, file_size, file_hash, file_md5) = item\n        try:\n            if local_deploy:\n                guessed_type = guess_type(relative_path)[0]\n                if (guessed_type is None):\n                    guessed_type = ''\n                params = {\n                    'file.content_type': guessed_type,\n                    'file.name': relative_path,\n                    'file.path': deploy_path,\n                    'session': hub_session,\n                    'hash': file_hash,\n                    'length': str(file_size),\n                    'md5': file_md5,\n                }\n                if deploy_path.endswith('.gz'):\n                    params['encoding'] = 'gzip'\n                r = hub_pool.request('POST', '/dynamic/upload/file', fields=params, headers={\n                    'Cookie': hub_cookie,\n                }, timeout=self.hub_timeout)\n            else:\n                params = [MultipartParam('file', filename=relative_path, filetype=guess_type(relative_path)[0], fileobj=open(deploy_path, 'rb')), ('session', hub_session), ('hash', file_hash), ('length', file_size), ('md5', file_md5)]\n                if deploy_path.endswith('.gz'):\n                    params.append(('encoding', 'gzip'))\n                headers = get_headers(params, boundary)\n                headers['Cookie'] = hub_cookie\n                params = MultipartParam.from_params(params)\n                params = MultipartReader(params, boundary)\n                r = hub_pool.urlopen('POST', '/dynamic/upload/file', params, headers=headers, timeout=self.hub_timeout)\n        except IOError:\n            self.stop(('Error opening file \"%s\".' % deploy_path))\n            continue\n        except (HTTPError, SSLError, ValueError) as e:\n            self.stop(('Error uploading file \"%s\": \"%s\".' % (relative_path, e)))\n            continue\n        if (r.headers.get('content-type', '') != 'application/json; charset=utf-8'):\n            self.stop(('Hub error uploading file \"%s\".' % relative_path))\n            continue\n        answer = json_loads(r.data)\n        if (r.status != 200):\n            if answer.get('corrupt', False):\n                self.stop(('File \"%s\" corrupted on transit.' % relative_path))\n            else:\n                msg = answer.get('msg', None)\n                if msg:\n                    self.stop(('Error when uploading file \"%s\".\\n%s' % (relative_path, msg)))\n                else:\n                    self.stop(('Error when uploading file \"%s\": \"%s\"' % (relative_path, r.reason)))\n            continue\n        if (not answer.get('ok', False)):\n            self.stop(('Error uploading file \"%s\".' % relative_path))\n            continue\n        uploaded_queue_put((relative_path, file_size, file_hash))\n        answer = None\n        r = None\n        params = None\n        relative_path = None\n        deploy_path = None\n        item = None\n", "label": 1}
{"function": "\n\ndef __init__(self, test, host, output_format=None):\n    '\\n\\n        :rtype : object\\n        '\n    self.passed = 0\n    self.failed = 0\n    self.duration_ms = 0\n    try:\n        validictory.validate(test, SCHEMA)\n        LOG.debug('Valid schema')\n    except ValueError as error:\n        LOG.error('Error, invalid test format: {0}.  Tests now use v0.2 format. v0.1 branch is still available.'.format(error))\n        raise\n    try:\n        self.port = test['port']\n    except (AttributeError, KeyError):\n        print('Warning: No port definition found in the first test, using port 80 as default.')\n        try:\n            if (test['protocol'] == 'https'):\n                self.port = 443\n            else:\n                self.port = 80\n        except (AttributeError, KeyError):\n            self.port = 80\n    self.output = Output(output_format=output_format)\n    LOG.debug('Test: {0}'.format(test))\n    test_defaults = dict(inputs=dict(allow_redirects=False, timeout=30), method='get', outcomes=dict(expect_status_code=200, colour_output=True))\n    host_overrides = get_host_overrides.get_host_overrides(host, self.port)\n    if (host_overrides['hostname'] is not None):\n        self.host = host_overrides['hostname']\n    else:\n        self.host = host\n    intermediate_dict = deepupdate(test_defaults, host_overrides)\n    final_dict = deepupdate(intermediate_dict, test)\n    if (('tcp_test' in test) and test['tcp_test']):\n        tcptest.tcp_test(self.host, self.port)\n    try:\n        verify = final_dict['inputs']['verify']\n    except (AttributeError, KeyError):\n        verify = None\n    try:\n        proto = final_dict['protocol']\n    except (AttributeError, KeyError):\n        proto = None\n    (self.verify, self.verify_specified) = get_verify.get_verify(verify, proto)\n    self.test = deepcopy(final_dict)\n    LOG.debug('Test with defaults: {0}'.format(self.test))\n    if ('verify' in self.test['inputs']):\n        del self.test['inputs']['verify']\n    self.inputs = deepcopy(self.test['inputs'])\n    request_url_format = '{protocol}://{host}:{port}{uri}'\n    try:\n        self.inputs['url'] = request_url_format.format(protocol=self.test['protocol'], host=self.host, port=self.test['port'], uri=self.test['uri'])\n    except KeyError:\n        self.inputs['url'] = request_url_format.format(protocol=self.test['protocol'], host=self.host, port=self.test['port'], uri='')\n    LOG.debug('Testing {0}'.format(self.inputs['url']))\n    self.output.append(('-' * OUTPUT_WIDTH))\n    this_name = (('{0:^' + str(OUTPUT_WIDTH)) + '}').format(self.test['name'][:OUTPUT_WIDTH])\n    self.output.append(this_name)\n    this_url = (('{0:^' + str(OUTPUT_WIDTH)) + '}').format(self.inputs['url'][:OUTPUT_WIDTH])\n    self.output.append(this_url)\n    self.output.append(self.__repr__())\n    self.output.append(('-' * OUTPUT_WIDTH))\n    self.output.append(self.inputs, sec='request_inputs')\n    self.run()\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    urls = self.websocket.url_map.bind_to_environ(environ)\n    try:\n        (endpoint, args) = urls.match()\n        handler = self.websocket.view_functions[endpoint]\n    except HTTPException:\n        handler = None\n    if ((not handler) or ('HTTP_SEC_WEBSOCKET_KEY' not in environ)):\n        return self.wsgi_app(environ, start_response)\n    uwsgi.websocket_handshake(environ['HTTP_SEC_WEBSOCKET_KEY'], environ.get('HTTP_ORIGIN', ''))\n    send_event = Event()\n    send_queue = Queue()\n    recv_event = Event()\n    recv_queue = Queue(maxsize=1)\n    client = self.client(environ, uwsgi.connection_fd(), send_event, send_queue, recv_event, recv_queue, self.websocket.timeout)\n    handler = spawn(handler, client, **args)\n\n    def listener(client):\n        select([client.fd], [], [], client.timeout)\n        recv_event.set()\n    listening = spawn(listener, client)\n    while True:\n        if (not client.connected):\n            recv_queue.put(None)\n            listening.kill()\n            handler.join(client.timeout)\n            return ''\n        wait([handler, send_event, recv_event], None, 1)\n        if send_event.is_set():\n            try:\n                while True:\n                    uwsgi.websocket_send(send_queue.get_nowait())\n            except Empty:\n                send_event.clear()\n            except IOError:\n                client.connected = False\n        elif recv_event.is_set():\n            recv_event.clear()\n            try:\n                recv_queue.put(uwsgi.websocket_recv_nb())\n                listening = spawn(listener, client)\n            except IOError:\n                client.connected = False\n        elif handler.ready():\n            listening.kill()\n            return ''\n", "label": 1}
{"function": "\n\ndef assetstore(self, name, type, root=None, db=None, mongohost=None, replicaset='', bucket=None, prefix='', accessKeyId=None, secret=None, service='s3.amazonaws.com', host=None, port=None, path=None, user=None, webHdfsPort=None, readOnly=False, current=False):\n    if (type not in self.assetstore_types.keys()):\n        self.fail(('assetstore type %s is not implemented!' % type))\n    argument_hash = {\n        'filesystem': {\n            'name': name,\n            'type': self.assetstore_types[type],\n            'root': root,\n        },\n        'gridfs': {\n            'name': name,\n            'type': self.assetstore_types[type],\n            'db': db,\n            'mongohost': mongohost,\n            'replicaset': replicaset,\n        },\n        's3': {\n            'name': name,\n            'type': self.assetstore_types[type],\n            'bucket': bucket,\n            'prefix': prefix,\n            'accessKeyId': accessKeyId,\n            'secret': secret,\n            'service': service,\n        },\n        'hdfs': {\n            'name': name,\n            'type': self.assetstore_types[type],\n            'host': host,\n            'port': port,\n            'path': path,\n            'user': user,\n            'webHdfsPort': webHdfsPort,\n        },\n    }\n    for (k, v) in argument_hash[type].items():\n        if (v is None):\n            self.fail(('assetstores of type %s require attribute %s' % (type, k)))\n    argument_hash[type]['readOnly'] = readOnly\n    argument_hash[type]['current'] = current\n    ret = []\n    assetstores = {a['name']: a for a in self.get('assetstore')}\n    self.message['debug']['assetstores'] = assetstores\n    if (self.module.params['state'] == 'present'):\n        if (name in assetstores.keys()):\n            id = assetstores[name]['_id']\n            updateable = ['root', 'mongohost', 'replicaset', 'bucket', 'prefix', 'db', 'accessKeyId', 'secret', 'service', 'host', 'port', 'path', 'user', 'webHdfsPort', 'current']\n            assetstore_items = set(((k, assetstores[name][k]) for k in updateable if (k in assetstores[name].keys())))\n            arg_hash_items = set(((k, argument_hash[type][k]) for k in updateable if (k in argument_hash[type].keys())))\n            if (not (arg_hash_items <= assetstore_items)):\n                ret = self.put(('assetstore/%s' % id), parameters=argument_hash[type])\n                self.changed = True\n        else:\n            try:\n                getattr(self, ('__validate_%s_assetstore' % type))(**argument_hash)\n            except AttributeError:\n                pass\n            ret = self.post('assetstore', parameters=argument_hash[type])\n            self.changed = True\n    elif (self.module.params['state'] == 'absent'):\n        if (name in assetstores.keys()):\n            id = assetstores[name]['_id']\n            ret = self.delete(('assetstore/%s' % id), parameters=argument_hash[type])\n    return ret\n", "label": 1}
{"function": "\n\ndef __init__(self, toklist, name=None, asList=True, modal=True, isinstance=isinstance):\n    if self.__doinit:\n        self.__doinit = False\n        self.__name = None\n        self.__parent = None\n        self.__accumNames = {\n            \n        }\n        if isinstance(toklist, list):\n            self.__toklist = toklist[:]\n        elif isinstance(toklist, _generatorType):\n            self.__toklist = list(toklist)\n        else:\n            self.__toklist = [toklist]\n        self.__tokdict = dict()\n    if ((name is not None) and name):\n        if (not modal):\n            self.__accumNames[name] = 0\n        if isinstance(name, int):\n            name = _ustr(name)\n        self.__name = name\n        if (not (isinstance(toklist, (type(None), basestring, list)) and (toklist in (None, '', [])))):\n            if isinstance(toklist, basestring):\n                toklist = [toklist]\n            if asList:\n                if isinstance(toklist, ParseResults):\n                    self[name] = _ParseResultsWithOffset(toklist.copy(), 0)\n                else:\n                    self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]), 0)\n                self[name].__name = name\n            else:\n                try:\n                    self[name] = toklist[0]\n                except (KeyError, TypeError, IndexError):\n                    self[name] = toklist\n", "label": 1}
{"function": "\n\ndef test_code_interpolation(self):\n\n    def f():\n        i = 1500\n        while (i > 0):\n            i -= 1\n            i\n    with tracebin.record() as recorder:\n        f()\n    [trace] = recorder.traces\n    [entry, preamble, loop] = trace.sections\n    assert (len(loop.chunks) == 7)\n    assert (loop.chunks[0].get_op_names() == ['label'])\n    assert (loop.chunks[1].sourcelines == ['        def f():\\n', '            i = 1500\\n', '            while i > 0:\\n'])\n    assert (loop.chunks[1].linenos == [87, 88, 89])\n    assert (loop.chunks[2].get_op_names() == ['debug_merge_point', 'debug_merge_point', 'debug_merge_point', 'int_gt', 'guard_true', 'debug_merge_point'])\n    assert (loop.chunks[3].sourcelines == ['                i -= 1\\n'])\n    assert (loop.chunks[3].linenos == [90])\n    assert (loop.chunks[4].get_op_names() == ['debug_merge_point', 'debug_merge_point', 'debug_merge_point', 'int_sub', 'debug_merge_point'])\n    assert (loop.chunks[5].sourcelines == ['                i\\n'])\n    if self.thread_spawned():\n        assert (loop.chunks[6].get_op_names() == ['debug_merge_point', 'debug_merge_point', 'debug_merge_point', 'guard_not_invalidated', 'getfield_raw', 'int_sub', 'setfield_raw', 'int_lt', 'guard_false', 'debug_merge_point', 'jump'])\n    else:\n        assert (loop.chunks[6].get_op_names() == ['debug_merge_point', 'debug_merge_point', 'debug_merge_point', 'guard_not_invalidated', 'getfield_raw', 'int_lt', 'guard_false', 'debug_merge_point', 'jump'])\n", "label": 1}
{"function": "\n\ndef get_value(self, var, cast=None, default=NOTSET, parse_default=False):\n    'Return value for given environment variable.\\n\\n        :param var: Name of variable.\\n        :param cast: Type to cast return value as.\\n        :param default: If var not present in environ, return this instead.\\n        :param parse_default: force to parse default..\\n\\n        :returns: Value from environment or default (if set)\\n        '\n    logger.debug(\"get '{0}' casted as '{1}' with default '{2}'\".format(var, cast, default))\n    if (var in self.scheme):\n        var_info = self.scheme[var]\n        try:\n            has_default = (len(var_info) == 2)\n        except TypeError:\n            has_default = False\n        if has_default:\n            if (not cast):\n                cast = var_info[0]\n            if (default is self.NOTSET):\n                try:\n                    default = var_info[1]\n                except IndexError:\n                    pass\n        elif (not cast):\n            cast = var_info\n    try:\n        value = self.ENVIRON[var]\n    except KeyError:\n        if (default is self.NOTSET):\n            error_msg = 'Set the {0} environment variable'.format(var)\n            raise ImproperlyConfigured(error_msg)\n        value = default\n    if (hasattr(value, 'startswith') and value.startswith('$')):\n        value = value.lstrip('$')\n        value = self.get_value(value, cast=cast, default=default)\n    if ((value != default) or (parse_default and value)):\n        value = self.parse_value(value, cast)\n    return value\n", "label": 1}
{"function": "\n\ndef addCallback(self, callback):\n    'Adds a callback to the callbacks list.\\n\\n        :param callback: A callback object\\n        :type callback: supybot.irclib.IrcCallback\\n        '\n    assert (not self.getCallback(callback.name()))\n    self.callbacks.append(callback)\n    cbs = []\n    edges = set()\n    for cb in self.callbacks:\n        (before, after) = cb.callPrecedence(self)\n        assert (cb not in after), 'cb was in its own after.'\n        assert (cb not in before), 'cb was in its own before.'\n        for otherCb in before:\n            edges.add((otherCb, cb))\n        for otherCb in after:\n            edges.add((cb, otherCb))\n\n    def getFirsts():\n        firsts = (set(self.callbacks) - set(cbs))\n        for (before, after) in edges:\n            firsts.discard(after)\n        return firsts\n    firsts = getFirsts()\n    while firsts:\n        for cb in firsts:\n            cbs.append(cb)\n            edgesToRemove = []\n            for edge in edges:\n                if (edge[0] is cb):\n                    edgesToRemove.append(edge)\n            for edge in edgesToRemove:\n                edges.remove(edge)\n        firsts = getFirsts()\n    assert (len(cbs) == len(self.callbacks)), ('cbs: %s, self.callbacks: %s' % (cbs, self.callbacks))\n    self.callbacks[:] = cbs\n", "label": 1}
{"function": "\n\n@periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)\ndef _poll_bandwidth_usage(self, context):\n    if (not self._bw_usage_supported):\n        return\n    (prev_time, start_time) = utils.last_completed_audit_period()\n    curr_time = time.time()\n    if ((curr_time - self._last_bw_usage_poll) > CONF.bandwidth_poll_interval):\n        self._last_bw_usage_poll = curr_time\n        LOG.info(_LI('Updating bandwidth usage cache'))\n        cells_update_interval = CONF.cells.bandwidth_update_interval\n        if ((cells_update_interval > 0) and ((curr_time - self._last_bw_usage_cell_update) > cells_update_interval)):\n            self._last_bw_usage_cell_update = curr_time\n            update_cells = True\n        else:\n            update_cells = False\n        instances = objects.InstanceList.get_by_host(context, self.host, use_slave=True)\n        try:\n            bw_counters = self.driver.get_all_bw_counters(instances)\n        except NotImplementedError:\n            LOG.info(_LI('Bandwidth usage not supported by hypervisor.'))\n            self._bw_usage_supported = False\n            return\n        refreshed = timeutils.utcnow()\n        for bw_ctr in bw_counters:\n            greenthread.sleep(0)\n            bw_in = 0\n            bw_out = 0\n            last_ctr_in = None\n            last_ctr_out = None\n            usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(context, bw_ctr['uuid'], bw_ctr['mac_address'], start_period=start_time, use_slave=True)\n            if usage:\n                bw_in = usage.bw_in\n                bw_out = usage.bw_out\n                last_ctr_in = usage.last_ctr_in\n                last_ctr_out = usage.last_ctr_out\n            else:\n                usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(context, bw_ctr['uuid'], bw_ctr['mac_address'], start_period=prev_time, use_slave=True)\n                if usage:\n                    last_ctr_in = usage.last_ctr_in\n                    last_ctr_out = usage.last_ctr_out\n            if (last_ctr_in is not None):\n                if (bw_ctr['bw_in'] < last_ctr_in):\n                    bw_in += bw_ctr['bw_in']\n                else:\n                    bw_in += (bw_ctr['bw_in'] - last_ctr_in)\n            if (last_ctr_out is not None):\n                if (bw_ctr['bw_out'] < last_ctr_out):\n                    bw_out += bw_ctr['bw_out']\n                else:\n                    bw_out += (bw_ctr['bw_out'] - last_ctr_out)\n            objects.BandwidthUsage(context=context).create(bw_ctr['uuid'], bw_ctr['mac_address'], bw_in, bw_out, bw_ctr['bw_in'], bw_ctr['bw_out'], start_period=start_time, last_refreshed=refreshed, update_cells=update_cells)\n", "label": 1}
{"function": "\n\ndef test_intervallookupone():\n    table = (('start', 'stop', 'value'), (1, 4, 'foo'), (3, 7, 'bar'), (4, 9, 'baz'))\n    lkp = intervallookupone(table, 'start', 'stop', value='value')\n    actual = lkp.search(0, 1)\n    expect = None\n    eq_(expect, actual)\n    actual = lkp.search(1, 2)\n    expect = 'foo'\n    eq_(expect, actual)\n    try:\n        lkp.search(2, 4)\n    except DuplicateKeyError:\n        pass\n    else:\n        assert False, 'expected error'\n    try:\n        lkp.search(2, 5)\n    except DuplicateKeyError:\n        pass\n    else:\n        assert False, 'expected error'\n    try:\n        lkp.search(4, 5)\n    except DuplicateKeyError:\n        pass\n    else:\n        assert False, 'expected error'\n    try:\n        lkp.search(5, 7)\n    except DuplicateKeyError:\n        pass\n    else:\n        assert False, 'expected error'\n    actual = lkp.search(8, 9)\n    expect = 'baz'\n    eq_(expect, actual)\n    actual = lkp.search(9, 14)\n    expect = None\n    eq_(expect, actual)\n    actual = lkp.search(19, 140)\n    expect = None\n    eq_(expect, actual)\n    actual = lkp.search(0)\n    expect = None\n    eq_(expect, actual)\n    actual = lkp.search(1)\n    expect = 'foo'\n    eq_(expect, actual)\n    actual = lkp.search(2)\n    expect = 'foo'\n    eq_(expect, actual)\n    try:\n        lkp.search(4)\n    except DuplicateKeyError:\n        pass\n    else:\n        assert False, 'expected error'\n    try:\n        lkp.search(5)\n    except DuplicateKeyError:\n        pass\n    else:\n        assert False, 'expected error'\n    actual = lkp.search(8)\n    expect = 'baz'\n    eq_(expect, actual)\n", "label": 1}
{"function": "\n\ndef get_esky_freezer_includes(self):\n    freezer_includes = ['zmq.core.*', 'zmq.utils.*', 'ast', 'csv', 'difflib', 'distutils', 'distutils.version', 'numbers', 'json', 'M2Crypto', 'Cookie', 'asyncore', 'fileinput', 'sqlite3', 'email', 'email.mime.*', 'requests', 'sqlite3']\n    if (HAS_ZMQ and hasattr(zmq, 'pyzmq_version_info')):\n        if (HAS_ZMQ and (zmq.pyzmq_version_info() >= (0, 14))):\n            if ('zmq.core.*' in freezer_includes):\n                freezer_includes.remove('zmq.core.*')\n    if IS_WINDOWS_PLATFORM:\n        freezer_includes.extend(['imp', 'win32api', 'win32file', 'win32con', 'win32com', 'win32net', 'win32netcon', 'win32gui', 'win32security', 'ntsecuritycon', 'pywintypes', 'pythoncom', '_winreg', 'wmi', 'site', 'psutil'])\n    elif IS_SMARTOS_PLATFORM:\n        freezer_includes.extend(['cherrypy', 'dateutils', 'pyghmi', 'croniter', 'mako', 'gnupg'])\n    elif sys.platform.startswith('linux'):\n        freezer_includes.append('spwd')\n        try:\n            import yum\n            freezer_includes.append('yum')\n        except ImportError:\n            pass\n    elif sys.platform.startswith('sunos'):\n        try:\n            from bbfreeze.modulegraph.modulegraph import ModuleGraph\n            mgraph = ModuleGraph(sys.path[:])\n            for arg in glob.glob('salt/modules/*.py'):\n                mgraph.run_script(arg)\n            for mod in mgraph.flatten():\n                if ((type(mod).__name__ != 'Script') and mod.filename):\n                    freezer_includes.append(str(os.path.basename(mod.identifier)))\n        except ImportError:\n            pass\n        freezer_includes.extend(['sodium_grabber', 'ioflo', 'raet', 'libnacl'])\n    return freezer_includes\n", "label": 1}
{"function": "\n\ndef test_video_fromname_episode(episodes):\n    video = Video.fromname(episodes['bbt_s07e05'].name)\n    assert (type(video) is Episode)\n    assert (video.name == episodes['bbt_s07e05'].name)\n    assert (video.format == episodes['bbt_s07e05'].format)\n    assert (video.release_group == episodes['bbt_s07e05'].release_group)\n    assert (video.resolution == episodes['bbt_s07e05'].resolution)\n    assert (video.video_codec == episodes['bbt_s07e05'].video_codec)\n    assert (video.audio_codec is None)\n    assert (video.imdb_id is None)\n    assert (video.hashes == {\n        \n    })\n    assert (video.size is None)\n    assert (video.subtitle_languages == set())\n    assert (video.series == episodes['bbt_s07e05'].series)\n    assert (video.season == episodes['bbt_s07e05'].season)\n    assert (video.episode == episodes['bbt_s07e05'].episode)\n    assert (video.title is None)\n    assert (video.year is None)\n    assert (video.tvdb_id is None)\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Main master loop.'\n    self.start()\n    util._setproctitle(('master [%s]' % self.proc_name))\n    try:\n        self.manage_workers()\n        while True:\n            sig = (self.SIG_QUEUE.pop(0) if len(self.SIG_QUEUE) else None)\n            if (sig is None):\n                self.sleep()\n                self.murder_workers()\n                self.manage_workers()\n                continue\n            if (sig not in self.SIG_NAMES):\n                self.log.info('Ignoring unknown signal: %s', sig)\n                continue\n            signame = self.SIG_NAMES.get(sig)\n            handler = getattr(self, ('handle_%s' % signame), None)\n            if (not handler):\n                self.log.error('Unhandled signal: %s', signame)\n                continue\n            self.log.info('Handling signal: %s', signame)\n            handler()\n            self.wakeup()\n    except StopIteration:\n        self.halt()\n    except KeyboardInterrupt:\n        self.halt()\n    except HaltServer as inst:\n        self.halt(reason=inst.reason, exit_status=inst.exit_status)\n    except SystemExit:\n        raise\n    except Exception:\n        self.log.info('Unhandled exception in main loop:\\n%s', traceback.format_exc())\n        self.stop(False)\n        if (self.pidfile is not None):\n            self.pidfile.unlink()\n        sys.exit((- 1))\n", "label": 1}
{"function": "\n\ndef modify(name, beacon_data, **kwargs):\n    '\\n    Modify an existing beacon\\n\\n    :param name:            Name of the beacon to configure\\n    :param beacon_data:     Dictionary or list containing updated configuration for beacon.\\n    :return:                Boolean and status message on success or failure of modify.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt \\'*\\' beacons.modify ps \"{\\'salt-master\\': \\'stopped\\', \\'apache2\\': \\'stopped\\'}\"\\n    '\n    ret = {\n        'comment': '',\n        'result': True,\n    }\n    current_beacons = list_(return_yaml=False)\n    if (name not in current_beacons):\n        ret['comment'] = 'Beacon {0} is not configured.'.format(name)\n        return ret\n    if (('test' in kwargs) and kwargs['test']):\n        ret['result'] = True\n        ret['comment'] = 'Beacon: {0} would be added.'.format(name)\n    else:\n        try:\n            beacon_module = __import__(('salt.beacons.' + name), fromlist=['validate'])\n            log.debug('Successfully imported beacon.')\n        except ImportError:\n            ret['comment'] = 'Beacon {0} does not exist'.format(name)\n            return ret\n        if hasattr(beacon_module, 'validate'):\n            _beacon_data = beacon_data\n            if ('enabled' in _beacon_data):\n                del _beacon_data['enabled']\n            (valid, vcomment) = beacon_module.validate(_beacon_data)\n        else:\n            log.info('Beacon {0} does not have a validate function,  skipping validation.'.format(name))\n            valid = True\n        if (not valid):\n            ret['result'] = False\n            ret['comment'] = 'Beacon {0} configuration invalid, not adding.\\n{1}'.format(name, vcomment)\n            return ret\n        _current = current_beacons[name]\n        _new = beacon_data\n        if (_new == _current):\n            ret['comment'] = 'Job {0} in correct state'.format(name)\n            return ret\n        _current_lines = ['{0}:{1}\\n'.format(key, value) for (key, value) in sorted(_current.items())]\n        _new_lines = ['{0}:{1}\\n'.format(key, value) for (key, value) in sorted(_new.items())]\n        _diff = difflib.unified_diff(_current_lines, _new_lines)\n        ret['changes'] = {\n            \n        }\n        ret['changes']['diff'] = ''.join(_diff)\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({\n                'name': name,\n                'beacon_data': beacon_data,\n                'func': 'modify',\n            }, 'manage_beacons')\n            if res:\n                event_ret = eventer.get_event(tag='/salt/minion/minion_beacon_modify_complete', wait=30)\n                if (event_ret and event_ret['complete']):\n                    beacons = event_ret['beacons']\n                    if ((name in beacons) and (beacons[name] == beacon_data)):\n                        ret['result'] = True\n                        ret['comment'] = 'Modified beacon: {0}.'.format(name)\n                        return ret\n        except KeyError:\n            ret['comment'] = 'Event module not available. Beacon add failed.'\n    return ret\n", "label": 1}
{"function": "\n\ndef foreignkey_autocomplete(self, request):\n    '\\n        Searches in the fields of the given related model and returns the\\n        result as a simple string to be used by the jQuery Autocomplete plugin\\n        '\n    query = request.GET.get('q', None)\n    app_label = request.GET.get('app_label', None)\n    model_name = request.GET.get('model_name', None)\n    search_fields = request.GET.get('search_fields', None)\n    object_pk = request.GET.get('object_pk', None)\n    try:\n        to_string_function = self.related_string_functions[model_name]\n    except KeyError:\n        to_string_function = (lambda x: x.__unicode__())\n    if (search_fields and app_label and model_name and (query or object_pk)):\n\n        def construct_search(field_name):\n            if field_name.startswith('^'):\n                return ('%s__istartswith' % field_name[1:])\n            elif field_name.startswith('='):\n                return ('%s__iexact' % field_name[1:])\n            elif field_name.startswith('@'):\n                return ('%s__search' % field_name[1:])\n            else:\n                return ('%s__icontains' % field_name)\n        model = models.get_model(app_label, model_name)\n        queryset = model._default_manager.all()\n        data = ''\n        if query:\n            for bit in query.split():\n                or_queries = [models.Q(**{\n                    construct_search(smart_str(field_name)): smart_str(bit),\n                }) for field_name in search_fields.split(',')]\n                other_qs = QuerySet(model)\n                other_qs.dup_select_related(queryset)\n                other_qs = other_qs.filter(reduce(operator.or_, or_queries))\n                queryset = (queryset & other_qs)\n            data = ''.join([('%s|%s\\n' % (to_string_function(f), f.pk)) for f in queryset])\n        elif object_pk:\n            try:\n                obj = queryset.get(pk=object_pk)\n            except:\n                pass\n            else:\n                data = to_string_function(obj)\n        return HttpResponse(data)\n    return HttpResponseNotFound()\n", "label": 1}
{"function": "\n\ndef _get_fetch_info_from_stderr(self, proc, progress):\n    output = IterableList('name')\n    fetch_info_lines = list()\n    cmds = (set(PushInfo._flag_map.keys()) & set(FetchInfo._flag_map.keys()))\n    progress_handler = progress.new_message_handler()\n    for line in proc.stderr.readlines():\n        line = line.decode(defenc)\n        for pline in progress_handler(line):\n            if (line.startswith('fatal:') or line.startswith('error:')):\n                raise GitCommandError((('Error when fetching: %s' % line),), 2)\n            for cmd in cmds:\n                if ((len(line) > 1) and (line[0] == ' ') and (line[1] == cmd)):\n                    fetch_info_lines.append(line)\n                    continue\n    finalize_process(proc)\n    fp = open(join(self.repo.git_dir, 'FETCH_HEAD'), 'rb')\n    fetch_head_info = [l.decode(defenc) for l in fp.readlines()]\n    fp.close()\n    l_fil = len(fetch_info_lines)\n    l_fhi = len(fetch_head_info)\n    assert (l_fil >= l_fhi), ('len(%s) <= len(%s)' % (l_fil, l_fhi))\n    output.extend((FetchInfo._from_line(self.repo, err_line, fetch_line) for (err_line, fetch_line) in zip(fetch_info_lines, fetch_head_info)))\n    return output\n", "label": 1}
{"function": "\n\n@non_atomic_requests\ndef guid_search(request, api_version, guids):\n    lang = request.LANG\n\n    def guid_search_cache_key(guid):\n        key = ('guid_search:%s:%s:%s' % (api_version, lang, guid))\n        return hashlib.md5(smart_str(key)).hexdigest()\n    guids = ([g.strip() for g in guids.split(',')] if guids else [])\n    addons_xml = cache.get_many([guid_search_cache_key(g) for g in guids])\n    dirty_keys = set()\n    for g in guids:\n        key = guid_search_cache_key(g)\n        if (key not in addons_xml):\n            dirty_keys.add(key)\n            try:\n                addon = Addon.objects.get(guid=g, disabled_by_user=False, status__in=SEARCHABLE_STATUSES)\n            except Addon.DoesNotExist:\n                addons_xml[key] = ''\n            else:\n                addon_xml = render_xml_to_string(request, 'legacy_api/includes/addon.xml', {\n                    'addon': addon,\n                    'api_version': api_version,\n                    'api': legacy_api,\n                })\n                addons_xml[key] = addon_xml\n    cache.set_many(dict(((k, v) for (k, v) in addons_xml.iteritems() if (k in dirty_keys))))\n    compat = CompatOverride.objects.filter(guid__in=guids).transform(CompatOverride.transformer)\n    addons_xml = [v for v in addons_xml.values() if v]\n    return render_xml(request, 'legacy_api/search.xml', {\n        'addons_xml': addons_xml,\n        'total': len(addons_xml),\n        'compat': compat,\n        'api_version': api_version,\n        'api': legacy_api,\n    })\n", "label": 1}
{"function": "\n\ndef parse_revision_spec(self, revisions=[]):\n    'Parses the given revision spec.\\n\\n        The \\'revisions\\' argument is a list of revisions as specified by the\\n        user. Items in the list do not necessarily represent a single revision,\\n        since the user can use SCM-native syntaxes such as \"r1..r2\" or \"r1:r2\".\\n        SCMTool-specific overrides of this method are expected to deal with\\n        such syntaxes.\\n\\n        This will return a dictionary with the following keys:\\n            \\'base\\':        A revision to use as the base of the resulting diff.\\n            \\'tip\\':         A revision to use as the tip of the resulting diff.\\n            \\'parent_base\\': (optional) The revision to use as the base of a\\n                           parent diff.\\n            \\'commit_id\\':   (optional) The ID of the single commit being posted,\\n                           if not using a range.\\n\\n        These will be used to generate the diffs to upload to Review Board (or\\n        print). The diff for review will include the changes in (base, tip],\\n        and the parent diff (if necessary) will include (parent_base, base].\\n\\n        If a single revision is passed in, this will return the parent of that\\n        revision for \\'base\\' and the passed-in revision for \\'tip\\'.\\n\\n        If zero revisions are passed in, this will return the current HEAD as\\n        \\'tip\\', and the upstream branch as \\'base\\', taking into account parent\\n        branches explicitly specified via --parent.\\n        '\n    n_revs = len(revisions)\n    result = {\n        \n    }\n    if (n_revs == 0):\n        parent_branch = self.get_parent_branch()\n        head_ref = self._rev_parse(self.get_head_ref())[0]\n        merge_base = self._rev_parse(self._get_merge_base(head_ref, self.upstream_branch))[0]\n        result = {\n            'tip': head_ref,\n            'commit_id': head_ref,\n        }\n        if parent_branch:\n            result['base'] = self._rev_parse(parent_branch)[0]\n            result['parent_base'] = merge_base\n        else:\n            result['base'] = merge_base\n        if self.has_pending_changes():\n            logging.warning('Your working directory is not clean. Any changes which have not been committed to a branch will not be included in your review request.')\n    elif ((n_revs == 1) or (n_revs == 2)):\n        parsed = self._rev_parse(revisions)\n        n_parsed_revs = len(parsed)\n        assert (n_parsed_revs <= 3)\n        if (n_parsed_revs == 1):\n            parent = self._rev_parse(('%s^' % parsed[0]))[0]\n            result = {\n                'base': parent,\n                'tip': parsed[0],\n                'commit_id': parsed[0],\n            }\n        elif (n_parsed_revs == 2):\n            if parsed[1].startswith('^'):\n                result = {\n                    'base': parsed[1][1:],\n                    'tip': parsed[0],\n                }\n            else:\n                result = {\n                    'base': parsed[0],\n                    'tip': parsed[1],\n                }\n        elif ((n_parsed_revs == 3) and parsed[2].startswith('^')):\n            merge_base = execute([self.git, 'merge-base', parsed[0], parsed[1]]).strip()\n            result = {\n                'base': merge_base,\n                'tip': parsed[0],\n            }\n        else:\n            raise InvalidRevisionSpecError('Unexpected result while parsing revision spec')\n        parent_base = self._get_merge_base(result['base'], self.upstream_branch)\n        if (parent_base != result['base']):\n            result['parent_base'] = parent_base\n    else:\n        raise TooManyRevisionsError\n    return result\n", "label": 1}
{"function": "\n\ndef convert_to_datetime(input, tz, arg_name):\n    '\\n    Converts the given object to a timezone aware datetime object.\\n\\n    If a timezone aware datetime object is passed, it is returned unmodified.\\n    If a native datetime object is passed, it is given the specified timezone.\\n    If the input is a string, it is parsed as a datetime with the given timezone.\\n\\n    Date strings are accepted in three different forms: date only (Y-m-d), date with time\\n    (Y-m-d H:M:S) or with date+time with microseconds (Y-m-d H:M:S.micro).\\n\\n    :param str|datetime input: the datetime or string to convert to a timezone aware datetime\\n    :param datetime.tzinfo tz: timezone to interpret ``input`` in\\n    :param str arg_name: the name of the argument (used in an error message)\\n    :rtype: datetime\\n\\n    '\n    if (input is None):\n        return\n    elif isinstance(input, datetime):\n        datetime_ = input\n    elif isinstance(input, date):\n        datetime_ = datetime.combine(input, time())\n    elif isinstance(input, six.string_types):\n        m = _DATE_REGEX.match(input)\n        if (not m):\n            raise ValueError('Invalid date string')\n        values = [(k, int((v or 0))) for (k, v) in m.groupdict().items()]\n        values = dict(values)\n        datetime_ = datetime(**values)\n    else:\n        raise TypeError(('Unsupported type for %s: %s' % (arg_name, input.__class__.__name__)))\n    if (datetime_.tzinfo is not None):\n        return datetime_\n    if (tz is None):\n        raise ValueError(('The \"tz\" argument must be specified if %s has no timezone information' % arg_name))\n    if isinstance(tz, six.string_types):\n        tz = timezone(tz)\n    try:\n        return tz.localize(datetime_, is_dst=None)\n    except AttributeError:\n        raise TypeError('Only pytz timezones are supported (need the localize() and normalize() methods)')\n", "label": 1}
{"function": "\n\ndef appdata_dir(appname=None, roaming=False, macAsLinux=False):\n    ' appdata_dir(appname=None, roaming=False,  macAsLinux=False)\\n    Get the path to the application directory, where applications are allowed\\n    to write user specific files (e.g. configurations). For non-user specific\\n    data, consider using common_appdata_dir().\\n    If appname is given, a subdir is appended (and created if necessary).\\n    If roaming is True, will prefer a roaming directory (Windows Vista/7).\\n    If macAsLinux is True, will return the Linux-like location on Mac.\\n    '\n    userDir = os.path.expanduser('~')\n    path = None\n    if sys.platform.startswith('win'):\n        (path1, path2) = (os.getenv('LOCALAPPDATA'), os.getenv('APPDATA'))\n        path = ((path2 or path1) if roaming else (path1 or path2))\n    elif (sys.platform.startswith('darwin') and (not macAsLinux)):\n        path = os.path.join(userDir, 'Library', 'Application Support')\n    if (not (path and os.path.isdir(path))):\n        path = userDir\n    prefix = sys.prefix\n    if getattr(sys, 'frozen', None):\n        prefix = os.path.abspath(os.path.dirname(sys.path[0]))\n    for reldir in ('settings', '../settings'):\n        localpath = os.path.abspath(os.path.join(prefix, reldir))\n        if os.path.isdir(localpath):\n            try:\n                open(os.path.join(localpath, 'test.write'), 'wb').close()\n                os.remove(os.path.join(localpath, 'test.write'))\n            except IOError:\n                pass\n            else:\n                path = localpath\n                break\n    if appname:\n        if (path == userDir):\n            appname = ('.' + appname.lstrip('.'))\n        path = os.path.join(path, appname)\n        if (not os.path.isdir(path)):\n            os.mkdir(path)\n    return path\n", "label": 1}
{"function": "\n\ndef test_responsive_resizes_width_and_height(output_file_url, selenium):\n    aspect_ratio = 2\n    plot_height = 400\n    plot_width = (400 * aspect_ratio)\n    lower_bound = (aspect_ratio * 0.95)\n    upper_bound = (aspect_ratio * 1.05)\n    template = Template('<!DOCTYPE html>\\n    <html lang=\"en\">\\n        <head>\\n            <meta charset=\"utf-8\">\\n            {{ js_resources }}\\n            {{ css_resources }}\\n        </head>\\n        <body>\\n        {{ plot_div.red }}\\n        {{ plot_script }}\\n        <script>\\n        // Set things up to resize the plot on a window resize. You can play with\\n        // the arguments of resize_width_height() to change the plot\\'s behavior.\\n        var plot_resize_setup = function () {\\n            var plotid = Object.keys(Bokeh.index)[0]; // assume we have just one plot\\n            var plot = Bokeh.index[plotid];\\n            var plotresizer = function() {\\n                // arguments: use width, use height, maintain aspect ratio\\n                plot.resize_width_height(true, true, true);\\n            };\\n            window.addEventListener(\\'resize\\', plotresizer);\\n            plotresizer();\\n        };\\n        window.addEventListener(\\'load\\', plot_resize_setup);\\n        </script>\\n        <style>\\n        /* Need this to get the page in \"desktop mode\"; not having an infinite height.*/\\n        html, body {height: 100%;}\\n        </style>\\n        </body>\\n    </html>\\n    ')\n    PLOT_OPTIONS = dict(plot_width=plot_width, plot_height=plot_height)\n    SCATTER_OPTIONS = dict(size=12, alpha=0.5)\n    data = (lambda : [random.choice([i for i in range(100)]) for r in range(10)])\n    red = figure(responsive=False, tools='pan', **PLOT_OPTIONS)\n    red.scatter(data(), data(), color='red', **SCATTER_OPTIONS)\n    resources = INLINE\n    js_resources = resources.render_js()\n    css_resources = resources.render_css()\n    (script, div) = components({\n        'red': red,\n    })\n    html = template.render(js_resources=js_resources, css_resources=css_resources, plot_script=script, plot_div=div)\n    filename = output_file_url.split('/', 3)[(- 1)]\n    with open(filename, 'w') as f:\n        f.write(html)\n    selenium.set_window_size(width=1200, height=600)\n    selenium.get(output_file_url)\n    canvas = selenium.find_element_by_tag_name('canvas')\n    wait_for_canvas_resize(canvas, selenium)\n    height1 = canvas.size['height']\n    width1 = canvas.size['width']\n    aspect_ratio1 = (width1 / height1)\n    assert (aspect_ratio1 > lower_bound)\n    assert (aspect_ratio1 < upper_bound)\n    selenium.set_window_size(width=800, height=600)\n    wait_for_canvas_resize(canvas, selenium)\n    height2 = canvas.size['height']\n    width2 = canvas.size['width']\n    aspect_ratio2 = (width2 / height2)\n    assert (aspect_ratio2 > lower_bound)\n    assert (aspect_ratio2 < upper_bound)\n    assert (width2 < (width1 - 20))\n    assert (height2 < (height1 - 20))\n    selenium.set_window_size(width=1200, height=600)\n    wait_for_canvas_resize(canvas, selenium)\n    height3 = canvas.size['height']\n    width3 = canvas.size['width']\n    assert (width3 == width1)\n    assert (height3 == height1)\n    selenium.set_window_size(width=1200, height=400)\n    wait_for_canvas_resize(canvas, selenium)\n    height4 = canvas.size['height']\n    width4 = canvas.size['width']\n    aspect_ratio4 = (width4 / height4)\n    assert (aspect_ratio4 > lower_bound)\n    assert (aspect_ratio4 < upper_bound)\n    assert (width4 < (width1 - 20))\n    assert (height4 < (height1 - 20))\n    selenium.set_window_size(width=1200, height=600)\n    wait_for_canvas_resize(canvas, selenium)\n    height5 = canvas.size['height']\n    width5 = canvas.size['width']\n    assert (width5 == width1)\n    assert (height5 == height1)\n", "label": 1}
{"function": "\n\ndef resolveAxesStructure(view, viewTblELR):\n    if isinstance(viewTblELR, (ModelEuTable, ModelTable)):\n        table = viewTblELR\n        for rel in view.modelXbrl.relationshipSet((XbrlConst.tableBreakdown, XbrlConst.tableBreakdownMMDD, XbrlConst.tableBreakdown201305, XbrlConst.tableBreakdown201301, XbrlConst.tableAxis2011)).fromModelObject(table):\n            view.axisSubtreeRelSet = view.modelXbrl.relationshipSet((XbrlConst.tableBreakdownTree, XbrlConst.tableBreakdownTreeMMDD, XbrlConst.tableBreakdownTree201305, XbrlConst.tableDefinitionNodeSubtree, XbrlConst.tableDefinitionNodeSubtreeMMDD, XbrlConst.tableDefinitionNodeSubtree201305, XbrlConst.tableDefinitionNodeSubtree201301, XbrlConst.tableAxisSubtree2011), rel.linkrole)\n            return resolveTableAxesStructure(view, table, view.modelXbrl.relationshipSet((XbrlConst.tableBreakdown, XbrlConst.tableBreakdownMMDD, XbrlConst.tableBreakdown201305, XbrlConst.tableBreakdown201301, XbrlConst.tableAxis2011), rel.linkrole))\n        return (None, None, None, None)\n    tblAxisRelSet = view.modelXbrl.relationshipSet(XbrlConst.euTableAxis, viewTblELR)\n    if (len(tblAxisRelSet.modelRelationships) > 0):\n        view.axisSubtreeRelSet = view.modelXbrl.relationshipSet(XbrlConst.euAxisMember, viewTblELR)\n    else:\n        tblAxisRelSet = view.modelXbrl.relationshipSet((XbrlConst.tableBreakdown, XbrlConst.tableBreakdownMMDD, XbrlConst.tableBreakdown201305, XbrlConst.tableBreakdown201301, XbrlConst.tableAxis2011), viewTblELR)\n        view.axisSubtreeRelSet = view.modelXbrl.relationshipSet((XbrlConst.tableBreakdownTree, XbrlConst.tableBreakdownTreeMMDD, XbrlConst.tableBreakdownTree201305, XbrlConst.tableDefinitionNodeSubtree, XbrlConst.tableDefinitionNodeSubtreeMMDD, XbrlConst.tableDefinitionNodeSubtree201305, XbrlConst.tableDefinitionNodeSubtree201301, XbrlConst.tableAxisSubtree2011), viewTblELR)\n    if ((tblAxisRelSet is None) or (len(tblAxisRelSet.modelRelationships) == 0)):\n        view.modelXbrl.modelManager.addToLog(_('no table relationships for {0}').format(viewTblELR))\n        return (None, None, None, None)\n    modelRoleTypes = view.modelXbrl.roleTypes.get(viewTblELR)\n    if ((modelRoleTypes is not None) and (len(modelRoleTypes) > 0)):\n        view.roledefinition = modelRoleTypes[0].definition\n        if ((view.roledefinition is None) or (view.roledefinition == '')):\n            view.roledefinition = os.path.basename(viewTblELR)\n    try:\n        for table in tblAxisRelSet.rootConcepts:\n            return resolveTableAxesStructure(view, table, tblAxisRelSet)\n    except ResolutionException as ex:\n        view.modelXbrl.error(ex.code, ex.message, exc_info=True, **ex.kwargs)\n    return (None, None, None, None)\n", "label": 1}
{"function": "\n\ndef derive_datasets(self, input_dataset_dict, stack_output_info, tile_type_info):\n    \" Abstract function for calling in stack_derived() function. Should be overridden\\n        in a descendant class.\\n        \\n        Arguments:\\n            input_dataset_dict: Dict keyed by processing level (e.g. ORTHO, NBAR, PQA, DEM)\\n                containing all tile info which can be used within the function\\n                A sample is shown below (including superfluous band-specific information):\\n                \\n{\\n'NBAR': {'band_name': 'Visible Blue',\\n    'band_tag': 'B10',\\n    'end_datetime': datetime.datetime(2000, 2, 9, 23, 46, 36, 722217),\\n    'end_row': 77,\\n    'level_name': 'NBAR',\\n    'nodata_value': -999L,\\n    'path': 91,\\n    'satellite_tag': 'LS7',\\n    'sensor_name': 'ETM+',\\n    'start_datetime': datetime.datetime(2000, 2, 9, 23, 46, 12, 722217),\\n    'start_row': 77,\\n    'tile_layer': 1,\\n    'tile_pathname': '/g/data/v10/datacube/EPSG4326_1deg_0.00025pixel/LS7_ETM/150_-025/2000/LS7_ETM_NBAR_150_-025_2000-02-09T23-46-12.722217.tif',\\n    'x_index': 150,\\n    'y_index': -25},\\n'ORTHO': {'band_name': 'Thermal Infrared (Low Gain)',\\n     'band_tag': 'B61',\\n     'end_datetime': datetime.datetime(2000, 2, 9, 23, 46, 36, 722217),\\n     'end_row': 77,\\n     'level_name': 'ORTHO',\\n     'nodata_value': 0L,\\n     'path': 91,\\n     'satellite_tag': 'LS7',\\n     'sensor_name': 'ETM+',\\n     'start_datetime': datetime.datetime(2000, 2, 9, 23, 46, 12, 722217),\\n     'start_row': 77,\\n     'tile_layer': 1,\\n     'tile_pathname': '/g/data/v10/datacube/EPSG4326_1deg_0.00025pixel/LS7_ETM/150_-025/2000/LS7_ETM_ORTHO_150_-025_2000-02-09T23-46-12.722217.tif',\\n     'x_index': 150,\\n     'y_index': -25},\\n'PQA': {'band_name': 'Pixel Quality Assurance',\\n    'band_tag': 'PQA',\\n    'end_datetime': datetime.datetime(2000, 2, 9, 23, 46, 36, 722217),\\n    'end_row': 77,\\n    'level_name': 'PQA',\\n    'nodata_value': None,\\n    'path': 91,\\n    'satellite_tag': 'LS7',\\n    'sensor_name': 'ETM+',\\n    'start_datetime': datetime.datetime(2000, 2, 9, 23, 46, 12, 722217),\\n    'start_row': 77,\\n    'tile_layer': 1,\\n    'tile_pathname': '/g/data/v10/datacube/EPSG4326_1deg_0.00025pixel/LS7_ETM/150_-025/2000/LS7_ETM_PQA_150_-025_2000-02-09T23-46-12.722217.tif,\\n    'x_index': 150,\\n    'y_index': -25}\\n}                \\n                \\n        Arguments (Cont'd):\\n            stack_output_info: dict containing stack output information. \\n                Obtained from stacker object. \\n                A sample is shown below\\n                \\nstack_output_info = {'x_index': 144, \\n                      'y_index': -36,\\n                      'stack_output_dir': '/g/data/v10/tmp/ndvi',\\n                      'start_datetime': None, # Datetime object or None\\n                      'end_datetime': None, # Datetime object or None \\n                      'satellite': None, # String or None \\n                      'sensor': None} # String or None \\n                      \\n        Arguments (Cont'd):\\n            tile_type_info: dict containing tile type information. \\n                Obtained from stacker object (e.g: stacker.tile_type_dict[tile_type_id]).\\n                A sample is shown below\\n                \\n    {'crs': 'EPSG:4326',\\n    'file_extension': '.tif',\\n    'file_format': 'GTiff',\\n    'format_options': 'COMPRESS=LZW,BIGTIFF=YES',\\n    'tile_directory': 'EPSG4326_1deg_0.00025pixel',\\n    'tile_type_id': 1L,\\n    'tile_type_name': 'Unprojected WGS84 1-degree at 4000 pixels/degree',\\n    'unit': 'degree',\\n    'x_origin': 0.0,\\n    'x_pixel_size': Decimal('0.00025000000000000000'),\\n    'x_pixels': 4000L,\\n    'x_size': 1.0,\\n    'y_origin': 0.0,\\n    'y_pixel_size': Decimal('0.00025000000000000000'),\\n    'y_pixels': 4000L,\\n    'y_size': 1.0}\\n                            \\n        Function must create one or more GDAL-supported output datasets. Useful functions in the\\n        Stacker class include Stacker.get_pqa_mask(), but it is left to the coder to produce exactly\\n        what is required for a single slice of the temporal stack of derived quantities.\\n            \\n        Returns:\\n            output_dataset_info: Dict keyed by stack filename\\n                containing metadata info for GDAL-supported output datasets created by this function.\\n                Note that the key(s) will be used as the output filename for the VRT temporal stack\\n                and each dataset created must contain only a single band. An example is as follows:\\n{'/g/data/v10/tmp/ndvi/NDVI_stack_150_-025.vrt': \\n    {'band_name': 'Normalised Differential Vegetation Index with PQA applied',\\n    'band_tag': 'NDVI',\\n    'end_datetime': datetime.datetime(2000, 2, 9, 23, 46, 36, 722217),\\n    'end_row': 77,\\n    'level_name': 'NDVI',\\n    'nodata_value': None,\\n    'path': 91,\\n    'satellite_tag': 'LS7',\\n    'sensor_name': 'ETM+',\\n    'start_datetime': datetime.datetime(2000, 2, 9, 23, 46, 12, 722217),\\n    'start_row': 77,\\n    'tile_layer': 1,\\n    'tile_pathname': '/g/data/v10/tmp/ndvi/LS7_ETM_NDVI_150_-025_2000-02-09T23-46-12.722217.tif',\\n    'x_index': 150,\\n    'y_index': -25}\\n}\\n                \\n                \\n        \"\n    assert (type(input_dataset_dict) == dict), 'input_dataset_dict must be a dict'\n    log_multiline(logger.debug, input_dataset_dict, 'input_dataset_dict', '\\t')\n    output_dataset_dict = {\n        \n    }\n    for input_level in ['NBAR', 'ORTHO']:\n        input_dataset_info = input_dataset_dict[input_level]\n        input_path = input_dataset_info['tile_pathname']\n        band_dict = self.bands[tile_type_info['tile_type_id']][(input_dataset_info['satellite_tag'], input_dataset_info['sensor_name'])]\n        band_info_list = [band_dict[tile_layer] for tile_layer in sorted(band_dict.keys()) if (band_dict[tile_layer]['level_name'] == input_level)]\n        pqa_mask = self.get_pqa_mask(input_dataset_dict['PQA']['tile_pathname'])\n        input_dataset = gdal.Open(input_path)\n        assert input_dataset, ('Unable to open dataset %s' % input_dataset)\n        no_data_value = input_dataset_info['nodata_value']\n        for band_index in range(input_dataset.RasterCount):\n            output_stack_path = os.path.join(self.output_dir, ('%s_%s_pqa_masked.vrt' % (input_level, band_info_list[band_index]['band_tag'])))\n            output_tile_path = os.path.join(self.output_dir, re.sub('\\\\.\\\\w+$', ('_%s%s' % (band_info_list[band_index]['band_tag'], tile_type_info['file_extension'])), os.path.basename(input_path)))\n            output_dataset_info = dict(input_dataset_info)\n            output_dataset_info['tile_pathname'] = output_tile_path\n            output_dataset_info['band_name'] = ('%s with PQA mask applied' % band_info_list[band_index]['band_name'])\n            output_dataset_info['band_tag'] = ('%s-PQA' % band_info_list[band_index]['band_tag'])\n            output_dataset_info['tile_layer'] = 1\n            if (self.refresh or (not os.path.exists(output_tile_path)) or (not gdal.Open(output_tile_path))):\n                if self.lock_object(output_tile_path):\n                    input_band = input_dataset.GetRasterBand((band_index + 1))\n                    gdal_driver = gdal.GetDriverByName(tile_type_info['file_format'])\n                    output_dataset = gdal_driver.Create(output_tile_path, input_dataset.RasterXSize, input_dataset.RasterYSize, 1, input_band.DataType, tile_type_info['format_options'].split(','))\n                    assert output_dataset, ('Unable to open output dataset %s' % output_dataset)\n                    output_dataset.SetGeoTransform(input_dataset.GetGeoTransform())\n                    output_dataset.SetProjection(input_dataset.GetProjection())\n                    output_band = output_dataset.GetRasterBand(1)\n                    data_array = input_band.ReadAsArray()\n                    self.apply_pqa_mask(data_array, pqa_mask, no_data_value)\n                    output_band.WriteArray(data_array)\n                    output_band.SetNoDataValue(no_data_value)\n                    output_band.FlushCache()\n                    output_dataset_metadata = input_dataset.GetMetadata()\n                    output_dataset_metadata.update(input_band.GetMetadata())\n                    if output_dataset_metadata:\n                        output_dataset.SetMetadata(output_dataset_metadata)\n                        log_multiline(logger.debug, output_dataset_metadata, 'output_dataset_metadata', '\\t')\n                    output_dataset.FlushCache()\n                    self.unlock_object(output_tile_path)\n                    logger.info('Finished writing dataset %s', output_tile_path)\n                else:\n                    logger.info('Skipped locked dataset %s', output_tile_path)\n                    sleep(5)\n            else:\n                logger.info('Skipped existing, valid dataset %s', output_tile_path)\n            output_dataset_dict[output_stack_path] = output_dataset_info\n    log_multiline(logger.debug, output_dataset_dict, 'output_dataset_dict', '\\t')\n    return output_dataset_dict\n", "label": 1}
{"function": "\n\ndef lookup_allowed(self, lookup, value):\n    model = self.model\n    for l in model._meta.related_fkey_lookups:\n        for (k, v) in widgets.url_params_from_lookup_dict(l).items():\n            if ((k == lookup) and (v == value)):\n                return True\n    parts = lookup.split(LOOKUP_SEP)\n    if ((len(parts) > 1) and (parts[(- 1)] in QUERY_TERMS)):\n        parts.pop()\n    pk_attr_name = None\n    for part in parts[:(- 1)]:\n        (field, _, _, _) = model._meta.get_field_by_name(part)\n        if hasattr(field, 'rel'):\n            model = field.rel.to\n            pk_attr_name = model._meta.pk.name\n        elif isinstance(field, RelatedObject):\n            model = field.model\n            pk_attr_name = model._meta.pk.name\n        else:\n            pk_attr_name = None\n    if (pk_attr_name and (len(parts) > 1) and (parts[(- 1)] == pk_attr_name)):\n        parts.pop()\n    try:\n        self.model._meta.get_field_by_name(parts[0])\n    except FieldDoesNotExist:\n        return True\n    else:\n        if (len(parts) == 1):\n            return True\n        clean_lookup = LOOKUP_SEP.join(parts)\n        return ((clean_lookup in self.list_filter) or (clean_lookup == self.date_hierarchy))\n", "label": 1}
{"function": "\n\ndef checkFilterAspectModel(val, variableSet, filterRelationships, xpathContext, uncoverableAspects=None):\n    result = set()\n    if (uncoverableAspects is None):\n        oppositeAspectModel = (_DICT_SET({'dimensional', 'non-dimensional'}) - _DICT_SET({variableSet.aspectModel})).pop()\n        try:\n            uncoverableAspects = (aspectModels[oppositeAspectModel] - aspectModels[variableSet.aspectModel])\n        except KeyError:\n            return result\n    acfAspectsCovering = {\n        \n    }\n    for varFilterRel in filterRelationships:\n        _filter = varFilterRel.toModelObject\n        isAllAspectCoverFilter = False\n        if isinstance(_filter, ModelAspectCover):\n            for aspect in _filter.aspectsCovered(None, xpathContext):\n                if (aspect in acfAspectsCovering):\n                    (otherFilterCover, otherFilterLabel) = acfAspectsCovering[aspect]\n                    if (otherFilterCover != varFilterRel.isCovered):\n                        val.modelXbrl.error('xbrlacfe:inconsistentAspectCoverFilters', _('Variable set %(xlinkLabel)s, aspect cover filter %(filterLabel)s, aspect %(aspect)s, conflicts with %(filterLabel2)s with inconsistent cover attribute'), modelObject=variableSet, xlinkLabel=variableSet.xlinkLabel, filterLabel=_filter.xlinkLabel, aspect=(str(aspect) if isinstance(aspect, QName) else Aspect.label[aspect]), filterLabel2=otherFilterLabel)\n                else:\n                    acfAspectsCovering[aspect] = (varFilterRel.isCovered, _filter.xlinkLabel)\n            isAllAspectCoverFilter = _filter.isAll\n        if True:\n            try:\n                aspectsCovered = _filter.aspectsCovered(None)\n                if ((not isAllAspectCoverFilter) and ((any((isinstance(aspect, QName) for aspect in aspectsCovered)) and (Aspect.DIMENSIONS in uncoverableAspects)) or (aspectsCovered & uncoverableAspects))):\n                    val.modelXbrl.error('xbrlve:filterAspectModelMismatch', _('Variable set %(xlinkLabel)s, aspect model %(aspectModel)s filter %(filterName)s %(filterLabel)s can cover aspect not in aspect model'), modelObject=variableSet, xlinkLabel=variableSet.xlinkLabel, aspectModel=variableSet.aspectModel, filterName=_filter.localName, filterLabel=_filter.xlinkLabel)\n                result |= aspectsCovered\n            except Exception:\n                pass\n            if hasattr(_filter, 'filterRelationships'):\n                result |= checkFilterAspectModel(val, variableSet, _filter.filterRelationships, xpathContext, uncoverableAspects)\n    return result\n", "label": 1}
{"function": "\n\ndef to_xml(self, include_namespaces=True, include_schemalocs=False, ns_dict=None, schemaloc_dict=None, pretty=True, auto_namespace=True, encoding='utf-8'):\n    'Serializes a :class:`Entity` instance to an XML string.\\n\\n        The default character encoding is ``utf-8`` and can be set via the\\n        `encoding` parameter. If `encoding` is ``None``, a unicode string\\n        is returned.\\n\\n        Args:\\n            auto_namespace: Automatically discover and export XML namespaces\\n                for a STIX :class:`Entity` instance.\\n            include_namespaces: Export namespace definitions in the output\\n                XML. Default is ``True``.\\n            include_schemalocs: Export ``xsi:schemaLocation`` attribute\\n                in the output document. This will attempt to associate\\n                namespaces declared in the STIX document with schema locations.\\n                If a namespace cannot be resolved to a schemaLocation, a\\n                Python warning will be raised. Schemalocations will only be\\n                exported if `include_namespaces` is also ``True``.\\n            ns_dict: Dictionary of XML definitions (namespace is key, alias is\\n                value) to include in the exported document. This must be\\n                passed in if `auto_namespace` is ``False``.\\n            schemaloc_dict: Dictionary of XML ``namespace: schema location``\\n                mappings to include in the exported document. These will\\n                only be included if `auto_namespace` is ``False``.\\n            pretty: Pretty-print the XML.\\n            encoding: The output character encoding. Default is ``utf-8``. If\\n                `encoding` is set to ``None``, a unicode string is returned.\\n\\n        Returns:\\n            An XML string for this\\n            :class:`Entity` instance. Default character encoding is ``utf-8``.\\n\\n        '\n    from .utils import nsparser\n    parser = nsparser.NamespaceParser()\n    if auto_namespace:\n        ns_info = nsparser.NamespaceInfo()\n    else:\n        ns_info = None\n    obj = self.to_obj(ns_info=ns_info)\n    if ((not auto_namespace) and (not ns_dict)):\n        raise Exception('Auto-namespacing was disabled but ns_dict was empty or missing.')\n    if auto_namespace:\n        ns_info.finalize(ns_dict=ns_dict, schemaloc_dict=schemaloc_dict)\n        obj_ns_dict = ns_info.binding_namespaces\n    else:\n        ns_info = nsparser.NamespaceInfo()\n        ns_info.finalized_namespaces = (ns_dict or {\n            \n        })\n        ns_info.finalized_schemalocs = (schemaloc_dict or {\n            \n        })\n        obj_ns_dict = dict(itertools.chain(ns_dict.iteritems(), nsparser.DEFAULT_STIX_NAMESPACES.iteritems()))\n    namespace_def = ''\n    if include_namespaces:\n        xmlns = parser.get_xmlns_str(ns_info.finalized_namespaces)\n        namespace_def += ('\\n\\t' + xmlns)\n    if (include_schemalocs and include_namespaces):\n        schemaloc = parser.get_schemaloc_str(ns_info.finalized_schemalocs)\n        namespace_def += ('\\n\\t' + schemaloc)\n    if (not pretty):\n        namespace_def = namespace_def.replace('\\n\\t', ' ')\n    with save_encoding(encoding):\n        sio = StringIO.StringIO()\n        obj.export(sio.write, 0, obj_ns_dict, pretty_print=pretty, namespacedef_=namespace_def)\n    s = unicode(sio.getvalue())\n    if encoding:\n        return s.encode(encoding)\n    return s\n", "label": 1}
{"function": "\n\n@glacier_connect\n@sdb_connect\n@log_class_call('Download an archive.', 'Download archive done.')\ndef download(self, vault_name, archive_id, part_size, out_file_name=None, overwrite=False):\n    '\\n        Download a file from Glacier, and store it in out_file.\\n        If no out_file is given, the file will be dumped on stdout.\\n        '\n    self._check_vault_name(vault_name)\n    self._check_id(archive_id, 'ArchiveId')\n    job_list = self.list_jobs(vault_name)\n    job_id = None\n    for job in job_list:\n        if (job['ArchiveId'] == archive_id):\n            download_job = job\n            if (not job['Completed']):\n                raise CommunicationException('Archive retrieval request not completed yet. Please try again later.', code='NotReady')\n            self.logger.debug('Archive retrieval completed; archive is available for download now.')\n            break\n    else:\n        raise InputException(\"Requested archive not available. Please make sure your archive ID is correct, and start a retrieval job using 'getarchive' if necessary.\", code='IdError')\n    out_file = None\n    if out_file_name:\n        if (os.path.isfile(out_file_name) and (not overwrite)):\n            raise InputException('File exists already, aborting. Use the overwrite flag to overwrite existing file.', code='FileError')\n        try:\n            out_file = open(out_file_name, 'w')\n        except IOError as e:\n            raise InputException('Cannot access the ouput file.', cause=e, code='FileError')\n    total_size = download_job['ArchiveSizeInBytes']\n    part_size_in_bytes = ((self._check_part_size(part_size, total_size) * 1024) * 1024)\n    start_bytes = downloaded_size = 0\n    hash_list = []\n    start_time = current_time = previous_time = time.time()\n    if out_file:\n        self.logger.debug(('Starting download of archive to file %s.' % out_file_name))\n    else:\n        self.logger.debug('Starting download of archive to stdout.')\n    while (downloaded_size < total_size):\n        from_bytes = downloaded_size\n        to_bytes = min((downloaded_size + part_size_in_bytes), total_size)\n        try:\n            response = self.glacierconn.get_job_output(vault_name, download_job['JobId'], byte_range=(from_bytes, (to_bytes - 1)))\n            data = response.read()\n        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:\n            raise ResponseException(('Failed to download archive %s.' % archive_id), cause=self._decode_error_message(e.body), code=e.code)\n        hash_list.append(glaciercorecalls.chunk_hashes(data)[0])\n        downloaded_size = to_bytes\n        if out_file:\n            try:\n                out_file.write(response.read())\n            except IOError as e:\n                raise InputException('Cannot write data to the specified file.', cause=e, code='FileError')\n        else:\n            sys.stdout.write(response.read())\n            sys.stdout.flush()\n        current_time = time.time()\n        overall_rate = int(((downloaded_size - start_bytes) / (current_time - start_time)))\n        current_rate = int((part_size_in_bytes / (current_time - previous_time)))\n        time_left = ((total_size - downloaded_size) / overall_rate)\n        eta_seconds = (current_time + time_left)\n        if (datetime.fromtimestamp(eta_seconds).day is not datetime.now().day):\n            eta_template = '%a, %d %b, %H:%M:%S'\n        else:\n            eta_template = '%H:%M:%S'\n        eta = time.strftime(eta_template, time.localtime(eta_seconds))\n        msg = ('Read %s of %s (%s%%). Rate %s/s, average %s/s, ETA %s.' % (self._size_fmt(downloaded_size), self._size_fmt(total_size), self._bold(str(int(((100 * downloaded_size) / total_size)))), self._size_fmt(current_rate, 2), self._size_fmt(overall_rate, 2), eta))\n        self._progress(msg)\n        previous_time = current_time\n        self.logger.debug(msg)\n    if out_file:\n        out_file.close()\n    if (glaciercorecalls.bytes_to_hex(glaciercorecalls.tree_hash(hash_list)) != download_job['SHA256TreeHash']):\n        raise CommunicationException('Downloaded data hash mismatch', code='DownloadError', cause=None)\n    self.logger.debug('Download of archive finished successfully.')\n    current_time = time.time()\n    overall_rate = int((downloaded_size / (current_time - start_time)))\n    msg = ('Wrote %s. Rate %s/s.\\n' % (self._size_fmt(downloaded_size), self._size_fmt(overall_rate, 2)))\n    self._progress(msg)\n    self.logger.info(msg)\n", "label": 1}
{"function": "\n\ndef init(name, cpu, mem, image, hyper=None, hypervisor='kvm', host=None, seed=True, nic='default', install=True, start=True, disk='default', saltenv='base', enable_vnc=False):\n    '\\n    This routine is used to create a new virtual machine. This routines takes\\n    a number of options to determine what the newly created virtual machine\\n    will look like.\\n\\n    name\\n        The mandatory name of the new virtual machine. The name option is\\n        also the minion id, all minions must have an id.\\n\\n    cpu\\n        The number of cpus to allocate to this new virtual machine.\\n\\n    mem\\n        The amount of memory to allocate tot his virtual machine. The number\\n        is interpreted in megabytes.\\n\\n    image\\n        The network location of the virtual machine image, commonly a location\\n        on the salt fileserver, but http, https and ftp can also be used.\\n\\n    hypervisor\\n        The hypervisor to use for the new virtual machine. Default is \\'kvm\\'.\\n\\n    host\\n        The host to use for the new virtual machine, if this is omitted\\n        Salt will automatically detect what host to use.\\n\\n    seed\\n        Set to False to prevent Salt from seeding the new virtual machine.\\n\\n    nic\\n        The nic profile to use, defaults to the \"default\" nic profile which\\n        assumes a single network interface per VM associated with the \"br0\"\\n        bridge on the master.\\n\\n    install\\n        Set to False to prevent Salt from installing a minion on the new VM\\n        before it spins up.\\n\\n    disk\\n        The disk profile to use\\n\\n    saltenv\\n        The Salt environment to use\\n    '\n    if (hyper is not None):\n        salt.utils.warn_until('Carbon', 'Please use \"host\" instead of \"hyper\". The \"hyper\" argument will be removed in the Carbon release of Salt')\n        host = hyper\n    __jid_event__.fire_event({\n        'message': 'Searching for hosts',\n    }, 'progress')\n    data = query(host, quiet=True)\n    for node in data:\n        if ('vm_info' in data[node]):\n            if (name in data[node]['vm_info']):\n                __jid_event__.fire_event({\n                    'message': 'Virtual machine {0} is already deployed'.format(name),\n                }, 'progress')\n                return 'fail'\n    if (host is None):\n        host = _determine_host(data)\n    if ((host not in data) or (not host)):\n        __jid_event__.fire_event({\n            'message': 'Host {0} was not found'.format(host),\n        }, 'progress')\n        return 'fail'\n    pub_key = None\n    priv_key = None\n    if seed:\n        __jid_event__.fire_event({\n            'message': 'Minion will be preseeded',\n        }, 'progress')\n        (priv_key, pub_key) = salt.utils.cloud.gen_keys()\n        accepted_key = os.path.join(__opts__['pki_dir'], 'minions', name)\n        with salt.utils.fopen(accepted_key, 'w') as fp_:\n            fp_.write(pub_key)\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    __jid_event__.fire_event({\n        'message': 'Creating VM {0} on host {1}'.format(name, host),\n    }, 'progress')\n    try:\n        cmd_ret = client.cmd_iter(host, 'virt.init', [name, cpu, mem, image, nic, hypervisor, start, disk, saltenv, seed, install, pub_key, priv_key, enable_vnc], timeout=600)\n    except SaltClientError as client_error:\n        print(client_error)\n    ret = next(cmd_ret)\n    if (not ret):\n        __jid_event__.fire_event({\n            'message': 'VM {0} was not initialized.'.format(name),\n        }, 'progress')\n        return 'fail'\n    for minion_id in ret:\n        if (ret[minion_id]['ret'] is False):\n            print('VM {0} initialization failed. Returned error: {1}'.format(name, ret[minion_id]['ret']))\n            return 'fail'\n    __jid_event__.fire_event({\n        'message': 'VM {0} initialized on host {1}'.format(name, host),\n    }, 'progress')\n    return 'good'\n", "label": 1}
{"function": "\n\ndef explore(self):\n    'INTERACTIVE exploration source capabilities\\n        \\n        Will use sitemap URI taken either from explicit self.sitemap_name\\n        or derived from the mappings supplied.\\n        '\n    starts = []\n    if (self.sitemap_name is not None):\n        print(('Starting from explicit --sitemap %s' % uri))\n        starts.append(XResource(uri))\n    elif (len(self.mapper) > 0):\n        uri = self.mapper.default_src_uri()\n        (scheme, netloc, path, params, query, fragment) = urlparse(uri)\n        if ((not scheme) and (not netloc)):\n            if os.path.isdir(path):\n                print(('Looking for capability documents in local directory %s' % path))\n                for name in ['resourcesync', 'capabilities.xml', 'resourcelist.xml', 'changelist.xml']:\n                    file = os.path.join(path, name)\n                    if os.path.isfile(file):\n                        starts.append(XResource(file))\n                if (len(starts) == 0):\n                    raise ClientFatalError(('No likely capability files found in local directory %s' % path))\n            else:\n                print(('Starting from local file %s' % path))\n                starts.append(XResource(path))\n        else:\n            well_known = urlunparse([scheme, netloc, '/.well-known/resourcesync', '', '', ''])\n            if (not path):\n                starts.append(XResource(well_known, ['capabilitylist', 'capabilitylistindex']))\n            else:\n                starts.append(XResource(uri))\n                starts.append(XResource(well_known, ['capabilitylist', 'capabilitylistindex']))\n            print('Looking for discovery information based on mappings')\n    else:\n        raise ClientFatalError('No source information (server base uri or capability uri) specified, use -h for help')\n    try:\n        for start in starts:\n            history = [start]\n            input = None\n            while (len(history) > 0):\n                print\n                xr = history.pop()\n                new_xr = self.explore_uri(xr, (len(history) > 0))\n                if new_xr:\n                    history.append(xr)\n                    history.append(new_xr)\n    except ExplorerQuit:\n        pass\n    print('\\nresync-explorer done, bye...\\n')\n", "label": 1}
{"function": "\n\ndef parse_gml(lines, relabel=True):\n    'Parse GML graph from a string or iterable.\\n\\n    Parameters\\n    ----------\\n    lines : string or iterable\\n       Data in GML format.\\n\\n    relabel : bool, optional       \\n       If True use the GML node label attribute for node names otherwise use\\n       the node id.\\n\\n    Returns\\n    -------\\n    G : MultiGraph or MultiDiGraph\\n\\n    Raises\\n    ------\\n    ImportError\\n        If the pyparsing module is not available.\\n\\n    See Also\\n    --------\\n    write_gml, read_gml\\n    \\n    Notes\\n    -----\\n    This stores nested GML attributes as dictionaries in the \\n    NetworkX graph, node, and edge attribute structures.\\n\\n    Requires pyparsing: http://pyparsing.wikispaces.com/\\n\\n    References\\n    ----------\\n    GML specification:\\n    http://www.infosun.fim.uni-passau.de/Graphlet/GML/gml-tr.html\\n    '\n    try:\n        from pyparsing import ParseException\n    except ImportError:\n        try:\n            from matplotlib.pyparsing import ParseException\n        except:\n            raise ImportError('Import Error: not able to import pyparsing:', 'http://pyparsing.wikispaces.com/')\n    try:\n        data = ''.join(lines)\n        gml = pyparse_gml()\n        tokens = gml.parseString(data)\n    except ParseException as err:\n        print(err.line)\n        print(((' ' * (err.column - 1)) + '^'))\n        print(err)\n        raise\n\n    def wrap(tok):\n        listtype = type(tok)\n        result = {\n            \n        }\n        for (k, v) in tok:\n            if (type(v) == listtype):\n                result[str(k)] = wrap(v)\n            else:\n                result[str(k)] = v\n        return result\n    multigraph = False\n    if (tokens.directed == 1):\n        G = nx.MultiDiGraph()\n    else:\n        G = nx.MultiGraph()\n    for (k, v) in tokens.asList():\n        if (k == 'node'):\n            vdict = wrap(v)\n            node = vdict['id']\n            G.add_node(node, attr_dict=vdict)\n        elif (k == 'edge'):\n            vdict = wrap(v)\n            source = vdict.pop('source')\n            target = vdict.pop('target')\n            if G.has_edge(source, target):\n                multigraph = True\n            G.add_edge(source, target, attr_dict=vdict)\n        else:\n            G.graph[k] = v\n    if (not multigraph):\n        if G.is_directed():\n            G = nx.DiGraph(G)\n        else:\n            G = nx.Graph(G)\n    if relabel:\n        mapping = [(n, d['label']) for (n, d) in G.node.items()]\n        (x, y) = zip(*mapping)\n        if (len(set(y)) != len(G)):\n            raise NetworkXError('Failed to relabel nodes: duplicate node labels found. Use relabel=False.')\n        G = nx.relabel_nodes(G, dict(mapping))\n    return G\n", "label": 1}
{"function": "\n\n@classmethod\ndef different_attributes(cls, old, new):\n    '\\n        Backwards-compat comparison that ignores orm. on the RHS and not the left\\n        and which knows django.db.models.fields.CharField = models.CharField.\\n        Has a whole load of tests in tests/autodetection.py.\\n        '\n    if ((not cls.is_triple(old)) or (not cls.is_triple(new))):\n        return (old != new)\n    (old_field, old_pos, old_kwd) = old\n    (new_field, new_pos, new_kwd) = new\n    (old_pos, new_pos) = (old_pos[:], new_pos[:])\n    old_kwd = dict(old_kwd.items())\n    new_kwd = dict(new_kwd.items())\n    if ('unique' in old_kwd):\n        del old_kwd['unique']\n    if ('unique' in new_kwd):\n        del new_kwd['unique']\n    if (old_field != new_field):\n        if (old_field.startswith('models.') and (new_field.startswith('django.db.models') or new_field.startswith('django.contrib.gis'))):\n            if (old_field.split('.')[(- 1)] != new_field.split('.')[(- 1)]):\n                return True\n            else:\n                old_field = new_field = ''\n    if ((old_pos and ('to' in new_kwd)) and (('orm' in new_kwd['to']) and ('orm' not in old_pos[0]))):\n        try:\n            if (old_pos[0] != new_kwd['to'].split(\"'\")[1].split('.')[1]):\n                return True\n        except IndexError:\n            pass\n        old_pos = old_pos[1:]\n        del new_kwd['to']\n    return ((old_field != new_field) or (old_pos != new_pos) or (old_kwd != new_kwd))\n", "label": 1}
{"function": "\n\ndef main(argv):\n    'Main program.\\n\\n  Arguments:\\n    argv: command-line arguments, such as sys.argv (including the program name\\n      in argv[0]).\\n\\n  Returns:\\n    0 if there were no changes, non-zero otherwise.\\n\\n  Raises:\\n    YapfError: if none of the supplied files were Python files.\\n  '\n    parser = argparse.ArgumentParser(description='Formatter for Python code.')\n    parser.add_argument('-v', '--version', action='store_true', help='show version number and exit')\n    diff_inplace_group = parser.add_mutually_exclusive_group()\n    diff_inplace_group.add_argument('-d', '--diff', action='store_true', help='print the diff for the fixed source')\n    diff_inplace_group.add_argument('-i', '--in-place', action='store_true', help='make changes to files in place')\n    lines_recursive_group = parser.add_mutually_exclusive_group()\n    lines_recursive_group.add_argument('-r', '--recursive', action='store_true', help='run recursively over directories')\n    lines_recursive_group.add_argument('-l', '--lines', metavar='START-END', action='append', default=None, help='range of lines to reformat, one-based')\n    parser.add_argument('-e', '--exclude', metavar='PATTERN', action='append', default=None, help='patterns for files to exclude from formatting')\n    parser.add_argument('--style', action='store', help=('specify formatting style: either a style name (for example \"pep8\" or \"google\"), or the name of a file with style settings. The default is pep8 unless a %s or %s file located in one of the parent directories of the source file (or current directory for stdin)' % (style.LOCAL_STYLE, style.SETUP_CONFIG)))\n    parser.add_argument('--style-help', action='store_true', help='show style settings and exit')\n    parser.add_argument('--no-local-style', action='store_true', help=\"don't search for local style definition\")\n    parser.add_argument('--verify', action='store_true', help='try to verify reformatted code for syntax errors')\n    parser.add_argument('files', nargs='*')\n    args = parser.parse_args(argv[1:])\n    if args.version:\n        print('yapf {}'.format(__version__))\n        return 0\n    if args.style_help:\n        style.SetGlobalStyle(style.CreateStyleFromConfig(args.style))\n        for (option, docstring) in sorted(style.Help().items()):\n            print(option, '=', style.Get(option), sep='')\n            for line in docstring.splitlines():\n                print('  ', line)\n            print()\n        return 0\n    if (args.lines and (len(args.files) > 1)):\n        parser.error('cannot use -l/--lines with more than one file')\n    lines = (_GetLines(args.lines) if (args.lines is not None) else None)\n    if (not args.files):\n        if (args.in_place or args.diff):\n            parser.error('cannot use --in-place or --diff flags when reading from stdin')\n        original_source = []\n        while True:\n            try:\n                original_source.append(py3compat.raw_input())\n            except EOFError:\n                break\n        style_config = args.style\n        if ((style_config is None) and (not args.no_local_style)):\n            style_config = file_resources.GetDefaultStyleForDir(os.getcwd())\n        (reformatted_source, changed) = yapf_api.FormatCode(py3compat.unicode(('\\n'.join(original_source) + '\\n')), filename='<stdin>', style_config=style_config, lines=lines, verify=args.verify)\n        sys.stdout.write(reformatted_source)\n        return (2 if changed else 0)\n    files = file_resources.GetCommandLineFiles(args.files, args.recursive, args.exclude)\n    if (not files):\n        raise errors.YapfError('Input filenames did not match any python files')\n    changed = FormatFiles(files, lines, style_config=args.style, no_local_style=args.no_local_style, in_place=args.in_place, print_diff=args.diff, verify=args.verify)\n    return (2 if changed else 0)\n", "label": 1}
{"function": "\n\ndef _do_listen(self, addr, family, backlog, slave):\n    '\\n        A callback method to be used with\\n        :meth:`~pants._channel._Channel._resolve_addr` - either listens\\n        immediately or notifies the user of an error.\\n\\n        =========  =====================================================\\n        Argument   Description\\n        =========  =====================================================\\n        backlog    The maximum size of the connection queue.\\n        slave      If True, this will cause a Server listening on\\n                   IPv6 INADDR_ANY to create a slave Server that\\n                   listens on the IPv4 INADDR_ANY.\\n        addr       The address to listen on or None if address\\n                   resolution failed.\\n        family     The detected socket family or None if address\\n                   resolution failed.\\n        error      *Optional.* Error information or None if no error\\n                   occurred.\\n        =========  =====================================================\\n        '\n    if self._socket:\n        if (self._socket.family != family):\n            self.engine.remove_channel(self)\n            self._socket_close()\n            self._closed = False\n    sock = socket.socket(family, socket.SOCK_STREAM)\n    self._socket_set(sock)\n    self.engine.add_channel(self)\n    try:\n        self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n    except AttributeError:\n        pass\n    if (hasattr(socket, 'IPPROTO_IPV6') and hasattr(socket, 'IPV6_V6ONLY') and (family == socket.AF_INET6)):\n        self._socket.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)\n        slave = False\n    try:\n        self._socket_bind(addr)\n        self._socket_listen(backlog)\n    except socket.error as err:\n        self.close()\n        raise\n    self.listening = True\n    self._safely_call(self.on_listen)\n    if (slave and (not isinstance(addr, str)) and (addr[0] == '') and HAS_IPV6):\n        try:\n            self._slave = _SlaveServer(self.engine, self, addr, backlog)\n        except Exception:\n            self._slave = None\n", "label": 1}
{"function": "\n\ndef get_filters(self, request):\n    lookup_params = self.get_filters_params()\n    use_distinct = False\n    for (key, value) in lookup_params.items():\n        if (not self.model_admin.lookup_allowed(key, value)):\n            raise DisallowedModelAdminLookup(('Filtering by %s not allowed' % key))\n    filter_specs = []\n    if self.list_filter:\n        for list_filter in self.list_filter:\n            if callable(list_filter):\n                spec = list_filter(request, lookup_params, self.model, self.model_admin)\n            else:\n                field_path = None\n                if isinstance(list_filter, (tuple, list)):\n                    (field, field_list_filter_class) = list_filter\n                else:\n                    (field, field_list_filter_class) = (list_filter, FieldListFilter.create)\n                if (not isinstance(field, models.Field)):\n                    field_path = field\n                    field = get_fields_from_path(self.model, field_path)[(- 1)]\n                spec = field_list_filter_class(field, request, lookup_params, self.model, self.model_admin, field_path=field_path)\n                use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, field_path))\n            if (spec and spec.has_output()):\n                filter_specs.append(spec)\n    try:\n        for (key, value) in lookup_params.items():\n            lookup_params[key] = prepare_lookup_value(key, value)\n            use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, key))\n        return (filter_specs, bool(filter_specs), lookup_params, use_distinct)\n    except FieldDoesNotExist as e:\n        six.reraise(IncorrectLookupParameters, IncorrectLookupParameters(e), sys.exc_info()[2])\n", "label": 1}
{"function": "\n\ndef main():\n    from suite import benchmarks\n    if (not args.log_file):\n        args.log_file = os.path.abspath(os.path.join(REPO_PATH, 'vb_suite.log'))\n    saved_dir = os.path.curdir\n    if args.outdf:\n        args.outdf = os.path.realpath(args.outdf)\n    if args.log_file:\n        args.log_file = os.path.realpath(args.log_file)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    if (args.base_pickle and args.target_pickle):\n        baseline_res = prep_pickle_for_total(pd.load(args.base_pickle))\n        target_res = prep_pickle_for_total(pd.load(args.target_pickle))\n        report_comparative(target_res, baseline_res)\n        sys.exit(0)\n    if (args.affinity is not None):\n        try:\n            import psutil\n            if hasattr(psutil.Process, 'set_cpu_affinity'):\n                psutil.Process(os.getpid()).set_cpu_affinity([args.affinity])\n                print(('CPU affinity set to %d' % args.affinity))\n        except ImportError:\n            print(\"-a/--affinity specified, but the 'psutil' module is not available, aborting.\\n\")\n            sys.exit(1)\n    print('\\n')\n    prprint(('LOG_FILE = %s' % args.log_file))\n    if args.outdf:\n        prprint(('PICKE_FILE = %s' % args.outdf))\n    print('\\n')\n    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n    benchmarks = [x for x in benchmarks if re.search(args.regex, x.name)]\n    for b in benchmarks:\n        b.repeat = args.repeats\n        if args.ncalls:\n            b.ncalls = args.ncalls\n    if benchmarks:\n        if args.head:\n            profile_head(benchmarks)\n        else:\n            profile_comparative(benchmarks)\n    else:\n        print('No matching benchmarks')\n    os.chdir(saved_dir)\n", "label": 1}
{"function": "\n\ndef __init__(self, app, gcfg=None, host='127.0.0.1', port=None, *args, **kwargs):\n    self.cfg = Config()\n    self.gcfg = gcfg\n    self.app = app\n    self.callable = None\n    gcfg = (gcfg or {\n        \n    })\n    cfgfname = gcfg.get('__file__')\n    if (cfgfname is not None):\n        self.cfgurl = ('config:%s' % cfgfname)\n        self.relpath = os.path.dirname(cfgfname)\n        self.cfgfname = cfgfname\n    cfg = kwargs.copy()\n    if (port and (not host.startswith('unix:'))):\n        bind = ('%s:%s' % (host, port))\n    else:\n        bind = host\n    cfg['bind'] = bind.split(',')\n    if gcfg:\n        for (k, v) in gcfg.items():\n            cfg[k] = v\n        cfg['default_proc_name'] = cfg['__file__']\n    try:\n        for (k, v) in cfg.items():\n            if ((k.lower() in self.cfg.settings) and (v is not None)):\n                self.cfg.set(k.lower(), v)\n    except Exception as e:\n        print(('\\nConfig error: %s' % str(e)), file=sys.stderr)\n        sys.stderr.flush()\n        sys.exit(1)\n    if cfg.get('config'):\n        self.load_config_from_file(cfg['config'])\n    else:\n        default_config = get_default_config_file()\n        if (default_config is not None):\n            self.load_config_from_file(default_config)\n", "label": 1}
{"function": "\n\ndef test_update_crossing_duration1_not_duration_not_at_1st_step(self, duration1, duration2):\n    global rec\n    if ((duration1 == 0.0) or (duration2 == 0.0)):\n        return\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    composite = ac.sequence(a1, a2)\n    rec = []\n    node.do(composite)\n    elapsed = 0.0\n    next_elapsed = (duration1 / 2.0)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    elapsed = next_elapsed\n    next_elapsed = ((duration1 + duration2) / 2.0)\n    dt = (next_elapsed - elapsed)\n    rec = []\n    node._step(dt)\n    recx = [e for e in rec if (e[1] != 'step')]\n    rec = [e for e in recx if (e[0] == name1)]\n    print('rec', rec)\n    assert ((rec[0][1] == 'update') and (rec[0][2] == 1.0))\n    assert (rec[1][1] == 'stop')\n    assert (len(rec) == 2)\n    rec = [e for e in recx if (e[0] == name2)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert (rec[1][1] == 'update')\n    assert (abs((rec[1][2] - ((next_elapsed - duration1) / duration2))) < fe)\n    assert (len(rec) == 2)\n", "label": 1}
{"function": "\n\n@nottest\ndef test_models():\n    optimize = False\n    plot = True\n    examples_path = os.path.dirname(GPy.examples.__file__)\n    failing_models = {\n        \n    }\n    for (loader, module_name, is_pkg) in pkgutil.iter_modules([examples_path]):\n        module_examples = loader.find_module(module_name).load_module(module_name)\n        print('MODULE', module_examples)\n        print('Before')\n        print(inspect.getmembers(module_examples, predicate=inspect.isfunction))\n        functions = [func for func in inspect.getmembers(module_examples, predicate=inspect.isfunction) if (func[0].startswith('_') is False)][::(- 1)]\n        print('After')\n        print(functions)\n        for example in functions:\n            if (example[0] in ['epomeo_gpx']):\n                if ((example[0] == 'epomeo_gpx') and (not GPy.util.datasets.gpxpy_available)):\n                    print('Skipping as gpxpy is not available to parse GPS')\n                    continue\n            print('Testing example: ', example[0])\n            try:\n                models = [example[1](optimize=optimize, plot=plot)]\n                models = flatten_nested(models)\n            except Exception as e:\n                failing_models[example[0]] = 'Cannot make model: \\n{e}'.format(e=e)\n            else:\n                print(models)\n                model_checkgrads.description = ('test_checkgrads_%s' % example[0])\n                try:\n                    for model in models:\n                        if (not model_checkgrads(model)):\n                            failing_models[model_checkgrads.description] = False\n                except Exception as e:\n                    failing_models[model_checkgrads.description] = e\n                model_instance.description = ('test_instance_%s' % example[0])\n                try:\n                    for model in models:\n                        if (not model_instance(model)):\n                            failing_models[model_instance.description] = False\n                except Exception as e:\n                    failing_models[model_instance.description] = e\n        print('Finished checking module {m}'.format(m=module_name))\n        if (len(failing_models.keys()) > 0):\n            print('Failing models: ')\n            print(failing_models)\n    if (len(failing_models.keys()) > 0):\n        print(failing_models)\n        raise Exception(failing_models)\n", "label": 1}
{"function": "\n\ndef show(path_info, example_name):\n    fn = resource_filename((example_name + '.html'))\n    out = StringIO()\n    assert (default_app is not None), 'No default_app set'\n    url = (default_url + path_info)\n    out.write(('<span class=\"doctest-url\"><a href=\"%s\">%s</a></span><br>\\n' % (url, url)))\n    out.write('<div class=\"doctest-example\">\\n')\n    proc = subprocess.Popen(['paster', 'serve--server=console', '--no-verbose', ('--url=' + path_info)], stderr=subprocess.PIPE, stdout=subprocess.PIPE, env=_make_env())\n    (stdout, errors) = proc.communicate()\n    stdout = StringIO(stdout)\n    headers = rfc822.Message(stdout)\n    content = stdout.read()\n    for (header, value) in headers.items():\n        if ((header.lower() == 'status') and (int(value.split()[0]) == 200)):\n            continue\n        if (header.lower() in ('content-type', 'content-length')):\n            continue\n        if ((header.lower() == 'set-cookie') and value.startswith('_SID_')):\n            continue\n        out.write(('<span class=\"doctest-header\">%s: %s</span><br>\\n' % (header, value)))\n    lines = [l for l in content.splitlines() if l.strip()]\n    for line in lines:\n        out.write((line + '\\n'))\n    if errors:\n        out.write(('<pre class=\"doctest-errors\">%s</pre>' % errors))\n    out.write('</div>\\n')\n    result = out.getvalue()\n    if (not os.path.exists(fn)):\n        f = open(fn, 'wb')\n        f.write(result)\n        f.close()\n    else:\n        f = open(fn, 'rb')\n        expected = f.read()\n        f.close()\n        if (not html_matches(expected, result)):\n            print(('Pages did not match.  Expected from %s:' % fn))\n            print(('-' * 60))\n            print(expected)\n            print(('=' * 60))\n            print('Actual output:')\n            print(('-' * 60))\n            print(result)\n", "label": 1}
{"function": "\n\ndef test_episode_fromname(episodes):\n    video = Episode.fromname(episodes['bbt_s07e05'].name)\n    assert (video.name == episodes['bbt_s07e05'].name)\n    assert (video.format == episodes['bbt_s07e05'].format)\n    assert (video.release_group == episodes['bbt_s07e05'].release_group)\n    assert (video.resolution == episodes['bbt_s07e05'].resolution)\n    assert (video.video_codec == episodes['bbt_s07e05'].video_codec)\n    assert (video.audio_codec is None)\n    assert (video.imdb_id is None)\n    assert (video.hashes == {\n        \n    })\n    assert (video.size is None)\n    assert (video.subtitle_languages == set())\n    assert (video.series == episodes['bbt_s07e05'].series)\n    assert (video.season == episodes['bbt_s07e05'].season)\n    assert (video.episode == episodes['bbt_s07e05'].episode)\n    assert (video.title is None)\n    assert (video.year is None)\n    assert (video.tvdb_id is None)\n", "label": 1}
{"function": "\n\ndef addtoken(self, type, value, context):\n    'Add a token; return True iff this is the end of the program.'\n    ilabel = self.classify(type, value, context)\n    while True:\n        (dfa, state, node) = self.stack[(- 1)]\n        (states, first) = dfa\n        arcs = states[state]\n        for (i, newstate) in arcs:\n            (t, v) = self.grammar.labels[i]\n            if (ilabel == i):\n                assert (t < 256)\n                self.shift(type, value, newstate, context)\n                state = newstate\n                while (states[state] == [(0, state)]):\n                    self.pop()\n                    if (not self.stack):\n                        return True\n                    (dfa, state, node) = self.stack[(- 1)]\n                    (states, first) = dfa\n                return False\n            elif (t >= 256):\n                itsdfa = self.grammar.dfas[t]\n                (itsstates, itsfirst) = itsdfa\n                if (ilabel in itsfirst):\n                    self.push(t, self.grammar.dfas[t], newstate, context)\n                    break\n        else:\n            if ((0, state) in arcs):\n                self.pop()\n                if (not self.stack):\n                    raise ParseError('too much input', type, value, context)\n            else:\n                raise ParseError('bad input', type, value, context)\n", "label": 1}
{"function": "\n\ndef Key_Stats(gather='Total Debt/Equity (mrq)'):\n    statspath = (path + '/_KeyStats')\n    stock_list = [x[0] for x in os.walk(statspath)]\n    df = pd.DataFrame(columns=['Date', 'Unix', 'Ticker', 'DE Ratio', 'Price', 'stock_p_change', 'SP500', 'sp500_p_change', 'Difference'])\n    sp500_df = pd.DataFrame.from_csv('YAHOO-INDEX_GSPC.csv')\n    ticker_list = []\n    for each_dir in stock_list[1:25]:\n        each_file = os.listdir(each_dir)\n        ticker = os.path.basename(os.path.normpath(each_dir))\n        ticker_list.append(ticker)\n        starting_stock_value = False\n        starting_sp500_value = False\n        if (len(each_file) > 0):\n            for file in each_file:\n                date_stamp = datetime.strptime(file, '%Y%m%d%H%M%S.html')\n                unix_time = time.mktime(date_stamp.timetuple())\n                full_file_path = ((each_dir + '/') + file)\n                source = open(full_file_path, 'r').read()\n                try:\n                    try:\n                        value = float(source.split((gather + ':</td><td class=\"yfnc_tabledata1\">'))[1].split('</td>')[0])\n                    except Exception as e:\n                        try:\n                            value = float(source.split((gather + ':</td>\\n<td class=\"yfnc_tabledata1\">'))[1].split('</td>')[0])\n                        except Exception as e:\n                            pass\n                    try:\n                        sp500_date = datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d')\n                        row = sp500_df[(sp500_df.index == sp500_date)]\n                        sp500_value = float(row['Adj Close'])\n                    except:\n                        sp500_date = datetime.fromtimestamp((unix_time - 259200)).strftime('%Y-%m-%d')\n                        row = sp500_df[(sp500_df.index == sp500_date)]\n                        sp500_value = float(row['Adj Close'])\n                    try:\n                        stock_price = float(source.split('</small><big><b>')[1].split('</b></big>')[0])\n                    except Exception as e:\n                        try:\n                            stock_price = source.split('</small><big><b>')[1].split('</b></big>')[0]\n                            stock_price = re.search('(\\\\d{1,8}\\\\.\\\\d{1,8})', stock_price)\n                            stock_price = float(stock_price.group(1))\n                        except Exception as e:\n                            try:\n                                stock_price = source.split('<span class=\"time_rtq_ticker\">')[1].split('</span>')[0]\n                                stock_price = re.search('(\\\\d{1,8}\\\\.\\\\d{1,8})', stock_price)\n                                stock_price = float(stock_price.group(1))\n                            except Exception as e:\n                                print(str(e), 'a;lsdkfh', file, ticker)\n                    if (not starting_stock_value):\n                        starting_stock_value = stock_price\n                    if (not starting_sp500_value):\n                        starting_sp500_value = sp500_value\n                    stock_p_change = (((stock_price - starting_stock_value) / starting_stock_value) * 100)\n                    sp500_p_change = (((sp500_value - starting_sp500_value) / starting_sp500_value) * 100)\n                    df = df.append({\n                        'Date': date_stamp,\n                        'Unix': unix_time,\n                        'Ticker': ticker,\n                        'DE Ratio': value,\n                        'Price': stock_price,\n                        'stock_p_change': stock_p_change,\n                        'SP500': sp500_value,\n                        'sp500_p_change': sp500_p_change,\n                        'Difference': (stock_p_change - sp500_p_change),\n                    }, ignore_index=True)\n                except Exception as e:\n                    pass\n    for each_ticker in ticker_list:\n        try:\n            plot_df = df[(df['Ticker'] == each_ticker)]\n            plot_df = plot_df.set_index(['Date'])\n            plot_df['Difference'].plot(label=each_ticker)\n            plt.legend()\n        except:\n            pass\n    plt.show()\n    save = (gather.replace(' ', '').replace(')', '').replace('(', '').replace('/', '') + '.csv')\n    print(save)\n    df.to_csv(save)\n", "label": 1}
{"function": "\n\n@csrf_exempt\ndef dispatch(self, request, method='', json_encoder=None):\n    from django.http import HttpResponse\n    json_encoder = (json_encoder or self.json_encoder)\n    try:\n        response = self.empty_response()\n        if (request.method.lower() == 'get'):\n            (valid, D) = self.validate_get(request, method)\n            if (not valid):\n                raise InvalidRequestError('The method you are trying to access is not available by GET requests')\n        elif (not (request.method.lower() == 'post')):\n            raise RequestPostError\n        else:\n            try:\n                if hasattr(request, 'body'):\n                    D = loads(request.body.decode('utf-8'))\n                else:\n                    D = loads(request.raw_post_data.decode('utf-8'))\n            except:\n                raise InvalidRequestError\n        if (type(D) is list):\n            response = [self.response_dict(request, d, is_batch=True, json_encoder=json_encoder)[0] for d in D]\n            status = 200\n        else:\n            (response, status) = self.response_dict(request, D, json_encoder=json_encoder)\n            if ((response is None) and ((not ('id' in D)) or (D['id'] is None))):\n                return HttpResponse('', status=status)\n        json_rpc = dumps(response, cls=json_encoder)\n    except Error as e:\n        response['error'] = e.json_rpc_format\n        status = e.status\n        json_rpc = dumps(response, cls=json_encoder)\n    except Exception as e:\n        signals.got_request_exception.send(sender=self.__class__, request=request)\n        if settings.DEBUG:\n            other_error = OtherError(e)\n        else:\n            other_error = OtherError('Internal Server Error')\n        response['result'] = None\n        response['error'] = other_error.json_rpc_format\n        status = other_error.status\n        json_rpc = dumps(response, cls=json_encoder)\n    return HttpResponse(json_rpc, status=status, content_type='application/json-rpc')\n", "label": 1}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01):\n    ''\n    N = int(N)\n    assert (N > 0)\n    samplerate = self.audio.samplerate\n    assert (1 <= freq_base <= freq_max <= (samplerate / 2.0))\n    step = 1024\n    win = step\n    assert (0 < step <= win)\n    coeffies = self.make_erb_filter_coeffiences(samplerate, N, freq_base, freq_max)\n    zi = None\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for samples in self.audio.walk(win, step, start, end, join_channels=True):\n        (y, zi) = self.filter(samples, coeffies, zi)\n        if (not combine):\n            if each:\n                for frame in y:\n                    (yield frame)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_2d_time():\n    ' reading/writing of 2D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_2d', 'time_2d.sec'))\n    assert (data.shape == (332, 1500))\n    assert (np.abs((data[(0, 1)].real - 360.07)) <= 0.01)\n    assert (np.abs((data[(0, 1)].imag - (- 223.2))) <= 0.01)\n    assert (np.abs((data[(10, 18)].real - 17.93)) <= 0.01)\n    assert (np.abs((data[(10, 18)].imag - (- 67.2))) <= 0.01)\n    assert (dic['sw'][1] == 50000.0)\n    assert (dic['sf'][1] == 125.69)\n    assert (dic['ppm'][1] == 55.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 50.65)\n    assert (dic['ppm'][0] == 120.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse(wmsg):\n    '\\n        Verifies and parses an unserialized raw message into an actual WAMP message instance.\\n\\n        :param wmsg: The unserialized raw message.\\n        :type wmsg: list\\n\\n        :returns: An instance of this class.\\n        '\n    assert ((len(wmsg) > 0) and (wmsg[0] == Hello.MESSAGE_TYPE))\n    if (len(wmsg) != 3):\n        raise ProtocolError('invalid message length {0} for HELLO'.format(len(wmsg)))\n    realm = check_or_raise_uri(wmsg[1], \"'realm' in HELLO\", allow_none=True)\n    details = check_or_raise_extra(wmsg[2], \"'details' in HELLO\")\n    roles = {\n        \n    }\n    if ('roles' not in details):\n        raise ProtocolError('missing mandatory roles attribute in options in HELLO')\n    details_roles = check_or_raise_extra(details['roles'], \"'roles' in 'details' in HELLO\")\n    if (len(details_roles) == 0):\n        raise ProtocolError(\"empty 'roles' in 'details' in HELLO\")\n    for role in details_roles:\n        if (role not in ['subscriber', 'publisher', 'caller', 'callee']):\n            raise ProtocolError(\"invalid role '{0}' in 'roles' in 'details' in HELLO\".format(role))\n        role_cls = ROLE_NAME_TO_CLASS[role]\n        details_role = check_or_raise_extra(details_roles[role], \"role '{0}' in 'roles' in 'details' in HELLO\".format(role))\n        if ('features' in details_role):\n            check_or_raise_extra(details_role['features'], \"'features' in role '{0}' in 'roles' in 'details' in HELLO\".format(role))\n            role_features = role_cls(**details_role['features'])\n        else:\n            role_features = role_cls()\n        roles[role] = role_features\n    authmethods = None\n    if ('authmethods' in details):\n        details_authmethods = details['authmethods']\n        if (type(details_authmethods) != list):\n            raise ProtocolError(\"invalid type {0} for 'authmethods' detail in HELLO\".format(type(details_authmethods)))\n        for auth_method in details_authmethods:\n            if (type(auth_method) != six.text_type):\n                raise ProtocolError(\"invalid type {0} for item in 'authmethods' detail in HELLO\".format(type(auth_method)))\n        authmethods = details_authmethods\n    authid = None\n    if ('authid' in details):\n        details_authid = details['authid']\n        if (type(details_authid) != six.text_type):\n            raise ProtocolError(\"invalid type {0} for 'authid' detail in HELLO\".format(type(details_authid)))\n        authid = details_authid\n    authrole = None\n    if ('authrole' in details):\n        details_authrole = details['authrole']\n        if (type(details_authrole) != six.text_type):\n            raise ProtocolError(\"invalid type {0} for 'authrole' detail in HELLO\".format(type(details_authrole)))\n        authrole = details_authrole\n    authextra = None\n    if ('authextra' in details):\n        details_authextra = details['authextra']\n        if (type(details_authextra) != dict):\n            raise ProtocolError(\"invalid type {0} for 'authextra' detail in HELLO\".format(type(details_authextra)))\n        authextra = details_authextra\n    obj = Hello(realm, roles, authmethods, authid, authrole, authextra)\n    return obj\n", "label": 1}
{"function": "\n\ndef batch_sender(self, my_queue, stream_name, send_interval, max_batch_size, max_batch_count):\n    msg = None\n\n    def size(msg):\n        return (len(msg['message']) + 26)\n    while (msg != self.END):\n        cur_batch = ([] if (msg is None) else [msg])\n        cur_batch_size = sum((size(msg) for msg in cur_batch))\n        cur_batch_msg_count = len(cur_batch)\n        cur_batch_deadline = (time.time() + send_interval)\n        while True:\n            try:\n                msg = my_queue.get(block=True, timeout=max(0, (cur_batch_deadline - time.time())))\n            except Queue.Empty:\n                msg = None\n            if ((msg is None) or (msg == self.END) or ((cur_batch_size + size(msg)) > max_batch_size) or (cur_batch_msg_count >= max_batch_count) or (time.time() >= cur_batch_deadline)):\n                self._submit_batch(cur_batch, stream_name)\n                if (msg is not None):\n                    my_queue.task_done()\n                break\n            elif msg:\n                cur_batch_size += size(msg)\n                cur_batch_msg_count += 1\n                cur_batch.append(msg)\n                my_queue.task_done()\n", "label": 1}
{"function": "\n\ndef patch_list(self, request, **kwargs):\n    '\\n        Since there is no RESTful way to do what we want to do, and since\\n        ``PATCH`` is poorly defined with regards to RESTfulness, we are\\n        overloading ``PATCH`` to take a single request that performs\\n        combinatorics and creates multiple objects.\\n        '\n    import itertools\n    from django.db import transaction\n    from tastypie.utils import dict_strip_unicode_keys\n    deserialized = self.deserialize(request, request.body, format=request.META.get('CONTENT_TYPE', 'application/json'))\n    categories = deserialized.pop('categories', [])\n    if ((not categories) or (not isinstance(categories, list))):\n        error_msg = 'PATCH request must contain categories list.'\n        logger.error(error_msg)\n        raise ImmediateHttpResponse(response=http.HttpBadRequest(error_msg))\n    elem_lists = []\n    for cat in categories:\n        if isinstance(cat, basestring):\n            cat = Category.objects.filter(id=self._id_from_uri(cat))\n            elem_list = Element.objects.filter(category=cat)\n        elif isinstance(cat, dict):\n            category = Category.objects.filter(id=self._id_from_uri(cat['category']))\n            elem_list = Element.objects.filter(category=category)\n            if ('exclude' in cat):\n                exclude_uris = cat['exclude']\n                exclude_ids = [int(self._id_from_uri(x)) for x in exclude_uris]\n                elem_list = [elem for elem in elem_list if (elem.id not in exclude_ids)]\n            elif ('include' in cat):\n                include_uris = cat['include']\n                include_ids = [int(self._id_from_uri(x)) for x in include_uris]\n                elem_list = [elem for elem in elem_list if (elem.id in include_ids)]\n            else:\n                pass\n        else:\n            error_msg = 'categories list must contain resource uris or hashes.'\n            logger.error(error_msg)\n            raise ImmediateHttpResponse(response=http.HttpBadRequest(error_msg))\n        elem_lists.append(elem_list)\n    combinatorics = itertools.product(*elem_lists)\n    with transaction.commit_on_success():\n        for combo in combinatorics:\n            deserialized['elements'] = combo\n            bundle = self.build_bundle(data=dict_strip_unicode_keys(deserialized))\n            bundle.request.META['REQUEST_METHOD'] = 'PATCH'\n            if getattr(request, 'user', None):\n                bundle.request.user = request.user\n            self.is_valid(bundle)\n            self.obj_create(bundle, request=request)\n    return http.HttpAccepted()\n", "label": 1}
{"function": "\n\ndef downloadManga(self):\n    print('Parsing XML File...')\n    if self.verbose_FLAG:\n        print(('XML Path = %s' % self.xmlfile_path))\n    dom = minidom.parse(self.xmlfile_path)\n    threadPool = []\n    self.options.auto = True\n    SetOutputPathToName_Flag = False\n    if (self.options.outputDir == 'DEFAULT_VALUE'):\n        SetOutputPathToName_Flag = True\n    for node in dom.getElementsByTagName('MangaSeries'):\n        seriesOptions = copy.copy(self.options)\n        seriesOptions.manga = getText(node.getElementsByTagName('name')[0])\n        seriesOptions.site = getText(node.getElementsByTagName('HostSite')[0])\n        try:\n            lastDownloaded = getText(node.getElementsByTagName('LastChapterDownloaded')[0])\n        except IndexError:\n            lastDownloaded = ''\n        try:\n            download_path = getText(node.getElementsByTagName('downloadPath')[0])\n        except IndexError:\n            download_path = ('./' + fixFormatting(seriesOptions.manga, seriesOptions.spaceToken))\n        if ((self.options.downloadPath != 'DEFAULT_VALUE') and (not os.path.isabs(download_path))):\n            download_path = os.path.join(self.options.downloadPath, download_path)\n        seriesOptions.downloadPath = download_path\n        seriesOptions.lastDownloaded = lastDownloaded\n        if SetOutputPathToName_Flag:\n            seriesOptions.outputDir = download_path\n        threadPool.append(SiteParserThread(seriesOptions, dom, node))\n    for thread in threadPool:\n        thread.start()\n        thread.join()\n    backupFileName = (self.xmlfile_path + '_bak')\n    os.rename(self.xmlfile_path, backupFileName)\n    f = open(self.xmlfile_path, 'w')\n    outputStr = '\\n'.join([line for line in dom.toprettyxml().split('\\n') if line.strip()])\n    outputStr = outputStr.encode('utf-8')\n    f.write(outputStr)\n    os.remove(backupFileName)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _from_blocks(cls, block_iter, is_binary, sep, endcard, padding):\n    '\\n        The meat of `Header.fromfile`; in a separate method so that\\n        `Header.fromfile` itself is just responsible for wrapping file\\n        handling.  Also used by `_BaseHDU.fromstring`.\\n\\n        ``block_iter`` should be a callable which, given a block size n\\n        (typically 2880 bytes as used by the FITS standard) returns an iterator\\n        of byte strings of that block size.\\n\\n        ``is_binary`` specifies whether the returned blocks are bytes or text\\n\\n        Returns both the entire header *string*, and the `Header` object\\n        returned by Header.fromstring on that string.\\n        '\n    actual_block_size = _block_size(sep)\n    clen = (Card.length + len(sep))\n    blocks = block_iter(actual_block_size)\n    try:\n        block = next(blocks)\n    except StopIteration:\n        raise EOFError()\n    if (not is_binary):\n        block = encode_ascii(block)\n    read_blocks = []\n    is_eof = False\n    end_found = False\n    while True:\n        (end_found, block) = cls._find_end_card(block, clen)\n        read_blocks.append(decode_ascii(block))\n        if end_found:\n            break\n        try:\n            block = next(blocks)\n        except StopIteration:\n            is_eof = True\n            break\n        if (not block):\n            is_eof = True\n            break\n        if (not is_binary):\n            block = encode_ascii(block)\n    if ((not end_found) and is_eof and endcard):\n        raise IOError('Header missing END card.')\n    header_str = ''.join(read_blocks)\n    if (header_str and (header_str[(- 1)] == '\\x00')):\n        if (is_eof and (header_str.strip('\\x00') == '')):\n            warnings.warn('Unexpected extra padding at the end of the file.  This padding may not be preserved when saving changes.')\n            raise EOFError()\n        else:\n            warnings.warn('Header block contains null bytes instead of spaces for padding, and is not FITS-compliant. Nulls may be replaced with spaces upon writing.')\n            header_str.replace('\\x00', ' ')\n    if (padding and ((len(header_str) % actual_block_size) != 0)):\n        actual_len = ((len(header_str) - actual_block_size) + BLOCK_SIZE)\n        raise ValueError(('Header size is not multiple of %d: %d' % (BLOCK_SIZE, actual_len)))\n    return (header_str, cls.fromstring(header_str, sep=sep))\n", "label": 1}
{"function": "\n\ndef _colordiff(self, a, b, highlight='red', minor_highlight='lightgray'):\n    'Given two values, return the same pair of strings except with\\n        their differences highlighted in the specified color. Strings are\\n        highlighted intelligently to show differences; other values are\\n        stringified and highlighted in their entirety.\\n        '\n    if ((not isinstance(a, basestring)) or (not isinstance(b, basestring))):\n        a = unicode(a)\n        b = unicode(b)\n        if (a == b):\n            return (a, b)\n        else:\n            return (self.colorize(highlight, a), self.colorize(highlight, b))\n    if (isinstance(a, bytes) or isinstance(b, bytes)):\n        a = self.displayable_path(a)\n        b = self.displayable_path(b)\n    a_out = []\n    b_out = []\n    matcher = SequenceMatcher((lambda x: False), a, b)\n    for (op, a_start, a_end, b_start, b_end) in matcher.get_opcodes():\n        if (op == 'equal'):\n            a_out.append(a[a_start:a_end])\n            b_out.append(b[b_start:b_end])\n        elif (op == 'insert'):\n            b_out.append(self.colorize(highlight, b[b_start:b_end]))\n        elif (op == 'delete'):\n            a_out.append(self.colorize(highlight, a[a_start:a_end]))\n        elif (op == 'replace'):\n            if (a[a_start:a_end].lower() != b[b_start:b_end].lower()):\n                color = highlight\n            else:\n                color = minor_highlight\n            a_out.append(self.colorize(color, a[a_start:a_end]))\n            b_out.append(self.colorize(color, b[b_start:b_end]))\n        else:\n            assert False\n    return (''.join(a_out), ''.join(b_out))\n", "label": 1}
{"function": "\n\ndef process_urlencoded(entity):\n    'Read application/x-www-form-urlencoded data into entity.params.'\n    qs = entity.fp.read()\n    for charset in entity.attempt_charsets:\n        try:\n            params = {\n                \n            }\n            for aparam in qs.split(ntob('&')):\n                for pair in aparam.split(ntob(';')):\n                    if (not pair):\n                        continue\n                    atoms = pair.split(ntob('='), 1)\n                    if (len(atoms) == 1):\n                        atoms.append(ntob(''))\n                    key = unquote_plus(atoms[0]).decode(charset)\n                    value = unquote_plus(atoms[1]).decode(charset)\n                    if (key in params):\n                        if (not isinstance(params[key], list)):\n                            params[key] = [params[key]]\n                        params[key].append(value)\n                    else:\n                        params[key] = value\n        except UnicodeDecodeError:\n            pass\n        else:\n            entity.charset = charset\n            break\n    else:\n        raise cherrypy.HTTPError(400, ('The request entity could not be decoded. The following charsets were attempted: %s' % repr(entity.attempt_charsets)))\n    for (key, value) in params.items():\n        if (key in entity.params):\n            if (not isinstance(entity.params[key], list)):\n                entity.params[key] = [entity.params[key]]\n            entity.params[key].append(value)\n        else:\n            entity.params[key] = value\n", "label": 1}
{"function": "\n\ndef unknown_endtag(self, tag):\n    if (tag.find(':') != (- 1)):\n        (prefix, suffix) = tag.split(':', 1)\n    else:\n        (prefix, suffix) = ('', tag)\n    prefix = self.namespacemap.get(prefix, prefix)\n    if prefix:\n        prefix = (prefix + '_')\n    if ((suffix == 'svg') and self.svgOK):\n        self.svgOK -= 1\n    methodname = (('_end_' + prefix) + suffix)\n    try:\n        if self.svgOK:\n            raise AttributeError()\n        method = getattr(self, methodname)\n        method()\n    except AttributeError:\n        self.pop((prefix + suffix))\n    if (self.incontent and (not self.contentparams.get('type', 'xml').endswith('xml'))):\n        if (tag in ('xhtml:div', 'div')):\n            return\n        self.contentparams['type'] = 'application/xhtml+xml'\n    if (self.incontent and (self.contentparams.get('type') == 'application/xhtml+xml')):\n        tag = tag.split(':')[(- 1)]\n        self.handle_data(('</%s>' % tag), escape=0)\n    if self.basestack:\n        self.basestack.pop()\n        if (self.basestack and self.basestack[(- 1)]):\n            self.baseuri = self.basestack[(- 1)]\n    if self.langstack:\n        self.langstack.pop()\n        if self.langstack:\n            self.lang = self.langstack[(- 1)]\n    self.depth -= 1\n", "label": 1}
{"function": "\n\ndef addError(self, test, err):\n    'Overrides normal addError to add support for errorClasses.\\n        If the exception is a registered class, the error will be added\\n        to the list for that class, not errors.\\n        '\n    stream = getattr(self, 'stream', None)\n    (ec, ev, tb) = err\n    try:\n        exc_info = self._exc_info_to_string(err, test)\n    except TypeError:\n        exc_info = self._exc_info_to_string(err)\n    for (cls, (storage, label, isfail)) in self.errorClasses.items():\n        if (result.isclass(ec) and issubclass(ec, cls)):\n            if isfail:\n                test.passwd = False\n            storage.append((test, exc_info))\n            if (stream is not None):\n                if self.showAll:\n                    message = [label]\n                    detail = result._exception_detail(err[1])\n                    if detail:\n                        message.append(detail)\n                    stream.writeln(': '.join(message))\n                elif self.dots:\n                    stream.write(label[:1])\n            return\n    self.errors.append((test, exc_info))\n    test.passed = False\n    if (stream is not None):\n        if self.showAll:\n            self.colorizer.write('ERROR', 'red')\n            self.stream.writeln()\n        elif self.dots:\n            stream.write('E')\n", "label": 1}
{"function": "\n\n@signalcommand\ndef handle(self, *scripts, **options):\n    NOTICE = self.style.SQL_TABLE\n    NOTICE2 = self.style.SQL_FIELD\n    ERROR = self.style.ERROR\n    ERROR2 = self.style.NOTICE\n    subdirs = []\n    if (not options.get('noscripts')):\n        subdirs.append('scripts')\n    if options.get('infixtures'):\n        subdirs.append('fixtures')\n    verbosity = int(options.get('verbosity', 1))\n    show_traceback = options.get('traceback', True)\n    if (show_traceback is None):\n        show_traceback = True\n    no_traceback = options.get('no_traceback', False)\n    if no_traceback:\n        show_traceback = False\n    silent = options.get('silent', False)\n    if silent:\n        verbosity = 0\n    email_notifications = options.get('email_notifications', False)\n    if (len(subdirs) < 1):\n        print(NOTICE('No subdirs to run left.'))\n        return\n    if (len(scripts) < 1):\n        print(ERROR('Script name required.'))\n        return\n\n    def run_script(mod, *script_args):\n        try:\n            mod.run(*script_args)\n            if email_notifications:\n                self.send_email_notification(notification_id=mod.__name__)\n        except Exception:\n            if silent:\n                return\n            if (verbosity > 0):\n                print(ERROR((\"Exception while running run() in '%s'\" % mod.__name__)))\n            if email_notifications:\n                self.send_email_notification(notification_id=mod.__name__, include_traceback=True)\n            if show_traceback:\n                raise\n\n    def my_import(mod):\n        if (verbosity > 1):\n            print(NOTICE(('Check for %s' % mod)))\n        try:\n            importlib.import_module(mod)\n            t = __import__(mod, [], [], [' '])\n        except (ImportError, AttributeError) as e:\n            if str(e).startswith('No module named'):\n                try:\n                    (exc_type, exc_value, exc_traceback) = sys.exc_info()\n                    try:\n                        if (exc_traceback.tb_next.tb_next is None):\n                            return False\n                    except AttributeError:\n                        pass\n                finally:\n                    exc_traceback = None\n            if (verbosity > 1):\n                if (verbosity > 2):\n                    traceback.print_exc()\n                print(ERROR((\"Cannot import module '%s': %s.\" % (mod, e))))\n            return False\n        if hasattr(t, 'run'):\n            if (verbosity > 1):\n                print(NOTICE2((\"Found script '%s' ...\" % mod)))\n            return t\n        elif (verbosity > 1):\n            print(ERROR2((\"Find script '%s' but no run() function found.\" % mod)))\n\n    def find_modules_for_script(script):\n        \" find script module which contains 'run' attribute \"\n        modules = []\n        for app in list_apps():\n            for subdir in subdirs:\n                mod = my_import(('%s.%s.%s' % (app, subdir, script)))\n                if mod:\n                    modules.append(mod)\n        sa = script.split('.')\n        for subdir in subdirs:\n            nn = '.'.join((sa[:(- 1)] + [subdir, sa[(- 1)]]))\n            mod = my_import(nn)\n            if mod:\n                modules.append(mod)\n        if (script.find('.') != (- 1)):\n            mod = my_import(script)\n            if mod:\n                modules.append(mod)\n        return modules\n    if options.get('script_args'):\n        script_args = options['script_args']\n    else:\n        script_args = []\n    for script in scripts:\n        modules = find_modules_for_script(script)\n        if (not modules):\n            if ((verbosity > 0) and (not silent)):\n                print(ERROR((\"No (valid) module for script '%s' found\" % script)))\n                if (verbosity < 2):\n                    print(ERROR('Try running with a higher verbosity level like: -v2 or -v3'))\n        for mod in modules:\n            if (verbosity > 1):\n                print(NOTICE2((\"Running script '%s' ...\" % mod.__name__)))\n            run_script(mod, *script_args)\n", "label": 1}
{"function": "\n\ndef unpack(input):\n    m = magic.Magic()\n    try:\n        file_type = m.from_file(input)\n    except:\n        raise dxpy.AppError('Unable to identify compression format')\n    if (file_type == 'application/x-tar'):\n        raise dxpy.AppError('App does not support tar files.  Please unpack.')\n    uncomp_util = None\n    if (file_type == 'XZ compressed data'):\n        uncomp_util = 'xzcat'\n    elif (file_type[:21] == 'bzip2 compressed data'):\n        uncomp_util = 'bzcat'\n    elif (file_type[:20] == 'gzip compressed data'):\n        uncomp_util = 'zcat'\n    elif ((file_type == 'POSIX tar archive (GNU)') or ('tar' in file_type)):\n        raise dxpy.AppError('Found a tar archive.  Please untar your sequences before importing')\n    else:\n        return input\n    if (uncomp_util != None):\n        test_util = None\n        if (uncomp_util == 'xzcat'):\n            test_util = 'xz'\n        elif (uncomp_util == 'bzcat'):\n            test_util = 'bzip2'\n        elif (uncomp_util == 'zcat'):\n            test_util = 'gzip'\n        try:\n            subprocess.check_call(' '.join([test_util, '-t', input]), shell=True)\n        except subprocess.CalledProcessError:\n            raise dxpy.AppError((('File failed integrity check by ' + uncomp_util) + '.  Compressed file is corrupted.'))\n    try:\n        with subprocess.Popen([uncomp_util, input], stdout=subprocess.PIPE).stdout as pipe:\n            line = pipe.next()\n        uncomp_type = m.from_buffer(line)\n    except:\n        raise dxpy.AppError('Error detecting file format after decompression')\n    if ((uncomp_type == 'POSIX tar archive (GNU)') or ('tar' in uncomp_type)):\n        raise dxpy.AppError('Found a tar archive after decompression.  Please untar your files before importing')\n    try:\n        subprocess.check_call(' '.join([uncomp_util, '--stdout', input, '>', 'uncompressed.gtf']), shell=True)\n        return 'uncompressed.gtf'\n    except subprocess.CalledProcessError:\n        raise dxpy.AppError('Unable to open compressed input for reading')\n", "label": 1}
{"function": "\n\ndef namedtuple(typename, field_names):\n    \"Returns a new subclass of tuple with named fields.\\n\\n    >>> Point = namedtuple('Point', 'x y')\\n    >>> Point.__doc__                   # docstring for the new class\\n    'Point(x, y)'\\n    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\\n    >>> p[0] + p[1]                     # indexable like a plain tuple\\n    33\\n    >>> x, y = p                        # unpack like a regular tuple\\n    >>> x, y\\n    (11, 22)\\n    >>> p.x + p.y                       # fields also accessable by name\\n    33\\n    >>> d = p._asdict()                 # convert to a dictionary\\n    >>> d['x']\\n    11\\n    >>> Point(**d)                      # convert from a dictionary\\n    Point(x=11, y=22)\\n    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\\n    Point(x=100, y=22)\\n\\n    \"\n    if isinstance(field_names, basestring):\n        field_names = field_names.replace(',', ' ').split()\n    field_names = tuple(map(str, field_names))\n    for name in ((typename,) + field_names):\n        if (not all(((c.isalnum() or (c == '_')) for c in name))):\n            raise ValueError(('Type names and field names can only contain alphanumeric characters and underscores: %r' % name))\n        if _iskeyword(name):\n            raise ValueError(('Type names and field names cannot be a keyword: %r' % name))\n        if name[0].isdigit():\n            raise ValueError(('Type names and field names cannot start with a number: %r' % name))\n    seen_names = set()\n    for name in field_names:\n        if name.startswith('_'):\n            raise ValueError(('Field names cannot start with an underscore: %r' % name))\n        if (name in seen_names):\n            raise ValueError(('Encountered duplicate field name: %r' % name))\n        seen_names.add(name)\n    numfields = len(field_names)\n    argtxt = repr(field_names).replace(\"'\", '')[1:(- 1)]\n    reprtxt = ', '.join((('%s=%%r' % name) for name in field_names))\n    dicttxt = ', '.join((('%r: t[%d]' % (name, pos)) for (pos, name) in enumerate(field_names)))\n    template = (\"class %(typename)s(tuple):\\n        '%(typename)s(%(argtxt)s)' \\n\\n        __slots__ = () \\n\\n        _fields = %(field_names)r \\n\\n        def __new__(_cls, %(argtxt)s):\\n            return _tuple.__new__(_cls, (%(argtxt)s)) \\n\\n        @classmethod\\n        def _make(cls, iterable, new=tuple.__new__, len=len):\\n            'Make a new %(typename)s object from a sequence or iterable'\\n            result = new(cls, iterable)\\n            if len(result) != %(numfields)d:\\n                raise TypeError('Expected %(numfields)d arguments, got %%d' %% len(result))\\n            return result \\n\\n        def __repr__(self):\\n            return '%(typename)s(%(reprtxt)s)' %% self \\n\\n        def _asdict(t):\\n            'Return a new dict which maps field names to their values'\\n            return {%(dicttxt)s} \\n\\n        def _replace(_self, **kwds):\\n            'Return a new %(typename)s object replacing specified fields with new values'\\n            result = _self._make(map(kwds.pop, %(field_names)r, _self))\\n            if kwds:\\n                raise ValueError('Got unexpected field names: %%r' %% kwds.keys())\\n            return result \\n\\n        def __getnewargs__(self):\\n            return tuple(self) \\n\\n\" % locals())\n    for (i, name) in enumerate(field_names):\n        template += ('        %s = _property(_itemgetter(%d))\\n' % (name, i))\n    namespace = dict(_itemgetter=_itemgetter, __name__=('namedtuple_%s' % typename), _property=property, _tuple=tuple)\n    try:\n        exec(template, namespace)\n    except SyntaxError:\n        e = _sys.exc_info()[1]\n        raise SyntaxError(((e.message + ':\\n') + template))\n    result = namespace[typename]\n    if hasattr(_sys, '_getframe'):\n        result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__')\n    return result\n", "label": 1}
{"function": "\n\ndef get_current_config(facts):\n    ' Get current openshift config\\n\\n        Args:\\n            facts (dict): existing facts\\n        Returns:\\n            dict: the facts dict updated with the current openshift config\\n    '\n    current_config = dict()\n    roles = [role for role in facts if (role not in ['common', 'provider'])]\n    for role in roles:\n        if ('roles' in current_config):\n            current_config['roles'].append(role)\n        else:\n            current_config['roles'] = [role]\n        kubeconfig_dir = '/var/lib/origin/openshift.local.certificates'\n        if (role == 'node'):\n            kubeconfig_dir = os.path.join(kubeconfig_dir, ('node-%s' % facts['common']['hostname']))\n        kubeconfig_path = os.path.join(kubeconfig_dir, '.kubeconfig')\n        if (os.path.isfile('/usr/bin/openshift') and os.path.isfile(kubeconfig_path)):\n            try:\n                (_, output, _) = module.run_command(['/usr/bin/openshift', 'ex', 'config', 'view', '-o', 'json', ('--kubeconfig=%s' % kubeconfig_path)], check_rc=False)\n                config = json.loads(output)\n                cad = 'certificate-authority-data'\n                try:\n                    for cluster in config['clusters']:\n                        config['clusters'][cluster][cad] = 'masked'\n                except KeyError:\n                    pass\n                try:\n                    for user in config['users']:\n                        config['users'][user][cad] = 'masked'\n                        config['users'][user]['client-key-data'] = 'masked'\n                except KeyError:\n                    pass\n                current_config['kubeconfig'] = config\n            except Exception:\n                pass\n    return current_config\n", "label": 1}
{"function": "\n\ndef get_instances(name, lifecycle_state='InService', health_status='Healthy', attribute='private_ip_address', attributes=None, region=None, key=None, keyid=None, profile=None):\n    '\\n    return attribute of all instances in the named autoscale group.\\n\\n    CLI example::\\n\\n        salt-call boto_asg.get_instances my_autoscale_group_name\\n\\n    '\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    ec2_conn = _get_ec2_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        asgs = conn.get_all_groups(names=[name])\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        return False\n    if (len(asgs) != 1):\n        log.debug(\"name '{0}' returns multiple ASGs: {1}\".format(name, [asg.name for asg in asgs]))\n        return False\n    asg = asgs[0]\n    instance_ids = []\n    for i in asg.instances:\n        if ((lifecycle_state is not None) and (i.lifecycle_state != lifecycle_state)):\n            continue\n        if ((health_status is not None) and (i.health_status != health_status)):\n            continue\n        instance_ids.append(i.instance_id)\n    instances = ec2_conn.get_only_instances(instance_ids=instance_ids)\n    if attributes:\n        return [[getattr(instance, attr).encode('ascii') for attr in attributes] for instance in instances]\n    else:\n        return [getattr(instance, attribute).encode('ascii') for instance in instances if getattr(instance, attribute)]\n    return [getattr(instance, attribute).encode('ascii') for instance in instances]\n", "label": 1}
{"function": "\n\ndef _private_key_bytes(self, encoding, format, encryption_algorithm, evp_pkey, cdata):\n    if (not isinstance(format, serialization.PrivateFormat)):\n        raise TypeError('format must be an item from the PrivateFormat enum')\n    if (not isinstance(encryption_algorithm, serialization.KeySerializationEncryption)):\n        raise TypeError('Encryption algorithm must be a KeySerializationEncryption instance')\n    if isinstance(encryption_algorithm, serialization.NoEncryption):\n        password = b''\n        passlen = 0\n        evp_cipher = self._ffi.NULL\n    elif isinstance(encryption_algorithm, serialization.BestAvailableEncryption):\n        evp_cipher = self._lib.EVP_get_cipherbyname(b'aes-256-cbc')\n        password = encryption_algorithm.password\n        passlen = len(password)\n        if (passlen > 1023):\n            raise ValueError('Passwords longer than 1023 bytes are not supported by this backend')\n    else:\n        raise ValueError('Unsupported encryption type')\n    key_type = self._lib.Cryptography_EVP_PKEY_id(evp_pkey)\n    if (encoding is serialization.Encoding.PEM):\n        if (format is serialization.PrivateFormat.PKCS8):\n            write_bio = self._lib.PEM_write_bio_PKCS8PrivateKey\n            key = evp_pkey\n        else:\n            assert (format is serialization.PrivateFormat.TraditionalOpenSSL)\n            if (key_type == self._lib.EVP_PKEY_RSA):\n                write_bio = self._lib.PEM_write_bio_RSAPrivateKey\n            elif (key_type == self._lib.EVP_PKEY_DSA):\n                write_bio = self._lib.PEM_write_bio_DSAPrivateKey\n            else:\n                assert (self._lib.Cryptography_HAS_EC == 1)\n                assert (key_type == self._lib.EVP_PKEY_EC)\n                write_bio = self._lib.PEM_write_bio_ECPrivateKey\n            key = cdata\n    elif (encoding is serialization.Encoding.DER):\n        if (format is serialization.PrivateFormat.TraditionalOpenSSL):\n            if (not isinstance(encryption_algorithm, serialization.NoEncryption)):\n                raise ValueError('Encryption is not supported for DER encoded traditional OpenSSL keys')\n            return self._private_key_bytes_traditional_der(key_type, cdata)\n        else:\n            assert (format is serialization.PrivateFormat.PKCS8)\n            write_bio = self._lib.i2d_PKCS8PrivateKey_bio\n            key = evp_pkey\n    else:\n        raise TypeError('encoding must be an item from the Encoding enum')\n    bio = self._create_mem_bio_gc()\n    res = write_bio(bio, key, evp_cipher, password, passlen, self._ffi.NULL, self._ffi.NULL)\n    self.openssl_assert((res == 1))\n    return self._read_mem_bio(bio)\n", "label": 1}
{"function": "\n\ndef _Dynamic_Get(self, get_request, get_response):\n    txid = None\n    if get_request.has_transaction():\n        txid = get_request.transaction().handle()\n        txdata = self.__transactions[txid]\n        assert (txdata.thread_id == thread.get_ident()), 'Transactions are single-threaded.'\n        keys = [(k, k.Encode()) for k in get_request.key_list()]\n        new_request = datastore_pb.GetRequest()\n        for (key, enckey) in keys:\n            if (enckey not in txdata.entities):\n                new_request.add_key().CopyFrom(key)\n    else:\n        new_request = get_request\n    if (new_request.key_size() > 0):\n        super(RemoteDatastoreStub, self).MakeSyncCall('datastore_v3', 'Get', new_request, get_response)\n    if (txid is not None):\n        newkeys = new_request.key_list()\n        entities = get_response.entity_list()\n        for (key, entity) in zip(newkeys, entities):\n            entity_hash = None\n            if entity.has_entity():\n                entity_hash = HashEntity(entity.entity())\n            txdata.preconditions[key.Encode()] = (key, entity_hash)\n        new_response = datastore_pb.GetResponse()\n        it = iter(get_response.entity_list())\n        for (key, enckey) in keys:\n            if (enckey in txdata.entities):\n                cached_entity = txdata.entities[enckey][1]\n                if cached_entity:\n                    new_response.add_entity().mutable_entity().CopyFrom(cached_entity)\n                else:\n                    new_response.add_entity()\n            else:\n                new_entity = it.next()\n                if new_entity.has_entity():\n                    assert (new_entity.entity().key() == key)\n                    new_response.add_entity().CopyFrom(new_entity)\n                else:\n                    new_response.add_entity()\n        get_response.CopyFrom(new_response)\n", "label": 1}
{"function": "\n\ndef __init__(self, request, registration, args=None, kwargs=None, payload=None, timeout=None, receive_progress=None, caller=None, caller_authid=None, caller_authrole=None, procedure=None, enc_algo=None, enc_key=None, enc_serializer=None):\n    '\\n\\n        :param request: The WAMP request ID of this request.\\n        :type request: int\\n        :param registration: The registration ID of the endpoint to be invoked.\\n        :type registration: int\\n        :param args: Positional values for application-defined event payload.\\n           Must be serializable using any serializers in use.\\n        :type args: list or tuple or None\\n        :param kwargs: Keyword values for application-defined event payload.\\n           Must be serializable using any serializers in use.\\n        :type kwargs: dict or None\\n        :param payload: Alternative, transparent payload. If given, `args` and `kwargs` must be left unset.\\n        :type payload: unicode or bytes\\n        :param timeout: If present, let the callee automatically cancels\\n           the invocation after this ms.\\n        :type timeout: int or None\\n        :param receive_progress: Indicates if the callee should produce progressive results.\\n        :type receive_progress: bool or None\\n        :param caller: The WAMP session ID of the caller. Only filled if caller is disclosed.\\n        :type caller: None or int\\n        :param caller_authid: The WAMP authid of the caller. Only filled if caller is disclosed.\\n        :type caller_authid: None or unicode\\n        :param caller_authrole: The WAMP authrole of the caller. Only filled if caller is disclosed.\\n        :type caller_authrole: None or unicode\\n        :param procedure: For pattern-based registrations, the invocation MUST include the actual procedure being called.\\n        :type procedure: unicode or None\\n        :param enc_algo: If using payload encryption, the algorithm used (currently, only \"cryptobox\" is valid).\\n        :type enc_algo: unicode\\n        :param enc_key: If using payload encryption, the message encryption key.\\n        :type enc_key: unicode or binary\\n        :param enc_serializer: If using payload encryption, the encrypted payload object serializer.\\n        :type enc_serializer: unicode\\n        '\n    assert (type(request) in six.integer_types)\n    assert (type(registration) in six.integer_types)\n    assert ((args is None) or (type(args) in [list, tuple]))\n    assert ((kwargs is None) or (type(kwargs) == dict))\n    assert ((payload is None) or (type(payload) in [six.text_type, six.binary_type]))\n    assert ((payload is None) or ((payload is not None) and (args is None) and (kwargs is None)))\n    assert ((timeout is None) or (type(timeout) in six.integer_types))\n    assert ((receive_progress is None) or (type(receive_progress) == bool))\n    assert ((caller is None) or (type(caller) in six.integer_types))\n    assert ((caller_authid is None) or (type(caller_authid) == six.text_type))\n    assert ((caller_authrole is None) or (type(caller_authrole) == six.text_type))\n    assert ((procedure is None) or (type(procedure) == six.text_type))\n    assert ((enc_algo is None) or (enc_algo in [PAYLOAD_ENC_CRYPTO_BOX]))\n    assert ((enc_key is None) or (type(enc_key) in [six.text_type, six.binary_type]))\n    assert ((enc_serializer is None) or (enc_serializer in ['json', 'msgpack', 'cbor', 'ubjson']))\n    assert (((enc_algo is None) and (enc_key is None) and (enc_serializer is None)) or ((enc_algo is not None) and (payload is not None)))\n    Message.__init__(self)\n    self.request = request\n    self.registration = registration\n    self.args = args\n    self.kwargs = kwargs\n    self.payload = payload\n    self.timeout = timeout\n    self.receive_progress = receive_progress\n    self.caller = caller\n    self.caller_authid = caller_authid\n    self.caller_authrole = caller_authrole\n    self.procedure = procedure\n    self.enc_algo = enc_algo\n    self.enc_key = enc_key\n    self.enc_serializer = enc_serializer\n", "label": 1}
{"function": "\n\ndef execute(self, fullpath, fstat, test=False):\n    result = []\n    for arg in self.fmt:\n        if (arg == 'path'):\n            result.append(fullpath)\n        elif (arg == 'name'):\n            result.append(os.path.basename(fullpath))\n        elif (arg == 'size'):\n            result.append(fstat[stat.ST_SIZE])\n        elif (arg == 'type'):\n            result.append(_FILE_TYPES.get(stat.S_IFMT(fstat[stat.ST_MODE]), '?'))\n        elif (arg == 'mode'):\n            result.append(int(oct(fstat[stat.ST_MODE])[(- 3):]))\n        elif (arg == 'mtime'):\n            result.append(fstat[stat.ST_MTIME])\n        elif (arg == 'user'):\n            uid = fstat[stat.ST_UID]\n            try:\n                result.append(pwd.getpwuid(uid).pw_name)\n            except KeyError:\n                result.append(uid)\n        elif (arg == 'group'):\n            gid = fstat[stat.ST_GID]\n            try:\n                result.append(grp.getgrgid(gid).gr_name)\n            except KeyError:\n                result.append(gid)\n        elif (arg == 'md5'):\n            if stat.S_ISREG(fstat[stat.ST_MODE]):\n                md5digest = salt.utils.get_hash(fullpath, 'md5')\n                result.append(md5digest)\n            else:\n                result.append('')\n    if (len(result) == 1):\n        return result[0]\n    else:\n        return result\n", "label": 1}
{"function": "\n\n@register_specialize\n@gof.local_optimizer([T.Sum, T.elemwise.Prod])\ndef local_opt_alloc(node):\n    '\\n    sum(alloc(constant,shapes...)) => constant*prod(shapes)\\n    or\\n    prod(alloc(constant,shapes...)) => constant**prod(shapes)\\n\\n    '\n    if (isinstance(node.op, T.Sum) or isinstance(node.op, T.elemwise.Prod)):\n        (node_inps,) = node.inputs\n        if (node_inps.owner and isinstance(node_inps.owner.op, T.Alloc)):\n            input = node_inps.owner.inputs[0]\n            shapes = node_inps.owner.inputs[1:]\n            if ((node.op.axis is None) or (node.op.axis == tuple(range(input.ndim)))):\n                try:\n                    val = get_scalar_constant_value(input)\n                    assert (val.size == 1)\n                    if isinstance(node.op, T.Sum):\n                        val = (val.reshape(1)[0] * T.mul(*shapes))\n                    else:\n                        val = (val.reshape(1)[0] ** T.mul(*shapes))\n                    return [T.cast(val, dtype=node.outputs[0].dtype)]\n                except NotScalarConstantError:\n                    pass\n            else:\n                try:\n                    val = get_scalar_constant_value(input)\n                    assert (val.size == 1)\n                    val = val.reshape(1)[0]\n                    to_prod = [shapes[i] for i in xrange(len(shapes)) if (i in node.op.axis)]\n                    if to_prod:\n                        if isinstance(node.op, T.Sum):\n                            val *= T.mul(*to_prod)\n                        else:\n                            val = (val ** T.mul(*to_prod))\n                    return [T.alloc(T.cast(val, dtype=node.outputs[0].dtype), *[shapes[i] for i in xrange(len(shapes)) if (i not in node.op.axis)])]\n                except NotScalarConstantError:\n                    pass\n", "label": 1}
{"function": "\n\ndef search(self, query_string, user, global_and_search=False):\n    elapsed_time = 0\n    start_time = datetime.datetime.now()\n    result_set = set()\n    search_dict = {\n        \n    }\n    if (not self.model):\n        self.model = apps.get_model(self.app_label, self.model_name)\n        if (not self.label):\n            self.label = self.model._meta.verbose_name\n    if ('q' in query_string):\n        for search_field in self.get_all_search_fields():\n            search_dict.setdefault(search_field.get_model(), {\n                'searches': [],\n                'label': search_field.label,\n                'return_value': search_field.return_value,\n            })\n            search_dict[search_field.get_model()]['searches'].append({\n                'field_name': [search_field.field],\n                'terms': self.normalize_query(query_string.get('q', '').strip()),\n            })\n    else:\n        for search_field in self.get_all_search_fields():\n            if ((search_field.field in query_string) and query_string[search_field.field]):\n                search_dict.setdefault(search_field.get_model(), {\n                    'searches': [],\n                    'label': search_field.label,\n                    'return_value': search_field.return_value,\n                })\n                search_dict[search_field.get_model()]['searches'].append({\n                    'field_name': [search_field.field],\n                    'terms': self.normalize_query(query_string[search_field.field]),\n                })\n    for (model, data) in search_dict.items():\n        logger.debug('model: %s', model)\n        model_result_set = set()\n        for query_entry in data['searches']:\n            field_query_list = self.assemble_query(query_entry['terms'], query_entry['field_name'])\n            logger.debug('field_query_list: %s', field_query_list)\n            field_result_set = set()\n            for query in field_query_list:\n                logger.debug('query: %s', query)\n                term_query_result_set = set(model.objects.filter(query).values_list(data['return_value'], flat=True))\n                if (not field_result_set):\n                    field_result_set = term_query_result_set\n                else:\n                    field_result_set &= term_query_result_set\n                logger.debug('term_query_result_set: %s', term_query_result_set)\n                logger.debug('field_result_set: %s', field_result_set)\n            if global_and_search:\n                if (not model_result_set):\n                    model_result_set = field_result_set\n                else:\n                    model_result_set &= field_result_set\n            else:\n                model_result_set |= field_result_set\n        result_set = (result_set | model_result_set)\n    elapsed_time = unicode((datetime.datetime.now() - start_time)).split(':')[2]\n    queryset = self.model.objects.filter(pk__in=list(result_set)[:setting_limit.value])\n    if self.permission:\n        try:\n            Permission.check_permissions(user, [self.permission])\n        except PermissionDenied:\n            queryset = AccessControlList.objects.filter_by_access(self.permission, user, queryset)\n    RecentSearch.objects.add_query_for_user(user, query_string, len(result_set))\n    return (queryset, result_set, elapsed_time)\n", "label": 1}
{"function": "\n\ndef apt_cache_info(apt_cache_cmd, package_name):\n    if (apt_cache_cmd not in ('showsrc', 'show')):\n        raise NotImplementedError((\"don't know how to run apt-cache command '%s'\" % apt_cache_cmd))\n    result_list = []\n    args = ['apt-cache', apt_cache_cmd, package_name]\n    cmd = subprocess.Popen(args, stdin=subprocess.PIPE, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n    returncode = cmd.wait()\n    if returncode:\n        errline = cmd.stderr.read()\n        if (not ((returncode == 100) and (errline == \"E: You must put some 'source' URIs in your sources.list\\n\"))):\n            log.error('ERROR running: %s', ' '.join(args))\n            raise RuntimeError(('returncode %d from subprocess %s' % (returncode, args)))\n    inlines = cmd.stdout.read()\n    version_blocks = inlines.split('\\n\\n')\n    for version_block in version_blocks:\n        block_dict = {\n            \n        }\n        if (len(version_block) == 0):\n            continue\n        version_lines = version_block.split('\\n')\n        assert version_lines[0].startswith('Package: ')\n        block_dict['Package'] = version_lines[0][len('Package: '):]\n        if (apt_cache_cmd == 'showsrc'):\n            assert version_lines[1].startswith('Binary: ')\n            block_dict['Binary'] = version_lines[1][len('Binary: '):]\n            block_dict['Binary'] = block_dict['Binary'].split(', ')\n        elif (apt_cache_cmd == 'show'):\n            for start in ('Provides: ', 'Conflicts: ', 'Replaces: '):\n                key = start[:(- 2)]\n                for line in version_lines[2:]:\n                    if line.startswith(start):\n                        unsplit_line_result = line[len(start):]\n                        split_result = unsplit_line_result.split(', ')\n                        block_dict[key] = split_result\n                if (key not in block_dict):\n                    block_dict[key] = []\n        result_list.append(block_dict)\n    return result_list\n", "label": 1}
{"function": "\n\ndef __init__(self, url, cookie_file=None, username=None, password=None, api_token=None, agent=None, session=None, disable_proxy=False, auth_callback=None, otp_token_callback=None, verify_ssl=True, save_cookies=True, ext_auth_cookies=None):\n    if (not url.endswith('/')):\n        url += '/'\n    self.url = (url + 'api/')\n    self.save_cookies = save_cookies\n    self.ext_auth_cookies = ext_auth_cookies\n    if self.save_cookies:\n        (self.cookie_jar, self.cookie_file) = create_cookie_jar(cookie_file=cookie_file)\n        try:\n            self.cookie_jar.load(ignore_expires=True)\n        except IOError:\n            pass\n    else:\n        self.cookie_jar = CookieJar()\n        self.cookie_file = None\n    if self.ext_auth_cookies:\n        try:\n            self.cookie_jar.load(ext_auth_cookies, ignore_expires=True)\n        except IOError as e:\n            logging.critical('There was an error while loading a cookie file: %s', e)\n            pass\n    parsed_url = urlparse(url)\n    self.domain = parsed_url[1].partition(':')[0]\n    if (self.domain.count('.') < 1):\n        self.domain = ('%s.local' % self.domain)\n    if session:\n        cookie = Cookie(version=0, name=RB_COOKIE_NAME, value=session, port=None, port_specified=False, domain=self.domain, domain_specified=True, domain_initial_dot=True, path=parsed_url[2], path_specified=True, secure=False, expires=None, discard=False, comment=None, comment_url=None, rest={\n            'HttpOnly': None,\n        })\n        self.cookie_jar.set_cookie(cookie)\n        if self.save_cookies:\n            self.cookie_jar.save()\n    if username:\n        try:\n            self.cookie_jar.clear(self.domain, parsed_url[2], RB_COOKIE_NAME)\n        except KeyError:\n            pass\n    password_mgr = ReviewBoardHTTPPasswordMgr(self.url, username, password, api_token, auth_callback, otp_token_callback)\n    self.preset_auth_handler = PresetHTTPAuthHandler(self.url, password_mgr)\n    handlers = []\n    if (not verify_ssl):\n        context = ssl._create_unverified_context()\n        handlers.append(HTTPSHandler(context=context))\n    if disable_proxy:\n        handlers.append(ProxyHandler({\n            \n        }))\n    handlers += [HTTPCookieProcessor(self.cookie_jar), ReviewBoardHTTPBasicAuthHandler(password_mgr), HTTPDigestAuthHandler(password_mgr), self.preset_auth_handler, ReviewBoardHTTPErrorProcessor()]\n    if agent:\n        self.agent = agent\n    else:\n        self.agent = ('RBTools/' + get_package_version()).encode('utf-8')\n    opener = build_opener(*handlers)\n    opener.addheaders = [(b'User-agent', self.agent)]\n    install_opener(opener)\n    self._cache = None\n    self._urlopen = urlopen\n", "label": 1}
{"function": "\n\ndef __new__(cls, latitude=None, longitude=None, altitude=None):\n    single_arg = ((longitude is None) and (altitude is None))\n    if (single_arg and (not isinstance(latitude, util.NUMBER_TYPES))):\n        arg = latitude\n        if (arg is None):\n            pass\n        elif isinstance(arg, Point):\n            return cls.from_point(arg)\n        elif isinstance(arg, basestring):\n            return cls.from_string(arg)\n        else:\n            try:\n                seq = iter(arg)\n            except TypeError:\n                raise TypeError(('Failed to create Point instance from %r.' % (arg,)))\n            else:\n                return cls.from_sequence(seq)\n    latitude = float((latitude or 0))\n    if (abs(latitude) > 90):\n        raise ValueError(('Latitude out of range [-90, 90]: %r' % latitude))\n    longitude = float((longitude or 0))\n    if (abs(longitude) > 180):\n        raise ValueError(('Longitude out of range [-180, 180]: %r' % longitude))\n    altitude = float((altitude or 0))\n    self = super(Point, cls).__new__(cls)\n    self.latitude = latitude\n    self.longitude = longitude\n    self.altitude = altitude\n    return self\n", "label": 1}
{"function": "\n\ndef _write_file_ifaces(iface, data, **settings):\n    '\\n    Writes a file to disk\\n    '\n    try:\n        eth_template = JINJA.get_template('debian_eth.jinja')\n        source_template = JINJA.get_template('debian_source.jinja')\n    except jinja2.exceptions.TemplateNotFound:\n        log.error('Could not load template debian_eth.jinja')\n        return ''\n    adapters = _parse_interfaces()\n    adapters[iface] = data\n    ifcfg = ''\n    for adapter in adapters:\n        if (('type' in adapters[adapter]) and (adapters[adapter]['type'] == 'slave')):\n            adapters[adapter]['data']['inet']['addrfam'] = 'inet'\n            adapters[adapter]['data']['inet']['proto'] = 'manual'\n            adapters[adapter]['data']['inet']['master'] = adapters[adapter]['master']\n        if (('type' in adapters[adapter]) and (adapters[adapter]['type'] == 'source')):\n            tmp = source_template.render({\n                'name': adapter,\n                'data': adapters[adapter],\n            })\n        else:\n            tmp = eth_template.render({\n                'name': adapter,\n                'data': adapters[adapter],\n            })\n        ifcfg = (ifcfg + tmp)\n        if (adapter == iface):\n            saved_ifcfg = tmp\n    _SEPERATE_FILE = False\n    if ('filename' in settings):\n        if (not settings['filename'].startswith('/')):\n            filename = '{0}/{1}'.format(_DEB_NETWORK_DIR, settings['filename'])\n        else:\n            filename = settings['filename']\n        _SEPERATE_FILE = True\n    elif ('filename' in adapters[adapter]['data']):\n        filename = adapters[adapter]['data']\n    else:\n        filename = _DEB_NETWORK_FILE\n    if (not os.path.exists(os.path.dirname(filename))):\n        msg = '{0} cannot be written.'\n        msg = msg.format(os.path.dirname(filename))\n        log.error(msg)\n        raise AttributeError(msg)\n    with salt.utils.flopen(filename, 'w') as fout:\n        if _SEPERATE_FILE:\n            fout.write(saved_ifcfg)\n        else:\n            fout.write(ifcfg)\n    return saved_ifcfg.split('\\n')\n", "label": 1}
{"function": "\n\ndef iterintervalsubtract(left, right, lstart, lstop, rstart, rstop, lkey, rkey, include_stop):\n    lit = iter(left)\n    lhdr = next(lit)\n    lflds = list(map(text_type, lhdr))\n    rit = iter(right)\n    rhdr = next(rit)\n    asindices(lhdr, lstart)\n    asindices(lhdr, lstop)\n    if (lkey is not None):\n        asindices(lhdr, lkey)\n    asindices(rhdr, rstart)\n    asindices(rhdr, rstop)\n    if (rkey is not None):\n        asindices(rhdr, rkey)\n    outhdr = list(lflds)\n    (yield tuple(outhdr))\n    (lstartidx, lstopidx) = asindices(lhdr, (lstart, lstop))\n    getlcoords = itemgetter(lstartidx, lstopidx)\n    getrcoords = itemgetter(*asindices(rhdr, (rstart, rstop)))\n    if (rkey is None):\n        lookup = intervallookup(right, rstart, rstop, include_stop=include_stop)\n        search = lookup.search\n        for lrow in lit:\n            (start, stop) = getlcoords(lrow)\n            rrows = search(start, stop)\n            if (not rrows):\n                (yield tuple(lrow))\n            else:\n                rivs = sorted([getrcoords(rrow) for rrow in rrows], key=itemgetter(0))\n                for (x, y) in _subtract(start, stop, rivs):\n                    out = list(lrow)\n                    out[lstartidx] = x\n                    out[lstopidx] = y\n                    (yield tuple(out))\n    else:\n        lookup = facetintervallookup(right, key=rkey, start=rstart, stop=rstop, include_stop=include_stop)\n        getlkey = itemgetter(*asindices(lhdr, lkey))\n        for lrow in lit:\n            lkey = getlkey(lrow)\n            (start, stop) = getlcoords(lrow)\n            try:\n                rrows = lookup[lkey].search(start, stop)\n            except KeyError:\n                rrows = None\n            except AttributeError:\n                rrows = None\n            if (not rrows):\n                (yield tuple(lrow))\n            else:\n                rivs = sorted([getrcoords(rrow) for rrow in rrows], key=itemgetter(0))\n                for (x, y) in _subtract(start, stop, rivs):\n                    out = list(lrow)\n                    out[lstartidx] = x\n                    out[lstopidx] = y\n                    (yield tuple(out))\n", "label": 1}
{"function": "\n\n@requires_search\ndef search_contributor(query, page=0, size=10, exclude=None, current_user=None):\n    'Search for contributors to add to a project using elastic search. Request must\\n    include JSON data with a \"query\" field.\\n\\n    :param query: The substring of the username to search for\\n    :param page: For pagination, the page number to use for results\\n    :param size: For pagination, the number of results per page\\n    :param exclude: A list of User objects to exclude from the search\\n    :param current_user: A User object of the current user\\n\\n    :return: List of dictionaries, each containing the ID, full name,\\n        most recent employment and education, gravatar URL of an OSF user\\n\\n    '\n    start = (page * size)\n    items = re.split('[\\\\s-]+', query)\n    exclude = (exclude or [])\n    normalized_items = []\n    for item in items:\n        try:\n            normalized_item = six.u(item)\n        except TypeError:\n            normalized_item = item\n        normalized_item = unicodedata.normalize('NFKD', normalized_item).encode('ascii', 'ignore')\n        normalized_items.append(normalized_item)\n    items = normalized_items\n    query = ('  AND '.join(('{}*~'.format(re.escape(item)) for item in items)) + ''.join((' NOT id:\"{}\"'.format(excluded._id) for excluded in exclude)))\n    results = search(build_query(query, start=start, size=size), index=INDEX, doc_type='user')\n    docs = results['results']\n    pages = math.ceil((results['counts'].get('user', 0) / size))\n    validate_page_num(page, pages)\n    users = []\n    for doc in docs:\n        user = User.load(doc['id'])\n        if (current_user and (current_user._id == user._id)):\n            n_projects_in_common = (- 1)\n        elif current_user:\n            n_projects_in_common = current_user.n_projects_in_common(user)\n        else:\n            n_projects_in_common = 0\n        if (user is None):\n            logger.error('Could not load user {0}'.format(doc['id']))\n            continue\n        if user.is_active:\n            current_employment = None\n            education = None\n            if user.jobs:\n                current_employment = user.jobs[0]['institution']\n            if user.schools:\n                education = user.schools[0]['institution']\n            users.append({\n                'fullname': doc['user'],\n                'id': doc['id'],\n                'employment': current_employment,\n                'education': education,\n                'n_projects_in_common': n_projects_in_common,\n                'gravatar_url': gravatar(user, use_ssl=True, size=settings.PROFILE_IMAGE_MEDIUM),\n                'profile_url': user.profile_url,\n                'registered': user.is_registered,\n                'active': user.is_active,\n            })\n    return {\n        'users': users,\n        'total': results['counts']['total'],\n        'pages': pages,\n        'page': page,\n    }\n", "label": 1}
{"function": "\n\ndef _make_cfg_defaults(self, module_name=NotGiven, default_distribution=NotGiven, guess_maintainer=NotGiven):\n    defaults = {\n        \n    }\n    default_re = re.compile('^.* \\\\(Default: (.*)\\\\)$')\n    for (longopt, shortopt, description) in stdeb_cfg_options:\n        assert longopt.endswith('=')\n        assert (longopt.lower() == longopt)\n        key = longopt[:(- 1)]\n        matchobj = default_re.search(description)\n        if (matchobj is not None):\n            groups = matchobj.groups()\n            assert (len(groups) == 1)\n            value = groups[0]\n            if (value == '<source-debianized-setup-name>'):\n                assert (key == 'source')\n                value = source_debianize_name(module_name)\n            elif (value == 'python-<debianized-setup-name>'):\n                assert (key == 'package')\n                value = ('python-' + debianize_name(module_name))\n            elif (value == 'python3-<debianized-setup-name>'):\n                assert (key == 'package3')\n                value = ('python3-' + debianize_name(module_name))\n            elif (value == '<setup-maintainer-or-author>'):\n                assert (key == 'maintainer')\n                value = guess_maintainer\n            if (key == 'suite'):\n                if (default_distribution is not None):\n                    value = default_distribution\n                    log.warn('Deprecation warning: you are using the --default-distribution option. Switch to the --suite option.')\n        else:\n            value = ''\n        defaults[key] = value\n    return defaults\n", "label": 1}
{"function": "\n\ndef _non_repeating(self, decorate):\n    for delay in (0, 1):\n        time = TestingTimeFunction()\n        callback = MockCallback()\n        if decorate:\n            timer = Timer.decorate(10, _time_function=time)(callback)\n        else:\n            timer = Timer(callback, 10, _time_function=time)\n        still_alive = timer.run()\n        assert still_alive\n        assert (callback.nb_calls == 0)\n        assert (timer.sleep_time() == 10)\n        time.time = 9\n        still_alive = timer.run()\n        assert still_alive\n        assert (callback.nb_calls == 0)\n        assert (timer.sleep_time() == 1)\n        time.time = (10 + delay)\n        assert (timer.sleep_time() == 0)\n        still_alive = timer.run()\n        assert (not still_alive)\n        assert (callback.nb_calls == 1)\n        assert (timer.sleep_time() == Decimal('inf'))\n        still_alive = timer.run()\n        assert (not still_alive)\n        assert (callback.nb_calls == 1)\n        assert (timer.sleep_time() == Decimal('inf'))\n        time.time = 15\n        still_alive = timer.run()\n        assert (not still_alive)\n        assert (callback.nb_calls == 1)\n        assert (timer.sleep_time() == Decimal('inf'))\n", "label": 1}
{"function": "\n\ndef convert(self, value, param, ctx):\n    column_properties = value.split(':')\n    name = column_properties[0]\n    if (not valid_underscore_name(name)):\n        ctx.fail('The name provided is not a valid variable name')\n    try:\n        type_properties = column_properties[1].split(',')\n        type = (type_properties[0].lower() or COLUMN_TYPE_DEFAULT)\n    except IndexError:\n        type_properties = []\n        type = COLUMN_TYPE_DEFAULT\n    if (type not in COLUMN_TYPE_MAPPING):\n        ctx.fail(('The type specified for column %s is invalid' % name))\n    try:\n        if type_properties[1]:\n            length = int(type_properties[1])\n        else:\n            length = None\n    except IndexError:\n        length = None\n    except ValueError:\n        ctx.fail(('The length specified for column %s is invalid' % name))\n    if (length and (type not in COLUMN_TYPES_SUPPORTING_LENGTH)):\n        ctx.fail(('The length specified for column %s is not allowed for %s types' % (name, type)))\n    try:\n        modifiers = column_properties[2].lower().split(',')\n    except IndexError:\n        modifiers = []\n    for modifier in modifiers:\n        if (modifier not in COLUMN_MODIFIER_MAPPING):\n            ctx.fail(('The column modifier %s for column %s is invalid' % (modifier, name)))\n    return (name, type, length, modifiers)\n", "label": 1}
{"function": "\n\ndef resolve(self, context, ignore_failures=False):\n    if isinstance(self.var, Variable):\n        try:\n            obj = self.var.resolve(context)\n        except VariableDoesNotExist:\n            if ignore_failures:\n                obj = None\n            else:\n                string_if_invalid = context.template.engine.string_if_invalid\n                if string_if_invalid:\n                    if ('%s' in string_if_invalid):\n                        return (string_if_invalid % self.var)\n                    else:\n                        return string_if_invalid\n                else:\n                    obj = string_if_invalid\n    else:\n        obj = self.var\n    for (func, args) in self.filters:\n        arg_vals = []\n        for (lookup, arg) in args:\n            if (not lookup):\n                arg_vals.append(mark_safe(arg))\n            else:\n                arg_vals.append(arg.resolve(context))\n        if getattr(func, 'expects_localtime', False):\n            obj = template_localtime(obj, context.use_tz)\n        if getattr(func, 'needs_autoescape', False):\n            new_obj = func(obj, *arg_vals, autoescape=context.autoescape)\n        else:\n            new_obj = func(obj, *arg_vals)\n        if (getattr(func, 'is_safe', False) and isinstance(obj, SafeData)):\n            obj = mark_safe(new_obj)\n        elif isinstance(obj, EscapeData):\n            obj = mark_for_escaping(new_obj)\n        else:\n            obj = new_obj\n    return obj\n", "label": 1}
{"function": "\n\ndef test_symmetrize():\n    assert (symmetrize(0, x, y, z) == (0, 0))\n    assert (symmetrize(1, x, y, z) == (1, 0))\n    s1 = ((x + y) + z)\n    s2 = (((x * y) + (x * z)) + (y * z))\n    s3 = ((x * y) * z)\n    assert (symmetrize(1) == (1, 0))\n    assert (symmetrize(1, formal=True) == (1, 0, []))\n    assert (symmetrize(x) == (x, 0))\n    assert (symmetrize((x + 1)) == ((x + 1), 0))\n    assert (symmetrize(x, x, y) == ((x + y), (- y)))\n    assert (symmetrize((x + 1), x, y) == (((x + y) + 1), (- y)))\n    assert (symmetrize(x, x, y, z) == (s1, ((- y) - z)))\n    assert (symmetrize((x + 1), x, y, z) == ((s1 + 1), ((- y) - z)))\n    assert (symmetrize((x ** 2), x, y, z) == (((s1 ** 2) - (2 * s2)), ((- (y ** 2)) - (z ** 2))))\n    assert (symmetrize(((x ** 2) + (y ** 2))) == (((((- 2) * x) * y) + ((x + y) ** 2)), 0))\n    assert (symmetrize(((x ** 2) - (y ** 2))) == (((((- 2) * x) * y) + ((x + y) ** 2)), ((- 2) * (y ** 2))))\n    assert (symmetrize(((((x ** 3) + (y ** 2)) + (a * (x ** 2))) + (b * (y ** 3))), x, y) == ((((((((- 3) * x) * y) * (x + y)) - (((2 * a) * x) * y)) + (a * ((x + y) ** 2))) + ((x + y) ** 3)), (((y ** 2) * (1 - a)) + ((y ** 3) * (b - 1)))))\n    U = [u0, u1, u2] = symbols('u:3')\n    assert (symmetrize((x + 1), x, y, z, formal=True, symbols=U) == ((u0 + 1), ((- y) - z), [(u0, ((x + y) + z)), (u1, (((x * y) + (x * z)) + (y * z))), (u2, ((x * y) * z))]))\n    assert (symmetrize([1, 2, 3]) == [(1, 0), (2, 0), (3, 0)])\n    assert (symmetrize([1, 2, 3], formal=True) == ([(1, 0), (2, 0), (3, 0)], []))\n    assert (symmetrize([(x + y), (x - y)]) == [((x + y), 0), ((x + y), ((- 2) * y))])\n", "label": 1}
{"function": "\n\ndef poll(self, event, timeout):\n    if self.state.SHUTDOWN:\n        raise Error(errno.EBADF)\n    if (not (event in ('recv', 'send', 'acks'))):\n        raise Error(errno.EINVAL)\n    if (event == 'recv'):\n        if (self.state.ESTABLISHED or self.state.CLOSE_WAIT):\n            ptype = super(DataLinkConnection, self).poll(event, timeout)\n            if (self.state.ESTABLISHED or self.state.CLOSE_WAIT):\n                return (ptype == ProtocolDataUnit.Information)\n            else:\n                return False\n    if (event == 'send'):\n        if self.state.ESTABLISHED:\n            if super(DataLinkConnection, self).poll(event, timeout):\n                return self.state.ESTABLISHED\n    if (event == 'acks'):\n        with self.acks_ready:\n            while (not (self.acks_recvd > 0)):\n                self.acks_ready.wait(timeout)\n            if (self.acks_recvd > 0):\n                self.acks_recvd = (self.acks_recvd - 1)\n                return True\n    return False\n", "label": 1}
{"function": "\n\ndef compareAddressEq(self, addr1, addr2):\n    'Checks if two addresses are equal, considering local/useable associations'\n    if (addr1 is None):\n        return (addr2 is None)\n    if (addr2 is None):\n        return False\n    try:\n        if (addr1.addressDetails == addr2.addressDetails):\n            return True\n    except AttributeError:\n        pass\n    if (isinstance(addr1, ActorAddress) and isinstance(addr1.addressDetails, ActorLocalAddress)):\n        if (isinstance(addr2, ActorAddress) and isinstance(addr2.addressDetails, ActorLocalAddress)):\n            return (addr1.addressDetails == addr2.addressDetails)\n        try:\n            return ((addr1.addressDetails.generatingActor == self._thisActorAddr) and self._managed[addr1.addressDetails.addressInstanceNum] and (self._managed[addr1.addressDetails.addressInstanceNum].addressDetails == addr2.addressDetails))\n        except AttributeError:\n            return False\n    if (isinstance(addr2, ActorAddress) and isinstance(addr2.addressDetails, ActorLocalAddress)):\n        try:\n            return ((addr2.addressDetails.generatingActor == self._thisActorAddr) and self._managed[addr2.addressDetails.addressInstanceNum] and (self._managed[addr2.addressDetails.addressInstanceNum].addressDetails == addr1.addressDetails))\n        except AttributeError:\n            return False\n    return False\n", "label": 1}
{"function": "\n\ndef on_import(self, fgraph, node, reason):\n    if (node.outputs[0] in self.shape_of):\n        for r in (node.outputs + node.inputs):\n            assert (r in self.shape_of)\n        return\n    for (i, r) in enumerate(node.inputs):\n        self.init_r(r)\n    o_shapes = self.get_node_infer_shape(node)\n    if (len(o_shapes) != len(node.outputs)):\n        raise Exception(((('The infer_shape method for the Op \"%s\" returned a list ' + 'with the wrong number of element: len(o_shapes) = %d ') + ' != len(node.outputs) = %d') % (str(node.op), len(o_shapes), len(node.outputs))))\n    for (sh_idx, sh) in enumerate(o_shapes):\n        if (sh is None):\n            continue\n        if (not isinstance(sh, (list, tuple))):\n            raise ValueError((\"infer_shape of %s didn't return a list of list. It returned '%s'\" % (str(node), str(o_shapes))))\n        new_shape = []\n        for (i, d) in enumerate(sh):\n            if (getattr(d, 'dtype', 'int64') != 'int64'):\n                assert (d.dtype in theano.tensor.discrete_dtypes), (node, d.dtype)\n                assert (str(d.dtype) != 'uint64'), node\n                new_shape += sh[len(new_shape):(i + 1)]\n                new_shape[i] = theano.tensor.cast(d, 'int64')\n        if new_shape:\n            new_shape += sh[len(new_shape):]\n            o_shapes[sh_idx] = tuple(new_shape)\n    for (r, s) in izip(node.outputs, o_shapes):\n        self.set_shape(r, s)\n", "label": 1}
{"function": "\n\ndef populate(self, installed_apps=None):\n    '\\n        Loads application configurations and models.\\n\\n        This method imports each application module and then each model module.\\n\\n        It is thread safe and idempotent, but not reentrant.\\n        '\n    if self.ready:\n        return\n    with self._lock:\n        if self.ready:\n            return\n        if self.app_configs:\n            raise RuntimeError(\"populate() isn't reentrant\")\n        for entry in installed_apps:\n            if isinstance(entry, AppConfig):\n                app_config = entry\n            else:\n                app_config = AppConfig.create(entry)\n            if (app_config.label in self.app_configs):\n                raise ImproperlyConfigured((\"Application labels aren't unique, duplicates: %s\" % app_config.label))\n            self.app_configs[app_config.label] = app_config\n        counts = Counter((app_config.name for app_config in self.app_configs.values()))\n        duplicates = [name for (name, count) in counts.most_common() if (count > 1)]\n        if duplicates:\n            raise ImproperlyConfigured((\"Application names aren't unique, duplicates: %s\" % ', '.join(duplicates)))\n        self.apps_ready = True\n        for app_config in self.app_configs.values():\n            all_models = self.all_models[app_config.label]\n            app_config.import_models(all_models)\n        self.clear_cache()\n        self.models_ready = True\n        for app_config in self.get_app_configs():\n            app_config.ready()\n        self.ready = True\n", "label": 1}
{"function": "\n\ndef install(self, install_options, global_options=[], root=None):\n    if self.editable:\n        self.install_editable(install_options, global_options)\n        return\n    if self.is_wheel:\n        version = pip.wheel.wheel_version(self.source_dir)\n        pip.wheel.check_compatibility(version, self.name)\n        self.move_wheel_files(self.source_dir, root=root)\n        self.install_succeeded = True\n        return\n    global_options += self.options.get('global_options', [])\n    install_options += self.options.get('install_options', [])\n    if self.isolated:\n        global_options = (list(global_options) + ['--no-user-cfg'])\n    temp_location = tempfile.mkdtemp('-record', 'pip-')\n    record_filename = os.path.join(temp_location, 'install-record.txt')\n    try:\n        install_args = [sys.executable]\n        install_args.append('-c')\n        install_args.append((\"import setuptools, tokenize;__file__=%r;exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\\\r\\\\n', '\\\\n'), __file__, 'exec'))\" % self.setup_py))\n        install_args += (list(global_options) + ['install', '--record', record_filename])\n        if (not self.as_egg):\n            install_args += ['--single-version-externally-managed']\n        if (root is not None):\n            install_args += ['--root', root]\n        if self.pycompile:\n            install_args += ['--compile']\n        else:\n            install_args += ['--no-compile']\n        if running_under_virtualenv():\n            py_ver_str = ('python' + sysconfig.get_python_version())\n            install_args += ['--install-headers', os.path.join(sys.prefix, 'include', 'site', py_ver_str, self.name)]\n        logger.info('Running setup.py install for %s', self.name)\n        with indent_log():\n            call_subprocess((install_args + install_options), cwd=self.source_dir, show_stdout=False)\n        if (not os.path.exists(record_filename)):\n            logger.debug('Record file %s not found', record_filename)\n            return\n        self.install_succeeded = True\n        if self.as_egg:\n            return\n\n        def prepend_root(path):\n            if ((root is None) or (not os.path.isabs(path))):\n                return path\n            else:\n                return change_root(root, path)\n        with open(record_filename) as f:\n            for line in f:\n                directory = os.path.dirname(line)\n                if directory.endswith('.egg-info'):\n                    egg_info_dir = prepend_root(directory)\n                    break\n            else:\n                logger.warning('Could not find .egg-info directory in install record for %s', self)\n                return\n        new_lines = []\n        with open(record_filename) as f:\n            for line in f:\n                filename = line.strip()\n                if os.path.isdir(filename):\n                    filename += os.path.sep\n                new_lines.append(make_path_relative(prepend_root(filename), egg_info_dir))\n        inst_files_path = os.path.join(egg_info_dir, 'installed-files.txt')\n        with open(inst_files_path, 'w') as f:\n            f.write(('\\n'.join(new_lines) + '\\n'))\n    finally:\n        if os.path.exists(record_filename):\n            os.remove(record_filename)\n        rmtree(temp_location)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _readfrom_internal(cls, data, header=None, checksum=False, ignore_missing_end=False, **kwargs):\n    '\\n        Provides the bulk of the internal implementation for readfrom and\\n        fromstring.\\n\\n        For some special cases, supports using a header that was already\\n        created, and just using the input data for the actual array data.\\n        '\n    hdu_buffer = None\n    hdu_fileobj = None\n    header_offset = 0\n    if isinstance(data, _File):\n        from_file = True\n        if (header is None):\n            header_offset = data.tell()\n            header = Header.fromfile(data, endcard=(not ignore_missing_end))\n        hdu_fileobj = data\n        data_offset = data.tell()\n    else:\n        from_file = False\n        try:\n            np.ndarray((), dtype='ubyte', buffer=data)\n        except TypeError:\n            raise TypeError(('The provided object %r does not contain an underlying memory buffer.  fromstring() requires an object that supports the buffer interface such as bytes, str (in Python 2.x but not in 3.x), buffer, memoryview, ndarray, etc.  This restriction is to ensure that efficient access to the array/table data is possible.' % data))\n        if (header is None):\n\n            def block_iter(nbytes):\n                idx = 0\n                while (idx < len(data)):\n                    (yield data[idx:(idx + nbytes)])\n                    idx += nbytes\n            (header_str, header) = Header._from_blocks(block_iter, True, '', (not ignore_missing_end), True)\n            if (len(data) > len(header_str)):\n                hdu_buffer = data\n        elif data:\n            hdu_buffer = data\n        header_offset = 0\n        data_offset = len(header_str)\n    cls = _hdu_class_from_header(cls, header)\n    (args, varargs, varkwargs, defaults) = getargspec(cls.__init__)\n    new_kwargs = kwargs.copy()\n    if (not varkwargs):\n        for key in kwargs:\n            if (key not in args):\n                del new_kwargs[key]\n    hdu = cls(data=DELAYED, header=header, **new_kwargs)\n    hdu._file = hdu_fileobj\n    hdu._buffer = hdu_buffer\n    hdu._header_offset = header_offset\n    hdu._data_offset = data_offset\n    size = hdu.size\n    hdu._data_size = (size + _pad_length(size))\n    if (checksum and (checksum != 'remove') and isinstance(hdu, _ValidHDU)):\n        hdu._verify_checksum_datasum(checksum)\n    return hdu\n", "label": 1}
{"function": "\n\ndef lock(self):\n    '\\n        Acquire this lock.\\n\\n        @rtype: C{bool}\\n        @return: True if the lock is acquired, false otherwise.\\n\\n        @raise: Any exception os.symlink() may raise, other than\\n        EEXIST.\\n        '\n    clean = True\n    while True:\n        try:\n            symlink(str(os.getpid()), self.name)\n        except OSError as e:\n            if (_windows and (e.errno in (errno.EACCES, errno.EIO))):\n                return False\n            if (e.errno == errno.EEXIST):\n                try:\n                    pid = readlink(self.name)\n                except (IOError, OSError) as e:\n                    if (e.errno == errno.ENOENT):\n                        continue\n                    elif (_windows and (e.errno == errno.EACCES)):\n                        return False\n                    raise\n                try:\n                    if (kill is not None):\n                        kill(int(pid), 0)\n                except OSError as e:\n                    if (e.errno == errno.ESRCH):\n                        try:\n                            rmlink(self.name)\n                        except OSError as e:\n                            if (e.errno == errno.ENOENT):\n                                continue\n                            raise\n                        clean = False\n                        continue\n                    raise\n                return False\n            raise\n        self.locked = True\n        self.clean = clean\n        return True\n", "label": 1}
{"function": "\n\ndef versionUpgrade(self):\n    '(internal) Do a version upgrade.\\n        '\n    bases = _aybabtu(self.__class__)\n    bases.reverse()\n    bases.append(self.__class__)\n    if self.__dict__.has_key('persistenceVersion'):\n        pver = self.__dict__['persistenceVersion']\n        del self.__dict__['persistenceVersion']\n        highestVersion = 0\n        highestBase = None\n        for base in bases:\n            if (not base.__dict__.has_key('persistenceVersion')):\n                continue\n            if (base.persistenceVersion > highestVersion):\n                highestBase = base\n                highestVersion = base.persistenceVersion\n        if highestBase:\n            self.__dict__[('%s.persistenceVersion' % reflect.qual(highestBase))] = pver\n    for base in bases:\n        if ((Versioned not in base.__bases__) and (not base.__dict__.has_key('persistenceVersion'))):\n            continue\n        currentVers = base.persistenceVersion\n        pverName = ('%s.persistenceVersion' % reflect.qual(base))\n        persistVers = (self.__dict__.get(pverName) or 0)\n        if persistVers:\n            del self.__dict__[pverName]\n        assert (persistVers <= currentVers), \"Sorry, can't go backwards in time.\"\n        while (persistVers < currentVers):\n            persistVers = (persistVers + 1)\n            method = base.__dict__.get(('upgradeToVersion%s' % persistVers), None)\n            if method:\n                log.msg(('Upgrading %s (of %s @ %s) to version %s' % (reflect.qual(base), reflect.qual(self.__class__), id(self), persistVers)))\n                method(self)\n            else:\n                log.msg(('Warning: cannot upgrade %s to version %s' % (base, persistVers)))\n", "label": 1}
{"function": "\n\ndef test_SeqMul():\n    per = SeqPer((1, 2, 3), (n, 0, oo))\n    form = SeqFormula((n ** 2))\n    per_bou = SeqPer((1, 2), (n, 1, 5))\n    form_bou = SeqFormula((n ** 2), (n, 6, 10))\n    form_bou2 = SeqFormula((n ** 2), (1, 5))\n    assert (SeqMul() == S.EmptySequence)\n    assert (SeqMul(S.EmptySequence) == S.EmptySequence)\n    assert (SeqMul(per) == per)\n    assert (SeqMul(per, S.EmptySequence) == S.EmptySequence)\n    assert (SeqMul(per_bou, form_bou) == S.EmptySequence)\n    s = SeqMul(per_bou, form_bou2, evaluate=False)\n    assert (s.args == (form_bou2, per_bou))\n    assert (s[:] == [1, 8, 9, 32, 25])\n    assert (list(s) == [1, 8, 9, 32, 25])\n    assert isinstance(SeqMul(per, per_bou, evaluate=False), SeqMul)\n    s1 = SeqMul(per, per_bou)\n    assert isinstance(s1, SeqPer)\n    assert (s1 == SeqPer((1, 4, 3, 2, 2, 6), (n, 1, 5)))\n    s2 = SeqMul(form, form_bou)\n    assert isinstance(s2, SeqFormula)\n    assert (s2 == SeqFormula((n ** 4), (6, 10)))\n    assert (SeqMul(form, form_bou, per) == SeqMul(per, SeqFormula((n ** 4), (6, 10))))\n    assert (SeqMul(form, SeqMul(form_bou, per)) == SeqMul(per, SeqFormula((n ** 4), (6, 10))))\n    assert (SeqMul(per, SeqMul(form, form_bou2, evaluate=False), evaluate=False) == SeqMul(form, per, form_bou2, evaluate=False))\n    assert (SeqMul(SeqPer((1, 2), (n, 0, oo)), SeqPer((1, 2), (n, 0, oo))) == SeqPer((1, 4), (n, 0, oo)))\n", "label": 1}
{"function": "\n\ndef fetch(self, max_size=None, retry_limit=3, ignore_failures=False):\n    \"Return a FetchResult, which can be iterated over as a list of \\n        MessageSets. A MessageSet is returned for every broker partition that\\n        is successfully queried, even if that MessageSet is empty.\\n\\n        FIXME: This is where the adjustment needs to happen. Regardless of \\n        whether a rebalance has occurred or not, we can very easily see if we\\n        are still responsible for the same partitions as we were the last time\\n        we ran, and set self._bps_to_next_offsets --> we just need to check if\\n        it's not None and if we still have the same offsets, and adjust \\n        accordingly.\\n        \"\n\n    def needs_offset_values_from_zk(bps_to_offsets):\n        'We need to pull offset values from ZK if we have no \\n            BrokerPartitions in our BPs -> Offsets mapping, or if some of those\\n            Offsets are unknown (None)'\n        return ((not bps_to_offsets) or (None in bps_to_offsets.values()))\n    log.debug('Fetch called on ZKConsumer {0}'.format(self.id))\n    if self._needs_rebalance:\n        self.rebalance()\n    bps_to_offsets = dict(self._bps_to_next_offsets)\n    offsets_pulled_from_zk = False\n    if needs_offset_values_from_zk(bps_to_offsets):\n        if bps_to_offsets:\n            bps_needing_offsets = [bp for (bp, offset) in bps_to_offsets.items() if (offset is None)]\n        else:\n            bps_needing_offsets = self.broker_partitions\n        bps_to_offsets.update(self._zk_util.offsets_for(self.consumer_group, self._id, bps_needing_offsets))\n        offsets_pulled_from_zk = True\n    message_sets = []\n    for bp in bps_to_offsets:\n        offset = bps_to_offsets[bp]\n        kafka = self._connections[bp.broker_id]\n        partition = kafka.partition(bp.topic, bp.partition)\n        if (offset is None):\n            offset = partition.latest_offset()\n        try:\n            offsets_msgs = kafka.fetch(bp.topic, offset, partition=bp.partition, max_size=max_size)\n        except OffsetOutOfRange as ex:\n            if offsets_pulled_from_zk:\n                log.error('Offset {0} from ZooKeeper is out of range for {1}'.format(offset, bp))\n                offset = partition.latest_offset()\n                log.error('Retrying with offset {0} for {1}'.format(offset, bp))\n                offsets_msgs = kafka.fetch(bp.topic, offset, partition=bp.partition, max_size=max_size)\n            else:\n                raise\n        except KafkaError as k_err:\n            if ignore_failures:\n                log.error('Ignoring failed fetch on {0}'.format(bp))\n                log.exception(k_err)\n                continue\n            else:\n                raise\n        msg_set = MessageSet(bp, offset, offsets_msgs)\n        old_stats = self._stats[bp]\n        self._stats[bp] = ConsumerStats(fetches=(old_stats.fetches + 1), bytes=(old_stats.bytes + msg_set.size), messages=(old_stats.messages + len(msg_set)), max_fetch=max(old_stats.max_fetch, msg_set.size))\n        message_sets.append(msg_set)\n    result = FetchResult(sorted(message_sets))\n    for msg_set in result:\n        self._bps_to_next_offsets[msg_set.broker_partition] = msg_set.next_offset\n    if self._autocommit:\n        self.commit_offsets()\n    return result\n", "label": 1}
{"function": "\n\ndef parse_shorts(tokens, options):\n    \"shorts ::= '-' ( chars )* [ [ ' ' ] chars ] ;\"\n    token = tokens.move()\n    assert (token.startswith('-') and (not token.startswith('--')))\n    left = token.lstrip('-')\n    parsed = []\n    while (left != ''):\n        (short, left) = (('-' + left[0]), left[1:])\n        similar = [o for o in options if (o.short == short)]\n        if (len(similar) > 1):\n            raise tokens.error(('%s is specified ambiguously %d times' % (short, len(similar))))\n        elif (len(similar) < 1):\n            o = Option(short, None, 0)\n            options.append(o)\n            if (tokens.error is DocoptExit):\n                o = Option(short, None, 0, True)\n        else:\n            o = Option(short, similar[0].long, similar[0].argcount, similar[0].value)\n            value = None\n            if (o.argcount != 0):\n                if (left == ''):\n                    if (tokens.current() is None):\n                        raise tokens.error(('%s requires argument' % short))\n                    value = tokens.move()\n                else:\n                    value = left\n                    left = ''\n            if (tokens.error is DocoptExit):\n                o.value = (value if (value is not None) else True)\n        parsed.append(o)\n    return parsed\n", "label": 1}
{"function": "\n\ndef resolve_address(self, hostname, callback, allow_cname=True):\n    'Start looking up an A or AAAA record.\\n\\n            `callback` will be called with a list of (family, address) tuples\\n            (each holiding socket.AF_*  and IPv4 or IPv6 address literal) on\\n            success. The list will be empty on error.\\n\\n            :Parameters:\\n                - `hostname`: the host name to look up\\n                - `callback`: a function to be called with a list of received\\n                  addresses\\n                - `allow_cname`: `True` if CNAMEs should be followed\\n            :Types:\\n                - `hostname`: `str`\\n                - `callback`: function accepting a single argument\\n                - `allow_cname`: `bool`\\n            '\n    if isinstance(hostname, str):\n        hostname = hostname.encode('idna').decode('us-ascii')\n    rtypes = []\n    if self.settings['ipv6']:\n        rtypes.append(('AAAA', socket.AF_INET6))\n    if self.settings['ipv4']:\n        rtypes.append(('A', socket.AF_INET))\n    if (not self.settings['prefer_ipv6']):\n        rtypes.reverse()\n    exception = None\n    result = []\n    for (rtype, rfamily) in rtypes:\n        try:\n            try:\n                records = dns.resolver.query(hostname, rtype)\n            except dns.exception.DNSException:\n                records = dns.resolver.query((hostname + '.'), rtype)\n        except dns.exception.DNSException as err:\n            exception = err\n            continue\n        if ((not allow_cname) and (records.rrset.name != dns.name.from_text(hostname))):\n            logger.warning('Unexpected CNAME record found for {0!r}'.format(hostname))\n            continue\n        if records:\n            for record in records:\n                result.append((rfamily, record.to_text()))\n    if ((not result) and exception):\n        logger.warning('Could not resolve {0!r}: {1}'.format(hostname, exception.__class__.__name__))\n    callback(result)\n", "label": 1}
{"function": "\n\ndef sort_tables_and_constraints(tables, filter_fn=None, extra_dependencies=None):\n    'sort a collection of :class:`.Table`  / :class:`.ForeignKeyConstraint`\\n    objects.\\n\\n    This is a dependency-ordered sort which will emit tuples of\\n    ``(Table, [ForeignKeyConstraint, ...])`` such that each\\n    :class:`.Table` follows its dependent :class:`.Table` objects.\\n    Remaining :class:`.ForeignKeyConstraint` objects that are separate due to\\n    dependency rules not satisifed by the sort are emitted afterwards\\n    as ``(None, [ForeignKeyConstraint ...])``.\\n\\n    Tables are dependent on another based on the presence of\\n    :class:`.ForeignKeyConstraint` objects, explicit dependencies\\n    added by :meth:`.Table.add_is_dependent_on`, as well as dependencies\\n    stated here using the :paramref:`~.sort_tables_and_constraints.skip_fn`\\n    and/or :paramref:`~.sort_tables_and_constraints.extra_dependencies`\\n    parameters.\\n\\n    :param tables: a sequence of :class:`.Table` objects.\\n\\n    :param filter_fn: optional callable which will be passed a\\n     :class:`.ForeignKeyConstraint` object, and returns a value based on\\n     whether this constraint should definitely be included or excluded as\\n     an inline constraint, or neither.   If it returns False, the constraint\\n     will definitely be included as a dependency that cannot be subject\\n     to ALTER; if True, it will **only** be included as an ALTER result at\\n     the end.   Returning None means the constraint is included in the\\n     table-based result unless it is detected as part of a dependency cycle.\\n\\n    :param extra_dependencies: a sequence of 2-tuples of tables which will\\n     also be considered as dependent on each other.\\n\\n    .. versionadded:: 1.0.0\\n\\n    .. seealso::\\n\\n        :func:`.sort_tables`\\n\\n\\n    '\n    fixed_dependencies = set()\n    mutable_dependencies = set()\n    if (extra_dependencies is not None):\n        fixed_dependencies.update(extra_dependencies)\n    remaining_fkcs = set()\n    for table in tables:\n        for fkc in table.foreign_key_constraints:\n            if (fkc.use_alter is True):\n                remaining_fkcs.add(fkc)\n                continue\n            if filter_fn:\n                filtered = filter_fn(fkc)\n                if (filtered is True):\n                    remaining_fkcs.add(fkc)\n                    continue\n            dependent_on = fkc.referred_table\n            if (dependent_on is not table):\n                mutable_dependencies.add((dependent_on, table))\n        fixed_dependencies.update(((parent, table) for parent in table._extra_dependencies))\n    try:\n        candidate_sort = list(topological.sort(fixed_dependencies.union(mutable_dependencies), tables, deterministic_order=True))\n    except exc.CircularDependencyError as err:\n        for edge in err.edges:\n            if (edge in mutable_dependencies):\n                table = edge[1]\n                can_remove = [fkc for fkc in table.foreign_key_constraints if ((filter_fn is None) or (filter_fn(fkc) is not False))]\n                remaining_fkcs.update(can_remove)\n                for fkc in can_remove:\n                    dependent_on = fkc.referred_table\n                    if (dependent_on is not table):\n                        mutable_dependencies.discard((dependent_on, table))\n        candidate_sort = list(topological.sort(fixed_dependencies.union(mutable_dependencies), tables, deterministic_order=True))\n    return ([(table, table.foreign_key_constraints.difference(remaining_fkcs)) for table in candidate_sort] + [(None, list(remaining_fkcs))])\n", "label": 1}
{"function": "\n\ndef handle_make(environ, options, make_path):\n    if os.path.exists(make_path):\n        raise exceptions.VirtualenvAlreadyMade('virtualenv already exists: {0!r}'.format(make_path))\n    ve_base = os.path.dirname(make_path)\n    if (not os.path.exists(ve_base)):\n        os.mkdir(ve_base)\n    elif (not os.path.isdir(ve_base)):\n        raise exceptions.VirtualenvNotMade('could not make virtualenv: {0!r} already exists but is not a directory. Choose a different virtualenvs path using ~/.vexrc or $WORKON_HOME, or remove the existing file; then rerun your vex --make command.'.format(ve_base))\n    if ((os.name == 'nt') and (not os.environ.get('VIRTUAL_ENV', ''))):\n        ve = os.path.join(os.path.dirname(sys.executable), 'Scripts', 'virtualenv')\n    else:\n        ve = 'virtualenv'\n    args = [ve, make_path]\n    if options.python:\n        if (os.name == 'nt'):\n            python = distutils.spawn.find_executable(options.python)\n            if python:\n                options.python = python\n        args += ['--python', options.python]\n    if options.site_packages:\n        args += ['--system-site-packages']\n    if options.always_copy:\n        args += ['--always-copy']\n    returncode = run(args, env=environ, cwd=ve_base)\n    if (returncode != 0):\n        raise exceptions.VirtualenvNotMade('error creating virtualenv')\n    if (os.name != 'nt'):\n        pydoc_path = os.path.join(make_path, 'bin', 'pydoc')\n        with open(pydoc_path, 'wb') as out:\n            out.write(PYDOC_SCRIPT)\n        perms = os.stat(pydoc_path).st_mode\n        os.chmod(pydoc_path, (perms | 73))\n    else:\n        pydoc_path = os.path.join(make_path, 'Scripts', 'pydoc.bat')\n        with open(pydoc_path, 'wb') as out:\n            out.write(PYDOC_BATCH)\n", "label": 1}
{"function": "\n\ndef fit(self, feed_dicts, n_epoch=10, val_feed_dicts=None, show_metric=False, snapshot_step=None, snapshot_epoch=True, shuffle_all=None, run_id=None):\n    \" fit.\\n\\n        Train network with feeded data dicts.\\n\\n        Examples:\\n            ```python\\n            # 1 Optimizer\\n            trainer.fit(feed_dicts={input1: X, output1: Y},\\n                        val_feed_dicts={input1: X, output1: Y})\\n            trainer.fit(feed_dicts={input1: X1, input2: X2, output1: Y},\\n                        val_feed_dicts=0.1) # 10% of data used for validation\\n\\n            # 2 Optimizers\\n            trainer.fit(feed_dicts=[{in1: X1, out1:Y}, {in2: X2, out2:Y2}],\\n                        val_feed_dicts=[{in1: X1, out1:Y}, {in2: X2, out2:Y2}])\\n            ```\\n\\n        Arguments:\\n            feed_dicts: `dict` or list of `dict`. The dictionary to feed\\n                data to the network. It follows Tensorflow feed dict\\n                specifications: '{placeholder: data}'. In case of multiple\\n                optimizers, a list of dict is expected, that will\\n                respectively feed optimizers.\\n            n_epoch: `int`. Number of epoch to runs.\\n            val_feed_dicts: `dict`, list of `dict`, `float` or list of\\n                `float`. The data used for validation. Feed dict are\\n                following the same specification as `feed_dicts` above. It\\n                is also possible to provide a `float` for splitting training\\n                data for validation.\\n            show_metric: `bool`. If True, accuracy will be calculated and\\n                displayed at every step. Might give slower training.\\n            snapshot_step: `int`. If not None, the network will be snapshot\\n                every provided step (calculate validation loss/accuracy and\\n                save model, if a `checkpoint_path` is specified in `Trainer`).\\n            snapshot_epoch: `bool`. If True, snapshot the network at the end\\n                of every epoch.\\n            shuffle_all: `bool`. If True, shuffle all data batches (overrides\\n                `TrainOp` shuffle parameter behavior).\\n            run_id: `str`. A name for the current run. Used for Tensorboard\\n                display. If no name provided, a random one will be generated.\\n\\n        \"\n    if (not run_id):\n        run_id = id_generator(6)\n    print('---------------------------------')\n    print(('Run id: ' + run_id))\n    print(('Log directory: ' + self.tensorboard_dir))\n    if isinstance(shuffle_all, bool):\n        for t in self.train_ops:\n            t.shuffle = shuffle_all\n    with self.graph.as_default():\n        self.summ_writer = tf.train.SummaryWriter((self.tensorboard_dir + run_id), self.session.graph_def)\n        feed_dicts = to_list(feed_dicts)\n        for d in feed_dicts:\n            standarize_dict(d)\n        val_feed_dicts = to_list(val_feed_dicts)\n        if val_feed_dicts:\n            [standarize_dict(d) for d in val_feed_dicts]\n        validation_split(val_feed_dicts, feed_dicts)\n        termlogger = callbacks.TermLogger(self.training_step)\n        modelsaver = callbacks.ModelSaver(self.save, self.training_step, self.checkpoint_path, snapshot_epoch)\n        for (i, train_op) in enumerate(self.train_ops):\n            vd = (val_feed_dicts[i] if val_feed_dicts else None)\n            train_op.initialize_fit(feed_dicts[i], vd, show_metric, self.summ_writer)\n            metric_term_name = None\n            if (train_op.metric is not None):\n                if hasattr(train_op.metric, 'm_name'):\n                    metric_term_name = train_op.metric.m_name\n                else:\n                    metric_term_name = train_op.metric.name.split(':')[0]\n            termlogger.add(train_op.n_train_samples, val_size=train_op.n_val_samples, metric_name=metric_term_name, name=train_op.name)\n        max_batches_len = np.max([t.n_batches for t in self.train_ops])\n        termlogger.on_train_begin()\n        modelsaver.on_epoch_begin()\n        try:\n            for epoch in range(n_epoch):\n                termlogger.on_epoch_begin()\n                modelsaver.on_epoch_begin()\n                for batch_step in range(max_batches_len):\n                    self.training_step += 1\n                    termlogger.on_batch_begin()\n                    modelsaver.on_batch_begin()\n                    (global_loss, global_acc) = (0.0, 0.0)\n                    for (i, train_op) in enumerate(self.train_ops):\n                        termlogger.on_sub_epoch_begin()\n                        modelsaver.on_sub_batch_begin()\n                        snapshot = train_op._train(self.training_step, snapshot_epoch, snapshot_step, show_metric)\n                        global_loss += train_op.loss_value\n                        if (train_op.acc_value and global_acc):\n                            global_acc += (train_op.acc_value / len(self.train_ops))\n                        else:\n                            global_acc = None\n                        termlogger.on_sub_batch_end(i, train_op.epoch, train_op.step, train_op.loss_value, train_op.acc_value, train_op.val_loss, train_op.val_acc)\n                        modelsaver.on_sub_batch_end()\n                    self.session.run(self.incr_global_step)\n                    termlogger.on_batch_end(global_loss, global_acc, snapshot)\n                    modelsaver.on_batch_end(snapshot)\n                termlogger.on_epoch_end()\n                modelsaver.on_epoch_end()\n        finally:\n            termlogger.on_train_end()\n            modelsaver.on_train_end()\n", "label": 1}
{"function": "\n\ndef fix(obj, stacklevel=0):\n    prefix = ''.join((['.'] * stacklevel))\n    oid = id(obj)\n    canary_oid = oid\n    print(((prefix + 'fixing ') + str(oid)))\n    if (oid in already_fixed):\n        return already_fixed[oid]\n    if (oid in currently_fixing):\n        print(('returning placeholder for ' + str(oid)))\n        return Placeholder(oid)\n    currently_fixing.append(oid)\n    if hasattr(obj, 'set_value'):\n        rval = shared(obj.get_value())\n        obj.__getstate__ = None\n    elif (obj is None):\n        rval = None\n    elif isinstance(obj, list):\n        print((prefix + 'fixing a list'))\n        rval = []\n        for (i, elem) in enumerate(obj):\n            print((prefix + ('.fixing elem %d' % i)))\n            fixed_elem = fix(elem, (stacklevel + 2))\n            if isinstance(fixed_elem, Placeholder):\n                raise NotImplementedError()\n            rval.append(fixed_elem)\n    elif isinstance(obj, dict):\n        print((prefix + 'fixing a dict'))\n        rval = obj\n        \"\\n            rval = {}\\n            for key in obj:\\n                if key in blacklisted_keys or (isinstance(key, str) and key.endswith('Error')):\\n                    print(prefix + '.%s is blacklisted' % str(key))\\n                    rval[key] = obj[key]\\n                    continue\\n                print(prefix + '.fixing key ' + str(key) + ' of type '+str(type(key)))\\n                fixed_key = fix(key, stacklevel + 2)\\n                if isinstance(fixed_key, Placeholder):\\n                    raise NotImplementedError()\\n                print(prefix + '.fixing value for key '+str(key))\\n                fixed_value = fix(obj[key], stacklevel + 2)\\n                if isinstance(fixed_value, Placeholder):\\n                    raise NotImplementedError()\\n                rval[fixed_key] = fixed_value\\n            \"\n    elif isinstance(obj, tuple):\n        print((prefix + 'fixing a tuple'))\n        rval = []\n        for (i, elem) in enumerate(obj):\n            print((prefix + ('.fixing elem %d' % i)))\n            fixed_elem = fix(elem, (stacklevel + 2))\n            if isinstance(fixed_elem, Placeholder):\n                raise NotImplementedError()\n            rval.append(fixed_elem)\n        rval = tuple(rval)\n    elif isinstance(obj, (int, float, str)):\n        rval = obj\n    else:\n        print((prefix + 'fixing a generic object'))\n        field_names = dir(obj)\n        for field in field_names:\n            if isinstance(getattr(obj, field), types.MethodType):\n                print((prefix + ('.%s is an instancemethod' % field)))\n                continue\n            if ((field in blacklist) or field.startswith('__')):\n                print((prefix + ('.%s is blacklisted' % field)))\n                continue\n            print((prefix + ('.fixing field %s' % field)))\n            updated_field = fix(getattr(obj, field), (stacklevel + 2))\n            print((prefix + ('.applying fix to field %s' % field)))\n            if isinstance(updated_field, Placeholder):\n                postponed_fixes.append(FieldFixer(obj, field, updated_field))\n            else:\n                try:\n                    setattr(obj, field, updated_field)\n                except Exception as e:\n                    print((\"Couldn't do that because of exception: \" + str(e)))\n        rval = obj\n    already_fixed[oid] = rval\n    print(((prefix + 'stored fix for ') + str(oid)))\n    assert (canary_oid == oid)\n    del currently_fixing[currently_fixing.index(oid)]\n    return rval\n", "label": 1}
{"function": "\n\ndef test_read_session1():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test', 'test1'],\n        }, key='test')\n        key = Key.load(h.get_key('test'))\n        assert (key.api_key == 'test')\n        assert (key.is_admin() == False)\n        assert (key.can_create_key() == False)\n        assert (key.can_create_user() == False)\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef init_colors(self):\n    'Configure the coloring of the widget'\n    if self.pure:\n        return\n    try:\n        colors = self.config.ZMQInteractiveShell.colors\n    except AttributeError:\n        colors = None\n    try:\n        style = self.config.IPythonWidget.colors\n    except AttributeError:\n        style = None\n    if colors:\n        colors = colors.lower()\n        if (colors in ('lightbg', 'light')):\n            colors = 'lightbg'\n        elif (colors in ('dark', 'linux')):\n            colors = 'linux'\n        else:\n            colors = 'nocolor'\n    elif style:\n        if (style == 'bw'):\n            colors = 'nocolor'\n        elif styles.dark_style(style):\n            colors = 'linux'\n        else:\n            colors = 'lightbg'\n    else:\n        colors = None\n    widget = self.widget\n    if style:\n        widget.style_sheet = styles.sheet_from_template(style, colors)\n        widget.syntax_style = style\n        widget._syntax_style_changed()\n        widget._style_sheet_changed()\n    elif colors:\n        widget.set_default_style(colors=colors)\n    else:\n        widget.set_default_style()\n    if self.stylesheet:\n        if os.path.isfile(self.stylesheet):\n            with open(self.stylesheet) as f:\n                sheet = f.read()\n            widget.style_sheet = sheet\n            widget._style_sheet_changed()\n        else:\n            raise IOError(('Stylesheet %r not found.' % self.stylesheet))\n", "label": 1}
{"function": "\n\ndef parse(self, parse_until=None):\n    if (parse_until is None):\n        parse_until = []\n    nodelist = self.create_nodelist()\n    while self.tokens:\n        token = self.next_token()\n        if (token.token_type == 0):\n            self.extend_nodelist(nodelist, TextNode(token.contents), token)\n        elif (token.token_type == 1):\n            if (not token.contents):\n                self.empty_variable(token)\n            filter_expression = self.compile_filter(token.contents)\n            var_node = self.create_variable_node(filter_expression)\n            self.extend_nodelist(nodelist, var_node, token)\n        elif (token.token_type == 2):\n            try:\n                command = token.contents.split()[0]\n            except IndexError:\n                self.empty_block_tag(token)\n            if (command in parse_until):\n                self.prepend_token(token)\n                return nodelist\n            self.enter_command(command, token)\n            try:\n                compile_func = self.tags[command]\n            except KeyError:\n                self.invalid_block_tag(token, command, parse_until)\n            try:\n                compiled_result = compile_func(self, token)\n            except TemplateSyntaxError as e:\n                if (not self.compile_function_error(token, e)):\n                    raise\n            self.extend_nodelist(nodelist, compiled_result, token)\n            self.exit_command()\n    if parse_until:\n        self.unclosed_block_tag(parse_until)\n    return nodelist\n", "label": 1}
{"function": "\n\ndef extend_schema(schema, documentAST=None):\n    \"Produces a new schema given an existing schema and a document which may\\n    contain GraphQL type extensions and definitions. The original schema will\\n    remain unaltered.\\n\\n    Because a schema represents a graph of references, a schema cannot be\\n    extended without effectively making an entire copy. We do not know until it's\\n    too late if subgraphs remain unchanged.\\n\\n    This algorithm copies the provided schema, applying extensions while\\n    producing the copy. The original schema remains unaltered.\"\n    assert isinstance(schema, GraphQLSchema), 'Must provide valid GraphQLSchema'\n    assert (documentAST and isinstance(documentAST, ast.Document)), 'Must provide valid Document AST'\n    type_definition_map = {\n        \n    }\n    type_extensions_map = defaultdict(list)\n    for _def in documentAST.definitions:\n        if isinstance(_def, (ast.ObjectTypeDefinition, ast.InterfaceTypeDefinition, ast.EnumTypeDefinition, ast.UnionTypeDefinition, ast.ScalarTypeDefinition, ast.InputObjectTypeDefinition)):\n            type_name = _def.name.value\n            if schema.get_type(type_name):\n                raise GraphQLError(('Type \"{}\" already exists in the schema. It cannot also ' + 'be defined in this type definition.').format(type_name), [_def])\n            type_definition_map[type_name] = _def\n        elif isinstance(_def, ast.TypeExtensionDefinition):\n            extended_type_name = _def.definition.name.value\n            existing_type = schema.get_type(extended_type_name)\n            if (not existing_type):\n                raise GraphQLError(('Cannot extend type \"{}\" because it does not ' + 'exist in the existing schema.').format(extended_type_name), [_def.definition])\n            if (not isinstance(existing_type, GraphQLObjectType)):\n                raise GraphQLError('Cannot extend non-object type \"{}\".'.format(extended_type_name), [_def.definition])\n            type_extensions_map[extended_type_name].append(_def)\n\n    def get_type_from_def(type_def):\n        type = _get_named_type(type_def.name)\n        assert type, 'Invalid schema'\n        return type\n\n    def get_type_from_AST(astNode):\n        type = _get_named_type(astNode.name.value)\n        if (not type):\n            raise GraphQLError(('Unknown type: \"{}\". Ensure that this type exists ' + 'either in the original schema, or is added in a type definition.').format(astNode.name.value), [astNode])\n        return type\n\n    def _get_named_type(typeName):\n        cached_type_def = type_def_cache.get(typeName)\n        if cached_type_def:\n            return cached_type_def\n        existing_type = schema.get_type(typeName)\n        if existing_type:\n            type_def = extend_type(existing_type)\n            type_def_cache[typeName] = type_def\n            return type_def\n        type_ast = type_definition_map.get(typeName)\n        if type_ast:\n            type_def = build_type(type_ast)\n            type_def_cache[typeName] = type_def\n            return type_def\n\n    def extend_type(type):\n        if isinstance(type, GraphQLObjectType):\n            return extend_object_type(type)\n        if isinstance(type, GraphQLInterfaceType):\n            return extend_interface_type(type)\n        if isinstance(type, GraphQLUnionType):\n            return extend_union_type(type)\n        return type\n\n    def extend_object_type(type):\n        return GraphQLObjectType(name=type.name, description=type.description, interfaces=(lambda : extend_implemented_interfaces(type)), fields=(lambda : extend_field_map(type)))\n\n    def extend_interface_type(type):\n        return GraphQLInterfaceType(name=type.name, description=type.description, fields=(lambda : extend_field_map(type)), resolve_type=raise_client_schema_execution_error)\n\n    def extend_union_type(type):\n        return GraphQLUnionType(name=type.name, description=type.description, types=list(map(get_type_from_def, type.get_possible_types())), resolve_type=raise_client_schema_execution_error)\n\n    def extend_implemented_interfaces(type):\n        interfaces = list(map(get_type_from_def, type.get_interfaces()))\n        extensions = type_extensions_map[type.name]\n        for extension in extensions:\n            for namedType in extension.definition.interfaces:\n                interface_name = namedType.name.value\n                if any([(_def.name == interface_name) for _def in interfaces]):\n                    raise GraphQLError(('Type \"{}\" already implements \"{}\". ' + 'It cannot also be implemented in this type extension.').format(type.name, interface_name), [namedType])\n                interfaces.append(get_type_from_AST(namedType))\n        return interfaces\n\n    def extend_field_map(type):\n        new_field_map = OrderedDict()\n        old_field_map = type.get_fields()\n        for (field_name, field) in old_field_map.items():\n            new_field_map[field_name] = GraphQLField(extend_field_type(field.type), description=field.description, deprecation_reason=field.deprecation_reason, args={arg.name: arg for arg in field.args}, resolver=raise_client_schema_execution_error)\n        extensions = type_extensions_map[type.name]\n        for extension in extensions:\n            for field in extension.definition.fields:\n                field_name = field.name.value\n                if (field_name in old_field_map):\n                    raise GraphQLError(('Field \"{}.{}\" already exists in the ' + 'schema. It cannot also be defined in this type extension.').format(type.name, field_name), [field])\n                new_field_map[field_name] = GraphQLField(build_field_type(field.type), args=build_input_values(field.arguments), resolver=raise_client_schema_execution_error)\n        return new_field_map\n\n    def extend_field_type(type):\n        if isinstance(type, GraphQLList):\n            return GraphQLList(extend_field_type(type.of_type))\n        if isinstance(type, GraphQLNonNull):\n            return GraphQLNonNull(extend_field_type(type.of_type))\n        return get_type_from_def(type)\n\n    def build_type(type_ast):\n        _type_build = {\n            ast.ObjectTypeDefinition: build_object_type,\n            ast.InterfaceTypeDefinition: build_interface_type,\n            ast.UnionTypeDefinition: build_union_type,\n            ast.ScalarTypeDefinition: build_scalar_type,\n            ast.EnumTypeDefinition: build_enum_type,\n            ast.InputObjectTypeDefinition: build_input_object_type,\n        }\n        func = _type_build.get(type(type_ast))\n        if func:\n            return func(type_ast)\n\n    def build_object_type(type_ast):\n        return GraphQLObjectType(type_ast.name.value, interfaces=(lambda : build_implemented_interfaces(type_ast)), fields=(lambda : build_field_map(type_ast)))\n\n    def build_interface_type(type_ast):\n        return GraphQLInterfaceType(type_ast.name.value, fields=(lambda : build_field_map(type_ast)), resolve_type=raise_client_schema_execution_error)\n\n    def build_union_type(type_ast):\n        return GraphQLUnionType(type_ast.name.value, types=list(map(get_type_from_AST, type_ast.types)), resolve_type=raise_client_schema_execution_error)\n\n    def build_scalar_type(type_ast):\n        return GraphQLScalarType(type_ast.name.value, serialize=(lambda *args, **kwargs: None), parse_value=(lambda *args, **kwargs: False), parse_literal=(lambda *args, **kwargs: False))\n\n    def build_enum_type(type_ast):\n        return GraphQLEnumType(type_ast.name.value, values={v.name.value: GraphQLEnumValue() for v in type_ast.values})\n\n    def build_input_object_type(type_ast):\n        return GraphQLInputObjectType(type_ast.name.value, fields=(lambda : build_input_values(type_ast.fields, GraphQLInputObjectField)))\n\n    def build_implemented_interfaces(type_ast):\n        return list(map(get_type_from_AST, type_ast.interfaces))\n\n    def build_field_map(type_ast):\n        return {field.name.value: GraphQLField(build_field_type(field.type), args=build_input_values(field.arguments), resolver=raise_client_schema_execution_error) for field in type_ast.fields}\n\n    def build_input_values(values, input_type=GraphQLArgument):\n        input_values = OrderedDict()\n        for value in values:\n            type = build_field_type(value.type)\n            input_values[value.name.value] = input_type(type, default_value=value_from_ast(value.default_value, type))\n        return input_values\n\n    def build_field_type(type_ast):\n        if isinstance(type_ast, ast.ListType):\n            return GraphQLList(build_field_type(type_ast.type))\n        if isinstance(type_ast, ast.NonNullType):\n            return GraphQLNonNull(build_field_type(type_ast.type))\n        return get_type_from_AST(type_ast)\n    if ((not type_extensions_map) and (not type_definition_map)):\n        return schema\n    type_def_cache = {\n        'String': GraphQLString,\n        'Int': GraphQLInt,\n        'Float': GraphQLFloat,\n        'Boolean': GraphQLBoolean,\n        'ID': GraphQLID,\n    }\n    query_type = get_type_from_def(schema.get_query_type())\n    existing_mutation_type = schema.get_mutation_type()\n    mutationType = ((existing_mutation_type and get_type_from_def(existing_mutation_type)) or None)\n    existing_subscription_type = schema.get_subscription_type()\n    subscription_type = ((existing_subscription_type and get_type_from_def(existing_subscription_type)) or None)\n    for (typeName, _def) in schema.get_type_map().items():\n        get_type_from_def(_def)\n    for (typeName, _def) in type_definition_map.items():\n        get_type_from_AST(_def)\n    return GraphQLSchema(query=query_type, mutation=mutationType, subscription=subscription_type, directives=schema.get_directives())\n", "label": 1}
{"function": "\n\ndef _Dynamic_Get(self, get_request, get_response):\n    txid = None\n    if get_request.has_transaction():\n        txid = get_request.transaction().handle()\n        txdata = self.__transactions[txid]\n        assert (txdata.thread_id == thread.get_ident()), 'Transactions are single-threaded.'\n        keys = [(k, k.Encode()) for k in get_request.key_list()]\n        new_request = datastore_pb.GetRequest()\n        for (key, enckey) in keys:\n            if (enckey not in txdata.entities):\n                new_request.add_key().CopyFrom(key)\n    else:\n        new_request = get_request\n    if (new_request.key_size() > 0):\n        super(RemoteDatastoreStub, self).MakeSyncCall('datastore_v3', 'Get', new_request, get_response)\n    if (txid is not None):\n        newkeys = new_request.key_list()\n        entities = get_response.entity_list()\n        for (key, entity) in zip(newkeys, entities):\n            entity_hash = None\n            if entity.has_entity():\n                entity_hash = sha.new(entity.entity().Encode()).digest()\n            txdata.preconditions[key.Encode()] = (key, entity_hash)\n        new_response = datastore_pb.GetResponse()\n        it = iter(get_response.entity_list())\n        for (key, enckey) in keys:\n            if (enckey in txdata.entities):\n                cached_entity = txdata.entities[enckey][1]\n                if cached_entity:\n                    new_response.add_entity().mutable_entity().CopyFrom(cached_entity)\n                else:\n                    new_response.add_entity()\n            else:\n                new_entity = it.next()\n                if new_entity.has_entity():\n                    assert (new_entity.entity().key() == key)\n                    new_response.add_entity().CopyFrom(new_entity)\n                else:\n                    new_response.add_entity()\n        get_response.CopyFrom(new_response)\n", "label": 1}
{"function": "\n\ndef test_schema(schema_app):\n    app_client = schema_app.app.test_client()\n    headers = {\n        'Content-type': 'application/json',\n    }\n    empty_request = app_client.post('/v1.0/test_schema', headers=headers, data=json.dumps({\n        \n    }))\n    assert (empty_request.status_code == 400)\n    assert (empty_request.content_type == 'application/problem+json')\n    empty_request_response = json.loads(empty_request.data.decode())\n    assert (empty_request_response['title'] == 'Bad Request')\n    assert empty_request_response['detail'].startswith(\"'image_version' is a required property\")\n    bad_type = app_client.post('/v1.0/test_schema', headers=headers, data=json.dumps({\n        'image_version': 22,\n    }))\n    assert (bad_type.status_code == 400)\n    assert (bad_type.content_type == 'application/problem+json')\n    bad_type_response = json.loads(bad_type.data.decode())\n    assert (bad_type_response['title'] == 'Bad Request')\n    assert bad_type_response['detail'].startswith(\"22 is not of type 'string'\")\n    good_request = app_client.post('/v1.0/test_schema', headers=headers, data=json.dumps({\n        'image_version': 'version',\n    }))\n    assert (good_request.status_code == 200)\n    good_request_response = json.loads(good_request.data.decode())\n    assert (good_request_response['image_version'] == 'version')\n    good_request_extra = app_client.post('/v1.0/test_schema', headers=headers, data=json.dumps({\n        'image_version': 'version',\n        'extra': 'stuff',\n    }))\n    assert (good_request_extra.status_code == 200)\n    good_request_extra_response = json.loads(good_request.data.decode())\n    assert (good_request_extra_response['image_version'] == 'version')\n    wrong_type = app_client.post('/v1.0/test_schema', headers=headers, data=json.dumps(42))\n    assert (wrong_type.status_code == 400)\n    assert (wrong_type.content_type == 'application/problem+json')\n    wrong_type_response = json.loads(wrong_type.data.decode())\n    assert (wrong_type_response['title'] == 'Bad Request')\n    assert wrong_type_response['detail'].startswith(\"42 is not of type 'object'\")\n", "label": 1}
{"function": "\n\ndef __init__(self, subject, time, event, actions, content_type, using=None, skip=None):\n    self.subject = subject\n    self.time = time\n    self.event = event\n    self.content_type = content_type\n    self.content_type_field = None\n    self.actions = []\n    self.append(actions)\n    self.using = using\n    if self.using:\n        self.connection = connections[self.using]\n    else:\n        self.connection = connection\n    try:\n        from django.contrib.contenttypes.fields import GenericRelation\n    except ImportError:\n        from django.contrib.contenttypes.generic import GenericRelation\n    if isinstance(subject, models.ManyToManyField):\n        self.model = None\n        self.db_table = subject.m2m_db_table()\n        self.fields = [(subject.m2m_column_name(), ''), (subject.m2m_reverse_name(), '')]\n    elif isinstance(subject, GenericRelation):\n        self.model = None\n        self.db_table = remote_field_model(subject)._meta.db_table\n        self.fields = [(k.attname, k.db_type(connection=self.connection)) for (k, v) in get_fields_with_model(remote_field_model(subject), remote_field_model(subject)._meta) if (not v)]\n        self.content_type_field = (subject.content_type_field_name + '_id')\n    elif isinstance(subject, models.ForeignKey):\n        self.model = subject.model\n        self.db_table = self.model._meta.db_table\n        skip = (skip or (() + getattr(self.model, 'denorm_always_skip', ())))\n        self.fields = [(k.attname, k.db_type(connection=self.connection)) for (k, v) in get_fields_with_model(subject.model, self.model._meta) if ((not v) and (k.attname not in skip))]\n    elif hasattr(subject, '_meta'):\n        self.model = subject\n        self.db_table = self.model._meta.db_table\n        skip = (skip or (() + getattr(self.model, 'denorm_always_skip', ())))\n        self.fields = []\n        try:\n            from django.db.models.fields.related import ForeignObjectRel\n        except:\n            from django.db.models.related import RelatedObject as ForeignObjectRel\n        for (k, v) in get_fields_with_model(subject, self.model._meta):\n            if isinstance(k, ForeignObjectRel):\n                pass\n            elif ((not v) and (k.attname not in skip)):\n                self.fields.append((k.attname, k.db_type(connection=self.connection)))\n    else:\n        raise NotImplementedError\n", "label": 1}
{"function": "\n\ndef parse_args():\n    \"Parses the command line for user run-time settings.\\n\\n    Returns:\\n        dictionary with the following keys::\\n\\n            user: string user name to search for tweets from\\n            search: string to search for tweets with\\n            max_tweets: integer max number of tweets to display\\n            stream: boolean to stream the tweets instead of printing once\\n            date: boolean to display the date ahead of the tweet\\n            time: boolean to display the time ahead of the tweet\\n            json: boolean to display the tweet's json structure instead\\n            spam: boolean to skip the anti spam measures\\n    \"\n    settings = {\n        'max': 3,\n        'search': [],\n        'stream': False,\n        'user': None,\n        'date': False,\n        'time': False,\n        'json': False,\n        'spam': False,\n    }\n    max_set = False\n    get_next = False\n    for arg in sys.argv[1:]:\n        if get_next:\n            settings[get_next] = arg\n            get_next = False\n            continue\n        arg_copy = arg\n        while arg.startswith('-'):\n            arg = ''.join(arg[1:])\n        if (arg == 's'):\n            settings['stream'] = True\n        elif (arg == 'u'):\n            get_next = 'user'\n        elif (arg == 'd'):\n            settings['date'] = True\n        elif (arg == 't'):\n            settings['time'] = True\n        elif (arg == 'j'):\n            settings['json'] = True\n        elif (arg == 'n'):\n            settings['spam'] = True\n        elif (arg == 'h'):\n            raise SystemExit(__doc__.strip())\n        elif (not max_set):\n            try:\n                settings['max'] = int(arg)\n                max_set = True\n            except (TypeError, ValueError):\n                settings['search'].append(arg_copy)\n        else:\n            settings['search'].append(arg_copy)\n    if (settings['stream'] and (not (settings['search'] or settings['user']))):\n        raise SystemExit((__doc__ + 'Streaming requires search phrases or the -u flag'))\n    return settings\n", "label": 1}
{"function": "\n\ndef save(self, path=None, encoding=None, fallback_encoding=None):\n    '\\n        Save the editor content to a file.\\n\\n        :param path: optional file path. Set it to None to save using the\\n                     current path (save), set a new path to save as.\\n        :param encoding: optional encoding, will use the current\\n                         file encoding if None.\\n        :param fallback_encoding: Fallback encoding to use in case of encoding\\n            error. None to use the locale preferred encoding\\n\\n        '\n    if ((not self.editor.dirty) and ((encoding is None) and (encoding == self.encoding)) and ((path is None) and (path == self.path))):\n        return\n    if (fallback_encoding is None):\n        fallback_encoding = locale.getpreferredencoding()\n    _logger().log(5, 'saving %r with %r encoding', path, encoding)\n    if (path is None):\n        if self.path:\n            path = self.path\n        else:\n            _logger().debug('failed to save file, path argument cannot be None if FileManager.path is also None')\n            return False\n    if (encoding is None):\n        encoding = self._encoding\n    self.saving = True\n    self.editor.text_saving.emit(str(path))\n    try:\n        st_mode = os.stat(path).st_mode\n    except (ImportError, TypeError, AttributeError, OSError):\n        st_mode = None\n    if self.safe_save:\n        tmp_path = (path + '~')\n    else:\n        tmp_path = path\n    try:\n        with open(tmp_path, 'wb') as file:\n            file.write(self._get_text(encoding))\n    except UnicodeEncodeError:\n        with open(tmp_path, 'wb') as file:\n            file.write(self._get_text(fallback_encoding))\n    except (IOError, OSError) as e:\n        self._rm(tmp_path)\n        self.saving = False\n        self.editor.text_saved.emit(str(path))\n        raise e\n    else:\n        Cache().set_file_encoding(path, encoding)\n        self._encoding = encoding\n        if self.safe_save:\n            self._rm(path)\n            os.rename(tmp_path, path)\n            self._rm(tmp_path)\n        self.editor.document().setModified(False)\n        self._path = os.path.normpath(path)\n        self.editor.text_saved.emit(str(path))\n        self.saving = False\n        _logger().debug('file saved: %s', path)\n        self._check_for_readonly()\n        if st_mode:\n            try:\n                os.chmod(path, st_mode)\n            except (ImportError, TypeError, AttributeError):\n                pass\n", "label": 1}
{"function": "\n\ndef clean(self):\n    super(MenuItemFormMixin, self).clean()\n    content_type = self.cleaned_data['content_type']\n    object_id = self.cleaned_data['object_id']\n    if ((content_type and (not object_id)) or ((not content_type) and object_id)):\n        raise forms.ValidationError(\"Both 'Content type' and 'Object id' must be specified to use generic relationship\")\n    if (content_type and object_id):\n        try:\n            obj = content_type.get_object_for_this_type(pk=object_id)\n        except ObjectDoesNotExist as e:\n            raise forms.ValidationError(str(e))\n        try:\n            obj.get_absolute_url()\n        except AttributeError as e:\n            raise forms.ValidationError(str(e))\n    if (('is_enabled' in self.cleaned_data) and self.cleaned_data['is_enabled'] and ('link' in self.cleaned_data) and self.cleaned_data['link'].startswith('^')):\n        raise forms.ValidationError('Menu items with regular expression URLs must be disabled.')\n    return self.cleaned_data\n", "label": 1}
{"function": "\n\ndef api_public_add_followup(self):\n    try:\n        ticket = Ticket.objects.get(id=self.request.POST.get('ticket', False))\n    except Ticket.DoesNotExist:\n        return api_return(STATUS_ERROR, 'Invalid ticket ID')\n    message = self.request.POST.get('message', None)\n    public = self.request.POST.get('public', 'n')\n    if (public not in ['y', 'n']):\n        return api_return(STATUS_ERROR, \"Invalid 'public' flag\")\n    if (not message):\n        return api_return(STATUS_ERROR, 'Blank message')\n    f = FollowUp(ticket=ticket, date=timezone.now(), comment=message, user=self.request.user, title='Comment Added')\n    if public:\n        f.public = True\n    f.save()\n    context = safe_template_context(ticket)\n    context['comment'] = f.comment\n    messages_sent_to = []\n    if (public and ticket.submitter_email):\n        send_templated_mail('updated_submitter', context, recipients=ticket.submitter_email, sender=ticket.queue.from_address, fail_silently=True)\n        messages_sent_to.append(ticket.submitter_email)\n    if public:\n        for cc in ticket.ticketcc_set.all():\n            if (cc.email_address not in messages_sent_to):\n                send_templated_mail('updated_submitter', context, recipients=cc.email_address, sender=ticket.queue.from_address, fail_silently=True)\n                messages_sent_to.append(cc.email_address)\n    if (ticket.queue.updated_ticket_cc and (ticket.queue.updated_ticket_cc not in messages_sent_to)):\n        send_templated_mail('updated_cc', context, recipients=ticket.queue.updated_ticket_cc, sender=ticket.queue.from_address, fail_silently=True)\n        messages_sent_to.append(ticket.queue.updated_ticket_cc)\n    if (ticket.assigned_to and (self.request.user != ticket.assigned_to) and ticket.assigned_to.usersettings.settings.get('email_on_ticket_apichange', False) and ticket.assigned_to.email and (ticket.assigned_to.email not in messages_sent_to)):\n        send_templated_mail('updated_owner', context, recipients=ticket.assigned_to.email, sender=ticket.queue.from_address, fail_silently=True)\n    ticket.save()\n    return api_return(STATUS_OK)\n", "label": 1}
{"function": "\n\ndef run(self, argv):\n    'Equivalent to the main program for the application.\\n\\n        :param argv: input arguments and options\\n        :paramtype argv: list of str\\n        '\n    try:\n        index = 0\n        command_pos = (- 1)\n        help_pos = (- 1)\n        help_command_pos = (- 1)\n        for arg in argv:\n            if (arg == 'bash-completion'):\n                self._bash_completion()\n                return 0\n            if (arg in self.commands[self.api_version]):\n                if (command_pos == (- 1)):\n                    command_pos = index\n            elif (arg in ('-h', '--help')):\n                if (help_pos == (- 1)):\n                    help_pos = index\n            elif (arg == 'help'):\n                if (help_command_pos == (- 1)):\n                    help_command_pos = index\n            index = (index + 1)\n        if ((command_pos > (- 1)) and (help_pos > command_pos)):\n            argv = ['help', argv[command_pos]]\n        if ((help_command_pos > (- 1)) and (command_pos == (- 1))):\n            argv[help_command_pos] = '--help'\n        (self.options, remainder) = self.parser.parse_known_args(argv)\n        self.configure_logging()\n        self.interactive_mode = (not remainder)\n        self.initialize_app(remainder)\n    except Exception as err:\n        if (self.options.verbose_level >= self.DEBUG_LEVEL):\n            self.log.exception(unicode(err))\n            raise\n        else:\n            self.log.error(unicode(err))\n        return 1\n    result = 1\n    if self.interactive_mode:\n        _argv = [sys.argv[0]]\n        sys.argv = _argv\n        result = self.interact()\n    else:\n        result = self.run_subcommand(remainder)\n    return result\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    formfield_callback = attrs.pop('formfield_callback', None)\n    try:\n        parents = [b for b in bases if issubclass(b, ModelForm)]\n    except NameError:\n        parents = None\n    declared_fields = get_declared_fields(bases, attrs, False)\n    new_class = super(ModelFormMetaclass, cls).__new__(cls, name, bases, attrs)\n    if (not parents):\n        return new_class\n    if ('media' not in attrs):\n        new_class.media = media_property(new_class)\n    opts = new_class._meta = ModelFormOptions(getattr(new_class, 'Meta', None))\n    for opt in ['fields', 'exclude', 'localized_fields']:\n        value = getattr(opts, opt)\n        if (isinstance(value, six.string_types) and (value != ALL_FIELDS)):\n            msg = (\"%(model)s.Meta.%(opt)s cannot be a string. Did you mean to type: ('%(value)s',)?\" % {\n                'model': new_class.__name__,\n                'opt': opt,\n                'value': value,\n            })\n            raise TypeError(msg)\n    if opts.model:\n        if ((opts.fields is None) and (opts.exclude is None)):\n            warnings.warn((\"Creating a ModelForm without either the 'fields' attribute or the 'exclude' attribute is deprecated - form %s needs updating\" % name), PendingDeprecationWarning, stacklevel=2)\n        if (opts.fields == ALL_FIELDS):\n            opts.fields = None\n        fields = fields_for_model(opts.model, opts.fields, opts.exclude, opts.widgets, formfield_callback, opts.localized_fields, opts.labels, opts.help_texts, opts.error_messages)\n        none_model_fields = [k for (k, v) in six.iteritems(fields) if (not v)]\n        missing_fields = (set(none_model_fields) - set(declared_fields.keys()))\n        if missing_fields:\n            message = 'Unknown field(s) (%s) specified for %s'\n            message = (message % (', '.join(missing_fields), opts.model.__name__))\n            raise FieldError(message)\n        fields.update(declared_fields)\n    else:\n        fields = declared_fields\n    new_class.declared_fields = declared_fields\n    new_class.base_fields = fields\n    return new_class\n", "label": 1}
{"function": "\n\ndef process_modules_and_forms_sheet(rows, app):\n    '\\n    Modify the translations and media references for the modules and forms in\\n    the given app as per the data provided in rows.\\n    This does not save the changes to the database.\\n    :param rows:\\n    :param app:\\n    :return:  Returns a list of message tuples. The first item in each tuple is\\n    a function like django.contrib.messages.error, and the second is a string.\\n    '\n    msgs = []\n    for row in rows:\n        identifying_text = row.get('sheet_name', '').split('_')\n        if (len(identifying_text) not in (1, 2)):\n            msgs.append((messages.error, ('Invalid sheet_name \"%s\", skipping row.' % row.get('sheet_name', ''))))\n            continue\n        module_index = (int(identifying_text[0].replace('module', '')) - 1)\n        try:\n            document = app.get_module(module_index)\n        except ModuleNotFoundException:\n            msgs.append((messages.error, ('Invalid module in row \"%s\", skipping row.' % row.get('sheet_name'))))\n            continue\n        if (len(identifying_text) == 2):\n            form_index = (int(identifying_text[1].replace('form', '')) - 1)\n            try:\n                document = document.get_form(form_index)\n            except FormNotFoundException:\n                msgs.append((messages.error, ('Invalid form in row \"%s\", skipping row.' % row.get('sheet_name'))))\n                continue\n        for lang in app.langs:\n            translation = row[('default_%s' % lang)]\n            if translation:\n                document.name[lang] = translation\n            elif (lang in document.name):\n                del document.name[lang]\n        if (has_at_least_one_translation(row, 'label_for_cases', app.langs) and hasattr(document, 'case_label')):\n            for lang in app.langs:\n                translation = row[('label_for_cases_%s' % lang)]\n                if translation:\n                    document.case_label[lang] = translation\n                elif (lang in document.case_label):\n                    del document.case_label[lang]\n        for lang in app.langs:\n            document.set_icon(lang, row.get(('icon_filepath_%s' % lang), ''))\n            document.set_audio(lang, row.get(('audio_filepath_%s' % lang), ''))\n    return msgs\n", "label": 1}
{"function": "\n\n@expose(help='Get example.com information')\ndef info(self):\n    if (not self.app.pargs.site_name):\n        try:\n            while (not self.app.pargs.site_name):\n                self.app.pargs.site_name = input('Enter site name : ').strip()\n        except IOError as e:\n            Log.error(self, 'could not input site name')\n    self.app.pargs.site_name = self.app.pargs.site_name.strip()\n    (ee_domain, ee_www_domain) = ValidateDomain(self.app.pargs.site_name)\n    ee_db_name = ''\n    ee_db_user = ''\n    ee_db_pass = ''\n    hhvm = ''\n    if (not check_domain_exists(self, ee_domain)):\n        Log.error(self, 'site {0} does not exist'.format(ee_domain))\n    if os.path.isfile('/etc/nginx/sites-available/{0}'.format(ee_domain)):\n        siteinfo = getSiteInfo(self, ee_domain)\n        sitetype = siteinfo.site_type\n        cachetype = siteinfo.cache_type\n        ee_site_webroot = siteinfo.site_path\n        access_log = (ee_site_webroot + '/logs/access.log')\n        error_log = (ee_site_webroot + '/logs/error.log')\n        ee_db_name = siteinfo.db_name\n        ee_db_user = siteinfo.db_user\n        ee_db_pass = siteinfo.db_password\n        ee_db_host = siteinfo.db_host\n        if (sitetype != 'html'):\n            hhvm = ('enabled' if siteinfo.is_hhvm else 'disabled')\n        if (sitetype == 'proxy'):\n            access_log = '/var/log/nginx/{0}.access.log'.format(ee_domain)\n            error_log = '/var/log/nginx/{0}.error.log'.format(ee_domain)\n            ee_site_webroot = ''\n        php_version = siteinfo.php_version\n        pagespeed = ('enabled' if siteinfo.is_pagespeed else 'disabled')\n        ssl = ('enabled' if siteinfo.is_ssl else 'disabled')\n        if (ssl == 'enabled'):\n            sslprovider = 'Lets Encrypt'\n            sslexpiry = str(SSL.getExpirationDate(self, ee_domain))\n        else:\n            sslprovider = ''\n            sslexpiry = ''\n        data = dict(domain=ee_domain, webroot=ee_site_webroot, accesslog=access_log, errorlog=error_log, dbname=ee_db_name, dbuser=ee_db_user, php_version=php_version, dbpass=ee_db_pass, hhvm=hhvm, pagespeed=pagespeed, ssl=ssl, sslprovider=sslprovider, sslexpiry=sslexpiry, type=(((sitetype + ' ') + cachetype) + ' ({0})'.format(('enabled' if siteinfo.is_enabled else 'disabled'))))\n        self.app.render(data, 'siteinfo.mustache')\n    else:\n        Log.error(self, 'nginx configuration file does not exist'.format(ee_domain))\n", "label": 1}
{"function": "\n\ndef _minmax_impl(a_gpu, axis, min_or_max, stream=None, keepdims=False):\n    ' Returns both max and argmax (min/argmin) along an axis.'\n    assert (len(a_gpu.shape) < 3)\n    if iscomplextype(a_gpu.dtype):\n        raise ValueError('Cannot compute min/max of complex values')\n    if ((axis is None) or (len(a_gpu.shape) <= 1)):\n        out_shape = ((1,) * len(a_gpu.shape))\n        if (min_or_max == 'max'):\n            return (gpuarray.max(a_gpu).reshape(out_shape), None)\n        else:\n            return (gpuarray.min(a_gpu).reshape(out_shape), None)\n    elif (axis < 0):\n        axis += 2\n    assert (axis in (0, 1))\n    global _global_cublas_allocator\n    alloc = _global_cublas_allocator\n    (n, m) = (a_gpu.shape if a_gpu.flags.c_contiguous else (a_gpu.shape[1], a_gpu.shape[0]))\n    (col_kernel, row_kernel) = _get_minmax_kernel(a_gpu.dtype, min_or_max)\n    if (((axis == 0) and a_gpu.flags.c_contiguous) or ((axis == 1) and a_gpu.flags.f_contiguous)):\n        if keepdims:\n            out_shape = ((1, m) if (axis == 0) else (m, 1))\n        else:\n            out_shape = (m,)\n        target = gpuarray.empty(out_shape, dtype=a_gpu.dtype, allocator=alloc)\n        idx = gpuarray.empty(out_shape, dtype=np.uint32, allocator=alloc)\n        col_kernel(a_gpu, target, idx, np.uint32(m), np.uint32(n), block=(32, 1, 1), grid=(m, 1, 1), stream=stream)\n    else:\n        if keepdims:\n            out_shape = ((1, n) if (axis == 0) else (n, 1))\n        else:\n            out_shape = (n,)\n        target = gpuarray.empty(out_shape, dtype=a_gpu, allocator=alloc)\n        idx = gpuarray.empty(out_shape, dtype=np.uint32, allocator=alloc)\n        row_kernel(a_gpu, target, idx, np.uint32(m), np.uint32(n), block=(32, 1, 1), grid=(n, 1, 1), stream=stream)\n    return (target, idx)\n", "label": 1}
{"function": "\n\n@define_check\ndef check_copy_relations(output):\n    from cms.plugin_pool import plugin_pool\n    from cms.extensions import extension_pool\n    from cms.extensions.models import BaseExtension\n    from cms.models.pluginmodel import CMSPlugin\n    c_to_s = (lambda klass: ('%s.%s' % (klass.__module__, klass.__name__)))\n\n    def get_class(method_name, model):\n        for cls in inspect.getmro(model):\n            if (method_name in cls.__dict__):\n                return cls\n        return None\n    with output.section('Presence of \"copy_relations\"') as section:\n        plugin_pool.discover_plugins()\n        for plugin in plugin_pool.plugins.values():\n            plugin_class = plugin.model\n            if ((get_class('copy_relations', plugin_class) is not CMSPlugin) or (plugin_class is CMSPlugin)):\n                continue\n            for rel in plugin_class._meta.many_to_many:\n                section.warn(('%s has a many-to-many relation to %s,\\n    but no \"copy_relations\" method defined.' % (c_to_s(plugin_class), c_to_s(rel.model))))\n            for rel in plugin_class._get_related_objects():\n                if ((rel.model != CMSPlugin) and (not issubclass(rel.model, plugin.model)) and (rel.model != AliasPluginModel)):\n                    section.warn(('%s has a foreign key from %s,\\n    but no \"copy_relations\" method defined.' % (c_to_s(plugin_class), c_to_s(rel.model))))\n        for extension in chain(extension_pool.page_extensions, extension_pool.title_extensions):\n            if (get_class('copy_relations', extension) is not BaseExtension):\n                continue\n            for rel in extension._meta.many_to_many:\n                if DJANGO_1_8:\n                    section.warn(('%s has a many-to-many relation to %s,\\n    but no \"copy_relations\" method defined.' % (c_to_s(extension), c_to_s(rel.related.model))))\n                else:\n                    section.warn(('%s has a many-to-many relation to %s,\\n    but no \"copy_relations\" method defined.' % (c_to_s(extension), c_to_s(rel.remote_field.model))))\n            for rel in extension._get_related_objects():\n                if (rel.model != extension):\n                    section.warn(('%s has a foreign key from %s,\\n    but no \"copy_relations\" method defined.' % (c_to_s(extension), c_to_s(rel.model))))\n        if (not section.warnings):\n            section.finish_success('All plugins and page/title extensions have \"copy_relations\" method if needed.')\n        else:\n            section.finish_success('Some plugins or page/title extensions do not define a \"copy_relations\" method.\\nThis might lead to data loss when publishing or copying plugins/extensions.\\nSee https://django-cms.readthedocs.org/en/latest/extending_cms/custom_plugins.html#handling-relations or https://django-cms.readthedocs.org/en/latest/extending_cms/extending_page_title.html#handling-relations.')\n", "label": 1}
{"function": "\n\ndef __init__(self, bits, length=None):\n    if isinstance(bits, numpy.ndarray):\n        if (bits.dtype == numpy.bool):\n            if bits.flags.writeable:\n                bits = bits.copy()\n                bits.writeable = False\n        else:\n            bits = numpy.array(bits, bool)\n            bits.flags.writeable = False\n        hash_value = None\n    elif isinstance(bits, int):\n        if (length is None):\n            length = bits.bit_length()\n        else:\n            assert (length >= bits.bit_length())\n        if (bits < 0):\n            bits &= ((1 << length) - 1)\n        bit_values = []\n        for _ in range(length):\n            bit_values.append((bits % 2))\n            bits >>= 1\n        bit_values.reverse()\n        bits = numpy.array(bit_values, bool)\n        bits.flags.writeable = False\n        hash_value = None\n    elif isinstance(bits, BitString):\n        (bits, hash_value) = (bits._bits, bits._hash)\n    elif isinstance(bits, str):\n        bit_list = []\n        for char in bits:\n            if (char == '1'):\n                bit_list.append(True)\n            elif (char == '0'):\n                bit_list.append(False)\n            elif (char == '#'):\n                raise ValueError('BitStrings cannot contain wildcards. Did you mean to create a BitCondition?')\n            else:\n                raise ValueError(('Invalid character: ' + repr(char)))\n        bits = numpy.array(bit_list, bool)\n        bits.flags.writeable = False\n        hash_value = None\n    else:\n        bits = numpy.array(bits, bool)\n        bits.flags.writeable = False\n        hash_value = None\n    if ((length is not None) and (len(bits) != length)):\n        raise ValueError('Sequence has incorrect length.')\n    super().__init__(bits, hash_value)\n", "label": 1}
{"function": "\n\ndef startElementNS(self, ns_and_name, qname, attrs):\n    filename = self.sourceFilename\n    lineNumber = self.locator.getLineNumber()\n    columnNumber = self.locator.getColumnNumber()\n    (ns, name) = ns_and_name\n    if (ns == nevow.namespace):\n        if (name == 'invisible'):\n            name = ''\n        elif (name == 'slot'):\n            try:\n                default = attrs[(None, 'default')]\n            except KeyError:\n                default = None\n            el = slot(attrs[(None, 'name')], default=default, filename=filename, lineNumber=lineNumber, columnNumber=columnNumber)\n            self.stack.append(el)\n            self.current.append(el)\n            self.current = el.children\n            return\n    attrs = dict(attrs)\n    specials = {\n        \n    }\n    attributes = self.attributeList\n    directives = self.directiveMapping\n    for (k, v) in attrs.items():\n        (att_ns, nons) = k\n        if (att_ns != nevow.namespace):\n            continue\n        if (nons in directives):\n            specials[directives[nons]] = directive(v)\n            del attrs[k]\n        if (nons in attributes):\n            specials[nons] = v\n            del attrs[k]\n    no_ns_attrs = {\n        \n    }\n    for ((attrNs, attrName), v) in attrs.items():\n        nsPrefix = self.prefixMap.get(attrNs)\n        if (nsPrefix is None):\n            no_ns_attrs[attrName] = v\n        else:\n            no_ns_attrs[('%s:%s' % (nsPrefix, attrName))] = v\n    if ((ns == nevow.namespace) and (name == 'attr')):\n        if (not self.stack):\n            raise AssertionError('<nevow:attr> as top-level element')\n        if ('name' not in no_ns_attrs):\n            raise AssertionError('<nevow:attr> requires a name attribute')\n        el = Tag('', specials=specials, filename=filename, lineNumber=lineNumber, columnNumber=columnNumber)\n        self.stack[(- 1)].attributes[no_ns_attrs['name']] = el\n        self.stack.append(el)\n        self.current = el.children\n        return\n    if self.xmlnsAttrs:\n        no_ns_attrs.update(dict(self.xmlnsAttrs))\n        self.xmlnsAttrs = []\n    if ((ns != nevow.namespace) and (ns is not None)):\n        prefix = self.prefixMap[ns]\n        if (prefix is not None):\n            name = ('%s:%s' % (self.prefixMap[ns], name))\n    el = Tag(name, attributes=dict(no_ns_attrs), specials=specials, filename=filename, lineNumber=lineNumber, columnNumber=columnNumber)\n    self.stack.append(el)\n    self.current.append(el)\n    self.current = el.children\n", "label": 1}
{"function": "\n\ndef unpack(self, s_i):\n    'Return a symbolic integer scalar for the shape element s_i.\\n\\n        The s_i argument was produced by the infer_shape() of an Op subclass.\\n\\n        '\n    assert (s_i is not None)\n    if (s_i == 1):\n        return self.lscalar_one\n    if ((type(s_i) in integer_types) or isinstance(s_i, numpy.integer) or (isinstance(s_i, numpy.ndarray) and (s_i.ndim == 0))):\n        assert (s_i >= 0)\n        return T.constant(s_i, dtype='int64')\n    if (type(s_i) in (tuple, list)):\n        raise NotImplementedError(s_i)\n    if (s_i.owner and isinstance(s_i.owner.op, Subtensor) and s_i.owner.inputs[0].owner and isinstance(s_i.owner.inputs[0].owner.op, T.Shape)):\n        assert (s_i.ndim == 0)\n        assert (len(s_i.owner.op.idx_list) == 1)\n        idx = get_idx_list(s_i.owner.inputs, s_i.owner.op.idx_list)\n        assert (len(idx) == 1)\n        idx = idx[0]\n        try:\n            i = get_scalar_constant_value(idx)\n        except NotScalarConstantError:\n            pass\n        else:\n            x = s_i.owner.inputs[0].owner.inputs[0]\n            s_i = self.shape_of[x][i]\n    if (s_i.type.dtype[:3] in ('int', 'uint')):\n        if getattr(s_i.type, 'ndim', 0):\n            raise TypeError('Shape element must be scalar', s_i)\n        return s_i\n    else:\n        raise TypeError('Unsupported shape element', s_i, type(s_i), getattr(s_i, 'type', None))\n", "label": 1}
{"function": "\n\ndef ensure_running(retries=15, wait=10):\n    \"Ensure cassandra is running on all nodes.\\n    Runs 'nodetool ring' on a single node continuously until it\\n    reaches the specified number of retries.\\n\\n    INTENDED TO BE RUN ON ONE NODE, NOT ALL.\\n    \"\n    time.sleep(15)\n    for attempt in range(retries):\n        ring = StringIO(fab.run('JAVA_HOME={java_home} {nodetool_bin} ring'.format(java_home=config['java_home'], nodetool_bin=_nodetool_cmd())))\n        broadcast_ips = [x.get('external_ip', x['internal_ip']) for x in config['hosts'].values()]\n        nodes_up = dict(((host, False) for host in broadcast_ips))\n        for line in ring:\n            for host in broadcast_ips:\n                try:\n                    if ((host in line) and (' Up ' in line)):\n                        nodes_up[host] = True\n                except UnicodeDecodeError:\n                    pass\n        for (node, up) in nodes_up.items():\n            if (not up):\n                fab.puts(('Node is not up (yet): %s' % node))\n        if (False not in nodes_up.values()):\n            fab.puts('All nodes available!')\n            return\n        fab.puts(('waiting %d seconds to try again..' % wait))\n        time.sleep(wait)\n    else:\n        fab.abort('Timed out waiting for all nodes to startup')\n", "label": 1}
{"function": "\n\ndef __init__(self, options):\n    self.actions = []\n    self.maxdepth = None\n    self.mindepth = 0\n    self.test = False\n    criteria = {\n        _REQUIRES_PATH: list(),\n        _REQUIRES_STAT: list(),\n        _REQUIRES_CONTENTS: list(),\n    }\n    if ('mindepth' in options):\n        self.mindepth = options['mindepth']\n        del options['mindepth']\n    if ('maxdepth' in options):\n        self.maxdepth = options['maxdepth']\n        del options['maxdepth']\n    if ('test' in options):\n        self.test = options['test']\n        del options['test']\n    for (key, value) in six.iteritems(options):\n        if key.startswith('_'):\n            continue\n        if ((value is None) or (len(str(value)) == 0)):\n            raise ValueError('missing value for \"{0}\" option'.format(key))\n        try:\n            obj = globals()[(key.title() + 'Option')](key, value)\n        except KeyError:\n            raise ValueError('invalid option \"{0}\"'.format(key))\n        if hasattr(obj, 'match'):\n            requires = obj.requires()\n            if (requires & _REQUIRES_CONTENTS):\n                criteria[_REQUIRES_CONTENTS].append(obj)\n            elif (requires & _REQUIRES_STAT):\n                criteria[_REQUIRES_STAT].append(obj)\n            else:\n                criteria[_REQUIRES_PATH].append(obj)\n        if hasattr(obj, 'execute'):\n            self.actions.append(obj)\n    if (len(self.actions) == 0):\n        self.actions.append(PrintOption('print', ''))\n    self.criteria = ((criteria[_REQUIRES_PATH] + criteria[_REQUIRES_STAT]) + criteria[_REQUIRES_CONTENTS])\n", "label": 1}
{"function": "\n\ndef handle(self, listener, client, addr):\n    req = None\n    try:\n        parser = http.RequestParser(self.cfg, client)\n        try:\n            listener_name = listener.getsockname()\n            if (not self.cfg.keepalive):\n                req = six.next(parser)\n                self.handle_request(listener_name, req, client, addr)\n            else:\n                proxy_protocol_info = {\n                    \n                }\n                while True:\n                    req = None\n                    with self.timeout_ctx():\n                        req = six.next(parser)\n                    if (not req):\n                        break\n                    if req.proxy_protocol_info:\n                        proxy_protocol_info = req.proxy_protocol_info\n                    else:\n                        req.proxy_protocol_info = proxy_protocol_info\n                    self.handle_request(listener_name, req, client, addr)\n        except http.errors.NoMoreData as e:\n            self.log.debug('Ignored premature client disconnection. %s', e)\n        except StopIteration as e:\n            self.log.debug('Closing connection. %s', e)\n        except ssl.SSLError:\n            six.reraise(*sys.exc_info())\n        except EnvironmentError:\n            six.reraise(*sys.exc_info())\n        except Exception as e:\n            self.handle_error(req, client, addr, e)\n    except ssl.SSLError as e:\n        if (e.args[0] == ssl.SSL_ERROR_EOF):\n            self.log.debug('ssl connection closed')\n            client.close()\n        else:\n            self.log.debug('Error processing SSL request.')\n            self.handle_error(req, client, addr, e)\n    except EnvironmentError as e:\n        if (e.errno not in (errno.EPIPE, errno.ECONNRESET)):\n            self.log.exception('Socket error processing request.')\n        elif (e.errno == errno.ECONNRESET):\n            self.log.debug('Ignoring connection reset')\n        else:\n            self.log.debug('Ignoring EPIPE')\n    except Exception as e:\n        self.handle_error(req, client, addr, e)\n    finally:\n        util.close(client)\n", "label": 1}
{"function": "\n\ndef main(argv=None):\n    'script main.\\n\\n    parses command line options in sys.argv, unless *argv* is given.\\n    '\n    if (not argv):\n        argv = sys.argv\n    parser = E.OptionParser(version='%prog version: $Id: snp2table.py 2861 2010-02-23 17:36:32Z andreas $', usage=globals()['__doc__'])\n    parser.add_option('-g', '--genome-file', dest='genome_file', type='string', help='filename with genome [default=%default].')\n    parser.add_option('-a', '--annotations-tsv-file', dest='filename_annotations', type='string', help='filename with base annotations (output from gtf2fasta.py) [default=%default].')\n    parser.add_option('-f', '--exons-file', dest='filename_exons', type='string', help='filename with exon information (gff formatted file)  [default=%default].')\n    parser.add_option('-j', '--junctions-bed-file', dest='filename_junctions', type='string', help='filename with junction information (filename with exon junctions)  [default=%default].')\n    parser.add_option('-c', '--vcf-file', dest='filename_vcf', type='string', help='vcf file to parse [default=%default].')\n    parser.add_option('-i', '--input-format', dest='input_format', type='choice', choices=('pileup', 'vcf'), help='input format [default=%default].')\n    parser.add_option('--vcf-sample', dest='vcf_sample', type='string', help='sample id in vcf file to analyse [default=%default].')\n    parser.set_defaults(genome_file=None, filename_annotations=None, filename_exons=None, filename_junctions=None, input_format='pileup', vcf_sample=None, filename_vcf=None)\n    (options, args) = E.Start(parser, argv=argv)\n    (ninput, nskipped, noutput) = (0, 0, 0)\n    if options.genome_file:\n        fasta = IndexedFasta.IndexedFasta(options.genome_file)\n    else:\n        fasta = None\n    if options.filename_junctions:\n        junctions = readJunctions(options.filename_junctions)\n    else:\n        junctions = None\n    if (options.input_format == 'pileup'):\n        iterator = pysam.Pileup.iterate(sys.stdin)\n    elif (options.input_format == 'vcf'):\n        if (not options.vcf_sample):\n            raise ValueError('vcf format requires sample id (--vcf-sample) to be set')\n        if (not options.filename_vcf):\n            raise ValueError('reading from vcf requires vcf filename (--filename-vcf) to be set)')\n        iterator = pysam.Pileup.iterate_from_vcf(options.filename_vcf, options.vcf_sample)\n    modules = []\n    modules.append(BaseAnnotatorSNP())\n    if options.filename_exons:\n        modules.append(BaseAnnotatorExons(options.filename_exons, fasta=fasta))\n    if options.filename_annotations:\n        modules.append(BaseAnnotatorCodon(options.filename_annotations, fasta=fasta, junctions=junctions))\n    if options.filename_junctions:\n        modules.append(BaseAnnotatorSpliceSites(options.filename_junctions, fasta=fasta))\n    options.stdout.write(('\\t'.join([x.getHeader() for x in modules]) + '\\n'))\n    for snp in iterator:\n        ninput += 1\n        if fasta:\n            try:\n                snp = snp._replace(chromosome=fasta.getToken(snp.chromosome))\n            except KeyError:\n                E.warn(('unknown contig `%s` for snp `%s`' % (snp.chromosome, str(snp))))\n                continue\n        for module in modules:\n            module.update(snp)\n        options.stdout.write(('\\t'.join(map(str, modules)) + '\\n'))\n        noutput += 1\n    E.info(('ninput=%i, noutput=%i, nskipped=%i' % (ninput, noutput, nskipped)))\n    E.Stop()\n", "label": 1}
{"function": "\n\ndef test_reset_late(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, _time_function=time)\n    time.time = 13\n    assert (timer.sleep_time() == 0)\n    still_alive = timer.run()\n    assert (not still_alive)\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == Decimal('inf'))\n    timer.reset()\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 10)\n    time.time = 21\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 2)\n    time.time = 30\n    assert (timer.sleep_time() == 0)\n    still_alive = timer.run()\n    assert (not still_alive)\n    assert (callback.nb_calls == 2)\n    assert (timer.sleep_time() == Decimal('inf'))\n", "label": 1}
{"function": "\n\ndef parse_args(self, flags, namespace):\n    \"Set values for this parser's options on the namespace object.\"\n    flag_value_map = self._create_flag_value_map(flags)\n    for (args, kwargs) in self._unnormalized_option_registrations_iter():\n        self._validate(args, kwargs)\n        dest = (kwargs.get('dest') or self._select_dest(args))\n\n        def consume_flag(flag):\n            self._check_deprecated(dest, kwargs)\n            del flag_value_map[flag]\n        implicit_value = kwargs.get('implicit_value')\n        if ((implicit_value is None) and (kwargs.get('type') == bool)):\n            implicit_value = True\n        flag_vals = []\n\n        def add_flag_val(v):\n            if (v is None):\n                if (implicit_value is None):\n                    raise ParseError('Missing value for command line flag {} in {}'.format(arg, self._scope_str()))\n                else:\n                    flag_vals.append(implicit_value)\n            else:\n                flag_vals.append(v)\n        for arg in args:\n            if (kwargs.get('type') == bool):\n                inverse_arg = self._inverse_arg(arg)\n                if (inverse_arg in flag_value_map):\n                    flag_value_map[arg] = [self._invert(v) for v in flag_value_map[inverse_arg]]\n                    implicit_value = self._invert(implicit_value)\n                    del flag_value_map[inverse_arg]\n            if (arg in flag_value_map):\n                for v in flag_value_map[arg]:\n                    add_flag_val(v)\n                consume_flag(arg)\n        try:\n            val = self._compute_value(dest, kwargs, flag_vals)\n        except ParseError as e:\n            raise type(e)('Error computing value for {} in {} (may also be from PANTS_* environment variables).\\nCaused by:\\n{}'.format(', '.join(args), self._scope_str(), traceback.format_exc()))\n        setattr(namespace, dest, val)\n    if flag_value_map:\n        raise ParseError('Unrecognized command line flags on {}: {}'.format(self._scope_str(), ', '.join(flag_value_map.keys())))\n    return namespace\n", "label": 1}
{"function": "\n\ndef execute(self, job, stats, localTempDir, globalTempDir, memoryAvailable, cpuAvailable, defaultMemory, defaultCpu, depth):\n    self.tempDirAccessed = False\n    self.localTempDir = localTempDir\n    self.globalTempDir = globalTempDir\n    if (stats != None):\n        startTime = time.time()\n        startClock = getTotalCpuTime()\n    baseDir = os.getcwd()\n    self.target.setStack(self)\n    targetMemory = self.target.getMemory()\n    if (targetMemory != sys.maxint):\n        assert (targetMemory <= memoryAvailable)\n    targetCpu = self.target.getCpu()\n    if (targetCpu != sys.maxint):\n        assert (targetCpu <= cpuAvailable)\n    self.target.run()\n    if (os.getcwd() != baseDir):\n        os.chdir(baseDir)\n    if self.tempDirAccessed:\n        system(('rm -rf %s/*' % self.localTempDir))\n        self.tempDirAccessed = False\n    followOn = self.target.getFollowOn()\n    if (followOn is not None):\n        if self.target.isGlobalTempDirSet():\n            followOn.setGlobalTempDir(self.target.getGlobalTempDir())\n        followOnStack = Stack(followOn)\n        job.followOnCommands.append((followOnStack.makeRunnable(self.globalTempDir), followOnStack.getMemory(defaultMemory), followOnStack.getCpu(defaultCpu), depth))\n    newChildren = self.target.getChildren()\n    newChildren.reverse()\n    assert (len(job.children) == 0)\n    while (len(newChildren) > 0):\n        childStack = Stack(newChildren.pop())\n        job.children.append((childStack.makeRunnable(self.globalTempDir), childStack.getMemory(defaultMemory), childStack.getCpu(defaultCpu)))\n    for (childCommand, runTime) in self.target.getChildCommands():\n        job.children.append((childCommand, defaultMemory, defaultCpu))\n    for message in self.target.getMasterLoggingMessages():\n        job.messages.append(message)\n    if (stats != None):\n        stats = ET.SubElement(stats, 'target')\n        stats.attrib['time'] = str((time.time() - startTime))\n        (totalCpuTime, totalMemoryUsage) = getTotalCpuTimeAndMemoryUsage()\n        stats.attrib['clock'] = str((totalCpuTime - startClock))\n        stats.attrib['class'] = '.'.join((self.target.__class__.__name__,))\n        stats.attrib['memory'] = str(totalMemoryUsage)\n", "label": 1}
{"function": "\n\ndef _cardindex(self, key):\n    'Returns an index into the ._cards list given a valid lookup key.'\n    if isinstance(key, string_types):\n        keyword = key\n        n = 0\n    elif isinstance(key, int):\n        if (key < 0):\n            key += len(self._cards)\n        if ((key < 0) or (key >= len(self._cards))):\n            raise IndexError('Header index out of range.')\n        return key\n    elif isinstance(key, slice):\n        return key\n    elif isinstance(key, tuple):\n        if ((len(key) != 2) or (not isinstance(key[0], string_types)) or (not isinstance(key[1], int))):\n            raise ValueError('Tuple indices must be 2-tuples consisting of a keyword string and an integer index.')\n        (keyword, n) = key\n    else:\n        raise ValueError('Header indices must be either a string, a 2-tuple, or an integer.')\n    keyword = Card.normalize_keyword(keyword)\n    indices = self._keyword_indices.get(keyword, None)\n    if (keyword and (not indices)):\n        if ((len(keyword) > KEYWORD_LENGTH) or ('.' in keyword)):\n            raise KeyError(('Keyword %r not found.' % keyword))\n        else:\n            indices = self._rvkc_indices.get(keyword, None)\n    if (not indices):\n        raise KeyError(('Keyword %r not found.' % keyword))\n    try:\n        return indices[n]\n    except IndexError:\n        raise IndexError(('There are only %d %r cards in the header.' % (len(indices), keyword)))\n", "label": 1}
{"function": "\n\ndef _generate_events(self, event):\n    try:\n        if (not any([self._read, self._write])):\n            return\n        timeout = event.time_left\n        if (timeout < 0):\n            (r, w, _) = select.select(self._read, self._write, [])\n        else:\n            (r, w, _) = select.select(self._read, self._write, [], timeout)\n    except ValueError as e:\n        return self._preenDescriptors()\n    except TypeError as e:\n        return self._preenDescriptors()\n    except (SelectError, SocketError, IOError) as e:\n        if (e.args[0] in (0, 2)):\n            if ((not self._read) and (not self._write)):\n                return\n            else:\n                raise\n        elif (e.args[0] == EINTR):\n            return\n        elif (e.args[0] == EBADF):\n            return self._preenDescriptors()\n        else:\n            raise\n    for sock in w:\n        if self.isWriting(sock):\n            self.fire(_write(sock), self.getTarget(sock))\n    for sock in r:\n        if (sock == self._ctrl_recv):\n            self._read_ctrl()\n            continue\n        if self.isReading(sock):\n            self.fire(_read(sock), self.getTarget(sock))\n", "label": 1}
{"function": "\n\ndef resolve_address(self, hostname, callback, allow_cname=True):\n    'Start looking up an A or AAAA record.\\n\\n        `callback` will be called with a list of (family, address) tuples\\n        on success. Family is :std:`socket.AF_INET` or :std:`socket.AF_INET6`,\\n        the address is IPv4 or IPv6 literal. The list will be empty on error.\\n\\n        :Parameters:\\n            - `hostname`: the host name to look up\\n            - `callback`: a function to be called with a list of received\\n              addresses\\n            - `allow_cname`: `True` if CNAMEs should be followed\\n        :Types:\\n            - `hostname`: `str`\\n            - `callback`: function accepting a single argument\\n            - `allow_cname`: `bool`\\n        '\n    if self.settings['ipv6']:\n        if self.settings['ipv4']:\n            family = socket.AF_UNSPEC\n        else:\n            family = socket.AF_INET6\n    elif self.settings['ipv4']:\n        family = socket.AF_INET\n    else:\n        logger.warning('Neither IPv6 or IPv4 allowed.')\n        callback([])\n        return\n    try:\n        ret = socket.getaddrinfo(hostname, 0, family, socket.SOCK_STREAM, 0)\n    except socket.gaierror as err:\n        logger.warning(\"Couldn't resolve {0!r}: {1}\".format(hostname, err))\n        callback([])\n        return\n    if (family == socket.AF_UNSPEC):\n        tmp = ret\n        if self.settings['prefer_ipv6']:\n            ret = [addr for addr in tmp if (addr[0] == socket.AF_INET6)]\n            ret += [addr for addr in tmp if (addr[0] == socket.AF_INET)]\n        else:\n            ret = [addr for addr in tmp if (addr[0] == socket.AF_INET)]\n            ret += [addr for addr in tmp if (addr[0] == socket.AF_INET6)]\n    callback([(addr[0], addr[4][0]) for addr in ret])\n", "label": 1}
{"function": "\n\n@pytest.mark.gen_test\ndef test_audit_end_to_end(session, users, groups, http_client, base_url):\n    ' Tests an end-to-end audit cycle. '\n    groupname = 'audited-team'\n    zay_id = users['zay@a.co'].id\n    gary_id = users['gary@a.co'].id\n    add_member(groups['auditors'], users['gary@a.co'])\n    add_member(groups['auditors'], users['oliver@a.co'])\n    add_member(groups['auditors'], users['zay@a.co'])\n    add_member(groups['auditors'], users['figurehead@a.co'])\n    add_member(groups[groupname], users['zay@a.co'])\n    add_member(groups[groupname], users['gary@a.co'])\n    end_at_str = (datetime.now() + timedelta(days=10)).strftime('%m/%d/%Y')\n    fe_url = url(base_url, '/audits/create')\n    resp = (yield http_client.fetch(fe_url, method='POST', body=urlencode({\n        'ends_at': end_at_str,\n    }), headers={\n        'X-Grouper-User': 'zorkian@a.co',\n    }))\n    assert (resp.code == 200)\n    open_audits = get_audits(session, only_open=True).all()\n    assert (len(open_audits) == 4), 'audits created'\n    assert (groupname in [x.group.name for x in open_audits]), 'group we expect also gets audit'\n    AuditMember = namedtuple('AuditMember', 'am_id, edge_type, edge_id')\n    Audit = namedtuple('Audit', 'audit_id, owner_name, group_name, audit_members')\n    all_group_ids = [x.group.id for x in open_audits]\n    open_audits = [Audit(x.id, x.group.my_owners().iterkeys().next(), x.group.name, [AuditMember(am.id, am.edge.member_type, am.edge_id) for am in x.my_members()]) for x in open_audits]\n    for one_audit in open_audits:\n        fe_url = url(base_url, '/audits/{}/complete'.format(one_audit.audit_id))\n        if (one_audit.group_name == groupname):\n            continue\n        body = urlencode({'audit_{}'.format(am.am_id): 'approved' for am in one_audit.audit_members})\n        resp = (yield http_client.fetch(fe_url, method='POST', body=body, headers={\n            'X-Grouper-User': one_audit.owner_name,\n        }))\n        assert (resp.code == 200)\n    open_audits = get_audits(session, only_open=True).all()\n    assert (len(open_audits) == 1), 'only our test group remaining'\n    one_audit = open_audits[0]\n    one_audit.id\n    body_dict = {\n        \n    }\n    for am in one_audit.my_members():\n        if (gary_id == am.member.id):\n            body_dict['audit_{}'.format(am.id)] = 'remove'\n        else:\n            body_dict['audit_{}'.format(am.id)] = 'approved'\n    owner_name = one_audit.group.my_owners().iterkeys().next()\n    fe_url = url(base_url, '/audits/{}/complete'.format(one_audit.id))\n    resp = (yield http_client.fetch(fe_url, method='POST', body=urlencode(body_dict), headers={\n        'X-Grouper-User': owner_name,\n    }))\n    assert (resp.code == 200)\n    assert (len(AuditLog.get_entries(session, action='start_audit')) == 1), 'global start is logged'\n    assert (len(AuditLog.get_entries(session, action='complete_global_audit')) == 1), 'global complete is logged'\n    for group_id in all_group_ids:\n        assert (len(AuditLog.get_entries(session, on_group_id=group_id, action='complete_audit', category=AuditLogCategory.audit)) == 1), 'complete entry for each group'\n    assert (len(AuditLog.get_entries(session, on_user_id=gary_id, category=AuditLogCategory.audit)) == 1), 'removal AuditLog entry on user'\n", "label": 1}
{"function": "\n\ndef append_data(self, proc, bData, end=False):\n    if self.debug:\n        print('DEBUG: append_data start')\n    data = bData.decode('utf-8')\n    if self.debug:\n        print(('DEBUG: data= ' + data))\n    self.buffered_data = (self.buffered_data + data)\n    data = self.buffered_data.replace(self.file_path, self.file_name).replace('\\r\\n', '\\n').replace('\\r', '\\n')\n    if (end is False):\n        rsep_pos = data.rfind('\\n')\n        if (rsep_pos == (- 1)):\n            return\n        self.buffered_data = data[(rsep_pos + 1):]\n        data = data[:(rsep_pos + 1)]\n    text = data\n    if ((len(self.ignore_errors) > 0) and (not self.use_node_jslint)):\n        text = ''\n        for line in data.split('\\n'):\n            if (len(line) == 0):\n                continue\n            ignored = False\n            for rule in self.ignore_errors:\n                if re.search(rule, line):\n                    ignored = True\n                    self.ignored_error_count += 1\n                    if self.debug:\n                        print('text match line ')\n                        print(('rule = ' + rule))\n                        print(('line = ' + line))\n                        print('---------')\n                    break\n            if (ignored is False):\n                text += (line + '\\n')\n    self.show_tests_panel()\n    selection_was_at_end = ((len(self.output_view.sel()) == 1) and (self.output_view.sel()[0] == sublime.Region(self.output_view.size())))\n    with Edit(self.output_view, True) as edit:\n        edit.insert(self.output_view.size(), text)\n    if (end and (not self.use_node_jslint)):\n        text = (('\\njslint: ignored ' + str(self.ignored_error_count)) + ' errors.\\n\\n')\n        with Edit(self.output_view, True) as edit:\n            edit.insert(0, text)\n", "label": 1}
{"function": "\n\ndef _click_autocomplete(root, text):\n    'Completer generator for click applications.'\n    try:\n        parts = shlex.split(text)\n    except ValueError:\n        raise StopIteration\n    (location, incomplete) = _click_resolve_command(root, parts)\n    if ((not text.endswith(' ')) and (not incomplete) and text):\n        raise StopIteration\n    if (incomplete and (not incomplete[0:2].isalnum())):\n        for param in location.params:\n            if (not isinstance(param, click.Option)):\n                continue\n            for opt in itertools.chain(param.opts, param.secondary_opts):\n                if opt.startswith(incomplete):\n                    (yield completion.Completion(opt, (- len(incomplete)), display_meta=param.help))\n    elif isinstance(location, (click.MultiCommand, click.core.Group)):\n        ctx = click.Context(location)\n        commands = location.list_commands(ctx)\n        for command in commands:\n            if command.startswith(incomplete):\n                cmd = location.get_command(ctx, command)\n                (yield completion.Completion(command, (- len(incomplete)), display_meta=cmd.short_help))\n", "label": 1}
{"function": "\n\ndef print_tweet(tweet, settings):\n    'Format and print the tweet dict.\\n\\n    Returns:\\n        boolean status of if the tweet was printed\\n    '\n    tweet_text = tweet.get('text')\n    if ((tweet_text is None) or ((not settings['spam']) and AntiSpam.is_spam(tweet_text))):\n        return False\n    if (sys.version_info[0] == 2):\n        for encoding in ['utf-8', 'latin-1']:\n            try:\n                tweet_text.decode(encoding)\n            except UnicodeEncodeError:\n                pass\n            else:\n                break\n        else:\n            return False\n    if settings.get('json'):\n        print(json.dumps(tweet, indent=4, sort_keys=True))\n    else:\n        prepend = []\n        if (settings.get('date') or settings.get('time')):\n            date = parse_date(tweet['created_at'])\n            if settings.get('date'):\n                prepend.append('{0:%b} {1}'.format(date, int(datetime.strftime(date, '%d'))))\n            if settings.get('time'):\n                prepend.append('{0:%H}:{0:%M}:{0:%S}'.format(date))\n        tweet = '{0}{1}@{2}: {3}'.format(' '.join(prepend), (' ' * int((prepend != []))), tweet.get('user', {\n            \n        }).get('screen_name', ''), unescape(tweet_text))\n        print(highlight_tweet(tweet))\n    return True\n", "label": 1}
{"function": "\n\ndef parse_changes_file(file_path, versions=None):\n    '\\n    Parse CHANGES file and return a dictionary with contributors.\\n\\n    Dictionary maps contributor name to the JIRA tickets or Github pull\\n    requests the user has worked on.\\n    '\n    contributors_map = defaultdict(set)\n    in_entry = False\n    active_version = None\n    active_tickets = []\n    with open(file_path, 'r') as fp:\n        for line in fp:\n            line = line.strip()\n            match = re.search('Changes with Apache Libcloud (\\\\d+\\\\.\\\\d+\\\\.\\\\d+(-\\\\w+)?).*?$', line)\n            if match:\n                active_version = match.groups()[0]\n            if (versions and (active_version not in versions)):\n                continue\n            if (line.startswith('-') or line.startswith('*)')):\n                in_entry = True\n                active_tickets = []\n            if (in_entry and (line == '')):\n                in_entry = False\n            if in_entry:\n                match = re.search('\\\\((.+?)\\\\)$', line)\n                if match:\n                    active_tickets = match.groups()[0]\n                    active_tickets = active_tickets.split(', ')\n                    active_tickets = [ticket for ticket in active_tickets if (ticket.startswith('LIBCLOUD-') or ticket.startswith('GITHUB-'))]\n                match = re.search('^\\\\[(.+?)\\\\]$', line)\n                if match:\n                    contributors = match.groups()[0]\n                    contributors = contributors.split(',')\n                    contributors = [name.strip() for name in contributors]\n                    for name in contributors:\n                        name = name.title()\n                        contributors_map[name].update(set(active_tickets))\n    return contributors_map\n", "label": 1}
{"function": "\n\ndef iterModules(self):\n    '\\n        Loop over the modules present below this entry or package on PYTHONPATH.\\n\\n        For modules which are not packages, this will yield nothing.\\n\\n        For packages and path entries, this will only yield modules one level\\n        down; i.e. if there is a package a.b.c, iterModules on a will only\\n        return a.b.  If you want to descend deeply, use walkModules.\\n\\n        @return: a generator which yields PythonModule instances that describe\\n        modules which can be, or have been, imported.\\n        '\n    yielded = {\n        \n    }\n    if (not self.filePath.exists()):\n        return\n    for placeToLook in self._packagePaths():\n        try:\n            children = placeToLook.children()\n        except UnlistableError:\n            continue\n        children.sort()\n        for potentialTopLevel in children:\n            ext = potentialTopLevel.splitext()[1]\n            potentialBasename = potentialTopLevel.basename()[:(- len(ext))]\n            if (ext in PYTHON_EXTENSIONS):\n                if (not _isPythonIdentifier(potentialBasename)):\n                    continue\n                modname = self._subModuleName(potentialBasename)\n                if (modname.split('.')[(- 1)] == '__init__'):\n                    continue\n                if (modname not in yielded):\n                    yielded[modname] = True\n                    pm = PythonModule(modname, potentialTopLevel, self._getEntry())\n                    assert (pm != self)\n                    (yield pm)\n            else:\n                if (ext or (not _isPythonIdentifier(potentialBasename)) or (not potentialTopLevel.isdir())):\n                    continue\n                modname = self._subModuleName(potentialTopLevel.basename())\n                for ext in PYTHON_EXTENSIONS:\n                    initpy = potentialTopLevel.child(('__init__' + ext))\n                    if initpy.exists():\n                        yielded[modname] = True\n                        pm = PythonModule(modname, initpy, self._getEntry())\n                        assert (pm != self)\n                        (yield pm)\n                        break\n", "label": 1}
{"function": "\n\ndef assert_tables_equal(self, table, reflected_table, strict_types=False):\n    assert (len(table.c) == len(reflected_table.c))\n    for (c, reflected_c) in zip(table.c, reflected_table.c):\n        eq_(c.name, reflected_c.name)\n        assert (reflected_c is reflected_table.c[c.name])\n        eq_(c.primary_key, reflected_c.primary_key)\n        eq_(c.nullable, reflected_c.nullable)\n        if strict_types:\n            msg = \"Type '%s' doesn't correspond to type '%s'\"\n            assert isinstance(reflected_c.type, type(c.type)), (msg % (reflected_c.type, c.type))\n        else:\n            self.assert_types_base(reflected_c, c)\n        if isinstance(c.type, sqltypes.String):\n            eq_(c.type.length, reflected_c.type.length)\n        eq_(set([f.column.name for f in c.foreign_keys]), set([f.column.name for f in reflected_c.foreign_keys]))\n        if c.server_default:\n            assert isinstance(reflected_c.server_default, schema.FetchedValue)\n    assert (len(table.primary_key) == len(reflected_table.primary_key))\n    for c in table.primary_key:\n        assert (reflected_table.primary_key.columns[c.name] is not None)\n", "label": 1}
{"function": "\n\ndef clean(self):\n    try:\n        super(ManyToManyFieldDefinition, self).clean()\n    except ValidationError as e:\n        messages = e.message_dict\n    else:\n        messages = {\n            \n        }\n    if ((self.symmetrical is not None) and (not self.is_recursive_relationship)):\n        msg = _(\"The relationship can only be symmetrical or not if it's recursive, i. e. it points to 'self'\")\n        messages['symmetrical'] = [msg]\n    if self.through:\n        if self.db_table:\n            msg = _('Cannot specify a db_table if an intermediate model is used.')\n            messages['db_table'] = [msg]\n        if self.symmetrical:\n            msg = _('Many-to-many fields with intermediate model cannot be symmetrical.')\n            messages.setdefault('symmetrical', []).append(msg)\n        (seen_from, seen_to) = (0, 0)\n        to_model = self.to.model_class()\n        through_class = self.through.model_class()\n        from_model = self.model_def.cached_model\n        for field in through_class._meta.fields:\n            rel_to = getattr(get_remote_field(field), 'to', None)\n            if (rel_to == from_model):\n                seen_from += 1\n            elif (rel_to == to_model):\n                seen_to += 1\n        if self.is_recursive_relationship():\n            if (seen_from > 2):\n                msg = _('Intermediary model %s has more than two foreign keys to %s, which is ambiguous and is not permitted.')\n                formated_msg = (msg % (through_class._meta.object_name, from_model._meta.object_name))\n                messages.setdefault('through', []).append(formated_msg)\n        else:\n            msg = _('Intermediary model %s has more than one foreign key  to %s, which is ambiguous and is not permitted.')\n            if (seen_from > 1):\n                formated_msg = (msg % (through_class._meta.object_name, from_model._meta.object_name))\n                messages.setdefault('through', []).append(formated_msg)\n            if (seen_to > 1):\n                formated_msg = (msg % (through_class._meta.object_name, to_model._meta.object_name))\n                messages.setdefault('through', []).append(formated_msg)\n    if messages:\n        raise ValidationError(messages)\n", "label": 1}
{"function": "\n\ndef _save(self, name, content):\n    name = name.replace('\\\\', '/')\n    if isinstance(content, (AppEngineFile, AppEngineUploadedFile)):\n        data = content.blobstore_info\n    elif (hasattr(content, 'file') and isinstance(content.file, (AppEngineFile, AppEngineUploadedFile))):\n        data = content.file.blobstore_info\n    elif isinstance(content, File):\n        guessed_type = mimetypes.guess_type(name)[0]\n        if (self.storage_service == CLOUD_STORAGE_SERVICE):\n            assert cloudstorage, 'cloudstorage module is not available.'\n            file_name = ('/%s/%s' % (self.cloud_storage_bucket, name))\n            with cloudstorage.open(file_name, 'w', (guessed_type or 'application/octet-stream')) as f:\n                for chunk in content.chunks():\n                    f.write(chunk)\n            data = self._get_info(('/gs' + file_name))\n        else:\n            file_name = files.blobstore.create(mime_type=(guessed_type or 'application/octet-stream'), _blobinfo_uploaded_filename=name)\n            with files.open(file_name, 'a') as f:\n                for chunk in content.chunks():\n                    f.write(chunk)\n            files.finalize(file_name)\n            data = files.blobstore.get_blob_key(file_name)\n    else:\n        raise ValueError('The App Engine storage backend only supports AppEngineFile instances or File instances.')\n    if isinstance(data, CloudStorageInfo):\n        return ('/gs' + data.fullname)\n    if isinstance(data, BlobInfo):\n        data = data.key()\n    if isinstance(data, BlobKey):\n        return ('%s/%s' % (data, name.lstrip('/')))\n    raise ValueError(('Unknown type returned from saving: %s' % type(data)))\n", "label": 1}
{"function": "\n\ndef main():\n    'Entry point for this script.'\n    (parser, args) = parse_command_line_arguments()\n    logger = initialize_logging(args.debug, args.less_verbose)\n    result = 0\n    if args.download:\n        try:\n            download_listing(args.listing)\n        except DownloadRetfListingFailed as ex:\n            logger.error('Downloading latest RETF listing failed: %s.', ex)\n            result = 1\n    if ((not args.path) and (not args.file) and (not args.download)):\n        parser.print_help()\n        result = 2\n    if ((not result) and (not os.path.isfile(args.listing))):\n        logger.error('RETF listing not found at %s.', args.listing)\n        logger.info('Please download the RETF listing first by using the parameter --download.')\n        result = 1\n    if (not result):\n        files = get_file_listing(args.path, args.file, args.extension)\n        rules = generate_listing(args.listing)\n        disabled = load_disabled_rules(args.disabled)\n        all_findings = 0\n        for check in files:\n            if (not os.path.isfile(check)):\n                continue\n            (findings, content) = check_file(check, rules, disabled)\n            if (findings > 0):\n                all_findings += findings\n                logger.warning('%s finding(s) in file %s.', findings, check)\n            if ((findings > 0) and args.write_changes):\n                write_text_to_file(check, content, args.no_backup, args.in_place)\n        if (all_findings > 0):\n            logger.warning('%s finding(s) in all checked files.', all_findings)\n            result = 1\n    return result\n", "label": 1}
{"function": "\n\ndef begin(self, environ, server_env, start_response, session, key):\n    'Step 1: Get a access grant.\\n\\n        :param environ:\\n        :param start_response:\\n        :param server_env:\\n        :param session:\\n        '\n    try:\n        logger.debug('FLOW type: %s', self.flow_type)\n        logger.debug('begin environ: %s', server_env)\n        client = session['client']\n        if ((client is not None) and self.srv_discovery_url):\n            data = {\n                'client_id': client.client_id,\n            }\n            resp = requests.get((self.srv_discovery_url + 'verifyClientId'), params=data, verify=False)\n            if ((not resp.ok) and (resp.status_code == 400)):\n                client = None\n                server_env['OIC_CLIENT'].pop(key, None)\n        _state = ''\n        if (client is None):\n            callback = (server_env['base_url'] + key)\n            logout_callback = server_env['base_url']\n            if self.srv_discovery_url:\n                client = self.dynamic(server_env, callback, logout_callback, session, key)\n            else:\n                client = self.static(server_env, callback, logout_callback, key)\n            _state = session['state']\n            session['client'] = client\n        acr_value = session.get_acr_value(client.authorization_endpoint)\n        try:\n            acr_values = client.provider_info['acr_values_supported']\n            session['acr_values'] = acr_values\n        except KeyError:\n            acr_values = None\n        if ((acr_value is None) and (acr_values is not None) and (len(acr_values) > 1)):\n            resp_headers = [('Location', str('/rpAcr'))]\n            start_response('302 Found', resp_headers)\n            return []\n        elif ((acr_values is not None) and (len(acr_values) == 1)):\n            acr_value = acr_values[0]\n        return self.create_authnrequest(environ, server_env, start_response, session, acr_value, _state)\n    except Exception:\n        message = traceback.format_exception(*sys.exc_info())\n        logger.error(message)\n        return self.result(environ, start_response, server_env, (False, 'Cannot find the OP! Please view your configuration.'))\n", "label": 1}
{"function": "\n\ndef WorkOnItems(self):\n    'Perform the work of a WorkerThread.'\n    while (not self.exit_flag):\n        item = None\n        self.__thread_gate.StartWork()\n        try:\n            (status, instruction) = (WorkItem.FAILURE, ThreadGate.DECREASE)\n            try:\n                if self.exit_flag:\n                    instruction = ThreadGate.HOLD\n                    break\n                try:\n                    item = self.__work_queue.get(block=True, timeout=1.0)\n                except Queue.Empty:\n                    instruction = ThreadGate.HOLD\n                    continue\n                if ((item == _THREAD_SHOULD_EXIT) or self.exit_flag):\n                    (status, instruction) = (WorkItem.SUCCESS, ThreadGate.HOLD)\n                    break\n                logger.debug('[%s] Got work item %s', self.getName(), item)\n                (status, instruction) = item.PerformWork(self.__thread_pool)\n            except RetryException:\n                (status, instruction) = (WorkItem.RETRY, ThreadGate.HOLD)\n            except:\n                self.SetError()\n                raise\n        finally:\n            try:\n                if item:\n                    if (status == WorkItem.SUCCESS):\n                        self.__work_queue.task_done()\n                    elif (status == WorkItem.RETRY):\n                        try:\n                            self.__work_queue.reput(item, block=False)\n                        except Queue.Full:\n                            logger.error('[%s] Failed to reput work item.', self.getName())\n                            raise Error('Failed to reput work item')\n                    elif (not self.__error):\n                        if item.error:\n                            self.__error = item.error\n                            self.__traceback = item.traceback\n                        else:\n                            self.__error = WorkItemError(('Fatal error while processing %s' % item))\n                        raise self.__error\n            finally:\n                self.__thread_gate.FinishWork(instruction=instruction)\n", "label": 1}
{"function": "\n\ndef genrepo(opts=None, fire_event=True):\n    '\\n    Generate winrepo_cachefile based on sls files in the winrepo_dir\\n\\n    opts\\n        Specify an alternate opts dict. Should not be used unless this function\\n        is imported into an execution module.\\n\\n    fire_event : True\\n        Fire an event on failure. Only supported on the master.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt-run winrepo.genrepo\\n    '\n    if (opts is None):\n        opts = __opts__\n    if ('win_repo' in opts):\n        salt.utils.warn_until('Nitrogen', \"The 'win_repo' config option is deprecated, please use 'winrepo_dir' instead.\")\n        winrepo_dir = opts['win_repo']\n    else:\n        winrepo_dir = opts['winrepo_dir']\n    if ('win_repo_mastercachefile' in opts):\n        salt.utils.warn_until('Nitrogen', \"The 'win_repo_mastercachefile' config option is deprecated, please use 'winrepo_cachefile' instead.\")\n        winrepo_cachefile = opts['win_repo_mastercachefile']\n    else:\n        winrepo_cachefile = opts['winrepo_cachefile']\n    ret = {\n        \n    }\n    if (not os.path.exists(winrepo_dir)):\n        os.makedirs(winrepo_dir)\n    renderers = salt.loader.render(opts, __salt__)\n    for (root, _, files) in os.walk(winrepo_dir):\n        for name in files:\n            if name.endswith('.sls'):\n                try:\n                    config = salt.template.compile_template(os.path.join(root, name), renderers, opts['renderer'])\n                except SaltRenderError as exc:\n                    log.debug('Failed to render {0}.'.format(os.path.join(root, name)))\n                    log.debug('Error: {0}.'.format(exc))\n                    continue\n                if config:\n                    revmap = {\n                        \n                    }\n                    for (pkgname, versions) in six.iteritems(config):\n                        log.debug(\"Compiling winrepo data for package '{0}'\".format(pkgname))\n                        for (version, repodata) in six.iteritems(versions):\n                            log.debug('Compiling winrepo data for {0} version {1}'.format(pkgname, version))\n                            if (not isinstance(version, six.string_types)):\n                                config[pkgname][str(version)] = config[pkgname].pop(version)\n                            if (not isinstance(repodata, dict)):\n                                log.debug('Failed to compile {0}.'.format(os.path.join(root, name)))\n                                if fire_event:\n                                    msg = 'Failed to compile {0}.'.format(os.path.join(root, name))\n                                    try:\n                                        __jid_event__.fire_event({\n                                            'error': msg,\n                                        }, 'progress')\n                                    except NameError:\n                                        log.error('Attempted to fire the an event with the following error, but event firing is not supported: {0}'.format(msg))\n                                continue\n                            revmap[repodata['full_name']] = pkgname\n                    ret.setdefault('repo', {\n                        \n                    }).update(config)\n                    ret.setdefault('name_map', {\n                        \n                    }).update(revmap)\n    with salt.utils.fopen(os.path.join(winrepo_dir, winrepo_cachefile), 'w+b') as repo:\n        repo.write(msgpack.dumps(ret))\n    return ret\n", "label": 1}
{"function": "\n\ndef render_admin_panel(self, req, category, page, path_info):\n    req.perm.require('VERSIONCONTROL_ADMIN')\n    if (req.method == 'POST'):\n        perms = {\n            \n        }\n        for setting in req.args:\n            try:\n                setting = json.loads(setting)\n            except ValueError:\n                continue\n            if ((not isinstance(setting, dict)) or ('perm' not in setting) or ('user' not in setting) or ('repo' not in setting)):\n                continue\n            repo = setting['repo']\n            perm = setting['perm']\n            user = setting['user']\n            if (repo not in perms):\n                perms[repo] = {\n                    \n                }\n            if (perm not in perms[repo]):\n                perms[repo][perm] = []\n            if (user not in perms[repo][perm]):\n                perms[repo][perm].append(user)\n        gitolite_admin_perms = perms.get(self.gitolite_admin_real_reponame, {\n            \n        })\n        if ((self.gitolite_admin_system_user not in gitolite_admin_perms.get('R', [])) or (self.gitolite_admin_system_user not in gitolite_admin_perms.get('W', []))):\n            add_warning(req, _(('Read and write permissions on the gitolite admin repo must not be revoked for user %s -- otherwise this plugin will no longer work!' % self.gitolite_admin_system_user)))\n            req.redirect(req.href.admin(category, page))\n        utils.save_file(self.gitolite_admin_ssh_path, 'conf/gitolite.conf', utils.to_string(perms), _('Updating repository permissions'))\n        add_notice(req, _('The permissions have been updated.'))\n        req.redirect(req.href.admin(category, page))\n    perms = self.read_config()\n    users_listed_in_perms = set()\n    flattened_perms = set()\n    for p in perms.values():\n        for perm in p:\n            flattened_perms.add(perm)\n            users_listed_in_perms.update(p[perm])\n    flattened_perms = list(flattened_perms)\n\n    def sort_perms(perms):\n        tail = []\n        if ('+' in perms):\n            perms.remove('+')\n            tail.append('+')\n        perms = sorted(perms)\n        perms.extend(tail)\n        return perms\n    flattened_perms = sort_perms(flattened_perms)\n    users = sorted(list(set((list(self.get_users()) + list(users_listed_in_perms)))))\n    data = {\n        'repositories': perms,\n        'permissions': flattened_perms,\n        'users': users,\n        'sort_perms': sort_perms,\n    }\n    return ('admin_repository_permissions.html', data)\n", "label": 1}
{"function": "\n\ndef check(self, app, sha, config):\n    token = current_app.config['GITHUB_TOKEN']\n    if (not token):\n        raise CheckFailed('GITHUB_TOKEN is not set')\n    api_root = (config.get('api_root') or current_app.config['GITHUB_API_ROOT']).rstrip('/')\n    contexts = set((config.get('contexts') or []))\n    repo = config['repo']\n    url = '{api_root}/repos/{repo}/commits/{ref}/statuses'.format(api_root=api_root, repo=repo, ref=sha)\n    headers = {\n        'Accepts': 'application/json',\n        'Authorization': 'token {}'.format(token),\n    }\n    resp = http.get(url, headers=headers)\n    context_list = resp.json()\n    if (not context_list):\n        raise CheckFailed('No contexts were present in GitHub')\n    valid_contexts = set()\n    for data in context_list:\n        if (data['state'] == 'success'):\n            valid_contexts.add(data['context'])\n            try:\n                contexts.remove(data['context'])\n            except KeyError:\n                pass\n        if (data['context'] in valid_contexts):\n            continue\n        if (contexts and (data['context'] not in contexts)):\n            continue\n        if (data['state'] == 'pending'):\n            raise CheckPending(ERR_CHECK.format(data['context'], data['state']))\n        elif (data['state'] != 'success'):\n            raise CheckFailed(ERR_CHECK.format(data['context'], data['state']))\n        contexts.remove(data['context'])\n    if contexts:\n        raise CheckFailed(ERR_MISSING_CONTEXT.format(iter(contexts).next()))\n", "label": 1}
{"function": "\n\ndef parse_record(context, record, repos=None, mtype='http://www.opengis.net/cat/csw/2.0.2', identifier=None, pagesize=10):\n    ' parse metadata '\n    if (identifier is None):\n        identifier = uuid.uuid4().urn\n    if ((mtype == 'http://www.opengis.net/cat/csw/2.0.2') and isinstance(record, str) and record.startswith('http')):\n        LOGGER.debug('CSW service detected, fetching via HTTP')\n        try:\n            return _parse_csw(context, repos, record, identifier, pagesize)\n        except Exception as err:\n            if (str(err).find('ExceptionReport') != (- 1)):\n                msg = ('CSW harvesting error: %s' % str(err))\n                LOGGER.debug(msg)\n                raise RuntimeError(msg)\n            LOGGER.debug('Not a CSW, attempting to fetch Dublin Core')\n            try:\n                content = util.http_request('GET', record)\n            except Exception as err:\n                raise RuntimeError(('HTTP error: %s' % str(err)))\n            return [_parse_dc(context, repos, etree.fromstring(content, context.parser))]\n    elif (mtype == 'urn:geoss:waf'):\n        LOGGER.debug('WAF detected, fetching via HTTP')\n        return _parse_waf(context, repos, record, identifier)\n    elif (mtype == 'http://www.opengis.net/wms'):\n        LOGGER.debug('WMS detected, fetching via OWSLib')\n        return _parse_wms(context, repos, record, identifier)\n    elif (mtype == 'http://www.opengis.net/wmts/1.0'):\n        LOGGER.debug('WMTS 1.0.0 detected, fetching via OWSLib')\n        return _parse_wmts(context, repos, record, identifier)\n    elif (mtype == 'http://www.opengis.net/wps/1.0.0'):\n        LOGGER.debug('WPS detected, fetching via OWSLib')\n        return [_parse_wps(context, repos, record, identifier)]\n    elif (mtype == 'http://www.opengis.net/wfs'):\n        LOGGER.debug('WFS detected, fetching via OWSLib')\n        return _parse_wfs(context, repos, record, identifier)\n    elif (mtype == 'http://www.opengis.net/wcs'):\n        LOGGER.debug('WCS detected, fetching via OWSLib')\n        return _parse_wcs(context, repos, record, identifier)\n    elif (mtype == 'http://www.opengis.net/sos/1.0'):\n        LOGGER.debug('SOS 1.0.0 detected, fetching via OWSLib')\n        return _parse_sos(context, repos, record, identifier, '1.0.0')\n    elif (mtype == 'http://www.opengis.net/sos/2.0'):\n        LOGGER.debug('SOS 2.0.0 detected, fetching via OWSLib')\n        return _parse_sos(context, repos, record, identifier, '2.0.0')\n    elif ((mtype == 'http://www.opengis.net/cat/csw/csdgm') and record.startswith('http')):\n        LOGGER.debug('FGDC detected, fetching via HTTP')\n        record = util.http_request('GET', record)\n    return _parse_metadata(context, repos, record)\n", "label": 1}
{"function": "\n\ndef save(self, locale):\n    '\\n        Load the source resource, modify it with changes made to this\\n        Resource instance, and save it over the locale-specific\\n        resource.\\n        '\n    if (self.source_resource is None):\n        raise SyncError('Cannot save silme resource {0}: No source resource given.'.format(self.path))\n    new_structure = self.parser.get_structure(read_file(self.source_resource.path, uncomment_moz_langpack=self.entities.get('MOZ_LANGPACK_CONTRIBUTORS', False)))\n    entities = [SilmeEntity(obj) for obj in new_structure if isinstance(obj, silme.core.entity.Entity)]\n    for silme_entity in entities:\n        key = silme_entity.key\n        translated_entity = self.entities.get(key)\n        if (translated_entity and (None in translated_entity.strings)):\n            translation = translated_entity.strings[None]\n            if self.escape_quotes_on:\n                translation = self.escape_quotes(translation)\n            new_structure.modify_entity(key, translation)\n        else:\n            pos = new_structure.entity_pos(key)\n            new_structure.remove_entity(key)\n            try:\n                line = new_structure[pos]\n            except IndexError:\n                continue\n            if ((type(line) == unicode) and line.startswith('\\n')):\n                line = line[len('\\n'):]\n                new_structure[pos] = line\n                if (len(line) is 0):\n                    new_structure.remove_element(pos)\n    if (self.path.endswith('browser/chrome/browser/browser.properties') and (locale.code == 'zh-CN')):\n        new_entity = silme.core.entity.Entity('browser.startup.homepage', 'http://start.firefoxchina.cn')\n        new_structure.add_entity(new_entity)\n        new_structure.add_string('\\n')\n    try:\n        os.makedirs(os.path.dirname(self.path))\n    except OSError:\n        pass\n    with codecs.open(self.path, 'w', 'utf-8') as f:\n        f.write(self.parser.dump_structure(new_structure))\n", "label": 1}
{"function": "\n\ndef loadPlugins(self, callback=None):\n    '\\n        Load the candidate plugins that have been identified through a\\n        previous call to locatePlugins.  For each plugin candidate\\n        look for its category, load it and store it in the appropriate\\n        slot of the ``category_mapping``.\\n\\n        If a callback function is specified, call it before every load\\n        attempt.  The ``plugin_info`` instance is passed as an argument to\\n        the callback.\\n        '\n    if (not hasattr(self, '_candidates')):\n        raise ValueError('locatePlugins must be called before loadPlugins')\n    processed_plugins = []\n    for (candidate_infofile, candidate_filepath, plugin_info) in self._candidates:\n        plugin_module_name_template = (NormalizePluginNameForModuleName(('yapsy_loaded_plugin_' + plugin_info.name)) + '_%d')\n        for plugin_name_suffix in range(len(sys.modules)):\n            plugin_module_name = (plugin_module_name_template % plugin_name_suffix)\n            if (plugin_module_name not in sys.modules):\n                break\n        if candidate_filepath.endswith('.py'):\n            candidate_filepath = candidate_filepath[:(- 3)]\n        if (callback is not None):\n            callback(plugin_info)\n        if ('__init__' in os.path.basename(candidate_filepath)):\n            candidate_filepath = os.path.dirname(candidate_filepath)\n        try:\n            if os.path.isdir(candidate_filepath):\n                candidate_module = imp.load_module(plugin_module_name, None, candidate_filepath, ('py', 'r', imp.PKG_DIRECTORY))\n            else:\n                with open((candidate_filepath + '.py'), 'r') as plugin_file:\n                    candidate_module = imp.load_module(plugin_module_name, plugin_file, (candidate_filepath + '.py'), ('py', 'r', imp.PY_SOURCE))\n        except Exception:\n            exc_info = sys.exc_info()\n            log.error(('Unable to import plugin: %s' % candidate_filepath), exc_info=exc_info)\n            plugin_info.error = exc_info\n            processed_plugins.append(plugin_info)\n            continue\n        processed_plugins.append(plugin_info)\n        if ('__init__' in os.path.basename(candidate_filepath)):\n            sys.path.remove(plugin_info.path)\n        for element in (getattr(candidate_module, name) for name in dir(candidate_module)):\n            plugin_info_reference = None\n            for category_name in self.categories_interfaces:\n                try:\n                    is_correct_subclass = issubclass(element, self.categories_interfaces[category_name])\n                except Exception:\n                    continue\n                if (is_correct_subclass and (element is not self.categories_interfaces[category_name])):\n                    current_category = category_name\n                    if (candidate_infofile not in self._category_file_mapping[current_category]):\n                        if (not plugin_info_reference):\n                            try:\n                                plugin_info.plugin_object = self.instanciateElement(element)\n                                plugin_info_reference = plugin_info\n                            except Exception:\n                                exc_info = sys.exc_info()\n                                log.error(('Unable to create plugin object: %s' % candidate_filepath), exc_info=exc_info)\n                                plugin_info.error = exc_info\n                                break\n                        plugin_info.categories.append(current_category)\n                        self.category_mapping[current_category].append(plugin_info_reference)\n                        self._category_file_mapping[current_category].append(candidate_infofile)\n    delattr(self, '_candidates')\n    return processed_plugins\n", "label": 1}
{"function": "\n\n@register.filter(name='kml_desc')\ndef kml_desc(place):\n    properties = place.get('properties')\n    media = place.get('media')\n    comments = place.get('comments')\n    description = '<![CDATA['\n    if properties:\n        description += '<table>'\n        for key in properties:\n            name = key\n            value = properties[key]\n            try:\n                field = Field.objects.get(key=key, category_id=place.get('meta').get('category').get('id'))\n                name = field.name.encode('utf-8')\n                if (value is not None):\n                    if isinstance(field, LookupField):\n                        value = field.lookupvalues.get(pk=value).name\n                    elif isinstance(field, MultipleLookupField):\n                        values = field.lookupvalues.filter(pk__in=value)\n                        value = '<br />'.join([v.name for v in values])\n            except Field.DoesNotExist:\n                pass\n            if (type(value) in [str, unicode]):\n                value = value.encode('utf-8')\n            if (properties[key] is not None):\n                description = '{desc}<tr><td>{name}</td><td>{value}</td></tr>'.format(desc=description, name=name, value=value)\n        description += '</table>'\n    if media:\n        description += '<table>'\n        for file in media:\n            if ((file['file_type'] == 'ImageFile') or (file['file_type'] == 'VideoFile')):\n                if (file['file_type'] == 'VideoFile'):\n                    file['url'] = file['url'].replace('embed/', 'watch?v=')\n                description += '<tr><td><strong>{name}</strong>{desc}<br /><a href=\"{url}\"><img src=\"{thumbnail_url}\" /></a></td></tr>'.format(name=file['name'], desc=(('<br />%s' % file['description']) if file['description'] else ''), url=file['url'], thumbnail_url=file['thumbnail_url'])\n            elif (file['file_type'] == 'AudioFile'):\n                description += '<tr><td><a href=\"{url}\"><strong>{name}</strong></a>{desc}</td></tr>'.format(name=file['name'], desc=(('<br />%s' % file['description']) if file['description'] else ''), url=file['url'])\n        description += '</table>'\n    if comments:\n\n        def render_comments(comments):\n            description = '<table>'\n            for comment in comments:\n                description += '<tr><td><strong>{name}</strong>{text}'.format(name=comment['creator']['display_name'], text=(('<br />%s' % comment['text']) if comment['text'] else ''))\n                if (len(comment['responses']) > 0):\n                    description += render_comments(comment['responses'])\n                description += '</td></tr>'\n            description += '</table>'\n            return description\n        description += render_comments(comments)\n    description += ']]>'\n    return description\n", "label": 1}
{"function": "\n\ndef _get_session(self, record=None, email=None, password=None, skip_cache=False):\n    from tapiriik.auth.credential_storage import CredentialStore\n    cached = self._sessionCache.Get((record.ExternalID if record else email))\n    if (cached and (not skip_cache)):\n        return cached\n    if record:\n        password = CredentialStore.Decrypt(record.ExtendedAuthorization['Password'])\n        email = CredentialStore.Decrypt(record.ExtendedAuthorization['Email'])\n    session = requests.Session()\n    self._rate_limit()\n    mPreResp = session.get((self._urlRoot + '/api/tapiriikProfile'), allow_redirects=False)\n    if (mPreResp.status_code == 403):\n        data = {\n            '_username': email,\n            '_password': password,\n            '_remember_me': 'true',\n        }\n        preResp = session.post((self._urlRoot + '/api/login'), data=data)\n        if (preResp.status_code != 200):\n            raise APIException(('Login error %s %s' % (preResp.status_code, preResp.text)))\n        try:\n            preResp = preResp.json()\n        except ValueError:\n            raise APIException(('Parse error %s %s' % (preResp.status_code, preResp.text)), block=True, user_exception=UserException(UserExceptionType.Authorization, intervention_required=True))\n        if (('success' not in preResp) and ('error' not in preResp)):\n            raise APIException('Login error', block=True, user_exception=UserException(UserExceptionType.Authorization, intervention_required=True))\n        success = True\n        error = ''\n        if ('success' in preResp):\n            success = ['success']\n        if ('error' in preResp):\n            error = preResp['error']\n        if (not success):\n            logger.debug(('Login error %s' % error))\n            raise APIException('Invalid login', block=True, user_exception=UserException(UserExceptionType.Authorization, intervention_required=True))\n        self._rate_limit()\n        mRedeemResp1 = session.get((self._urlRoot + '/api/tapiriikProfile'), allow_redirects=False)\n        if (mRedeemResp1.status_code != 200):\n            raise APIException(('Motivato redeem error %s %s' % (mRedeemResp1.status_code, mRedeemResp1.text)))\n    else:\n        logger.debug(('code %s' % mPreResp.status_code))\n        raise APIException(('Unknown Motivato prestart response %s %s' % (mPreResp.status_code, mPreResp.text)))\n    self._sessionCache.Set((record.ExternalID if record else email), session)\n    session.headers.update(self._obligatory_headers)\n    return session\n", "label": 1}
{"function": "\n\ndef fit(self):\n    groups = list(set(self.data[:, self.groupIn]))\n    groupList = []\n    for i in range(len(groups)):\n        groupList.append([])\n    for i in range(len(groups)):\n        for j in range(len(self.data)):\n            if (self.data[(j, self.groupIn)] == groups[i]):\n                groupList[i].append(self.data[j])\n    timeList = []\n    for i in range(len(groupList)):\n        times = []\n        for j in range(len(groupList[i])):\n            times.append(groupList[i][j][self.timesIn])\n        times = list(sorted(set(times)))\n        timeList.append(times)\n    timeCounts = []\n    points = []\n    censoredPoints = []\n    for i in range(len(groupList)):\n        survival = 1\n        varSum = 0\n        riskCounter = len(groupList[i])\n        counts = []\n        x = []\n        y = []\n        for j in range(len(timeList[i])):\n            if (j != 0):\n                if (j == 1):\n                    groupInd = 1\n                    x.append(0)\n                    y.append(1)\n                    x.append(timeList[i][(j - 1)])\n                    y.append(1)\n                    x.append(timeList[i][(j - 1)])\n                    y.append(survival)\n                    y.append(survival)\n                else:\n                    groupInd = 0\n                    y.append(survival)\n                    y.append(survival)\n                    x.append(timeList[i][(j - 1)])\n                    x.append(timeList[i][(j - 1)])\n                censoredPoints.append([timeList[i][(j - 1)], censoringNum, survival, groupInd])\n                counts.append([timeList[i][(j - 1)], riskCounter, eventCounter, survival, sqrt(((survival ** 2) * varSum))])\n                riskCounter += ((- 1) * riskChange)\n            riskChange = 0\n            eventCounter = 0\n            censoringCounter = 0\n            censoringNum = 0\n            for k in range(len(groupList[i])):\n                if (groupList[i][k][self.timesIn] == timeList[i][j]):\n                    riskChange += 1\n                    if (groupList[i][k][self.censoringIn] == 1):\n                        eventCounter += 1\n                    else:\n                        censoringNum += 1\n            if (eventCounter != censoringCounter):\n                censoringCounter = eventCounter\n                survival *= ((float(riskCounter) - eventCounter) / riskCounter)\n                try:\n                    varSum += (eventCounter / (riskCounter * (float(riskCounter) - eventCounter)))\n                except ZeroDivisionError:\n                    varSum = 0\n        counts.append([timeList[i][(len(timeList[i]) - 1)], riskCounter, eventCounter, survival, sqrt(((survival ** 2) * varSum))])\n        x.append(timeList[i][(len(timeList[i]) - 1)])\n        x.append(timeList[i][(len(timeList[i]) - 1)])\n        y.append(survival)\n        censoredPoints.append([timeList[i][(len(timeList[i]) - 1)], censoringNum, survival, 1])\n        timeCounts.append(np.array(counts))\n        points.append([x, y])\n    self.results = timeCounts\n    self.points = points\n    self.censoredPoints = censoredPoints\n", "label": 1}
{"function": "\n\ndef multipart_encode(self, v_vars, files, boundary=None, buf=None):\n    if six.PY3:\n        if (boundary is None):\n            boundary = choose_boundary()\n        if (buf is None):\n            buf = io.BytesIO()\n        for (key, value) in v_vars:\n            buf.write(((b'--' + boundary.encode('utf-8')) + b'\\r\\n'))\n            buf.write(((b'Content-Disposition: form-data; name=\"' + key.encode('utf-8')) + b'\"'))\n            buf.write(((b'\\r\\n\\r\\n' + value.encode('utf-8')) + b'\\r\\n'))\n        for (key, fd) in files:\n            try:\n                filename = fd.name.split('/')[(- 1)]\n            except AttributeError:\n                filename = 'temp.pdf'\n            contenttype = (mimetypes.guess_type(filename)[0] or b'application/octet-stream')\n            buf.write(((b'--' + boundary.encode('utf-8')) + b'\\r\\n'))\n            buf.write(((((((b'Content-Disposition: form-data; ' + b'name=\"') + key.encode('utf-8')) + b'\"; ') + b'filename=\"') + filename.encode('utf-8')) + b'\"\\r\\n'))\n            buf.write(((b'Content-Type: ' + contenttype.encode('utf-8')) + b'\\r\\n'))\n            fd.seek(0)\n            buf.write(((b'\\r\\n' + fd.read()) + b'\\r\\n'))\n        buf.write(b'--')\n        buf.write(boundary.encode('utf-8'))\n        buf.write(b'--\\r\\n\\r\\n')\n        buf = buf.getvalue()\n        return (boundary, buf)\n    else:\n        if (boundary is None):\n            boundary = choose_boundary()\n        if (buf is None):\n            buf = io.StringIO()\n        for (key, value) in v_vars:\n            buf.write(('--%s\\r\\n' % boundary))\n            buf.write(('Content-Disposition: form-data; name=\"%s\"' % key))\n            buf.write((('\\r\\n\\r\\n' + value) + '\\r\\n'))\n        for (key, fd) in files:\n            try:\n                filename = fd.name.split('/')[(- 1)]\n            except AttributeError:\n                filename = 'temp.pdf'\n            contenttype = (mimetypes.guess_type(filename)[0] or 'application/octet-stream')\n            buf.write(('--%s\\r\\n' % boundary))\n            buf.write(('Content-Disposition: form-data;     name=\"%s\"; filename=\"%s\"\\r\\n' % (key, filename)))\n            buf.write(('Content-Type: %s\\r\\n' % contenttype))\n            fd.seek(0)\n            buf.write((('\\r\\n' + fd.read()) + '\\r\\n'))\n        buf.write((('--' + boundary) + '--\\r\\n\\r\\n'))\n        buf = buf.getvalue()\n        return (boundary, buf)\n", "label": 1}
{"function": "\n\ndef set_cookie(cookiejar, kaka):\n    'PLaces a cookie (a cookielib.Cookie based on a set-cookie header\\n    line) in the cookie jar.\\n    Always chose the shortest expires time.\\n\\n    :param cookiejar:\\n    :param kaka: Cookie\\n    '\n    for (cookie_name, morsel) in kaka.items():\n        std_attr = ATTRS.copy()\n        std_attr['name'] = cookie_name\n        _tmp = morsel.coded_value\n        if (_tmp.startswith('\"') and _tmp.endswith('\"')):\n            std_attr['value'] = _tmp[1:(- 1)]\n        else:\n            std_attr['value'] = _tmp\n        std_attr['version'] = 0\n        attr = ''\n        try:\n            for attr in morsel.keys():\n                if (attr in ATTRS):\n                    if morsel[attr]:\n                        if (attr == 'expires'):\n                            std_attr[attr] = http2time(morsel[attr])\n                        else:\n                            std_attr[attr] = morsel[attr]\n                elif (attr == 'max-age'):\n                    if morsel[attr]:\n                        std_attr['expires'] = http2time(morsel[attr])\n        except TimeFormatError:\n            logger.info(('Time format error on %s parameter in received cookie' % (attr,)))\n            continue\n        for (att, spec) in PAIRS.items():\n            if std_attr[att]:\n                std_attr[spec] = True\n        if (std_attr['domain'] and std_attr['domain'].startswith('.')):\n            std_attr['domain_initial_dot'] = True\n        if (morsel['max-age'] is 0):\n            try:\n                cookiejar.clear(domain=std_attr['domain'], path=std_attr['path'], name=std_attr['name'])\n            except ValueError:\n                pass\n        else:\n            if ('version' in std_attr):\n                try:\n                    std_attr['version'] = std_attr['version'].split(',')[0]\n                except (TypeError, AttributeError):\n                    pass\n            new_cookie = cookielib.Cookie(**std_attr)\n            cookiejar.set_cookie(new_cookie)\n", "label": 1}
{"function": "\n\n@csrf_exempt\n@require_POST\n@login_or_digest\n@require_can_edit_fixtures\ndef upload_fixture_api(request, domain, **kwargs):\n    '\\n        Use following curl-command to test.\\n        > curl -v --digest http://127.0.0.1:8000/a/gsid/fixtures/fixapi/ -u user@domain.com:password\\n                -F \"file-to-upload=@hqtest_fixtures.xlsx\"\\n                -F \"replace=true\"\\n    '\n    response_codes = {\n        'fail': 405,\n        'warning': 402,\n        'success': 200,\n    }\n    error_messages = {\n        'invalid_post_req': \"Invalid post request. Submit the form with field 'file-to-upload' and POST parameter 'replace'\",\n        'has_no_permission': \"User {attr} doesn't have permission to upload fixtures\",\n        'invalid_file': 'Error processing your file. Submit a valid (.xlsx) file',\n        'has_no_sheet': 'Workbook does not have a sheet called {attr}',\n        'unknown_fail': \"Fixture upload couldn't succeed due to the following error: {attr}\",\n    }\n\n    def _return_response(code, message):\n        resp_json = {\n            \n        }\n        resp_json['code'] = code\n        resp_json['message'] = message\n        return HttpResponse(json.dumps(resp_json), content_type='application/json')\n    try:\n        upload_file = request.FILES['file-to-upload']\n        replace = request.POST['replace']\n        if (replace.lower() == 'true'):\n            replace = True\n        elif (replace.lower() == 'false'):\n            replace = False\n    except Exception:\n        return _return_response(response_codes['fail'], error_messages['invalid_post_req'])\n    if (not request.couch_user.has_permission(domain, Permissions.edit_data.name)):\n        error_message = error_messages['has_no_permission'].format(attr=request.couch_user.username)\n        return _return_response(response_codes['fail'], error_message)\n    try:\n        workbook = get_workbook(upload_file)\n    except Exception:\n        return _return_response(response_codes['fail'], error_messages['invalid_file'])\n    try:\n        upload_resp = run_upload(domain, workbook, replace=replace)\n    except WorksheetNotFound as e:\n        error_message = error_messages['has_no_sheet'].format(attr=e.title)\n        return _return_response(response_codes['fail'], error_message)\n    except ExcelMalformatException as e:\n        return _return_response(response_codes['fail'], str(e))\n    except DuplicateFixtureTagException as e:\n        return _return_response(response_codes['fail'], str(e))\n    except FixtureAPIException as e:\n        return _return_response(response_codes['fail'], str(e))\n    except Exception as e:\n        error_message = error_messages['unknown_fail'].format(attr=e)\n        return _return_response(response_codes['fail'], error_message)\n    num_unknown_groups = len(upload_resp.unknown_groups)\n    num_unknown_users = len(upload_resp.unknown_users)\n    resp_json = {\n        \n    }\n    if ((not num_unknown_users) and (not num_unknown_groups)):\n        num_uploads = upload_resp.number_of_fixtures\n        success_message = ('Successfully uploaded %d fixture%s.' % (num_uploads, ('s' if (num_uploads > 1) else '')))\n        return _return_response(response_codes['success'], success_message)\n    else:\n        resp_json['code'] = response_codes['warning']\n    warn_groups = ('%d group%s unknown' % (num_unknown_groups, ('s are' if (num_unknown_groups > 1) else ' is')))\n    warn_users = ('%d user%s unknown' % (num_unknown_users, ('s are' if (num_unknown_users > 1) else ' is')))\n    resp_json['message'] = 'Fixtures have been uploaded. But following '\n    if num_unknown_groups:\n        resp_json['message'] += ('%s %s' % (warn_groups, upload_resp.unknown_groups))\n    if num_unknown_users:\n        resp_json['message'] += ('%s%s%s' % (('and following ' if num_unknown_groups else ''), warn_users, upload_resp.unknown_users))\n    return HttpResponse(json.dumps(resp_json), content_type='application/json')\n", "label": 1}
{"function": "\n\ndef resumeProducing(self):\n    self.paused = False\n    if self._buffer:\n        data = ''.join(self._buffer)\n        bytesSent = self._writeSomeData(data)\n        if (bytesSent < len(data)):\n            unsent = data[bytesSent:]\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer[:] = [unsent]\n        else:\n            self._buffer[:] = []\n    else:\n        bytesSent = 0\n    if (self.unregistered and bytesSent and (not self._buffer) and (self.consumer is not None)):\n        self.consumer.unregisterProducer()\n    if (not self.iAmStreaming):\n        self.outstandingPull = (not bytesSent)\n    if (self.producer is not None):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (self.producerPaused and (bytesBuffered < self.bufferSize)):\n            self.producerPaused = False\n            self.producer.resumeProducing()\n        elif self.outstandingPull:\n            self.producer.resumeProducing()\n", "label": 1}
{"function": "\n\ndef _find_tests(self, start_dir, pattern):\n    'Used by discovery. Yields test suites it loads.'\n    paths = os.listdir(start_dir)\n    for path in paths:\n        full_path = os.path.join(start_dir, path)\n        if os.path.isfile(full_path):\n            if (not VALID_MODULE_NAME.match(path)):\n                continue\n            if (not self._match_path(path, full_path, pattern)):\n                continue\n            name = self._get_name_from_path(full_path)\n            try:\n                module = self._get_module_from_name(name)\n            except:\n                (yield _make_failed_import_test(name, self.suiteClass))\n            else:\n                mod_file = os.path.abspath(getattr(module, '__file__', full_path))\n                realpath = _jython_aware_splitext(os.path.realpath(mod_file))\n                fullpath_noext = _jython_aware_splitext(os.path.realpath(full_path))\n                if (realpath.lower() != fullpath_noext.lower()):\n                    module_dir = os.path.dirname(realpath)\n                    mod_name = _jython_aware_splitext(os.path.basename(full_path))\n                    expected_dir = os.path.dirname(full_path)\n                    msg = '%r module incorrectly imported from %r. Expected %r. Is this module globally installed?'\n                    raise ImportError((msg % (mod_name, module_dir, expected_dir)))\n                (yield self.loadTestsFromModule(module))\n        elif os.path.isdir(full_path):\n            if (not os.path.isfile(os.path.join(full_path, '__init__.py'))):\n                continue\n            load_tests = None\n            tests = None\n            if fnmatch(path, pattern):\n                name = self._get_name_from_path(full_path)\n                package = self._get_module_from_name(name)\n                load_tests = getattr(package, 'load_tests', None)\n                tests = self.loadTestsFromModule(package, use_load_tests=False)\n            if (load_tests is None):\n                if (tests is not None):\n                    (yield tests)\n                for test in self._find_tests(full_path, pattern):\n                    (yield test)\n            else:\n                try:\n                    (yield load_tests(self, tests, pattern))\n                except Exception as e:\n                    (yield _make_failed_load_tests(package.__name__, e, self.suiteClass))\n", "label": 1}
{"function": "\n\ndef find_tar_gz(package_name, pypi_url='https://pypi.python.org/pypi', verbose=0, release=None):\n    transport = RequestsTransport()\n    transport.user_agent = USER_AGENT\n    if pypi_url.startswith('https://'):\n        transport.use_https = True\n    pypi = xmlrpclib.ServerProxy(pypi_url, transport=transport)\n    download_url = None\n    expected_md5_digest = None\n    if (verbose >= 2):\n        myprint(('querying PyPI (%s) for package name \"%s\"' % (pypi_url, package_name)))\n    show_hidden = True\n    all_releases = pypi.package_releases(package_name, show_hidden)\n    if (release is not None):\n        if (verbose >= 2):\n            myprint(('found all available releases: %s' % (', '.join(all_releases),)))\n        if (release not in all_releases):\n            raise ValueError(('your desired release %r is not among available releases %r' % (release, all_releases)))\n        version = release\n    else:\n        default_releases = pypi.package_releases(package_name)\n        if (len(default_releases) != 1):\n            raise RuntimeError(('Expected one and only one release. Non-hidden: %r. All: %r' % (default_releases, all_releases)))\n        default_release = default_releases[0]\n        if (verbose >= 2):\n            myprint(('found default release: %s' % (', '.join(default_releases),)))\n        version = default_release\n    urls = pypi.release_urls(package_name, version)\n    for url in urls:\n        if (url['packagetype'] == 'sdist'):\n            assert (url['python_version'] == 'source'), 'how can an sdist not be a source?'\n            if url['url'].endswith('.tar.gz'):\n                download_url = url['url']\n                if ('md5_digest' in url):\n                    expected_md5_digest = url['md5_digest']\n                break\n    if (download_url is None):\n        result = pypi.release_data(package_name, version)\n        if (result['download_url'] != 'UNKNOWN'):\n            download_url = result['download_url']\n            urls = pypi.release_urls(result['name'], result['version'])\n    if (download_url is None):\n        raise ValueError(('no package \"%s\" was found' % package_name))\n    return (download_url, expected_md5_digest)\n", "label": 1}
{"function": "\n\ndef parse(self):\n    '\\n    Parse the vmstat file\\n    :return: status of the metric parse\\n    '\n    file_status = True\n    for input_file in self.infile_list:\n        file_status = (file_status and naarad.utils.is_valid_file(input_file))\n    if (not file_status):\n        return False\n    status = True\n    data = {\n        \n    }\n    for input_file in self.infile_list:\n        logger.info('Processing : %s', input_file)\n        timestamp_format = None\n        with open(input_file) as fh:\n            for line in fh:\n                words = line.split()\n                if (len(words) < 3):\n                    continue\n                ts = ((words[0] + ' ') + words[1])\n                if ((not timestamp_format) or (timestamp_format == 'unknown')):\n                    timestamp_format = naarad.utils.detect_timestamp_format(ts)\n                if (timestamp_format == 'unknown'):\n                    continue\n                ts = naarad.utils.get_standardized_timestamp(ts, timestamp_format)\n                if self.ts_out_of_range(ts):\n                    continue\n                col = words[2]\n                if (self.sub_metrics and (col not in self.sub_metrics)):\n                    continue\n                self.sub_metric_unit[col] = 'pages'\n                if (col in self.column_csv_map):\n                    out_csv = self.column_csv_map[col]\n                else:\n                    out_csv = self.get_csv(col)\n                    data[out_csv] = []\n                data[out_csv].append(((ts + ',') + words[3]))\n    for csv in data.keys():\n        self.csv_files.append(csv)\n        with open(csv, 'w') as fh:\n            fh.write('\\n'.join(data[csv]))\n    return status\n", "label": 1}
{"function": "\n\ndef test_interval_identity():\n    iv.dps = 15\n    assert (mpi(2) == mpi(2, 2))\n    assert (mpi(2) != mpi((- 2), 2))\n    assert (not (mpi(2) != mpi(2, 2)))\n    assert (mpi((- 1), 1) == mpi((- 1), 1))\n    assert (str(mpi('0.1')) == '[0.099999999999999991673, 0.10000000000000000555]')\n    assert (repr(mpi('0.1')) == \"mpi('0.099999999999999992', '0.10000000000000001')\")\n    u = mpi((- 1), 3)\n    assert ((- 1) in u)\n    assert (2 in u)\n    assert (3 in u)\n    assert ((- 1.1) not in u)\n    assert (3.1 not in u)\n    assert (mpi((- 1), 3) in u)\n    assert (mpi(0, 1) in u)\n    assert (mpi((- 1.1), 2) not in u)\n    assert (mpi(2.5, 3.1) not in u)\n    w = mpi((- inf), inf)\n    assert (mpi((- 5), 5) in w)\n    assert (mpi(2, inf) in w)\n    assert (mpi(0, 2) in mpi(0, 10))\n    assert (not (3 in mpi((- inf), 0)))\n", "label": 1}
{"function": "\n\ndef v1_to_v3_property(self, property_name, is_multi, is_projection, v1_value, v3_property):\n    'Converts info from a v1 Property to a v3 Property.\\n\\n    v1_value must not have an array_value.\\n\\n    Args:\\n      property_name: the name of the property, unicode\\n      is_multi: whether the property contains multiple values\\n      is_projection: whether the property is projected\\n      v1_value: an googledatastore.Value\\n      v3_property: an entity_pb.Property to populate\\n    '\n    v1_value_type = v1_value.WhichOneof('value_type')\n    if (v1_value_type == 'array_value'):\n        assert False, 'v1 array_value not convertable to v3'\n    v3_property.Clear()\n    v3_property.set_name(property_name.encode('utf-8'))\n    v3_property.set_multiple(is_multi)\n    self.v1_value_to_v3_property_value(v1_value, v3_property.mutable_value())\n    v1_meaning = None\n    if v1_value.meaning:\n        v1_meaning = v1_value.meaning\n    if (v1_value_type == 'timestamp_value'):\n        v3_property.set_meaning(entity_pb.Property.GD_WHEN)\n    elif (v1_value_type == 'blob_value'):\n        if (v1_meaning == MEANING_ZLIB):\n            v3_property.set_meaning_uri(URI_MEANING_ZLIB)\n        if (v1_meaning == entity_pb.Property.BYTESTRING):\n            if (not v1_value.exclude_from_indexes):\n                pass\n        else:\n            if (not v1_value.exclude_from_indexes):\n                v3_property.set_meaning(entity_pb.Property.BYTESTRING)\n            else:\n                v3_property.set_meaning(entity_pb.Property.BLOB)\n            v1_meaning = None\n    elif (v1_value_type == 'entity_value'):\n        if (v1_meaning != MEANING_PREDEFINED_ENTITY_USER):\n            v3_property.set_meaning(entity_pb.Property.ENTITY_PROTO)\n        v1_meaning = None\n    elif (v1_value_type == 'geo_point_value'):\n        if (v1_meaning != MEANING_POINT_WITHOUT_V3_MEANING):\n            v3_property.set_meaning(MEANING_GEORSS_POINT)\n        v1_meaning = None\n    elif (v1_value_type == 'integer_value'):\n        if (v1_meaning == MEANING_NON_RFC_3339_TIMESTAMP):\n            v3_property.set_meaning(entity_pb.Property.GD_WHEN)\n            v1_meaning = None\n    else:\n        pass\n    if (v1_meaning is not None):\n        v3_property.set_meaning(v1_meaning)\n    if is_projection:\n        v3_property.set_meaning(entity_pb.Property.INDEX_VALUE)\n", "label": 1}
{"function": "\n\ndef delete_file(self, path, prefixed_path, source_storage, **options):\n    symlink = options['link']\n    if self.storage.exists(prefixed_path):\n        try:\n            target_last_modified = self.storage.modified_time(prefixed_path)\n        except (OSError, NotImplementedError):\n            pass\n        else:\n            try:\n                source_last_modified = source_storage.modified_time(path)\n            except (OSError, NotImplementedError):\n                pass\n            else:\n                if self.local:\n                    full_path = self.storage.path(prefixed_path)\n                else:\n                    full_path = None\n                if (target_last_modified >= source_last_modified):\n                    if (not ((symlink and full_path and (not os.path.islink(full_path))) or ((not symlink) and full_path and os.path.islink(full_path)))):\n                        if (prefixed_path not in self.unmodified_files):\n                            self.unmodified_files.append(prefixed_path)\n                        self.log((\"Skipping '%s' (not modified)\" % path))\n                        return False\n        if options['dry_run']:\n            self.log((\"Pretending to delete '%s'\" % path))\n        else:\n            self.log((\"Deleting '%s'\" % path))\n            self.storage.delete(prefixed_path)\n    return True\n", "label": 1}
{"function": "\n\n@register_specialize\n@gof.local_optimizer([T.mul, T.true_div])\ndef local_abs_merge(node):\n    \"\\n    Merge abs generated by local_abs_lift when the canonizer don't\\n    need it anymore\\n\\n    \"\n    if ((node.op == T.mul) and (sum([(i.owner.op == T.abs_) for i in node.inputs if i.owner]) > 1)):\n        inputs = []\n        for i in node.inputs:\n            if (i.owner and (i.owner.op == T.abs_)):\n                inputs.append(i.owner.inputs[0])\n            elif isinstance(i, Constant):\n                try:\n                    const = get_scalar_constant_value(i)\n                except NotScalarConstantError:\n                    return False\n                if (not (const >= 0).all()):\n                    return False\n                inputs.append(i)\n            else:\n                return False\n        return [T.abs_(T.mul(*inputs))]\n    if ((node.op == T.true_div) and (sum([(i.owner.op == T.abs_) for i in node.inputs if i.owner]) == 2)):\n        return [T.abs_(T.true_div(node.inputs[0].owner.inputs[0], node.inputs[1].owner.inputs[0]))]\n", "label": 1}
{"function": "\n\ndef export(filename=None):\n    lines = ['import IPython.core.ipapi', 'ip = IPython.core.ipapi.get()', '']\n    vars = ip.db.keys('autorestore/*')\n    vars.sort()\n    varstomove = []\n    get = ip.db.get\n    macros = []\n    variables = []\n    for var in vars:\n        k = os.path.basename(var)\n        v = get(var)\n        if k.startswith('_'):\n            continue\n        if isinstance(v, macro.Macro):\n            macros.append((k, v))\n        if (type(v) in [int, str, float]):\n            variables.append((k, v))\n    if macros:\n        lines.extend(['# === Macros ===', ''])\n    for (k, v) in macros:\n        lines.append((\"ip.defmacro('%s',\" % k))\n        for line in v.value.splitlines():\n            lines.append((' ' + repr((line + '\\n'))))\n        lines.extend([')', ''])\n    if variables:\n        lines.extend(['', '# === Variables ===', ''])\n        for (k, v) in variables:\n            varstomove.append(k)\n            lines.append(('%s = %s' % (k, repr(v))))\n        lines.append(('ip.push(\"%s\")' % ' '.join(varstomove)))\n    bkms = ip.db.get('bookmarks', {\n        \n    })\n    if bkms:\n        lines.extend(['', '# === Bookmarks ===', ''])\n        lines.append((\"ip.db['bookmarks'] = %s \" % pprint.pformat(bkms, indent=2)))\n    aliases = ip.db.get('stored_aliases', {\n        \n    })\n    if aliases:\n        lines.extend(['', '# === Alias definitions ===', ''])\n        for (k, v) in list(aliases.items()):\n            try:\n                lines.append((\"ip.define_alias('%s', %s)\" % (k, repr(v[1]))))\n            except (AttributeError, TypeError):\n                pass\n    env = ip.db.get('stored_env')\n    if env:\n        lines.extend(['', '# === Stored env vars ===', ''])\n        lines.append((\"ip.db['stored_env'] = %s \" % pprint.pformat(env, indent=2)))\n    out = '\\n'.join(lines)\n    if filename:\n        open(filename, 'w').write(out)\n    else:\n        print(out)\n", "label": 1}
{"function": "\n\ndef do(repo_configs, sys_admin_emails, sleep_secs, is_no_loop, external_report_command, mail_sender, max_workers, overrun_secs):\n    conduit_manager = _ConduitManager()\n    fs_accessor = abdt_fs.make_default_accessor()\n    url_watcher_wrapper = phlurl_watcher.FileCacheWatcherWrapper(fs_accessor.layout.urlwatcher_cache_path)\n    if (max_workers == 0):\n        max_workers = determine_max_workers_default()\n    repo_list = []\n    for (name, config) in repo_configs:\n        repo_list.append(_ArcydManagedRepository(name, config, conduit_manager, url_watcher_wrapper, sys_admin_emails, mail_sender))\n    max_overrun_workers = (max_workers // 2)\n    pool = phlmp_cyclingpool.CyclingPool(repo_list, max_workers, max_overrun_workers)\n    cycle_timer = phlsys_timer.Timer()\n    cycle_timer.start()\n    exit_code = None\n    while (exit_code is None):\n        sleep_timer = phlsys_timer.Timer()\n        sleep_timer.start()\n        with abdt_logging.remote_io_read_event_context('refresh-git-snoop', ''):\n            abdt_tryloop.critical_tryloop(url_watcher_wrapper.watcher.refresh, abdt_errident.GIT_SNOOP, '')\n        with abdt_logging.remote_io_read_event_context('refresh-conduit', ''):\n            conduit_manager.refresh_conduits()\n        with abdt_logging.misc_operation_event_context('process-repos', '{} workers, {} repos'.format(max_workers, len(repo_list))):\n            if (max_workers > 1):\n                for (i, res) in pool.cycle_results(overrun_secs=overrun_secs):\n                    repo = repo_list[i]\n                    repo.merge_from_worker(res)\n            else:\n                for r in repo_list:\n                    r()\n        url_watcher_wrapper.save()\n        report = {\n            'cycle_time_secs': cycle_timer.restart(),\n            'overrun_jobs': pool.num_active_jobs,\n        }\n        _LOGGER.debug('cycle-stats: {}'.format(report))\n        if external_report_command:\n            report_json = json.dumps(report)\n            full_path = os.path.abspath(external_report_command)\n            with abdt_logging.misc_operation_event_context('external-report-command', external_report_command):\n                try:\n                    phlsys_subprocess.run(full_path, stdin=report_json)\n                except phlsys_subprocess.CalledProcessError as e:\n                    _LOGGER.error('External command: {} failed with exception: {}.'.format(external_report_command, type(e).__name__))\n                    _LOGGER.error('VERBOSE MESSAGE: CycleReportJson:{}'.format(e))\n        if is_no_loop:\n            exit_code = abdi_processexitcodes.ExitCodes.ec_exit\n        elif os.path.isfile(fs_accessor.layout.killfile):\n            exit_code = abdi_processexitcodes.ExitCodes.ec_exit\n            if phlsys_fs.read_text_file(fs_accessor.layout.killfile):\n                _LOGGER.info('Killfile observed, reason given: {}'.format(phlsys_fs.read_text_file(fs_accessor.layout.killfile)))\n            else:\n                _LOGGER.info('Killfile observed, arcyd will stop')\n            os.remove(fs_accessor.layout.killfile)\n        elif os.path.isfile(fs_accessor.layout.reloadfile):\n            _LOGGER.info('Reloadfile observed, arcyd will reload')\n            exit_code = abdi_processexitcodes.ExitCodes.ec_reload\n            os.remove(fs_accessor.layout.reloadfile)\n        secs_to_sleep = (float(sleep_secs) - float(sleep_timer.duration))\n        if ((secs_to_sleep > 0) and (exit_code is None)):\n            with abdt_logging.misc_operation_event_context('sleep', secs_to_sleep):\n                time.sleep(secs_to_sleep)\n    for (i, res) in pool.finish_results():\n        repo = repo_list[i]\n        repo.merge_from_worker(res)\n    url_watcher_wrapper.save()\n    return exit_code\n", "label": 1}
{"function": "\n\ndef main():\n\n    class JutArgParser(argparse.ArgumentParser):\n        '\\n        custom argument parser so we show the full comand line help menu\\n\\n        '\n\n        def error(self, message):\n            error(message)\n            self.print_help()\n            sys.exit(2)\n    parser = JutArgParser(description='jut - jut command line tools')\n    commands = parser.add_subparsers(dest='subcommand')\n    config_parser = commands.add_parser('config', help='configuration management')\n    config_commands = config_parser.add_subparsers(dest='config_subcommand')\n    _ = config_commands.add_parser('list', help='list configurations')\n    defaults_config = config_commands.add_parser('defaults', help='change the configuration defaults')\n    defaults_config.add_argument('-u', '--username', help='username to use')\n    defaults_config.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    add_config = config_commands.add_parser('add', help='add another configuration (default when no sub command is provided)')\n    add_config.add_argument('-u', '--username', help='username to use')\n    add_config.add_argument('-p', '--password', help='password to use')\n    add_config.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    add_config.add_argument('-d', '--default', action='store_true', help='sets this configuration to the default')\n    add_config.add_argument('-s', '--show-password', action='store_true', default=False, help='shows password as you type it interactively')\n    rm_config = config_commands.add_parser('rm', help='remove a configuration')\n    rm_config.add_argument('-u', '--username', help='username to use')\n    rm_config.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    jobs_parser = commands.add_parser('jobs', help='jobs management')\n    jobs_commands = jobs_parser.add_subparsers(dest='jobs_subcommand')\n    list_jobs = jobs_commands.add_parser('list', help='list running jobs')\n    list_jobs.add_argument('-d', '--deployment', default=None, help='specify the deployment name')\n    list_jobs.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    list_jobs.add_argument('-f', '--format', default='table', help='available formats are text, table with default: table')\n    kill_job = jobs_commands.add_parser('kill', help='kill running job')\n    kill_job.add_argument('job_id', help='specify the job_id to kill')\n    kill_job.add_argument('-d', '--deployment', default=None, help='specify the deployment name')\n    kill_job.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    kill_job.add_argument('-y', '--yes', action='store_true', default=False, help='kill without prompting for confirmation')\n    connect_job = jobs_commands.add_parser('connect', help='connect to a persistent job')\n    connect_job.add_argument('job_id', help='specify the job_id to connect to')\n    connect_job.add_argument('-d', '--deployment', default=None, help='specify the deployment name')\n    connect_job.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    connect_job.add_argument('-s', '--show-progress', action='store_true', default=False, help='writes the progress out to stderr on how many points were streamed thus far')\n    connect_job.add_argument('--retry', type=int, default=0, help='retry running the program N times,default 0. Use -1 to retry forever.')\n    connect_job.add_argument('--retry-delay', type=int, default=10, help='number of seconds to wait between retries.')\n    connect_job.add_argument('-f', '--format', default='json', help='available formats are json, text, csv with default: json')\n    programs_parser = commands.add_parser('programs', help='programs management')\n    programs_commands = programs_parser.add_subparsers(dest='programs_subcommand')\n    list_programs = programs_commands.add_parser('list', help='list programs')\n    list_programs.add_argument('-d', '--deployment', default=None, help='specify the deployment name')\n    list_programs.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    list_programs.add_argument('-f', '--format', default='table', help='available formats are text, table with default: table')\n    list_programs.add_argument('--all', default=False, help='list all programs, default is to list your own programs')\n    run_programs = programs_commands.add_parser('run', help='run a program in your local browser')\n    run_programs.add_argument('program_name', help='specify the program name you wish to kick off')\n    pull_programs = programs_commands.add_parser('pull', help='pull programs')\n    pull_programs.add_argument('directory', help='directory to pull remote programs into')\n    pull_programs.add_argument('-d', '--deployment', default=None, help='specify the deployment name')\n    pull_programs.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    pull_programs.add_argument('-p', '--per-user-directory', action='store_true', default=False, help='save the programs per user into a separate directory')\n    pull_programs.add_argument('--all', action='store_true', default=False, help='pull all programs, default is to list your own programs')\n    push_programs = programs_commands.add_parser('push', help='push programs')\n    push_programs.add_argument('directory', help='directory to pick up programs to push to the running Jut instance')\n    push_programs.add_argument('-d', '--deployment', default=None, help='specify the deployment name')\n    push_programs.add_argument('-a', '--app-url', default=defaults.APP_URL, help='app url (default: https://app.jut.io INTERNAL USE)')\n    push_programs.add_argument('--all', default=False, help='pull all programs, default is to list your own programs')\n    upload_parser = commands.add_parser('upload', help='upload local JSON file(s) to Jut')\n    if sys.stdin.isatty():\n        upload_parser.add_argument('source', help='The name of a JSON file or directory containing JSON files to process')\n    upload_parser.add_argument('-u', '--url', help='The URL to POST data points to, if none is specified we will push to the webhook for the default configuration')\n    upload_parser.add_argument('-d', '--deployment', dest='deployment', default=None, help='specify the deployment name')\n    upload_parser.add_argument('-s', '--space', dest='space', default='default', help='specify the destination space')\n    upload_parser.add_argument('--dry-run', action='store_true', dest='dry_run', default=False, help='Just log the data that would have been POSTed to the specified URL.')\n    upload_parser.add_argument('--batch-size', dest='batch_size', default=100, type=int, help='Maximum set of data points to send in each POST, default: 100.')\n    upload_parser.add_argument('--anonymize-fields', metavar='field_name', dest='anonymize_fields', nargs='+', default=[], help='space separated field names to anonymize in the data before uploading. Currently we anonymize hashing the field value with md5 hash')\n    upload_parser.add_argument('--remove-fields', metavar='field_name', dest='remove_fields', nargs='+', default=[], help='space separated field names to remove from the data before uploading')\n    upload_parser.add_argument('--rename-fields', metavar='field_name=new_field_name', dest='rename_fields', type=parse_key_value, nargs='+', default=[], help='space separated field names to rename from the data before uploading.')\n    run_parser = commands.add_parser('run', help='run juttle program from the import command line')\n    run_parser.add_argument('juttle', help='juttle program to execute or the filename of a juttle program.')\n    run_parser.add_argument('-d', '--deployment', dest='deployment', default=None, help='specify the deployment name')\n    run_parser.add_argument('-f', '--format', default='json', help='available formats are json, text, csv with default: json')\n    run_parser.add_argument('-n', '--name', help='give your program a name to appear in the Jobs application')\n    run_parser.add_argument('-p', '--persist', action='store_true', default=False, help='allow the program containing background outputs to become a persistent job by disconnecting form the running job (ie essentially backgrounding your program)')\n    run_parser.add_argument('-s', '--show-progress', action='store_true', default=False, help='writes the progress out to stderr on how many points were streamed thus far')\n    run_parser.add_argument('--retry', type=int, default=0, help='retry running the program N times,default 0. Use -1 to retry forever.')\n    run_parser.add_argument('--retry-delay', type=int, default=10, help='number of seconds to wait between retries.')\n    options = parser.parse_args()\n    try:\n        if (options.subcommand == 'config'):\n            if (options.config_subcommand == 'list'):\n                config.show()\n            elif (options.config_subcommand == 'add'):\n                configs.add_configuration(options)\n            elif (options.config_subcommand == 'rm'):\n                configs.rm_configuration(options)\n            elif (options.config_subcommand == 'defaults'):\n                configs.change_defaults(options)\n            else:\n                raise Exception(('Unexpected config subcommand \"%s\"' % options.command))\n        elif (options.subcommand == 'jobs'):\n            if (options.jobs_subcommand == 'list'):\n                jobs.list(options)\n            elif (options.jobs_subcommand == 'kill'):\n                jobs.kill(options)\n            elif (options.jobs_subcommand == 'connect'):\n                jobs.connect(options)\n            else:\n                raise Exception(('Unexpected jobs subcommand \"%s\"' % options.command))\n        elif (options.subcommand == 'programs'):\n            if (options.programs_subcommand == 'list'):\n                programs.list(options)\n            elif (options.programs_subcommand == 'pull'):\n                programs.pull(options)\n            elif (options.programs_subcommand == 'push'):\n                programs.push(options)\n            elif (options.programs_subcommand == 'run'):\n                programs.run(options)\n            else:\n                raise Exception(('Unexpected programs subcommand \"%s\"' % options.command))\n        elif (options.subcommand == 'upload'):\n            upload.upload_file(options)\n        elif (options.subcommand == 'run'):\n            run.run_juttle(options)\n        else:\n            raise Exception(('Unexpected jut command \"%s\"' % options.command))\n    except JutException as exception:\n        if is_debug_enabled():\n            traceback.print_exc()\n        error(str(exception))\n        sys.exit(255)\n", "label": 1}
{"function": "\n\ndef ParseISO8601(datetimeStr):\n    '\\n   Parse ISO 8601 date time from string.\\n   Returns datetime if ok, None otherwise\\n   Note: Allows YYYY / YYYY-MM, but truncate YYYY -> YYYY-01-01,\\n                                             YYYY-MM -> YYYY-MM-01\\n         Truncate microsecond to most significant 6 digits\\n   '\n    datetimeVal = None\n    match = _dtExpr.match(datetimeStr)\n    if match:\n        try:\n            dt = {\n                \n            }\n            for (key, defaultVal) in iteritems(_dtExprKeyDefValMap):\n                val = match.group(key)\n                if val:\n                    if (key == 'microsecond'):\n                        val = (val[:6] + ('0' * (6 - len(val))))\n                    dt[key] = int(val)\n                elif defaultVal:\n                    dt[key] = defaultVal\n            delta = None\n            if (dt.get('hour', 0) == 24):\n                if ((dt.get('minute', 0) == 0) and (dt.get('second', 0) == 0) and (dt.get('microsecond', 0) == 0)):\n                    dt['hour'] = 23\n                    delta = timedelta(hours=1)\n                else:\n                    return None\n            tzInfo = None\n            val = match.group('tzutc')\n            if val:\n                tzInfo = TZManager.GetTZInfo()\n            else:\n                val = match.group('tzhr')\n                if val:\n                    tzhr = int(val)\n                    utcsign = val[0]\n                    tzmin = 0\n                    val = match.group('tzmin')\n                    if val:\n                        tzmin = (((tzhr >= 0) and int(val)) or (- int(val)))\n                    tzname = 'UTC'\n                    if ((tzhr != 0) or (tzmin != 0)):\n                        tzname += (' %s%02d:%02d' % (utcsign, abs(tzhr), abs(tzmin)))\n                    tzInfo = TZManager.GetTZInfo(tzname=tzname, utcOffset=timedelta(hours=tzhr, minutes=tzmin))\n            if tzInfo:\n                dt['tzinfo'] = tzInfo\n            datetimeVal = datetime(**dt)\n            if delta:\n                datetimeVal += delta\n        except Exception as e:\n            pass\n    return datetimeVal\n", "label": 1}
{"function": "\n\n@scope.define_pure\ndef idxs_map(idxs, cmd, *args, **kwargs):\n    '\\n    Return the cmd applied at positions idxs, by retrieving args and kwargs\\n    from the (idxs, vals) pair elements of `args` and `kwargs`.\\n\\n    N.B. args and kwargs may generally include information for more idx values\\n    than are requested by idxs.\\n    '\n    if 0:\n        for (ii, (idxs_ii, vals_ii)) in enumerate(args):\n            for jj in idxs:\n                assert (jj in idxs_ii)\n        for (kw, (idxs_kw, vals_kw)) in kwargs.items():\n            for jj in idxs:\n                assert (jj in idxs_kw)\n    args_imap = []\n    for (idxs_j, vals_j) in args:\n        if len(idxs_j):\n            args_imap.append(dict(zip(idxs_j, vals_j)))\n        else:\n            args_imap.append({\n                \n            })\n    kwargs_imap = {\n        \n    }\n    for (kw, (idxs_j, vals_j)) in kwargs.items():\n        if len(idxs_j):\n            kwargs_imap[kw] = dict(zip(idxs_j, vals_j))\n        else:\n            kwargs_imap[kw] = {\n                \n            }\n    f = scope._impls[cmd]\n    rval = []\n    for ii in idxs:\n        try:\n            args_nn = [arg_imap[ii] for arg_imap in args_imap]\n        except:\n            ERR(('args_nn %s' % cmd))\n            ERR(('ii %s' % ii))\n            ERR(('arg_imap %s' % str(arg_imap)))\n            ERR(('args_imap %s' % str(args_imap)))\n            raise\n        try:\n            kwargs_nn = dict([(kw, arg_imap[ii]) for (kw, arg_imap) in kwargs_imap.items()])\n        except:\n            ERR(('args_nn %s' % cmd))\n            ERR(('ii %s' % ii))\n            ERR(('kw %s' % kw))\n            ERR(('arg_imap %s' % str(arg_imap)))\n            raise\n        try:\n            rval_nn = f(*args_nn, **kwargs_nn)\n        except:\n            ERR(('error calling impl of %s' % cmd))\n            raise\n        rval.append(rval_nn)\n    return rval\n", "label": 1}
{"function": "\n\ndef scan_asset_folder(self, folder):\n    pak_path = os.path.join(folder, 'packed.pak')\n    if os.path.isfile(pak_path):\n        db = starbound.open_file(pak_path)\n        index = [(x, pak_path) for x in db.get_index()]\n        return index\n    else:\n        index = []\n        mod_assets = None\n        files = os.listdir(folder)\n        found_mod_info = False\n        for f in files:\n            if f.endswith('.modinfo'):\n                modinfo = os.path.join(folder, f)\n                try:\n                    modinfo_data = load_asset_file(modinfo)\n                    path = './'\n                    if ('path' in modinfo_data.keys()):\n                        path = modinfo_data['path']\n                    mod_assets = os.path.join(folder, path)\n                    found_mod_info = True\n                except ValueError:\n                    folder = os.path.join(folder, 'assets')\n                    if os.path.isdir(folder):\n                        mod_assets = folder\n        if (mod_assets is None):\n            return index\n        elif (found_mod_info and self.is_packed_file(mod_assets)):\n            pak_path = os.path.normpath(mod_assets)\n            db = starbound.open_file(pak_path)\n            for x in db.get_index():\n                if (re.match(ignore_assets, x) is None):\n                    index.append((x, pak_path))\n            return index\n        elif (not os.path.isdir(mod_assets)):\n            return index\n        for (root, dirs, files) in os.walk(mod_assets):\n            for f in files:\n                if (re.match(ignore_assets, f) is None):\n                    asset_folder = os.path.normpath(mod_assets)\n                    asset_file = os.path.normpath(os.path.join(root.replace(folder, ''), f))\n                    index.append((asset_file, asset_folder))\n        return index\n", "label": 1}
{"function": "\n\ndef initialize_layers(self, layers=None):\n    if (layers is not None):\n        self.layers = layers\n    self.layers_ = Layers()\n    if isinstance(self.layers[0], Layer):\n        for (i, layer) in enumerate(get_all_layers(self.layers[0])):\n            name = (layer.name or self._layer_name(layer.__class__, i))\n            self.layers_[name] = layer\n            if (self._get_params_for(name) != {\n                \n            }):\n                raise ValueError(\"You can't use keyword params when passing a Lasagne instance object as the 'layers' parameter of 'NeuralNet'.\")\n        return self.layers[0]\n    layer = None\n    for (i, layer_def) in enumerate(self.layers):\n        if isinstance(layer_def[0], basestring):\n            (layer_name, layer_factory) = layer_def\n            layer_kw = {\n                'name': layer_name,\n            }\n        else:\n            (layer_factory, layer_kw) = layer_def\n            layer_kw = layer_kw.copy()\n        if ('name' not in layer_kw):\n            layer_kw['name'] = self._layer_name(layer_factory, i)\n        more_params = self._get_params_for(layer_kw['name'])\n        layer_kw.update(more_params)\n        if (layer_kw['name'] in self.layers_):\n            raise ValueError('Two layers with name {}.'.format(layer_kw['name']))\n        if (not issubclass(layer_factory, InputLayer)):\n            if ('incoming' in layer_kw):\n                layer_kw['incoming'] = self.layers_[layer_kw['incoming']]\n            elif ('incomings' in layer_kw):\n                layer_kw['incomings'] = [self.layers_[name] for name in layer_kw['incomings']]\n            else:\n                layer_kw['incoming'] = layer\n        for attr in ('W', 'b'):\n            if isinstance(layer_kw.get(attr), str):\n                name = layer_kw[attr]\n                layer_kw[attr] = getattr(self.layers_[name], attr, None)\n        try:\n            layer_wrapper = layer_kw.pop('layer_wrapper', None)\n            layer = layer_factory(**layer_kw)\n        except TypeError as e:\n            msg = 'Failed to instantiate {} with args {}.\\nMaybe parameter names have changed?'.format(layer_factory, layer_kw)\n            chain_exception(TypeError(msg), e)\n        self.layers_[layer_kw['name']] = layer\n        if (layer_wrapper is not None):\n            layer = layer_wrapper(layer)\n            self.layers_[('LW_%s' % layer_kw['name'])] = layer\n    return layer\n", "label": 1}
{"function": "\n\ndef test_mpcfun_real_imag():\n    mp.dps = 15\n    x = mpf(0.3)\n    y = mpf(0.4)\n    assert (exp(mpc(x, 0)) == exp(x))\n    assert (exp(mpc(0, y)) == mpc(cos(y), sin(y)))\n    assert (cos(mpc(x, 0)) == cos(x))\n    assert (sin(mpc(x, 0)) == sin(x))\n    assert (cos(mpc(0, y)) == cosh(y))\n    assert (sin(mpc(0, y)) == mpc(0, sinh(y)))\n    assert (cospi(mpc(x, 0)) == cospi(x))\n    assert (sinpi(mpc(x, 0)) == sinpi(x))\n    assert cospi(mpc(0, y)).ae(cosh((pi * y)))\n    assert sinpi(mpc(0, y)).ae(mpc(0, sinh((pi * y))))\n    (c, s) = cospi_sinpi(mpc(x, 0))\n    assert (c == cospi(x))\n    assert (s == sinpi(x))\n    (c, s) = cospi_sinpi(mpc(0, y))\n    assert c.ae(cosh((pi * y)))\n    assert s.ae(mpc(0, sinh((pi * y))))\n    (c, s) = cos_sin(mpc(x, 0))\n    assert (c == cos(x))\n    assert (s == sin(x))\n    (c, s) = cos_sin(mpc(0, y))\n    assert (c == cosh(y))\n    assert (s == mpc(0, sinh(y)))\n", "label": 1}
{"function": "\n\ndef test_complex_inverse_functions():\n    mp.dps = 15\n    iv.dps = 15\n    for (z1, z2) in random_complexes(30):\n        assert sinh(asinh(z1)).ae(z1)\n        assert acosh(z1).ae(cmath.acosh(z1))\n        assert atanh(z1).ae(cmath.atanh(z1))\n        assert atan(z1).ae(cmath.atan(z1))\n        assert asin(z1).ae(cmath.asin(z1), rel_eps=1e-12)\n        assert acos(z1).ae(cmath.acos(z1), rel_eps=1e-12)\n        one = mpf(1)\n    for i in range((- 9), 10, 3):\n        for k in range((- 9), 10, 3):\n            a = (((0.9 * j) * (10 ** k)) + ((0.8 * one) * (10 ** i)))\n            b = cos(acos(a))\n            assert b.ae(a)\n            b = sin(asin(a))\n            assert b.ae(a)\n    one = mpf(1)\n    err = (2 * (10 ** (- 15)))\n    for i in range((- 9), 9, 3):\n        for k in range((- 9), 9, 3):\n            a = (((- 0.9) * (10 ** k)) + (((j * 0.8) * one) * (10 ** i)))\n            b = cosh(acosh(a))\n            assert b.ae(a, err)\n            b = sinh(asinh(a))\n            assert b.ae(a, err)\n", "label": 1}
{"function": "\n\ndef test_get_memory_maps(self):\n    p = psutil.Process(os.getpid())\n    maps = p.get_memory_maps()\n    paths = [x for x in maps]\n    self.assertEqual(len(paths), len(set(paths)))\n    ext_maps = p.get_memory_maps(grouped=False)\n    for nt in maps:\n        if (not nt.path.startswith('[')):\n            assert os.path.isabs(nt.path), nt.path\n            if POSIX:\n                assert os.path.exists(nt.path), nt.path\n            elif ('64' not in os.path.basename(nt.path)):\n                assert os.path.exists(nt.path), nt.path\n    for nt in ext_maps:\n        for fname in nt._fields:\n            value = getattr(nt, fname)\n            if (fname == 'path'):\n                continue\n            elif (fname in ('addr', 'perms')):\n                assert value, value\n            else:\n                assert isinstance(value, (int, long))\n                assert (value >= 0), value\n", "label": 1}
{"function": "\n\ndef _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight, max_samples, seeds, total_n_estimators, verbose):\n    'Private function used to build a batch of estimators within a job.'\n    (n_samples, n_features) = X.shape\n    max_features = ensemble.max_features\n    if ((not isinstance(max_samples, (numbers.Integral, np.integer))) and (0.0 < max_samples <= 1.0)):\n        max_samples = int((max_samples * n_samples))\n    if ((not isinstance(max_features, (numbers.Integral, np.integer))) and (0.0 < max_features <= 1.0)):\n        max_features = int((max_features * n_features))\n    bootstrap = ensemble.bootstrap\n    bootstrap_features = ensemble.bootstrap_features\n    support_sample_weight = has_fit_parameter(ensemble.base_estimator_, 'sample_weight')\n    if ((not support_sample_weight) and (sample_weight is not None)):\n        raise ValueError(\"The base estimator doesn't support sample weight\")\n    estimators = []\n    estimators_samples = []\n    estimators_features = []\n    for i in range(n_estimators):\n        if (verbose > 1):\n            print(('Building estimator %d of %d for this parallel run (total %d)...' % ((i + 1), n_estimators, total_n_estimators)))\n        random_state = check_random_state(seeds[i])\n        seed = random_state.randint(MAX_INT)\n        estimator = ensemble._make_estimator(append=False)\n        try:\n            estimator.set_params(random_state=seed)\n        except ValueError:\n            pass\n        if bootstrap_features:\n            features = random_state.randint(0, n_features, max_features)\n        else:\n            features = sample_without_replacement(n_features, max_features, random_state=random_state)\n        if support_sample_weight:\n            if (sample_weight is None):\n                curr_sample_weight = np.ones((n_samples,))\n            else:\n                curr_sample_weight = sample_weight.copy()\n            if bootstrap:\n                indices = random_state.randint(0, n_samples, max_samples)\n                sample_counts = bincount(indices, minlength=n_samples)\n                curr_sample_weight *= sample_counts\n            else:\n                not_indices = sample_without_replacement(n_samples, (n_samples - max_samples), random_state=random_state)\n                curr_sample_weight[not_indices] = 0\n            estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)\n            samples = (curr_sample_weight > 0.0)\n        else:\n            if bootstrap:\n                indices = random_state.randint(0, n_samples, max_samples)\n            else:\n                indices = sample_without_replacement(n_samples, max_samples, random_state=random_state)\n            sample_counts = bincount(indices, minlength=n_samples)\n            estimator.fit(X[indices][:, features], y[indices])\n            samples = (sample_counts > 0.0)\n        estimators.append(estimator)\n        estimators_samples.append(samples)\n        estimators_features.append(features)\n    return (estimators, estimators_samples, estimators_features)\n", "label": 1}
{"function": "\n\ndef test_cyclotomic():\n    mp.dps = 15\n    assert ([cyclotomic(n, 1) for n in range(31)] == [1, 0, 2, 3, 2, 5, 1, 7, 2, 3, 1, 11, 1, 13, 1, 1, 2, 17, 1, 19, 1, 1, 1, 23, 1, 5, 1, 3, 1, 29, 1])\n    assert ([cyclotomic(n, (- 1)) for n in range(31)] == [1, (- 2), 0, 1, 2, 1, 3, 1, 2, 1, 5, 1, 1, 1, 7, 1, 2, 1, 3, 1, 1, 1, 11, 1, 1, 1, 13, 1, 1, 1, 1])\n    assert ([cyclotomic(n, j) for n in range(21)] == [1, ((- 1) + j), (1 + j), j, 0, 1, (- j), j, 2, (- j), 1, j, 3, 1, (- j), 1, 2, 1, j, j, 5])\n    assert ([cyclotomic(n, (- j)) for n in range(21)] == [1, ((- 1) - j), (1 - j), (- j), 0, 1, j, (- j), 2, j, 1, (- j), 3, 1, j, 1, 2, 1, (- j), (- j), 5])\n    assert (cyclotomic(1624, j) == 1)\n    assert (cyclotomic(33600, j) == 1)\n    u = sqrt(j, prec=500)\n    assert cyclotomic(8, u).ae(0)\n    assert cyclotomic(30, u).ae(5.82842712474619)\n    assert cyclotomic(2040, u).ae(1)\n    assert (cyclotomic(0, 2.5) == 1)\n    assert (cyclotomic(1, 2.5) == (2.5 - 1))\n    assert (cyclotomic(2, 2.5) == (2.5 + 1))\n    assert (cyclotomic(3, 2.5) == (((2.5 ** 2) + 2.5) + 1))\n    assert (cyclotomic(7, 2.5) == 406.234375)\n", "label": 1}
{"function": "\n\ndef _validate_python(self, value, state):\n    if (not value):\n        raise Invalid(self.message('empty', state), value, state)\n    value = value.strip()\n    splitted = value.split('@', 1)\n    try:\n        (username, domain) = splitted\n    except ValueError:\n        raise Invalid(self.message('noAt', state), value, state)\n    if (not self.usernameRE.search(username)):\n        raise Invalid(self.message('badUsername', state, username=username), value, state)\n    try:\n        idna_domain = [idna.ToASCII(p) for p in domain.split('.')]\n        if (six.text_type is str):\n            idna_domain = [p.decode('ascii') for p in idna_domain]\n        idna_domain = '.'.join(idna_domain)\n    except UnicodeError:\n        raise Invalid(self.message('badDomain', state, domain=domain), value, state)\n    if (not self.domainRE.search(idna_domain)):\n        raise Invalid(self.message('badDomain', state, domain=domain), value, state)\n    if self.resolve_domain:\n        assert have_dns, 'dnspython should be available'\n        global socket\n        if (socket is None):\n            import socket\n        try:\n            try:\n                dns.resolver.query(domain, 'MX')\n            except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                try:\n                    dns.resolver.query(domain, 'A')\n                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                    raise Invalid(self.message('domainDoesNotExist', state, domain=domain), value, state)\n        except (socket.error, dns.exception.DNSException) as e:\n            raise Invalid(self.message('socketError', state, error=e), value, state)\n", "label": 1}
{"function": "\n\ndef get_id(opts, cache_minion_id=False):\n    '\\n    Guess the id of the minion.\\n\\n    If CONFIG_DIR/minion_id exists, use the cached minion ID from that file.\\n    If no minion id is configured, use multiple sources to find a FQDN.\\n    If no FQDN is found you may get an ip address.\\n\\n    Returns two values: the detected ID, and a boolean value noting whether or\\n    not an IP address is being used for the ID.\\n    '\n    if (opts['root_dir'] is None):\n        root_dir = salt.syspaths.ROOT_DIR\n    else:\n        root_dir = opts['root_dir']\n    config_dir = salt.syspaths.CONFIG_DIR\n    if config_dir.startswith(salt.syspaths.ROOT_DIR):\n        config_dir = config_dir.split(salt.syspaths.ROOT_DIR, 1)[(- 1)]\n    id_cache = os.path.join(root_dir, config_dir.lstrip(os.path.sep), 'minion_id')\n    if opts.get('minion_id_caching', True):\n        try:\n            with salt.utils.fopen(id_cache) as idf:\n                name = idf.readline().strip()\n                bname = salt.utils.to_bytes(name)\n                if bname.startswith(codecs.BOM):\n                    name = salt.utils.to_str(bname.replace(codecs.BOM, '', 1))\n            if name:\n                log.debug('Using cached minion ID from {0}: {1}'.format(id_cache, name))\n                return (name, False)\n        except (IOError, OSError):\n            pass\n    if (('__role' in opts) and (opts.get('__role') == 'minion')):\n        log.debug('Guessing ID. The id can be explicitly set in {0}'.format(os.path.join(salt.syspaths.CONFIG_DIR, 'minion')))\n    newid = salt.utils.network.generate_minion_id()\n    if (('__role' in opts) and (opts.get('__role') == 'minion')):\n        log.info('Found minion id from generate_minion_id(): {0}'.format(newid))\n    if (cache_minion_id and opts.get('minion_id_caching', True)):\n        _cache_id(newid, id_cache)\n    is_ipv4 = ((newid.count('.') == 3) and (not any((c.isalpha() for c in newid))))\n    return (newid, is_ipv4)\n", "label": 1}
{"function": "\n\ndef _call(self, transport_dispatcher, transport_domain, transport_address, whole_msg):\n    if (not whole_msg):\n        return\n    msg_version = int(api.decodeMessageVersion(whole_msg))\n    if (msg_version in api.protoModules):\n        proto_module = api.protoModules[msg_version]\n    else:\n        stats.incr('unsupported-notification', 1)\n        logging.error('Unsupported SNMP version %s', msg_version)\n        return\n    host = transport_address[0]\n    version = SNMP_VERSIONS[msg_version]\n    try:\n        (req_msg, whole_msg) = decoder.decode(whole_msg, asn1Spec=proto_module.Message())\n    except (ProtocolError, ValueConstraintError) as err:\n        stats.incr('unsupported-notification', 1)\n        logging.warning('Failed to receive trap (%s) from %s: %s', version, host, err)\n        return\n    req_pdu = proto_module.apiMessage.getPDU(req_msg)\n    community = proto_module.apiMessage.getCommunity(req_msg)\n    if (self.community and (community != self.community)):\n        stats.incr('unauthenticated-notification', 1)\n        logging.warning('Received trap from %s with invalid community: %s... discarding', host, community)\n        return\n    if (not req_pdu.isSameTypeWith(proto_module.TrapPDU())):\n        stats.incr('unsupported-notification', 1)\n        logging.warning('Received non-trap notification from %s', host)\n        return\n    if (msg_version not in (api.protoVersion1, api.protoVersion2c)):\n        stats.incr('unsupported-notification', 1)\n        logging.warning('Received trap not in v1 or v2c')\n        return\n    trap = Notification.from_pdu(host, proto_module, version, req_pdu)\n    if (trap is None):\n        stats.incr('unsupported-notification', 1)\n        logging.warning('Invalid trap from %s: %s', host, req_pdu)\n        return\n    dde = DdeNotification(trap, self.config.handlers[trap.oid])\n    dde_run(dde)\n    handler = dde.handler\n    trap.severity = handler['severity']\n    trap.manager = self.hostname\n    if handler.get('expiration', None):\n        expires = parse_time_string(handler['expiration'])\n        expires = timedelta(**expires)\n        trap.expires = (trap.sent + expires)\n    stats.incr('traps_received', 1)\n    objid = ObjectId(trap.oid)\n    if handler.get('blackhole', False):\n        stats.incr('traps_blackholed', 1)\n        logging.debug('Blackholed %s from %s', objid.name, host)\n        return\n    logging.info('Trap Received (%s) from %s', objid.name, host)\n    stats.incr('traps_accepted', 1)\n    duplicate = False\n    try:\n        stats.incr('db_write_attempted', 1)\n        self.conn.add(trap)\n        self.conn.commit()\n        stats.incr('db_write_successful', 1)\n    except OperationalError as err:\n        self.conn.rollback()\n        logging.warning('Failed to commit: %s', err)\n        stats.incr('db_write_failed', 1)\n    except InvalidRequestError as err:\n        stats.incr('db_write_failed', 1)\n        self.conn.rollback()\n        logging.warning('Bad state, rolling back transaction: %s', err)\n    except IntegrityError as err:\n        stats.incr('db_write_duplicate', 1)\n        duplicate = True\n        self.conn.rollback()\n        logging.info('Duplicate Trap (%s) from %s. Likely inserted by another manager.', objid.name, host)\n        logging.debug(err)\n    self._send_mail(handler, trap, duplicate)\n", "label": 1}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a datetime. Returns a\\n        Python datetime.datetime object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value\n    if isinstance(value, datetime.date):\n        return datetime.datetime(value.year, value.month, value.day)\n    if isinstance(value, list):\n        if (len(value) != 4):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES) and (value[2] in validators.EMPTY_VALUES) and (value[3] in validators.EMPTY_VALUES)):\n            return None\n        start_value = ('%s %s' % tuple(value[:2]))\n        end_value = ('%s %s' % tuple(value[2:]))\n    start_datetime = None\n    end_datetime = None\n    for format in (self.input_formats or formats.get_format('DATETIME_INPUT_FORMATS')):\n        try:\n            start_datetime = datetime.datetime(*time.strptime(start_value, format)[:6])\n        except ValueError:\n            continue\n    for format in (self.input_formats or formats.get_format('DATETIME_INPUT_FORMATS')):\n        try:\n            end_datetime = datetime.datetime(*time.strptime(end_value, format)[:6])\n        except ValueError:\n            continue\n    return (start_datetime, end_datetime)\n", "label": 1}
{"function": "\n\ndef test_lrucache(self):\n    c = LRUCache(2, dispose=(lambda _: None))\n    assert (len(c) == 0)\n    assert (c.items() == set())\n    for (i, x) in enumerate('abc'):\n        c[x] = i\n    assert (len(c) == 2)\n    assert (c.items() == set([('b', 1), ('c', 2)]))\n    assert ('a' not in c)\n    assert ('b' in c)\n    with pytest.raises(KeyError):\n        c['a']\n    assert (c['b'] == 1)\n    assert (c['c'] == 2)\n    c['d'] = 3\n    assert (len(c) == 2)\n    assert (c['c'] == 2)\n    assert (c['d'] == 3)\n    del c['c']\n    assert (len(c) == 1)\n    with pytest.raises(KeyError):\n        c['c']\n    assert (c['d'] == 3)\n    c.clear()\n    assert (c.items() == set())\n", "label": 1}
{"function": "\n\ndef __init__(self, app=None, handler=None):\n    ':func:`burpui.misc.auth.ldap.LdapLoader.__init__` establishes a\\n        connection to the LDAP server.\\n\\n        :param app: Instance of the app we are running in\\n        :type app: :class:`burpui.server.BUIServer`\\n        '\n    self.app = app\n    conf = self.app.config['CFG']\n    defaults = {\n        'host': 'localhost',\n        'port': None,\n        'encryption': None,\n        'binddn': None,\n        'bindpw': None,\n        'filter': None,\n        'base': None,\n        'searchattr': 'uid',\n        'validate': 'none',\n        'version': None,\n        'cafile': None,\n    }\n    mapping = {\n        'host': 'host',\n        'port': 'port',\n        'encryption': 'encryption',\n        'filt': 'filter',\n        'base': 'base',\n        'attr': 'searchattr',\n        'binddn': 'binddn',\n        'bindpw': 'bindpw',\n        'validate': 'validate',\n        'version': 'version',\n        'cafile': 'cafile',\n    }\n    c = ConfigParser.ConfigParser(defaults)\n    with open(conf) as fp:\n        c.readfp(fp)\n        try:\n            handler.priority = c.getint('LDAP', 'priority')\n        except:\n            pass\n        for (opt, key) in viewitems(mapping):\n            try:\n                setattr(self, opt, c.get('LDAP', key))\n            except ConfigParser.NoOptionError as e:\n                self.builogger.info(str(e))\n            except ConfigParser.NoSectionError as e:\n                self.builogger.error(str(e))\n    if (self.validate and (self.validate.lower() in ['none', 'optional', 'required'])):\n        self.validate = getattr(ssl, 'CERT_{}'.format(self.validate.upper()))\n    else:\n        self.validate = None\n    if (self.version and (self.version in ['SSLv2', 'SSLv3', 'SSLv23', 'TLSv1', 'TLSv1_1'])):\n        self.version = getattr(ssl, 'PROTOCOL_{}'.format(self.version))\n    else:\n        self.version = None\n    self.tls = None\n    self.ssl = False\n    self.auto_bind = AUTO_BIND_NONE\n    if (self.encryption == 'ssl'):\n        self.ssl = True\n    elif (self.encryption == 'tls'):\n        self.tls = Tls(local_certificate_file=self.cafile, validate=self.validate, version=self.version)\n        self.auto_bind = AUTO_BIND_TLS_BEFORE_BIND\n    if self.port:\n        try:\n            self.port = int(self.port)\n        except ValueError:\n            self.builogger.error('LDAP port must be a valid integer')\n            self.port = None\n    self.builogger.info('LDAP host: {0}'.format(self.host))\n    self.builogger.info('LDAP port: {0}'.format(self.port))\n    self.builogger.info('LDAP encryption: {0}'.format(self.encryption))\n    self.builogger.info('LDAP filter: {0}'.format(self.filt))\n    self.builogger.info('LDAP base: {0}'.format(self.base))\n    self.builogger.info('LDAP search attr: {0}'.format(self.attr))\n    self.builogger.info('LDAP binddn: {0}'.format(self.binddn))\n    self.builogger.info('LDAP bindpw: {0}'.format(('*****' if self.bindpw else 'None')))\n    self.builogger.info('TLS object: {0}'.format(self.tls))\n    try:\n        self.server = Server(host=self.host, port=self.port, use_ssl=self.ssl, get_info=ALL, tls=self.tls)\n        self.builogger.debug('LDAP Server = {0}'.format(str(self.server)))\n        if self.binddn:\n            self.ldap = Connection(self.server, user=self.binddn, password=self.bindpw, raise_exceptions=True, client_strategy=RESTARTABLE, auto_bind=self.auto_bind, authentication=SIMPLE)\n        else:\n            self.ldap = Connection(self.server, raise_exceptions=True, client_strategy=RESTARTABLE, auto_bind=self.auto_bind)\n        with self.ldap:\n            self.builogger.debug('LDAP Connection = {0}'.format(str(self.ldap)))\n            self.builogger.info('OK, connected to LDAP')\n            return\n        raise Exception('Not connected')\n    except Exception as e:\n        self.builogger.error('Could not connect to LDAP: {0}'.format(str(e)))\n        self.server = None\n        self.ldap = None\n", "label": 1}
{"function": "\n\ndef test_disk_partitions(self):\n    for disk in psutil.disk_partitions(all=False):\n        assert os.path.exists(disk.device), disk\n        assert os.path.isdir(disk.mountpoint), disk\n        assert disk.fstype, disk\n        assert isinstance(disk.opts, str)\n    for disk in psutil.disk_partitions(all=True):\n        if (not WINDOWS):\n            try:\n                os.stat(disk.mountpoint)\n            except OSError:\n                err = sys.exc_info()[1]\n                if (err.errno not in (errno.EPERM, errno.EACCES)):\n                    raise\n            else:\n                assert os.path.isdir(disk.mountpoint), disk.mountpoint\n        assert isinstance(disk.fstype, str)\n        assert isinstance(disk.opts, str)\n\n    def find_mount_point(path):\n        path = os.path.abspath(path)\n        while (not os.path.ismount(path)):\n            path = os.path.dirname(path)\n        return path\n    mount = find_mount_point(__file__)\n    mounts = [x.mountpoint for x in psutil.disk_partitions(all=True)]\n    self.assertIn(mount, mounts)\n    psutil.disk_usage(mount)\n", "label": 1}
{"function": "\n\n@pytest.inlineCallbacks\ndef test_dht_multi(self, monkeypatch):\n    iface = '0.0.0.0'\n    a = None\n    b = None\n    q = Queue.Queue()\n\n    def server_started(aa, *args):\n        for b in args:\n            if isinstance(b, twisted.python.failure.Failure):\n                b.printTraceback()\n            else:\n                _log.debug(('** %s' % b))\n        q.put([aa, args])\n    try:\n        amount_of_servers = 5\n        servers = []\n        callbacks = []\n        for servno in range(0, amount_of_servers):\n            a = AutoDHTServer()\n            servers.append(a)\n            callback = CalvinCB(server_started, str(servno))\n            servers[servno].start(iface, network='Niklas', cb=callback, name=(name + '{}'.format(servno)))\n            callbacks.append(callback)\n        started = []\n        while (len(started) < amount_of_servers):\n            try:\n                server = (yield threads.defer_to_thread(q.get))\n            except Queue.Empty:\n                _log.debug('Queue empty!')\n            if (server not in started):\n                started.append(server)\n                callbacks[int(server[0][0])].func = (lambda *args, **kvargs: None)\n            else:\n                print('Server: {} already started. {} out of {}'.format(started, len(started), amount_of_servers))\n        print('All {} out of {} started'.format(started, len(started), amount_of_servers))\n        for servno in range(0, amount_of_servers):\n            assert ([str(servno), self._sucess_start] in started)\n        (yield threads.defer_to_thread(q.queue.clear))\n        (yield threads.defer_to_thread(time.sleep, 8))\n        key = 'HARE'\n        value = json.dumps(['morot'])\n        set_def = servers[0].append(key=key, value=value)\n        set_value = (yield threads.defer_to_thread(set_def.wait, 10))\n        assert set_value\n        print('Node with port {} posted append key={}, value={}'.format(servers[0].dht_server.port.getHost().port, key, value))\n        value = json.dumps(['selleri'])\n        set_def = servers[0].append(key=key, value=value)\n        set_value = (yield threads.defer_to_thread(set_def.wait, 10))\n        assert set_value\n        print('Node with port {} posted append key={}, value={}'.format(servers[0].dht_server.port.getHost().port, key, value))\n        get_def = servers[0].get_concat(key=key)\n        get_value = (yield threads.defer_to_thread(get_def.wait, 10))\n        assert (set(json.loads(get_value)) == set(['morot', 'selleri']))\n        print('Node with port {} confirmed key={}, value={} was reachable'.format(servers[0].dht_server.port.getHost().port, key, get_value))\n        drawNetworkState('1nice_graph.png', servers, amount_of_servers)\n        (yield threads.defer_to_thread(time.sleep, 7))\n        drawNetworkState('1middle_graph.png', servers, amount_of_servers)\n        (yield threads.defer_to_thread(time.sleep, 7))\n        drawNetworkState('1end_graph.png', servers, amount_of_servers)\n        get_def = servers[0].get_concat(key=key)\n        get_value = (yield threads.defer_to_thread(get_def.wait, 10))\n        assert (set(json.loads(get_value)) == set(['morot', 'selleri']))\n        print('Node with port {} got right value: {}'.format(servers[0].dht_server.port.getHost().port, get_value))\n        value = json.dumps(['morot'])\n        set_def = servers[0].remove(key=key, value=value)\n        set_value = (yield threads.defer_to_thread(set_def.wait, 10))\n        assert set_value\n        print('Node with port {} posted remove key={}, value={}'.format(servers[0].dht_server.port.getHost().port, key, value))\n        get_def = servers[1].get_concat(key=key)\n        get_value = (yield threads.defer_to_thread(get_def.wait, 10))\n        assert (set(json.loads(get_value)) == set(['selleri']))\n        print('Node with port {} got right value: {}'.format(servers[0].dht_server.port.getHost().port, get_value))\n        for i in range(0, amount_of_servers):\n            name_dir = os.path.join(_cert_conf['CA_default']['runtimes_dir'], '{}{}'.format(name, i))\n            filenames = os.listdir(os.path.join(name_dir, 'others'))\n            print('Node with port {} has {} certificates in store'.format(servers[i].dht_server.port.getHost().port, len(filenames)))\n    except AssertionError as e:\n        print('Node with port {} got wrong value: {}, should have been {}'.format(servers[0].dht_server.port.getHost().port, get_value, value))\n        pytest.fail(traceback.format_exc())\n    except Exception as e:\n        traceback.print_exc()\n        pytest.fail(traceback.format_exc())\n    finally:\n        (yield threads.defer_to_thread(time.sleep, 10))\n        i = 0\n        for server in servers:\n            name_dir = os.path.join(_cert_conf['CA_default']['runtimes_dir'], (name + '{}'.format(i)))\n            shutil.rmtree(os.path.join(name_dir, 'others'), ignore_errors=True)\n            os.mkdir(os.path.join(name_dir, 'others'))\n            i += 1\n            server.stop()\n", "label": 1}
{"function": "\n\n@coroutine\ndef _wrap_awaitable(x):\n    if hasattr(x, '__await__'):\n        _i = x.__await__()\n    else:\n        _i = iter(x)\n    try:\n        _y = next(_i)\n    except StopIteration as _e:\n        _r = _value_from_stopiteration(_e)\n    else:\n        while 1:\n            try:\n                _s = (yield _y)\n            except GeneratorExit as _e:\n                try:\n                    _m = _i.close\n                except AttributeError:\n                    pass\n                else:\n                    _m()\n                raise _e\n            except BaseException as _e:\n                _x = sys.exc_info()\n                try:\n                    _m = _i.throw\n                except AttributeError:\n                    raise _e\n                else:\n                    try:\n                        _y = _m(*_x)\n                    except StopIteration as _e:\n                        _r = _value_from_stopiteration(_e)\n                        break\n            else:\n                try:\n                    if (_s is None):\n                        _y = next(_i)\n                    else:\n                        _y = _i.send(_s)\n                except StopIteration as _e:\n                    _r = _value_from_stopiteration(_e)\n                    break\n    raise Return(_r)\n", "label": 1}
{"function": "\n\ndef _init(self, trcback):\n    'format a traceback from sys.exc_info() into 7-item tuples,\\n        containing the regular four traceback tuple items, plus the original\\n        template filename, the line number adjusted relative to the template\\n        source, and code line from that line number of the template.'\n    import mako.template\n    mods = {\n        \n    }\n    rawrecords = traceback.extract_tb(trcback)\n    new_trcback = []\n    for (filename, lineno, function, line) in rawrecords:\n        if (not line):\n            line = ''\n        try:\n            (line_map, template_lines) = mods[filename]\n        except KeyError:\n            try:\n                info = mako.template._get_module_info(filename)\n                module_source = info.code\n                template_source = info.source\n                template_filename = (info.template_filename or filename)\n            except KeyError:\n                if (not compat.py3k):\n                    try:\n                        fp = open(filename, 'rb')\n                        encoding = util.parse_encoding(fp)\n                        fp.close()\n                    except IOError:\n                        encoding = None\n                    if encoding:\n                        line = line.decode(encoding)\n                    else:\n                        line = line.decode('ascii', 'replace')\n                new_trcback.append((filename, lineno, function, line, None, None, None, None))\n                continue\n            template_ln = module_ln = 1\n            line_map = {\n                \n            }\n            for line in module_source.split('\\n'):\n                match = re.match('\\\\s*# SOURCE LINE (\\\\d+)', line)\n                if match:\n                    template_ln = int(match.group(1))\n                module_ln += 1\n                line_map[module_ln] = template_ln\n            template_lines = [line for line in template_source.split('\\n')]\n            mods[filename] = (line_map, template_lines)\n        template_ln = line_map[lineno]\n        if (template_ln <= len(template_lines)):\n            template_line = template_lines[(template_ln - 1)]\n        else:\n            template_line = None\n        new_trcback.append((filename, lineno, function, line, template_filename, template_ln, template_line, template_source))\n    if (not self.source):\n        for l in range((len(new_trcback) - 1), 0, (- 1)):\n            if new_trcback[l][5]:\n                self.source = new_trcback[l][7]\n                self.lineno = new_trcback[l][5]\n                break\n        else:\n            if new_trcback:\n                try:\n                    fp = open(new_trcback[(- 1)][0], 'rb')\n                    encoding = util.parse_encoding(fp)\n                    fp.seek(0)\n                    self.source = fp.read()\n                    fp.close()\n                    if encoding:\n                        self.source = self.source.decode(encoding)\n                except IOError:\n                    self.source = ''\n                self.lineno = new_trcback[(- 1)][1]\n    return new_trcback\n", "label": 1}
{"function": "\n\ndef _caas_2_2_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_tag_tag_ALLPARAMS(self, method, url, body, headers):\n    (_, params) = url.split('?')\n    parameters = params.split('&')\n    for parameter in parameters:\n        (key, value) = parameter.split('=')\n        if (key == 'assetId'):\n            assert (value == 'fake_asset_id')\n        elif (key == 'assetType'):\n            assert (value == 'fake_asset_type')\n        elif (key == 'valueRequired'):\n            assert (value == 'false')\n        elif (key == 'displayOnReport'):\n            assert (value == 'false')\n        elif (key == 'pageSize'):\n            assert (value == '250')\n        elif (key == 'datacenterId'):\n            assert (value == 'fake_location')\n        elif (key == 'value'):\n            assert (value == 'fake_value')\n        elif (key == 'tagKeyName'):\n            assert (value == 'fake_tag_key_name')\n        elif (key == 'tagKeyId'):\n            assert (value == 'fake_tag_key_id')\n        else:\n            raise ValueError('Could not find in url parameters {0}:{1}'.format(key, value))\n    body = self.fixtures.load('tag_tag_list.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 1}
{"function": "\n\ndef _caas_2_2_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_network_vlan_ALLFILTERS(self, method, url, body, headers):\n    (_, params) = url.split('?')\n    parameters = params.split('&')\n    for parameter in parameters:\n        (key, value) = parameter.split('=')\n        if (key == 'datacenterId'):\n            assert (value == 'fake_location')\n        elif (key == 'networkDomainId'):\n            assert (value == 'fake_network_domain')\n        elif (key == 'ipv6Address'):\n            assert (value == 'fake_ipv6')\n        elif (key == 'privateIpv4Address'):\n            assert (value == 'fake_ipv4')\n        elif (key == 'name'):\n            assert (value == 'fake_name')\n        elif (key == 'state'):\n            assert (value == 'fake_state')\n        else:\n            raise ValueError('Could not find in url parameters {0}:{1}'.format(key, value))\n    body = self.fixtures.load('network_vlan.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 1}
{"function": "\n\ndef _complete_authz(self, user, areq, sid, **kwargs):\n    _log_debug = logger.debug\n    _log_debug('- in authenticated() -')\n    try:\n        permission = self.authz(user, client_id=areq['client_id'])\n        self.sdb.update(sid, 'permission', permission)\n    except Exception:\n        raise\n    _log_debug(('response type: %s' % areq['response_type']))\n    if self.sdb.is_revoked(sid):\n        return self._error(error='access_denied', descr='Token is revoked')\n    info = self.create_authn_response(areq, sid)\n    if isinstance(info, Response):\n        return info\n    else:\n        (aresp, fragment_enc) = info\n    try:\n        redirect_uri = self.get_redirect_uri(areq)\n    except (RedirectURIError, ParameterError) as err:\n        return BadRequest(('%s' % err))\n    info = self.aresp_check(aresp, areq)\n    if isinstance(info, Response):\n        return info\n    headers = []\n    try:\n        _kaka = kwargs['cookie']\n    except KeyError:\n        pass\n    else:\n        if (_kaka and (self.cookie_name not in _kaka)):\n            headers.append(self.cookie_func(user, typ='sso', ttl=self.sso_ttl))\n    if ('response_mode' in areq):\n        try:\n            resp = self.response_mode(areq, fragment_enc, aresp=aresp, redirect_uri=redirect_uri, headers=headers)\n        except InvalidRequest as err:\n            return self._error('invalid_request', err)\n        else:\n            if (resp is not None):\n                return resp\n    return (aresp, headers, redirect_uri, fragment_enc)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse(wmsg):\n    '\\n        Verifies and parses an unserialized raw message into an actual WAMP message instance.\\n\\n        :param wmsg: The unserialized raw message.\\n        :type wmsg: list\\n\\n        :returns: An instance of this class.\\n        '\n    assert ((len(wmsg) > 0) and (wmsg[0] == Call.MESSAGE_TYPE))\n    if (len(wmsg) not in (4, 5, 6)):\n        raise ProtocolError('invalid message length {0} for CALL'.format(len(wmsg)))\n    request = check_or_raise_id(wmsg[1], \"'request' in CALL\")\n    options = check_or_raise_extra(wmsg[2], \"'options' in CALL\")\n    procedure = check_or_raise_uri(wmsg[3], \"'procedure' in CALL\")\n    args = None\n    kwargs = None\n    payload = None\n    enc_algo = None\n    enc_key = None\n    enc_serializer = None\n    if ((len(wmsg) == 5) and (type(wmsg[4]) in [six.text_type, six.binary_type])):\n        payload = wmsg[4]\n        enc_algo = options.get('enc_algo', None)\n        if (enc_algo and (enc_algo not in [PAYLOAD_ENC_CRYPTO_BOX])):\n            raise ProtocolError(\"invalid value {0} for 'enc_algo' detail in EVENT\".format(enc_algo))\n        enc_key = options.get('enc_key', None)\n        if (enc_key and (type(enc_key) not in [six.text_type, six.binary_type])):\n            raise ProtocolError(\"invalid type {0} for 'enc_key' detail in EVENT\".format(type(enc_key)))\n        enc_serializer = options.get('enc_serializer', None)\n        if (enc_serializer and (enc_serializer not in ['json', 'msgpack', 'cbor', 'ubjson'])):\n            raise ProtocolError(\"invalid value {0} for 'enc_serializer' detail in EVENT\".format(enc_serializer))\n    else:\n        if (len(wmsg) > 4):\n            args = wmsg[4]\n            if (type(args) != list):\n                raise ProtocolError(\"invalid type {0} for 'args' in CALL\".format(type(args)))\n        if (len(wmsg) > 5):\n            kwargs = wmsg[5]\n            if (type(kwargs) != dict):\n                raise ProtocolError(\"invalid type {0} for 'kwargs' in CALL\".format(type(kwargs)))\n    timeout = None\n    receive_progress = None\n    if ('timeout' in options):\n        option_timeout = options['timeout']\n        if (type(option_timeout) not in six.integer_types):\n            raise ProtocolError(\"invalid type {0} for 'timeout' option in CALL\".format(type(option_timeout)))\n        if (option_timeout < 0):\n            raise ProtocolError(\"invalid value {0} for 'timeout' option in CALL\".format(option_timeout))\n        timeout = option_timeout\n    if ('receive_progress' in options):\n        option_receive_progress = options['receive_progress']\n        if (type(option_receive_progress) != bool):\n            raise ProtocolError(\"invalid type {0} for 'receive_progress' option in CALL\".format(type(option_receive_progress)))\n        receive_progress = option_receive_progress\n    obj = Call(request, procedure, args=args, kwargs=kwargs, payload=payload, timeout=timeout, receive_progress=receive_progress, enc_algo=enc_algo, enc_key=enc_key, enc_serializer=enc_serializer)\n    return obj\n", "label": 1}
{"function": "\n\ndef leastsq(self, **kws):\n    '\\n        use Levenberg-Marquardt minimization to perform fit.\\n        This assumes that ModelParameters have been stored,\\n        and a function to minimize has been properly set up.\\n\\n        This wraps scipy.optimize.leastsq, and keyward arguments are passed\\n        directly as options to scipy.optimize.leastsq\\n\\n        When possible, this calculates the estimated uncertainties and\\n        variable correlations from the covariance matrix.\\n\\n        writes outputs to many internal attributes, and\\n        returns True if fit was successful, False if not.\\n        '\n    self.prepare_fit(force=True)\n    toler = self.toler\n    lskws = dict(xtol=toler, ftol=toler, gtol=toler, maxfev=(1000 * (self.nvarys + 1)), Dfun=None)\n    lskws.update(self.kws)\n    lskws.update(kws)\n    if (lskws['Dfun'] is not None):\n        self.jacfcn = lskws['Dfun']\n        lskws['Dfun'] = self.__jacobian\n    lsout = leastsq(self.__residual, self.vars, **lskws)\n    del self.vars\n    (_best, cov, infodict, errmsg, ier) = lsout\n    resid = infodict['fvec']\n    ndata = len(resid)\n    chisqr = (resid ** 2).sum()\n    nfree = (ndata - self.nvarys)\n    redchi = (chisqr / nfree)\n    group = self.paramgroup\n    grad = ones_like(_best)\n    vbest = ones_like(_best)\n    named_params = {\n        \n    }\n    for (ivar, name) in enumerate(self.var_names):\n        named_params[name] = par = getattr(group, name)\n        grad[ivar] = par.scale_gradient(_best[ivar])\n        vbest[ivar] = par.value\n        par.stderr = 0\n        par.correl = {\n            \n        }\n        par._uval = None\n    infodict['fjac'] = transpose((transpose(infodict['fjac']) / take(grad, (infodict['ipvt'] - 1))))\n    rvec = dot(triu(transpose(infodict['fjac'])[:self.nvarys, :]), take(eye(self.nvarys), (infodict['ipvt'] - 1), 0))\n    try:\n        cov = inv(dot(transpose(rvec), rvec))\n    except (LinAlgError, ValueError):\n        cov = None\n    if (cov is not None):\n        if self.scale_covar:\n            cov = ((cov * chisqr) / nfree)\n        try:\n            uvars = uncertainties.correlated_values(vbest, cov)\n        except (LinAlgError, ValueError):\n            (cov, uvars) = (None, None)\n        group.covar_vars = self.var_names\n        group.covar = cov\n        if (uvars is not None):\n            for (iv, name) in enumerate(self.var_names):\n                p = named_params[name]\n                p.stderr = uvars[iv].std_dev()\n                p._uval = uvars[iv]\n                p.correl = {\n                    \n                }\n                for (jv, name2) in enumerate(self.var_names):\n                    if (jv != iv):\n                        p.correl[name2] = (cov[(iv, jv)] / (p.stderr * sqrt(cov[(jv, jv)])))\n            for nam in dir(self.paramgroup):\n                obj = getattr(self.paramgroup, nam)\n                eval_stderr(obj, uvars, self.var_names, named_params, self._larch)\n            for (uval, nam) in zip(uvars, self.var_names):\n                named_params[nam]._val = uval.nominal_value\n        if self._larch.error:\n            self._larch.error = []\n    message = 'Fit succeeded.'\n    if (ier == 0):\n        message = 'Invalid Input Parameters.'\n    elif (ier == 5):\n        message = (self.err_maxfev % lskws['maxfev'])\n    elif (ier > 5):\n        message = 'See lmdif_message.'\n    if (cov is None):\n        message = ('%s Could not estimate error-bars' % message)\n    ofit = group\n    if (Group is not None):\n        ofit = group.fit_details = Group()\n    ofit.method = 'leastsq'\n    ofit.fjac = infodict['fjac']\n    ofit.fvec = infodict['fvec']\n    ofit.qtf = infodict['qtf']\n    ofit.ipvt = infodict['ipvt']\n    ofit.nfev = infodict['nfev']\n    ofit.status = ier\n    ofit.message = errmsg\n    ofit.success = (ier in [1, 2, 3, 4])\n    ofit.toler = self.toler\n    group.residual = resid\n    group.message = message\n    group.chi_square = chisqr\n    group.chi_reduced = redchi\n    group.nvarys = self.nvarys\n    group.nfree = nfree\n    group.errorbars = (cov is not None)\n    return ier\n", "label": 1}
{"function": "\n\ndef _match_url(self, request):\n    if (self._url is ANY):\n        return True\n    if hasattr(self._url, 'search'):\n        return (self._url.search(request.url) is not None)\n    if (self._url_parts.scheme and (request.scheme != self._url_parts.scheme)):\n        return False\n    if (self._url_parts.netloc and (request.netloc != self._url_parts.netloc)):\n        return False\n    if ((request.path or '/') != (self._url_parts.path or '/')):\n        return False\n    request_qs = urlparse.parse_qs(request.query)\n    matcher_qs = urlparse.parse_qs(self._url_parts.query)\n    for (k, vals) in six.iteritems(matcher_qs):\n        for v in vals:\n            try:\n                request_qs.get(k, []).remove(v)\n            except ValueError:\n                return False\n    if self._complete_qs:\n        for v in six.itervalues(request_qs):\n            if v:\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, fp, headers, params=None, parts=None):\n    self.processors = self.processors.copy()\n    self.fp = fp\n    self.headers = headers\n    if (params is None):\n        params = {\n            \n        }\n    self.params = params\n    if (parts is None):\n        parts = []\n    self.parts = parts\n    self.content_type = headers.elements('Content-Type')\n    if self.content_type:\n        self.content_type = self.content_type[0]\n    else:\n        self.content_type = httputil.HeaderElement.from_str(self.default_content_type)\n    dec = self.content_type.params.get('charset', None)\n    if dec:\n        self.attempt_charsets = ([dec] + [c for c in self.attempt_charsets if (c != dec)])\n    else:\n        self.attempt_charsets = self.attempt_charsets[:]\n    self.length = None\n    clen = headers.get('Content-Length', None)\n    if ((clen is not None) and ('chunked' not in headers.get('Transfer-Encoding', ''))):\n        try:\n            self.length = int(clen)\n        except ValueError:\n            pass\n    self.name = None\n    self.filename = None\n    disp = headers.elements('Content-Disposition')\n    if disp:\n        disp = disp[0]\n        if ('name' in disp.params):\n            self.name = disp.params['name']\n            if (self.name.startswith('\"') and self.name.endswith('\"')):\n                self.name = self.name[1:(- 1)]\n        if ('filename' in disp.params):\n            self.filename = disp.params['filename']\n            if (self.filename.startswith('\"') and self.filename.endswith('\"')):\n                self.filename = self.filename[1:(- 1)]\n", "label": 1}
{"function": "\n\ndef partition_source(src):\n    'Partitions source into a list of `CodePartition`s for import\\n    refactoring.\\n    '\n    if (type(src) is not six.text_type):\n        raise TypeError('Expected text but got `{0}`'.format(type(src)))\n    ast_obj = ast.parse(src.encode('UTF-8'))\n    visitor = TopLevelImportVisitor()\n    visitor.visit(ast_obj)\n    line_offsets = get_line_offsets_by_line_no(src)\n    chunks = []\n    startpos = 0\n    pending_chunk_type = None\n    possible_ending_tokens = None\n    seen_import = False\n    for (token_type, token_text, (srow, scol), (erow, ecol), _) in tokenize.generate_tokens(io.StringIO(src).readline):\n        if (pending_chunk_type is None):\n            if ((not seen_import) and (token_type == tokenize.COMMENT)):\n                if ('noreorder' in token_text):\n                    chunks.append(CodePartition(CodeType.CODE, src[startpos:]))\n                    break\n                else:\n                    pending_chunk_type = CodeType.PRE_IMPORT_CODE\n                    possible_ending_tokens = TERMINATES_COMMENT\n            elif ((not seen_import) and (token_type == tokenize.STRING)):\n                pending_chunk_type = CodeType.PRE_IMPORT_CODE\n                possible_ending_tokens = TERMINATES_DOCSTRING\n            elif ((scol == 0) and (srow in visitor.top_level_import_line_numbers)):\n                seen_import = True\n                pending_chunk_type = CodeType.IMPORT\n                possible_ending_tokens = TERMINATES_IMPORT\n            elif (token_type == tokenize.NL):\n                endpos = (line_offsets[erow] + ecol)\n                srctext = src[startpos:endpos]\n                startpos = endpos\n                chunks.append(CodePartition(CodeType.NON_CODE, srctext))\n            elif (token_type == tokenize.COMMENT):\n                if ('noreorder' in token_text):\n                    chunks.append(CodePartition(CodeType.CODE, src[startpos:]))\n                    break\n                else:\n                    pending_chunk_type = CodeType.CODE\n                    possible_ending_tokens = TERMINATES_COMMENT\n            elif (token_type == tokenize.ENDMARKER):\n                pass\n            else:\n                chunks.append(CodePartition(CodeType.CODE, src[startpos:]))\n                break\n        elif (token_type in possible_ending_tokens):\n            endpos = (line_offsets[erow] + ecol)\n            srctext = src[startpos:endpos]\n            startpos = endpos\n            chunks.append(CodePartition(pending_chunk_type, srctext))\n            pending_chunk_type = None\n            possible_ending_tokens = None\n        elif ((token_type == tokenize.COMMENT) and ('noreorder' in token_text)):\n            chunks.append(CodePartition(CodeType.CODE, src[startpos:]))\n            break\n    assert (_partitions_to_src(chunks) == src)\n    return chunks\n", "label": 1}
{"function": "\n\ndef splice(self, new_str, start, end=None):\n    'Returns a new FmtStr with the input string spliced into the\\n        the original FmtStr at start and end.\\n        If end is provided, new_str will replace the substring self.s[start:end-1].\\n        '\n    if (len(new_str) == 0):\n        return self\n    new_fs = (new_str if isinstance(new_str, FmtStr) else fmtstr(new_str))\n    assert (len(new_fs.basefmtstrs) > 0), (new_fs.basefmtstrs, new_fs)\n    new_components = []\n    inserted = False\n    if (end is None):\n        end = start\n    tail = None\n    for (bfs, bfs_start, bfs_end) in zip(self.basefmtstrs, self.divides[:(- 1)], self.divides[1:]):\n        if (end == bfs_start == 0):\n            new_components.extend(new_fs.basefmtstrs)\n            new_components.append(bfs)\n            inserted = True\n        elif (bfs_start <= start < bfs_end):\n            divide = (start - bfs_start)\n            head = Chunk(bfs.s[:divide], atts=bfs.atts)\n            tail = Chunk(bfs.s[(end - bfs_start):], atts=bfs.atts)\n            new_components.extend(([head] + new_fs.basefmtstrs))\n            inserted = True\n            if (bfs_start < end < bfs_end):\n                tail = Chunk(bfs.s[(end - bfs_start):], atts=bfs.atts)\n                new_components.append(tail)\n        elif (bfs_start < end < bfs_end):\n            divide = (start - bfs_start)\n            tail = Chunk(bfs.s[(end - bfs_start):], atts=bfs.atts)\n            new_components.append(tail)\n        elif ((bfs_start >= end) or (bfs_end <= start)):\n            new_components.append(bfs)\n    if (not inserted):\n        new_components.extend(new_fs.basefmtstrs)\n        inserted = True\n    return FmtStr(*[s for s in new_components if s.s])\n", "label": 1}
{"function": "\n\ndef add_dataset(self, dataset, mode='sequential', batch_size=None, num_batches=None, seed=None):\n    \"\\n        Determines the data used to calculate the values of each channel.\\n\\n        Parameters\\n        ----------\\n        dataset : object\\n            A `pylearn2.datasets.Dataset` object.\\n        mode : str or object, optional\\n            Iteration mode; see the docstring of the `iterator` method\\n            on `pylearn2.datasets.Dataset` for details.\\n        batch_size : int, optional\\n            The size of an individual batch. Optional if `mode` is\\n            'sequential' and `num_batches` is specified (batch size\\n            will be calculated based on full dataset size).\\n        num_batches : int, optional\\n            The total number of batches. Unnecessary if `mode` is\\n            'sequential' and `batch_size` is specified (number of\\n            batches will be calculated based on full dataset size).\\n        seed : int, optional\\n            Optional. The seed to be used for random iteration modes.\\n        \"\n    if (not isinstance(dataset, list)):\n        dataset = [dataset]\n    if (not isinstance(mode, list)):\n        mode = [mode]\n    if (not isinstance(batch_size, list)):\n        batch_size = [batch_size]\n    if (not isinstance(num_batches, list)):\n        num_batches = [num_batches]\n    if (seed is None):\n        seed = ([None] * len(dataset))\n    if (not isinstance(seed, list)):\n        seed = [seed]\n    if (len(mode) != len(dataset)):\n        raise ValueError((((('Received ' + str(len(dataset))) + ' dataset but ') + str(len(mode))) + ' modes.'))\n    if any([(len(l) != len(dataset)) for l in [batch_size, seed]]):\n        raise ValueError(('make sure each dataset has its iteration ' + 'batch size and number of batches.'))\n    for (d, m, b, n, sd) in safe_izip(dataset, mode, batch_size, num_batches, seed):\n        try:\n            it = d.iterator(mode=m, batch_size=b, num_batches=n, data_specs=self._flat_data_specs, return_tuple=True, rng=sd)\n        except ValueError as exc:\n            reraise_as(ValueError((('invalid iteration parameters in ' + 'Monitor.add_dataset: ') + str(exc))))\n        if it.stochastic:\n            if (sd is None):\n                raise TypeError(('Monitor requires a seed when using ' + 'stochastic iteration modes.'))\n            if (not isinstance(sd, (list, tuple, int))):\n                raise TypeError((('Monitor requires a seed (not a random ' + 'number generator) when using ') + 'stochastic iteration modes.'))\n        else:\n            assert (sd is None)\n        if (d not in self._datasets):\n            self._datasets.append(d)\n            self._iteration_mode.append(m)\n            self._batch_size.append(b)\n            self._num_batches.append(n)\n            self._rng_seed.append(sd)\n", "label": 1}
{"function": "\n\ndef format(self, record):\n    'Uses contextstring if request_id is set, otherwise default.'\n    if (not isinstance(record.msg, six.text_type)):\n        record.msg = six.text_type(record.msg)\n    record.project = self.project\n    record.version = self.version\n    instance_extra = ''\n    instance = getattr(record, 'instance', None)\n    instance_uuid = getattr(record, 'instance_uuid', None)\n    context = _update_record_with_context(record)\n    if instance:\n        try:\n            instance_extra = (self.conf.instance_format % instance)\n        except TypeError:\n            instance_extra = instance\n    elif instance_uuid:\n        instance_extra = (self.conf.instance_uuid_format % {\n            'uuid': instance_uuid,\n        })\n    elif context:\n        instance = getattr(context, 'instance', None)\n        instance_uuid = getattr(context, 'instance_uuid', None)\n        resource_uuid = getattr(context, 'resource_uuid', None)\n        if instance:\n            instance_extra = (self.conf.instance_format % {\n                'uuid': instance,\n            })\n        elif instance_uuid:\n            instance_extra = (self.conf.instance_uuid_format % {\n                'uuid': instance_uuid,\n            })\n        elif resource_uuid:\n            instance_extra = (self.conf.instance_uuid_format % {\n                'uuid': resource_uuid,\n            })\n    record.instance = instance_extra\n    for key in ('instance', 'color', 'user_identity', 'resource', 'user_name', 'project_name'):\n        if (key not in record.__dict__):\n            record.__dict__[key] = ''\n    if context:\n        record.user_identity = (self.conf.logging_user_identity_format % _ReplaceFalseValue(context.__dict__))\n    if record.__dict__.get('request_id'):\n        fmt = self.conf.logging_context_format_string\n    else:\n        fmt = self.conf.logging_default_format_string\n    if ((record.levelno == logging.DEBUG) and self.conf.logging_debug_format_suffix):\n        fmt += (' ' + self.conf.logging_debug_format_suffix)\n    self._compute_iso_time(record)\n    if (sys.version_info < (3, 2)):\n        self._fmt = fmt\n    else:\n        self._style = logging.PercentStyle(fmt)\n        self._fmt = self._style._fmt\n    if record.exc_info:\n        record.exc_text = self.formatException(record.exc_info, record)\n    return logging.Formatter.format(self, record)\n", "label": 1}
{"function": "\n\ndef get(self, getme=None, fromEnd=False):\n    if (not getme):\n        return self\n    try:\n        getme = int(getme)\n        if (getme < 0):\n            return self[:((- 1) * getme)]\n        else:\n            return [self[(getme - 1)]]\n    except IndexError:\n        return []\n    except ValueError:\n        rangeResult = self.rangePattern.search(getme)\n        if rangeResult:\n            start = (rangeResult.group('start') or None)\n            end = (rangeResult.group('start') or None)\n            if start:\n                start = (int(start) - 1)\n            if end:\n                end = int(end)\n            return self[start:end]\n        getme = getme.strip()\n        if (getme.startswith('/') and getme.endswith('/')):\n            finder = re.compile(getme[1:(- 1)], ((re.DOTALL | re.MULTILINE) | re.IGNORECASE))\n\n            def isin(hi):\n                return finder.search(hi)\n        else:\n\n            def isin(hi):\n                return (getme.lower() in hi.lowercase)\n        return [itm for itm in self if isin(itm)]\n", "label": 1}
{"function": "\n\ndef get_field(self, field):\n    field = field.split(':')\n    field_type = field[0]\n    if (field_type.lower() == 'char'):\n        try:\n            length = field[2]\n        except IndexError:\n            length = 255\n        try:\n            null = field[3]\n            null = 'False'\n        except IndexError:\n            null = 'True'\n        return (CHARFIELD_TEMPLATE % {\n            'name': field[1],\n            'length': length,\n            'null': null,\n        })\n    elif (field_type.lower() == 'text'):\n        try:\n            null = field[2]\n            null = 'False'\n        except IndexError:\n            null = 'True'\n        return (TEXTFIELD_TEMPLATE % {\n            'name': field[1],\n            'null': null,\n        })\n    elif (field_type.lower() == 'int'):\n        try:\n            null = field[2]\n            null = 'False'\n        except IndexError:\n            null = 'True'\n        try:\n            default = field[3]\n        except IndexError:\n            default = None\n        return (INTEGERFIELD_TEMPLATE % {\n            'name': field[1],\n            'null': null,\n            'default': default,\n        })\n    elif (field_type.lower() == 'decimal'):\n        try:\n            null = field[4]\n            null = 'False'\n        except IndexError:\n            null = 'True'\n        try:\n            default = field[5]\n        except IndexError:\n            default = None\n        return (DECIMALFIELD_TEMPLATE % {\n            'name': field[1],\n            'digits': field[2],\n            'places': field[3],\n            'null': null,\n            'default': default,\n        })\n    elif (field_type.lower() == 'datetime'):\n        try:\n            null = field[2]\n            null = 'False'\n        except IndexError:\n            null = 'True'\n        try:\n            default = field[3]\n        except IndexError:\n            default = None\n        return (DATETIMEFIELD_TEMPLATE % {\n            'name': field[1],\n            'null': null,\n            'default': default,\n        })\n    elif (field_type.lower() == 'foreign'):\n        foreign = field[2]\n        name = field[1]\n        if (foreign in ('User', 'Group')):\n            if (not self.is_imported('{0}{1}/models.py'.format(self.SCAFFOLD_APPS_DIR, self.app), foreign)):\n                self.imports.append('\\nfrom django.contrib.auth.models import User, Group\\n')\n            return (FOREIGNFIELD_TEMPLATE % {\n                'name': name,\n                'foreign': foreign,\n                'null': 'True',\n            })\n        if self.is_imported('{0}{1}/models.py'.format(self.SCAFFOLD_APPS_DIR, self.app), foreign):\n            return (FOREIGNFIELD_TEMPLATE % {\n                'name': name,\n                'foreign': foreign,\n                'null': 'True',\n            })\n        if self.get_import(foreign):\n            self.imports.append(self.get_import(foreign))\n            return (FOREIGNFIELD_TEMPLATE % {\n                'name': name,\n                'foreign': foreign,\n                'null': 'True',\n            })\n        self._info('error\\t{0}{1}/models.py\\t{2} class not found'.format(self.SCAFFOLD_APPS_DIR, self.app, foreign), 1)\n        return None\n", "label": 1}
{"function": "\n\n@plumbing.route('/repos/<repo_key>/git/commits/')\n@corsify\n@jsonify\ndef get_commit_list(repo_key):\n    ref_name = (request.args.get('ref_name') or None)\n    start_sha = (request.args.get('start_sha') or None)\n    limit = (request.args.get('limit') or current_app.config['RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT'])\n    try:\n        limit = int(limit)\n    except ValueError:\n        raise BadRequest('invalid limit')\n    if (limit < 0):\n        raise BadRequest('invalid limit')\n    repo = get_repo(repo_key)\n    start_commit_id = None\n    if (start_sha is not None):\n        start_commit_id = start_sha\n    else:\n        if (ref_name is None):\n            ref_name = 'HEAD'\n        ref = lookup_ref(repo, ref_name)\n        if (ref is None):\n            raise NotFound('reference not found')\n        start_ref = lookup_ref(repo, ref_name)\n        try:\n            start_commit_id = start_ref.resolve().target\n        except KeyError:\n            if (ref_name == 'HEAD'):\n                return []\n            else:\n                raise NotFound('reference not found')\n    try:\n        walker = repo.walk(start_commit_id, GIT_SORT_TIME)\n    except ValueError:\n        raise BadRequest('invalid start_sha')\n    except KeyError:\n        raise NotFound('commit not found')\n    commits = [convert_commit(repo_key, commit) for commit in islice(walker, limit)]\n    return commits\n", "label": 1}
{"function": "\n\n@contextfunction\ndef pager(context, items, plength=None):\n    'Pager'\n    request = context['request']\n    response_format = 'html'\n    if ('response_format' in context):\n        response_format = context['response_format']\n    skip = 0\n    if ('page_skip' in request.GET):\n        try:\n            skip = long(request.GET['page_skip'])\n        except Exception:\n            pass\n    if (not plength):\n        plength = getattr(settings, 'HARDTREE_PAGINATOR_LENGTH', 10)\n    if hasattr(items, 'count'):\n        try:\n            items_length = items.count()\n        except:\n            items_length = len(items)\n    else:\n        items_length = len(items)\n    pagenum = (items_length // plength)\n    if (items_length % plength):\n        pagenum += 1\n    maxpages = getattr(settings, 'HARDTREE_PAGINATOR_PAGES', 20)\n    current = (skip // plength)\n    start = (current - (maxpages // 2))\n    if ((pagenum - start) < maxpages):\n        start = (pagenum - maxpages)\n    if (start <= 0):\n        start = 1\n    pages = []\n    if (pagenum > 1):\n        if (current >= 1):\n            pages.append({\n                'page': '&larr;',\n                'skip': ((current - 1) * plength),\n                'mover': True,\n            })\n        else:\n            pages.append({\n                'page': '&larr;',\n                'skip': 0,\n                'mover': True,\n            })\n        if (start > 1):\n            pages.append({\n                'page': '...',\n                'skip': ((start - 1) * plength),\n                'mover': False,\n            })\n            maxpages -= 1\n        for i in range(start, (pagenum + 1)):\n            if (i > ((maxpages + start) + 1)):\n                pages.append({\n                    'page': '...',\n                    'skip': (i * plength),\n                    'mover': False,\n                })\n                break\n            else:\n                pages.append({\n                    'page': i,\n                    'skip': ((i - 1) * plength),\n                })\n        if ((current + 1) == pagenum):\n            pages.append({\n                'page': '&rarr;',\n                'skip': (current * plength),\n                'mover': True,\n            })\n        else:\n            pages.append({\n                'page': '&rarr;',\n                'skip': ((current + 1) * plength),\n                'mover': True,\n            })\n    url = (request.path + '?')\n    if request.GET:\n        for arg in request.GET:\n            if (arg != 'page_skip'):\n                values = request.GET.getlist(arg)\n                for value in values:\n                    url += (((unicode(arg) + '=') + value) + '&')\n    return Markup(render_to_string('core/tags/pager', {\n        'url': url,\n        'pages': pages,\n        'skip': skip,\n    }, response_format=response_format))\n", "label": 1}
{"function": "\n\ndef bootstrap(name, config=None, approve_key=True, install=True, pub_key=None, priv_key=None, bootstrap_url=None, force_install=False, unconditional_install=False, path=None, bootstrap_delay=None, bootstrap_args=None, bootstrap_shell=None):\n    \"\\n    Install and configure salt in a container.\\n\\n    config\\n        Minion configuration options. By default, the ``master`` option is set\\n        to the target host's master.\\n\\n    approve_key\\n        Request a pre-approval of the generated minion key. Requires\\n        that the salt-master be configured to either auto-accept all keys or\\n        expect a signing request from the target host. Default: ``True``\\n\\n    path\\n        path to the container parent\\n        default: /var/lib/lxc (system default)\\n\\n        .. versionadded:: 2015.8.0\\n\\n    pub_key\\n        Explicit public key to pressed the minion with (optional).\\n        This can be either a filepath or a string representing the key\\n\\n    priv_key\\n        Explicit private key to pressed the minion with (optional).\\n        This can be either a filepath or a string representing the key\\n\\n    bootstrap_delay\\n        Delay in seconds between end of container creation and bootstrapping.\\n        Useful when waiting for container to obtain a DHCP lease.\\n\\n        .. versionadded:: 2015.5.0\\n\\n    bootstrap_url\\n        url, content or filepath to the salt bootstrap script\\n\\n    bootstrap_args\\n        salt bootstrap script arguments\\n\\n    bootstrap_shell\\n        shell to execute the script into\\n\\n    install\\n        Whether to attempt a full installation of salt-minion if needed.\\n\\n    force_install\\n        Force installation even if salt-minion is detected,\\n        this is the way to run vendor bootstrap scripts even\\n        if a salt minion is already present in the container\\n\\n    unconditional_install\\n        Run the script even if the container seems seeded\\n\\n    CLI Examples:\\n\\n    .. code-block:: bash\\n\\n        salt 'minion' lxc.bootstrap container_name [config=config_data] \\\\\\n                [approve_key=(True|False)] [install=(True|False)]\\n\\n    \"\n    wait_started(name, path=path)\n    if (bootstrap_delay is not None):\n        try:\n            log.info('LXC {0}: bootstrap_delay: {1}'.format(name, bootstrap_delay))\n            time.sleep(bootstrap_delay)\n        except TypeError:\n            time.sleep(5)\n    c_info = info(name, path=path)\n    if (not c_info):\n        return None\n    if bootstrap_args:\n        if ('{0}' not in bootstrap_args):\n            bootstrap_args += ' -c {0}'\n    else:\n        bootstrap_args = '-c {0}'\n    if (not bootstrap_shell):\n        bootstrap_shell = 'sh'\n    orig_state = _ensure_running(name, path=path)\n    if (not orig_state):\n        return orig_state\n    if (not force_install):\n        needs_install = _needs_install(name, path=path)\n    else:\n        needs_install = True\n    seeded = (retcode(name, \"test -e '{0}'\".format(SEED_MARKER), path=path, chroot_fallback=True, ignore_retcode=True) == 0)\n    tmp = tempfile.mkdtemp()\n    if (seeded and (not unconditional_install)):\n        ret = True\n    else:\n        ret = False\n        cfg_files = __salt__['seed.mkconfig'](config, tmp=tmp, id_=name, approve_key=approve_key, pub_key=pub_key, priv_key=priv_key)\n        if (needs_install or force_install or unconditional_install):\n            if install:\n                rstr = __salt__['test.rand_str']()\n                configdir = '/var/tmp/.c_{0}'.format(rstr)\n                cmd = 'install -m 0700 -d {0}'.format(configdir)\n                if run(name, cmd, python_shell=False):\n                    log.error('tmpdir {0} creation failed ({1}'.format(configdir, cmd))\n                    return False\n                bs_ = __salt__['config.gather_bootstrap_script'](bootstrap=bootstrap_url)\n                script = '/sbin/{0}_bootstrap.sh'.format(rstr)\n                copy_to(name, bs_, script, path=path)\n                result = run_all(name, 'sh -c \"chmod +x {0}\"'.format(script), python_shell=True)\n                copy_to(name, cfg_files['config'], os.path.join(configdir, 'minion'), path=path)\n                copy_to(name, cfg_files['privkey'], os.path.join(configdir, 'minion.pem'), path=path)\n                copy_to(name, cfg_files['pubkey'], os.path.join(configdir, 'minion.pub'), path=path)\n                bootstrap_args = bootstrap_args.format(configdir)\n                cmd = '{0} {2} {1}'.format(bootstrap_shell, bootstrap_args.replace(\"'\", \"''\"), script)\n                log.info(\"Running {0} in LXC container '{1}'\".format(cmd, name))\n                ret = (retcode(name, cmd, output_loglevel='info', path=path, use_vt=True) == 0)\n                run_all(name, 'sh -c \\'if [ -f \"{0}\" ];then rm -f \"{0}\";fi\\''.format(script), ignore_retcode=True, python_shell=True)\n            else:\n                ret = False\n        else:\n            minion_config = salt.config.minion_config(cfg_files['config'])\n            pki_dir = minion_config['pki_dir']\n            copy_to(name, cfg_files['config'], '/etc/salt/minion', path=path)\n            copy_to(name, cfg_files['privkey'], os.path.join(pki_dir, 'minion.pem'), path=path)\n            copy_to(name, cfg_files['pubkey'], os.path.join(pki_dir, 'minion.pub'), path=path)\n            run(name, 'salt-call --local service.enable salt-minion', path=path, python_shell=False)\n            ret = True\n        shutil.rmtree(tmp)\n        if (orig_state == 'stopped'):\n            stop(name, path=path)\n        elif (orig_state == 'frozen'):\n            freeze(name, path=path)\n        if ret:\n            run(name, \"touch '{0}'\".format(SEED_MARKER), path=path, python_shell=False)\n    return ret\n", "label": 1}
{"function": "\n\ndef _download_url(resp, link, temp_location):\n    fp = open(temp_location, 'wb')\n    download_hash = None\n    if (link.hash and link.hash_name):\n        try:\n            download_hash = hashlib.new(link.hash_name)\n        except ValueError:\n            logger.warn(('Unsupported hash name %s for package %s' % (link.hash_name, link)))\n    try:\n        total_length = int(resp.info()['content-length'])\n    except (ValueError, KeyError, TypeError):\n        total_length = 0\n    downloaded = 0\n    show_progress = ((total_length > (40 * 1000)) or (not total_length))\n    show_url = link.show_url\n    try:\n        if show_progress:\n            if total_length:\n                logger.start_progress(('Downloading %s (%s): ' % (show_url, format_size(total_length))))\n            else:\n                logger.start_progress(('Downloading %s (unknown size): ' % show_url))\n        else:\n            logger.notify(('Downloading %s' % show_url))\n        logger.info(('Downloading from URL %s' % link))\n        while True:\n            chunk = resp.read(4096)\n            if (not chunk):\n                break\n            downloaded += len(chunk)\n            if show_progress:\n                if (not total_length):\n                    logger.show_progress(('%s' % format_size(downloaded)))\n                else:\n                    logger.show_progress(('%3i%%  %s' % (((100 * downloaded) / total_length), format_size(downloaded))))\n            if (download_hash is not None):\n                download_hash.update(chunk)\n            fp.write(chunk)\n        fp.close()\n    finally:\n        if show_progress:\n            logger.end_progress(('%s downloaded' % format_size(downloaded)))\n    return download_hash\n", "label": 1}
{"function": "\n\n@expose('/edit/', methods=('GET', 'POST'))\ndef edit(self):\n    '\\n            Edit view method\\n        '\n    next_url = None\n    path = request.args.getlist('path')\n    if (not path):\n        return redirect(self.get_url('.index'))\n    if (len(path) > 1):\n        next_url = self.get_url('.edit', path=path[1:])\n    path = path[0]\n    (base_path, full_path, path) = self._normalize_path(path)\n    if ((not self.is_accessible_path(path)) or (not self.is_file_editable(path))):\n        flash(gettext('Permission denied.'), 'error')\n        return redirect(self._get_dir_url('.index'))\n    dir_url = self._get_dir_url('.index', os.path.dirname(path))\n    next_url = (next_url or dir_url)\n    form = self.edit_form()\n    error = False\n    if self.validate_form(form):\n        form.process(request.form, content='')\n        if form.validate():\n            try:\n                with open(full_path, 'w') as f:\n                    f.write(request.form['content'])\n            except IOError:\n                flash(gettext('Error saving changes to %(name)s.', name=path), 'error')\n                error = True\n            else:\n                self.on_edit_file(full_path, path)\n                flash(gettext('Changes to %(name)s saved successfully.', name=path))\n                return redirect(next_url)\n    else:\n        helpers.flash_errors(form, message='Failed to edit file. %(error)s')\n        try:\n            with open(full_path, 'rb') as f:\n                content = f.read()\n        except IOError:\n            flash(gettext('Error reading %(name)s.', name=path), 'error')\n            error = True\n        except:\n            flash(gettext('Unexpected error while reading from %(name)s', name=path), 'error')\n            error = True\n        else:\n            try:\n                content = content.decode('utf8')\n            except UnicodeDecodeError:\n                flash(gettext('Cannot edit %(name)s.', name=path), 'error')\n                error = True\n            except:\n                flash(gettext('Unexpected error while reading from %(name)s', name=path), 'error')\n                error = True\n            else:\n                form.content.data = content\n    return self.render(self.edit_template, dir_url=dir_url, path=path, form=form, error=error)\n", "label": 1}
{"function": "\n\ndef verify_header(reqresp, body_type):\n    logger.debug(('resp.headers: %s' % (reqresp.headers,)))\n    logger.debug(('resp.txt: %s' % (reqresp.text,)))\n    if (body_type == ''):\n        _ctype = reqresp.headers['content-type']\n        if match_to_('application/json', _ctype):\n            body_type = 'json'\n        elif match_to_('application/jwt', _ctype):\n            body_type = 'jwt'\n        elif match_to_(URL_ENCODED, _ctype):\n            body_type = 'urlencoded'\n        else:\n            body_type = 'txt'\n    elif (body_type == 'json'):\n        try:\n            assert match_to_('application/json', reqresp.headers['content-type'])\n        except AssertionError:\n            try:\n                assert match_to_('application/jwt', reqresp.headers['content-type'])\n                body_type = 'jwt'\n            except AssertionError:\n                raise AssertionError(('content-type: %s' % (reqresp.headers['content-type'],)))\n    elif (body_type == 'jwt'):\n        try:\n            assert match_to_('application/jwt', reqresp.headers['content-type'])\n        except AssertionError:\n            raise AssertionError(\"Wrong content-type in header, got: {} expected 'application/jwt'\".format(reqresp.headers['content-type']))\n    elif (body_type == 'urlencoded'):\n        try:\n            assert match_to_(DEFAULT_POST_CONTENT_TYPE, reqresp.headers['content-type'])\n        except AssertionError:\n            assert match_to_('text/plain', reqresp.headers['content-type'])\n    else:\n        raise ValueError(('Unknown return format: %s' % body_type))\n    return body_type\n", "label": 1}
{"function": "\n\ndef test_islice():\n    sl = SortedList(load=7)\n    assert ([] == list(sl.islice()))\n    values = list(range(53))\n    sl.update(values)\n    for start in range(53):\n        for stop in range(53):\n            assert (list(sl.islice(start, stop)) == values[start:stop])\n    for start in range(53):\n        for stop in range(53):\n            assert (list(sl.islice(start, stop, reverse=True)) == values[start:stop][::(- 1)])\n    for start in range(53):\n        assert (list(sl.islice(start=start)) == values[start:])\n        assert (list(sl.islice(start=start, reverse=True)) == values[start:][::(- 1)])\n    for stop in range(53):\n        assert (list(sl.islice(stop=stop)) == values[:stop])\n        assert (list(sl.islice(stop=stop, reverse=True)) == values[:stop][::(- 1)])\n", "label": 1}
{"function": "\n\ndef test_new_term_with_composite_fields():\n    rules = {\n        'fields': [['a', 'b', 'c'], ['d', 'e.f']],\n        'timestamp_field': '@timestamp',\n        'es_host': 'example.com',\n        'es_port': 10,\n        'index': 'logstash',\n    }\n    mock_res = {\n        'aggregations': {\n            'filtered': {\n                'values': {\n                    'buckets': [{\n                        'key': 'key1',\n                        'doc_count': 5,\n                        'values': {\n                            'buckets': [{\n                                'key': 'key2',\n                                'doc_count': 5,\n                                'values': {\n                                    'buckets': [{\n                                        'key': 'key3',\n                                        'doc_count': 3,\n                                    }, {\n                                        'key': 'key4',\n                                        'doc_count': 2,\n                                    }],\n                                },\n                            }],\n                        },\n                    }],\n                },\n            },\n        },\n    }\n    with mock.patch('elastalert.ruletypes.Elasticsearch') as mock_es:\n        mock_es.return_value = mock.Mock()\n        mock_es.return_value.search.return_value = mock_res\n        rule = NewTermsRule(rules)\n        assert (rule.es.search.call_count == 2)\n    rule.add_data([{\n        '@timestamp': ts_now(),\n        'a': 'key1',\n        'b': 'key2',\n        'c': 'key3',\n    }])\n    assert (rule.matches == [])\n    rule.add_data([{\n        '@timestamp': ts_now(),\n        'a': 'key1',\n        'b': 'key2',\n        'c': 'key5',\n    }])\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0]['new_field'] == ('a', 'b', 'c'))\n    assert (rule.matches[0]['a'] == 'key1')\n    assert (rule.matches[0]['b'] == 'key2')\n    assert (rule.matches[0]['c'] == 'key5')\n    rule.matches = []\n    rule.add_data([{\n        '@timestamp': ts_now(),\n        'a': 'key1',\n        'b': 'key2',\n        'c': 'key4',\n        'd': 'unrelated_value',\n    }])\n    assert (len(rule.matches) == 0)\n    rule.matches = []\n    rule.add_data([{\n        '@timestamp': ts_now(),\n        'd': 'key4',\n        'e': {\n            'f': 'key6',\n        },\n    }])\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0]['new_field'] == ('d', 'e.f'))\n    assert (rule.matches[0]['d'] == 'key4')\n    assert (rule.matches[0]['e']['f'] == 'key6')\n    rule.matches = []\n    rules['alert_on_missing_field'] = True\n    with mock.patch('elastalert.ruletypes.Elasticsearch') as mock_es:\n        mock_es.return_value = mock.Mock()\n        mock_es.return_value.search.return_value = mock_res\n        rule = NewTermsRule(rules)\n    rule.add_data([{\n        '@timestamp': ts_now(),\n        'a': 'key2',\n    }])\n    assert (len(rule.matches) == 2)\n    assert (rule.matches[0]['missing_field'] == ('a', 'b', 'c'))\n    assert (rule.matches[1]['missing_field'] == ('d', 'e.f'))\n", "label": 1}
{"function": "\n\ndef _get_page_by_untyped_arg(page_lookup, request, site_id):\n    \"\\n    The `page_lookup` argument can be of any of the following types:\\n    - Integer: interpreted as `pk` of the desired page\\n    - String: interpreted as `reverse_id` of the desired page\\n    - `dict`: a dictionary containing keyword arguments to find the desired page\\n    (for instance: `{'pk': 1}`)\\n    - `Page`: you can also pass a Page object directly, in which case there will be no database lookup.\\n    - `None`: the current page will be used\\n    \"\n    if (page_lookup is None):\n        return request.current_page\n    if isinstance(page_lookup, Page):\n        if (request.current_page and (request.current_page.pk == page_lookup.pk)):\n            return request.current_page\n        return page_lookup\n    if isinstance(page_lookup, six.string_types):\n        page_lookup = {\n            'reverse_id': page_lookup,\n        }\n    elif isinstance(page_lookup, six.integer_types):\n        page_lookup = {\n            'pk': page_lookup,\n        }\n    elif (not isinstance(page_lookup, dict)):\n        raise TypeError('The page_lookup argument can be either a Dictionary, Integer, Page, or String.')\n    page_lookup.update({\n        'site': site_id,\n    })\n    try:\n        if ('pk' in page_lookup):\n            page = Page.objects.all().get(**page_lookup)\n            if (request and use_draft(request)):\n                if page.publisher_is_draft:\n                    return page\n                else:\n                    return page.publisher_draft\n            elif page.publisher_is_draft:\n                return page.publisher_public\n            else:\n                return page\n        else:\n            return get_page_queryset(request).get(**page_lookup)\n    except Page.DoesNotExist:\n        site = Site.objects.get_current()\n        subject = (_('Page not found on %(domain)s') % {\n            'domain': site.domain,\n        })\n        body = (_(\"A template tag couldn't find the page with lookup arguments `%(page_lookup)s\\n`. The URL of the request was: http://%(host)s%(path)s\") % {\n            'page_lookup': repr(page_lookup),\n            'host': site.domain,\n            'path': request.path_info,\n        })\n        if settings.DEBUG:\n            raise Page.DoesNotExist(body)\n        else:\n            if getattr(settings, 'SEND_BROKEN_LINK_EMAILS', False):\n                mail_managers(subject, body, fail_silently=True)\n            elif ('django.middleware.common.BrokenLinkEmailsMiddleware' in settings.MIDDLEWARE_CLASSES):\n                middle = BrokenLinkEmailsMiddleware()\n                domain = request.get_host()\n                path = request.get_full_path()\n                referer = force_text(request.META.get('HTTP_REFERER', ''), errors='replace')\n                if (not middle.is_ignorable_request(request, path, domain, referer)):\n                    mail_managers(subject, body, fail_silently=True)\n            return None\n", "label": 1}
{"function": "\n\ndef get_slices(self, builder, array_arg_vars, axes, index_input_vars, cartesian_product=False, index_offsets=None):\n    slice_values = []\n    axes = self.normalize_axes(array_arg_vars, axes)\n    if cartesian_product:\n        idx_counter = 0\n        for (i, curr_array) in enumerate(array_arg_vars):\n            axis = axes[i]\n            rank = curr_array.type.rank\n            if ((rank <= 1) and (axis is None)):\n                axis = 0\n            if (axis is None):\n                start = idx_counter\n                stop = (idx_counter + rank)\n                curr_indices = index_input_vars[start:stop]\n                idx_counter = stop\n                curr_slice = builder.index(curr_array, curr_indices)\n            elif (rank > axis):\n                idx = index_input_vars[idx_counter]\n                idx_counter += 1\n                curr_slice = builder.slice_along_axis(curr_array, axis, idx)\n            else:\n                curr_slice = curr_array\n            slice_values.append(curr_slice)\n    else:\n        for (i, curr_array) in enumerate(array_arg_vars):\n            axis = axes[i]\n            rank = curr_array.type.rank\n            if ((rank <= 1) and (axis is None)):\n                axis = 0\n            if (axis is None):\n                assert (len(index_input_vars) <= rank), ('Insufficient indices for array arg %s : %s' % (curr_array, curr_array.type))\n                curr_indices = index_input_vars[(- rank):]\n                curr_slice = builder.index(curr_array, curr_indices)\n            elif (rank > axis):\n                curr_idx = index_input_vars[0]\n                if (index_offsets is not None):\n                    assert (len(index_offsets) > i)\n                    curr_offset = index_offsets[i]\n                    if (not isinstance(curr_offset, Expr)):\n                        curr_offset = builder.int(curr_offset)\n                    curr_idx = builder.add(curr_idx, curr_offset)\n                curr_slice = builder.slice_along_axis(curr_array, axis, curr_idx)\n            else:\n                curr_slice = curr_array\n            slice_values.append(curr_slice)\n    return slice_values\n", "label": 1}
{"function": "\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    'Setup the ISY994 platform.'\n    logger = logging.getLogger(__name__)\n    devs = []\n    if ((ISY is None) or (not ISY.connected)):\n        logger.error('A connection has not been made to the ISY controller.')\n        return False\n    for (path, node) in ISY.nodes:\n        if ((not node.dimmable) and (SENSOR_STRING not in node.name)):\n            if (HIDDEN_STRING in path):\n                node.name += HIDDEN_STRING\n            devs.append(ISYSwitchDevice(node))\n    for (folder_name, states) in (('HA.doors', [STATE_ON, STATE_OFF]), ('HA.switches', [STATE_ON, STATE_OFF])):\n        try:\n            folder = ISY.programs['My Programs'][folder_name]\n        except KeyError:\n            pass\n        else:\n            for (dtype, name, node_id) in folder.children:\n                if (dtype is 'folder'):\n                    custom_switch = folder[node_id]\n                    try:\n                        actions = custom_switch['actions'].leaf\n                        assert (actions.dtype == 'program'), 'Not a program'\n                        node = custom_switch['status'].leaf\n                    except (KeyError, AssertionError):\n                        pass\n                    else:\n                        devs.append(ISYProgramDevice(name, node, actions, states))\n    add_devices(devs)\n", "label": 1}
{"function": "\n\ndef testSubstitute1(self):\n    config = Configuration()\n    config.readfp(StringIO(CONFIG1))\n    assert config.has_section('section1')\n    assert (not config.has_section('section2'))\n    assert (not config.has_section('foo'))\n    assert (not config.has_section('bar'))\n    assert (not config.has_section('bar2'))\n    assert config.has_option('section1', 'foo')\n    assert config.has_option('section1', 'name')\n    assert (config.get('section1', 'name') == os.path.basename(sys.argv[0]))\n    assert (config.get('section1', 'cwd') == os.getcwd())\n    assert config.has_option('section1', 'bar')\n    assert config.has_option('section1', 'bar2')\n    assert (config.get('section1', 'foo') == 'bar')\n    assert (config.get('section1', 'bar') == 'bar')\n    assert (config.get('section1', 'bar2') == 'bar')\n", "label": 1}
{"function": "\n\ndef sc_complex_dot_batched(bx_gpu, by_gpu, bc_gpu, transa='N', transb='N', handle=None):\n    '\\n    Uses cublasCgemmBatched to compute a bunch of complex dot products\\n    in parallel.\\n\\n    '\n    if (handle is None):\n        handle = scikits.cuda.misc._global_cublas_handle\n    assert (len(bx_gpu.shape) == 3)\n    assert (len(by_gpu.shape) == 3)\n    assert (len(bc_gpu.shape) == 3)\n    assert (bx_gpu.dtype == np.complex64)\n    assert (by_gpu.dtype == np.complex64)\n    assert (bc_gpu.dtype == np.complex64)\n    bx_shape = bx_gpu.shape\n    by_shape = by_gpu.shape\n    alpha = np.complex64(1.0)\n    beta = np.complex64(0.0)\n    transa = string.lower(transa)\n    transb = string.lower(transb)\n    if (transb in ['t', 'c']):\n        (N, m, k) = by_shape\n    elif (transb in ['n']):\n        (N, k, m) = by_shape\n    else:\n        raise ValueError('invalid value for transb')\n    if (transa in ['t', 'c']):\n        (N2, l, n) = bx_shape\n    elif (transa in ['n']):\n        (N2, n, l) = bx_shape\n    else:\n        raise ValueError('invalid value for transa')\n    if (l != k):\n        raise ValueError('objects are not aligned')\n    if (N != N2):\n        raise ValueError('batch sizes are not the same')\n    if (transb == 'n'):\n        lda = max(1, m)\n    else:\n        lda = max(1, k)\n    if (transa == 'n'):\n        ldb = max(1, k)\n    else:\n        ldb = max(1, n)\n    ldc = max(1, m)\n    bx_arr = bptrs(bx_gpu)\n    by_arr = bptrs(by_gpu)\n    bc_arr = bptrs(bc_gpu)\n    cublas.cublasCgemmBatched(handle, transb, transa, m, n, k, alpha, by_arr.gpudata, lda, bx_arr.gpudata, ldb, beta, bc_arr.gpudata, ldc, N)\n", "label": 1}
{"function": "\n\ndef test_indexed_integrals():\n    A = IndexedBase('A')\n    (i, j) = symbols('i j', integer=True)\n    (a1, a2) = symbols('a1:3', cls=Idx)\n    assert isinstance(a1, Idx)\n    assert (IndexedIntegral(1, A[i]).doit() == A[i])\n    assert (IndexedIntegral(A[i], A[i]).doit() == ((A[i] ** 2) / 2))\n    assert (IndexedIntegral(A[j], A[i]).doit() == (A[i] * A[j]))\n    assert (IndexedIntegral((A[i] * A[j]), A[i]).doit() == (((A[i] ** 2) * A[j]) / 2))\n    assert (IndexedIntegral(sin(A[i]), A[i]).doit() == (- cos(A[i])))\n    assert (IndexedIntegral(sin(A[j]), A[i]).doit() == (sin(A[j]) * A[i]))\n    assert (IndexedIntegral(1, A[a1]).doit() == A[a1])\n    assert (IndexedIntegral(A[a1], A[a1]).doit() == ((A[a1] ** 2) / 2))\n    assert (IndexedIntegral(A[a2], A[a1]).doit() == (A[a1] * A[a2]))\n    assert (IndexedIntegral((A[a1] * A[a2]), A[a1]).doit() == (((A[a1] ** 2) * A[a2]) / 2))\n    assert (IndexedIntegral(sin(A[a1]), A[a1]).doit() == (- cos(A[a1])))\n    assert (IndexedIntegral(sin(A[a2]), A[a1]).doit() == (sin(A[a2]) * A[a1]))\n", "label": 1}
{"function": "\n\ndef transform_IndexMap(self, expr, output=None):\n    shape = expr.shape\n    fn = expr.fn\n    dims = self.tuple_elts(shape)\n    n_dims = len(dims)\n    if (n_dims == 1):\n        shape = dims[0]\n    if (output is None):\n        output = self.create_output_array(fn, [shape], shape)\n    old_closure_args = self.closure_elts(fn)\n    old_closure_arg_types = get_types(old_closure_args)\n    fn = self.get_fn(fn)\n    closure_arg_names = [self.fresh_input_name(clos_arg) for clos_arg in old_closure_args]\n    new_closure_vars = [Var(name, type=t) for (name, t) in zip(closure_arg_names, old_closure_arg_types)]\n    old_input_types = fn.input_types\n    last_input_type = old_input_types[(- 1)]\n    index_is_tuple = isinstance(last_input_type, TupleT)\n    if index_is_tuple:\n        index_types = last_input_type.elt_types\n    else:\n        index_types = old_input_types[(- n_dims):]\n    idx_names = self.fresh_index_names(n_dims)\n    assert (len(index_types) == n_dims), ('Mismatch between bounds of IndexMap %s and %d index formal arguments' % (dims, len(index_types)))\n    output_name = names.refresh('output')\n    new_input_names = (([output_name] + closure_arg_names) + idx_names)\n    new_input_types = (([output.type] + old_closure_arg_types) + list(index_types))\n    new_fn_name = names.fresh(('idx_' + names.original(fn.name)))\n    (new_fn, builder, input_vars) = build_fn(new_input_types, NoneType, name=new_fn_name, input_names=new_input_names)\n    new_fn.created_by = self.fn.created_by\n    output_var = input_vars[0]\n    idx_vars = input_vars[(- n_dims):]\n    if (not self.is_none(expr.start_index)):\n        if isinstance(expr.start_index.type, ScalarT):\n            idx_vars = [builder.add(idx, expr.start_index, 'idx') for idx in idx_vars]\n        else:\n            start_indices = builder.tuple_elts(expr.start_index)\n            assert (len(start_indices) == len(idx_vars)), ('Mismatch between number of indices %s and start offsets %s' % (idx_vars, start_indices))\n            idx_vars = [builder.add(idx, offset, ('idx%d' % i)) for (i, (idx, offset)) in enumerate(zip(idx_vars, start_indices))]\n    if index_is_tuple:\n        elt_result = builder.call(fn, (new_closure_vars + [builder.tuple(idx_vars)]))\n    else:\n        elt_result = builder.call(fn, (new_closure_vars + idx_vars))\n    if (len(idx_vars) == 1):\n        builder.setidx(output_var, idx_vars[0], elt_result)\n    else:\n        builder.setidx(output_var, builder.tuple(idx_vars), elt_result)\n    builder.return_(none)\n    new_closure = self.closure(new_fn, ((output,) + tuple(old_closure_args)))\n    self.insert_parfor(new_closure, shape, len(old_closure_args))\n    return output\n", "label": 1}
{"function": "\n\ndef apply_local_fixes(source, options):\n    'Ananologus to apply_global_fixes, but runs only those which makes sense\\n    for the given line_range.\\n\\n    Do as much as we can without breaking code.\\n\\n    '\n\n    def find_ge(a, x):\n        'Find leftmost item greater than or equal to x.'\n        i = bisect.bisect_left(a, x)\n        if (i != len(a)):\n            return (i, a[i])\n        return ((len(a) - 1), a[(- 1)])\n\n    def find_le(a, x):\n        'Find rightmost value less than or equal to x.'\n        i = bisect.bisect_right(a, x)\n        if i:\n            return ((i - 1), a[(i - 1)])\n        return (0, a[0])\n\n    def local_fix(source, start_log, end_log, start_lines, end_lines, indents, last_line):\n        \"apply_global_fixes to the source between start_log and end_log.\\n\\n        The subsource must be the correct syntax of a complete python program\\n        (but all lines may share an indentation). The subsource's shared indent\\n        is removed, fixes are applied and the indent prepended back. Taking\\n        care to not reindent strings.\\n\\n        last_line is the strict cut off (options.line_range[1]), so that\\n        lines after last_line are not modified.\\n\\n        \"\n        if (end_log < start_log):\n            return source\n        ind = indents[start_log]\n        indent = _get_indentation(source[start_lines[start_log]])\n        sl = slice(start_lines[start_log], (end_lines[end_log] + 1))\n        subsource = source[sl]\n        if ind:\n            for line_no in start_lines[start_log:(end_log + 1)]:\n                pos = (line_no - start_lines[start_log])\n                subsource[pos] = subsource[pos][ind:]\n        fixed_subsource = apply_global_fixes(''.join(subsource), options, where='local')\n        fixed_subsource = fixed_subsource.splitlines(True)\n        msl = multiline_string_lines(''.join(fixed_subsource), include_docstrings=False)\n        for (i, line) in enumerate(fixed_subsource):\n            if (not ((i + 1) in msl)):\n                fixed_subsource[i] = ((indent + line) if (line != '\\n') else line)\n        changed_lines = len(fixed_subsource)\n        if ((start_lines[end_log] != end_lines[end_log]) and (end_lines[end_log] > last_line)):\n            after_end = (end_lines[end_log] - last_line)\n            fixed_subsource = (fixed_subsource[:(- after_end)] + source[sl][(- after_end):])\n            changed_lines -= after_end\n            options.line_range[1] = ((options.line_range[0] + changed_lines) - 1)\n        return ((source[:start_lines[start_log]] + fixed_subsource) + source[(end_lines[end_log] + 1):])\n\n    def is_continued_stmt(line, continued_stmts=frozenset(['else', 'elif', 'finally', 'except'])):\n        return (re.split('[ :]', line.strip(), 1)[0] in continued_stmts)\n    assert options.line_range\n    (start, end) = options.line_range\n    start -= 1\n    end -= 1\n    last_line = end\n    logical = _find_logical(source)\n    if (not logical[0]):\n        return apply_global_fixes(source, options)\n    (start_lines, indents) = zip(*logical[0])\n    (end_lines, _) = zip(*logical[1])\n    source = source.splitlines(True)\n    (start_log, start) = find_ge(start_lines, start)\n    (end_log, end) = find_le(start_lines, end)\n    if ((start_log > 0) and (indents[(start_log - 1)] < indents[start_log]) and (not is_continued_stmt(source[(start_log - 1)]))):\n        start_log -= 1\n        start = start_lines[start_log]\n    while (start < end):\n        if is_continued_stmt(source[start]):\n            start_log += 1\n            start = start_lines[start_log]\n            continue\n        ind = indents[start_log]\n        for t in itertools.takewhile((lambda t: (t[1][1] >= ind)), enumerate(logical[0][start_log:])):\n            (n_log, n) = ((start_log + t[0]), t[1][0])\n        if (n <= end):\n            source = local_fix(source, start_log, n_log, start_lines, end_lines, indents, last_line)\n            start_log = (n_log if (n == end) else (n_log + 1))\n            start = start_lines[start_log]\n            continue\n        else:\n            (after_end_log, after_end) = find_ge(start_lines, (end + 1))\n            if (indents[after_end_log] > indents[start_log]):\n                (start_log, start) = find_ge(start_lines, (start + 1))\n                continue\n            if ((indents[after_end_log] == indents[start_log]) and is_continued_stmt(source[after_end])):\n                only_block = True\n                for (n, n_ind) in logical[0][start_log:(end_log + 1)][::(- 1)]:\n                    if ((n_ind == ind) and (not is_continued_stmt(source[n]))):\n                        n_log = start_lines.index(n)\n                        source = local_fix(source, start_log, (n_log - 1), start_lines, end_lines, indents, last_line)\n                        start_log = (n_log + 1)\n                        start = start_lines[start_log]\n                        only_block = False\n                        break\n                if only_block:\n                    (end_log, end) = find_le(start_lines, (end - 1))\n                continue\n            source = local_fix(source, start_log, end_log, start_lines, end_lines, indents, last_line)\n            break\n    return ''.join(source)\n", "label": 1}
{"function": "\n\ndef test_insertComps_K1_D3(self, K=1, D=3):\n    A = ParamBag(K=K, D=D)\n    s = 123.456\n    A.setField('scalar', s, dims=None)\n    A.setField('N', [1.0], dims='K')\n    A.setField('x', np.random.rand(K, D), dims=('K', 'D'))\n    A.setField('xxT', np.random.rand(K, D, D), dims=('K', 'D', 'D'))\n    Abig = A.copy()\n    Abig.insertComps(A)\n    assert (Abig.K == 2)\n    assert np.allclose(Abig.N, np.hstack([A.N, A.N]))\n    assert (Abig.scalar == (2 * s))\n    assert (Abig.xxT.shape == (2, 3, 3))\n    assert np.allclose(Abig.xxT[0], A.xxT)\n    assert np.allclose(Abig.xxT[1], A.xxT)\n    Abig.insertComps(A)\n    assert (Abig.K == 3)\n    assert np.allclose(Abig.N, np.hstack([A.N, A.N, A.N]))\n    assert (Abig.scalar == (3 * s))\n    assert (Abig.xxT.shape == (3, 3, 3))\n    assert np.allclose(Abig.xxT[0], A.xxT)\n    assert np.allclose(Abig.xxT[1], A.xxT)\n    A.insertComps(Abig)\n    assert (A.K == 4)\n    assert (A.scalar == (4 * s))\n    assert np.allclose(A.N, np.hstack([1, 1, 1, 1]))\n", "label": 1}
{"function": "\n\ndef open(self, path, encoding=None, use_cached_encoding=True):\n    \"\\n        Open a file and set its content on the editor widget.\\n\\n        pyqode does not try to guess encoding. It's up to the client code to\\n        handle encodings. You can either use a charset detector to detect\\n        encoding or rely on a settings in your application. It is also up to\\n        you to handle UnicodeDecodeError, unless you've added\\n        class:`pyqode.core.panels.EncodingPanel` on the editor.\\n\\n        pyqode automatically caches file encoding that you can later reuse it\\n        automatically.\\n\\n        :param path: Path of the file to open.\\n        :param encoding: Default file encoding. Default is to use the locale\\n                         encoding.\\n        :param use_cached_encoding: True to use the cached encoding instead\\n            of ``encoding``. Set it to True if you want to force reload with a\\n            new encoding.\\n\\n        :raises: UnicodeDecodeError in case of error if no EncodingPanel\\n            were set on the editor.\\n        \"\n    ret_val = False\n    if (encoding is None):\n        encoding = locale.getpreferredencoding()\n    self.opening = True\n    settings = Cache()\n    self._path = path\n    if use_cached_encoding:\n        try:\n            cached_encoding = settings.get_file_encoding(path, preferred_encoding=encoding)\n        except KeyError:\n            pass\n        else:\n            encoding = cached_encoding\n    enable_modes = (os.path.getsize(path) < self._limit)\n    for m in self.editor.modes:\n        if m.enabled:\n            m.enabled = enable_modes\n    if (not enable_modes):\n        self.editor.modes.clear()\n    try:\n        with open(path, 'Ur', encoding=encoding) as file:\n            content = file.read()\n            if self.autodetect_eol:\n                self._eol = file.newlines\n                if isinstance(self._eol, tuple):\n                    self._eol = self._eol[0]\n                if (self._eol is None):\n                    self._eol = self.EOL.string(self.preferred_eol)\n            else:\n                self._eol = self.EOL.string(self.preferred_eol)\n    except (UnicodeDecodeError, UnicodeError) as e:\n        try:\n            from pyqode.core.panels import EncodingPanel\n            panel = self.editor.panels.get(EncodingPanel)\n        except KeyError:\n            raise e\n        else:\n            panel.on_open_failed(path, encoding)\n    else:\n        settings.set_file_encoding(path, encoding)\n        self._encoding = encoding\n        if self.replace_tabs_by_spaces:\n            content = content.replace('\\t', (' ' * self.editor.tab_length))\n        self.editor.setPlainText(content, self.get_mimetype(path), self.encoding)\n        self.editor.setDocumentTitle(self.editor.file.name)\n        ret_val = True\n        _logger().debug('file open: %s', path)\n    self.opening = False\n    if self.restore_cursor:\n        self._restore_cached_pos()\n    self._check_for_readonly()\n    return ret_val\n", "label": 1}
{"function": "\n\ndef test_profiling(self):\n    config1 = theano.config.profile\n    config2 = theano.config.profile_memory\n    config3 = theano.config.profiling.min_peak_memory\n    try:\n        theano.config.profile = True\n        theano.config.profile_memory = True\n        theano.config.profiling.min_peak_memory = True\n        x = [T.fvector(('val%i' % i)) for i in range(3)]\n        z = []\n        z += [T.outer(x[i], x[(i + 1)]).sum(axis=1) for i in range((len(x) - 1))]\n        z += [(x[i] + x[(i + 1)]) for i in range((len(x) - 1))]\n        p = theano.ProfileStats(False)\n        if (theano.config.mode in ['DebugMode', 'DEBUG_MODE', 'FAST_COMPILE']):\n            m = 'FAST_RUN'\n        else:\n            m = None\n        f = theano.function(x, z, profile=p, name='test_profiling', mode=m)\n        inp = [(numpy.arange(1024, dtype='float32') + 1) for i in range(len(x))]\n        output = f(*inp)\n        buf = StringIO()\n        f.profile.summary(buf)\n        the_string = buf.getvalue()\n        lines1 = [l for l in the_string.split('\\n') if ('Max if linker' in l)]\n        lines2 = [l for l in the_string.split('\\n') if ('Minimum peak' in l)]\n        if (theano.config.device == 'cpu'):\n            assert ('CPU: 4112KB (8204KB)' in the_string), (lines1, lines2)\n            assert ('CPU: 8204KB (12296KB)' in the_string), (lines1, lines2)\n            assert ('CPU: 8208KB' in the_string), (lines1, lines2)\n            assert ('Minimum peak from all valid apply node order is 4104KB' in the_string), (lines1, lines2)\n        else:\n            assert ('CPU: 16KB (16KB)' in the_string), (lines1, lines2)\n            assert ('GPU: 8204KB (8204KB)' in the_string), (lines1, lines2)\n            assert ('GPU: 12300KB (12300KB)' in the_string), (lines1, lines2)\n            assert ('GPU: 8212KB' in the_string), (lines1, lines2)\n            assert ('Minimum peak from all valid apply node order is 4116KB' in the_string), (lines1, lines2)\n    finally:\n        theano.config.profile = config1\n        theano.config.profile_memory = config2\n        theano.config.profiling.min_peak_memory = config3\n", "label": 1}
{"function": "\n\ndef isSqlConnection(host, port, timeout=10, product=None):\n    t = 2\n    while (t < timeout):\n        try:\n            if ((product == 'postgres') and hasPostgres):\n                pgConnect(user='', host=host, port=int((port or 5432)), socket_timeout=t)\n            elif ((product == 'mysql') and hasMySql):\n                mysqlConnect(user='', host=host, port=int((port or 5432)), socket_timeout=t)\n            elif ((product == 'orcl') and hasOracle):\n                orclConnect = oracleConnect('{}/{}@{}:{}'.format('', '', host, (':{}'.format(port) if port else '')))\n            elif ((product == 'mssql') and hasMSSql):\n                mssqlConnect(user='', host=host, socket_timeout=t)\n            elif ((product == 'sqlite') and hasSQLite):\n                sqliteConnect('', t)\n        except (pgProgrammingError, mysqlProgrammingError, oracleDatabaseError, sqliteProgrammingError):\n            return True\n        except (pgInterfaceError, mysqlInterfaceError, oracleInterfaceError, mssqlOperationalError, mssqlInterfaceError, sqliteOperationalError, sqliteInterfaceError):\n            return False\n        except socket.timeout:\n            t = (t + 2)\n    return False\n", "label": 1}
{"function": "\n\ndef __call__(self, tag=None, ns=None, children=False, root=False, error=True):\n    'Search (even in child nodes) and return a child tag by name'\n    try:\n        if root:\n            return SimpleXMLElement(elements=[self.__document.documentElement], document=self.__document, namespace=self.__ns, prefix=self.__prefix, jetty=self.__jetty, namespaces_map=self.__namespaces_map)\n        if (tag is None):\n            return self.__iter__()\n        if children:\n            return self.children()\n        elements = None\n        if isinstance(tag, int):\n            elements = [self.__elements[tag]]\n        if (ns and (not elements)):\n            for ns_uri in ((isinstance(ns, (tuple, list)) and ns) or (ns,)):\n                elements = self._element.getElementsByTagNameNS(ns_uri, tag)\n                if elements:\n                    break\n        if (self.__ns and (not elements)):\n            elements = self._element.getElementsByTagNameNS(self.__ns, tag)\n        if (not elements):\n            elements = self._element.getElementsByTagName(tag)\n        if (not elements):\n            if error:\n                raise AttributeError('No elements found')\n            else:\n                return\n        return SimpleXMLElement(elements=elements, document=self.__document, namespace=self.__ns, prefix=self.__prefix, jetty=self.__jetty, namespaces_map=self.__namespaces_map)\n    except AttributeError as e:\n        raise AttributeError(('Tag not found: %s (%s)' % (tag, e)))\n", "label": 1}
{"function": "\n\ndef test():\n    pid = get_player('Tim', 'Duncan')\n    vs_pid = get_player('Stephen', 'Curry')\n    assert player.PlayerList()\n    assert player.PlayerSummary(pid)\n    assert player.PlayerLastNGamesSplits(pid)\n    assert player.PlayerInGameSplits(pid)\n    assert player.PlayerClutchSplits(pid)\n    assert player.PlayerPerformanceSplits(pid)\n    assert player.PlayerYearOverYearSplits(pid)\n    assert player.PlayerCareer(pid)\n    assert player.PlayerProfile(pid)\n    assert player.PlayerGameLogs(pid)\n    assert player.PlayerShotTracking(pid)\n    assert player.PlayerReboundTracking(pid)\n    assert player.PlayerPassTracking(pid)\n    assert player.PlayerDefenseTracking(pid)\n    assert player.PlayerVsPlayer(pid, vs_pid)\n", "label": 1}
{"function": "\n\ndef main(script):\n    'Tests the functions in this module.\\n\\n    script: string script name\\n    '\n    preg = ReadFemPreg()\n    print(preg.shape)\n    assert (len(preg) == 13593)\n    assert (preg.caseid[13592] == 12571)\n    assert (preg.pregordr.value_counts()[1] == 5033)\n    assert (preg.nbrnaliv.value_counts()[1] == 8981)\n    assert (preg.babysex.value_counts()[1] == 4641)\n    assert (preg.birthwgt_lb.value_counts()[7] == 3049)\n    assert (preg.birthwgt_oz.value_counts()[0] == 1037)\n    assert (preg.prglngth.value_counts()[39] == 4744)\n    assert (preg.outcome.value_counts()[1] == 9148)\n    assert (preg.birthord.value_counts()[1] == 4413)\n    assert (preg.agepreg.value_counts()[22.75] == 100)\n    assert (preg.totalwgt_lb.value_counts()[7.5] == 302)\n    weights = preg.finalwgt.value_counts()\n    key = max(weights.keys())\n    assert (preg.finalwgt.value_counts()[key] == 6)\n    print(('%s: All tests passed.' % script))\n", "label": 1}
{"function": "\n\ndef installed(name, pkgs=None, dir=None, user=None, force_reinstall=False, registry=None, env=None):\n    '\\n    Verify that the given package is installed and is at the correct version\\n    (if specified).\\n\\n    .. code-block:: yaml\\n\\n        coffee-script:\\n          npm.installed:\\n            - user: someuser\\n\\n        coffee-script@1.0.1:\\n          npm.installed: []\\n\\n    name\\n        The package to install\\n\\n        .. versionchanged:: 2014.7.2\\n            This parameter is no longer lowercased by salt so that\\n            case-sensitive NPM package names will work.\\n\\n    pkgs\\n        A list of packages to install with a single npm invocation; specifying\\n        this argument will ignore the ``name`` argument\\n\\n        .. versionadded:: 2014.7.0\\n\\n    dir\\n        The target directory in which to install the package, or None for\\n        global installation\\n\\n    user\\n        The user to run NPM with\\n\\n        .. versionadded:: 0.17.0\\n\\n    registry\\n        The NPM registry from which to install the package\\n\\n        .. versionadded:: 2014.7.0\\n\\n    env\\n        A list of environment variables to be set prior to execution. The\\n        format is the same as the :py:func:`cmd.run <salt.states.cmd.run>`.\\n        state function.\\n\\n        .. versionadded:: 2014.7.0\\n\\n    force_reinstall\\n        Install the package even if it is already installed\\n    '\n    ret = {\n        'name': name,\n        'result': None,\n        'comment': '',\n        'changes': {\n            \n        },\n    }\n    if (pkgs is not None):\n        pkg_list = pkgs\n    else:\n        pkg_list = [name]\n    try:\n        installed_pkgs = __salt__['npm.list'](dir=dir, runas=user, env=env)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Error looking up {0!r}: {1}'.format(name, err)\n        return ret\n    else:\n        installed_pkgs = dict(((p, info) for (p, info) in six.iteritems(installed_pkgs)))\n    pkgs_satisfied = []\n    pkgs_to_install = []\n\n    def _pkg_is_installed(pkg, installed_pkgs):\n        '\\n        Helper function to determine if a package is installed\\n\\n        This performs more complex comparison than just checking\\n        keys, such as examining source repos to see if the package\\n        was installed by a different name from the same repo\\n\\n        :pkg str: The package to compare\\n        :installed_pkgs: A dictionary produced by npm list --json\\n        '\n        if ((pkg_name in installed_pkgs) and ('version' in installed_pkgs[pkg_name])):\n            return True\n        elif ('://' in pkg_name):\n            for pkg_details in installed_pkgs.values():\n                try:\n                    pkg_from = pkg_details.get('from', '').split('://')[1]\n                    if (pkg_name.split('://')[1] == pkg_from):\n                        return True\n                except IndexError:\n                    pass\n        return False\n    for pkg in pkg_list:\n        (pkg_name, _, pkg_ver) = pkg.partition('@')\n        pkg_name = pkg_name.strip()\n        if (force_reinstall is True):\n            pkgs_to_install.append(pkg)\n            continue\n        if (not _pkg_is_installed(pkg, installed_pkgs)):\n            pkgs_to_install.append(pkg)\n            continue\n        installed_name_ver = '{0}@{1}'.format(pkg_name, installed_pkgs[pkg_name]['version'])\n        if pkg_ver:\n            if (installed_pkgs[pkg_name].get('version') != pkg_ver):\n                pkgs_to_install.append(pkg)\n            else:\n                pkgs_satisfied.append(installed_name_ver)\n            continue\n        else:\n            pkgs_satisfied.append(installed_name_ver)\n            continue\n    if __opts__['test']:\n        ret['result'] = None\n        comment_msg = []\n        if pkgs_to_install:\n            comment_msg.append('NPM package(s) {0!r} are set to be installed'.format(', '.join(pkgs_to_install)))\n            ret['changes'] = {\n                'old': [],\n                'new': pkgs_to_install,\n            }\n        if pkgs_satisfied:\n            comment_msg.append('Package(s) {0!r} satisfied by {1}'.format(', '.join(pkg_list), ', '.join(pkgs_satisfied)))\n            ret['result'] = True\n        ret['comment'] = '. '.join(comment_msg)\n        return ret\n    if (not pkgs_to_install):\n        ret['result'] = True\n        ret['comment'] = 'Package(s) {0!r} satisfied by {1}'.format(', '.join(pkg_list), ', '.join(pkgs_satisfied))\n        return ret\n    try:\n        cmd_args = {\n            'dir': dir,\n            'runas': user,\n            'registry': registry,\n            'env': env,\n            'pkgs': pkg_list,\n        }\n        call = __salt__['npm.install'](**cmd_args)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Error installing {0!r}: {1}'.format(', '.join(pkg_list), err)\n        return ret\n    if (call and (isinstance(call, list) or isinstance(call, dict))):\n        ret['result'] = True\n        ret['changes'] = {\n            'old': [],\n            'new': pkgs_to_install,\n        }\n        ret['comment'] = 'Package(s) {0!r} successfully installed'.format(', '.join(pkgs_to_install))\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Could not install package(s) {0!r}'.format(', '.join(pkg_list))\n    return ret\n", "label": 1}
{"function": "\n\ndef convert_item(self, dest_dir, keep_new, path_formats, fmt, pretend=False):\n    (command, ext) = get_format(fmt)\n    (item, original, converted) = (None, None, None)\n    while True:\n        item = (yield (item, original, converted))\n        dest = item.destination(basedir=dest_dir, path_formats=path_formats)\n        if keep_new:\n            original = dest\n            converted = item.path\n            if should_transcode(item, fmt):\n                converted = replace_ext(converted, ext)\n        else:\n            original = item.path\n            if should_transcode(item, fmt):\n                dest = replace_ext(dest, ext)\n            converted = dest\n        if (not pretend):\n            with _fs_lock:\n                util.mkdirall(dest)\n        if os.path.exists(util.syspath(dest)):\n            self._log.info('Skipping {0} (target file exists)', util.displayable_path(item.path))\n            continue\n        if keep_new:\n            if pretend:\n                self._log.info('mv {0} {1}', util.displayable_path(item.path), util.displayable_path(original))\n            else:\n                self._log.info('Moving to {0}', util.displayable_path(original))\n                util.move(item.path, original)\n        if should_transcode(item, fmt):\n            try:\n                self.encode(command, original, converted, pretend)\n            except subprocess.CalledProcessError:\n                continue\n        elif pretend:\n            self._log.info('cp {0} {1}', util.displayable_path(original), util.displayable_path(converted))\n        else:\n            self._log.info('Copying {0}', util.displayable_path(item.path))\n            util.copy(original, converted)\n        if pretend:\n            continue\n        item.try_write(path=converted)\n        if keep_new:\n            item.path = converted\n            item.read()\n            item.store()\n        if self.config['embed']:\n            album = item.get_album()\n            if (album and album.artpath):\n                self._log.debug('embedding album art from {}', util.displayable_path(album.artpath))\n                art.embed_item(self._log, item, album.artpath, itempath=converted)\n        if keep_new:\n            plugins.send('after_convert', item=item, dest=dest, keepnew=True)\n        else:\n            plugins.send('after_convert', item=item, dest=converted, keepnew=False)\n", "label": 1}
{"function": "\n\ndef copy_children(srcfile, dstfile, srcgroup, dstgroup, title, recursive, filters, copyuserattrs, overwritefile, overwrtnodes, stats, start, stop, step, chunkshape, sortby, check_CSI, propindexes, upgradeflavors, use_hardlinks=True):\n    'Copy the children from source group to destination group'\n    srcfileh = open_file(srcfile, 'r', root_uep=srcgroup)\n    srcgroup = srcfileh.root\n    created_dstgroup = False\n    if (os.path.isfile(dstfile) and (not overwritefile)):\n        dstfileh = open_file(dstfile, 'a', pytables_sys_attrs=createsysattrs)\n        try:\n            dstgroup = dstfileh.get_node(dstgroup)\n        except NoSuchNodeError:\n            dstgroup = newdst_group(dstfileh, dstgroup, title, filters)\n            created_dstgroup = True\n        else:\n            if (not isinstance(dstgroup, Group)):\n                if overwrtnodes:\n                    parent = dstgroup._v_parent\n                    last_slash = dstgroup._v_pathname.rindex('/')\n                    dstgroupname = dstgroup._v_pathname[(last_slash + 1):]\n                    dstgroup.remove()\n                    dstgroup = dstfileh.create_group(parent, dstgroupname, title=title, filters=filters)\n                else:\n                    raise RuntimeError('Please check that the node names are not duplicated in destination, and if so, add the --overwrite-nodes flag if desired.')\n    else:\n        dstfileh = open_file(dstfile, 'w', title=title, filters=filters, pytables_sys_attrs=createsysattrs)\n        dstgroup = newdst_group(dstfileh, dstgroup, title='', filters=filters)\n        created_dstgroup = True\n    if (created_dstgroup and copyuserattrs):\n        srcgroup._v_attrs._f_copy(dstgroup)\n    try:\n        srcgroup._f_copy_children(dstgroup, recursive=recursive, filters=filters, copyuserattrs=copyuserattrs, overwrite=overwrtnodes, stats=stats, start=start, stop=stop, step=step, chunkshape=chunkshape, sortby=sortby, check_CSI=check_CSI, propindexes=propindexes, use_hardlinks=use_hardlinks)\n    except:\n        (type_, value, traceback) = sys.exc_info()\n        print((\"Problems doing the copy from '%s:%s' to '%s:%s'\" % (srcfile, srcgroup, dstfile, dstgroup)))\n        print(('The error was --> %s: %s' % (type_, value)))\n        print('The destination file looks like:\\n', dstfileh)\n        srcfileh.close()\n        dstfileh.close()\n        raise RuntimeError('Please check that the node names are not duplicated in destination, and if so, add the --overwrite-nodes flag if desired. In particular, pay attention that root_uep is not fooling you.')\n    if upgradeflavors:\n        for dstnode in dstgroup._f_walknodes('Leaf'):\n            if srcfileh.format_version.startswith('1'):\n                dstnode.del_attr('FLAVOR')\n            elif (srcfileh.format_version < '2.1'):\n                if (dstnode.get_attr('FLAVOR') in numpy_aliases):\n                    dstnode.set_attr('FLAVOR', internal_flavor)\n    for table in srcgroup._f_walknodes('Table'):\n        dsttable = dstfileh.get_node(dstgroup, table._v_pathname)\n        recreate_indexes(table, dstfileh, dsttable)\n    srcfileh.close()\n    dstfileh.close()\n", "label": 1}
{"function": "\n\ndef send(self, test=None):\n    inactive_recipient = False\n    self._check_values()\n    if (test is None):\n        try:\n            from django.conf import settings as django_settings\n            test = getattr(django_settings, 'POSTMARK_TEST_MODE', None)\n        except ImportError:\n            pass\n    for messages in _chunks(self.messages, PMBatchMail.MAX_MESSAGES):\n        json_message = []\n        for message in messages:\n            json_message.append(message.to_json_message())\n        req = Request((__POSTMARK_URL__ + 'email/batch'), json.dumps(json_message, cls=PMJSONEncoder).encode('utf8'), {\n            'Accept': 'application/json',\n            'Content-Type': 'application/json',\n            'X-Postmark-Server-Token': self.__api_key,\n            'User-agent': self.__user_agent,\n        })\n        if test:\n            print(('JSON message is:\\n%s' % json.dumps(json_message, cls=PMJSONEncoder)))\n            continue\n        try:\n            result = urlopen(req)\n            jsontxt = result.read().decode()\n            result.close()\n            if (result.code == 200):\n                results = json.loads(jsontxt)\n                for (i, res) in enumerate(results):\n                    self.__messages[i].message_id = res.get('MessageID', None)\n            else:\n                raise PMMailSendException(('Return code %d: %s' % (result.code, result.msg)))\n        except HTTPError as err:\n            if (err.code == 401):\n                raise PMMailUnauthorizedException('Sending Unauthorized - incorrect API key.', err)\n            elif (err.code == 422):\n                try:\n                    jsontxt = err.read().decode()\n                    jsonobj = json.loads(jsontxt)\n                    desc = jsonobj['Message']\n                    error_code = jsonobj['ErrorCode']\n                except KeyError:\n                    raise PMMailUnprocessableEntityException('Unprocessable Entity: Description not given')\n                if (error_code == 406):\n                    inactive_recipient = True\n                    continue\n                raise PMMailUnprocessableEntityException(('Unprocessable Entity: %s' % desc))\n            elif (err.code == 500):\n                raise PMMailServerErrorException('Internal server error at Postmark. Admins have been alerted.', err)\n        except URLError as err:\n            if hasattr(err, 'reason'):\n                raise PMMailURLException(('URLError: Failed to reach the server: %s (See \"inner_exception\" for details)' % err.reason), err)\n            elif hasattr(err, 'code'):\n                raise PMMailURLException(('URLError: %d: The server couldn\\'t fufill the request. (See \"inner_exception\" for details)' % err.code), err)\n            else:\n                raise PMMailURLException('URLError: The server couldn\\'t fufill the request. (See \"inner_exception\" for details)', err)\n    if inactive_recipient:\n        raise PMMailInactiveRecipientException('You tried to send email to a recipient that has been marked as inactive.')\n    return True\n", "label": 1}
{"function": "\n\n@mod.route('/action/backup-container', methods=['GET', 'POST'])\n@if_logged_in()\ndef backup_container():\n    '\\n    Verify the form to backup a container\\n    '\n    if (request.method == 'POST'):\n        container = request.form['orig']\n        sr_type = request.form['dest']\n        if ('push' in request.form):\n            push = request.form['push']\n        else:\n            push = False\n        sr_path = None\n        for sr in storage_repos:\n            if (sr_type in sr):\n                sr_path = sr[1]\n                break\n        backup_failed = True\n        try:\n            backup_file = lxc.backup(container=container, sr_type=sr_type, destination=sr_path)\n            bucket_token = get_bucket_token(container)\n            if (push and bucket_token and USE_BUCKET):\n                os.system('curl http://{}:{}/{} -F file=@{}'.format(BUCKET_HOST, BUCKET_PORT, bucket_token, backup_file))\n            backup_failed = False\n        except lxc.ContainerDoesntExists:\n            flash(('The Container %s does not exist !' % container), 'error')\n        except lxc.DirectoryDoesntExists:\n            flash(('Local backup directory \"%s\" does not exist !' % sr_path), 'error')\n        except lxc.NFSDirectoryNotMounted:\n            flash(('NFS repository \"%s\" not mounted !' % sr_path), 'error')\n        except subprocess.CalledProcessError:\n            flash('Error during transfert !', 'error')\n        except:\n            flash('Error during transfert !', 'error')\n        if (backup_failed is not True):\n            flash(('Container %s backed up successfully' % container), 'success')\n        else:\n            flash(('Failed to backup %s container' % container), 'error')\n    return redirect(url_for('main.home'))\n", "label": 1}
{"function": "\n\ndef UploadActivity(self, serviceRecord, activity):\n    logger.debug('Motivato UploadActivity')\n    session = self._get_session(record=serviceRecord)\n    dic = dict(training_at=activity.StartTime.strftime('%Y-%m-%d'), distance=activity.Stats.Distance.asUnits(ActivityStatisticUnit.Kilometers).Value, duration='', user_comment=activity.Notes, updated_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'), created_at=activity.StartTime.strftime('%Y-%m-%d %H:%M:%S'), discipline_id=self._activityMappings[activity.Type], source_id=8, metas=dict(distance=activity.Stats.Distance.asUnits(ActivityStatisticUnit.Kilometers).Value, duration='', time_start=activity.StartTime.strftime('%H:%M:%S')), track={\n        \n    })\n    if (activity.Stats.TimerTime.Value is not None):\n        secs = activity.Stats.TimerTime.asUnits(ActivityStatisticUnit.Seconds).Value\n    elif (activity.Stats.MovingTime.Value is not None):\n        secs = activity.Stats.MovingTime.asUnits(ActivityStatisticUnit.Seconds).Value\n    else:\n        secs = (activity.EndTime - activity.StartTime).total_seconds()\n    dic['metas']['duration'] = str(timedelta(seconds=secs))\n    dic['duration'] = str(timedelta(seconds=secs))\n    pace = str(timedelta(seconds=(secs / activity.Stats.Distance.Value)))\n    meta_hr_avg = activity.Stats.HR.Average\n    meta_hr_max = activity.Stats.HR.Max\n    if pace:\n        dic['metas']['pace'] = pace\n    if meta_hr_avg:\n        dic['metas']['meta_hr_avg'] = meta_hr_avg\n    if meta_hr_max:\n        dic['metas']['meta_hr_max'] = meta_hr_max\n    if (len(activity.Laps) > 0):\n        dic['track'] = dict(name=activity.Name, mtime=secs, points=[])\n        for tk in activity.Laps:\n            for wpt in tk.Waypoints:\n                pt = dict(lat=wpt.Location.Latitude, lon=wpt.Location.Longitude, ele=wpt.Location.Altitude, bpm=wpt.HR, moment=wpt.Timestamp.strftime('%Y-%m-%d %H:%M:%S'))\n                if (wpt.Speed and (wpt.Speed != None) and (wpt.Speed != 0)):\n                    pt['pace'] = (1000.0 / wpt.Speed)\n                dic['track']['points'].append(pt)\n    toSend = json.dumps(dic)\n    try:\n        res = session.post((self._urlRoot + '/api/workout'), data=toSend)\n    except APIWarning as e:\n        raise APIException(str(e))\n    if (res.status_code != 201):\n        raise APIException((\"Activity didn't upload: %s, %s\" % (res.status_code, res.text)))\n    try:\n        retJson = res.json()\n    except ValueError:\n        raise APIException(('Activity upload parse error for %s, %s' % (res.status_code, res.text)))\n    return retJson['id']\n", "label": 1}
{"function": "\n\ndef test_dataset_elements_grid_set():\n    'Test parsing a gridSet from a dataset element'\n    xml = '<gridSet name=\"time1 isobaric3 y x\"><projectionBox><minx>-2959.1533203125</minx><maxx>2932.8466796875</maxx><miny>-1827.929443359375</miny><maxy>1808.070556640625</maxy></projectionBox><axisRef name=\"time1\"/><axisRef name=\"isobaric3\"/><axisRef name=\"y\"/><axisRef name=\"x\"/><coordTransRef name=\"LambertConformal_Projection\"/><grid name=\"Relative_humidity_isobaric\" desc=\"Relative humidity @ Isobaric surface\" shape=\"time1 isobaric3 y x\" type=\"float\"><attribute name=\"long_name\" value=\"Relative humidity @ Isobaric surface\"/><attribute name=\"units\" value=\"%\"/><attribute name=\"abbreviation\" value=\"RH\"/><attribute name=\"missing_value\" type=\"float\" value=\"NaN\"/><attribute name=\"grid_mapping\" value=\"LambertConformal_Projection\"/><attribute name=\"coordinates\" value=\"reftime time1 isobaric3 y x \"/><attribute name=\"Grib_Variable_Id\" value=\"VAR_0-1-1_L100\"/><attribute name=\"Grib2_Parameter\" type=\"int\" value=\"0 1 1\"/><attribute name=\"Grib2_Parameter_Discipline\" value=\"Meteorological products\"/><attribute name=\"Grib2_Parameter_Category\" value=\"Moisture\"/><attribute name=\"Grib2_Parameter_Name\" value=\"Relative humidity\"/><attribute name=\"Grib2_Level_Type\" value=\"Isobaric surface\"/><attribute name=\"Grib2_Generating_Process_Type\" value=\"Forecast\"/></grid><grid name=\"Temperature_isobaric\" desc=\"Temperature @ Isobaric surface\" shape=\"time1 isobaric3 y x\" type=\"float\"><attribute name=\"long_name\" value=\"Temperature @ Isobaric surface\"/><attribute name=\"units\" value=\"K\"/><attribute name=\"abbreviation\" value=\"TMP\"/><attribute name=\"missing_value\" type=\"float\" value=\"NaN\"/><attribute name=\"grid_mapping\" value=\"LambertConformal_Projection\"/><attribute name=\"coordinates\" value=\"reftime time1 isobaric3 y x \"/><attribute name=\"Grib_Variable_Id\" value=\"VAR_0-0-0_L100\"/><attribute name=\"Grib2_Parameter\" type=\"int\" value=\"0 0 0\"/><attribute name=\"Grib2_Parameter_Discipline\" value=\"Meteorological products\"/><attribute name=\"Grib2_Parameter_Category\" value=\"Temperature\"/><attribute name=\"Grib2_Parameter_Name\" value=\"Temperature\"/><attribute name=\"Grib2_Level_Type\" value=\"Isobaric surface\"/><attribute name=\"Grib2_Generating_Process_Type\" value=\"Forecast\"/></grid></gridSet>'\n    element = ET.fromstring(xml)\n    actual = NCSSDataset(element).gridsets\n    assert actual\n    assert (len(actual) == 1)\n    assert actual['time1 isobaric3 y x']\n    gs = actual['time1 isobaric3 y x']\n    assert gs['axisRef']\n    assert (len(gs['axisRef']) == 4)\n    assert gs['coordTransRef']\n    assert gs['projectionBox']\n    assert (len(gs['projectionBox']) == 4)\n    assert gs['grid']\n    assert (len(gs['grid']) == 2)\n    for grid in gs['grid']:\n        assert (len(gs['grid'][grid]) == 4)\n        assert gs['grid'][grid]['desc']\n        assert gs['grid'][grid]['shape']\n        assert gs['grid'][grid]['type']\n        assert (gs['grid'][grid]['type'] == 'float')\n        assert (len(gs['grid'][grid]['attributes']) == 13)\n", "label": 1}
{"function": "\n\ndef transfer_maplight_data_to_we_vote_tables(request):\n    politician_name_mapping_list = []\n    one_mapping = {\n        'google_civic_name': 'Betty T. Yee',\n        'maplight_display_name': 'Betty Yee',\n        'maplight_original_name': 'Betty T Yee',\n    }\n    politician_name_mapping_list.append(one_mapping)\n    one_mapping = {\n        'google_civic_name': 'Edmund G. \"Jerry\" Brown',\n        'maplight_display_name': 'Jerry Brown',\n        'maplight_original_name': '',\n    }\n    politician_name_mapping_list.append(one_mapping)\n    candidate_campaign_manager = CandidateCampaignManager()\n    maplight_candidates_current_query = MapLightCandidate.objects.all()\n    for one_candidate_from_maplight_table in maplight_candidates_current_query:\n        found_by_id = False\n        results = candidate_campaign_manager.retrieve_candidate_campaign_from_id_maplight(one_candidate_from_maplight_table.candidate_id)\n        if (not results['success']):\n            logger.warn('Candidate NOT found by MapLight id: {name}'.format(name=one_candidate_from_maplight_table.candidate_id))\n            results = candidate_campaign_manager.retrieve_candidate_campaign_from_candidate_name(one_candidate_from_maplight_table.display_name)\n            if (not results['success']):\n                logger.warn('Candidate NOT found by display_name: {name}'.format(name=one_candidate_from_maplight_table.display_name))\n                results = candidate_campaign_manager.retrieve_candidate_campaign_from_candidate_name(one_candidate_from_maplight_table.original_name)\n                if (not results['success']):\n                    logger.warn('Candidate NOT found by original_name: {name}'.format(name=one_candidate_from_maplight_table.original_name))\n                    one_mapping_google_civic_name = ''\n                    for one_mapping_found in politician_name_mapping_list:\n                        if (value_exists(one_mapping_found['maplight_display_name']) and (one_mapping_found['maplight_display_name'] == one_candidate_from_maplight_table.display_name)):\n                            one_mapping_google_civic_name = one_mapping_found['google_civic_name']\n                            break\n                    if value_exists(one_mapping_google_civic_name):\n                        results = candidate_campaign_manager.retrieve_candidate_campaign_from_candidate_name(one_mapping_google_civic_name)\n                    if ((not results['success']) or (not value_exists(one_mapping_google_civic_name))):\n                        logger.warn('Candidate NOT found by mapping to google_civic name: {name}'.format(name=one_mapping_google_civic_name))\n                        continue\n        candidate_campaign_on_stage = results['candidate_campaign']\n        if (not value_exists(candidate_campaign_on_stage.candidate_name)):\n            continue\n        logger.debug('Candidate {name} found'.format(name=candidate_campaign_on_stage.candidate_name))\n        try:\n            if (not found_by_id):\n                candidate_campaign_on_stage.id_maplight = one_candidate_from_maplight_table.candidate_id\n            candidate_campaign_on_stage.photo_url_from_maplight = one_candidate_from_maplight_table.photo\n            candidate_campaign_on_stage.save()\n        except Exception as e:\n            handle_record_not_saved_exception(e, logger=logger)\n    messages.add_message(request, messages.INFO, 'MapLight data woven into We Vote tables.')\n    return HttpResponseRedirect(reverse('import_export:import_export_index', args=()))\n", "label": 1}
{"function": "\n\ndef change_aliases(self, change_map):\n    '\\n        Changes the aliases in change_map (which maps old-alias -> new-alias),\\n        relabelling any references to them in select columns and the where\\n        clause.\\n        '\n    assert (set(change_map.keys()).intersection(set(change_map.values())) == set())\n    self.where.relabel_aliases(change_map)\n    self.having.relabel_aliases(change_map)\n    for columns in [self.select, (self.group_by or [])]:\n        for (pos, col) in enumerate(columns):\n            if isinstance(col, (list, tuple)):\n                old_alias = col[0]\n                columns[pos] = (change_map.get(old_alias, old_alias), col[1])\n            else:\n                col.relabel_aliases(change_map)\n    for mapping in [self.aggregates]:\n        for (key, col) in mapping.items():\n            if isinstance(col, (list, tuple)):\n                old_alias = col[0]\n                mapping[key] = (change_map.get(old_alias, old_alias), col[1])\n            else:\n                col.relabel_aliases(change_map)\n    for (k, aliases) in self.join_map.items():\n        aliases = tuple([change_map.get(a, a) for a in aliases])\n        self.join_map[k] = aliases\n    for (old_alias, new_alias) in six.iteritems(change_map):\n        alias_data = self.alias_map[old_alias]\n        alias_data = alias_data._replace(rhs_alias=new_alias)\n        self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n        del self.alias_refcount[old_alias]\n        self.alias_map[new_alias] = alias_data\n        del self.alias_map[old_alias]\n        table_aliases = self.table_map[alias_data.table_name]\n        for (pos, alias) in enumerate(table_aliases):\n            if (alias == old_alias):\n                table_aliases[pos] = new_alias\n                break\n        for (pos, alias) in enumerate(self.tables):\n            if (alias == old_alias):\n                self.tables[pos] = new_alias\n                break\n    for (key, alias) in self.included_inherited_models.items():\n        if (alias in change_map):\n            self.included_inherited_models[key] = change_map[alias]\n    for (alias, data) in six.iteritems(self.alias_map):\n        lhs = data.lhs_alias\n        if (lhs in change_map):\n            data = data._replace(lhs_alias=change_map[lhs])\n            self.alias_map[alias] = data\n", "label": 1}
{"function": "\n\ndef reset(self, **traits):\n    'Creates the dataset afresh or resets existing data source.'\n    self.set(trait_change_notify=False, **traits)\n    vectors = self.vectors\n    scalars = self.scalars\n    (x, y, z) = [np.atleast_3d(a) for a in (self.x, self.y, self.z)]\n    (u, v, w) = (self.u, self.v, self.w)\n    if ('vectors' in traits):\n        u = vectors[:, 0].ravel()\n        v = vectors[:, 1].ravel()\n        w = vectors[:, 2].ravel()\n        self.set(u=u, v=v, w=w, trait_change_notify=False)\n    elif ((u is not None) and (len(u) > 0)):\n        vectors = np.c_[(u.ravel(), v.ravel(), w.ravel())].ravel()\n        vectors.shape = (u.shape[0], u.shape[1], w.shape[2], 3)\n        self.set(vectors=vectors, trait_change_notify=False)\n    if ((vectors is not None) and (len(vectors) > 0) and (scalars is not None)):\n        assert (len(scalars) == len(vectors))\n    if (x.shape[0] <= 1):\n        dx = 1\n    else:\n        dx = (x[(1, 0, 0)] - x[(0, 0, 0)])\n    if (y.shape[1] <= 1):\n        dy = 1\n    else:\n        dy = (y[(0, 1, 0)] - y[(0, 0, 0)])\n    if (z.shape[2] <= 1):\n        dz = 1\n    else:\n        dz = (z[(0, 0, 1)] - z[(0, 0, 0)])\n    if (self.m_data is None):\n        ds = ArraySource(transpose_input_array=True)\n    else:\n        ds = self.m_data\n    old_scalar = ds.scalar_data\n    ds.set(vector_data=vectors, origin=[x.min(), y.min(), z.min()], spacing=[dx, dy, dz], scalar_data=scalars)\n    if (scalars is old_scalar):\n        ds._scalar_data_changed(scalars)\n    self.dataset = ds.image_data\n    self.m_data = ds\n", "label": 1}
{"function": "\n\ndef __init__(self, domain=None, prediction_domain=None, prediction_protocol=None, protocol=None, verify=None, prediction_verify=None):\n    'Domain object constructor.\\n\\n            @param: domain string Domain name\\n            @param: prediction_domain string Domain for the prediction server\\n                    (when different from the general domain)\\n            @param: prediction_protocol string Protocol for prediction server\\n                    (when different from the general protocol)\\n            @param: protocol string Protocol for the service\\n                    (when different from HTTPS)\\n            @param: verify boolean Sets on/off the SSL verification\\n            @param: prediction_verify boolean Sets on/off the SSL verification\\n                    for the prediction server (when different from the general\\n                    SSL verification)\\n\\n        '\n    self.general_domain = (domain or BIGML_DOMAIN)\n    self.general_protocol = (protocol or BIGML_PROTOCOL)\n    if (prediction_domain is None):\n        if (domain is not None):\n            self.prediction_domain = domain\n            self.prediction_protocol = (protocol or BIGML_PROTOCOL)\n        else:\n            self.prediction_domain = BIGML_PREDICTION_DOMAIN\n            self.prediction_protocol = BIGML_PREDICTION_PROTOCOL\n    else:\n        self.prediction_domain = prediction_domain\n        self.prediction_protocol = (prediction_protocol or BIGML_PREDICTION_PROTOCOL)\n    self.verify = None\n    self.verify_prediction = None\n    if ((self.general_protocol == BIGML_PROTOCOL) and ((verify is not None) or (BIGML_SSL_VERIFY is not None))):\n        try:\n            self.verify = (verify if (verify is not None) else bool(int(BIGML_SSL_VERIFY)))\n        except ValueError:\n            pass\n    if (self.verify is None):\n        self.verify = self.general_domain.lower().endswith(DEFAULT_DOMAIN)\n    if ((self.prediction_protocol == BIGML_PROTOCOL) and (prediction_verify or (BIGML_PREDICTION_SSL_VERIFY is not None))):\n        try:\n            self.verify_prediction = (prediction_verify if (prediction_verify is not None) else bool(int(BIGML_PREDICTION_SSL_VERIFY)))\n        except ValueError:\n            pass\n    if (self.verify_prediction is None):\n        self.verify_prediction = (self.prediction_domain.lower().endswith(DEFAULT_DOMAIN) and (self.prediction_protocol == DEFAULT_PROTOCOL))\n", "label": 1}
{"function": "\n\ndef _compose(self, public=None, private=None, no_cache=None, no_store=False, max_age=None, s_maxage=None, no_transform=False, **extensions):\n    assert isinstance(max_age, (type(None), int))\n    assert isinstance(s_maxage, (type(None), int))\n    expires = 0\n    result = []\n    if (private is True):\n        assert ((not public) and (not no_cache) and (not s_maxage))\n        result.append('private')\n    elif (no_cache is True):\n        assert ((not public) and (not private) and (not max_age))\n        result.append('no-cache')\n    else:\n        assert ((public is None) or (public is True))\n        assert ((not private) and (not no_cache))\n        expires = max_age\n        result.append('public')\n    if no_store:\n        result.append('no-store')\n    if no_transform:\n        result.append('no-transform')\n    if (max_age is not None):\n        result.append(('max-age=%d' % max_age))\n    if (s_maxage is not None):\n        result.append(('s-maxage=%d' % s_maxage))\n    for (k, v) in six.iteritems(extensions):\n        if (k not in self.extensions):\n            raise AssertionError((\"unexpected extension used: '%s'\" % k))\n        result.append(('%s=\"%s\"' % (k.replace('_', '-'), v)))\n    return (result, expires)\n", "label": 1}
{"function": "\n\ndef uniform(self, size, low=0.0, high=1.0, ndim=None, dtype=None, nstreams=None):\n    '\\n        Sample a tensor of given size whose element from a uniform\\n        distribution between low and high.\\n\\n        If the size argument is ambiguous on the number of dimensions,\\n        ndim may be a plain integer to supplement the missing information.\\n\\n        Parameters\\n        ----------\\n        low\\n            Lower bound of the interval on which values are sampled. \\n            If the ``dtype`` arg is provided, ``low`` will be cast into\\n            dtype. This bound is excluded.\\n        high\\n            Higher bound of the interval on which values are sampled.\\n            If the ``dtype`` arg is provided, ``high`` will be cast into\\n            dtype. This bound is excluded.\\n        size\\n          Can be a list of integer or Theano variable (ex: the shape\\n          of other Theano Variable).\\n        dtype\\n            The output data type. If dtype is not specified, it will be\\n            inferred from the dtype of low and high, but will be at\\n            least as precise as floatX.\\n\\n        '\n    low = as_tensor_variable(low)\n    high = as_tensor_variable(high)\n    if (dtype is None):\n        dtype = scal.upcast(config.floatX, low.dtype, high.dtype)\n    low = cast(low, dtype=dtype)\n    high = cast(high, dtype=dtype)\n    if isinstance(size, tuple):\n        msg = 'size must be a tuple of int or a Theano variable'\n        assert all([isinstance(i, (numpy.integer, int, Variable)) for i in size]), msg\n        if any([(isinstance(i, (numpy.integer, int)) and (i <= 0)) for i in size]):\n            raise ValueError('The specified size contains a dimension with value <= 0', size)\n    elif (not (isinstance(size, Variable) and (size.ndim == 1))):\n        raise TypeError(((('size must be a tuple of int or a Theano Variable with 1 dimension, got ' + str(size)) + ' of type ') + str(type(size))))\n    orig_nstreams = nstreams\n    if (nstreams is None):\n        nstreams = self.n_streams(size)\n    rstates = self.get_substream_rstates(nstreams, dtype)\n    if (self.use_cuda and (dtype == 'float32')):\n        node_rstate = float32_shared_constructor(rstates)\n        assert isinstance(node_rstate.type, CudaNdarrayType)\n        u = self.pretty_return(node_rstate, *GPU_mrg_uniform.new(node_rstate, ndim, dtype, size), size=size, nstreams=orig_nstreams)\n    else:\n        node_rstate = shared(rstates)\n        u = self.pretty_return(node_rstate, *mrg_uniform.new(node_rstate, ndim, dtype, size), size=size, nstreams=orig_nstreams)\n    node_rstate.tag.is_rng = True\n    r = ((u * (high - low)) + low)\n    if (u.type.broadcastable != r.type.broadcastable):\n        raise NotImplementedError('Increase the size to match the broadcasting pattern of `low` and `high` arguments')\n    assert (r.dtype == dtype)\n    return r\n", "label": 1}
{"function": "\n\ndef get_substrings(self, min_freq=2, check_positive=True, sort_by_length=False):\n    movetos = set()\n    for (idx, tok) in enumerate(self.rev_keymap):\n        if (isinstance(tok, basestring) and (tok[(- 6):] == 'moveto')):\n            movetos.add(idx)\n    try:\n        hmask = self.rev_keymap.index('hintmask')\n    except ValueError:\n        hmask = None\n    matches = {\n        \n    }\n    for (glyph_idx, program) in enumerate(self.data):\n        cur_start = 0\n        last_op = (- 1)\n        for (pos, tok) in enumerate(program):\n            if (tok in movetos):\n                stop = (last_op + 1)\n                if ((stop - cur_start) > 0):\n                    if (program[cur_start:stop] in matches):\n                        matches[program[cur_start:stop]].freq += 1\n                    else:\n                        span = pyCompressor.CandidateSubr((stop - cur_start), (glyph_idx, cur_start), 1, self.data, self.cost_map)\n                        matches[program[cur_start:stop]] = span\n                cur_start = (pos + 1)\n            elif (tok == hmask):\n                last_op = (pos + 1)\n            elif (type(self.rev_keymap[tok]) == str):\n                last_op = pos\n    constraints = (lambda s: ((s.freq >= min_freq) and ((s.subr_saving() > 0) or (not check_positive))))\n    self.substrings = filter(constraints, matches.values())\n    if sort_by_length:\n        self.substrings.sort(key=(lambda s: len(s)))\n    else:\n        self.substrings.sort(key=(lambda s: s.subr_saving()), reverse=True)\n    return self.substrings\n", "label": 1}
{"function": "\n\ndef splot(ctx, f, u=[(- 5), 5], v=[(- 5), 5], points=100, keep_aspect=True, wireframe=False, file=None, dpi=None, axes=None):\n    '\\n    Plots the surface defined by `f`.\\n\\n    If `f` returns a single component, then this plots the surface\\n    defined by `z = f(x,y)` over the rectangular domain with\\n    `x = u` and `y = v`.\\n\\n    If `f` returns three components, then this plots the parametric\\n    surface `x, y, z = f(u,v)` over the pairs of intervals `u` and `v`.\\n\\n    For example, to plot a simple function::\\n\\n        >>> from mpmath import *\\n        >>> f = lambda x, y: sin(x+y)*cos(y)\\n        >>> splot(f, [-pi,pi], [-pi,pi])    # doctest: +SKIP\\n\\n    Plotting a donut::\\n\\n        >>> r, R = 1, 2.5\\n        >>> f = lambda u, v: [r*cos(u), (R+r*sin(u))*cos(v), (R+r*sin(u))*sin(v)]\\n        >>> splot(f, [0, 2*pi], [0, 2*pi])    # doctest: +SKIP\\n\\n    .. note :: This function requires matplotlib (pylab) 0.98.5.3 or higher.\\n    '\n    import pylab\n    import mpl_toolkits.mplot3d as mplot3d\n    if file:\n        axes = None\n    fig = None\n    if (not axes):\n        fig = pylab.figure()\n        axes = mplot3d.axes3d.Axes3D(fig)\n    (ua, ub) = u\n    (va, vb) = v\n    du = (ub - ua)\n    dv = (vb - va)\n    if (not isinstance(points, (list, tuple))):\n        points = [points, points]\n    (M, N) = points\n    u = pylab.linspace(ua, ub, M)\n    v = pylab.linspace(va, vb, N)\n    (x, y, z) = [pylab.zeros((M, N)) for i in xrange(3)]\n    (xab, yab, zab) = [[0, 0] for i in xrange(3)]\n    for n in xrange(N):\n        for m in xrange(M):\n            fdata = f(ctx.convert(u[m]), ctx.convert(v[n]))\n            try:\n                (x[(m, n)], y[(m, n)], z[(m, n)]) = fdata\n            except TypeError:\n                (x[(m, n)], y[(m, n)], z[(m, n)]) = (u[m], v[n], fdata)\n            for (c, cab) in [(x[(m, n)], xab), (y[(m, n)], yab), (z[(m, n)], zab)]:\n                if (c < cab[0]):\n                    cab[0] = c\n                if (c > cab[1]):\n                    cab[1] = c\n    if wireframe:\n        axes.plot_wireframe(x, y, z, rstride=4, cstride=4)\n    else:\n        axes.plot_surface(x, y, z, rstride=4, cstride=4)\n    axes.set_xlabel('x')\n    axes.set_ylabel('y')\n    axes.set_zlabel('z')\n    if keep_aspect:\n        (dx, dy, dz) = [(cab[1] - cab[0]) for cab in [xab, yab, zab]]\n        maxd = max(dx, dy, dz)\n        if (dx < maxd):\n            delta = (maxd - dx)\n            axes.set_xlim3d((xab[0] - (delta / 2.0)), (xab[1] + (delta / 2.0)))\n        if (dy < maxd):\n            delta = (maxd - dy)\n            axes.set_ylim3d((yab[0] - (delta / 2.0)), (yab[1] + (delta / 2.0)))\n        if (dz < maxd):\n            delta = (maxd - dz)\n            axes.set_zlim3d((zab[0] - (delta / 2.0)), (zab[1] + (delta / 2.0)))\n    if fig:\n        if file:\n            pylab.savefig(file, dpi=dpi)\n        else:\n            pylab.show()\n", "label": 1}
{"function": "\n\ndef decode(self, source):\n    \"Decode a source map object into a SourceMapIndex.\\n\\n        The index is keyed on (dst_line, dst_column) for lookups,\\n        and a per row index is kept to help calculate which Token to retrieve.\\n\\n        For example:\\n            A minified source file has two rows and two tokens per row.\\n\\n            # All parsed tokens\\n            tokens = [\\n                Token(dst_row=0, dst_col=0),\\n                Token(dst_row=0, dst_col=5),\\n                Token(dst_row=1, dst_col=0),\\n                Token(dst_row=1, dst_col=12),\\n            ]\\n\\n            Two dimentional array of columns -> row\\n            rows = [\\n                [0, 5],\\n                [0, 12],\\n            ]\\n\\n            Token lookup, based on location\\n            index = {\\n                (0, 0):  tokens[0],\\n                (0, 5):  tokens[1],\\n                (1, 0):  tokens[2],\\n                (1, 12): tokens[3],\\n            }\\n\\n            To find the token at (1, 20):\\n              - Check if there's a direct hit on the index (1, 20) => False\\n              - Pull rows[1] => [0, 12]\\n              - bisect_right to find the closest match:\\n                  bisect_right([0, 12], 20) => 2\\n              - Fetch the column number before, since we want the column\\n                lte to the bisect_right: 2-1 => row[2-1] => 12\\n              - At this point, we know the token location, (1, 12)\\n              - Pull (1, 12) from index => tokens[3]\\n        \"\n    if ((source[:4] == \")]}'\") or (source[:3] == ')]}')):\n        source = source.split('\\n', 1)[1]\n    smap = json.loads(source)\n    sources = smap['sources']\n    sourceRoot = smap.get('sourceRoot')\n    names = list(map(text_type, smap['names']))\n    mappings = smap['mappings']\n    lines = mappings.split(';')\n    if (sourceRoot is not None):\n        sources = map(partial(os.path.join, sourceRoot), sources)\n    tokens = []\n    line_index = []\n    index = {\n        \n    }\n    (dst_col, src_id, src_line, src_col, name_id) = (0, 0, 0, 0, 0)\n    for (dst_line, line) in enumerate(lines):\n        line_index.append([])\n        segments = line.split(',')\n        dst_col = 0\n        for segment in segments:\n            if (not segment):\n                continue\n            parse = self.parse_vlq(segment)\n            dst_col += parse[0]\n            src = None\n            name = None\n            if (len(parse) > 1):\n                try:\n                    src_id += parse[1]\n                    if (not (0 <= src_id < len(sources))):\n                        raise SourceMapDecodeError(('Segment %s references source %d; there are %d sources' % (segment, src_id, len(sources))))\n                    src = sources[src_id]\n                    src_line += parse[2]\n                    src_col += parse[3]\n                    if (len(parse) > 4):\n                        name_id += parse[4]\n                        if (not (0 <= name_id < len(names))):\n                            raise SourceMapDecodeError(('Segment %s references name %d; there are %d names' % (segment, name_id, len(names))))\n                        name = names[name_id]\n                except IndexError:\n                    raise SourceMapDecodeError(('Invalid segment %s, parsed as %r' % (segment, parse)))\n            try:\n                assert (dst_line >= 0), ('dst_line', dst_line)\n                assert (dst_col >= 0), ('dst_col', dst_col)\n                assert (src_line >= 0), ('src_line', src_line)\n                assert (src_col >= 0), ('src_col', src_col)\n            except AssertionError as e:\n                raise SourceMapDecodeError(('Segment %s has negative %s (%d), in file %s' % (segment, e.message[0], e.message[1], src)))\n            token = Token(dst_line, dst_col, src, src_line, src_col, name)\n            tokens.append(token)\n            index[(dst_line, dst_col)] = token\n            line_index[dst_line].append(dst_col)\n    return SourceMapIndex(smap, tokens, line_index, index, sources)\n", "label": 1}
{"function": "\n\ndef test_key_manager():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.set_key('test', {\n            'permission': {\n                \n            },\n        })\n        with pytest.raises(KeyConflict):\n            h.set_key('test', {\n                'permission': {\n                    \n                },\n            })\n        assert (h.has_key('test') == True)\n        key = h.get_key('test')\n        assert (key == {\n            'key': 'test',\n            'permission': {\n                \n            },\n        })\n        assert (len(h._cache) == 1)\n        assert ('test' in h._cache)\n        assert (len(h._entries) == 1)\n        assert (list(h._entries) == ['test'])\n        key = h.delete_key('test')\n        with pytest.raises(KeyNotFound):\n            key = h.get_key('test')\n        h.set_key('test', {\n            'permission': {\n                \n            },\n        })\n        h.set_key('test1', {\n            'permission': {\n                \n            },\n        }, 'test')\n        assert (h.has_key('test1') == True)\n        assert (h.all_subkeys('test') == [{\n            'key': 'test1',\n            'permission': {\n                \n            },\n        }])\n        assert (len(h.all_keys()) == 2)\n        assert (h.all_keys() == ['test', 'test1'])\n        assert (h.all_keys(include_key=True) == [{\n            'key': 'test',\n            'permission': {\n                \n            },\n        }, {\n            'key': 'test1',\n            'permission': {\n                \n            },\n        }])\n        h.get_key('test')\n        h.get_key('test1')\n        assert (len(h._cache) == 2)\n        assert ('test' in h._cache)\n        assert ('test1' in h._cache)\n        assert (len(h._entries) == 2)\n        assert (list(h._entries) == ['test', 'test1'])\n        h.delete_key('test')\n        assert (len(h._cache) == 0)\n        assert (len(h._entries) == 0)\n        with pytest.raises(KeyNotFound):\n            key = h.get_key('test')\n        with pytest.raises(KeyNotFound):\n            key = h.get_key('test1')\n", "label": 1}
{"function": "\n\ndef testIteration(self):\n    dataoffset = 16384\n    filler = b'ham\\n'\n    assert (not (dataoffset % len(filler))), 'dataoffset must be multiple of len(filler)'\n    nchunks = (dataoffset // len(filler))\n    testlines = [b'spam, spam and eggs\\n', b'eggs, spam, ham and spam\\n', b'saussages, spam, spam and eggs\\n', b'spam, ham, spam and eggs\\n', b'spam, spam, spam, spam, spam, ham, spam\\n', b'wonderful spaaaaaam.\\n']\n    methods = [('readline', ()), ('read', ()), ('readlines', ()), ('readinto', (array('b', (b' ' * 100)),))]\n    try:\n        bag = self.open(TESTFN, 'wb')\n        bag.write((filler * nchunks))\n        bag.writelines(testlines)\n        bag.close()\n        for (methodname, args) in methods:\n            f = self.open(TESTFN, 'rb')\n            if (next(f) != filler):\n                (self.fail, 'Broken testfile')\n            meth = getattr(f, methodname)\n            meth(*args)\n            f.close()\n        f = self.open(TESTFN, 'rb')\n        for i in range(nchunks):\n            next(f)\n        testline = testlines.pop(0)\n        try:\n            line = f.readline()\n        except ValueError:\n            self.fail('readline() after next() with supposedly empty iteration-buffer failed anyway')\n        if (line != testline):\n            self.fail(('readline() after next() with empty buffer failed. Got %r, expected %r' % (line, testline)))\n        testline = testlines.pop(0)\n        buf = array('b', (b'\\x00' * len(testline)))\n        try:\n            f.readinto(buf)\n        except ValueError:\n            self.fail('readinto() after next() with supposedly empty iteration-buffer failed anyway')\n        line = buf.tobytes()\n        if (line != testline):\n            self.fail(('readinto() after next() with empty buffer failed. Got %r, expected %r' % (line, testline)))\n        testline = testlines.pop(0)\n        try:\n            line = f.read(len(testline))\n        except ValueError:\n            self.fail('read() after next() with supposedly empty iteration-buffer failed anyway')\n        if (line != testline):\n            self.fail(('read() after next() with empty buffer failed. Got %r, expected %r' % (line, testline)))\n        try:\n            lines = f.readlines()\n        except ValueError:\n            self.fail('readlines() after next() with supposedly empty iteration-buffer failed anyway')\n        if (lines != testlines):\n            self.fail(('readlines() after next() with empty buffer failed. Got %r, expected %r' % (line, testline)))\n        f.close()\n        f = self.open(TESTFN, 'rb')\n        try:\n            for line in f:\n                pass\n            try:\n                f.readline()\n                f.readinto(buf)\n                f.read()\n                f.readlines()\n            except ValueError:\n                self.fail('read* failed after next() consumed file')\n        finally:\n            f.close()\n    finally:\n        os.unlink(TESTFN)\n", "label": 1}
{"function": "\n\ndef same_shape(self, x, y, dim_x=None, dim_y=None):\n    'Return True if we are able to assert that x and y have the\\n        same shape.\\n\\n        dim_x and dim_y are optional. If used, they should be an index\\n        to compare only 1 dimension of x and y.\\n\\n        '\n    sx = self.shape_of[x]\n    sy = self.shape_of[y]\n    if ((sx is None) or (sy is None)):\n        return False\n    if (dim_x is not None):\n        sx = [sx[dim_x]]\n    if (dim_y is not None):\n        sy = [sy[dim_y]]\n    assert (len(sx) == len(sy))\n    for (dx, dy) in zip(sx, sy):\n        if (dx is dy):\n            continue\n        if ((not dx.owner) or (not dy.owner)):\n            return False\n        if ((not isinstance(dx.owner.op, Shape_i)) or (not isinstance(dy.owner.op, Shape_i))):\n            return False\n        opx = dx.owner.op\n        opy = dy.owner.op\n        if (not (opx.i == opy.i)):\n            return False\n        if (dx.owner.inputs[0] == dy.owner.inputs[0]):\n            continue\n        from theano.scan_module.scan_utils import equal_computations\n        if (not equal_computations([dx], [dy])):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef _create_(cls, class_name, names=None, module=None, type=None):\n    'Convenience method to create a new Enum class.\\n\\n        `names` can be:\\n\\n        * A string containing member names, separated either with spaces or\\n          commas.  Values are auto-numbered from 1.\\n        * An iterable of member names.  Values are auto-numbered from 1.\\n        * An iterable of (member name, value) pairs.\\n        * A mapping of member name -> value.\\n\\n        '\n    if (pyver < 3.0):\n        if isinstance(class_name, unicode):\n            try:\n                class_name = class_name.encode('ascii')\n            except UnicodeEncodeError:\n                raise TypeError(('%r is not representable in ASCII' % class_name))\n    metacls = cls.__class__\n    if (type is None):\n        bases = (cls,)\n    else:\n        bases = (type, cls)\n    classdict = metacls.__prepare__(class_name, bases)\n    __order__ = []\n    if isinstance(names, basestring):\n        names = names.replace(',', ' ').split()\n    if (isinstance(names, (tuple, list)) and isinstance(names[0], basestring)):\n        names = [(e, (i + 1)) for (i, e) in enumerate(names)]\n    for item in names:\n        if isinstance(item, basestring):\n            (member_name, member_value) = (item, names[item])\n        else:\n            (member_name, member_value) = item\n        classdict[member_name] = member_value\n        __order__.append(member_name)\n    if (not isinstance(item, basestring)):\n        classdict['__order__'] = ' '.join(__order__)\n    enum_class = metacls.__new__(metacls, class_name, bases, classdict)\n    if (module is None):\n        try:\n            module = _sys._getframe(2).f_globals['__name__']\n        except (AttributeError, ValueError):\n            pass\n    if (module is None):\n        _make_class_unpicklable(enum_class)\n    else:\n        enum_class.__module__ = module\n    return enum_class\n", "label": 1}
{"function": "\n\ndef test_02_atom_init_with_statement(self):\n    s = Atom_Sword_Statement(ATOM_TEST_STATEMENT)\n    assert (len(s.states) == 1)\n    assert (len(s.original_deposits) == 1)\n    assert (len(s.resources) == 1)\n    assert (s.xml_document != None)\n    assert (s.dom != None)\n    assert s.parsed\n    assert s.valid\n    (uri, description) = s.states[0]\n    assert (uri == 'http://purl.org/net/sword/terms/state/Testing')\n    assert (description == 'The work has passed through review and is now in the archive')\n    t = datetime.strptime('2011-03-02T20:50:06Z', '%Y-%m-%dT%H:%M:%SZ')\n    entry = s.resources[0]\n    assert (len(entry.packaging) == 1)\n    assert (entry.deposited_by == 'sword')\n    assert (entry.deposited_on_behalf_of == 'jbloggs')\n    assert (entry.deposited_on == t)\n    assert (entry.uri == 'http://localhost:8080/part-IRI/43/my_deposit/example.zip')\n    assert (entry.packaging[0] == 'http://purl.org/net/sword/package/SimpleZip')\n", "label": 1}
{"function": "\n\ndef run_cli(self, hql, schema=None, verbose=True):\n    '\\n        Run an hql statement using the hive cli\\n\\n        >>> hh = HiveCliHook()\\n        >>> result = hh.run_cli(\"USE airflow;\")\\n        >>> (\"OK\" in result)\\n        True\\n        '\n    conn = self.conn\n    schema = (schema or conn.schema)\n    if schema:\n        hql = 'USE {schema};\\n{hql}'.format(**locals())\n    with TemporaryDirectory(prefix='airflow_hiveop_') as tmp_dir:\n        with NamedTemporaryFile(dir=tmp_dir) as f:\n            f.write(hql.encode('UTF-8'))\n            f.flush()\n            fname = f.name\n            hive_bin = 'hive'\n            cmd_extra = []\n            if self.use_beeline:\n                hive_bin = 'beeline'\n                jdbc_url = 'jdbc:hive2://{conn.host}:{conn.port}/{conn.schema}'\n                if (configuration.get('core', 'security') == 'kerberos'):\n                    template = conn.extra_dejson.get('principal', 'hive/_HOST@EXAMPLE.COM')\n                    if ('_HOST' in template):\n                        template = utils.replace_hostname_pattern(utils.get_components(template))\n                    proxy_user = ''\n                    if ((conn.extra_dejson.get('proxy_user') == 'login') and conn.login):\n                        proxy_user = 'hive.server2.proxy.user={0}'.format(conn.login)\n                    elif ((conn.extra_dejson.get('proxy_user') == 'owner') and self.run_as):\n                        proxy_user = 'hive.server2.proxy.user={0}'.format(self.run_as)\n                    jdbc_url += ';principal={template};{proxy_user}'\n                elif self.auth:\n                    jdbc_url += (';auth=' + self.auth)\n                jdbc_url = jdbc_url.format(**locals())\n                cmd_extra += ['-u', jdbc_url]\n                if conn.login:\n                    cmd_extra += ['-n', conn.login]\n                if conn.password:\n                    cmd_extra += ['-p', conn.password]\n            hive_cmd = ([hive_bin, '-f', fname] + cmd_extra)\n            if self.hive_cli_params:\n                hive_params_list = self.hive_cli_params.split()\n                hive_cmd.extend(hive_params_list)\n            if verbose:\n                logging.info(' '.join(hive_cmd))\n            sp = subprocess.Popen(hive_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=tmp_dir)\n            self.sp = sp\n            stdout = ''\n            while True:\n                line = sp.stdout.readline()\n                if (not line):\n                    break\n                stdout += line.decode('UTF-8')\n                if verbose:\n                    logging.info(line.decode('UTF-8').strip())\n            sp.wait()\n            if sp.returncode:\n                raise AirflowException(stdout)\n            return stdout\n", "label": 1}
{"function": "\n\ndef main():\n    'Command line interface for the ``humanfriendly`` program.'\n    try:\n        (options, arguments) = getopt.getopt(sys.argv[1:], 'cd:hn:s:t:', ['delimiter=', 'format-length=', 'format-number=', 'format-size=', 'format-table', 'format-timespan=', 'parse-length=', 'parse-size=', 'run-command', 'help'])\n    except getopt.GetoptError as e:\n        sys.stderr.write(('Error: %s\\n' % e))\n        sys.exit(1)\n    actions = []\n    delimiter = None\n    should_format_table = False\n    for (option, value) in options:\n        if (option in ('-d', '--delimiter')):\n            delimiter = value\n        elif (option == '--parse-size'):\n            actions.append(functools.partial(print_parsed_size, value))\n        elif (option == '--parse-length'):\n            actions.append(functools.partial(print_parsed_length, value))\n        elif (option in ('-c', '--run-command')):\n            actions.append(functools.partial(run_command, arguments))\n        elif (option in ('-l', '--format-length')):\n            actions.append(functools.partial(print_formatted_length, value))\n        elif (option in ('-n', '--format-number')):\n            actions.append(functools.partial(print_formatted_number, value))\n        elif (option in ('-s', '--format-size')):\n            actions.append(functools.partial(print_formatted_size, value))\n        elif (option == '--format-table'):\n            should_format_table = True\n        elif (option in ('-t', '--format-timespan')):\n            actions.append(functools.partial(print_formatted_timespan, value))\n        elif (option in ('-h', '--help')):\n            usage(__doc__)\n            return\n    if should_format_table:\n        actions.append(functools.partial(print_formatted_table, delimiter))\n    if (not actions):\n        usage(__doc__)\n        return\n    for partial in actions:\n        partial()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef formlist():\n    '\\n            Retrieve a list of available XForms\\n\\n            @return: a list of tuples (url, title) of available XForms\\n        '\n    resources = current.deployment_settings.get_xforms_resources()\n    xforms = []\n    if resources:\n        s3db = current.s3db\n        for item in resources:\n            options = {\n                \n            }\n            if isinstance(item, (tuple, list)):\n                if (len(item) == 2):\n                    (title, tablename) = item\n                    if isinstance(tablename, dict):\n                        (tablename, options) = (title, tablename)\n                        title = None\n                elif (len(item) == 3):\n                    (title, tablename, options) = item\n                else:\n                    continue\n            else:\n                (title, tablename) = (None, item)\n            try:\n                resource = s3db.resource(tablename)\n            except AttributeError:\n                current.log.warning(('XForms: non-existent resource %s' % tablename))\n                continue\n            if (title is None):\n                title = ' '.join((w.capitalize() for w in resource.name.split('_')))\n            c = options.get('c', 'xforms')\n            f = options.get('f', 'forms')\n            url_vars = options.get('vars', {\n                \n            })\n            config = resource.get_config('xform')\n            if config:\n                if isinstance(config, dict):\n                    collection = config.get('collection')\n                    if (not collection):\n                        continue\n                    table = resource.table\n                    DELETED = current.xml.DELETED\n                    if (DELETED in table):\n                        query = (table[DELETED] != True)\n                    else:\n                        query = (table.id > 0)\n                    public = options.get('check', 'public')\n                    if (public in table):\n                        query &= (table[public] == True)\n                    title_field = options.get('title', 'name')\n                    rows = current.db(query).select(table._id, table[title_field])\n                    native = (c == 'xforms')\n                    for row in rows:\n                        if native:\n                            args = [tablename, row[table._id]]\n                        else:\n                            args = [row[table._id], 'xform.xhtml']\n                        url = URL(c=c, f=f, args=args, vars=url_vars, host=True, extension='')\n                        xforms.append((url, row[title_field]))\n                else:\n                    continue\n            else:\n                if (c == 'xforms'):\n                    args = [tablename]\n                else:\n                    args = ['xform.xhtml']\n                url = URL(c=c, f=f, args=args, vars=url_vars, host=True, extension='')\n                xforms.append((url, title))\n    return xforms\n", "label": 1}
{"function": "\n\ndef update_header(self):\n    old_naxis = self._header.get('NAXIS', 0)\n    if self._data_loaded:\n        if isinstance(self.data, GroupData):\n            self._axes = list(self.data.data.shape)[1:]\n            self._axes.reverse()\n            self._axes = ([0] + self._axes)\n            field0 = self.data.dtype.names[0]\n            field0_code = self.data.dtype.fields[field0][0].name\n        elif (self.data is None):\n            self._axes = [0]\n            field0_code = 'uint8'\n        else:\n            raise ValueError('incorrect array type')\n        self._header['BITPIX'] = DTYPE2BITPIX[field0_code]\n    self._header['NAXIS'] = len(self._axes)\n    for (idx, axis) in enumerate(self._axes):\n        if (idx == 0):\n            after = 'NAXIS'\n        else:\n            after = ('NAXIS' + str(idx))\n        self._header.set(('NAXIS' + str((idx + 1))), axis, after=after)\n    for idx in range((len(self._axes) + 1), (old_naxis + 1)):\n        try:\n            del self._header[('NAXIS' + str(idx))]\n        except KeyError:\n            pass\n    if (self._has_data and isinstance(self.data, GroupData)):\n        self._header.set('GROUPS', True, after=('NAXIS' + str(len(self._axes))))\n        self._header.set('PCOUNT', len(self.data.parnames), after='GROUPS')\n        self._header.set('GCOUNT', len(self.data), after='PCOUNT')\n        column = self.data._coldefs[self._data_field]\n        (scale, zero) = self.data._get_scale_factors(column)[3:5]\n        if scale:\n            self._header.set('BSCALE', column.bscale)\n        if zero:\n            self._header.set('BZERO', column.bzero)\n        for (idx, name) in enumerate(self.data.parnames):\n            self._header.set(('PTYPE' + str((idx + 1))), name)\n            column = self.data._coldefs[idx]\n            (scale, zero) = self.data._get_scale_factors(column)[3:5]\n            if scale:\n                self._header.set(('PSCAL' + str((idx + 1))), column.bscale)\n            if zero:\n                self._header.set(('PZERO' + str((idx + 1))), column.bzero)\n    if ('EXTEND' in self._header):\n        if len(self._axes):\n            after = ('NAXIS' + str(len(self._axes)))\n        else:\n            after = 'NAXIS'\n        self._header.set('EXTEND', after=after)\n", "label": 1}
{"function": "\n\ndef prettyIn(self, value):\n    if (isinstance(value, tuple) and (len(value) == 3)):\n        if ((not isinstance(value[0], numericTypes)) or (not isinstance(value[1], intTypes)) or (not isinstance(value[2], intTypes))):\n            raise error.PyAsn1Error(('Lame Real value syntax: %s' % (value,)))\n        if (isinstance(value[0], float) and self._inf and (value[0] in self._inf)):\n            return value[0]\n        if (value[1] not in (2, 10)):\n            raise error.PyAsn1Error(('Prohibited base for Real value: %s' % (value[1],)))\n        if (value[1] == 10):\n            value = self.__normalizeBase10(value)\n        return value\n    elif isinstance(value, intTypes):\n        return self.__normalizeBase10((value, 10, 0))\n    elif isinstance(value, (str, float)):\n        if isinstance(value, str):\n            try:\n                value = float(value)\n            except ValueError:\n                raise error.PyAsn1Error(('Bad real value syntax: %s' % (value,)))\n        if (self._inf and (value in self._inf)):\n            return value\n        else:\n            e = 0\n            while (int(value) != value):\n                value = (value * 10)\n                e = (e - 1)\n            return self.__normalizeBase10((int(value), 10, e))\n    elif isinstance(value, Real):\n        return tuple(value)\n    raise error.PyAsn1Error(('Bad real value syntax: %s' % (value,)))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse(wmsg):\n    '\\n        Verifies and parses an unserialized raw message into an actual WAMP message instance.\\n\\n        :param wmsg: The unserialized raw message.\\n        :type wmsg: list\\n\\n        :returns: An instance of this class.\\n        '\n    assert ((len(wmsg) > 0) and (wmsg[0] == Yield.MESSAGE_TYPE))\n    if (len(wmsg) not in (3, 4, 5)):\n        raise ProtocolError('invalid message length {0} for YIELD'.format(len(wmsg)))\n    request = check_or_raise_id(wmsg[1], \"'request' in YIELD\")\n    options = check_or_raise_extra(wmsg[2], \"'options' in YIELD\")\n    args = None\n    kwargs = None\n    payload = None\n    enc_algo = None\n    enc_key = None\n    enc_serializer = None\n    if ((len(wmsg) == 4) and (type(wmsg[3]) in [six.text_type, six.binary_type])):\n        payload = wmsg[3]\n        enc_algo = options.get('enc_algo', None)\n        if (enc_algo and (enc_algo not in [PAYLOAD_ENC_CRYPTO_BOX])):\n            raise ProtocolError(\"invalid value {0} for 'enc_algo' detail in EVENT\".format(enc_algo))\n        enc_key = options.get('enc_key', None)\n        if (enc_key and (type(enc_key) not in [six.text_type, six.binary_type])):\n            raise ProtocolError(\"invalid type {0} for 'enc_key' detail in EVENT\".format(type(enc_key)))\n        enc_serializer = options.get('enc_serializer', None)\n        if (enc_serializer and (enc_serializer not in ['json', 'msgpack', 'cbor', 'ubjson'])):\n            raise ProtocolError(\"invalid value {0} for 'enc_serializer' detail in EVENT\".format(enc_serializer))\n    else:\n        if (len(wmsg) > 3):\n            args = wmsg[3]\n            if (type(args) != list):\n                raise ProtocolError(\"invalid type {0} for 'args' in YIELD\".format(type(args)))\n        if (len(wmsg) > 4):\n            kwargs = wmsg[4]\n            if (type(kwargs) != dict):\n                raise ProtocolError(\"invalid type {0} for 'kwargs' in YIELD\".format(type(kwargs)))\n    progress = None\n    if ('progress' in options):\n        option_progress = options['progress']\n        if (type(option_progress) != bool):\n            raise ProtocolError(\"invalid type {0} for 'progress' option in YIELD\".format(type(option_progress)))\n        progress = option_progress\n    obj = Yield(request, args=args, kwargs=kwargs, payload=payload, progress=progress, enc_algo=enc_algo, enc_key=enc_key, enc_serializer=enc_serializer)\n    return obj\n", "label": 1}
{"function": "\n\n@classmethod\ndef run_simple_topology(cls, config, emitters, result_type=NAMEDTUPLE, max_spout_emits=None):\n    'Tests a simple topology. \"Simple\" means there it has no branches\\n        or cycles. \"emitters\" is a list of emitters, starting with a spout\\n        followed by 0 or more bolts that run in a chain.'\n    if (config is not None):\n        for emitter in emitters:\n            emitter.initialize(config, {\n                \n            })\n    with cls() as self:\n        spout = emitters[0]\n        spout_id = self.emitter_id(spout)\n        old_length = (- 1)\n        length = len(self.pending[spout_id])\n        while ((length > old_length) and ((max_spout_emits is None) or (length < max_spout_emits))):\n            old_length = length\n            self.activate(spout)\n            spout.nextTuple()\n            length = len(self.pending[spout_id])\n        for (i, bolt) in enumerate(emitters[1:]):\n            previous = emitters[i]\n            self.activate(bolt)\n            while (len(self.pending[self.emitter_id(previous)]) > 0):\n                bolt.process(self.read(previous))\n\n    def make_storm_tuple(t, emitter):\n        return t\n\n    def make_python_list(t, emitter):\n        return list(t.values)\n\n    def make_python_tuple(t, emitter):\n        return tuple(t.values)\n\n    def make_named_tuple(t, emitter):\n        return self.get_output_type(emitter)(*t.values)\n    if (result_type == STORM_TUPLE):\n        make = make_storm_tuple\n    elif (result_type == LIST):\n        make = make_python_list\n    elif (result_type == NAMEDTUPLE):\n        make = make_named_tuple\n    else:\n        assert False, ('Invalid result type specified: %s' % result_type)\n    result_values = ([[make(t, emitter) for t in self.processed[self.emitter_id(emitter)]] for emitter in emitters[:(- 1)]] + [[make(t, emitters[(- 1)]) for t in self.pending[self.emitter_id(emitters[(- 1)])]]])\n    return dict(((k, v) for (k, v) in zip(emitters, result_values)))\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    verbose = (options.get('verbosity') > 1)\n    only_empty = options.get('only_empty')\n    copy_content = options.get('copy_content')\n    from_lang = options.get('from_lang')\n    to_lang = options.get('to_lang')\n    try:\n        site = int(options.get('site', None))\n    except Exception:\n        site = settings.SITE_ID\n    try:\n        assert (from_lang in get_language_list(site))\n        assert (to_lang in get_language_list(site))\n    except AssertionError:\n        raise CommandError('Both languages have to be present in settings.LANGUAGES and settings.CMS_LANGUAGES')\n    for page in Page.objects.on_site(site).drafts():\n        if (from_lang in page.get_languages()):\n            title = page.get_title_obj(to_lang, fallback=False)\n            if isinstance(title, EmptyTitle):\n                title = page.get_title_obj(from_lang)\n                if verbose:\n                    self.stdout.write(('copying title %s from language %s\\n' % (title.title, from_lang)))\n                title.id = None\n                title.publisher_public_id = None\n                title.publisher_state = 0\n                title.language = to_lang\n                title.save()\n            if copy_content:\n                if verbose:\n                    self.stdout.write(('copying plugins for %s from %s\\n' % (page.get_page_title(from_lang), from_lang)))\n                copy_plugins_to_language(page, from_lang, to_lang, only_empty)\n        elif verbose:\n            self.stdout.write(('Skipping page %s, language %s not defined\\n' % (page.get_page_title(page.get_languages()[0]), from_lang)))\n    if copy_content:\n        for static_placeholder in StaticPlaceholder.objects.all():\n            plugin_list = []\n            for plugin in static_placeholder.draft.get_plugins():\n                if (plugin.language == from_lang):\n                    plugin_list.append(plugin)\n            if plugin_list:\n                if verbose:\n                    self.stdout.write(('copying plugins from static_placeholder \"%s\" in \"%s\" to \"%s\"\\n' % (static_placeholder.name, from_lang, to_lang)))\n                copy_plugins_to(plugin_list, static_placeholder.draft, to_lang)\n    self.stdout.write('all done')\n", "label": 1}
{"function": "\n\ndef getCoverage(self, identifier=None, bbox=None, time=None, format=None, store=False, rangesubset=None, gridbaseCRS=None, gridtype=None, gridCS=None, gridorigin=None, gridoffsets=None, method='Get', **kwargs):\n    \"Request and return a coverage from the WCS as a file-like object\\n        note: additional **kwargs helps with multi-version implementation\\n        core keyword arguments should be supported cross version\\n        example:\\n        cvg=wcs.getCoverageRequest(identifier=['TuMYrRQ4'], time=['2792-06-01T00:00:00.0'], bbox=(-112,36,-106,41),format='application/netcdf', store='true')\\n\\n        is equivalent to:\\n        http://myhost/mywcs?SERVICE=WCS&REQUEST=GetCoverage&IDENTIFIER=TuMYrRQ4&VERSION=1.1.0&BOUNDINGBOX=-180,-90,180,90&TIMESEQUENCE=2792-06-01T00:00:00.0&FORMAT=application/netcdf\\n        \\n        if store = true, returns a coverages XML file\\n        if store = false, returns a multipart mime\\n        \"\n    if log.isEnabledFor(logging.DEBUG):\n        log.debug(('WCS 1.1.0 DEBUG: Parameters passed to GetCoverage: identifier=%s, bbox=%s, time=%s, format=%s, rangesubset=%s, gridbaseCRS=%s, gridtype=%s, gridCS=%s, gridorigin=%s, gridoffsets=%s, method=%s, other_arguments=%s' % (identifier, bbox, time, format, rangesubset, gridbaseCRS, gridtype, gridCS, gridorigin, gridoffsets, method, str(kwargs))))\n    if (method == 'Get'):\n        method = self.ns.WCS_OWS('Get')\n    try:\n        base_url = next((m.get('url') for m in self.getOperationByName('GetCoverage').methods if (m.get('type').lower() == method.lower())))\n    except StopIteration:\n        base_url = self.url\n    request = {\n        'version': self.version,\n        'request': 'GetCoverage',\n        'service': 'WCS',\n    }\n    assert (len(identifier) > 0)\n    request['identifier'] = identifier\n    if bbox:\n        request['boundingbox'] = ','.join([repr(x) for x in bbox])\n    if time:\n        request['timesequence'] = ','.join(time)\n    request['format'] = format\n    request['store'] = store\n    if rangesubset:\n        request['RangeSubset'] = rangesubset\n    if gridbaseCRS:\n        request['gridbaseCRS'] = gridbaseCRS\n    if gridtype:\n        request['gridtype'] = gridtype\n    if gridCS:\n        request['gridCS'] = gridCS\n    if gridorigin:\n        request['gridorigin'] = gridorigin\n    if gridoffsets:\n        request['gridoffsets'] = gridoffsets\n    if kwargs:\n        for kw in kwargs:\n            request[kw] = kwargs[kw]\n    data = urlencode(request)\n    u = openURL(base_url, data, method, self.cookies)\n    return u\n", "label": 1}
{"function": "\n\ndef replicate_attributes(source, target, cache=None):\n    'Replicates common SQLAlchemy attributes from the `source` object to the\\n    `target` object.'\n    target_manager = manager_of_class(type(target))\n    column_attrs = set()\n    relationship_attrs = set()\n    relationship_columns = set()\n    for attr in manager_of_class(type(source)).attributes:\n        if (attr.key not in target_manager):\n            continue\n        target_attr = target_manager[attr.key]\n        if isinstance(attr.property, ColumnProperty):\n            assert isinstance(target_attr.property, ColumnProperty)\n            column_attrs.add(attr)\n        elif isinstance(attr.property, RelationshipProperty):\n            assert isinstance(target_attr.property, RelationshipProperty)\n            relationship_attrs.add(attr)\n            if (attr.property.direction is MANYTOONE):\n                relationship_columns.update(attr.property.local_columns)\n    for attr in column_attrs:\n        if _column_property_in_registry(attr.property, _excluded):\n            continue\n        elif ((not _column_property_in_registry(attr.property, _included)) and all(((column in relationship_columns) for column in attr.property.columns))):\n            continue\n        setattr(target, attr.key, getattr(source, attr.key))\n    for attr in relationship_attrs:\n        target_attr_model = target_manager[attr.key].property.argument\n        if (not is_relation_replicatable(attr)):\n            continue\n        replicate_relation(source, target, attr, target_manager[attr.key], cache=cache)\n", "label": 1}
{"function": "\n\ndef __getattribute__(self, name):\n    try:\n        result = super(GsxElement, self).__getattribute__(name)\n    except AttributeError:\n        \"\\n            The XML returned by GSX can be pretty inconsistent, especially\\n            between the different environments. It's therefore more\\n            practical to return None than to expect AttributeErrors all\\n            over your application.\\n            \"\n        return\n    if (name in STRING_TYPES):\n        return unicode((result.text or ''))\n    if isinstance(result, objectify.NumberElement):\n        return result.pyval\n    if isinstance(result, objectify.StringElement):\n        name = result.tag\n        result = (result.text or '')\n        result = unicode(result)\n        if (not result):\n            return\n        if (name in DATETIME_TYPES):\n            return gsx_datetime(result)\n        if (name in DIAGS_TIMESTAMP_TYPES):\n            return gsx_diags_timestamp(result)\n        if (name in BASE64_TYPES):\n            return gsx_attachment(result)\n        if (name in FLOAT_TYPES):\n            return gsx_price(result)\n        if name.endswith('Date'):\n            return gsx_date(result)\n        if name.endswith('Timestamp'):\n            return gsx_timestamp(result)\n        if re.search('^[YN]$', result):\n            return gsx_boolean(result)\n    return result\n", "label": 1}
{"function": "\n\ndef __init__(self, env, options=None):\n    global _maybe_wrap_wsgi_stream\n    self.env = env\n    self.options = (options if options else RequestOptions())\n    self._wsgierrors = env['wsgi.errors']\n    self.stream = env['wsgi.input']\n    self.method = env['REQUEST_METHOD']\n    path = env['PATH_INFO']\n    if path:\n        if six.PY3:\n            path = path.encode('latin1').decode('utf-8', 'replace')\n        if ((len(path) != 1) and path.endswith('/')):\n            self.path = path[:(- 1)]\n        else:\n            self.path = path\n    else:\n        self.path = '/'\n    if ('QUERY_STRING' in env):\n        self.query_string = env['QUERY_STRING']\n        if self.query_string:\n            self._params = parse_query_string(self.query_string, keep_blank_qs_values=self.options.keep_blank_qs_values)\n        else:\n            self._params = {\n                \n            }\n    else:\n        self.query_string = ''\n        self._params = {\n            \n        }\n    self._cookies = None\n    self._cached_headers = None\n    self._cached_uri = None\n    self._cached_relative_uri = None\n    self._cached_access_route = None\n    try:\n        self.content_type = self.env['CONTENT_TYPE']\n    except KeyError:\n        self.content_type = None\n    if _maybe_wrap_wsgi_stream:\n        if isinstance(self.stream, (NativeStream, InputWrapper)):\n            self._wrap_stream()\n        else:\n            _maybe_wrap_wsgi_stream = False\n    if (self.options.auto_parse_form_urlencoded and (self.content_type is not None) and ('application/x-www-form-urlencoded' in self.content_type)):\n        self._parse_form_urlencoded()\n    if (self.context_type is None):\n        self.context = {\n            \n        }\n    else:\n        self.context = self.context_type()\n", "label": 1}
{"function": "\n\ndef test_splitlines(self):\n    assert (evalpy('\"\".splitlines()') == \"[ '' ]\")\n    assert (evalpy('\"\".splitlines(true)') == \"[ '' ]\")\n    assert (evalpy('\"\\\\n\".splitlines()') == \"[ '' ]\")\n    assert (evalpy('\"\\\\n\".splitlines(True)') == \"[ '\\\\n' ]\")\n    assert (evalpy('\"abc def\".splitlines()') == \"[ 'abc def' ]\")\n    assert (evalpy('\"abc\\\\ndef\".splitlines()') == \"[ 'abc', 'def' ]\")\n    assert (evalpy('\"abc\\\\ndef\\\\n\".splitlines()') == \"[ 'abc', 'def' ]\")\n    assert (evalpy('\"abc\\\\rdef\".splitlines()') == \"[ 'abc', 'def' ]\")\n    assert (evalpy('\"abc\\\\r\\\\ndef\".splitlines()') == \"[ 'abc', 'def' ]\")\n    assert (evalpy('\"abc\\\\n\\\\rdef\".splitlines()') == \"[ 'abc', '', 'def' ]\")\n    assert (evalpy('\"abc def\".splitlines(True)') == \"[ 'abc def' ]\")\n    assert (evalpy('\"abc\\\\ndef\".splitlines(True)') == \"[ 'abc\\\\n', 'def' ]\")\n    assert (evalpy('\"abc\\\\ndef\\\\n\".splitlines(True)') == \"[ 'abc\\\\n', 'def\\\\n' ]\")\n    assert (evalpy('\"abc\\\\rdef\".splitlines(True)') == \"[ 'abc\\\\r', 'def' ]\")\n    assert (evalpy('\"abc\\\\r\\\\ndef\".splitlines(True)') == \"[ 'abc\\\\r\\\\n', 'def' ]\")\n    res = repr('X\\n\\nX\\r\\rX\\r\\n\\rX\\n\\r\\nX'.splitlines(True)).replace(' ', '')\n    res = res.replace('u\"', '\"').replace(\"u'\", \"'\")\n    assert (nowhitespace(evalpy('\"X\\\\n\\\\nX\\\\r\\\\rX\\\\r\\\\n\\\\rX\\\\n\\\\r\\\\nX\".splitlines(true)')) == res)\n", "label": 1}
{"function": "\n\ndef _init(self, trcback):\n    'format a traceback from sys.exc_info() into 7-item tuples,\\n        containing the regular four traceback tuple items, plus the original\\n        template filename, the line number adjusted relative to the template\\n        source, and code line from that line number of the template.'\n    import mako.template\n    mods = {\n        \n    }\n    rawrecords = traceback.extract_tb(trcback)\n    new_trcback = []\n    for (filename, lineno, function, line) in rawrecords:\n        if (not line):\n            line = ''\n        try:\n            (line_map, template_lines) = mods[filename]\n        except KeyError:\n            try:\n                info = mako.template._get_module_info(filename)\n                module_source = info.code\n                template_source = info.source\n                template_filename = (info.template_filename or filename)\n            except KeyError:\n                if (not util.py3k):\n                    try:\n                        fp = open(filename, 'rb')\n                        encoding = util.parse_encoding(fp)\n                        fp.close()\n                    except IOError:\n                        encoding = None\n                    if encoding:\n                        line = line.decode(encoding)\n                    else:\n                        line = line.decode('ascii', 'replace')\n                new_trcback.append((filename, lineno, function, line, None, None, None, None))\n                continue\n            template_ln = module_ln = 1\n            line_map = {\n                \n            }\n            for line in module_source.split('\\n'):\n                match = re.match('\\\\s*# SOURCE LINE (\\\\d+)', line)\n                if match:\n                    template_ln = int(match.group(1))\n                module_ln += 1\n                line_map[module_ln] = template_ln\n            template_lines = [line for line in template_source.split('\\n')]\n            mods[filename] = (line_map, template_lines)\n        template_ln = line_map[lineno]\n        if (template_ln <= len(template_lines)):\n            template_line = template_lines[(template_ln - 1)]\n        else:\n            template_line = None\n        new_trcback.append((filename, lineno, function, line, template_filename, template_ln, template_line, template_source))\n    if (not self.source):\n        for l in range((len(new_trcback) - 1), 0, (- 1)):\n            if new_trcback[l][5]:\n                self.source = new_trcback[l][7]\n                self.lineno = new_trcback[l][5]\n                break\n        else:\n            if new_trcback:\n                try:\n                    fp = open(new_trcback[(- 1)][0], 'rb')\n                    encoding = util.parse_encoding(fp)\n                    fp.seek(0)\n                    self.source = fp.read()\n                    fp.close()\n                    if encoding:\n                        self.source = self.source.decode(encoding)\n                except IOError:\n                    self.source = ''\n                self.lineno = new_trcback[(- 1)][1]\n    return new_trcback\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('variant', (None,))\n@pytest.mark.parametrize('input_instance, input_init, init,  missing_obj', [(False, None, True, None), (False, None, False, Undefined), (True, False, True, None), (True, False, False, Undefined), (True, True, True, None), (True, True, False, None)])\ndef test_conversion_to_dictl(input, input_instance, init, missing_obj):\n    orig_input = copy(input)\n    m = convert(M, input, init_values=init, partial=True)\n    assert (type(m) is dict)\n    assert (type(m['intfield']) is int)\n    assert (type(m['modelfield']['modelfield']['intfield']) is int)\n    assert (type(m['modelfield']['modelfield']['matrixfield'][2][3]) is int)\n    assert (type(m['listfield']) is list)\n    assert (type(m['modelfield']) is dict)\n    assert (type(m['modelfield']['modelfield']) is dict)\n    assert (type(m['modelfield']['modelfield']['modelfield']) is dict)\n    assert (type(m['modelfield']['listfield']) is list)\n    assert (type(m['modelfield']['modelfield']['matrixfield']) is list)\n    assert (type(m['modelfield']['modelfield']['matrixfield'][2]) is list)\n    assert (m['listfield'] == [])\n    assert (m['modelfield'].get('intfield', Undefined) is missing_obj)\n    assert (m['modelfield']['modelfield']['listfield'] is None)\n    assert (m['modelfield']['modelfield'].get('reqfield', Undefined) is missing_obj)\n    assert (input == orig_input)\n    if input_instance:\n        assert (m['modelfield'] is not input['modelfield'])\n    assert (m['modelfield']['listfield'] is not input['modelfield']['listfield'])\n", "label": 1}
{"function": "\n\ndef coveralls(http=None, url=None, data_file=None, repo_token=None, git=None, service_name=None, service_job_id=None, strip_dirs=None, ignore_errors=False, stream=None):\n    'Send a coverage report to coveralls.io.\\n\\n    :param http: optional http client\\n    :param url: optional url to send data to. It defaults to ``coveralls``\\n        api url.\\n    :param data_file: optional data file to load coverage data from. By\\n        default, coverage uses ``.coverage``.\\n    :param repo_token: required when not submitting from travis.\\n\\n    https://coveralls.io/docs/api\\n    '\n    if ((repo_token is None) and os.path.isfile('.coveralls-repo-token')):\n        with open('.coveralls-repo-token') as f:\n            repo_token = f.read().strip()\n    if (strip_dirs is None):\n        strip_dirs = [os.getcwd()]\n    stream = (stream or sys.stdout)\n    coverage = Coverage(data_file=data_file)\n    coverage.load()\n    if (http is None):\n        http = HttpClient(loop=new_event_loop())\n    if (not git):\n        try:\n            git = gitrepo()\n        except Exception:\n            pass\n    data = {\n        'source_files': coverage.coveralls(strip_dirs=strip_dirs, ignore_errors=ignore_errors),\n    }\n    if git:\n        data['git'] = git\n    if os.environ.get('TRAVIS'):\n        data['service_name'] = (service_name or 'travis-ci')\n        data['service_job_id'] = os.environ.get('TRAVIS_JOB_ID')\n    else:\n        assert repo_token, 'Requires repo_token if not submitting from travis'\n    if repo_token:\n        data['repo_token'] = repo_token\n    url = (url or COVERALLS_URL)\n    stream.write(('Submitting coverage report to %s\\n' % url))\n    response = http.post(url, files={\n        'json_file': json.dumps(data),\n    })\n    stream.write(('Response code: %s\\n' % response.status_code))\n    try:\n        info = response.json()\n        code = 0\n        if ('error' in info):\n            stream.write('An error occured while sending coverage report to coverall.io')\n            code = 1\n        stream.write(('\\n%s\\n' % info['message']))\n    except Exception:\n        code = 1\n        stream.write(('Critical error %s\\n' % response.status_code))\n    return code\n", "label": 1}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_start_timestamp_milliseconds():\n        self.set_start_timestamp_milliseconds(x.start_timestamp_milliseconds())\n    if x.has_http_method():\n        self.set_http_method(x.http_method())\n    if x.has_http_path():\n        self.set_http_path(x.http_path())\n    if x.has_http_query():\n        self.set_http_query(x.http_query())\n    if x.has_http_status():\n        self.set_http_status(x.http_status())\n    if x.has_duration_milliseconds():\n        self.set_duration_milliseconds(x.duration_milliseconds())\n    if x.has_api_mcycles():\n        self.set_api_mcycles(x.api_mcycles())\n    if x.has_processor_mcycles():\n        self.set_processor_mcycles(x.processor_mcycles())\n    for i in xrange(x.rpc_stats_size()):\n        self.add_rpc_stats().CopyFrom(x.rpc_stats(i))\n    for i in xrange(x.cgi_env_size()):\n        self.add_cgi_env().CopyFrom(x.cgi_env(i))\n    if x.has_overhead_walltime_milliseconds():\n        self.set_overhead_walltime_milliseconds(x.overhead_walltime_milliseconds())\n    if x.has_user_email():\n        self.set_user_email(x.user_email())\n    if x.has_is_admin():\n        self.set_is_admin(x.is_admin())\n    for i in xrange(x.individual_stats_size()):\n        self.add_individual_stats().CopyFrom(x.individual_stats(i))\n", "label": 1}
{"function": "\n\ndef dumpf(self, gzip=False):\n    '\\n        Generate a file containing shell code and all file contents.\\n        '\n    if ((0 < len(self.sources)) or self.templates):\n        os.mkdir(self.name)\n        filename = os.path.join(self.name, 'bootstrap.sh')\n        f = codecs.open(filename, 'w', encoding='utf-8')\n    elif gzip:\n        filename = '{0}.sh.gz'.format(self.name)\n        f = gziplib.open(filename, 'w')\n    else:\n        filename = '{0}.sh'.format(self.name)\n        f = codecs.open(filename, 'w', encoding='utf-8')\n    if self.templates:\n        os.mkdir(os.path.join(self.name, 'etc'))\n        os.mkdir(os.path.join(self.name, 'etc', 'blueprint-template.d'))\n        os.mkdir(os.path.join(self.name, 'lib'))\n        os.mkdir(os.path.join(self.name, 'lib', 'blueprint-template.d'))\n        copyfile(os.path.join(os.path.dirname(__file__), 'mustache.sh'), os.path.join(self.name, 'lib', 'mustache.sh'))\n        for (src, dest) in [('/etc/blueprint-template.d', 'etc'), (os.path.join(os.path.dirname(__file__), 'blueprint-template.d'), 'lib')]:\n            try:\n                for filename2 in os.listdir(src):\n                    if filename2.endswith('.sh'):\n                        copyfile(os.path.join(src, filename2), os.path.join(self.name, dest, 'blueprint-template.d', filename2))\n            except OSError:\n                pass\n    for out in self.out:\n        if isinstance(out, unicode):\n            out = unicodedata.normalize('NFKD', out).encode('utf-8', 'ignore')\n        f.write('{0}\\n'.format(out))\n    f.close()\n    for (filename2, blob) in sorted(self.sources.iteritems()):\n        git.cat_file(blob, os.path.join(self.name, filename2))\n    if (gzip and ((0 < len(self.sources)) or self.templates)):\n        filename = 'sh-{0}.tar.gz'.format(self.name)\n        tarball = tarfile.open(filename, 'w:gz')\n        tarball.add(self.name)\n        tarball.close()\n        return filename\n    return filename\n", "label": 1}
{"function": "\n\ndef _datalist(self, r, widget, **attr):\n    '\\n            Generate a data list\\n\\n            @param r: the S3Request instance\\n            @param widget: the widget definition as dict\\n            @param attr: controller attributes for the request\\n        '\n    T = current.T\n    context = widget.get('context')\n    tablename = widget.get('tablename')\n    (resource, context) = self._resolve_context(r, tablename, context)\n    config = resource.get_config\n    list_fields = widget.get('list_fields', config('list_fields', None))\n    list_layout = widget.get('list_layout', config('list_layout', None))\n    orderby = widget.get('orderby', config('list_orderby', config('orderby', (~ resource.table.created_on))))\n    widget_filter = widget.get('filter')\n    if widget_filter:\n        resource.add_filter(widget_filter)\n    list_id = ('profile-list-%s-%s' % (tablename, widget['index']))\n    pagesize = widget.get('pagesize', 4)\n    representation = r.representation\n    if (representation == 'dl'):\n        get_vars = r.get_vars\n        record_id = get_vars.get('record', None)\n        if (record_id is not None):\n            resource.add_filter((FS('id') == record_id))\n            (start, limit) = (0, 1)\n        else:\n            start = get_vars.get('start', None)\n            limit = get_vars.get('limit', None)\n            if (limit is not None):\n                try:\n                    start = int(start)\n                    limit = int(limit)\n                except ValueError:\n                    (start, limit) = (0, pagesize)\n            else:\n                start = None\n    else:\n        (start, limit) = (0, pagesize)\n    if ((representation == 'dl') and (r.http in ('DELETE', 'POST'))):\n        if ('delete' in r.get_vars):\n            return self._dl_ajax_delete(r, resource)\n        else:\n            r.error(405, current.ERROR.BAD_METHOD)\n    (datalist, numrows, ids) = resource.datalist(fields=list_fields, start=start, limit=limit, list_id=list_id, orderby=orderby, layout=list_layout)\n    ajaxurl = r.url(vars={\n        'update': widget['index'],\n    }, representation='dl')\n    data = datalist.html(ajaxurl=ajaxurl, pagesize=pagesize, empty=P(ICON('folder-open-alt'), BR(), S3CRUD.crud_string(tablename, 'msg_no_match'), _class='empty_card-holder'))\n    if (representation == 'dl'):\n        current.response.view = 'plain.html'\n        return data\n    label = widget.get('label', '')\n    if label:\n        label = T(label)\n    icon = widget.get('icon', '')\n    if icon:\n        icon = ICON(icon)\n    if (pagesize and (numrows > pagesize)):\n        more = (numrows - pagesize)\n        get_vars_new = {\n            \n        }\n        if context:\n            filters = context.serialize_url(resource)\n            for f in filters:\n                get_vars_new[f] = filters[f]\n        if widget_filter:\n            filters = widget_filter.serialize_url(resource)\n            for f in filters:\n                get_vars_new[f] = filters[f]\n        (c, f) = tablename.split('_', 1)\n        f = widget.get('function', f)\n        url = URL(c=c, f=f, args=['datalist.popup'], vars=get_vars_new)\n        more = DIV(A(BUTTON(('%s (%s)' % (T('see more'), more)), _class='btn btn-mini tiny button', _type='button'), _class='s3_modal', _href=url, _title=label), _class='more_profile')\n    else:\n        more = ''\n    create_popup = self._create_popup(r, widget, list_id, resource, context, numrows)\n    _class = self._lookup_class(r, widget)\n    output = DIV(create_popup, H4(icon, label, _class='profile-sub-header'), DIV(data, more, _class='card-holder'), _class=_class)\n    return output\n", "label": 1}
{"function": "\n\ndef _run_options(self, name, flow, jobs=None, disabled_jobs=None, concurrent=True, properties=None, on_failure='finish', notify_early=False, emails=None):\n    'Construct data dict for run related actions.\\n\\n    See :meth:`run_workflow` for parameter documentation.\\n\\n    '\n    if (jobs and disabled_jobs):\n        raise ValueError('`jobs` and `disabled_jobs` are mutually exclusive.')\n    if (not jobs):\n        if (not disabled_jobs):\n            disabled = '[]'\n        else:\n            disabled = ('[%s]' % (','.join((('\"%s\"' % (n,)) for n in disabled_jobs)),))\n    else:\n        all_names = set((n['id'] for n in self.get_workflow_info(name, flow)['nodes']))\n        run_names = set(jobs)\n        missing_names = (run_names - all_names)\n        if missing_names:\n            raise AzkabanError(('Jobs not found in flow %r: %s.' % (flow, ', '.join(missing_names))))\n        else:\n            disabled = ('[%s]' % (','.join((('\"%s\"' % (n,)) for n in (all_names - run_names))),))\n    try:\n        failure_action = {\n            'finish': 'finishCurrent',\n            'continue': 'finishPossible',\n            'cancel': 'cancelImmediately',\n        }[on_failure]\n    except KeyError:\n        raise ValueError(('Invalid `on_failure` value: %r.' % (on_failure,)))\n    request_data = {\n        'disabled': disabled,\n        'concurrentOption': ('concurrent' if concurrent else 'skip'),\n        'failureAction': failure_action,\n        'notifyFailureFirst': ('true' if notify_early else 'false'),\n    }\n    if properties:\n        request_data.update(dict(((('flowOverride[%s]' % (key,)), value) for (key, value) in flatten(properties).items())))\n    if emails:\n        if isinstance(emails[0], string_types):\n            failure_emails = ','.join(emails)\n            success_emails = failure_emails\n        else:\n            failure_emails = ','.join(emails[0])\n            success_emails = ','.join(emails[1])\n        request_data.update({\n            'failureEmails': failure_emails,\n            'failureEmailsOverride': 'true',\n            'successEmails': success_emails,\n            'successEmailsOverride': 'true',\n        })\n    return request_data\n", "label": 1}
{"function": "\n\ndef test_is_constant():\n    from sympy.solvers.solvers import checksol\n    (Sum(x, (x, 1, 10)).is_constant() is True)\n    (Sum(x, (x, 1, n)).is_constant() is False)\n    (Sum(x, (x, 1, n)).is_constant(y) is True)\n    (Sum(x, (x, 1, n)).is_constant(n) is False)\n    (Sum(x, (x, 1, n)).is_constant(x) is True)\n    eq = (((a * (cos(x) ** 2)) + (a * (sin(x) ** 2))) - a)\n    (eq.is_constant() is True)\n    assert (eq.subs({\n        x: pi,\n        a: 2,\n    }) == eq.subs({\n        x: pi,\n        a: 3,\n    }) == 0)\n    assert (x.is_constant() is False)\n    assert (x.is_constant(y) is True)\n    assert (checksol(x, x, Sum(x, (x, 1, n))) is False)\n    assert (checksol(x, x, Sum(x, (x, 1, n))) is False)\n    f = Function('f')\n    assert (checksol(x, x, f(x)) is False)\n    p = symbols('p', positive=True)\n    assert (Pow(x, S(0), evaluate=False).is_constant() is True)\n    assert (Pow(S(0), x, evaluate=False).is_constant() is False)\n    assert ((2 ** x).is_constant() is False)\n    assert (Pow(S(2), S(3), evaluate=False).is_constant() is True)\n    (z1, z2) = symbols('z1 z2', zero=True)\n    assert ((z1 + (2 * z2)).is_constant() is True)\n    assert (meter.is_constant() is True)\n    assert ((3 * meter).is_constant() is True)\n    assert ((x * meter).is_constant() is False)\n", "label": 1}
{"function": "\n\ndef check(auth_ref, args):\n    keystone = get_keystone_client(auth_ref)\n    auth_token = keystone.auth_token\n    VOLUME_ENDPOINT = 'http://{hostname}:8776/v1/{tenant}'.format(hostname=args.hostname, tenant=keystone.tenant_id)\n    s = requests.Session()\n    s.headers.update({\n        'Content-type': 'application/json',\n        'x-auth-token': auth_token,\n    })\n    try:\n        r = s.get(('%s/os-services' % VOLUME_ENDPOINT), verify=False, timeout=10)\n    except (exc.ConnectionError, exc.HTTPError, exc.Timeout) as e:\n        status_err(str(e))\n    if (not r.ok):\n        status_err('Could not get response from Cinder API')\n    services = r.json()['services']\n    if args.host:\n        backend = ''.join((args.host, '@'))\n        services = [service for service in services if (service['host'].startswith(backend) or (service['host'] == args.host))]\n    if (len(services) == 0):\n        status_err('No host(s) found in the service list')\n    status_ok()\n    if args.host:\n        all_services_are_up = True\n        for service in services:\n            service_is_up = True\n            if ((service['status'] == 'enabled') and (service['state'] != 'up')):\n                service_is_up = False\n                all_services_are_up = False\n            if ('@' in service['host']):\n                [host, backend] = service['host'].split('@')\n                name = ('%s-%s_status' % (service['binary'], backend))\n                metric_bool(name, service_is_up)\n        name = ('%s_status' % service['binary'])\n        metric_bool(name, all_services_are_up)\n    else:\n        for service in services:\n            service_is_up = True\n            if ((service['status'] == 'enabled') and (service['state'] != 'up')):\n                service_is_up = False\n            name = ('%s_on_host_%s' % (service['binary'], service['host']))\n            metric_bool(name, service_is_up)\n", "label": 1}
{"function": "\n\ndef unsubscribe(self, subscriber, timeout=None):\n    \"Must be called with 'yield' as, for example,\\n        'yield channel.unsubscribe(coro)'.\\n\\n        Future messages will not be delivered after unsubscribing.\\n\\n        Can also be used on remote channels.\\n        \"\n    if ((not isinstance(subscriber, Coro)) and (not isinstance(subscriber, Channel))):\n        logger.warning('invalid subscriber ignored')\n        raise StopIteration((- 1))\n    if (self._location == Channel._asyncoro._location):\n        if (subscriber._location != self._location):\n            if isinstance(subscriber, Coro):\n                subscriber._id = int(subscriber._id)\n                for s in self._subscribers:\n                    if (isinstance(s, Coro) and (s._id == subscriber._id) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n            elif isinstance(subscriber, Channel):\n                for s in self._subscribers:\n                    if (isinstance(s, Channel) and (s._name == subscriber._name) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n        try:\n            self._subscribers.remove(subscriber)\n        except KeyError:\n            reply = (- 1)\n        else:\n            reply = 0\n    else:\n        kwargs = {\n            'channel': self._name,\n        }\n        kwargs['subscriber'] = subscriber\n        request = _NetRequest('unsubscribe', kwargs=kwargs, dst=self._location, timeout=timeout)\n        reply = (yield Channel._asyncoro._sync_reply(request))\n    raise StopIteration(reply)\n", "label": 1}
{"function": "\n\ndef dequeue(self, maxlen):\n    self.super = super(DataLinkConnection, self)\n    with self.lock:\n        if self.state.ESTABLISHED:\n            if (self.mode.RECV_BUSY_SENT != self.mode.RECV_BUSY):\n                self.mode.RECV_BUSY_SENT = self.mode.RECV_BUSY\n                Ack = (ReceiveReady, ReceiveNotReady)[self.mode.RECV_BUSY]\n                return Ack(self.peer, self.addr, self.recv_ack)\n        try:\n            pdu = self.super.dequeue(maxlen, notify=False)\n        except IndexError:\n            pdu = None\n        if pdu:\n            self.log('dequeue {0} PDU'.format(pdu.name))\n        if isinstance(pdu, FrameReject):\n            self.state.SHUTDOWN = True\n            self.close()\n        elif (isinstance(pdu, Information) and self.state.ESTABLISHED):\n            if (self.recv_confs and (self.recv_cnt != self.recv_ack)):\n                self.log(('piggyback ack ' + str(self)))\n                self.recv_ack = ((self.recv_ack + self.recv_confs) % 16)\n                self.recv_confs = 0\n            pdu.nr = self.recv_ack\n            self.send_ready.notify()\n        elif (isinstance(pdu, DisconnectedMode) and self.state.CLOSE_WAIT):\n            dm = Disconnect(dsap=self.peer, ssap=self.addr)\n            self.recv_queue.append(dm)\n            self.recv_ready.notify()\n            self.send_token.notify_all()\n        elif ((pdu is None) and self.state.ESTABLISHED):\n            if (self.recv_confs and (maxlen >= 3) and (self.recv_window_slots == 0)):\n                self.log(('necessary ack ' + str(self)))\n                self.recv_ack = ((self.recv_ack + self.recv_confs) % 16)\n                self.recv_confs = 0\n                Ack = (ReceiveReady, ReceiveNotReady)[self.mode.RECV_BUSY]\n                pdu = Ack(self.peer, self.addr, self.recv_ack)\n        return pdu\n", "label": 1}
{"function": "\n\ndef ui_command_create(self, storage_object, lun=None, add_mapped_luns=None):\n    '\\n        Creates a new LUN in the Target Portal Group, attached to a storage\\n        object. If the I{lun} parameter is omitted, the first available LUN in\\n        the TPG will be used. If present, it must be a number greater than 0.\\n        Alternatively, the syntax I{lunX} where I{X} is a positive number is\\n        also accepted.\\n\\n        The I{storage_object} may be the path of an existing storage object,\\n        i.e. B{/backstore/pscsi0/mydisk} to reference the B{mydisk} storage\\n        object of the virtual HBA B{pscsi0}. It also may be the path to an\\n        existing block device or image file, in which case a storage object\\n        will be created for it first, with default parameters.\\n\\n        If I{add_mapped_luns} is omitted, the global parameter\\n        B{auto_add_mapped_luns} will be used, else B{true} or B{false} are\\n        accepted. If B{true}, then after creating the LUN, mapped LUNs will be\\n        automatically created for all existing node ACLs, mapping the new LUN.\\n\\n        SEE ALSO\\n        ========\\n        B{delete}\\n        '\n    self.assert_root()\n    add_mapped_luns = self.ui_eval_param(add_mapped_luns, 'bool', self.shell.prefs['auto_add_mapped_luns'])\n    try:\n        so = self.get_node(storage_object).rtsnode\n    except ValueError:\n        try:\n            so = StorageObjectFactory(storage_object)\n            self.shell.log.info(('Created storage object %s.' % so.name))\n        except RTSLibError:\n            raise ExecutionError('storage object or path not valid')\n        self.get_node('/backstores').refresh()\n    if (so in (l.storage_object for l in self.parent.rtsnode.luns)):\n        raise ExecutionError(('lun for storage object %s/%s already exists' % (so.plugin, so.name)))\n    if (lun and lun.lower().startswith('lun')):\n        lun = lun[3:]\n    lun_object = LUN(self.tpg, lun, so)\n    self.shell.log.info(('Created LUN %s.' % lun_object.lun))\n    ui_lun = UILUN(lun_object, self)\n    if add_mapped_luns:\n        for acl in self.tpg.node_acls:\n            if lun:\n                mapped_lun = lun\n            else:\n                mapped_lun = 0\n            existing_mluns = [mlun.mapped_lun for mlun in acl.mapped_luns]\n            if (mapped_lun in existing_mluns):\n                mapped_lun = None\n                for possible_mlun in six.moves.range(LUN.MAX_LUN):\n                    if (possible_mlun not in existing_mluns):\n                        mapped_lun = possible_mlun\n                        break\n            if (mapped_lun == None):\n                self.shell.log.warning(('Cannot map new lun %s into ACL %s' % (lun_object.lun, acl.node_wwn)))\n            else:\n                mlun = MappedLUN(acl, mapped_lun, lun_object, write_protect=False)\n                self.shell.log.info(('Created LUN %d->%d mapping in node ACL %s' % (mlun.tpg_lun.lun, mlun.mapped_lun, acl.node_wwn)))\n        self.parent.refresh()\n    return self.new_node(ui_lun)\n", "label": 1}
{"function": "\n\ndef predictive_columns(M_c, X_L, X_D, columns_list, optional_settings=False, seed=0):\n    ' Generates rows of data from the inferred distributions\\n\\tInputs:\\n\\t\\t- M_c: crosscat metadata (See documentation)\\n\\t\\t- X_L: crosscat metadata (See documentation)\\n\\t\\t- X_D: crosscat metadata (See documentation)\\n\\t\\t- columns_list: a list of columns to sample\\n\\t\\t- optinal_settings: list of dicts of optional arguments. Each column\\n\\t\\t  in columns_list should have its own list entry which is either None\\n\\t\\t  or a dict with possible keys:\\n\\t\\t\\t- missing_data: Proportion missing data\\n\\tReturns:\\n\\t\\t- a num_rows by len(columns_list) numpy array, where n_rows is the\\n\\t\\toriginal number of rows in the crosscat table. \\n\\t'\n    supported_arguments = ['missing_data']\n    num_rows = len(X_D[0])\n    num_cols = len(M_c['column_metadata'])\n    if (not isinstance(columns_list, list)):\n        raise TypeError('columns_list should be a list')\n    for col in columns_list:\n        if (not isinstance(col, int)):\n            raise TypeError('every entry in columns_list shuold be an integer')\n        if ((col < 0) or (col >= num_cols)):\n            raise ValueError(('%i is not a valid column. Should be valid entries\\t\\t\\t are 0-%i' % (col, num_cols)))\n    if (not isinstance(seed, int)):\n        raise TypeError('seed should be an int')\n    if (seed < 0):\n        raise ValueError('seed should be positive')\n    if optional_settings:\n        if (not isinstance(optional_settings, list)):\n            raise TypeError('optional_settings should be a list')\n        for col_setting in optional_settings:\n            if isinstance(col_setting, dict):\n                for (key, value) in six.iteritems(col_setting):\n                    if (key not in supported_arguments):\n                        raise KeyError((\"Invalid key in optional_settings, '%s'\" % key))\n    else:\n        optional_settings = ([None] * len(columns_list))\n    random.seed(seed)\n    X = numpy.zeros((num_rows, len(columns_list)))\n    get_next_seed = (lambda : random.randrange(2147483647))\n    for c in range(len(columns_list)):\n        col = columns_list[c]\n        for row in range(num_rows):\n            X[(row, c)] = su.simple_predictive_sample(M_c, X_L, X_D, [], [(row, col)], get_next_seed, n=1)[0][0]\n        if isinstance(optional_settings[c], dict):\n            if has_key(optional_settings[c], 'missing_data'):\n                proportion = optional_settings[c]['missing_data']\n                X = add_missing_data_to_column(X, c, proportion)\n    assert (X.shape[0] == num_rows)\n    assert (X.shape[1] == len(columns_list))\n    return X\n", "label": 1}
{"function": "\n\ndef spin_process(self, i, unique_metrics):\n    '\\n        Assign a bunch of metrics for a process to analyze.\\n        '\n    keys_per_processor = int(ceil((float(len(unique_metrics)) / float(settings.ANALYZER_PROCESSES))))\n    if (i == settings.ANALYZER_PROCESSES):\n        assigned_max = len(unique_metrics)\n    else:\n        assigned_max = (i * keys_per_processor)\n    assigned_min = (assigned_max - keys_per_processor)\n    assigned_keys = range(assigned_min, assigned_max)\n    assigned_metrics = [unique_metrics[index] for index in assigned_keys]\n    if (len(assigned_metrics) == 0):\n        return\n    raw_assigned = self.redis_conn.mget(assigned_metrics)\n    exceptions = defaultdict(int)\n    anomaly_breakdown = defaultdict(int)\n    for (i, metric_name) in enumerate(assigned_metrics):\n        self.check_if_parent_is_alive()\n        try:\n            raw_series = raw_assigned[i]\n            unpacker = Unpacker(use_list=False)\n            unpacker.feed(raw_series)\n            timeseries = list(unpacker)\n            (anomalous, ensemble, datapoint) = run_selected_algorithm(timeseries, metric_name)\n            if anomalous:\n                base_name = metric_name.replace(settings.FULL_NAMESPACE, '', 1)\n                metric = [datapoint, base_name]\n                self.anomalous_metrics.append(metric)\n                for (index, value) in enumerate(ensemble):\n                    if value:\n                        algorithm = settings.ALGORITHMS[index]\n                        anomaly_breakdown[algorithm] += 1\n        except TypeError:\n            exceptions['DeletedByRoomba'] += 1\n        except TooShort:\n            exceptions['TooShort'] += 1\n        except Stale:\n            exceptions['Stale'] += 1\n        except Boring:\n            exceptions['Boring'] += 1\n        except:\n            exceptions['Other'] += 1\n            logger.info(traceback.format_exc())\n    for (key, value) in anomaly_breakdown.items():\n        self.anomaly_breakdown_q.put((key, value))\n    for (key, value) in exceptions.items():\n        self.exceptions_q.put((key, value))\n", "label": 1}
{"function": "\n\ndef _find_all_versions(self, project_name):\n    'Find all available versions for project_name\\n\\n        This checks index_urls, find_links and dependency_links\\n        All versions found are returned\\n\\n        See _link_package_versions for details on which files are accepted\\n        '\n    index_locations = self._get_index_urls_locations(project_name)\n    (index_file_loc, index_url_loc) = self._sort_locations(index_locations)\n    (fl_file_loc, fl_url_loc) = self._sort_locations(self.find_links, expand_dir=True)\n    (dep_file_loc, dep_url_loc) = self._sort_locations(self.dependency_links)\n    file_locations = (Link(url) for url in itertools.chain(index_file_loc, fl_file_loc, dep_file_loc))\n    url_locations = [link for link in itertools.chain((Link(url, trusted=True) for url in index_url_loc), (Link(url, trusted=True) for url in fl_url_loc), (Link(url) for url in dep_url_loc)) if self._validate_secure_origin(logger, link)]\n    logger.debug('%d location(s) to search for versions of %s:', len(url_locations), project_name)\n    for location in url_locations:\n        logger.debug('* %s', location)\n    canonical_name = pkg_resources.safe_name(project_name).lower()\n    formats = fmt_ctl_formats(self.format_control, canonical_name)\n    search = Search(project_name.lower(), canonical_name, formats)\n    find_links_versions = self._package_versions((Link(url, '-f', trusted=True) for url in self.find_links), search)\n    page_versions = []\n    for page in self._get_pages(url_locations, project_name):\n        logger.debug('Analyzing links from page %s', page.url)\n        with indent_log():\n            page_versions.extend(self._package_versions(page.links, search))\n    dependency_versions = self._package_versions((Link(url) for url in self.dependency_links), search)\n    if dependency_versions:\n        logger.debug('dependency_links found: %s', ', '.join([version.location.url for version in dependency_versions]))\n    file_versions = self._package_versions(file_locations, search)\n    if file_versions:\n        file_versions.sort(reverse=True)\n        logger.debug('Local files found: %s', ', '.join([url_to_path(candidate.location.url) for candidate in file_versions]))\n    return (((file_versions + find_links_versions) + page_versions) + dependency_versions)\n", "label": 1}
{"function": "\n\ndef token(self, restrict, context=None):\n    'Scan for another token.'\n    while 1:\n        if self.stack:\n            try:\n                return self.stack.token(restrict, context)\n            except StopIteration:\n                self.stack = None\n        self.grab_input()\n        if (self.stacked and (self.pos == len(self.input))):\n            raise StopIteration\n        best_match = (- 1)\n        best_pat = '(error)'\n        best_m = None\n        for (p, regexp) in self.patterns:\n            if (restrict and (p not in restrict) and (p not in self.ignore)):\n                continue\n            m = regexp.match(self.input, self.pos)\n            if (m and ((m.end() - m.start()) > best_match)):\n                best_pat = p\n                best_match = (m.end() - m.start())\n                best_m = m\n        if ((best_pat == '(error)') and (best_match < 0)):\n            msg = 'Bad Token'\n            if restrict:\n                msg = ('Trying to find one of ' + ', '.join(restrict))\n            raise SyntaxError(self.get_pos(), msg, context=context)\n        ignore = (best_pat in self.ignore)\n        value = self.input[self.pos:(self.pos + best_match)]\n        if (not ignore):\n            tok = Token(type=best_pat, value=value, pos=self.get_pos())\n        self.pos += best_match\n        npos = value.rfind('\\n')\n        if (npos > (- 1)):\n            self.col = (best_match - npos)\n            self.line += value.count('\\n')\n        else:\n            self.col += best_match\n        if (not ignore):\n            if (len(self.tokens) >= 10):\n                del self.tokens[0]\n            self.tokens.append(tok)\n            self.last_read_token = tok\n            return tok\n        else:\n            ignore = self.ignore[best_pat]\n            if ignore:\n                ignore(self, best_m)\n", "label": 1}
{"function": "\n\ndef albums(self, lib, query, move, pretend, write):\n    'Retrieve and apply info from the autotagger for albums matched by\\n        query and their items.\\n        '\n    for a in lib.albums(query):\n        album_formatted = format(a)\n        if (not a.mb_albumid):\n            self._log.info('Skipping album with no mb_albumid: {0}', album_formatted)\n            continue\n        items = list(a.items())\n        album_info = hooks.album_for_mbid(a.mb_albumid)\n        if (not album_info):\n            self._log.info('Release ID {0} not found for album {1}', a.mb_albumid, album_formatted)\n            continue\n        track_index = defaultdict(list)\n        for track_info in album_info.tracks:\n            track_index[track_info.track_id].append(track_info)\n        mapping = {\n            \n        }\n        for item in items:\n            candidates = track_index[item.mb_trackid]\n            if (len(candidates) == 1):\n                mapping[item] = candidates[0]\n            else:\n                for c in candidates:\n                    if ((c.medium_index == item.track) and (c.medium == item.disc)):\n                        mapping[item] = c\n                        break\n        self._log.debug('applying changes to {}', album_formatted)\n        with lib.transaction():\n            autotag.apply_metadata(album_info, mapping)\n            changed = False\n            for item in items:\n                item_changed = ui.show_model_changes(item)\n                changed |= item_changed\n                if item_changed:\n                    apply_item_changes(lib, item, move, pretend, write)\n            if (not changed):\n                continue\n            if (not pretend):\n                for key in library.Album.item_keys:\n                    a[key] = items[0][key]\n                a.store()\n                if (move and (lib.directory in util.ancestry(items[0].path))):\n                    self._log.debug('moving album {0}', album_formatted)\n                    a.move()\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    foo = 'aaa\\naaa\\naaa\\n'\n    result = list(chunked(foo, 5))\n    assert (len(result) == 3)\n    assert (result[0] == 'aaa\\n')\n    assert (result[1] == 'aaa\\n')\n    assert (result[2] == 'aaa\\n')\n    result = list(chunked(foo, 8))\n    assert (len(result) == 2)\n    assert (result[0] == 'aaa\\naaa\\n')\n    assert (result[1] == 'aaa\\n')\n    result = list(chunked(foo, 4))\n    assert (len(result) == 3)\n    assert (result[0] == 'aaa\\n')\n    assert (result[1] == 'aaa\\n')\n    assert (result[2] == 'aaa\\n')\n    foo = ('a' * 10)\n    result = list(chunked(foo, 2))\n    assert (len(result) == 5)\n    assert all(((r == 'aa') for r in result))\n    foo = 'aaaa\\naaaa'\n    result = list(chunked(foo, 3))\n    assert (len(result) == 4)\n", "label": 1}
{"function": "\n\ndef loadConfig(path):\n    c = Configuration()\n    if (not os.access(path, os.R_OK)):\n        return c\n    p = ConfigParser()\n    p.read(path)\n    au = c.allowUIDs\n    du = c.denyUIDs\n    ag = c.allowGIDs\n    dg = c.denyGIDs\n    for (section, a, d) in (('useraccess', au, du), ('groupaccess', ag, dg)):\n        if p.has_section(section):\n            for (mode, L) in (('allow', a), ('deny', d)):\n                if (p.has_option(section, mode) and p.get(section, mode)):\n                    for id in p.get(section, mode).split(','):\n                        try:\n                            id = int(id)\n                        except ValueError:\n                            log('Illegal %sID in [%s] section: %s', section[0].upper(), section, id)\n                        else:\n                            L.append(id)\n            order = p.get(section, 'order')\n            order = map(str.split, map(str.lower, order.split(',')))\n            if (order[0] == 'allow'):\n                setattr(c, section, 'allow')\n            else:\n                setattr(c, section, 'deny')\n    if p.has_section('identity'):\n        for (host, up) in p.items('identity'):\n            parts = up.split(':', 1)\n            if (len(parts) != 2):\n                log('Illegal entry in [identity] section: %s', up)\n                continue\n            p.identities[host] = parts\n    if p.has_section('addresses'):\n        if p.has_option('addresses', 'smarthost'):\n            c.smarthost = p.get('addresses', 'smarthost')\n        if p.has_option('addresses', 'default_domain'):\n            c.domain = p.get('addresses', 'default_domain')\n    return c\n", "label": 1}
{"function": "\n\ndef bind_sockets(port, address=None, family=socket.AF_UNSPEC, backlog=_DEFAULT_BACKLOG, flags=None):\n    \"Creates listening sockets bound to the given port and address.\\n\\n    Returns a list of socket objects (multiple sockets are returned if\\n    the given address maps to multiple IP addresses, which is most common\\n    for mixed IPv4 and IPv6 use).\\n\\n    Address may be either an IP address or hostname.  If it's a hostname,\\n    the server will listen on all IP addresses associated with the\\n    name.  Address may be an empty string or None to listen on all\\n    available interfaces.  Family may be set to either `socket.AF_INET`\\n    or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise\\n    both will be used if available.\\n\\n    The ``backlog`` argument has the same meaning as for\\n    `socket.listen() <socket.socket.listen>`.\\n\\n    ``flags`` is a bitmask of AI_* flags to `~socket.getaddrinfo`, like\\n    ``socket.AI_PASSIVE | socket.AI_NUMERICHOST``.\\n    \"\n    sockets = []\n    if (address == ''):\n        address = None\n    if ((not socket.has_ipv6) and (family == socket.AF_UNSPEC)):\n        family = socket.AF_INET\n    if (flags is None):\n        flags = socket.AI_PASSIVE\n    bound_port = None\n    for res in set(socket.getaddrinfo(address, port, family, socket.SOCK_STREAM, 0, flags)):\n        (af, socktype, proto, canonname, sockaddr) = res\n        if ((platform.system() == 'Darwin') and (address == 'localhost') and (af == socket.AF_INET6) and (sockaddr[3] != 0)):\n            continue\n        try:\n            sock = socket.socket(af, socktype, proto)\n        except socket.error as e:\n            if (errno_from_exception(e) == errno.EAFNOSUPPORT):\n                continue\n            raise\n        set_close_exec(sock.fileno())\n        if (os.name != 'nt'):\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        if (af == socket.AF_INET6):\n            if hasattr(socket, 'IPPROTO_IPV6'):\n                sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)\n        (host, requested_port) = sockaddr[:2]\n        if ((requested_port == 0) and (bound_port is not None)):\n            sockaddr = tuple(([host, bound_port] + list(sockaddr[2:])))\n        sock.setblocking(0)\n        sock.bind(sockaddr)\n        bound_port = sock.getsockname()[1]\n        sock.listen(backlog)\n        sockets.append(sock)\n    return sockets\n", "label": 1}
{"function": "\n\ndef process_dirs_to_atlas():\n    for dirname in [d for d in os.listdir('.') if (os.path.isdir(d) and (not (d in excluded_dirs)))]:\n        filenames = glob.glob(os.path.join(dirname, '*.png'))\n        dest_filename = os.path.join(out_path, (dirname + '.tex'))\n        xml_filename = os.path.join(out_path, (dirname + '.xml'))\n        if (util.IsAnyFileNewer(filenames, [dest_filename, xml_filename]) or args.force):\n            print(('Processing ' + dirname))\n            working_dir = os.path.join(tempfile.gettempdir(), dirname)\n            try:\n                if os.path.exists(working_dir):\n                    shutil.rmtree(working_dir)\n                os.makedirs(working_dir)\n                size = None\n                border_size = 0\n                if (dirname in ImageProperties):\n                    width = ImageProperties[dirname].get('width', None)\n                    height = ImageProperties[dirname].get('height', None)\n                    border_size = ImageProperties[dirname].get('border', 0)\n                    if ((width != None) and (height != None)):\n                        size = (width, height)\n                images = []\n                for filename in filenames:\n                    im = imgutil.OpenImage(filename, size=size, border_size=border_size)\n                    (filename, ext) = os.path.splitext(os.path.basename(filename))\n                    im.name = (filename + '.tex')\n                    images.append(im)\n                atlas_data = atlas.Atlas(images, dirname, minimize_num_textures=True, ignore_exceptions=args.ignoreexceptions, force_square=args.square)\n                assert (len(atlas_data) == 1)\n                page = atlas_data[0]\n                mip_filenames = []\n                for mip_idx in range(len(page.mips)):\n                    mip = page.mips[mip_idx]\n                    mip_filename = (mip.name + '-mip{}.png'.format(mip_idx))\n                    full_mip_filename = os.path.join(working_dir, mip_filename)\n                    mip_filenames.append(full_mip_filename)\n                    mip.im.save(full_mip_filename)\n                textureconverter.Convert(src_filenames=mip_filenames, dest_filename=dest_filename, platform=args.platform, texture_format=args.textureformat, force=args.force, ignore_exceptions=args.ignoreexceptions)\n                tex_filename = os.path.relpath(dest_filename, ROOT).replace('\\\\', '/')\n                xml = atlas.GenerateXMLTree(tex_filename, page.mips[0].im.size, page.bboxes, (border_size + 0.5))\n                xml.write(xml_filename)\n            finally:\n                if os.path.exists(working_dir):\n                    shutil.rmtree(working_dir)\n", "label": 1}
{"function": "\n\ndef interpolate(agg, low=None, high=None, cmap=None, how='eq_hist'):\n    'Convert a 2D DataArray to an image.\\n\\n    Data is converted to an image either by interpolating between a `low` and\\n    `high` color [default], or by a specified colormap.\\n\\n    Parameters\\n    ----------\\n    agg : DataArray\\n    low, high : color name or tuple, optional\\n        Deprecated. The color for the low and high ends of the scale. Can be\\n        specified either by name, hexcode, or as a tuple of ``(red, green,\\n        blue)`` values.\\n    cmap : list of colors or matplotlib.colors.Colormap, optional\\n        The colormap to use. Can be either a list of colors (in any of the\\n        formats described above), or a matplotlib colormap object.\\n        Default is `[\"lightblue\", \"darkblue\"]`\\n    how : str or callable, optional\\n        The interpolation method to use. Valid strings are \\'cbrt\\' [default],\\n        \\'log\\', \\'linear\\', and \\'eq_hist\\'. Callables take 2 arguments - a\\n        2-dimensional array of magnitudes at each pixel, and a boolean mask\\n        array indicating missingness. They should return a numeric array of the\\n        same shape, with `NaN`s where the mask was True.\\n    '\n    if (not isinstance(agg, xr.DataArray)):\n        raise TypeError('agg must be instance of DataArray')\n    if (agg.ndim != 2):\n        raise ValueError('agg must be 2D')\n    if (cmap is not None):\n        if (low or high):\n            raise ValueError(\"Can't provide both `cmap` and `low` or `high`\")\n    else:\n        cmap = ['lightblue', 'darkblue']\n        if (low or high):\n            with warnings.catch_warnings():\n                warnings.simplefilter('always', DeprecationWarning)\n                w = DeprecationWarning('Using `low` and `high` is deprecated. Instead use `cmap=[low, high]`')\n                warnings.warn(w)\n            cmap = [(low or cmap[0]), (high or cmap[1])]\n    how = _normalize_interpolate_how(how)\n    data = agg.data\n    if np.issubdtype(data.dtype, np.bool_):\n        mask = (~ data)\n        interp = data\n    else:\n        if np.issubdtype(data.dtype, np.integer):\n            mask = (data == 0)\n        else:\n            mask = np.isnan(data)\n        offset = data[(~ mask)].min()\n        interp = (data - offset)\n    data = how(interp, mask)\n    span = [np.nanmin(data), np.nanmax(data)]\n    if isinstance(cmap, Iterator):\n        cmap = list(cmap)\n    if isinstance(cmap, list):\n        (rspan, gspan, bspan) = np.array(list(zip(*map(rgb, cmap))))\n        span = np.linspace(span[0], span[1], len(cmap))\n        r = np.interp(data, span, rspan, left=255).astype(np.uint8)\n        g = np.interp(data, span, gspan, left=255).astype(np.uint8)\n        b = np.interp(data, span, bspan, left=255).astype(np.uint8)\n        a = np.where(np.isnan(data), 0, 255).astype(np.uint8)\n        img = np.dstack([r, g, b, a]).view(np.uint32).reshape(a.shape)\n    elif callable(cmap):\n        img = cmap(((data - span[0]) / (span[1] - span[0])), bytes=True)\n        img[:, :, 3] = np.where(np.isnan(data), 0, 255).astype(np.uint8)\n        img = img.view(np.uint32).reshape(data.shape)\n    else:\n        raise TypeError(\"Expected `cmap` of `matplotlib.colors.Colormap` or `list`, got: '{0}'\".format(type(cmap)))\n    return Image(img, coords=agg.coords, dims=agg.dims)\n", "label": 1}
{"function": "\n\ndef read(self, size=None, fp_out=None):\n    \"Read bytes from the request body and return or write them to a file.\\n\\n        A number of bytes less than or equal to the 'size' argument are read\\n        off the socket. The actual number of bytes read are tracked in\\n        self.bytes_read. The number may be smaller than 'size' when 1) the\\n        client sends fewer bytes, 2) the 'Content-Length' request header\\n        specifies fewer bytes than requested, or 3) the number of bytes read\\n        exceeds self.maxbytes (in which case, 413 is raised).\\n\\n        If the 'fp_out' argument is None (the default), all bytes read are\\n        returned in a single byte string.\\n\\n        If the 'fp_out' argument is not None, it must be a file-like object that\\n        supports the 'write' method; all bytes read will be written to the fp,\\n        and None is returned.\\n        \"\n    if (self.length is None):\n        if (size is None):\n            remaining = inf\n        else:\n            remaining = size\n    else:\n        remaining = (self.length - self.bytes_read)\n        if (size and (size < remaining)):\n            remaining = size\n    if (remaining == 0):\n        self.finish()\n        if (fp_out is None):\n            return ntob('')\n        else:\n            return None\n    chunks = []\n    if self.buffer:\n        if (remaining is inf):\n            data = self.buffer\n            self.buffer = ntob('')\n        else:\n            data = self.buffer[:remaining]\n            self.buffer = self.buffer[remaining:]\n        datalen = len(data)\n        remaining -= datalen\n        self.bytes_read += datalen\n        if (self.maxbytes and (self.bytes_read > self.maxbytes)):\n            raise cherrypy.HTTPError(413)\n        if (fp_out is None):\n            chunks.append(data)\n        else:\n            fp_out.write(data)\n    while (remaining > 0):\n        chunksize = min(remaining, self.bufsize)\n        try:\n            data = self.fp.read(chunksize)\n        except Exception:\n            e = sys.exc_info()[1]\n            if (e.__class__.__name__ == 'MaxSizeExceeded'):\n                raise cherrypy.HTTPError(413, ('Maximum request length: %r' % e.args[1]))\n            else:\n                raise\n        if (not data):\n            self.finish()\n            break\n        datalen = len(data)\n        remaining -= datalen\n        self.bytes_read += datalen\n        if (self.maxbytes and (self.bytes_read > self.maxbytes)):\n            raise cherrypy.HTTPError(413)\n        if (fp_out is None):\n            chunks.append(data)\n        else:\n            fp_out.write(data)\n    if (fp_out is None):\n        return ntob('').join(chunks)\n", "label": 1}
{"function": "\n\ndef assetstore(self, name, type, root=None, db=None, mongohost=None, replicaset='', bucket=None, prefix='', accessKeyId=None, secret=None, service='s3.amazonaws.com', host=None, port=None, path=None, user=None, webHdfsPort=None, readOnly=False, current=False):\n    if (type not in self.assetstore_types.keys()):\n        self.fail('assetstore type {} is not implemented!'.format(type))\n    argument_hash = {\n        'filesystem': {\n            'name': name,\n            'type': self.assetstore_types[type],\n            'root': root,\n        },\n        'gridfs': {\n            'name': name,\n            'type': self.assetstore_types[type],\n            'db': db,\n            'mongohost': mongohost,\n            'replicaset': replicaset,\n        },\n        's3': {\n            'name': name,\n            'type': self.assetstore_types[type],\n            'bucket': bucket,\n            'prefix': prefix,\n            'accessKeyId': accessKeyId,\n            'secret': secret,\n            'service': service,\n        },\n        'hdfs': {\n            'name': name,\n            'type': self.assetstore_types[type],\n            'host': host,\n            'port': port,\n            'path': path,\n            'user': user,\n            'webHdfsPort': webHdfsPort,\n        },\n    }\n    for (k, v) in argument_hash[type].items():\n        if (v is None):\n            self.fail('assetstores of type {} require attribute {}'.format(type, k))\n    argument_hash[type]['readOnly'] = readOnly\n    argument_hash[type]['current'] = current\n    ret = []\n    assetstores = {a['name']: a for a in self.get('assetstore')}\n    self.message['debug']['assetstores'] = assetstores\n    if (self.module.params['state'] == 'present'):\n        if (name in assetstores.keys()):\n            id = assetstores[name]['_id']\n            updateable = ['root', 'mongohost', 'replicaset', 'bucket', 'prefix', 'db', 'accessKeyId', 'secret', 'service', 'host', 'port', 'path', 'user', 'webHdfsPort', 'current']\n            assetstore_items = set(((k, assetstores[name][k]) for k in updateable if (k in assetstores[name].keys())))\n            arg_hash_items = set(((k, argument_hash[type][k]) for k in updateable if (k in argument_hash[type].keys())))\n            if (not (arg_hash_items <= assetstore_items)):\n                ret = self.put('assetstore/{}'.format(id), parameters=argument_hash[type])\n                self.changed = True\n        else:\n            try:\n                getattr(self, '__validate_{}_assetstore'.format(type))(**argument_hash)\n            except AttributeError:\n                pass\n            ret = self.post('assetstore', parameters=argument_hash[type])\n            self.changed = True\n    elif (self.module.params['state'] == 'absent'):\n        if (name in assetstores.keys()):\n            id = assetstores[name]['_id']\n            ret = self.delete('assetstore/{}'.format(id), parameters=argument_hash[type])\n    return ret\n", "label": 1}
{"function": "\n\ndef __parse_number(self):\n    'Parse a numeric literal.'\n    literal_buffer = []\n    all_digits = True\n    while (not self.__scanner.at_end):\n        peek = self.__scanner.peek_next_ubyte()\n        if ((peek != '+') and (peek != '-') and (peek != 'e') and (peek != 'E') and (peek != '.') and (not ('0' <= peek <= '9'))):\n            break\n        if (len(literal_buffer) >= 100):\n            self.__error('numeric literal too long (limit 100 characters)')\n        next_char = self.__scanner.read_ubyte()\n        all_digits = (all_digits and ('0' <= next_char <= '9'))\n        literal_buffer.append(next_char)\n    if (all_digits and (len(literal_buffer) <= 18)):\n        value = 0\n        for digit in literal_buffer:\n            value = (((value * 10) + ord(digit)) - ord('0'))\n        return value\n    number_string = ''.join(literal_buffer)\n    if ((number_string.find('.') < 0) and (number_string.find('e') < 0) and (number_string.find('E') < 0)):\n        try:\n            return long(number_string)\n        except ValueError:\n            self.__error((\"Could not parse number as long '%s'\" % number_string))\n    else:\n        try:\n            return float(number_string)\n        except ValueError:\n            self.__error((\"Could not parse number as float '%s'\" % number_string))\n", "label": 1}
{"function": "\n\ndef read_lines_to_boundary(self, fp_out=None):\n    \"Read bytes from self.fp and return or write them to a file.\\n\\n        If the 'fp_out' argument is None (the default), all bytes read are\\n        returned in a single byte string.\\n\\n        If the 'fp_out' argument is not None, it must be a file-like object that\\n        supports the 'write' method; all bytes read will be written to the fp,\\n        and that fp is returned.\\n        \"\n    endmarker = (self.boundary + ntob('--'))\n    delim = ntob('')\n    prev_lf = True\n    lines = []\n    seen = 0\n    while True:\n        line = self.fp.readline((1 << 16))\n        if (not line):\n            raise EOFError('Illegal end of multipart body.')\n        if (line.startswith(ntob('--')) and prev_lf):\n            strippedline = line.strip()\n            if (strippedline == self.boundary):\n                break\n            if (strippedline == endmarker):\n                self.fp.finish()\n                break\n        line = (delim + line)\n        if line.endswith(ntob('\\r\\n')):\n            delim = ntob('\\r\\n')\n            line = line[:(- 2)]\n            prev_lf = True\n        elif line.endswith(ntob('\\n')):\n            delim = ntob('\\n')\n            line = line[:(- 1)]\n            prev_lf = True\n        else:\n            delim = ntob('')\n            prev_lf = False\n        if (fp_out is None):\n            lines.append(line)\n            seen += len(line)\n            if (seen > self.maxrambytes):\n                fp_out = self.make_file()\n                for line in lines:\n                    fp_out.write(line)\n        else:\n            fp_out.write(line)\n    if (fp_out is None):\n        result = ntob('').join(lines)\n        for charset in self.attempt_charsets:\n            try:\n                result = result.decode(charset)\n            except UnicodeDecodeError:\n                pass\n            else:\n                self.charset = charset\n                return result\n        else:\n            raise cherrypy.HTTPError(400, ('The request entity could not be decoded. The following charsets were attempted: %s' % repr(self.attempt_charsets)))\n    else:\n        fp_out.seek(0)\n        return fp_out\n", "label": 1}
{"function": "\n\ndef localize(self, dt, is_dst=False):\n    \"Convert naive time to local time.\\n\\n        This method should be used to construct localtimes, rather\\n        than passing a tzinfo argument to a datetime constructor.\\n\\n        is_dst is used to determine the correct timezone in the ambigous\\n        period at the end of daylight saving time.\\n\\n        >>> from pytz import timezone\\n        >>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'\\n        >>> amdam = timezone('Europe/Amsterdam')\\n        >>> dt  = datetime(2004, 10, 31, 2, 0, 0)\\n        >>> loc_dt1 = amdam.localize(dt, is_dst=True)\\n        >>> loc_dt2 = amdam.localize(dt, is_dst=False)\\n        >>> loc_dt1.strftime(fmt)\\n        '2004-10-31 02:00:00 CEST (+0200)'\\n        >>> loc_dt2.strftime(fmt)\\n        '2004-10-31 02:00:00 CET (+0100)'\\n        >>> str(loc_dt2 - loc_dt1)\\n        '1:00:00'\\n\\n        Use is_dst=None to raise an AmbiguousTimeError for ambiguous\\n        times at the end of daylight saving time\\n\\n        >>> try:\\n        ...     loc_dt1 = amdam.localize(dt, is_dst=None)\\n        ... except AmbiguousTimeError:\\n        ...     print('Ambiguous')\\n        Ambiguous\\n\\n        is_dst defaults to False\\n\\n        >>> amdam.localize(dt) == amdam.localize(dt, False)\\n        True\\n\\n        is_dst is also used to determine the correct timezone in the\\n        wallclock times jumped over at the start of daylight saving time.\\n\\n        >>> pacific = timezone('US/Pacific')\\n        >>> dt = datetime(2008, 3, 9, 2, 0, 0)\\n        >>> ploc_dt1 = pacific.localize(dt, is_dst=True)\\n        >>> ploc_dt2 = pacific.localize(dt, is_dst=False)\\n        >>> ploc_dt1.strftime(fmt)\\n        '2008-03-09 02:00:00 PDT (-0700)'\\n        >>> ploc_dt2.strftime(fmt)\\n        '2008-03-09 02:00:00 PST (-0800)'\\n        >>> str(ploc_dt2 - ploc_dt1)\\n        '1:00:00'\\n\\n        Use is_dst=None to raise a NonExistentTimeError for these skipped\\n        times.\\n\\n        >>> try:\\n        ...     loc_dt1 = pacific.localize(dt, is_dst=None)\\n        ... except NonExistentTimeError:\\n        ...     print('Non-existent')\\n        Non-existent\\n        \"\n    if (dt.tzinfo is not None):\n        raise ValueError('Not naive datetime (tzinfo is already set)')\n    possible_loc_dt = set()\n    for delta in [timedelta(days=(- 1)), timedelta(days=1)]:\n        loc_dt = (dt + delta)\n        idx = max(0, (bisect_right(self._utc_transition_times, loc_dt) - 1))\n        inf = self._transition_info[idx]\n        tzinfo = self._tzinfos[inf]\n        loc_dt = tzinfo.normalize(dt.replace(tzinfo=tzinfo))\n        if (loc_dt.replace(tzinfo=None) == dt):\n            possible_loc_dt.add(loc_dt)\n    if (len(possible_loc_dt) == 1):\n        return possible_loc_dt.pop()\n    if (len(possible_loc_dt) == 0):\n        if (is_dst is None):\n            raise NonExistentTimeError(dt)\n        elif is_dst:\n            return (self.localize((dt + timedelta(hours=6)), is_dst=True) - timedelta(hours=6))\n        else:\n            return (self.localize((dt - timedelta(hours=6)), is_dst=False) + timedelta(hours=6))\n    if (is_dst is None):\n        raise AmbiguousTimeError(dt)\n    filtered_possible_loc_dt = [p for p in possible_loc_dt if (bool(p.tzinfo._dst) == is_dst)]\n    if (len(filtered_possible_loc_dt) == 1):\n        return filtered_possible_loc_dt[0]\n    if (len(filtered_possible_loc_dt) == 0):\n        filtered_possible_loc_dt = list(possible_loc_dt)\n    dates = {\n        \n    }\n    for local_dt in filtered_possible_loc_dt:\n        utc_time = (local_dt.replace(tzinfo=None) - local_dt.tzinfo._utcoffset)\n        assert (utc_time not in dates)\n        dates[utc_time] = local_dt\n    return dates[[min, max][(not is_dst)](dates)]\n", "label": 1}
{"function": "\n\ndef test_legend_structure():\n    df = get_test_df()\n    gg = ggplot(df, aes(xmin='xmin', xmax='xmax', ymin='ymin', ymax='ymax', colour='quality', fill='fill', alpha='alpha', linetype='texture'))\n    (new_df, legend) = assign_visual_mapping(df, gg.aesthetics, gg)\n    for aesthetic in ('color', 'fill', 'alpha', 'linetype'):\n        assert (aesthetic in legend)\n    assert ('size' not in legend)\n    assert ('shape' not in legend)\n    assert (legend['fill']['column_name'] == 'fill')\n    assert (legend['color']['column_name'] == 'quality')\n    assert (legend['linetype']['column_name'] == 'texture')\n    assert (legend['alpha']['column_name'] == 'alpha')\n    assert (legend['fill']['scale_type'] == 'discrete')\n    assert (legend['color']['scale_type'] == 'discrete')\n    assert (legend['linetype']['scale_type'] == 'discrete')\n    assert (legend['alpha']['scale_type'] == 'discrete')\n    df2 = pd.DataFrame.copy(df)\n    df2['fill'] = [90, 3.2, 8.1]\n    gg = ggplot(df2, aes(xmin='xmin', xmax='xmax', ymin='ymin', ymax='ymax', colour='quality', fill='fill', alpha='alpha', linetype='texture'))\n    (new_df, legend) = assign_visual_mapping(df2, gg.aesthetics, gg)\n    assert (legend['fill']['scale_type'] == 'discrete')\n    df3 = pd.DataFrame({\n        'xmin': [1, 3, 5, 8, 2, 1, 4, 7, 9],\n        'xmax': [2, 3.5, 7, 12, 3, 2, 6, 8, 10],\n        'ymin': [1, 4, 6, 0, 0, 0, 0, 0, 0],\n        'ymax': [5, 5, 9, 1, 1, 1, 1, 1, 1],\n        'fill': ['blue', 'red', 'green', 'green', 'green', 'green', 'green', 'green', 'brown'],\n        'quality': ['good', 'bad', 'ugly', 'horrible', 'quite awful', 'impertinent', 'jolly', 'hazardous', 'ok'],\n        'alpha': [0.1, 0.2, 0.4, 0.5, 0.6, 0.65, 0.8, 0.82, 0.83],\n        'texture': ['hard', 'soft', 'medium', 'fluffy', 'slimy', 'rough', 'edgy', 'corny', 'slanted'],\n    })\n    gg = ggplot(df2, aes(xmin='xmin', xmax='xmax', ymin='ymin', ymax='ymax', colour='quality', fill='fill', alpha='alpha', linetype='texture'))\n    (new_df, legend) = assign_visual_mapping(df3, gg.aesthetics, gg)\n    assert (legend['alpha']['scale_type'] == 'continuous')\n    gg = ggplot(df3, aes(size='fill'))\n    assert_raises(GgplotError, assign_visual_mapping, df3, gg.aesthetics, gg)\n    gg = ggplot(df3, aes(alpha='fill'))\n    assert_raises(GgplotError, assign_visual_mapping, df3, gg.aesthetics, gg)\n", "label": 1}
{"function": "\n\ndef handle_request(self, req, conn):\n    environ = {\n        \n    }\n    resp = None\n    try:\n        self.cfg.pre_request(self, req)\n        request_start = datetime.now()\n        (resp, environ) = wsgi.create(req, conn.sock, conn.addr, conn.listener.getsockname(), self.cfg)\n        environ['wsgi.multithread'] = True\n        self.nr += 1\n        if (self.alive and (self.nr >= self.max_requests)):\n            self.log.info('Autorestarting worker after current request.')\n            resp.force_close()\n            self.alive = False\n        if (not self.cfg.keepalive):\n            resp.force_close()\n        elif (len(self._keep) >= self.max_keepalived):\n            resp.force_close()\n        respiter = self.wsgi(environ, resp.start_response)\n        try:\n            if isinstance(respiter, environ['wsgi.file_wrapper']):\n                resp.write_file(respiter)\n            else:\n                for item in respiter:\n                    resp.write(item)\n            resp.close()\n            request_time = (datetime.now() - request_start)\n            self.log.access(resp, req, environ, request_time)\n        finally:\n            if hasattr(respiter, 'close'):\n                respiter.close()\n        if resp.should_close():\n            self.log.debug('Closing connection.')\n            return False\n    except socket.error:\n        exc_info = sys.exc_info()\n        six.reraise(exc_info[0], exc_info[1], exc_info[2])\n    except Exception:\n        if (resp and resp.headers_sent):\n            self.log.exception('Error handling request')\n            try:\n                conn.sock.shutdown(socket.SHUT_RDWR)\n                conn.sock.close()\n            except socket.error:\n                pass\n            raise StopIteration()\n        raise\n    finally:\n        try:\n            self.cfg.post_request(self, req, environ, resp)\n        except Exception:\n            self.log.exception('Exception in post_request hook')\n    return True\n", "label": 1}
{"function": "\n\ndef add_edge(self, start, end, **kwargs):\n    \"\\n        Add an edge between two nodes.\\n\\n        The nodes will be automatically added if they are not present in the network.\\n\\n        Parameters\\n        ----------\\n        start: tuple\\n               Both the start and end nodes should specify the time slice as\\n               (node_name, time_slice). Here, node_name can be any hashable\\n               python object while the time_slice is an integer value,\\n               which denotes the time slice that the node belongs to.\\n\\n        end: tuple\\n               Both the start and end nodes should specify the time slice as\\n               (node_name, time_slice). Here, node_name can be any hashable\\n               python object while the time_slice is an integer value,\\n               which denotes the time slice that the node belongs to.\\n\\n        Examples\\n        --------\\n        >>> from pgmpy.models import DynamicBayesianNetwork as DBN\\n        >>> model = DBN()\\n        >>> model.add_nodes_from(['D', 'I'])\\n        >>> model.add_edge(('D',0), ('I',0))\\n        >>> model.edges()\\n        [(('D', 1), ('I', 1)), (('D', 0), ('I', 0))]\\n        \"\n    try:\n        if ((len(start) != 2) or (len(end) != 2)):\n            raise ValueError('Nodes must be of type (node, time_slice).')\n        elif ((not isinstance(start[1], int)) or (not isinstance(end[1], int))):\n            raise ValueError('Nodes must be of type (node, time_slice).')\n        elif (start[1] == end[1]):\n            start = (start[0], 0)\n            end = (end[0], 0)\n        elif (start[1] == (end[1] - 1)):\n            start = (start[0], 0)\n            end = (end[0], 1)\n        elif (start[1] > end[1]):\n            raise NotImplementedError('Edges in backward direction are not allowed.')\n        elif (start[1] != end[1]):\n            raise ValueError('Edges over multiple time slices is not currently supported')\n    except TypeError:\n        raise ValueError('Nodes must be of type (node, time_slice).')\n    if (start == end):\n        raise ValueError('Self Loops are not allowed')\n    elif ((start in super(DynamicBayesianNetwork, self).nodes()) and (end in super(DynamicBayesianNetwork, self).nodes()) and nx.has_path(self, end, start)):\n        raise ValueError('Loops are not allowed. Adding the edge from ({start} --> {end}) forms a loop.'.format(start=str(start), end=str(end)))\n    super(DynamicBayesianNetwork, self).add_edge(start, end, **kwargs)\n    if (start[1] == end[1]):\n        super(DynamicBayesianNetwork, self).add_edge((start[0], (1 - start[1])), (end[0], (1 - end[1])))\n    else:\n        super(DynamicBayesianNetwork, self).add_node((end[0], (1 - end[1])))\n", "label": 1}
{"function": "\n\ndef _parse_conf(conf_file=default_conf):\n    \"\\n    Parse a logrotate configuration file.\\n\\n    Includes will also be parsed, and their configuration will be stored in the\\n    return dict, as if they were part of the main config file. A dict of which\\n    configs came from which includes will be stored in the 'include files' dict\\n    inside the return dict, for later reference by the user or module.\\n    \"\n    ret = {\n        \n    }\n    mode = 'single'\n    multi_names = []\n    multi = {\n        \n    }\n    prev_comps = None\n    with salt.utils.fopen(conf_file, 'r') as ifile:\n        for line in ifile:\n            line = line.strip()\n            if (not line):\n                continue\n            if line.startswith('#'):\n                continue\n            comps = line.split()\n            if (('{' in line) and ('}' not in line)):\n                mode = 'multi'\n                if ((len(comps) == 1) and prev_comps):\n                    multi_names = prev_comps\n                else:\n                    multi_names = comps\n                    multi_names.pop()\n                continue\n            if ('}' in line):\n                mode = 'single'\n                for multi_name in multi_names:\n                    ret[multi_name] = multi\n                multi_names = []\n                multi = {\n                    \n                }\n                continue\n            if (mode == 'single'):\n                key = ret\n            else:\n                key = multi\n            if (comps[0] == 'include'):\n                if ('include files' not in ret):\n                    ret['include files'] = {\n                        \n                    }\n                for include in os.listdir(comps[1]):\n                    if (include not in ret['include files']):\n                        ret['include files'][include] = []\n                    include_path = '{0}/{1}'.format(comps[1], include)\n                    include_conf = _parse_conf(include_path)\n                    for file_key in include_conf:\n                        ret[file_key] = include_conf[file_key]\n                        ret['include files'][include].append(file_key)\n            prev_comps = comps\n            if (len(comps) > 1):\n                key[comps[0]] = ' '.join(comps[1:])\n            else:\n                key[comps[0]] = True\n    return ret\n", "label": 1}
{"function": "\n\ndef job_logs(self, job, delay=5):\n    'Job log generator.\\n\\n    :param job: job name\\n    :param delay: time in seconds between each server poll\\n\\n    Yields line by line.\\n\\n    '\n    finishing = False\n    offset = 0\n    while True:\n        try:\n            logs = self._session.get_job_logs(exec_id=self.exec_id, job=job, offset=offset)\n        except HTTPError as err:\n            preparing = False\n            while True:\n                sleep(delay)\n                preparing_jobs = set((e['id'] for e in self.status['nodes'] if (e['status'] == 'PREPARING')))\n                if (job in preparing_jobs):\n                    if (not preparing):\n                        preparing = True\n                        _logger.debug('Job %s in execution %s is still preparing.', job, self.exec_id)\n                else:\n                    break\n            if (not preparing):\n                raise err\n        else:\n            if logs['length']:\n                offset += logs['length']\n                lines = (e for e in logs['data'].split('\\n') if e)\n                for line in lines:\n                    (yield line)\n            elif finishing:\n                break\n            else:\n                running_jobs = set((e['id'] for e in self.status['nodes'] if (e['status'] == 'RUNNING')))\n                if (job not in running_jobs):\n                    finishing = True\n        sleep(delay)\n", "label": 1}
{"function": "\n\ndef execute(self, data, length):\n    if (length == 0):\n        self.on_message_complete = True\n        return length\n    nb_parsed = 0\n    while True:\n        if (not self.__on_firstline):\n            idx = data.find(b('\\r\\n'))\n            if (idx < 0):\n                self._buf.append(data)\n                return len(data)\n            else:\n                self.__on_firstline = True\n                self._buf.append(data[:idx])\n                first_line = bytes_to_str(b('').join(self._buf))\n                nb_parsed = ((nb_parsed + idx) + 2)\n                rest = data[(idx + 2):]\n                data = b('')\n                if self._parse_firstline(first_line):\n                    self._buf = [rest]\n                else:\n                    return nb_parsed\n        elif (not self.__on_headers_complete):\n            if data:\n                self._buf.append(data)\n                data = b('')\n            try:\n                to_parse = b('').join(self._buf)\n                ret = self._parse_headers(to_parse)\n                if (not ret):\n                    return length\n                nb_parsed = (nb_parsed + (len(to_parse) - ret))\n            except InvalidHeader as e:\n                self.errno = INVALID_HEADER\n                self.errstr = str(e)\n                return nb_parsed\n        elif (not self.__on_message_complete):\n            if (not self.__on_message_begin):\n                self.__on_message_begin = True\n            if data:\n                self._buf.append(data)\n                data = b('')\n            ret = self._parse_body()\n            if (ret is None):\n                return length\n            elif (ret < 0):\n                return ret\n            elif (ret == 0):\n                self.__on_message_complete = True\n                return length\n            else:\n                nb_parsed = max(length, ret)\n        else:\n            return 0\n", "label": 1}
{"function": "\n\ndef stats_topic_data(bucket_days, start, end, locale=None, product=None):\n    'Gets a zero filled histogram for each question topic.\\n\\n    Uses elastic search.\\n    '\n    search = Sphilastic(QuestionMappingType)\n    bucket = (((24 * 60) * 60) * bucket_days)\n    if isinstance(start, date):\n        start = int(time.mktime(start.timetuple()))\n    if isinstance(end, date):\n        end = int(time.mktime(end.timetuple()))\n    f = F(model='questions_question')\n    f &= F(created__gt=start)\n    f &= F(created__lt=end)\n    if locale:\n        f &= F(question_locale=locale)\n    if product:\n        f &= F(product=product.slug)\n    topics = Topic.objects.values('slug', 'title')\n    facets = {\n        \n    }\n    for topic in topics:\n        filters = search._process_filters([(f & F(topic=topic['slug']))])\n        facets[topic['title']] = {\n            'histogram': {\n                'interval': bucket,\n                'field': 'created',\n            },\n            'facet_filter': filters,\n        }\n    search = search.facet_raw(**facets).values_dict()\n    try:\n        histograms_data = search.facet_counts()\n    except ES_EXCEPTIONS:\n        return []\n    for series in histograms_data.itervalues():\n        if series['entries']:\n            earliest_point = series['entries'][0]['key']\n            break\n    else:\n        return []\n    latest_point = earliest_point\n    interim_data = {\n        \n    }\n    for (key, data) in histograms_data.iteritems():\n        if (not data):\n            continue\n        for point in data:\n            timestamp = point['key']\n            value = point['count']\n            earliest_point = min(earliest_point, timestamp)\n            latest_point = max(latest_point, timestamp)\n            datum = interim_data.get(timestamp, {\n                'date': timestamp,\n            })\n            datum[key] = value\n            interim_data[timestamp] = datum\n    timestamp = earliest_point\n    while (timestamp <= latest_point):\n        datum = interim_data.get(timestamp, {\n            'date': timestamp,\n        })\n        for key in histograms_data.iterkeys():\n            if (key not in datum):\n                datum[key] = 0\n        timestamp += bucket\n    return interim_data.values()\n", "label": 1}
{"function": "\n\n@classmethod\ndef _crawl(cls, url, spider, url_hashes, collectors, post_processors):\n    _blank = []\n    with cls._lock:\n        if (url_hash(url) in url_hashes):\n            return _blank\n        else:\n            url_hashes.add(url_hash(url))\n    try:\n        logger.info('Crawling: %s', url)\n        response = urlopen(url)\n    except Exception as e:\n        logger.error('Request failed for url %s Exception: %s', url, e)\n        return _blank\n    try:\n        htmlparser = etree.HTMLParser()\n        tree = etree.parse(response, htmlparser)\n    except Exception as e:\n        logger.error('Failed parsing response for url %s Exception: %s', url, e)\n        return _blank\n    try:\n        abs_urls = tree.xpath(cls.abs_url_xpath, namespaces={\n            're': 'http://exslt.org/regular-expressions',\n        })\n    except Exception as e:\n        logger.error('Absolute url extraction failed for %s', url)\n        abs_urls = []\n    try:\n        relative_urls = tree.xpath(cls.relative_url_xpath, namespaces={\n            're': 'http://exslt.org/regular-expressions',\n        })\n    except Exception as e:\n        logger.error('Relative url extraction failed for %s', url)\n        relative_urls = []\n    abs_urls = filter((lambda x: is_url_in_domain(x, spider.domains)), abs_urls)\n    abs_urls = (abs_urls + [urljoin(url, r_url) for r_url in relative_urls])\n    logger.info('%s more urls discovered on %s', len(abs_urls), url)\n    urls_to_crawl = []\n    for abs_url in abs_urls:\n        _hash = url_hash(abs_url)\n        if (_hash not in url_hashes):\n            urls_to_crawl.append(abs_url)\n    try:\n        parsed = spider.parse(response, tree)\n    except Exception as e:\n        logger.error('Error parsing HTML for %s: Exception %s', url, e)\n    else:\n        logger.info('Parsed HTML for %s', url)\n        try:\n            parsed = reduce((lambda x, y: y.clean(x)), collectors, parsed)\n        except Exception as e:\n            logger.error('Error cleaning %s: Exception %s', url, e)\n        else:\n            try:\n                for post_procesor in post_processors:\n                    post_procesor.process(parsed)\n            except Exception as e:\n                logger.error('Error post processing %s: Exception %s', url, e)\n    return urls_to_crawl\n", "label": 1}
{"function": "\n\ndef cancelUpload(self, upload):\n    '\\n        Delete the temporary files associated with a given upload.\\n        '\n    if ('s3' not in upload):\n        return\n    if ('key' not in upload['s3']):\n        return\n    bucket = self._getBucket()\n    if bucket:\n        key = bucket.get_key(upload['s3']['key'], validate=True)\n        if key:\n            bucket.delete_key(key)\n        if (('s3' in upload) and ('uploadId' in upload['s3']) and ('key' in upload['s3'])):\n            getParams = {\n                \n            }\n            while True:\n                try:\n                    multipartUploads = bucket.get_all_multipart_uploads(**getParams)\n                except boto.exception.S3ResponseError:\n                    break\n                if (not len(multipartUploads)):\n                    break\n                for multipartUpload in multipartUploads:\n                    if ((multipartUpload.id == upload['s3']['uploadId']) and (multipartUpload.key_name == upload['s3']['key'])):\n                        multipartUpload.cancel_upload()\n                if (not multipartUploads.is_truncated):\n                    break\n                getParams['key_marker'] = multipartUploads.next_key_marker\n                getParams['upload_id_marker'] = multipartUploads.next_upload_id_marker\n", "label": 1}
{"function": "\n\ndef scan(self):\n    if self.scanned:\n        return\n    updatemsg = ('[updated]' if self.__modified else '[cached]')\n    Console.info('Scanning project %s %s...', self.__name, Console.colorize(updatemsg, 'grey'))\n    Console.indent()\n    setup = self.__setup\n    if (setup and self.__modified):\n        Console.info('Running setup...')\n        Console.indent()\n        for cmd in setup:\n            Console.info('Executing %s...', cmd)\n            result = None\n            try:\n                result = None\n                result = Util.executeCommand(cmd, ('Failed to execute setup command %s' % cmd), path=self.__path)\n            except Exception as ex:\n                if result:\n                    Console.error(result)\n                raise UserError(('Could not scan project %s: %s' % (self.__name, ex)))\n        Console.outdent()\n    if self.__config.has('content'):\n        self.kind = 'manual'\n        self.__addContent(self.__config.get('content'))\n    elif self.__hasDir('source'):\n        self.kind = 'application'\n        if self.__hasDir('source/class'):\n            self.__addDir('source/class', 'classes')\n        if self.__hasDir('source/asset'):\n            self.__addDir('source/asset', 'assets')\n        if self.__hasDir('source/translation'):\n            self.__addDir('source/translation', 'translations')\n    elif self.__hasDir('src'):\n        self.kind = 'resource'\n        self.__addDir('src', 'classes')\n    else:\n        self.kind = 'resource'\n        if self.__hasDir('class'):\n            self.__addDir('class', 'classes')\n        if self.__hasDir('asset'):\n            self.__addDir('asset', 'assets')\n        if self.__hasDir('translation'):\n            self.__addDir('translation', 'translations')\n    summary = []\n    for section in ['classes', 'assets', 'translations']:\n        content = getattr(self, section, None)\n        if content:\n            summary.append(('%s %s' % (len(content), section)))\n    if summary:\n        Console.info(('Done %s: %s' % (Console.colorize(('[%s]' % self.kind), 'grey'), Console.colorize(', '.join(summary), 'green'))))\n    else:\n        Console.error('Project is empty!')\n    self.scanned = True\n    Console.outdent()\n", "label": 1}
{"function": "\n\ndef adjust(self, to):\n    '\\n        Adjusts the time from kwargs to timedelta\\n        **Will change this object**\\n\\n        return new copy of self\\n        '\n    if (self.date == 'infinity'):\n        return\n    new = copy(self)\n    if (type(to) in (str, unicode)):\n        to = to.lower()\n        res = TIMESTRING_RE.search(to)\n        if res:\n            rgroup = res.groupdict()\n            if (rgroup.get('delta') or rgroup.get('delta_2')):\n                i = (int(text2num(rgroup.get('num', 'one'))) * ((- 1) if to.startswith('-') else 1))\n                delta = (rgroup.get('delta') or rgroup.get('delta_2')).lower()\n                if delta.startswith('y'):\n                    try:\n                        new.date = new.date.replace(year=(new.date.year + i))\n                    except ValueError:\n                        new.date = (new.date + timedelta(days=(365 * i)))\n                elif delta.startswith('month'):\n                    if ((new.date.month + i) > 12):\n                        new.date = new.date.replace(month=(i - (i / 12)), year=((new.date.year + 1) + (i / 12)))\n                    elif ((new.date.month + i) < 1):\n                        new.date = new.date.replace(month=12, year=(new.date.year - 1))\n                    else:\n                        new.date = new.date.replace(month=(new.date.month + i))\n                elif delta.startswith('q'):\n                    pass\n                elif delta.startswith('w'):\n                    new.date = (new.date + timedelta(days=(7 * i)))\n                elif delta.startswith('s'):\n                    new.date = (new.date + timedelta(seconds=i))\n                else:\n                    new.date = (new.date + timedelta(**{\n                        ('days' if delta.startswith('d') else ('hours' if delta.startswith('h') else ('minutes' if delta.startswith('m') else 'seconds'))): i,\n                    }))\n                return new\n    else:\n        new.date = (new.date + timedelta(seconds=int(to)))\n        return new\n    raise TimestringInvalid('Invalid addition request')\n", "label": 1}
{"function": "\n\ndef export_network(user, directory):\n    m_texts = bc.network.matrix_directed_weighted(user, 'text')\n    m_calls = bc.network.matrix_directed_weighted(user, 'call')\n    nb_users = len(m_texts)\n    links = []\n    for i in range(nb_users):\n        for j in range(nb_users):\n            source_calls = (m_calls[j][i] or 0)\n            target_calls = (m_calls[i][j] or 0)\n            source_texts = (m_texts[j][i] or 0)\n            target_texts = (m_texts[i][j] or 0)\n            if ((i < j) and ((((source_calls + target_calls) + source_texts) + target_texts) > 0)):\n                links.append([i, j, (source_calls, target_calls, source_texts, target_texts)])\n    names = ([user.name] + [i for i in user.network.keys() if (i != user.name)])\n    nodes = (set((l[0] for l in links)) | set((l[1] for l in links)))\n    with open(os.path.join(directory, 'nodes.csv'), 'wb') as f:\n        f.write('name,no_network_info\\n')\n        for n in nodes:\n            f.write('{},{}\\n'.format(names[n], int(((None in m_texts[n]) or (None in m_calls[n])))))\n    compress_ids = dict(zip(nodes, range(len(nodes))))\n    with open(os.path.join(directory, 'links.csv'), 'wb') as f:\n        f.write('source,target,source_calls,target_calls,source_texts,target_texts\\n')\n        for (i, j, data) in links:\n            f.write('{},{},{},{},{},{}\\n'.format(*((compress_ids[i], compress_ids[j]) + data)))\n", "label": 1}
{"function": "\n\n@expose(hide=True)\ndef debug_php(self):\n    'Start/Stop PHP debug'\n    if ((self.app.pargs.php == 'on') and (not self.app.pargs.site_name)):\n        if (not EEShellExec.cmd_exec(self, 'sed -n \"/upstream php{/,/}/p \" /etc/nginx/conf.d/upstream.conf | grep 9001')):\n            Log.info(self, 'Enabling PHP debug')\n            nc = NginxConfig()\n            nc.loadf('/etc/nginx/conf.d/upstream.conf')\n            nc.set([('upstream', 'php'), 'server'], '127.0.0.1:9001')\n            if os.path.isfile('/etc/nginx/common/wpfc-hhvm.conf'):\n                nc.set([('upstream', 'hhvm'), 'server'], '127.0.0.1:9001')\n            nc.savef('/etc/nginx/conf.d/upstream.conf')\n            EEFileUtils.searchreplace(self, ('/etc/{0}/mods-available/'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')) + 'xdebug.ini'), ';zend_extension', 'zend_extension')\n            config = configparser.ConfigParser()\n            config.read('/etc/{0}/fpm/pool.d/debug.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')))\n            config['debug']['slowlog'] = '/var/log/{0}/slow.log'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5'))\n            config['debug']['request_slowlog_timeout'] = '10s'\n            with open('/etc/{0}/fpm/pool.d/debug.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')), encoding='utf-8', mode='w') as confifile:\n                Log.debug(self, 'Writting debug.conf configuration into /etc/{0}/fpm/pool.d/debug.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')))\n                config.write(confifile)\n            self.trigger_php = True\n            self.trigger_nginx = True\n        else:\n            Log.info(self, 'PHP debug is already enabled')\n        self.msg = (self.msg + ['/var/log/{0}/slow.log'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5'))])\n    elif ((self.app.pargs.php == 'off') and (not self.app.pargs.site_name)):\n        if EEShellExec.cmd_exec(self, ' sed -n \"/upstream php {/,/}/p\" /etc/nginx/conf.d/upstream.conf | grep 9001'):\n            Log.info(self, 'Disabling PHP debug')\n            nc = NginxConfig()\n            nc.loadf('/etc/nginx/conf.d/upstream.conf')\n            nc.set([('upstream', 'php'), 'server'], '127.0.0.1:9000')\n            if os.path.isfile('/etc/nginx/common/wpfc-hhvm.conf'):\n                nc.set([('upstream', 'hhvm'), 'server'], '127.0.0.1:8000')\n            nc.savef('/etc/nginx/conf.d/upstream.conf')\n            EEFileUtils.searchreplace(self, ('/etc/{0}/mods-available/'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')) + 'xdebug.ini'), 'zend_extension', ';zend_extension')\n            self.trigger_php = True\n            self.trigger_nginx = True\n        else:\n            Log.info(self, 'PHP debug is already disabled')\n", "label": 1}
{"function": "\n\n@classmethod\ndef put(cls, kvs, entity=None):\n    for k in kvs.keys():\n        if (not (k in cls.properties())):\n            del kvs[k]\n            continue\n        v = cls.__dict__[k]\n        if isinstance(v, db.IntegerProperty):\n            kvs[k] = int(kvs[k])\n        elif isinstance(v, db.FloatProperty):\n            kvs[k] = float(kvs[k])\n        elif isinstance(v, db.BooleanProperty):\n            kvs[k] = (True if (kvs[k] == 'True') else False)\n        elif (isinstance(v, db.StringProperty) or isinstance(v, db.TextProperty)):\n            kvs[k] = kvs[k]\n        elif isinstance(v, db.DateProperty):\n            kvs[k] = datetime.strptime(kvs[k], settings.DATE_FORMAT).date()\n        elif isinstance(v, db.LinkProperty):\n            kvs[k] = kvs[k]\n        elif isinstance(v, db.EmailProperty):\n            kvs[k] = kvs[k]\n        else:\n            raise UnsupportedFieldTypeError(v)\n    cls.validate(kvs)\n    try:\n        if (not entity):\n            entity = cls(**kvs)\n        else:\n            for k in kvs.keys():\n                setattr(entity, k, kvs[k])\n        db.put(entity)\n    except Exception as e:\n        logging.error((\"Couldn't put entity: %s\" % kvs))\n        logging.error(e)\n        return None\n    return entity\n", "label": 1}
{"function": "\n\ndef get_show(maze_id=None, tvdb_id=None, tvrage_id=None, imdb_id=None, show_name=None, show_year=None, show_network=None, show_language=None, show_country=None, show_web_channel=None, embed=None):\n    \"\\n    Get Show object directly via id or indirectly via name + optional qualifiers\\n\\n    If only a show_name is given, the show with the highest score using the\\n    tvmaze algorithm will be returned.\\n    If you provide extra qualifiers such as network or language they will be\\n    used for a more specific match, if one exists.\\n    :param maze_id: Show maze_id\\n    :param tvdb_id: Show tvdb_id\\n    :param tvrage_id: Show tvrage_id\\n    :param show_name: Show name to be searched\\n    :param show_year: Show premiere year\\n    :param show_network: Show TV Network (like ABC, NBC, etc.)\\n    :param show_web_channel: Show Web Channel (like Netflix, Amazon, etc.)\\n    :param show_language: Show language\\n    :param show_country: Show country\\n    :param embed: embed parameter to include additional data. Currently 'episodes' and 'cast' are supported\\n    :return:\\n    \"\n    errors = []\n    if (not (maze_id or tvdb_id or tvrage_id or imdb_id or show_name)):\n        raise MissingParameters('Either maze_id, tvdb_id, tvrage_id, imdb_id or show_name are required to get show, none provided,')\n    if maze_id:\n        try:\n            return show_main_info(maze_id, embed=embed)\n        except IDNotFound as e:\n            errors.append(e.value)\n    if tvdb_id:\n        try:\n            return show_main_info(lookup_tvdb(tvdb_id).id, embed=embed)\n        except IDNotFound as e:\n            errors.append(e.value)\n    if tvrage_id:\n        try:\n            return show_main_info(lookup_tvrage(tvrage_id).id, embed=embed)\n        except IDNotFound as e:\n            errors.append(e.value)\n    if imdb_id:\n        try:\n            return show_main_info(lookup_imdb(imdb_id).id, embed=embed)\n        except IDNotFound as e:\n            errors.append(e.value)\n    if show_name:\n        try:\n            show = _get_show_by_search(show_name, show_year, show_network, show_language, show_country, show_web_channel, embed=embed)\n            return show\n        except ShowNotFound as e:\n            errors.append(e.value)\n    raise ShowNotFound(' ,'.join(errors))\n", "label": 1}
{"function": "\n\ndef test_write_session():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'write': ['test'],\n        }, key='test')\n        key = Key.load(h.get_key('test'))\n        assert (key.api_key == 'test')\n        assert (key.is_admin() == False)\n        assert (key.can_create_key() == False)\n        assert (key.can_create_user() == False)\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == False)\n        assert (key.can_write('test') == True)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\n@register_specialize_device\n@gof.local_optimizer([T.pow])\ndef local_pow_specialize_device(node):\n    '\\n    This optimization is not the same on all device. We do it only on cpu here.\\n    '\n    if (node.op == T.pow):\n        odtype = node.outputs[0].dtype\n        xsym = node.inputs[0]\n        ysym = node.inputs[1]\n        y = local_mul_canonizer.get_constant(ysym)\n        if isinstance(y, numpy.ndarray):\n            assert (y.size == 1)\n            try:\n                y = y[0]\n            except IndexError:\n                pass\n        if ((y is not None) and encompasses_broadcastable(xsym.type.broadcastable, ysym.type.broadcastable)):\n            rval = None\n            if ((abs(y) == int(abs(y))) and (abs(y) <= 512)):\n                pow2 = [xsym]\n                pow2_scal = [theano.scalar.get_scalar_type(xsym.dtype)()]\n                y_to_do = abs(y)\n                for i in xrange(int(numpy.log2(y_to_do))):\n                    pow2.append(T.sqr(pow2[i]))\n                    pow2_scal.append(theano.scalar.sqr(pow2_scal[i]))\n                rval1 = None\n                rval1_scal = None\n                while (y_to_do > 0):\n                    log_to_do = int(numpy.log2(y_to_do))\n                    if rval1:\n                        rval1 *= pow2[log_to_do]\n                        rval1_scal *= pow2_scal[log_to_do]\n                    else:\n                        rval1 = pow2[log_to_do]\n                        rval1_scal = pow2_scal[log_to_do]\n                    y_to_do -= (2 ** log_to_do)\n                if (abs(y) > 2):\n                    rval1 = Elemwise(theano.scalar.Composite([pow2_scal[0]], [rval1_scal])).make_node(xsym)\n                if (y < 0):\n                    rval = [T.inv(rval1)]\n                else:\n                    rval = [rval1]\n            if rval:\n                rval[0] = T.cast(rval[0], odtype)\n                assert (rval[0].type == node.outputs[0].type), (rval, node.outputs)\n                return rval\n", "label": 1}
{"function": "\n\ndef _symbolic_factor_list(expr, opt, method):\n    'Helper function for :func:`_symbolic_factor`. '\n    (coeff, factors) = (S.One, [])\n    args = [(i._eval_factor() if hasattr(i, '_eval_factor') else i) for i in Mul.make_args(expr)]\n    for arg in args:\n        if arg.is_Number:\n            coeff *= arg\n            continue\n        if arg.is_Mul:\n            args.extend(arg.args)\n            continue\n        if arg.is_Pow:\n            (base, exp) = arg.args\n            if base.is_Number:\n                factors.append((base, exp))\n                continue\n        else:\n            (base, exp) = (arg, S.One)\n        try:\n            (poly, _) = _poly_from_expr(base, opt)\n        except PolificationFailed as exc:\n            factors.append((exc.expr, exp))\n        else:\n            func = getattr(poly, (method + '_list'))\n            (_coeff, _factors) = func()\n            if (_coeff is not S.One):\n                if exp.is_Integer:\n                    coeff *= (_coeff ** exp)\n                elif _coeff.is_positive:\n                    factors.append((_coeff, exp))\n                else:\n                    _factors.append((_coeff, S.One))\n            if (exp is S.One):\n                factors.extend(_factors)\n            elif exp.is_integer:\n                factors.extend([(f, (k * exp)) for (f, k) in _factors])\n            else:\n                other = []\n                for (f, k) in _factors:\n                    if f.as_expr().is_positive:\n                        factors.append((f, (k * exp)))\n                    else:\n                        other.append((f, k))\n                factors.append((_factors_product(other), exp))\n    return (coeff, factors)\n", "label": 1}
{"function": "\n\ndef convertStronglyConnectedComponentWithOneNode(self, dependencyGraph, stronglyConnectedComponent, objectIdToObjectDefinition):\n    objectId = stronglyConnectedComponent[0]\n    objectDefinition = objectIdToObjectDefinition[objectId]\n    if (TypeDescription.isPrimitive(objectDefinition) or isinstance(objectDefinition, list)):\n        self.convertedValues[objectId] = self.convertPrimitive(objectDefinition)\n    elif isinstance(objectDefinition, (TypeDescription.FunctionDefinition, TypeDescription.ClassDefinition)):\n        if isinstance(objectDefinition, TypeDescription.ClassDefinition):\n            for baseId in objectDefinition.baseClassIds:\n                if (baseId not in self.convertedValues):\n                    self._convert(baseId, dependencyGraph, objectIdToObjectDefinition)\n                assert (baseId in self.convertedValues)\n        self.convertStronglyConnectedComponentWithOneFunctionOrClass(objectId, objectDefinition, objectIdToObjectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.ClassInstanceDescription):\n        self.convertClassInstanceDescription(objectId, objectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.List):\n        self.convertedValues[objectId] = self.convertList(objectId, self._computeRestrictedGraph(objectId, dependencyGraph), objectIdToObjectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.Tuple):\n        self.convertedValues[objectId] = self.convertTuple(objectId, self._computeRestrictedGraph(objectId, dependencyGraph), objectIdToObjectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.Dict):\n        self.convertedValues[objectId] = self.convertDict(objectId, objectDefinition, self._computeRestrictedGraph(objectId, dependencyGraph), objectIdToObjectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.File):\n        self.convertedValues[objectId] = self.convertFile(objectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.RemotePythonObject):\n        self.convertedValues[objectId] = self.convertRemotePythonObject(objectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.BuiltinExceptionInstance):\n        self.convertedValues[objectId] = self.convertBuiltinExceptionInstance(objectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.NamedSingleton):\n        self.convertedValues[objectId] = self.convertNamedSingleton(objectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.WithBlockDescription):\n        self.convertWithBlock(objectId, objectDefinition, objectIdToObjectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.InstanceMethod):\n        self.convertedValues[objectId] = self.convertInstanceMethod(objectId, objectIdToObjectDefinition)\n    elif isinstance(objectDefinition, TypeDescription.Unconvertible):\n        self.convertedValues[objectId] = Symbol_unconvertible\n    else:\n        assert False, (\"haven't gotten to this yet %s\" % type(objectDefinition))\n", "label": 1}
{"function": "\n\ndef get_value(name, default_value=None, path=os.curdir, print_warnings=True):\n    value = None\n    term = get_term()\n    if (name == VALUE_WERCKER_URL):\n        value = os.getenv('wercker_url', None)\n        if (value is None):\n            value = os.getenv(ENV_KEY_WERCKER_URL, DEFAULT_WERCKER_URL)\n        return value\n    elif (name == VALUE_MIXPANEL_TOKEN):\n        value = os.getenv(ENV_KEY_MIXPANEL_TOKEN, DEFAULT_MIXPANEL_TOKEN)\n    elif (name == VALUE_USER_TOKEN):\n        wercker_url = get_value(VALUE_WERCKER_URL)\n        url = urlparse(wercker_url)\n        file = _get_or_create_netrc_location()\n        rc = netrc.netrc(file)\n        if (url.hostname in rc.hosts):\n            value = rc.hosts[url.hostname][2]\n    elif (name == VALUE_USER_NAME):\n        wercker_url = get_value(VALUE_WERCKER_URL)\n        url = urlparse(wercker_url)\n        file = _get_or_create_netrc_location()\n        rc = netrc.netrc(file)\n        if (url.hostname in rc.hosts):\n            value = rc.hosts[url.hostname][0]\n    elif (name == VALUE_HEROKU_TOKEN):\n        file = _get_or_create_netrc_location()\n        rc = netrc.netrc(file)\n        result = rc.authenticators('api.heroku.com')\n        if (result and (len(result) == 3)):\n            value = result[2]\n    elif (name == VALUE_PROJECT_ID):\n        path = find_git_root(path)\n        if (not path):\n            if print_warnings:\n                puts((term.red('Warning:') + ' Could not find a git repository.'))\n            return\n        file = os.path.join(path, DEFAULT_DOT_WERCKER_NAME)\n        if (not os.path.isfile(file)):\n            if print_warnings:\n                puts((term.yellow('Warning:') + (' Could not find a %s file in the application root' % DEFAULT_DOT_WERCKER_NAME)))\n            return\n        Config = ConfigParser.ConfigParser()\n        Config.read(file)\n        try:\n            value = Config.get('project', 'id')\n        except (ConfigParser.NoOptionError, ConfigParser.NoSectionError):\n            value = None\n    elif (name == VALUE_DISPLAY_DEBUG):\n        env_value = os.environ.get(ENV_KEY_DISPLAY_DEBUG)\n        if ((env_value is not None) and (env_value.lower() == 'true')):\n            value = True\n        else:\n            value = DEFAULT_DISPLAY_DEBUG\n    return value\n", "label": 1}
{"function": "\n\ndef find_file(path, tgt_env='base', **kwargs):\n    '\\n    Find the first file to match the path and ref, read the file out of hg\\n    and send the path to the newly cached file\\n    '\n    fnd = {\n        'path': '',\n        'rel': '',\n    }\n    if (os.path.isabs(path) or (tgt_env not in envs())):\n        return fnd\n    dest = os.path.join(__opts__['cachedir'], 'hgfs/refs', tgt_env, path)\n    hashes_glob = os.path.join(__opts__['cachedir'], 'hgfs/hash', tgt_env, '{0}.hash.*'.format(path))\n    blobshadest = os.path.join(__opts__['cachedir'], 'hgfs/hash', tgt_env, '{0}.hash.blob_sha1'.format(path))\n    lk_fn = os.path.join(__opts__['cachedir'], 'hgfs/hash', tgt_env, '{0}.lk'.format(path))\n    destdir = os.path.dirname(dest)\n    hashdir = os.path.dirname(blobshadest)\n    if (not os.path.isdir(destdir)):\n        try:\n            os.makedirs(destdir)\n        except OSError:\n            os.remove(destdir)\n            os.makedirs(destdir)\n    if (not os.path.isdir(hashdir)):\n        try:\n            os.makedirs(hashdir)\n        except OSError:\n            os.remove(hashdir)\n            os.makedirs(hashdir)\n    for repo in init():\n        if (repo['mountpoint'] and (not path.startswith((repo['mountpoint'] + os.path.sep)))):\n            continue\n        repo_path = path[len(repo['mountpoint']):].lstrip(os.path.sep)\n        if repo['root']:\n            repo_path = os.path.join(repo['root'], repo_path)\n        repo['repo'].open()\n        ref = _get_ref(repo, tgt_env)\n        if (not ref):\n            repo['repo'].close()\n            continue\n        salt.fileserver.wait_lock(lk_fn, dest)\n        if (os.path.isfile(blobshadest) and os.path.isfile(dest)):\n            with salt.utils.fopen(blobshadest, 'r') as fp_:\n                sha = fp_.read()\n                if (sha == ref[2]):\n                    fnd['rel'] = path\n                    fnd['path'] = dest\n                    repo['repo'].close()\n                    return fnd\n        try:\n            repo['repo'].cat(['path:{0}'.format(repo_path)], rev=ref[2], output=dest)\n        except hglib.error.CommandError:\n            repo['repo'].close()\n            continue\n        with salt.utils.fopen(lk_fn, 'w+') as fp_:\n            fp_.write('')\n        for filename in glob.glob(hashes_glob):\n            try:\n                os.remove(filename)\n            except Exception:\n                pass\n        with salt.utils.fopen(blobshadest, 'w+') as fp_:\n            fp_.write(ref[2])\n        try:\n            os.remove(lk_fn)\n        except (OSError, IOError):\n            pass\n        fnd['rel'] = path\n        fnd['path'] = dest\n        fnd['stat'] = list(os.stat(dest))\n        repo['repo'].close()\n        return fnd\n    return fnd\n", "label": 1}
{"function": "\n\ndef telescopic(L, R, limits):\n    'Tries to perform the summation using the telescopic property\\n\\n    return None if not possible\\n    '\n    (i, a, b) = limits\n    if (L.is_Add or R.is_Add):\n        return None\n    k = Wild('k')\n    sol = (- R).match(L.subs(i, (i + k)))\n    s = None\n    if (sol and (k in sol)):\n        s = sol[k]\n        if (not (s.is_Integer and (L.subs(i, (i + s)) == (- R)))):\n            s = None\n    if (s is None):\n        m = Dummy('m')\n        try:\n            sol = (solve((L.subs(i, (i + m)) + R), m) or [])\n        except NotImplementedError:\n            return None\n        sol = [si for si in sol if (si.is_Integer and (L.subs(i, (i + si)) + R).expand().is_zero)]\n        if (len(sol) != 1):\n            return None\n        s = sol[0]\n    if (s < 0):\n        return telescopic_direct(R, L, abs(s), (i, a, b))\n    elif (s > 0):\n        return telescopic_direct(L, R, s, (i, a, b))\n", "label": 1}
{"function": "\n\ndef get(self, request, *args, **kwargs):\n    meter = request.GET.get('meter', None)\n    meter_name = meter.replace('.', '_')\n    date_options = request.GET.get('date_options', None)\n    date_from = request.GET.get('date_from', None)\n    date_to = request.GET.get('date_to', None)\n    stats_attr = request.GET.get('stats_attr', 'avg')\n    if (date_options == 'other'):\n        try:\n            if date_from:\n                date_from = datetime.strptime(date_from, '%Y-%m-%d')\n            else:\n                pass\n            if date_to:\n                date_to = datetime.strptime(date_to, '%Y-%m-%d')\n                date_to = ((date_to + timedelta(days=1)) - timedelta(seconds=1))\n            else:\n                date_to = datetime.now()\n        except Exception:\n            raise ValueError(\"The dates haven't been recognized\")\n    else:\n        try:\n            date_from = (datetime.now() - timedelta(days=int(date_options)))\n            date_to = datetime.now()\n        except Exception:\n            raise ValueError('The time delta must be an integer representing days.')\n    if (date_from and date_to):\n        if (date_to < date_from):\n            raise ValueError('Date to must be bigger than date from.')\n        delta = (date_to - date_from)\n        if (delta.days <= 0):\n            delta_in_seconds = (3600 * 24)\n        else:\n            delta_in_seconds = (((delta.days * 24) * 3600) + delta.seconds)\n        number_of_samples = 400\n        period = (delta_in_seconds / number_of_samples)\n    else:\n        period = (3600 * 24)\n    additional_query = []\n    if date_from:\n        additional_query += [{\n            'field': 'timestamp',\n            'op': 'ge',\n            'value': date_from,\n        }]\n    if date_to:\n        additional_query += [{\n            'field': 'timestamp',\n            'op': 'le',\n            'value': date_to,\n        }]\n    try:\n        meter_list = [m for m in ceilometer.meter_list(request) if (m.name == meter)]\n        unit = meter_list[0].unit\n    except Exception:\n        unit = ''\n    if (request.GET.get('group_by', None) == 'project'):\n        try:\n            (tenants, more) = api.keystone.tenant_list(request, domain=None, paginate=True, marker='tenant_marker')\n        except Exception:\n            tenants = []\n            exceptions.handle(request, _('Unable to retrieve tenant list.'))\n        queries = {\n            \n        }\n        for tenant in tenants:\n            tenant_query = [{\n                'field': 'project_id',\n                'op': 'eq',\n                'value': tenant.id,\n            }]\n            queries[tenant.name] = tenant_query\n        ceilometer_usage = ceilometer.CeilometerUsage(request)\n        resources = ceilometer_usage.resource_aggregates_with_statistics(queries, [meter], period=period, stats_attr=None, additional_query=additional_query)\n        series = self._series_for_meter(resources, 'id', meter_name, stats_attr, unit)\n    else:\n        query = []\n\n        def filter_by_meter_name(resource):\n            ' Function for filtering of the list of resources.\\n\\n                Will pick the right resources according to currently selected\\n                meter.\\n                '\n            for link in resource.links:\n                if (link['rel'] == meter):\n                    return True\n            return False\n        ceilometer_usage = ceilometer.CeilometerUsage(request)\n        try:\n            resources = ceilometer_usage.resources_with_statistics(query, [meter], period=period, stats_attr=None, additional_query=additional_query, filter_func=filter_by_meter_name)\n        except Exception:\n            resources = []\n            exceptions.handle(request, _('Unable to retrieve statistics.'))\n        series = self._series_for_meter(resources, 'resource_id', meter_name, stats_attr, unit)\n    ret = {\n        \n    }\n    ret['series'] = series\n    ret['settings'] = {\n        \n    }\n    return HttpResponse(json.dumps(ret), mimetype='application/json')\n", "label": 1}
{"function": "\n\ndef insert_children(self, before, insert):\n    \" Insert children into this object at the given location.\\n\\n        The children will be automatically parented and inserted into\\n        the object's children. If any children are already children of\\n        this object, then they will be moved appropriately.\\n\\n        Parameters\\n        ----------\\n        before : Object, int or None\\n            A child object or int to use as the marker for inserting\\n            the new children. The new children will be inserted before\\n            this marker. If the Object is None or not a child, or if\\n            the int is not a valid index, then the new children will be\\n            added to the end of the children.\\n\\n        insert : iterable\\n            An iterable of Object children to insert into this object.\\n\\n        Notes\\n        -----\\n        It is the responsibility of the caller to initialize and activate\\n        the object as needed, if it is reparented dynamically at runtime.\\n\\n        \"\n    insert_list = list(insert)\n    insert_set = set(insert_list)\n    if (self in insert_set):\n        raise ValueError('cannot use `self` as Object child')\n    if (len(insert_list) != len(insert_set)):\n        raise ValueError('cannot insert duplicate children')\n    if (not all((isinstance(child, Object) for child in insert_list))):\n        raise TypeError('children must be an Object instances')\n    if isinstance(before, int):\n        try:\n            before = self._children[before]\n        except IndexError:\n            before = None\n    new = []\n    added = False\n    for child in self._children:\n        if (child in insert_set):\n            insert_set.remove(child)\n            continue\n        if (child is before):\n            new.extend(insert_list)\n            added = True\n        new.append(child)\n    if (not added):\n        new.extend(insert_list)\n    for child in insert_list:\n        old_parent = child._parent\n        if (old_parent is not self):\n            child._parent = self\n            child.parent_changed(old_parent, self)\n            if (old_parent is not None):\n                old_parent.child_removed(child)\n    self._children = new\n    child_added = self.child_added\n    child_moved = self.child_moved\n    for child in insert_list:\n        if (child in insert_set):\n            child_added(child)\n        else:\n            child_moved(child)\n", "label": 1}
{"function": "\n\ndef _connect_attempt(self, host, port, retry):\n    client = self.client\n    TimeoutError = self.handler.timeout_exception\n    close_connection = False\n    self._socket = None\n    if self._rw_server:\n        self.logger.log(BLATHER, 'Found r/w server to use, %s:%s', host, port)\n        (host, port) = self._rw_server\n        self._rw_server = None\n    if (client._state != KeeperState.CONNECTING):\n        client._session_callback(KeeperState.CONNECTING)\n    try:\n        (read_timeout, connect_timeout) = self._connect(host, port)\n        read_timeout = (read_timeout / 1000.0)\n        connect_timeout = (connect_timeout / 1000.0)\n        retry.reset()\n        self._xid = 0\n        while (not close_connection):\n            jitter_time = (random.randint(0, 40) / 100.0)\n            timeout = max([((read_timeout / 2.0) - jitter_time), jitter_time])\n            s = self.handler.select([self._socket, self._read_pipe], [], [], timeout)[0]\n            if (not s):\n                if self.ping_outstanding.is_set():\n                    self.ping_outstanding.clear()\n                    raise ConnectionDropped('outstanding heartbeat ping not received')\n                self._send_ping(connect_timeout)\n            elif (s[0] == self._socket):\n                response = self._read_socket(read_timeout)\n                close_connection = (response == CLOSE_RESPONSE)\n            else:\n                self._send_request(read_timeout, connect_timeout)\n        self.logger.info('Closing connection to %s:%s', host, port)\n        client._session_callback(KeeperState.CLOSED)\n        return STOP_CONNECTING\n    except (ConnectionDropped, TimeoutError) as e:\n        if isinstance(e, ConnectionDropped):\n            self.logger.warning('Connection dropped: %s', e)\n        else:\n            self.logger.warning('Connection time-out')\n        if (client._state != KeeperState.CONNECTING):\n            self.logger.warning('Transition to CONNECTING')\n            client._session_callback(KeeperState.CONNECTING)\n    except AuthFailedError:\n        retry.reset()\n        self.logger.warning('AUTH_FAILED closing')\n        client._session_callback(KeeperState.AUTH_FAILED)\n        return STOP_CONNECTING\n    except SessionExpiredError:\n        retry.reset()\n        self.logger.warning('Session has expired')\n        client._session_callback(KeeperState.EXPIRED_SESSION)\n    except RWServerAvailable:\n        retry.reset()\n        self.logger.warning('Found a RW server, dropping connection')\n        client._session_callback(KeeperState.CONNECTING)\n    except Exception:\n        self.logger.exception('Unhandled exception in connection loop')\n        raise\n    finally:\n        if (self._socket is not None):\n            self._socket.close()\n", "label": 1}
{"function": "\n\n@skip\ndef test_pulsar_objectstore(self):\n    object_store_config_file = join(self.temp_directory, 'object_store_conf.xml')\n    with open(object_store_config_file, 'w') as configf:\n        config_template = Template('<?xml version=\"1.0\"?>\\n<object_store type=\"disk\">\\n    <files_dir path=\"${temp_directory}\"/>\\n    <extra_dir type=\"temp\" path=\"${temp_directory}\"/>\\n    <extra_dir type=\"job_work\" path=\"${temp_directory}\"/>\\n</object_store>\\n')\n        config_contents = config_template.safe_substitute(temp_directory=self.temp_directory)\n        configf.write(config_contents)\n    app_conf = dict(object_store_config_file=object_store_config_file, private_token='12345')\n    from .test_utils import test_pulsar_server\n    with test_pulsar_server(app_conf=app_conf) as server:\n        url = server.application_url\n        proxy_object_store_config_file = join(self.temp_directory, 'proxy_object_store_conf.xml')\n        with open(proxy_object_store_config_file, 'w') as configf:\n            config_template = Template('<?xml version=\"1.0\"?>\\n<object_store type=\"pulsar\" url=\"$url\" private_token=\"12345\" transport=\"urllib\">\\n  <!-- private_token is optional - see Pulsar documentation for more information. -->\\n  <!-- transport is optional, set to curl to use libcurl instead of urllib for communication with Pulsar. -->\\n</object_store>\\n')\n            contents = config_template.safe_substitute(url=url)\n            configf.write(contents)\n        config = Bunch(object_store_config_file=proxy_object_store_config_file)\n        object_store = build_object_store_from_config(config=config)\n        absent_dataset = MockDataset(1)\n        assert (not object_store.exists(absent_dataset))\n        empty_dataset = MockDataset(2)\n        self.__write(b'', '000/dataset_2.dat')\n        assert object_store.exists(empty_dataset)\n        assert object_store.empty(empty_dataset)\n        hello_world_dataset = MockDataset(3)\n        self.__write(b'Hello World!', '000/dataset_3.dat')\n        assert object_store.exists(hello_world_dataset)\n        assert (not object_store.empty(hello_world_dataset))\n        data = object_store.get_data(hello_world_dataset)\n        assert (data == 'Hello World!')\n        data = object_store.get_data(hello_world_dataset, start=1, count=6)\n        assert (data == 'ello W')\n        assert (object_store.size(absent_dataset) == 0)\n        assert (object_store.size(empty_dataset) == 0)\n        assert (object_store.size(hello_world_dataset) > 0)\n        percent_store_used = object_store.get_store_usage_percent()\n        assert (percent_store_used > 0.0)\n        assert (percent_store_used < 100.0)\n        output_dataset = MockDataset(4)\n        output_real_path = join(self.temp_directory, '000', 'dataset_4.dat')\n        assert (not exists(output_real_path))\n        output_working_path = self.__write(b'NEW CONTENTS', 'job_working_directory1/example_output')\n        object_store.update_from_file(output_dataset, file_name=output_working_path, create=True)\n        assert exists(output_real_path)\n        to_delete_dataset = MockDataset(5)\n        to_delete_real_path = self.__write(b'content to be deleted!', '000/dataset_5.dat')\n        assert object_store.exists(to_delete_dataset)\n        assert object_store.delete(to_delete_dataset)\n        assert (not object_store.exists(to_delete_dataset))\n        assert (not exists(to_delete_real_path))\n        complex_contents_dataset = MockDataset(6)\n        complex_content = b'{\"a\":6}'\n        self.__write(complex_content, '000/dataset_6.dat')\n        assert object_store.exists(complex_contents_dataset)\n        data = (object_store.get_data(complex_contents_dataset) == complex_content)\n", "label": 1}
{"function": "\n\ndef handle(self, request, data):\n    try:\n        usages = quotas.tenant_limit_usages(self.request)\n        availableGB = (usages['maxTotalVolumeGigabytes'] - usages['gigabytesUsed'])\n        availableVol = (usages['maxTotalVolumes'] - usages['volumesUsed'])\n        snapshot_id = None\n        image_id = None\n        volume_id = None\n        source_type = data.get('volume_source_type', None)\n        az = (data.get('availability_zone', None) or None)\n        if (data.get('snapshot_source', None) and (source_type in [None, 'snapshot_source'])):\n            snapshot = self.get_snapshot(request, data['snapshot_source'])\n            snapshot_id = snapshot.id\n            if (data['size'] < snapshot.size):\n                error_message = (_('The volume size cannot be less than the snapshot size (%sGB)') % snapshot.size)\n                raise ValidationError(error_message)\n            az = None\n        elif (data.get('image_source', None) and (source_type in [None, 'image_source'])):\n            image = self.get_image(request, data['image_source'])\n            image_id = image.id\n            image_size = functions.bytes_to_gigabytes(image.size)\n            if (data['size'] < image_size):\n                error_message = (_('The volume size cannot be less than the image size (%s)') % filesizeformat(image.size))\n                raise ValidationError(error_message)\n            properties = getattr(image, 'properties', {\n                \n            })\n            min_disk_size = (getattr(image, 'min_disk', 0) or properties.get('min_disk', 0))\n            if ((min_disk_size > 0) and (data['size'] < min_disk_size)):\n                error_message = (_('The volume size cannot be less than the image minimum disk size (%sGB)') % min_disk_size)\n                raise ValidationError(error_message)\n        elif (data.get('volume_source', None) and (source_type in [None, 'volume_source'])):\n            volume = self.get_volume(request, data['volume_source'])\n            volume_id = volume.id\n            if (data['size'] < volume.size):\n                error_message = (_('The volume size cannot be less than the source volume size (%sGB)') % volume.size)\n                raise ValidationError(error_message)\n        elif (type(data['size']) is str):\n            data['size'] = int(data['size'])\n        if (availableGB < data['size']):\n            error_message = _('A volume of %(req)iGB cannot be created as you only have %(avail)iGB of your quota available.')\n            params = {\n                'req': data['size'],\n                'avail': availableGB,\n            }\n            raise ValidationError((error_message % params))\n        elif (availableVol <= 0):\n            error_message = _('You are already using all of your available volumes.')\n            raise ValidationError(error_message)\n        metadata = {\n            \n        }\n        volume = cinder.volume_create(request, data['size'], data['name'], data['description'], data['type'], snapshot_id=snapshot_id, image_id=image_id, metadata=metadata, availability_zone=az, source_volid=volume_id)\n        message = (_('Creating volume \"%s\"') % data['name'])\n        messages.info(request, message)\n        return volume\n    except ValidationError as e:\n        self.api_error(e.messages[0])\n        return False\n    except Exception:\n        exceptions.handle(request, ignore=True)\n        self.api_error(_('Unable to create volume.'))\n        return False\n", "label": 1}
{"function": "\n\ndef _read_object(self, relpath, max_symlinks):\n    path_so_far = ''\n    components = list(relpath.split(os.path.sep))\n    symlinks = 0\n    while components:\n        component = components.pop(0)\n        if ((component == '') or (component == '.')):\n            continue\n        parent_tree = self._read_tree(path_so_far)\n        parent_path = path_so_far\n        if (path_so_far != ''):\n            path_so_far += '/'\n        path_so_far += component\n        try:\n            obj = parent_tree[component]\n        except KeyError:\n            raise self.MissingFileException(self.rev, relpath)\n        if isinstance(obj, self.File):\n            if components:\n                raise self.NotADirException(self.rev, relpath)\n            else:\n                return (obj, path_so_far)\n        elif isinstance(obj, self.Dir):\n            if (not components):\n                return (obj, (path_so_far + '/'))\n        elif isinstance(obj, self.Symlink):\n            symlinks += 1\n            if (symlinks > max_symlinks):\n                return (obj, path_so_far)\n            (object_type, path_data) = self._read_object_from_repo(sha=obj.sha)\n            assert (object_type == 'blob')\n            if (path_data[0] == '/'):\n                raise self.ExternalSymlinkException(self.rev, relpath)\n            link_to = os.path.normpath(os.path.join(parent_path, path_data))\n            if (link_to.startswith('../') or (link_to[0] == '/')):\n                raise self.ExternalSymlinkException(self.rev, relpath)\n            components = (link_to.split(SLASH) + components)\n            path_so_far = ''\n        else:\n            raise self.UnexpectedGitObjectTypeException()\n    return (self.Dir('./', None), './')\n", "label": 1}
{"function": "\n\ndef run_query(self, query):\n    from apiclient.errors import HttpError\n    from oauth2client.client import AccessTokenRefreshError\n    _check_google_client_version()\n    job_collection = self.service.jobs()\n    job_data = {\n        'configuration': {\n            'query': {\n                'query': query,\n            },\n        },\n    }\n    self._start_timer()\n    try:\n        self._print('Requesting query... ', end='')\n        query_reply = job_collection.insert(projectId=self.project_id, body=job_data).execute()\n        self._print('ok.\\nQuery running...')\n    except (AccessTokenRefreshError, ValueError):\n        if self.private_key:\n            raise AccessDenied('The service account credentials are not valid')\n        else:\n            raise AccessDenied('The credentials have been revoked or expired, please re-run the application to re-authorize')\n    except HttpError as ex:\n        self.process_http_error(ex)\n    job_reference = query_reply['jobReference']\n    while (not query_reply.get('jobComplete', False)):\n        self.print_elapsed_seconds('  Elapsed', 's. Waiting...')\n        try:\n            query_reply = job_collection.getQueryResults(projectId=job_reference['projectId'], jobId=job_reference['jobId']).execute()\n        except HttpError as ex:\n            self.process_http_error(ex)\n    if self.verbose:\n        if query_reply['cacheHit']:\n            self._print('Query done.\\nCache hit.\\n')\n        else:\n            bytes_processed = int(query_reply.get('totalBytesProcessed', '0'))\n            self._print('Query done.\\nProcessed: {}\\n'.format(self.sizeof_fmt(bytes_processed)))\n        self._print('Retrieving results...')\n    total_rows = int(query_reply['totalRows'])\n    result_pages = list()\n    seen_page_tokens = list()\n    current_row = 0\n    schema = query_reply['schema']\n    while (('rows' in query_reply) and (current_row < total_rows)):\n        page = query_reply['rows']\n        result_pages.append(page)\n        current_row += len(page)\n        self.print_elapsed_seconds('  Got page: {}; {}% done. Elapsed'.format(len(result_pages), round(((100.0 * current_row) / total_rows))))\n        if (current_row == total_rows):\n            break\n        page_token = query_reply.get('pageToken', None)\n        if ((not page_token) and (current_row < total_rows)):\n            raise InvalidPageToken('Required pageToken was missing. Received {0} of {1} rows'.format(current_row, total_rows))\n        elif (page_token in seen_page_tokens):\n            raise InvalidPageToken('A duplicate pageToken was returned')\n        seen_page_tokens.append(page_token)\n        try:\n            query_reply = job_collection.getQueryResults(projectId=job_reference['projectId'], jobId=job_reference['jobId'], pageToken=page_token).execute()\n        except HttpError as ex:\n            self.process_http_error(ex)\n    if (current_row < total_rows):\n        raise InvalidPageToken()\n    self._print('Got {} rows.\\n'.format(total_rows))\n    return (schema, result_pages)\n", "label": 1}
{"function": "\n\ndef test_has_basics():\n    f = Function('f')\n    g = Function('g')\n    p = Wild('p')\n    assert sin(x).has(x)\n    assert sin(x).has(sin)\n    assert (not sin(x).has(y))\n    assert (not sin(x).has(cos))\n    assert f(x).has(x)\n    assert f(x).has(f)\n    assert (not f(x).has(y))\n    assert (not f(x).has(g))\n    assert f(x).diff(x).has(x)\n    assert f(x).diff(x).has(f)\n    assert f(x).diff(x).has(Derivative)\n    assert (not f(x).diff(x).has(y))\n    assert (not f(x).diff(x).has(g))\n    assert (not f(x).diff(x).has(sin))\n    assert (x ** 2).has(Symbol)\n    assert (not (x ** 2).has(Wild))\n    assert (2 * p).has(Wild)\n    assert (not x.has())\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    with open('bihar_case_cleanup.csv', 'wb') as f:\n        csv_file = csv.writer(f)\n        csv_file.writerow(CaseRow.headers)\n        blank_case_ids = get_case_ids_in_domain('care-bihar', type=('', None))\n        task_case_ids = get_case_ids_in_domain('care-bihar', type='task')\n        case_ids = (set(blank_case_ids) | set(task_case_ids))\n        to_save = []\n        logger.info('Total cases to process: {}'.format(len(case_ids)))\n        for (i, doc) in enumerate(iter_docs(CommCareCase.get_db(), case_ids)):\n            case = CommCareCase.wrap(doc)\n            if (case.type and (case.type != 'task')):\n                continue\n            parent = None\n            if case.indices:\n                parent_id = case.indices[0].referenced_id\n                try:\n                    parent = CommCareCase.get(parent_id)\n                except ResourceNotFound:\n                    parent = MissingParent(get_id=parent_id, owner_id='Parent Missing')\n            case_row = CaseRow(case, parent)\n            if (case.type != 'task'):\n                if (case.user_id == MOTECH_ID):\n                    case_row.update_type('task')\n            if (parent and (not isinstance(parent, MissingParent)) and (parent.owner_id != case.owner_id)):\n                case_row.update_owner(parent.owner_id)\n            if case_row.save:\n                csv_file.writerow(case_row.to_row())\n                to_save.append(case_row.case)\n            if (len(to_save) > 100):\n                CommCareCase.get_db().bulk_save(to_save)\n                to_save = []\n            if ((i % 100) == 0):\n                logger.info('{current}/{count} cases completed'.format(current=i, count=len(case_ids)))\n        if to_save:\n            CommCareCase.get_db().bulk_save(to_save)\n", "label": 1}
{"function": "\n\n@api.doc(params={\n    'server': 'Which server to collect data from when in multi-agent mode',\n    'name': 'Client name',\n    'backup': 'Backup number',\n}, responses={\n    200: 'Success',\n    400: 'Missing parameter',\n    403: 'Insufficient permissions',\n    500: 'Internal failure',\n}, parser=parser)\ndef post(self, server=None, name=None, backup=None):\n    'Performs an online restoration\\n\\n        **POST** method provided by the webservice.\\n        This method returns a :mod:`flask.Response` object.\\n\\n        :param server: Which server to collect data from when in multi-agent mode\\n        :type server: str\\n\\n        :param name: The client we are working on\\n        :type name: str\\n\\n        :param backup: The backup we are working on\\n        :type backup: int\\n\\n        :returns: A :mod:`flask.Response` object representing an archive of the restored files\\n        '\n    args = self.parser.parse_args()\n    l = args['list']\n    s = args['strip']\n    f = (args['format'] or 'zip')\n    p = args['pass']\n    resp = None\n    if ((not l) or (not name) or (not backup)):\n        api.abort(400, 'missing arguments')\n    if (api.bui.acl and ((not api.bui.acl.is_client_allowed(current_user.get_id(), name, server)) and (not api.bui.acl.is_admin(current_user.get_id())))):\n        api.abort(403, 'You are not allowed to perform a restoration for this client')\n    if server:\n        filename = ('restoration_%d_%s_on_%s_at_%s.%s' % (backup, name, server, strftime('%Y-%m-%d_%H_%M_%S', gmtime()), f))\n    else:\n        filename = ('restoration_%d_%s_at_%s.%s' % (backup, name, strftime('%Y-%m-%d_%H_%M_%S', gmtime()), f))\n    if (not server):\n        (archive, err) = api.bui.cli.restore_files(name, backup, l, s, f, p)\n        if (not archive):\n            if err:\n                return make_response(err, 500)\n            api.abort(500)\n        try:\n            fh = open(archive, 'r')\n\n            @after_this_request\n            def remove_file(response):\n                'Callback function to run after the client has handled\\n                    the request to remove temporary files.\\n                    '\n                import os\n                os.remove(archive)\n                return response\n            resp = send_file(fh, as_attachment=True, attachment_filename=filename, mimetype='application/zip')\n            resp.set_cookie('fileDownload', 'true')\n        except Exception as e:\n            api.bui.cli._logger('error', str(e))\n            api.abort(500, str(e))\n    else:\n        socket = None\n        try:\n            (socket, length, err) = api.bui.cli.restore_files(name, backup, l, s, f, p, server)\n            api.bui.cli._logger('debug', 'Need to get {} Bytes : {}'.format(length, socket))\n            if err:\n                api.bui.cli._logger('debug', 'Something went wrong: {}'.format(err))\n                socket.sendall(struct.pack('!Q', 2))\n                socket.sendall(b'RE')\n                socket.close()\n                return make_response(err, 500)\n\n            def stream_file(sock, l):\n                'The restoration took place on another server so we need\\n                    to stream the file that is not present on the current\\n                    machine.\\n                    '\n                bsize = 1024\n                received = 0\n                if (l < bsize):\n                    bsize = l\n                while (received < l):\n                    buf = b''\n                    (r, _, _) = select.select([sock], [], [], 5)\n                    if (not r):\n                        raise Exception('Socket timed-out')\n                    buf += sock.recv(bsize)\n                    if (not buf):\n                        continue\n                    received += len(buf)\n                    api.bui.cli._logger('debug', '{}/{}'.format(received, l))\n                    (yield buf)\n                sock.sendall(struct.pack('!Q', 2))\n                sock.sendall(b'RE')\n                sock.close()\n            headers = Headers()\n            headers.add('Content-Disposition', 'attachment', filename=filename)\n            headers['Content-Length'] = length\n            resp = Response(stream_file(socket, length), mimetype='application/zip', headers=headers, direct_passthrough=True)\n            resp.set_cookie('fileDownload', 'true')\n            resp.set_etag(('flask-%s-%s-%s' % (time(), length, (adler32(filename.encode('utf-8')) & 4294967295))))\n        except HTTPException as e:\n            raise e\n        except Exception as e:\n            api.bui.cli._logger('error', str(e))\n            api.abort(500, str(e))\n    return resp\n", "label": 1}
{"function": "\n\ndef __exit__(self, exc_type, exc_value, traceback):\n    connection = get_connection(self.using)\n    if connection.savepoint_ids:\n        sid = connection.savepoint_ids.pop()\n    else:\n        connection.in_atomic_block = False\n    try:\n        if connection.closed_in_transaction:\n            pass\n        elif ((exc_type is None) and (not connection.needs_rollback)):\n            if connection.in_atomic_block:\n                if (sid is not None):\n                    try:\n                        connection.savepoint_commit(sid)\n                    except DatabaseError:\n                        try:\n                            connection.savepoint_rollback(sid)\n                            connection.savepoint_commit(sid)\n                        except Error:\n                            connection.needs_rollback = True\n                        raise\n            else:\n                try:\n                    connection.commit()\n                except DatabaseError:\n                    try:\n                        connection.rollback()\n                    except Error:\n                        connection.close()\n                    raise\n        else:\n            connection.needs_rollback = False\n            if connection.in_atomic_block:\n                if (sid is None):\n                    connection.needs_rollback = True\n                else:\n                    try:\n                        connection.savepoint_rollback(sid)\n                        connection.savepoint_commit(sid)\n                    except Error:\n                        connection.needs_rollback = True\n            else:\n                try:\n                    connection.rollback()\n                except Error:\n                    connection.close()\n    finally:\n        if (not connection.in_atomic_block):\n            if connection.closed_in_transaction:\n                connection.connection = None\n            else:\n                connection.set_autocommit(True)\n        elif ((not connection.savepoint_ids) and (not connection.commit_on_exit)):\n            if connection.closed_in_transaction:\n                connection.connection = None\n            else:\n                connection.in_atomic_block = False\n", "label": 1}
{"function": "\n\ndef get_children(self, obj):\n    'Returns the child objects of a particular tvtk object in a\\n        dictionary, the keys are the trait names.  This is used to\\n        generate the tree in the browser.'\n    vtk_obj = tvtk.to_vtk(obj)\n    methods = self._get_methods(vtk_obj)\n    kids = {\n        \n    }\n\n    def _add_kid(key, x):\n        if (x is None):\n            kids[key] = None\n        elif (type(x) in (type([]), type(()))):\n            x1 = [i for i in x if isinstance(i, TVTKBase)]\n            if x1:\n                kids[key] = x1\n        elif isinstance(x, TVTKBase):\n            if hasattr(x, '__iter__'):\n                if (len(list(x)) and isinstance(list(x)[0], TVTKBase)):\n                    kids[key] = x\n            else:\n                kids[key] = x\n    for method in methods:\n        attr = camel2enthought(method[0])\n        if hasattr(obj, attr):\n            _add_kid(attr, getattr(obj, attr))\n    if hasattr(obj, 'number_of_sources'):\n        srcs = [obj.get_source(i) for i in range(obj.number_of_sources)]\n        _add_kid('source', srcs)\n    elif hasattr(obj, 'source'):\n        _add_kid('source', obj.source)\n    if hasattr(obj, 'get_input'):\n        inputs = []\n        if hasattr(obj, 'number_of_input_ports'):\n            if obj.number_of_input_ports:\n                inputs = list()\n                for i in range(obj.number_of_input_ports):\n                    try:\n                        inputs.append(obj.get_input(i))\n                    except TypeError:\n                        pass\n                if (not inputs):\n                    inputs = [obj.get_input()]\n        else:\n            inputs = [obj.get_input(i) for i in range(obj.number_of_inputs)]\n        _add_kid('input', inputs)\n    elif hasattr(obj, 'input'):\n        _add_kid('input', obj.input)\n    if hasattr(obj, 'producer_port'):\n        _add_kid('producer_port', obj.producer_port)\n    return kids\n", "label": 1}
{"function": "\n\ndef diff():\n    description = '\\n    Produce a diffed IPython Notebook from before and after notebooks.\\n\\n    If no arguments are given, nbdiff looks for modified notebook files in\\n    the version control system.\\n\\n    The resulting diff is presented to the user in the browser at\\n    http://localhost:5000.\\n    '\n    usage = ('nbdiff [-h] [--check] [--debug] ' + '[--browser=<browser] [before after]')\n    parser = argparse.ArgumentParser(description=description, usage=usage)\n    parser.add_argument('--browser', '-b', default=None, help='Browser to launch nbdiff/nbmerge in')\n    parser.add_argument('--check', '-c', action='store_true', default=False, help='Run nbdiff algorithm but do not display the result.')\n    parser.add_argument('--debug', '-d', action='store_true', default=False, help='Pass debug=True to the Flask server to ease debugging.')\n    parser.add_argument('before', nargs='?', help='The notebook to diff against.')\n    parser.add_argument('after', nargs='?', help='The notebook to compare `before` to.')\n    args = parser.parse_args()\n    parser = NotebookParser()\n    if (args.before and args.after):\n        invalid_notebooks = []\n        try:\n            notebook1 = parser.parse(open(args.before))\n        except NotJSONError:\n            invalid_notebooks.append(args.before)\n        try:\n            notebook2 = parser.parse(open(args.after))\n        except NotJSONError:\n            invalid_notebooks.append(args.after)\n        if (len(invalid_notebooks) == 0):\n            result = notebook_diff(notebook1, notebook2)\n            filename_placeholder = '{} and {}'.format(args.before, args.after)\n            app.add_notebook(result, filename_placeholder)\n            if (not args.check):\n                open_browser(args.browser)\n                app.run(debug=args.debug)\n        else:\n            print('The notebooks could not be diffed.')\n            print((('There was a problem parsing the following notebook ' + 'files:\\n') + '\\n'.join(invalid_notebooks)))\n            return (- 1)\n    elif (not (args.before or args.after)):\n        try:\n            vcs = HgAdapter()\n        except NoVCSError as hg_err:\n            try:\n                vcs = GitAdapter()\n            except NoVCSError:\n                print(hg_err.value)\n                sys.exit((- 1))\n        modified_notebooks = vcs.get_modified_notebooks()\n        if (not (len(modified_notebooks) == 0)):\n            invalid_notebooks = []\n            for nbook in modified_notebooks:\n                try:\n                    filename = nbook[2]\n                    current_notebook = parser.parse(nbook[0])\n                    head_version = parser.parse(nbook[1])\n                    result = notebook_diff(head_version, current_notebook)\n                    app.add_notebook(result, filename)\n                except NotJSONError:\n                    invalid_notebooks.append(filename)\n            if (len(invalid_notebooks) > 0):\n                print((('There was a problem parsing the following notebook ' + 'files:\\n') + '\\n'.join(invalid_notebooks)))\n            if (len(modified_notebooks) == len(invalid_notebooks)):\n                print('There are no valid notebooks to diff.')\n                return (- 1)\n            if (not args.check):\n                open_browser(args.browser)\n                app.run(debug=False)\n        else:\n            print('No modified files to diff.')\n            return 0\n    else:\n        print('Invalid number of arguments. Run nbdiff --help')\n        return (- 1)\n", "label": 1}
{"function": "\n\ndef general_toposort(r_out, deps, debug_print=False, compute_deps_cache=None, deps_cache=None, clients=None):\n    '\\n    WRITEME\\n\\n    Parameters\\n    ----------\\n    deps\\n        A python function that takes a node as input and returns its dependence.\\n    compute_deps_cache : optional\\n        If provided deps_cache should also be provided. This is a function like\\n        deps, but that also cache its results in a dict passed as deps_cache.\\n    deps_cache : dict\\n        Must be used with compute_deps_cache.\\n    clients : dict\\n        If a dict is passed it will be filled with a mapping of node\\n        -> clients for each node in the subgraph.\\n\\n    Notes\\n    -----\\n        deps(i) should behave like a pure function (no funny business with\\n        internal state).\\n\\n        deps(i) will be cached by this function (to be fast).\\n\\n        The order of the return value list is determined by the order of nodes\\n        returned by the deps() function.\\n\\n        deps should be provided or can be None and the caller provides\\n        compute_deps_cache and deps_cache. The second option removes a Python\\n        function call, and allows for more specialized code, so it can be\\n        faster.\\n\\n    '\n    if (compute_deps_cache is None):\n        deps_cache = {\n            \n        }\n\n        def compute_deps_cache(io):\n            if (io not in deps_cache):\n                d = deps(io)\n                if d:\n                    if (not isinstance(d, (list, OrderedSet))):\n                        raise TypeError('Non-deterministic collections here make toposort non-deterministic.')\n                    deps_cache[io] = list(d)\n                else:\n                    deps_cache[io] = d\n                return d\n            else:\n                return deps_cache[io]\n    assert (deps_cache is not None)\n    assert isinstance(r_out, (tuple, list, deque))\n    (reachable, _clients) = stack_search(deque(r_out), compute_deps_cache, 'dfs', True)\n    if (clients is not None):\n        clients.update(_clients)\n    sources = deque([r for r in reachable if (not deps_cache.get(r, None))])\n    rset = set()\n    rlist = []\n    while sources:\n        node = sources.popleft()\n        if (node not in rset):\n            rlist.append(node)\n            rset.add(node)\n            for client in _clients.get(node, []):\n                deps_cache[client] = [a for a in deps_cache[client] if (a is not node)]\n                if (not deps_cache[client]):\n                    sources.append(client)\n    if (len(rlist) != len(reachable)):\n        if debug_print:\n            print('')\n            print(reachable)\n            print(rlist)\n        raise ValueError('graph contains cycles')\n    return rlist\n", "label": 1}
{"function": "\n\ndef run_one_entry_point(job_id, function, input_hash, run_spec, depends_on, name=None):\n    \"\\n    :param job_id: job ID of the local job to run\\n    :type job_id: string\\n    :param function: function to run\\n    :type function: string\\n    :param input_hash: input for the job (may include job-based object references)\\n    :type input_hash: dict\\n    :param run_spec: run specification from the dxapp.json of the app\\n    :type run_spec: dict\\n\\n    Runs the specified entry point and retrieves the job's output,\\n    updating job_outputs.json (in $DX_TEST_JOB_HOMEDIRS) appropriately.\\n    \"\n    print('======')\n    job_homedir = os.path.join(environ['DX_TEST_JOB_HOMEDIRS'], job_id)\n    job_env = environ.copy()\n    job_env['HOME'] = os.path.join(environ['DX_TEST_JOB_HOMEDIRS'], job_id)\n    all_job_outputs_path = os.path.join(environ['DX_TEST_JOB_HOMEDIRS'], 'job_outputs.json')\n    with open(all_job_outputs_path, 'r') as fd:\n        all_job_outputs = json.load(fd, object_pairs_hook=collections.OrderedDict)\n    if isinstance(name, basestring):\n        name += ((((' (' + job_id) + ':') + function) + ')')\n    else:\n        name = ((job_id + ':') + function)\n    job_name = (((BLUE() + BOLD()) + name) + ENDC())\n    print(job_name)\n    try:\n        resolve_job_references(input_hash, all_job_outputs)\n    except Exception as e:\n        exit_with_error(((((job_name + ' ') + JOB_STATES('failed')) + ' when resolving input:\\n') + fill(str(e))))\n    if (depends_on is None):\n        depends_on = []\n    get_implicit_depends_on(input_hash, depends_on)\n    try:\n        wait_for_depends_on(depends_on, all_job_outputs)\n    except Exception as e:\n        exit_with_error(((((job_name + ' ') + JOB_STATES('failed')) + ' when processing depends_on:\\n') + fill(str(e))))\n    with open(os.path.join(job_homedir, 'job_input.json'), 'wb') as fd:\n        json.dump(input_hash, fd, indent=4)\n        fd.write(b'\\n')\n    print(job_output_to_str(input_hash, title=((BOLD() + 'Input: ') + ENDC()), title_len=len('Input: ')).lstrip())\n    if (run_spec['interpreter'] == 'bash'):\n        env_path = os.path.join(job_homedir, 'environment')\n        with open(env_path, 'w') as fd:\n            job_input_file = os.path.join(job_homedir, 'job_input.json')\n            var_defs_hash = file_load_utils.gen_bash_vars(job_input_file, job_homedir=job_homedir)\n            for (key, val) in var_defs_hash.iteritems():\n                fd.write('{}={}\\n'.format(key, val))\n    print(((BOLD() + 'Logs:') + ENDC()))\n    start_time = datetime.datetime.now()\n    if (run_spec['interpreter'] == 'bash'):\n        script = '\\n          cd {homedir};\\n          . {env_path};\\n          . {code_path};\\n          if [[ $(type -t {function}) == \"function\" ]];\\n          then {function};\\n          else echo \"$0: Global scope execution complete. Not invoking entry point function {function} because it was not found\" 1>&2;\\n          fi'.format(homedir=pipes.quote(job_homedir), env_path=pipes.quote(os.path.join(job_env['HOME'], 'environment')), code_path=pipes.quote(environ['DX_TEST_CODE_PATH']), function=function)\n        invocation_args = ((['bash', '-c', '-e'] + (['-x'] if environ.get('DX_TEST_X_FLAG') else [])) + [script])\n    elif (run_spec['interpreter'] == 'python2.7'):\n        script = '#!/usr/bin/env python\\nimport os\\nos.chdir({homedir})\\n\\n{code}\\n\\nimport dxpy, json\\nif dxpy.utils.exec_utils.RUN_COUNT == 0:\\n    dxpy.run()\\n'.format(homedir=repr(job_homedir), code=run_spec['code'])\n        job_env['DX_TEST_FUNCTION'] = function\n        invocation_args = ['python', '-c', script]\n    if USING_PYTHON2:\n        invocation_args = [arg.encode(sys.stdout.encoding) for arg in invocation_args]\n        env = {k: v.encode(sys.stdout.encoding) for (k, v) in job_env.items()}\n    else:\n        env = job_env\n    fn_process = subprocess.Popen(invocation_args, env=env)\n    fn_process.communicate()\n    end_time = datetime.datetime.now()\n    if (fn_process.returncode != 0):\n        exit_with_error(((((((job_name + ' ') + JOB_STATES('failed')) + ', exited with error code ') + str(fn_process.returncode)) + ' after ') + str((end_time - start_time))))\n    job_output_path = os.path.join(job_env['HOME'], 'job_output.json')\n    if os.path.exists(job_output_path):\n        try:\n            with open(job_output_path, 'r') as fd:\n                job_output = json.load(fd, object_pairs_hook=collections.OrderedDict)\n        except Exception as e:\n            exit_with_error(((('Error: Could not load output of ' + job_name) + ':\\n') + fill(((str(e.__class__) + ': ') + str(e)))))\n    else:\n        job_output = {\n            \n        }\n    print(((((((job_name + ' -> ') + GREEN()) + 'finished running') + ENDC()) + ' after ') + str((end_time - start_time))))\n    print(job_output_to_str(job_output, title=((BOLD() + 'Output: ') + ENDC()), title_len=len('Output: ')).lstrip())\n    with open(os.path.join(environ['DX_TEST_JOB_HOMEDIRS'], 'job_outputs.json'), 'r') as fd:\n        all_job_outputs = json.load(fd, object_pairs_hook=collections.OrderedDict)\n    all_job_outputs[job_id] = job_output\n    for other_job_id in all_job_outputs:\n        if (all_job_outputs[other_job_id] is None):\n            continue\n        resolve_job_references(all_job_outputs[other_job_id], all_job_outputs, should_resolve=False)\n    with open(os.path.join(environ['DX_TEST_JOB_HOMEDIRS'], 'job_outputs.json'), 'wb') as fd:\n        json.dump(all_job_outputs, fd, indent=4)\n        fd.write(b'\\n')\n", "label": 1}
{"function": "\n\ndef do_spsso_descriptor(conf, cert=None):\n    spsso = md.SPSSODescriptor()\n    spsso.protocol_support_enumeration = samlp.NAMESPACE\n    exts = conf.getattr('extensions', 'sp')\n    if exts:\n        if (spsso.extensions is None):\n            spsso.extensions = md.Extensions()\n        for (key, val) in exts.items():\n            _ext = do_extensions(key, val)\n            if _ext:\n                for _e in _ext:\n                    spsso.extensions.add_extension_element(_e)\n    endps = conf.getattr('endpoints', 'sp')\n    if endps:\n        for (endpoint, instlist) in do_endpoints(endps, ENDPOINTS['sp']).items():\n            setattr(spsso, endpoint, instlist)\n    ext = do_endpoints(endps, ENDPOINT_EXT['sp'])\n    if ext:\n        if (spsso.extensions is None):\n            spsso.extensions = md.Extensions()\n        for vals in ext.values():\n            for val in vals:\n                spsso.extensions.add_extension_element(val)\n    ui_info = conf.getattr('ui_info', 'sp')\n    if ui_info:\n        if (spsso.extensions is None):\n            spsso.extensions = md.Extensions()\n        spsso.extensions.add_extension_element(do_uiinfo(ui_info))\n    if cert:\n        encryption_type = conf.encryption_type\n        spsso.key_descriptor = do_key_descriptor(cert, encryption_type)\n    for key in ['want_assertions_signed', 'authn_requests_signed']:\n        try:\n            val = conf.getattr(key, 'sp')\n            if (val is None):\n                setattr(spsso, key, DEFAULT[key])\n            else:\n                strval = '{0:>s}'.format(str(val))\n                setattr(spsso, key, strval.lower())\n        except KeyError:\n            setattr(spsso, key, DEFAULTS[key])\n    do_attribute_consuming_service(conf, spsso)\n    _do_nameid_format(spsso, conf, 'sp')\n    return spsso\n", "label": 1}
{"function": "\n\ndef monitor(run_once=False, broker=None):\n    if (not broker):\n        broker = get_broker()\n    term = Terminal()\n    broker.ping()\n    with term.fullscreen(), term.hidden_cursor(), term.cbreak():\n        val = None\n        start_width = int((term.width / 8))\n        while (val not in ('q', 'Q')):\n            col_width = int((term.width / 8))\n            if (col_width != start_width):\n                print(term.clear())\n                start_width = col_width\n            print((term.move(0, 0) + term.black_on_green(term.center(_('Host'), width=(col_width - 1)))))\n            print((term.move(0, (1 * col_width)) + term.black_on_green(term.center(_('Id'), width=(col_width - 1)))))\n            print((term.move(0, (2 * col_width)) + term.black_on_green(term.center(_('State'), width=(col_width - 1)))))\n            print((term.move(0, (3 * col_width)) + term.black_on_green(term.center(_('Pool'), width=(col_width - 1)))))\n            print((term.move(0, (4 * col_width)) + term.black_on_green(term.center(_('TQ'), width=(col_width - 1)))))\n            print((term.move(0, (5 * col_width)) + term.black_on_green(term.center(_('RQ'), width=(col_width - 1)))))\n            print((term.move(0, (6 * col_width)) + term.black_on_green(term.center(_('RC'), width=(col_width - 1)))))\n            print((term.move(0, (7 * col_width)) + term.black_on_green(term.center(_('Up'), width=(col_width - 1)))))\n            i = 2\n            stats = Stat.get_all(broker=broker)\n            print(term.clear_eos())\n            for stat in stats:\n                status = stat.status\n                if (stat.status == Conf.WORKING):\n                    status = term.green(str(Conf.WORKING))\n                elif (stat.status == Conf.STOPPING):\n                    status = term.yellow(str(Conf.STOPPING))\n                elif (stat.status == Conf.STOPPED):\n                    status = term.red(str(Conf.STOPPED))\n                elif (stat.status == Conf.IDLE):\n                    status = str(Conf.IDLE)\n                tasks = str(stat.task_q_size)\n                if (stat.task_q_size > 0):\n                    tasks = term.cyan(str(stat.task_q_size))\n                    if (Conf.QUEUE_LIMIT and (stat.task_q_size == Conf.QUEUE_LIMIT)):\n                        tasks = term.green(str(stat.task_q_size))\n                results = stat.done_q_size\n                if (results > 0):\n                    results = term.cyan(str(results))\n                workers = len(stat.workers)\n                if (workers < Conf.WORKERS):\n                    workers = term.yellow(str(workers))\n                uptime = (timezone.now() - stat.tob).total_seconds()\n                (hours, remainder) = divmod(uptime, 3600)\n                (minutes, seconds) = divmod(remainder, 60)\n                uptime = ('%d:%02d:%02d' % (hours, minutes, seconds))\n                print((term.move(i, 0) + term.center(stat.host[:(col_width - 1)], width=(col_width - 1))))\n                print((term.move(i, (1 * col_width)) + term.center(stat.cluster_id, width=(col_width - 1))))\n                print((term.move(i, (2 * col_width)) + term.center(status, width=(col_width - 1))))\n                print((term.move(i, (3 * col_width)) + term.center(workers, width=(col_width - 1))))\n                print((term.move(i, (4 * col_width)) + term.center(tasks, width=(col_width - 1))))\n                print((term.move(i, (5 * col_width)) + term.center(results, width=(col_width - 1))))\n                print((term.move(i, (6 * col_width)) + term.center(stat.reincarnations, width=(col_width - 1))))\n                print((term.move(i, (7 * col_width)) + term.center(uptime, width=(col_width - 1))))\n                i += 1\n            i += 1\n            queue_size = broker.queue_size()\n            lock_size = broker.lock_size()\n            if lock_size:\n                queue_size = '{}({})'.format(queue_size, lock_size)\n            print((term.move(i, 0) + term.white_on_cyan(term.center(broker.info(), width=(col_width * 2)))))\n            print((term.move(i, (2 * col_width)) + term.black_on_cyan(term.center(_('Queued'), width=col_width))))\n            print((term.move(i, (3 * col_width)) + term.white_on_cyan(term.center(queue_size, width=col_width))))\n            print((term.move(i, (4 * col_width)) + term.black_on_cyan(term.center(_('Success'), width=col_width))))\n            print((term.move(i, (5 * col_width)) + term.white_on_cyan(term.center(models.Success.objects.count(), width=col_width))))\n            print((term.move(i, (6 * col_width)) + term.black_on_cyan(term.center(_('Failures'), width=col_width))))\n            print((term.move(i, (7 * col_width)) + term.white_on_cyan(term.center(models.Failure.objects.count(), width=col_width))))\n            if run_once:\n                return Stat.get_all(broker=broker)\n            print((term.move((i + 2), 0) + term.center(_('[Press q to quit]'))))\n            val = term.inkey(timeout=1)\n", "label": 1}
{"function": "\n\ndef set_cluster_package_config(self, cluster_id):\n    'set cluster package config.'\n    package_config = {\n        'security': {\n            \n        },\n    }\n    service_credentials = [service_credential for service_credential in CONF.service_credentials.split(',') if service_credential]\n    service_credential_cfg = {\n        \n    }\n    LOG.info('service credentials: %s', service_credentials)\n    for service_credential in service_credentials:\n        if (':' not in service_credential):\n            raise Exception(('no : in service credential %s' % service_credential))\n        (service_name, service_pair) = service_credential.split(':', 1)\n        if ('=' not in service_pair):\n            raise Exception(('there is no = in service %s security' % service_name))\n        (username, password) = service_pair.split('=', 1)\n        service_credential_cfg[service_name] = {\n            'username': username,\n            'password': password,\n        }\n    console_credentials = [console_credential for console_credential in CONF.console_credentials.split(',') if console_credential]\n    LOG.info('console credentials: %s', console_credentials)\n    console_credential_cfg = {\n        \n    }\n    for console_credential in console_credentials:\n        if (':' not in console_credential):\n            raise Exception(('there is no : in console credential %s' % console_credential))\n        (console_name, console_pair) = console_credential.split(':', 1)\n        if ('=' not in console_pair):\n            raise Exception(('there is no = in console %s security' % console_name))\n        (username, password) = console_pair.split('=', 1)\n        console_credential_cfg[console_name] = {\n            'username': username,\n            'password': password,\n        }\n    package_config['security'] = {\n        'service_credentials': service_credential_cfg,\n        'console_credentials': console_credential_cfg,\n    }\n    network_mapping = dict([network_pair.split('=', 1) for network_pair in CONF.network_mapping.split(',') if ('=' in network_pair)])\n    package_config['network_mapping'] = network_mapping\n    assert os.path.exists(CONF.network_cfg)\n    network_cfg = yaml.load(open(CONF.network_cfg))\n    package_config['network_cfg'] = network_cfg\n    assert os.path.exists(CONF.neutron_cfg)\n    neutron_cfg = yaml.load(open(CONF.neutron_cfg))\n    package_config['neutron_config'] = neutron_cfg\n    '\\n        package_config_filename = CONF.package_config_json_file\\n        if package_config_filename:\\n            util.merge_dict(\\n                package_config, _load_config(package_config_filename)\\n            )\\n        '\n    package_config['ha_proxy'] = {\n        \n    }\n    if CONF.cluster_vip:\n        package_config['ha_proxy']['vip'] = CONF.cluster_vip\n    package_config['enable_secgroup'] = (CONF.enable_secgroup == 'true')\n    (status, resp) = self.client.update_cluster_config(cluster_id, package_config=package_config)\n    LOG.info('set package config %s to cluster %s status: %s, resp: %s', package_config, cluster_id, status, resp)\n    if (not self.is_ok(status)):\n        raise RuntimeError('set cluster package_config failed')\n", "label": 1}
{"function": "\n\ndef _get_type(dtype):\n    \"\\n  Defensively try to extract a scalar type from any wacky thing\\n  that might get passed in as a 'dtype' to an array constructor\\n  \"\n    if isinstance(dtype, macro):\n        dtype = dtype.as_fn()\n    while isinstance(dtype, jit):\n        dtype = dtype.f\n    while isinstance(dtype, Expr):\n        if isinstance(dtype, UntypedFn):\n            if (len(dtype.body) == 1):\n                stmt = dtype.body[0]\n                if (stmt.__class__ is Return):\n                    expr = stmt.value\n                    if (expr.__class__ is Cast):\n                        dtype = expr.type\n                        break\n            assert False, (\"Don't know how to convert function %s into Parakeet type\" % dtype)\n        elif isinstance(dtype, TypedFn):\n            dtype = dtype.return_type\n        if isinstance(dtype, TypeValue):\n            dtype = dtype.type_value\n        elif isinstance(dtype.type, TypeValueT):\n            dtype = dtype.type\n        elif isinstance(dtype, Closure):\n            dtype = dtype.fn\n        elif isinstance(dtype.type, (ClosureT, FnT)):\n            dtype = dtype.type.fn\n        else:\n            assert False, (\"Don't know how to turn %s : %s into Parakeet type\" % (dtype, dtype.type))\n    if isinstance(dtype, Type):\n        if isinstance(dtype, TypeValueT):\n            return dtype.type\n        else:\n            return dtype\n    elif isinstance(dtype, (np.dtype, type)):\n        return type_conv.equiv_type(dtype)\n    elif isinstance(dtype, str):\n        return type_conv.equiv_type(np.dtype(dtype))\n    assert False, (\"Don't know how to turn %s into Parakeet type\" % dtype)\n", "label": 1}
{"function": "\n\ndef cached_request(self, request):\n    '\\n        Return a cached response if it exists in the cache, otherwise\\n        return False.\\n        '\n    cache_url = self.cache_url(request.url)\n    logger.debug('Looking up \"%s\" in the cache', cache_url)\n    cc = self.parse_cache_control(request.headers)\n    if ('no-cache' in cc):\n        logger.debug('Request header has \"no-cache\", cache bypassed')\n        return False\n    if (('max-age' in cc) and (cc['max-age'] == 0)):\n        logger.debug('Request header has \"max_age\" as 0, cache bypassed')\n        return False\n    cache_data = self.cache.get(cache_url)\n    if (cache_data is None):\n        logger.debug('No cache entry available')\n        return False\n    resp = self.serializer.loads(request, cache_data)\n    if (not resp):\n        logger.warning('Cache entry deserialization failed, entry ignored')\n        return False\n    if (resp.status == 301):\n        msg = 'Returning cached \"301 Moved Permanently\" response (ignoring date and etag information)'\n        logger.debug(msg)\n        return resp\n    headers = CaseInsensitiveDict(resp.headers)\n    if ((not headers) or ('date' not in headers)):\n        if ('etag' not in headers):\n            logger.debug('Purging cached response: no date or etag')\n            self.cache.delete(cache_url)\n        logger.debug('Ignoring cached response: no date')\n        return False\n    now = time.time()\n    date = calendar.timegm(parsedate_tz(headers['date']))\n    current_age = max(0, (now - date))\n    logger.debug('Current age based on date: %i', current_age)\n    resp_cc = self.parse_cache_control(headers)\n    freshness_lifetime = 0\n    if (('max-age' in resp_cc) and resp_cc['max-age'].isdigit()):\n        freshness_lifetime = int(resp_cc['max-age'])\n        logger.debug('Freshness lifetime from max-age: %i', freshness_lifetime)\n    elif ('expires' in headers):\n        expires = parsedate_tz(headers['expires'])\n        if (expires is not None):\n            expire_time = (calendar.timegm(expires) - date)\n            freshness_lifetime = max(0, expire_time)\n            logger.debug('Freshness lifetime from expires: %i', freshness_lifetime)\n    if ('max-age' in cc):\n        try:\n            freshness_lifetime = int(cc['max-age'])\n            logger.debug('Freshness lifetime from request max-age: %i', freshness_lifetime)\n        except ValueError:\n            freshness_lifetime = 0\n    if ('min-fresh' in cc):\n        try:\n            min_fresh = int(cc['min-fresh'])\n        except ValueError:\n            min_fresh = 0\n        current_age += min_fresh\n        logger.debug('Adjusted current age from min-fresh: %i', current_age)\n    if (freshness_lifetime > current_age):\n        logger.debug('The response is \"fresh\", returning cached response')\n        logger.debug('%i > %i', freshness_lifetime, current_age)\n        return resp\n    if ('etag' not in headers):\n        logger.debug('The cached response is \"stale\" with no etag, purging')\n        self.cache.delete(cache_url)\n    return False\n", "label": 1}
{"function": "\n\ndef push(self, task):\n    '\\n            Extract new updates from the local database and send\\n            them to the peer repository (active push)\\n\\n            @param task: the synchronization task (sync_task Row)\\n\\n            @return: tuple (error, mtime), with error=None if successful,\\n                     else error=message, and mtime=modification timestamp\\n                     of the youngest record sent\\n        '\n    repository = self.repository\n    resource_name = task.resource_name\n    log = repository.log\n    remote = False\n    output = None\n    current.log.debug(('S3SyncRepository.push(%s, %s)' % (repository.url, resource_name)))\n    resource = current.s3db.resource(resource_name)\n    filters = current.sync.get_filters(task.id)\n    table = resource.table\n    tablename = resource.tablename\n    if filters:\n        queries = S3URLQuery.parse(resource, filters[tablename])\n        [resource.add_filter(q) for a in queries for q in queries[a]]\n    msince = task.last_push\n    if msince:\n        strategy = task.strategy\n        created = ('create' in strategy)\n        updated = ('update' in strategy)\n        if (created and updated):\n            mtime_filter = (table.modified_on > msince)\n        elif created:\n            mtime_filter = (table.created_on > msince)\n        elif updated:\n            mtime_filter = ((table.created_on <= msince) & (table.modified_on > msince))\n        else:\n            mtime_filter = None\n        if mtime_filter:\n            resource.add_filter(mtime_filter)\n    mtime = resource.muntil\n    resource_ids = resource.get_id()\n    if (resource_ids is None):\n        message = 'No Changes since last push'\n        result = log.WARNING\n    else:\n        settings = current.deployment_settings\n        placeholders = {\n            'systemname': settings.get_system_name(),\n            'systemname_short': settings.get_system_name_short(),\n            'resource': resource_name,\n            'public_url': settings.get_base_public_url(),\n        }\n        from string import Template\n        filename = resource.get_config('upload_filename')\n        if (not filename):\n            filename = settings.get_sync_upload_filename()\n        filename = Template(filename).safe_substitute(s='%(systemname_short)s', r='%(resource)s')\n        filename = (filename % placeholders)\n        representation = task.representation\n        filename = ('%s.%s' % (filename, representation))\n        remote = True\n        import ftplib\n        ftp_connection = self.ftp_connection\n        if task.multiple_file:\n            if (type(resource_ids) is not list):\n                resource_ids = [resource_ids]\n            for resource_id in resource_ids:\n                resource.clear_query()\n                resource.add_filter((FS('id') == resource_id))\n                data = self._get_data(resource, representation)\n                try:\n                    ftp_connection.storbinary(('STOR %s' % filename), StringIO(data))\n                except ftplib.error_perm:\n                    message = sys.exc_info()[1]\n                    result = log.ERROR\n                    output = message\n                else:\n                    message = 'FTP Transfer Successful'\n                    result = log.SUCCESS\n                current.log.debug(message)\n        else:\n            data = self._get_data(resource, representation)\n            try:\n                ftp_connection.storbinary(('STOR %s' % filename), StringIO(data))\n            except ftplib.error_perm:\n                message = sys.exc_info()[1]\n                result = log.ERROR\n                output = message\n            else:\n                message = 'FTP Transfer Successful'\n                result = log.SUCCESS\n            current.log.debug(message)\n        ftp_connection.quit()\n    log.write(repository_id=repository.id, resource_name=resource_name, transmission=log.OUT, mode=log.PUSH, action='send', remote=remote, result=result, message=message)\n    if (output is not None):\n        mtime = None\n    return (output, mtime)\n", "label": 1}
{"function": "\n\ndef test_relational_assumptions():\n    from sympy import Lt, Gt, Le, Ge\n    m1 = Symbol('m1', nonnegative=False)\n    m2 = Symbol('m2', positive=False)\n    m3 = Symbol('m3', nonpositive=False)\n    m4 = Symbol('m4', negative=False)\n    assert ((m1 < 0) == Lt(m1, 0))\n    assert ((m2 <= 0) == Le(m2, 0))\n    assert ((m3 > 0) == Gt(m3, 0))\n    assert ((m4 >= 0) == Ge(m4, 0))\n    m1 = Symbol('m1', nonnegative=False, real=True)\n    m2 = Symbol('m2', positive=False, real=True)\n    m3 = Symbol('m3', nonpositive=False, real=True)\n    m4 = Symbol('m4', negative=False, real=True)\n    assert ((m1 < 0) is S.true)\n    assert ((m2 <= 0) is S.true)\n    assert ((m3 > 0) is S.true)\n    assert ((m4 >= 0) is S.true)\n    m1 = Symbol('m1', negative=True)\n    m2 = Symbol('m2', nonpositive=True)\n    m3 = Symbol('m3', positive=True)\n    m4 = Symbol('m4', nonnegative=True)\n    assert ((m1 < 0) is S.true)\n    assert ((m2 <= 0) is S.true)\n    assert ((m3 > 0) is S.true)\n    assert ((m4 >= 0) is S.true)\n    m1 = Symbol('m1', negative=False, real=True)\n    m2 = Symbol('m2', nonpositive=False, real=True)\n    m3 = Symbol('m3', positive=False, real=True)\n    m4 = Symbol('m4', nonnegative=False, real=True)\n    assert ((m1 < 0) is S.false)\n    assert ((m2 <= 0) is S.false)\n    assert ((m3 > 0) is S.false)\n    assert ((m4 >= 0) is S.false)\n", "label": 1}
{"function": "\n\ndef __init__(self, version=LACP_VERSION_NUMBER, actor_system_priority=0, actor_system='00:00:00:00:00:00', actor_key=0, actor_port_priority=0, actor_port=0, actor_state_activity=0, actor_state_timeout=0, actor_state_aggregation=0, actor_state_synchronization=0, actor_state_collecting=0, actor_state_distributing=0, actor_state_defaulted=0, actor_state_expired=0, partner_system_priority=0, partner_system='00:00:00:00:00:00', partner_key=0, partner_port_priority=0, partner_port=0, partner_state_activity=0, partner_state_timeout=0, partner_state_aggregation=0, partner_state_synchronization=0, partner_state_collecting=0, partner_state_distributing=0, partner_state_defaulted=0, partner_state_expired=0, collector_max_delay=0):\n    super(lacp, self).__init__()\n    assert (1 == (actor_state_activity | 1))\n    assert (1 == (actor_state_timeout | 1))\n    assert (1 == (actor_state_aggregation | 1))\n    assert (1 == (actor_state_synchronization | 1))\n    assert (1 == (actor_state_collecting | 1))\n    assert (1 == (actor_state_distributing | 1))\n    assert (1 == (actor_state_defaulted | 1))\n    assert (1 == (actor_state_expired | 1))\n    assert (1 == (partner_state_activity | 1))\n    assert (1 == (partner_state_timeout | 1))\n    assert (1 == (partner_state_aggregation | 1))\n    assert (1 == (partner_state_synchronization | 1))\n    assert (1 == (partner_state_collecting | 1))\n    assert (1 == (partner_state_distributing | 1))\n    assert (1 == (partner_state_defaulted | 1))\n    assert (1 == (partner_state_expired | 1))\n    self._subtype = SLOW_SUBTYPE_LACP\n    self.version = version\n    self._actor_tag = self.LACP_TLV_TYPE_ACTOR\n    self._actor_length = self._ACTPRT_INFO_PACK_LEN\n    self.actor_system_priority = actor_system_priority\n    self.actor_system = actor_system\n    self.actor_key = actor_key\n    self.actor_port_priority = actor_port_priority\n    self.actor_port = actor_port\n    self.actor_state_activity = actor_state_activity\n    self.actor_state_timeout = actor_state_timeout\n    self.actor_state_aggregation = actor_state_aggregation\n    self.actor_state_synchronization = actor_state_synchronization\n    self.actor_state_collecting = actor_state_collecting\n    self.actor_state_distributing = actor_state_distributing\n    self.actor_state_defaulted = actor_state_defaulted\n    self.actor_state_expired = actor_state_expired\n    self._actor_state = ((((((((self.actor_state_activity << 0) | (self.actor_state_timeout << 1)) | (self.actor_state_aggregation << 2)) | (self.actor_state_synchronization << 3)) | (self.actor_state_collecting << 4)) | (self.actor_state_distributing << 5)) | (self.actor_state_defaulted << 6)) | (self.actor_state_expired << 7))\n    self._partner_tag = self.LACP_TLV_TYPE_PARTNER\n    self._partner_length = self._ACTPRT_INFO_PACK_LEN\n    self.partner_system_priority = partner_system_priority\n    self.partner_system = partner_system\n    self.partner_key = partner_key\n    self.partner_port_priority = partner_port_priority\n    self.partner_port = partner_port\n    self.partner_state_activity = partner_state_activity\n    self.partner_state_timeout = partner_state_timeout\n    self.partner_state_aggregation = partner_state_aggregation\n    self.partner_state_synchronization = partner_state_synchronization\n    self.partner_state_collecting = partner_state_collecting\n    self.partner_state_distributing = partner_state_distributing\n    self.partner_state_defaulted = partner_state_defaulted\n    self.partner_state_expired = partner_state_expired\n    self._partner_state = ((((((((self.partner_state_activity << 0) | (self.partner_state_timeout << 1)) | (self.partner_state_aggregation << 2)) | (self.partner_state_synchronization << 3)) | (self.partner_state_collecting << 4)) | (self.partner_state_distributing << 5)) | (self.partner_state_defaulted << 6)) | (self.partner_state_expired << 7))\n    self._collector_tag = self.LACP_TLV_TYPE_COLLECTOR\n    self._collector_length = self._COL_INFO_PACK_LEN\n    self.collector_max_delay = collector_max_delay\n    self._terminator_tag = self.LACP_TLV_TYPE_TERMINATOR\n    self._terminator_length = 0\n", "label": 1}
{"function": "\n\ndef read_stdin(self, coro=None):\n    coro.set_daemon()\n    thread_pool = asyncoro.AsyncThreadPool(1)\n    while True:\n        sys.stdout.write('\\nEnter \"quit\" or \"exit\" to terminate dispynode,\\n  \"stop\" to stop service, \"start\" to restart service,\\n  \"cpus\" to change CPUs used, anything else to get status: ')\n        sys.stdout.flush()\n        try:\n            cmd = (yield thread_pool.async_task(raw_input))\n        except:\n            continue\n        cmd = cmd.strip().lower()\n        if (cmd in ('quit', 'exit')):\n            break\n        elif (cmd in ('stop', 'start', 'cpus')):\n            if (cmd == 'stop'):\n                cpus = 0\n            elif (cmd == 'start'):\n                cpus = self.num_cpus\n            elif (cmd == 'cpus'):\n                cpus = multiprocessing.cpu_count()\n                sys.stdout.write(('Enter number of CPUs to use in range -%s to %s: ' % ((cpus - 1), cpus)))\n                sys.stdout.flush()\n                try:\n                    cpus = (yield thread_pool.async_task(raw_input))\n                    cpus = int(cpus)\n                    if (cpus >= 0):\n                        assert (cpus <= multiprocessing.cpu_count())\n                    else:\n                        cpus += multiprocessing.cpu_count()\n                        assert (cpus >= 0)\n                except:\n                    print('  Invalid cpus ignored')\n                    continue\n                self.num_cpus = cpus\n            self.avail_cpus = (cpus - len(self.job_infos))\n            if self.scheduler['ip_addr']:\n                sock = AsyncSocket(socket.socket(socket.AF_INET, socket.SOCK_STREAM), keyfile=self.keyfile, certfile=self.certfile)\n                sock.settimeout(MsgTimeout)\n                try:\n                    (yield sock.connect((self.scheduler['ip_addr'], self.scheduler['port'])))\n                    info = {\n                        'ip_addr': self.ext_ip_addr,\n                        'sign': self.sign,\n                        'cpus': cpus,\n                    }\n                    (yield sock.send_msg(('NODE_CPUS:'.encode() + serialize(info))))\n                except:\n                    pass\n                finally:\n                    sock.close()\n            elif (self.num_cpus > 0):\n                Coro(self.broadcast_ping_msg)\n        else:\n            print(('\\n  Serving %d CPUs%s%s%s' % ((self.avail_cpus + len(self.job_infos)), ((' from %s' % self.serivce_start) if self.service_start else ''), ((' to %s' % self.service_end) if self.service_end else ''), ((' for %d clients' % self.serve) if (self.serve > 0) else ''))))\n            print(('  Completed:\\n    %d Computations, %d jobs, %.3f sec CPU time' % (self.num_computations, self.num_jobs, self.cpu_time)))\n            print('  Running:')\n            for (i, compute) in enumerate(self.computations.itervalues(), start=1):\n                print(('    Client %s: %s @ %s running %s jobs' % (i, compute.name, compute.scheduler_ip_addr, compute.pending_jobs)))\n            print('')\n    self.shutdown(quit=True)\n", "label": 1}
{"function": "\n\n@register_specialize\n@register_stabilize\n@register_canonicalize\n@gof.local_optimizer([IncSubtensor, AdvancedIncSubtensor, AdvancedIncSubtensor1])\ndef local_useless_inc_subtensor_alloc(node):\n    '\\n    Replaces an [Advanced]IncSubtensor[1], whose increment is an `alloc` of\\n    a fully or partially broadcastable variable, by one that skips the\\n    intermediate `alloc` where possible.\\n\\n    '\n    if isinstance(node.op, (IncSubtensor, AdvancedIncSubtensor, AdvancedIncSubtensor1)):\n        x = node.inputs[0]\n        y = node.inputs[1]\n        i = node.inputs[2:]\n        if ((y.owner is not None) and isinstance(y.owner.op, T.Alloc)):\n            z = y.owner.inputs[0]\n            try:\n                shape_feature = node.fgraph.shape_feature\n            except AttributeError:\n                return False\n            shape_of = shape_feature.shape_of\n            same_shape = shape_feature.same_shape\n            if isinstance(node.op, IncSubtensor):\n                xi = Subtensor(node.op.idx_list)(x, *i)\n            elif isinstance(node.op, AdvancedIncSubtensor):\n                xi = advanced_subtensor(x, *i)\n            elif isinstance(node.op, AdvancedIncSubtensor1):\n                xi = advanced_subtensor1(x, *i)\n            else:\n                raise Exception('Should never happen!')\n            reason = 'local_useless_incsubtensor_alloc'\n            if (xi not in shape_of):\n                shape_feature.on_import(node.fgraph, xi.owner, ('%s: add `xi`' % reason))\n            if (xi.ndim > y.ndim):\n                y = T.shape_padleft(y, (xi.ndim - y.ndim))\n                if (y not in shape_of):\n                    shape_feature.on_import(node.fgraph, y.owner, ('%s: add `y`' % reason))\n            z_broad = (((True,) * (xi.ndim - z.ndim)) + z.broadcastable)\n            cond = [T.or_(T.eq(y.shape[k], 1), T.eq(y.shape[k], xi.shape[k])) for k in xrange(xi.ndim) if (z_broad[k] and (not same_shape(xi, y, dim_x=k, dim_y=k)) and (shape_of[y][k] != 1))]\n            if (len(cond) > 0):\n                msg = '`x[i]` and `y` do not have the same shape.'\n                z = Assert(msg)(z, *cond)\n            r = node.op(x, z, *i)\n            copy_stack_trace(node.outputs, r)\n            return [r]\n", "label": 1}
{"function": "\n\n@register_specialize\n@register_canonicalize('fast_compile_gpu')\n@gof.local_optimizer([Subtensor, AdvancedSubtensor1])\ndef local_subtensor_make_vector(node):\n    '\\n    Replace all subtensor(make_vector) like:\\n    [a,b,c][0] -> a\\n    [a,b,c][0:2] -> [a,b]\\n\\n    Replace all AdvancedSubtensor1(make_vector) like:\\n    [a,b,c][[0,2]] -> [a,c]\\n\\n    We can do this for constant indexes.\\n\\n    '\n    x = node.inputs[0]\n    if ((not x.owner) or (x.owner.op != make_vector)):\n        return\n    if isinstance(node.op, Subtensor):\n        try:\n            (idx,) = node.op.idx_list\n        except Exception:\n            raise\n        if isinstance(idx, (scalar.Scalar, T.TensorType)):\n            (old_idx, idx) = (idx, node.inputs[1])\n            assert (idx.type == old_idx)\n    elif isinstance(node.op, AdvancedSubtensor1):\n        idx = node.inputs[1]\n    else:\n        return\n    if isinstance(idx, (int, numpy.integer)):\n        return [x.owner.inputs[idx]]\n    elif isinstance(idx, Variable):\n        if (idx.ndim == 0):\n            try:\n                v = get_scalar_constant_value(idx)\n                if isinstance(v, numpy.integer):\n                    v = int(v)\n                try:\n                    ret = [x.owner.inputs[v]]\n                except IndexError:\n                    raise NotScalarConstantError('Bad user graph!')\n                return ret\n            except NotScalarConstantError:\n                pass\n        elif ((idx.ndim == 1) and isinstance(idx, T.Constant)):\n            values = list(map(int, list(idx.value)))\n            ret = [make_vector(*[x.owner.inputs[v] for v in values])]\n            copy_stack_trace(node.outputs[0], ret)\n            return ret\n        else:\n            raise TypeError('case not expected')\n    elif isinstance(idx, slice):\n        try:\n            const_slice = node.op.get_constant_idx(node.inputs, allow_partial=False)[0]\n            ret = make_vector(*x.owner.inputs[const_slice])\n            copy_stack_trace(node.outputs, ret)\n            return [ret]\n        except NotScalarConstantError:\n            pass\n    else:\n        raise TypeError('case not expected')\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_3d_transpose():\n    ' reading/writing of transposed 3D RNMRTK time domain file '\n    dir_3d = os.path.join(DATA_DIR, 'rnmrtk_3d')\n    (dic, data) = ng.rnmrtk.read(os.path.join(dir_3d, 'time_3d_t1_t2_t3.sec'))\n    assert (data.shape == (128, 88, 36))\n    assert (np.abs((data[(2, 6, 4)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read(os.path.join(dir_3d, 'time_3d_t1_t3_t2.sec'))\n    assert (data.shape == (128, 72, 44))\n    assert (np.abs((data[(2, 8, 3)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read(os.path.join(dir_3d, 'time_3d_t2_t1_t3.sec'))\n    assert (data.shape == (88, 128, 36))\n    assert (np.abs((data[(6, 2, 4)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read(os.path.join(dir_3d, 'time_3d_t2_t3_t1.sec'))\n    assert (data.shape == (88, 72, 64))\n    assert (np.abs((data[(6, 8, 1)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read(os.path.join(dir_3d, 'time_3d_t3_t1_t2.sec'))\n    assert (data.shape == (72, 128, 44))\n    assert (np.abs((data[(8, 2, 3)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read(os.path.join(dir_3d, 'time_3d_t3_t2_t1.sec'))\n    assert (data.shape == (72, 88, 64))\n    assert (np.abs((data[(8, 6, 1)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef main(*args):\n    if (not args):\n        args = sys.argv[1:]\n    try:\n        (options, arguments) = parse_options(args)\n    except CommandLineUsageException as e:\n        print_error_line(('Usage error: %s\\n' % e))\n        print_error(e.usage)\n        return 1\n    start = datetime.datetime.now()\n    logger = init_logger(options)\n    reactor = init_reactor(logger)\n    if options.start_project:\n        return start_project()\n    if options.update_project:\n        return update_project()\n    if (options.list_tasks or options.list_plan_tasks):\n        try:\n            reactor.prepare_build(property_overrides=options.property_overrides, project_directory=options.project_directory, exclude_optional_tasks=options.exclude_optional_tasks, exclude_tasks=options.exclude_tasks, exclude_all_optional=options.exclude_all_optional)\n            if options.list_tasks:\n                print_list_of_tasks(reactor, quiet=options.very_quiet)\n            if options.list_plan_tasks:\n                print_plan_list_of_tasks(options, arguments, reactor, quiet=options.very_quiet)\n            return 0\n        except PyBuilderException as e:\n            print_build_status(str(e), options, successful=False)\n            return 1\n    if (not options.very_quiet):\n        print_styled_text_line('PyBuilder version {0}'.format(__version__), options, BOLD)\n        print_text_line(('Build started at %s' % format_timestamp(start)))\n        draw_line()\n    successful = True\n    failure_message = None\n    summary = None\n    try:\n        try:\n            reactor.prepare_build(property_overrides=options.property_overrides, project_directory=options.project_directory, exclude_optional_tasks=options.exclude_optional_tasks, exclude_tasks=options.exclude_tasks, exclude_all_optional=options.exclude_all_optional)\n            if (options.verbose or options.debug):\n                logger.debug('Verbose output enabled.\\n')\n                reactor.project.set_property('verbose', True)\n            summary = reactor.build(environments=options.environments, tasks=arguments)\n        except KeyboardInterrupt:\n            raise PyBuilderException('Build aborted')\n    except (Exception, SystemExit) as e:\n        failure_message = str(e)\n        if options.debug:\n            traceback.print_exc(file=sys.stderr)\n        successful = False\n    finally:\n        end = datetime.datetime.now()\n        if (not options.very_quiet):\n            print_summary(successful, summary, start, end, options, failure_message)\n        if (not successful):\n            return 1\n        return 0\n", "label": 1}
{"function": "\n\ndef test_setdefault(session):\n    set_ = session.set(key('test_sortedset_setdefault'), {\n        'h': 1,\n        'o': 2,\n        'n': 3,\n        'g': 4,\n    }, SortedSet)\n    assert (1 == set_.setdefault('h'))\n    assert ({\n        'h': 1,\n        'o': 2,\n        'n': 3,\n        'g': 4,\n    } == dict(set_))\n    assert (1 == set_.setdefault('h', 123))\n    assert ({\n        'h': 1,\n        'o': 2,\n        'n': 3,\n        'g': 4,\n    } == dict(set_))\n    assert (1 == set_.setdefault('m'))\n    assert ({\n        'h': 1,\n        'o': 2,\n        'n': 3,\n        'g': 4,\n        'm': 1,\n    } == dict(set_))\n    assert (123 == set_.setdefault('i', 123))\n    assert ({\n        'h': 1,\n        'o': 2,\n        'n': 3,\n        'g': 4,\n        'm': 1,\n        'i': 123,\n    } == dict(set_))\n    with raises(TypeError):\n        set_.setdefault('e', None)\n    with raises(TypeError):\n        set_.setdefault('e', '123')\n    setx = session.set(key('test_sortedsetx_setdefault'), {\n        100: 1,\n        200: 2,\n        300: 3,\n        400: 4,\n    }, IntSet)\n    assert (1 == setx.setdefault(100))\n    assert ({\n        100: 1,\n        200: 2,\n        300: 3,\n        400: 4,\n    } == dict(setx))\n    assert (1 == setx.setdefault(100, 123))\n    assert ({\n        100: 1,\n        200: 2,\n        300: 3,\n        400: 4,\n    } == dict(setx))\n    assert (1 == setx.setdefault(500))\n    assert ({\n        100: 1,\n        200: 2,\n        300: 3,\n        400: 4,\n        500: 1,\n    } == dict(setx))\n    assert (123 == setx.setdefault(600, 123))\n    assert ({\n        100: 1,\n        200: 2,\n        300: 3,\n        400: 4,\n        500: 1,\n        600: 123,\n    } == dict(setx))\n    with raises(TypeError):\n        setx.setdefault(700, None)\n    with raises(TypeError):\n        setx.setdefault(700, '123')\n", "label": 1}
{"function": "\n\ndef paginate_query(query, model, limit, sort_keys, marker=None, sort_dir=None, sort_dirs=None):\n    \"Returns a query with sorting / pagination criteria added.\\n\\n    Pagination works by requiring a unique sort_key, specified by sort_keys.\\n    (If sort_keys is not unique, then we risk looping through values.)\\n    We use the last row in the previous page as the 'marker' for pagination.\\n    So we must return values that follow the passed marker in the order.\\n    With a single-valued sort_key, this would be easy: sort_key > X.\\n    With a compound-values sort_key, (k1, k2, k3) we must do this to repeat\\n    the lexicographical ordering:\\n    (k1 > X1) or (k1 == X1 && k2 > X2) or (k1 == X1 && k2 == X2 && k3 > X3)\\n\\n    We also have to cope with different sort_directions.\\n\\n    Typically, the id of the last row is used as the client-facing pagination\\n    marker, then the actual marker object must be fetched from the db and\\n    passed in to us as marker.\\n\\n    :param query: the query object to which we should add paging/sorting\\n    :param model: the ORM model class\\n    :param limit: maximum number of items to return\\n    :param sort_keys: array of attributes by which results should be sorted\\n    :param marker: the last item of the previous page; we returns the next\\n                    results after this value.\\n    :param sort_dir: direction in which results should be sorted (asc, desc)\\n                     suffix -nullsfirst, -nullslast can be added to defined\\n                     the ordering of null values\\n    :param sort_dirs: per-column array of sort_dirs, corresponding to sort_keys\\n\\n    :rtype: sqlalchemy.orm.query.Query\\n    :return: The query with sorting/pagination added.\\n    \"\n    if ('id' not in sort_keys):\n        LOG.warning(_LW('Id not in sort_keys; is sort_keys unique?'))\n    assert (not (sort_dir and sort_dirs))\n    if ((sort_dirs is None) and (sort_dir is None)):\n        sort_dir = 'asc'\n    if (sort_dirs is None):\n        sort_dirs = [sort_dir for _sort_key in sort_keys]\n    assert (len(sort_dirs) == len(sort_keys))\n    for (current_sort_key, current_sort_dir) in zip(sort_keys, sort_dirs):\n        try:\n            inspect(model).all_orm_descriptors[current_sort_key]\n        except KeyError:\n            raise exception.InvalidSortKey(current_sort_key)\n        else:\n            sort_key_attr = getattr(model, current_sort_key)\n        try:\n            (main_sort_dir, __, null_sort_dir) = current_sort_dir.partition('-')\n            sort_dir_func = {\n                'asc': sqlalchemy.asc,\n                'desc': sqlalchemy.desc,\n            }[main_sort_dir]\n            null_order_by_stmt = {\n                '': None,\n                'nullsfirst': sort_key_attr.is_(None),\n                'nullslast': sort_key_attr.isnot(None),\n            }[null_sort_dir]\n        except KeyError:\n            raise ValueError((_('Unknown sort direction, must be one of: %s') % ', '.join(_VALID_SORT_DIR)))\n        if (null_order_by_stmt is not None):\n            query = query.order_by(sqlalchemy.desc(null_order_by_stmt))\n        query = query.order_by(sort_dir_func(sort_key_attr))\n    if (marker is not None):\n        marker_values = []\n        for sort_key in sort_keys:\n            v = getattr(marker, sort_key)\n            marker_values.append(v)\n        criteria_list = []\n        for i in range(len(sort_keys)):\n            crit_attrs = []\n            for j in range(i):\n                model_attr = getattr(model, sort_keys[j])\n                crit_attrs.append((model_attr == marker_values[j]))\n            model_attr = getattr(model, sort_keys[i])\n            if sort_dirs[i].startswith('desc'):\n                crit_attrs.append((model_attr < marker_values[i]))\n            else:\n                crit_attrs.append((model_attr > marker_values[i]))\n            criteria = sqlalchemy.sql.and_(*crit_attrs)\n            criteria_list.append(criteria)\n        f = sqlalchemy.sql.or_(*criteria_list)\n        query = query.filter(f)\n    if (limit is not None):\n        query = query.limit(limit)\n    return query\n", "label": 1}
{"function": "\n\n@expose(hide=True)\ndef debug_fpm(self):\n    'Start/Stop PHP5-FPM debug'\n    if ((self.app.pargs.fpm == 'on') and (not self.app.pargs.site_name)):\n        if (not EEShellExec.cmd_exec(self, 'grep \"log_level = debug\" /etc/{0}/fpm/php-fpm.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')))):\n            Log.info(self, 'Setting up PHP5-FPM log_level = debug')\n            config = configparser.ConfigParser()\n            config.read('/etc/{0}/fpm/php-fpm.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')))\n            config.remove_option('global', 'include')\n            config['global']['log_level'] = 'debug'\n            config['global']['include'] = '/etc/{0}/fpm/pool.d/*.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5'))\n            with open('/etc/{0}/fpm/php-fpm.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')), encoding='utf-8', mode='w') as configfile:\n                Log.debug(self, 'Writting php5-FPM configuration into /etc/{0}/fpm/php-fpm.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')))\n                config.write(configfile)\n            self.trigger_php = True\n        else:\n            Log.info(self, 'PHP5-FPM log_level = debug already setup')\n        self.msg = (self.msg + ['/var/log/{0}/fpm.log'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5'))])\n    elif ((self.app.pargs.fpm == 'off') and (not self.app.pargs.site_name)):\n        if EEShellExec.cmd_exec(self, 'grep \"log_level = debug\" /etc/{0}/fpm/php-fpm.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5'))):\n            Log.info(self, 'Disabling PHP5-FPM log_level = debug')\n            config = configparser.ConfigParser()\n            config.read('/etc/{0}/fpm/php-fpm.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')))\n            config.remove_option('global', 'include')\n            config['global']['log_level'] = 'notice'\n            config['global']['include'] = '/etc/{0}/fpm/pool.d/*.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5'))\n            with open('/etc/{0}/fpm/php-fpm.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')), encoding='utf-8', mode='w') as configfile:\n                Log.debug(self, 'writting php5 configuration into /etc/{0}/fpm/php-fpm.conf'.format(('php/5.6' if (EEVariables.ee_platform_codename == 'trusty') else 'php5')))\n                config.write(configfile)\n            self.trigger_php = True\n        else:\n            Log.info(self, 'PHP5-FPM log_level = debug  already disabled')\n", "label": 1}
{"function": "\n\ndef post(self):\n    'Reports get handler.\\n\\n    Returns:\\n      A webapp.Response() response.\\n    '\n    session = gaeserver.DoMunkiAuth()\n    uuid = main_common.SanitizeUUID(session.uuid)\n    report_type = self.request.get('_report_type')\n    report_feedback = {\n        \n    }\n    message = None\n    details = None\n    client_id = None\n    computer = None\n    if ((report_type == 'preflight') or (report_type == 'postflight')):\n        client_id_str = urllib.unquote(self.request.get('client_id'))\n        client_id = common.ParseClientId(client_id_str, uuid=uuid)\n        user_settings_str = self.request.get('user_settings')\n        user_settings = None\n        try:\n            if user_settings_str:\n                user_settings = util.Deserialize(urllib.unquote(str(user_settings_str)))\n        except util.DeserializeError:\n            logging.warning('Client %s sent broken user_settings: %s', client_id_str, user_settings_str)\n        pkgs_to_install = self.request.get_all('pkgs_to_install')\n        apple_updates_to_install = self.request.get_all('apple_updates_to_install')\n        computer = models.Computer.get_by_key_name(uuid)\n        ip_address = os.environ.get('REMOTE_ADDR', '')\n        if (report_type == 'preflight'):\n            client_exit = self.request.get('client_exit', None)\n            report_feedback = self.GetReportFeedback(uuid, report_type, computer=computer, ip_address=ip_address, client_exit=client_exit)\n            if (self.request.get('json') == '1'):\n                self.response.out.write((JSON_PREFIX + json.dumps(report_feedback)))\n            else:\n                feedback_to_send = 'OK'\n                for feedback in LEGACY_FEEDBACK_LIST:\n                    if report_feedback.get(feedback.lower()):\n                        feedback_to_send = feedback\n                self.response.out.write(feedback_to_send)\n            if report_feedback.get('exit'):\n                if (not client_exit):\n                    client_exit = 'Connection from defined exit IP address'\n                common.WriteClientLog(models.PreflightExitLog, uuid, computer=computer, exit_reason=client_exit)\n        common.LogClientConnection(report_type, client_id, user_settings, pkgs_to_install, apple_updates_to_install, computer=computer, ip_address=ip_address, report_feedback=report_feedback)\n    elif (report_type == 'install_report'):\n        computer = models.Computer.get_by_key_name(uuid)\n        self._LogInstalls(self.request.get_all('installs'), computer)\n        for removal in self.request.get_all('removals'):\n            common.WriteClientLog(models.ClientLog, uuid, computer=computer, action='removal', details=removal)\n        for problem in self.request.get_all('problem_installs'):\n            common.WriteClientLog(models.ClientLog, uuid, computer=computer, action='install_problem', details=problem)\n    elif (report_type == 'broken_client'):\n        reason = self.request.get('reason', 'objc')\n        details = self.request.get('details')\n        logging.warning('Broken Munki client (%s): %s', reason, details)\n        common.WriteBrokenClient(uuid, reason, details)\n    elif (report_type == 'msu_log'):\n        details = {\n            \n        }\n        for k in ['time', 'user', 'source', 'event', 'desc']:\n            details[k] = self.request.get(k, None)\n        common.WriteComputerMSULog(uuid, details)\n    else:\n        params = []\n        for param in self.request.arguments():\n            params.append(('%s=%s' % (param, self.request.get_all(param))))\n        common.WriteClientLog(models.ClientLog, uuid, action='unknown', details=str(params))\n", "label": 1}
{"function": "\n\ndef make_node(self, *inputs):\n    _inputs = [gpu_contiguous(as_cuda_ndarray_variable(i)) for i in inputs]\n    if ((self.nin > 0) and (len(_inputs) != self.nin)):\n        raise TypeError('Wrong argument count', (self.nin, len(_inputs)))\n    for i in _inputs[1:]:\n        if (i.type.ndim != inputs[0].type.ndim):\n            raise TypeError('different ranks among inputs')\n    if any([any(i.type.broadcastable) for i in inputs]):\n        raise Exception(\"pycuda don't support broadcasted dimensions\")\n    assert (len(inputs) == 2)\n    otype = CudaNdarrayType(broadcastable=([False] * _inputs[0].type.ndim))\n    assert (self.nout == 1)\n    fct_name = ('pycuda_elemwise_%s' % str(self.scalar_op))\n    out_node = Apply(self, _inputs, [otype() for o in xrange(self.nout)])\n    in_name = [('i' + str(id)) for id in range(len(inputs))]\n    out_name = [('o' + str(id)) for id in range(self.nout)]\n    c_code = self.scalar_op.c_code(out_node, 'some_name', tuple([(n + '[i]') for n in in_name]), tuple(((n + '[i]') for n in out_name)), {\n        \n    })\n    c_code_param = ', '.join(([((_replace_npy_types(var.type.dtype_specs()[1]) + ' *') + name) for (var, name) in chain(izip(inputs, in_name), izip(out_node.outputs, out_name))] + ['int size']))\n    mod = SourceModule(('\\n  __global__ void %s(%s)\\n  {\\n    int i = (blockIdx.x+blockIdx.y*gridDim.x)*(blockDim.x*blockDim.y);\\n    i += threadIdx.x + threadIdx.y*blockDim.x;\\n    if(i<size){\\n        %s\\n    }\\n  }\\n  ' % (fct_name, c_code_param, c_code)))\n    self.pycuda_fct = mod.get_function(fct_name)\n    return out_node\n", "label": 1}
{"function": "\n\n@nonraw_instance\ndef model_definition_post_save(sender, instance, created, **kwargs):\n    model_class = instance.model_class(force_create=True)\n    opts = model_class._meta\n    db_table = opts.db_table\n    if created:\n        primary_key = opts.pk\n        fields = [(field.get_attname_column()[1], field) for field in opts.fields if (field is not primary_key)]\n        try:\n            extra_fields = getattr(instance._state, '_create_extra_fields')\n        except AttributeError:\n            pass\n        else:\n            for (column, field) in extra_fields:\n                remote_field = get_remote_field(field)\n                if field.primary_key:\n                    assert isinstance(primary_key, models.AutoField)\n                    primary_key = field\n                elif (remote_field and remote_field.parent_link and isinstance(primary_key, models.AutoField)):\n                    field.primary_key = True\n                    primary_key = field\n                else:\n                    fields.append((column, field))\n            delattr(instance._state, '_create_extra_fields')\n        fields.insert(0, (primary_key.get_attname_column()[1], primary_key))\n        try:\n            delayed_save = getattr(instance._state, '_create_delayed_save')\n        except AttributeError:\n            pass\n        else:\n            for obj in delayed_save:\n                obj.model_def = instance\n                obj.save(force_insert=True, force_create_model_class=False)\n            delattr(instance._state, '_create_delayed_save')\n        model_class = instance.model_class(force_create=True)\n        perform_ddl('create_model', model_class)\n    else:\n        old_model_class = instance._model_class\n        if old_model_class:\n            old_db_table = old_model_class._meta.db_table\n            if (db_table != old_db_table):\n                perform_ddl('alter_db_table', model_class, old_db_table, db_table)\n            ContentType.objects.clear_cache()\n    instance._model_class = model_class.model\n", "label": 1}
{"function": "\n\n@login_required\ndef misc(request):\n    if ('action' in request.GET):\n        action = request.GET['action']\n        if (action == 'markread'):\n            user = request.user\n            PostTracking.objects.filter(user__id=user.id).update(last_read=timezone.now(), topics=None)\n            messages.info(request, _('All topics marked as read.'))\n            return HttpResponseRedirect(reverse('djangobb:index'))\n        elif (action == 'report'):\n            if request.GET.get('post_id', ''):\n                post_id = request.GET['post_id']\n                post = get_object_or_404(Post, id=post_id)\n                form = build_form(ReportForm, request, reported_by=request.user, post=post_id)\n                if ((request.method == 'POST') and form.is_valid()):\n                    form.save()\n                    messages.info(request, _('Post reported.'))\n                    return HttpResponseRedirect(post.get_absolute_url())\n                return render(request, 'djangobb_forum/report.html', {\n                    'form': form,\n                })\n    elif (('submit' in request.POST) and ('mail_to' in request.GET)):\n        if ((not forum_settings.USER_TO_USER_EMAIL) and (not request.user.is_superuser)):\n            raise PermissionDenied\n        form = MailToForm(request.POST)\n        if form.is_valid():\n            user = get_object_or_404(User, username=request.GET['mail_to'])\n            subject = form.cleaned_data['subject']\n            body = (form.cleaned_data['body'] + ('\\n %s %s [%s]' % (Site.objects.get_current().domain, request.user.username, request.user.email)))\n            try:\n                user.email_user(subject, body, request.user.email)\n                messages.success(request, _('Email send.'))\n            except Exception:\n                messages.error(request, _('Email could not be sent.'))\n            return HttpResponseRedirect(reverse('djangobb:index'))\n    elif ('mail_to' in request.GET):\n        if ((not forum_settings.USER_TO_USER_EMAIL) and (not request.user.is_superuser)):\n            raise PermissionDenied\n        mailto = get_object_or_404(User, username=request.GET['mail_to'])\n        form = MailToForm()\n        return render(request, 'djangobb_forum/mail_to.html', {\n            'form': form,\n            'mailto': mailto,\n        })\n", "label": 1}
{"function": "\n\ndef handle_endtag(self, tag):\n    if (tag in self.as_div):\n        tag = 'div'\n    if (tag in self.strip):\n        return\n    try:\n        while True:\n            previous = self.stack.pop()\n            self.output += (('</' + tag) + '>')\n            if re.search('h[1-6]', tag):\n                self.output += '<div class=\"horizontal-rule\"></div>'\n                if (not self.navigate_rendered):\n                    self.navigate_rendered = True\n                    self.output += (((((((('<a href=\"history.back\">back</a>' if self.can_back else 'back') + '&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"http://php.net/manual/') + self.language) + '/') + self.symbol) + '.php\">online</a>') + '&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;') + re.sub('.*?(<a.*?</a>).*', '\\\\1', self.navigate_up))\n                    (languages, _) = getLanguageList(format='raw', getAll=False)\n                    if (len(languages) > 1):\n                        self.output += '&nbsp;&nbsp;&nbsp;&nbsp;Change language:'\n                        for lang in languages:\n                            self.output += ((((' <a href=\"changeto.' + lang['shortName']) + '\">') + lang['nativeName']) + '</a>')\n            if self.shall_border(previous['tag'], previous['attrs']):\n                self.output += '</div>'\n            for k in previous['attrs']:\n                v = previous['attrs'][k]\n                if ((k == 'id') and (v == self.symbol)):\n                    raise FinishError\n                if ((k == 'class') and (v == 'up')):\n                    self.navigate_up = self.output\n            if (tag == previous['tag']):\n                break\n    except IndexError:\n        pass\n", "label": 1}
{"function": "\n\ndef collect(self, delay=None):\n    if delay:\n        time.sleep(delay)\n    pdu_list = list()\n    max_data = None\n    with self.lock:\n        active_sap_list = [sap for sap in self.sap if (sap is not None)]\n        for sap in active_sap_list:\n            pdu = sap.dequeue((max_data if max_data else 2179))\n            if (pdu is not None):\n                if (self.cfg['send-agf'] == False):\n                    return pdu\n                pdu_list.append(pdu)\n                if (max_data is None):\n                    max_data = (self.cfg['send-miu'] + 2)\n                max_data -= len(pdu)\n                if (max_data < (((bool((len(pdu_list) == 1)) * 2) + 2) + 2)):\n                    break\n        else:\n            max_data = (self.cfg['send-miu'] + 2)\n        for sap in active_sap_list:\n            if (sap.mode == DATA_LINK_CONNECTION):\n                pdu = sap.sendack(max_data)\n                if (not (pdu is None)):\n                    if (self.cfg['send-agf'] == False):\n                        return pdu\n                    pdu_list.append(pdu)\n                    max_data -= len(pdu)\n                    if (max_data < (((bool((len(pdu_list) == 1)) * 2) + 2) + 3)):\n                        break\n    if (len(pdu_list) > 1):\n        return AggregatedFrame(aggregate=pdu_list)\n    if (len(pdu_list) == 1):\n        return pdu_list[0]\n    return None\n", "label": 1}
{"function": "\n\ndef v4_to_v3_property(self, property_name, is_multi, is_projection, v4_value, v3_property):\n    'Converts info from a v4 Property to a v3 Property.\\n\\n    v4_value must not have a list_value.\\n\\n    Args:\\n      property_name: the name of the property\\n      is_multi: whether the property contains multiple values\\n      is_projection: whether the property is projected\\n      v4_value: an entity_v4_pb.Value\\n      v3_property: an entity_pb.Property to populate\\n    '\n    assert (not v4_value.list_value_list()), 'v4 list_value not convertable to v3'\n    v3_property.Clear()\n    v3_property.set_name(property_name)\n    if (v4_value.has_meaning() and (v4_value.meaning() == MEANING_EMPTY_LIST)):\n        v3_property.set_meaning(MEANING_EMPTY_LIST)\n        v3_property.set_multiple(False)\n        v3_property.mutable_value()\n        return\n    v3_property.set_multiple(is_multi)\n    self.v4_value_to_v3_property_value(v4_value, v3_property.mutable_value())\n    v4_meaning = None\n    if v4_value.has_meaning():\n        v4_meaning = v4_value.meaning()\n    if v4_value.has_timestamp_microseconds_value():\n        v3_property.set_meaning(entity_pb.Property.GD_WHEN)\n    elif v4_value.has_blob_key_value():\n        v3_property.set_meaning(entity_pb.Property.BLOBKEY)\n    elif v4_value.has_blob_value():\n        if (v4_meaning == MEANING_ZLIB):\n            v3_property.set_meaning_uri(URI_MEANING_ZLIB)\n        if (v4_meaning == entity_pb.Property.BYTESTRING):\n            if v4_value.indexed():\n                pass\n        else:\n            if v4_value.indexed():\n                v3_property.set_meaning(entity_pb.Property.BYTESTRING)\n            else:\n                v3_property.set_meaning(entity_pb.Property.BLOB)\n            v4_meaning = None\n    elif v4_value.has_entity_value():\n        if (v4_meaning != MEANING_GEORSS_POINT):\n            if ((v4_meaning != MEANING_PREDEFINED_ENTITY_POINT) and (v4_meaning != MEANING_PREDEFINED_ENTITY_USER)):\n                v3_property.set_meaning(entity_pb.Property.ENTITY_PROTO)\n            v4_meaning = None\n    elif v4_value.has_geo_point_value():\n        v3_property.set_meaning(MEANING_GEORSS_POINT)\n    else:\n        pass\n    if (v4_meaning is not None):\n        v3_property.set_meaning(v4_meaning)\n    if is_projection:\n        v3_property.set_meaning(entity_pb.Property.INDEX_VALUE)\n", "label": 1}
{"function": "\n\ndef merge():\n    description = '\\n    nbmerge is a tool for resolving merge conflicts in IPython Notebook\\n    files.\\n\\n    If no arguments are given, nbmerge attempts to find the conflicting\\n    file in the version control system.\\n\\n    Positional arguments are available for integration with version\\n    control systems such as Git and Mercurial.\\n    '\n    usage = 'nbmerge [-h] [--check] [--debug] [--browser=<browser>][local base remote [result]]'\n    parser = argparse.ArgumentParser(description=description, usage=usage)\n    parser.add_argument('notebook', nargs='*')\n    parser.add_argument('--check', '-c', action='store_true', default=False, help='Run nbmerge algorithm but do not display the result.')\n    parser.add_argument('--debug', '-d', action='store_true', default=False, help='Pass debug=True to the Flask server to ease debugging.')\n    parser.add_argument('--browser', '-b', default=None, help='Browser to launch nbdiff/nbmerge in')\n    args = parser.parse_args()\n    length = len(args.notebook)\n    parser = NotebookParser()\n    valid_notebooks = False\n    if (length == 0):\n        try:\n            vcs = HgAdapter()\n        except NoVCSError as hg_err:\n            try:\n                vcs = GitAdapter()\n            except NoVCSError:\n                print(hg_err.value)\n                sys.exit((- 1))\n        unmerged_notebooks = vcs.get_unmerged_notebooks()\n        if (not (len(unmerged_notebooks) == 0)):\n            invalid_notebooks = []\n            for nbook in unmerged_notebooks:\n                try:\n                    filename = nbook[3]\n                    nb_local = parser.parse(nbook[0])\n                    nb_base = parser.parse(nbook[1])\n                    nb_remote = parser.parse(nbook[2])\n                    pre_merged_notebook = notebook_merge(nb_local, nb_base, nb_remote)\n                    app.add_notebook(pre_merged_notebook, filename)\n                except NotJSONError:\n                    invalid_notebooks.append(filename)\n            if (len(invalid_notebooks) > 0):\n                print((('There was a problem parsing the following notebook ' + 'files:\\n') + '\\n'.join(invalid_notebooks)))\n            if (len(unmerged_notebooks) == len(invalid_notebooks)):\n                print('There are no valid notebooks to merge.')\n                return (- 1)\n            else:\n                valid_notebooks = True\n        else:\n            print('There are no files to be merged.')\n            return (- 1)\n    elif ((length == 3) or (length == 4)):\n        invalid_notebooks = []\n        if (length == 3):\n            filename = args.notebook[0]\n        elif (length == 4):\n            filename = args.notebook[3]\n        try:\n            nb_local = parser.parse(open(args.notebook[0]))\n        except NotJSONError:\n            invalid_notebooks.append(args.notebook[0])\n        try:\n            nb_base = parser.parse(open(args.notebook[1]))\n        except NotJSONError:\n            invalid_notebooks.append(args.notebook[1])\n        try:\n            nb_remote = parser.parse(open(args.notebook[2]))\n        except NotJSONError:\n            invalid_notebooks.append(args.notebook[2])\n        if (len(invalid_notebooks) == 0):\n            pre_merged_notebook = notebook_merge(nb_local, nb_base, nb_remote)\n            app.add_notebook(pre_merged_notebook, filename)\n            valid_notebooks = True\n        elif (len(invalid_notebooks) > 0):\n            print((('There was a problem parsing the following notebook ' + 'files:\\n') + '\\n'.join(invalid_notebooks)))\n            print('There are no valid notebooks to merge.')\n            return (- 1)\n    else:\n        sys.stderr.write('Incorrect number of arguments. Quitting.\\n')\n        sys.exit((- 1))\n\n    def save_notebook(notebook_result, filename):\n        parsed = nbformat.reads(notebook_result, 'json')\n        with open(filename, 'w') as targetfile:\n            nbformat.write(parsed, targetfile, 'ipynb')\n    if valid_notebooks:\n        if (not args.check):\n            app.shutdown_callback(save_notebook)\n            open_browser(args.browser)\n            app.run(debug=args.debug)\n", "label": 1}
{"function": "\n\ndef check(self):\n    'Do the actual testing.'\n    scene = self.scene\n    src = scene.children[0]\n    ud = src.children[0]\n    o = ud.children[0].children[0].children[0]\n    mm = o.children[0]\n    assert (src.get_output_dataset().point_data.scalars.name == 'temperature')\n    assert (src.get_output_dataset().point_data.vectors.name == 'velocity')\n    expect = ['ScalarGradient', 'Vorticity']\n    expect1 = [(x + '-y') for x in expect]\n    expect2 = [(x + ' magnitude') for x in expect]\n    o.enabled = True\n    assert (o.get_output_dataset().point_data.scalars.name in expect1)\n    assert (o.get_output_dataset().point_data.vectors.name in expect)\n    assert (mm.scalar_lut_manager.data_name in expect1)\n    o.enabled = False\n    assert (o.get_output_dataset().point_data.scalars.name in expect2)\n    assert (o.get_output_dataset().point_data.vectors.name in expect)\n    assert (mm.scalar_lut_manager.data_name in expect2)\n    ud.filter.vector_mode = 'compute_vorticity'\n    assert (o.get_output_dataset().point_data.scalars.name == 'Vorticity magnitude')\n    assert (o.get_output_dataset().point_data.vectors.name == 'Vorticity')\n    assert (mm.scalar_lut_manager.data_name == 'Vorticity magnitude')\n    o.enabled = True\n    assert (o.get_output_dataset().point_data.scalars.name == 'Vorticity-y')\n    assert (o.get_output_dataset().point_data.vectors.name == 'Vorticity')\n    assert (mm.scalar_lut_manager.data_name == 'Vorticity-y')\n    o.enabled = False\n", "label": 1}
{"function": "\n\ndef process(self):\n    for (k, v) in self.config_st.items():\n        setattr(self, k, v)\n    for (k, v) in self.config_st['server'].items():\n        setattr(self, k, v)\n    for l in self.listeners:\n        proto = l.get('proto', 'client')\n        self.ctx.logger.info('opening listener at {0}:{1} [{2}] {3}'.format(l['host'], l['port'], proto, ('SSL' if l['ssl'] else '')))\n        if l['ssl']:\n            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n            try:\n                context.set_ciphers('kEECDH+HIGH:kEDH+HIGH:HIGH:!RC4:!aNULL')\n            except ssl.SSLError:\n                print('mammon: error: no ciphers could be selected. SSL is not available for any listener.')\n                break\n            keyfile = os.path.expanduser(l.get('keyfile', ''))\n            if (not keyfile):\n                print('mammon: error: SSL listener {}:{} [{}] does not have a `keyfile`, skipping'.format(l['host'], l['port'], proto))\n                continue\n            certfile = os.path.expanduser(l.get('certfile', ''))\n            if (not certfile):\n                print('mammon: error: SSL listener {}:{} [{}] does not have a `certfile`, skipping'.format(l['host'], l['port'], proto))\n                continue\n            if ssl.HAS_ECDH:\n                context.set_ecdh_curve('secp384r1')\n                context.options |= ssl.OP_SINGLE_ECDH_USE\n            if ('dhparams' in l):\n                DHparams = os.path.expanduser(l.get('dhparams', ''))\n                if DHparams:\n                    context.load_dh_params(DHparams)\n                    context.options |= ssl.OP_SINGLE_DH_USE\n            context.load_cert_chain(certfile, keyfile=keyfile)\n            context.options |= ssl.OP_NO_SSLv2\n            context.options |= ssl.OP_NO_SSLv3\n            context.options |= ssl.OP_NO_COMPRESSION\n            context.options |= ssl.OP_CIPHER_SERVER_PREFERENCE\n            context.options |= 16384\n            print('mammon: note: SSL support is not yet optimized and may cause slowdowns in your server')\n        else:\n            context = None\n        lstn = self.ctx.eventloop.create_server(self.listener_protos[proto], l['host'], l['port'], ssl=context)\n        self.ctx.listeners.append(lstn)\n    if (self.metadata.get('limit', None) is not None):\n        try:\n            self.metadata['limit'] = int(self.metadata['limit'])\n        except:\n            print('mammon: error: config key metadata.limit must be an integer or commented out')\n            print('mammon: error: setting metadata.limit to default 20')\n            self.metadata['limit'] = 20\n    if (self.metadata.get('restricted_keys', []) is None):\n        self.metadata['restricted_keys'] = []\n    self.metadata['restricted_keys'] = CaseInsensitiveList(self.metadata['restricted_keys'])\n    roles = {\n        \n    }\n    roles_extending = {\n        None: {\n            \n        },\n    }\n    for (k, v) in self.roles.items():\n        extends = v.get('extends', None)\n        if (extends not in roles_extending):\n            roles_extending[extends] = {\n                \n            }\n        roles_extending[extends][k] = v\n    base_roles = roles_extending[None]\n    for (k, v) in base_roles.items():\n        roles[k] = Role(self.ctx, k, roles=roles, **v)\n        roles = load_extended_roles(self.ctx, k, roles, roles_extending)\n    self.ctx.roles = roles\n", "label": 1}
{"function": "\n\ndef take_action(self, opts):\n    for template in opts.scaffold_name:\n        template_filename = None\n        template_relpath = None\n        if template.endswith('.template'):\n            template_filename = template\n            template_relpath = '.'\n        else:\n            template_filename = self._lookup(template, '.')\n            if template_filename:\n                template_relpath = os.path.dirname(template_filename)\n            if ((template_filename is None) and opts.lookup):\n                template_filename = self._lookup(template, opts.lookup)\n                if template_filename:\n                    template_relpath = os.path.relpath(os.path.dirname(template_filename), opts.lookup)\n        if ((not template_filename) or (not os.path.exists(template_filename))):\n            print(('Template %s Not Found!' % template))\n            continue\n        print(('Using %s for %s' % (template_filename, opts.target)))\n        (template_with_ext, __) = os.path.splitext(template_filename)\n        (__, output_ext) = os.path.splitext(template_with_ext)\n        output_dir = opts.path\n        if (not output_dir):\n            output_dir = template_relpath\n        if opts.subdir:\n            output_dir = os.path.join(output_dir, opts.subdir)\n        if (not os.path.exists(output_dir)):\n            os.makedirs(output_dir)\n            if opts.subdir:\n                package_init = os.path.join(output_dir, '__init__.py')\n                if (not os.path.exists(package_init)):\n                    with open(package_init, 'w') as pif:\n                        pif.write('# -*- coding: utf-8 -*-\\n')\n        output_path = (os.path.join(output_dir, opts.target) + output_ext)\n        print(('Creating %s...' % output_path))\n        with open(template_filename, 'r') as tf:\n            try:\n                subdir_as_package = (opts.subdir.replace(os.sep, '.') if opts.subdir else '')\n                text = GearBoxTemplate().template_renderer(tf.read(), {\n                    'target': opts.target,\n                    'subdir': opts.subdir,\n                    'subpackage': subdir_as_package,\n                    'dotted_subpackage': ('.' + subdir_as_package),\n                    'packages': self._find_toplevel_packages(),\n                })\n            except NameError as e:\n                print(('!! Error while processing template: %s' % e))\n                continue\n            with open(output_path, 'w') as of:\n                of.write(text)\n", "label": 1}
{"function": "\n\ndef __archive_update_many(fh, header, archive, points):\n    step = archive['secondsPerPoint']\n    alignedPoints = [((timestamp - (timestamp % step)), value) for (timestamp, value) in points]\n    alignedPoints = dict(alignedPoints).items()\n    packedStrings = []\n    previousInterval = None\n    currentString = b''\n    for (interval, value) in alignedPoints:\n        if ((not previousInterval) or (interval == (previousInterval + step))):\n            currentString += struct.pack(pointFormat, interval, value)\n            previousInterval = interval\n        else:\n            numberOfPoints = (len(currentString) // pointSize)\n            startInterval = (previousInterval - (step * (numberOfPoints - 1)))\n            packedStrings.append((startInterval, currentString))\n            currentString = struct.pack(pointFormat, interval, value)\n            previousInterval = interval\n    if currentString:\n        numberOfPoints = (len(currentString) // pointSize)\n        startInterval = (previousInterval - (step * (numberOfPoints - 1)))\n        packedStrings.append((startInterval, currentString))\n    fh.seek(archive['offset'])\n    packedBasePoint = fh.read(pointSize)\n    (baseInterval, baseValue) = struct.unpack(pointFormat, packedBasePoint)\n    if (baseInterval == 0):\n        baseInterval = packedStrings[0][0]\n    for (interval, packedString) in packedStrings:\n        timeDistance = (interval - baseInterval)\n        pointDistance = (timeDistance // step)\n        byteDistance = (pointDistance * pointSize)\n        myOffset = (archive['offset'] + (byteDistance % archive['size']))\n        fh.seek(myOffset)\n        archiveEnd = (archive['offset'] + archive['size'])\n        bytesBeyond = ((myOffset + len(packedString)) - archiveEnd)\n        if (bytesBeyond > 0):\n            fh.write(packedString[:(- bytesBeyond)])\n            assert (fh.tell() == archiveEnd), ('archiveEnd=%d fh.tell=%d bytesBeyond=%d len(packedString)=%d' % (archiveEnd, fh.tell(), bytesBeyond, len(packedString)))\n            fh.seek(archive['offset'])\n            fh.write(packedString[(- bytesBeyond):])\n        else:\n            fh.write(packedString)\n    higher = archive\n    lowerArchives = [arc for arc in header['archives'] if (arc['secondsPerPoint'] > archive['secondsPerPoint'])]\n    for lower in lowerArchives:\n        fit = (lambda i: (i - (i % lower['secondsPerPoint'])))\n        lowerIntervals = [fit(p[0]) for p in alignedPoints]\n        uniqueLowerIntervals = set(lowerIntervals)\n        propagateFurther = False\n        for interval in uniqueLowerIntervals:\n            if __propagate(fh, header, interval, higher, lower):\n                propagateFurther = True\n        if (not propagateFurther):\n            break\n        higher = lower\n", "label": 1}
{"function": "\n\ndef fetch_pack(self, path, determine_wants, graph_walker, pack_data, progress):\n    'Retrieve a pack from a git smart server.\\n\\n        :param determine_wants: Callback that returns list of commits to fetch\\n        :param graph_walker: Object with next() and ack().\\n        :param pack_data: Callback called for each bit of data in the pack\\n        :param progress: Callback for progress reports (strings)\\n        '\n    (proto, can_read) = self._connect('upload-pack', path)\n    (refs, server_capabilities) = self.read_refs(proto)\n    wants = determine_wants(refs)\n    if (not wants):\n        proto.write_pkt_line(None)\n        return refs\n    assert (isinstance(wants, list) and (type(wants[0]) == str))\n    proto.write_pkt_line(('want %s %s\\n' % (wants[0], ' '.join(self._fetch_capabilities))))\n    for want in wants[1:]:\n        proto.write_pkt_line(('want %s\\n' % want))\n    proto.write_pkt_line(None)\n    have = graph_walker.next()\n    while have:\n        proto.write_pkt_line(('have %s\\n' % have))\n        if can_read():\n            pkt = proto.read_pkt_line()\n            parts = pkt.rstrip('\\n').split(' ')\n            if (parts[0] == 'ACK'):\n                graph_walker.ack(parts[1])\n                assert (parts[2] == 'continue')\n        have = graph_walker.next()\n    proto.write_pkt_line('done\\n')\n    pkt = proto.read_pkt_line()\n    while pkt:\n        parts = pkt.rstrip('\\n').split(' ')\n        if (parts[0] == 'ACK'):\n            graph_walker.ack(pkt.split(' ')[1])\n        if ((len(parts) < 3) or (parts[2] != 'continue')):\n            break\n        pkt = proto.read_pkt_line()\n    for pkt in proto.read_pkt_seq():\n        channel = ord(pkt[0])\n        pkt = pkt[1:]\n        if (channel == 1):\n            pack_data(pkt)\n        elif (channel == 2):\n            if (progress is not None):\n                progress(pkt)\n        else:\n            raise AssertionError(('Invalid sideband channel %d' % channel))\n    return refs\n", "label": 1}
{"function": "\n\ndef _harmonize_columns(self, parse_dates=None):\n    \"\\n        Make the DataFrame's column types align with the SQL table\\n        column types.\\n        Need to work around limited NA value support. Floats are always\\n        fine, ints must always be floats if there are Null values.\\n        Booleans are hard because converting bool column with None replaces\\n        all Nones with false. Therefore only convert bool if there are no\\n        NA values.\\n        Datetimes should already be converted to np.datetime64 if supported,\\n        but here we also force conversion if required\\n        \"\n    if ((parse_dates is True) or (parse_dates is None) or (parse_dates is False)):\n        parse_dates = []\n    if (not hasattr(parse_dates, '__iter__')):\n        parse_dates = [parse_dates]\n    for sql_col in self.table.columns:\n        col_name = sql_col.name\n        try:\n            df_col = self.frame[col_name]\n            col_type = self._get_dtype(sql_col.type)\n            if ((col_type is datetime) or (col_type is date) or (col_type is DatetimeTZDtype)):\n                self.frame[col_name] = _handle_date_column(df_col)\n            elif (col_type is float):\n                self.frame[col_name] = df_col.astype(col_type, copy=False)\n            elif (len(df_col) == df_col.count()):\n                if ((col_type is np.dtype('int64')) or (col_type is bool)):\n                    self.frame[col_name] = df_col.astype(col_type, copy=False)\n            if (col_name in parse_dates):\n                try:\n                    fmt = parse_dates[col_name]\n                except TypeError:\n                    fmt = None\n                self.frame[col_name] = _handle_date_column(df_col, format=fmt)\n        except KeyError:\n            pass\n", "label": 1}
{"function": "\n\ndef ParseAttributes(self, problems):\n    'Parse all attributes, calling problems as needed.\\n\\n    Return True if all of the values are valid.\\n    '\n    if util.IsEmpty(self.shape_id):\n        problems.MissingValue('shape_id')\n        return\n    try:\n        if (not isinstance(self.shape_pt_sequence, int)):\n            self.shape_pt_sequence = util.NonNegIntStringToInt(self.shape_pt_sequence, problems)\n        elif (self.shape_pt_sequence < 0):\n            problems.InvalidValue('shape_pt_sequence', self.shape_pt_sequence, 'Value should be a number (0 or higher)')\n    except (TypeError, ValueError):\n        problems.InvalidValue('shape_pt_sequence', self.shape_pt_sequence, 'Value should be a number (0 or higher)')\n        return\n    try:\n        if (not isinstance(self.shape_pt_lat, (int, float))):\n            self.shape_pt_lat = util.FloatStringToFloat(self.shape_pt_lat, problems)\n        if (abs(self.shape_pt_lat) > 90.0):\n            problems.InvalidValue('shape_pt_lat', self.shape_pt_lat)\n            return\n    except (TypeError, ValueError):\n        problems.InvalidValue('shape_pt_lat', self.shape_pt_lat)\n        return\n    try:\n        if (not isinstance(self.shape_pt_lon, (int, float))):\n            self.shape_pt_lon = util.FloatStringToFloat(self.shape_pt_lon, problems)\n        if (abs(self.shape_pt_lon) > 180.0):\n            problems.InvalidValue('shape_pt_lon', self.shape_pt_lon)\n            return\n    except (TypeError, ValueError):\n        problems.InvalidValue('shape_pt_lon', self.shape_pt_lon)\n        return\n    if ((abs(self.shape_pt_lat) < 1.0) and (abs(self.shape_pt_lon) < 1.0)):\n        problems.InvalidValue('shape_pt_lat', self.shape_pt_lat, \"Point location too close to 0, 0, which means that it's probably an incorrect location.\", type=problems_module.TYPE_WARNING)\n        return\n    if (self.shape_dist_traveled == ''):\n        self.shape_dist_traveled = None\n    if ((self.shape_dist_traveled is not None) and (not isinstance(self.shape_dist_traveled, (int, float)))):\n        try:\n            self.shape_dist_traveled = util.FloatStringToFloat(self.shape_dist_traveled, problems)\n        except (TypeError, ValueError):\n            problems.InvalidValue('shape_dist_traveled', self.shape_dist_traveled, 'This value should be a positive number.')\n            return\n    if ((self.shape_dist_traveled is not None) and (self.shape_dist_traveled < 0)):\n        problems.InvalidValue('shape_dist_traveled', self.shape_dist_traveled, 'This value should be a positive number.')\n        return\n    return True\n", "label": 1}
{"function": "\n\ndef _open(self, devpath, baudrate, databits, parity, stopbits, xonxoff, rtscts):\n    if (not isinstance(devpath, str)):\n        raise TypeError('Invalid devpath type, should be string.')\n    elif (not isinstance(baudrate, int)):\n        raise TypeError('Invalid baud rate type, should be integer.')\n    elif (not isinstance(databits, int)):\n        raise TypeError('Invalid data bits type, should be integer.')\n    elif (not isinstance(parity, str)):\n        raise TypeError('Invalid parity type, should be string.')\n    elif (not isinstance(stopbits, int)):\n        raise TypeError('Invalid stop bits type, should be integer.')\n    elif (not isinstance(xonxoff, bool)):\n        raise TypeError('Invalid xonxoff type, should be boolean.')\n    elif (not isinstance(rtscts, bool)):\n        raise TypeError('Invalid rtscts type, should be boolean.')\n    if (baudrate not in Serial._BAUDRATE_TO_OSPEED):\n        raise ValueError(('Unknown baud rate %d.' % baudrate))\n    elif (databits not in [5, 6, 7, 8]):\n        raise ValueError('Invalid data bits, can be 5, 6, 7, 8.')\n    elif (parity.lower() not in ['none', 'even', 'odd']):\n        raise ValueError('Invalid parity, can be: \"none\", \"even\", \"odd\".')\n    elif (stopbits not in [1, 2]):\n        raise ValueError('Invalid stop bits, can be 1, 2.')\n    try:\n        self._fd = os.open(devpath, (os.O_RDWR | os.O_NOCTTY))\n    except OSError as e:\n        raise SerialError(e.errno, ('Opening serial port: ' + e.strerror))\n    self._devpath = devpath\n    parity = parity.lower()\n    (iflag, oflag, cflag, lflag, ispeed, ospeed, cc) = (0, 0, 0, 0, 0, 0, ([0] * 32))\n    iflag = termios.IGNBRK\n    if (parity != 'none'):\n        iflag |= (termios.INPCK | termios.ISTRIP)\n    if xonxoff:\n        iflag |= (termios.IXON | termios.IXOFF)\n    oflag = 0\n    lflag = 0\n    cflag = (termios.CREAD | termios.CLOCAL)\n    cflag |= Serial._DATABITS_TO_CFLAG[databits]\n    if (parity == 'even'):\n        cflag |= termios.PARENB\n    elif (parity == 'odd'):\n        cflag |= (termios.PARENB | termios.PARODD)\n    if (stopbits == 2):\n        cflag |= termios.CSTOPB\n    if rtscts:\n        cflag |= termios.CRTSCTS\n    cflag |= Serial._BAUDRATE_TO_OSPEED[baudrate]\n    ispeed = Serial._BAUDRATE_TO_OSPEED[baudrate]\n    ospeed = Serial._BAUDRATE_TO_OSPEED[baudrate]\n    try:\n        termios.tcsetattr(self._fd, termios.TCSANOW, [iflag, oflag, cflag, lflag, ispeed, ospeed, cc])\n    except termios.error as e:\n        raise SerialError(e.errno, ('Setting serial port attributes: ' + e.strerror))\n", "label": 1}
{"function": "\n\ndef explore_uri(self, explorer_resource, show_back=True):\n    'INTERACTIVE exploration of capabilities document(s) starting at a given URI\\n\\n        Will flag warnings if the document is not of type listed in caps\\n        '\n    uri = explorer_resource.uri\n    caps = explorer_resource.acceptable_capabilities\n    checks = explorer_resource.checks\n    print(('Reading %s' % uri))\n    options = {\n        \n    }\n    capability = None\n    try:\n        if (caps == 'resource'):\n            self.explore_show_head(uri, check_headers=checks)\n        else:\n            s = Sitemap()\n            list = s.parse_xml(urllib.urlopen(uri))\n            (options, capability) = self.explore_show_summary(list, s.parsed_index, caps, context=uri)\n    except IOError as e:\n        print(('Cannot read %s (%s)' % (uri, str(e))))\n    except Exception as e:\n        print(('Cannot parse %s (%s)' % (uri, str(e))))\n    while True:\n        num_prompt = ('' if (len(options) == 0) else 'number, ')\n        up_prompt = ('b(ack), ' if show_back else '')\n        input = raw_input(('Follow [%s%sq(uit)]?' % (num_prompt, up_prompt)))\n        if (input in options.keys()):\n            break\n        if (input == 'q'):\n            raise ExplorerQuit()\n        if (input == 'b'):\n            return None\n    checks = {\n        \n    }\n    r = options[input]\n    if (r.capability is None):\n        if (capability in ['resourcelist', 'changelist', 'resourcedump', 'changedump']):\n            caps = 'resource'\n        else:\n            caps = self.allowed_entries(capability)\n    elif (r.capability is 'resource'):\n        caps = r.capability\n    else:\n        caps = [r.capability]\n    if (r.length is not None):\n        checks['content-length'] = r.length\n    if (r.lastmod is not None):\n        checks['last-modified'] = r.lastmod\n    if (r.mime_type is not None):\n        checks['content-type'] = r.mime_type\n    return XResource(options[input].uri, caps, checks)\n", "label": 1}
{"function": "\n\ndef parse_declaration(self, i):\n    rawdata = self.rawdata\n    j = (i + 2)\n    assert (rawdata[i:j] == '<!'), 'unexpected call to parse_declaration'\n    if (rawdata[j:(j + 1)] == '>'):\n        return (j + 1)\n    if (rawdata[j:(j + 1)] in ('-', '')):\n        return (- 1)\n    n = len(rawdata)\n    if (rawdata[j:(j + 2)] == '--'):\n        return self.parse_comment(i)\n    elif (rawdata[j] == '['):\n        return self.parse_marked_section(i)\n    else:\n        (decltype, j) = self._scan_name(j, i)\n    if (j < 0):\n        return j\n    if (decltype == 'doctype'):\n        self._decl_otherchars = ''\n    while (j < n):\n        c = rawdata[j]\n        if (c == '>'):\n            data = rawdata[(i + 2):j]\n            if (decltype == 'doctype'):\n                self.handle_decl(data)\n            else:\n                self.unknown_decl(data)\n            return (j + 1)\n        if (c in '\"\\''):\n            m = _declstringlit_match(rawdata, j)\n            if (not m):\n                return (- 1)\n            j = m.end()\n        elif (c in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n            (name, j) = self._scan_name(j, i)\n        elif (c in self._decl_otherchars):\n            j = (j + 1)\n        elif (c == '['):\n            if (decltype == 'doctype'):\n                j = self._parse_doctype_subset((j + 1), i)\n            elif (decltype in ('attlist', 'linktype', 'link', 'element')):\n                self.error((\"unsupported '[' char in %s declaration\" % decltype))\n            else:\n                self.error(\"unexpected '[' char in declaration\")\n        else:\n            self.error(('unexpected %r char in declaration' % rawdata[j]))\n        if (j < 0):\n            return j\n    return (- 1)\n", "label": 1}
{"function": "\n\ndef main():\n    parser = OptionParser()\n    parser.add_option('--tree', action='store_true', default=False, help='show tree view by default')\n    parser.add_option('--refresh', action='store', type='int', default=1, help='Refresh display every <seconds>')\n    parser.add_option('--follow', action='store', type='string', default='', help='Follow cgroup path')\n    parser.add_option('--fold', action='append', help='Fold cgroup sub tree')\n    parser.add_option('--type', action='append', help='Only show containers of this type')\n    parser.add_option('--columns', action='store', type='string', default='owner,type,processes,memory,cpu-sys,cpu-user,blkio,cpu-time', help=\"List of optional columns to display. Always includes 'name'\")\n    parser.add_option('--sort-col', action='store', type='string', default='cpu-user', help='Select column to sort by initially. Can be changed dynamically.')\n    (options, args) = parser.parse_args()\n    CONFIGURATION['tree'] = options.tree\n    CONFIGURATION['refresh_interval'] = float(options.refresh)\n    CONFIGURATION['columns'] = []\n    CONFIGURATION['fold'] = (options.fold or list())\n    CONFIGURATION['type'] = (options.type or list())\n    if options.follow:\n        CONFIGURATION['selected_line_name'] = options.follow\n        CONFIGURATION['follow'] = True\n    for col in options.columns.split(','):\n        col = col.strip()\n        if (col in COLUMNS_MANDATORY):\n            continue\n        if (not (col in COLUMNS_AVAILABLE)):\n            print('Invalid column name', col, file=sys.stderr)\n            print(__doc__)\n            sys.exit(1)\n        CONFIGURATION['columns'].append(col)\n    rebuild_columns()\n    if (options.sort_col not in COLUMNS_AVAILABLE):\n        print('Invalid sort column name', options.sort_col, file=sys.stderr)\n        print(__doc__)\n        sys.exit(1)\n    CONFIGURATION['sort_by'] = COLUMNS_AVAILABLE[options.sort_col].col_sort\n    measures = {\n        'data': defaultdict(dict),\n        'global': {\n            'total_cpu': multiprocessing.cpu_count(),\n            'total_memory': get_total_memory(),\n            'scheduler_frequency': os.sysconf('SC_CLK_TCK'),\n        },\n    }\n    init()\n    if (not CGROUP_MOUNTPOINTS):\n        print('[ERROR] Failed to locate cgroup mountpoints.', file=sys.stderr)\n        diagnose()\n        sys.exit(1)\n    results = None\n    try:\n        stdscr = curses.initscr()\n        init_screen()\n        stdscr.keypad(1)\n        curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_GREEN)\n        curses.init_pair(2, curses.COLOR_BLACK, curses.COLOR_CYAN)\n        curses.init_pair(3, curses.COLOR_WHITE, (- 1))\n        curses.init_pair(4, curses.COLOR_CYAN, (- 1))\n        while True:\n            collect(measures)\n            results = built_statistics(measures, CONFIGURATION)\n            display(stdscr, results, CONFIGURATION)\n            sleep_start = time.time()\n            while (CONFIGURATION['pause_refresh'] or (time.time() < (sleep_start + CONFIGURATION['refresh_interval']))):\n                if CONFIGURATION['pause_refresh']:\n                    to_sleep = (- 1)\n                else:\n                    to_sleep = int((((sleep_start + CONFIGURATION['refresh_interval']) - time.time()) * 1000))\n                ret = event_listener(stdscr, to_sleep)\n                if (ret == 2):\n                    display(stdscr, results, CONFIGURATION)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        curses.nocbreak()\n        stdscr.keypad(0)\n        curses.echo()\n        curses.endwin()\n    if ((results is not None) and (len(results) < 2)):\n        print('[WARN] Failed to find any relevant cgroup/container.', file=sys.stderr)\n        diagnose()\n", "label": 1}
{"function": "\n\ndef extract_fb_data(data):\n    'Extact and normalize facebook data as parsed from the graph JSON'\n    nick = None\n    link = data.get('link')\n    if link:\n        last = link.split('/')[(- 1)]\n        if (last != data['id']):\n            nick = last\n    profile = {\n        'accounts': [{\n            'domain': 'facebook.com',\n            'userid': data['id'],\n        }],\n        'displayName': data['name'],\n        'preferredUsername': (nick or data['name']),\n    }\n    gender = data.get('gender')\n    if gender:\n        profile['gender'] = gender\n    email = data.get('email')\n    if email:\n        profile['emails'] = [{\n            'value': email,\n            'primary': True,\n        }]\n        if (data.get('verified') and email):\n            profile['verifiedEmail'] = email\n    tz = data.get('timezone')\n    if tz:\n        offset = float(tz)\n        h = int(offset)\n        m = int((abs((offset - h)) * 60))\n        profile['utcOffset'] = '{h:+03d}:{m:02d}'.format(h=h, m=m)\n    bday = data.get('birthday')\n    if bday:\n        try:\n            (mth, day, yr) = bday.split('/')\n            date = datetime.date(int(yr), int(mth), int(day))\n            profile['birthday'] = date.strftime('%Y-%m-%d')\n        except ValueError:\n            pass\n    name = {\n        \n    }\n    pcard_map = {\n        'first_name': 'givenName',\n        'last_name': 'familyName',\n    }\n    for (key, val) in pcard_map.items():\n        part = data.get(key)\n        if part:\n            name[val] = part\n    name['formatted'] = data['name']\n    profile['name'] = name\n    for (k, v) in profile.items():\n        if ((not v) or (isinstance(v, list) and (not v[0]))):\n            del profile[k]\n    return profile\n", "label": 1}
{"function": "\n\ndef assert_both_values(actual, expected_plain, expected_color, kind=None):\n    'Handle asserts for color and non-color strings in color and non-color tests.\\n\\n    :param ColorStr actual: Return value of ColorStr class method.\\n    :param expected_plain: Expected non-color value.\\n    :param expected_color: Expected color value.\\n    :param str kind: Type of string to test.\\n    '\n    if kind.endswith('plain'):\n        assert (actual.value_colors == expected_plain)\n        assert (actual.value_no_colors == expected_plain)\n        assert (actual.has_colors is False)\n    elif kind.endswith('color'):\n        assert (actual.value_colors == expected_color)\n        assert (actual.value_no_colors == expected_plain)\n        if ('\\x1b' in actual.value_colors):\n            assert (actual.has_colors is True)\n        else:\n            assert (actual.has_colors is False)\n    else:\n        assert (actual == expected_plain)\n    if kind.startswith('ColorStr'):\n        assert (actual.__class__ == ColorStr)\n    elif kind.startswith('Color'):\n        assert (actual.__class__ == Color)\n", "label": 1}
{"function": "\n\ndef post(self, request, *args, **kwargs):\n    obj_ids = request.POST.getlist('object_ids')\n    action = request.POST['action']\n    m = re.search('.delete([a-z]+)', action).group(1)\n    if (obj_ids == []):\n        obj_ids.append(re.search('([0-9a-z-]+)$', action).group(1))\n    if (m == 'monitor'):\n        for obj_id in obj_ids:\n            try:\n                api.lbaas.pool_health_monitor_delete(request, obj_id)\n                messages.success(request, (_('Deleted monitor %s') % obj_id))\n            except Exception as e:\n                exceptions.handle(request, (_('Unable to delete monitor. %s') % e))\n    if (m == 'pool'):\n        for obj_id in obj_ids:\n            try:\n                api.lbaas.pool_delete(request, obj_id)\n                messages.success(request, (_('Deleted pool %s') % obj_id))\n            except Exception as e:\n                exceptions.handle(request, (_('Unable to delete pool. %s') % e))\n    if (m == 'member'):\n        for obj_id in obj_ids:\n            try:\n                api.lbaas.member_delete(request, obj_id)\n                messages.success(request, (_('Deleted member %s') % obj_id))\n            except Exception as e:\n                exceptions.handle(request, (_('Unable to delete member. %s') % e))\n    if (m == 'vip'):\n        for obj_id in obj_ids:\n            try:\n                vip_id = api.lbaas.pool_get(request, obj_id).vip_id\n            except Exception as e:\n                exceptions.handle(request, (_('Unable to locate VIP to delete. %s') % e))\n            if (vip_id is not None):\n                try:\n                    api.lbaas.vip_delete(request, vip_id)\n                    messages.success(request, (_('Deleted VIP %s') % vip_id))\n                except Exception as e:\n                    exceptions.handle(request, (_('Unable to delete VIP. %s') % e))\n    return self.get(request, *args, **kwargs)\n", "label": 1}
{"function": "\n\ndef _findPathsRecurse(self, source_stp, dest_stp, bandwidth, exclude_networks=None):\n    source_network = self.getNetwork(source_stp.network)\n    dest_network = self.getNetwork(dest_stp.network)\n    source_port = source_network.getPort(source_stp.port)\n    dest_port = dest_network.getPort(dest_stp.port)\n    if (not (source_port.canMatchLabel(source_stp.label) or dest_port.canMatchLabel(dest_stp.label))):\n        return []\n    if (source_port.isBidirectional() and dest_port.isBidirectional()):\n        if (source_stp.network == dest_stp.network):\n            try:\n                if source_network.canSwapLabel(source_stp.label.type_):\n                    source_label = source_port.label().intersect(source_stp.label)\n                    dest_label = dest_port.label().intersect(dest_stp.label)\n                else:\n                    source_label = source_port.label().intersect(dest_port.label()).intersect(source_stp.label).intersect(dest_stp.label)\n                    dest_label = source_label\n                link = nsa.Link(source_stp.network, source_stp.port, dest_stp.port, source_label, dest_label)\n                return [[link]]\n            except nsa.EmptyLabelSet:\n                return []\n        else:\n            link_ports = source_network.findPorts(True, source_stp.label, source_stp.port)\n            link_ports = [port for port in link_ports if port.hasRemote()]\n            links = []\n            for lp in link_ports:\n                demarcation = self.findDemarcationPort(lp)\n                if (demarcation is None):\n                    continue\n                (d_network_id, d_port_id) = demarcation\n                if ((exclude_networks is not None) and (demarcation[0] in exclude_networks)):\n                    continue\n                demarcation_label = (lp.label() if source_network.canSwapLabel(source_stp.label.type_) else source_stp.label.intersect(lp.label()))\n                demarcation_stp = nsa.STP(demarcation[0], demarcation[1], demarcation_label)\n                sub_exclude_networks = ([source_network.id_] + (exclude_networks or []))\n                sub_links = self._findPathsRecurse(demarcation_stp, dest_stp, bandwidth, sub_exclude_networks)\n                if (not sub_links):\n                    continue\n                for sl in sub_links:\n                    if source_network.canSwapLabel(source_stp.label.type_):\n                        source_label = source_port.label().intersect(source_stp.label)\n                        dest_label = lp.label().intersect(sl[0].src_stp.label)\n                    else:\n                        source_label = source_port.label().intersect(source_stp.label).intersect(lp.label()).intersect(sl[0].src_stp.label)\n                        dest_label = source_label\n                    first_link = nsa.Link(source_stp.network, source_stp.port, lp.id_, source_label, dest_label)\n                    path = ([first_link] + sl)\n                    links.append(path)\n            return sorted(links, key=len)\n    else:\n        raise error.TopologyError('Unidirectional path-finding not implemented yet')\n", "label": 1}
{"function": "\n\ndef _verify_bundle_with_manifest(catalog, manifest, check_shas=True, should_lock=False, **kwargs):\n    if (not check_shas):\n        (yield Message.warn('Skipping SHA digest verification for bundle files.'))\n    for (path, info) in manifest.files.iteritems():\n        sha = manifest.sha_for_file(path)\n        if (kwargs.get('verified_files') is not None):\n            if (sha in kwargs['verified_files']):\n                log.debug(('Skipping %s' % sha))\n                continue\n            else:\n                kwargs['verified_files'].add(sha)\n        meta = catalog._get_file_info(sha)\n        if (meta is None):\n            (yield Message.error(('File %s not exist' % sha)))\n            continue\n        format = meta['format']\n        meta_size = meta['size']\n        manifest_size = info['formats'][format]['size']\n        log.debug(('file=%s format=%s meta_size=%s manifest_size=%s' % (sha, format, meta_size, manifest_size)))\n        if (meta_size != manifest_size):\n            (yield Message.error(('File %s wrong size' % sha)))\n            continue\n        if check_shas:\n            ext = helpers.file_extension_for_format(format)\n            with catalog._read_file(sha, ext=ext) as f:\n                b = f.read()\n                if (format == Formats.GZ):\n                    b = utils.gunzip_bytes(b)\n                digest = hashlib.sha1(b).hexdigest()\n                if (digest != sha):\n                    (yield Message.error(('File %s wrong hash' % sha)))\n                    continue\n        (yield Message.info(('File %s OK' % sha)))\n    flavors = list(manifest.flavors)\n    flavors.append(None)\n    for flavor in flavors:\n        archive_name = catalog.path_helper.archive_name(manifest.bundle_name, manifest.version, flavor=flavor)\n        meta = catalog._get_archive_info(manifest.bundle_name, manifest.version, flavor=flavor)\n        if (meta is None):\n            if (len(manifest.get_all_files(flavor=flavor)) == 1):\n                continue\n            elif ((flavor is None) and (len(flavors) > 1)):\n                Message.warn(('Archive %s not found.' % archive_name))\n                continue\n            else:\n                (yield Message.error(('Archive %s not found.' % archive_name)))\n        for result in _verify_archive(catalog, manifest, flavor=flavor, check_shas=check_shas):\n            (yield result)\n", "label": 1}
{"function": "\n\n@register_opt('fast_compile')\n@op_lifter([tensor.CAReduce, tensor.Sum, tensor.elemwise.Prod])\ndef local_gpua_careduce(node, context_name):\n    if isinstance(node.op.scalar_op, (scalar.Add, scalar.Mul, scalar.Maximum, scalar.Minimum)):\n        ctx = get_context(context_name)\n        if (ctx.kind == 'opencl'):\n            op = GpuCAReduceCPY\n            if (node.op.scalar_op not in [scalar.add, scalar.mul]):\n                return\n        elif (ctx.kind == 'cuda'):\n            op = GpuCAReduceCuda\n        else:\n            return False\n        (x,) = node.inputs\n        greduce = op(node.op.scalar_op, axis=node.op.axis, dtype=getattr(node.op, 'dtype', None), acc_dtype=getattr(node.op, 'acc_dtype', None))\n        gvar = greduce(x)\n        if ((op is GpuCAReduceCPY) or gvar.owner.op.supports_c_code([GpuFromHost(context_name)(x)])):\n            return greduce\n        else:\n            if (node.op.axis is None):\n                reduce_mask = ([1] * x.type.ndim)\n            else:\n                reduce_mask = ([0] * x.type.ndim)\n                for a in node.op.axis:\n                    assert (reduce_mask[a] == 0)\n                    reduce_mask[a] = 1\n            shape_of = node.fgraph.shape_feature.shape_of\n            x_shape = shape_of[x]\n            new_in_shp = [x_shape[0]]\n            new_mask = [reduce_mask[0]]\n            for i in xrange(1, x.type.ndim):\n                if (reduce_mask[i] == reduce_mask[(i - 1)]):\n                    new_in_shp[(- 1)] *= x_shape[i]\n                else:\n                    new_mask.append(reduce_mask[i])\n                    new_in_shp.append(x_shape[i])\n            new_axis = []\n            for (idx, m) in enumerate(new_mask):\n                if (m == 1):\n                    new_axis.append(idx)\n            greduce = op(node.op.scalar_op, axis=new_axis, reduce_mask=new_mask, dtype=getattr(node.op, 'dtype', None), acc_dtype=getattr(node.op, 'acc_dtype', None))\n            reshaped_x = x.reshape(tensor.stack(new_in_shp))\n            gpu_reshaped_x = GpuFromHost(context_name)(reshaped_x)\n            gvar = greduce(gpu_reshaped_x)\n            reshaped_gpu_inputs = [gpu_reshaped_x]\n            if greduce.supports_c_code(reshaped_gpu_inputs):\n                reduce_reshaped_x = host_from_gpu(greduce(gpu_reshaped_x))\n                if (reduce_reshaped_x.ndim != node.outputs[0].ndim):\n                    unreshaped_reduce = reduce_reshaped_x.reshape(tensor.stack(shape_of[node.outputs[0]]))\n                else:\n                    unreshaped_reduce = reduce_reshaped_x\n                return [unreshaped_reduce]\n", "label": 1}
{"function": "\n\ndef input(self, prompt, hint=None, validators=None, repeat_until_valid=False, task=None, leave_when_cancel=None, **kwargs):\n    has_default = ('default' in kwargs)\n    default = kwargs.get('default', None)\n    leave_when_cancel = (leave_when_cancel or bool(task))\n    message_components = [prompt, ' ']\n    if (hint is not None):\n        message_components.append('({0})'.format(hint))\n    if has_default:\n        message_components.append('[{0}]'.format(default))\n    message = (''.join(message_components).strip() + ': ')\n    while True:\n        has_result = True\n        try:\n            result = six.moves.input(message).strip()\n        except KeyboardInterrupt:\n            self.show('')\n            error_msg = 'User cancelled input.'\n            self.error(error_msg)\n            if (task and leave_when_cancel):\n                task.exit(1)\n        else:\n            if (((result is None) or (len(result) == 0)) and has_default):\n                result = default\n            if validators:\n                for validator in validators:\n                    try:\n                        result = validator(result)\n                    except Exception as e:\n                        has_result = False\n                        if has_default:\n                            result = validator(default)\n                        else:\n                            self.error(str(e))\n                            break\n            if ((not repeat_until_valid) or has_result):\n                return result\n", "label": 1}
{"function": "\n\ndef _get_localzone(_root='/'):\n    'Tries to find the local timezone configuration.\\n    This method prefers finding the timezone name and passing that to pytz,\\n    over passing in the localtime file, as in the later case the zoneinfo\\n    name is unknown.\\n    The parameter _root makes the function look for files like /etc/localtime\\n    beneath the _root directory. This is primarily used by the tests.\\n    In normal usage you call the function without parameters.\\n    '\n    tzenv = os.environ.get('TZ')\n    if tzenv:\n        return _tz_from_env(tzenv)\n    try:\n        link_dst = os.readlink('/etc/localtime')\n    except OSError:\n        pass\n    else:\n        pos = link_dst.find('/zoneinfo/')\n        if (pos >= 0):\n            zone_name = link_dst[(pos + 10):]\n            try:\n                return pytz.timezone(zone_name)\n            except pytz.UnknownTimeZoneError:\n                pass\n    if (sys.platform == 'darwin'):\n        c = subprocess.Popen(['systemsetup', '-gettimezone'], stdout=subprocess.PIPE)\n        sys_result = c.communicate()[0]\n        c.wait()\n        tz_match = _systemconfig_tz.search(sys_result)\n        if (tz_match is not None):\n            zone_name = tz_match.group(1)\n            try:\n                return pytz.timezone(zone_name)\n            except pytz.UnknownTimeZoneError:\n                pass\n    tzpath = os.path.join(_root, 'etc/timezone')\n    if os.path.exists(tzpath):\n        with open(tzpath, 'rb') as tzfile:\n            data = tzfile.read()\n            if (data[:5] != 'TZif2'):\n                etctz = data.strip().decode()\n                if (' ' in etctz):\n                    (etctz, dummy) = etctz.split(' ', 1)\n                if ('#' in etctz):\n                    (etctz, dummy) = etctz.split('#', 1)\n                return pytz.timezone(etctz.replace(' ', '_'))\n    zone_re = re.compile('\\\\s*ZONE\\\\s*=\\\\s*\"')\n    timezone_re = re.compile('\\\\s*TIMEZONE\\\\s*=\\\\s*\"')\n    end_re = re.compile('\"')\n    for filename in ('etc/sysconfig/clock', 'etc/conf.d/clock'):\n        tzpath = os.path.join(_root, filename)\n        if (not os.path.exists(tzpath)):\n            continue\n        with open(tzpath, 'rt') as tzfile:\n            data = tzfile.readlines()\n        for line in data:\n            match = zone_re.match(line)\n            if (match is None):\n                match = timezone_re.match(line)\n            if (match is not None):\n                line = line[match.end():]\n                etctz = line[:end_re.search(line).start()]\n                return pytz.timezone(etctz.replace(' ', '_'))\n    for filename in ('etc/localtime', 'usr/local/etc/localtime'):\n        tzpath = os.path.join(_root, filename)\n        if (not os.path.exists(tzpath)):\n            continue\n        with open(tzpath, 'rb') as tzfile:\n            return pytz.tzfile.build_tzinfo('local', tzfile)\n    raise pytz.UnknownTimeZoneError('Can not find any timezone configuration')\n", "label": 1}
{"function": "\n\ndef update_menu(self, menu_name, menus, submenu):\n    'Update the menu.'\n    if exists(self.menu_path):\n        menu = join(self.menu_path, menu_name)\n        vc_internal = []\n        vc_internal_menu = None\n        if self.show_int:\n            if (not self.svn_disabled):\n                vc_internal.append(menus['svn']['internal'])\n            if (not self.git_disabled):\n                vc_internal.append(menus['git']['internal'])\n            if (not self.hg_disabled):\n                vc_internal.append(menus['hg']['internal'])\n            if len(vc_internal):\n                vc_internal_menu = ',\\n'.join(vc_internal)\n        vc_external = []\n        vc_external_menu = None\n        if self.show_ext:\n            if (not self.svn_disabled):\n                vc_external.append(menus['svn']['external'])\n            if (not self.git_disabled):\n                vc_external.append(menus['git']['external'])\n            if (not self.hg_disabled):\n                vc_external.append(menus['hg']['external'])\n            if len(vc_external):\n                vc_external_menu = ',\\n'.join(vc_external)\n        with open(menu, 'w') as f:\n            f.write(((DIFF_SUBMENU if submenu else DIFF_MENU) % {\n                'internal': ('' if (not self.show_int) else (menus['internal'] % {\n                    'file_name': self.name,\n                })),\n                'external': ('' if (not self.show_ext) else (menus['external'] % {\n                    'file_name': self.name,\n                })),\n                'vc_internal': ('' if ((vc_internal_menu is None) or (not self.show_int)) else (VC_INTERNAL_MENU % {\n                    'vc': vc_internal_menu,\n                })),\n                'vc_external': ('' if ((vc_external_menu is None) or (not self.show_ext)) else (VC_EXTERNAL_MENU % {\n                    'vc': vc_external_menu,\n                })),\n            }))\n", "label": 1}
{"function": "\n\ndef _process_recv_buffer(self):\n    '\\n        Process the :attr:`~pants.stream.Stream._recv_buffer`, passing\\n        chunks of data to :meth:`~pants.stream.Stream.on_read`.\\n        '\n    while self._recv_buffer:\n        delimiter = self.read_delimiter\n        if (delimiter is None):\n            data = self._recv_buffer\n            self._recv_buffer = ''\n            self._safely_call(self.on_read, data)\n        elif isinstance(delimiter, (int, long)):\n            if (len(self._recv_buffer) < delimiter):\n                break\n            data = self._recv_buffer[:delimiter]\n            self._recv_buffer = self._recv_buffer[delimiter:]\n            self._safely_call(self.on_read, data)\n        elif isinstance(delimiter, basestring):\n            mark = self._recv_buffer.find(delimiter)\n            if (mark == (- 1)):\n                break\n            data = self._recv_buffer[:mark]\n            self._recv_buffer = self._recv_buffer[(mark + len(delimiter)):]\n            self._safely_call(self.on_read, data)\n        elif isinstance(delimiter, Struct):\n            if (len(self._recv_buffer) < delimiter.size):\n                break\n            data = self._recv_buffer[:delimiter.size]\n            self._recv_buffer = self._recv_buffer[delimiter.size:]\n            try:\n                data = delimiter.unpack(data)\n            except struct.error:\n                log.exception(('Unable to unpack data on %r.' % self))\n                self.close()\n                break\n            self._safely_call(self.on_read, *data)\n        elif isinstance(delimiter, _NetStruct):\n            if (not self._netstruct_iter):\n                self._netstruct_iter = delimiter.iter_unpack()\n                self._netstruct_needed = next(self._netstruct_iter)\n            if (len(self._recv_buffer) < self._netstruct_needed):\n                break\n            data = self._netstruct_iter.send(self._recv_buffer[:self._netstruct_needed])\n            self._recv_buffer = self._recv_buffer[self._netstruct_needed:]\n            if isinstance(data, (int, long)):\n                self._netstruct_needed = data\n                continue\n            self._netstruct_needed = None\n            self._netstruct_iter = None\n            self._safely_call(self.on_read, *data)\n        elif isinstance(delimiter, RegexType):\n            if self.regex_search:\n                match = delimiter.search(self._recv_buffer)\n                if (not match):\n                    break\n                data = self._recv_buffer[:match.start()]\n                self._recv_buffer = self._recv_buffer[match.end():]\n            else:\n                data = delimiter.match(self._recv_buffer)\n                if (not data):\n                    break\n                self._recv_buffer = self._recv_buffer[data.end():]\n            self._safely_call(self.on_read, data)\n        else:\n            err = InvalidReadDelimiterError(('Invalid read delimiter on %r.' % self))\n            self._safely_call(self.on_error, err)\n            break\n        if (self._closed or (not self.connected)):\n            break\n", "label": 1}
{"function": "\n\ndef _parseAddressIPv6(ipstr):\n    \"\\n    Internal function used by parseAddress() to parse IPv6 address with ':'.\\n\\n    >>> print(_parseAddressIPv6('::'))\\n    0\\n    >>> print(_parseAddressIPv6('::1'))\\n    1\\n    >>> print(_parseAddressIPv6('0:0:0:0:0:0:0:1'))\\n    1\\n    >>> print(_parseAddressIPv6('0:0:0::0:0:1'))\\n    1\\n    >>> print(_parseAddressIPv6('0:0:0:0:0:0:0:0'))\\n    0\\n    >>> print(_parseAddressIPv6('0:0:0::0:0:0'))\\n    0\\n\\n    >>> print(_parseAddressIPv6('FEDC:BA98:7654:3210:FEDC:BA98:7654:3210'))\\n    338770000845734292534325025077361652240\\n    >>> print(_parseAddressIPv6('1080:0000:0000:0000:0008:0800:200C:417A'))\\n    21932261930451111902915077091070067066\\n    >>> print(_parseAddressIPv6('1080:0:0:0:8:800:200C:417A'))\\n    21932261930451111902915077091070067066\\n    >>> print(_parseAddressIPv6('1080:0::8:800:200C:417A'))\\n    21932261930451111902915077091070067066\\n    >>> print(_parseAddressIPv6('1080::8:800:200C:417A'))\\n    21932261930451111902915077091070067066\\n    >>> print(_parseAddressIPv6('FF01:0:0:0:0:0:0:43'))\\n    338958331222012082418099330867817087043\\n    >>> print(_parseAddressIPv6('FF01:0:0::0:0:43'))\\n    338958331222012082418099330867817087043\\n    >>> print(_parseAddressIPv6('FF01::43'))\\n    338958331222012082418099330867817087043\\n    >>> print(_parseAddressIPv6('0:0:0:0:0:0:13.1.68.3'))\\n    218186755\\n    >>> print(_parseAddressIPv6('::13.1.68.3'))\\n    218186755\\n    >>> print(_parseAddressIPv6('0:0:0:0:0:FFFF:129.144.52.38'))\\n    281472855454758\\n    >>> print(_parseAddressIPv6('::FFFF:129.144.52.38'))\\n    281472855454758\\n    >>> print(_parseAddressIPv6('1080:0:0:0:8:800:200C:417A'))\\n    21932261930451111902915077091070067066\\n    >>> print(_parseAddressIPv6('1080::8:800:200C:417A'))\\n    21932261930451111902915077091070067066\\n    >>> print(_parseAddressIPv6('::1:2:3:4:5:6'))\\n    1208962713947218704138246\\n    >>> print(_parseAddressIPv6('1:2:3:4:5:6::'))\\n    5192455318486707404433266432802816\\n    \"\n    items = []\n    index = 0\n    fill_pos = None\n    while (index < len(ipstr)):\n        text = ipstr[index:]\n        if text.startswith('::'):\n            if (fill_pos is not None):\n                raise ValueError((\"%r: Invalid IPv6 address: more than one '::'\" % ipstr))\n            fill_pos = len(items)\n            index += 2\n            continue\n        pos = text.find(':')\n        if (pos == 0):\n            raise ValueError(('%r: Invalid IPv6 address' % ipstr))\n        if (pos != (- 1)):\n            items.append(text[:pos])\n            if (text[pos:(pos + 2)] == '::'):\n                index += pos\n            else:\n                index += (pos + 1)\n            if (index == len(ipstr)):\n                raise ValueError(('%r: Invalid IPv6 address' % ipstr))\n        else:\n            items.append(text)\n            break\n    if (items and ('.' in items[(- 1)])):\n        if ((fill_pos is not None) and (not (fill_pos <= (len(items) - 1)))):\n            raise ValueError((\"%r: Invalid IPv6 address: '::' after IPv4\" % ipstr))\n        value = parseAddress(items[(- 1)])[0]\n        items = (items[:(- 1)] + [('%04x' % (value >> 16)), ('%04x' % (value & 65535))])\n    if (fill_pos is not None):\n        diff = (8 - len(items))\n        if (diff <= 0):\n            raise ValueError((\"%r: Invalid IPv6 address: '::' is not needed\" % ipstr))\n        items = ((items[:fill_pos] + (['0'] * diff)) + items[fill_pos:])\n    if (len(items) != 8):\n        raise ValueError(('%r: Invalid IPv6 address: should have 8 hextets' % ipstr))\n    value = 0\n    index = 0\n    for item in items:\n        try:\n            item = int(item, 16)\n            error = (not (0 <= item <= 65535))\n        except ValueError:\n            error = True\n        if error:\n            raise ValueError(('%r: Invalid IPv6 address: invalid hexlet %r' % (ipstr, item)))\n        value = ((value << 16) + item)\n        index += 1\n    return value\n", "label": 1}
{"function": "\n\ndef map_partitions(func, metadata, *args, **kwargs):\n    ' Apply Python function on each DataFrame block\\n\\n    Parameters\\n    ----------\\n\\n    metadata: _Frame, columns, name\\n        Metadata for output\\n    targets : list\\n        List of target DataFrame / Series.\\n    '\n    metadata = _extract_pd(metadata)\n    assert callable(func)\n    token = kwargs.pop('token', 'map-partitions')\n    token_key = tokenize((token or func), metadata, kwargs, *args)\n    name = '{0}-{1}'.format(token, token_key)\n    if all((isinstance(arg, Scalar) for arg in args)):\n        dask = {\n            (name, 0): (apply, func, (tuple, [(arg._name, 0) for arg in args]), kwargs),\n        }\n        return Scalar(merge(dask, *[arg.dask for arg in args]), name)\n    args = _maybe_from_pandas(args)\n    from .multi import _maybe_align_partitions\n    args = _maybe_align_partitions(args)\n    dfs = [df for df in args if isinstance(df, _Frame)]\n    if (metadata is no_default):\n        try:\n            metadata = _emulate(func, *args, **kwargs)\n        except Exception:\n            metadata = None\n    if isinstance(metadata, pd.DataFrame):\n        columns = metadata.columns\n    elif isinstance(metadata, pd.Series):\n        columns = metadata.name\n    else:\n        columns = metadata\n    return_type = _get_return_type(dfs[0], metadata)\n    dsk = {\n        \n    }\n    for i in range(dfs[0].npartitions):\n        values = [((arg._name, (i if isinstance(arg, _Frame) else 0)) if isinstance(arg, (_Frame, Scalar)) else arg) for arg in args]\n        values = (apply, func, (tuple, values), kwargs)\n        dsk[(name, i)] = (_rename, columns, values)\n    dasks = [arg.dask for arg in args if isinstance(arg, (_Frame, Scalar))]\n    return return_type(merge(dsk, *dasks), name, metadata, args[0].divisions)\n", "label": 1}
{"function": "\n\ndef parse_tag_input(input):\n    '\\n    Parses tag input, with multiple word input being activated and\\n    delineated by commas and double quotes. Quotes take precedence, so\\n    they may contain commas.\\n\\n    Returns a sorted list of unique tag names.\\n    '\n    if (not input):\n        return []\n    input = force_text(input)\n    if ((',' not in input) and ('\"' not in input)):\n        words = list(set(split_strip(input, ' ')))\n        words.sort()\n        return words\n    words = []\n    buffer = []\n    to_be_split = []\n    saw_loose_comma = False\n    open_quote = False\n    i = iter(input)\n    try:\n        while 1:\n            c = next(i)\n            if (c == '\"'):\n                if buffer:\n                    to_be_split.append(''.join(buffer))\n                    buffer = []\n                open_quote = True\n                c = next(i)\n                while (c != '\"'):\n                    buffer.append(c)\n                    c = next(i)\n                if buffer:\n                    word = ''.join(buffer).strip()\n                    if word:\n                        words.append(word)\n                    buffer = []\n                open_quote = False\n            else:\n                if ((not saw_loose_comma) and (c == ',')):\n                    saw_loose_comma = True\n                buffer.append(c)\n    except StopIteration:\n        if buffer:\n            if (open_quote and (',' in buffer)):\n                saw_loose_comma = True\n            to_be_split.append(''.join(buffer))\n    if to_be_split:\n        if saw_loose_comma:\n            delimiter = ','\n        else:\n            delimiter = ' '\n        for chunk in to_be_split:\n            words.extend(split_strip(chunk, delimiter))\n    words = list(set(words))\n    words.sort()\n    return words\n", "label": 1}
{"function": "\n\ndef run(self):\n    selectorClass = getattr(eval(self.config['selector_type']), (self.config['selector_type'].title() + 'Selector'))\n    results = dict()\n    results['project'] = self.args['<projectname>']\n    results['data'] = list()\n    try:\n        result = dict()\n        tabular_data_headers = dict()\n        if (self.args['--verbosity'] > 0):\n            print()\n            print(((Back.YELLOW + Fore.BLUE) + 'Loading page '), ((self.config['scraping']['url'] + Back.RESET) + Fore.RESET), end='')\n        selector = selectorClass(self.config['scraping']['url'])\n        for attribute in self.config['scraping']['data']:\n            if (attribute['field'] != ''):\n                if (self.args['--verbosity'] > 1):\n                    print('\\nExtracting', attribute['field'], 'attribute', sep=' ', end='')\n                result[attribute['field']] = selector.extract_content(attribute['selector'], attribute['attr'], attribute['default'])\n        if (not self.config['scraping'].get('table')):\n            result_list = [result]\n        else:\n            tables = self.config['scraping'].get('table')\n            for table in tables:\n                (table_headers, result_list) = selector.extract_tabular(result=result, table_type=table.get('table_type', 'rows'), header=table.get('header', []), prefix=table.get('prefix', ''), suffix=table.get('suffix', ''), selector=table.get('selector', ''), attr=table.get('attr', 'text'), default=table.get('default', ''), verbosity=self.args['--verbosity'])\n                for th in table_headers:\n                    if (not (th in tabular_data_headers)):\n                        tabular_data_headers[th] = len(tabular_data_headers)\n        if (not self.config['scraping'].get('next')):\n            results['data'].extend(result_list)\n        else:\n            for next in self.config['scraping']['next']:\n                for (tdh, r) in traverse_next(selector, next, result, verbosity=self.args['--verbosity']):\n                    results['data'].append(r)\n                    for th in tdh:\n                        if (not (th in tabular_data_headers)):\n                            tabular_data_headers[th] = len(tabular_data_headers)\n    except KeyboardInterrupt:\n        pass\n    except Exception as e:\n        print(e)\n    finally:\n        if (self.args['--output_type'] == 'json'):\n            import json\n            with open(os.path.join(os.getcwd(), (self.args['<output_filename>'] + '.json')), 'w') as f:\n                json.dump(results, f)\n        elif (self.args['--output_type'] == 'csv'):\n            import csv\n            with open(os.path.join(os.getcwd(), (self.args['<output_filename>'] + '.csv')), 'w') as f:\n                fields = extract_fieldnames(self.config)\n                data_headers = sorted(tabular_data_headers, key=(lambda x: tabular_data_headers[x]))\n                fields.extend(data_headers)\n                writer = csv.DictWriter(f, fieldnames=fields)\n                writer.writeheader()\n                writer.writerows(results['data'])\n        if (self.args['--verbosity'] > 0):\n            print()\n            print(((Back.WHITE + Fore.RED) + self.args['<output_filename>']), '.', self.args['--output_type'], ((' has been created' + Back.RESET) + Fore.RESET), sep='')\n", "label": 1}
{"function": "\n\ndef _truncate_html(self, length, truncate, text, truncate_len, words):\n    '\\n        Truncates HTML to a certain number of chars (not counting tags and\\n        comments), or, if words is True, then to a certain number of words.\\n        Closes opened tags if they were correctly closed in the given HTML.\\n\\n        Newlines in the HTML are preserved.\\n        '\n    if (words and (length <= 0)):\n        return ''\n    html4_singlets = ('br', 'col', 'link', 'base', 'img', 'param', 'area', 'hr', 'input')\n    pos = 0\n    end_text_pos = 0\n    current_len = 0\n    open_tags = []\n    regex = (re_words if words else re_chars)\n    while (current_len <= length):\n        m = regex.search(text, pos)\n        if (not m):\n            break\n        pos = m.end(0)\n        if m.group(1):\n            current_len += 1\n            if (current_len == truncate_len):\n                end_text_pos = pos\n            continue\n        tag = re_tag.match(m.group(0))\n        if ((not tag) or (current_len >= truncate_len)):\n            continue\n        (closing_tag, tagname, self_closing) = tag.groups()\n        tagname = tagname.lower()\n        if (self_closing or (tagname in html4_singlets)):\n            pass\n        elif closing_tag:\n            try:\n                i = open_tags.index(tagname)\n            except ValueError:\n                pass\n            else:\n                open_tags = open_tags[(i + 1):]\n        else:\n            open_tags.insert(0, tagname)\n    if (current_len <= length):\n        return text\n    out = text[:end_text_pos]\n    truncate_text = self.add_truncation_text('', truncate)\n    if truncate_text:\n        out += truncate_text\n    for tag in open_tags:\n        out += ('</%s>' % tag)\n    return out\n", "label": 1}
{"function": "\n\n@require_can_edit_fixtures\ndef update_tables(request, domain, data_type_id, test_patch=None):\n    '\\n    receives a JSON-update patch like following\\n    {\\n        \"_id\":\"0920fe1c6d4c846e17ee33e2177b36d6\",\\n        \"tag\":\"growth\",\\n        \"view_link\":\"/a/gsid/fixtures/view_lookup_tables/?table_id:0920fe1c6d4c846e17ee33e2177b36d6\",\\n        \"is_global\":false,\\n        \"fields\":{\"genderr\":{\"update\":\"gender\"},\"grade\":{}}\\n    }\\n    '\n    if (test_patch is None):\n        test_patch = {\n            \n        }\n    if data_type_id:\n        try:\n            data_type = FixtureDataType.get(data_type_id)\n        except ResourceNotFound:\n            raise Http404()\n        assert (data_type.doc_type == FixtureDataType._doc_type)\n        assert (data_type.domain == domain)\n        if (request.method == 'GET'):\n            return json_response(strip_json(data_type))\n        elif (request.method == 'DELETE'):\n            with CouchTransaction() as transaction:\n                data_type.recursive_delete(transaction)\n            return json_response({\n                \n            })\n        elif (not (request.method == 'PUT')):\n            return HttpResponseBadRequest()\n    if ((request.method == 'POST') or (request.method == 'PUT')):\n        fields_update = (test_patch or _to_kwargs(request))\n        fields_patches = fields_update['fields']\n        data_tag = fields_update['tag']\n        is_global = fields_update['is_global']\n        validation_errors = []\n        if is_identifier_invalid(data_tag):\n            validation_errors.append(data_tag)\n        for (field_name, options) in fields_update['fields'].items():\n            method = options.keys()\n            if ('update' in method):\n                field_name = options['update']\n            if (is_identifier_invalid(field_name) and ('remove' not in method)):\n                validation_errors.append(field_name)\n        validation_errors = map((lambda e: (_('\"%s\" cannot include special characters or begin with \"xml\".') % e)), validation_errors)\n        if validation_errors:\n            return json_response({\n                'validation_errors': validation_errors,\n                'error_msg': _('Could not update table because field names were not correctly formatted'),\n            })\n        with CouchTransaction() as transaction:\n            if data_type_id:\n                data_type = update_types(fields_patches, domain, data_type_id, data_tag, is_global, transaction)\n                update_items(fields_patches, domain, data_type_id, transaction)\n            elif FixtureDataType.fixture_tag_exists(domain, data_tag):\n                return HttpResponseBadRequest('DuplicateFixture')\n            else:\n                data_type = create_types(fields_patches, domain, data_tag, is_global, transaction)\n        return json_response(strip_json(data_type))\n", "label": 1}
{"function": "\n\ndef copyWiki(syn, entity, destinationId, entitySubPageId=None, destinationSubPageId=None, updateLinks=True, updateSynIds=True, entityMap=None):\n    \"\\n    Copies wikis and updates internal links\\n\\n    :param entity:                  A synapse ID of an entity whose wiki you want to copy\\n\\n    :param destinationId:           Synapse ID of a folder/project that the wiki wants to be copied to\\n    \\n    :param updateLinks:             Update all the internal links\\n                                    Defaults to True\\n\\n    :param updateSynIds:            Update all the synapse ID's referenced in the wikis\\n                                    Defaults to True but needs an entityMap\\n\\n    :param entityMap:               An entity map {'oldSynId','newSynId'} to update the synapse IDs referenced in the wiki\\n                                    Defaults to None \\n\\n    :param entitySubPageId:         Can specify subPageId and copy all of its subwikis\\n                                    Defaults to None, which copies the entire wiki\\n                                    subPageId can be found: https://www.synapse.org/#!Synapse:syn123/wiki/1234\\n                                    In this case, 1234 is the subPageId. \\n\\n    :param destinationSubPageId:    Can specify destination subPageId to copy wikis to\\n                                    Defaults to None\\n    \"\n    oldOwn = syn.get(entity, downloadFile=False)\n    try:\n        oldWh = syn.getWikiHeaders(oldOwn)\n        store = True\n    except SynapseHTTPError:\n        store = False\n    if store:\n        if (entitySubPageId is not None):\n            oldWh = _getSubWikiHeaders(oldWh, entitySubPageId, mapping=[])\n        newOwn = syn.get(destinationId, downloadFile=False)\n        wikiIdMap = dict()\n        newWikis = dict()\n        for i in oldWh:\n            attDir = tempfile.NamedTemporaryFile(prefix='attdir', suffix='')\n            wiki = syn.getWiki(oldOwn, i.id)\n            print(('Got wiki %s' % i.id))\n            if (wiki['attachmentFileHandleIds'] == []):\n                attachments = []\n            elif (wiki['attachmentFileHandleIds'] != []):\n                uri = ('/entity/%s/wiki/%s/attachmenthandles' % (wiki.ownerId, wiki.id))\n                results = syn.restGET(uri)\n                file_handles = {fh['id']: fh for fh in results['list']}\n                attachments = []\n                tempdir = tempfile.gettempdir()\n                for fhid in wiki.attachmentFileHandleIds:\n                    file_info = syn._downloadWikiAttachment(wiki.ownerId, wiki, file_handles[fhid]['fileName'], destination=tempdir)\n                    attachments.append(file_info['path'])\n            if hasattr(i, 'parentId'):\n                wNew = Wiki(owner=newOwn, title=wiki.get('title', ''), markdown=wiki.markdown, attachments=attachments, parentWikiId=wikiIdMap[wiki.parentWikiId])\n                wNew = syn.store(wNew)\n            else:\n                wNew = Wiki(owner=newOwn, title=wiki.get('title', ''), markdown=wiki.markdown, attachments=attachments, parentWikiId=destinationSubPageId)\n                wNew = syn.store(wNew)\n                parentWikiId = wNew.id\n            newWikis[wNew.id] = wNew\n            wikiIdMap[wiki.id] = wNew.id\n        if updateLinks:\n            print('Updating internal links:\\n')\n            for oldWikiId in wikiIdMap.keys():\n                newWikiId = wikiIdMap[oldWikiId]\n                newWiki = newWikis[newWikiId]\n                print(('\\tUpdating internal links for Page: %s\\n' % newWikiId))\n                s = newWiki.markdown\n                for oldWikiId2 in wikiIdMap.keys():\n                    oldProjectAndWikiId = ('%s/wiki/%s' % (entity, oldWikiId2))\n                    newProjectAndWikiId = ('%s/wiki/%s' % (destinationId, wikiIdMap[oldWikiId2]))\n                    s = re.sub(oldProjectAndWikiId, newProjectAndWikiId, s)\n                s = re.sub(entity, destinationId, s)\n                newWikis[newWikiId].markdown = s\n        if (updateSynIds and (entityMap is not None)):\n            print('Updating Synapse references:\\n')\n            for oldWikiId in wikiIdMap.keys():\n                newWikiId = wikiIdMap[oldWikiId]\n                newWiki = newWikis[newWikiId]\n                print(('Updated Synapse references for Page: %s\\n' % newWikiId))\n                s = newWiki.markdown\n                for oldSynId in entityMap.keys():\n                    newSynId = entityMap[oldSynId]\n                    s = re.sub(oldSynId, newSynId, s)\n                print('Done updating Synpase IDs.\\n')\n                newWikis[newWikiId].markdown = s\n        print('Storing new Wikis\\n')\n        for oldWikiId in wikiIdMap.keys():\n            newWikiId = wikiIdMap[oldWikiId]\n            newWikis[newWikiId] = syn.store(newWikis[newWikiId])\n            print('\\tStored: %s\\n', newWikiId)\n        newWh = syn.getWikiHeaders(newOwn)\n        return newWh\n    else:\n        return 'no wiki'\n", "label": 1}
{"function": "\n\ndef _handle_actions(self, state, current_run, func, sp_addr, accessed_registers):\n    se = state.se\n    if ((func is not None) and (sp_addr is not None)):\n        new_sp_addr = (sp_addr + self.project.arch.call_sp_fix)\n        actions = [a for a in state.log.actions if (a.bbl_addr == current_run.addr)]\n        for a in actions:\n            if ((a.type == 'mem') and (a.action == 'read')):\n                try:\n                    addr = se.exactly_int(a.addr.ast, default=0)\n                except claripy.ClaripyError:\n                    continue\n                if ((self.project.arch.call_pushes_ret and (addr >= new_sp_addr)) or ((not self.project.arch.call_pushes_ret) and (addr >= new_sp_addr))):\n                    offset = (addr - new_sp_addr)\n                    func._add_argument_stack_variable(offset)\n            elif (a.type == 'reg'):\n                offset = a.offset\n                if ((a.action == 'read') and (offset not in accessed_registers)):\n                    func._add_argument_register(offset)\n                elif (a.action == 'write'):\n                    accessed_registers.add(offset)\n    else:\n        l.error('handle_actions: Function not found, or stack pointer is None. It might indicates unbalanced stack.')\n", "label": 1}
{"function": "\n\ndef _verify(self, option='warn'):\n    errs = _ErrList([], unit='Card')\n    is_valid = (lambda v: (v in [8, 16, 32, 64, (- 32), (- 64)]))\n    if isinstance(self, ExtensionHDU):\n        firstkey = 'XTENSION'\n        firstval = self._extension\n    else:\n        firstkey = 'SIMPLE'\n        firstval = True\n    self.req_cards(firstkey, 0, None, firstval, option, errs)\n    self.req_cards('BITPIX', 1, (lambda v: (_is_int(v) and is_valid(v))), 8, option, errs)\n    self.req_cards('NAXIS', 2, (lambda v: (_is_int(v) and (v >= 0) and (v <= 999))), 0, option, errs)\n    naxis = self._header.get('NAXIS', 0)\n    if (naxis < 1000):\n        for ax in range(3, (naxis + 3)):\n            self.req_cards(('NAXIS' + str((ax - 2))), ax, (lambda v: (_is_int(v) and (v >= 0))), 1, option, errs)\n        for keyword in self._header:\n            if (keyword.startswith('NAXIS') and (len(keyword) > 5)):\n                try:\n                    number = int(keyword[5:])\n                    if ((number <= 0) or (number > naxis)):\n                        raise ValueError\n                except ValueError:\n                    err_text = (\"NAXISj keyword out of range ('%s' when NAXIS == %d)\" % (keyword, naxis))\n\n                    def fix(self=self, keyword=keyword):\n                        del self._header[keyword]\n                    errs.append(self.run_option(option=option, err_text=err_text, fix=fix, fix_text='Deleted.'))\n    if ('EXTNAME' in self._header):\n        if (not isinstance(self._header['EXTNAME'], string_types)):\n            err_text = 'The EXTNAME keyword must have a string value.'\n            fix_text = 'Converted the EXTNAME keyword to a string value.'\n\n            def fix(header=self._header):\n                header['EXTNAME'] = str(header['EXTNAME'])\n            errs.append(self.run_option(option, err_text=err_text, fix_text=fix_text, fix=fix))\n    for card in self._header.cards:\n        errs.append(card._verify(option))\n    return errs\n", "label": 1}
{"function": "\n\ndef run_distribute(self):\n    info_main('# Creating Android project from build and {} bootstrap'.format(self.name))\n    info('This currently just copies the SDL2 build stuff straight from the build dir.')\n    shprint(sh.rm, '-rf', self.dist_dir)\n    shprint(sh.cp, '-r', self.build_dir, self.dist_dir)\n    with current_directory(self.dist_dir):\n        with open('local.properties', 'w') as fileh:\n            fileh.write('sdk.dir={}'.format(self.ctx.sdk_dir))\n    arch = self.ctx.archs[0]\n    if (len(self.ctx.archs) > 1):\n        raise ValueError('built for more than one arch, but bootstrap cannot handle that yet')\n    info('Bootstrap running with arch {}'.format(arch))\n    with current_directory(self.dist_dir):\n        info('Copying python distribution')\n        if ((not exists('private')) and (not self.ctx.python_recipe.from_crystax)):\n            shprint(sh.mkdir, 'private')\n        if ((not exists('crystax_python')) and self.ctx.python_recipe.from_crystax):\n            shprint(sh.mkdir, 'crystax_python')\n            shprint(sh.mkdir, 'crystax_python/crystax_python')\n        if (not exists('assets')):\n            shprint(sh.mkdir, 'assets')\n        hostpython = sh.Command(self.ctx.hostpython)\n        if (not self.ctx.python_recipe.from_crystax):\n            try:\n                shprint(hostpython, '-OO', '-m', 'compileall', self.ctx.get_python_install_dir(), _tail=10, _filterout='^Listing')\n            except sh.ErrorReturnCode:\n                pass\n            if (not exists('python-install')):\n                shprint(sh.cp, '-a', self.ctx.get_python_install_dir(), './python-install')\n        self.distribute_libs(arch, [self.ctx.get_libs_dir(arch.arch)])\n        self.distribute_aars(arch)\n        self.distribute_javaclasses(self.ctx.javaclass_dir)\n        if (not self.ctx.python_recipe.from_crystax):\n            info('Filling private directory')\n            if (not exists(join('private', 'lib'))):\n                info('private/lib does not exist, making')\n                shprint(sh.cp, '-a', join('python-install', 'lib'), 'private')\n            shprint(sh.mkdir, '-p', join('private', 'include', 'python2.7'))\n            if exists(join('libs', arch.arch, 'libpymodules.so')):\n                shprint(sh.mv, join('libs', arch.arch, 'libpymodules.so'), 'private/')\n            shprint(sh.cp, join('python-install', 'include', 'python2.7', 'pyconfig.h'), join('private', 'include', 'python2.7/'))\n            info('Removing some unwanted files')\n            shprint(sh.rm, '-f', join('private', 'lib', 'libpython2.7.so'))\n            shprint(sh.rm, '-rf', join('private', 'lib', 'pkgconfig'))\n            libdir = join(self.dist_dir, 'private', 'lib', 'python2.7')\n            site_packages_dir = join(libdir, 'site-packages')\n            with current_directory(libdir):\n                removes = []\n                for (dirname, something, filens) in walk('.'):\n                    for filename in filens:\n                        for suffix in ('py', 'pyc', 'so.o', 'so.a', 'so.libs'):\n                            if filename.endswith(suffix):\n                                removes.append(filename)\n                shprint(sh.rm, '-f', *removes)\n                info('Deleting some other stuff not used on android')\n                shprint(sh.rm, '-rf', 'lib2to3')\n                shprint(sh.rm, '-rf', 'idlelib')\n                for filename in glob.glob('config/libpython*.a'):\n                    shprint(sh.rm, '-f', filename)\n                shprint(sh.rm, '-rf', 'config/python.o')\n        else:\n            ndk_dir = self.ctx.ndk_dir\n            py_recipe = self.ctx.python_recipe\n            python_dir = join(ndk_dir, 'sources', 'python', py_recipe.version, 'libs', arch.arch)\n            shprint(sh.cp, '-r', join(python_dir, 'stdlib.zip'), 'crystax_python/crystax_python')\n            shprint(sh.cp, '-r', join(python_dir, 'modules'), 'crystax_python/crystax_python')\n            shprint(sh.cp, '-r', self.ctx.get_python_install_dir(), 'crystax_python/crystax_python/site-packages')\n            info('Renaming .so files to reflect cross-compile')\n            site_packages_dir = 'crystax_python/crystax_python/site-packages'\n            filens = shprint(sh.find, site_packages_dir, '-iname', '*.so').stdout.decode('utf-8').split('\\n')[:(- 1)]\n            for filen in filens:\n                parts = filen.split('.')\n                if (len(parts) <= 2):\n                    continue\n                shprint(sh.mv, filen, (filen.split('.')[0] + '.so'))\n            site_packages_dir = join(abspath(curdir), site_packages_dir)\n    self.strip_libraries(arch)\n    self.fry_eggs(site_packages_dir)\n    super(SDL2Bootstrap, self).run_distribute()\n", "label": 1}
{"function": "\n\ndef docstring_errors(filename, global_dict=None):\n    '\\n    Run a Python file, parse the docstrings of all the classes\\n    and functions it declares, and return them.\\n\\n    Parameters\\n    ----------\\n    filename : str\\n        Filename of the module to run.\\n\\n    global_dict : dict, optional\\n        Globals dictionary to pass along to `execfile()`.\\n\\n    Returns\\n    -------\\n    all_errors : list\\n        Each entry of the list is a tuple, of length 2 or 3, with\\n        format either\\n\\n        (func_or_class_name, docstring_error_description)\\n        or\\n        (class_name, method_name, docstring_error_description)\\n    '\n    if (global_dict is None):\n        global_dict = {\n            \n        }\n    if ('__file__' not in global_dict):\n        global_dict['__file__'] = filename\n    if ('__doc__' not in global_dict):\n        global_dict['__doc__'] = None\n    try:\n        with open(filename) as f:\n            code = compile(f.read(), filename, 'exec')\n            exec(code, global_dict)\n    except SystemExit:\n        pass\n    except SkipTest:\n        raise AssertionError(((\"Couldn't verify format of \" + filename) + 'due to SkipTest'))\n    all_errors = []\n    for (key, val) in six.iteritems(global_dict):\n        if (not key.startswith('_')):\n            module_name = ''\n            if hasattr(inspect.getmodule(val), '__name__'):\n                module_name = inspect.getmodule(val).__name__\n            if ((inspect.isfunction(val) or inspect.isclass(val)) and ((inspect.getmodule(val) is None) or (module_name == '__builtin__'))):\n                if inspect.isfunction(val):\n                    all_errors.extend(handle_function(val, key))\n                elif inspect.isclass(val):\n                    all_errors.extend(handle_class(val, key))\n        elif (key == '__doc__'):\n            all_errors.extend(handle_module(val, key))\n    if all_errors:\n        all_errors.insert(0, (('%s:' % filename),))\n    return all_errors\n", "label": 1}
{"function": "\n\ndef main(self, raw_args=None):\n    'Start to parse the raw arguments and send them to a\\n        :py:class:`~clime.core.Command` instance.\\n\\n        :param raw_args: The arguments from command line. By default, it takes from ``sys.argv``.\\n        :type raw_args: list\\n        '\n    if (raw_args is None):\n        raw_args = sys.argv[1:]\n    elif isinstance(raw_args, str):\n        raw_args = raw_args.split()\n    cmd_name = None\n    cmd_func = None\n    if (len(raw_args) == 0):\n        pass\n    elif ((not self.ignore_help) and (raw_args[0] in ('--help', '-h'))):\n        self.print_usage()\n        return\n    else:\n        cmd_func = self.command_funcs.get(raw_args[0].replace('-', '_'))\n        if (cmd_func is not None):\n            cmd_name = raw_args.pop(0).replace('-', '_')\n    if (cmd_func is None):\n        if self.default:\n            cmd_name = cmd_name\n            cmd_func = self.command_funcs[self.default]\n        else:\n            self.print_usage()\n            return\n    if ((not self.ignore_help) and ('--help' in raw_args)):\n        self.print_usage(cmd_name)\n        return\n    cmd = Command(cmd_func, cmd_name)\n    try:\n        return_val = cmd.execute(raw_args)\n    except BaseException as e:\n        if self.debug:\n            raise\n        self.complain('exception: {}: {}'.format(e.__class__.__name__, e))\n        sys.exit(1)\n    if ((not self.ignore_return) and (return_val is not None)):\n        if inspect.isgenerator(return_val):\n            for i in return_val:\n                print(i)\n        else:\n            print(return_val)\n", "label": 1}
{"function": "\n\ndef test_scan_video_episode(episodes, tmpdir, monkeypatch):\n    video = episodes['bbt_s07e05']\n    monkeypatch.chdir(str(tmpdir))\n    tmpdir.ensure(video.name)\n    scanned_video = scan_video(video.name)\n    assert scanned_video.name, video.name\n    assert (scanned_video.format == video.format)\n    assert (scanned_video.release_group == video.release_group)\n    assert (scanned_video.resolution == video.resolution)\n    assert (scanned_video.video_codec == video.video_codec)\n    assert (scanned_video.audio_codec is None)\n    assert (scanned_video.imdb_id is None)\n    assert (scanned_video.hashes == {\n        \n    })\n    assert (scanned_video.size == 0)\n    assert (scanned_video.subtitle_languages == set())\n    assert (scanned_video.series == video.series)\n    assert (scanned_video.season == video.season)\n    assert (scanned_video.episode == video.episode)\n    assert (scanned_video.title is None)\n    assert (scanned_video.year is None)\n    assert (scanned_video.tvdb_id is None)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _select(cls):\n    if cls._selecting[0]:\n        return\n    try:\n        cls._selecting[0] = True\n        for inst in cls._instances:\n            if ((not inst.connected) or (minisix.PY3 and inst.conn._closed) or (minisix.PY2 and (inst.conn._sock.__class__ is socket._closedsocket))):\n                cls._instances.remove(inst)\n            elif (inst.conn.fileno() == (- 1)):\n                inst.reconnect()\n        if (not cls._instances):\n            return\n        (rlist, wlist, xlist) = select.select([x.conn for x in cls._instances], [], [], conf.supybot.drivers.poll())\n        for instance in cls._instances:\n            if (instance.conn in rlist):\n                instance._read()\n    except select.error as e:\n        if (e.args[0] != errno.EINTR):\n            raise\n    finally:\n        cls._selecting[0] = False\n    for instance in cls._instances:\n        if (instance.irc and (not instance.irc.zombie)):\n            instance._sendIfMsgs()\n", "label": 1}
{"function": "\n\ndef egg_info_path(self, filename):\n    if (self._egg_info_path is None):\n        if self.editable:\n            base = self.source_dir\n        else:\n            base = os.path.join(self.source_dir, 'pip-egg-info')\n        filenames = os.listdir(base)\n        if self.editable:\n            filenames = []\n            for (root, dirs, files) in os.walk(base):\n                for dir in vcs.dirnames:\n                    if (dir in dirs):\n                        dirs.remove(dir)\n                for dir in list(dirs):\n                    if (os.path.exists(os.path.join(root, dir, 'bin', 'python')) or os.path.exists(os.path.join(root, dir, 'Scripts', 'Python.exe'))):\n                        dirs.remove(dir)\n                    elif ((dir == 'test') or (dir == 'tests')):\n                        dirs.remove(dir)\n                filenames.extend([os.path.join(root, dir) for dir in dirs])\n            filenames = [f for f in filenames if f.endswith('.egg-info')]\n        if (not filenames):\n            raise InstallationError(('No files/directories in %s (from %s)' % (base, filename)))\n        assert filenames, ('No files/directories in %s (from %s)' % (base, filename))\n        if (len(filenames) > 1):\n            filenames.sort(key=(lambda x: (x.count(os.path.sep) + ((os.path.altsep and x.count(os.path.altsep)) or 0))))\n        self._egg_info_path = os.path.join(base, filenames[0])\n    return os.path.join(self._egg_info_path, filename)\n", "label": 1}
{"function": "\n\ndef determine_result(self, returncode, returnsignal, output, isTimeout):\n\n    def sanitizeXML(s):\n        return s.replace('<>', '<emptyTag>').replace('</>', '</emptyTag>')\n    if ((returnsignal == 0) and ((returncode == 0) or (returncode == 10))):\n        try:\n            tree = ET.fromstringlist(map(sanitizeXML, output))\n            status = tree.findtext('cprover-status')\n            if (status is None):\n\n                def isErrorMessage(msg):\n                    return (msg.get('type', None) == 'ERROR')\n                messages = list(filter(isErrorMessage, tree.getiterator('message')))\n                if messages:\n                    msg = messages[0].findtext('text')\n                    if (msg == 'Out of memory'):\n                        status = 'OUT OF MEMORY'\n                    elif msg:\n                        status = 'ERROR ({0})'.format(msg)\n                    else:\n                        status = 'ERROR'\n                else:\n                    status = 'INVALID OUTPUT'\n            elif (status == 'FAILURE'):\n                assert (returncode == 10)\n                reason = tree.find('goto_trace').find('failure').findtext('reason')\n                if (not reason):\n                    reason = tree.find('goto_trace').find('failure').get('reason')\n                if ('unwinding assertion' in reason):\n                    status = result.RESULT_UNKNOWN\n                else:\n                    status = result.RESULT_FALSE_REACH\n            elif (status == 'SUCCESS'):\n                assert (returncode == 0)\n                if ('--no-unwinding-assertions' in self.options):\n                    status = result.RESULT_UNKNOWN\n                else:\n                    status = result.RESULT_TRUE_PROP\n        except Exception:\n            if isTimeout:\n                status = 'TIMEOUT'\n            elif ('Minisat::OutOfMemoryException' in output):\n                status = 'OUT OF MEMORY'\n            else:\n                status = 'INVALID OUTPUT'\n                logging.exception('Error parsing impara output for returncode %d', returncode)\n    elif ((returncode == 64) and ('Usage error!' in output)):\n        status = 'INVALID ARGUMENTS'\n    else:\n        status = result.RESULT_ERROR\n    return status\n", "label": 1}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_transpose_lowmem():\n    ' low mem. reading/writing of transposed 3D RNMRTK time domain file '\n    dir_3d = os.path.join(DATA_DIR, 'rnmrtk_3d')\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(dir_3d, 'time_3d_t1_t2_t3.sec'))\n    assert (data.shape == (128, 88, 36))\n    assert (np.abs((data[(2, 6, 4)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    lowmem_write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(dir_3d, 'time_3d_t1_t3_t2.sec'))\n    assert (data.shape == (128, 72, 44))\n    assert (np.abs((data[(2, 8, 3)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    lowmem_write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(dir_3d, 'time_3d_t2_t1_t3.sec'))\n    assert (data.shape == (88, 128, 36))\n    assert (np.abs((data[(6, 2, 4)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    lowmem_write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(dir_3d, 'time_3d_t2_t3_t1.sec'))\n    assert (data.shape == (88, 72, 64))\n    assert (np.abs((data[(6, 8, 1)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    lowmem_write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(dir_3d, 'time_3d_t3_t1_t2.sec'))\n    assert (data.shape == (72, 128, 44))\n    assert (np.abs((data[(8, 2, 3)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    lowmem_write_readback(dic, data)\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(dir_3d, 'time_3d_t3_t2_t1.sec'))\n    assert (data.shape == (72, 88, 64))\n    assert (np.abs((data[(8, 6, 1)].real - (- 1.82))) <= 0.01)\n    assert (dic['npts'] == [64, 44, 36])\n    lowmem_write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef loadTestsFromName(self, name, module=None):\n    'Return a suite of all tests cases given a string specifier.\\n\\n        The name may resolve either to a module, a test case class, a\\n        test method within a test case class, or a callable object which\\n        returns a TestCase or TestSuite instance.\\n\\n        The method optionally resolves the names relative to a given module.\\n        '\n    parts = name.split('.')\n    if (module is None):\n        parts_copy = parts[:]\n        while parts_copy:\n            try:\n                module = __import__('.'.join(parts_copy))\n                break\n            except ImportError:\n                del parts_copy[(- 1)]\n                if (not parts_copy):\n                    raise\n        parts = parts[1:]\n    obj = module\n    for part in parts:\n        (parent, obj) = (obj, getattr(obj, part))\n    if isinstance(obj, types.ModuleType):\n        return self.loadTestsFromModule(obj)\n    elif (isinstance(obj, type) and issubclass(obj, case.TestCase)):\n        return self.loadTestsFromTestCase(obj)\n    elif (isinstance(obj, types.FunctionType) and isinstance(parent, type) and issubclass(parent, case.TestCase)):\n        name = parts[(- 1)]\n        inst = parent(name)\n        if (not isinstance(getattr(inst, name), types.FunctionType)):\n            return self.suiteClass([inst])\n    elif isinstance(obj, suite.TestSuite):\n        return obj\n    if callable(obj):\n        test = obj()\n        if isinstance(test, suite.TestSuite):\n            return test\n        elif isinstance(test, case.TestCase):\n            return self.suiteClass([test])\n        else:\n            raise TypeError(('calling %s returned %s, not a test' % (obj, test)))\n    else:\n        raise TypeError((\"don't know how to make test from: %s\" % obj))\n", "label": 1}
{"function": "\n\ndef run(self, stdin, callbacks):\n    \"\\n        The input 'event loop'.\\n        \"\n    assert isinstance(stdin, Input)\n    assert isinstance(callbacks, EventLoopCallbacks)\n    assert (not self._running)\n    if self.closed:\n        raise Exception('Event loop already closed.')\n    self._running = True\n    self._callbacks = callbacks\n    inputstream = InputStream(callbacks.feed_key)\n    current_timeout = [INPUT_TIMEOUT]\n    stdin_reader = PosixStdinReader(stdin.fileno())\n    if in_main_thread():\n        ctx = call_on_sigwinch(self.received_winch)\n    else:\n        ctx = DummyContext()\n\n    def read_from_stdin():\n        ' Read user input. '\n        data = stdin_reader.read()\n        inputstream.feed(data)\n        current_timeout[0] = INPUT_TIMEOUT\n        if stdin_reader.closed:\n            self.stop()\n    self.add_reader(stdin, read_from_stdin)\n    self.add_reader(self._schedule_pipe[0], None)\n    with ctx:\n        while self._running:\n            with TimeIt() as inputhook_timer:\n                if self._inputhook_context:\n\n                    def ready(wait):\n                        ' True when there is input ready. The inputhook should return control. '\n                        return (self._ready_for_reading((current_timeout[0] if wait else 0)) != [])\n                    self._inputhook_context.call_inputhook(ready)\n            if (current_timeout[0] is None):\n                remaining_timeout = None\n            else:\n                remaining_timeout = max(0, (current_timeout[0] - inputhook_timer.duration))\n            fds = self._ready_for_reading(remaining_timeout)\n            if fds:\n                tasks = []\n                low_priority_tasks = []\n                now = _now()\n                for fd in fds:\n                    if (fd == self._schedule_pipe[0]):\n                        for (c, max_postpone_until) in self._calls_from_executor:\n                            if ((max_postpone_until is None) or (max_postpone_until < now)):\n                                tasks.append(c)\n                            else:\n                                low_priority_tasks.append((c, max_postpone_until))\n                        self._calls_from_executor = []\n                        os.read(self._schedule_pipe[0], 1024)\n                    else:\n                        handler = self._read_fds.get(fd)\n                        if handler:\n                            tasks.append(handler)\n                random.shuffle(tasks)\n                random.shuffle(low_priority_tasks)\n                if tasks:\n                    for t in tasks:\n                        t()\n                    for (t, max_postpone_until) in low_priority_tasks:\n                        self.call_from_executor(t, _max_postpone_until=max_postpone_until)\n                else:\n                    for (t, _) in low_priority_tasks:\n                        t()\n            else:\n                inputstream.flush()\n                callbacks.input_timeout()\n                current_timeout[0] = None\n    self.remove_reader(stdin)\n    self.remove_reader(self._schedule_pipe[0])\n    self._callbacks = None\n", "label": 1}
{"function": "\n\ndef main(self, args, initial_options):\n    (options, args) = self.parser.parse_args(args)\n    self.merge_options(initial_options, options)\n    level = 1\n    level += options.verbose\n    level -= options.quiet\n    level = logger.level_for_integer((4 - level))\n    complete_log = []\n    logger.consumers.extend([(level, sys.stdout), (logger.DEBUG, complete_log.append)])\n    if options.log_explicit_levels:\n        logger.explicit_levels = True\n    self.setup_logging()\n    if options.no_input:\n        os.environ['PIP_NO_INPUT'] = '1'\n    if options.exists_action:\n        os.environ['PIP_EXISTS_ACTION'] = ''.join(options.exists_action)\n    if options.require_venv:\n        if (not os.environ.get('VIRTUAL_ENV')):\n            logger.fatal('Could not find an activated virtualenv (required).')\n            sys.exit(VIRTUALENV_NOT_FOUND)\n    if options.log:\n        log_fp = open_logfile(options.log, 'a')\n        logger.consumers.append((logger.DEBUG, log_fp))\n    else:\n        log_fp = None\n    socket.setdefaulttimeout((options.timeout or None))\n    urlopen.setup(proxystr=options.proxy, prompting=(not options.no_input))\n    exit = SUCCESS\n    store_log = False\n    try:\n        status = self.run(options, args)\n        if isinstance(status, int):\n            exit = status\n    except (InstallationError, UninstallationError):\n        e = sys.exc_info()[1]\n        logger.fatal(str(e))\n        logger.info(('Exception information:\\n%s' % format_exc()))\n        store_log = True\n        exit = ERROR\n    except BadCommand:\n        e = sys.exc_info()[1]\n        logger.fatal(str(e))\n        logger.info(('Exception information:\\n%s' % format_exc()))\n        store_log = True\n        exit = ERROR\n    except CommandError:\n        e = sys.exc_info()[1]\n        logger.fatal(('ERROR: %s' % e))\n        logger.info(('Exception information:\\n%s' % format_exc()))\n        exit = ERROR\n    except KeyboardInterrupt:\n        logger.fatal('Operation cancelled by user')\n        logger.info(('Exception information:\\n%s' % format_exc()))\n        store_log = True\n        exit = ERROR\n    except:\n        logger.fatal(('Exception:\\n%s' % format_exc()))\n        store_log = True\n        exit = UNKNOWN_ERROR\n    if (log_fp is not None):\n        log_fp.close()\n    if store_log:\n        log_fn = options.log_file\n        text = '\\n'.join(complete_log)\n        try:\n            log_fp = open_logfile(log_fn, 'w')\n        except IOError:\n            temp = tempfile.NamedTemporaryFile(delete=False)\n            log_fn = temp.name\n            log_fp = open_logfile(log_fn, 'w')\n        logger.fatal(('Storing complete log in %s' % log_fn))\n        log_fp.write(text)\n        log_fp.close()\n    return exit\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, x, k):\n    x = sympify(x)\n    k = sympify(k)\n    if ((x is S.NaN) or (k is S.NaN)):\n        return S.NaN\n    elif (x is S.One):\n        return factorial(k)\n    elif k.is_Integer:\n        if (k is S.Zero):\n            return S.One\n        elif k.is_positive:\n            if (x is S.Infinity):\n                return S.Infinity\n            elif (x is S.NegativeInfinity):\n                if k.is_odd:\n                    return S.NegativeInfinity\n                else:\n                    return S.Infinity\n            else:\n                try:\n                    (F, opt) = poly_from_expr(x)\n                except PolificationFailed:\n                    return reduce((lambda r, i: (r * (x + i))), range(0, int(k)), 1)\n                if ((len(opt.gens) > 1) or (F.degree() <= 1)):\n                    return reduce((lambda r, i: (r * (x + i))), range(0, int(k)), 1)\n                else:\n                    v = opt.gens[0]\n                    return reduce((lambda r, i: (r * F.subs(v, (v + i)).expand())), range(0, int(k)), 1)\n        elif (x is S.Infinity):\n            return S.Infinity\n        elif (x is S.NegativeInfinity):\n            return S.Infinity\n        else:\n            try:\n                (F, opt) = poly_from_expr(x)\n            except PolificationFailed:\n                return (1 / reduce((lambda r, i: (r * (x - i))), range(1, (abs(int(k)) + 1)), 1))\n            if ((len(opt.gens) > 1) or (F.degree() <= 1)):\n                return (1 / reduce((lambda r, i: (r * (x - i))), range(1, (abs(int(k)) + 1)), 1))\n            else:\n                v = opt.gens[0]\n                return (1 / reduce((lambda r, i: (r * F.subs(v, (v - i)).expand())), range(1, (abs(int(k)) + 1)), 1))\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@register_uncanonicalize\n@gof.local_optimizer(ALL_REDUCE)\ndef local_reduce_join(node):\n    '\\n    Reduce{scalar.op}(Join(axis=0, a, b), axis=0) -> Elemwise{scalar.op}(a, b)\\n\\n    Notes\\n    -----\\n    Supported scalar.op are Maximum, Mimimum in some cases and Add and Mul in\\n    all cases.\\n\\n    Currently we must reduce on axis 0. It is probably extensible to the case\\n    where we join and reduce on the same set of axis.\\n\\n    '\n    if (isinstance(node.op, T.CAReduce) and node.inputs[0].owner and isinstance(node.inputs[0].owner.op, T.Join)):\n        join = node.inputs[0].owner\n        if (T.extract_constant(join.inputs[0]) != 0):\n            return\n        if isinstance(node.op.scalar_op, (scalar.Maximum, scalar.Minimum)):\n            if (len(join.inputs) != 3):\n                return\n        elif (not isinstance(node.op.scalar_op, (scalar.Add, scalar.Mul))):\n            return\n        elif (len(join.inputs) <= 2):\n            return\n        new_inp = []\n        for inp in join.inputs[1:]:\n            inp = inp.owner\n            if (not inp):\n                return\n            if ((not isinstance(inp.op, DimShuffle)) or (inp.op.new_order != (('x',) + tuple(range(inp.inputs[0].ndim))))):\n                return\n            new_inp.append(inp.inputs[0])\n        ret = Elemwise(node.op.scalar_op)(*new_inp)\n        if (ret.dtype != node.outputs[0].dtype):\n            return\n        reduce_axis = node.op.axis\n        if (reduce_axis is None):\n            reduce_axis = tuple(xrange(node.inputs[0].ndim))\n        if ((len(reduce_axis) != 1) or (0 not in reduce_axis)):\n            if theano.config.warn.reduce_join:\n                warnings.warn('Your current code is fine, but Theano versions prior to 0.7 (or this development version Sept 2014) might have given an incorrect result for this code. To disable this warning, set the Theano flag warn.reduce_join to False. The problem was an optimization, that modified the pattern \"Reduce{scalar.op}(Join(axis=0, a, b), axis=0)\", did not check the reduction axis. So if the reduction axis was not 0, you got a wrong answer.')\n            return\n        try:\n            join_axis = get_scalar_constant_value(join.inputs[0])\n            if (join_axis != reduce_axis[0]):\n                return\n        except NotScalarConstantError:\n            return\n        return [ret]\n", "label": 1}
{"function": "\n\ndef _shorten_line(tokens, source, indentation, indent_word, aggressive=False, previous_line=''):\n    'Separate line at OPERATOR.\\n\\n    The input is expected to be free of newlines except for inside multiline\\n    strings and at the end.\\n\\n    Multiple candidates will be yielded.\\n\\n    '\n    for (token_type, token_string, start_offset, end_offset) in token_offsets(tokens):\n        if ((token_type == tokenize.COMMENT) and (not is_probably_part_of_multiline(previous_line)) and (not is_probably_part_of_multiline(source)) and (not source[(start_offset + 1):].strip().lower().startswith(('noqa', 'pragma:', 'pylint:')))):\n            first = source[:start_offset]\n            second = source[start_offset:]\n            (yield (((((indentation + second.strip()) + '\\n') + indentation) + first.strip()) + '\\n'))\n        elif ((token_type == token.OP) and (token_string != '=')):\n            assert (token_type != token.INDENT)\n            first = source[:end_offset]\n            second_indent = indentation\n            if first.rstrip().endswith('('):\n                second_indent += indent_word\n            elif ('(' in first):\n                second_indent += (' ' * (1 + first.find('(')))\n            else:\n                second_indent += indent_word\n            second = (second_indent + source[end_offset:].lstrip())\n            if ((not second.strip()) or second.lstrip().startswith('#')):\n                continue\n            if second.lstrip().startswith(','):\n                continue\n            if first.rstrip().endswith('.'):\n                continue\n            if (token_string in '+-*/'):\n                fixed = (((first + ' \\\\') + '\\n') + second)\n            else:\n                fixed = ((first + '\\n') + second)\n            if check_syntax((normalize_multiline(fixed) if aggressive else fixed)):\n                (yield (indentation + fixed))\n", "label": 1}
{"function": "\n\ndef test_command(self):\n    connection = pyorient.OrientSocket('localhost', 2424)\n    factory = pyorient.OrientDB(connection)\n    session_id = factory.get_message(pyorient.CONNECT).prepare(('root', 'root')).send().fetch_response()\n    db_name = 'tmp_test1'\n    try:\n        factory.get_message(pyorient.DB_DROP).prepare([db_name, pyorient.STORAGE_TYPE_MEMORY]).send().fetch_response()\n    except pyorient.PyOrientCommandException as e:\n        print(str(e))\n    finally:\n        factory.get_message(pyorient.DB_CREATE).prepare((db_name, pyorient.DB_TYPE_GRAPH, pyorient.STORAGE_TYPE_MEMORY)).send().fetch_response()\n    msg = factory.get_message(pyorient.DB_OPEN)\n    cluster_info = msg.prepare((db_name, 'admin', 'admin', pyorient.DB_TYPE_DOCUMENT, '', pyorient.OrientSerialization.CSV)).send().fetch_response()\n    create_class = factory.get_message(pyorient.COMMAND)\n    ins_msg1 = factory.get_message(pyorient.COMMAND)\n    ins_msg2 = factory.get_message(pyorient.COMMAND)\n    ins_msg3 = factory.get_message(pyorient.COMMAND)\n    ins_msg4 = factory.get_message(pyorient.COMMAND)\n    upd_msg5 = factory.get_message(pyorient.RECORD_UPDATE)\n    req_msg = factory.get_message(pyorient.COMMAND)\n    create_class.prepare((pyorient.QUERY_CMD, 'create class c_test extends V'))\n    ins_msg1.prepare((pyorient.QUERY_CMD, \"insert into c_test ( Band, Song ) values( 'AC/DC', 'Hells Bells' )\"))\n    ins_msg2.prepare((pyorient.QUERY_CMD, \"insert into c_test ( Band, Song ) values( 'AC/DC', 'Who Made Who' )\"))\n    ins_msg3.prepare((pyorient.QUERY_CMD, \"insert into c_test ( Band, Song ) values( 'AC/DC', 'T.N.T.' )\"))\n    ins_msg4.prepare((pyorient.QUERY_CMD, \"insert into c_test ( Band, Song ) values( 'AC/DC', 'High Voltage' )\"))\n    cluster = create_class.send().fetch_response()\n    rec1 = ins_msg1.send().fetch_response()\n    rec2 = ins_msg2.send().fetch_response()\n    rec3 = ins_msg3.send().fetch_response()\n    rec4 = ins_msg4.send().fetch_response()\n    rec1 = rec1[0]\n    upd_res = upd_msg5.prepare((rec1._rid, rec1._rid, {\n        'Band': 'Metallica',\n        'Song': 'One',\n    })).send().fetch_response()\n    res = req_msg.prepare([pyorient.QUERY_SYNC, 'select from c_test']).send().fetch_response()\n    assert isinstance(cluster, list)\n    assert (rec1._rid == res[0]._rid)\n    assert (rec1._version != res[0]._version)\n    assert (res[0]._version == upd_res[0]._version)\n    assert (len(res) == 4)\n    assert (res[0]._rid == '#11:0')\n    assert (res[0].Band == 'Metallica')\n    assert (res[0].Song == 'One')\n    assert (res[3].Song == 'High Voltage')\n    rec = {\n        '@c_test': {\n            'alloggio': 'casa',\n            'lavoro': 'ufficio',\n            'vacanza': 'mare',\n        },\n    }\n    rec_position = factory.get_message(pyorient.RECORD_CREATE).prepare((cluster[0], rec)).send().fetch_response()\n    print(('New Rec Position: %s' % rec_position._rid))\n    assert (rec_position._rid is not None)\n    rec = {\n        '@c_test': {\n            'alloggio': 'albergo',\n            'lavoro': 'ufficio',\n            'vacanza': 'montagna',\n        },\n    }\n    update_success = factory.get_message(pyorient.RECORD_UPDATE).prepare((rec_position._rid, rec_position._rid, rec)).send().fetch_response()\n    req_msg = factory.get_message(pyorient.RECORD_LOAD)\n    res = req_msg.prepare([rec_position._rid, '*:-1']).send().fetch_response()\n    assert (res._rid == '#11:4')\n    assert (res._class == 'c_test')\n    assert (res.alloggio == 'albergo')\n    assert (not hasattr(res, 'Band'))\n    assert (not hasattr(res, 'Song'))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse(wmsg):\n    '\\n        Verifies and parses an unserialized raw message into an actual WAMP message instance.\\n\\n        :param wmsg: The unserialized raw message.\\n        :type wmsg: list\\n\\n        :returns: An instance of this class.\\n        '\n    assert ((len(wmsg) > 0) and (wmsg[0] == Result.MESSAGE_TYPE))\n    if (len(wmsg) not in (3, 4, 5)):\n        raise ProtocolError('invalid message length {0} for RESULT'.format(len(wmsg)))\n    request = check_or_raise_id(wmsg[1], \"'request' in RESULT\")\n    details = check_or_raise_extra(wmsg[2], \"'details' in RESULT\")\n    args = None\n    kwargs = None\n    payload = None\n    enc_algo = None\n    enc_key = None\n    enc_serializer = None\n    if ((len(wmsg) == 4) and (type(wmsg[3]) in [six.text_type, six.binary_type])):\n        payload = wmsg[3]\n        enc_algo = details.get('enc_algo', None)\n        if (enc_algo and (enc_algo not in [PAYLOAD_ENC_CRYPTO_BOX])):\n            raise ProtocolError(\"invalid value {0} for 'enc_algo' detail in EVENT\".format(enc_algo))\n        enc_key = details.get('enc_key', None)\n        if (enc_key and (type(enc_key) not in [six.text_type, six.binary_type])):\n            raise ProtocolError(\"invalid type {0} for 'enc_key' detail in EVENT\".format(type(enc_key)))\n        enc_serializer = details.get('enc_serializer', None)\n        if (enc_serializer and (enc_serializer not in ['json', 'msgpack', 'cbor', 'ubjson'])):\n            raise ProtocolError(\"invalid value {0} for 'enc_serializer' detail in EVENT\".format(enc_serializer))\n    else:\n        if (len(wmsg) > 3):\n            args = wmsg[3]\n            if (type(args) != list):\n                raise ProtocolError(\"invalid type {0} for 'args' in RESULT\".format(type(args)))\n        if (len(wmsg) > 4):\n            kwargs = wmsg[4]\n            if (type(kwargs) != dict):\n                raise ProtocolError(\"invalid type {0} for 'kwargs' in RESULT\".format(type(kwargs)))\n    progress = None\n    if ('progress' in details):\n        detail_progress = details['progress']\n        if (type(detail_progress) != bool):\n            raise ProtocolError(\"invalid type {0} for 'progress' option in RESULT\".format(type(detail_progress)))\n        progress = detail_progress\n    obj = Result(request, args=args, kwargs=kwargs, payload=payload, progress=progress, enc_algo=enc_algo, enc_key=enc_key, enc_serializer=enc_serializer)\n    return obj\n", "label": 1}
{"function": "\n\ndef reduce_timeout_with_windbg(target_cmd, timeout=_common.DEFAULT_TIMEOUT, init_wait=None, symbols=None, env=None, callback=None, callback_args=None):\n    '\\n    Uses a super-sekrit algorithm to minimize a command which results\\n    in TIMEOUT classifications using nothing but WinDBG and raw nerve.\\n    It returns a :class:`FuzzResult`.\\n\\n    Note: A timeout limit of less than 30 seconds may not produce\\n    accurate results.\\n\\n    TODO: Add support for callback\\n\\n    Availability: Windows.\\n    '\n    if (callback is not None):\n        raise RuntimeError('callback support not implemented')\n    if (timeout <= 0):\n        raise ValueError('timeout must be greater than zero')\n    if (init_wait is None):\n        init_wait = (timeout / 4)\n    if (env is None):\n        env = dict(os.environ)\n    env['LIBC_FATAL_STDERR_'] = '1'\n    if (symbols is not None):\n        env['_NT_ALT_SYMBOL_PATH'] = symbols\n    elif target_cmd:\n        env['_NT_ALT_SYMBOL_PATH'] = os.path.abspath(os.path.dirname(target_cmd[0]))\n    dbg_script = os.path.join(_common.PATH_DBG, 'scripts', 'WinDBGTrace.py')\n    timeout_time = (time.time() + init_wait)\n    cdb_cmd = [TOOL_CDB, '-x', '-y', 'SRV*c:\\\\websymbols*http://msdl.microsoft.com/download/symbols', '-c', ('.load pykd.pyd;g;!py %s -t %d -v;q' % (dbg_script, timeout))]\n    gflags_args = _common._disable_gflags(target_cmd[0])\n    with tempfile.TemporaryFile(mode='w+t') as f:\n        f.write(('Timeout: %0.2f\\n' % timeout))\n        if (timeout < 30):\n            f.write('A timeout less than 30 seconds may not produce accurate results\\n')\n        f.write(('Init wait: %0.2f\\n' % init_wait))\n        p = subprocess.Popen((cdb_cmd + target_cmd), stderr=f, stdout=f, env=env, stdin=subprocess.PIPE, creationflags=_common.POPEN_FLAGS)\n        kill_debugger = False\n        while (p.poll() is None):\n            if (time.time() > timeout_time):\n                if (not kill_debugger):\n                    timeout_time = (time.time() + (timeout * 5))\n                    kill_debugger = True\n                    try:\n                        os.kill(p.pid, signal.CTRL_C_EVENT)\n                    except OSError:\n                        pass\n                else:\n                    with open(os.devnull, 'w') as fp:\n                        subprocess.call(['taskkill', '/pid', str(p.pid), '/f'], stdout=fp, stderr=fp)\n                    p.wait()\n            time.sleep(0.5)\n        f.seek(0, os.SEEK_SET)\n        output = []\n        for line in f.readlines():\n            if (line.strip() not in ['Breakpoint 0 hit', 'Breakpoint 1 hit']):\n                output.append(line)\n    if (gflags_args is not None):\n        _common._set_gflags(target_cmd[0], **gflags_args)\n    output = ''.join(output)\n    has_tb = output.find('Traceback (most recent call last):')\n    if (has_tb != (- 1)):\n        raise RuntimeError(('CDB Python Failure\\n%s\\n%s' % (('-' * 80), output[has_tb:])))\n    call_stack = _common.process_exploitable_output(output)[0]\n    if (not call_stack):\n        return _common.FuzzResult(text=output)\n    return _common.FuzzResult(classification=_common.TIMEOUT, text=output, backtrace=call_stack)\n", "label": 1}
{"function": "\n\ndef _check_url_exists(self, url, state):\n    global http_client, urlparse, socket\n    if (http_client is None):\n        from six.moves import http_client\n    if (urlparse is None):\n        from six.moves.urllib import parse as urlparse\n    if (socket is None):\n        import socket\n    (scheme, netloc, path, params, query, fragment) = urlparse.urlparse(url, 'http')\n    if params:\n        path += (';' + params)\n    if query:\n        path += ('?' + query)\n    try:\n        conn = (http_client.HTTPSConnection if (scheme == 'https') else http_client.HTTPConnection)(netloc)\n        try:\n            conn.request('HEAD', path)\n            res = conn.getresponse()\n        finally:\n            conn.close()\n    except http_client.HTTPException as e:\n        e = str(e)\n        if (str is not six.text_type):\n            try:\n                e = e.decode('utf-8')\n            except UnicodeDecodeError:\n                try:\n                    e = e.decode('latin-1')\n                except UnicodeDecodeError:\n                    e = e.decode('ascii', 'replace')\n        raise Invalid(self.message('httpError', state, error=e), state, url)\n    except socket.error as e:\n        e = str(e)\n        if (str is not six.text_type):\n            try:\n                e = e.decode('utf-8')\n            except UnicodeDecodeError:\n                try:\n                    e = e.decode('latin-1')\n                except UnicodeDecodeError:\n                    e = e.decode('ascii', 'replace')\n        raise Invalid(self.message('socketError', state, error=e), state, url)\n    else:\n        if (res.status == 404):\n            raise Invalid(self.message('notFound', state), state, url)\n        if (not (200 <= res.status < 500)):\n            raise Invalid(self.message('status', state, status=res.status), state, url)\n", "label": 1}
{"function": "\n\ndef process_packages(self, action):\n    'Install/uninstall packages'\n    if (action == 'install'):\n        (text, table) = ('Installing', self.table)\n        if (not self.get_packages_to_be_installed()):\n            return\n    elif (action == 'uninstall'):\n        (text, table) = ('Uninstalling', self.untable)\n    else:\n        raise AssertionError\n    packages = table.get_selected_packages()\n    if (not packages):\n        return\n    func = getattr(self.distribution, action)\n    thread = Thread(self)\n    for widget in self.children():\n        if isinstance(widget, QWidget):\n            widget.setEnabled(False)\n    try:\n        status = self.statusBar()\n    except AttributeError:\n        status = self.parent().statusBar()\n    progress = QProgressDialog(self, Qt.FramelessWindowHint)\n    progress.setMaximum(len(packages))\n    for (index, package) in enumerate(packages):\n        progress.setValue(index)\n        progress.setLabelText(('%s %s %s...' % (text, package.name, package.version)))\n        QApplication.processEvents()\n        if progress.wasCanceled():\n            break\n        if (package in table.model.actions):\n            try:\n                thread.callback = (lambda : func(package))\n                thread.start()\n                while thread.isRunning():\n                    QApplication.processEvents()\n                    if progress.wasCanceled():\n                        status.setEnabled(True)\n                        status.showMessage('Cancelling operation...')\n                table.remove_package(package)\n                error = thread.error\n            except Exception as error:\n                error = to_text_string(error)\n            if (error is not None):\n                pstr = ((package.name + ' ') + package.version)\n                QMessageBox.critical(self, 'Error', ('<b>Unable to %s <i>%s</i></b><br><br>Error message:<br>%s' % (action, pstr, error)))\n    progress.setValue(progress.maximum())\n    status.clearMessage()\n    for widget in self.children():\n        if isinstance(widget, QWidget):\n            widget.setEnabled(True)\n    thread = None\n    for table in (self.table, self.untable):\n        table.refresh_distribution(self.distribution)\n", "label": 1}
{"function": "\n\n@register_canonicalize('fast_compile')\n@gof.local_optimizer([Subtensor])\ndef local_subtensor_lift(node):\n    '\\n    unary(x)[idx] -> unary(x[idx])#any broadcast pattern.\\n\\n    Handles the following unary ops:\\n    elemwise(x,...)[idx] -> elemwise(x[idx],...)\\n      when x,... are broadcasted scalar or not broadcasted at all\\n    rebroadcast(x)[idx] => rebroadcast(x[idx])\\n\\n    '\n    if isinstance(node.op, Subtensor):\n        u = node.inputs[0]\n        if ((not u.owner) or (len(u.clients) > 1)):\n            return False\n        if (isinstance(u.owner.op, T.Elemwise) and (len(u.owner.inputs) == 1)):\n            idx = node.inputs[1:]\n            x_idx = node.op(u.owner.inputs[0], *idx)\n            copy_stack_trace(node.outputs, x_idx)\n            ret = u.owner.op(x_idx)\n            copy_stack_trace([node.outputs[0], node.inputs[0]], ret)\n            return [ret]\n        if isinstance(u.owner.op, T.Elemwise):\n            new_inputs = []\n            if all([(sum(i.type.broadcastable) == 0) for i in u.owner.inputs]):\n                idx = node.inputs[1:]\n                new_inputs = [node.op(i, *idx) for i in u.owner.inputs]\n                copy_stack_trace(node.outputs[0], new_inputs)\n                ret = u.owner.op(*new_inputs)\n                copy_stack_trace([node.outputs[0], node.inputs[0]], ret)\n                return [ret]\n            elif all([(sum(i.type.broadcastable) in [i.ndim, 0]) for i in u.owner.inputs]):\n                idx = node.inputs[1:]\n                new_inputs = []\n                for i in u.owner.inputs:\n                    if (sum(i.type.broadcastable) == 0):\n                        new_inputs.append(node.op(i, *idx))\n                    elif (node.outputs[0].ndim == i.ndim):\n                        new_inputs.append(i)\n                    else:\n                        new_inputs.append(i.dimshuffle((['x'] * node.outputs[0].ndim)))\n                copy_stack_trace(node.outputs[0], new_inputs)\n                ret = u.owner.op(*new_inputs)\n                copy_stack_trace([node.outputs[0], node.inputs[0]], ret)\n                return [ret]\n        if isinstance(u.owner.op, T.Rebroadcast):\n            assert (len(u.owner.inputs) == 1)\n            new_axis = []\n            j = 0\n            for (i, x) in enumerate(node.op.idx_list):\n                if isinstance(x, slice):\n                    new_axis += [(j, u.broadcastable[i])]\n                    j += 1\n            for i in xrange(len(node.op.idx_list), len(u.broadcastable)):\n                new_axis += [(j, u.broadcastable[i])]\n                j += 1\n            subt_x = node.op(u.owner.inputs[0], *node.inputs[1:])\n            copy_stack_trace(node.outputs[0], subt_x)\n            rbcast_subt_x = T.Rebroadcast(*new_axis)(subt_x)\n            copy_stack_trace([node.outputs[0], node.inputs[0]], rbcast_subt_x)\n            return [rbcast_subt_x]\n", "label": 1}
{"function": "\n\ndef write_file(self):\n    '\\n        Write the package file.\\n\\n        Returns\\n        -------\\n        None\\n\\n        '\n    f = open(self.fn_path, 'w')\n    f.write('{}\\n'.format(self.heading))\n    auxParamString = ''\n    if (self.aux != None):\n        for param in self.aux:\n            auxParamString = (auxParamString + ('AUX %s ' % param))\n    f.write('{:10d}{:10d}{:10d} {}\\n'.format(self.mnwmax, self.ipakcb, self.mnwprnt, auxParamString))\n    for i in range(self.mnwmax):\n        f.write('{}{:10d}\\n'.format(self.wellid[i], self.nnodes[i]))\n        f.write('{} {:10d}{:10d}{:10d}{:10d}\\n'.format(self.losstype[i], self.pumploc[i], self.qlimit[i], self.ppflag[i], self.pumpcap[i]))\n        if (self.losstype[i] == 'THIEM'):\n            f.write('{:10.4g}\\n'.format(self.rw[i]))\n        elif (self.losstype[i] == 'SKIN'):\n            f.write('{:10.4g}{:10.4g}{:10.4g}\\n'.format(self.rw[i], self.rskin[i], self.kskin[i]))\n        elif (self.losstype[i] == 'GENERAL'):\n            f.write('{:10.4g}{:10.4g}{:10.4g}{:10.4g}\\n'.format(self.rw[i], self.b[i], self.c[i], self.p[i]))\n        elif (self.losstype[i] == 'SPECIFYcwc'):\n            f.write('{:10.4g}\\n'.format(self.cwc[i]))\n        absNnodes = abs(self.nnodes[i])\n        if (self.nnodes[i] > 0):\n            for n in range(absNnodes):\n                f.write('{:10d}{:10d}{:10d}\\n'.format((self.lay_row_col[i][(n, 0)] + 1), (self.lay_row_col[i][(n, 1)] + 1), (self.lay_row_col[i][(n, 2)] + 1)))\n        elif (self.nnodes[i] < 0):\n            for n in range(absNnodes):\n                f.write('{:10.4g} {:10.4g} {:10d} {:10d}\\n'.format(self.ztop_zbotm_row_col[i][(n, 0)], self.ztop_zbotm_row_col[i][(n, 1)], (int(self.ztop_zbotm_row_col[i][(n, 2)]) + 1), (int(self.ztop_zbotm_row_col[i][(n, 3)]) + 1)))\n    for p in range(self.nper):\n        f.write('{:10d}\\n'.format(self.itmp[p]))\n        if (self.itmp[p] > 0):\n            '\\n                Create an array that will hold well names to be simulated during this stress period and find their corresponding\\n                index number in the \"wellid\" array so the right parameters (Hlim Qcut {Qfrcmn Qfrcmx}) are accessed.\\n                '\n            itmp_wellid_index_array = np.empty((self.itmp[p], 2), dtype='object')\n            for well in range(self.itmp[p]):\n                itmp_wellid_index_array[(well, 0)] = self.wellid_qdes[p][(well, 0)]\n                itmp_wellid_index_array[(well, 1)] = np.where((self.wellid == self.wellid_qdes[p][(well, 0)]))\n            for j in range(self.itmp[p]):\n                assert (self.wellid_qdes[p][(j, 0)] in self.wellid), 'WELLID for pumping well is not present in \"wellid\" array'\n                f.write('{} {:10.4g}\\n'.format(self.wellid_qdes[p][(j, 0)], float(self.wellid_qdes[p][(j, 1)])))\n    f.close()\n", "label": 1}
{"function": "\n\ndef parse_xcodeproject_plist_via_json(text, dictionarytype=dict):\n    'The CPython implementation comes with a fast JSON parser that is written in C.\\n    Instead of simply parsing the Plist we can split it into tokens with a regular expression\\n    and do a plist-to-json syntax transformation.\\n    This gives us about 80% faster parsing over our classic recursive descent parser.\\n    '\n    t0 = time.time()\n    text = unistr(text)\n    tokens = []\n    emit = tokens.append\n    jsondumps = json.JSONEncoder().encode\n    formatdesc = 'Xcode plist via JSON'\n    prjname = None\n    pos = skip_whitespace(text)\n    for m in r_tokenize.finditer(text, pos):\n        if (m.start() != pos):\n            return (None, error_report_dict(text, m.start(), m.end(), formatdesc))\n        pos = m.end()\n        rulenr = m.lastindex\n        if (rulenr == RULE_UNQUOTEDSTRING):\n            emit('\"')\n            emit(m.group(rulenr))\n            emit('\"')\n        elif (rulenr == RULE_SEMICOLON):\n            emit(',')\n        elif (rulenr == RULE_EQUALS):\n            emit(':')\n        elif (rulenr == RULE_COMMENT):\n            if (prjname is None):\n                prjname = projectname_from_comment(m.group(rulenr))\n            continue\n        elif (rulenr == RULE_DICTIONARY):\n            emit('{')\n        elif (rulenr == RULE_DICTIONARYEND):\n            if (tokens[(- 1)] == ','):\n                tokens.pop()\n            elif (tokens[(- 1)] != '{'):\n                return (None, error_report_dict(text, m.start(), m.end(), formatdesc))\n            emit('}')\n        elif (rulenr == RULE_COMMA):\n            emit(',')\n        elif (rulenr == RULE_QUOTEDSTRING):\n            emit(jsondumps(unescape_str(m.group(rulenr))))\n        elif (rulenr == RULE_ARRAY):\n            emit('[')\n        elif (rulenr == RULE_ARRAYEND):\n            if (tokens[(- 1)] == ','):\n                tokens.pop()\n            emit(']')\n    try:\n        jsontext = ''.join(tokens)\n        root = json.loads(jsontext, object_pairs_hook=dictionarytype)\n    except ValueError as e:\n        (linenr, column, errortext) = error_report_from(formatdesc, jsontext, text_type(e))\n        return (None, {\n            'error_column': column,\n            'error_line_number': linenr,\n            'error_text': errortext,\n        })\n    parseinfo = {\n        'format': 'xcode',\n        'parsetime': (time.time() - t0),\n        'parser': 'fast',\n    }\n    if (prjname is not None):\n        parseinfo['projectname'] = prjname\n    return (root, parseinfo)\n", "label": 1}
{"function": "\n\ndef get_query_set(self):\n    qs = self.root_query_set\n    lookup_params = self.params.copy()\n    for i in (ALL_VAR, ORDER_VAR, ORDER_TYPE_VAR, SEARCH_VAR, IS_POPUP_VAR):\n        if (i in lookup_params):\n            del lookup_params[i]\n    for (key, value) in lookup_params.items():\n        if (not isinstance(key, str)):\n            del lookup_params[key]\n            lookup_params[smart_str(key)] = value\n        if key.endswith('__in'):\n            lookup_params[key] = value.split(',')\n    try:\n        qs = qs.filter(**lookup_params)\n    except:\n        raise IncorrectLookupParameters\n    if (not qs._select_related):\n        if self.list_select_related:\n            qs = qs.select_related()\n        else:\n            for field_name in self.list_display:\n                try:\n                    f = self.lookup_opts.get_field(field_name)\n                except models.FieldDoesNotExist:\n                    pass\n                else:\n                    if isinstance(f.rel, models.ManyToOneRel):\n                        qs = qs.select_related()\n                        break\n    if self.order_field:\n        qs = qs.order_by(('%s%s' % ((((self.order_type == 'desc') and '-') or ''), self.order_field)))\n    if self.query:\n        qs = qs.query(self.query)\n    if (not (lookup_params or self.query)):\n        return qs.none()\n    return qs\n", "label": 1}
{"function": "\n\ndef save_milestone(self, req, milestone):\n    warnings = []\n\n    def warn(msg):\n        add_warning(req, msg)\n        warnings.append(msg)\n    milestone.description = req.args.get('description', '')\n    if ('due' in req.args):\n        duedate = req.args.get('duedate')\n        milestone.due = (user_time(req, parse_date, duedate, hint='datetime') if duedate else None)\n    else:\n        milestone.due = None\n    if ('completed' in req.args):\n        completed = req.args.get('completeddate', '')\n        completed = (user_time(req, parse_date, completed, hint='datetime') if completed else None)\n        if (completed and (completed > datetime_now(utc))):\n            warn(_('Completion date may not be in the future'))\n    else:\n        completed = None\n    milestone.completed = completed\n    new_name = req.args.get('name')\n    try:\n        new_milestone = Milestone(self.env, new_name)\n    except ResourceNotFound:\n        milestone.name = new_name\n    else:\n        if (new_milestone.name != milestone.name):\n            if new_milestone.name:\n                warn(_('Milestone \"%(name)s\" already exists, please choose another name.', name=new_milestone.name))\n            else:\n                warn(_('You must provide a name for the milestone.'))\n    if warnings:\n        return False\n    if milestone.exists:\n        milestone.update(author=req.authname)\n        if (completed and ('retarget' in req.args)):\n            comment = req.args.get('comment', '')\n            retarget_to = (req.args.get('target') or None)\n            retargeted_tickets = milestone.move_tickets(retarget_to, req.authname, comment, exclude_closed=True)\n            add_notice(req, _('The open tickets associated with milestone \"%(name)s\" have been retargeted to milestone \"%(retarget)s\".', name=milestone.name, retarget=retarget_to))\n            new_values = {\n                'milestone': retarget_to,\n            }\n            comment = (comment or _('Open tickets retargeted after milestone closed'))\n            event = BatchTicketChangeEvent(retargeted_tickets, None, req.authname, comment, new_values, None)\n            try:\n                NotificationSystem(self.env).notify(event)\n            except Exception as e:\n                self.log.error('Failure sending notification on ticket batch change: %s', exception_to_unicode(e))\n                add_warning(req, tag_('The changes have been saved, but an error occurred while sending notifications: %(message)s', message=to_unicode(e)))\n        add_notice(req, _('Your changes have been saved.'))\n    else:\n        milestone.insert()\n        add_notice(req, _('The milestone \"%(name)s\" has been added.', name=milestone.name))\n    return True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse(wmsg):\n    '\\n        Verifies and parses an unserialized raw message into an actual WAMP message instance.\\n\\n        :param wmsg: The unserialized raw message.\\n        :type wmsg: list\\n\\n        :returns: An instance of this class.\\n        '\n    assert ((len(wmsg) > 0) and (wmsg[0] == Error.MESSAGE_TYPE))\n    if (len(wmsg) not in (5, 6, 7)):\n        raise ProtocolError('invalid message length {0} for ERROR'.format(len(wmsg)))\n    request_type = wmsg[1]\n    if (type(request_type) not in six.integer_types):\n        raise ProtocolError(\"invalid type {0} for 'request_type' in ERROR\".format(request_type))\n    if (request_type not in [Subscribe.MESSAGE_TYPE, Unsubscribe.MESSAGE_TYPE, Publish.MESSAGE_TYPE, Register.MESSAGE_TYPE, Unregister.MESSAGE_TYPE, Call.MESSAGE_TYPE, Invocation.MESSAGE_TYPE]):\n        raise ProtocolError(\"invalid value {0} for 'request_type' in ERROR\".format(request_type))\n    request = check_or_raise_id(wmsg[2], \"'request' in ERROR\")\n    details = check_or_raise_extra(wmsg[3], \"'details' in ERROR\")\n    error = check_or_raise_uri(wmsg[4], \"'error' in ERROR\")\n    args = None\n    kwargs = None\n    payload = None\n    enc_algo = None\n    enc_key = None\n    enc_serializer = None\n    if ((len(wmsg) == 6) and (type(wmsg[5]) in [six.text_type, six.binary_type])):\n        payload = wmsg[5]\n        enc_algo = details.get('enc_algo', None)\n        if (enc_algo and (enc_algo not in [PAYLOAD_ENC_CRYPTO_BOX])):\n            raise ProtocolError(\"invalid value {0} for 'enc_algo' detail in EVENT\".format(enc_algo))\n        enc_key = details.get('enc_key', None)\n        if (enc_key and (type(enc_key) not in [six.text_type, six.binary_type])):\n            raise ProtocolError(\"invalid type {0} for 'enc_key' detail in EVENT\".format(type(enc_key)))\n        enc_serializer = details.get('enc_serializer', None)\n        if (enc_serializer and (enc_serializer not in ['json', 'msgpack', 'cbor', 'ubjson'])):\n            raise ProtocolError(\"invalid value {0} for 'enc_serializer' detail in EVENT\".format(enc_serializer))\n    else:\n        if (len(wmsg) > 5):\n            args = wmsg[5]\n            if (type(args) != list):\n                raise ProtocolError(\"invalid type {0} for 'args' in ERROR\".format(type(args)))\n        if (len(wmsg) > 6):\n            kwargs = wmsg[6]\n            if (type(kwargs) != dict):\n                raise ProtocolError(\"invalid type {0} for 'kwargs' in ERROR\".format(type(kwargs)))\n    obj = Error(request_type, request, error, args=args, kwargs=kwargs, payload=payload, enc_algo=enc_algo, enc_key=enc_key, enc_serializer=enc_serializer)\n    return obj\n", "label": 1}
{"function": "\n\n@classmethod\ndef create(cls, index, theirs, repo=None):\n    diff = messages.DiffMessage()\n    diff.indexes = []\n    diff.indexes.append((theirs or messages.IndexMessage()))\n    diff.indexes.append((index or messages.IndexMessage()))\n    index_paths_to_shas = {\n        \n    }\n    their_paths_to_shas = {\n        \n    }\n    for file_message in index.files:\n        index_paths_to_shas[file_message.path] = file_message.sha\n    for file_message in theirs.files:\n        their_paths_to_shas[file_message.path] = file_message.sha\n    for (path, sha) in index_paths_to_shas.iteritems():\n        if (path in their_paths_to_shas):\n            if (index_paths_to_shas[path] == their_paths_to_shas[path]):\n                file_message = messages.FileMessage()\n                file_message.path = path\n                file_message.deployed = theirs.deployed\n                file_message.deployed_by = theirs.deployed_by\n                diff.nochanges.append(file_message)\n            else:\n                file_message = messages.FileMessage()\n                file_message.path = path\n                file_message.deployed = theirs.deployed\n                file_message.deployed_by = theirs.deployed_by\n                diff.edits.append(file_message)\n            del their_paths_to_shas[path]\n        else:\n            file_message = messages.FileMessage()\n            file_message.path = path\n            diff.adds.append(file_message)\n    for (path, sha) in their_paths_to_shas.iteritems():\n        file_message = messages.FileMessage()\n        file_message.path = path\n        file_message.deployed = theirs.deployed\n        file_message.deployed_by = theirs.deployed_by\n        diff.deletes.append(file_message)\n    if ((repo is not None) and index.commit and index.commit.sha and theirs.commit and theirs.commit.sha):\n        try:\n            what_changed = repo.git.log('--date=short', '--pretty=format:[%h] %ad <%ae> %s', '{}..{}'.format(theirs.commit.sha, index.commit.sha))\n            diff.what_changed = what_changed.decode('utf-8')\n        except git.exc.GitCommandError:\n            logging.info('Unable to determine changes between deploys.')\n    elif ((repo is not None) and index.commit and index.commit.sha):\n        what_changed = repo.git.log('--date=short', '--pretty=format:[%h] %ad <%ae> %s')\n        diff.what_changed = what_changed.decode('utf-8')\n    return diff\n", "label": 1}
{"function": "\n\ndef cp_to_noexistent_destination(args, dest_path, dx_dest, dest_proj):\n    ' Copy the source to a destination that does not currently\\n    exist. This involves creating the target file/folder.\\n    '\n    if (len(args.sources) != 1):\n        raise DXCLIError('The destination folder does not exist')\n    last_slash_pos = get_last_pos_of_char('/', dest_path)\n    if (last_slash_pos == 0):\n        dest_folder = '/'\n    else:\n        dest_folder = dest_path[:last_slash_pos]\n    dest_name = dest_path[(last_slash_pos + 1):].replace('\\\\/', '/')\n    try:\n        dx_dest.list_folder(folder=dest_folder, only='folders')\n    except dxpy.DXAPIError as details:\n        if (details.code == requests.codes['not_found']):\n            raise DXCLIError('The destination folder does not exist')\n        else:\n            raise\n    except:\n        err_exit()\n    (src_proj, src_path, src_results) = try_call(resolve_existing_path, args.sources[0], allow_mult=True, all_mult=args.all)\n    if (src_proj == dest_proj):\n        if is_hashid(args.sources[0]):\n            raise DXCLIError(fill(('Error: You must specify a source project for ' + args.sources[0])))\n        else:\n            raise DXCLIError((fill((('A source path and the destination path resolved to the ' + 'same project or container.  Please specify different source ') + 'and destination containers, e.g.')) + '\\n  dx cp source-project:source-id-or-path dest-project:dest-path'))\n    if (src_results is None):\n        try:\n            contents = dxpy.api.project_list_folder(src_proj, {\n                'folder': src_path,\n                'includeHidden': True,\n            })\n            dxpy.api.project_new_folder(dest_proj, {\n                'folder': dest_path,\n            })\n            exists = dxpy.api.project_clone(src_proj, {\n                'folders': contents['folders'],\n                'objects': [result['id'] for result in contents['objects']],\n                'project': dest_proj,\n                'destination': dest_path,\n            })['exists']\n            if (len(exists) > 0):\n                print(((fill(('The following objects already existed in the destination ' + 'container and were not copied:')) + '\\n ') + '\\n '.join(exists)))\n                return\n        except:\n            err_exit()\n    else:\n        try:\n            exists = dxpy.api.project_clone(src_proj, {\n                'objects': [result['id'] for result in src_results],\n                'project': dest_proj,\n                'destination': dest_folder,\n            })['exists']\n            if (len(exists) > 0):\n                print(((fill(('The following objects already existed in the destination ' + 'container and were not copied:')) + '\\n ') + '\\n '.join(exists)))\n            for result in src_results:\n                if (result['id'] not in exists):\n                    dxpy.DXHTTPRequest((('/' + result['id']) + '/rename'), {\n                        'project': dest_proj,\n                        'name': dest_name,\n                    })\n            return\n        except:\n            err_exit()\n", "label": 1}
{"function": "\n\ndef extract_update_count(update, all_apps=None):\n    doc = {\n        'addon': update.addon_id,\n        'date': update.date,\n        'count': update.count,\n        'id': update.id,\n        '_id': '{0}-{1}'.format(update.addon_id, update.date),\n        'versions': es_dict(update.versions),\n        'os': [],\n        'locales': [],\n        'apps': [],\n        'status': [],\n    }\n    if update.oses:\n        os = collections.defaultdict(int)\n        for (key, count) in update.oses.items():\n            platform = None\n            if (unicode(key).lower() in amo.PLATFORM_DICT):\n                platform = amo.PLATFORM_DICT[unicode(key).lower()]\n            elif (key in amo.PLATFORMS):\n                platform = amo.PLATFORMS[key]\n            if (platform is not None):\n                os[platform.name] += count\n                doc['os'] = es_dict(((unicode(k), v) for (k, v) in os.items()))\n    if update.locales:\n        locales = collections.defaultdict(int)\n        for (locale, count) in update.locales.items():\n            try:\n                locales[locale.lower()] += int(count)\n            except ValueError:\n                pass\n        doc['locales'] = es_dict(locales)\n    if update.applications:\n        apps = collections.defaultdict(dict)\n        for (guid, version_counts) in update.applications.items():\n            if (guid not in amo.APP_GUIDS):\n                continue\n            app = amo.APP_GUIDS[guid]\n            for (version, count) in version_counts.items():\n                try:\n                    apps[app.guid][version] = int(count)\n                except ValueError:\n                    pass\n        doc['apps'] = dict(((app, es_dict(vals)) for (app, vals) in apps.items()))\n    if update.statuses:\n        doc['status'] = es_dict(((k, v) for (k, v) in update.statuses.items() if (k != 'null')))\n    return doc\n", "label": 1}
{"function": "\n\ndef test_aggregate_request(graph, groups, permissions, session, standard_graph, users):\n    gary = users['gary@a.co']\n    testuser = users['testuser@a.co']\n    not_involved = [user for (name, user) in users.items() if (name not in ('gary@a.co', 'testuser@a.co'))]\n    assert (not any([u.my_requests_aggregate().all() for u in users.values()])), 'should have no pending requests to begin with'\n    groups['team-sre'].add_member(users['testuser@a.co'], users['testuser@a.co'], reason='for the lulz')\n    session.commit()\n    assert (len(gary.my_requests_aggregate().all()) == 1), 'one pending request for owner'\n    assert (not any([u.my_requests_aggregate().all() for u in not_involved])), \"no pending requests if you're not the owner\"\n    groups['team-infra'].add_member(users['testuser@a.co'], users['testuser@a.co'], reason='for the lulz')\n    session.commit()\n    request_gary = gary.my_requests_aggregate().all()\n    assert (len(request_gary) == 2), 'two pending request for owner'\n    assert (not any([u.my_requests_aggregate().all() for u in not_involved])), \"no pending requests if you're not the owner\"\n    request = session.query(Request).filter_by(id=request_gary[0].id).scalar()\n    request.update_status(users['gary@a.co'], 'actioned', 'for being a good person')\n    session.commit()\n    assert (len(gary.my_requests_aggregate().all()) == 1), 'one pending request for owner'\n    assert (not any([u.my_requests_aggregate().all() for u in not_involved])), \"no pending requests if you're not the owner\"\n    groups['security-team'].add_member(users['testuser@a.co'], users['testuser@a.co'], reason='for the lulz')\n    session.commit()\n    assert (len(gary.my_requests_aggregate().all()) == 1), 'super owner should not get request'\n    assert (len(users['oliver@a.co'].my_requests_aggregate().all()) == 1), 'owner should get request'\n    user_not_gary_oliver = [u for (n, u) in users.items() if (n not in ('gary@a.co', 'oliver@a.co'))]\n    assert (not any([u.my_requests_aggregate().all() for u in user_not_gary_oliver]))\n    figurehead = users['figurehead@a.co']\n    add_member(groups['audited-team'], figurehead, role='manager')\n    assert (len(figurehead.my_requests_aggregate().all()) == 0), 'no request for np-owner at first'\n    groups['tech-ops'].add_member(users['testuser@a.co'], users['testuser@a.co'], reason='for the lulz')\n    assert (len(figurehead.my_requests_aggregate().all()) == 1), 'request for np-owner'\n    groups['audited-team'].add_member(users['testuser@a.co'], users['testuser@a.co'], reason='for the lulz')\n    assert (len(figurehead.my_requests_aggregate().all()) == 2), 'request for np-owner and manager'\n", "label": 1}
{"function": "\n\ndef run_objects(self, objects):\n    'Invoke the `run` method on a list of mixed runnable objects.\\n\\n        Arguments:\\n            objects:\\n                A list of runnable objects. A runnable object is basically\\n                something that has a callable named \"run\" that takes a\\n                configuration object as a parameter.\\n\\n        May raise TestRunnerError if an object is not runnable by this test\\n        runner.\\n        '\n    rv = 0\n    testcases = []\n    for obj in objects:\n        objecttype = type(obj)\n        if ((objecttype is ModuleType) and hasattr(obj, 'run')):\n            rv = self.run_module(obj)\n        elif ((objecttype is TypeType) and issubclass(obj, core.Test)):\n            testcases.append(obj)\n        elif isinstance(obj, core.TestSuite):\n            rv = self.run_object(obj)\n        elif ((objecttype is type) and hasattr(obj, 'run')):\n            rv = self.run_class(obj)\n        else:\n            logging.warn(('%r is not a runnable object.' % (obj,)))\n    if testcases:\n        if (len(testcases) > 1):\n            rv = self.run_tests(testcases)\n        else:\n            args = []\n            kwargs = {\n                \n            }\n            opts = self.config.options_override.copy()\n            for (name, value) in opts.items():\n                if name.startswith('arg'):\n                    try:\n                        index = int(name[3])\n                    except (ValueError, IndexError):\n                        logging.warn('{!r} not converted to argument.'.format(name))\n                    else:\n                        try:\n                            args[index] = value\n                        except IndexError:\n                            need = (index - len(args))\n                            while need:\n                                args.append(None)\n                                need -= 1\n                            args.append(value)\n                    del self.config.options_override[name]\n                    del self.config[name]\n                elif name.startswith('kwarg_'):\n                    kwargs[name[6:]] = value\n                    del self.config.options_override[name]\n                    del self.config[name]\n            rv = self.run_test(testcases[0], *args, **kwargs)\n    return rv\n", "label": 1}
{"function": "\n\ndef __setattr__(self, name, value):\n    '\\n\\t\\tUpdates :attr:`._changed_fields` when new values are set for fields.\\n\\t\\t\\n\\t\\t'\n    if (hasattr(self, '_changed_fields') and (name in super(BaseChangeTracker, self).__getattribute__('_meta')._forward_fields_map)):\n        try:\n            field = self._meta.get_field(name)\n        except FieldDoesNotExist:\n            field = None\n        if (field and (not field.hidden) and (field.__class__ not in (ManyToManyField, ManyToOneRel))):\n            old = self.__dict__.get(field.attname, DoesNotExist)\n            if ((old is not DoesNotExist) and field.is_relation):\n                try:\n                    hydrated_old = getattr(self, getattr(self.__class__, field.name).cache_name)\n                    if (hydrated_old.pk != old):\n                        hydrated_old = DoesNotExist\n                except AttributeError:\n                    hydrated_old = DoesNotExist\n            else:\n                hydrated_old = DoesNotExist\n            super(BaseChangeTracker, self).__setattr__(name, value)\n            new = self.__dict__.get(field.attname, DoesNotExist)\n            try:\n                changed = (old != new)\n            except Exception:\n                changed = True\n            if changed:\n                if (field.attname in self._changed_fields):\n                    if (self._changed_fields[field.attname] == new):\n                        self._changed_fields.pop(field.attname, None)\n                        if (field.attname != field.name):\n                            self._changed_fields.pop(field.name, None)\n                else:\n                    self._changed_fields[field.attname] = copy(old)\n                    if ((field.attname != field.name) and (hydrated_old is not DoesNotExist)):\n                        self._changed_fields[field.name] = copy(hydrated_old)\n        else:\n            super(BaseChangeTracker, self).__setattr__(name, value)\n    else:\n        super(BaseChangeTracker, self).__setattr__(name, value)\n", "label": 1}
{"function": "\n\ndef transform(self, node, results):\n    assert results\n    bare_print = results.get('bare')\n    if bare_print:\n        bare_print.replace(Call(Name('print'), [], prefix=bare_print.prefix))\n        return\n    assert (node.children[0] == Name('print'))\n    args = node.children[1:]\n    if ((len(args) == 1) and parend_expr.match(args[0])):\n        return\n    sep = end = file = None\n    if (args and (args[(- 1)] == Comma())):\n        args = args[:(- 1)]\n        end = ' '\n    if (args and (args[0] == pytree.Leaf(token.RIGHTSHIFT, '>>'))):\n        assert (len(args) >= 2)\n        file = args[1].clone()\n        args = args[3:]\n    l_args = [arg.clone() for arg in args]\n    if l_args:\n        l_args[0].prefix = ''\n    if ((sep is not None) or (end is not None) or (file is not None)):\n        if (sep is not None):\n            self.add_kwarg(l_args, 'sep', String(repr(sep)))\n        if (end is not None):\n            self.add_kwarg(l_args, 'end', String(repr(end)))\n        if (file is not None):\n            self.add_kwarg(l_args, 'file', file)\n    n_stmt = Call(Name('print'), l_args)\n    n_stmt.prefix = node.prefix\n    return n_stmt\n", "label": 1}
{"function": "\n\ndef _triage_read(node):\n    h5py = _check_h5py()\n    type_str = node.attrs['TITLE']\n    if isinstance(type_str, bytes):\n        type_str = type_str.decode()\n    if isinstance(node, h5py.Group):\n        if (type_str == 'dict'):\n            data = dict()\n            for (key, subnode) in node.items():\n                data[key[4:]] = _triage_read(subnode)\n        elif (type_str in ['list', 'tuple']):\n            data = list()\n            ii = 0\n            while True:\n                subnode = node.get('idx_{0}'.format(ii), None)\n                if (subnode is None):\n                    break\n                data.append(_triage_read(subnode))\n                ii += 1\n            assert (len(data) == ii)\n            data = (tuple(data) if (type_str == 'tuple') else data)\n            return data\n        elif (type_str == 'csc_matrix'):\n            if (sparse is None):\n                raise RuntimeError('scipy must be installed to read this data')\n            data = sparse.csc_matrix((_triage_read(node['data']), _triage_read(node['indices']), _triage_read(node['indptr'])))\n        else:\n            raise NotImplementedError('Unknown group type: {0}'.format(type_str))\n    elif (type_str == 'ndarray'):\n        data = np.array(node)\n    elif (type_str in ('int', 'float')):\n        cast = (int if (type_str == 'int') else float)\n        data = cast(np.array(node)[0])\n    elif (type_str in ('unicode', 'ascii', 'str')):\n        decoder = ('utf-8' if (type_str == 'unicode') else 'ASCII')\n        cast = (text_type if (type_str == 'unicode') else str)\n        data = cast(np.array(node).tostring().decode(decoder))\n    elif (type_str == 'None'):\n        data = None\n    else:\n        raise TypeError('Unknown node type: {0}'.format(type_str))\n    return data\n", "label": 1}
{"function": "\n\ndef _get_methods(self, vtk_obj):\n    'Obtain the various methods from the passed object.'\n\n    def _remove_method(name, methods, method_names):\n        'Removes methods if they have a particular name.'\n        try:\n            idx = method_names.index(name)\n        except ValueError:\n            pass\n        else:\n            del methods[idx], method_names[idx]\n        return (methods, method_names)\n    if (not hasattr(vtk_obj, 'GetClassName')):\n        return []\n    methods = str(vtk_obj)\n    methods = methods.split('\\n')\n    del methods[0]\n    patn = re.compile('  \\\\S')\n    for method in methods[:]:\n        if patn.match(method):\n            if (method.find(':') == (- 1)):\n                methods.remove(method)\n            elif (method[1].find('none') > (- 1)):\n                methods.remove(method)\n        else:\n            methods.remove(method)\n    for method in methods[:]:\n        if (method.strip()[:6] == 'Props:'):\n            if hasattr(vtk_obj, 'GetViewProps'):\n                methods.remove(method)\n                methods.append('ViewProps: ')\n        elif (method.strip()[:5] == 'Prop:'):\n            if hasattr(vtk_obj, 'GetViewProp'):\n                methods.remove(method)\n                methods.append('ViewProp: ')\n    method_names = []\n    for i in range(0, len(methods)):\n        strng = methods[i].replace(' ', '')\n        methods[i] = strng.split(':')\n        method_names.append(methods[i][0])\n    if re.match('vtk\\\\w*Renderer', vtk_obj.GetClassName()):\n        methods.append(['ActiveCamera', ''])\n    if re.match('vtk\\\\w*Assembly', vtk_obj.GetClassName()):\n        methods.append(['Parts', ''])\n        methods.append(['Volumes', ''])\n        methods.append(['Actors', ''])\n    if vtk_obj.IsA('vtkAbstractTransform'):\n        if (self.last_transform > 0):\n            _remove_method('Inverse', methods, method_names)\n        else:\n            self.last_transform += 1\n    else:\n        self.last_transform = 0\n    for name in ('Output', 'FieldData', 'CellData', 'PointData', 'Source', 'Input', 'ExtentTranslator', 'Interactor', 'Lights', 'Information', 'Executive'):\n        _remove_method(name, methods, method_names)\n    return methods\n", "label": 1}
{"function": "\n\ndef inline_function(self, node):\n    name = self.visit(node.func)\n    fnode = self._global_functions[name]\n    fnode = copy.deepcopy(fnode)\n    finfo = inspect_function(fnode)\n    remap = {\n        \n    }\n    for n in finfo['name_nodes']:\n        if (n.id not in finfo['locals']):\n            continue\n        if isinstance(n.id, ast.Name):\n            raise RuntimeError\n        if (n.id not in remap):\n            new_name = (n.id + ('_%s' % self._inline_ids))\n            remap[n.id] = new_name\n            self._inline_ids += 1\n        n.id = remap[n.id]\n    if remap:\n        self.writer.write(self.inline_helper_remap_names(remap))\n        for n in remap:\n            if (n in finfo['typedefs']):\n                self._func_typedefs[remap[n]] = finfo['typedefs'][n]\n    offset = (len(fnode.args.args) - len(fnode.args.defaults))\n    for (i, ad) in enumerate(fnode.args.args):\n        if (i < len(node.args)):\n            ac = self.visit(node.args[i])\n        else:\n            assert fnode.args.defaults\n            dindex = (i - offset)\n            ac = self.visit(fnode.args.defaults[dindex])\n        ad = remap[self.visit(ad)]\n        self.writer.write(('%s = %s' % (ad, ac)))\n    return_id = (name + str(self._inline_ids))\n    self._inline.append(return_id)\n    self.writer.write(self.inline_helper_return_id(return_id))\n    if True:\n        self._inline_breakout = True\n        self.writer.write('while True:')\n        self.writer.push()\n        for b in fnode.body:\n            self.visit(b)\n        if (not len(finfo['return_nodes'])):\n            self.writer.write('break')\n        self.writer.pull()\n    else:\n        for b in fnode.body:\n            self.visit(b)\n    if (self._inline.pop() != return_id):\n        raise RuntimeError\n    for n in remap:\n        gname = remap[n]\n        for n in finfo['name_nodes']:\n            if (n.id == gname):\n                n.id = n\n    return ('__returns__%s' % return_id)\n", "label": 1}
{"function": "\n\ndef test_result_generation():\n    jobstep = JobStep(id=uuid.uuid4(), project_id=uuid.uuid4(), job_id=uuid.uuid4())\n    fp = StringIO(SAMPLE_XUNIT)\n    handler = XunitHandler(jobstep)\n    results = handler.get_tests(fp)\n    assert (len(results) == 2)\n    r1 = results[0]\n    assert (type(r1) == TestResult)\n    assert (r1.step == jobstep)\n    assert (r1.package is None)\n    assert (r1.name == 'tests.test_report')\n    assert (r1.duration == 0.0)\n    assert (r1.result == Result.failed)\n    assert (r1.message == 'tests/test_report.py:1: in <module>\\n>   import mock\\nE   ImportError: No module named mock')\n    assert (r1.owner == 'foo')\n    r2 = results[1]\n    assert (type(r2) == TestResult)\n    assert (r2.step == jobstep)\n    assert (r2.package is None)\n    assert (r2.name == 'tests.test_report.ParseTestResultsTest.test_simple')\n    assert (r2.duration == 1.65796279907)\n    assert (r2.result == Result.passed)\n    assert (r2.message == '')\n    assert (r2.reruns == 1)\n    assert (r2.owner is None)\n", "label": 1}
{"function": "\n\ndef reset(self, **traits):\n    'Creates the dataset afresh or resets existing data source.'\n    for name in ('x', 'y', 'z', 'u', 'v', 'w', 'scalars'):\n        if ((name in traits) and (traits[name] is not None)):\n            traits[name] = np.atleast_1d(traits[name])\n    self.set(trait_change_notify=False, **traits)\n    vectors = self.vectors\n    scalars = self.scalars\n    points = self.points\n    (x, y, z) = (self.x, self.y, self.z)\n    x = np.atleast_1d(x)\n    y = np.atleast_1d(y)\n    z = np.atleast_1d(z)\n    if ('points' in traits):\n        x = points[:, 0].ravel()\n        y = points[:, 1].ravel()\n        z = points[:, 2].ravel()\n        self.set(x=x, y=y, z=z, trait_change_notify=False)\n    else:\n        points = np.c_[(x.ravel(), y.ravel(), z.ravel())].ravel()\n        points.shape = ((- 1), 3)\n        self.set(points=points, trait_change_notify=False)\n    (u, v, w) = (self.u, self.v, self.w)\n    if (u is not None):\n        u = np.atleast_1d(u)\n        v = np.atleast_1d(v)\n        w = np.atleast_1d(w)\n        if (len(u) > 0):\n            vectors = np.c_[(u.ravel(), v.ravel(), w.ravel())].ravel()\n            vectors.shape = ((- 1), 3)\n            self.set(vectors=vectors, trait_change_notify=False)\n    if ('vectors' in traits):\n        u = vectors[:, 0].ravel()\n        v = vectors[:, 1].ravel()\n        w = vectors[:, 2].ravel()\n        self.set(u=u, v=v, w=w, trait_change_notify=False)\n    elif ((u is not None) and (len(u) > 0)):\n        vectors = np.c_[(u.ravel(), v.ravel(), w.ravel())].ravel()\n        vectors.shape = ((- 1), 3)\n        self.set(vectors=vectors, trait_change_notify=False)\n    if ((vectors is not None) and (len(vectors) > 0)):\n        assert (len(points) == len(vectors))\n    if (scalars is not None):\n        scalars = np.atleast_1d(scalars)\n        if (len(scalars) > 0):\n            assert (len(points) == len(scalars.ravel()))\n    polys = np.arange(0, len(points), 1, 'l')\n    polys = np.reshape(polys, (len(points), 1))\n    if (self.dataset is None):\n        pd = tvtk.PolyData()\n    else:\n        pd = self.dataset\n    pd.set(points=points, polys=polys)\n    if (self.vectors is not None):\n        pd.point_data.vectors = self.vectors\n        pd.point_data.vectors.name = 'vectors'\n    if (self.scalars is not None):\n        pd.point_data.scalars = self.scalars.ravel()\n        pd.point_data.scalars.name = 'scalars'\n    self.dataset = pd\n", "label": 1}
{"function": "\n\ndef _load(self, event):\n    if (event.type == START_OBJECT):\n        value = start = '{'\n        end = '}'\n    elif (event.type == START_ARRAY):\n        value = start = '['\n        end = ']'\n    else:\n        raise JSONParseError(JSON_UNEXPECTED_ELEMENT_ERROR, ('Unexpected event: ' + event.type))\n    count = 1\n    tokens = self.tokens\n    tokenIndex = self.tokenIndex\n    inString = False\n    inEscape = False\n    try:\n        while True:\n            startIndex = tokenIndex\n            for token in tokens[startIndex:]:\n                tokenIndex += 1\n                if (token == ''):\n                    pass\n                elif inString:\n                    if inEscape:\n                        inEscape = False\n                    elif (token == '\"'):\n                        inString = False\n                    elif (token == '\\\\'):\n                        inEscape = True\n                elif (token == '\"'):\n                    inString = True\n                elif (token == start):\n                    count += 1\n                elif (token == end):\n                    count -= 1\n                    if (count == 0):\n                        value += ''.join(tokens[startIndex:tokenIndex])\n                        raise StopIteration()\n            value += ''.join(tokens[startIndex:])\n            data = self.stream.read(self.size)\n            if (data == ''):\n                raise JSONParseError(JSON_INCOMPLETE_ERROR, 'Reached end of input before reaching end of JSON structures.')\n            tokens = self.pattern.split(data)\n            tokenIndex = 0\n    except StopIteration:\n        pass\n    self.tokens = tokens\n    self.tokenIndex = tokenIndex\n    try:\n        return json.loads(value, parse_float=decimal.Decimal, parse_int=decimal.Decimal)\n    except ValueError as e:\n        raise JSONParseError(JSON_SYNTAX_ERROR, ''.join(e.args))\n", "label": 1}
{"function": "\n\n@set_ev_cls(ofp_event.EventOFPStateChange, [MAIN_DISPATCHER, DEAD_DISPATCHER])\ndef state_change_handler(self, ev):\n    dp = ev.datapath\n    assert (dp is not None)\n    LOG.debug(dp)\n    if (ev.state == MAIN_DISPATCHER):\n        dp_multiple_conns = False\n        if (dp.id in self.dps):\n            LOG.warning('Multiple connections from %s', dpid_to_str(dp.id))\n            dp_multiple_conns = True\n            self.dps[dp.id].close()\n        self._register(dp)\n        switch = self._get_switch(dp.id)\n        LOG.debug('register %s', switch)\n        if (not dp_multiple_conns):\n            self.send_event_to_observers(event.EventSwitchEnter(switch))\n        else:\n            self.send_event_to_observers(event.EventSwitchReconnected(switch))\n        if (not self.link_discovery):\n            return\n        if self.install_flow:\n            ofproto = dp.ofproto\n            ofproto_parser = dp.ofproto_parser\n            if (ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION):\n                rule = nx_match.ClsRule()\n                rule.set_dl_dst(addrconv.mac.text_to_bin(lldp.LLDP_MAC_NEAREST_BRIDGE))\n                rule.set_dl_type(ETH_TYPE_LLDP)\n                actions = [ofproto_parser.OFPActionOutput(ofproto.OFPP_CONTROLLER, self.LLDP_PACKET_LEN)]\n                dp.send_flow_mod(rule=rule, cookie=0, command=ofproto.OFPFC_ADD, idle_timeout=0, hard_timeout=0, actions=actions, priority=65535)\n            elif (ofproto.OFP_VERSION >= ofproto_v1_2.OFP_VERSION):\n                match = ofproto_parser.OFPMatch(eth_type=ETH_TYPE_LLDP, eth_dst=lldp.LLDP_MAC_NEAREST_BRIDGE)\n                parser = ofproto_parser\n                actions = [parser.OFPActionOutput(ofproto.OFPP_CONTROLLER, ofproto.OFPCML_NO_BUFFER)]\n                inst = [parser.OFPInstructionActions(ofproto.OFPIT_APPLY_ACTIONS, actions)]\n                mod = parser.OFPFlowMod(datapath=dp, match=match, idle_timeout=0, hard_timeout=0, instructions=inst, priority=65535)\n                dp.send_msg(mod)\n            else:\n                LOG.error('cannot install flow. unsupported version. %x', dp.ofproto.OFP_VERSION)\n        if (not dp_multiple_conns):\n            for port in switch.ports:\n                if (not port.is_reserved()):\n                    self._port_added(port)\n        self.lldp_event.set()\n    elif (ev.state == DEAD_DISPATCHER):\n        if (dp.id is None):\n            return\n        switch = self._get_switch(dp.id)\n        if switch:\n            if (switch.dp is dp):\n                self._unregister(dp)\n                LOG.debug('unregister %s', switch)\n                self.send_event_to_observers(event.EventSwitchLeave(switch))\n                if (not self.link_discovery):\n                    return\n                for port in switch.ports:\n                    if (not port.is_reserved()):\n                        self.ports.del_port(port)\n                        self._link_down(port)\n                self.lldp_event.set()\n", "label": 1}
{"function": "\n\ndef _find_volume(self, volume_id=None, name=None, device=None):\n    vol = None\n    if volume_id:\n        vol = self._volume_by_id(volume_id)\n    else:\n        all_volumes = [v for v in self.ec2.get_all_volumes() if (v.status in ('in-use', 'available'))]\n        if device:\n            for v in all_volumes:\n                if (v.attach_data and (v.attach_data.device == device) and (v.attach_data.instance_id == self.resource.env.config.aws.instance_id)):\n                    vol = v\n                    break\n        if ((not vol) and name):\n            if ('{index}' in name):\n                allnames = dict(((v.tags.get('Name'), v) for v in all_volumes))\n                for i in itertools.count(1):\n                    vname = name.format(index=i)\n                    try:\n                        v = allnames[vname]\n                        if (v.status == 'available'):\n                            vol = v\n                            break\n                    except KeyError:\n                        self.resource.name = vname\n                        break\n            else:\n                for v in all_volumes:\n                    if (v.tags.get('Name') == name):\n                        vol = v\n                        break\n    if (vol and ('{index}' in name)):\n        self.resource.name = vol.tags.get('Name')\n    return vol\n", "label": 1}
{"function": "\n\ndef _handle_request(self, method):\n    'Perform some common checks and call appropriate method.'\n    url = urlparse(self.path)\n    data = {key: data[(- 1)] for (key, data) in parse_qs(url.query).items()}\n    content_length = int(self.headers.get(HTTP_HEADER_CONTENT_LENGTH, 0))\n    if content_length:\n        body_content = self.rfile.read(content_length).decode('UTF-8')\n        try:\n            data.update(json.loads(body_content))\n        except (TypeError, ValueError):\n            _LOGGER.exception('Exception parsing JSON: %s', body_content)\n            self.write_json_message('Error parsing JSON', HTTP_UNPROCESSABLE_ENTITY)\n            return\n    if self.verify_session():\n        self.authenticated = True\n    elif (self.server.api_password is None):\n        self.authenticated = True\n    elif hmac.compare_digest(self.headers.get(HTTP_HEADER_HA_AUTH, ''), self.server.api_password):\n        self.authenticated = True\n    elif hmac.compare_digest(data.get(DATA_API_PASSWORD, ''), self.server.api_password):\n        self.authenticated = True\n    else:\n        self.authenticated = False\n    if (url.path not in [URL_ROOT, URL_API_EVENT_FORWARD]):\n        data.pop(DATA_API_PASSWORD, None)\n    if ('_METHOD' in data):\n        method = data.pop('_METHOD')\n    path_matched_but_not_method = False\n    handle_request_method = False\n    require_auth = True\n    for (t_method, t_path, t_handler, t_auth) in self.server.paths:\n        if isinstance(t_path, str):\n            path_match = (url.path == t_path)\n        else:\n            path_match = t_path.match(url.path)\n        if (path_match and (method == t_method)):\n            handle_request_method = t_handler\n            require_auth = t_auth\n            break\n        elif path_match:\n            path_matched_but_not_method = True\n    if handle_request_method:\n        msg = 'API password missing or incorrect.'\n        if (require_auth and (not self.authenticated)):\n            self.write_json_message(msg, HTTP_UNAUTHORIZED)\n            _LOGGER.warning('%s Source IP: %s', msg, self.client_address[0])\n            return\n        handle_request_method(self, path_match, data)\n    elif path_matched_but_not_method:\n        self.send_response(HTTP_METHOD_NOT_ALLOWED)\n        self.end_headers()\n    else:\n        self.send_response(HTTP_NOT_FOUND)\n        self.end_headers()\n", "label": 1}
{"function": "\n\ndef test_triangulate(self, huang_darwiche_moralized):\n\n    def priority_func_override(node):\n        introduced_arcs = 0\n        cluster = ([node] + node.neighbours)\n        for (node_a, node_b) in combinations(cluster, 2):\n            if (node_a not in node_b.neighbours):\n                assert (node_b not in node_a.neighbours)\n                introduced_arcs += 1\n        if (node.name == 'f_h'):\n            return [introduced_arcs, 0]\n        if (node.name == 'f_g'):\n            return [introduced_arcs, 1]\n        if (node.name == 'f_c'):\n            return [introduced_arcs, 2]\n        if (node.name == 'f_b'):\n            return [introduced_arcs, 3]\n        if (node.name == 'f_d'):\n            return [introduced_arcs, 4]\n        if (node.name == 'f_e'):\n            return [introduced_arcs, 5]\n        return [introduced_arcs, 10]\n    (cliques, elimination_ordering) = triangulate(huang_darwiche_moralized, priority_func_override)\n    nodes = dict([(node.name, node) for node in huang_darwiche_moralized.nodes])\n    assert (len(cliques) == 6)\n    assert (cliques[0].nodes == set([nodes['f_e'], nodes['f_g'], nodes['f_h']]))\n    assert (cliques[1].nodes == set([nodes['f_c'], nodes['f_e'], nodes['f_g']]))\n    assert (cliques[2].nodes == set([nodes['f_d'], nodes['f_e'], nodes['f_f']]))\n    assert (cliques[3].nodes == set([nodes['f_a'], nodes['f_c'], nodes['f_e']]))\n    assert (cliques[4].nodes == set([nodes['f_a'], nodes['f_b'], nodes['f_d']]))\n    assert (cliques[5].nodes == set([nodes['f_a'], nodes['f_d'], nodes['f_e']]))\n    assert (elimination_ordering == ['f_h', 'f_g', 'f_f', 'f_c', 'f_b', 'f_d', 'f_e', 'f_a'])\n    nodes = dict([(node.name, node) for node in huang_darwiche_moralized.nodes])\n    assert (set(nodes['f_a'].neighbours) == set([nodes['f_b'], nodes['f_c'], nodes['f_d'], nodes['f_e']]))\n    assert (set(nodes['f_b'].neighbours) == set([nodes['f_a'], nodes['f_d']]))\n    assert (set(nodes['f_c'].neighbours) == set([nodes['f_a'], nodes['f_e'], nodes['f_g']]))\n    assert (set(nodes['f_d'].neighbours) == set([nodes['f_b'], nodes['f_f'], nodes['f_e'], nodes['f_a']]))\n    assert (set(nodes['f_e'].neighbours) == set([nodes['f_c'], nodes['f_f'], nodes['f_h'], nodes['f_d'], nodes['f_g'], nodes['f_a']]))\n    assert (set(nodes['f_f'].neighbours) == set([nodes['f_d'], nodes['f_e']]))\n    assert (set(nodes['f_g'].neighbours) == set([nodes['f_c'], nodes['f_h'], nodes['f_e']]))\n    assert (set(nodes['f_h'].neighbours) == set([nodes['f_e'], nodes['f_g']]))\n", "label": 1}
{"function": "\n\ndef execute(self, parameters, messages):\n    'The source code of the tool.'\n    arcpy.env.workspace = parameters[0].value\n    schema_version = parameters[1].value\n    hi_batchsize = parameters[2].value\n    generate_ID_table_name = 'GenerateID'\n    seqnameField = 'name'\n    seqcounterField = 'hi'\n    seqintervalField = 'low'\n    if ((schema_version == '1.4') or (schema_version == '1.5')):\n        generate_ID_table_name = 'GenerateId'\n        seqnameField = 'SEQNAME'\n        seqcounterField = 'SEQCOUNTER'\n        seqintervalField = 'SEQINTERV'\n    new_table = None\n    counter_tbl_list = arcpy.ListTables(generate_ID_table_name)\n    if (not counter_tbl_list):\n        arcpy.AddMessage((('Creating new ' + generate_ID_table_name) + ' table.'))\n        new_table = True\n        generate_ID_table = arcpy.CreateTable_management(arcpy.env.workspace, generate_ID_table_name)\n        if (schema_version == '1.3'):\n            arcpy.AddField_management(generate_ID_table, seqnameField, 'TEXT', None, None, 50, 'Feature Class Name', 'NON_NULLABLE', 'REQUIRED')\n            arcpy.AddField_management(generate_ID_table, seqcounterField, 'LONG', None, None, None, 'Hi counter', 'NON_NULLABLE', 'REQUIRED')\n            arcpy.AddField_management(generate_ID_table, seqintervalField, 'LONG', None, None, None, 'Low counter', 'NON_NULLABLE', 'REQUIRED')\n        if ((schema_version == '1.4') or (schema_version == '1.5')):\n            arcpy.AddField_management(generate_ID_table, seqnameField, 'TEXT', None, None, 50, 'Sequence Name', 'NON_NULLABLE', 'NON_REQUIRED')\n            arcpy.AddField_management(generate_ID_table, seqcounterField, 'LONG', None, None, None, 'Sequence Counter', 'NON_NULLABLE', 'NON_REQUIRED')\n            arcpy.AddField_management(generate_ID_table, seqintervalField, 'SHORT', None, None, None, 'Interval Value', 'NULLABLE', 'NON_REQUIRED')\n            arcpy.AddField_management(generate_ID_table, 'COMMENTS', 'TEXT', None, None, 255, 'Comments', 'NULLABLE', 'NON_REQUIRED')\n    else:\n        new_table = False\n        generate_ID_table = counter_tbl_list[0]\n    fc_list = arcpy.ListFeatureClasses()\n    for fc in fc_list:\n        arcpy.AddMessage(('Processing ' + fc))\n        counter = 0\n        baseCount = 0\n        interval = hi_batchsize\n        if new_table:\n            insert_new_counter_cursor = arcpy.da.InsertCursor(generate_ID_table_name, [seqnameField, seqcounterField, seqintervalField])\n            insert_new_counter_cursor.insertRow((fc, 0, hi_batchsize))\n            del insert_new_counter_cursor\n        counterParams = None\n        escaped_name = arcpy.AddFieldDelimiters(generate_ID_table_name, seqnameField)\n        where_clause = ((((escaped_name + ' = ') + \"'\") + fc) + \"'\")\n        with arcpy.da.SearchCursor(generate_ID_table_name, [seqnameField, seqcounterField, seqintervalField], where_clause) as rows:\n            for counterRow in rows:\n                counterParams = counterRow\n                break\n        if (counterParams != None):\n            baseCount = counterParams[1]\n            interval = counterParams[2]\n        else:\n            insert_new_counter_cursor = arcpy.da.InsertCursor(generate_ID_table_name, [seqnameField, seqcounterField, seqintervalField])\n            insert_new_counter_cursor.insertRow((fc, 0, hi_batchsize))\n            del insert_new_counter_cursor\n        with arcpy.da.SearchCursor(generate_ID_table_name, [seqnameField, seqcounterField, seqintervalField]) as rows:\n            for row in rows:\n                if (row[0] == fc):\n                    baseCount = row[1]\n                    interval = row[2]\n                    break\n        self.incrementCounter(generate_ID_table_name, seqnameField, seqcounterField, fc, (baseCount + interval))\n        fid_name = (fc + 'FID')\n        fields_list = arcpy.ListFields(fc, fid_name)\n        if (not fields_list):\n            arcpy.AddField_management(fc, fid_name, 'TEXT', None, None, 50, 'Feature ID', None, None)\n        with arcpy.da.UpdateCursor(fc, [fid_name]) as rows:\n            for row in rows:\n                if (row[0] == None):\n                    if (counter >= interval):\n                        arcpy.AddMessage('Interval exhausted, getting next Interval.')\n                        with arcpy.da.SearchCursor(generate_ID_table_name, [seqcounterField], where_clause) as rows:\n                            for counterRow in rows:\n                                baseCount = counterRow[0]\n                                break\n                        counter = 0\n                    row[0] = ((fc + '/') + str((baseCount + counter)))\n                    counter += 1\n                    rows.updateRow(row)\n        with arcpy.da.UpdateCursor(generate_ID_table_name, [seqnameField, seqcounterField]) as rows:\n            for newRow in rows:\n                if (newRow[0] == fc):\n                    newRow[1] = (baseCount + counter)\n                    rows.updateRow(newRow)\n                    break\n    arcpy.AddMessage('Completed adding of Feature IDs.')\n    return\n", "label": 1}
{"function": "\n\ndef test_build_bbn_from_conditionals():\n    UPDATE = {\n        'prize_door': [[[], {\n            'A': (1 / 3),\n            'B': (1 / 3),\n            'C': (1 / 3),\n        }]],\n        'guest_door': [[[], {\n            'A': (1 / 3),\n            'B': (1 / 3),\n            'C': (1 / 3),\n        }]],\n        'monty_door': [[[['prize_door', 'A'], ['guest_door', 'A']], {\n            'A': 0,\n            'B': 0.5,\n            'C': 0.5,\n        }], [[['prize_door', 'A'], ['guest_door', 'B']], {\n            'A': 0,\n            'B': 0,\n            'C': 1,\n        }], [[['prize_door', 'A'], ['guest_door', 'C']], {\n            'A': 0,\n            'B': 1,\n            'C': 0,\n        }], [[['prize_door', 'B'], ['guest_door', 'A']], {\n            'A': 0,\n            'B': 0,\n            'C': 1,\n        }], [[['prize_door', 'B'], ['guest_door', 'B']], {\n            'A': 0.5,\n            'B': 0,\n            'C': 0.5,\n        }], [[['prize_door', 'B'], ['guest_door', 'C']], {\n            'A': 1,\n            'B': 0,\n            'C': 0,\n        }], [[['prize_door', 'C'], ['guest_door', 'A']], {\n            'A': 0,\n            'B': 1,\n            'C': 0,\n        }], [[['prize_door', 'C'], ['guest_door', 'B']], {\n            'A': 1,\n            'B': 0,\n            'C': 0,\n        }], [[['prize_door', 'C'], ['guest_door', 'C']], {\n            'A': 0.5,\n            'B': 0.5,\n            'C': 0,\n        }]],\n    }\n    g = build_bbn_from_conditionals(UPDATE)\n    result = g.query()\n    assert close_enough(result[('guest_door', 'A')], 0.333)\n    assert close_enough(result[('guest_door', 'B')], 0.333)\n    assert close_enough(result[('guest_door', 'C')], 0.333)\n    assert close_enough(result[('monty_door', 'A')], 0.333)\n    assert close_enough(result[('monty_door', 'B')], 0.333)\n    assert close_enough(result[('monty_door', 'C')], 0.333)\n    assert close_enough(result[('prize_door', 'A')], 0.333)\n    assert close_enough(result[('prize_door', 'B')], 0.333)\n    assert close_enough(result[('prize_door', 'C')], 0.333)\n    result = g.query(guest_door='A', monty_door='B')\n    assert close_enough(result[('guest_door', 'A')], 1)\n    assert close_enough(result[('guest_door', 'B')], 0)\n    assert close_enough(result[('guest_door', 'C')], 0)\n    assert close_enough(result[('monty_door', 'A')], 0)\n    assert close_enough(result[('monty_door', 'B')], 1)\n    assert close_enough(result[('monty_door', 'C')], 0)\n    assert close_enough(result[('prize_door', 'A')], 0.333)\n    assert close_enough(result[('prize_door', 'B')], 0)\n    assert close_enough(result[('prize_door', 'C')], 0.667)\n", "label": 1}
{"function": "\n\ndef _check_repository(self, scmtool_class, path, username, password, local_site, trust_host, ret_cert, request):\n    if local_site:\n        local_site_name = local_site.name\n    else:\n        local_site_name = None\n    while 1:\n        try:\n            scmtool_class.check_repository(path, username, password, local_site_name)\n            return None\n        except RepositoryNotFoundError:\n            return MISSING_REPOSITORY\n        except BadHostKeyError as e:\n            if trust_host:\n                try:\n                    client = SSHClient(namespace=local_site_name)\n                    client.replace_host_key(e.hostname, e.raw_expected_key, e.raw_key)\n                except IOError as e:\n                    return (SERVER_CONFIG_ERROR, {\n                        'reason': six.text_type(e),\n                    })\n            else:\n                return (BAD_HOST_KEY, {\n                    'hostname': e.hostname,\n                    'expected_key': e.raw_expected_key.get_base64(),\n                    'key': e.raw_key.get_base64(),\n                })\n        except UnknownHostKeyError as e:\n            if trust_host:\n                try:\n                    client = SSHClient(namespace=local_site_name)\n                    client.add_host_key(e.hostname, e.raw_key)\n                except IOError as e:\n                    return (SERVER_CONFIG_ERROR, {\n                        'reason': six.text_type(e),\n                    })\n            else:\n                return (UNVERIFIED_HOST_KEY, {\n                    'hostname': e.hostname,\n                    'key': e.raw_key.get_base64(),\n                })\n        except UnverifiedCertificateError as e:\n            if trust_host:\n                try:\n                    cert = scmtool_class.accept_certificate(path, local_site_name)\n                    if cert:\n                        ret_cert.update(cert)\n                except IOError as e:\n                    return (SERVER_CONFIG_ERROR, {\n                        'reason': six.text_type(e),\n                    })\n            else:\n                return (UNVERIFIED_HOST_CERT, {\n                    'certificate': {\n                        'failures': e.certificate.failures,\n                        'fingerprint': e.certificate.fingerprint,\n                        'hostname': e.certificate.hostname,\n                        'issuer': e.certificate.issuer,\n                        'valid': {\n                            'from': e.certificate.valid_from,\n                            'until': e.certificate.valid_until,\n                        },\n                    },\n                })\n        except AuthenticationError as e:\n            if (('publickey' in e.allowed_types) and (e.user_key is None)):\n                return MISSING_USER_KEY\n            else:\n                return (REPO_AUTHENTICATION_ERROR, {\n                    'reason': six.text_type(e),\n                })\n        except SSHError as e:\n            logging.error(('Got unexpected SSHError when checking repository: %s' % e), exc_info=1, request=request)\n            return (REPO_INFO_ERROR, {\n                'error': six.text_type(e),\n            })\n        except SCMError as e:\n            logging.error(('Got unexpected SCMError when checking repository: %s' % e), exc_info=1, request=request)\n            return (REPO_INFO_ERROR, {\n                'error': six.text_type(e),\n            })\n        except Exception as e:\n            logging.error('Unknown error in checking repository %s: %s', path, e, exc_info=1, request=request)\n            raise\n", "label": 1}
{"function": "\n\ndef _resolve_lookup(self, context):\n    \"\\n        Performs resolution of a real variable (i.e. not a literal) against the\\n        given context.\\n\\n        As indicated by the method's name, this method is an implementation\\n        detail and shouldn't be called by external code. Use Variable.resolve()\\n        instead.\\n        \"\n    current = context\n    try:\n        for bit in self.lookups:\n            try:\n                current = current[bit]\n            except (TypeError, AttributeError, KeyError, ValueError, IndexError):\n                try:\n                    if (isinstance(current, BaseContext) and getattr(type(current), bit)):\n                        raise AttributeError\n                    current = getattr(current, bit)\n                except (TypeError, AttributeError) as e:\n                    if (isinstance(e, AttributeError) and (not isinstance(current, BaseContext)) and (bit in dir(current))):\n                        raise\n                    try:\n                        current = current[int(bit)]\n                    except (IndexError, ValueError, KeyError, TypeError):\n                        raise VariableDoesNotExist('Failed lookup for key [%s] in %r', (bit, current))\n            if callable(current):\n                if getattr(current, 'do_not_call_in_templates', False):\n                    pass\n                elif getattr(current, 'alters_data', False):\n                    current = context.template.engine.string_if_invalid\n                else:\n                    try:\n                        current = current()\n                    except TypeError:\n                        try:\n                            inspect.getcallargs(current)\n                        except TypeError:\n                            current = context.template.engine.string_if_invalid\n                        else:\n                            raise\n    except Exception as e:\n        template_name = (getattr(context, 'template_name', None) or 'unknown')\n        logger.debug(\"Exception while resolving variable '%s' in template '%s'.\", bit, template_name, exc_info=True)\n        if getattr(e, 'silent_variable_failure', False):\n            current = context.template.engine.string_if_invalid\n        else:\n            raise\n    return current\n", "label": 1}
{"function": "\n\ndef main():\n    try:\n        signal.signal(signal.SIGTSTP, signal.SIG_IGN)\n        signal.signal(signal.SIGINT, signal_handler)\n    except AttributeError:\n        pass\n    if (len(sys.argv) == 1):\n        usage()\n    dnsrecord = 'A'\n    count = 10\n    waittime = 5\n    inputfilename = None\n    fromfile = False\n    hostname = 'wikipedia.org'\n    try:\n        (opts, args) = getopt.getopt(sys.argv[1:], 'hf:c:t:w:', ['help', 'file=', 'count=', 'type=', 'wait='])\n    except getopt.GetoptError as err:\n        print(err)\n        usage()\n    if (args and (len(args) == 1)):\n        hostname = args[0]\n    else:\n        usage()\n    for (o, a) in opts:\n        if (o in ('-h', '--help')):\n            usage()\n        elif (o in ('-c', '--count')):\n            count = int(a)\n        elif (o in ('-f', '--file')):\n            inputfilename = a\n            fromfile = True\n        elif (o in ('-w', '--wait')):\n            waittime = int(a)\n        elif (o in ('-t', '--type')):\n            dnsrecord = a\n        else:\n            print(('Invalid option: %s' % o))\n            usage()\n    try:\n        if fromfile:\n            with open(inputfilename, 'rt') as flist:\n                f = flist.read().splitlines()\n        else:\n            f = resolvers\n        if (len(f) == 0):\n            print('No nameserver specified')\n        f = [name.strip() for name in f]\n        width = maxlen(f)\n        blanks = ((width - 5) * ' ')\n        print('server ', blanks, ' avg(ms)     min(ms)     max(ms)     stddev(ms)  lost(%)')\n        print(((60 + width) * '-'))\n        for server in f:\n            if (not server):\n                continue\n            (server, r_avg, r_min, r_max, r_stddev, r_lost_percent) = dnsping(hostname, server, dnsrecord, waittime, count)\n            server = server.ljust((width + 1))\n            print(('%s    %-8.3f    %-8.3f    %-8.3f    %-8.3f    %%%d' % (server, r_avg, r_min, r_max, r_stddev, r_lost_percent)))\n    except Exception as e:\n        print(('error: %s' % e))\n        exit(1)\n", "label": 1}
{"function": "\n\n@verbose\ndef read_inverse_operator(fname, verbose=None):\n    'Read the inverse operator decomposition from a FIF file\\n\\n    Parameters\\n    ----------\\n    fname : string\\n        The name of the FIF file, which ends with -inv.fif or -inv.fif.gz.\\n    verbose : bool, str, int, or None\\n        If not None, override default verbose level (see mne.verbose).\\n\\n    Returns\\n    -------\\n    inv : instance of InverseOperator\\n        The inverse operator.\\n\\n    See Also\\n    --------\\n    write_inverse_operator, make_inverse_operator\\n    '\n    check_fname(fname, 'inverse operator', ('-inv.fif', '-inv.fif.gz'))\n    logger.info(('Reading inverse operator decomposition from %s...' % fname))\n    (f, tree, _) = fiff_open(fname, preload=True)\n    with f as fid:\n        invs = dir_tree_find(tree, FIFF.FIFFB_MNE_INVERSE_SOLUTION)\n        if ((invs is None) or (len(invs) < 1)):\n            raise Exception(('No inverse solutions in %s' % fname))\n        invs = invs[0]\n        parent_mri = dir_tree_find(tree, FIFF.FIFFB_MNE_PARENT_MRI_FILE)\n        if (len(parent_mri) == 0):\n            raise Exception(('No parent MRI information in %s' % fname))\n        parent_mri = parent_mri[0]\n        logger.info('    Reading inverse operator info...')\n        tag = find_tag(fid, invs, FIFF.FIFF_MNE_INCLUDED_METHODS)\n        if (tag is None):\n            raise Exception('Modalities not found')\n        inv = dict()\n        inv['methods'] = int(tag.data)\n        tag = find_tag(fid, invs, FIFF.FIFF_MNE_SOURCE_ORIENTATION)\n        if (tag is None):\n            raise Exception('Source orientation constraints not found')\n        inv['source_ori'] = int(tag.data)\n        tag = find_tag(fid, invs, FIFF.FIFF_MNE_SOURCE_SPACE_NPOINTS)\n        if (tag is None):\n            raise Exception('Number of sources not found')\n        inv['nsource'] = int(tag.data)\n        inv['nchan'] = 0\n        tag = find_tag(fid, invs, FIFF.FIFF_MNE_COORD_FRAME)\n        if (tag is None):\n            raise Exception('Coordinate frame tag not found')\n        inv['coord_frame'] = tag.data\n        tag = find_tag(fid, invs, FIFF.FIFF_MNE_INVERSE_SOURCE_UNIT)\n        unit_dict = {\n            FIFF.FIFF_UNIT_AM: 'Am',\n            FIFF.FIFF_UNIT_AM_M2: 'Am/m^2',\n            FIFF.FIFF_UNIT_AM_M3: 'Am/m^3',\n        }\n        inv['units'] = unit_dict.get(int(getattr(tag, 'data', (- 1))), None)\n        tag = find_tag(fid, invs, FIFF.FIFF_MNE_INVERSE_SOURCE_ORIENTATIONS)\n        if (tag is None):\n            raise Exception('Source orientation information not found')\n        inv['source_nn'] = tag.data\n        logger.info('    [done]')\n        logger.info('    Reading inverse operator decomposition...')\n        tag = find_tag(fid, invs, FIFF.FIFF_MNE_INVERSE_SING)\n        if (tag is None):\n            raise Exception('Singular values not found')\n        inv['sing'] = tag.data\n        inv['nchan'] = len(inv['sing'])\n        inv['eigen_leads_weighted'] = False\n        inv['eigen_leads'] = _read_named_matrix(fid, invs, FIFF.FIFF_MNE_INVERSE_LEADS, transpose=True)\n        if (inv['eigen_leads'] is None):\n            inv['eigen_leads_weighted'] = True\n            inv['eigen_leads'] = _read_named_matrix(fid, invs, FIFF.FIFF_MNE_INVERSE_LEADS_WEIGHTED, transpose=True)\n        if (inv['eigen_leads'] is None):\n            raise ValueError('Eigen leads not found in inverse operator.')\n        inv['eigen_fields'] = _read_named_matrix(fid, invs, FIFF.FIFF_MNE_INVERSE_FIELDS)\n        logger.info('    [done]')\n        inv['noise_cov'] = Covariance(**_read_cov(fid, invs, FIFF.FIFFV_MNE_NOISE_COV, limited=True))\n        logger.info('    Noise covariance matrix read.')\n        inv['source_cov'] = _read_cov(fid, invs, FIFF.FIFFV_MNE_SOURCE_COV)\n        logger.info('    Source covariance matrix read.')\n        inv['orient_prior'] = _read_cov(fid, invs, FIFF.FIFFV_MNE_ORIENT_PRIOR_COV)\n        if (inv['orient_prior'] is not None):\n            logger.info('    Orientation priors read.')\n        inv['depth_prior'] = _read_cov(fid, invs, FIFF.FIFFV_MNE_DEPTH_PRIOR_COV)\n        if (inv['depth_prior'] is not None):\n            logger.info('    Depth priors read.')\n        inv['fmri_prior'] = _read_cov(fid, invs, FIFF.FIFFV_MNE_FMRI_PRIOR_COV)\n        if (inv['fmri_prior'] is not None):\n            logger.info('    fMRI priors read.')\n        inv['src'] = _read_source_spaces_from_tree(fid, tree, patch_stats=False)\n        for s in inv['src']:\n            s['id'] = find_source_space_hemi(s)\n        tag = find_tag(fid, parent_mri, FIFF.FIFF_COORD_TRANS)\n        if (tag is None):\n            raise Exception('MRI/head coordinate transformation not found')\n        mri_head_t = _ensure_trans(tag.data, 'mri', 'head')\n        inv['mri_head_t'] = mri_head_t\n        inv['info'] = _read_forward_meas_info(tree, fid)\n        if (inv['coord_frame'] not in (FIFF.FIFFV_COORD_MRI, FIFF.FIFFV_COORD_HEAD)):\n            raise Exception('Only inverse solutions computed in MRI or head coordinates are acceptable')\n        inv['nave'] = 1\n        inv['projs'] = _read_proj(fid, tree)\n        inv['proj'] = []\n        inv['whitener'] = []\n        inv['reginv'] = []\n        inv['noisenorm'] = []\n        nuse = 0\n        for k in range(len(inv['src'])):\n            try:\n                inv['src'][k] = transform_surface_to(inv['src'][k], inv['coord_frame'], mri_head_t)\n            except Exception as inst:\n                raise Exception(('Could not transform source space (%s)' % inst))\n            nuse += inv['src'][k]['nuse']\n        logger.info('    Source spaces transformed to the inverse solution coordinate frame')\n    return InverseOperator(inv)\n", "label": 1}
{"function": "\n\n@verbose\ndef __init__(self, input_fname, events, event_id=None, tmin=0, baseline=None, reject=None, flat=None, reject_tmin=None, reject_tmax=None, mrk=None, elp=None, hsp=None, verbose=None):\n    if isinstance(events, string_types):\n        events = read_events(events)\n    if isinstance(mrk, list):\n        mrk = [(read_mrk(marker) if isinstance(marker, string_types) else marker) for marker in mrk]\n        mrk = np.mean(mrk, axis=0)\n    if ((mrk is not None) and (elp is not None) and (hsp is not None)):\n        (dig_points, dev_head_t) = _set_dig_kit(mrk, elp, hsp)\n        self.info['dig'] = dig_points\n        self.info['dev_head_t'] = dev_head_t\n    elif ((mrk is not None) or (elp is not None) or (hsp is not None)):\n        err = 'mrk, elp and hsp need to be provided as a group (all or none)'\n        raise ValueError(err)\n    logger.info(('Extracting KIT Parameters from %s...' % input_fname))\n    input_fname = op.abspath(input_fname)\n    (self.info, kit_info) = get_kit_info(input_fname)\n    self._raw_extras = [kit_info]\n    if (len(events) != self._raw_extras[0]['n_epochs']):\n        raise ValueError('Event list does not match number of epochs.')\n    if (self._raw_extras[0]['acq_type'] == 3):\n        self._raw_extras[0]['data_offset'] = KIT.RAW_OFFSET\n        self._raw_extras[0]['data_length'] = KIT.INT\n        self._raw_extras[0]['dtype'] = 'h'\n    else:\n        err = 'SQD file contains raw data, not epochs or average. Wrong reader.'\n        raise TypeError(err)\n    if (event_id is None):\n        event_id = dict(((str(e), int(e)) for e in np.unique(events[:, 2])))\n    for (key, val) in event_id.items():\n        if (val not in events[:, 2]):\n            raise ValueError(('No matching events found for %s (event id %i)' % (key, val)))\n    self._filename = input_fname\n    data = self._read_kit_data()\n    assert (data.shape == (self._raw_extras[0]['n_epochs'], self.info['nchan'], self._raw_extras[0]['frame_length']))\n    tmax = (((data.shape[2] - 1) / self.info['sfreq']) + tmin)\n    super(EpochsKIT, self).__init__(self.info, data, events, event_id, tmin, tmax, baseline, reject=reject, flat=flat, reject_tmin=reject_tmin, reject_tmax=reject_tmax, verbose=verbose)\n    logger.info('Ready.')\n", "label": 1}
{"function": "\n\ndef train_all(self, dataset, mu=None):\n    '\\n        Process kmeans algorithm on the input to localize clusters.\\n\\n        Parameters\\n        ----------\\n        dataset : WRITEME\\n        mu : WRITEME\\n\\n        Returns\\n        -------\\n        rval : bool\\n            WRITEME\\n        '\n    X = dataset.get_design_matrix()\n    (n, m) = X.shape\n    k = self.k\n    if (milk is not None):\n        (cluster_ids, mu) = milk.kmeans(X, k)\n    else:\n        if (mu is not None):\n            if (not (len(mu) == k)):\n                raise Exception(('You gave %i clusters, but k=%i were expected' % (len(mu), k)))\n        else:\n            indices = numpy.random.randint(X.shape[0], size=k)\n            mu = X[indices]\n        try:\n            dists = numpy.zeros((n, k))\n        except MemoryError as e:\n            improve_memory_error_message(e, 'dying trying to allocate dists matrix for {0} examples and {1} means'.format(n, k))\n        old_kills = {\n            \n        }\n        iter = 0\n        mmd = prev_mmd = float('inf')\n        while True:\n            if self.verbose:\n                logger.info('kmeans iter {0}'.format(iter))\n            if contains_nan(mu):\n                logger.info('nan found')\n                return X\n            for i in xrange(k):\n                dists[:, i] = numpy.square((X - mu[i, :])).sum(axis=1)\n            if (iter > 0):\n                prev_mmd = mmd\n            min_dists = dists.min(axis=1)\n            mmd = min_dists.mean()\n            logger.info('cost: {0}'.format(mmd))\n            if ((iter > 0) and ((iter >= self.max_iter) or (abs((mmd - prev_mmd)) < self.convergence_th))):\n                break\n            min_dist_inds = dists.argmin(axis=1)\n            i = 0\n            blacklist = []\n            new_kills = {\n                \n            }\n            while (i < k):\n                b = (min_dist_inds == i)\n                if (not numpy.any(b)):\n                    killed_on_prev_iter = True\n                    if (i in old_kills):\n                        d = (old_kills[i] - 1)\n                        if (d == 0):\n                            d = 50\n                        new_kills[i] = d\n                    else:\n                        d = 5\n                    mu[i, :] = 0\n                    for j in xrange(d):\n                        idx = numpy.argmax(min_dists)\n                        min_dists[idx] = 0\n                        mu[i, :] += X[idx, :]\n                        blacklist.append(idx)\n                    mu[i, :] /= float(d)\n                    dists[:, i] = numpy.square((X - mu[i, :])).sum(axis=1)\n                    min_dists = dists.min(axis=1)\n                    for idx in blacklist:\n                        min_dists[idx] = 0\n                    min_dist_inds = dists.argmin(axis=1)\n                    i += 1\n                else:\n                    mu[i, :] = numpy.mean(X[b, :], axis=0)\n                    if contains_nan(mu):\n                        logger.info('nan found at {0}'.format(i))\n                        return X\n                    i += 1\n            old_kills = new_kills\n            iter += 1\n    self.mu = sharedX(mu)\n    self._params = [self.mu]\n", "label": 1}
{"function": "\n\ndef _build_and_run_instance(self, context, instance, image, injected_files, admin_password, requested_networks, security_groups, block_device_mapping, node, limits, filter_properties):\n    image_name = image.get('name')\n    self._notify_about_instance_usage(context, instance, 'create.start', extra_usage_info={\n        'image_name': image_name,\n    })\n    try:\n        rt = self._get_resource_tracker(node)\n        with rt.instance_claim(context, instance, limits):\n            self._validate_instance_group_policy(context, instance, filter_properties)\n            image_meta = objects.ImageMeta.from_dict(image)\n            with self._build_resources(context, instance, requested_networks, security_groups, image_meta, block_device_mapping) as resources:\n                instance.vm_state = vm_states.BUILDING\n                instance.task_state = task_states.SPAWNING\n                instance.save(expected_task_state=task_states.BLOCK_DEVICE_MAPPING)\n                block_device_info = resources['block_device_info']\n                network_info = resources['network_info']\n                LOG.debug('Start spawning the instance on the hypervisor.', instance=instance)\n                with timeutils.StopWatch() as timer:\n                    self.driver.spawn(context, instance, image_meta, injected_files, admin_password, network_info=network_info, block_device_info=block_device_info)\n                LOG.info(_LI('Took %0.2f seconds to spawn the instance on the hypervisor.'), timer.elapsed(), instance=instance)\n    except (exception.InstanceNotFound, exception.UnexpectedDeletingTaskStateError) as e:\n        with excutils.save_and_reraise_exception():\n            self._notify_about_instance_usage(context, instance, 'create.end', fault=e)\n    except exception.ComputeResourcesUnavailable as e:\n        LOG.debug(e.format_message(), instance=instance)\n        self._notify_about_instance_usage(context, instance, 'create.error', fault=e)\n        raise exception.RescheduledException(instance_uuid=instance.uuid, reason=e.format_message())\n    except exception.BuildAbortException as e:\n        with excutils.save_and_reraise_exception():\n            LOG.debug(e.format_message(), instance=instance)\n            self._notify_about_instance_usage(context, instance, 'create.error', fault=e)\n    except (exception.FixedIpLimitExceeded, exception.NoMoreNetworks, exception.NoMoreFixedIps) as e:\n        LOG.warning(_LW('No more network or fixed IP to be allocated'), instance=instance)\n        self._notify_about_instance_usage(context, instance, 'create.error', fault=e)\n        msg = (_('Failed to allocate the network(s) with error %s, not rescheduling.') % e.format_message())\n        raise exception.BuildAbortException(instance_uuid=instance.uuid, reason=msg)\n    except (exception.VirtualInterfaceCreateException, exception.VirtualInterfaceMacAddressException) as e:\n        LOG.exception(_LE('Failed to allocate network(s)'), instance=instance)\n        self._notify_about_instance_usage(context, instance, 'create.error', fault=e)\n        msg = _('Failed to allocate the network(s), not rescheduling.')\n        raise exception.BuildAbortException(instance_uuid=instance.uuid, reason=msg)\n    except (exception.FlavorDiskTooSmall, exception.FlavorMemoryTooSmall, exception.ImageNotActive, exception.ImageUnacceptable, exception.InvalidDiskInfo) as e:\n        self._notify_about_instance_usage(context, instance, 'create.error', fault=e)\n        raise exception.BuildAbortException(instance_uuid=instance.uuid, reason=e.format_message())\n    except Exception as e:\n        self._notify_about_instance_usage(context, instance, 'create.error', fault=e)\n        raise exception.RescheduledException(instance_uuid=instance.uuid, reason=six.text_type(e))\n    instance.system_metadata.pop('network_allocated', None)\n    network_name = CONF.default_access_ip_network_name\n    if (network_name and (not instance.access_ip_v4) and (not instance.access_ip_v6)):\n        for vif in network_info:\n            if (vif['network']['label'] == network_name):\n                for ip in vif.fixed_ips():\n                    if ((not instance.access_ip_v4) and (ip['version'] == 4)):\n                        instance.access_ip_v4 = ip['address']\n                    if ((not instance.access_ip_v6) and (ip['version'] == 6)):\n                        instance.access_ip_v6 = ip['address']\n                break\n    self._update_instance_after_spawn(context, instance)\n    try:\n        instance.save(expected_task_state=task_states.SPAWNING)\n    except (exception.InstanceNotFound, exception.UnexpectedDeletingTaskStateError) as e:\n        with excutils.save_and_reraise_exception():\n            self._notify_about_instance_usage(context, instance, 'create.end', fault=e)\n    self._update_scheduler_instance_info(context, instance)\n    self._notify_about_instance_usage(context, instance, 'create.end', extra_usage_info={\n        'message': _('Success'),\n    }, network_info=network_info)\n", "label": 1}
{"function": "\n\ndef tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0), scale_rows_to_unit_interval=True, output_pixel_vals=True):\n    '\\n    Transform an array with one flattened image per row, into an array in\\n    which images are reshaped and layed out like tiles on a floor.\\n\\n    This function is useful for visualizing datasets whose rows are images,\\n    and also columns of matrices for transforming those rows\\n    (such as the first layer of a neural net).\\n\\n    Parameters\\n    ----------\\n    x : numpy.ndarray\\n        2-d ndarray or 4 tuple of 2-d ndarrays or None for channels,\\n        in which every row is a flattened image.\\n\\n    shape : 2-tuple of ints\\n        The first component is the height of each image,\\n        the second component is the width.\\n\\n    tile_shape : 2-tuple of ints\\n        The number of images to tile in (row, columns) form.\\n\\n    scale_rows_to_unit_interval : bool\\n        Whether or not the values need to be before being plotted to [0, 1].\\n\\n    output_pixel_vals : bool\\n        Whether or not the output should be pixel values (int8) or floats.\\n\\n    Returns\\n    -------\\n    y : 2d-ndarray\\n        The return value has the same dtype as X, and is suitable for\\n        viewing as an image with PIL.Image.fromarray.\\n    '\n    assert (len(img_shape) == 2)\n    assert (len(tile_shape) == 2)\n    assert (len(tile_spacing) == 2)\n    out_shape = [(((ishp + tsp) * tshp) - tsp) for (ishp, tshp, tsp) in zip(img_shape, tile_shape, tile_spacing)]\n    if isinstance(X, tuple):\n        assert (len(X) == 4)\n        if output_pixel_vals:\n            out_array = np.zeros((out_shape[0], out_shape[1], 4), dtype='uint8')\n        else:\n            out_array = np.zeros((out_shape[0], out_shape[1], 4), dtype=X.dtype)\n        if output_pixel_vals:\n            channel_defaults = [0, 0, 0, 255]\n        else:\n            channel_defaults = [0.0, 0.0, 0.0, 1.0]\n        for i in xrange(4):\n            if (X[i] is None):\n                dt = out_array.dtype\n                if output_pixel_vals:\n                    dt = 'uint8'\n                out_array[:, :, i] = (np.zeros(out_shape, dtype=dt) + channel_defaults[i])\n            else:\n                out_array[:, :, i] = tile_raster_images(X[i], img_shape, tile_shape, tile_spacing, scale_rows_to_unit_interval, output_pixel_vals)\n        return out_array\n    else:\n        (H, W) = img_shape\n        (Hs, Ws) = tile_spacing\n        dt = X.dtype\n        if output_pixel_vals:\n            dt = 'uint8'\n        out_array = np.zeros(out_shape, dtype=dt)\n        for tile_row in xrange(tile_shape[0]):\n            for tile_col in xrange(tile_shape[1]):\n                if (((tile_row * tile_shape[1]) + tile_col) < X.shape[0]):\n                    this_x = X[((tile_row * tile_shape[1]) + tile_col)]\n                    if scale_rows_to_unit_interval:\n                        this_img = scale_to_unit_interval(this_x.reshape(img_shape))\n                    else:\n                        this_img = this_x.reshape(img_shape)\n                    c = 1\n                    if output_pixel_vals:\n                        c = 255\n                    out_array[(tile_row * (H + Hs)):((tile_row * (H + Hs)) + H), (tile_col * (W + Ws)):((tile_col * (W + Ws)) + W)] = (this_img * c)\n        return out_array\n", "label": 1}
{"function": "\n\ndef get_fd_info():\n    \"\\n    Gathers and correlates fd info from 3 sources:\\n\\n    1. gc over all objects that have a fileno(); this is most 'socket-like' things\\n    2. infra Context object data structures\\n    3. psutil information\\n    4. /proc filesystem (if available)\\n\\n    This function is a little bit open-ended, we can probably continue\\n    to find additional sources of information\\n    \"\n    import psutil\n    fd_info = collections.defaultdict((lambda : {\n        'gc_objs': [],\n        '/proc': {\n            \n        },\n        'context': [],\n        'psutil': {\n            \n        },\n    }))\n    for obj in gc.get_objects():\n        try:\n            fd = None\n            if (hasattr(obj, 'fileno') and callable(obj.fileno)):\n                fd = obj.fileno()\n        except:\n            pass\n        if isinstance(fd, (int, long)):\n            fd_info[fd]['gc_objs'].append(obj)\n    ctx = context.get_context()\n    socks = []\n    if ctx.connection_mgr:\n        for model in ctx.connection_mgr.server_models.values():\n            socks.extend(model.active_connections)\n    if ctx.client_sockets:\n        socks.extend(ctx.client_sockets)\n    if ctx.server_group:\n        socks.extend(ctx.server_group.socks.values())\n    for sock in socks:\n        fd_info[sock.fileno()]['context'].append(sock)\n    process = psutil.Process()\n    for f in process.get_open_files():\n        if (f.fd == (- 1)):\n            continue\n        fd_info[f.fd]['psutil']['path'] = f.path\n    for conn in process.get_connections(kind='all'):\n        if (conn.fd == (- 1)):\n            continue\n        fd_info[conn.fd]['psutil'].update(vars(conn))\n        del fd_info[conn.fd]['psutil']['fd']\n    proc_path = ('/proc/' + str(os.getpid()))\n    if (not os.path.exists(proc_path)):\n        return dict(fd_info)\n    for fd in os.listdir((proc_path + '/fd')):\n        try:\n            inode = os.readlink(((proc_path + '/fd/') + fd))\n        except OSError:\n            continue\n        else:\n            fd = int(fd)\n            fd_info[fd]['/proc']['inode'] = inode\n    return dict(fd_info)\n", "label": 1}
{"function": "\n\n@login_required()\ndef submit_mc_exam_answer(request, course_id, exam_id):\n    response_data = {\n        'status': 'failed',\n        'message': 'error submitting',\n    }\n    if request.is_ajax():\n        if (request.method == 'POST'):\n            question_id = int(request.POST['question_id'])\n            answer = request.POST['answer']\n            course = Course.objects.get(id=course_id)\n            exam = Exam.objects.get(exam_id=exam_id)\n            student = Student.objects.get(user=request.user)\n            try:\n                question = MultipleChoiceQuestion.objects.get(exam=exam, question_id=question_id)\n            except MultipleChoiceQuestion.DoesNotExist:\n                response_data = {\n                    'status': 'failed',\n                    'message': 'cannot find question',\n                }\n                return HttpResponse(json.dumps(response_data), content_type='application/json')\n            try:\n                submission = MultipleChoiceSubmission.objects.get(student=student, question=question)\n            except MultipleChoiceSubmission.DoesNotExist:\n                submission = MultipleChoiceSubmission.objects.create(student=student, question=question)\n            if (answer == 'A'):\n                submission.a = (not submission.a)\n            if (answer == 'B'):\n                submission.b = (not submission.b)\n            if (answer == 'C'):\n                submission.c = (not submission.c)\n            if (answer == 'D'):\n                submission.d = (not submission.d)\n            if (answer == 'E'):\n                submission.e = (not submission.e)\n            if (answer == 'F'):\n                submission.f = (not submission.f)\n            submission.save()\n            total = 6\n            correct = 0\n            if (submission.a == submission.question.a_is_correct):\n                correct += 1\n            if (submission.b == submission.question.b_is_correct):\n                correct += 1\n            if (submission.c == submission.question.c_is_correct):\n                correct += 1\n            if (submission.d == submission.question.d_is_correct):\n                correct += 1\n            if (submission.e == submission.question.e_is_correct):\n                correct += 1\n            if (submission.f == submission.question.f_is_correct):\n                correct += 1\n            if (total == correct):\n                submission.marks = submission.question.marks\n            else:\n                submission.marks = 0\n            submission.save()\n            response_data = {\n                'status': 'success',\n                'message': 'submitted',\n            }\n    return HttpResponse(json.dumps(response_data), content_type='application/json')\n", "label": 1}
{"function": "\n\ndef _got_sol_payload(self, payload):\n    'SOL payload callback\\n        '\n    if (type(payload) == dict):\n        self.activated = False\n        self._print_error(payload)\n        return\n    newseq = (payload[0] & 15)\n    ackseq = (payload[1] & 15)\n    ackcount = payload[2]\n    nacked = (payload[3] & 64)\n    poweredoff = (payload[3] & 32)\n    deactivated = (payload[3] & 16)\n    breakdetected = (payload[3] & 4)\n    remdata = ''\n    remdatalen = 0\n    if (newseq != 0):\n        if (len(payload) > 4):\n            remdatalen = len(payload[4:])\n            remdata = struct.pack(('%dB' % remdatalen), *payload[4:])\n        if (newseq == self.remseq):\n            if (remdatalen > self.lastsize):\n                remdata = remdata[(4 + self.lastsize):]\n            else:\n                remdata = ''\n        else:\n            self.remseq = newseq\n        self.lastsize = remdatalen\n        if remdata:\n            self._print_data(remdata)\n        ackpayload = (0, self.remseq, remdatalen, 0)\n        try:\n            self.send_payload(ackpayload, retry=False)\n        except exc.IpmiException:\n            self.close()\n    if ((self.myseq != 0) and (ackseq == self.myseq)):\n        self.awaitingack = False\n        if (nacked and (not breakdetected)):\n            if poweredoff:\n                self._print_info('Remote system is powered down')\n            if deactivated:\n                self.activated = False\n                self._print_error('Remote IPMI console disconnected')\n            else:\n                newtext = self.lastpayload[(4 + ackcount):]\n                newtext = struct.pack(('B' * len(newtext)), *newtext)\n                if (self.pendingoutput and (not isinstance(self.pendingoutput[0], dict))):\n                    self.pendingoutput[0] = (newtext + self.pendingoutput[0])\n                else:\n                    self.pendingoutput = ([newtext] + self.pendingoutput)\n                self._sendpendingoutput()\n        if (len(self.pendingoutput) > 0):\n            self._sendpendingoutput()\n    elif ((ackseq != 0) and self.awaitingack):\n        self.send_payload(payload=self.lastpayload)\n", "label": 1}
{"function": "\n\n@expose(hide=True)\ndef debug_nginx(self):\n    'Start/Stop Nginx debug'\n    if ((self.app.pargs.nginx == 'on') and (not self.app.pargs.site_name)):\n        try:\n            debug_address = self.app.config.get('stack', 'ip-address').split()\n        except Exception as e:\n            debug_address = ['0.0.0.0/0']\n        if ((debug_address == ['127.0.0.1']) or (debug_address == [])):\n            debug_address = ['0.0.0.0/0']\n        for ip_addr in debug_address:\n            if (not (('debug_connection ' + ip_addr) in open('/etc/nginx/nginx.conf', encoding='utf-8').read())):\n                Log.info(self, ('Setting up Nginx debug connection for ' + ip_addr))\n                EEShellExec.cmd_exec(self, 'sed -i \"/events {{/a\\\\ \\\\ \\\\ \\\\ $(echo debug_connection {ip}\\\\;)\" /etc/nginx/nginx.conf'.format(ip=ip_addr))\n                self.trigger_nginx = True\n        if (not self.trigger_nginx):\n            Log.info(self, 'Nginx debug connection already enabled')\n        self.msg = (self.msg + ['/var/log/nginx/*.error.log'])\n    elif ((self.app.pargs.nginx == 'off') and (not self.app.pargs.site_name)):\n        if ('debug_connection ' in open('/etc/nginx/nginx.conf', encoding='utf-8').read()):\n            Log.info(self, 'Disabling Nginx debug connections')\n            EEShellExec.cmd_exec(self, 'sed -i \"/debug_connection.*/d\" /etc/nginx/nginx.conf')\n            self.trigger_nginx = True\n        else:\n            Log.info(self, 'Nginx debug connection already disabled')\n    elif ((self.app.pargs.nginx == 'on') and self.app.pargs.site_name):\n        config_path = '/etc/nginx/sites-available/{0}'.format(self.app.pargs.site_name)\n        if os.path.isfile(config_path):\n            if (not EEShellExec.cmd_exec(self, 'grep \"error.log debug\" {0}'.format(config_path))):\n                Log.info(self, 'Starting NGINX debug connection for {0}'.format(self.app.pargs.site_name))\n                EEShellExec.cmd_exec(self, 'sed -i \"s/error.log;/error.log debug;/\" {0}'.format(config_path))\n                self.trigger_nginx = True\n            else:\n                Log.info(self, 'Nginx debug for site already enabled')\n            self.msg = (self.msg + ['{0}{1}/logs/error.log'.format(EEVariables.ee_webroot, self.app.pargs.site_name)])\n        else:\n            Log.info(self, '{0} domain not valid'.format(self.app.pargs.site_name))\n    elif ((self.app.pargs.nginx == 'off') and self.app.pargs.site_name):\n        config_path = '/etc/nginx/sites-available/{0}'.format(self.app.pargs.site_name)\n        if os.path.isfile(config_path):\n            if EEShellExec.cmd_exec(self, 'grep \"error.log debug\" {0}'.format(config_path)):\n                Log.info(self, 'Stoping NGINX debug connection for {0}'.format(self.app.pargs.site_name))\n                EEShellExec.cmd_exec(self, 'sed -i \"s/error.log debug;/error.log;/\" {0}'.format(config_path))\n                self.trigger_nginx = True\n            else:\n                Log.info(self, 'Nginx debug for site already disabled')\n        else:\n            Log.info(self, '{0} domain not valid'.format(self.app.pargs.site_name))\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef handle_new_client_event(self, requester, event, context, ratelimit=True, extra_users=[]):\n    if ratelimit:\n        self.ratelimit(requester)\n    self.auth.check(event, auth_events=context.current_state)\n    (yield self.maybe_kick_guest_users(event, context.current_state.values()))\n    if (event.type == EventTypes.CanonicalAlias):\n        room_alias_str = event.content.get('alias', None)\n        if room_alias_str:\n            room_alias = RoomAlias.from_string(room_alias_str)\n            directory_handler = self.hs.get_handlers().directory_handler\n            mapping = (yield directory_handler.get_association(room_alias))\n            if (mapping['room_id'] != event.room_id):\n                raise SynapseError(400, ('Room alias %s does not point to the room' % (room_alias_str,)))\n    federation_handler = self.hs.get_handlers().federation_handler\n    if (event.type == EventTypes.Member):\n        if (event.content['membership'] == Membership.INVITE):\n\n            def is_inviter_member_event(e):\n                return ((e.type == EventTypes.Member) and (e.sender == event.sender))\n            event.unsigned['invite_room_state'] = [{\n                'type': e.type,\n                'state_key': e.state_key,\n                'content': e.content,\n                'sender': e.sender,\n            } for (k, e) in context.current_state.items() if ((e.type in self.hs.config.room_invite_state_types) or is_inviter_member_event(e))]\n            invitee = UserID.from_string(event.state_key)\n            if (not self.hs.is_mine(invitee)):\n                returned_invite = (yield federation_handler.send_invite(invitee.domain, event))\n                event.unsigned.pop('room_state', None)\n                event.signatures.update(returned_invite.signatures)\n    if (event.type == EventTypes.Redaction):\n        if self.auth.check_redaction(event, auth_events=context.current_state):\n            original_event = (yield self.store.get_event(event.redacts, check_redacted=False, get_prev_content=False, allow_rejected=False, allow_none=False))\n            if (event.user_id != original_event.user_id):\n                raise AuthError(403, \"You don't have permission to redact events\")\n    if ((event.type == EventTypes.Create) and context.current_state):\n        raise AuthError(403, 'Changing the room create event is forbidden')\n    action_generator = ActionGenerator(self.hs)\n    (yield action_generator.handle_push_actions_for_event(event, context, self))\n    (event_stream_id, max_stream_id) = (yield self.store.persist_event(event, context=context))\n    destinations = set()\n    for (k, s) in context.current_state.items():\n        try:\n            if (k[0] == EventTypes.Member):\n                if (s.content['membership'] == Membership.JOIN):\n                    destinations.add(UserID.from_string(s.state_key).domain)\n        except SynapseError:\n            logger.warn('Failed to get destination from event %s', s.event_id)\n    with PreserveLoggingContext():\n        self.notifier.on_new_room_event(event, event_stream_id, max_stream_id, extra_users=extra_users)\n    event.unsigned.pop('invite_room_state', None)\n    federation_handler.handle_new_event(event, destinations=destinations)\n", "label": 1}
{"function": "\n\ndef createSTP(self, stp_filename, parameters):\n    '\\n        Creates an STP file to find a linear characteristic for SIMON with\\n        the given parameters.\\n        '\n    wordsize = parameters['wordsize']\n    rounds = parameters['rounds']\n    weight = parameters['sweight']\n    if ('rotationconstants' in parameters):\n        self.rot_alpha = parameters['rotationconstants'][0]\n        self.rot_beta = parameters['rotationconstants'][1]\n        self.rot_gamma = parameters['rotationconstants'][2]\n    with open(stp_filename, 'w') as stp_file:\n        header = '% Input File for STP\\n% Simon w={} alpha={} beta={} gamma={} rounds={}\\n\\n\\n'.format(wordsize, self.rot_alpha, self.rot_beta, self.rot_gamma, rounds)\n        stp_file.write(header)\n        x = ['x{}'.format(i) for i in range((rounds + 1))]\n        y = ['y{}'.format(i) for i in range((rounds + 1))]\n        b = ['b{}'.format(i) for i in range((rounds + 1))]\n        c = ['c{}'.format(i) for i in range((rounds + 1))]\n        and_out = ['andout{}'.format(i) for i in range((rounds + 1))]\n        abits = ['abits{}'.format(i) for i in range((rounds + 1))]\n        tmpWeight = ['tmp{}r{}'.format(j, i) for i in range(rounds) for j in range(wordsize)]\n        sbits = ['sbits{}r{}'.format(j, i) for i in range(rounds) for j in range(wordsize)]\n        pbits = ['pbits{}r{}'.format(j, i) for i in range(rounds) for j in range(wordsize)]\n        w = ['w{}'.format(i) for i in range(rounds)]\n        stpcommands.setupVariables(stp_file, x, wordsize)\n        stpcommands.setupVariables(stp_file, y, wordsize)\n        stpcommands.setupVariables(stp_file, and_out, wordsize)\n        stpcommands.setupVariables(stp_file, b, wordsize)\n        stpcommands.setupVariables(stp_file, c, wordsize)\n        stpcommands.setupVariables(stp_file, abits, wordsize)\n        stpcommands.setupVariables(stp_file, w, wordsize)\n        stpcommands.setupVariables(stp_file, tmpWeight, wordsize)\n        stpcommands.setupVariables(stp_file, sbits, wordsize)\n        stpcommands.setupVariables(stp_file, pbits, wordsize)\n        stpcommands.setupWeightComputation(stp_file, weight, w, wordsize)\n        for i in range(rounds):\n            indicesFrom = (i * wordsize)\n            indicesTo = ((i + 1) * wordsize)\n            self.setupSimonRound(stp_file, x[i], y[i], x[(i + 1)], y[(i + 1)], and_out[i], b[i], c[i], abits[i], w[i], tmpWeight[indicesFrom:indicesTo], sbits[indicesFrom:indicesTo], pbits[indicesFrom:indicesTo], wordsize)\n        stpcommands.assertNonZero(stp_file, (x + y), wordsize)\n        if parameters['iterative']:\n            stpcommands.assertVariableValue(stp_file, x[0], x[rounds])\n            stpcommands.assertVariableValue(stp_file, y[0], y[rounds])\n        for (key, value) in parameters['fixedVariables'].items():\n            stpcommands.assertVariableValue(stp_file, key, value)\n        for char in parameters['blockedCharacteristics']:\n            stpcommands.blockCharacteristic(stp_file, char, wordsize)\n        stpcommands.setupQuery(stp_file)\n    return\n", "label": 1}
{"function": "\n\ndef test_write_session1():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'write': ['test', 'test1'],\n        }, key='test')\n        key = Key.load(h.get_key('test'))\n        assert (key.api_key == 'test')\n        assert (key.is_admin() == False)\n        assert (key.can_create_key() == False)\n        assert (key.can_create_user() == False)\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == True)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == True)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef Parse(self, statentry, file_object, knowledge_base):\n    'Parse the Plist file.'\n    _ = knowledge_base\n    kwargs = {\n        \n    }\n    kwargs['aff4path'] = statentry.aff4path\n    direct_copy_items = ['Label', 'Disabled', 'UserName', 'GroupName', 'Program', 'StandardInPath', 'StandardOutPath', 'StandardErrorPath', 'LimitLoadToSessionType', 'EnableGlobbing', 'EnableTransactions', 'OnDemand', 'RunAtLoad', 'RootDirectory', 'WorkingDirectory', 'Umask', 'TimeOut', 'ExitTimeOut', 'ThrottleInterval', 'InitGroups', 'StartOnMount', 'StartInterval', 'Debug', 'WaitForDebugger', 'Nice', 'ProcessType', 'AbandonProcessGroup', 'LowPriorityIO', 'LaunchOnlyOnce']\n    string_array_items = ['LimitLoadToHosts', 'LimitLoadFromHosts', 'LimitLoadToSessionType', 'ProgramArguments', 'WatchPaths', 'QueueDirectories']\n    flag_only_items = ['SoftResourceLimits', 'HardResourceLimits', 'Sockets']\n    plist = {\n        \n    }\n    try:\n        plist = binplist.readPlist(file_object)\n    except (binplist.FormatError, ValueError, IOError) as e:\n        plist['Label'] = ('Could not parse plist: %s' % e)\n    for key in direct_copy_items:\n        kwargs[key] = plist.get(key)\n    for key in string_array_items:\n        elements = plist.get(key)\n        if isinstance(elements, basestring):\n            kwargs[key] = [elements]\n        else:\n            kwargs[key] = elements\n    for key in flag_only_items:\n        if plist.get(key):\n            kwargs[key] = True\n    if (plist.get('inetdCompatability') is not None):\n        kwargs['inetdCompatabilityWait'] = plist.get('inetdCompatability').get('Wait')\n    keepalive = plist.get('KeepAlive')\n    if (isinstance(keepalive, bool) or (keepalive is None)):\n        kwargs['KeepAlive'] = keepalive\n    else:\n        keepalivedict = {\n            \n        }\n        keepalivedict['SuccessfulExit'] = keepalive.get('SuccessfulExit')\n        keepalivedict['NetworkState'] = keepalive.get('NetworkState')\n        pathstates = keepalive.get('PathState')\n        if (pathstates is not None):\n            keepalivedict['PathState'] = []\n            for pathstate in pathstates:\n                keepalivedict['PathState'].append(rdf_plist.PlistBoolDictEntry(name=pathstate, value=pathstates[pathstate]))\n        otherjobs = keepalive.get('OtherJobEnabled')\n        if (otherjobs is not None):\n            keepalivedict['OtherJobEnabled'] = []\n            for otherjob in otherjobs:\n                keepalivedict['OtherJobEnabled'].append(rdf_plist.PlistBoolDictEntry(name=otherjob, value=otherjobs[otherjob]))\n        kwargs['KeepAliveDict'] = rdf_plist.LaunchdKeepAlive(**keepalivedict)\n    envvars = plist.get('EnvironmentVariables')\n    if (envvars is not None):\n        kwargs['EnvironmentVariables'] = []\n        for envvar in envvars:\n            kwargs['EnvironmentVariables'].append(rdf_plist.PlistStringDictEntry(name=envvar, value=envvars[envvar]))\n    startcalendarinterval = plist.get('StartCalendarInterval')\n    if (startcalendarinterval is not None):\n        if isinstance(startcalendarinterval, dict):\n            kwargs['StartCalendarInterval'] = [rdf_plist.LaunchdStartCalendarIntervalEntry(Minute=startcalendarinterval.get('Minute'), Hour=startcalendarinterval.get('Hour'), Day=startcalendarinterval.get('Day'), Weekday=startcalendarinterval.get('Weekday'), Month=startcalendarinterval.get('Month'))]\n        else:\n            kwargs['StartCalendarInterval'] = []\n            for entry in startcalendarinterval:\n                kwargs['StartCalendarInterval'].append(rdf_plist.LaunchdStartCalendarIntervalEntry(Minute=entry.get('Minute'), Hour=entry.get('Hour'), Day=entry.get('Day'), Weekday=entry.get('Weekday'), Month=entry.get('Month')))\n    (yield rdf_plist.LaunchdPlist(**kwargs))\n", "label": 1}
{"function": "\n\ndef main():\n    steps_modules = []\n    files = []\n    before_all_methods = []\n    before_each_methods = []\n    after_all_methods = []\n    after_each_methods = []\n    stories_dirname = 'stories'\n    for arg in sys.argv[1:]:\n        if arg.startswith('-'):\n            break\n        files.append(arg)\n        stories_dirname = (os.path.dirname(arg) or '.')\n    parser = OptionParser()\n    parser.add_option('-s', '--stories-dir', default=None, dest='stories_dir')\n    parser.add_option('-t', '--steps-dir', default=None, dest='steps_dir')\n    parser.add_option('-n', '--no-colors', default=None, action='store_true', dest='no_colors')\n    parser.add_option('-c', '--colored', default=None, action='store_true', dest='colored')\n    parser.add_option('-l', '--language', default='en-us', dest='language')\n    (values, args) = parser.parse_args()\n    try:\n        if values.stories_dir:\n            files.extend([((values.stories_dir + '/') + filename) for filename in os.listdir(values.stories_dir) if filename.endswith('.story')])\n            stories_dirname = values.stories_dir\n        elif (files == []):\n            files.extend([((stories_dirname + '/') + filename) for filename in os.listdir(stories_dirname) if filename.endswith('.story')])\n        steps_modules = find_steps_modules((values.steps_dir or (stories_dirname + '/step_definitions')))\n    except OSError:\n        pass\n    if os.path.exists((stories_dirname + '/support')):\n        before_all_methods = find_before_all((stories_dirname + '/support'))\n        after_all_methods = find_after_all((stories_dirname + '/support'))\n        before_each_methods = find_before_each((stories_dirname + '/support'))\n        after_each_methods = find_after_each((stories_dirname + '/support'))\n    colored = True\n    if (values.no_colors and (not values.colored)):\n        colored = False\n    exit_code = True\n    for (index, story) in enumerate(files):\n        story_status = StoryRunner(open(story).read(), sys.stdout, colored=colored, modules=steps_modules, language=values.language, before_all=before_all_methods, before_each=before_each_methods, after_all=after_all_methods, after_each=after_each_methods).run()\n        exit_code = (exit_code and story_status)\n        if (index < (len(files) - 1)):\n            sys.stdout.write('\\n\\n')\n    exit(int((not exit_code)))\n", "label": 1}
{"function": "\n\ndef test_Term():\n    a = Term(((((4 * x) * (y ** 2)) / z) / (t ** 3)))\n    b = Term((((2 * (x ** 3)) * (y ** 5)) / (t ** 3)))\n    assert (a == Term(4, Factors({\n        x: 1,\n        y: 2,\n    }), Factors({\n        z: 1,\n        t: 3,\n    })))\n    assert (b == Term(2, Factors({\n        x: 3,\n        y: 5,\n    }), Factors({\n        t: 3,\n    })))\n    assert (a.as_expr() == ((((4 * x) * (y ** 2)) / z) / (t ** 3)))\n    assert (b.as_expr() == (((2 * (x ** 3)) * (y ** 5)) / (t ** 3)))\n    assert (a.inv() == Term((S(1) / 4), Factors({\n        z: 1,\n        t: 3,\n    }), Factors({\n        x: 1,\n        y: 2,\n    })))\n    assert (b.inv() == Term((S(1) / 2), Factors({\n        t: 3,\n    }), Factors({\n        x: 3,\n        y: 5,\n    })))\n    assert (a.mul(b) == (a * b) == Term(8, Factors({\n        x: 4,\n        y: 7,\n    }), Factors({\n        z: 1,\n        t: 6,\n    })))\n    assert (a.quo(b) == (a / b) == Term(2, Factors({\n        \n    }), Factors({\n        x: 2,\n        y: 3,\n        z: 1,\n    })))\n    assert (a.pow(3) == (a ** 3) == Term(64, Factors({\n        x: 3,\n        y: 6,\n    }), Factors({\n        z: 3,\n        t: 9,\n    })))\n    assert (b.pow(3) == (b ** 3) == Term(8, Factors({\n        x: 9,\n        y: 15,\n    }), Factors({\n        t: 9,\n    })))\n    assert (a.pow((- 3)) == (a ** (- 3)) == Term((S(1) / 64), Factors({\n        z: 3,\n        t: 9,\n    }), Factors({\n        x: 3,\n        y: 6,\n    })))\n    assert (b.pow((- 3)) == (b ** (- 3)) == Term((S(1) / 8), Factors({\n        t: 9,\n    }), Factors({\n        x: 9,\n        y: 15,\n    })))\n    assert (a.gcd(b) == Term(2, Factors({\n        x: 1,\n        y: 2,\n    }), Factors({\n        t: 3,\n    })))\n    assert (a.lcm(b) == Term(4, Factors({\n        x: 3,\n        y: 5,\n    }), Factors({\n        z: 1,\n        t: 3,\n    })))\n    a = Term(((((4 * x) * (y ** 2)) / z) / (t ** 3)))\n    b = Term((((2 * (x ** 3)) * (y ** 5)) * (t ** 7)))\n    assert (a.mul(b) == Term(8, Factors({\n        x: 4,\n        y: 7,\n        t: 4,\n    }), Factors({\n        z: 1,\n    })))\n    assert (Term((((2 * x) + 2) ** 3)) == Term(8, Factors({\n        (x + 1): 3,\n    }), Factors({\n        \n    })))\n    assert (Term((((2 * x) + 2) * (((3 * x) + 6) ** 2))) == Term(18, Factors({\n        (x + 1): 1,\n        (x + 2): 2,\n    }), Factors({\n        \n    })))\n", "label": 1}
{"function": "\n\ndef execute(self, command, verbose=False, color=None, output_lines=None, output_file=None, quiet=False, exec_options=None):\n    exec_options = (exec_options or {\n        \n    })\n    if (output_file is not None):\n        stdout_file = output_file\n        stderr_file = output_file\n    elif (not quiet):\n        stdout_file = sys.stdout\n        stderr_file = sys.stderr\n    else:\n        stdout_file = None\n        stderr_file = None\n    result = None\n    output_chunks = []\n    color = self.get_color(color, out_file=stdout_file)\n    self.tag_line(color('BEGIN', 'header'), command, verbose=verbose, color=color, out_file=stdout_file)\n    start = time.time()\n    try:\n        while True:\n            for (code, output) in self.execute_command(command, **exec_options):\n                if (code == STDOUT):\n                    if (output_lines is not None):\n                        output_chunks.append(output)\n                    elif stdout_file:\n                        if ((not verbose) or stdout_file):\n                            stdout_file.write(output)\n                        else:\n                            for line in output.splitlines(True):\n                                stdout_file.write(('[%s] %s' % (color(self.node.name, 'node'), line)))\n                        stdout_file.flush()\n                elif (code == STDERR):\n                    if ((not verbose) or stdout_file):\n                        stderr_file.write(output)\n                    elif stderr_file:\n                        for line in output.splitlines(True):\n                            stderr_file.write(('{%s} %s' % (color(self.node.name, 'node'), line)))\n                    stderr_file.flush()\n                else:\n                    if (output_lines is not None):\n                        output_lines.extend(''.join(output_chunks).splitlines())\n                    if (not output):\n                        result = color('OK', 'op_ok')\n                    else:\n                        result = color(output, 'op_error')\n                    return output\n    except Exception as error:\n        result = color(('%s: %s' % (error.__class__.__name__, error)), 'op_error')\n        raise\n    finally:\n        elapsed = (time.time() - start)\n        self.tag_line(color(('END %.1fs' % elapsed), 'header'), command, result=result, verbose=verbose, color=color, out_file=stdout_file)\n", "label": 1}
{"function": "\n\ndef run(self, args=[], create_fhs=[], attach_fhs={\n    \n}):\n    if self.handles:\n        raise RuntimeError('Already running')\n    standard_handles = {\n        'stdin': (sys.stdin, 'wb'),\n        'stdout': (sys.stdout, 'rb'),\n        'stderr': (sys.stderr, 'rb'),\n    }\n    gpg_handles = {\n        'passphrase': 'wb',\n    }\n    self.handles = {\n        \n    }\n    self.parent_fds = []\n    child_fds = []\n    fd_args = []\n    std_fds = {\n        \n    }\n    for (name, value) in standard_handles.items():\n        if ((name not in create_fhs) and (name not in attach_fhs)):\n            (fobj, mode) = value\n            attach_fhs[name] = fobj\n    for name in create_fhs:\n        if (name in standard_handles):\n            (_, mode) = standard_handles[name]\n        elif (name in gpg_handles):\n            mode = gpg_handles[name]\n        else:\n            close_fds(self.parent_fds, child_fds)\n            raise ValueError(('Invalid handle: %s' % name))\n        (pipe_r, pipe_w) = os.pipe()\n        if (mode == 'wb'):\n            for_parent = pipe_w\n            for_child = pipe_r\n        elif (mode == 'rb'):\n            for_parent = pipe_r\n            for_child = pipe_w\n        else:\n            assert 0\n        if (name in standard_handles):\n            std_fds[name] = for_child\n        else:\n            fd_args += [('--%s-fd' % name), str(for_child)]\n        self.handles[name] = os.fdopen(for_parent, mode)\n        self.parent_fds.append(for_parent)\n        child_fds.append(for_child)\n    for (name, fobj) in attach_fhs.items():\n        if (name in standard_handles):\n            (_, mode) = standard_handles[name]\n            target_fd = fobj.fileno()\n        elif (name in gpg_handles):\n            mode = gpg_handles[name]\n            target_fd = None\n        else:\n            close_fds(self.parent_fds, child_fds)\n            raise ValueError(('Invalid handle %s' % name))\n        if (hasattr(fobj, 'fileno') and callable(fobj.fileno)):\n            source_fd = fobj.fileno()\n            self.handles[name] = fobj\n        elif isinstance(fobj, int):\n            source_fd = fobj\n            self.handles[name] = os.fdopen(source_fd, mode)\n        else:\n            close_fds(self.parent_fds, child_fds)\n            raise ValueError(('No file descriptor to attach for %s' % name))\n        if (name in standard_handles):\n            std_fds[name] = target_fd\n        else:\n            fd_args += [('--%s-fd' % name), str(source_fd)]\n\n    def preexec():\n        close_fds(self.parent_fds)\n    cmdline = ((((['gpg'] + fd_args) + self.default_args) + self.args) + args)\n    self.p = subprocess.Popen(cmdline, preexec_fn=preexec, close_fds=False, **std_fds)\n    close_fds(child_fds)\n", "label": 1}
{"function": "\n\ndef _newton_rhaphson(self, X, T, E, initial_beta=None, step_size=1.0, precision=0.0001, show_progress=True, include_likelihood=False):\n    '\\n        Newton Rhaphson algorithm for fitting CPH model.\\n\\n        Note that data is assumed to be sorted on T!\\n\\n        Parameters:\\n            X: (n,d) Pandas DataFrame of observations.\\n            T: (n) Pandas Series representing observed durations.\\n            E: (n) Pandas Series representing death events.\\n            initial_beta: (1,d) numpy array of initial starting point for\\n                          NR algorithm. Default 0.\\n            step_size: float > 0.001 to determine a starting step size in NR algorithm.\\n            precision: the convergence halts if the norm of delta between\\n                     successive positions is less than epsilon.\\n            include_likelihood: saves the final log-likelihood to the CoxPHFitter under _log_likelihood.\\n\\n        Returns:\\n            beta: (1,d) numpy array.\\n        '\n    assert (precision <= 1.0), 'precision must be less than or equal to 1.'\n    (n, d) = X.shape\n    E = E.astype(bool)\n    if (initial_beta is not None):\n        assert (initial_beta.shape == (d, 1))\n        beta = initial_beta\n    else:\n        beta = np.zeros((d, 1))\n    if (self.tie_method == 'Efron'):\n        get_gradients = self._get_efron_values\n    else:\n        raise NotImplementedError('Only Efron is available.')\n    i = 1\n    converging = True\n    while (converging and (i < 50) and (step_size > 0.001)):\n        if (self.strata is None):\n            output = get_gradients(X.values, beta, T.values, E.values, include_likelihood=include_likelihood)\n            (h, g) = output[:2]\n        else:\n            g = np.zeros_like(beta).T\n            h = np.zeros((beta.shape[0], beta.shape[0]))\n            ll = 0\n            for strata in np.unique(X.index):\n                (stratified_X, stratified_T, stratified_E) = (X.loc[[strata]], T.loc[[strata]], E.loc[[strata]])\n                output = get_gradients(stratified_X.values, beta, stratified_T.values, stratified_E.values, include_likelihood=include_likelihood)\n                (_h, _g) = output[:2]\n                g += _g\n                h += _h\n                ll += (output[2] if include_likelihood else 0)\n        if (self.penalizer > 0):\n            g -= (self.penalizer * beta.T)\n            h.flat[::(d + 1)] -= self.penalizer\n        delta = solve((- h), (step_size * g.T))\n        if np.any(np.isnan(delta)):\n            raise ValueError('delta contains nan value(s). Convergence halted.')\n        (hessian, gradient) = (h, g)\n        if (norm(delta) < precision):\n            converging = False\n        if (norm(delta) > 10):\n            step_size *= 0.5\n            continue\n        beta += delta\n        if (((i % 10) == 0) and show_progress):\n            print(('Iteration %d: delta = %.5f' % (i, norm(delta))))\n        i += 1\n    self._hessian_ = hessian\n    self._score_ = gradient\n    if include_likelihood:\n        self._log_likelihood = (output[(- 1)] if (self.strata is None) else ll)\n    if show_progress:\n        print(('Convergence completed after %d iterations.' % i))\n    return beta\n", "label": 1}
{"function": "\n\n@login_required()\ndef submit_mc_assignment_answer(request, course_id, assignment_id):\n    if request.is_ajax():\n        if (request.method == 'POST'):\n            question_id = int(request.POST['question_id'])\n            answer = request.POST['answer']\n            course = Course.objects.get(id=course_id)\n            assignment = Assignment.objects.get(assignment_id=assignment_id)\n            student = Student.objects.get(user=request.user)\n            try:\n                question = MultipleChoiceQuestion.objects.get(assignment=assignment, question_id=question_id)\n            except MultipleChoiceQuestion.DoesNotExist:\n                response_data = {\n                    'status': 'failed',\n                    'message': 'cannot find question',\n                }\n                return HttpResponse(json.dumps(response_data), content_type='application/json')\n            try:\n                submission = MultipleChoiceSubmission.objects.get(student=student, question=question)\n            except MultipleChoiceSubmission.DoesNotExist:\n                submission = MultipleChoiceSubmission.objects.create(student=student, question=question)\n            if (answer == 'A'):\n                submission.a = (not submission.a)\n            if (answer == 'B'):\n                submission.b = (not submission.b)\n            if (answer == 'C'):\n                submission.c = (not submission.c)\n            if (answer == 'D'):\n                submission.d = (not submission.d)\n            if (answer == 'E'):\n                submission.e = (not submission.e)\n            if (answer == 'F'):\n                submission.f = (not submission.f)\n            submission.save()\n            total = 6\n            correct = 0\n            if (submission.a == submission.question.a_is_correct):\n                correct += 1\n            if (submission.b == submission.question.b_is_correct):\n                correct += 1\n            if (submission.c == submission.question.c_is_correct):\n                correct += 1\n            if (submission.d == submission.question.d_is_correct):\n                correct += 1\n            if (submission.e == submission.question.e_is_correct):\n                correct += 1\n            if (submission.f == submission.question.f_is_correct):\n                correct += 1\n            if (total == correct):\n                submission.marks = submission.question.marks\n            else:\n                submission.marks = 0\n            submission.save()\n            response_data = {\n                'status': 'success',\n                'message': 'submitted',\n            }\n            return HttpResponse(json.dumps(response_data), content_type='application/json')\n    response_data = {\n        'status': 'failed',\n        'message': 'error submitting',\n    }\n    return HttpResponse(json.dumps(response_data), content_type='application/json')\n", "label": 1}
{"function": "\n\ndef _get_frame(self, key):\n    if (isinstance(self.hmap, DynamicMap) and self.overlaid and self.current_frame):\n        self.current_key = key\n        return self.current_frame\n    elif self.dynamic:\n        (key, frame) = util.get_dynamic_item(self.hmap, self.dimensions, key)\n        if (not isinstance(key, tuple)):\n            key = (key,)\n        if (not (key in self.keys)):\n            self.keys.append(key)\n        self.current_frame = frame\n        self.current_key = key\n        return frame\n    if isinstance(key, int):\n        key = self.hmap.keys()[min([key, (len(self.hmap) - 1)])]\n    if (key == self.current_key):\n        return self.current_frame\n    else:\n        self.current_key = key\n    if self.uniform:\n        if (not isinstance(key, tuple)):\n            key = (key,)\n        kdims = [d.name for d in self.hmap.kdims]\n        if (self.dimensions is None):\n            dimensions = kdims\n        else:\n            dimensions = [d.name for d in self.dimensions]\n        if ((kdims == ['Frame']) and (kdims != dimensions)):\n            select = dict(Frame=0)\n        else:\n            select = {d: key[dimensions.index(d)] for d in kdims}\n    else:\n        select = dict(zip(self.hmap.dimensions('key', label=True), key))\n    try:\n        selection = self.hmap.select((HoloMap, DynamicMap), **select)\n    except KeyError:\n        selection = None\n    selection = (selection.last if isinstance(selection, HoloMap) else selection)\n    self.current_frame = selection\n    return selection\n", "label": 1}
{"function": "\n\n@glacier_connect\n@sdb_connect\n@log_class_call('Requesting inventory overview.', 'Inventory response received.')\ndef inventory(self, vault_name, refresh):\n    \"\\n        Retrieves inventory and returns retrieval job, or if it's already retrieved\\n        returns overview of the inventoy. If force=True it will force start a new\\n        inventory taking job.\\n\\n        :param vault_name: Vault name\\n        :type vault_name: str\\n        :param refresh: Force new inventory retrieval.\\n        :type refresh: boolean\\n\\n        :returns: Tuple of retrieval job and inventory data (as list) if available.\\n\\n            .. code-block:: python\\n\\n                ({u'CompletionDate': None,\\n                  u'VaultARN':\\n                  u'arn:aws:glacier:us-east-1:012345678901:vaults/your_vault_name',\\n                  u'SNSTopic': None,\\n                  u'SHA256TreeHash': None,\\n                  u'Completed': False,\\n                  u'InventorySizeInBytes': None,\\n                  u'JobId': u'Example_d3tgAbCuQ9vPRqRJkD2vjNQ6wBgga7Xaw9ifwACvhjhIeKtNnZqeSIuMYRo3JUKsK_0M-VNYvb0-_Example',\\n                  u'ArchiveId': None,\\n                  u'JobDescription': None,\\n                  u'StatusCode': u'InProgress',\\n                  u'Action': u'InventoryRetrieval',\\n                  u'CreationDate': u'2012-10-01T14:54:51.919Z',\\n                  u'StatusMessage': None,\\n                  u'ArchiveSizeInBytes': None},\\n                  None\\n                )\\n        :rtype: (list, list)\\n\\n        :raises: :py:exc:`glacier.glacierexception.CommunicationException`,\\n                 :py:exc:`glacier.glacierexception.ResponseException`\\n        \"\n    self._check_vault_name(vault_name)\n    inventory = None\n    inventory_job = None\n    if (not refresh):\n        job_list = self.list_jobs(vault_name)\n        inventory_done = False\n        for job in job_list:\n            if (job['Action'] == 'InventoryRetrieval'):\n                if job['Completed']:\n                    self.logger.debug(('Found finished inventory job %s.' % job))\n                    d = dtparse(job['CompletionDate']).replace(tzinfo=pytz.utc)\n                    job['inventory_date'] = d\n                    inventory_done = True\n                    inventory_job = job\n                    break\n                self.logger.debug(('Found running inventory job %s.' % job))\n                inventory_job = job\n        if inventory_done:\n            self.logger.debug('Fetching results of finished inventory retrieval.')\n            response = self.glacierconn.get_job_output(vault_name, inventory_job['JobId'])\n            inventory = response.copy()\n            archives = []\n            if (self.bookkeeping and (len(inventory['ArchiveList']) > 0)):\n                self.logger.debug('Updating the bookkeeping with the latest inventory.')\n                items = {\n                    \n                }\n                for item in inventory['ArchiveList']:\n                    items[item['ArchiveId']] = {\n                        'vault': vault_name,\n                        'archive_id': item['ArchiveId'],\n                        'description': item['ArchiveDescription'],\n                        'date': ('%s' % dtparse(item['CreationDate']).replace(tzinfo=pytz.utc)),\n                        'hash': item['SHA256TreeHash'],\n                        'size': item['Size'],\n                        'region': self.region,\n                    }\n                    archives.append(item['ArchiveId'])\n                    if (len(items) == 25):\n                        self.logger.debug('Writing batch of 25 inventory items to the bookkeeping db.')\n                        try:\n                            self.sdb_domain.batch_put_attributes(items)\n                        except boto.exception.SDBResponseError as e:\n                            raise CommunicationException('Cannot update inventory cache, Amazon SimpleDB is not happy.', cause=e, code='SdbWriteError')\n                        items = {\n                            \n                        }\n                if items:\n                    self.logger.debug(('Writing final batch of %s inventory items to the bookkeeping db.' % len(items)))\n                    try:\n                        self.sdb_domain.batch_put_attributes(items)\n                    except boto.exception.SDBResponseError as e:\n                        raise CommunicationException('Cannot update inventory cache, Amazon SimpleDB is not happy.', cause=e, code='SdbWriteError')\n                query = (\"select * from `%s` where vault='%s'\" % (self.bookkeeping_domain_name, vault_name))\n                result = self.sdb_domain.select(query)\n                try:\n                    for item in result:\n                        if (not (item.name in archives)):\n                            self.sdb_domain.delete_item(item)\n                            self.logger.debug(('Deleted orphaned archive from the database: %s.' % item.name))\n                except boto.exception.SDBResponseError as e:\n                    raise ResponseException('SimpleDB did not respond correctly to our inventory check.', cause=self._decode_error_message(e.body), code=e.code)\n    if (refresh or (not inventory_job)):\n        self.logger.debug('No inventory jobs finished or running; starting a new job.')\n        job_data = {\n            'Type': 'inventory-retrieval',\n        }\n        try:\n            new_job = self.glacierconn.initiate_job(vault_name, job_data)\n        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:\n            raise ResponseException(('Failed to create a new inventory retrieval job for vault %s.' % vault_name), cause=self._decode_error_message(e.body), code=e.code)\n        inventory_job = self.describejob(vault_name, new_job['JobId'])\n    return (inventory_job, inventory)\n", "label": 1}
{"function": "\n\n@qc(1)\ndef init_db():\n    db = db_utils.init_db('sqlite:///:memory:')\n    assert (type(db) is neat.db.Database)\n    assert isinstance(db.hosts, Table)\n    assert isinstance(db.vms, Table)\n    assert isinstance(db.vm_resource_usage, Table)\n    assert isinstance(db.host_states, Table)\n    assert (db.hosts.c.keys() == ['id', 'hostname', 'cpu_mhz', 'cpu_cores', 'ram'])\n    assert (db.host_resource_usage.c.keys() == ['id', 'host_id', 'timestamp', 'cpu_mhz'])\n    assert (list(db.host_resource_usage.foreign_keys)[0].target_fullname == 'hosts.id')\n    assert (db.vms.c.keys() == ['id', 'uuid'])\n    assert (db.vm_resource_usage.c.keys() == ['id', 'vm_id', 'timestamp', 'cpu_mhz'])\n    assert (list(db.vm_resource_usage.foreign_keys)[0].target_fullname == 'vms.id')\n    assert (db.vm_migrations.c.keys() == ['id', 'vm_id', 'host_id', 'timestamp'])\n    keys = set([list(db.vm_migrations.foreign_keys)[0].target_fullname, list(db.vm_migrations.foreign_keys)[1].target_fullname])\n    assert (keys == set(['vms.id', 'hosts.id']))\n    assert (db.host_states.c.keys() == ['id', 'host_id', 'timestamp', 'state'])\n    assert (list(db.host_states.foreign_keys)[0].target_fullname == 'hosts.id')\n    assert (db.host_overload.c.keys() == ['id', 'host_id', 'timestamp', 'overload'])\n    assert (list(db.host_overload.foreign_keys)[0].target_fullname == 'hosts.id')\n", "label": 1}
{"function": "\n\ndef auth_init(self, request, request_class=AuthorizationRequest):\n    '\\n\\n        :param request: The AuthorizationRequest\\n        :return:\\n        '\n    logger.debug((\"Request: '%s'\" % request))\n    try:\n        areq = self.server.parse_authorization_request(request=request_class, query=request)\n    except (MissingRequiredValue, MissingRequiredAttribute) as err:\n        logger.debug(('%s' % err))\n        areq = request_class().deserialize(request, 'urlencoded')\n        try:\n            redirect_uri = self.get_redirect_uri(areq)\n        except (RedirectURIError, ParameterError) as err:\n            return self._error('invalid_request', ('%s' % err))\n        try:\n            _rtype = areq['response_type']\n        except:\n            _rtype = ['code']\n        return self._redirect_authz_error('invalid_request', redirect_uri, ('%s' % err), areq['state'], _rtype)\n    except KeyError:\n        areq = request_class().deserialize(request, 'urlencoded')\n        try:\n            self.get_redirect_uri(areq)\n        except (RedirectURIError, ParameterError) as err:\n            return self._error('invalid_request', ('%s' % err))\n    except Exception as err:\n        message = traceback.format_exception(*sys.exc_info())\n        logger.error(message)\n        logger.debug(('Bad request: %s (%s)' % (err, err.__class__.__name__)))\n        return BadRequest(('%s' % err))\n    if (not areq):\n        logger.debug('No AuthzRequest')\n        return self._error('invalid_request', 'Can not parse AuthzRequest')\n    areq = self.filter_request(areq)\n    if self.events:\n        self.events.store('protocol request', areq)\n    if self.trace:\n        self.trace.info('{}'.format(areq.to_dict()))\n        if ('request' in areq):\n            if areq['request'].jwe_header:\n                self.trace.info('request.jwe_header: {}'.format(areq['request'].jwe_header))\n            if areq['request'].jws_header:\n                self.trace.info('request.jws_header: {}'.format(areq['request'].jws_header))\n    try:\n        _cinfo = self.cdb[areq['client_id']]\n    except KeyError:\n        return self._error('unauthorized_client', 'unknown client')\n    else:\n        try:\n            rtypes = _cinfo['response_types']\n        except KeyError:\n            rtypes = ['code']\n        if (' '.join(areq['response_type']) not in rtypes):\n            return self._error('invalid_request', 'Trying to use unregistered response_typ')\n    logger.debug(('AuthzRequest: %s' % (areq.to_dict(),)))\n    try:\n        redirect_uri = self.get_redirect_uri(areq)\n    except (RedirectURIError, ParameterError, UnknownClient) as err:\n        return self._error('invalid_request', ('%s' % err))\n    try:\n        keyjar = self.keyjar\n    except AttributeError:\n        keyjar = ''\n    try:\n        areq.verify(keyjar=keyjar, opponent_id=areq['client_id'])\n    except (MissingRequiredAttribute, ValueError) as err:\n        return self._redirect_authz_error('invalid_request', redirect_uri, ('%s' % err))\n    return {\n        'areq': areq,\n        'redirect_uri': redirect_uri,\n    }\n", "label": 1}
{"function": "\n\ndef _read_smp_message(self):\n    with self._lock:\n        (smid, flags, sid, l, seq_num, wnd) = self._smp_header.unpack(readall(self._transport, self._smp_header.size))\n        if (smid != self._smid):\n            self._bad_stm('Invalid SMP packet signature')\n        try:\n            session = self._sessions[sid]\n        except KeyError:\n            self._bad_stm('Invalid SMP packet session id')\n        if (wnd < session._high_water_for_send):\n            self._bad_stm('Invalid WNDW in packet from server')\n        if (seq_num > session._high_water_for_recv):\n            self._bad_stm('Invalid SEQNUM in packet from server')\n        session._last_recv_seq_num = seq_num\n        if (flags == self._ACK):\n            if (session._state in ('FIN RECEIVED', 'CLOSED')):\n                self._bad_stm('Unexpected SMP ACK packet from server')\n            if (seq_num != session._seq_num_for_recv):\n                self._bad_stm('Invalid SEQNUM in ACK packet from server')\n            session._high_water_for_send = wnd\n            self._send_queued_packets(session)\n        elif (flags == self._DATA):\n            if (session._state == 'SESSION ESTABLISHED'):\n                if (seq_num != self._add_one_wrap(session._seq_num_for_recv)):\n                    self._bad_stm('Invalid SEQNUM in ACK packet from server')\n                session._seq_num_for_recv = seq_num\n                data = readall(self._transport, (l - self._smp_header.size))\n                session._recv_queue.append(data)\n                if (wnd > session._high_water_for_send):\n                    session._high_water_for_send = wnd\n                    self._send_queued_packets(session)\n            elif (session._state == 'FIN SENT'):\n                skipall(self._transport, (l - self._smp_header.size))\n            else:\n                self._bad_stm('Unexpected DATA packet from server')\n        elif (flags == self._FIN):\n            if (session._state == 'SESSION ESTABLISHED'):\n                session._state = 'FIN RECEIVED'\n            elif (session._state == 'FIN SENT'):\n                session._state = 'CLOSED'\n                del self._sessions[session._session_id]\n                self._used_ids_ba[session._session_id] = False\n            elif (session._state == 'FIN RECEIVED'):\n                self._bad_stm('Unexpected SMP FIN packet from server')\n            else:\n                self._bad_stm(('Invalid state: ' + session._state))\n        elif (flags == self._SYN):\n            self._bad_stm('Unexpected SMP SYN packet from server')\n        else:\n            self._bad_stm('Unexpected SMP flags in packet from server')\n", "label": 1}
{"function": "\n\ndef present(name, DomainName, ElasticsearchClusterConfig=None, EBSOptions=None, AccessPolicies=None, SnapshotOptions=None, AdvancedOptions=None, Tags=None, region=None, key=None, keyid=None, profile=None):\n    '\\n    Ensure domain exists.\\n\\n    name\\n        The name of the state definition\\n\\n    DomainName\\n        Name of the domain.\\n\\n    ElasticsearchClusterConfig\\n        Configuration options for an Elasticsearch domain. Specifies the\\n        instance type and number of instances in the domain cluster.\\n\\n        InstanceType (string) --\\n        The instance type for an Elasticsearch cluster.\\n\\n        InstanceCount (integer) --\\n        The number of instances in the specified domain cluster.\\n\\n        DedicatedMasterEnabled (boolean) --\\n        A boolean value to indicate whether a dedicated master node is enabled.\\n        See About Dedicated Master Nodes for more information.\\n\\n        ZoneAwarenessEnabled (boolean) --\\n        A boolean value to indicate whether zone awareness is enabled. See About\\n        Zone Awareness for more information.\\n\\n        DedicatedMasterType (string) --\\n        The instance type for a dedicated master node.\\n\\n        DedicatedMasterCount (integer) --\\n        Total number of dedicated master nodes, active and on standby, for the\\n        cluster.\\n\\n    EBSOptions\\n        Options to enable, disable and specify the type and size of EBS storage\\n        volumes.\\n\\n        EBSEnabled (boolean) --\\n        Specifies whether EBS-based storage is enabled.\\n\\n        VolumeType (string) --\\n        Specifies the volume type for EBS-based storage.\\n\\n        VolumeSize (integer) --\\n        Integer to specify the size of an EBS volume.\\n\\n        Iops (integer) --\\n        Specifies the IOPD for a Provisioned IOPS EBS volume (SSD).\\n\\n    AccessPolicies\\n        IAM access policy\\n\\n    SnapshotOptions\\n        Option to set time, in UTC format, of the daily automated snapshot.\\n        Default value is 0 hours.\\n\\n        AutomatedSnapshotStartHour (integer) --\\n        Specifies the time, in UTC format, when the service takes a daily\\n        automated snapshot of the specified Elasticsearch domain. Default value\\n        is 0 hours.\\n\\n    AdvancedOptions\\n        Option to allow references to indices in an HTTP request body. Must be\\n        false when configuring access to individual sub-resources. By default,\\n        the value is true .\\n\\n    region\\n        Region to connect to.\\n\\n    key\\n        Secret key to be used.\\n\\n    keyid\\n        Access key to be used.\\n\\n    profile\\n        A dict with region, key and keyid, or a pillar key (string) that\\n        contains a dict with region, key and keyid.\\n    '\n    ret = {\n        'name': DomainName,\n        'result': True,\n        'comment': '',\n        'changes': {\n            \n        },\n    }\n    if (ElasticsearchClusterConfig is None):\n        ElasticsearchClusterConfig = {\n            'DedicatedMasterEnabled': False,\n            'InstanceCount': 1,\n            'InstanceType': 'm3.medium.elasticsearch',\n            'ZoneAwarenessEnabled': False,\n        }\n    if (EBSOptions is None):\n        EBSOptions = {\n            'EBSEnabled': False,\n        }\n    if (SnapshotOptions is None):\n        SnapshotOptions = {\n            'AutomatedSnapshotStartHour': 0,\n        }\n    if (AdvancedOptions is None):\n        AdvancedOptions = {\n            'rest.action.multi.allow_explicit_index': 'true',\n        }\n    if (Tags is None):\n        Tags = {\n            \n        }\n    if ((AccessPolicies is not None) and isinstance(AccessPolicies, string_types)):\n        try:\n            AccessPolicies = json.loads(AccessPolicies)\n        except ValueError as e:\n            ret['result'] = False\n            ret['comment'] = 'Failed to create domain: {0}.'.format(e.message)\n            return ret\n    r = __salt__['boto_elasticsearch_domain.exists'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile)\n    if ('error' in r):\n        ret['result'] = False\n        ret['comment'] = 'Failed to create domain: {0}.'.format(r['error']['message'])\n        return ret\n    if (not r.get('exists')):\n        if __opts__['test']:\n            ret['comment'] = 'Domain {0} is set to be created.'.format(DomainName)\n            ret['result'] = None\n            return ret\n        r = __salt__['boto_elasticsearch_domain.create'](DomainName=DomainName, ElasticsearchClusterConfig=ElasticsearchClusterConfig, EBSOptions=EBSOptions, AccessPolicies=AccessPolicies, SnapshotOptions=SnapshotOptions, AdvancedOptions=AdvancedOptions, region=region, key=key, keyid=keyid, profile=profile)\n        if (not r.get('created')):\n            ret['result'] = False\n            ret['comment'] = 'Failed to create domain: {0}.'.format(r['error']['message'])\n            return ret\n        _describe = __salt__['boto_elasticsearch_domain.describe'](DomainName, region=region, key=key, keyid=keyid, profile=profile)\n        ret['changes']['old'] = {\n            'domain': None,\n        }\n        ret['changes']['new'] = _describe\n        ret['comment'] = 'Domain {0} created.'.format(DomainName)\n        return ret\n    ret['comment'] = os.linesep.join([ret['comment'], 'Domain {0} is present.'.format(DomainName)])\n    ret['changes'] = {\n        \n    }\n    _describe = __salt__['boto_elasticsearch_domain.describe'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile)['domain']\n    _describe['AccessPolicies'] = json.loads(_describe['AccessPolicies'])\n    if (not _describe.get('EBSOptions', {\n        \n    }).get('EBSEnabled')):\n        opts = _describe.get('EBSOptions', {\n            \n        })\n        opts.pop('VolumeSize', None)\n        opts.pop('VolumeType', None)\n    comm_args = {\n        \n    }\n    need_update = False\n    for (k, v) in {\n        'ElasticsearchClusterConfig': ElasticsearchClusterConfig,\n        'EBSOptions': EBSOptions,\n        'AccessPolicies': AccessPolicies,\n        'SnapshotOptions': SnapshotOptions,\n        'AdvancedOptions': AdvancedOptions,\n    }.iteritems():\n        if (v != _describe[k]):\n            need_update = True\n            comm_args[k] = v\n            ret['changes'].setdefault('new', {\n                \n            })[k] = v\n            ret['changes'].setdefault('old', {\n                \n            })[k] = _describe[k]\n    if need_update:\n        if __opts__['test']:\n            msg = 'Domain {0} set to be modified.'.format(DomainName)\n            ret['comment'] = msg\n            ret['result'] = None\n            return ret\n        ret['comment'] = os.linesep.join([ret['comment'], 'Domain to be modified'])\n        r = __salt__['boto_elasticsearch_domain.update'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile, **comm_args)\n        if (not r.get('updated')):\n            ret['result'] = False\n            ret['comment'] = 'Failed to update domain: {0}.'.format(r['error'])\n            ret['changes'] = {\n                \n            }\n            return ret\n    return ret\n", "label": 1}
{"function": "\n\ndef run(self, jid=None):\n    '\\n        Execute the overall routine, print results via outputters\\n        '\n    fstr = '{0}.prep_jid'.format(self.opts['master_job_cache'])\n    jid = self.returners[fstr](passed_jid=(jid or self.opts.get('jid', None)))\n    argv = self.opts['argv']\n    if self.opts.get('raw_shell', False):\n        fun = 'ssh._raw'\n        args = argv\n    else:\n        fun = (argv[0] if argv else '')\n        args = argv[1:]\n    job_load = {\n        'jid': jid,\n        'tgt_type': self.tgt_type,\n        'tgt': self.opts['tgt'],\n        'user': self.opts['user'],\n        'fun': fun,\n        'arg': args,\n    }\n    try:\n        if isinstance(jid, bytes):\n            jid = jid.decode('utf-8')\n        if (self.opts['master_job_cache'] == 'local_cache'):\n            self.returners['{0}.save_load'.format(self.opts['master_job_cache'])](jid, job_load, minions=self.targets.keys())\n        else:\n            self.returners['{0}.save_load'.format(self.opts['master_job_cache'])](jid, job_load)\n    except Exception as exc:\n        log.exception(exc)\n        log.error('Could not save load with returner {0}: {1}'.format(self.opts['master_job_cache'], exc))\n    if self.opts.get('verbose'):\n        msg = 'Executing job with jid {0}'.format(jid)\n        print(msg)\n        print((('-' * len(msg)) + '\\n'))\n        print('')\n    sret = {\n        \n    }\n    outputter = self.opts.get('output', 'nested')\n    final_exit = 0\n    for ret in self.handle_ssh():\n        host = next(six.iterkeys(ret))\n        if isinstance(ret[host], dict):\n            host_ret = ret[host].get('retcode', 0)\n            if (host_ret != 0):\n                final_exit = 1\n        else:\n            final_exit = 1\n        self.cache_job(jid, host, ret[host], fun)\n        ret = self.key_deploy(host, ret)\n        if (isinstance(ret[host], dict) and ret[host].get('stderr', '').startswith('ssh:')):\n            ret[host] = ret[host]['stderr']\n        if (not isinstance(ret[host], dict)):\n            p_data = {\n                host: ret[host],\n            }\n        elif ('return' not in ret[host]):\n            p_data = ret\n        else:\n            outputter = ret[host].get('out', self.opts.get('output', 'nested'))\n            p_data = {\n                host: ret[host].get('return', {\n                    \n                }),\n            }\n        if self.opts.get('static'):\n            sret.update(p_data)\n        else:\n            salt.output.display_output(p_data, outputter, self.opts)\n        if self.event:\n            self.event.fire_event(ret, salt.utils.event.tagify([jid, 'ret', host], 'job'))\n    if self.opts.get('static'):\n        salt.output.display_output(sret, outputter, self.opts)\n    if final_exit:\n        sys.exit(salt.defaults.exitcodes.EX_AGGREGATE)\n", "label": 1}
{"function": "\n\ndef __init__(self, v, aspectValues):\n    self.modelXbrl = v.modelXbrl\n    self.segDimVals = {\n        \n    }\n    self.scenDimVals = {\n        \n    }\n    self.qnameDims = {\n        \n    }\n    self.entityIdentifierHash = self.entityIdentifier = None\n    self.isStartEndPeriod = self.isInstantPeriod = self.isForeverPeriod = False\n    for (aspect, aspectValue) in aspectValues.items():\n        if (aspect == Aspect.PERIOD_TYPE):\n            if (aspectValue == 'forever'):\n                self.isForeverPeriod = True\n            elif (aspectValue == 'instant'):\n                self.isInstantPeriod = True\n            elif (aspectValue == 'duration'):\n                self.isStartEndPeriod = True\n        elif (aspect == Aspect.START):\n            self.isStartEndPeriod = True\n            self.startDatetime = aspectValue\n        elif (aspect == Aspect.END):\n            self.isStartEndPeriod = True\n            self.endDatetime = aspectValue\n        elif (aspect == Aspect.INSTANT):\n            self.isInstantPeriod = True\n            self.endDatetime = self.instantDatetime = aspectValue\n        elif isinstance(aspect, QName):\n            try:\n                contextElement = aspectValue.contextElement\n                aspectValue = (aspectValue.memberQname or aspectValue.typedMember)\n            except AttributeError:\n                contextElement = v.modelXbrl.qnameDimensionContextElement.get(aspect)\n            if (v.modelXbrl.qnameDimensionDefaults.get(aspect) != aspectValue):\n                try:\n                    dimConcept = v.modelXbrl.qnameConcepts[aspect]\n                    dimValPrototype = DimValuePrototype(v, dimConcept, aspect, aspectValue, contextElement)\n                    self.qnameDims[aspect] = dimValPrototype\n                    if (contextElement != 'scenario'):\n                        self.segDimVals[dimConcept] = dimValPrototype\n                    else:\n                        self.scenDimVals[dimConcept] = dimValPrototype\n                except KeyError:\n                    pass\n        elif isinstance(aspectValue, ModelObject):\n            if (aspect == Aspect.PERIOD):\n                context = aspectValue.getparent()\n                for contextPeriodAttribute in ('isForeverPeriod', 'isStartEndPeriod', 'isInstantPeriod', 'startDatetime', 'endDatetime', 'instantDatetime', 'periodHash'):\n                    setattr(self, contextPeriodAttribute, getattr(context, contextPeriodAttribute, None))\n            elif (aspect == Aspect.ENTITY_IDENTIFIER):\n                context = aspectValue.getparent().getparent()\n                for entityIdentAttribute in ('entityIdentifier', 'entityIdentifierHash'):\n                    setattr(self, entityIdentAttribute, getattr(context, entityIdentAttribute, None))\n", "label": 1}
{"function": "\n\ndef add_publication(self, publish_request, commiter=None):\n    logger.debug(('--------------Requested publication---------\\n %s' % repr(publish_request)))\n    current_time = len(self._deltas)\n    delta = BlockDelta(publish_request.msg, publish_request.tag, versiontag=publish_request.versiontag, date=time.time(), origin=publish_request.origin, commiter=commiter)\n    target_time = publish_request.parent.time\n    if (target_time != (current_time - 1)):\n        raise PublishException(('Block %s outdated: %d < %d' % (publish_request.block_name, target_time, (current_time - 1))))\n    if (not publish_request):\n        if (current_time > 0):\n            old_delta = self._deltas[(- 1)]\n            if (old_delta.tag < publish_request.tag):\n                self._deltas[(- 1)] = delta\n                return ([], [], [], [])\n        raise PublishException('Up to date, nothing to publish')\n    if ((current_time > 0) and (self._deltas[(current_time - 1)].tag == DEV)):\n        if (publish_request.parent_time != self._deltas[(- 1)].date):\n            raise PublishException('Concurrent modification, cannot publish, check diff and try again')\n        current_time = (current_time - 1)\n        self._deltas[(- 1)] = delta\n    else:\n        self._deltas.append(delta)\n    self._deps_table.pop_dev(current_time)\n    (_, old_table) = self._deps_table.last()\n    if (old_table != publish_request.deptable):\n        self._deps_table.append(current_time, publish_request.deptable)\n    contents = []\n    cells = []\n    dev_cell_ids = []\n    dev_content_ids = []\n    for name in publish_request.deleted:\n        self._cells_table.delete(name, current_time)\n        try:\n            self._contents_table.delete(name, current_time)\n        except KeyError:\n            pass\n    for cell in publish_request.cells:\n        old_id = self._cells_table.pop_dev(cell.name.cell_name, current_time)\n        if (old_id is not None):\n            old_id = (self._numeric_id + old_id[0])\n            dev_cell_ids.append(old_id)\n        id_ = (self._numeric_id + self._cell_count)\n        self._cell_count += 1\n        cell.ID = id_\n        cells.append(cell)\n        self._cells_table.create(cell.name.cell_name, cell.ID, current_time)\n    for (name, content) in publish_request.contents.iteritems():\n        old_id = self._contents_table.pop_dev(name, current_time)\n        if (old_id is not None):\n            old_id = (self._numeric_id + old_id[0])\n            dev_content_ids.append(old_id)\n        if (content is not None):\n            id_ = (self._numeric_id + self._content_count)\n            self._content_count += 1\n            content.ID = id_\n            contents.append(content)\n            self._contents_table.create(name, content.ID, current_time)\n        else:\n            self._contents_table.delete(name, current_time)\n    for (name, content_id) in publish_request.contents_ids.iteritems():\n        self._contents_table.create(name, content_id, current_time)\n    self._renames.pop_dev(current_time)\n    if publish_request.renames:\n        self._renames.append(current_time, publish_request.renames)\n    return (cells, contents, dev_cell_ids, dev_content_ids)\n", "label": 1}
{"function": "\n\ndef modify_item(self, product, relative=None, absolute=None, recalculate=True, data=None, item=None, force_new=False):\n    '\\n        Updates order with the given product\\n\\n        - ``relative`` or ``absolute``: Add/subtract or define order item\\n          amount exactly\\n        - ``recalculate``: Recalculate order after cart modification\\n          (defaults to ``True``)\\n        - ``data``: Additional data for the order item; replaces the contents\\n          of the JSON field if it is not ``None``. Pass an empty dictionary\\n          if you want to reset the contents.\\n        - ``item``: The order item which should be modified. Will be\\n          automatically detected using the product if unspecified.\\n        - ``force_new``: Force the creation of a new order item, even if the\\n          product exists already in the cart (especially useful if the\\n          product is configurable).\\n\\n        Returns the ``OrderItem`` instance; if quantity is zero, the order\\n        item instance is deleted, the ``pk`` attribute set to ``None`` but\\n        the order item is returned anyway.\\n        '\n    assert ((relative is None) != (absolute is None)), 'One of relative or absolute must be provided.'\n    assert (not (force_new and item)), 'Cannot set item and force_new at the same time.'\n    if self.is_confirmed():\n        raise ValidationError(_('Cannot modify order once it has been confirmed.'), code='order_sealed')\n    if ((item is None) and (not force_new)):\n        try:\n            item = self.items.get(product=product)\n        except self.items.model.DoesNotExist:\n            pass\n        except self.items.model.MultipleObjectsReturned:\n            if (not force_new):\n                raise ValidationError(_('The product already exists several times in the cart, and neither item nor force_new were given.'), code='multiple')\n    if (item is None):\n        item = self.items.model(order=self, product=product, quantity=0, currency=self.currency)\n    if (relative is not None):\n        item.quantity += relative\n    else:\n        item.quantity = absolute\n    if (item.quantity > 0):\n        try:\n            price = product.get_price(currency=self.currency, orderitem=item)\n        except ObjectDoesNotExist:\n            logger.error(('No price could be found for %s with currency %s' % (product, self.currency)))\n            raise ValidationError(_('The price could not be determined.'), code='unknown_price')\n        if (data is not None):\n            item.data = data\n        price.handle_order_item(item)\n        product.handle_order_item(item)\n        item.save()\n    elif item.pk:\n        item.delete()\n        item.pk = None\n    if recalculate:\n        self.recalculate_total()\n        if item.pk:\n            item = self.items.get(pk=item.pk)\n    try:\n        self.validate(self.VALIDATE_BASE)\n    except ValidationError:\n        if item.pk:\n            item.delete()\n        raise\n    return item\n", "label": 1}
{"function": "\n\ndef get_logger(conf, name=None, log_to_console=False, log_route=None, fmt='%(server)s: %(message)s'):\n    '\\n    Get the current system logger using config settings.\\n\\n    **Log config and defaults**::\\n\\n        log_facility = LOG_LOCAL0\\n        log_level = INFO\\n        log_name = swift\\n        log_max_line_length = 0\\n        log_udp_host = (disabled)\\n        log_udp_port = logging.handlers.SYSLOG_UDP_PORT\\n        log_address = /dev/log\\n        log_statsd_host = (disabled)\\n        log_statsd_port = 8125\\n        log_statsd_default_sample_rate = 1.0\\n        log_statsd_sample_rate_factor = 1.0\\n        log_statsd_metric_prefix = (empty-string)\\n\\n    :param conf: Configuration dict to read settings from\\n    :param name: Name of the logger\\n    :param log_to_console: Add handler which writes to console on stderr\\n    :param log_route: Route for the logging, not emitted to the log, just used\\n                      to separate logging configurations\\n    :param fmt: Override log format\\n    '\n    if (not conf):\n        conf = {\n            \n        }\n    if (name is None):\n        name = conf.get('log_name', 'swift')\n    if (not log_route):\n        log_route = name\n    logger = logging.getLogger(log_route)\n    logger.propagate = False\n    formatter = SwiftLogFormatter(fmt=fmt, max_line_length=int(conf.get('log_max_line_length', 0)))\n    if (not hasattr(get_logger, 'handler4logger')):\n        get_logger.handler4logger = {\n            \n        }\n    if (logger in get_logger.handler4logger):\n        logger.removeHandler(get_logger.handler4logger[logger])\n    facility = getattr(SysLogHandler, conf.get('log_facility', 'LOG_LOCAL0'), SysLogHandler.LOG_LOCAL0)\n    udp_host = conf.get('log_udp_host')\n    if udp_host:\n        udp_port = int(conf.get('log_udp_port', logging.handlers.SYSLOG_UDP_PORT))\n        handler = SysLogHandler(address=(udp_host, udp_port), facility=facility)\n    else:\n        log_address = conf.get('log_address', '/dev/log')\n        try:\n            handler = SysLogHandler(address=log_address, facility=facility)\n        except socket.error as e:\n            if (e.errno not in [errno.ENOTSOCK, errno.ENOENT]):\n                raise e\n            handler = SysLogHandler(facility=facility)\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    get_logger.handler4logger[logger] = handler\n    if (log_to_console or hasattr(get_logger, 'console_handler4logger')):\n        if (not hasattr(get_logger, 'console_handler4logger')):\n            get_logger.console_handler4logger = {\n                \n            }\n        if (logger in get_logger.console_handler4logger):\n            logger.removeHandler(get_logger.console_handler4logger[logger])\n        console_handler = logging.StreamHandler(sys.__stderr__)\n        console_handler.setFormatter(formatter)\n        logger.addHandler(console_handler)\n        get_logger.console_handler4logger[logger] = console_handler\n    logger.setLevel(getattr(logging, conf.get('log_level', 'INFO').upper(), logging.INFO))\n    statsd_host = conf.get('log_statsd_host')\n    if statsd_host:\n        statsd_port = int(conf.get('log_statsd_port', 8125))\n        base_prefix = conf.get('log_statsd_metric_prefix', '')\n        default_sample_rate = float(conf.get('log_statsd_default_sample_rate', 1))\n        sample_rate_factor = float(conf.get('log_statsd_sample_rate_factor', 1))\n        statsd_client = StatsdClient(statsd_host, statsd_port, base_prefix, name, default_sample_rate, sample_rate_factor, logger=logger)\n        logger.statsd_client = statsd_client\n    else:\n        logger.statsd_client = None\n    adapted_logger = LogAdapter(logger, name)\n    other_handlers = conf.get('log_custom_handlers', None)\n    if other_handlers:\n        log_custom_handlers = [s.strip() for s in other_handlers.split(',') if s.strip()]\n        for hook in log_custom_handlers:\n            try:\n                (mod, fnc) = hook.rsplit('.', 1)\n                logger_hook = getattr(__import__(mod, fromlist=[fnc]), fnc)\n                logger_hook(conf, name, log_to_console, log_route, fmt, logger, adapted_logger)\n            except (AttributeError, ImportError):\n                print(('Error calling custom handler [%s]' % hook), file=sys.stderr)\n            except ValueError:\n                print(('Invalid custom handler format [%s]' % hook), file=sys.stderr)\n    return adapted_logger\n", "label": 1}
{"function": "\n\n@csrf_protect\n@require_POST\ndef post_comment(request, next=None, using=None):\n    '\\n    Post a comment.\\n\\n    HTTP POST is required. If ``POST[\\'submit\\'] == \"preview\"`` or if there are\\n    errors a preview template, ``comments/preview.html``, will be rendered.\\n    '\n    data = request.POST.copy()\n    if request.user.is_authenticated():\n        if (not data.get('name', '')):\n            data['name'] = (request.user.get_full_name() or request.user.get_username())\n        if (not data.get('email', '')):\n            data['email'] = request.user.email\n    ctype = data.get('content_type')\n    object_pk = data.get('object_pk')\n    if ((ctype is None) or (object_pk is None)):\n        return CommentPostBadRequest('Missing content_type or object_pk field.')\n    try:\n        model = models.get_model(*ctype.split('.', 1))\n        target = model._default_manager.using(using).get(pk=object_pk)\n    except TypeError:\n        return CommentPostBadRequest(('Invalid content_type value: %r' % escape(ctype)))\n    except AttributeError:\n        return CommentPostBadRequest(('The given content-type %r does not resolve to a valid model.' % escape(ctype)))\n    except ObjectDoesNotExist:\n        return CommentPostBadRequest(('No object matching content-type %r and object PK %r exists.' % (escape(ctype), escape(object_pk))))\n    except (ValueError, ValidationError) as e:\n        return CommentPostBadRequest(('Attempting go get content-type %r and object PK %r exists raised %s' % (escape(ctype), escape(object_pk), e.__class__.__name__)))\n    preview = ('preview' in data)\n    form = comments.get_form()(target, data=data)\n    if form.security_errors():\n        return CommentPostBadRequest(('The comment form failed security verification: %s' % escape(str(form.security_errors()))))\n    if (form.errors or preview):\n        template_list = [('comments/%s_%s_preview.html' % (model._meta.app_label, model._meta.module_name)), ('comments/%s_preview.html' % model._meta.app_label), ('comments/%s/%s/preview.html' % (model._meta.app_label, model._meta.module_name)), ('comments/%s/preview.html' % model._meta.app_label), 'comments/preview.html']\n        return render_to_response(template_list, {\n            'comment': form.data.get('comment', ''),\n            'form': form,\n            'next': data.get('next', next),\n        }, RequestContext(request, {\n            \n        }))\n    comment = form.get_comment_object()\n    comment.ip_address = request.META.get('REMOTE_ADDR', None)\n    if request.user.is_authenticated():\n        comment.user = request.user\n    responses = signals.comment_will_be_posted.send(sender=comment.__class__, comment=comment, request=request)\n    for (receiver, response) in responses:\n        if (response == False):\n            return CommentPostBadRequest(('comment_will_be_posted receiver %r killed the comment' % receiver.__name__))\n    comment.save()\n    signals.comment_was_posted.send(sender=comment.__class__, comment=comment, request=request)\n    return next_redirect(request, fallback=(next or 'comments-comment-done'), c=comment._get_pk_val())\n", "label": 1}
{"function": "\n\ndef _fit_stochastic(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental):\n    rng = check_random_state(self.random_state)\n    if ((not incremental) or (not hasattr(self, '_optimizer'))):\n        params = (self.coefs_ + self.intercepts_)\n        if (self.algorithm == 'sgd'):\n            self._optimizer = SGDOptimizer(params, self.learning_rate_init, self.learning_rate, self.momentum, self.nesterovs_momentum, self.power_t)\n        elif (self.algorithm == 'adam'):\n            self._optimizer = AdamOptimizer(params, self.learning_rate_init, self.beta_1, self.beta_2, self.epsilon)\n    early_stopping = (self.early_stopping and (not incremental))\n    if early_stopping:\n        (X, X_val, y, y_val) = train_test_split(X, y, random_state=self.random_state, test_size=self.validation_fraction)\n        if isinstance(self, ClassifierMixin):\n            y_val = self.label_binarizer_.inverse_transform(y_val)\n    else:\n        X_val = None\n        y_val = None\n    n_samples = X.shape[0]\n    if (self.batch_size == 'auto'):\n        batch_size = min(200, n_samples)\n    else:\n        batch_size = np.clip(self.batch_size, 1, n_samples)\n    try:\n        for it in range(self.max_iter):\n            (X, y) = shuffle(X, y, random_state=rng)\n            accumulated_loss = 0.0\n            for batch_slice in gen_batches(n_samples, batch_size):\n                activations[0] = X[batch_slice]\n                (batch_loss, coef_grads, intercept_grads) = self._backprop(X[batch_slice], y[batch_slice], activations, deltas, coef_grads, intercept_grads)\n                accumulated_loss += (batch_loss * (batch_slice.stop - batch_slice.start))\n                grads = (coef_grads + intercept_grads)\n                self._optimizer.update_params(grads)\n            self.n_iter_ += 1\n            self.loss_ = (accumulated_loss / X.shape[0])\n            self.t_ += n_samples\n            self.loss_curve_.append(self.loss_)\n            if self.verbose:\n                print(('Iteration %d, loss = %.8f' % (self.n_iter_, self.loss_)))\n            self._update_no_improvement_count(early_stopping, X_val, y_val)\n            self._optimizer.iteration_ends(self.t_)\n            if (self._no_improvement_count > 2):\n                if early_stopping:\n                    msg = ('Validation score did not improve more than tol=%f for two consecutive epochs.' % self.tol)\n                else:\n                    msg = ('Training loss did not improve more than tol=%f for two consecutive epochs.' % self.tol)\n                is_stopping = self._optimizer.trigger_stopping(msg, self.verbose)\n                if is_stopping:\n                    break\n                else:\n                    self._no_improvement_count = 0\n            if incremental:\n                break\n            if (self.n_iter_ == self.max_iter):\n                warnings.warn((\"Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\" % ()), ConvergenceWarning)\n    except KeyboardInterrupt:\n        pass\n    if early_stopping:\n        self.coefs_ = self._best_coefs\n        self.intercepts_ = self._best_intercepts\n", "label": 1}

{"function": "\n\ndef test_s3_domain_with_default_root_object(self):\n    cmdline = ((self.prefix + '--origin-domain-name foo.s3.amazonaws.com ') + '--default-root-object index.html')\n    result = {\n        'DistributionConfig': {\n            'Origins': {\n                'Quantity': 1,\n                'Items': [{\n                    'S3OriginConfig': mock.ANY,\n                    'DomainName': 'foo.s3.amazonaws.com',\n                    'Id': mock.ANY,\n                    'OriginPath': '',\n                }],\n            },\n            'CallerReference': mock.ANY,\n            'Comment': '',\n            'Enabled': True,\n            'DefaultCacheBehavior': mock.ANY,\n            'DefaultRootObject': 'index.html',\n        },\n    }\n    self.run_cmd(cmdline)\n    self.assertEqual(self.last_kwargs, result)\n", "label": 0}
{"function": "\n\ndef pages_dynamic_tree_menu(context, page, url='/'):\n    '\\n    Render a \"dynamic\" tree menu, with all nodes expanded which are either\\n    ancestors or the current page itself.\\n\\n    Override ``pages/dynamic_tree_menu.html`` if you want to change the\\n    design.\\n\\n    :param page: the current page\\n    :param url: not used anymore\\n    '\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    children = None\n    if (page and ('current_page' in context)):\n        current_page = context['current_page']\n        if ((page.tree_id == current_page.tree_id) and (page.lft <= current_page.lft) and (page.rght >= current_page.rght)):\n            children = page.get_children_for_frontend()\n    context.update({\n        'children': children,\n        'page': page,\n    })\n    return context\n", "label": 0}
{"function": "\n\ndef remove(path, recursive=False, use_sudo=False):\n    '\\n    Remove a file or directory\\n    '\n    func = ((use_sudo and run_as_root) or run)\n    options = ('-r ' if recursive else '')\n    func('/bin/rm {0}{1}'.format(options, quote(path)))\n", "label": 0}
{"function": "\n\ndef marshal_dump(code, f):\n    if isinstance(f, file):\n        marshal.dump(code, f)\n    else:\n        f.write(marshal.dumps(code))\n", "label": 0}
{"function": "\n\ndef test_show_body(self):\n    client = Mock()\n    client.indices.get_settings.return_value = testvars.settings_one\n    client.cluster.state.return_value = testvars.clu_state_one\n    client.indices.stats.return_value = testvars.stats_one\n    ilo = curator.IndexList(client)\n    ao = curator.Alias(name='alias')\n    ao.remove(ilo)\n    ao.add(ilo)\n    body = ao.body()\n    self.assertEqual(testvars.alias_one_body['actions'][0], body['actions'][0])\n    self.assertEqual(testvars.alias_one_body['actions'][1], body['actions'][1])\n", "label": 0}
{"function": "\n\n@mock.patch('SoftLayer.API.BaseClient.iter_call')\ndef test_iterate(self, _iter_call):\n    self.client['SERVICE'].METHOD(iter=True)\n    _iter_call.assert_called_with('SERVICE', 'METHOD')\n", "label": 0}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    try:\n        self._call(*args, **kwargs)\n    except Exception as err:\n        stats.incr('callback-failure', 1)\n        logging.exception('Callback Failed: %s', err)\n", "label": 0}
{"function": "\n\n@register.filter\ndef lookup(h, key):\n    try:\n        return h[key]\n    except KeyError:\n        return ''\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    return self.build('add', self, other)\n", "label": 0}
{"function": "\n\n@classmethod\ndef read(cls, handle):\n    self = cls()\n    self._read_luminosity(handle)\n    self.name = handle.attrs['name'].decode('utf-8')\n    self.peeloff = str2bool(handle.attrs['peeloff'])\n    if (handle.attrs['spectrum'] == b'spectrum'):\n        self.spectrum = Table(np.array(handle['spectrum']))\n    elif (handle.attrs['spectrum'] == b'temperature'):\n        self.temperature = handle.attrs['temperature']\n    elif (handle.attrs['spectrum'] == b'lte'):\n        pass\n    else:\n        raise ValueError(('Unexpected value for `spectrum`: %s' % handle.attrs['spectrum']))\n    return self\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    category = resolve(self.category, context)\n    if isinstance(category, CategoryBase):\n        cat = category\n    else:\n        cat = get_category(category, self.model)\n    try:\n        if (cat is not None):\n            context[self.varname] = drilldown_tree_for_node(cat)\n        else:\n            context[self.varname] = []\n    except:\n        context[self.varname] = []\n    return ''\n", "label": 0}
{"function": "\n\ndef __init__(self, manufacturer, data):\n    self.manufacturer = manufacturer\n    self.data = list(data)\n", "label": 0}
{"function": "\n\ndef process_response(self, request, response):\n    from django.conf import settings\n    from rest_framework.exceptions import PermissionDenied\n    from .access_control import upload_prefix_for_request\n    cookie_name = getattr(settings, 'UPLOAD_PREFIX_COOKIE_NAME', 'upload_prefix')\n    try:\n        response.set_cookie(cookie_name, upload_prefix_for_request(request))\n    except PermissionDenied:\n        response.delete_cookie(cookie_name)\n    return response\n", "label": 0}
{"function": "\n\ndef createLineSet(self, indices, inputlist, materialid):\n    'Create a set of lines for use in this geometry instance.\\n\\n        :param numpy.array indices:\\n          unshaped numpy array that contains the indices for\\n          the inputs referenced in inputlist\\n        :param collada.source.InputList inputlist:\\n          The inputs for this primitive\\n        :param str materialid:\\n          A string containing a symbol that will get used to bind this lineset\\n          to a material when instantiating into a scene\\n\\n        :rtype: :class:`collada.lineset.LineSet`\\n        '\n    inputdict = primitive.Primitive._getInputsFromList(self.collada, self.sourceById, inputlist.getList())\n    return lineset.LineSet(inputdict, materialid, indices)\n", "label": 0}
{"function": "\n\ndef stepSlice(self, offset):\n    ' Move the selected structure one slice up or down\\n        :param offset: +1 or -1\\n        :return:\\n        '\n    volumeId = self.volumeSelector.currentNodeId\n    if (volumeId == ''):\n        self.showUnselectedVolumeWarningMessage()\n        return\n    selectedStructure = self.getCurrentSelectedStructure()\n    if (selectedStructure == self.logic.NONE):\n        self.showUnselectedStructureWarningMessage()\n        return\n    if (selectedStructure == self.logic.BOTH):\n        self.logic.stepSlice(volumeId, self.logic.AORTA, offset)\n        newSlice = self.logic.stepSlice(volumeId, self.logic.PA, offset)\n    else:\n        newSlice = self.logic.stepSlice(volumeId, selectedStructure, offset)\n    self.moveRedWindowToSlice(newSlice)\n", "label": 0}
{"function": "\n\ndef normalize_diff_filename(self, filename):\n    \"Normalize filenames in diffs.\\n\\n        The default behavior of stripping off leading slashes doesn't work for\\n        Perforce (because depot paths start with //), so this overrides it to\\n        just return the filename un-molested.\\n        \"\n    return filename\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/item/shared_item_chair_metal_s1.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef test_content_url_encoding_safe(self):\n    s = Site(self.SITE_PATH, config=self.config)\n    s.load()\n    path = '\".jpg/abc'\n    print(s.content_url(path, ''))\n    print(('/' + quote(path, '')))\n    assert (s.content_url(path, '') == ('/' + quote(path, '')))\n", "label": 0}
{"function": "\n\ndef post_undelete(self, *args, **kwargs):\n    self.post_undelete_called = True\n", "label": 0}
{"function": "\n\ndef timecolon(data):\n    match = re.search('(\\\\d+:\\\\d+:\\\\d+):(\\\\d+)', data)\n    return ('%s,%s' % (match.group(1), match.group(2)))\n", "label": 0}
{"function": "\n\ndef getUpdatedBatchJob(self, maxWait):\n    while True:\n        try:\n            (jobID, status, wallTime) = self.updatedJobsQueue.get(timeout=maxWait)\n        except Empty:\n            return None\n        try:\n            self.runningJobs.remove(jobID)\n        except KeyError:\n            pass\n        else:\n            return (jobID, status, wallTime)\n", "label": 0}
{"function": "\n\ndef package_private_devel_path(self, package):\n    'The path to the linked devel space for a given package.'\n    return os.path.join(self.private_devel_path, package.name)\n", "label": 0}
{"function": "\n\ndef test_getset_owner(self):\n    m = meta.Metadata()\n    o = m.get_owner('files/one')\n    m.set_owner('files/one', *o)\n", "label": 0}
{"function": "\n\ndef indent(string, prefix='    '):\n    '\\n    Indent every line of this string.\\n    '\n    return ''.join((('%s%s\\n' % (prefix, s)) for s in string.split('\\n')))\n", "label": 0}
{"function": "\n\ndef stories(self, task, params={\n    \n}, **options):\n    'Returns a compact representation of all of the stories on the task.\\n\\n        Parameters\\n        ----------\\n        task : {Id} The task containing the stories to get.\\n        [params] : {Object} Parameters for the request\\n        '\n    path = ('/tasks/%s/stories' % task)\n    return self.client.get_collection(path, params, **options)\n", "label": 0}
{"function": "\n\ndef get_context_types_value(context_id, source, filter_func, codebase):\n    query = CodeElementLink.objects.filter(code_element__kind__is_type=True).filter(index=0).filter(code_element__codebase=codebase).filter(code_reference__source=source)\n    query = filter_func(query, context_id)\n    context_types = []\n    pk_set = set()\n    for link in query.all():\n        code_element = link.code_element\n        if (code_element.pk not in pk_set):\n            context_types.append(code_element)\n            pk_set.add(code_element.pk)\n    return context_types\n", "label": 0}
{"function": "\n\ndef test_elemwise_thats_also_a_column():\n    t = symbol('t', 'var * {x: int, time: datetime, y: int}')\n    expr = t[(t.x > 0)].time.truncate(months=1)\n    expected = t[['time', 'x']]\n    result = lean_projection(expr)\n    assert result._child._child._child.isidentical(t[['time', 'x']])\n", "label": 0}
{"function": "\n\ndef test_list(self):\n    lists = {('l%s' % i): list(string.ascii_lowercase[:i]) for i in range(1, 10)}\n    self.collections_common_tests(lists, 'l')\n", "label": 0}
{"function": "\n\ndef fit_transform(self, X, y):\n    'Fit and transform.'\n    self.fit(X, y)\n    return self.transform(X)\n", "label": 0}
{"function": "\n\ndef update_app(self, app_id, app, force=False, minimal=True):\n    'Update an app.\\n\\n        Applies writable settings in `app` to `app_id`\\n        Note: this method can not be used to rename apps.\\n\\n        :param str app_id: target application ID\\n        :param app: application settings\\n        :type app: :class:`marathon.models.app.MarathonApp`\\n        :param bool force: apply even if a deployment is in progress\\n        :param bool minimal: ignore nulls and empty collections\\n\\n        :returns: a dict containing the deployment id and version\\n        :rtype: dict\\n        '\n    app.version = None\n    params = {\n        'force': force,\n    }\n    data = app.to_json(minimal=minimal)\n    response = self._do_request('PUT', '/v2/apps/{app_id}'.format(app_id=app_id), params=params, data=data)\n    return response.json()\n", "label": 0}
{"function": "\n\ndef create_toplevel_ws(self, wsname, width, height, group=2, x=None, y=None):\n    root = self.app.make_window()\n    ws = self.make_ws(wsname, wstype='tabs')\n    vbox = Widgets.VBox()\n    vbox.set_border_width(0)\n    self._add_toolbar(vbox, ws)\n    vbox.add_widget(bnch.widget)\n    root.set_widget(vbox)\n    root.resize(width, height)\n    root.show()\n    self.toplevels.append(root)\n    if (x is not None):\n        root.move(x, y)\n    return bnch\n", "label": 0}
{"function": "\n\ndef get_queryset(self):\n    queryset = self.queryset\n    if isinstance(queryset, (QuerySet, Manager)):\n        queryset = queryset.all()\n    return queryset\n", "label": 0}
{"function": "\n\ndef get_current_editor(self):\n    page = self.notebook.get_current_page()\n    if (page is None):\n        return None\n    return page.get_text_widget()\n", "label": 0}
{"function": "\n\ndef test_error_load_single_field_type(single_schema):\n    (data, errors) = single_schema.load({\n        'child': {\n            'id': 'foo',\n        },\n    })\n    assert (not data)\n    assert (errors == {\n        'child': {\n            'id': [fields.Integer().error_messages['invalid']],\n        },\n    })\n", "label": 0}
{"function": "\n\ndef start(self, fileStore):\n    subprocess.check_call((self.cmd + ' 1'), shell=True)\n", "label": 0}
{"function": "\n\ndef test_reraise(self):\n    self.assertRaises(RuntimeError, reraise, RuntimeError, RuntimeError())\n    try:\n        raise RuntimeError('bla')\n    except Exception:\n        exc_info = sys.exc_info()\n    self.assertRaises(RuntimeError, reraise, *exc_info)\n", "label": 0}
{"function": "\n\ndef check_write_package(self, username, package_reference):\n    '\\n        username: User that request to write the package\\n        package_reference: PackageReference\\n        '\n    self.check_write_conan(username, package_reference.conan)\n", "label": 0}
{"function": "\n\ndef test_it_knows_how_many_total_errors_it_contains(self):\n    errors = [mock.MagicMock() for _ in range(8)]\n    tree = exceptions.ErrorTree(errors)\n    self.assertEqual(tree.total_errors, 8)\n", "label": 0}
{"function": "\n\ndef test_get_lock_multiple_coords(self):\n    member_id2 = self._get_random_uuid()\n    client2 = tooz.coordination.get_coordinator(self.url, member_id2)\n    client2.start()\n    lock_name = self._get_random_uuid()\n    lock = self._coord.get_lock(lock_name)\n    self.assertTrue(lock.acquire())\n    lock2 = client2.get_lock(lock_name)\n    self.assertFalse(lock2.acquire(blocking=False))\n    self.assertTrue(lock.release())\n    self.assertTrue(lock2.acquire(blocking=True))\n    self.assertTrue(lock2.release())\n", "label": 0}
{"function": "\n\ndef test_keys(self):\n    getkeys = self.ts.keys\n    self.assertIs(getkeys(), self.ts.index)\n", "label": 0}
{"function": "\n\ndef user_add_stage(request):\n    if (not request.user.has_perm('auth.change_user')):\n        raise PermissionDenied\n    manipulator = UserCreationForm()\n    if (request.method == 'POST'):\n        new_data = request.POST.copy()\n        errors = manipulator.get_validation_errors(new_data)\n        if (not errors):\n            new_user = manipulator.save(new_data)\n            msg = (_('The %(name)s \"%(obj)s\" was added successfully.') % {\n                'name': 'user',\n                'obj': new_user,\n            })\n            if request.POST.has_key('_addanother'):\n                request.user.message_set.create(message=msg)\n                return HttpResponseRedirect(request.path)\n            else:\n                request.user.message_set.create(message=((msg + ' ') + _('You may edit it again below.')))\n                return HttpResponseRedirect(('../%s/' % new_user.id))\n    else:\n        errors = new_data = {\n            \n        }\n    form = oldforms.FormWrapper(manipulator, new_data, errors)\n    return render_to_response('admin/auth/user/add_form.html', {\n        'title': _('Add user'),\n        'form': form,\n        'is_popup': request.REQUEST.has_key('_popup'),\n        'add': True,\n        'change': False,\n        'has_delete_permission': False,\n        'has_change_permission': True,\n        'has_file_field': False,\n        'has_absolute_url': False,\n        'auto_populated_fields': (),\n        'bound_field_sets': (),\n        'first_form_field_id': 'id_username',\n        'opts': User._meta,\n        'username_help_text': User._meta.get_field('username').help_text,\n    }, context_instance=template.RequestContext(request))\n", "label": 0}
{"function": "\n\ndef generateDictOperationInCode(to_name, expression, emit, context):\n    inverted = expression.isExpressionDictOperationNOTIn()\n    (dict_name, key_name) = generateChildExpressionsCode(expression=expression, emit=emit, context=context)\n    res_name = context.getIntResName()\n    emit(('%s = PyDict_Contains( %s, %s );' % (res_name, key_name, dict_name)))\n    getReleaseCodes(release_names=(dict_name, key_name), emit=emit, context=context)\n    getErrorExitBoolCode(condition=('%s == -1' % res_name), needs_check=expression.mayRaiseException(BaseException), emit=emit, context=context)\n    emit(('%s = BOOL_FROM( %s == %s );' % (to_name, res_name, ('1' if (not inverted) else '0'))))\n", "label": 0}
{"function": "\n\ndef Add(self, node):\n    self.binary('Add', node)\n", "label": 0}
{"function": "\n\ndef random_orthogonal(dim, special=True):\n    if (dim == 1):\n        if (np.random.uniform() < 0.5):\n            return np.ones((1, 1))\n        return (- np.ones((1, 1)))\n    P = np.random.randn(dim, dim)\n    while (np.linalg.matrix_rank(P) != dim):\n        P = np.random.randn(dim, dim)\n    (U, S, V) = np.linalg.svd(P)\n    P = np.dot(U, V)\n    if special:\n        if (np.linalg.det(P) < 0):\n            P[:, [0, 1]] = P[:, [1, 0]]\n    return P\n", "label": 0}
{"function": "\n\ndef register(self, observer):\n    ' Called when an observer wants to be notified\\n        about project changes\\n\\n        '\n    self._observers.append(observer)\n", "label": 0}
{"function": "\n\ndef __iadd__(self, other):\n    self.extend(other)\n    return self\n", "label": 0}
{"function": "\n\n@classmethod\ndef __subclasshook__(cls, other_cls):\n    if (cls is Tombola):\n        interface_names = function_names(cls)\n        found_names = set()\n        for a_cls in other_cls.__mro__:\n            found_names |= function_names(a_cls)\n        if (found_names >= interface_names):\n            return True\n    return NotImplemented\n", "label": 0}
{"function": "\n\ndef destroy(self):\n    ' Destroy the dock manager.\\n\\n        This method will free all of the resources held by the dock\\n        manager. The primary dock area and dock items will not be\\n        destroyed. After the method is called, the dock manager is\\n        invalid and should no longer be used.\\n\\n        '\n    for frame in self._dock_frames:\n        if isinstance(frame, QDockContainer):\n            frame.setDockItem(None)\n            frame.setParent(None, Qt.Widget)\n            frame.hide()\n    for frame in self._dock_frames:\n        if isinstance(frame, QDockWindow):\n            frame.setParent(None, Qt.Widget)\n            frame.hide()\n    for item in self._dock_items:\n        item._manager = None\n    self._dock_area.setCentralWidget(None)\n    self._dock_area.setMaximizedWidget(None)\n    del self._dock_area\n    del self._dock_frames\n    del self._dock_items\n    del self._proximity_handler\n    del self._container_monitor\n    del self._overlay\n", "label": 0}
{"function": "\n\ndef test_incr_sample_rate(self):\n    client = statsd.StatsdClient('localhost', 8125, prefix='', sample_rate=0.999)\n    client.incr('buck.counter', 5)\n    self.assertEqual(client._socket.data, b'buck.counter:5|c|@0.999')\n    if (client._socket.data != 'buck.counter:5|c'):\n        self.assertTrue(client._socket.data.endswith(b'|@0.999'))\n", "label": 0}
{"function": "\n\ndef format_author(self, entry):\n    try:\n        persons = entry.persons['author']\n        if (sys.version_info[0] == 2):\n            authors = [unicode(au) for au in persons]\n        elif (sys.version_info[0] == 3):\n            authors = [str(au) for au in persons]\n    except KeyError:\n        authors = ['']\n    authors = self.strip_chars('; '.join(authors))\n    return authors\n", "label": 0}
{"function": "\n\ndef test_delete_group_inuse_process(self):\n    url = ('/v1/groups/' + GID)\n    req = get_request(url, 'DELETE')\n    self.stubs.Set(db, 'keypair_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'securitygroup_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'network_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'process_get_all', fake_not_group_data_exists)\n    res = req.get_response(self.app)\n    self.assertEqual(res.status_code, 409)\n", "label": 0}
{"function": "\n\ndef synchro_connect(self):\n    try:\n        self.synchronize(self.delegate.open)()\n    except AutoReconnect as e:\n        raise ConnectionFailure(str(e))\n", "label": 0}
{"function": "\n\ndef test_validate_configuration_invalid_disk_type(self):\n    raid_config = json.loads(raid_constants.RAID_CONFIG_INVALID_DISK_TYPE)\n    self.assertRaises(exception.InvalidParameterValue, raid.validate_configuration, raid_config, raid_config_schema=self.schema)\n", "label": 0}
{"function": "\n\ndef test_notification_no_pause(self):\n    self.displayer.notification('message', 10)\n    string = self.mock_stdout.write.call_args[0][0]\n    self.assertTrue(('message' in string))\n", "label": 0}
{"function": "\n\ndef _augmented_orthonormal_cols(x, k):\n    (n, m) = x.shape\n    y = np.empty((n, (m + k)), dtype=x.dtype)\n    y[:, :m] = x\n    for i in range(k):\n        v = np.random.randn(n)\n        if np.iscomplexobj(x):\n            v = (v + (1j * np.random.randn(n)))\n        for j in range((m + i)):\n            u = y[:, j]\n            v -= ((np.dot(v, u.conj()) / np.dot(u, u.conj())) * u)\n        v /= np.sqrt(np.dot(v, v.conj()))\n        y[:, (m + i)] = v\n    return y\n", "label": 0}
{"function": "\n\ndef test_call_and_missing_check_with_obj_list(self):\n\n    def yield_hashes(device, partition, policy, suffixes=None, **kwargs):\n        if ((device == 'dev') and (partition == '9') and (suffixes == ['abc']) and (policy == POLICIES.legacy)):\n            (yield ('/srv/node/dev/objects/9/abc/9d41d8cd98f00b204e9800998ecf0abc', '9d41d8cd98f00b204e9800998ecf0abc', {\n                'ts_data': Timestamp(1380144470.0),\n            }))\n        else:\n            raise Exception(('No match for %r %r %r' % (device, partition, suffixes)))\n    job = {\n        'device': 'dev',\n        'partition': '9',\n        'policy': POLICIES.legacy,\n        'frag_index': 0,\n    }\n    self.sender = ssync_sender.Sender(self.daemon, None, job, ['abc'], ['9d41d8cd98f00b204e9800998ecf0abc'])\n    self.sender.connection = FakeConnection()\n    self.sender.response = FakeResponse(chunk_body=':MISSING_CHECK: START\\r\\n:MISSING_CHECK: END\\r\\n')\n    self.sender.daemon._diskfile_mgr.yield_hashes = yield_hashes\n    self.sender.connect = mock.MagicMock()\n    self.sender.updates = mock.MagicMock()\n    self.sender.disconnect = mock.MagicMock()\n    (success, candidates) = self.sender()\n    self.assertTrue(success)\n    self.assertEqual(candidates, dict([('9d41d8cd98f00b204e9800998ecf0abc', {\n        'ts_data': Timestamp(1380144470.0),\n    })]))\n    self.assertEqual(self.sender.failures, 0)\n", "label": 0}
{"function": "\n\ndef test_try_failure_bad_arg(self):\n    rv = self.app.get('/trivial_fn?nothing=1')\n    assert (rv.status_code == 200)\n    data = rv.data.decode('utf8')\n    jsn = json.loads(data)\n    assert (jsn['success'] == False), 'We expect this call failed as it has the wrong argument'\n", "label": 0}
{"function": "\n\ndef test_queryset_deleted_on(self):\n    'qs delete() sets deleted_on to same time as parent on cascade.'\n    p = self.F.ProductFactory.create()\n    s = self.F.SuiteFactory.create(product=p)\n    self.model.Product.objects.all().delete()\n    p = self.refresh(p)\n    s = self.refresh(s)\n    self.assertIsNot(p.deleted_on, None)\n    self.assertEqual(s.deleted_on, p.deleted_on)\n", "label": 0}
{"function": "\n\ndef __init__(self, status):\n    super(RCException, self).__init__(('RAMCloud error ' + str(status)))\n    self.status = status\n", "label": 0}
{"function": "\n\ndef test_delete_all_lines_inversed(self):\n    text = '1\\n22\\n3\\n44\\n5\\n66\\n'\n    self.fillAndClear(text)\n    self.buffer.delete(Range(6, 1))\n    assert (str(self.buffer) == '')\n    assert (self.buffer.lines == [])\n    assert (self.deleted('afterPosition') == Position(1, 1))\n    assert (self.deleted('startPosition') == Position(7, 1))\n", "label": 0}
{"function": "\n\ndef run_osprey(self, config):\n    '\\n        Run osprey-worker.\\n\\n        Parameters\\n        ----------\\n        config : str\\n            Configuration string.\\n        '\n    (fh, filename) = tempfile.mkstemp(dir=self.temp_dir)\n    with open(filename, 'wb') as f:\n        f.write(config)\n    args = Namespace(config=filename, n_iters=1, output='json')\n    execute_worker.execute(args, None)\n    dump = json.loads(execute_dump.execute(args, None))\n    assert (len(dump) == 1)\n    assert (dump[0]['status'] == 'SUCCEEDED'), dump[0]['status']\n", "label": 0}
{"function": "\n\ndef from_text(cls, rdclass, rdtype, tok, origin=None, relativize=True):\n    address = tok.get_string()\n    tok.get_eol()\n    return cls(rdclass, rdtype, address)\n", "label": 0}
{"function": "\n\ndef update_parser_common(self, parser):\n    parser.add_argument('network', metavar='<network>', nargs='+', help='Network(s) to delete (name or ID)')\n    return parser\n", "label": 0}
{"function": "\n\n@process_multiple\ndef to_html(self, values, fields, context):\n    toks = []\n    for value in values:\n        if (value in self.html_map):\n            tok = self.html_map[value]\n        elif (value is None):\n            continue\n        elif (type(value) is float):\n            tok = filters.floatformat(value)\n        else:\n            tok = unicode(value)\n        toks.append(tok)\n    return self.delimiter.join(toks)\n", "label": 0}
{"function": "\n\ndef test_dependency_sorting_4(self):\n    sorted_deps = sort_dependencies([('fixtures_regress', [Store, Person, Book])])\n    self.assertEqual(sorted_deps, [Store, Person, Book])\n", "label": 0}
{"function": "\n\ndef _eval_rewrite_as_gamma(self, x, k):\n    from sympy import gamma\n    return ((((- 1) ** k) * gamma((k - x))) / gamma((- x)))\n", "label": 0}
{"function": "\n\ndef _get_x(self):\n    if (len(self.names) > 1):\n        return ([self.__getattribute__(name) for name in self.names] + list(self.args))\n    return ([self.__getattribute__(self.names[0])] + list(self.args))\n", "label": 0}
{"function": "\n\ndef turn_off(self, **kwargs):\n    'Turn the device off/open the device.'\n    self.action_node.runElse()\n", "label": 0}
{"function": "\n\ndef test_set_rewrite(self):\n    '`LocalMemStorage` set method of existing key'\n    s = LocalMemStorage()\n    s.set('key', 'value')\n    s.set('key', 'value1')\n    self.assertEqual(s.storage['key'], 'value1')\n", "label": 0}
{"function": "\n\ndef do_create(self, max_size=0, dir=None, pre='', suf=''):\n    if (dir is None):\n        dir = tempfile.gettempdir()\n    file = tempfile.SpooledTemporaryFile(max_size=max_size, dir=dir, prefix=pre, suffix=suf)\n    return file\n", "label": 0}
{"function": "\n\ndef mapToJson(self, objects, writer):\n    writer.write(self.header)\n    writer.write('\\n')\n    for (ind, obj) in enumerate(objects):\n        if (ind > 0):\n            writer.write(',\\n')\n        else:\n            writer.write('\\n')\n        writer.write(self.jsonDumpser(self.objConverter(obj)))\n    writer.write(self.footer)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (self is other):\n        return True\n    if (not isinstance(other, collections.Sequence)):\n        raise TypeError('Can only compare repeated scalar fields against sequences.')\n    return (other == self[slice(None, None, None)])\n", "label": 0}
{"function": "\n\ndef test_download_file_proxies_to_transfer_object(self):\n    with mock.patch('boto3.s3.inject.S3Transfer') as transfer:\n        inject.download_file(mock.sentinel.CLIENT, Bucket='bucket', Key='key', Filename='filename')\n        transfer.return_value.download_file.assert_called_with(bucket='bucket', key='key', filename='filename', extra_args=None, callback=None)\n", "label": 0}
{"function": "\n\ndef RemoveMenu(self, menu):\n    ' Remove a wx menu from the Menu.\\n\\n        If the menu does not exist in the menu, this is a no-op.\\n\\n        Parameters\\n        ----------\\n        menu : wxMenu\\n            The wxMenu instance to remove from this menu.\\n\\n        '\n    all_items = self._all_items\n    if (menu in all_items):\n        all_items.remove(menu)\n        menu.Unbind(EVT_MENU_CHANGED, handler=self.OnMenuChanged)\n        menu_item = self._menus_map.pop(menu, None)\n        if (menu_item is not None):\n            self.RemoveItem(menu_item)\n            menu_item.SetSubMenu(None)\n", "label": 0}
{"function": "\n\ndef get_latest_dist():\n    lib = file(os.path.join('petlib', '__init__.py')).read()\n    v = re.findall('VERSION.*=.*[\\'\"](.*)[\\'\"]', lib)[0]\n    return os.path.join('dist', ('petlib-%s.tar.gz' % v))\n", "label": 0}
{"function": "\n\ndef dump(self):\n    out = []\n    for key in self._keys:\n        att_key = self._att_key(key)\n        value = self[att_key]\n        if hasattr(self, ('dump_%s' % att_key)):\n            value = getattr(self, ('dump_%s' % att_key))(value)\n        out.append(('%s: %s' % (key, value)))\n    return '\\n'.join(out)\n", "label": 0}
{"function": "\n\ndef get(self, request):\n    form = bforms.PasswordResetForm()\n    self.payload['form'] = form\n    return render(request, self.payload, 'registration/reset_password.html')\n", "label": 0}
{"function": "\n\ndef test_no_repeats(self):\n    with self.assertNumQueries(2):\n        authors = Author.objects.sql_calc_found_rows().sql_calc_found_rows()[:5]\n        list(authors)\n        assert (authors.found_rows == 10)\n", "label": 0}
{"function": "\n\ndef test_default_theme_is_empty(self):\n    doc = Document()\n    for (class_name, props) in doc.theme._json['attrs'].items():\n        self._compare_dict_to_model_defaults(props, class_name)\n    self.assertEqual(0, len(doc.theme._json['attrs']))\n    self._compare_dict_to_model_class_defaults(doc.theme._fill_defaults, FillProps)\n    self.assertEqual(0, len(doc.theme._fill_defaults))\n    self._compare_dict_to_model_class_defaults(doc.theme._text_defaults, TextProps)\n    self.assertEqual(0, len(doc.theme._text_defaults))\n    self._compare_dict_to_model_class_defaults(doc.theme._line_defaults, LineProps)\n    self.assertEqual(0, len(doc.theme._line_defaults))\n", "label": 0}
{"function": "\n\ndef test_reindex():\n    s = pd.Series([0.5, 1.0, 1.5], index=[2, 1, 3])\n    s2 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n    assert (list(reindex(s, s2).values) == [1.0, 0.5, 1.5])\n", "label": 0}
{"function": "\n\ndef default(self, obj):\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    return super(_JSONEncoder, self).default(obj)\n", "label": 0}
{"function": "\n\ndef _is_numeric(self, value):\n    try:\n        int(value)\n    except (TypeError, ValueError):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef set_service_target(context, policy_target_id, relationship):\n    session = context.session\n    with session.begin(subtransactions=True):\n        owner = ServiceTarget(policy_target_id=policy_target_id, servicechain_instance_id=context.instance['id'], servicechain_node_id=context.current_node['id'], position=context.current_position, relationship=relationship)\n        session.add(owner)\n", "label": 0}
{"function": "\n\ndef __init__(self, r, color4):\n    super(ProbeQuad, self).__init__()\n    self.color4 = color4\n    self.vertexes = [(r, 0, 0), (0, r, 0), ((- r), 0, 0), (0, (- r), 0)]\n", "label": 0}
{"function": "\n\ndef format(self, value):\n    if isinstance(value, types.StringTypes):\n        return value\n    else:\n        return str(value)\n", "label": 0}
{"function": "\n\ndef find_item_before(menu, index=0):\n    _items = menu['menu'][:index][:]\n    _items.reverse()\n    for item in _items:\n        if item['enabled']:\n            return menu['menu'].index(item)\n    return find_item_before(menu, index=len(menu['menu']))\n", "label": 0}
{"function": "\n\ndef onWindowResized(self, width, height):\n    shortcutHeight = ((height - self.shortcuts.getAbsoluteTop()) - 8)\n    if (shortcutHeight < 1):\n        shortcutHeight = 1\n    self.shortcuts.setHeight(('%dpx' % shortcutHeight))\n    self.mailDetail.adjustSize(width, height)\n", "label": 0}
{"function": "\n\ndef test_ex_get_node_security_groups(self):\n    node = Node(id='1c01300f-ef97-4937-8f03-ac676d6234be', name=None, state=None, public_ips=None, private_ips=None, driver=self.driver)\n    security_groups = self.driver.ex_get_node_security_groups(node)\n    self.assertEqual(len(security_groups), 2, 'Wrong security groups count')\n    security_group = security_groups[1]\n    self.assertEqual(security_group.id, 4)\n    self.assertEqual(security_group.tenant_id, '68')\n    self.assertEqual(security_group.name, 'ftp')\n    self.assertEqual(security_group.description, 'FTP Client-Server - Open 20-21 ports')\n    self.assertEqual(security_group.rules[0].id, 1)\n    self.assertEqual(security_group.rules[0].parent_group_id, 4)\n    self.assertEqual(security_group.rules[0].ip_protocol, 'tcp')\n    self.assertEqual(security_group.rules[0].from_port, 20)\n    self.assertEqual(security_group.rules[0].to_port, 21)\n    self.assertEqual(security_group.rules[0].ip_range, '0.0.0.0/0')\n", "label": 0}
{"function": "\n\ndef test_call_chooses_correct_handler(self):\n    (sentinel1, sentinel2, sentinel3) = (object(), object(), object())\n    self.commands.add('foo')((lambda context: sentinel1))\n    self.commands.add('bar')((lambda context: sentinel2))\n    self.commands.add('baz')((lambda context: sentinel3))\n    self.assertEqual(sentinel1, self.commands.call(['foo']))\n    self.assertEqual(sentinel2, self.commands.call(['bar']))\n    self.assertEqual(sentinel3, self.commands.call(['baz']))\n", "label": 0}
{"function": "\n\ndef apply_linear(self, params, unknowns, dparams, dunknowns, dresids, mode):\n    \"\\n        Multiplies incoming vector by the Jacobian (fwd mode) or the\\n        transpose Jacobian (rev mode). If the user doesn't provide this\\n        method, then we just multiply by the cached jacobian.\\n\\n        Args\\n        ----\\n        params : `VecWrapper`\\n            `VecWrapper` containing parameters. (p)\\n\\n        unknowns : `VecWrapper`\\n            `VecWrapper` containing outputs and states. (u)\\n\\n        dparams : `VecWrapper`\\n            `VecWrapper` containing either the incoming vector in forward mode\\n            or the outgoing result in reverse mode. (dp)\\n\\n        dunknowns : `VecWrapper`\\n            In forward mode, this `VecWrapper` contains the incoming vector for\\n            the states. In reverse mode, it contains the outgoing vector for\\n            the states. (du)\\n\\n        dresids : `VecWrapper`\\n            `VecWrapper` containing either the outgoing result in forward mode\\n            or the incoming vector in reverse mode. (dr)\\n\\n        mode : string\\n            Derivative mode, can be 'fwd' or 'rev'.\\n        \"\n    self._apply_linear_jac(params, unknowns, dparams, dunknowns, dresids, mode)\n", "label": 0}
{"function": "\n\ndef test_context(self):\n    order = Order(name='Dummy Order')\n    order.save()\n    for i in range(10):\n        item = Item(name=('Item %i' % i), sku=(str(i) * 13), price=D('9.99'), order=order, status=0)\n        item.save()\n    res = self.client.get('/modelformset/simple/')\n    self.assertTrue(('object_list' in res.context))\n    self.assertEqual(len(res.context['object_list']), 10)\n", "label": 0}
{"function": "\n\ndef read_channel(model, channel_name, monitor_name='monitor'):\n    '\\n    Returns the last value recorded in a channel.\\n\\n    Parameters\\n    ----------\\n    model : Model\\n        The model to read the channel from\\n    channel_name : str\\n        The name of the channel to read from\\n    monitor_name : str, optional\\n        The name of the Monitor to read from\\n        (In case you want to read from an old Monitor moved by\\n        `push_monitor`)\\n\\n    Returns\\n    -------\\n    value : float\\n        The last value recorded in this monitoring channel\\n    '\n    return getattr(model, monitor_name).channels[channel_name].val_record[(- 1)]\n", "label": 0}
{"function": "\n\ndef test_add_listener_exception(self):\n    cap = [':candidate']\n    obj = Session(cap)\n    listener = Session(None)\n    with self.assertRaises(SessionError):\n        obj.add_listener(listener)\n", "label": 0}
{"function": "\n\ndef set_h_ffactor(self, *args, **kwargs):\n    return apply(self._cobj.set_h_ffactor, args, kwargs)\n", "label": 0}
{"function": "\n\ndef init_stroke(self, g, touch):\n    l = [touch.x, touch.y]\n    col = g.color\n    new_line = Line(points=l, width=self.line_width, group=g.id)\n    g._strokes[str(touch.uid)] = new_line\n    if self.line_width:\n        canvas_add = self.canvas.add\n        canvas_add(Color(col[0], col[1], col[2], mode='rgb', group=g.id))\n        canvas_add(new_line)\n    g.update_bbox(touch)\n    if self.draw_bbox:\n        self._update_canvas_bbox(g)\n    g.add_stroke(touch, new_line)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/item/shared_item_music_microphone_s2.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef test_scan_clear_product(self):\n    with HTTMock(wechat_api_mock):\n        res = self.client.scan.clear_product('ean13', '6900873042720')\n    self.assertEqual(0, res['errcode'])\n", "label": 0}
{"function": "\n\ndef __init__(self, dateTime, frequency):\n    super(IntraDayRange, self).__init__()\n    assert isinstance(frequency, int)\n    assert (frequency > 1)\n    assert (frequency < bar.Frequency.DAY)\n    ts = int(dt.datetime_to_timestamp(dateTime))\n    slot = int((ts / frequency))\n    slotTs = (slot * frequency)\n    self.__begin = dt.timestamp_to_datetime(slotTs, (not dt.datetime_is_naive(dateTime)))\n    if (not dt.datetime_is_naive(dateTime)):\n        self.__begin = dt.localize(self.__begin, dateTime.tzinfo)\n    self.__end = (self.__begin + datetime.timedelta(seconds=frequency))\n", "label": 0}
{"function": "\n\ndef autodiscover():\n    'Auto-discover INSTALLED_APPS mails.py modules.'\n    for app in settings.INSTALLED_APPS:\n        module = ('%s.mails' % app)\n        try:\n            import_module(module)\n        except:\n            app_module = import_module(app)\n            if module_has_submodule(app_module, 'mails'):\n                raise\n", "label": 0}
{"function": "\n\ndef setup_basic_delete_test(self, user, with_local_site, local_site_name):\n    review_request = self.create_review_request(with_local_site=with_local_site, publish=True)\n    profile = user.get_profile()\n    profile.starred_review_requests.add(review_request)\n    return (get_watched_review_request_item_url(user.username, review_request.display_id, local_site_name), [profile, review_request])\n", "label": 0}
{"function": "\n\ndef unregister_module(self, module):\n    if (module not in self.modules):\n        raise NotRegistered(('The module %s is not registered' % module.__name__))\n    del self.modules[module]\n", "label": 0}
{"function": "\n\ndef sh(cmdline, stdout=subprocess.PIPE, stderr=subprocess.PIPE):\n    'run cmd in a subprocess and return its output.\\n    raises RuntimeError on error.\\n    '\n    p = subprocess.Popen(cmdline, shell=True, stdout=stdout, stderr=stderr)\n    (stdout, stderr) = p.communicate()\n    if (p.returncode != 0):\n        raise RuntimeError(stderr)\n    if stderr:\n        warn(stderr)\n    if PY3:\n        stdout = str(stdout, sys.stdout.encoding)\n    return stdout.strip()\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.content_type == other.content_type) and (_join_b(self.iter_bytes()) == _join_b(other.iter_bytes())))\n", "label": 0}
{"function": "\n\ndef _libname(self, libpath):\n    \"Converts a full library filepath to the library's name.\\n    Ex: /path/to/libhello.a --> hello\\n    \"\n    return os.path.basename(libpath)[3:(- 2)]\n", "label": 0}
{"function": "\n\ndef test_deprecated_simple(self):\n\n    @deprecated()\n    def f(arg):\n        return arg\n    ARG = object()\n    with warnings.catch_warnings(record=True) as recorded:\n        returned = f(ARG)\n    self.assertIs(returned, ARG)\n    self.assertEqual(len(recorded), 1)\n", "label": 0}
{"function": "\n\ndef description(self, around=False):\n    if around:\n        return 'Expand Selection to Quotes'\n    else:\n        return 'Expand Selection to Quoted'\n", "label": 0}
{"function": "\n\ndef get_form(self, request, obj=None, **kwargs):\n    _thread_locals.request = request\n    _thread_locals.obj = obj\n    return super(XOSAdminMixin, self).get_form(request, obj, **kwargs)\n", "label": 0}
{"function": "\n\ndef input(self, data):\n    if ('type' in data):\n        function_name = ('process_' + data['type'])\n        self._dbg('got {}'.format(function_name))\n        for plugin in self.bot_plugins:\n            plugin.register_jobs()\n            plugin.do(function_name, data)\n", "label": 0}
{"function": "\n\ndef db_exists(database_name, **kwargs):\n    \"\\n    Find if a specific database exists on the MS SQL server.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt minion mssql.db_exists database_name='DBNAME'\\n    \"\n    return (len(tsql_query(\"SELECT database_id FROM sys.databases WHERE NAME='{0}'\".format(database_name), **kwargs)) == 1)\n", "label": 0}
{"function": "\n\ndef load_proposal_rlp(self, blockhash):\n    try:\n        prlp = self.chainservice.db.get(('blockproposal:%s' % blockhash))\n        assert isinstance(prlp, bytes)\n        return prlp\n    except KeyError:\n        return None\n", "label": 0}
{"function": "\n\ndef test_sort_mapping_reverse(self):\n    stream = DataStream(IterableDataset(self.data))\n    transformer = Mapping(stream, SortMapping(operator.itemgetter(0), reverse=True))\n    assert_equal(list(transformer.get_epoch_iterator()), list(zip(([[3, 2, 1]] * 3))))\n", "label": 0}
{"function": "\n\ndef load_train_data(self, input_data_file=''):\n    '\\n        Load train data\\n        Please check dataset/logistic_regression_train.dat to understand the data format\\n        Each feature of data x separated with spaces\\n        And the ground truth y put in the end of line separated by a space\\n        '\n    self.status = 'load_train_data'\n    if (input_data_file == ''):\n        input_data_file = os.path.normpath(os.path.join(os.path.join(os.getcwd(), os.path.dirname(__file__)), 'dataset/logistic_regression_train.dat'))\n    elif (os.path.isfile(input_data_file) is not True):\n        print('Please make sure input_data_file path is correct.')\n        return (self.train_X, self.train_Y)\n    (self.train_X, self.train_Y) = utility.DatasetLoader.load(input_data_file)\n    return (self.train_X, self.train_Y)\n", "label": 0}
{"function": "\n\ndef test_realtime_with_batch_computation(self):\n    with self._get_swap_context():\n        user_id = 'uid'\n        exp_id = 'eid'\n        self.save_new_valid_exploration(exp_id, 'owner')\n        feedback_services.create_thread(exp_id, 'a_state_name', None, 'a subject', 'some text')\n        self.process_and_flush_pending_tasks()\n        ModifiedFeedbackAnalyticsAggregator.start_computation()\n        self.assertEqual(self.count_jobs_in_taskqueue(), 1)\n        self.process_and_flush_pending_tasks()\n        ModifiedFeedbackAnalyticsAggregator.stop_computation(user_id)\n        self.assertEqual(self.count_jobs_in_taskqueue(), 0)\n        self._flush_tasks_and_check_analytics(exp_id, {\n            'num_open_threads': 1,\n            'num_total_threads': 1,\n        })\n        feedback_services.create_thread(exp_id, 'a_state_name', None, 'a subject', 'some text')\n        self._flush_tasks_and_check_analytics(exp_id, {\n            'num_open_threads': 2,\n            'num_total_threads': 2,\n        })\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    s = ('%s' % self._name)\n    if self._location:\n        s = ('%s@%s' % (s, self._location))\n    return s\n", "label": 0}
{"function": "\n\ndef test_dispose1():\n    h = event.HasEvents()\n\n    @h.connect('x1', 'x2')\n    def handler(*events):\n        pass\n    handler_ref = weakref.ref(handler)\n    del handler\n    gc.collect()\n    assert (handler_ref() is not None)\n    handler_ref().dispose()\n    gc.collect()\n    assert (handler_ref() is None)\n", "label": 0}
{"function": "\n\ndef test_add(self):\n    'Test that we can add an image via the s3 backend'\n    expected_image_id = utils.generate_uuid()\n    expected_s3_size = FIVE_KB\n    expected_s3_contents = ('*' * expected_s3_size)\n    expected_checksum = hashlib.md5(expected_s3_contents).hexdigest()\n    expected_location = format_s3_location(S3_CONF['s3_store_access_key'], S3_CONF['s3_store_secret_key'], S3_CONF['s3_store_host'], S3_CONF['s3_store_bucket'], expected_image_id)\n    image_s3 = StringIO.StringIO(expected_s3_contents)\n    (location, size, checksum) = self.store.add(expected_image_id, image_s3, expected_s3_size)\n    self.assertEquals(expected_location, location)\n    self.assertEquals(expected_s3_size, size)\n    self.assertEquals(expected_checksum, checksum)\n    loc = get_location_from_uri(expected_location)\n    (new_image_s3, new_image_size) = self.store.get(loc)\n    new_image_contents = StringIO.StringIO()\n    for chunk in new_image_s3:\n        new_image_contents.write(chunk)\n    new_image_s3_size = new_image_contents.len\n    self.assertEquals(expected_s3_contents, new_image_contents.getvalue())\n    self.assertEquals(expected_s3_size, new_image_s3_size)\n", "label": 0}
{"function": "\n\ndef move_cat(self):\n    speed = random.randint(20, 200)\n    self.cat_body.angle -= random.randint((- 1), 1)\n    direction = Vec2d(1, 0).rotated(self.cat_body.angle)\n    self.cat_body.velocity = (speed * direction)\n", "label": 0}
{"function": "\n\ndef job_status(self, job_id=None):\n    job_id = (job_id or self.lookup_job_id(batch_id))\n    uri = urlparse.urljoin((self.endpoint + '/'), 'job/{0}'.format(job_id))\n    response = requests.get(uri, headers=self.headers())\n    if (response.status_code != 200):\n        self.raise_error(response.content, response.status_code)\n    tree = ET.fromstring(response.content)\n    result = {\n        \n    }\n    for child in tree:\n        result[re.sub('{.*?}', '', child.tag)] = child.text\n    return result\n", "label": 0}
{"function": "\n\ndef test_issue(self):\n    'Show that one can retrieve the associated issue of a PR.'\n    cassette_name = self.cassette_name('issue')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        issue = p.issue()\n        assert isinstance(issue, github3.issues.Issue)\n", "label": 0}
{"function": "\n\ndef __init__(self, view=False):\n    self.view = view\n    if self.view:\n        self.view_map = {\n            0: [0],\n        }\n", "label": 0}
{"function": "\n\ndef replace_body(self, body, payload):\n    if (body is None):\n        return body\n    if (self.fsig in body):\n        return body.replace(self.fsig, urllib.quote_plus(payload))\n    template_sig = self.template_signature(body)\n    if template_sig:\n        tp = TemplateParser()\n        tp.set_payload(payload)\n        new_payload = repr(tp.transform(self.template_signature(body), self.sig))[1:(- 1)]\n        return body.replace(template_sig, new_payload)\n    return body\n", "label": 0}
{"function": "\n\ndef delete(self, image):\n    \"\\n        Delete an image.\\n        \\n        It should go without saying that you can't delete an image \\n        that you didn't create.\\n        \\n        :param image: The :class:`Image` (or its ID) to delete.\\n        \"\n    self._delete(('/images/%s' % base.getid(image)))\n", "label": 0}
{"function": "\n\ndef load_parent(parent_id):\n    parent = Node.load(parent_id)\n    if (parent is None):\n        return None\n    parent_info = {\n        \n    }\n    if ((parent is not None) and parent.is_public):\n        parent_info['title'] = parent.title\n        parent_info['url'] = parent.url\n        parent_info['is_registration'] = parent.is_registration\n        parent_info['id'] = parent._id\n    else:\n        parent_info['title'] = '-- private project --'\n        parent_info['url'] = ''\n        parent_info['is_registration'] = None\n        parent_info['id'] = None\n    return parent_info\n", "label": 0}
{"function": "\n\ndef test_sys_stderr_should_have_no_output_when_no_logger_is_set(memcached):\n    mc = cmemcached.Client([memcached])\n    with patch('sys.stderr') as mock_stderr:\n        mc.get('test_key_with_no_logger')\n        mc.set('test_key_with_no_logger', 'test_value_with_no_logger')\n        assert (not mock_stderr.write.called)\n", "label": 0}
{"function": "\n\ndef write(self, filename, content):\n    create_dirs(self.webd, dirname(filename))\n    buff = BytesIO(content)\n    self.webd.upload_from(buff, b(filename))\n", "label": 0}
{"function": "\n\ndef test_gtkApplicationActivate(self):\n    '\\n        L{Gtk.Application} instances can be registered with a gtk3reactor.\\n        '\n    reactor = gtk3reactor.Gtk3Reactor()\n    self.addCleanup(self.unbuildReactor, reactor)\n    app = Gtk.Application(application_id='com.twistedmatrix.trial.gtk3reactor', flags=Gio.ApplicationFlags.FLAGS_NONE)\n    self.runReactor(app, reactor)\n", "label": 0}
{"function": "\n\ndef test_patch_mocksignature_callable(self):\n    original_something = something\n    something_name = ('%s.something' % __name__)\n\n    @patch(something_name, mocksignature=True)\n    def test(MockSomething):\n        something(3, 4)\n        MockSomething.assert_called_with(3, 4)\n        something(6)\n        MockSomething.assert_called_with(6, 5)\n        self.assertRaises(TypeError, something)\n    test()\n    self.assertIs(something, original_something)\n", "label": 0}
{"function": "\n\ndef __init__(self, allure_helper, title):\n    self.allure_helper = allure_helper\n    self.title = title\n    self.step = None\n", "label": 0}
{"function": "\n\ndef load_xml_config(self, path=None):\n    if (path is not None):\n        self.path = path\n    if (not os.path.isfile(self.path)):\n        raise KaresasnuiServiceConfigParamException(('service.xml not found. path=%s' % str(self.path)))\n    document = XMLParse(self.path)\n    self.services = []\n    service_num = XMLXpathNum(document, '/services/service')\n    for n in xrange(1, (service_num + 1)):\n        system_name = XMLXpath(document, ('/services/service[%i]/system/name/text()' % n))\n        system_command = XMLXpath(document, ('/services/service[%i]/system/command/text()' % n))\n        system_readonly = XMLXpath(document, ('/services/service[%i]/system/readonly/text()' % n))\n        display_name = XMLXpath(document, ('/services/service[%i]/display/name/text()' % n))\n        display_description = XMLXpath(document, ('/services/service[%i]/display/description/text()' % n))\n        self.add_service(str(system_name), str(system_command), str(system_readonly), str(display_name), str(display_description))\n", "label": 0}
{"function": "\n\ndef _should_create_constraint(self, compiler):\n    return (not compiler.dialect.supports_native_boolean)\n", "label": 0}
{"function": "\n\ndef put_object(self, name, fp, metadata):\n    '\\n        Store object into memory\\n\\n        :param name: standard object name\\n        :param fp: `StringIO` in-memory representation object\\n        :param metadata: dictionary of metadata to be written\\n        '\n    self._filesystem[name] = (fp, metadata)\n", "label": 0}
{"function": "\n\ndef _raise_test_exc(self, exc_msg):\n    raise TestException(exc_msg)\n", "label": 0}
{"function": "\n\ndef install_inplace(pkg):\n    'Install scripts of pkg in the current directory.'\n    for (basename, executable) in pkg.executables.items():\n        version_str = '.'.join([str(i) for i in sys.version_info[:2]])\n        scripts_node = root._ctx.srcnode\n        for name in [basename, ('%s-%s' % (basename, version_str))]:\n            nodes = _create_executable(name, executable, scripts_node)\n            installed = ','.join([n.path_from(scripts_node) for n in nodes])\n            pprint('GREEN', ('installing %s in current directory' % installed))\n", "label": 0}
{"function": "\n\ndef testSuccess(self):\n    vor = rapi.testutils.VerifyOpResult\n    vor(opcodes.OpClusterVerify.OP_ID, {\n        constants.JOB_IDS_KEY: [(False, 'error message')],\n    })\n", "label": 0}
{"function": "\n\ndef kilobyte(self, value=None):\n    return self.convertb(value, self.byte)\n", "label": 0}
{"function": "\n\ndef _irfft_out_chunks(a, n, axis):\n    if (n is None):\n        n = (2 * (a.chunks[axis][0] - 1))\n    chunks = list(a.chunks)\n    chunks[axis] = (n,)\n    return chunks\n", "label": 0}
{"function": "\n\n@skipUnless(cache_file_exists('ia', '20100608__ia__primary__adair__precinct.xls'), CACHED_FILE_MISSING_MSG)\ndef test_results(self):\n    filename = '20100608__ia__primary__adair__precinct.xls'\n    mapping = self._get_mapping(filename)\n    self._prep_loader_attrs(mapping)\n    results = self.loader._results(mapping)\n    us_rep_dist_5_rep_results = [r for r in results if ((r.office == 'U.S. REPRESENTATIVE') and (r.district == '5') and (r.primary_party == 'REPUBLICAN'))]\n    self.assertEqual(len(us_rep_dist_5_rep_results), 35)\n    result = us_rep_dist_5_rep_results[0]\n    self.assertEqual(result.source, mapping['generated_filename'])\n    self.assertEqual(result.election_id, mapping['election'])\n    self.assertEqual(result.state, 'IA')\n    self.assertEqual(result.election_type, 'primary')\n    self.assertEqual(result.district, '5')\n    self.assertEqual(result.party, 'REPUBLICAN')\n    self.assertEqual(result.jurisdiction, '1 NW')\n    self.assertEqual(result.reporting_level, 'precinct')\n    self.assertEqual(result.full_name, 'STEVE KING')\n    self.assertEqual(result.votes, 123)\n", "label": 0}
{"function": "\n\ndef get_context_data(self, **kwargs):\n    ctx = super(AddressCreateView, self).get_context_data(**kwargs)\n    ctx['title'] = _('Add a new address')\n    return ctx\n", "label": 0}
{"function": "\n\ndef find_by_selector(self, selector, search_regions=None):\n    search_regions = (search_regions or self.regions)\n    return GrammarParser.filter_by_selector(selector, search_regions)\n", "label": 0}
{"function": "\n\ndef nova(context):\n    global _nova_api_version\n    if (not _nova_api_version):\n        _nova_api_version = _get_nova_api_version(context)\n    clnt = novaclient.Client(_nova_api_version, session=context.session, service_type=CONF.nova_service_type)\n    if (not hasattr(clnt.client, 'last_request_id')):\n        setattr(clnt.client, 'last_request_id', None)\n    return clnt\n", "label": 0}
{"function": "\n\ndef iteritems(self):\n    for tag in self.tags:\n        (yield (tag.name, tag))\n", "label": 0}
{"function": "\n\ndef split_multiline(value):\n    value = [element for element in (line.strip() for line in value.split('\\n')) if element]\n    return value\n", "label": 0}
{"function": "\n\ndef get_tags_count(journal):\n    'Returns a set of tuples (count, tag) for all tags present in the journal.'\n    tags = [tag for entry in journal.entries for tag in set(entry.tags)]\n    tag_counts = set([(tags.count(tag), tag) for tag in tags])\n    return tag_counts\n", "label": 0}
{"function": "\n\ndef vmstats():\n    \"\\n    Return the virtual memory stats for this minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' status.vmstats\\n    \"\n\n    def linux_vmstats():\n        '\\n        linux specific implementation of vmstats\\n        '\n        procf = '/proc/vmstat'\n        if (not os.path.isfile(procf)):\n            return {\n                \n            }\n        stats = salt.utils.fopen(procf, 'r').read().splitlines()\n        ret = {\n            \n        }\n        for line in stats:\n            if (not line):\n                continue\n            comps = line.split()\n            ret[comps[0]] = _number(comps[1])\n        return ret\n\n    def freebsd_vmstats():\n        '\\n        freebsd specific implementation of vmstats\\n        '\n        ret = {\n            \n        }\n        for line in __salt__['cmd.run']('vmstat -s').splitlines():\n            comps = line.split()\n            if comps[0].isdigit():\n                ret[' '.join(comps[1:])] = _number(comps[0])\n        return ret\n    get_version = {\n        'Linux': linux_vmstats,\n        'FreeBSD': freebsd_vmstats,\n    }\n    errmsg = 'This method is unsupported on the current operating system!'\n    return get_version.get(__grains__['kernel'], (lambda : errmsg))()\n", "label": 0}
{"function": "\n\ndef test_get_children_duplicates(self):\n    from psutil._compat import defaultdict\n    table = defaultdict(int)\n    for p in psutil.process_iter():\n        try:\n            table[p.ppid] += 1\n        except psutil.Error:\n            pass\n    pid = max(sorted(table, key=(lambda x: table[x])))\n    p = psutil.Process(pid)\n    try:\n        c = p.get_children(recursive=True)\n    except psutil.AccessDenied:\n        pass\n    else:\n        self.assertEqual(len(c), len(set(c)))\n", "label": 0}
{"function": "\n\ndef __call__(self, driver):\n    try:\n        return _element_if_visible(_find_element(driver, self.locator))\n    except StaleElementReferenceException:\n        return False\n", "label": 0}
{"function": "\n\ndef _test_update_routing_table(self, is_snat_host=True):\n    router = l3_test_common.prepare_router_data()\n    uuid = router['id']\n    s_netns = ('snat-' + uuid)\n    q_netns = ('qrouter-' + uuid)\n    fake_route1 = {\n        'destination': '135.207.0.0/16',\n        'nexthop': '19.4.4.200',\n    }\n    calls = [mock.call('replace', fake_route1, q_netns)]\n    agent = l3_agent.L3NATAgent(HOSTNAME, self.conf)\n    ri = dvr_router.DvrEdgeRouter(agent, HOSTNAME, uuid, router, **self.ri_kwargs)\n    ri._update_routing_table = mock.Mock()\n    with mock.patch.object(ri, '_is_this_snat_host') as snat_host:\n        snat_host.return_value = is_snat_host\n        ri.update_routing_table('replace', fake_route1)\n        if is_snat_host:\n            ri._update_routing_table('replace', fake_route1, s_netns)\n            calls += [mock.call('replace', fake_route1, s_netns)]\n        ri._update_routing_table.assert_has_calls(calls, any_order=True)\n", "label": 0}
{"function": "\n\ndef install_ssl_certs(instances):\n    certs = []\n    if CONF.object_store_access.public_identity_ca_file:\n        certs.append(CONF.object_store_access.public_identity_ca_file)\n    if CONF.object_store_access.public_object_store_ca_file:\n        certs.append(CONF.object_store_access.public_object_store_ca_file)\n    if (not certs):\n        return\n    with context.ThreadGroup() as tg:\n        for inst in instances:\n            tg.spawn(('configure-ssl-cert-%s' % inst.instance_id), _install_ssl_certs, inst, certs)\n", "label": 0}
{"function": "\n\ndef test_node_site():\n    s = Site(TEST_SITE_ROOT)\n    r = RootNode(TEST_SITE_ROOT.child_folder('content'), s)\n    assert (r.site == s)\n    n = Node(r.source_folder.child_folder('blog'), r)\n    assert (n.site == s)\n", "label": 0}
{"function": "\n\ndef render(self, name, value, attrs=None):\n    value = util.serialize_references(value)\n    return super(ReferencesFieldWidget, self).render(name, value, attrs)\n", "label": 0}
{"function": "\n\ndef on_request(self, context, request):\n    if ('PowerView.ps1' == request.path[1:]):\n        request.send_response(200)\n        request.end_headers()\n        with open('data/PowerSploit/Recon/PowerView.ps1', 'r') as ps_script:\n            ps_script = obfs_ps_script(ps_script.read())\n            request.wfile.write(ps_script)\n    else:\n        request.send_response(404)\n        request.end_headers()\n", "label": 0}
{"function": "\n\ndef test_column_expr(self):\n    c = Column('x', Integer)\n    is_(inspect(c), c)\n    assert (not c.is_selectable)\n    assert (not hasattr(c, 'selectable'))\n", "label": 0}
{"function": "\n\ndef inline_assets(self, base_path, content):\n    for type in self.asset_types:\n        for (statement, path) in self.get_matches(type['pattern'], base_path, content):\n            asset_content = self.get_binary_file_contents(path)\n            encoded_content = urllib.quote(base64.encodestring(asset_content))\n            new_statement = ('url(data:%s;base64,%s)' % (type['mime'], encoded_content))\n            content = content.replace(statement, new_statement)\n    return content\n", "label": 0}
{"function": "\n\ndef test_choice_update(self):\n    self.choice.choice_text = 'third text'\n    self.choice.save()\n    p = Choice.objects.get()\n    self.assertEqual(p.choice_text, 'third text')\n", "label": 0}
{"function": "\n\ndef abort_run(self, drain=False):\n    self._aborting_run = drain\n", "label": 0}
{"function": "\n\ndef test_oldPythonPy3(self):\n    '\\n        L{_checkRequirements} raises L{ImportError} when run on a version of\\n        Python that is too old.\\n        '\n    sys.version_info = self.Py3unsupportedPythonVersion\n    with self.assertRaises(ImportError) as raised:\n        _checkRequirements()\n    self.assertEqual(('Twisted on Python 3 requires Python %d.%d or later.' % self.Py3supportedPythonVersion), str(raised.exception))\n", "label": 0}
{"function": "\n\ndef get_oauth_request(request):\n    ' Converts a Django request object into an `oauth2.Request` object. '\n    headers = {\n        \n    }\n    if ('HTTP_AUTHORIZATION' in request.META):\n        headers['Authorization'] = request.META['HTTP_AUTHORIZATION']\n    return oauth.Request.from_request(request.method, request.build_absolute_uri(request.path), headers, dict(request.REQUEST))\n", "label": 0}
{"function": "\n\n@property\ndef vcf(self):\n    'serialize to VCARD as specified in RFC2426,\\n        if no UID is specified yet, one will be added (as a UID is mandatory\\n        for carddav as specified in RFC6352\\n        TODO make shure this random uid is unique'\n    import string\n    import random\n\n    def generate_random_uid():\n        \"generate a random uid, when random isn't broken, getting a\\n            random UID from a pool of roughly 10^56 should be good enough\"\n        choice = (string.ascii_uppercase + string.digits)\n        return ''.join([random.choice(choice) for _ in range(36)])\n    if ('UID' not in self.keys()):\n        self['UID'] = [(generate_random_uid(), dict())]\n    collector = list()\n    collector.append('BEGIN:VCARD')\n    collector.append('VERSION:3.0')\n    for key in ['FN', 'N']:\n        try:\n            collector.append(((key + ':') + self[key][0][0]))\n        except IndexError:\n            collector.append((key + ':'))\n    for prop in self.alt_keys():\n        for line in self[prop]:\n            types = self._line_helper(line)\n            collector.append((((prop + types) + ':') + line[0]))\n    collector.append('END:VCARD')\n    return '\\n'.join(collector)\n", "label": 0}
{"function": "\n\ndef close_review_request(server_url, username, password, review_request_id, description):\n    'Closes the specified review request as submitted.'\n    (api_client, api_root) = get_api(server_url, username, password)\n    review_request = get_review_request(review_request_id, api_root)\n    if (review_request.status == SUBMITTED):\n        logging.warning('Review request #%s is already %s.', review_request_id, SUBMITTED)\n        return\n    if description:\n        review_request = review_request.update(status=SUBMITTED, description=description)\n    else:\n        review_request = review_request.update(status=SUBMITTED)\n    print(('Review request #%s is set to %s.' % (review_request_id, review_request.status)))\n", "label": 0}
{"function": "\n\ndef GetLastRequestTimedelta(api_query, from_time=None):\n    'Returns how long since the API Query response was last requested.\\n\\n  Args:\\n    api_query: The API Query from which to retrieve the last request timedelta.\\n    from_time: A DateTime object representing the start time to calculate the\\n               timedelta from.\\n\\n  Returns:\\n    A string that describes how long since the API Query response was last\\n    requested in the form of \"HH hours, MM minutes, ss seconds ago\" or None\\n    if the API Query response has never been requested.\\n  '\n    if (not from_time):\n        from_time = datetime.utcnow()\n    if api_query.last_request:\n        time_delta = (from_time - api_query.last_request)\n        return FormatTimedelta(time_delta)\n    return None\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/particle/shared_particle_test_16.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef __init__(self, name):\n    Exception.__init__(self, (\"Method not found: '%s'\" % name))\n", "label": 0}
{"function": "\n\ndef testUnshareSecondLevelRemoved(self):\n    'Re-share photos, remove the reshared viewpoint, then unshare the source viewpoint.'\n    (child_vp_id, child_ep_ids) = self._tester.ShareNew(self._cookie2, [(self._new_ep_id, self._photo_ids)], [self._user3.user_id], **self._CreateViewpointDict(self._cookie2))\n    self._tester.RemoveViewpoint(self._cookie3, child_vp_id)\n    self._tester.Unshare(self._cookie, self._new_vp_id, [(self._new_ep_id, self._photo_ids[:1])])\n", "label": 0}
{"function": "\n\ndef test_search_comment(self):\n    result = self.search(comment=['fantastic'])\n    self.assertEqual(list(result[0].tracks), self.tracks[3:4])\n    result = self.search(comment=['antasti'])\n    self.assertEqual(list(result[0].tracks), self.tracks[3:4])\n", "label": 0}
{"function": "\n\n@converts('ImageField')\ndef conv_Image(self, model, field, kwargs):\n    return f.FileField(**kwargs)\n", "label": 0}
{"function": "\n\ndef parse(self, text):\n    return [ErrorLine(m) for m in self.regex.finditer(text)]\n", "label": 0}
{"function": "\n\ndef clamp_vect(self, v):\n    'Returns a copy of the vector v clamped to the bounding box'\n    return cpffi.cpBBClampVect(self._bb, v)\n", "label": 0}
{"function": "\n\ndef __exit__(self, *args):\n    self.delegate.disconnect()\n", "label": 0}
{"function": "\n\n@view_config(context='velruse.AuthenticationComplete', renderer='{}:templates/result.mako'.format(__name__))\ndef login_complete_view(request):\n    context = request.context\n    result = {\n        'profile': context.profile,\n        'credentials': context.credentials,\n    }\n    return {\n        'result': json.dumps(result, indent=4),\n    }\n", "label": 0}
{"function": "\n\ndef emit(self, *args, **kwargs):\n    try:\n        self.__emitting = True\n        for handler in self.__handlers:\n            handler(*args, **kwargs)\n    finally:\n        self.__emitting = False\n        self.__applyChanges()\n", "label": 0}
{"function": "\n\ndef test_existing_spawn(self):\n    child = pexpect.spawnu('bash', timeout=5, echo=False)\n    repl = replwrap.REPLWrapper(child, re.compile('[$#]'), \"PS1='{0}' PS2='{1}' PROMPT_COMMAND=''\")\n    res = repl.run_command('echo $HOME')\n    assert res.startswith('/'), res\n", "label": 0}
{"function": "\n\ndef child_removed(self, child):\n    ' Handle the child removed event for a QtWindow.\\n\\n        '\n    if isinstance(child, WxContainer):\n        self.widget.SetCentralWidget(self.central_widget())\n", "label": 0}
{"function": "\n\ndef test_stubs(self):\n    df = pd.DataFrame([[0, 1, 2, 3, 8], [4, 5, 6, 7, 9]])\n    df.columns = ['id', 'inc1', 'inc2', 'edu1', 'edu2']\n    stubs = ['inc', 'edu']\n    df_long = pd.wide_to_long(df, stubs, i='id', j='age')\n    self.assertEqual(stubs, ['inc', 'edu'])\n", "label": 0}
{"function": "\n\ndef collectstreamuuid(self, streamname):\n    if (not streamname):\n        return\n    shouter.shout(('Get UUID of configured stream ' + streamname))\n    showuuidcommand = ('%s --show-alias n --show-uuid y show attributes -r %s -w %s' % (self.scmcommand, self.repo, streamname))\n    output = shell.getoutput(showuuidcommand)\n    splittedfirstline = output[0].split(' ')\n    streamuuid = splittedfirstline[0].strip()[1:(- 1)]\n    return streamuuid\n", "label": 0}
{"function": "\n\n@classmethod\ndef resource_uri(cls, obj=None):\n    object_id = 'id'\n    if (obj is not None):\n        object_id = obj.id\n    return ('api_events', [object_id])\n", "label": 0}
{"function": "\n\ndef __init__(self, text):\n    if (len(text) >= 10000):\n        text = (text[:9995] + '\\n...')\n    self.text = text\n    self.recipient = None\n", "label": 0}
{"function": "\n\ndef info(self, msg_format, *values):\n    'For progress and other informative messages.'\n    if (len(values) > 0):\n        msg_format = (msg_format % values)\n    print(msg_format)\n", "label": 0}
{"function": "\n\ndef freeze(self, skipSet=None):\n    assert (len(self()) in self.allowedSize)\n    return StringStream.freeze(self, skipSet=skipSet)\n", "label": 0}
{"function": "\n\ndef get_result(self, vlan_range_len):\n    self.intersect()\n    if (vlan_range_len > 1):\n        return self.get_final_available_vlan_range(vlan_range_len)\n    else:\n        return self.get_final_available_vlan()\n", "label": 0}
{"function": "\n\ndef __init__(self, collector, callback=None, *args, **kw):\n    '\\n        Create a pager with a Reference to a remote collector and\\n        an optional callable to invoke upon completion.\\n        '\n    if callable(callback):\n        self.callback = callback\n        self.callbackArgs = args\n        self.callbackKeyword = kw\n    else:\n        self.callback = None\n    self._stillPaging = 1\n    self.collector = collector\n    collector.broker.registerPageProducer(self)\n", "label": 0}
{"function": "\n\ndef __init__(self, name, consumer_key, consumer_secret):\n    self.name = name\n    self.type = 'twitter'\n    self.consumer_key = consumer_key\n    self.consumer_secret = consumer_secret\n    self.login_route = ('velruse.%s-login' % name)\n    self.callback_route = ('velruse.%s-callback' % name)\n", "label": 0}
{"function": "\n\n@mock.patch(('%s.flavors.osclients.Clients' % CTX))\ndef test_cleanup(self, mock_clients):\n    real_context = {\n        'flavors': {\n            'flavor_name': {\n                'flavor_name': 'flavor_name',\n                'id': 'flavor_name',\n            },\n        },\n        'admin': {\n            'credential': mock.MagicMock(),\n        },\n        'task': mock.MagicMock(),\n    }\n    flavors_ctx = flavors.FlavorsGenerator(real_context)\n    flavors_ctx.cleanup()\n    mock_clients.assert_called_with(real_context['admin']['credential'])\n    mock_flavors_delete = mock_clients().nova().flavors.delete\n    mock_flavors_delete.assert_called_with('flavor_name')\n", "label": 0}
{"function": "\n\ndef get_all_vms(self):\n    '\\n        Returns a generator over all VMs known to this vCenter host.\\n        '\n    for folder in self.get_first_level_of_vm_folders():\n        for vm in get_all_vms_in_folder(folder):\n            (yield vm)\n", "label": 0}
{"function": "\n\ndef wait_for_responses(self, client):\n    'Waits for all responses to come back and resolves the\\n        eventual results.\\n        '\n    assert_open(self)\n    if self.has_pending_requests:\n        raise RuntimeError('Cannot wait for responses if there are pending requests outstanding.  You need to wait for pending requests to be sent first.')\n    pending = self.pending_responses\n    self.pending_responses = []\n    for (command_name, promise) in pending:\n        value = client.parse_response(self.connection, command_name)\n        promise.resolve(value)\n", "label": 0}
{"function": "\n\ndef test_write_job3():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'write': ['test.test', 'test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef __update_copyright(self):\n    'Finds the copyright text and replaces it.'\n    region = self.__find_copyright()\n    self.__replace_copyright(region)\n", "label": 0}
{"function": "\n\ndef __init__(self, idx):\n    self.idx = _uidx()\n    self.isBatch = False\n    self.isSeq = True\n    if isinstance(idx, BaseArray):\n        arr = ct.c_void_p(0)\n        if (idx.type() == Dtype.b8.value):\n            safe_call(backend.get().af_where(ct.pointer(arr), idx.arr))\n        else:\n            safe_call(backend.get().af_retain_array(ct.pointer(arr), idx.arr))\n        self.idx.arr = arr\n        self.isSeq = False\n    elif isinstance(idx, ParallelRange):\n        self.idx.seq = idx\n        self.isBatch = True\n    else:\n        self.idx.seq = Seq(idx)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/armor/component/shared_deflector_shield_generator_energy_ray.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef leq(levels, int_time=1.0):\n    '\\n    Equivalent level :math:`L_{eq}`.\\n    \\n    :param levels: Levels as function of time.\\n    :param int_time: Integration time. Default value is 1.0 second.\\n    :returns: Equivalent level L_{eq}.\\n    \\n    Sum of levels in dB.\\n    '\n    levels = np.asarray(levels)\n    time = (levels.size * int_time)\n    return _leq(levels, time)\n", "label": 0}
{"function": "\n\ndef get_next_instruction(self):\n    dis = self.disassemble(address=self.program_counter()[1], count=1)\n    return dis.partition('\\n')[0].strip()\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/munition/shared_detonator_thermal_imperial_issue.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef get_dates(self, resource):\n    '\\n        Retrieve dates from mercurial\\n        '\n    try:\n        commits = subprocess.check_output(['hg', 'log', '--template={date|isodatesec}\\n', resource.path]).split('\\n')\n        commits = commits[:(- 1)]\n    except subprocess.CalledProcessError:\n        self.logger.warning(('Unable to get mercurial history for [%s]' % resource))\n        commits = None\n    if (not commits):\n        self.logger.warning(('No mercurial history for [%s]' % resource))\n        return (None, None)\n    created = parse(commits[(- 1)].strip())\n    modified = parse(commits[0].strip())\n    return (created, modified)\n", "label": 0}
{"function": "\n\ndef test_tx_out_bitcoin_address(self):\n    coinbase_bytes = h2b('04ed66471b02c301')\n    tx = Tx.coinbase_tx(COINBASE_PUB_KEY_FROM_80971, int((50 * 100000000.0)), COINBASE_BYTES_FROM_80971)\n    self.assertEqual(tx.txs_out[0].bitcoin_address(), '1DmapcnrJNGeJB13fv9ngRFX1iRvR4zamn')\n", "label": 0}
{"function": "\n\ndef alert_smtp(alert, metric):\n    if ('@' in alert[1]):\n        sender = settings.ALERT_SENDER\n        recipient = alert[1]\n    else:\n        sender = settings.SMTP_OPTS['sender']\n        recipients = settings.SMTP_OPTS['recipients'][alert[0]]\n    if (type(recipients) is str):\n        recipients = [recipients]\n    for recipient in recipients:\n        msg = MIMEMultipart('alternative')\n        msg['Subject'] = ('[skyline alert] ' + metric[1])\n        msg['From'] = sender\n        msg['To'] = recipient\n        link = (settings.GRAPH_URL % metric[1])\n        body = ('Anomalous value: %s <br> Next alert in: %s seconds <a href=\"%s\"><img src=\"%s\"/></a>' % (metric[0], alert[2], link, link))\n        msg.attach(MIMEText(body, 'html'))\n        s = SMTP('127.0.0.1')\n        s.sendmail(sender, recipient, msg.as_string())\n        s.quit()\n", "label": 0}
{"function": "\n\ndef delete_key(self, key):\n    self.server.request('delete', ('/keys/%s' % key))\n", "label": 0}
{"function": "\n\ndef _create_placeholders(self, n_features, n_classes):\n    ' Create the TensorFlow placeholders for the model.\\n        :param n_features: number of features of the first layer\\n        :param n_classes: number of classes\\n        :return: self\\n        '\n    self.keep_prob = tf.placeholder('float')\n    self.hrand = [tf.placeholder('float', [None, self.layers[(l + 1)]]) for l in range((self.n_layers - 1))]\n    self.vrand = [tf.placeholder('float', [None, self.layers[l]]) for l in range((self.n_layers - 1))]\n    self.x = tf.placeholder('float', [None, n_features])\n    self.y_ = tf.placeholder('float', [None, n_classes])\n", "label": 0}
{"function": "\n\ndef __init__(self, name_suggestion):\n    self.name_suggestion = name_suggestion\n", "label": 0}
{"function": "\n\ndef render_datalist(self, list_id):\n    return ''.join([('<datalist id=\"%s\">' % list_id), ''.join([('<option>%s</option>' % color) for color in self.colors]), '</datalist>'])\n", "label": 0}
{"function": "\n\ndef _do_remove(self, section, option):\n    if (not self.config.has_option(section, option)):\n        raise AdminCommandError(_(\"Option '%(option)s' doesn't exist in section '%(section)s'\", option=option, section=section))\n    self.config.remove(section, option)\n    self.config.save()\n    if ((section == 'inherit') and (option == 'file')):\n        self.config.parse_if_needed(force=True)\n", "label": 0}
{"function": "\n\ndef _create_flavor(self, description=None):\n    flavor = {\n        'flavor': {\n            'name': 'GOLD',\n            'service_type': constants.DUMMY,\n            'description': (description or 'the best flavor'),\n            'enabled': True,\n        },\n    }\n    return (self.plugin.create_flavor(self.ctx, flavor), flavor)\n", "label": 0}
{"function": "\n\ndef test_logout(self):\n    'Tests when logging out with and without continue URL.'\n    host = 'foo.com:1234'\n    path_info = '/_ah/login'\n    cookie_dict = {\n        'dev_appserver_login': ('%s:False:%s' % (EMAIL, USER_ID)),\n    }\n    action = 'Logout'\n    set_email = ''\n    set_admin = False\n    continue_url = ''\n    expected_set = login._clear_user_info_cookie().strip()\n    (status, location, set_cookie, _) = self._run_test(host, path_info, cookie_dict, action, set_email, set_admin, continue_url)\n    self.assertEqual(302, status)\n    self.assertEqual(('http://%s%s' % (host, path_info)), location)\n    self.assertEqual(expected_set, set_cookie)\n    self.assertIsInstance(location, str)\n    self.assertIsInstance(set_cookie, str)\n    continue_url = 'http://foo.com/blah'\n    (status, location, set_cookie, _) = self._run_test(host, path_info, cookie_dict, action, set_email, set_admin, continue_url)\n    self.assertEqual(302, status)\n    self.assertEqual(continue_url, location)\n    self.assertEqual(expected_set, set_cookie)\n    self.assertIsInstance(location, str)\n    self.assertIsInstance(set_cookie, str)\n", "label": 0}
{"function": "\n\ndef test04c__getitem__(self):\n    'Checking cols.__getitem__() with subgroups with a range index with\\n        step.'\n    tbl = self.h5file.create_table('/', 'test', self._TestTDescr, title=self._getMethodName())\n    tbl.append(self._testAData)\n    if self.reopen:\n        self._reopen()\n        tbl = self.h5file.root.test\n    nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)\n    tblcols = tbl.cols._f_col('Info')[0::2]\n    nrarrcols = nrarr['Info'][0::2]\n    if common.verbose:\n        print('Read cols:', tblcols)\n        print('Should look like:', nrarrcols)\n    self.assertTrue(common.areArraysEqual(nrarrcols, tblcols), \"Original array are retrieved doesn't match.\")\n", "label": 0}
{"function": "\n\ndef test_key_has_correct_repr(self):\n    '\\n        Calling repr on a Key instance returns the proper string.\\n        '\n    key = pem.Key(b'test')\n    assert ('<Key({0})>'.format(TEST_DIGEST) == repr(key))\n", "label": 0}
{"function": "\n\ndef calcScale(self, testCount):\n    import math\n    scale = int((self.size / (math.sqrt(testCount) + 1)))\n    return scale\n", "label": 0}
{"function": "\n\ndef get_repository_info(self):\n    '\\n        Find out information about the current Bazaar branch (if any) and\\n        return it.\\n        '\n    if (not check_install(['bzr', 'help'])):\n        logging.debug('Unable to execute \"bzr help\": skipping Bazaar')\n        return None\n    bzr_info = execute(['bzr', 'info'], ignore_errors=True)\n    if ('ERROR: Not a branch:' in bzr_info):\n        repository_info = None\n    else:\n        branch_match = re.search(self.BRANCH_REGEX, bzr_info, re.MULTILINE)\n        path = branch_match.group('branch_path')\n        if (path == '.'):\n            path = os.getcwd()\n        repository_info = RepositoryInfo(path=path, base_path='/', supports_parent_diffs=True)\n    return repository_info\n", "label": 0}
{"function": "\n\ndef do_access_token_response(self, access_token, atinfo, state, refresh_token=None):\n    _tinfo = {\n        'access_token': access_token,\n        'expires_in': atinfo['exp'],\n        'token_type': 'bearer',\n        'state': state,\n    }\n    try:\n        _tinfo['scope'] = atinfo['scope']\n    except KeyError:\n        pass\n    if refresh_token:\n        _tinfo['refresh_token'] = refresh_token\n    return AccessTokenResponse(**by_schema(AccessTokenResponse, **_tinfo))\n", "label": 0}
{"function": "\n\ndef test_set_messages_success(self):\n    author = {\n        'name': 'John Doe',\n        'slug': 'success-msg',\n    }\n    add_url = reverse('add_success_msg')\n    req = self.client.post(add_url, author)\n    self.assertIn((ContactFormViewWithMsg.success_message % author), req.cookies['messages'].value)\n", "label": 0}
{"function": "\n\ndef __init__(self, question, docs):\n    super(Extractor, self).__init__(question, docs, tag=TAG)\n", "label": 0}
{"function": "\n\ndef expect(self, method=None, uri=None, params={\n    \n}):\n    if method:\n        self.assertEqual(method, self.executor.request.method)\n    if uri:\n        self.assertEqual(self.executor.request.uri, ('https://api-ssl.bitly.com/v3' + uri))\n    if params:\n        params.update({\n            'access_token': 'my-access-token',\n        })\n        self.assertEqual(self.executor.request.params, params)\n", "label": 0}
{"function": "\n\ndef test_handle_error_401_sends_challege_default_realm(self):\n    api = restplus.Api(self.app, serve_challenge_on_401=True)\n    exception = HTTPException()\n    exception.code = 401\n    exception.data = {\n        'foo': 'bar',\n    }\n    with self.app.test_request_context('/foo'):\n        resp = api.handle_error(exception)\n        self.assertEqual(resp.status_code, 401)\n        self.assertEqual(resp.headers['WWW-Authenticate'], 'Basic realm=\"flask-restplus\"')\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    if isinstance(other, BaseNull):\n        return other\n    return self.map((Q + _unwrap(other)))\n", "label": 0}
{"function": "\n\ndef next_hop(tokeniser):\n    value = tokeniser()\n    if (value.lower() == 'self'):\n        return (IPSelf(tokeniser.afi), NextHopSelf(tokeniser.afi))\n    else:\n        ip = IP.create(value)\n        if (ip.afi == AFI.ipv4):\n            return (ip, NextHop(ip.top()))\n        return (ip, None)\n", "label": 0}
{"function": "\n\ndef test_is_variant(self):\n    expander = GvcfExpander()\n    self.assertTrue(expander.is_variant(json.loads(self.snp_1)))\n    self.assertTrue(expander.is_variant(json.loads(self.snp_2)))\n    self.assertTrue(expander.is_variant(json.loads(self.insertion_1)))\n    self.assertTrue(expander.is_variant(json.loads(self.deletion_1)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_a)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_b)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_c)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_d)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_ambiguous)))\n    self.assertFalse(expander.is_variant(json.loads(self.no_call_1)))\n", "label": 0}
{"function": "\n\ndef test_coerce_on_select(nyc):\n    t = symbol('t', discover(nyc))\n    t = t[(((((((((t.pickup_latitude >= 40.477399) & (t.pickup_latitude <= 40.917577)) & (t.dropoff_latitude >= 40.477399)) & (t.dropoff_latitude <= 40.917577)) & (t.pickup_longitude >= (- 74.25909))) & (t.pickup_longitude <= (- 73.700272))) & (t.dropoff_longitude >= (- 74.25909))) & (t.dropoff_longitude <= (- 73.700272))) & (t.passenger_count < 6))]\n    t = transform(t, pass_count=(t.passenger_count + 1))\n    result = compute(t.pass_count.coerce('float64'), nyc, return_type='native')\n    s = odo(result, pd.Series)\n    expected = (compute(t, nyc, return_type=pd.DataFrame).passenger_count.astype('float64') + 1.0)\n    assert (list(s) == list(expected))\n", "label": 0}
{"function": "\n\ndef modify_updates(self, updates):\n    '\"\\n        Modifies the parameters before a learning update is applied. Behavior\\n        is defined by subclass\\'s implementation of _modify_updates and any\\n        ModelExtension\\'s implementation of post_modify_updates.\\n\\n        Parameters\\n        ----------\\n        updates : dict\\n            A dictionary mapping shared variables to symbolic values they\\n            will be updated to\\n\\n        Notes\\n        -----\\n        For example, if a given parameter is not meant to be learned, a\\n        subclass or extension\\n        should remove it from the dictionary. If a parameter has a restricted\\n        range, e.g.. if it is the precision of a normal distribution,\\n        a subclass or extension should clip its update to that range. If a\\n        parameter\\n        has any other special properties, its updates should be modified\\n        to respect that here, e.g. a matrix that must be orthogonal should\\n        have its update value modified to be orthogonal here.\\n\\n        This is the main mechanism used to make sure that generic training\\n        algorithms such as those found in pylearn2.training_algorithms\\n        respect the specific properties of the models passed to them.\\n        '\n    self._modify_updates(updates)\n    self._ensure_extensions()\n    for extension in self.extensions:\n        extension.post_modify_updates(updates, self)\n", "label": 0}
{"function": "\n\ndef set_flipped(self, x, y):\n    ' Sets the specified piece as flipped.\\n        '\n    self.pieces[(x + (y * self.width))].set_flipped()\n", "label": 0}
{"function": "\n\ndef on_leave(self, details):\n    self.disconnect()\n", "label": 0}
{"function": "\n\n@needs_mail\n@needs_link\ndef proxy(request, mail, link):\n    return link.get_target(mail)(request, mail.person, mail.job.group_object)\n", "label": 0}
{"function": "\n\ndef TestParallelModify(instances):\n    'PERFORMANCE: Parallel instance modify.\\n\\n  @type instances: list of L{qa_config._QaInstance}\\n  @param instances: list of instances to issue modify commands against\\n\\n  '\n    job_driver = _JobQueueDriver()\n    new_min_mem = qa_config.get(constants.BE_MAXMEM)\n    for instance in instances:\n        cmd = ['gnt-instance', 'modify', '--submit', '-B', ('%s=%s' % (constants.BE_MINMEM, new_min_mem))]\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n        cmd = ['gnt-instance', 'modify', '--submit', '-O', 'fake_os_param=fake_value']\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n        cmd = ['gnt-instance', 'modify', '--submit', '-O', 'fake_os_param=fake_value', '-B', ('%s=%s' % (constants.BE_MINMEM, new_min_mem))]\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n    job_driver.WaitForCompletion()\n", "label": 0}
{"function": "\n\n@require_creds(True)\n@rpcmethod(signature=[SUCCESS_TYPE, URN_TYPE, CREDENTIALS_TYPE], url_name='openflow_gapi')\ndef DeleteSliver(slice_urn, credentials, **kwargs):\n    logger.debug('Called DeleteSliver')\n    try:\n        return gapi.DeleteSliver(slice_urn, kwargs['request'].user)\n    except Slice.DoesNotExist:\n        no_such_slice(slice_urn)\n", "label": 0}
{"function": "\n\ndef identity_provider_create(request, idp_id, description=None, enabled=False, remote_ids=None):\n    manager = keystoneclient(request, admin=True).federation.identity_providers\n    try:\n        return manager.create(id=idp_id, description=description, enabled=enabled, remote_ids=remote_ids)\n    except keystone_exceptions.Conflict:\n        raise exceptions.Conflict()\n", "label": 0}
{"function": "\n\ndef __init__(self, errmsg='You need override this method'):\n    super(NeedOverrideError, self).__init__(self, errmsg)\n", "label": 0}
{"function": "\n\ndef test_past_datetime(self):\n    value = self.sd.past_datetime()\n    self.assertTrue(isinstance(value, datetime.datetime))\n    self.assertTrue((value <= datetime.datetime.utcnow().replace(tzinfo=utc)))\n    self.assertTrue((value >= (datetime.datetime.utcnow().replace(tzinfo=utc) - datetime.timedelta(minutes=1440))))\n    value = self.sd.past_datetime(0, 10)\n    self.assertTrue((value <= datetime.datetime.utcnow().replace(tzinfo=utc)))\n    self.assertTrue((value >= (datetime.datetime.utcnow().replace(tzinfo=utc) - datetime.timedelta(minutes=10))))\n    with self.assertRaises(ParameterError):\n        self.sd.past_datetime(100, 0)\n    with self.assertRaises(ParameterError):\n        self.sd.past_datetime((- 10), 10)\n", "label": 0}
{"function": "\n\ndef play_rtmpdump_stream(player, url, params={\n    \n}):\n    cmdline = (\"rtmpdump -r '%s' \" % url)\n    for key in params.keys():\n        cmdline += (((key + ' ') + params[key]) if (params[key] != None) else ('' + ' '))\n    cmdline += (' -o - | %s -' % player)\n    print(cmdline)\n    os.system(cmdline)\n    return\n", "label": 0}
{"function": "\n\ndef test_disable_logging(self):\n    NAME = 'name'\n    before = {\n        'logging': {\n            'logBucket': 'logs',\n            'logObjectPrefix': 'pfx',\n        },\n    }\n    bucket = self._makeOne(name=NAME, properties=before)\n    self.assertTrue((bucket.get_logging() is not None))\n    bucket.disable_logging()\n    self.assertTrue((bucket.get_logging() is None))\n", "label": 0}
{"function": "\n\ndef test_benchmark_variance_06(self):\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['Monthly'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.three_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['3-Month'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.six_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['6-month'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.year_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['year'])\n", "label": 0}
{"function": "\n\n@classmethod\ndef size(cls, val, str_cache, object_cache, traits_cache):\n    if (val in object_cache):\n        index = object_cache.index(val)\n        return AMF3Integer.size((index << 1))\n    else:\n        object_cache.append(val)\n        size = 0\n        traits = type(val)\n        if (traits in traits_cache):\n            index = traits_cache.index(traits)\n            size += AMF3Integer.size(((index << 2) | 1))\n        else:\n            header = 3\n            if traits.__dynamic__:\n                header |= (2 << 2)\n            if traits.__externalizable__:\n                header |= (1 << 2)\n            header |= (len(traits.__members__) << 4)\n            size += AMF3Integer.size(header)\n            if isinstance(val, AMF3Object):\n                size += U8.size\n            else:\n                size += AMF3String.size(traits.__name__, cache=str_cache)\n                traits_cache.append(traits)\n            for member in traits.__members__:\n                size += AMF3String.size(member, cache=str_cache)\n        for member in traits.__members__:\n            value = getattr(val, member)\n            size += AMF3Value.size(value, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n        if traits.__dynamic__:\n            if isinstance(val, AMF3Object):\n                iterator = val.items()\n            else:\n                iterator = val.__dict__.items()\n            for (key, value) in iterator:\n                if (key in traits.__members__):\n                    continue\n                size += AMF3String.size(key, cache=str_cache)\n                size += AMF3Value.size(value, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n            size += U8.size\n        return size\n", "label": 1}
{"function": "\n\ndef all_job_data(jobs, job_type):\n    ' Return an iterator over all job data. Exclude config template dups. '\n    conf_tmpl_ids = []\n    for job in jobs:\n        for (jt, data) in job.iteritems():\n            if (jt == job_type):\n                if (data['config_template_id'] not in conf_tmpl_ids):\n                    conf_tmpl_ids.append(data['config_template_id'])\n                    (yield data)\n", "label": 0}
{"function": "\n\ndef take_action(self, parsed_args):\n    identity_client = self.app.client_manager.identity\n    consumer = utils.find_resource(identity_client.oauth1.consumers, parsed_args.consumer)\n    identity_client.oauth1.consumers.delete(consumer.id)\n", "label": 0}
{"function": "\n\ndef _download_pdf(self, url, base_path):\n    local_file_path = os.path.join(base_path, 'billing-temp-document.pdf')\n    response = requests.get(url, stream=True)\n    should_wipe_bad_headers = True\n    with open(local_file_path, 'wb') as out_file:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                if should_wipe_bad_headers:\n                    pdf_header_pos = chunk.find('%PDF-')\n                    if (pdf_header_pos > 0):\n                        chunk = chunk[pdf_header_pos:]\n                    should_wipe_bad_headers = False\n                out_file.write(chunk)\n                out_file.flush()\n    return local_file_path\n", "label": 0}
{"function": "\n\ndef test_validate_type_negative(self):\n    sla1 = TestCriterion(0)\n\n    class AnotherTestCriterion(TestCriterion):\n        pass\n    sla2 = AnotherTestCriterion(0)\n    self.assertRaises(TypeError, sla1.validate_type, sla2)\n", "label": 0}
{"function": "\n\ndef test_profile_topics_bookmarks(self):\n    \"\\n        profile user's topics with bookmarks\\n        \"\n    bookmark = CommentBookmark.objects.create(topic=self.topic, user=self.user)\n    utils.login(self)\n    response = self.client.get(reverse('spirit:user:topics', kwargs={\n        'pk': self.user2.pk,\n        'slug': self.user2.st.slug,\n    }))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(list(response.context['topics']), [self.topic])\n    self.assertEqual(response.context['topics'][0].bookmark, bookmark)\n", "label": 0}
{"function": "\n\ndef test_volume_create_properties(self):\n    arglist = ['--property', 'Alpha=a', '--property', 'Beta=b', '--size', str(self.new_volume.size), self.new_volume.name]\n    verifylist = [('property', {\n        'Alpha': 'a',\n        'Beta': 'b',\n    }), ('size', self.new_volume.size), ('name', self.new_volume.name)]\n    parsed_args = self.check_parser(self.cmd, arglist, verifylist)\n    (columns, data) = self.cmd.take_action(parsed_args)\n    self.volumes_mock.create.assert_called_with(size=self.new_volume.size, snapshot_id=None, name=self.new_volume.name, description=None, volume_type=None, user_id=None, project_id=None, availability_zone=None, metadata={\n        'Alpha': 'a',\n        'Beta': 'b',\n    }, imageRef=None, source_volid=None)\n    self.assertEqual(self.columns, columns)\n    self.assertEqual(self.datalist, data)\n", "label": 0}
{"function": "\n\ndef init_relation(self, models, relation):\n    '\\n        Initialize the relation on a set of models.\\n\\n        :type models: list\\n        :type relation: str\\n        '\n    for model in models:\n        model.set_relation(relation, Result(None, self, model))\n    return models\n", "label": 0}
{"function": "\n\ndef test_with_some_synonyms(self):\n    SynonymFactory(from_words='foo', to_words='bar')\n    SynonymFactory(from_words='baz', to_words='qux')\n    (_, body) = es_utils.es_get_synonym_filter('en-US')\n    expected = {\n        'type': 'synonym',\n        'synonyms': ['foo => bar', 'baz => qux'],\n    }\n    eq_(body, expected)\n", "label": 0}
{"function": "\n\ndef run(self):\n    f = self.output().open('w')\n    print('hello, world', file=f)\n    f.close()\n", "label": 0}
{"function": "\n\ndef __init__(self, area, width=4, char=' '):\n    self.width = width\n    self.char = char\n    area.install(('NORMAL', '<Key-greater>', (lambda event: event.widget.shift_sel_right(self.width, self.char))), ('NORMAL', '<Key-less>', (lambda event: event.widget.shift_sel_left(self.width))))\n", "label": 0}
{"function": "\n\n@classmethod\ndef validate(cls, level):\n    level = int(level)\n    if (level in (cls.NONE, cls.READ, cls.WRITE, cls.ADMIN, cls.SITE_ADMIN)):\n        return level\n    else:\n        raise ValueError(('Invalid AccessType: %d.' % level))\n", "label": 0}
{"function": "\n\ndef __init__(self, lib, dtype, N, C, K, H, W, P, Q, pad_h, pad_w, relu, bsum):\n    (R, S) = (3, 3)\n    GC32 = _ceil_div(C, 32)\n    GC16 = _ceil_div((GC32 * 32), 16)\n    GK16 = _ceil_div(K, 16)\n    self.filter_func = _get_bprop_filter_trans_4x4_kernel\n    self.filter_size = (((dtype.itemsize * 1152) * K) * GC32)\n    self.filter_args = [(GK16, GC16, 1), (256, 1, 1), None, None, None, ((R * S) * K), (S * K), ((S * K) * 2), K, C, (K * 1152)]\n    super(BpropWinograd_4x4_3x3, self).__init__(lib, dtype, N, K, C, P, Q, H, W, (2 - pad_h), (2 - pad_w), relu, bsum)\n", "label": 0}
{"function": "\n\n@records.post(validators=record_validator, permission='post_record')\ndef post_record(request):\n    'Saves a single model record.\\n\\n    Posted record attributes will be matched against the related model\\n    definition.\\n\\n    '\n    if (request.headers.get('Validate-Only', 'false') == 'true'):\n        return\n    model_id = request.matchdict['model_id']\n    if request.credentials_id:\n        credentials_id = request.credentials_id\n    else:\n        credentials_id = Everyone\n    record_id = request.db.put_record(model_id, request.data_clean, [credentials_id])\n    request.notify('RecordCreated', model_id, record_id)\n    created = ('%s/models/%s/records/%s' % (request.application_url, model_id, record_id))\n    request.response.status = '201 Created'\n    request.response.headers['location'] = str(created)\n    return {\n        'id': record_id,\n    }\n", "label": 0}
{"function": "\n\ndef on_files_selected(self, paths):\n    \" Handle the 'filesSelected' signal from the dialog.\\n\\n        \"\n    d = self.declaration\n    if (d is not None):\n        d.selected_paths = paths\n", "label": 0}
{"function": "\n\ndef _dispatch(self, inst, kws):\n    assert (self.current_block is not None)\n    fname = ('op_%s' % inst.opname.replace('+', '_'))\n    try:\n        fn = getattr(self, fname)\n    except AttributeError:\n        raise NotImplementedError(inst)\n    else:\n        try:\n            return fn(inst, **kws)\n        except errors.NotDefinedError as e:\n            if (e.loc is None):\n                e.loc = self.loc\n            raise e\n", "label": 0}
{"function": "\n\ndef set_context(self, serializer):\n    '\\n        This hook is called by the serializer instance,\\n        prior to the validation call being made.\\n        '\n    self.instance = getattr(serializer, 'instance', None)\n", "label": 0}
{"function": "\n\ndef db_add_ts_start(self, ts_start):\n    self._db_ts_start = ts_start\n", "label": 0}
{"function": "\n\n@expose('/<string:locale>')\ndef index(self, locale):\n    session['locale'] = locale\n    refresh()\n    self.update_redirect()\n    return redirect(self.get_redirect())\n", "label": 0}
{"function": "\n\ndef reselect(self, pos):\n\n    def select(view, edit):\n        region = pos\n        if hasattr(pos, '__call__'):\n            region = run_callback(pos, view)\n        if isinstance(region, int):\n            region = sublime.Region(region, region)\n        elif isinstance(region, (tuple, list)):\n            region = sublime.Region(*region)\n        view.sel().clear()\n        view.sel().add(region)\n        view.show(region, False)\n    self.callback(select)\n", "label": 0}
{"function": "\n\ndef test_mutual_info_regression():\n    (X, y) = make_regression(n_samples=100, n_features=10, n_informative=2, shuffle=False, random_state=0, noise=10)\n    univariate_filter = SelectKBest(mutual_info_regression, k=2)\n    X_r = univariate_filter.fit(X, y).transform(X)\n    assert_best_scores_kept(univariate_filter)\n    X_r2 = GenericUnivariateSelect(mutual_info_regression, mode='k_best', param=2).fit(X, y).transform(X)\n    assert_array_equal(X_r, X_r2)\n    support = univariate_filter.get_support()\n    gtruth = np.zeros(10)\n    gtruth[:2] = 1\n    assert_array_equal(support, gtruth)\n    univariate_filter = SelectPercentile(mutual_info_regression, percentile=20)\n    X_r = univariate_filter.fit(X, y).transform(X)\n    X_r2 = GenericUnivariateSelect(mutual_info_regression, mode='percentile', param=20).fit(X, y).transform(X)\n    assert_array_equal(X_r, X_r2)\n    support = univariate_filter.get_support()\n    gtruth = np.zeros(10)\n    gtruth[:2] = 1\n    assert_array_equal(support, gtruth)\n", "label": 0}
{"function": "\n\ndef test_derived(self):\n    import time\n\n    class Local(threading.local):\n\n        def __init__(self):\n            time.sleep(0.01)\n    local = Local()\n\n    def f(i):\n        local.x = i\n        self.assertEqual(local.x, i)\n    threads = []\n    for i in range(10):\n        t = threading.Thread(target=f, args=(i,))\n        t.start()\n        threads.append(t)\n    for t in threads:\n        t.join()\n", "label": 0}
{"function": "\n\ndef test_retry_in_graph_flow_requires_and_provides(self):\n    flow = gf.Flow('gf', retry.AlwaysRevert('rt', requires=['x', 'y'], provides=['a', 'b']))\n    self.assertEqual(set(['x', 'y']), flow.requires)\n    self.assertEqual(set(['a', 'b']), flow.provides)\n", "label": 0}
{"function": "\n\ndef test_review_comments(self):\n    \"Show that one can iterate over a PR's review comments.\"\n    cassette_name = self.cassette_name('review_comments')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        for comment in p.review_comments():\n            assert isinstance(comment, github3.pulls.ReviewComment)\n", "label": 0}
{"function": "\n\ndef update(self, t, dt):\n    if (random.random() < 0.02):\n        self.a += ((random.randint((- 1), 1) * pi) / 8)\n    dx = cos(self.a)\n    dz = sin(self.a)\n    self.x += (dx * dt)\n    self.z += (dz * dt)\n", "label": 0}
{"function": "\n\ndef encryption_oracle(rawInput):\n    key = generateAESKey()\n    iv = generateAESKey()\n    prependAmount = (5 + (getOneRandomByte() % 6))\n    appendAmount = (5 + (getOneRandomByte() % 6))\n    plaintext = (((b'x' * prependAmount) + rawInput) + (b'y' * appendAmount))\n    if (getOneRandomByte() & 1):\n        return aes_ecb_enc(addPKCS7Padding(plaintext, 16), key)\n    else:\n        return aes_cbc_enc(addPKCS7Padding(plaintext, 16), key, iv)\n", "label": 0}
{"function": "\n\ndef parse_status(self, lines):\n    activity = []\n    seen_times = set()\n    for line in lines:\n        (time, fields) = line.split('|')\n        if (time not in seen_times):\n            seen_times.add(time)\n            status_obj = status.Status(int(float(time)), fields)\n            activity.append(status_obj)\n    return activity\n", "label": 0}
{"function": "\n\ndef _partitions_to_src(partitions):\n    return ''.join((part.src for part in partitions))\n", "label": 0}
{"function": "\n\ndef test_input_extra_rewrite(self):\n    self.client_job_description.rewrite_paths = True\n    extra_file = os.path.join(self.input1_files_path, 'moo', 'cow.txt')\n    os.makedirs(os.path.dirname(extra_file))\n    open(extra_file, 'w').write('Hello World!')\n    command_line = ('test.exe %s' % extra_file)\n    self.client_job_description.command_line = command_line\n    self.client.expect_command_line('test.exe /pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt')\n    self.client.expect_put_paths(['/pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt'])\n    self._submit()\n    uploaded_file1 = self.client.put_files[0]\n    assert (uploaded_file1[1] == 'input')\n    assert (uploaded_file1[0] == extra_file)\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.LIST):\n                self.column_names = []\n                (_etype10, _size7) = iprot.readListBegin()\n                for _i11 in xrange(_size7):\n                    _elem12 = iprot.readString()\n                    self.column_names.append(_elem12)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.slice_range = SliceRange()\n                self.slice_range.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_member_addresses(self):\n    addresses = []\n    for member in self.config_members:\n        addresses.append(member.get_server().get_address())\n    return addresses\n", "label": 0}
{"function": "\n\ndef test_dispatch_on_heartbeat_frame(self):\n    frame = mock()\n    expect(frame.type).returns(HeartbeatFrame.type())\n    expect(self.ch.send_heartbeat)\n    self.ch.dispatch(frame)\n", "label": 0}
{"function": "\n\ndef __init__(self, output_type, inplace=False):\n    Op.__init__(self)\n    self.output_type = output_type\n    self.inplace = inplace\n    if inplace:\n        self.destroy_map = {\n            0: [0],\n        }\n    self.warned_numpy_version = False\n", "label": 0}
{"function": "\n\ndef CheckLoggingWorks(self):\n    logger = StringIO.StringIO()\n    expected_output = (';\\n'.join([sqlite.main._BEGIN, 'CREATE TABLE TEST(FOO INTEGER)', 'INSERT INTO TEST(FOO) VALUES (?)', 'ROLLBACK']) + ';\\n')\n    self.cnx = sqlite.connect(self.getfilename(), command_logfile=logger)\n    cu = self.cnx.cursor()\n    cu.execute('CREATE TABLE TEST(FOO INTEGER)')\n    cu.execute('INSERT INTO TEST(FOO) VALUES (?)', (5,))\n    self.cnx.rollback()\n    logger.seek(0)\n    real_output = logger.read()\n    if (expected_output != real_output):\n        self.fail(\"Logging didn't produce expected output.\")\n", "label": 0}
{"function": "\n\ndef bitand(self, other):\n    return self._combine(other, self.BITAND, False)\n", "label": 0}
{"function": "\n\ndef get_environment(self, app):\n    return app.extensions['gears']['environment']\n", "label": 0}
{"function": "\n\ndef load(self, odffile):\n    ' Loads a document into the parser and parses it.\\n            The argument can either be a filename or a document in memory.\\n        '\n    self.lines = []\n    self._wfunc = self._wlines\n    if isinstance(odffile, str):\n        self.document = load(odffile)\n    else:\n        self.document = odffile\n    self._walknode(self.document.topnode)\n", "label": 0}
{"function": "\n\ndef db_children(self, parent=(None, None), orphan=False):\n    children = []\n    children.append((self, parent[0], parent[1]))\n    return children\n", "label": 0}
{"function": "\n\ndef blacklist_delete_user_agents(self, request, queryset):\n    self.blacklist_user_agents(request, queryset)\n    self.delete_queryset(request, queryset)\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n\n    def format_row(row):\n        return ('(%s)' % ', '.join((format_number(value) for value in row)))\n    return ('Matrix44(%s)' % ', '.join((format_row(row) for row in self.rows())))\n", "label": 0}
{"function": "\n\ndef _createFile(self):\n    self.h5file.create_array('/', 'arr1', [1, 2])\n    group1 = self.h5file.create_group('/', 'group1')\n    arr2 = self.h5file.create_array(group1, 'arr2', [1, 2, 3])\n    lgroup1 = self.h5file.create_hard_link('/', 'lgroup1', '/group1')\n    self.assertTrue((lgroup1 is not None))\n    larr1 = self.h5file.create_hard_link(group1, 'larr1', '/arr1')\n    self.assertTrue((larr1 is not None))\n    larr2 = self.h5file.create_hard_link('/', 'larr2', arr2)\n    self.assertTrue((larr2 is not None))\n", "label": 0}
{"function": "\n\ndef saveXML(self, snode):\n    if (snode is None):\n        snode = self\n    elif (snode.ownerDocument is not self):\n        raise xml.dom.WrongDocumentErr()\n    return snode.toxml()\n", "label": 0}
{"function": "\n\ndef install(self, instance):\n    cfg = self.config\n    if (self.database == self.master_database):\n        template = 'template1'\n    else:\n        template = self.master_database\n    config_opt_map = dict(host='host', user='username', password='password', encoding='encoding', lc_collate='lc-collate', lc_ctype='lc-ctype', tablespace='tablespace')\n    options = [((('--' + opt) + '=') + str(cfg[cfg_name])) for (cfg_name, opt) in config_opt_map.iteritems() if (cfg_name in cfg)]\n    fail = instance.do(((['createdb', '--template', template] + options) + [self.database]))\n    if fail:\n        instance.do(['pg_dump', '-h', str(cfg['host']), '-U', str(cfg['user']), '-E', str(cfg['encoding']), '-f', (('/tmp/' + template) + '.sql'), template])\n        instance.do(((['createdb', '--template', 'template1'] + options) + [self.database]))\n        instance.do(['psql', '-h', str(cfg['host']), '-U', str(cfg['user']), '-f', (('/tmp/' + template) + '.sql'), self.database])\n    info = self.connection_info\n    info['database'] = self.database\n    return info\n", "label": 0}
{"function": "\n\n@contextmanager\ndef trace_ms(statName):\n    if (_instatrace is None):\n        (yield)\n        return\n    now = _instatrace.now_ms()\n    (yield)\n    _instatrace.trace(statName, (_instatrace.now_ms() - now))\n", "label": 0}
{"function": "\n\ndef test_get_bad_column_qualifier(self):\n    families = {\n        cf2: 'oberyn',\n    }\n    res = self.c.get(table, self.row_prefix, families=families)\n    self.assertEqual(result_to_dict(res), {\n        \n    })\n", "label": 0}
{"function": "\n\ndef db_add_portSpec(self, portSpec):\n    self.__db_portSpec = portSpec\n", "label": 0}
{"function": "\n\ndef do(args):\n    ' Main method '\n    if args.name:\n        tc_names = [args.name]\n    else:\n        tc_names = qitoolchain.get_tc_names()\n    for tc_name in tc_names:\n        toolchain = qitoolchain.get_toolchain(tc_name)\n        ui.info(str(toolchain))\n", "label": 0}
{"function": "\n\ndef test_IRParamStmt_dump():\n    stmt = {\n        'param': 'IR',\n        'angle': '45',\n    }\n    ir = IRParamStmt.from_dict(stmt)\n    assert_equal(ir.to_gerber(), '%IR45*%')\n", "label": 0}
{"function": "\n\ndef get_resources(self):\n    resources = []\n    resource = extensions.ResourceExtension('os-agents', AgentController())\n    resources.append(resource)\n    return resources\n", "label": 0}
{"function": "\n\ndef _save(self, key, attributes):\n    s_uuid = self.repo.save(key, attributes)\n    self.logger.debug(('creating object with uuid = %s' % s_uuid))\n    attribute_type = AT.UNIQUE_IDENTIFIER\n    attribute = self.attribute_factory.create_attribute(attribute_type, s_uuid)\n    attributes.append(attribute)\n    self.repo.update(s_uuid, key, attributes)\n    return (s_uuid, attribute)\n", "label": 0}
{"function": "\n\ndef to_array(self):\n    '\\n        Convert the RiakLinkPhase to a format that can be output into\\n        JSON. Used internally.\\n        '\n    stepdef = {\n        'bucket': self._bucket,\n        'tag': self._tag,\n        'keep': self._keep,\n    }\n    return {\n        'link': stepdef,\n    }\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Tangible()\n    result.template = 'object/tangible/ship/crafted/capacitor/shared_quick_recharge_battery_mk5.iff'\n    result.attribute_template_id = 8\n    result.stfName('space_crafting_n', 'quick_recharge_battery_mk5')\n    return result\n", "label": 0}
{"function": "\n\ndef abort_request(self, stream, ident, parent):\n    'abort a specifig msg by id'\n    msg_ids = parent['content'].get('msg_ids', None)\n    if isinstance(msg_ids, str):\n        msg_ids = [msg_ids]\n    if (not msg_ids):\n        self.abort_queues()\n    for mid in msg_ids:\n        self.aborted.add(str(mid))\n    content = dict(status='ok')\n    reply_msg = self.session.send(stream, 'abort_reply', content=content, parent=parent, ident=ident)\n    self.log.debug(str(reply_msg))\n", "label": 0}
{"function": "\n\ndef p_basic_statement(self, p):\n    'basic_statement : if_statement\\n        | case_statement\\n        | casex_statement\\n        | for_statement\\n        | while_statement\\n        | event_statement\\n        | wait_statement\\n        | forever_statement\\n        | block\\n        | namedblock\\n        | parallelblock\\n        | blocking_substitution\\n        | nonblocking_substitution\\n        | single_statement\\n        '\n    p[0] = p[1]\n    p.set_lineno(0, p.lineno(1))\n", "label": 0}
{"function": "\n\ndef write(self, handle, name):\n    self._check_all_set()\n    g = handle.create_group(name)\n    g.attrs['type'] = np.string_('extern_box'.encode('utf-8'))\n    g.attrs['xmin'] = self.bounds[0][0]\n    g.attrs['xmax'] = self.bounds[0][1]\n    g.attrs['ymin'] = self.bounds[1][0]\n    g.attrs['ymax'] = self.bounds[1][1]\n    g.attrs['zmin'] = self.bounds[2][0]\n    g.attrs['zmax'] = self.bounds[2][1]\n    Source.write(self, g)\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    self.one_to_one = True\n    return super(OneToOneFieldDefinition, self).save(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef testComplexLabels(self):\n    logging.debug('Running testComplexLabels method.')\n    expression = 'a_123'\n    self._RunMathQuery(expression, ['a_123'])\n    expression = 'a_1.b2'\n    self._RunMathQuery(expression, ['a_1.b2'])\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    ' Returns the materialized path '\n    return ('/'.join([x.value for x in self.parts]) + ('/' if self.is_dir else ''))\n", "label": 0}
{"function": "\n\ndef retry_subflow(self, retry):\n    'Prepares a retrys + its subgraph for execution.\\n\\n        This sets the retrys intention to ``EXECUTE`` and resets all of its\\n        subgraph (its successors) to the ``PENDING`` state with an ``EXECUTE``\\n        intention.\\n        '\n    tweaked = self.reset_atoms([retry], state=None, intention=st.EXECUTE)\n    tweaked.extend(self.reset_subgraph(retry))\n    return tweaked\n", "label": 0}
{"function": "\n\ndef draw_random(G, **kwargs):\n    'Draw the graph G with a random layout.'\n    draw(G, random_layout(G), **kwargs)\n", "label": 0}
{"function": "\n\ndef post(self):\n    ' pass additionalMetadata and file to global\\n        variables.\\n        '\n    global received_file\n    global received_meta\n    received_file = self.request.files['file'][0].body\n    received_meta = self.get_argument('additionalMetadata')\n", "label": 0}
{"function": "\n\ndef __init__(self, gates, system_desc, wh_codes):\n    self.gates = gates\n    self.system_desc = system_desc\n    self.wh_codes = wh_codes\n", "label": 0}
{"function": "\n\ndef __init__(self, args):\n    super(RemoveVariantSetRunner, self).__init__(args)\n    self.variantSetName = args.variantSetName\n", "label": 0}
{"function": "\n\n@override_djconfig(comments_per_page=1)\ndef test_profile_comments_paginate(self):\n    \"\\n        profile user's comments paginated\\n        \"\n    utils.create_comment(user=self.user2, topic=self.topic)\n    comment = utils.create_comment(user=self.user2, topic=self.topic)\n    utils.login(self)\n    response = self.client.get(reverse('spirit:user:detail', kwargs={\n        'pk': self.user2.pk,\n        'slug': self.user2.st.slug,\n    }))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(list(response.context['comments']), [comment])\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    self.maxDiff = None\n    filename = 'chart_data_labels24.xlsx'\n    test_dir = 'xlsxwriter/test/comparison/'\n    self.got_filename = ((test_dir + '_test_') + filename)\n    self.exp_filename = ((test_dir + 'xlsx_files/') + filename)\n    self.ignore_files = []\n    self.ignore_elements = {\n        \n    }\n", "label": 0}
{"function": "\n\ndef get_available_user_FIELD_transitions(instance, user, field):\n    '\\n    List of transitions available in current model state\\n    with all conditions met and user have rights on it\\n    '\n    for transition in get_available_FIELD_transitions(instance, field):\n        if transition.has_perm(instance, user):\n            (yield transition)\n", "label": 0}
{"function": "\n\ndef _item_position(self, item):\n    return self.items.index(item)\n", "label": 0}
{"function": "\n\ndef GetModifiedShellCommand(self, Command, PluginOutputDir):\n    self.RefreshReplacements()\n    NewCommand = ((('cd ' + self.ShellPathEscape(PluginOutputDir)) + '; ') + MultipleReplace(Command, self.DynamicReplacements))\n    self.OldCommands[NewCommand] = Command\n    return NewCommand\n", "label": 0}
{"function": "\n\ndef report_for_conf(self, conf):\n    'Returns the path to the ivy report for the provided conf.\\n\\n     Returns None if there is no path.\\n    '\n    return self._reports_by_conf.get(conf)\n", "label": 0}
{"function": "\n\ndef our_x2_iterates(n_iters=100):\n    history = []\n    searchspace = SearchSpace()\n    searchspace.add_float('x', (- 10), 10)\n    random = np.random.RandomState(0)\n\n    def fn(params):\n        return (- (params['x'] ** 2))\n    for i in range(n_iters):\n        params = HyperoptTPE(seed=random).suggest(history, searchspace)\n        history.append((params, fn(params), 'SUCCEEDED'))\n    return np.array([h[0]['x'] for h in history])\n", "label": 0}
{"function": "\n\ndef insert(self, window, first_line, *lines):\n    (row, column) = cursor = self.cursors[window]\n    (left, right) = (self[row][:column], self[row][column:])\n    added = len(lines)\n    if lines:\n        last_line = lines[(- 1)]\n        column = len(last_line)\n    else:\n        last_line = first_line\n        column += len(first_line)\n    self[row] = (left + first_line)\n    self[(row + 1):(row + 1)] = lines\n    self[(row + added)] += right\n    for other in self.cursors.itervalues():\n        if (other.row > row):\n            other._row += added\n    cursor.coords = ((row + added), column)\n", "label": 0}
{"function": "\n\ndef clean_message(self):\n    message = self.cleaned_data['message']\n    try:\n        message = message.decode('base64')\n    except TypeError as e:\n        raise ValidationError(('Cannot convert to binary: %r' % e.msg))\n    if (len(message) % 16):\n        raise ValidationError('Wrong block size for message !')\n    if (len(message) <= 16):\n        raise ValidationError('Message too short or missing IV !')\n    return message\n", "label": 0}
{"function": "\n\n@keep_alive('server')\ndef address_is_mine(self, address):\n    result = self.server.validateaddress(address)\n    return result['ismine']\n", "label": 0}
{"function": "\n\ndef _bump_version(self, version):\n    try:\n        parts = map(int, version.split('.'))\n    except ValueError:\n        self._fail('Current version is not numeric')\n    parts[(- 1)] += 1\n    return '.'.join(map(str, parts))\n", "label": 0}
{"function": "\n\ndef npm_command(self, args):\n    'Creates a command that can run `npm`, passing the given args to it.\\n\\n    :param list args: A list of arguments to pass to `npm`.\\n    :returns: An `npm` command that can be run later.\\n    :rtype: :class:`NodeDistribution.Command`\\n    '\n    return self._create_command('npm', args)\n", "label": 0}
{"function": "\n\ndef test_disenroll_with_no_enrollment(self):\n    courses = Course.objects.all()\n    for course in courses:\n        course.delete()\n    client = Client()\n    client.login(username=TEST_USER_USERNAME, password=TEST_USER_PASSWORD)\n    kwargs = {\n        'HTTP_X_REQUESTED_WITH': 'XMLHttpRequest',\n    }\n    response = client.post('/disenroll', {\n        'course_id': 1,\n    }, **kwargs)\n    self.assertEqual(response.status_code, 200)\n    json_string = response.content.decode(encoding='UTF-8')\n    array = json.loads(json_string)\n    self.assertEqual(array['message'], 'record does not exist')\n    self.assertEqual(array['status'], 'failed')\n", "label": 0}
{"function": "\n\n@parameterized.expand([('split', 2, 3, 3, 1.5), ('merger', 2, 3, 3, 1.8), ('dividend', 2, 3, 3, 2.88)])\ndef test_spot_price_adjustments(self, adjustment_type, liquid_day_0_price, liquid_day_1_price, illiquid_day_0_price, illiquid_day_1_price_adjusted):\n    'Test the behaviour of spot prices during adjustments.'\n    table_name = (adjustment_type + 's')\n    liquid_asset = getattr(self, (adjustment_type.upper() + '_ASSET'))\n    illiquid_asset = getattr(self, (('ILLIQUID_' + adjustment_type.upper()) + '_ASSET'))\n    adjustments = self.adjustment_reader.get_adjustments_for_sid(table_name, liquid_asset.sid)\n    self.assertEqual(1, len(adjustments))\n    adjustment = adjustments[0]\n    self.assertEqual(adjustment[0], pd.Timestamp('2016-01-06', tz='UTC'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[0]), 'daily')\n    self.assertEqual(liquid_day_0_price, bar_data.current(liquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[1]), 'daily')\n    self.assertEqual(liquid_day_1_price, bar_data.current(liquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[1]), 'daily')\n    self.assertEqual(illiquid_day_0_price, bar_data.current(illiquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[2]), 'daily')\n    self.assertAlmostEqual(illiquid_day_1_price_adjusted, bar_data.current(illiquid_asset, 'price'))\n", "label": 0}
{"function": "\n\ndef get_ud(self, cardinal, user, channel, msg):\n    try:\n        word = msg.split(' ', 1)[1]\n    except IndexError:\n        cardinal.sendMsg(channel, 'Syntax: .ud <word>')\n        return\n    try:\n        url = (URBANDICT_API_PREFIX + word)\n        f = urlopen(url).read()\n        data = json.loads(f)\n        word_def = data['list'][0]['definition']\n        link = data['list'][0]['permalink']\n        response = ('UD for %s: %s (%s)' % (word, word_def, link))\n        cardinal.sendMsg(channel, response.encode('utf-8'))\n    except Exception:\n        cardinal.sendMsg(channel, ('Could not retrieve definition for %s' % word))\n", "label": 0}
{"function": "\n\ndef test__merge(self):\n    seg1a = fake_neo(Block, seed=self.seed1, n=self.nchildren).segments[0]\n    assert_same_sub_schema(self.seg1, seg1a)\n    seg1a.spikes.append(self.spikes2[0])\n    seg1a.epocharrays.append(self.epcas2[0])\n    seg1a.annotate(seed=self.seed2)\n    seg1a.merge(self.seg2)\n    self.check_creation(self.seg2)\n    assert_same_sub_schema((self.sigs1a + self.sigs2), seg1a.analogsignals)\n    assert_same_sub_schema((self.sigarrs1a + self.sigarrs2), seg1a.analogsignalarrays)\n    assert_same_sub_schema((self.irsigs1a + self.irsigs2), seg1a.irregularlysampledsignals)\n    assert_same_sub_schema((self.epcs1 + self.epcs2), seg1a.epochs)\n    assert_same_sub_schema((self.epcas1 + self.epcas2), seg1a.epocharrays)\n    assert_same_sub_schema((self.evts1 + self.evts2), seg1a.events)\n    assert_same_sub_schema((self.evtas1 + self.evtas2), seg1a.eventarrays)\n    assert_same_sub_schema((self.spikes1 + self.spikes2), seg1a.spikes)\n    assert_same_sub_schema((self.trains1 + self.trains2), seg1a.spiketrains)\n", "label": 0}
{"function": "\n\ndef store_and_use_artifact(self, cache_key, src, results_dir=None):\n    'Read the content of a tarball from an iterator and return an artifact stored in the cache.'\n    with self._tmpfile(cache_key, 'read') as tmp:\n        for chunk in src:\n            tmp.write(chunk)\n        tmp.close()\n        tarball = self._store_tarball(cache_key, tmp.name)\n        artifact = self._artifact(tarball)\n        if (results_dir is not None):\n            safe_rmtree(results_dir)\n        artifact.extract()\n        return True\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    return ('Tuple(%s)' % ', '.join((str(elt) for elt in self.elts)))\n", "label": 0}
{"function": "\n\ndef test_create_ticket_ticket(self):\n    '\\n        A ticket ought to be created with a provided ticket string,\\n        if present.\\n        '\n    ticket = 'ST-0000000000-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'\n    st = ServiceTicket.objects.create_ticket(ticket=ticket, user=self.user)\n    self.assertEqual(st.ticket, ticket)\n", "label": 0}
{"function": "\n\ndef test_binary_infix_operators(self):\n    (a, b, h) = self.table.get_columns(['a', 'b', 'h'])\n    bool_col = (a > 0)\n    cases = [((a + b), '`a` + `b`'), ((a - b), '`a` - `b`'), ((a * b), '`a` * `b`'), ((a / b), '`a` / `b`'), ((a ** b), 'pow(`a`, `b`)'), ((a < b), '`a` < `b`'), ((a <= b), '`a` <= `b`'), ((a > b), '`a` > `b`'), ((a >= b), '`a` >= `b`'), ((a == b), '`a` = `b`'), ((a != b), '`a` != `b`'), ((h & bool_col), '`h` AND (`a` > 0)'), ((h | bool_col), '`h` OR (`a` > 0)'), ((h ^ bool_col), '(`h` OR (`a` > 0)) AND NOT (`h` AND (`a` > 0))')]\n    self._check_expr_cases(cases)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (other is None):\n        return (self._value is None)\n    other = str(other)\n    if (other not in self.values_range):\n        raise ConanException(bad_value_msg(self._name, other, self.values_range))\n    return (other == self.__str__())\n", "label": 0}
{"function": "\n\ndef _set_play_state(self, state):\n    'Helper method for play/pause/toggle.'\n    players = self._get_players()\n    if (len(players) != 0):\n        self._server.Player.PlayPause(players[0]['playerid'], state)\n    self.update_ha_state()\n", "label": 0}
{"function": "\n\ndef test_multiple_sequences(self):\n    msa = TabularMSA([DNA('ACGT'), DNA('AG-.'), DNA('AC-.')])\n    cons = msa.consensus()\n    self.assertEqual(cons, DNA('AC--'))\n", "label": 0}
{"function": "\n\ndef appletGetDetails(*args, **kwargs):\n    '\\n\\n    .. deprecated:: 0.42.0\\n       Use :func:`applet_get_details()` instead.\\n\\n    '\n    print('dxpy.appletGetDetails is deprecated; please use applet_get_details instead.', file=sys.stderr)\n    return applet_get_details(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _prepare_ivy_xml(self, frozen_resolution, ivyxml, resolve_hash_name_for_report):\n    default_resolution = frozen_resolution.get('default')\n    if (default_resolution is None):\n        raise IvyUtils.IvyError(\"Couldn't find the frozen resolution for the 'default' ivy conf.\")\n    try:\n        jars = default_resolution.jar_dependencies\n        IvyUtils.generate_fetch_ivy(jars, ivyxml, self.confs, resolve_hash_name_for_report)\n    except Exception as e:\n        raise IvyUtils.IvyError('Failed to prepare ivy resolve: {}'.format(e))\n", "label": 0}
{"function": "\n\n@continuation\ndef imp_struct_set_cont(orig_struct, setter, field, app, env, cont, _vals):\n    from pycket.interpreter import check_one_val\n    val = check_one_val(_vals)\n    if (setter is values.w_false):\n        return orig_struct.set_with_extra_info(field, val, app, env, cont)\n    return setter.call_with_extra_info([orig_struct, val], env, cont, app)\n", "label": 0}
{"function": "\n\ndef _do_if_else_condition(self, condition):\n    '\\n        Common logic for evaluating the conditions on #if, #ifdef and\\n        #ifndef lines.\\n        '\n    self.save()\n    d = self.dispatch_table\n    if condition:\n        self.start_handling_includes()\n        d['elif'] = self.stop_handling_includes\n        d['else'] = self.stop_handling_includes\n    else:\n        self.stop_handling_includes()\n        d['elif'] = self.do_elif\n        d['else'] = self.start_handling_includes\n", "label": 0}
{"function": "\n\ndef loadWordFile(self, pre_processor=None):\n    filename = self.getDictionaryPath()\n    with codecs.open(filename, 'r', 'utf-8') as fp:\n        for word in fp.readlines():\n            if pre_processor:\n                self.add(pre_processor(word.strip()))\n            else:\n                self.add(word.strip())\n    return\n", "label": 0}
{"function": "\n\ndef test_level(self):\n    key = EncryptionKey(data='', level='SL3')\n    self.assertEquals('SL3', key.level)\n", "label": 0}
{"function": "\n\ndef __setitem__(self, key, value):\n    'Dictionary style assignment.'\n    (rval, cval) = self.value_encode(value)\n    self.__set(key, rval, cval)\n", "label": 0}
{"function": "\n\n@sig((((H / ((H / 'a') >> bool)) >> ['a']) >> [int]))\ndef findIndicies(f, xs):\n    '\\n    findIndices :: (a -> Bool) -> [a] -> [Int]\\n\\n    The findIndices function extends findIndex, by returning the indices of all\\n    elements satisfying the predicate, in ascending order.\\n    '\n    return L[(i for (i, x) in enumerate(xs) if f(x))]\n", "label": 0}
{"function": "\n\ndef __init__(self, mediator=None):\n    'Initializes the scanner object.\\n\\n    Args:\\n      mediator: a volume scanner mediator (instance of\\n                VolumeScannerMediator) or None.\\n    '\n    super(VolumeScanner, self).__init__()\n    self._mediator = mediator\n    self._source_path = None\n    self._source_scanner = source_scanner.SourceScanner()\n    self._source_type = None\n    self._vss_stores = None\n", "label": 0}
{"function": "\n\ndef get_all_active_nodes(self, is_running=None):\n    if self.active_gen_id:\n        return self.get_all_nodes(self.active_gen_id, is_running=is_running)\n    return []\n", "label": 0}
{"function": "\n\ndef test_basic_start(self):\n    configjson = self.experiment.do_start_experiment()\n    self.assertIsNotNone(configjson)\n", "label": 0}
{"function": "\n\ndef test_options_disallowed(self):\n    request = factory.options('/', HTTP_AUTHORIZATION=self.disallowed_credentials)\n    response = root_view(request, pk='1')\n    self.assertEqual(response.status_code, status.HTTP_200_OK)\n    self.assertNotIn('actions', response.data)\n    request = factory.options('/1', HTTP_AUTHORIZATION=self.disallowed_credentials)\n    response = instance_view(request, pk='1')\n    self.assertEqual(response.status_code, status.HTTP_200_OK)\n    self.assertNotIn('actions', response.data)\n", "label": 0}
{"function": "\n\ndef test__build_key_none(self):\n    result = filecache._build_key(None, None)\n    self.assertEqual('None:None', result)\n", "label": 0}
{"function": "\n\ndef _option_required(self, key):\n    conf = S3_CONF.copy()\n    del conf[key]\n    try:\n        self.store = Store(test_utils.TestConfigOpts(conf))\n        return (self.store.add == self.store.add_disabled)\n    except:\n        return False\n    return False\n", "label": 0}
{"function": "\n\ndef show_analyzer_list_panel(self, callback):\n    list_panel = AnalyzerListPanel(self.window, self.client, self.settings.index)\n    list_panel.show(callback)\n", "label": 0}
{"function": "\n\ndef clone_settings(self, original):\n    self.replace_tabs_by_spaces = original.replace_tabs_by_spaces\n    self.safe_save = original.replace_tabs_by_spaces\n    self.clean_trailing_whitespaces = original.clean_trailing_whitespaces\n    self.restore_cursor = original.restore_cursor\n", "label": 0}
{"function": "\n\ndef _is_verified_address(self, address):\n    if (address in self.addresses):\n        return True\n    (user, host) = address.split('@', 1)\n    return (host in self.domains)\n", "label": 0}
{"function": "\n\ndef deserialize(self, raw_value):\n    if (raw_value.upper() in self.TRUE_RAW_VALUES):\n        return True\n    elif (raw_value.upper() in self.FALSE_RAW_VALUES):\n        return False\n    else:\n        raise DeserializationError('Value \"{}\" must be one of {} for \"{}\"!'.format(raw_value, self.ALLOWED_RAW_VALUES, self.name), raw_value, self.name)\n", "label": 0}
{"function": "\n\ndef _create_service_command(self, name, command):\n    command_table = self.create_command_table(command.get('subcommands', {\n        \n    }), self._create_operation_command)\n    service_command = ServiceCommand(name, None)\n    service_command._service_model = {\n        \n    }\n    service_command._command_table = command_table\n    return service_command\n", "label": 0}
{"function": "\n\n@override_settings(DEFAULT_FROM_EMAIL='foo@bar.com')\ndef test_sender_from_email(self):\n    '\\n        Should use DEFAULT_FROM_EMAIL instead of the default\\n        '\n\n    class SiteMock():\n        name = 'foo'\n        domain = 'bar.com'\n\n    def monkey_get_current_site(*args, **kw):\n        return SiteMock\n\n    def monkey_render_to_string(*args, **kw):\n        return 'email body'\n    req = RequestFactory().get('/')\n    token = 'token'\n    subject = SiteMock.name\n    template_name = 'template.html'\n    context = {\n        'user_id': self.user.pk,\n        'token': token,\n    }\n    (org_site, email.get_current_site) = (email.get_current_site, monkey_get_current_site)\n    (org_render_to_string, email.render_to_string) = (email.render_to_string, monkey_render_to_string)\n    try:\n        sender(req, subject, template_name, context, [self.user.email])\n    finally:\n        email.get_current_site = org_site\n        email.render_to_string = org_render_to_string\n    self.assertEquals(len(mail.outbox), 1)\n    self.assertEquals(mail.outbox[0].from_email, 'foo@bar.com')\n", "label": 0}
{"function": "\n\ndef extract_info(self, body):\n    '\\n        Extract metadata url\\n        '\n    xhr_url_match = re.search(self._XHR_REQUEST_PATH, body)\n    if (xhr_url_match is not None):\n        xhr_url = xhr_url_match.group(1)\n    else:\n        xhr_url = None\n    if ((xhr_url is not None) and xhr_url.endswith('xml')):\n        default_filename = xhr_url.split('/')[1]\n    else:\n        self.error(ExtractionError, \"ERROR: can't get default_filename.\")\n    return {\n        'default_filename': default_filename,\n        'xhr_url': xhr_url,\n    }\n", "label": 0}
{"function": "\n\ndef test_content_length_0(self):\n\n    class ContentLengthChecker(list):\n\n        def __init__(self):\n            list.__init__(self)\n            self.content_length = None\n\n        def append(self, item):\n            kv = item.split(b':', 1)\n            if ((len(kv) > 1) and (kv[0].lower() == b'content-length')):\n                self.content_length = kv[1].strip()\n            list.append(self, item)\n    conn = client.HTTPConnection('example.com')\n    conn.sock = FakeSocket(None)\n    conn._buffer = ContentLengthChecker()\n    conn.request('POST', '/', '')\n    self.assertEqual(conn._buffer.content_length, b'0', 'Header Content-Length not set')\n    conn = client.HTTPConnection('example.com')\n    conn.sock = FakeSocket(None)\n    conn._buffer = ContentLengthChecker()\n    conn.request('PUT', '/', '')\n    self.assertEqual(conn._buffer.content_length, b'0', 'Header Content-Length not set')\n", "label": 0}
{"function": "\n\ndef deserialize(self, obj):\n    return datetime.datetime.strptime(obj, self.format).time()\n", "label": 0}
{"function": "\n\ndef seek(self, offset, whence=os.SEEK_SET):\n    'Seek to the provided location in the file.\\n\\n        :param offset: location to seek to\\n        :type offset: int\\n        :param whence: determines whether `offset` represents a\\n                       location that is absolute, relative to the\\n                       beginning of the file, or relative to the end\\n                       of the file\\n        :type whence: os.SEEK_SET | os.SEEK_CUR | os.SEEK_END\\n        :returns: None\\n        :rtype: None\\n        '\n    if (whence == os.SEEK_SET):\n        self._cursor = (0 + offset)\n    elif (whence == os.SEEK_CUR):\n        self._cursor += offset\n    elif (whence == os.SEEK_END):\n        self._cursor = (self.size() + offset)\n    else:\n        raise ValueError('Unexpected value for `whence`: {}'.format(whence))\n", "label": 0}
{"function": "\n\ndef create_security_groups(self):\n    for hostdef in self.blueprint.host_definitions.all():\n        sg_name = 'stackdio-managed-{0}-stack-{1}'.format(hostdef.slug, self.pk)\n        sg_description = 'stackd.io managed security group'\n        account = hostdef.cloud_image.account\n        if (not account.create_security_groups):\n            logger.debug('Skipping creation of {0} because security group creation is turned off for the account'.format(sg_name))\n            continue\n        driver = account.get_driver()\n        try:\n            sg_id = driver.create_security_group(sg_name, sg_description, delete_if_exists=True)\n        except Exception as e:\n            err_msg = 'Error creating security group: {0}'.format(str(e))\n            self.set_status('create_security_groups', self.ERROR, err_msg, Level.ERROR)\n        logger.debug('Created security group {0}: {1}'.format(sg_name, sg_id))\n        for access_rule in hostdef.access_rules.all():\n            driver.authorize_security_group(sg_id, {\n                'protocol': access_rule.protocol,\n                'from_port': access_rule.from_port,\n                'to_port': access_rule.to_port,\n                'rule': access_rule.rule,\n            })\n        self.security_groups.create(account=account, blueprint_host_definition=hostdef, name=sg_name, description=sg_description, group_id=sg_id, is_managed=True)\n", "label": 0}
{"function": "\n\ndef test_perturb_inv(self):\n    pmat = perturb_inv(closure(self.cdata1), closure([0.1, 0.1, 0.1]))\n    imat = perturb(closure(self.cdata1), closure([10, 10, 10]))\n    npt.assert_allclose(pmat, imat)\n    pmat = perturb_inv(closure(self.cdata1), closure([1, 1, 1]))\n    npt.assert_allclose(pmat, closure([[0.2, 0.2, 0.6], [0.4, 0.4, 0.2]]))\n    pmat = perturb_inv(closure(self.cdata5), closure([0.1, 0.1, 0.1]))\n    imat = perturb(closure(self.cdata1), closure([10, 10, 10]))\n    npt.assert_allclose(pmat, imat)\n    with self.assertRaises(ValueError):\n        perturb_inv(closure(self.cdata1), self.bad1)\n    perturb_inv(self.cdata2, [1, 2, 3])\n    npt.assert_allclose(self.cdata2, np.array([2, 2, 6]))\n", "label": 0}
{"function": "\n\ndef get_style(self, attribute):\n    \"Get the document's named style at the caret's current position.\\n\\n        If there is a text selection and the style varies over the selection,\\n        `pyglet.text.document.STYLE_INDETERMINATE` is returned.\\n\\n        :Parameters:\\n            `attribute` : str\\n                Name of style attribute to retrieve.  See\\n                `pyglet.text.document` for a list of recognised attribute\\n                names.\\n\\n        :rtype: object\\n        \"\n    if ((self._mark is None) or (self._mark == self._position)):\n        try:\n            return self._next_attributes[attribute]\n        except KeyError:\n            return self._layout.document.get_style(attribute, self._position)\n    start = min(self._position, self._mark)\n    end = max(self._position, self._mark)\n    return self._layout.document.get_style_range(attribute, start, end)\n", "label": 0}
{"function": "\n\ndef __call__(self, feature=None):\n    if (not current_app):\n        log.warn(\"Got a request to check for {feature} but we're outside the request context. Returning False\".format(feature=feature))\n        return False\n    try:\n        return self.model.check(feature)\n    except NoResultFound:\n        raise NoFeatureFlagFound()\n", "label": 0}
{"function": "\n\ndef run_and_expect(self, joined_params, retcode, extra_args=['--local-scheduler', '--no-lock']):\n    with self.assertRaises(SystemExit) as cm:\n        luigi_run((joined_params.split(' ') + extra_args))\n    self.assertEqual(cm.exception.code, retcode)\n", "label": 0}
{"function": "\n\ndef process(self, value):\n    return format_date_time_sqlite(value)\n", "label": 0}
{"function": "\n\ndef populate_link(self, finder, upgrade):\n    'Ensure that if a link can be found for this, that it is found.\\n\\n        Note that self.link may still be None - if Upgrade is False and the\\n        requirement is already installed.\\n        '\n    if (self.link is None):\n        self.link = finder.find_requirement(self, upgrade)\n", "label": 0}
{"function": "\n\ndef get_list_display_links(self, request, list_display):\n    '\\n        Return a sequence containing the fields to be displayed as links\\n        on the changelist. The list_display parameter is the list of fields\\n        returned by get_list_display().\\n        '\n    if (self.list_display_links or (self.list_display_links is None) or (not list_display)):\n        return self.list_display_links\n    else:\n        return list(list_display)[:1]\n", "label": 0}
{"function": "\n\ndef __init__(self, canvas, pf, config):\n    super(Win32CanvasConfig, self).__init__(canvas, config)\n    self._pf = pf\n    self._pfd = PIXELFORMATDESCRIPTOR()\n    _gdi32.DescribePixelFormat(canvas.hdc, self._pf, sizeof(PIXELFORMATDESCRIPTOR), byref(self._pfd))\n    self.double_buffer = bool((self._pfd.dwFlags & PFD_DOUBLEBUFFER))\n    self.sample_buffers = 0\n    self.samples = 0\n    self.stereo = bool((self._pfd.dwFlags & PFD_STEREO))\n    self.buffer_size = self._pfd.cColorBits\n    self.red_size = self._pfd.cRedBits\n    self.green_size = self._pfd.cGreenBits\n    self.blue_size = self._pfd.cBlueBits\n    self.alpha_size = self._pfd.cAlphaBits\n    self.accum_red_size = self._pfd.cAccumRedBits\n    self.accum_green_size = self._pfd.cAccumGreenBits\n    self.accum_blue_size = self._pfd.cAccumBlueBits\n    self.accum_alpha_size = self._pfd.cAccumAlphaBits\n    self.depth_size = self._pfd.cDepthBits\n    self.stencil_size = self._pfd.cStencilBits\n    self.aux_buffers = self._pfd.cAuxBuffers\n", "label": 0}
{"function": "\n\ndef __set_db_what(self, what):\n    self._db_what = what\n    self.is_dirty = True\n", "label": 0}
{"function": "\n\ndef extendMarkdown(self, md, md_globals):\n    ' Replace subscript with SubscriptPattern '\n    md.inlinePatterns.add('subscript', SimpleTagPattern(SUBSCRIPT_RE, 'sub'), '<not_strong')\n", "label": 0}
{"function": "\n\ndef _services_dns_createRecord_php_WITH_EXTRA_PARAMS(self, method, url, body, headers):\n    body = self.fixtures.load('create_record_WITH_EXTRA_PARAMS.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 0}
{"function": "\n\n@defer.inlineCallbacks\ndef test_GetMoreCount(self):\n    counter = _CallCounter(MongoClientProtocol.send_GETMORE)\n    self.patch(MongoClientProtocol, 'send_GETMORE', counter)\n    (yield self.coll.insert([{\n        'x': 42,\n    } for _ in range(20)]))\n    result = (yield self.coll.find({\n        \n    }, limit=10))\n    self.assertEqual(len(result), 10)\n    self.assertEqual(counter.call_count, 0)\n", "label": 0}
{"function": "\n\ndef print_selection(self, *e):\n    if (self._root is None):\n        return\n    if (self._selection is None):\n        tkinter.messagebox.showerror('Print Error', 'No tree selected')\n    else:\n        c = self._cframe.canvas()\n        for widget in self._treewidgets:\n            if (widget is not self._selection):\n                self._cframe.destroy_widget(widget)\n        c.delete(self._selectbox)\n        (x1, y1, x2, y2) = self._selection.bbox()\n        self._selection.move((10 - x1), (10 - y1))\n        c['scrollregion'] = ('0 0 %s %s' % (((x2 - x1) + 20), ((y2 - y1) + 20)))\n        self._cframe.print_to_file()\n        self._treewidgets = [self._selection]\n        self.clear()\n        self.update()\n", "label": 0}
{"function": "\n\ndef smart_split(text):\n    '\\n    Generator that splits a string by spaces, leaving quoted phrases together.\\n    Supports both single and double quotes, and supports escaping quotes with\\n    backslashes. In the output, strings will keep their initial and trailing\\n    quote marks and escaped quotes will remain escaped (the results can then\\n    be further processed with unescape_string_literal()).\\n\\n    >>> list(smart_split(r\\'This is \"a person\\\\\\'s\" test.\\'))\\n    [\\'This\\', \\'is\\', \\'\"a person\\\\\\\\\\\\\\'s\"\\', \\'test.\\']\\n    >>> list(smart_split(r\"Another \\'person\\\\\\'s\\' test.\"))\\n    [\\'Another\\', \"\\'person\\\\\\\\\\'s\\'\", \\'test.\\']\\n    >>> list(smart_split(r\\'A \"\\\\\"funky\\\\\" style\" test.\\'))\\n    [\\'A\\', \\'\"\\\\\\\\\"funky\\\\\\\\\" style\"\\', \\'test.\\']\\n    '\n    text = force_text(text)\n    for bit in smart_split_re.finditer(text):\n        (yield bit.group(0))\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('model_class', (ModelWithVanillaMoneyField, ModelWithChoicesMoneyField))\ndef test_currency_querying(self, model_class):\n    model_class.objects.create(money=Money('100.0', moneyed.ZWN))\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.USD)).count() == 0)\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.ZWN)).count() == 1)\n", "label": 0}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    return shapes\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    quiet = options.get('quiet', False)\n    codes = self.get_codes()\n    if (not quiet):\n        if codes:\n            self.stdout.write(('Will now delete codes: %s \\n' % codes))\n        else:\n            self.stdout.write('No Object codes to delete. \\n')\n    revisions = self.get_revisions()\n    if (not quiet):\n        if codes:\n            self.stdout.write((\"Will now delete additional doc's revisions in docs: %s \\n\" % [d[0] for d in revisions]))\n        else:\n            self.stdout.write('No additional revision files to delete. \\n')\n    if (codes or revisions):\n        processor = core.document_processor.DocumentProcessor()\n        user = User.objects.filter(is_superuser=True)[0]\n        for code in codes:\n            processor.delete(code, {\n                'user': user,\n            })\n            if (not processor.errors):\n                if (not quiet):\n                    self.stdout.write(('Permanently deleted object with code: %s' % code))\n            else:\n                if (not quiet):\n                    self.stdout.write(processor.errors)\n                raise (Exception, processor.errors)\n        for rev in revisions:\n            processor.delete(rev[0], {\n                'user': user,\n                'delete_revision': rev[1],\n            })\n", "label": 1}
{"function": "\n\ndef put(self, key, value):\n    ' Updates or inserts data for a specified key '\n    url = ((self.base_url + '/') + str(key))\n    headers = {\n        'content-type': 'application/json',\n    }\n    jvalue = jsonpickle.encode(value)\n    data = self.session.put(url, data=jvalue, headers=headers)\n    logging.debug(('Sending request to ' + url))\n    if (data.status_code == 200):\n        logging.debug(((('The value ' + str(value)) + ' was put in the region for the key ') + str(key)))\n        return True\n    else:\n        self.error_response(data)\n", "label": 0}
{"function": "\n\ndef utcoffset(self, dt):\n    'Returns minutesEast from the constructor, as a datetime.timedelta.'\n    return self.offset\n", "label": 0}
{"function": "\n\ndef tearDown(self):\n    for fname in os.listdir(self.tempdir):\n        os.remove(os.path.join(self.tempdir, fname))\n    os.rmdir(self.tempdir)\n", "label": 0}
{"function": "\n\n@property\ndef responses(self):\n    return [response for (request, response) in self.data]\n", "label": 0}
{"function": "\n\ndef _create_hdfs_workflow_dir(self, where, job):\n    constructed_dir = ('/user/%s/' % self.get_hdfs_user())\n    constructed_dir = self._add_postfix(constructed_dir)\n    constructed_dir += ('%s/%s' % (job.name, six.text_type(uuid.uuid4())))\n    with remote.get_remote(where) as r:\n        self.create_hdfs_dir(r, constructed_dir)\n    return constructed_dir\n", "label": 0}
{"function": "\n\ndef _delegate_getter(self, object, name):\n    return getattr(self.delegate, self.name)\n", "label": 0}
{"function": "\n\n@mock.patch('pushmanager.servlets.testtag.urllib2.urlopen')\ndef test_generate_test_tag_no_url(self, mock_urlopen):\n    m = mock.Mock()\n    m.read.side_effect = ['{\"tag\" : \"tag 0 fails\"}', '{\"url\" : \"\"}']\n    mock_urlopen.return_value = m\n    MockedSettings['tests_tag'] = {\n        \n    }\n    MockedSettings['tests_tag']['tag'] = 'test'\n    MockedSettings['tests_tag']['tag_api_endpoint'] = 'example.com'\n    MockedSettings['tests_tag']['tag_api_body'] = '{ \"sha\" : \"%SHA%\" }'\n    MockedSettings['tests_tag']['url_api_endpoint'] = 'http://example.com/api/v1/test_results_url'\n    MockedSettings['tests_tag']['url_api_body'] = '{ \"sha\" : \"%SHA%\" }'\n    MockedSettings['tests_tag']['url_tmpl'] = 'www.example.com/%ID%'\n    request_info = {\n        'tags': 'test',\n        'branch': 'test',\n        'revision': 'abc123',\n    }\n    with mock.patch.dict(Settings, MockedSettings):\n        gen_tags = TestTagServlet._gen_test_tag_resp(request_info)\n        T.assert_equals({\n            'tag': 'tag 0 fails',\n            'url': '',\n        }, gen_tags)\n", "label": 0}
{"function": "\n\ndef _process_element(self, element):\n    'Process first level element of the stream.\\n\\n        The element may be stream error or features, StartTLS\\n        request/response, SASL request/response or a stanza.\\n\\n        :Parameters:\\n            - `element`: XML element\\n        :Types:\\n            - `element`: :etree:`ElementTree.Element`\\n        '\n    tag = element.tag\n    if (tag in self._element_handlers):\n        handler = self._element_handlers[tag]\n        logger.debug('Passing element {0!r} to method {1!r}'.format(element, handler))\n        handled = handler(self, element)\n        if handled:\n            return\n    if tag.startswith(self._stanza_namespace_p):\n        stanza = stanza_factory(element, self, self.language)\n        self.uplink_receive(stanza)\n    elif (tag == ERROR_TAG):\n        error = StreamErrorElement(element)\n        self.process_stream_error(error)\n    elif (tag == FEATURES_TAG):\n        logger.debug('Got features element: {0}'.format(serialize(element)))\n        self._got_features(element)\n    else:\n        logger.debug('Unhandled element: {0}'.format(serialize(element)))\n        logger.debug(' known handlers: {0!r}'.format(self._element_handlers))\n", "label": 0}
{"function": "\n\ndef relax():\n    selection = pm.ls(sl=1)\n    if (not selection):\n        return\n    verts = pm.ls(pm.polyListComponentConversion(tv=1))\n    if (not verts):\n        return\n    shape = verts[0].node()\n    dup = shape.duplicate()[0]\n    dup_shape = dup.getShape()\n    pm.polyAverageVertex(verts, i=1, ch=0)\n    ta_node = pm.transferAttributes(dup, verts, transferPositions=True, transferNormals=False, transferUVs=False, transferColors=False, sampleSpace=0, searchMethod=0, flipUVs=False, colorBorders=1)\n    pm.delete(shape, ch=1)\n    pm.delete(dup)\n    pm.select(selection)\n", "label": 0}
{"function": "\n\ndef archive(self):\n    'Archives an experiment'\n    pipe = self.redis.pipeline(transaction=True)\n    pipe.srem(ACTIVE_EXPERIMENTS_REDIS_KEY, self.name)\n    pipe.sadd(ARCHIVED_EXPERIMENTS_REDIS_KEY, self.name)\n    pipe.execute()\n", "label": 0}
{"function": "\n\ndef mutable_total_billed_ops(self, i):\n    return self.total_billed_ops_[i]\n", "label": 0}
{"function": "\n\ndef get_diffs(self, commit):\n    return commit.parents[0].diff(commit, create_patch=True)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/vehicle/military/shared_military_c.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef test_tee_del_backward(self):\n    (forward, backward) = tee(repeat(None, 20000000))\n    any(forward)\n    del backward\n", "label": 0}
{"function": "\n\ndef test_stats(self):\n    (key, stats) = self.memcache.get_stats()[0]\n    self.assertEqual('127.0.0.1:21122 (1)', key)\n    keys = ['bytes', 'pid', 'time', 'limit_maxbytes', 'cmd_get', 'version', 'bytes_written', 'cmd_set', 'get_misses', 'total_connections', 'curr_connections', 'curr_items', 'uptime', 'get_hits', 'total_items', 'rusage_system', 'rusage_user', 'bytes_read']\n    for key in keys:\n        self.assert_(stats.has_key(key), (\"key '%s' is not in stats\" % key))\n", "label": 0}
{"function": "\n\ndef onMouseUp(self, sender, x, y):\n    (x, y) = self.adjustMousePos(x, y)\n    if self.mousehandler:\n        self.mousetarget.onBrowserEvent(DOM.eventGetCurrentEvent())\n    else:\n        self.mousetarget.onMouseUp(sender, x, y)\n", "label": 0}
{"function": "\n\ndef query_lookupd(self):\n    self.logger.debug('querying lookupd...')\n    lookupd = next(self.iterlookupds)\n    try:\n        producers = lookupd.lookup(self.topic)['producers']\n        self.logger.debug(('found %d producers' % len(producers)))\n    except Exception as error:\n        msg = 'Failed to lookup %s on %s (%s)'\n        self.logger.warn((msg % (self.topic, lookupd.address, error)))\n        return\n    for producer in producers:\n        conn = Nsqd((producer.get('broadcast_address') or producer['address']), producer['tcp_port'], producer['http_port'], **self.conn_kwargs)\n        self.connect_to_nsqd(conn)\n", "label": 0}
{"function": "\n\ndef get_infra_name(host_id):\n    'Return DATABASE_INFRA_NAME'\n    from physical.models import Host\n    host = Host.objects.filter(id=host_id).select_related('instance').select_related('databaseinfra')\n    try:\n        host = host[0]\n    except IndexError as e:\n        LOG.warn('Host id does not exists: {}. {}'.format(host_id, e))\n        return None\n    return host.instance_set.all()[0].databaseinfra.name\n", "label": 0}
{"function": "\n\ndef buildIndex(self, l):\n    index = self.mIndex()\n    for (start, end, value) in self.l:\n        index.add(start, end)\n    return index\n", "label": 0}
{"function": "\n\ndef __init__(self, uri):\n    self.client = pymongo.MongoClient(uri)\n    self.cache = {\n        \n    }\n    self.uri = uri\n", "label": 0}
{"function": "\n\ndef test_escaping(self):\n    text = '<p>Hello World!'\n    app = flask.Flask(__name__)\n\n    @app.route('/')\n    def index():\n        return flask.render_template('escaping_template.html', text=text, html=flask.Markup(text))\n    lines = app.test_client().get('/').data.splitlines()\n    self.assert_equal(lines, ['&lt;p&gt;Hello World!', '<p>Hello World!', '<p>Hello World!', '<p>Hello World!', '&lt;p&gt;Hello World!', '<p>Hello World!'])\n", "label": 0}
{"function": "\n\ndef paid_totals_for(self, year, month):\n    return self.during(year, month).filter(paid=True).aggregate(total_amount=models.Sum('amount'), total_fee=models.Sum('fee'), total_refunded=models.Sum('amount_refunded'))\n", "label": 0}
{"function": "\n\ndef visit_Assign(self, node):\n    left = self.visit(node.left)\n    right = self.visit(node.right)\n    ldelay = (self.visit(node.ldelay.value) if (node.ldelay is not None) else None)\n    rdelay = (self.visit(node.rdelay.value) if (node.rdelay is not None) else None)\n    subst = vtypes.Subst(left, right, ldelay=ldelay, rdelay=rdelay)\n    assign = vtypes.Assign(subst)\n    self.add_object(assign)\n    return assign\n", "label": 0}
{"function": "\n\ndef copy(self):\n    res = LoopType()\n    for (key, value) in self.__dict__.iteritems():\n        setattr(res, key, value)\n    return res\n", "label": 0}
{"function": "\n\ndef test_object_list_delimiter(self):\n    self.requests_mock.register_uri('GET', (FAKE_URL + '/qaz?delimiter=%7C'), json=LIST_OBJECT_RESP, status_code=200)\n    ret = self.api.object_list(container='qaz', delimiter='|')\n    self.assertEqual(LIST_OBJECT_RESP, ret)\n", "label": 0}
{"function": "\n\ndef test_list_switch_machines(self):\n    url = '/switches/2/machines'\n    return_value = self.get(url)\n    resp = json.loads(return_value.get_data())\n    count = len(resp)\n    self.assertEqual(count, 2)\n    self.assertEqual(return_value.status_code, 200)\n    url = '/switches/99/machines'\n    return_value = self.get(url)\n    self.assertEqual(return_value.status_code, 404)\n", "label": 0}
{"function": "\n\ndef full_clean(self):\n    '\\n        Cleans all of self.data and populates self._errors.\\n        '\n    self._errors = []\n    if (not self.is_bound):\n        return\n    for i in range(0, self.total_form_count()):\n        form = self.forms[i]\n        self._errors.append(form.errors)\n    try:\n        self.clean()\n    except ValidationError as e:\n        self._non_form_errors = self.error_class(e.messages)\n", "label": 0}
{"function": "\n\n@retry()\ndef node_list(self):\n    return [item.name() for item in self.conn.listAllDomains()]\n", "label": 0}
{"function": "\n\n@classmethod\ndef convert_json(cls, d, convert):\n    new_d = {\n        \n    }\n    for (k, v) in d.iteritems():\n        new_d[convert(k)] = (cls.convert_json(v, convert) if isinstance(v, dict) else v)\n    return new_d\n", "label": 0}
{"function": "\n\ndef add_dependency_links(self, links):\n    if self.process_dependency_links:\n        warnings.warn('Dependency Links processing has been deprecated and will be removed in a future release.', RemovedInPip8Warning)\n        self.dependency_links.extend(links)\n", "label": 0}
{"function": "\n\ndef test_deepcopy_shared_container(self):\n    (a, x) = T.scalars('ax')\n    h = function([In(a, value=0.0)], a)\n    f = function([x, In(a, value=h.container[a], implicit=True)], (x + a))\n    try:\n        memo = {\n            \n        }\n        ac = copy.deepcopy(a)\n        memo.update({\n            id(a): ac,\n        })\n        hc = copy.deepcopy(h, memo=memo)\n        memo.update({\n            id(h): hc,\n        })\n        fc = copy.deepcopy(f, memo=memo)\n    except NotImplementedError as e:\n        if e[0].startswith('DebugMode is not picklable'):\n            return\n        else:\n            raise\n    h[a] = 1\n    hc[ac] = 2\n    self.assertTrue((f[a] == 1))\n    self.assertTrue((fc[ac] == 2))\n", "label": 0}
{"function": "\n\ndef test_rerun_after_depletion_calls_once(self):\n    'Ensure MessageIterator works when used manually.'\n    from furious.batcher import MessageIterator\n    payload = '[\"test\"]'\n    task = Mock(payload=payload, tag='tag')\n    iterator = MessageIterator('tag', 'qn', 1)\n    with patch.object(iterator, 'queue') as queue:\n        queue.lease_tasks_by_tag.return_value = [task]\n        results = [payload for payload in iterator]\n        self.assertEqual(results, [payload])\n        results = [payload for payload in iterator]\n    queue.lease_tasks_by_tag.assert_called_once_with(60, 1, tag='tag', deadline=10)\n", "label": 0}
{"function": "\n\ndef run(self, suite):\n    filtered_test = FilterSuite(suite, self.ShouldTestRun)\n    return super(_RunnerImpl, self).run(filtered_test)\n", "label": 0}
{"function": "\n\ndef _untagged_response(self, typ, dat, name):\n    if (typ == 'NO'):\n        return (typ, dat)\n    data = self._get_untagged_response(name)\n    if (not data):\n        return (typ, [None])\n    while True:\n        dat = self._get_untagged_response(name)\n        if (not dat):\n            break\n        data += dat\n    if __debug__:\n        self._log(4, ('_untagged_response(%s, ?, %s) => %.80r' % (typ, name, data)))\n    return (typ, data)\n", "label": 0}
{"function": "\n\n@mock.patch.object(shade.OpenStackCloud, 'search_subnets')\n@mock.patch.object(shade.OpenStackCloud, 'neutron_client')\ndef test_delete_subnet_not_found(self, mock_client, mock_search):\n    mock_search.return_value = []\n    r = self.cloud.delete_subnet('goofy')\n    self.assertFalse(r)\n    self.assertFalse(mock_client.delete_subnet.called)\n", "label": 0}
{"function": "\n\ndef get_conf_from_module(mod):\n    'return configuration from module with defaults no worry about None type\\n\\n    '\n    conf = ModuleConfig(CONF_SPEC)\n    mod = _get_correct_module(mod)\n    conf.set_module(mod)\n    if hasattr(mod, 'default'):\n        default = mod.default\n        conf = extract_conf_from(default, conf)\n    else:\n        conf = extract_conf_from(mod, conf)\n    return conf\n", "label": 0}
{"function": "\n\ndef onBeforeTabSelected(self, sender, tabIndex):\n    if (self.fTabs.getWidgetCount() == 6):\n        self.fTabs.add(HTML('2nd Test.<br />Tab should be on right'), '2nd Test', name='test2')\n        return True\n    self.fTabs.remove('test2')\n    return (tabIndex != 6)\n", "label": 0}
{"function": "\n\ndef test_create_handler_with_str_method_name(self):\n\n    @endpoint('/api', 'GET')\n    def fake_handler(request, *args, **kwargs):\n        pass\n    (path, handler, methods, name) = fake_handler()\n    self.assertEqual(methods, 'GET')\n", "label": 0}
{"function": "\n\ndef _prep_loader_attrs(self, mapping):\n    self.loader.source = mapping['generated_filename']\n    self.loader.election_id = mapping['election']\n    self.loader.timestamp = datetime.datetime.now()\n", "label": 0}
{"function": "\n\ndef test_remove_unpickables_http_exception(self):\n    try:\n        urllib2.urlopen('http://localhost/this.does.not.exist')\n        self.fail('exception expected')\n    except urllib2.URLError as e:\n        pass\n    except urllib2.HTTPError as e:\n        pass\n    removed = mapper.remove_unpickables(e)\n    pickled = pickle.dumps(removed)\n    pickle.loads(pickled)\n", "label": 0}
{"function": "\n\ndef test_has_error_type(self):\n    error = spotify.LibError(0)\n    self.assertEqual(error.error_type, 0)\n    error = spotify.LibError(1)\n    self.assertEqual(error.error_type, 1)\n", "label": 0}
{"function": "\n\n@classmethod\ndef make(cls, value, cache=None, timeout=None):\n    self = CacheKey(value)\n    self.cache = cache\n    self.timeout = timeout\n    return self\n", "label": 0}
{"function": "\n\ndef test_stdinCache_trailing_backslash_3(self):\n    stdinCache = StdinCache.StdinCache()\n    stdinCache.refreshFromText(dedentAndStrip('\\n                x+                z+                y\\n                f+                g+                h\\n                '))\n    self.assertEqual(len(stdinCache.blocks), 2)\n", "label": 0}
{"function": "\n\n@property\ndef parents(self):\n    if (self._parents is None):\n        self._parents = [self._odb.get_commit(hash) for hash in self._obj.parents]\n    return list(self._parents)\n", "label": 0}
{"function": "\n\ndef setHighlighted(self, highlighted):\n    GafferUI.PlugValueWidget.setHighlighted(self, highlighted)\n    self.__boolWidget.setHighlighted(highlighted)\n", "label": 0}
{"function": "\n\ndef gen_key(self, prefix=None):\n    if (not prefix):\n        prefix = 'python-couchbase-key_'\n    ret = '{0}{1}'.format(prefix, self._key_counter)\n    self._key_counter += 1\n    return ret\n", "label": 0}
{"function": "\n\ndef test_functions_unchanged(self):\n    s = 'def foo(): pass'\n    self.unchanged(s, from3=True)\n    s = '\\n        def foo():\\n            pass\\n            pass\\n        '\n    self.unchanged(s, from3=True)\n    s = \"\\n        def foo(bar='baz'):\\n            pass\\n            pass\\n        \"\n    self.unchanged(s, from3=True)\n", "label": 0}
{"function": "\n\ndef test_project_add_child(self):\n    project = Project()\n    child = Task()\n    project.add_child(child)\n    self.assertEquals(project, child.parent)\n    self.assertEquals(project, child.project)\n    project = Project()\n    child = Task()\n    grandchild = Task()\n    child.add_child(grandchild)\n    project.add_child(child)\n    self.assertEquals(project, child.parent)\n    self.assertEquals(project, child.project)\n    self.assertEquals(project, grandchild.project)\n", "label": 0}
{"function": "\n\ndef RegisterSubException(self, hunt_urn, plugin_name, exception):\n    self.exceptions_by_hunt.setdefault(hunt_urn, {\n        \n    }).setdefault(plugin_name, []).append(exception)\n", "label": 0}
{"function": "\n\ndef test_issue_1264(self):\n    n = 100\n    x = np.random.uniform(size=(n * 3)).reshape((n, 3))\n    expected = distance_matrix(x)\n    actual = njit(distance_matrix)(x)\n    np.testing.assert_array_almost_equal(expected, actual)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(CreateFlavorInfoAction, self).clean()\n    name = cleaned_data.get('name')\n    flavor_id = cleaned_data.get('flavor_id')\n    try:\n        flavors = api.nova.flavor_list(self.request, None)\n    except Exception:\n        flavors = []\n        msg = _('Unable to get flavor list')\n        exceptions.check_message(['Connection', 'refused'], msg)\n        raise\n    if (flavors is not None):\n        for flavor in flavors:\n            if (flavor.name == name):\n                raise forms.ValidationError((_('The name \"%s\" is already used by another flavor.') % name))\n            if (flavor.id == flavor_id):\n                raise forms.ValidationError((_('The ID \"%s\" is already used by another flavor.') % flavor_id))\n    return cleaned_data\n", "label": 0}
{"function": "\n\ndef test_start_run(self):\n    assert_false(os.path.exists(self.result_file_path))\n    self.run_results.start_run(self.scenario)\n    assert_equal(len(self.scenario.packb()), self._current_size())\n    assert_greater(self._current_size(), 0)\n    with open(self.result_file_path, 'rb') as f:\n        unpacker = msgpack.Unpacker(file_like=f)\n        got_scenario = Scenario.unpackb(unpacker)\n        for attr in ['name', '_scenario_data', 'user_count', 'operation_count', 'run_seconds', 'container_base', 'container_count', 'containers', 'container_concurrency', 'sizes_by_name', 'version', 'bench_size_thresholds']:\n            assert_equal(getattr(got_scenario, attr), getattr(self.scenario, attr))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef info(msg, *args):\n    ((print >> sys.stderr), (msg % args))\n", "label": 0}
{"function": "\n\ndef Layout(self, request, response):\n    self.default = str(self.descriptor.type().Generate())\n    response = super(AES128KeyFormRenderer, self).Layout(request, response)\n    return self.CallJavascript(response, 'AES128KeyFormRenderer.Layout', prefix=self.prefix)\n", "label": 0}
{"function": "\n\ndef new(self, user, repo, title, body=''):\n    'Create a new issue.'\n    return self._posted('/'.join(['issues', 'open', user, repo]), title=title, body=body)\n", "label": 0}
{"function": "\n\ndef __init__(self, n):\n    assert (n == 1)\n", "label": 0}
{"function": "\n\ndef _str_allocation_pools(allocation_pools):\n    if isinstance(allocation_pools, str):\n        return allocation_pools\n    return '\\n'.join([('%s,%s' % (pool['start'], pool['end'])) for pool in allocation_pools])\n", "label": 0}
{"function": "\n\ndef _put_n_deployments(self, id_prefix, number_of_deployments, skip_creation=None, add_modification=None):\n    for i in range(0, number_of_deployments):\n        deployment_id = '{0}{1}_{2}'.format(id_prefix, str(i), 'deployment')\n        blueprint_id = '{0}{1}_{2}'.format(id_prefix, str(i), 'blueprint')\n        if (not skip_creation):\n            self.put_deployment(deployment_id=deployment_id, blueprint_id=blueprint_id)\n        if add_modification:\n            response = self._put_deployment_modification(deployment_id=deployment_id)\n            self._mark_deployment_modification_finished(modification_id=response['id'])\n", "label": 0}
{"function": "\n\ndef get_extractor(coarse, fine):\n    log.debug(\"getting fine extractor for '{}: {}'\".format(coarse, fine))\n    try:\n        extractor = importlib.import_module(((__package__ + '.') + question_types[fine]))\n    except (ImportError, KeyError):\n        log.warn(\"Extractor for fine type '{}: {}' not implemented\".format(coarse, fine))\n        raise NoExtractorError(coarse, fine)\n    return extractor.Extractor\n", "label": 0}
{"function": "\n\ndef _eval_rewrite_as_FallingFactorial(self, n, k):\n    if k.is_integer:\n        return (ff(n, k) / factorial(k))\n", "label": 0}
{"function": "\n\n@classmethod\ndef _parse_repo(cls, repo, name=None):\n    regexp = '(?P<type>deb(-src)?) (?P<uri>[^\\\\s]+) (?P<suite>[^\\\\s]+)( (?P<section>[\\\\w\\\\s]*))?(,(?P<priority>[\\\\d]+))?'\n    match = re.match(regexp, repo)\n    if (not match):\n        raise errors.IncorrectRepository(\"Couldn't parse repository '{0}'\".format(repo))\n    repo_type = match.group('type')\n    repo_suite = match.group('suite')\n    repo_section = match.group('section')\n    repo_uri = match.group('uri')\n    repo_priority = match.group('priority')\n    return {\n        'name': name,\n        'type': repo_type,\n        'uri': repo_uri,\n        'priority': repo_priority,\n        'suite': repo_suite,\n        'section': (repo_section or ''),\n    }\n", "label": 0}
{"function": "\n\ndef register_category(self, category, label, index=None):\n    if index:\n        self._categories.insert(index, category, label)\n    else:\n        self._categories[category] = label\n", "label": 0}
{"function": "\n\ndef get_formsets_with_inlines(self, request, obj=None):\n    if (request.is_add_view and (obj is not None)):\n        raise Exception(\"'obj' passed to get_formsets_with_inlines wasn't None during add_view\")\n    if ((not request.is_add_view) and (obj is None)):\n        raise Exception(\"'obj' passed to get_formsets_with_inlines was None during change_view\")\n    return super(GetFormsetsArgumentCheckingAdmin, self).get_formsets_with_inlines(request, obj)\n", "label": 0}
{"function": "\n\ndef test_id(self):\n    'Each test annotation should be created with a unique ID.'\n    annotation_1 = factories.Annotation()\n    annotation_2 = factories.Annotation()\n    assert annotation_1.get('id')\n    assert annotation_2.get('id')\n    assert (annotation_1['id'] != annotation_2['id'])\n", "label": 0}
{"function": "\n\n@property\ndef primary_key_names(self):\n    'Primary keys of the table\\n        '\n    return [c.name for c in self.columns() if c.primary]\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    super(OrderPayment, self).save(*args, **kwargs)\n    self._recalculate_paid()\n    if (self.currency != self.order.currency):\n        self.order.notes += ('\\n' + (_('Currency of payment %s does not match.') % self))\n        self.order.save()\n", "label": 0}
{"function": "\n\ndef __init__(self, activation, dims=None, **kwargs):\n    super(SpeechBottom, self).__init__(**kwargs)\n    self.num_features = self.input_dims['recordings']\n    if (activation is None):\n        activation = Tanh()\n    if dims:\n        child = MLP(([activation] * len(dims)), ([self.num_features] + dims), name='bottom')\n        self.output_dim = child.output_dim\n    else:\n        child = Identity(name='bottom')\n        self.output_dim = self.num_features\n    self.children.append(child)\n    self.mask = tensor.matrix('recordings_mask')\n    self.batch_inputs = {\n        'recordings': tensor.tensor3('recordings'),\n    }\n    self.single_inputs = {\n        'recordings': tensor.matrix('recordings'),\n    }\n", "label": 0}
{"function": "\n\ndef load(self):\n    session_data = self._cache.get(self.session_key)\n    if (session_data is not None):\n        return session_data\n    self.create()\n    return {\n        \n    }\n", "label": 0}
{"function": "\n\ndef test_name_and_description(self):\n    '\\n        Tests that the benefit proxy classes all return a name and\\n        description. Unfortunately, the current implementations means\\n        a valid range is required.\\n        This test became necessary because the complex name/description logic\\n        broke with the python_2_unicode_compatible decorator.\\n        '\n    range = factories.RangeFactory()\n    for (type, __) in Benefit.TYPE_CHOICES:\n        benefit = Benefit(type=type, range=range)\n        self.assertTrue(all([benefit.name, benefit.description, six.text_type(benefit)]))\n", "label": 0}
{"function": "\n\n@defer.deferredGenerator\ndef allapps_action(self, argstr):\n    \"Usage allapps: <method> [args]\\n\\n  dispatch the same command to all application managers.\\n\\n    <method>\\tmethod to invoke on all appmanagers.\\n    [args]\\toptional arguments to pass along.\\n\\n  examples:\\n\\n    ''            #shows help documentation for all applications\\n    'status'      #invoke status assumes there is only one instance\\n    'status all'  #invoke status on all application instances\\n    'status 0'    #invoke status on application instance label '0'\\n\\n  full cli usage:\\n\\n    $ droneblaster allapps\\n    $ droneblaster allapps status\\n    $ droneblaster allapps status all\\n    $ droneblaster allapps status 0\\n\"\n    result = {\n        \n    }\n    descriptions = []\n    code = 0\n    for obj in AppManager.objects:\n        try:\n            action = obj.action\n            if (not action):\n                continue\n            d = action(argstr)\n            wfd = defer.waitForDeferred(d)\n            (yield wfd)\n            foo = wfd.getResult()\n            descriptions.append(foo.get('description', 'None'))\n            code += int(foo.get('code', 0))\n        except:\n            pass\n    result['description'] = '\\n'.join(descriptions)\n    if (not result['description']):\n        result['description'] = 'None'\n    result['code'] = code\n    (yield result)\n", "label": 0}
{"function": "\n\n@test.attr(type='benchmark')\ndef test_002_fill_volume(self):\n    'Fill volume with data'\n    if (self.ctx.ssh is None):\n        raise self.skipException('Booting failed')\n    if (not self.ctx.volume_ready):\n        raise self.skipException('Volume preparation failed')\n    self._start_test()\n    self.ctx.ssh.exec_command('sudo mkdir -m 777 /vol/data')\n    file_lines = (102 * int(self.volume_size))\n    for i in xrange(int(self.volume_fill)):\n        self.ctx.ssh.exec_command((((\"cat /dev/urandom | tr -d -c 'a-zA-Z0-9' | fold -w 1020 | head -n \" + str(file_lines)) + ' > /vol/data/file') + str(i)))\n    self._end_test('Volume filling')\n    self.ctx.volume_filled = True\n    self._check_test()\n", "label": 0}
{"function": "\n\n@classmethod\ndef _get_data_source_properties_from_case(cls, case_properties):\n    property_map = {\n        'closed': _('Case Closed'),\n        'user_id': _('User ID Last Updating Case'),\n        'owner_name': _('Case Owner'),\n        'mobile worker': _('Mobile Worker Last Updating Case'),\n    }\n    properties = OrderedDict()\n    for property in case_properties:\n        properties[property] = DataSourceProperty(type='case_property', id=property, column_id=get_column_name(property), text=property_map.get(property, property.replace('_', ' ')), source=property)\n    properties['computed/owner_name'] = cls._get_owner_name_pseudo_property()\n    properties['computed/user_name'] = cls._get_user_name_pseudo_property()\n    return properties\n", "label": 0}
{"function": "\n\n@httprettified\ndef test_likes_with_after(self):\n    HTTPretty.register_uri(HTTPretty.GET, 'https://api.tumblr.com/v2/user/likes', body='{\"meta\": {\"status\": 200, \"msg\": \"OK\"}, \"response\": []}')\n    response = self.client.likes(after=1418684291)\n    assert (response == [])\n", "label": 0}
{"function": "\n\ndef __init__(self, mu, var, **kwargs):\n    (self.mu, self.var) = (None, None)\n    if (not isinstance(mu, Layer)):\n        (self.mu, mu) = (mu, None)\n    if (not isinstance(var, Layer)):\n        (self.var, var) = (var, None)\n    input_lst = [i for i in [mu, var] if (not (i is None))]\n    super(GaussianMarginalLogDensityLayer, self).__init__(input_lst, **kwargs)\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.message = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.log_context = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.handle = QueryHandle()\n                self.handle.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.errorCode = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.STRING):\n                self.SQLState = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef do_undef(self, t):\n    '\\n        Default handling of a #undef line.\\n        '\n    try:\n        del self.cpp_namespace[t[1]]\n    except KeyError:\n        pass\n", "label": 0}
{"function": "\n\ndef run(self, command_line=''):\n    assert command_line, 'expected non-empty command line'\n    parsed = parse_command_line(command_line)\n    self.window.run_command('tab_control', {\n        'command': 'only',\n        'forced': parsed.command.forced,\n    })\n", "label": 0}
{"function": "\n\ndef confidence(self):\n    \"\\n        Returns a tuple (chi squared, confident) of the experiment. Confident\\n        is simply a boolean specifying whether we're > 95%% sure that the\\n        results are statistically significant.\\n        \"\n    choices = self.choices\n    if (len(choices) >= 2):\n        csq = chi_squared(*choices)\n        confident = (is_confident(csq, len(choices)) if (len(choices) <= 10) else None)\n    else:\n        csq = None\n        confident = False\n    return (csq, confident)\n", "label": 0}
{"function": "\n\ndef compile_function(code, arg_names, local_dict, global_dict, module_dir, compiler='', verbose=1, support_code=None, headers=[], customize=None, type_converters=None, auto_downcast=1, **kw):\n    code = ((ndarray_api_version + '\\n') + code)\n    module_path = function_catalog.unique_module_name(code, module_dir)\n    (storage_dir, module_name) = os.path.split(module_path)\n    mod = inline_ext_module(module_name, compiler)\n    ext_func = inline_ext_function('compiled_func', code, arg_names, local_dict, global_dict, auto_downcast, type_converters=type_converters)\n    mod.add_function(ext_func)\n    if customize:\n        mod.customize = customize\n    if support_code:\n        mod.customize.add_support_code(support_code)\n    for header in headers:\n        mod.customize.add_header(header)\n    if (verbose > 0):\n        print('<weave: compiling>')\n    mod.compile(location=storage_dir, compiler=compiler, verbose=verbose, **kw)\n    try:\n        sys.path.insert(0, storage_dir)\n        exec(('import ' + module_name))\n        func = eval((module_name + '.compiled_func'))\n    finally:\n        del sys.path[0]\n    return func\n", "label": 0}
{"function": "\n\ndef _load_allowed_remote_addresses(self, app):\n    key = 'PSDASH_ALLOWED_REMOTE_ADDRESSES'\n    addrs = app.config.get(key)\n    if (not addrs):\n        return\n    if isinstance(addrs, (str, unicode)):\n        app.config[key] = [a.strip() for a in addrs.split(',')]\n", "label": 0}
{"function": "\n\ndef validate_config(self):\n    self.config.set('boss', 'data_dir', fs.abspath(self.config.get('boss', 'data_dir')))\n    if (not os.path.exists(self.config.get('boss', 'data_dir'))):\n        os.makedirs(self.config.get('boss', 'data_dir'))\n    pth = os.path.join(self.config.get('boss', 'data_dir'), 'cache')\n    if (not os.path.exists(fs.abspath(pth))):\n        os.makedirs(fs.abspath(pth))\n    self.config.set('boss', 'cache_dir', pth)\n    pth = os.path.join(self.config.get('boss', 'data_dir'), 'boss.db')\n    self.config.set('boss', 'db_path', pth)\n", "label": 0}
{"function": "\n\ndef test_length(session):\n    set_ = session.set(key('test_sortedset_length'), S('abc'), SortedSet)\n    assert (len(set_) == 3)\n    setx = session.set(key('test_sortedsetx_length'), S([1, 2, 3]), IntSet)\n    assert (len(setx) == 3)\n", "label": 0}
{"function": "\n\ndef __init__(self, notifier=None):\n    if (self.__class__.__instance is None):\n        self.__class__.__instance = self\n        if (notifier is None):\n            self._notifier = _AsyncNotifier()\n        else:\n            self._notifier = notifier\n        self._location = None\n        self._name = None\n        self._coros = {\n            \n        }\n        self._scheduled = set()\n        self._suspended = set()\n        self._timeouts = []\n        self._lock = threading.RLock()\n        self._quit = False\n        self._complete = threading.Event()\n        self._daemons = 0\n        self._polling = False\n        self._channels = {\n            \n        }\n        self._atexit = []\n        Coro._asyncoro = Channel._asyncoro = self\n        self._scheduler = threading.Thread(target=self._schedule)\n        self._scheduler.daemon = True\n        self._scheduler.start()\n        atexit.register(self.finish)\n        logger.info('version %s with %s I/O notifier', __version__, self._notifier._poller_name)\n", "label": 0}
{"function": "\n\ndef from_jsobj(jsobj, cls=None):\n    'Create an instance of the given class from a JSON object.\\n\\n    Arguments:\\n      cls: a class that serves as a \"type hint.\"\\n    '\n    if isinstance(jsobj, LIST_TYPES):\n        return [from_jsobj(o, cls=cls) for o in jsobj]\n    if (cls is not None):\n        return cls.from_jsobj(jsobj)\n    if (jsobj is None):\n        return JS_NULL\n    return jsobj\n", "label": 0}
{"function": "\n\ndef shouldDestroyCircuit(self, circuit):\n    'Return **True** iff CircuitManager thinks the calling circuit\\n        should be destroyed.\\n\\n        Circuits call shouldDestroyCircuit() when their number of open\\n        streams drops to zero. Since CircuitManager knows about all open\\n        and pending circuits, it can make an informed judgement about whether\\n        the calling circuit should be destroyed or remain open.\\n\\n        Currently, CircuitManager maintains at least 4 open or pending IPv4\\n        circuits and one open or pending IPv6 circuit. If the number of\\n        streams on any circuit drops to zero and it can be closed while still\\n        satisfying these basic constraints, then CircuitManager tells it\\n        to begin destroying itself (returns True).\\n\\n        :param oppy.circuit.circuit.Circuit circuit: circuit to\\n            consider destroying.\\n        :returns: **bool** **True** if CircuitManager decides this circuit\\n            should be destroyed, **False** otherwise.\\n        '\n    if (circuit.circuit_type == CircuitType.IPv4):\n        return ((self._totalIPv4Count() - 1) > self._min_IPv4_count)\n    else:\n        return ((self._totalIPv6Count() - 1) > self._min_IPv6_count)\n", "label": 0}
{"function": "\n\ndef contains_subsequence(seq, subseq):\n    for i in range((len(seq) - len(subseq))):\n        if (seq[i:(i + len(subseq))] == subseq):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef tables(self, db=None):\n    '\\n        Enumerates all tables fro a given database. If not specified, use the\\n        current database.\\n        '\n    if self.has_cap(TABLES_ENUM):\n        if (db is None):\n            if (self.current_db is None):\n                self.database()\n            db = self.current_db\n        n = self.get_nb_tables(db)\n        for i in range(n):\n            (yield TableWrapper(self, self.get_table_name(i, db), db))\n    else:\n        raise Unavailable()\n", "label": 0}
{"function": "\n\ndef _is_us_state(abbr, result):\n    for sep in ('/', '-'):\n        if (result.source_base == 'us{sep}{abbr}'.format(**locals())):\n            return True\n        if result.source_base.startswith('us{sep}{abbr}.'.format(**locals())):\n            return True\n        if result.source_base.startswith('us{sep}{abbr}{sep}'.format(**locals())):\n            return True\n    return False\n", "label": 0}
{"function": "\n\n@util.positional(2)\ndef error(status_code, status_message=None, content_type='text/plain; charset=utf-8', headers=None, content=None):\n    'Create WSGI application that statically serves an error page.\\n\\n  Creates a static error page specifically for non-200 HTTP responses.\\n\\n  Browsers such as Internet Explorer will display their own error pages for\\n  error content responses smaller than 512 bytes.  For this reason all responses\\n  are right-padded up to 512 bytes.\\n\\n  Error pages that are not provided will content will contain the standard HTTP\\n  status message as their content.\\n\\n  Args:\\n    status_code: Integer status code of error.\\n    status_message: Status message.\\n\\n  Returns:\\n    Static WSGI application that sends static error response.\\n  '\n    if (status_message is None):\n        status_message = httplib.responses.get(status_code, 'Unknown Error')\n    if (content is None):\n        content = status_message\n    content = util.pad_string(content)\n    return static_page(content, status=(status_code, status_message), content_type=content_type, headers=headers)\n", "label": 0}
{"function": "\n\ndef test_params(self):\n    params = np.array([r.params for r in self.results])\n    params_1 = np.array(([self.results[0].params] * len(self.results)))\n    assert_allclose(params, params_1)\n", "label": 0}
{"function": "\n\ndef usesTime(self):\n    fmt = self._fmt\n    return ((fmt.find('$asctime') >= 0) or (fmt.find(self.asctime_format) >= 0))\n", "label": 0}
{"function": "\n\ndef get_list(self, *args, **kwargs):\n    (count, data) = super(TweetView, self).get_list(*args, **kwargs)\n    query = {\n        '_id': {\n            '$in': [x['user_id'] for x in data],\n        },\n    }\n    users = db.user.find(query, fields=('name',))\n    users_map = dict(((x['_id'], x['name']) for x in users))\n    for item in data:\n        item['user_name'] = users_map.get(item['user_id'])\n    return (count, data)\n", "label": 0}
{"function": "\n\n@conf.commands.register\ndef srbt1(peer, pkts, *args, **kargs):\n    'send and receive 1 packet using a bluetooth socket'\n    (a, b) = srbt(peer, pkts, *args, **kargs)\n    if (len(a) > 0):\n        return a[0][1]\n", "label": 0}
{"function": "\n\ndef find_payload_class(payload_type):\n    'Iterate through inherited classes to find a matching class name'\n    subclasses = set()\n    work = [Payload]\n    while work:\n        parent_subclass = work.pop()\n        for child_subclass in parent_subclass.__subclasses__():\n            if (child_subclass not in subclasses):\n                if (hasattr(child_subclass, 'payload_type') and (child_subclass.payload_type == payload_type)):\n                    return child_subclass\n                subclasses.add(child_subclass)\n                work.append(child_subclass)\n    return None\n", "label": 0}
{"function": "\n\ndef parse_inline(text):\n    '\\n    Takes a string of text from a text inline and returns a 3 tuple of\\n    (name, value, **kwargs).\\n    '\n    m = INLINE_SPLITTER.match(text)\n    if (not m):\n        raise InlineUnparsableError\n    args = m.group('args')\n    name = m.group('name')\n    value = ''\n    kwtxt = ''\n    kwargs = {\n        \n    }\n    if args:\n        kwtxt = INLINE_KWARG_PARSER.search(args).group('kwargs')\n        value = re.sub(('%s\\\\Z' % kwtxt), '', args)\n        value = value.strip()\n    if m.group('variant'):\n        kwargs['variant'] = m.group('variant')\n    if kwtxt:\n        for kws in kwtxt.split():\n            (k, v) = kws.split('=')\n            kwargs[str(k)] = v\n    return (name, value, kwargs)\n", "label": 0}
{"function": "\n\ndef write(self, *args, **kwargs):\n    if (not self.file):\n        self.file = tempfile.TemporaryFile()\n    self.file.write(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _Rotate(self, image, transform):\n    'Use PIL to rotate the given image with the given transform.\\n\\n    Args:\\n      image: PIL.Image.Image object to rotate.\\n      transform: images_service_pb.Transform to use when rotating.\\n\\n    Returns:\\n      PIL.Image.Image with transforms performed on it.\\n\\n    Raises:\\n      BadRequestError if the rotate data given is bad.\\n    '\n    degrees = transform.rotate()\n    if ((degrees < 0) or ((degrees % 90) != 0)):\n        raise apiproxy_errors.ApplicationError(images_service_pb.ImagesServiceError.BAD_TRANSFORM_DATA)\n    degrees %= 360\n    degrees = (360 - degrees)\n    return image.rotate(degrees)\n", "label": 0}
{"function": "\n\ndef publish_state(self, payload, state):\n    if (not state):\n        raise Exception('Unable to publish unassigned state.')\n    self._state_publisher.publish(payload, self._state_exchange, state)\n", "label": 0}
{"function": "\n\ndef test_iter_smart_pk_range(self):\n    seen = []\n    for (start_pk, end_pk) in Author.objects.iter_smart_pk_ranges():\n        seen.extend(Author.objects.filter(id__gte=start_pk, id__lt=end_pk).values_list('id', flat=True))\n    all_ids = list(Author.objects.order_by('id').values_list('id', flat=True))\n    assert (seen == all_ids)\n", "label": 0}
{"function": "\n\ndef no_translate_debug_logs(logical_line, filename):\n    \"Check for 'LOG.debug(_('\\n\\n    As per our translation policy,\\n    https://wiki.openstack.org/wiki/LoggingStandards#Log_Translation\\n    we shouldn't translate debug level logs.\\n\\n    * This check assumes that 'LOG' is a logger.\\n    * Use filename so we can start enforcing this in specific folders instead\\n      of needing to do so all at once.\\n    S373\\n    \"\n    msg = \"S373 Don't translate debug level logs\"\n    if logical_line.startswith('LOG.debug(_('):\n        (yield (0, msg))\n", "label": 0}
{"function": "\n\ndef get_command(self, ctx, name):\n    \"\\n        Get a specific command by looking up the module.\\n\\n        :param ctx: Click context\\n        :param name: Command name\\n        :return: Module's cli function\\n        \"\n    try:\n        if (sys.version_info[0] == 2):\n            name = name.encode('ascii', 'replace')\n        mod = __import__(('cli.commands.cmd_' + name), None, None, ['cli'])\n    except ImportError as e:\n        logging.error('Error importing module {0}:\\n{0}'.format(name, e))\n        exit(1)\n    return mod.cli\n", "label": 0}
{"function": "\n\ndef all_terms(f):\n    '\\n        Returns all terms from a univariate polynomial ``f``.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy import Poly\\n        >>> from sympy.abc import x\\n\\n        >>> Poly(x**3 + 2*x - 1, x).all_terms()\\n        [((3,), 1), ((2,), 0), ((1,), 2), ((0,), -1)]\\n\\n        '\n    return [(m, f.rep.dom.to_sympy(c)) for (m, c) in f.rep.all_terms()]\n", "label": 0}
{"function": "\n\ndef ensure_role(self, role, dry_run=False):\n    '\\n        Adds the role if it does not already exist, otherwise skips it.\\n        '\n    existing_roles = Role.objects.filter(slug=role.slug)\n    if existing_roles:\n        logger.info('Role already exists: %s', role.name)\n        return existing_roles[0]\n    elif dry_run:\n        logger.info('[DRY RUN] Creating role: %s', role.name)\n    else:\n        if self.verbose:\n            logger.info('Creating role: %s', role.name)\n        role.save()\n", "label": 0}
{"function": "\n\ndef _initPopulation(self, seeds):\n    if (self.parentChildAverage < 1):\n        for s in seeds:\n            s.parent = None\n    self.pop = self._extendPopulation(seeds, self.populationSize)\n", "label": 0}
{"function": "\n\ndef _get_task_with_policy(queue_name, task_id, owner):\n    'Fetches the specified task and enforces ownership policy.\\n\\n    Args:\\n        queue_name: Name of the queue the work item is on.\\n        task_id: ID of the task that is finished.\\n        owner: Who or what has the current lease on the task.\\n\\n    Returns:\\n        The valid WorkQueue task that is currently owned.\\n\\n    Raises:\\n        TaskDoesNotExistError if the task does not exist.\\n        LeaseExpiredError if the lease is no longer active.\\n        NotOwnerError if the specified owner no longer owns the task.\\n    '\n    now = datetime.datetime.utcnow()\n    task = WorkQueue.query.filter_by(queue_name=queue_name, task_id=task_id).with_lockmode('update').first()\n    if (not task):\n        raise TaskDoesNotExistError(('task_id=%r' % task_id))\n    lease_delta = (now - task.eta)\n    if (lease_delta > datetime.timedelta(0)):\n        db.session.rollback()\n        raise LeaseExpiredError(('queue=%r, task_id=%r expired %s' % (task.queue_name, task_id, lease_delta)))\n    if (task.last_owner != owner):\n        db.session.rollback()\n        raise NotOwnerError(('queue=%r, task_id=%r, owner=%r' % (task.queue_name, task_id, task.last_owner)))\n    return task\n", "label": 0}
{"function": "\n\ndef showSublimeContext(self, filename, line):\n    debug(((('showSublimeContext: ' + str(filename)) + ' : ') + str(line)))\n    console_output((((('@@@ Stopped at ' + str(filename.replace((self.projectDir + '/'), ''))) + ':') + str(line)) + ' @@@'))\n    window = sublime.active_window()\n    if window:\n        window.focus_group(0)\n        view = window.active_view()\n        if ((view is not None) and (view.size() >= 0)):\n            filename = os.path.join(self.projectDir, filename)\n            if (view.file_name() != filename):\n                self.activateViewWithFile(filename, line)\n            window.run_command('goto_line', {\n                'line': line,\n            })\n            view = window.active_view()\n            mark = [view.line(view.text_point((line - 1), 0))]\n            view.erase_regions('current_line')\n            view.add_regions('current_line', mark, 'current_line', 'dot', sublime.DRAW_OUTLINED)\n        else:\n            debug('No current view')\n", "label": 0}
{"function": "\n\ndef test_diff_nans(self):\n    'Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/204'\n    arr = np.empty((10, 10), dtype=np.float64)\n    arr[:5] = 1.0\n    arr[5:] = np.nan\n    arr2 = arr.copy()\n    table = np.rec.array([(1.0, 2.0), (3.0, np.nan), (np.nan, np.nan)], names=['cola', 'colb']).view(fits.FITS_rec)\n    table2 = table.copy()\n    assert ImageDataDiff(arr, arr2).identical\n    assert TableDataDiff(table, table2).identical\n    arr2[0][0] = 2.0\n    arr2[5][0] = 2.0\n    table2[0][0] = 2.0\n    table2[1][1] = 2.0\n    diff = ImageDataDiff(arr, arr2)\n    assert (not diff.identical)\n    assert (diff.diff_pixels[0] == ((0, 0), (1.0, 2.0)))\n    assert (diff.diff_pixels[1][0] == (5, 0))\n    assert np.isnan(diff.diff_pixels[1][1][0])\n    assert (diff.diff_pixels[1][1][1] == 2.0)\n    diff = TableDataDiff(table, table2)\n    assert (not diff.identical)\n    assert (diff.diff_values[0] == (('cola', 0), (1.0, 2.0)))\n    assert (diff.diff_values[1][0] == ('colb', 1))\n    assert np.isnan(diff.diff_values[1][1][0])\n    assert (diff.diff_values[1][1][1] == 2.0)\n", "label": 1}
{"function": "\n\ndef do_command(self, verb, args):\n    conn = http_client.HTTPConnection(self.host, self.port, timeout=self.http_timeout)\n    try:\n        body = ('cmd=' + urllib_parse.quote_plus(unicode(verb).encode('utf-8')))\n        for i in range(len(args)):\n            body += ((('&' + unicode((i + 1))) + '=') + urllib_parse.quote_plus(unicode(args[i]).encode('utf-8')))\n        if (None != self.sessionId):\n            body += ('&sessionId=' + unicode(self.sessionId))\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8',\n        }\n        conn.request('POST', '/selenium-server/driver/', body, headers)\n        response = conn.getresponse()\n        data = unicode(response.read(), 'UTF-8')\n        if (not data.startswith('OK')):\n            raise Exception(data)\n        return data\n    finally:\n        conn.close()\n", "label": 0}
{"function": "\n\ndef fdiff(self, argindex=1):\n    (z, m) = self.args\n    fm = sqrt((1 - (m * (sin(z) ** 2))))\n    if (argindex == 1):\n        return (1 / fm)\n    elif (argindex == 2):\n        return (((elliptic_e(z, m) / ((2 * m) * (1 - m))) - (elliptic_f(z, m) / (2 * m))) - (sin((2 * z)) / ((4 * (1 - m)) * fm)))\n    raise ArgumentIndexError(self, argindex)\n", "label": 0}
{"function": "\n\ndef bayesdb_generator_column_stattype(bdb, generator_id, colno):\n    'Return the statistical type of the column `colno` in `generator_id`.'\n    sql = '\\n        SELECT stattype FROM bayesdb_generator_column\\n            WHERE generator_id = ? AND colno = ?\\n    '\n    cursor = bdb.sql_execute(sql, (generator_id, colno))\n    try:\n        row = cursor.next()\n    except StopIteration:\n        generator = bayesdb_generator_name(bdb, generator_id)\n        sql = '\\n            SELECT COUNT(*)\\n                FROM bayesdb_generator AS g, bayesdb_column AS c\\n                WHERE g.id = :generator_id\\n                    AND g.tabname = c.tabname\\n                    AND c.colno = :colno\\n        '\n        cursor = bdb.sql_execute(sql, {\n            'generator_id': generator_id,\n            'colno': colno,\n        })\n        if (cursor_value(cursor) == 0):\n            raise ValueError(('No such column in generator %s: %d' % (generator, colno)))\n        else:\n            raise ValueError(('Column not modelled in generator %s: %d' % (generator, colno)))\n    else:\n        assert (len(row) == 1)\n        return row[0]\n", "label": 0}
{"function": "\n\ndef test_serialization_type(self):\n    activity_object = Pin(id=1)\n    activity = Activity(1, LoveVerb, activity_object)\n    assert isinstance(activity.serialization_id, (six.integer_types, float))\n", "label": 0}
{"function": "\n\n@pytest.mark.xfail\ndef test_fails(self):\n    contact = models.Contact(name='Example')\n    contact.put()\n    models.PhoneNumber(contact=self.contact_key, phone_type='home', number='(650) 555 - 2200').put()\n    numbers = contact.phone_numbers.fetch()\n    assert (1 == len(numbers))\n", "label": 0}
{"function": "\n\ndef _mount_shares_to_instance(self, instance):\n    for share in self.shares:\n        share.handler.allow_access_to_instance(instance, share.share_config)\n    with instance.remote() as remote:\n        share_types = set((type(share.handler) for share in self.shares))\n        for share_type in share_types:\n            share_type.setup_instance(remote)\n        for share in self.shares:\n            share.handler.mount_to_instance(remote, share.share_config)\n", "label": 0}
{"function": "\n\ndef get_all_hosts(self):\n    '\\n            Get list of all hosts in cluster\\n            Args:\\n                None\\n            Return:\\n                list of hostnames\\n            Raise:\\n                None\\n       '\n    zook = self.zk_client\n    broker_id_path = self.zk_paths[BROKER_IDS]\n    if zook.exists(broker_id_path):\n        broker_ids = zook.get_children(broker_id_path)\n        brokers = []\n        for broker_id in broker_ids:\n            brokers.append(self.get_host(broker_id))\n        return brokers\n", "label": 0}
{"function": "\n\ndef parse(cls, signed_request, application_secret_key):\n    'Parse a signed request, returning a dictionary describing its payload.'\n\n    def decode(encoded):\n        padding = ('=' * (len(encoded) % 4))\n        return base64.urlsafe_b64decode((encoded + padding))\n    try:\n        (encoded_signature, encoded_payload) = (str(string) for string in signed_request.split('.', 2))\n        signature = decode(encoded_signature)\n        signed_request_data = json.loads(decode(encoded_payload).decode('utf-8'))\n    except (TypeError, ValueError):\n        raise SignedRequestError('Signed request had a corrupt payload')\n    if (signed_request_data.get('algorithm', '').upper() != 'HMAC-SHA256'):\n        raise SignedRequestError('Signed request is using an unknown algorithm')\n    expected_signature = hmac.new(application_secret_key.encode('utf-8'), msg=encoded_payload.encode('utf-8'), digestmod=hashlib.sha256).digest()\n    if (signature != expected_signature):\n        raise SignedRequestError('Signed request signature mismatch')\n    return signed_request_data\n", "label": 0}
{"function": "\n\ndef assemble(self):\n    assembled = {\n        self.type: self.body,\n    }\n    if self.aggregations:\n        assembled['aggs'] = {\n            \n        }\n        for agg in self.aggregations:\n            assembled['aggs'][agg.name] = agg.assemble()\n    return assembled\n", "label": 0}
{"function": "\n\ndef _ml_train_iterative(self, database_matrix, params=[], sliding_window=168, k=1):\n    '\\n            Training method used by Fred 09 paper.\\n        '\n    p = (database_matrix.shape[1] - 1)\n    number_iterations = ((((database_matrix.shape[0] + p) - k) - sliding_window) + 1)\n    print('Number of iterations: ', number_iterations)\n    tr_size = ((sliding_window - p) - 1)\n    z = (database_matrix[0:(- k), 1].reshape((- 1), 1) * np.ones((1, p)))\n    database_matrix[k:, 1:] = (database_matrix[k:, 1:] - z)\n    pr_target = []\n    ex_target = []\n    for i in range(number_iterations):\n        self._ml_train(database_matrix[(k + i):(((k + i) + tr_size) - 1), :], params)\n        pr_t = self._ml_predict(horizon=1)\n        pr_t = (pr_t[0][0] + z[(i, 0)])\n        pr_target.append(pr_t)\n        ex_target.append(database_matrix[(((k + i) + tr_size), 0)])\n    pr_result = Error(expected=ex_target, predicted=pr_target)\n    return pr_result\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    \" Add an issue to the test repository and save it's id.\"\n    super(IssueCommentAuthenticatedMethodsTest, self).setUp()\n    (success, result) = self.bb.issue.create(title='Test Issue Bitbucket API', content='Test Issue Bitbucket API', responsible=self.bb.username, status='new', kind='bug')\n    assert success\n    self.bb.issue.comment.issue_id = result['local_id']\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    assert (not cls._meta.has_auto_field), \"A model can't have more than one AutoField.\"\n    super(AutoField, self).contribute_to_class(cls, name)\n    cls._meta.has_auto_field = True\n    cls._meta.auto_field = self\n", "label": 0}
{"function": "\n\ndef get(self, filepath, version=None, mode='r'):\n    'Returns a bytestring with the file content, but no metadata.'\n    file_stream = self.open(filepath, version=version, mode=mode)\n    if (file_stream is None):\n        raise IOError(('File %s (version %s) not found.' % (filepath, (version if version else 'latest'))))\n    return file_stream.read()\n", "label": 0}
{"function": "\n\ndef read_bytesmap(f):\n    numpairs = read_short(f)\n    bytesmap = {\n        \n    }\n    for _ in range(numpairs):\n        k = read_string(f)\n        bytesmap[k] = read_value(f)\n    return bytesmap\n", "label": 0}
{"function": "\n\ndef clearAlert(self):\n    ' Clear the current alert level, if any.\\n\\n        '\n    if (self._alert_data is not None):\n        self._alert_data.timer.stop()\n        self._alert_data = None\n        app = QApplication.instance()\n        app.focusChanged.disconnect(self._onAppFocusChanged)\n        self.alerted.emit('')\n", "label": 0}
{"function": "\n\ndef upload_form(self):\n    '\\n            Instantiate file upload form and return it.\\n\\n            Override to implement custom behavior.\\n        '\n    upload_form_class = self.get_upload_form()\n    if request.form:\n        formdata = request.form.copy()\n        formdata.update(request.files)\n        return upload_form_class(formdata, admin=self)\n    elif request.files:\n        return upload_form_class(request.files, admin=self)\n    else:\n        return upload_form_class(admin=self)\n", "label": 0}
{"function": "\n\ndef make_node(self, images):\n    '\\n        .. todo::\\n\\n            WRITEME\\n        '\n    images = as_cuda_ndarray_variable(images)\n    assert (images.ndim == 4)\n    channels_broadcastable = images.type.broadcastable[0]\n    batch_broadcastable = images.type.broadcastable[3]\n    rows_broadcastable = False\n    cols_broadcastable = False\n    targets_broadcastable = (channels_broadcastable, rows_broadcastable, cols_broadcastable, batch_broadcastable)\n    targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)\n    targets = targets_type()\n    return Apply(self, [images], [targets])\n", "label": 0}
{"function": "\n\ndef _get_url(self, url):\n    if (self.access == 'public'):\n        url = url.replace('https://', 'http://')\n        req = urllib.request.Request(url)\n        try:\n            return urllib.request.urlopen(req).read()\n        except urllib.error.HTTPError:\n            raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n    else:\n        raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n", "label": 0}
{"function": "\n\ndef downcaseTokens(s, l, t):\n    'Helper parse action to convert tokens to lower case.'\n    return [tt.lower() for tt in map(_ustr, t)]\n", "label": 0}
{"function": "\n\n@skipUnless(cache_file_exists('ia', '20101102__ia__general__poweshiek__precinct.xls'), CACHED_FILE_MISSING_MSG)\ndef test_results(self):\n    filename = '20101102__ia__general__poweshiek__precinct.xls'\n    mapping = self._get_mapping(filename)\n    self._prep_loader_attrs(mapping)\n    results = self.loader._results(mapping)\n    montezuma_abs_results = [r for r in results if ((r.jurisdiction == 'Montezuma') and (r.votes_type == 'absentee'))]\n    self.assertEqual(len(montezuma_abs_results), 34)\n    result = montezuma_abs_results[0]\n    self.assertEqual(result.office, 'United States Senator')\n    self.assertEqual(result.district, None)\n    self.assertEqual(result.full_name, 'Roxanne Conlin')\n    self.assertEqual(result.party, 'DEM')\n    self.assertEqual(result.write_in, None)\n    self.assertEqual(result.votes, 59)\n    result = montezuma_abs_results[(- 1)]\n    self.assertEqual(result.office, 'State Rep')\n    self.assertEqual(result.district, '75')\n    self.assertEqual(result.full_name, 'Write-In')\n    self.assertEqual(result.party, None)\n    self.assertEqual(result.write_in, 'Write-In')\n    self.assertEqual(result.votes, 0)\n", "label": 0}
{"function": "\n\ndef _as_vector(self, keep_channels=False):\n    '\\n        The vectorized form of this image.\\n\\n        Parameters\\n        ----------\\n        keep_channels : `bool`, optional\\n\\n            ========== =============================\\n            Value      Return shape\\n            ========== =============================\\n            `False`    ``(n_channels * n_pixels,)``\\n            `True`     ``(n_channels, n_pixels)``\\n            ========== =============================\\n\\n        Returns\\n        -------\\n        vec : (See ``keep_channels`` above) `ndarray`\\n            Flattened representation of this image, containing all pixel\\n            and channel information.\\n        '\n    if keep_channels:\n        return self.pixels.reshape([self.n_channels, (- 1)])\n    else:\n        return self.pixels.ravel()\n", "label": 0}
{"function": "\n\ndef show(mousetarget, **kwargs):\n    global mousecapturer\n    mc = getMouseCapturer(**kwargs)\n    mc.mousetarget = mousetarget\n    if isinstance(mousetarget, MouseHandler):\n        mc.mousehandler = True\n    mc.show()\n", "label": 0}
{"function": "\n\ndef write(self, bytes):\n    '\\n        Write C{bytes} to the underlying consumer unless\\n        C{_noMoreWritesExpected} has been called or there are/have been too\\n        many bytes.\\n        '\n    if (self._finished is None):\n        self._producer.stopProducing()\n        raise ExcessWrite()\n    if (len(bytes) <= self._length):\n        self._length -= len(bytes)\n        self._consumer.write(bytes)\n    else:\n        _callAppFunction(self._producer.stopProducing)\n        self._finished.errback(WrongBodyLength('too many bytes written'))\n        self._allowNoMoreWrites()\n", "label": 0}
{"function": "\n\ndef handle_socket_write(self):\n    'Write to socket'\n    try:\n        count = self.socket.send(bytes(self.buffer_ser2net))\n        self.buffer_ser2net = self.buffer_ser2net[count:]\n    except socket.error:\n        self.handle_socket_error()\n", "label": 0}
{"function": "\n\ndef update_fpointer(self, nid, mode=ADD):\n    'set _fpointer recursively'\n    if (nid is None):\n        return\n    if (mode is self.ADD):\n        self._fpointer.append(nid)\n    elif (mode is self.DELETE):\n        if (nid in self._fpointer):\n            self._fpointer.remove(nid)\n    elif (mode is self.INSERT):\n        print('WARNNING: INSERT is deprecated to ADD mode')\n        self.update_fpointer(nid)\n", "label": 0}
{"function": "\n\n@responses.activate\ndef test_bitly_total_clicks_bad_response():\n    body = '20'\n    params = urlencode(dict(link=shorten, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/link/clicks', params)\n    responses.add(responses.GET, url, body=body, status=400, match_querystring=True)\n    body = shorten\n    params = urlencode(dict(uri=expanded, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/shorten', params)\n    responses.add(responses.GET, url, body=body, match_querystring=True)\n    s.short(expanded)\n    assert (s.total_clicks() == 0)\n    assert (s.total_clicks(shorten) == 0)\n", "label": 0}
{"function": "\n\ndef test_RosdepDatabase():\n    from rosdep2.model import RosdepDatabase\n    db = RosdepDatabase()\n    assert (not db.is_loaded('foo'))\n    data = {\n        'a': 1,\n    }\n    db.set_view_data('foo', data, [], 'origin1')\n    assert db.is_loaded('foo')\n    entry = db.get_view_data('foo')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin1')\n    assert (entry.view_dependencies == [])\n    data['a'] = 2\n    assert (entry.rosdep_data != data)\n    data = {\n        'b': 2,\n    }\n    db.set_view_data('bar', data, ['foo'], 'origin2')\n    assert db.is_loaded('bar')\n    entry = db.get_view_data('bar')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin2')\n    assert (entry.view_dependencies == ['foo'])\n    data = {\n        'b': 3,\n    }\n    assert db.is_loaded('bar')\n    db.set_view_data('bar', data, ['baz', 'blah'], 'origin3')\n    assert db.is_loaded('bar')\n    entry = db.get_view_data('bar')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin3')\n    assert (set(entry.view_dependencies) == set(['baz', 'blah']))\n", "label": 1}
{"function": "\n\ndef __init__(self, **kwargs):\n    super(discreteBarChart, self).__init__(**kwargs)\n    self.model = 'discreteBarChart'\n    height = kwargs.get('height', 450)\n    width = kwargs.get('width', None)\n    if kwargs.get('x_is_date', False):\n        self.set_date_flag(True)\n        self.create_x_axis('xAxis', format=kwargs.get('x_axis_format', '%d %b %Y %H %S'), date=True)\n    else:\n        self.create_x_axis('xAxis', format=None)\n    self.create_y_axis('yAxis', format=kwargs.get('y_axis_format', '.0f'))\n    self.set_custom_tooltip_flag(True)\n    self.set_graph_height(height)\n    if width:\n        self.set_graph_width(width)\n    tooltips = kwargs.get('tooltips', True)\n    if (not tooltips):\n        self.chart_attr = {\n            'tooltips': 'false',\n        }\n", "label": 0}
{"function": "\n\n@access.public\ndef describeResource(self, resource, params):\n    if (resource not in docs.routes):\n        raise RestException(('Invalid resource: %s' % resource))\n    return {\n        'apiVersion': API_VERSION,\n        'swaggerVersion': SWAGGER_VERSION,\n        'basePath': getApiUrl(),\n        'models': dict(docs.models[resource], **docs.models[None]),\n        'apis': [{\n            'path': route,\n            'operations': sorted(op, key=functools.cmp_to_key(self._compareOperations)),\n        } for (route, op) in sorted(six.viewitems(docs.routes[resource]), key=functools.cmp_to_key(self._compareRoutes))],\n    }\n", "label": 0}
{"function": "\n\ndef _es_down_template(request, *args, **kwargs):\n    'Returns the appropriate \"Elasticsearch is down!\" template'\n    return ('search/mobile/down.html' if request.MOBILE else 'search/down.html')\n", "label": 0}
{"function": "\n\ndef capture(self, money, authorization, options=None):\n    options = (options or {\n        \n    })\n    params = {\n        'checkout_id': authorization,\n    }\n    token = options.pop('access_token', self.we_pay_settings['ACCESS_TOKEN'])\n    try:\n        response = self.we_pay.call('/checkout/capture', params, token=token)\n    except WePayError as error:\n        transaction_was_unsuccessful.send(sender=self, type='capture', response=error)\n        return {\n            'status': 'FAILURE',\n            'response': error,\n        }\n    transaction_was_successful.send(sender=self, type='capture', response=response)\n    return {\n        'status': 'SUCCESS',\n        'response': response,\n    }\n", "label": 0}
{"function": "\n\ndef __isub__(self, val):\n    if (type(val) in (int, float)):\n        self.x -= val\n        self.y -= val\n    else:\n        self.x -= val.x\n        self.y -= val.y\n    return self\n", "label": 0}
{"function": "\n\ndef reserve_provider_segment(self, session, segment):\n    filters = {\n        \n    }\n    physical_network = segment.get(api.PHYSICAL_NETWORK)\n    if (physical_network is not None):\n        filters['physical_network'] = physical_network\n        vlan_id = segment.get(api.SEGMENTATION_ID)\n        if (vlan_id is not None):\n            filters['vlan_id'] = vlan_id\n    if self.is_partial_segment(segment):\n        alloc = self.allocate_partially_specified_segment(session, **filters)\n        if (not alloc):\n            raise exc.NoNetworkAvailable()\n    else:\n        alloc = self.allocate_fully_specified_segment(session, **filters)\n        if (not alloc):\n            raise exc.VlanIdInUse(**filters)\n    return {\n        api.NETWORK_TYPE: p_const.TYPE_VLAN,\n        api.PHYSICAL_NETWORK: alloc.physical_network,\n        api.SEGMENTATION_ID: alloc.vlan_id,\n        api.MTU: self.get_mtu(alloc.physical_network),\n    }\n", "label": 0}
{"function": "\n\ndef put(self):\n    pet = self.json_args\n    if (not isinstance(pet['id'], int)):\n        self.set_status(400)\n    if (not self.db.update_(**pet)):\n        self.set_status(404)\n    else:\n        self.set_status(200)\n    self.finish()\n", "label": 0}
{"function": "\n\ndef test_getitem_slice_big():\n    slt = SortedList(range(4))\n    lst = list(range(4))\n    itr = ((start, stop, step) for start in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for stop in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for step in [(- 3), (- 2), (- 1), 1, 2, 3])\n    for (start, stop, step) in itr:\n        assert (slt[start:stop:step] == lst[start:stop:step])\n", "label": 0}
{"function": "\n\ndef get_response(self, cmd, fid, *args):\n    for source in self.select_best_source(fid.decode()):\n        dealer = None\n        try:\n            dealer = self.context.socket(zmq.DEALER)\n            dealer.connect(get_events_uri(self.session, source, 'router'))\n            dealer.send_multipart(((cmd, fid) + args))\n            response = dealer.recv_multipart()\n            if ((not response) or (response[0] == ERROR)):\n                self.logger.debug('Error with source {}', source)\n                continue\n            return response\n        finally:\n            if dealer:\n                dealer.close()\n    self.logger.debug('No more source available.')\n    return [ERROR]\n", "label": 0}
{"function": "\n\ndef httpapi(self, arg, opts):\n    sc = HttpAPIStatsCollector()\n    headers = ['#Item', 'Value']\n    table = []\n    for (k, v) in sc.get().getStats().iteritems():\n        if isinstance(v, dict):\n            v = json.dumps(v)\n        row = []\n        row.append(('#%s' % k))\n        if (k[(- 3):] == '_at'):\n            row.append(formatDateTime(v))\n        else:\n            row.append(v)\n        table.append(row)\n    self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))\n", "label": 0}
{"function": "\n\ndef test_cmovpe(self):\n    asm = ['cmovpe eax, ebx']\n    ctx_init = self.__init_context()\n    (x86_ctx_out, reil_ctx_out) = self.__run_code(asm, 3735928559, ctx_init)\n    cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)\n    if (not cmp_result):\n        self.__save_failing_context(ctx_init)\n    self.assertTrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))\n", "label": 0}
{"function": "\n\ndef _find_tab(self, widget):\n    for (key, bnch) in self.tab.items():\n        if (widget == bnch.widget):\n            return bnch\n    return None\n", "label": 0}
{"function": "\n\ndef redraw(self):\n    self.data.sort(self.sortfn)\n    rows = len(self.data)\n    cols = 0\n    if (rows > 0):\n        cols = len(self.data[0])\n    self.grid.resize(rows, cols)\n    self.header.resize(1, cols)\n    cf = self.grid.getCellFormatter()\n    for (nrow, row) in enumerate(self.data):\n        for (ncol, item) in enumerate(row):\n            self.grid.setHTML(nrow, ncol, str(item))\n            cf.setWidth(nrow, ncol, '200px')\n    cf = self.header.getCellFormatter()\n    self.sortbuttons = []\n    for ncol in range(cols):\n        sb = Button(('sort col %d' % ncol))\n        sb.addClickListener(self)\n        self.header.setWidget(0, ncol, sb)\n        cf.setWidth(0, ncol, '200px')\n        self.sortbuttons.append(sb)\n", "label": 0}
{"function": "\n\ndef is_user_notified(user_id, target_id):\n    res = dbsession.query(Notify).filter((Notify.target_id == target_id))\n    res = res.filter((Notify.user_id == user_id))\n    try:\n        r = res.all()[0]\n        return r\n    except:\n        return False\n", "label": 0}
{"function": "\n\n@users_delete_fixtures\ndef test_users_delete_redirect(User):\n    request = DummyRequest(params={\n        'username': 'bob',\n    })\n    User.get_by_username.return_value = None\n    result = views.users_delete(request)\n    assert (result.__class__ == httpexceptions.HTTPFound)\n", "label": 0}
{"function": "\n\ndef get_primary_address(self):\n    'Return the primary address of this partner.\\n\\n        '\n    Address = rt.modules.addresses.Address\n    try:\n        return Address.objects.get(partner=self, primary=True)\n    except Address.DoesNotExist:\n        pass\n", "label": 0}
{"function": "\n\ndef clear(self):\n    'od.clear() -> None.  Remove all items from od.'\n    try:\n        for node in self.__map.itervalues():\n            del node[:]\n        root = self.__root\n        root[:] = [root, root, None]\n        self.__map.clear()\n    except AttributeError:\n        pass\n    dict.clear(self)\n", "label": 0}
{"function": "\n\n@group_only\ndef seen(self, msg, matches):\n    chat_id = msg.dest.id\n    if (matches.group(2) is not None):\n        return self.seen_by_id(chat_id, matches.group(2))\n    elif (matches.group(3) is not None):\n        return self.seen_by_username(chat_id, matches.group(3))\n    else:\n        return self.seen_by_fullname(chat_id, matches.group(4))\n", "label": 0}
{"function": "\n\ndef Validate(self):\n    'Attempt to validate the artifact has been well defined.\\n\\n    This is used to enforce Artifact rules. Since it checks all dependencies are\\n    present, this method can only be called once all artifacts have been loaded\\n    into the registry. Use ValidateSyntax to check syntax for each artifact on\\n    import.\\n\\n    Raises:\\n      ArtifactDefinitionError: If artifact is invalid.\\n    '\n    self.ValidateSyntax()\n    try:\n        for dependency in self.GetArtifactDependencies():\n            dependency_obj = REGISTRY.GetArtifact(dependency)\n            if dependency_obj.error_message:\n                raise ArtifactDefinitionError(('Dependency %s has an error!' % dependency))\n    except ArtifactNotRegisteredError as e:\n        raise ArtifactDefinitionError(e)\n", "label": 0}
{"function": "\n\ndef filter_log_files_for_zipping(log_files):\n    \"Identify unzipped log files that are approporate for zipping.\\n\\n    Each unique log type found should have the most recent log file unzipped\\n    as it's probably still in use.\\n    \"\n    out_files = []\n    for lf in filter_log_files_for_active(log_files):\n        if lf.bzip:\n            continue\n        out_files.append(lf)\n    return out_files\n", "label": 0}
{"function": "\n\ndef http_method_not_allowed(self, request, *args, **kwargs):\n    allowed_methods = [m for m in self.http_method_names if hasattr(self, m)]\n    logger.warning(('Method Not Allowed (%s): %s' % (request.method, request.path)), extra={\n        'status_code': 405,\n        'request': self.request,\n    })\n    return http.HttpResponseNotAllowed(allowed_methods)\n", "label": 0}
{"function": "\n\ndef register(self, name, c):\n    if ((name in self.registered) and (c is not self.registered[name])):\n        raise NameError('{} has been registered by {}'.format(name, self.registered[name]))\n    self.registered[name] = c\n", "label": 0}
{"function": "\n\ndef __init__(self, pattern, flags=0):\n    'The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.'\n    super(Regex, self).__init__()\n    if isinstance(pattern, basestring):\n        if (len(pattern) == 0):\n            warnings.warn('null string passed to Regex; use Empty() instead', SyntaxWarning, stacklevel=2)\n        self.pattern = pattern\n        self.flags = flags\n        try:\n            self.re = re.compile(self.pattern, self.flags)\n            self.reString = self.pattern\n        except sre_constants.error:\n            warnings.warn(('invalid pattern (%s) passed to Regex' % pattern), SyntaxWarning, stacklevel=2)\n            raise\n    elif isinstance(pattern, Regex.compiledREtype):\n        self.re = pattern\n        self.pattern = self.reString = str(pattern)\n        self.flags = flags\n    else:\n        raise ValueError('Regex may only be constructed with a string or a compiled RE object')\n    self.name = _ustr(self)\n    self.errmsg = ('Expected ' + self.name)\n    self.mayIndexError = False\n    self.mayReturnEmpty = True\n", "label": 0}
{"function": "\n\ndef get_cpu_state(self):\n    '\\n        Retrieves CPU state from client\\n        '\n    state = c_int(0)\n    self.library.Cli_GetPlcStatus(self.pointer, byref(state))\n    try:\n        status_string = cpu_statuses[state.value]\n    except KeyError:\n        status_string = None\n    if (not status_string):\n        raise Snap7Exception(('The cpu state (%s) is invalid' % state.value))\n    logging.debug(('CPU state is %s' % status_string))\n    return status_string\n", "label": 0}
{"function": "\n\ndef test_local_bower_json_dependencies():\n    bower = bowerstatic.Bower()\n    components = bower.components('components', os.path.join(os.path.dirname(__file__), 'bower_components'))\n    local = bower.local_components('local', components)\n    path = os.path.join(os.path.dirname(__file__), 'local_component_deps')\n    local.component(path, version='2.0')\n\n    def wsgi(environ, start_response):\n        start_response('200 OK', [('Content-Type', 'text/html;charset=UTF-8')])\n        include = local.includer(environ)\n        include('local_component')\n        return [b'<html><head></head><body>Hello!</body></html>']\n    wrapped = bower.wrap(wsgi)\n    c = Client(wrapped)\n    response = c.get('/')\n    assert (response.body == b'<html><head><script type=\"text/javascript\" src=\"/bowerstatic/components/jquery/2.1.1/dist/jquery.js\"></script>\\n<script type=\"text/javascript\" src=\"/bowerstatic/local/local_component/2.0/local.js\"></script></head><body>Hello!</body></html>')\n", "label": 0}
{"function": "\n\ndef stackhut_api_call(endpoint, msg, secure=True, return_json=True):\n    url = urllib.parse.urljoin(utils.SERVER_URL, endpoint)\n    log.debug('Calling Stackhut Server at {} with \\n\\t{}'.format(url, json.dumps(msg)))\n    r = requests.post(url, data=json.dumps(msg), headers=json_header)\n    if (r.status_code == requests.codes.ok):\n        return (r.json() if return_json else r.text)\n    else:\n        log.error('Error {} talking to Stackhut Server'.format(r.status_code))\n        log.error(r.text)\n        r.raise_for_status()\n", "label": 0}
{"function": "\n\ndef test_basic(self):\n    'messages are sent and received properly'\n    question = b'sucess?'\n    answer = b'yeah, success'\n\n    def handler(sock):\n        text = sock.recv(1000)\n        assert (text == question)\n        sock.sendall(answer)\n    with Server(handler) as (host, port):\n        sock = socket.socket()\n        sock.connect((host, port))\n        sock.sendall(question)\n        text = sock.recv(1000)\n        assert (text == answer)\n        sock.close()\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    from pennyblack.models import Link, Newsletter\n    if ('mail' not in context):\n        return '#'\n    mail = context['mail']\n    newsletter = mail.job.newsletter\n    if newsletter.is_workflow():\n        job = newsletter.get_default_job()\n    else:\n        job = mail.job\n    try:\n        link = job.links.get(identifier=self.identifier)\n    except job.links.model.DoesNotExist:\n        link = Newsletter.add_view_link_to_job(self.identifier, job)\n    return (context['base_url'] + reverse('pennyblack.redirect_link', args=(mail.mail_hash, link.link_hash)))\n", "label": 0}
{"function": "\n\ndef IsAtEnd(self, string):\n    'Returns whether this position is at the end of the given string.\\n\\n    Args:\\n      string: The string to test for the end of.\\n\\n    Returns:\\n      Whether this position is at the end of the given string.\\n    '\n    return ((self.start == len(string)) and (self.length == 0))\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    'We want it to print as a Cycle, not as a dict.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.combinatorics import Cycle\\n        >>> Cycle(1, 2)\\n        (1 2)\\n        >>> print(_)\\n        (1 2)\\n        >>> list(Cycle(1, 2).items())\\n        [(1, 2), (2, 1)]\\n        '\n    if (not self):\n        return 'Cycle()'\n    cycles = Permutation(self).cyclic_form\n    s = ''.join((str(tuple(c)) for c in cycles))\n    big = (self.size - 1)\n    if (not any(((i == big) for c in cycles for i in c))):\n        s += ('(%s)' % big)\n    return ('Cycle%s' % s)\n", "label": 0}
{"function": "\n\n@plug.handler()\ndef delete_file(metadata):\n    try:\n        ignore_delete.add(metadata.path)\n        os.unlink(metadata.path)\n    except (IOError, OSError) as e:\n        ignore_delete.discard(metadata.path)\n        raise ServiceError(\"Error deleting file '{}': {}\".format(metadata.path, e))\n", "label": 0}
{"function": "\n\ndef test_write_a_file(self, gitfs_log):\n    content = 'Just a small file'\n    filename = '{}/new_file'.format(self.current_path)\n    with gitfs_log('SyncWorker: Set push_successful'):\n        with open(filename, 'w') as f:\n            f.write(content)\n    with open(filename) as f:\n        assert (f.read() == content)\n    with pull(self.sh):\n        self.assert_new_commit()\n        self.assert_commit_message('Update /new_file')\n", "label": 0}
{"function": "\n\ndef testConnectionLeaks(self):\n    for i in range(3):\n        self.assertEquals(self._countConnections(11211), 0)\n        new_conf = {\n            'init_config': {\n                \n            },\n            'instances': [{\n                'url': 'localhost',\n            }],\n        }\n        self.run_check(new_conf)\n        self.assertEquals(self._countConnections(11211), 0)\n", "label": 0}
{"function": "\n\ndef info2iob(sentence, chunks, informations):\n    info_list = ([], [], [])\n    for information in informations:\n        temp_list = positions(information, sentence)\n        for i in range(3):\n            if (temp_list[i] not in info_list[i]):\n                info_list[i].append(temp_list[i])\n    return tag_sent(chunks, info_list)\n", "label": 0}
{"function": "\n\n@classmethod\ndef load_missing(cls, cloud, object_id):\n    identity_client = clients.identity_client(cloud)\n    try:\n        raw_tenant = identity_client.tenants.get(object_id.id)\n        return cls.load_from_cloud(cloud, raw_tenant)\n    except exceptions.NotFound:\n        return None\n", "label": 0}
{"function": "\n\ndef find_duplicates(conn, limit=50, index=None):\n    query = 'SELECT f.id, fingerprint, length FROM fingerprint f LEFT JOIN fingerprint_deduplicate d ON f.id=d.id WHERE d.id IS NULL ORDER BY f.id LIMIT 1000'\n    for fingerprint in conn.execute(query):\n        find_track_duplicates(conn, fingerprint, index=index)\n", "label": 0}
{"function": "\n\ndef _delete(self, filter, multi=False):\n    if (filter is None):\n        filter = {\n            \n        }\n    if (not isinstance(filter, collections.Mapping)):\n        filter = {\n            '_id': filter,\n        }\n    to_delete = list(self.find(filter))\n    deleted_count = 0\n    for doc in to_delete:\n        doc_id = doc['_id']\n        if isinstance(doc_id, dict):\n            doc_id = helpers.hashdict(doc_id)\n        del self._documents[doc_id]\n        deleted_count += 1\n        if (not multi):\n            break\n    return {\n        'connectionId': self._database.client._id,\n        'n': deleted_count,\n        'ok': 1.0,\n        'err': None,\n    }\n", "label": 0}
{"function": "\n\ndef items(self):\n    'Dict-like items() that returns a list of name-value tuples from the jar.\\n        See keys() and values(). Allows client-code to call \"dict(RequestsCookieJar)\\n        and get a vanilla python dict of key value pairs.'\n    items = []\n    for cookie in iter(self):\n        items.append((cookie.name, cookie.value))\n    return items\n", "label": 0}
{"function": "\n\ndef intersects(self, other):\n    return ((other.start < self.end) or (self.start < other.end))\n", "label": 0}
{"function": "\n\ndef match(self, left, collected=None):\n    collected = ([] if (collected is None) else collected)\n    outcomes = []\n    for p in self.children:\n        (matched, _, _) = outcome = p.match(left, collected)\n        if matched:\n            outcomes.append(outcome)\n    if outcomes:\n        return min(outcomes, key=(lambda outcome: len(outcome[1])))\n    return (False, left, collected)\n", "label": 0}
{"function": "\n\ndef _get_input(self):\n    (title, msg) = ('Get Input Dialog', 'Enter your name:')\n    (text, resp) = QtGui.QInputDialog.getText(self, title, msg)\n    if resp:\n        self.label.setText(web.safeunicode(text))\n", "label": 0}
{"function": "\n\ndef _sanitize_mod_params(self, other):\n    \"Sanitize the object being modded with this Message.\\n\\n        - Add support for modding 'None' so translation supports it\\n        - Trim the modded object, which can be a large dictionary, to only\\n        those keys that would actually be used in a translation\\n        - Snapshot the object being modded, in case the message is\\n        translated, it will be used as it was when the Message was created\\n        \"\n    if (other is None):\n        params = (other,)\n    elif isinstance(other, dict):\n        params = self._trim_dictionary_parameters(other)\n    else:\n        params = self._copy_param(other)\n    return params\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (not (type(other) is STP)):\n        return False\n    return ((self.network == other.network) and (self.port == other.port) and (self.label == other.label))\n", "label": 0}
{"function": "\n\ndef validate(self, doc):\n    if (not doc.get('_id', '')):\n        raise Exception('Attempting to store empty password.')\n    return doc\n", "label": 0}
{"function": "\n\ndef __init__(self, **kwargs):\n    dict_type = (kwargs.pop('dict_type', None) or OrderedDict)\n    ConfigParser.__init__(self, dict_type=dict_type, **kwargs)\n", "label": 0}
{"function": "\n\ndef setup():\n    '\\n    Create necessary directories for testing.\\n    '\n    train_dir = join(_my_dir, 'train')\n    if (not exists(train_dir)):\n        os.makedirs(train_dir)\n    output_dir = join(_my_dir, 'output')\n    if (not exists(output_dir)):\n        os.makedirs(output_dir)\n", "label": 0}
{"function": "\n\ndef render_result(result_data, test_bundle):\n    result_data = lcase_keys(result_data)\n    result_string = (sublime.expand_variables(RESULTS_TEMPLATES['results'], filter_stats_dict(result_data)) + '\\n')\n    for bundle in result_data['bundlestats']:\n        if (len(test_bundle) and (bundle['path'] != test_bundle)):\n            continue\n        result_string += (('\\n' + sublime.expand_variables(RESULTS_TEMPLATES['bundle'], filter_stats_dict(bundle))) + '\\n')\n        if isinstance(bundle['globalexception'], dict):\n            result_string += (('\\n' + sublime.expand_variables(RESULTS_TEMPLATES['global_exception'], filter_exception_dict(bundle['globalexception']))) + '\\n')\n        for suite in bundle['suitestats']:\n            result_string += ('\\n' + gen_suite_report(suite))\n    result_string += ('\\n' + RESULTS_TEMPLATES['legend'])\n    return result_string\n", "label": 0}
{"function": "\n\ndef login_action(request):\n    username = request.REQUEST['username']\n    password = request.REQUEST['password']\n    user = authenticate(username=username, password=password)\n    if (user is None):\n        return json_failure(_('Invalid username or password'))\n    if (not user.is_active):\n        return json_failure(_('Account disabled.'))\n    login(request, user)\n    return json_response()\n", "label": 0}
{"function": "\n\ndef Check(self):\n    'Assertion verification for options.'\n    try:\n        assert (self.m0 >= 0), 'margin0'\n        assert (self.m1 >= self.m0), 'margin1'\n        assert (self.c0 >= 0), 'cost0'\n        assert (self.c1 >= 0), 'cost1'\n        assert (self.cb >= 0), 'costb'\n        assert (self.ind >= 0), 'indent'\n        assert (self.adj_comment >= 0), 'adj_comment'\n        assert (self.adj_flow >= 0), 'adj_flow'\n        assert (self.adj_call >= 0), 'adj_call'\n        assert (self.adj_arg >= 0), 'adj_arg'\n        assert (self.cpack >= 0), 'cpack'\n    except AssertionError as e:\n        raise Error((\"Illegal option value for '%s'\" % e.args[0]))\n", "label": 1}
{"function": "\n\ndef message_user(self, request, message, level=messages.INFO, extra_tags='', fail_silently=False):\n    '\\n        Send a message to the user. The default implementation\\n        posts a message using the django.contrib.messages backend.\\n\\n        Exposes almost the same API as messages.add_message(), but accepts the\\n        positional arguments in a different order to maintain backwards\\n        compatibility. For convenience, it accepts the `level` argument as\\n        a string rather than the usual level number.\\n        '\n    if (not isinstance(level, int)):\n        try:\n            level = getattr(messages.constants, level.upper())\n        except AttributeError:\n            levels = messages.constants.DEFAULT_TAGS.values()\n            levels_repr = ', '.join((('`%s`' % l) for l in levels))\n            raise ValueError(('Bad message level string: `%s`. Possible values are: %s' % (level, levels_repr)))\n    messages.add_message(request, level, message, extra_tags=extra_tags, fail_silently=fail_silently)\n", "label": 0}
{"function": "\n\ndef add(self, grid):\n    '\\n        Used to add quantities from another grid\\n\\n        Parameters\\n        ----------\\n        grid : 3D Numpy array or SphericalPolarGridView instance\\n            The grid to copy the quantity from\\n        '\n    if (type(self.quantities[self.viewed_quantity]) is list):\n        raise Exception('need to first specify the item to add to')\n    if isinstance(grid, SphericalPolarGridView):\n        if (type(grid.quantities[grid.viewed_quantity]) is list):\n            raise Exception('need to first specify the item to add')\n        self._check_array_dimensions(grid.quantities[grid.viewed_quantity])\n        self.quantities[self.viewed_quantity] += grid.quantities[grid.viewed_quantity]\n    elif isinstance(grid, np.ndarray):\n        self._check_array_dimensions(grid)\n        self.quantities[self.viewed_quantity] += grid\n    else:\n        raise ValueError('grid should be a Numpy array or a SphericalPolarGridView instance')\n", "label": 0}
{"function": "\n\ndef create_junk():\n    fileh = open_file(filename, mode='w')\n    group = fileh.create_group(fileh.root, 'newgroup')\n    for i in range(NLEAVES):\n        table = fileh.create_table(group, ('table' + str(i)), Particle, 'A table', Filters(1))\n        particle = table.row\n        print('Creating table-->', table._v_name)\n        for i in range(NROWS):\n            particle.append()\n        table.flush()\n    fileh.close()\n", "label": 0}
{"function": "\n\ndef _send_500(req, extra=None):\n    req.start_response('500 Error', [('Content-Type', 'text/html')])\n    req.write('<h1>500 Internal Server Error</h1>\\n')\n    req.write('The server encountered an internal error or misconfiguration and was unable to complete your request.\\n')\n    if (extra is not None):\n        req.write(extra)\n    req.close()\n", "label": 0}
{"function": "\n\ndef __iter__(self):\n    for d in reversed(self.dicts):\n        (yield d)\n", "label": 0}
{"function": "\n\ndef getopt(args, shortopts):\n    'getopt(args, options) -> opts, long_opts, args \\nReturns options as list of tuples, long options as entries in a dictionary, and\\nthe remaining arguments.'\n    opts = []\n    longopts = {\n        \n    }\n    while (args and args[0].startswith('-') and (args[0] != '-')):\n        if (args[0] == '--'):\n            args = args[1:]\n            break\n        if args[0].startswith('--'):\n            arg = args.pop(0)\n            _do_longs(longopts, arg)\n        else:\n            (opts, args) = _do_shorts(opts, args[0][1:], shortopts, args[1:])\n    return (opts, longopts, args)\n", "label": 0}
{"function": "\n\n@app.route('/api')\ndef api():\n    \"WebSocket endpoint; Takes a 'topic' GET param.\"\n    ws = request.environ.get('wsgi.websocket')\n    topic = request.args.get('topic')\n    if (None in (ws, topic)):\n        return\n    topic = topic.encode('ascii')\n    for (message, message_topic) in CircusConsumer(topic, endpoint=ZMQ_ENDPOINT):\n        response = json.dumps(dict(message=message, topic=message_topic))\n        ws.send(response)\n", "label": 0}
{"function": "\n\ndef _cleanup_groups(self):\n    exception_list = list()\n    for group in self.cloud.list_groups():\n        if group['name'].startswith(self.group_prefix):\n            try:\n                self.cloud.delete_group(group['id'])\n            except Exception as e:\n                exception_list.append(str(e))\n                continue\n    if exception_list:\n        raise OpenStackCloudException('\\n'.join(exception_list))\n", "label": 0}
{"function": "\n\ndef _expand_glob_path(file_roots):\n    '\\n    Applies shell globbing to a set of directories and returns\\n    the expanded paths\\n    '\n    unglobbed_path = []\n    for path in file_roots:\n        try:\n            if glob.has_magic(path):\n                unglobbed_path.extend(glob.glob(path))\n            else:\n                unglobbed_path.append(path)\n        except Exception:\n            unglobbed_path.append(path)\n    return unglobbed_path\n", "label": 0}
{"function": "\n\ndef authenticate(self):\n    (client_address, _) = self.client_address\n    NAMESPACE.machine = NAMESPACE.session.query(Builder).filter_by(ip=client_address).first()\n    NAMESPACE.user = NAMESPACE.session.query(Person).filter_by(ip=client_address).first()\n    return (NAMESPACE.machine or NAMESPACE.user)\n", "label": 0}
{"function": "\n\ndef listKeysAndSizes(self, bucketName):\n    'Return a list of (name, size) pairs of keys in the bucket'\n    with self.state_.lock:\n        if (bucketName not in self.state_.buckets_):\n            raise S3Interface.BucketNotFound(bucketName)\n        self.state_.validateAccess(bucketName, self.credentials_)\n        return [(key, len(val.value), val.mtime) for (key, val) in self.state_.buckets_[bucketName].iteritems()]\n", "label": 0}
{"function": "\n\ndef cleanup(self, tc_name=''):\n    if (tc_name != ''):\n        self._log.info(('%s: FAILED' % tc_name))\n    for obj in ['ruleset', 'rule', 'classifier', 'action']:\n        self.gbpcfg.gbp_del_all_anyobj(obj)\n", "label": 0}
{"function": "\n\ndef _geo_field(self, field_name=None):\n    \"\\n        Returns the first Geometry field encountered; or specified via the\\n        `field_name` keyword.  The `field_name` may be a string specifying\\n        the geometry field on this GeoQuery's model, or a lookup string\\n        to a geometry field via a ForeignKey relation.\\n        \"\n    if (field_name is None):\n        for fld in self.model._meta.fields:\n            if isinstance(fld, GeometryField):\n                return fld\n        return False\n    else:\n        return GeoWhereNode._check_geo_field(self.model._meta, field_name)\n", "label": 0}
{"function": "\n\ndef compute(self):\n    if self.has_input('foo'):\n        v1 = self.get_input('foo')\n    else:\n        v1 = 0\n    if (v1 != 12):\n        self.change_parameter('foo', (v1 + 1))\n", "label": 0}
{"function": "\n\ndef __init__(self, parentContainer, localId, randomSeed=1, numCalls=1, variantDensity=1):\n    super(SimulatedVariantSet, self).__init__(parentContainer, localId)\n    self._randomSeed = randomSeed\n    self._numCalls = numCalls\n    for j in range(numCalls):\n        self.addCallSet('simCallSet_{}'.format(j))\n    self._variantDensity = variantDensity\n    now = protocol.convertDatetime(datetime.datetime.now())\n    self._creationTime = now\n    self._updatedTime = now\n", "label": 0}
{"function": "\n\n@staticmethod\ndef overwrite_attribute(entity_id, attrs, vals):\n    'Overwrite any attribute of an entity.\\n\\n        This function should receive a list of attributes and a\\n        list of values. Set attribute to None to remove any overwritten\\n        value in place.\\n        '\n    for (attr, val) in zip(attrs, vals):\n        if (val is None):\n            _OVERWRITE[entity_id.lower()].pop(attr, None)\n        else:\n            _OVERWRITE[entity_id.lower()][attr] = val\n", "label": 0}
{"function": "\n\ndef _check_params(length, size):\n    _check_size(size)\n    if ((length % size) != 0):\n        raise error('not a whole number of frames')\n", "label": 0}
{"function": "\n\ndef ex_update_node_affinity_group(self, node, affinity_group_list):\n    '\\n        Updates the affinity/anti-affinity group associations of a virtual\\n        machine. The VM has to be stopped and restarted for the new properties\\n        to take effect.\\n\\n        :param node: Node to update.\\n        :type node: :class:`CloudStackNode`\\n\\n        :param affinity_group_list: List of CloudStackAffinityGroup to\\n                                    associate\\n        :type affinity_group_list: ``list`` of :class:`CloudStackAffinityGroup`\\n\\n        :rtype :class:`CloudStackNode`\\n        '\n    affinity_groups = ','.join((ag.id for ag in affinity_group_list))\n    result = self._async_request(command='updateVMAffinityGroup', params={\n        'id': node.id,\n        'affinitygroupids': affinity_groups,\n    }, method='GET')\n    return self._to_node(data=result['virtualmachine'])\n", "label": 0}
{"function": "\n\ndef html_body(input_string, source_path=None, destination_path=None, input_encoding='unicode', output_encoding='unicode', doctitle=1, initial_header_level=1):\n    '\\n    Given an input string, returns an HTML fragment as a string.\\n\\n    The return value is the contents of the <body> element.\\n\\n    Parameters (see `html_parts()` for the remainder):\\n\\n    - `output_encoding`: The desired encoding of the output.  If a Unicode\\n      string is desired, use the default value of \"unicode\" .\\n    '\n    parts = html_parts(input_string=input_string, source_path=source_path, destination_path=destination_path, input_encoding=input_encoding, doctitle=doctitle, initial_header_level=initial_header_level)\n    fragment = parts['html_body']\n    if (output_encoding != 'unicode'):\n        fragment = fragment.encode(output_encoding)\n    return fragment\n", "label": 0}
{"function": "\n\ndef init(self, modelDocument):\n    super(ModelRssItem, self).init(modelDocument)\n    try:\n        if (self.modelXbrl.modelManager.rssWatchOptions.latestPubDate and (self.pubDate <= self.modelXbrl.modelManager.rssWatchOptions.latestPubDate)):\n            self.status = _('tested')\n        else:\n            self.status = _('not tested')\n    except AttributeError:\n        self.status = _('not tested')\n    self.results = None\n    self.assertions = None\n", "label": 0}
{"function": "\n\ndef rgb_to_hsv(r, g, b):\n    maxc = max(r, g, b)\n    minc = min(r, g, b)\n    v = maxc\n    if (minc == maxc):\n        return (0.0, 0.0, v)\n    s = ((maxc - minc) / maxc)\n    rc = ((maxc - r) / (maxc - minc))\n    gc = ((maxc - g) / (maxc - minc))\n    bc = ((maxc - b) / (maxc - minc))\n    if (r == maxc):\n        h = (bc - gc)\n    elif (g == maxc):\n        h = ((2.0 + rc) - bc)\n    else:\n        h = ((4.0 + gc) - rc)\n    h = ((h / 6.0) % 1.0)\n    return (h, s, v)\n", "label": 0}
{"function": "\n\ndef build_xform(self):\n    xform = XFormBuilder(self.name)\n    for ig in self.iter_item_groups():\n        data_type = ('repeatGroup' if self.is_repeating else 'group')\n        group = xform.new_group(ig.question_name, ig.question_label, data_type)\n        for item in ig.iter_items():\n            group.new_question(item.question_name, item.question_label, ODK_DATA_TYPES[item.data_type], choices=item.choices)\n    return xform.tostring(pretty_print=True, encoding='utf-8', xml_declaration=True)\n", "label": 0}
{"function": "\n\ndef permutations(xs):\n    if (not xs):\n        (yield [])\n    else:\n        for (y, ys) in selections(xs):\n            for pys in permutations(ys):\n                (yield ([y] + pys))\n", "label": 0}
{"function": "\n\ndef on_response(self, response):\n    self.stop()\n    self.got_response = True\n    if (not (response.status_code == 418)):\n        self.response_valid = False\n", "label": 0}
{"function": "\n\ndef _annotate_local(self):\n    \"Annotate the primaryjoin and secondaryjoin\\n        structures with 'local' annotations.\\n\\n        This annotates all column elements found\\n        simultaneously in the parent table\\n        and the join condition that don't have a\\n        'remote' annotation set up from\\n        _annotate_remote() or user-defined.\\n\\n        \"\n    if self._has_annotation(self.primaryjoin, 'local'):\n        return\n    if self._local_remote_pairs:\n        local_side = util.column_set([l for (l, r) in self._local_remote_pairs])\n    else:\n        local_side = util.column_set(self.parent_selectable.c)\n\n    def locals_(elem):\n        if (('remote' not in elem._annotations) and (elem in local_side)):\n            return elem._annotate({\n                'local': True,\n            })\n    self.primaryjoin = visitors.replacement_traverse(self.primaryjoin, {\n        \n    }, locals_)\n", "label": 0}
{"function": "\n\ndef __cmp__(self, other):\n    'ensure that same seq intervals match in cmp()'\n    if (not isinstance(other, SeqPath)):\n        return (- 1)\n    if (self.path is other.path):\n        return cmp((self.start, self.stop), (other.start, other.stop))\n    else:\n        return NOT_ON_SAME_PATH\n", "label": 0}
{"function": "\n\n@attr('numpy')\ndef test_empty(self):\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest('numpy not available.')\n    G = networkx.Graph()\n    assert_equal(networkx.hits(G), ({\n        \n    }, {\n        \n    }))\n    assert_equal(networkx.hits_numpy(G), ({\n        \n    }, {\n        \n    }))\n    assert_equal(networkx.authority_matrix(G).shape, (0, 0))\n    assert_equal(networkx.hub_matrix(G).shape, (0, 0))\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    if ((not self.frozen_by_south) and (name not in [f.name for f in cls._meta.fields])):\n        super(CurrencyField, self).contribute_to_class(cls, name)\n", "label": 0}
{"function": "\n\ndef test_pos_list_append_with_nonexistent_key(self):\n    '\\n        Invoke list_append() with non-existent key\\n        '\n    charSet = 'abcdefghijklmnopqrstuvwxyz1234567890'\n    minLength = 5\n    maxLength = 30\n    length = random.randint(minLength, maxLength)\n    key = ('test', 'demo', (''.join(map((lambda unused: random.choice(charSet)), range(length))) + '.com'))\n    status = self.as_connection.list_append(key, 'abc', 122)\n    assert (status == 0)\n    (key, _, bins) = self.as_connection.get(key)\n    self.as_connection.remove(key)\n    assert (status == 0)\n    assert (bins == {\n        'abc': [122],\n    })\n", "label": 0}
{"function": "\n\ndef fingerprint(self):\n    try:\n        pubkey = sshpubkeys.SSHKey(self.key)\n        return pubkey.hash()\n    except:\n        'There are a small parcel of exceptions that can be throw to indicate invalid keys'\n        return ''\n", "label": 0}
{"function": "\n\ndef start(self):\n    'Start watching the directory for changes.'\n    with self._inotify_fd_lock:\n        if (self._inotify_fd < 0):\n            return\n        self._inotify_poll.register(self._inotify_fd, select.POLLIN)\n        for directory in self._directories:\n            self._add_watch_for_path(directory)\n", "label": 0}
{"function": "\n\n@property\ndef servicenames(self):\n    'Give the list of services available in this folder.'\n    return set([service['name'].rstrip('/').split('/')[(- 1)] for service in self._json_struct.get('services', [])])\n", "label": 0}
{"function": "\n\ndef is_lazy_user(user):\n    ' Return True if the passed user is a lazy user. '\n    if user.is_anonymous():\n        return False\n    backend = getattr(user, 'backend', None)\n    if (backend == 'lazysignup.backends.LazySignupBackend'):\n        return True\n    from lazysignup.models import LazyUser\n    return bool((LazyUser.objects.filter(user=user).count() > 0))\n", "label": 0}
{"function": "\n\ndef check_variable(self, name):\n    ' check_variable(name: str) -> Boolean\\n        Returns True if the vistrail already has the variable name\\n\\n        '\n    variableBox = self.parent().parent().parent()\n    if variableBox.controller:\n        return variableBox.controller.check_vistrail_variable(name)\n    return False\n", "label": 0}
{"function": "\n\n@click.command()\n@click.argument('identifier')\n@click.option('--postinstall', '-i', help='Post-install script to download')\n@click.option('--image', help=\"Image ID. The default is to use the current operating system.\\nSee: 'slcli image list' for reference\")\n@helpers.multi_option('--key', '-k', help='SSH keys to add to the root user')\n@environment.pass_env\ndef cli(env, identifier, postinstall, key, image):\n    'Reload operating system on a virtual server.'\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    keys = []\n    if key:\n        for single_key in key:\n            resolver = SoftLayer.SshKeyManager(env.client).resolve_ids\n            key_id = helpers.resolve_id(resolver, single_key, 'SshKey')\n            keys.append(key_id)\n    if (not (env.skip_confirmations or formatting.no_going_back(vs_id))):\n        raise exceptions.CLIAbort('Aborted')\n    vsi.reload_instance(vs_id, post_uri=postinstall, ssh_keys=keys, image_id=image)\n", "label": 0}
{"function": "\n\ndef findPeak(self, A):\n    '\\n        Binary search\\n        Microsoft Interview, Oct 2014\\n\\n        To reduce the complexity of dealing the edge cases:\\n        * add two anti-peak dummies on the both ends\\n\\n        :param A: An integers list. A[0] and A[-1] are dummies.\\n        :return: return any of peek positions.\\n        '\n    n = len(A)\n    l = 0\n    h = n\n    while (l < h):\n        m = ((l + h) / 2)\n        if (A[(m - 1)] < A[m] > A[(m + 1)]):\n            return m\n        elif (A[(m + 1)] > A[m]):\n            l = (m + 1)\n        else:\n            h = m\n    raise Exception\n", "label": 0}
{"function": "\n\ndef season_by_id(season_id):\n    url = endpoints.season_by_id.format(season_id)\n    q = _query_endpoint(url)\n    if q:\n        return Season(q)\n    else:\n        raise SeasonNotFound(\"Couldn't find Season with ID: {0}\".format(season_id))\n", "label": 0}
{"function": "\n\ndef check_migrations(self):\n    \"\\n        Checks to see if the set of migrations on disk matches the\\n        migrations in the database. Prints a warning if they don't match.\\n        \"\n    executor = MigrationExecutor(connections[DEFAULT_DB_ALIAS])\n    plan = executor.migration_plan(executor.loader.graph.leaf_nodes())\n    if (plan and self.show_startup_messages):\n        self.stdout.write(self.style.NOTICE('\\nYou have unapplied migrations; your app may not work properly until they are applied.'))\n        self.stdout.write(self.style.NOTICE(\"Run 'python manage.py migrate' to apply them.\\n\"))\n", "label": 0}
{"function": "\n\ndef friend_list(self, player_id):\n    return ' '.join([user_manager.id_to_name(friend_id) for friend_id in fetch_set_keys(friend_key(player_id))])\n", "label": 0}
{"function": "\n\ndef onModelChanged(self, model):\n    newTrackPosition = np.array(self.jointController.q[:3])\n    delta = (newTrackPosition - self.lastTrackPosition)\n    for i in xrange(3):\n        if (not self.followAxes[i]):\n            delta[i] = 0.0\n    self.lastTrackPosition = newTrackPosition\n    c = self.view.camera()\n    oldFocalPoint = np.array(c.GetFocalPoint())\n    oldPosition = np.array(c.GetPosition())\n    c.SetFocalPoint((oldFocalPoint + delta))\n    c.SetPosition((oldPosition + delta))\n    self.view.render()\n", "label": 0}
{"function": "\n\ndef test_invalid_base_fields(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.ForeignKey('testapp.Author'), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E005')\n    assert ('Base field for list must be' in errors[0].msg)\n", "label": 0}
{"function": "\n\n@contextlib.contextmanager\ndef _assert_warns_context_manager(warning_class=None, warnings_test=None):\n    '\\n    Builds a context manager for testing code that should throw a warning.\\n    This will look for a given class, call a custom test, or both.\\n\\n    Args:\\n        warning_class - a class or subclass of Warning. If not None, then\\n            the context manager will raise an AssertionError if the block\\n            does not throw at least one warning of that type.\\n        warnings_test - a function which takes a list of warnings caught,\\n            and makes a number of assertions about the result. If the function\\n            returns without an exception, the context manager will consider\\n            this a successful assertion.\\n    '\n    with warnings.catch_warnings(record=True) as caught:\n        warnings.resetwarnings()\n        if warning_class:\n            warnings.simplefilter('ignore')\n            warnings.simplefilter('always', category=warning_class)\n        else:\n            warnings.simplefilter('always')\n        (yield)\n        assert_gt(len(caught), 0, 'expected at least one warning to be thrown')\n        if warnings_test:\n            warnings_test(caught)\n", "label": 0}
{"function": "\n\ndef log_notifications(self, notifications):\n    main_logger = logging.getLogger(config.main_logger_name)\n    notification_logger = logging.getLogger(config.notifications_logger_name)\n    for notification in notifications:\n        try:\n            notification['content'] = notification['content'].encode('utf-8').replace(',', '\\\\,')\n            keys = ['status', 'login_id', 'content', 'message_id', 'campaign_id', 'sending_id', 'game', 'world_id', 'screen', 'time', 'time_to_live_ts_bigint', 'platform', 'receiver_id']\n            notification_logger.info(','.join([str(notification[key]) for key in keys]))\n        except:\n            main_logger.exception('Error while logging notification to csv log!')\n", "label": 0}
{"function": "\n\n@replace_call(BaseDatabaseWrapper.cursor)\ndef cursor(func, self):\n    djdt = DebugToolbarMiddleware.get_current()\n    if djdt:\n        djdt._panels[SQLDebugPanel] = djdt.get_panel(SQLLoggingPanel)\n    return func(self)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    if isinstance(key, int):\n        return list(self.values()).__getitem__(key)\n    elif isinstance(key, slice):\n        items = list(self.items()).__getitem__(key)\n        return Layers(items)\n    else:\n        return super(Layers, self).__getitem__(key)\n", "label": 0}
{"function": "\n\ndef load(self, config):\n    self.items = collections.OrderedDict()\n    values = config.get('axes', self.name).split(',')\n    if config.has_section(('axis:%s' % self.name)):\n        self.defaults = collections.OrderedDict(config.items(('axis:%s' % self.name)))\n    else:\n        self.defaults = {\n            \n        }\n    for value in values:\n        self.items[value.strip('*')] = AxisItem(self, value, config)\n", "label": 0}
{"function": "\n\ndef write_output(args, powerline, segment_info, write):\n    if args.renderer_arg:\n        segment_info.update(args.renderer_arg)\n    if args.side.startswith('above'):\n        for line in powerline.render_above_lines(width=args.width, segment_info=segment_info, mode=segment_info.get('mode', None)):\n            if line:\n                write((line + '\\n'))\n        args.side = args.side[len('above'):]\n    if args.side:\n        rendered = powerline.render(width=args.width, side=args.side, segment_info=segment_info, mode=segment_info.get('mode', None))\n        write(rendered)\n", "label": 0}
{"function": "\n\ndef lines(self, text):\n    for line in text.split('\\n'):\n        (yield ('         \"%s\\\\n\"' % escape_quote(line)))\n", "label": 0}
{"function": "\n\n@pg.production('binop_expr : binop_expr PIPELINE_FIRST_BIND binop_expr')\ndef binop_expr(p):\n    (left, _, right) = p\n    input_sym = get_temp_name()\n    return [Symbol('|>'), p[0], [Symbol('bind'), [Symbol('fn'), [input_sym], ([p[2][0], input_sym] + p[2][(1 if (len(p[2]) > 1) else len(p[2])):])]]]\n", "label": 0}
{"function": "\n\ndef write_file(filename, content):\n    'Write content to file.'\n    (_dir, _) = os.path.split(filename)\n    if (not os.path.exists(_dir)):\n        logging.debug('The directory %s not exists, create it', _dir)\n        mkdir_p(_dir)\n    with io.open(filename, 'wt', encoding='utf-8') as fd:\n        fd.write(content)\n", "label": 0}
{"function": "\n\ndef index(self, keypair_list):\n    return dict(keypairs=[self._base_response(keypair) for keypair in keypair_list])\n", "label": 0}
{"function": "\n\ndef validate(self, value):\n    super(Interval, self).validate(value)\n    if (not ((value is None) or (self.interval_type.is_valid(value) and (value >= self.start) and (value <= self.end)))):\n        raise ValueError(('expected a value of type %s in range [%s, %s], got %r' % (self.interval_type, self.start, self.end, value)))\n", "label": 0}
{"function": "\n\ndef _on_nick(self, c, e):\n    '[Internal]'\n    before = nm_to_n(e.source())\n    after = e.target()\n    for ch in self.channels.values():\n        if ch.has_user(before):\n            ch.change_nick(before, after)\n", "label": 0}
{"function": "\n\ndef test_K4_normalized(self):\n    'Betweenness centrality: K4'\n    G = networkx.complete_graph(4)\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True)\n    b_answer = {\n        0: 0.25,\n        1: 0.25,\n        2: 0.25,\n        3: 0.25,\n    }\n    for n in sorted(G):\n        assert_almost_equal(b[n], b_answer[n])\n    G.add_edge(0, 1, {\n        'weight': 0.5,\n        'other': 0.3,\n    })\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True, weight=None)\n    for n in sorted(G):\n        assert_almost_equal(b[n], b_answer[n])\n    wb_answer = {\n        0: 0.2222222,\n        1: 0.2222222,\n        2: 0.30555555,\n        3: 0.30555555,\n    }\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert_almost_equal(b[n], wb_answer[n])\n    wb_answer = {\n        0: 0.2051282,\n        1: 0.2051282,\n        2: 0.33974358,\n        3: 0.33974358,\n    }\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True, weight='other')\n    for n in sorted(G):\n        assert_almost_equal(b[n], wb_answer[n])\n", "label": 0}
{"function": "\n\ndef get_auth_params(self, request, action):\n    settings = self.get_settings()\n    ret = settings.get('AUTH_PARAMS', {\n        \n    })\n    dynamic_auth_params = request.GET.get('auth_params', None)\n    if dynamic_auth_params:\n        ret.update(dict(parse_qsl(dynamic_auth_params)))\n    return ret\n", "label": 0}
{"function": "\n\ndef _emit(self, record, stream):\n    self.stream = stream\n    try:\n        return logging.StreamHandler.emit(self, record)\n    except:\n        raise\n    else:\n        self.stream = None\n", "label": 0}
{"function": "\n\ndef data_path(path, createdir=False):\n    'If path is relative, return the given path inside the project data dir,\\n    otherwise return the path unmodified\\n    '\n    if (not isabs(path)):\n        path = join(project_data_dir(), path)\n    if (createdir and (not exists(path))):\n        os.makedirs(path)\n    return path\n", "label": 0}
{"function": "\n\ndef __init__(self, groupDateTime, bars, frequency):\n    resamplebase.Grouper.__init__(self, groupDateTime)\n    self.__barGroupers = {\n        \n    }\n    self.__frequency = frequency\n    for (instrument, bar_) in bars.items():\n        barGrouper = resampled.BarGrouper(groupDateTime, bar_, frequency)\n        self.__barGroupers[instrument] = barGrouper\n", "label": 0}
{"function": "\n\ndef ex_attach_nic_to_node(self, node, network, ip_address=None):\n    \"\\n        Add an extra Nic to a VM\\n\\n        :param  network: NetworkOffering object\\n        :type   network: :class:'CloudStackNetwork`\\n\\n        :param  node: Node Object\\n        :type   node: :class:'CloudStackNode`\\n\\n        :param  ip_address: Optional, specific IP for this Nic\\n        :type   ip_address: ``str``\\n\\n\\n        :rtype: ``bool``\\n        \"\n    args = {\n        'virtualmachineid': node.id,\n        'networkid': network.id,\n    }\n    if (ip_address is not None):\n        args['ipaddress'] = ip_address\n    self._async_request(command='addNicToVirtualMachine', params=args)\n    return True\n", "label": 0}
{"function": "\n\ndef longRunHighGrayLevelEmphasis(self, P_glrl, ivector, jvector, sumP_glrl, meanFlag=True):\n    try:\n        lrhgle = (numpy.sum(numpy.sum(((P_glrl * (ivector ** 2)[:, None, None]) * (jvector ** 2)[None, :, None]), 0), 0) / sumP_glrl[None, None, :])\n    except ZeroDivisionError:\n        lrhgle = 0\n    if meanFlag:\n        return lrhgle.mean()\n    else:\n        return lrhgle\n", "label": 0}
{"function": "\n\ndef bind(self, lan):\n    'bind to a LAN.'\n    if _debug:\n        Node._debug('bind %r', lan)\n    lan.add_node(self)\n", "label": 0}
{"function": "\n\ndef columns_used(self):\n    '\\n        Returns all the columns used across all models in the group\\n        for filtering and in the model expression.\\n\\n        '\n    return list(tz.unique(tz.concat((m.columns_used() for m in self.models.values()))))\n", "label": 0}
{"function": "\n\ndef S_e(self, prob):\n    '\\n        Electric source term\\n\\n        :param Problem prob: FDEM Problem\\n        :rtype: numpy.ndarray\\n        :return: electric source term on mesh\\n        '\n    if ((prob._formulation is 'EB') and (self.integrate is True)):\n        return (prob.Me * self._S_e)\n    return self._S_e\n", "label": 0}
{"function": "\n\ndef __call__(self, fn):\n\n    def wrapper(*args, **kwargs):\n        that = args[0]\n        that.logger.debug(self.start)\n        ret = fn(*args, **kwargs)\n        that.logger.debug(self.finish)\n        if self.getter:\n            that.logger.debug(pformat(self.getter(ret)))\n        else:\n            that.logger.debug(pformat(ret))\n        return ret\n    wrapper.func_name = fn.func_name\n    if hasattr(fn, '__name__'):\n        wrapper.__name__ = self.name = fn.__name__\n    if hasattr(fn, '__doc__'):\n        wrapper.__doc__ = fn.__doc__\n    if hasattr(fn, '__module__'):\n        wrapper.__module__ = fn.__module__\n    return wrapper\n", "label": 0}
{"function": "\n\ndef write_packed(self, outfile, rows):\n    '\\n        Write PNG file to `outfile`.  The pixel data comes from `rows`\\n        which should be in boxed row packed format.  Each row should be\\n        a sequence of packed bytes.\\n\\n        Technically, this method does work for interlaced images but it\\n        is best avoided.  For interlaced images, the rows should be\\n        presented in the order that they appear in the file.\\n\\n        This method should not be used when the source image bit depth\\n        is not one naturally supported by PNG; the bit depth should be\\n        1, 2, 4, 8, or 16.\\n        '\n    if self.rescale:\n        raise Error(('write_packed method not suitable for bit depth %d' % self.rescale[0]))\n    return self.write_passes(outfile, rows, packed=True)\n", "label": 0}
{"function": "\n\ndef clean_votes(self, value):\n    assert (value > 0), 'Must be greater than 0.'\n    assert (value < 51), 'Must be less than 51.'\n    return value\n", "label": 0}
{"function": "\n\ndef _print_slots(self):\n    slots = ', '.join((((\"'\" + snake(name)) + \"'\") for (type, name, nullable, plural) in self._fields))\n    print(\"    __slots__ = ('loc', {slots},)\".format(slots=slots))\n", "label": 0}
{"function": "\n\ndef register_scheme(scheme):\n    for method in dir(urlparse):\n        if method.startswith('uses_'):\n            getattr(urlparse, method).append(scheme)\n", "label": 0}
{"function": "\n\ndef change_primary_name(self, name):\n    '\\n        Changes the primary/default name of the policy to a specified name.\\n\\n        :param name: a string name to replace the current primary name.\\n        '\n    if (name == self.name):\n        return\n    elif (name in self.alias_list):\n        self.remove_name(name)\n    else:\n        self._validate_policy_name(name)\n    self.alias_list.insert(0, name)\n", "label": 0}
{"function": "\n\ndef configure_host(self):\n    if self.mail.use_ssl:\n        host = smtplib.SMTP_SSL(self.mail.server, self.mail.port)\n    else:\n        host = smtplib.SMTP(self.mail.server, self.mail.port)\n    host.set_debuglevel(int(self.mail.debug))\n    if self.mail.use_tls:\n        host.starttls()\n    if (self.mail.username and self.mail.password):\n        host.login(self.mail.username, self.mail.password)\n    return host\n", "label": 0}
{"function": "\n\ndef host_to_ip(host):\n    '\\n    Returns the IP address of a given hostname\\n    '\n    try:\n        (family, socktype, proto, canonname, sockaddr) = socket.getaddrinfo(host, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0]\n        if (family == socket.AF_INET):\n            (ip, port) = sockaddr\n        elif (family == socket.AF_INET6):\n            (ip, port, flow_info, scope_id) = sockaddr\n    except Exception:\n        ip = None\n    return ip\n", "label": 0}
{"function": "\n\ndef get(self, key):\n    key = key.lower()\n    if self.has_key(key):\n        return self._config.get(key.lower())\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef add_all_wordstarts_matching(self, lower_hits, query, max_hits_hint):\n    lower_query = query.lower()\n    if (lower_query in self.basenames_by_wordstarts):\n        for basename in self.basenames_by_wordstarts[lower_query]:\n            lower_hits.add(basename)\n            if (len(lower_hits) >= max_hits_hint):\n                return\n", "label": 0}
{"function": "\n\ndef _setup_nodes(self):\n    self.add_node(LocalNode())\n    nodes = self.app.config.get('PSDASH_NODES', [])\n    logger.info('Registering %d nodes', len(nodes))\n    for n in nodes:\n        self.register_node(n['name'], n['host'], int(n['port']))\n", "label": 0}
{"function": "\n\ndef __init__(self, parent_model, admin_site):\n    self.admin_site = admin_site\n    self.parent_model = parent_model\n    self.opts = self.model._meta\n    self.has_registered_model = admin_site.is_registered(self.model)\n    super(InlineModelAdmin, self).__init__()\n    if (self.verbose_name is None):\n        self.verbose_name = self.model._meta.verbose_name\n    if (self.verbose_name_plural is None):\n        self.verbose_name_plural = self.model._meta.verbose_name_plural\n", "label": 0}
{"function": "\n\ndef __init__(self, name, table=None, foreign_key=None, other_key=None, relation=None):\n    if isinstance(foreign_key, (types.FunctionType, types.MethodType)):\n        raise RuntimeError('morphed_by_many relation requires a name')\n    self._name = name\n    self._table = table\n    self._foreign_key = foreign_key\n    self._other_key = other_key\n    super(morphed_by_many, self).__init__(relation=relation)\n", "label": 0}
{"function": "\n\ndef get_object(self, bits):\n    if (len(bits) != 0):\n        raise models.Topic.DoesNotExist\n    return 'LatestFeed'\n", "label": 0}
{"function": "\n\n@ComputedGraph.Function\ndef extractVectorDataAsPythonArray(self):\n    if (self.computedValueVector.vectorImplVal is None):\n        return None\n    if ((len(self.vectorDataIds) > 0) and (not self.isLoaded)):\n        return None\n    result = ComputedValueGateway.getGateway().extractVectorDataAsPythonArray(self.computedValueVector, self.lowIndex, self.highIndex)\n    if ((result is None) and (not self.vdmThinksIsLoaded())):\n        logging.info('CumulusClient: %s was marked loaded but returned None. reloading', self)\n        self.isLoaded = False\n        ComputedValueGateway.getGateway().reloadVector(self)\n    return result\n", "label": 0}
{"function": "\n\ndef parse_policy(policy):\n    ret = {\n        \n    }\n    ret['name'] = policy['name']\n    ret['type'] = policy['type']\n    attrs = policy['Attributes']\n    if (policy['type'] != 'SSLNegotiationPolicyType'):\n        return ret\n    ret['sslv2'] = bool(attrs.get('Protocol-SSLv2'))\n    ret['sslv3'] = bool(attrs.get('Protocol-SSLv3'))\n    ret['tlsv1'] = bool(attrs.get('Protocol-TLSv1'))\n    ret['tlsv1_1'] = bool(attrs.get('Protocol-TLSv1.1'))\n    ret['tlsv1_2'] = bool(attrs.get('Protocol-TLSv1.2'))\n    ret['server_defined_cipher_order'] = bool(attrs.get('Server-Defined-Cipher-Order'))\n    ret['reference_security_policy'] = attrs.get('Reference-Security-Policy', None)\n    non_ciphers = ['Server-Defined-Cipher-Order', 'Protocol-SSLv2', 'Protocol-SSLv3', 'Protocol-TLSv1', 'Protocol-TLSv1.1', 'Protocol-TLSv1.2', 'Reference-Security-Policy']\n    ciphers = []\n    for cipher in attrs:\n        if (attrs[cipher] and (cipher not in non_ciphers)):\n            ciphers.append(cipher)\n    ciphers.sort()\n    ret['supported_ciphers'] = ciphers\n    return ret\n", "label": 0}
{"function": "\n\ndef _update(self, context):\n    'Update partial stats locally and populate them to Scheduler.'\n    if (not self._resource_change()):\n        return\n    self.scheduler_client.update_resource_stats(self.compute_node)\n    if self.pci_tracker:\n        self.pci_tracker.save(context)\n", "label": 0}
{"function": "\n\ndef conceptsUsed(self):\n    conceptsUsed = set((f.qname for f in self.modelXbrl.factsInInstance))\n    for cntx in self.modelXbrl.contexts.values():\n        for dim in cntx.qnameDims.values():\n            conceptsUsed.add(dim.dimensionQname)\n            if dim.isExplicit:\n                conceptsUsed.add(dim.memberQname)\n            else:\n                conceptsUsed.add(dim.typedMember.qname)\n    for (defaultDim, defaultDimMember) in self.modelXbrl.qnameDimensionDefaults.items():\n        conceptsUsed.add(defaultDim)\n        conceptsUsed.add(defaultDimMember)\n    for roleTypes in (self.modelXbrl.roleTypes, self.modelXbrl.arcroleTypes):\n        for modelRoleTypes in roleTypes.values():\n            for modelRoleType in modelRoleTypes:\n                for qn in modelRoleType.usedOns:\n                    conceptsUsed.add(qn)\n    for relationshipSetKey in self.relationshipSets:\n        relationshipSet = self.modelXbrl.relationshipSet(*relationshipSetKey)\n        for rel in relationshipSet.modelRelationships:\n            if isinstance(rel.fromModelObject, ModelConcept):\n                conceptsUsed.add(rel.fromModelObject)\n            if isinstance(rel.toModelObject, ModelConcept):\n                conceptsUsed.add(rel.toModelObject)\n    for qn in (XbrlConst.qnXbrliIdentifier, XbrlConst.qnXbrliPeriod, XbrlConst.qnXbrliUnit):\n        conceptsUsed.add(qn)\n    conceptsUsed -= {None}\n    return conceptsUsed\n", "label": 1}
{"function": "\n\ndef test_gzip():\n    res = app.get('/', extra_environ=dict(HTTP_ACCEPT_ENCODING='gzip'))\n    assert (int(res.header('content-length')) == len(res.body))\n    assert (res.body != b'this is a test')\n    actual = gzip.GzipFile(fileobj=six.BytesIO(res.body)).read()\n    assert (actual == b'this is a test')\n", "label": 0}
{"function": "\n\ndef get_command_aliases(self):\n    if (not self.config.has_option('commands', 'aliases')):\n        return []\n    value = self.config.get('commands', 'aliases')\n    return list(map((lambda x: x.strip()), value.split(',')))\n", "label": 0}
{"function": "\n\n@mock.patch('sys.platform', 'linux2')\n@mock.patch('bento.commands.configure.virtualenv_prefix', (lambda : None))\n@mock.patch('bento.core.platforms.sysconfig.bento.utils.path.find_root', (lambda ignored: '/'))\n@mock.patch('distutils.command.install.INSTALL_SCHEMES', {\n    'unix_local': MOCK_DEBIAN_SCHEME,\n}, create=True)\ndef test_scheme_debian(self):\n    bento_info = 'Name: foo\\n'\n    scheme = self._compute_scheme(bento_info, self.options)\n    prefix = scheme.pop('prefix')\n    eprefix = scheme.pop('eprefix')\n    sitedir = scheme.pop('sitedir')\n    includedir = scheme.pop('includedir')\n    self.assertEqual(prefix, '/usr/local')\n    self.assertEqual(eprefix, '/usr/local')\n    self.assertEqual(sitedir, ('/usr/local/lib/python%s/dist-packages' % PY_VERSION_SHORT))\n    self.assertEqual(includedir, ('/usr/local/include/python%s/foo' % PY_VERSION_SHORT))\n    scheme.pop('py_version_short')\n    scheme.pop('pkgname')\n    for (k, v) in scheme.items():\n        self.assertEqual(UNIX_REFERENCE[k], v)\n", "label": 0}
{"function": "\n\ndef visit_binop(self, obj):\n    lhs = obj.lhs.accept(self)\n    op = obj.op\n    rhs = obj.rhs.accept(self)\n    if (op == '+'):\n        return (lhs + rhs)\n    elif (op == '-'):\n        return (lhs - rhs)\n    elif (op == '*'):\n        return (lhs * rhs)\n    elif (op == '/'):\n        return (lhs / rhs)\n    else:\n        raise ValueError('invalid op', op)\n", "label": 0}
{"function": "\n\n@object_base.remotable\ndef update_test(self, context=None):\n    if (context and (context.tenant == 'alternate')):\n        self.bar = 'alternate-context'\n    else:\n        self.bar = 'updated'\n", "label": 0}
{"function": "\n\ndef StartTransform(self):\n    'Starts CSV transformation on Hadoop cluster.'\n    self._LoadMapper()\n    gcs_dir = self.config['hadoopTmpDir']\n    hadoop_input_filename = ('%s/inputs/input.csv' % gcs_dir)\n    logging.info('Starting Hadoop transform from %s to %s', self.config['sources'][0], self.config['sinks'][0])\n    logging.debug('Hadoop input file: %s', hadoop_input_filename)\n    output_file = self.cloud_storage_client.OpenObject(self.config['sinks'][0], mode='w')\n    input_file = self.cloud_storage_client.OpenObject(self.config['sources'][0])\n    hadoop_input = self.cloud_storage_client.OpenObject(hadoop_input_filename, mode='w')\n    line_count = 0\n    for line in input_file:\n        if (line_count < self.config['skipLeadingRows']):\n            output_file.write(line)\n        else:\n            hadoop_input.write(line)\n        line_count += 1\n    hadoop_input.close()\n    input_file.close()\n    mapreduce_id = self._StartHadoopMapReduce(gcs_dir)\n    self._WaitForMapReduce(mapreduce_id)\n    (bucket, hadoop_dir) = gcs.Gcs.UrlToBucketAndName(gcs_dir)\n    tab_strip_pattern = re.compile('\\t\\r?\\n')\n    for hadoop_result in self.cloud_storage_client.ListBucket(('/%s' % bucket), prefix=('%s/outputs/part-' % hadoop_dir)):\n        logging.debug('Hadoop result file: %s', hadoop_result)\n        hadoop_output = self.cloud_storage_client.OpenObject(hadoop_result)\n        for line in hadoop_output:\n            output_file.write(tab_strip_pattern.sub('\\n', line))\n    output_file.close()\n", "label": 0}
{"function": "\n\ndef file_upload_view_verify(request):\n    '\\n    Use the sha digest hash to verify the uploaded contents.\\n    '\n    form_data = request.POST.copy()\n    form_data.update(request.FILES)\n    for (key, value) in form_data.items():\n        if key.endswith('_hash'):\n            continue\n        if ((key + '_hash') not in form_data):\n            continue\n        submitted_hash = form_data[(key + '_hash')]\n        if isinstance(value, UploadedFile):\n            new_hash = hashlib.sha1(value.read()).hexdigest()\n        else:\n            new_hash = hashlib.sha1(force_bytes(value)).hexdigest()\n        if (new_hash != submitted_hash):\n            return HttpResponseServerError()\n    largefile = request.FILES['file_field2']\n    obj = FileModel()\n    obj.testfile.save(largefile.name, largefile)\n    return HttpResponse('')\n", "label": 0}
{"function": "\n\ndef __init__(self, name, loop_chain, tile_size):\n    if self._initialized:\n        return\n    if (not hasattr(self, '_inspected')):\n        self._inspected = 0\n    self._name = name\n    self._tile_size = tile_size\n    self._loop_chain = loop_chain\n", "label": 0}
{"function": "\n\ndef search(self, query, **kwargs):\n    qstring = query['query']['query_string']['query']\n    if (qstring in self._queries):\n        return load_by_bug(self._queries[qstring])\n    return load_empty()\n", "label": 0}
{"function": "\n\ndef test_handle_router_snat_rules_add_back_jump(self):\n    ri = l3router.RouterInfo(_uuid(), {\n        \n    }, **self.ri_kwargs)\n    ri.iptables_manager = mock.MagicMock()\n    port = {\n        'fixed_ips': [{\n            'ip_address': '192.168.1.4',\n        }],\n    }\n    ri._handle_router_snat_rules(port, 'iface')\n    nat = ri.iptables_manager.ipv4['nat']\n    nat.empty_chain.assert_any_call('snat')\n    nat.add_rule.assert_any_call('snat', '-j $float-snat')\n    for call in nat.mock_calls:\n        (name, args, kwargs) = call\n        if (name == 'add_rule'):\n            self.assertEqual(('snat', '-j $float-snat'), args)\n            self.assertEqual({\n                \n            }, kwargs)\n            break\n", "label": 0}
{"function": "\n\ndef test_should_exclude_with__returns_false_with_disabled_tag_and_more(self):\n    traits = self.traits\n    test_patterns = [([traits.category1_enabled_tag, traits.category1_disabled_tag], 'case: first'), ([traits.category1_disabled_tag, traits.category1_enabled_tag], 'case: last'), (['foo', traits.category1_enabled_tag, traits.category1_disabled_tag, 'bar'], 'case: middle')]\n    enabled = True\n    for (tags, case) in test_patterns:\n        self.assertEqual((not enabled), self.tag_matcher.should_exclude_with(tags), ('%s: tags=%s' % (case, tags)))\n", "label": 0}
{"function": "\n\n@classmethod\ndef handle_token(cls, parser, token):\n    bits = token.split_contents()\n    if ((len(bits) == 3) and (bits[1] == 'as')):\n        return cls(bits[2])\n    elif ((len(bits) == 4) and (bits[2] == 'as')):\n        return cls(bits[3], bits[1])\n    else:\n        raise template.TemplateSyntaxError((\"%r takes 'as var' or 'level as var'\" % bits[0]))\n", "label": 0}
{"function": "\n\ndef occurrence_view(request, event_pk, pk, template='swingtime/occurrence_detail.html', form_class=forms.SingleOccurrenceForm):\n    '\\n    View a specific occurrence and optionally handle any updates.\\n    \\n    Context parameters:\\n    \\n    ``occurrence``\\n        the occurrence object keyed by ``pk``\\n\\n    ``form``\\n        a form object for updating the occurrence\\n    '\n    occurrence = get_object_or_404(Occurrence, pk=pk, event__pk=event_pk)\n    if (request.method == 'POST'):\n        form = form_class(request.POST, instance=occurrence)\n        if form.is_valid():\n            form.save()\n            return http.HttpResponseRedirect(request.path)\n    else:\n        form = form_class(instance=occurrence)\n    return render(request, template, {\n        'occurrence': occurrence,\n        'form': form,\n    })\n", "label": 0}
{"function": "\n\ndef testWhitelisted(self):\n    mvp = vcluster.MakeVirtualPath\n    for path in vcluster._VPATH_WHITELIST:\n        self.assertEqual(mvp(path), path)\n        self.assertEqual(mvp(path, _noderoot=None), path)\n        self.assertEqual(mvp(path, _noderoot='/tmp'), path)\n", "label": 0}
{"function": "\n\ndef depack(self, args):\n    self.is_touch = True\n    self.sx = args['x']\n    self.sy = args['y']\n    self.profile = ['pos']\n    if (('size_w' in args) and ('size_h' in args)):\n        self.shape = ShapeRect()\n        self.shape.width = args['size_w']\n        self.shape.height = args['size_h']\n        self.profile.append('shape')\n    if ('pressure' in args):\n        self.pressure = args['pressure']\n        self.profile.append('pressure')\n    super(MTDMotionEvent, self).depack(args)\n", "label": 0}
{"function": "\n\ndef send(self, message, flags=0, copy=False, track=False):\n    zmq_msg = ffi.new('zmq_msg_t*')\n    c_message = ffi.new('char[]', message)\n    C.zmq_msg_init_size(zmq_msg, len(message))\n    C.memcpy(C.zmq_msg_data(zmq_msg), c_message, len(message))\n    if (zmq_version == 2):\n        ret = C.zmq_send(self.zmq_socket, zmq_msg, flags)\n    else:\n        ret = C.zmq_sendmsg(self.zmq_socket, zmq_msg, flags)\n    C.zmq_msg_close(zmq_msg)\n    if (ret < 0):\n        self.last_errno = C.zmq_errno()\n    return ret\n", "label": 0}
{"function": "\n\ndef GetBlendMethod(self):\n    'Get the blend method'\n    currentMethod = self.component.PropertyList.Find('SourceBlendMode').Data\n    for (method, idx) in self.kBlendMethods.iteritems():\n        if (currentMethod == idx):\n            return method\n", "label": 0}
{"function": "\n\ndef _read_track_origin(self, group):\n    self.track_origin = group.attrs['track_origin'].decode('ascii')\n    if ('track_n_scat' in group.attrs):\n        self.track_n_scat = group.attrs['track_n_scat']\n    else:\n        self.track_n_scat = 0\n", "label": 0}
{"function": "\n\ndef visit_NVARCHAR(self, type_, **kw):\n    if type_.length:\n        return self._extend_string(type_, {\n            'national': True,\n        }, ('VARCHAR(%(length)s)' % {\n            'length': type_.length,\n        }))\n    else:\n        raise exc.CompileError(('NVARCHAR requires a length on dialect %s' % self.dialect.name))\n", "label": 0}
{"function": "\n\n@webob.dec.wsgify\ndef process_request(self, req):\n    if (req.path != self._path):\n        return None\n    results = [ext.obj.healthcheck(req.server_port) for ext in self._backends]\n    healthy = self._are_results_healthy(results)\n    if (req.method == 'HEAD'):\n        functor = self._make_head_response\n        status = self.HEAD_HEALTHY_TO_STATUS_CODES[healthy]\n    else:\n        status = self.HEALTHY_TO_STATUS_CODES[healthy]\n        accept_type = req.accept.best_match(self._accept_order)\n        if (not accept_type):\n            accept_type = self._default_accept\n        functor = self._accept_to_functor[accept_type]\n    (body, content_type) = functor(results, healthy)\n    return webob.response.Response(status=status, body=body, content_type=content_type)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    TestCase.__init__(self, *args, **kwargs)\n    for attr in [x for x in dir(self) if x.startswith('test')]:\n        meth = getattr(self, attr)\n\n        def test_(self):\n            try:\n                meth()\n            except psutil.AccessDenied:\n                pass\n        setattr(self, attr, types.MethodType(test_, self))\n", "label": 0}
{"function": "\n\ndef get_shipping_method(self, basket, shipping_address=None, **kwargs):\n    '\\n        Return the selected shipping method instance from this checkout session\\n\\n        The shipping address is passed as we need to check that the method\\n        stored in the session is still valid for the shipping address.\\n        '\n    code = self.checkout_session.shipping_method_code(basket)\n    methods = Repository().get_shipping_methods(basket=basket, user=self.request.user, shipping_addr=shipping_address, request=self.request)\n    for method in methods:\n        if (method.code == code):\n            return method\n", "label": 0}
{"function": "\n\n@classmethod\ndef get_template(cls, message, messenger):\n    'Get a template path to compile a message.\\n\\n        1. `tpl` field of message context;\\n        2. `template` field of message class;\\n        3. deduced from message, messenger data and `template_ext` message type field\\n           (e.g. `sitemessage/messages/plain__smtp.txt` for `plain` message type).\\n\\n        :param Message message: Message model\\n        :param MessengerBase messenger: a MessengerBase heir\\n        :return: str\\n        :rtype: str\\n        '\n    template = message.context.get('tpl', None)\n    if template:\n        return template\n    if (cls.template is None):\n        cls.template = ('sitemessage/messages/%s__%s.%s' % (cls.get_alias(), messenger.get_alias(), cls.template_ext))\n    return cls.template\n", "label": 0}
{"function": "\n\ndef teardown(self):\n    for key in self._event_fns:\n        event.remove(*key)\n    super_ = super(RemovesEvents, self)\n    if hasattr(super_, 'teardown'):\n        super_.teardown()\n", "label": 0}
{"function": "\n\ndef write_file(self, filename):\n    '\\n        Pass schema object to template engine to be rendered for use.\\n\\n        :param filename: output filename\\n        :return:\\n        '\n    template = self.template_env.get_template('settings.py.j2')\n    settings = template.render(endpoints=OrderedDict([(endpoint, self.format_endpoint(schema)) for (endpoint, schema) in self.endpoints.iteritems()]))\n    with open(filename, 'w') as ofile:\n        ofile.write((settings + '\\n'))\n", "label": 0}
{"function": "\n\ndef process_data(self):\n    polar_data = build_wedge_source(self._data.df, cat_cols=self.attributes['label'].columns, agg_col=self.values.selection, agg=self.agg, level_width=self.level_width, level_spacing=self.level_spacing)\n    polar_data['color'] = ''\n    for group in self._data.groupby(**self.attributes):\n        polar_data.loc[(group['stack'], 'color')] = group['color']\n    self.chart_data = ColumnDataSource(polar_data)\n    self.text_data = build_wedge_text_source(polar_data)\n", "label": 0}
{"function": "\n\n@db_access\ndef _getContainerField(container, field, default):\n    ' Returns the metadata field for the given container or the default value. '\n    container_id = _getContainerId(container)\n    found = _getContainerFieldRecord(container_id, field)\n    return (found.value if found else default)\n", "label": 0}
{"function": "\n\ndef map(self, path):\n    'Map `path` through the aliases.\\n\\n        `path` is checked against all of the patterns.  The first pattern to\\n        match is used to replace the root of the path with the result root.\\n        Only one pattern is ever used.  If no patterns match, `path` is\\n        returned unchanged.\\n\\n        The separator style in the result is made to match that of the result\\n        in the alias.\\n\\n        '\n    for (regex, result, pattern_sep, result_sep) in self.aliases:\n        m = regex.match(path)\n        if m:\n            new = path.replace(m.group(0), result)\n            if (pattern_sep != result_sep):\n                new = new.replace(pattern_sep, result_sep)\n            if self.locator:\n                new = self.locator.canonical_filename(new)\n            return new\n    return path\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _filter_pools_for_numa_cells(pools, numa_cells):\n    numa_cells = ([None] + [cell.id for cell in numa_cells])\n    return [pool for pool in pools if any((utils.pci_device_prop_match(pool, [{\n        'numa_node': cell,\n    }]) for cell in numa_cells))]\n", "label": 0}
{"function": "\n\ndef _setup(self):\n    'Initiate lists for objects contained within this object'\n    for field in self._contains:\n        setattr(self, field, [])\n", "label": 0}
{"function": "\n\ndef test_install_one_host(self):\n    args = self.parser.parse_args('install host1'.split())\n    assert (args.host == ['host1'])\n", "label": 0}
{"function": "\n\ndef update(self, x1, x2, y):\n    self.phase = 'train'\n    for layer in self.layers:\n        x1 = layer.fprop(x1)\n    for layer in self.layers2:\n        x2 = layer.fprop(x2)\n    (grad1, grad2) = self.loss.grad(y, x1, x2)\n    layers = self.layers[self.bprop_until:]\n    for layer in reversed(layers[1:]):\n        grad1 = layer.bprop(grad1)\n    layers[0].bprop(grad1)\n    layers2 = self.layers2[self.bprop_until:]\n    for layer in reversed(layers2[1:]):\n        grad2 = layer.bprop(grad2)\n    layers2[0].bprop(grad2)\n    return self.loss.loss(y, x1, x2)\n", "label": 0}
{"function": "\n\ndef type_continue(self, node):\n    return self.__addSemicolon(('continue' if (not hasattr(node, 'label')) else ('continue %s' % node.label)))\n", "label": 0}
{"function": "\n\ndef __int(value):\n    'validate an integer'\n    (valid, _value) = (False, value)\n    try:\n        _value = int(value)\n        valid = True\n    except ValueError:\n        pass\n    return (valid, _value, 'integer')\n", "label": 0}
{"function": "\n\ndef test_can_update_status_via_trigger_on_participant_balance(self):\n    self.db.run(\"UPDATE participants SET balance=10, status_of_1_0_payout='pending-application' WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 10)\n    assert (alice.status_of_1_0_payout == 'pending-application')\n    self.db.run(\"UPDATE participants SET balance=0 WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 0)\n    assert (alice.status_of_1_0_payout == 'completed')\n", "label": 0}
{"function": "\n\n@app.route('/v1/repositories/<path:repository>/tags', methods=['GET'])\n@toolkit.parse_repository_name\n@toolkit.requires_auth\n@mirroring.source_lookup_tag\ndef _get_tags(namespace, repository):\n    logger.debug('[get_tags] namespace={0}; repository={1}'.format(namespace, repository))\n    try:\n        data = get_tags(namespace=namespace, repository=repository)\n    except exceptions.FileNotFoundError:\n        return toolkit.api_error('Repository not found', 404)\n    return toolkit.response(data)\n", "label": 0}
{"function": "\n\ndef __onNewValues(self, dateTime, value):\n    if (self.__range is None):\n        self.__range = resamplebase.build_range(dateTime, self.getFrequency())\n        self.__grouper = BarsGrouper(self.__range.getBeginning(), value, self.getFrequency())\n    elif self.__range.belongs(dateTime):\n        self.__grouper.addValue(value)\n    else:\n        self.__values.append(self.__grouper.getGrouped())\n        self.__range = resamplebase.build_range(dateTime, self.getFrequency())\n        self.__grouper = BarsGrouper(self.__range.getBeginning(), value, self.getFrequency())\n", "label": 0}
{"function": "\n\ndef __init__(self, client, data=None):\n    super(Zone, self).__init__()\n    self.client = client\n    self.zone_type = 'temperatureZone'\n    if (data is not None):\n        self.__dict__.update(data)\n", "label": 0}
{"function": "\n\ndef register(self, field_type, impl=None):\n    '\\n        Register form field data function.\\n        \\n        Could be used as decorator\\n        '\n\n    def _wrapper(func):\n        self.registry[field_type] = func\n        return func\n    if impl:\n        return _wrapper(impl)\n    return _wrapper\n", "label": 0}
{"function": "\n\ndef test_unknown_apps_are_ignored(self):\n    'Unknown engines get ignored.'\n    self.create_appversion('firefox', '33.0a1')\n    self.create_appversion('thunderbird', '33.0a1')\n    data = {\n        'engines': {\n            'firefox': '>=33.0a1',\n            'thunderbird': '>=33.0a1',\n            'node': '>=0.10',\n        },\n    }\n    apps = self.parse(data)['apps']\n    engines = [app.appdata.short for app in apps]\n    assert (sorted(engines) == ['firefox', 'thunderbird'])\n", "label": 0}
{"function": "\n\ndef _trending_for_month(metric=None):\n    this_month_date = month_for_date(datetime.date.today())\n    previous_month_date = get_previous_month(this_month_date)\n    previous_month_year_date = get_previous_year(this_month_date)\n    data = {\n        'month': 0,\n        'previous_month': 0,\n        'previous_month_year': 0,\n    }\n    try:\n        month = MetricMonth.objects.get(metric=metric, created=this_month_date)\n        data['month'] = month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)\n        data['previous_month'] = previous_month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)\n        data['previous_month_year'] = previous_month_year.num\n    except ObjectDoesNotExist:\n        pass\n    return data\n", "label": 0}
{"function": "\n\n@log_debug\ndef generate_room(self, section):\n    '\\n        Generate room\\n\\n        :param section: section for generator to draw to\\n        :type section: Section\\n        '\n    self.square_generator.generate_room(section)\n    offset = [(1, 1), ((- 1), 1), ((- 1), (- 1)), (1, (- 1))]\n    for (index, corner) in enumerate(self.square_generator.room_corners):\n        self.add_pillar(section, corner, offset[index])\n", "label": 0}
{"function": "\n\ndef report_to_ci_server(self, project):\n    for report in self.reports:\n        test_name = report['test']\n        test_failed = (report['success'] is not True)\n        with test_proxy_for(project).and_test_name(('Integrationtest.%s' % test_name)) as test:\n            if test_failed:\n                test.fails(report['exception'])\n", "label": 0}
{"function": "\n\n@patch('paasta_tools.cli.cmds.check.read_service_configuration')\n@patch('paasta_tools.cli.cmds.check.is_file_in_dir')\n@patch('sys.stdout', new_callable=StringIO)\ndef test_check_smartstack_check_missing_instance(mock_stdout, mock_is_file_in_dir, mock_read_service_info):\n    mock_is_file_in_dir.return_value = True\n    smartstack_dict = {\n        \n    }\n    mock_read_service_info.return_value = smartstack_dict\n    expected_output = ('%s\\n%s\\n' % (PaastaCheckMessages.SMARTSTACK_YAML_FOUND, PaastaCheckMessages.SMARTSTACK_PORT_MISSING))\n    smartstack_check(service='fake_service', service_path='path', soa_dir='path')\n    output = mock_stdout.getvalue()\n    assert (output == expected_output)\n", "label": 0}
{"function": "\n\ndef do_register_opts(opts, group=None, ignore_errors=False):\n    try:\n        cfg.CONF.register_opts(opts, group=group)\n    except:\n        if (not ignore_errors):\n            raise\n", "label": 0}
{"function": "\n\ndef get_select_precolumns(self, select):\n    'Called when building a ``SELECT`` statement, position is just\\n        before column list.\\n\\n        '\n    return ((select._distinct and 'DISTINCT ') or '')\n", "label": 0}
{"function": "\n\ndef visit_WaitStatement(self, node):\n    filename = getfilename(node)\n    template = self.get_template(filename)\n    template_dict = {\n        'cond': del_paren(self.visit(node.cond)),\n        'statement': (self.visit(node.statement) if node.statement else ''),\n    }\n    rslt = template.render(template_dict)\n    return rslt\n", "label": 0}
{"function": "\n\ndef test_None_on_sys_path(self):\n    new_path = sys.path[:]\n    new_path.insert(0, None)\n    new_path_importer_cache = sys.path_importer_cache.copy()\n    new_path_importer_cache.pop(None, None)\n    new_path_hooks = [zipimport.zipimporter, _bootstrap.FileFinder.path_hook(*_bootstrap._get_supported_file_loaders())]\n    missing = object()\n    email = sys.modules.pop('email', missing)\n    try:\n        with util.import_state(meta_path=sys.meta_path[:], path=new_path, path_importer_cache=new_path_importer_cache, path_hooks=new_path_hooks):\n            module = import_module('email')\n            self.assertIsInstance(module, ModuleType)\n    finally:\n        if (email is not missing):\n            sys.modules['email'] = email\n", "label": 0}
{"function": "\n\ndef on_text_changed(self):\n    \" Handle the 'textChanged' signal on the widget.\\n\\n        \"\n    d = self.declaration\n    if (d is not None):\n        d.text_changed()\n", "label": 0}
{"function": "\n\ndef onBrowserEvent(self, event):\n    etype = DOM.eventGetType(event)\n    if (etype == 'mousewheel'):\n        if self._mouseWheelPreventDefault:\n            DOM.eventPreventDefault(event)\n        velocity = DOM.eventGetMouseWheelVelocityY(event)\n        for listener in self._mouseWheelListeners:\n            listener.onMouseWheel(self, velocity)\n        return True\n", "label": 0}
{"function": "\n\ndef raw_field_definition_proxy_post_save(sender, instance, raw, **kwargs):\n    \"\\n    When proxy field definitions are loaded from a fixture they're not\\n    passing through the `field_definition_post_save` signal. Make sure they\\n    are.\\n    \"\n    if raw:\n        model_class = instance.content_type.model_class()\n        opts = model_class._meta\n        if (opts.proxy and (opts.concrete_model is sender)):\n            field_definition_post_save(sender=model_class, instance=instance.type_cast(), raw=raw, **kwargs)\n", "label": 0}
{"function": "\n\ndef test_moves_a_block_up_within_a_container(self):\n    for (idx, pos) in [(0, 0), (1, 1), (2, 2)]:\n        block = TextBlock.objects.get(id=self.left_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n    self.app.put(reverse('fp-api:block-move', kwargs={\n        'uuid': self.main_blocks[1].uuid,\n    }), params={\n        'container': self.left_container.uuid,\n        'index': 1,\n    }, user=self.user)\n    moved_block = TextBlock.objects.get(id=self.main_blocks[1].id)\n    self.assertEquals(moved_block.container, self.page.get_container_from_name('left-container'))\n    self.assertEquals(moved_block.display_order, 1)\n    for (idx, pos) in [(0, 0), (1, 2), (2, 3)]:\n        block = TextBlock.objects.get(id=self.left_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n    for (idx, pos) in [(0, 0), (2, 1)]:\n        block = TextBlock.objects.get(id=self.main_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n", "label": 0}
{"function": "\n\ndef run_cmd(self, util, toggle_active_mark_mode=False):\n    if (util.state.argument_supplied or toggle_active_mark_mode):\n        util.toggle_active_mark_mode()\n    else:\n        util.swap_point_and_mark()\n", "label": 0}
{"function": "\n\ndef test_asizer_limit(self):\n    'Test limit setting for Asizer.\\n        '\n    objs = [Foo(42), ThinFoo('spam'), OldFoo(67)]\n    sizer = [asizeof.Asizer() for _ in range(4)]\n    for (limit, asizer) in enumerate(sizer):\n        asizer.asizeof(objs, limit=limit)\n    limit_sizes = [asizer.total for asizer in sizer]\n    self.assertTrue((limit_sizes[0] < limit_sizes[1]), limit_sizes)\n    self.assertTrue((limit_sizes[1] < limit_sizes[2]), limit_sizes)\n    self.assertTrue((limit_sizes[2] < limit_sizes[3]), limit_sizes)\n", "label": 0}
{"function": "\n\ndef decode(self, file):\n    fStart = file.tell()\n    identifier = None\n    try:\n        identifier = self.iEIEncoder.decode(file)\n    except UDHInformationElementIdentifierUnknownError:\n        pass\n    length = self.int8Encoder.decode(file)\n    data = None\n    if (identifier in self.dataEncoders):\n        data = self.dataEncoders[identifier].decode(file)\n    elif (length > 0):\n        data = self.read(file, length)\n    parsed = (file.tell() - fStart)\n    if (parsed != (length + 2)):\n        raise UDHParseError(('Invalid length: expected %d, parsed %d' % ((length + 2), parsed)))\n    if (identifier is None):\n        return None\n    return gsm_types.InformationElement(identifier, data)\n", "label": 0}
{"function": "\n\ndef _op(self, method, other):\n    if isinstance(other, Counter):\n        other = other.value()\n    if (not isinstance(other, int)):\n        raise TypeError(('Cannot add %s, not an integer.' % other))\n    method(other)\n    return self\n", "label": 0}
{"function": "\n\ndef test_neighbors(self):\n    graph = nx.complete_graph(100)\n    pop = random.sample(list(graph), 1)\n    nbors = list(nx.neighbors(graph, pop[0]))\n    assert_equal(len(nbors), (len(graph) - 1))\n    graph = nx.path_graph(100)\n    node = random.sample(list(graph), 1)[0]\n    nbors = list(nx.neighbors(graph, node))\n    if ((node != 0) and (node != 99)):\n        assert_equal(len(nbors), 2)\n    else:\n        assert_equal(len(nbors), 1)\n    graph = nx.star_graph(99)\n    nbors = list(nx.neighbors(graph, 0))\n    assert_equal(len(nbors), 99)\n", "label": 0}
{"function": "\n\ndef list_hosts(self, filters):\n    host_data_list = (self._hosts_collection().find(filters) or [])\n    return [host.Host.from_dict(h_data, conf=self.config) for h_data in host_data_list]\n", "label": 0}
{"function": "\n\ndef _collapse(intervals):\n    '\\n    Collapse an iterable of intervals sorted by start coord.\\n    \\n    '\n    span = None\n    for (start, stop) in intervals:\n        if (span is None):\n            span = _Interval(start, stop)\n        elif (start <= span.stop < stop):\n            span = _Interval(span.start, stop)\n        elif (start > span.stop):\n            (yield span)\n            span = _Interval(start, stop)\n    if (span is not None):\n        (yield span)\n", "label": 0}
{"function": "\n\ndef claim_invitations(user):\n    \"Claims any pending invitations for the given user's email address.\"\n    invitation_user_id = ('%s:%s' % (models.User.EMAIL_INVITATION, user.email_address))\n    invitation_user = models.User.query.get(invitation_user_id)\n    if invitation_user:\n        invited_build_list = list(invitation_user.builds)\n        if (not invited_build_list):\n            return\n        db.session.add(user)\n        logging.debug('Found %d build admin invitations for id=%r, user=%r', len(invited_build_list), invitation_user_id, user)\n        for build in invited_build_list:\n            build.owners.remove(invitation_user)\n            if (not build.is_owned_by(user.id)):\n                build.owners.append(user)\n                logging.debug('Claiming invitation for build_id=%r', build.id)\n                save_admin_log(build, invite_accepted=True)\n            else:\n                logging.debug('User already owner of build. id=%r, build_id=%r', user.id, build.id)\n            db.session.add(build)\n        db.session.delete(invitation_user)\n        db.session.commit()\n        db.session.add(current_user)\n", "label": 0}
{"function": "\n\ndef process_exception(self, request, exception):\n    if (settings.DEBUG or isinstance(exception, Http404)):\n        return None\n    if isinstance(exception, apiproxy_errors.CapabilityDisabledError):\n        msg = 'Rietveld: App Engine is undergoing maintenance. Please try again in a while.'\n        status = 503\n    elif isinstance(exception, (DeadlineExceededError, MemoryError)):\n        msg = 'Rietveld is too hungry at the moment.Please try again in a while.'\n        status = 503\n    else:\n        msg = 'Unhandled exception.'\n        status = 500\n    logging.exception(('%s: ' % exception.__class__.__name__))\n    technical = ('%s [%s]' % (exception, exception.__class__.__name__))\n    if self._text_requested(request):\n        content = ('%s\\n\\n%s\\n' % (msg, technical))\n        content_type = 'text/plain'\n    else:\n        tpl = loader.get_template('exception.html')\n        ctx = Context({\n            'msg': msg,\n            'technical': technical,\n        })\n        content = tpl.render(ctx)\n        content_type = 'text/html'\n    return HttpResponse(content, status=status, content_type=content_type)\n", "label": 0}
{"function": "\n\n@property\ndef autofit(self):\n    \"\\n        Return |False| if there is a ``<w:tblLayout>`` child with ``w:type``\\n        attribute set to ``'fixed'``. Otherwise return |True|.\\n        \"\n    tblLayout = self.tblLayout\n    if (tblLayout is None):\n        return True\n    return (False if (tblLayout.type == 'fixed') else True)\n", "label": 0}
{"function": "\n\ndef update_dimension_fields(self, instance, force=False, *args, **kwargs):\n    if ((getattr(instance, 'mimetype', None) is not None) and ('image' in instance.mimetype)):\n        super(FileField, self).update_dimension_fields(instance, force, *args, **kwargs)\n    else:\n        pass\n", "label": 0}
{"function": "\n\ndef HKDF_extract(salt, IKM, hashmod=hashlib.sha256):\n    'HKDF-Extract; see RFC-5869 for the details.'\n    if (salt is None):\n        salt = (b'\\x00' * hashmod().digest_size)\n    if isinstance(salt, text_type):\n        salt = salt.encode('utf-8')\n    return python_hmac.new(salt, IKM, hashmod).digest()\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _options(fieldname):\n    \"\\n            Lookup the full set of options for a Filter Widget\\n            - for Subscriptions we don't want to see just the options available in current data\\n        \"\n    db = current.db\n    if (fieldname == 'location_id'):\n        table = current.s3db.gis_location\n        query = ((table.deleted == False) & (table.level == 'L1'))\n        rows = db(query).select(table.id)\n        options = [row.id for row in rows]\n    return options\n", "label": 0}
{"function": "\n\ndef test_radius_neighbors_classifier_when_no_neighbors():\n    X = np.array([[1.0, 1.0], [2.0, 2.0]])\n    y = np.array([1, 2])\n    radius = 0.1\n    z1 = np.array([[1.01, 1.01], [2.01, 2.01]])\n    z2 = np.array([[1.01, 1.01], [1.4, 1.4]])\n    weight_func = _weight_func\n    for outlier_label in [0, (- 1), None]:\n        for algorithm in ALGORITHMS:\n            for weights in ['uniform', 'distance', weight_func]:\n                rnc = neighbors.RadiusNeighborsClassifier\n                clf = rnc(radius=radius, weights=weights, algorithm=algorithm, outlier_label=outlier_label)\n                clf.fit(X, y)\n                assert_array_equal(np.array([1, 2]), clf.predict(z1))\n                if (outlier_label is None):\n                    assert_raises(ValueError, clf.predict, z2)\n                elif False:\n                    assert_array_equal(np.array([1, outlier_label]), clf.predict(z2))\n", "label": 0}
{"function": "\n\ndef OutputPartial(self, out):\n    if self.has_package_:\n        out.putVarInt32(10)\n        out.putPrefixedString(self.package_)\n    for i in xrange(len(self.capability_)):\n        out.putVarInt32(18)\n        out.putPrefixedString(self.capability_[i])\n    for i in xrange(len(self.call_)):\n        out.putVarInt32(26)\n        out.putPrefixedString(self.call_[i])\n", "label": 0}
{"function": "\n\ndef test_func_adds_roots(self):\n\n    def add_roots(doc):\n        doc.add_root(AnotherModelInTestFunction())\n        doc.add_root(SomeModelInTestFunction())\n    handler = FunctionHandler(add_roots)\n    doc = Document()\n    handler.modify_document(doc)\n    if handler.failed:\n        raise RuntimeError(handler.error)\n    assert (len(doc.roots) == 2)\n", "label": 0}
{"function": "\n\ndef has_valid_checksum(self, number):\n    (given_number, given_checksum) = (number[:(- 1)], number[(- 1)])\n    calculated_checksum = 0\n    fragment = ''\n    parameter = 7\n    for i in range(len(given_number)):\n        fragment = str((int(given_number[i]) * parameter))\n        if fragment.isalnum():\n            calculated_checksum += int(fragment[(- 1)])\n        if (parameter == 1):\n            parameter = 7\n        elif (parameter == 3):\n            parameter = 1\n        elif (parameter == 7):\n            parameter = 3\n    return (str(calculated_checksum)[(- 1)] == given_checksum)\n", "label": 0}
{"function": "\n\ndef _wait_async_done(self, reservation_id, reqids):\n    \"\\n        _wait_async_done(session_id, reqids)\\n        Helper methods that waits for the specified asynchronous requests to be finished,\\n        and which asserts that they were successful. Note that it doesn't actually return\\n        their responses.\\n        @param reqids Tuple containing the request ids for the commands to check.\\n        @return Nothing\\n        \"\n    reqsl = list(reqids)\n    max_count = 15\n    while (len(reqsl) > 0):\n        time.sleep(0.1)\n        max_count -= 1\n        if (max_count == 0):\n            raise Exception('Maximum time spent waiting async done')\n        requests = self.client.check_async_command_status(reservation_id, tuple(reqsl))\n        self.assertEquals(len(reqsl), len(requests))\n        for (rid, req) in six.iteritems(requests):\n            status = req[0]\n            self.assertTrue((status in ('running', 'ok', 'error')))\n            if (status != 'running'):\n                self.assertEquals('ok', status, ('Contents: ' + req[1]))\n                reqsl.remove(rid)\n", "label": 0}
{"function": "\n\ndef _unshorten_lnxlu(self, uri):\n    try:\n        r = requests.get(uri, headers=self._headers, timeout=self._timeout)\n        html = r.text\n        code = re.findall('/\\\\?click\\\\=(.*)\\\\.\"', html)\n        if (len(code) > 0):\n            payload = {\n                'click': code[0],\n            }\n            r = requests.get('http://lnx.lu/', params=payload, headers=self._headers, timeout=self._timeout)\n            return (r.url, r.status_code)\n        else:\n            return (uri, 'No click variable found')\n    except Exception as e:\n        return (uri, str(e))\n", "label": 0}
{"function": "\n\ndef list_names(self, **kwargs):\n    'Get a list of metric names.'\n    url_str = (self.base_url + '/names')\n    newheaders = self.get_headers()\n    if ('dimensions' in kwargs):\n        dimstr = self.get_dimensions_url_string(kwargs['dimensions'])\n        kwargs['dimensions'] = dimstr\n    if kwargs:\n        url_str = (url_str + ('?%s' % urlutils.urlencode(kwargs, True)))\n    (resp, body) = self.client.json_request('GET', url_str, headers=newheaders)\n    return (body['elements'] if (type(body) is dict) else body)\n", "label": 0}
{"function": "\n\ndef mayRaiseException(self, exception_type):\n    if self.tolerant:\n        return False\n    else:\n        if (self.variable_trace is not None):\n            variable = self.getTargetVariableRef().getVariable()\n            if variable.isTempVariable():\n                return False\n            if ((self.previous_trace is not None) and self.previous_trace.mustHaveValue()):\n                return False\n        return True\n", "label": 0}
{"function": "\n\ndef __exit__(self, exc_type, exc_value, traceback):\n    ':func:`burpui.misc.auth.ldap.LdapLoader.__exit__` closes the\\n        connection to the LDAP server.\\n        '\n    if (self.ldap and self.ldap.bound):\n        self.ldap.unbind()\n", "label": 0}
{"function": "\n\n@ConnectorExist(cid_key='smppc')\ndef smppc(self, arg, opts):\n    sc = SMPPClientStatsCollector()\n    headers = ['#Item', 'Value']\n    table = []\n    for (k, v) in sc.get(opts.smppc).getStats().iteritems():\n        if isinstance(v, dict):\n            v = json.dumps(v)\n        row = []\n        row.append(('#%s' % k))\n        if (k[(- 3):] == '_at'):\n            row.append(formatDateTime(v))\n        else:\n            row.append(v)\n        table.append(row)\n    self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))\n", "label": 0}
{"function": "\n\n@property\ndef event_description(self):\n    'complete description of this event in text form\\n\\n        :rtype: str\\n        :returns: event description\\n        '\n    location = (('\\nLocation: ' + self.location) if (self.location != '') else '')\n    description = (('\\nDescription: ' + self.description) if (self.description != '') else '')\n    repitition = (('\\nRepeat: ' + self.recurpattern) if (self.recurpattern != '') else '')\n    return '{}: {}{}{}{}'.format(self._rangestr, self.summary, location, repitition, description)\n", "label": 0}
{"function": "\n\ndef create(self):\n    ' Insert Action '\n    token = request.POST.pop('__token', '')\n    if (not self._is_token_match(token)):\n        success = False\n        error_message = 'Invalid Token'\n        data = None\n    else:\n        data = self.__model__()\n        data.set_state_insert()\n        data.assign_from_dict(request.POST)\n        data.save()\n        success = data.success\n        error_message = data.error_message\n    self._setup_view_parameter()\n    self._set_view_parameter(self.__model_name__, data)\n    self._set_view_parameter('success', success)\n    self._set_view_parameter('error_message', error_message)\n    if (request.is_xhr or request.POST.pop('__as_json', False)):\n        if success:\n            token = self._set_token()\n        self._set_view_parameter('__token', token)\n        return self._get_view_parameter_as_json()\n    return self._load_view('create')\n", "label": 0}
{"function": "\n\ndef is_editable(proposal, user):\n    return ((not proposal.scheduled) and (((proposal.proposer == user) and (proposal.status != 'A')) or topiclead(user, proposal.topic)))\n", "label": 0}
{"function": "\n\ndef visit_BIT(self, type_):\n    if type_.varying:\n        compiled = 'BIT VARYING'\n        if (type_.length is not None):\n            compiled += ('(%d)' % type_.length)\n    else:\n        compiled = ('BIT(%d)' % type_.length)\n    return compiled\n", "label": 0}
{"function": "\n\ndef process_get_results_metadata(self, seqid, iprot, oprot):\n    args = get_results_metadata_args()\n    args.read(iprot)\n    iprot.readMessageEnd()\n    result = get_results_metadata_result()\n    try:\n        result.success = self._handler.get_results_metadata(args.handle)\n        msg_type = TMessageType.REPLY\n    except (TTransport.TTransportException, KeyboardInterrupt, SystemExit):\n        raise\n    except QueryNotFoundException as error:\n        msg_type = TMessageType.REPLY\n        result.error = error\n    except Exception as ex:\n        msg_type = TMessageType.EXCEPTION\n        logging.exception(ex)\n        result = TApplicationException(TApplicationException.INTERNAL_ERROR, 'Internal error')\n    oprot.writeMessageBegin('get_results_metadata', msg_type, seqid)\n    result.write(oprot)\n    oprot.writeMessageEnd()\n    oprot.trans.flush()\n", "label": 0}
{"function": "\n\n@add_method(Return)\ndef handle(self, ch):\n    msg = ch.message\n    msg.rx_channel = ch\n    if ch.on_return:\n        try:\n            ch.on_return(msg)\n        except Exception:\n            logger.error('ERROR in on_return() callback', exc_info=True)\n", "label": 0}
{"function": "\n\ndef col_references_table(col, table):\n    for fk in col.foreign_keys:\n        if fk.references(table):\n            return True\n    return False\n", "label": 0}
{"function": "\n\n@auth\ndef _GET(self, *param, **params):\n    (host_id, guest_id) = self.chk_guestby1(param)\n    if (guest_id is None):\n        return web.notfound()\n    model = findbyguest1(self.orm, guest_id)\n    kvc = KaresansuiVirtConnection()\n    try:\n        domname = kvc.uuid_to_domname(model.uniq_key)\n        if (not domname):\n            return web.notfound()\n        virt = kvc.search_kvg_guests(domname)[0]\n        vcpus_info = virt.get_vcpus_info()\n        self.view.max_vcpus_limit = kvc.get_max_vcpus()\n        self.view.max_vcpus = vcpus_info['bootup_vcpus']\n        self.view.vcpus_limit = vcpus_info['max_vcpus']\n        self.view.vcpus = vcpus_info['vcpus']\n        self.view.cpuTime = virt.get_info()['cpuTime']\n        self.view.hypervisor = virt.get_info()['hypervisor']\n        self.view.guest = model\n    finally:\n        kvc.close()\n    return True\n", "label": 0}
{"function": "\n\ndef Execute(self, action, *args, **kw):\n    'Directly execute an action through an Environment\\n        '\n    action = self.Action(action, *args, **kw)\n    result = action([], [], self)\n    if isinstance(result, SCons.Errors.BuildError):\n        errstr = result.errstr\n        if result.filename:\n            errstr = ((result.filename + ': ') + errstr)\n        sys.stderr.write(('scons: *** %s\\n' % errstr))\n        return result.status\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef modify_cluster(self, **cluster_kwargs):\n    cluster_identifier = cluster_kwargs.pop('cluster_identifier')\n    new_cluster_identifier = cluster_kwargs.pop('new_cluster_identifier', None)\n    cluster = self.describe_clusters(cluster_identifier)[0]\n    for (key, value) in cluster_kwargs.items():\n        setattr(cluster, key, value)\n    if new_cluster_identifier:\n        self.delete_cluster(cluster_identifier)\n        cluster.cluster_identifier = new_cluster_identifier\n        self.clusters[new_cluster_identifier] = cluster\n    return cluster\n", "label": 0}
{"function": "\n\ndef categories(self, fileids=None, patterns=None):\n    meta = self._get_meta()\n    fileids = make_iterable(fileids, meta.keys())\n    result = sorted(set((cat for cat in itertools.chain(*(meta[str(doc_id)].categories for doc_id in fileids)))))\n    if patterns:\n        patterns = make_iterable(patterns)\n        result = [cat for cat in result if some_items_match([cat], patterns)]\n    return result\n", "label": 0}
{"function": "\n\ndef __dump_xml(self, filename):\n    self.log.info('Dumping final status as XML: %s', filename)\n    root = etree.Element('FinalStatus')\n    if self.last_sec:\n        for (label, kpiset) in iteritems(self.last_sec[DataPoint.CUMULATIVE]):\n            root.append(self.__get_xml_summary(label, kpiset))\n    with open(get_full_path(filename), 'wb') as fhd:\n        tree = etree.ElementTree(root)\n        tree.write(fhd, pretty_print=True, encoding='UTF-8', xml_declaration=True)\n", "label": 0}
{"function": "\n\ndef update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES):\n    'Update a wrapper function to look like the wrapped function\\n\\n       wrapper is the function to be updated\\n       wrapped is the original function\\n       assigned is a tuple naming the attributes assigned directly\\n       from the wrapped function to the wrapper function (defaults to\\n       functools.WRAPPER_ASSIGNMENTS)\\n       updated is a tuple naming the attributes of the wrapper that\\n       are updated with the corresponding attribute from the wrapped\\n       function (defaults to functools.WRAPPER_UPDATES)\\n    '\n    for attr in assigned:\n        setattr(wrapper, attr, getattr(wrapped, attr))\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {\n            \n        }))\n    return wrapper\n", "label": 0}
{"function": "\n\ndef backward_cpu(self, xs, gys):\n    assert (len(xs) == self.n_in)\n    assert (len(gys) == self.n_out)\n    return tuple((np.zeros_like(xs).astype(np.float32) for _ in six.moves.range(self.n_in)))\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    from ..classtypes.base import FieldsClassType\n    from ..classtypes.inputobjecttype import InputObjectType\n    if issubclass(cls, InputObjectType):\n        inputfield = self.as_inputfield()\n        return inputfield.contribute_to_class(cls, name)\n    elif issubclass(cls, FieldsClassType):\n        field = self.as_field()\n        return field.contribute_to_class(cls, name)\n", "label": 0}
{"function": "\n\ndef go_back(self):\n    isdir = is_dir(self.input)\n    input_stripped = self.input.rstrip(os.sep)\n    if (not input_stripped):\n        return\n    input_splitted = input_stripped.split(os.sep)\n    entry_name = input_splitted[(- 1)]\n    if isdir:\n        entry_name += os.sep\n    new_input = os.sep.join(input_splitted[0:(- 1)])\n    if new_input:\n        new_input += os.sep\n    self.set_input(new_input)\n    self.set_selected_entry(entry_name)\n", "label": 0}
{"function": "\n\ndef _iter_cursor_results(self):\n    col_names = [c[0] for c in self._cursor.description]\n    while 1:\n        row = self._cursor.fetchone()\n        if (row is None):\n            break\n        (yield self._make_row(row, col_names))\n", "label": 0}
{"function": "\n\ndef input_field(self, name, value, sample_values, back_uri):\n    string_value = (self.format(value) if value else '')\n    html = ('<input class=\"%s\" name=\"%s\" type=\"text\" size=\"%d\" value=\"%s\"/>' % (cgi.escape(self.name()), cgi.escape(name), self.input_field_size(), cgi.escape(string_value, True)))\n    if value:\n        html += ('<br><a href=\"/datastore/edit/%s?next=%s\">%s</a>' % (cgi.escape(string_value, True), urllib.quote_plus(back_uri), cgi.escape(_format_datastore_key(value), True)))\n    return html\n", "label": 0}
{"function": "\n\ndef ssq_error(correct, estimate, mask):\n    'Compute the sum-squared-error for an image, where the estimate is\\n    multiplied by a scalar which minimizes the error. Sums over all pixels\\n    where mask is True. If the inputs are color, each color channel can be\\n    rescaled independently.'\n    assert (correct.ndim == 2)\n    if (np.sum(((estimate ** 2) * mask)) > 1e-05):\n        alpha = (np.sum(((correct * estimate) * mask)) / np.sum(((estimate ** 2) * mask)))\n    else:\n        alpha = 0.0\n    return np.sum((mask * ((correct - (alpha * estimate)) ** 2)))\n", "label": 0}
{"function": "\n\ndef assertRedirects(self, response, expected_url, **kwargs):\n    '\\n        Wrapper for assertRedirects to handle Django pre-1.9.\\n        '\n    if ((VERSION >= (1, 9)) and expected_url.startswith('http://testserver')):\n        expected_url = expected_url[len('http://testserver'):]\n    return super(CommentTestCase, self).assertRedirects(response, expected_url, **kwargs)\n", "label": 0}
{"function": "\n\ndef html_tag(self, template, logical_path, debug=False):\n    environment = self.get_environment(current_app)\n    if (debug or self.debug(current_app)):\n        asset = build_asset(environment, logical_path)\n        urls = []\n        for requirement in asset.requirements:\n            logical_path = requirement.attributes.logical_path\n            url = url_for('static', filename=logical_path, body=1)\n            urls.append(url)\n    else:\n        if (logical_path in environment.manifest.files):\n            logical_path = environment.manifest.files[logical_path]\n        urls = (url_for('static', filename=logical_path),)\n    return Markup('\\n'.join((template.format(url=url) for url in urls)))\n", "label": 0}
{"function": "\n\ndef _send_mail(self, handler, trap, is_duplicate):\n    if (is_duplicate and (not handler['mail_on_duplicate'])):\n        return\n    mail = handler['mail']\n    if (not mail):\n        return\n    recipients = handler['mail'].get('recipients')\n    if (not recipients):\n        return\n    subject = (handler['mail']['subject'] % {\n        'trap_oid': trap.oid,\n        'trap_name': ObjectId(trap.oid).name,\n        'ipaddress': trap.host,\n        'hostname': self.resolver.hostname_or_ip(trap.host),\n    })\n    ctxt = dict(trap=trap, dest_host=self.hostname)\n    try:\n        stats.incr('mail_sent_attempted', 1)\n        send_trap_email(recipients, 'trapperkeeper', subject, self.template_env, ctxt)\n        stats.incr('mail_sent_successful', 1)\n    except socket.error as err:\n        stats.incr('mail_sent_failed', 1)\n        logging.warning('Failed to send e-mail for trap: %s', err)\n", "label": 0}
{"function": "\n\ndef testConversion(self):\n    for (expected_value, binary) in self.tests:\n        binary_sid = ''.join([chr(x) for x in binary])\n        if (expected_value is None):\n            self.assertRaises(ValueError, wmi_parser.BinarySIDtoStringSID, binary_sid)\n        else:\n            self.assertEqual(wmi_parser.BinarySIDtoStringSID(binary_sid), expected_value)\n", "label": 0}
{"function": "\n\ndef astar_path_length(G, source, target, heuristic=None, weight='weight'):\n    'Return the length of the shortest path between source and target using\\n    the A* (\"A-star\") algorithm.\\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n\\n    source : node\\n       Starting node for path\\n\\n    target : node\\n       Ending node for path\\n\\n    heuristic : function\\n       A function to evaluate the estimate of the distance\\n       from the a node to the target.  The function takes\\n       two nodes arguments and must return a number.\\n\\n    Raises\\n    ------\\n    NetworkXNoPath\\n        If no path exists between source and target.\\n\\n    See Also\\n    --------\\n    astar_path\\n\\n    '\n    if ((source not in G) or (target not in G)):\n        msg = 'Either source {} or target {} is not in G'\n        raise nx.NodeNotFound(msg.format(source, target))\n    path = astar_path(G, source, target, heuristic, weight)\n    return sum((G[u][v].get(weight, 1) for (u, v) in zip(path[:(- 1)], path[1:])))\n", "label": 0}
{"function": "\n\ndef serialize(self):\n    buf = bytearray(struct.pack(self._PACK_STR, self.type_, self.aux_len, self.num, addrconv.ipv6.text_to_bin(self.address)))\n    for src in self.srcs:\n        buf.extend(struct.pack('16s', addrconv.ipv6.text_to_bin(src)))\n    if (0 == self.num):\n        self.num = len(self.srcs)\n        struct.pack_into('!H', buf, 2, self.num)\n    if (self.aux is not None):\n        mod = (len(self.aux) % 4)\n        if mod:\n            self.aux += bytearray((4 - mod))\n            self.aux = six.binary_type(self.aux)\n        buf.extend(self.aux)\n        if (0 == self.aux_len):\n            self.aux_len = (len(self.aux) // 4)\n            struct.pack_into('!B', buf, 1, self.aux_len)\n    return six.binary_type(buf)\n", "label": 0}
{"function": "\n\ndef execute(self, cluster, commands):\n    pool = self.get_pool(commands)\n    for (db_num, command_list) in commands.iteritems():\n        for command in command_list:\n            pool.add(command, command.clone().resolve, [cluster[db_num]])\n    return dict(pool.join())\n", "label": 0}
{"function": "\n\ndef test_setslice2(self):\n    pyfunc = list_setslice2\n    cfunc = jit(nopython=True)(pyfunc)\n    sizes = [5, 40]\n    for (n, n_src) in itertools.product(sizes, sizes):\n        indices = [0, 1, (n - 2), (- 1), (- 2), ((- n) + 3), ((- n) - 1), (- n)]\n        for (start, stop) in itertools.product(indices, indices):\n            expected = pyfunc(n, n_src, start, stop)\n            self.assertPreciseEqual(cfunc(n, n_src, start, stop), expected)\n", "label": 0}
{"function": "\n\ndef __enter__(self):\n    if self._entered:\n        raise RuntimeError(('Cannot enter %r twice' % self))\n    self._entered = True\n    self._filters = self._module.filters\n    self._module.filters = self._filters[:]\n    self._showwarning = self._module.showwarning\n    if self._record:\n        log = []\n\n        def showwarning(*args, **kwargs):\n            log.append(WarningMessage(*args, **kwargs))\n        self._module.showwarning = showwarning\n        return log\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef _bytecode_filenames(self, py_filenames):\n    bytecode_files = []\n    for py_file in py_filenames:\n        if (not py_file.endswith('.py')):\n            continue\n        if self.compile:\n            bytecode_files.append((py_file + 'c'))\n        if (self.optimize > 0):\n            bytecode_files.append((py_file + 'o'))\n    return bytecode_files\n", "label": 0}
{"function": "\n\ndef _faster_to_representation(self, instance):\n    'Modified to_representation with optimizations.\\n\\n        1) Returns a plain old dict as opposed to OrderedDict.\\n            (Constructing ordered dict is ~100x slower than `{}`.)\\n        2) Ensure we use a cached list of fields\\n            (this optimization exists in DRF 3.2 but not 3.1)\\n\\n        Arguments:\\n            instance: a model instance or data object\\n        Returns:\\n            Dict of primitive datatypes.\\n        '\n    ret = {\n        \n    }\n    fields = self._readable_fields\n    for field in fields:\n        try:\n            attribute = field.get_attribute(instance)\n        except SkipField:\n            continue\n        if (attribute is None):\n            ret[field.field_name] = None\n        else:\n            ret[field.field_name] = field.to_representation(attribute)\n    return ret\n", "label": 0}
{"function": "\n\ndef store_catch_412_HTTPError(entity):\n    'Returns the stored Entity if the function succeeds or None if the 412 is caught.'\n    try:\n        return syn.store(entity)\n    except SynapseHTTPError as err:\n        if (err.response.status_code == 412):\n            return None\n        raise\n", "label": 0}
{"function": "\n\ndef check_func(conf, func, libs=None):\n    if (libs is None):\n        libs = []\n    code = ('\\nchar %(func)s (void);\\n\\n#ifdef _MSC_VER\\n#pragma function(%(func)s)\\n#endif\\n\\nint main (void)\\n{\\n    return %(func)s();\\n}\\n' % {\n        'func': func,\n    })\n    if libs:\n        msg = ('Checking for function %s in %s' % (func, ' '.join([(conf.env['LIB_FMT'] % lib) for lib in libs])))\n    else:\n        msg = ('Checking for function %s' % func)\n    conf.start_message(msg)\n    old_lib = copy.deepcopy(conf.env['LIBS'])\n    try:\n        for lib in libs[::(- 1)]:\n            conf.env['LIBS'].insert(0, lib)\n        ret = conf.builders['ctasks'].try_program('check_func', code, None)\n        if ret:\n            conf.end_message('yes')\n        else:\n            conf.end_message('no !')\n    finally:\n        conf.env['LIBS'] = old_lib\n    conf.conf_results.append({\n        'type': 'func',\n        'value': func,\n        'result': ret,\n    })\n    return ret\n", "label": 0}
{"function": "\n\ndef get_memory_amount(builder, installProfile, is_mandatory):\n    if (('hardwareSettings' in builder) and ('memory' in builder['hardwareSettings'])):\n        installProfile.memorySize = builder['hardwareSettings']['memory']\n        return installProfile\n    elif is_mandatory:\n        printer.out((('Error: no hardwareSettings part for builder [' + builder['type']) + ']'), printer.ERROR)\n        return 2\n    else:\n        return installProfile\n", "label": 0}
{"function": "\n\ndef __init__(self, raw, buffer_size=DEFAULT_BUFFER_SIZE, max_buffer_size=None):\n    if (not raw.writable()):\n        raise IOError('\"raw\" argument must be writable.')\n    _BufferedIOMixin.__init__(self, raw)\n    if (buffer_size <= 0):\n        raise ValueError('invalid buffer size')\n    if (max_buffer_size is not None):\n        warnings.warn('max_buffer_size is deprecated', DeprecationWarning, self._warning_stack_offset)\n    self.buffer_size = buffer_size\n    self._write_buf = bytearray()\n    self._write_lock = Lock()\n    self._ok = True\n", "label": 0}
{"function": "\n\ndef AnnotateMethod(self, unused_the_api, method, unused_resource):\n    'Annotate a Method with Java Proto specific elements.\\n\\n    Args:\\n      unused_the_api: (Api) The API tree which owns this method.\\n      method: (Method) The method to annotate.\\n      unused_resource: (Resource) The resource which owns this method.\\n\\n    Raises:\\n      ValueError: if missing externalTypeName\\n    '\n    for attr in ('requestType', 'responseType'):\n        schema = method.get(attr)\n        if (schema and (not isinstance(schema, data_types.Void))):\n            name = schema.get('externalTypeName')\n            if (not name):\n                raise ValueError(('missing externalTypeName for %s (%s of method %s)' % (schema['id'], attr, method['rpcMethod'])))\n            java_name = schema.get('javaTypeName')\n            proto_name = (java_name or ('TO_BE_COMPUTED.' + name[(name.rfind('.') + 1):]))\n            schema.SetTemplateValue('protoFullClassName', proto_name)\n", "label": 0}
{"function": "\n\ndef _notification(self, context, method, routers, operation, shuffle_agents):\n    'Notify all or individual Cisco cfg agents.'\n    if utils.is_extension_supported(self._l3plugin, L3AGENT_SCHED):\n        adm_context = ((context.is_admin and context) or context.elevated())\n        self._l3plugin.schedule_routers(adm_context, routers)\n        self._agent_notification(context, method, routers, operation, shuffle_agents)\n    else:\n        cctxt = self.client.prepare(topics=topics.L3_AGENT, fanout=True)\n        cctxt.cast(context, method, routers=[r['id'] for r in routers])\n", "label": 0}
{"function": "\n\ndef fireDNDEvent(self, name, target, widget):\n    if (name == 'dragstart'):\n        self.dragDataStore.setMode(READ_WRITE)\n    elif (name == 'drop'):\n        self.dragDataStore.setMode(READ_ONLY)\n    event = self.makeDragEvent(self.mouseEvent, name, target)\n    widget.onBrowserEvent(event)\n    self.finalize(event)\n    return event\n", "label": 0}
{"function": "\n\ndef directory_files(fpath):\n    for (dir, _, files) in walk(fpath):\n        for file_name in files:\n            (yield path.join(dir, file_name))\n", "label": 0}
{"function": "\n\ndef _get_longname(self, obj):\n    names = [obj.name]\n    parent = obj.parent\n    while (parent is not None):\n        if (isinstance(parent, TestCaseFile) or isinstance(parent, TestDataDirectory) or isinstance(parent, TestCase) or isinstance(parent, UserKeyword)):\n            names.insert(0, parent.name)\n        parent = parent.parent\n    return '.'.join(names)\n", "label": 0}
{"function": "\n\ndef bootstrap_unicel_gateway(apps):\n    currency = (apps.get_model('accounting.Currency') if apps else Currency).objects.get(code='INR')\n    sms_gateway_fee_class = (apps.get_model('smsbillables.SmsGatewayFee') if apps else SmsGatewayFee)\n    sms_gateway_fee_criteria_class = (apps.get_model('smsbillables.SmsGatewayFeeCriteria') if apps else SmsGatewayFeeCriteria)\n    SmsGatewayFee.create_new(SQLUnicelBackend.get_api_id(), INCOMING, 0.5, currency=currency, fee_class=sms_gateway_fee_class, criteria_class=sms_gateway_fee_criteria_class)\n    SmsGatewayFee.create_new(SQLUnicelBackend.get_api_id(), OUTGOING, 0.5, currency=currency, fee_class=sms_gateway_fee_class, criteria_class=sms_gateway_fee_criteria_class)\n    log_smsbillables_info('Updated Unicel gateway fees.')\n", "label": 0}
{"function": "\n\ndef lt(x, y):\n    if ((x is None) and (y is not None)):\n        return True\n    elif ((x is not None) and (y is None)):\n        return False\n    else:\n        return (x < y)\n", "label": 0}
{"function": "\n\ndef print_row(data):\n    'print a single db row in chr and str\\n    '\n    index_line = ''\n    pri_line1 = ''\n    chr_line2 = ''\n    asci = re.compile('[a-zA-Z0-9 ]')\n    for (i, xi) in enumerate(data):\n        if (not (i % 5)):\n            diff = (len(pri_line1) - len(index_line))\n            i = str(i)\n            index_line += (diff * ' ')\n            index_line += i\n        str_v = str(xi)\n        pri_line1 += (str(xi) + ',')\n        c = chr(xi)\n        c = (c if asci.match(c) else ' ')\n        w = len(str_v)\n        c = ((c + ((w - 1) * ' ')) + ',')\n        chr_line2 += c\n    print(index_line)\n    print(pri_line1)\n    print(chr_line2)\n", "label": 0}
{"function": "\n\ndef tick(self):\n    elapsed = (time.time() - self.startTime)\n    t = ((elapsed / float(self.flyTime)) if (self.flyTime > 0) else 1.0)\n    self.interp.InterpolateCamera(t, self.view.camera())\n    self.view.render()\n    if (t >= 1.0):\n        return False\n", "label": 0}
{"function": "\n\ndef battery_status(self):\n    'Attempts to get the battery charge percent.'\n    value = None\n    methods = [self.battery_status_read, self.battery_status_acpi, self.battery_status_upower]\n    for m in methods:\n        value = m()\n        if (value is not None):\n            break\n    return value\n", "label": 0}
{"function": "\n\ndef start(self, environment=None, user=None):\n    'Mark this result started.'\n    envs = [environment]\n    try:\n        latest = self.results.get(is_latest=True, tester=user, environment=environment)\n        if (latest.status == Result.STATUS.skipped):\n            envs = self.environments.all()\n    except ObjectDoesNotExist:\n        pass\n    for env in envs:\n        Result.objects.create(runcaseversion=self, tester=user, environment=env, status=Result.STATUS.started, user=user)\n", "label": 0}
{"function": "\n\ndef _connect_to_upstream(self):\n    if self.proxy_server:\n        upstream_sock = socks.socksocket()\n        upstream_sock.set_proxy(**self.proxy_server)\n    else:\n        upstream_sock = socket.socket()\n    try:\n        upstream_sock.settimeout(self.proxy_timeout)\n        upstream_sock.connect(self.upstream)\n        if self.use_ssl:\n            upstream_sock = wrap_ssl(upstream_sock)\n    except:\n        drop_socket(upstream_sock)\n        raise\n    self.logger.info(('Connected to upstream %s:%d' % self.upstream))\n    return upstream_sock\n", "label": 0}
{"function": "\n\ndef _regenerate(self, dev_mode=False):\n    if self._compiled:\n        for (module_name, (mtime, content, hash)) in self._compiled.items():\n            if ((module_name not in self._collected) or (not os.path.exists(self._collected[module_name])) or (os.path.getmtime(self._collected[module_name]) != mtime)):\n                self._compiled = {\n                    \n                }\n                break\n        else:\n            return\n    modules = [self.main_module, 'pyjslib']\n    while True:\n        if (not modules):\n            break\n        module_name = modules.pop()\n        path = self._collected[module_name]\n        mtime = os.path.getmtime(path)\n        source = read_text_file(path)\n        try:\n            (content, py_deps, js_deps) = self._compile(module_name, source, dev_mode=dev_mode)\n        except:\n            self._compiled = {\n                \n            }\n            raise\n        hash = sha1(smart_str(content)).hexdigest()\n        self._compiled[module_name] = (mtime, content, hash)\n        for name in py_deps:\n            if (name not in self._collected):\n                if (('.' in name) and (name.rsplit('.', 1)[0] in self._collected)):\n                    name = name.rsplit('.', 1)[0]\n                else:\n                    raise ImportError(('The pyjs module %s could not find the dependency %s' % (module_name, name)))\n            if (name not in self._compiled):\n                modules.append(name)\n", "label": 1}
{"function": "\n\ndef result_fail(self, environment=None, comment='', stepnumber=None, bug='', user=None):\n    'Create a failed result for this case.'\n    result = Result.objects.create(runcaseversion=self, tester=user, environment=environment, status=Result.STATUS.failed, comment=comment, user=user)\n    if (stepnumber is not None):\n        try:\n            step = self.caseversion.steps.get(number=stepnumber)\n        except CaseStep.DoesNotExist:\n            pass\n        else:\n            stepresult = StepResult(result=result, step=step)\n            stepresult.status = StepResult.STATUS.failed\n            stepresult.bug_url = bug\n            stepresult.save(user=user)\n    self.save(force_update=True, user=user)\n", "label": 0}
{"function": "\n\ndef __init__(self, content, url, headers=None, trusted=None):\n    encoding = None\n    if (headers and ('Content-Type' in headers)):\n        (content_type, params) = cgi.parse_header(headers['Content-Type'])\n        if ('charset' in params):\n            encoding = params['charset']\n    self.content = content\n    self.parsed = html5lib.parse(self.content, encoding=encoding, namespaceHTMLElements=False)\n    self.url = url\n    self.headers = headers\n    self.trusted = trusted\n", "label": 0}
{"function": "\n\ndef __init__(self, alias_format=None, param_stream=None):\n    self._format = (alias_format or '')\n    self._param_stream = (param_stream or '')\n", "label": 0}
{"function": "\n\ndef GetEditMediaLink(self):\n    'The Picasa API mistakenly returns media-edit rather than edit-media, but\\n    this may change soon.\\n    '\n    for a_link in self.link:\n        if (a_link.rel == 'edit-media'):\n            return a_link\n        if (a_link.rel == 'media-edit'):\n            return a_link\n    return None\n", "label": 0}
{"function": "\n\ndef _pd_assert_dibbler_calls(self, expected, actual):\n    \"Check the external process calls for dibbler are expected\\n\\n        in the case of multiple pd-enabled router ports, the exact sequence\\n        of these calls are not deterministic. It's known, though, that each\\n        external_process call is followed with either an enable() or disable()\\n        \"\n    num_ext_calls = (len(expected) // 2)\n    expected_ext_calls = []\n    actual_ext_calls = []\n    expected_action_calls = []\n    actual_action_calls = []\n    for c in range(num_ext_calls):\n        expected_ext_calls.append(expected[(c * 2)])\n        actual_ext_calls.append(actual[(c * 2)])\n        expected_action_calls.append(expected[((c * 2) + 1)])\n        actual_action_calls.append(actual[((c * 2) + 1)])\n    self.assertEqual(expected_action_calls, actual_action_calls)\n    for exp in expected_ext_calls:\n        for act in actual_ext_calls:\n            if (exp == act):\n                break\n        else:\n            msg = 'Unexpected dibbler external process call.'\n            self.fail(msg)\n", "label": 0}
{"function": "\n\ndef do_complete(self, code, cursor_pos):\n    no_complete = {\n        'status': 'ok',\n        'matches': [],\n        'cursor_start': 0,\n        'cursor_end': cursor_pos,\n        'metadata': dict(),\n    }\n    if ((not code) or (code[(- 1)] == ' ')):\n        return no_complete\n    tokens = code.split()\n    if (not tokens):\n        return no_complete\n    token = tokens[(- 1)]\n    start = (cursor_pos - len(token))\n    matches = self.qsh._complete(code, token)\n    return {\n        'status': 'ok',\n        'matches': sorted(matches),\n        'cursor_start': start,\n        'cursor_end': cursor_pos,\n        'metadata': dict(),\n    }\n", "label": 0}
{"function": "\n\ndef _check_expression(self, symbol, myself):\n    for (cre, state, action, next_state) in self._expressions:\n        mo = cre.match(symbol)\n        if ((state is self.current_state) and mo):\n            if (action is not None):\n                action(mo, self)\n            self.current_state = next_state\n", "label": 0}
{"function": "\n\ndef Tool(self, tool, toolpath=None, **kw):\n    if SCons.Util.is_String(tool):\n        tool = self.subst(tool)\n        if (toolpath is None):\n            toolpath = self.get('toolpath', [])\n        toolpath = list(map(self._find_toolpath_dir, toolpath))\n        tool = SCons.Tool.Tool(tool, toolpath, **kw)\n    tool(self)\n", "label": 0}
{"function": "\n\ndef get(self, *args):\n    self.preflight()\n    self.set_header('Content-Type', 'application/json')\n    m = self.settings.get('manager')\n    pname = ('%s.%s' % (args[0], args[1]))\n    if (not self.api_key.can_read(pname)):\n        raise HTTPError(403)\n    try:\n        info = m.info(pname)\n    except ProcessError:\n        self.set_status(404)\n        return self.write({\n            'error': 'not_found',\n        })\n    self.write(info)\n", "label": 0}
{"function": "\n\ndef send_test(self, test=True):\n    if (not test):\n        try:\n            del self._data['test']\n        except KeyError:\n            pass\n    else:\n        self._data['test'] = 1\n    return self\n", "label": 0}
{"function": "\n\ndef get_connection_mgr():\n    connection_mgr = context.get_context().connection_mgr\n    (server_models, sockpools) = ({\n        \n    }, {\n        \n    })\n    for k in connection_mgr.server_models:\n        server_model = connection_mgr.server_models[k]\n        server_models[repr(k)] = {\n            'info': repr(server_model),\n            'fds': [s.fileno() for s in server_model.active_connections.keys()],\n        }\n    for prot in connection_mgr.sockpools:\n        for sock_type in connection_mgr.sockpools[prot]:\n            sockpool = connection_mgr.sockpools[prot][sock_type]\n            sockpools[repr((prot, sock_type))] = {\n                'info': repr(sockpool),\n                'addresses': map(repr, sockpool.free_socks_by_addr.keys()),\n            }\n    return {\n        'server_models': server_models,\n        'sockpools': sockpools,\n    }\n", "label": 0}
{"function": "\n\ndef handler(self, zh, rc, data, stat):\n    'Handle zookeeper.aget() responses.\\n\\n    This code handles the zookeeper.aget callback. It does not handle watches.\\n\\n    Numeric arguments map to constants. See ``DATA`` in ``help(zookeeper)``\\n    for more information.\\n\\n    Args:\\n      zh Zookeeper handle that made this request.\\n      rc Return code.\\n      data Data stored in the znode.\\n\\n    Does not provide a return value.\\n    '\n    if (zookeeper.OK == rc):\n        logger.debug('This is where your application does work.')\n    else:\n        if (zookeeper.NONODE == rc):\n            logger.info('Node not found. Trying again to set the watch.')\n            time.sleep(1)\n        if (zookeeper.OK != self.aget()):\n            logger.critical('Unable to get znode! Exiting.')\n            sys.exit(1)\n", "label": 0}
{"function": "\n\ndef get_user_by_uri(self, uri):\n    for group in self.group_list:\n        user = group.get_user_by_uri(uri)\n        if user:\n            return user\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    super(BaseMigrationTestCase, self).setUp()\n    for model_class in MODELS:\n        model_class._meta.database = self.database\n    self.database.drop_tables(MODELS, True)\n    self.database.create_tables(MODELS)\n    self.migrator = self.migrator_class(self.database)\n    if ('newpages' in User._meta.reverse_rel):\n        del User._meta.reverse_rel['newpages']\n        delattr(User, 'newpages')\n", "label": 0}
{"function": "\n\ndef plot_connectivity_topos(self, fig=None):\n    ' Plot scalp projections of the sources.\\n\\n        This function only plots the topos. Use in combination with connectivity plotting.\\n\\n        Parameters\\n        ----------\\n        fig : {None, Figure object}, optional\\n            Where to plot the topos. f set to **None**, a new figure is created. Otherwise plot into the provided\\n            figure object.\\n\\n        Returns\\n        -------\\n        fig : Figure object\\n            Instance of the figure in which was plotted.\\n        '\n    self._prepare_plots(True, False)\n    if self.plot_outside_topo:\n        fig = self.plotting.plot_connectivity_topos('outside', self.topo_, self.mixmaps_, fig)\n    elif (self.plot_diagonal == 'topo'):\n        fig = self.plotting.plot_connectivity_topos('diagonal', self.topo_, self.mixmaps_, fig)\n    return fig\n", "label": 0}
{"function": "\n\ndef __getattr__(self, name):\n    if (name == 'x'):\n        return self._x\n    elif (name == 'y'):\n        return self._y\n    else:\n        return name\n", "label": 0}
{"function": "\n\ndef matchesPredicates(self, elem):\n    if ((self.elementName != None) and (self.elementName != elem.name)):\n        return 0\n    for p in self.predicates:\n        if (not p.value(elem)):\n            return 0\n    return 1\n", "label": 0}
{"function": "\n\ndef start_selection(self, event):\n    if (event.inaxes != self._axes):\n        return False\n    if (event.key == SCRUBBING_KEY):\n        if (not self._roi.defined()):\n            return False\n        elif (not self._roi.contains(event.xdata, event.ydata)):\n            return False\n    self._roi_store()\n    if (event.key == SCRUBBING_KEY):\n        self._scrubbing = True\n        self._dx = (event.xdata - self._roi.center())\n    else:\n        self.reset()\n        self._roi.set_range(event.xdata, event.xdata)\n        self._xi = event.xdata\n    self._mid_selection = True\n    self._sync_patch()\n", "label": 0}
{"function": "\n\ndef print_stats(stats):\n    console_logger.info((((' %-' + str(STATS_NAME_WIDTH)) + 's %7s %12s %7s %7s %7s  | %7s %7s') % ('Name', '# reqs', '# fails', 'Avg', 'Min', 'Max', 'Median', 'req/s')))\n    console_logger.info(('-' * (80 + STATS_NAME_WIDTH)))\n    total_rps = 0\n    total_reqs = 0\n    total_failures = 0\n    for key in sorted(stats.iterkeys()):\n        r = stats[key]\n        total_rps += r.current_rps\n        total_reqs += r.num_requests\n        total_failures += r.num_failures\n        console_logger.info(r)\n    console_logger.info(('-' * (80 + STATS_NAME_WIDTH)))\n    try:\n        fail_percent = ((total_failures / float(total_reqs)) * 100)\n    except ZeroDivisionError:\n        fail_percent = 0\n    console_logger.info((((' %-' + str(STATS_NAME_WIDTH)) + 's %7d %12s %42.2f') % ('Total', total_reqs, ('%d(%.2f%%)' % (total_failures, fail_percent)), total_rps)))\n    console_logger.info('')\n", "label": 0}
{"function": "\n\ndef __getattr__(self, name):\n    t = type(self)\n    try:\n        i = t.name_to_idx[name]\n    except KeyError:\n        raise AttributeError(name)\n    f = t.fields[i]\n    if (i < len(self.data)):\n        v = self.data[i]\n    else:\n        v = ''\n    if (len(f) >= 3):\n        if (v == ''):\n            return None\n        return f[2](v)\n    else:\n        return v\n", "label": 0}
{"function": "\n\ndef getUncontacted(self):\n    return [n for n in self if (n.id not in self.contacted)]\n", "label": 0}
{"function": "\n\ndef _test_sync_state_helper(self, known_net_ids, active_net_ids):\n    active_networks = set((mock.Mock(id=netid) for netid in active_net_ids))\n    with mock.patch(DHCP_PLUGIN) as plug:\n        mock_plugin = mock.Mock()\n        mock_plugin.get_active_networks_info.return_value = active_networks\n        plug.return_value = mock_plugin\n        dhcp = dhcp_agent.DhcpAgent(HOSTNAME)\n        attrs_to_mock = dict([(a, mock.DEFAULT) for a in ['disable_dhcp_helper', 'cache', 'safe_configure_dhcp_for_network']])\n        with mock.patch.multiple(dhcp, **attrs_to_mock) as mocks:\n            mocks['cache'].get_network_ids.return_value = known_net_ids\n            dhcp.sync_state()\n            diff = (set(known_net_ids) - set(active_net_ids))\n            exp_disable = [mock.call(net_id) for net_id in diff]\n            mocks['cache'].assert_has_calls([mock.call.get_network_ids()])\n            mocks['disable_dhcp_helper'].assert_has_calls(exp_disable)\n", "label": 0}
{"function": "\n\ndef write_lofl(lofl, filename):\n    a_file = open(filename, 'w')\n    for seq in lofl:\n        for it in seq:\n            a_file.write(('%s\\n' % str(it)))\n        a_file.write('\\n')\n    a_file.close()\n", "label": 0}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (node in self.nodes_seen):\n        self.nodes_seen.discard(node)\n        self.process_node(fgraph, node)\n    if (not isinstance(node, string_types)):\n        assert node.inputs\n    if isinstance(new_r, graph.Constant):\n        self.process_constant(fgraph, new_r)\n", "label": 0}
{"function": "\n\n@account_group.command(context_settings=CONTEXT_SETTINGS)\n@click.option('--token', '-t', type=str, help='digital ocean authentication token', metavar='<token>')\n@click.option('--tablefmt', '-f', type=click.Choice(['fancy_grid', 'simple', 'plain', 'grid', 'pipe', 'orgtbl', 'psql', 'rst', 'mediawiki', 'html', 'latex', 'latex_booktabs', 'tsv']), help='output table format', default='fancy_grid', metavar='<format>')\n@click.option('--proxy', '-p', help='proxy url to be used for this call', metavar='<http://ip:port>')\ndef account(token, tablefmt, proxy):\n    '\\n\\tget digital ocean account info\\n\\t'\n    method = 'GET'\n    url = ACCOUNT_INFO\n    result = DigitalOcean.do_request(method, url, token=token, proxy=proxy)\n    if result['has_error']:\n        click.echo()\n        click.echo(('Error: %s' % result['error_message']))\n    else:\n        record = 'account'\n        headers = ['Fields', 'Values']\n        table = []\n        for key in result['account'].keys():\n            table.append([key, result['account'][key]])\n        data = {\n            'headers': headers,\n            'table_data': table,\n        }\n        print_table(tablefmt, data, record)\n", "label": 0}
{"function": "\n\ndef test_current_time(self):\n    self.skipTest('time.xmlrpc.com is unreliable')\n    server = xmlrpclib.ServerProxy('http://time.xmlrpc.com/RPC2')\n    try:\n        t0 = server.currentTime.getCurrentTime()\n    except socket.error as e:\n        self.skipTest(('network error: %s' % e))\n        return\n    t1 = xmlrpclib.DateTime()\n    dt0 = xmlrpclib._datetime_type(t0.value)\n    dt1 = xmlrpclib._datetime_type(t1.value)\n    if (dt0 > dt1):\n        delta = (dt0 - dt1)\n    else:\n        delta = (dt1 - dt0)\n    self.assertTrue((delta.days <= 1))\n", "label": 0}
{"function": "\n\ndef write_content(self, content, content_type=None):\n    'Helper method to write content bytes to output stream.'\n    if (content_type is not None):\n        self.send_header(HTTP_HEADER_CONTENT_TYPE, content_type)\n    if ('gzip' in self.headers.get(HTTP_HEADER_ACCEPT_ENCODING, '')):\n        content = gzip.compress(content)\n        self.send_header(HTTP_HEADER_CONTENT_ENCODING, 'gzip')\n        self.send_header(HTTP_HEADER_VARY, HTTP_HEADER_ACCEPT_ENCODING)\n    self.send_header(HTTP_HEADER_CONTENT_LENGTH, str(len(content)))\n    self.end_headers()\n    if (self.command == 'HEAD'):\n        return\n    self.wfile.write(content)\n", "label": 0}
{"function": "\n\ndef test_get_vifs_by_ids(self):\n    for i in range(2):\n        self.create_ovs_port()\n    vif_ports = [self.create_ovs_vif_port() for i in range(3)]\n    by_id = self.br.get_vifs_by_ids([v.vif_id for v in vif_ports])\n    by_id = {vid: str(vport) for (vid, vport) in by_id.items()}\n    self.assertEqual({v.vif_id: str(v) for v in vif_ports}, by_id)\n", "label": 0}
{"function": "\n\ndef all_consumed_offsets(self):\n    'Returns consumed offsets as {TopicPartition: OffsetAndMetadata}'\n    all_consumed = {\n        \n    }\n    for (partition, state) in six.iteritems(self.assignment):\n        if state.has_valid_position:\n            all_consumed[partition] = OffsetAndMetadata(state.position, '')\n    return all_consumed\n", "label": 0}
{"function": "\n\ndef send(self):\n    '\\n        Sends the batch request to the server and returns a list of RpcResponse\\n        objects.  The list will be in the order that the requests were made to\\n        the batch.  Note that the RpcResponse objects may contain an error or a\\n        successful result.  When you iterate through the list, you must test for\\n        response.error.\\n\\n        send() may not be called more than once.\\n        '\n    if self.sent:\n        raise Exception('Batch already sent. Cannot send() again.')\n    else:\n        self.sent = True\n        results = self.client.transport.request(self.req_list)\n        id_to_method = {\n            \n        }\n        by_id = {\n            \n        }\n        for res in results:\n            reqid = res['id']\n            by_id[reqid] = res\n        in_req_order = []\n        for req in self.req_list:\n            reqid = req['id']\n            result = None\n            error = None\n            resp = safe_get(by_id, reqid)\n            if (resp is None):\n                msg = ('Batch response missing result for request id: %s' % reqid)\n                error = RpcException(ERR_INVALID_RESP, msg)\n            else:\n                r_err = safe_get(resp, 'error')\n                if (r_err is None):\n                    result = resp['result']\n                else:\n                    error = RpcException(r_err['code'], r_err['message'], safe_get(r_err, 'data'))\n            in_req_order.append(RpcResponse(req, result, error))\n        return in_req_order\n", "label": 0}
{"function": "\n\ndef handle(self, request, data):\n    volume_type_id = self.initial['id']\n    try:\n        cinder.volume_type_update(request, volume_type_id, data['name'], data['description'])\n        message = _('Successfully updated volume type.')\n        messages.success(request, message)\n        return True\n    except Exception as ex:\n        redirect = reverse('horizon:admin:volumes:index')\n        if (ex.code == 409):\n            error_message = _('New name conflicts with another volume type.')\n        else:\n            error_message = _('Unable to update volume type.')\n        exceptions.handle(request, error_message, redirect=redirect)\n", "label": 0}
{"function": "\n\ndef visit_Input(self, node):\n    name = node.name\n    width = (self.visit(node.width) if (node.width is not None) else None)\n    signed = node.signed\n    obj = vtypes.Input(width, signed=signed, name=name)\n    if (node.width is not None):\n        obj._set_raw_width(self.visit(node.width.msb), self.visit(node.width.lsb))\n    self.add_object(obj)\n    return obj\n", "label": 0}
{"function": "\n\ndef find(l, predicate):\n    results = [x for x in l if predicate(x)]\n    return (results[0] if (len(results) > 0) else None)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, position):\n    (i, j) = self._get_indexes(position)\n    if isinstance(i, slice):\n        if (j is None):\n            return Table(self.data[position])\n        else:\n            return Table((cells[j] for cells in self.data[i]))\n    else:\n        try:\n            row = self.data[i]\n        except IndexError:\n            msg = 'no row at index %r of %d-row table'\n            raise IndexError((msg % (position, len(self))))\n        if (j is None):\n            return row\n        else:\n            return row[j]\n", "label": 0}
{"function": "\n\ndef test_exists_many_with_none_keys(self):\n    try:\n        TestExistsMany.client.exists_many(None, {\n            \n        })\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'Keys should be specified as a list or tuple.')\n", "label": 0}
{"function": "\n\ndef ack(self, delivery_tag=None, multiple=False):\n    'Acknowledge Message.\\n\\n        :param int/long delivery_tag: Server-assigned delivery tag\\n        :param bool multiple: Acknowledge multiple messages\\n\\n        :raises AMQPInvalidArgument: Invalid Parameters\\n        :raises AMQPChannelError: Raises if the channel encountered an error.\\n        :raises AMQPConnectionError: Raises if the connection\\n                                     encountered an error.\\n\\n        :return:\\n        '\n    if ((delivery_tag is not None) and (not compatibility.is_integer(delivery_tag))):\n        raise AMQPInvalidArgument('delivery_tag should be an integer or None')\n    elif (not isinstance(multiple, bool)):\n        raise AMQPInvalidArgument('multiple should be a boolean')\n    ack_frame = pamqp_spec.Basic.Ack(delivery_tag=delivery_tag, multiple=multiple)\n    self._channel.write_frame(ack_frame)\n", "label": 0}
{"function": "\n\ndef tostring(self, sep='', endcard=True, padding=True):\n    \"\\n        Returns a string representation of the header.\\n\\n        By default this uses no separator between cards, adds the END card, and\\n        pads the string with spaces to the next multiple of 2880 bytes.  That\\n        is, it returns the header exactly as it would appear in a FITS file.\\n\\n        Parameters\\n        ----------\\n        sep : str, optional\\n            The character or string with which to separate cards.  By default\\n            there is no separator, but one could use ``'\\\\\\\\n'``, for example, to\\n            separate each card with a new line\\n\\n        endcard : bool, optional\\n            If True (default) adds the END card to the end of the header\\n            string\\n\\n        padding : bool, optional\\n            If True (default) pads the string with spaces out to the next\\n            multiple of 2880 characters\\n\\n        Returns\\n        -------\\n        s : string\\n            A string representing a FITS header.\\n        \"\n    lines = []\n    for card in self._cards:\n        s = str(card)\n        while s:\n            lines.append(s[:Card.length])\n            s = s[Card.length:]\n    s = sep.join(lines)\n    if endcard:\n        s += (sep + _pad('END'))\n    if padding:\n        s += (' ' * _pad_length(len(s)))\n    return s\n", "label": 0}
{"function": "\n\ndef mergeStatementsSequence(self, statement_sequence):\n    assert (statement_sequence.parent is self)\n    old_statements = list(self.getStatements())\n    assert (statement_sequence in old_statements), (statement_sequence, self)\n    merge_index = old_statements.index(statement_sequence)\n    new_statements = ((tuple(old_statements[:merge_index]) + statement_sequence.getStatements()) + tuple(old_statements[(merge_index + 1):]))\n    self.setChild('statements', new_statements)\n", "label": 0}
{"function": "\n\n@classmethod\n@quickcache(['domain', 'app_id'])\ndef _app_data(cls, domain, app_id):\n    defaults = MaltAppData(AMPLIFIES_NOT_SET, AMPLIFIES_NOT_SET, 15, 3, False)\n    if (not app_id):\n        return defaults\n    try:\n        app = get_app(domain, app_id)\n    except Http404:\n        logger.debug(('App not found %s' % app_id))\n        return defaults\n    return MaltAppData(getattr(app, 'amplifies_workers', AMPLIFIES_NOT_SET), getattr(app, 'amplifies_project', AMPLIFIES_NOT_SET), getattr(app, 'minimum_use_threshold', 15), getattr(app, 'experienced_threshold', 3), app.is_deleted())\n", "label": 0}
{"function": "\n\ndef run_conv_nnet2_classif(use_gpu, seed, isize, ksize, bsize, n_train=10, check_isfinite=True, pickle=False, verbose=0, version=(- 1)):\n    'Run the train function returned by build_conv_nnet2_classif on one device.\\n    '\n    utt.seed_rng(seed)\n    (train, params, x_shape, y_shape, mode) = build_conv_nnet2_classif(use_gpu=use_gpu, isize=isize, ksize=ksize, n_batch=bsize, verbose=verbose, version=version, check_isfinite=check_isfinite)\n    if use_gpu:\n        device = 'GPU'\n    else:\n        device = 'CPU'\n    xval = my_rand(*x_shape)\n    yval = my_rand(*y_shape)\n    lr = theano._asarray(0.01, dtype='float32')\n    rvals = my_zeros(n_train)\n    t0 = time.time()\n    for i in xrange(n_train):\n        rvals[i] = train(xval, yval, lr)[0]\n    t1 = time.time()\n    print_mode(mode)\n    if (pickle and isinstance(mode, theano.compile.ProfileMode)):\n        import pickle\n        print(('BEGIN %s profile mode dump' % device))\n        print(pickle.dumps(mode))\n        print(('END %s profile mode dump' % device))\n", "label": 0}
{"function": "\n\ndef iqilu_download(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    ''\n    if re.match('http://v.iqilu.com/\\\\w+', url):\n        html = get_content(url)\n        url = match1(html, \"<input type='hidden' id='playerId' url='(.+)'\")\n        title = match1(html, '<meta name=\"description\" content=\"(.*?)\\\\\"\\\\W')\n        (type_, ext, size) = url_info(url)\n        print_info(site_info, title, type_, size)\n        if (not info_only):\n            download_urls([url], title, ext, total_size=None, output_dir=output_dir, merge=merge)\n", "label": 0}
{"function": "\n\ndef bPrimary(self, prob):\n    '\\n        The primary magnetic flux density from a magnetic vector potential\\n\\n        :param Problem prob: FDEM problem\\n        :rtype: numpy.ndarray\\n        :return: primary magnetic field\\n        '\n    formulation = prob._formulation\n    if (formulation is 'EB'):\n        gridX = prob.mesh.gridEx\n        gridY = prob.mesh.gridEy\n        gridZ = prob.mesh.gridEz\n        C = prob.mesh.edgeCurl\n    elif (formulation is 'HJ'):\n        gridX = prob.mesh.gridFx\n        gridY = prob.mesh.gridFy\n        gridZ = prob.mesh.gridFz\n        C = prob.mesh.edgeCurl.T\n    if (prob.mesh._meshType is 'CYL'):\n        if (not prob.mesh.isSymmetric):\n            raise NotImplementedError('Non-symmetric cyl mesh not implemented yet!')\n        a = MagneticDipoleVectorPotential(self.loc, gridY, 'y', mu=self.mu, moment=self.moment)\n    else:\n        srcfct = MagneticDipoleVectorPotential\n        ax = srcfct(self.loc, gridX, 'x', mu=self.mu, moment=self.moment)\n        ay = srcfct(self.loc, gridY, 'y', mu=self.mu, moment=self.moment)\n        az = srcfct(self.loc, gridZ, 'z', mu=self.mu, moment=self.moment)\n        a = np.concatenate((ax, ay, az))\n    return (C * a)\n", "label": 0}
{"function": "\n\ndef __init__(self, getter, attribute, new, spec, create, spec_set, autospec, new_callable, kwargs):\n    if (new_callable is not None):\n        if (new is not DEFAULT):\n            raise ValueError(\"Cannot use 'new' and 'new_callable' together\")\n        if (autospec is not None):\n            raise ValueError(\"Cannot use 'autospec' and 'new_callable' together\")\n    self.getter = getter\n    self.attribute = attribute\n    self.new = new\n    self.new_callable = new_callable\n    self.spec = spec\n    self.create = create\n    self.has_local = False\n    self.spec_set = spec_set\n    self.autospec = autospec\n    self.kwargs = kwargs\n    self.additional_patchers = []\n", "label": 0}
{"function": "\n\ndef slide(*args, **kwargs):\n\n    def wrap(f):\n\n        @wraps(f)\n        def wrapped_f():\n            print_func(f)\n            sys.stdin.readline()\n            if kwargs.get('executable', False):\n                print_func_result(f)\n                sys.stdin.readline()\n        return wrapped_f\n    if ((len(args) == 1) and callable(args[0])):\n        return wrap(args[0])\n    else:\n        return wrap\n", "label": 0}
{"function": "\n\ndef decistmt(s):\n    'Substitute Decimals for floats in a string of statements.\\n\\n    >>> from decimal import Decimal\\n    >>> s = \\'print +21.3e-5*-.1234/81.7\\'\\n    >>> decistmt(s)\\n    \"print +Decimal (\\'21.3e-5\\')*-Decimal (\\'.1234\\')/Decimal (\\'81.7\\')\"\\n\\n    The format of the exponent is inherited from the platform C library.\\n    Known cases are \"e-007\" (Windows) and \"e-07\" (not Windows).  Since\\n    we\\'re only showing 12 digits, and the 13th isn\\'t close to 5, the\\n    rest of the output should be platform-independent.\\n\\n    >>> exec(s) #doctest: +ELLIPSIS\\n    -3.21716034272e-0...7\\n\\n    Output from calculations with Decimal should be identical across all\\n    platforms.\\n\\n    >>> exec(decistmt(s))\\n    -3.217160342717258261933904529E-7\\n    '\n    result = []\n    g = generate_tokens(StringIO(s).readline)\n    for (toknum, tokval, _, _, _) in g:\n        if ((toknum == NUMBER) and ('.' in tokval)):\n            result.extend([(NAME, 'Decimal'), (OP, '('), (STRING, repr(tokval)), (OP, ')')])\n        else:\n            result.append((toknum, tokval))\n    return untokenize(result)\n", "label": 0}
{"function": "\n\ndef onRadioSelect(self, sender, keyCode=None, modifiers=None):\n    if (sender == self.radLevel5):\n        self.level = 5\n    elif (sender == self.radLevel10):\n        self.level = 10\n    elif (sender == self.radLevel15):\n        self.level = 15\n    elif (sender == self.radLevel20):\n        self.level = 20\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _aggregator_counter_kind(combine_fn):\n    \"Returns the counter aggregation kind for the combine_fn passed in.\\n\\n    Args:\\n      combine_fn: The combining function used in an Aggregator.\\n\\n    Returns:\\n      The aggregation_kind (to use in a Counter) that matches combine_fn.\\n\\n    Raises:\\n      ValueError if the combine_fn doesn't map to any supported\\n      aggregation kind.\\n    \"\n    combine_kind_map = {\n        sum: Counter.SUM,\n        max: Counter.MAX,\n        min: Counter.MIN,\n        combiners.Mean: Counter.MEAN,\n    }\n    try:\n        return combine_kind_map[combine_fn]\n    except KeyError:\n        try:\n            return combine_kind_map[combine_fn.__class__]\n        except KeyError:\n            raise ValueError(('combine_fn %r (class %r) does not map to a supported aggregation kind' % (combine_fn, combine_fn.__class__)))\n", "label": 0}
{"function": "\n\ndef run(self):\n    '\\n        Kill any open Redshift sessions for the given database.\\n        '\n    connection = self.output().connect()\n    query = \"select pg_terminate_backend(process) from STV_SESSIONS where db_name=%s and user_name != 'rdsdb' and process != pg_backend_pid()\"\n    cursor = connection.cursor()\n    logger.info('Killing all open Redshift sessions for database: %s', self.database)\n    try:\n        cursor.execute(query, (self.database,))\n        cursor.close()\n        connection.commit()\n    except psycopg2.DatabaseError as e:\n        if (e.message and ('EOF' in e.message)):\n            connection.close()\n            logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n            time.sleep(self.connection_reset_wait_seconds)\n            logger.info('Reconnecting to Redshift')\n            connection = self.output().connect()\n        else:\n            raise\n    try:\n        self.output().touch(connection)\n        connection.commit()\n    finally:\n        connection.close()\n    logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n", "label": 0}
{"function": "\n\n@AccountRequired\n@PostOnly\ndef unfollow(request, user_id):\n    followed = get_object_or_404(User, pk=user_id)\n    try:\n        follow_instance = UserToUserFollow.objects.get(follower=request.user, followed=followed)\n        follow_instance.is_following = False\n        follow_instance.stopped_following = datetime.datetime.now()\n        follow_instance.save()\n    except UserToUserFollow.DoesNotExist:\n        pass\n    cache.bust(followed)\n    if request.is_ajax():\n        button = render_inclusiontag(request, 'follow_button followed', 'users_tags', {\n            'followed': followed,\n        })\n        return json_response({\n            'button': button,\n        })\n    else:\n        return redirect(followed)\n", "label": 0}
{"function": "\n\ndef alphanumeric_sort_key(key):\n    '\\n    Sort the given iterable in the way that humans expect.\\n    Thanks to http://stackoverflow.com/a/2669120/240553\\n    '\n    import re\n    convert = (lambda text: (int(text) if text.isdigit() else text))\n    return [convert(c) for c in re.split('([0-9]+)', key)]\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _fill_scope_refs(name, scope):\n    \"Put referenced name in 'ref' dictionary of a scope.\\n\\n        Walks up the scope tree and adds the name to 'ref' of every scope\\n        up in the tree until a scope that defines referenced name is reached.\\n        \"\n    symbol = scope.resolve(name)\n    if (symbol is None):\n        return\n    orig_scope = symbol.scope\n    scope.refs[name] = orig_scope\n    while (scope is not orig_scope):\n        scope = scope.get_enclosing_scope()\n        scope.refs[name] = orig_scope\n", "label": 0}
{"function": "\n\ndef _read_word_block(self, stream):\n    words = []\n    for i in range(20):\n        line = stream.readline()\n        if (not line):\n            continue\n        words.append(line.strip())\n    return words\n", "label": 0}
{"function": "\n\ndef deallocate_for_instance(self, context, instance, **kwargs):\n    'Deallocate all network resources related to the instance.'\n    LOG.debug('deallocate_for_instance()', instance=instance)\n    search_opts = {\n        'device_id': instance.uuid,\n    }\n    neutron = get_client(context)\n    data = neutron.list_ports(**search_opts)\n    ports = [port['id'] for port in data.get('ports', [])]\n    requested_networks = (kwargs.get('requested_networks') or [])\n    if isinstance(requested_networks, objects.NetworkRequestList):\n        requested_networks = requested_networks.as_tuples()\n    ports_to_skip = set([port_id for (nets, fips, port_id, pci_request_id) in requested_networks])\n    ports_to_skip |= set(self._get_preexisting_port_ids(instance))\n    ports = (set(ports) - ports_to_skip)\n    self._unbind_ports(context, ports_to_skip, neutron)\n    self._delete_ports(neutron, instance, ports, raise_if_fail=True)\n    base_api.update_instance_cache_with_nw_info(self, context, instance, network_model.NetworkInfo([]))\n", "label": 0}
{"function": "\n\ndef process_failure(self, partial_eval, error, feature, dpoint, d_index):\n    logger.warning(('Fail evaluating %s: %s %s' % (feature, type(error), error)))\n    self._fit_failure_stats['discarded_samples'].append(dpoint.get('pk', 'PK-NOT-FOUND'))\n    feature_errors = self._fit_failure_stats['features'][feature]\n    feature_errors.append(dpoint)\n    if (d_index < self.FEATURE_STRICT_UNTIL):\n        self.exclude_feature(feature, partial_eval)\n    elif (len(feature_errors) > self.FEATURE_MAX_ERRORS_ALLOWED):\n        self.exclude_feature(feature, partial_eval)\n", "label": 0}
{"function": "\n\ndef configure_formatter(self, config):\n    'Configure a formatter from a dictionary.'\n    if ('()' in config):\n        factory = config['()']\n        try:\n            result = self.configure_custom(config)\n        except TypeError as te:\n            if (\"'format'\" not in str(te)):\n                raise\n            config['fmt'] = config.pop('format')\n            config['()'] = factory\n            result = self.configure_custom(config)\n    else:\n        fmt = config.get('format', None)\n        dfmt = config.get('datefmt', None)\n        result = logging.Formatter(fmt, dfmt)\n    return result\n", "label": 0}
{"function": "\n\ndef compile_dir(env, src_path, dst_path, pattern='^.*\\\\.html$', encoding='utf-8', base_dir=None):\n    'Compiles a directory of Jinja2 templates to python code.\\n  \\n  :param env: a Jinja2 Environment instance.\\n  :param src_path: path to the source directory.\\n  :param dst_path: path to the destination directory.\\n  :param encoding: template encoding.\\n  :param base_dir: the base path to be removed from the compiled template filename.\\n  '\n    from os import path, listdir, mkdir\n    file_re = re.compile(pattern)\n    if (base_dir is None):\n        base_dir = src_path\n    for filename in listdir(src_path):\n        src_name = path.join(src_path, filename)\n        dst_name = path.join(dst_path, filename)\n        if path.isdir(src_name):\n            mkdir(dst_name)\n            compile_dir(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n        elif (path.isfile(src_name) and file_re.match(filename)):\n            compile_file(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n", "label": 0}
{"function": "\n\n@attr('slow')\ndef test_basic(self):\n    '\\n        Tests that basic correlations work for odd and even\\n        dimensions of image and filter shapes, as well as rectangular\\n        images and filters.\\n        '\n    border_modes = ['valid', 'full', 'half', (1, 1), (2, 1), (1, 2), (3, 3), 1]\n    img_shapes = [(2, 2, 3, 3), (3, 2, 8, 8), (3, 2, 7, 5), (3, 2, 7, 5), (3, 2, 8, 8), (3, 2, 7, 5)]\n    fil_shapes = [(2, 2, 2, 2), (4, 2, 5, 5), (5, 2, 2, 3), (5, 2, 3, 2), (4, 2, 5, 5), (5, 2, 2, 3)]\n    for border_mode in border_modes:\n        for (img, fil) in zip(img_shapes, fil_shapes):\n            self.validate(img, fil, border_mode, verify_grad=False)\n    self.validate((1, 10, 213, 129), (46, 10, 212, 1), 'valid', verify_grad=False)\n", "label": 0}
{"function": "\n\ndef _on_auth(self, user):\n    if (not user):\n        raise tornado.web.HTTPError(403, 'Google auth failed')\n    access_token = user['access_token']\n    try:\n        response = httpclient.HTTPClient().fetch('https://www.googleapis.com/plus/v1/people/me', headers={\n            'Authorization': ('Bearer %s' % access_token),\n        })\n    except Exception as e:\n        raise tornado.web.HTTPError(403, ('Google auth failed: %s' % e))\n    email = json.loads(response.body.decode('utf-8'))['emails'][0]['value']\n    if (not re.match(self.application.options.auth, email)):\n        message = \"Access denied to '{email}'. Please use another account or ask your admin to add your email to flower --auth.\".format(email=email)\n        raise tornado.web.HTTPError(403, message)\n    self.set_secure_cookie('user', str(email))\n    next = self.get_argument('next', '/')\n    self.redirect(next)\n", "label": 0}
{"function": "\n\ndef clean_content(form):\n    status = form.cleaned_data.get('status')\n    content = form.cleaned_data.get('content')\n    if ((status == CONTENT_STATUS_PUBLISHED) and (not content)):\n        raise ValidationError(_('This field is required if status is set to published.'))\n    return content\n", "label": 0}
{"function": "\n\ndef create_list_setting(self, name):\n    hlayout = QtGui.QHBoxLayout()\n    setting = self.get_setting(name)\n    button = None\n    if setting.button:\n        button = QtGui.QPushButton(setting.button)\n        button.clicked.connect((lambda : setting.button_callback(button)))\n    combo = QtGui.QComboBox()\n    combo.setObjectName(setting.name)\n    combo.currentIndexChanged.connect(self.call_with_object('setting_changed', combo, setting))\n    combo.editTextChanged.connect(self.call_with_object('setting_changed', combo, setting))\n    combo.setStatusTip(setting.description)\n    combo.setToolTip(setting.description)\n    for val in setting.values:\n        combo.addItem(val)\n    default_index = combo.findText(setting.default_value)\n    if (default_index != (- 1)):\n        combo.setCurrentIndex(default_index)\n    hlayout.addWidget(QtGui.QLabel())\n    hlayout.addWidget(combo)\n    if button:\n        hlayout.addWidget(button)\n    return hlayout\n", "label": 0}
{"function": "\n\ndef _fill_dict(self, xmldoc, element_name):\n    xmlelements = self._get_child_nodes(xmldoc, element_name)\n    if xmlelements:\n        return_obj = {\n            \n        }\n        for child in xmlelements[0].childNodes:\n            if child.firstChild:\n                return_obj[child.nodeName] = child.firstChild.nodeValue\n        return return_obj\n", "label": 0}
{"function": "\n\ndef onMouseUp(self, sender, x, y):\n    self.dragging = NOT_DRAGGING\n    if self.draggingImage:\n        GlassWidget.hide()\n        if ((self.currentDragOperation == 'none') or (not self.currentTargetElement)):\n            if self.currentTargetElement:\n                self.fireDNDEvent('dragleave', self.currentTargetElement, self.currentDropWidget)\n            else:\n                self.currentDragOperation = 'none'\n            self.returnDrag()\n        else:\n            drop_event = self.fireDNDEvent('drop', self.currentTargetElement, self.currentDropWidget)\n            if isCanceled(drop_event):\n                self.currentDragOperation = drop_event.dataTransfer.dropEffect\n            else:\n                self.currentDragOperation = 'none'\n            self.zapDragImage()\n        self.fireDNDEvent('dragend', None, self.dragWidget)\n", "label": 0}
{"function": "\n\n@blueprint.route('/remove', methods=['POST'])\n@api_wrapper\n@require_login\n@require_team\ndef team_remove_hook():\n    if (api.auth.is_logged_in() and api.user.in_team()):\n        uid = request.form.get('uid')\n        user = api.user.get_user()\n        if (uid == user['uid']):\n            confirm = request.form.get('confirm')\n            team = api.team.get_team()\n            if (confirm != team['teamname']):\n                raise WebException('Please confirm your name.')\n        message = api.team.remove(uid)\n        return {\n            'success': 1,\n            'message': message,\n        }\n    else:\n        raise WebException('Stop. Just stop.')\n", "label": 0}
{"function": "\n\ndef init_layout(self):\n    ' Initialize the layout for the menu bar.\\n\\n        '\n    super(QtMenuBar, self).init_layout()\n    widget = self.widget\n    for child in self.children():\n        if isinstance(child, QtMenu):\n            widget.addMenu(child.widget)\n", "label": 0}
{"function": "\n\ndef _plot_mean(self, canvas, helper_data, helper_prediction, levels=20, projection='2d', label=None, **kwargs):\n    (_, free_dims, Xgrid, x, y, _, _, resolution) = helper_data\n    if (len(free_dims) <= 2):\n        (mu, _, _) = helper_prediction\n        if (len(free_dims) == 1):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_1d)\n            plots = dict(gpmean=[pl().plot(canvas, Xgrid[:, free_dims], mu, label=label, **kwargs)])\n        elif (projection == '2d'):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_2d)\n            plots = dict(gpmean=[pl().contour(canvas, x[:, 0], y[0, :], mu.reshape(resolution, resolution).T, levels=levels, label=label, **kwargs)])\n        elif (projection == '3d'):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_3d)\n            plots = dict(gpmean=[pl().surface(canvas, x, y, mu.reshape(resolution, resolution), label=label, **kwargs)])\n    elif (len(free_dims) == 0):\n        pass\n    else:\n        raise RuntimeError('Cannot plot mean in more then 2 input dimensions')\n    return plots\n", "label": 0}
{"function": "\n\ndef clean_email(self):\n    value = self.cleaned_data['email']\n    if (UNIQUE_EMAIL or EMAIL_AUTHENTICATION):\n        try:\n            User.objects.get(email__iexact=value)\n        except User.DoesNotExist:\n            return value\n        raise forms.ValidationError(_('A user is registered with this e-mail address.'))\n    return value\n", "label": 0}
{"function": "\n\n@classmethod\ndef update_pd(cls, pd):\n    if (len(pd.data_units) > 0):\n        du_urls = [i.url for i in pd.data_units.values()]\n", "label": 0}
{"function": "\n\ndef missing_fetch_positions(self):\n    missing = set()\n    for (partition, state) in six.iteritems(self.assignment):\n        if (not state.has_valid_position):\n            missing.add(partition)\n    return missing\n", "label": 0}
{"function": "\n\ndef append(self, value):\n    'adds value to ring buffer'\n    idx = self.index = ((self.index + 1) % self.data.size)\n    self.data[idx] = max(0, (value - self.lastval))\n    self.lastval = value\n    dt = (time.time() - self.toffset)\n    if ((idx == 0) and (max(self.times) < 0)):\n        dt = 0\n    self.times[idx] = dt\n", "label": 0}
{"function": "\n\ndef get_contents_if_file(contents_or_file_name):\n    'Get the contents of a file.\\n\\n    If the value passed in is a file name or file URI, return the\\n    contents. If not, or there is an error reading the file contents,\\n    return the value passed in as the contents.\\n\\n    For example, a workflow definition will be returned if either the\\n    workflow definition file name, or file URI are passed in, or the\\n    actual workflow definition itself is passed in.\\n    '\n    try:\n        if parse.urlparse(contents_or_file_name).scheme:\n            definition_url = contents_or_file_name\n        else:\n            path = os.path.abspath(contents_or_file_name)\n            definition_url = parse.urljoin('file:', request.pathname2url(path))\n        return request.urlopen(definition_url).read().decode('utf8')\n    except Exception:\n        return contents_or_file_name\n", "label": 0}
{"function": "\n\ndef __init__(self, socket_name=None, socket_path=None, config_file=None, colors=None, **kwargs):\n    EnvironmentMixin.__init__(self, '-g')\n    self._windows = []\n    self._panes = []\n    if socket_name:\n        self.socket_name = socket_name\n    if socket_path:\n        self.socket_path = socket_path\n    if config_file:\n        self.config_file = config_file\n    if colors:\n        self.colors = colors\n", "label": 0}
{"function": "\n\ndef length_of_longest_string(list_of_strings):\n    if (len(list_of_strings) == 0):\n        return 0\n    result = 0\n    for string in list_of_strings:\n        length_of_string = len(string)\n        if (length_of_string > result):\n            result = length_of_string\n    return result\n", "label": 0}
{"function": "\n\ndef check_inline(self, cls, parent_model):\n    \" Validate inline class's fk field is not excluded. \"\n    fk = _get_foreign_key(parent_model, cls.model, fk_name=cls.fk_name, can_fail=True)\n    if (hasattr(cls, 'exclude') and cls.exclude):\n        if (fk and (fk.name in cls.exclude)):\n            raise ImproperlyConfigured((\"%s cannot exclude the field '%s' - this is the foreign key to the parent model %s.%s.\" % (cls.__name__, fk.name, parent_model._meta.app_label, parent_model.__name__)))\n", "label": 0}
{"function": "\n\ndef _get_kind(self, path):\n    obj = self.repository._repo[self._get_id_for_path(path)]\n    if isinstance(obj, objects.Blob):\n        return NodeKind.FILE\n    elif isinstance(obj, objects.Tree):\n        return NodeKind.DIR\n", "label": 0}
{"function": "\n\ndef reexecutable_tasks(self, task_filter):\n    'Keep only reexecutable tasks which match the filter.\\n\\n        Filter is the list of values. If task has reexecute_on key and its\\n        value matches the value from filter then task is not skipped.\\n        :param task_filter: filter (list)\\n        '\n    if (not task_filter):\n        return\n    task_filter = set(task_filter)\n    for task in six.itervalues(self.node):\n        reexecute_on = task.get('reexecute_on')\n        if ((reexecute_on is not None) and task_filter.issubset(reexecute_on)):\n            task['skipped'] = False\n        else:\n            self.make_skipped_task(task)\n", "label": 0}
{"function": "\n\ndef post(self, request):\n    email = request.POST.get('email')\n    api_key = request.POST.get('api_key')\n    if ((not email) or (not api_key)):\n        message = {\n            'success': False,\n            'errors': [],\n        }\n        if (not email):\n            message['errors'].append('Email is mandatory.')\n        if (not api_key):\n            message['errors'].append('API key is mandatory.')\n        return HttpResponseBadRequest(json.dumps(message))\n    member = self.get_member(email, api_key)\n    if (member is None):\n        return HttpResponseForbidden(json.dumps({\n            'success': False,\n            'errors': ['Bad credentials.'],\n        }))\n    return HttpResponse(json.dumps({\n        'success': True,\n        'output': member.get_storage_limit(),\n    }))\n", "label": 0}
{"function": "\n\ndef SetRepo(self, repo):\n    if (self.repo and (self.repo == self.mainRepo)):\n        self.mainRepo = self.repo\n        self.mainRepoSelection = [self.rows[row][0].commit.sha1 for row in self.selection]\n    repo_changed = (self.repo != repo)\n    if repo_changed:\n        self.selection = []\n        self.Scroll(0, 0)\n    if (not repo.parent):\n        self.mainRepo = repo\n    self.repo = repo\n    self.commits = self.repo.get_log(['--topo-order', '--all'])\n    self.CreateLogGraph()\n    if (repo_changed and (self.repo != self.mainRepo)):\n        for version in self.mainRepoSelection:\n            submodule_version = self.repo.parent.get_submodule_version(self.repo.name, version)\n            if submodule_version:\n                rows = [r for r in self.rows if (r[0].commit.sha1 == submodule_version)]\n                if rows:\n                    self.selection.append(self.rows.index(rows[0]))\n    self.SetVirtualSize(((- 1), ((len(self.rows) + 1) * LINH)))\n    self.SetScrollRate(LINH, LINH)\n    self.Refresh()\n", "label": 1}
{"function": "\n\n@property\ndef active_gen_id(self):\n    if self._active_gen_id:\n        return self._active_gen_id\n    active_nodes = InfrastructureNode.objects.filter(deployment_name=self.deployment_name, is_active_generation=1)\n    if (len(active_nodes) == 0):\n        return None\n    first_active_node = active_nodes[0]\n    gen_id = first_active_node.generation_id\n    for active_node in active_nodes:\n        if (active_node.generation_id != gen_id):\n            err_str = 'Inconsistent generation ids in simpledb. %s:%s and %s:%s both marked active'\n            context = (first_active_node.aws_id, first_active_node.generation_id, active_node.aws_id, active_node.generation_id)\n            raise Exception((err_str % context))\n    self._active_gen_id = gen_id\n    return self._active_gen_id\n", "label": 0}
{"function": "\n\ndef get_module_path(module_name):\n    try:\n        module = import_module(module_name)\n    except ImportError as e:\n        raise ImproperlyConfigured(('Error importing HIDE_IN_STACKTRACES: %s' % (e,)))\n    else:\n        source_path = inspect.getsourcefile(module)\n        if source_path.endswith('__init__.py'):\n            source_path = os.path.dirname(source_path)\n        return os.path.realpath(source_path)\n", "label": 0}
{"function": "\n\ndef _extend_network_dict_address_scope(self, network_res, network_db):\n    network_res[ext_address_scope.IPV4_ADDRESS_SCOPE] = None\n    network_res[ext_address_scope.IPV6_ADDRESS_SCOPE] = None\n    subnetpools = {subnet.subnetpool for subnet in network_db.subnets if subnet.subnetpool}\n    for subnetpool in subnetpools:\n        as_id = subnetpool[ext_address_scope.ADDRESS_SCOPE_ID]\n        if (subnetpool['ip_version'] == constants.IP_VERSION_4):\n            network_res[ext_address_scope.IPV4_ADDRESS_SCOPE] = as_id\n        if (subnetpool['ip_version'] == constants.IP_VERSION_6):\n            network_res[ext_address_scope.IPV6_ADDRESS_SCOPE] = as_id\n    return network_res\n", "label": 0}
{"function": "\n\n@property\ndef message(self):\n    esc = self.enhanced_status_code\n    msg = self._message\n    if (esc and msg):\n        return ' '.join((esc, msg))\n    else:\n        return msg\n", "label": 0}
{"function": "\n\n@dispatch(Expr, MongoQuery)\ndef post_compute(e, q, scope=None):\n    \"\\n    Execute a query using MongoDB's aggregation pipeline\\n\\n    The compute_up functions operate on Mongo Collection / list-of-dict\\n    queries.  Once they're done we need to actually execute the query on\\n    MongoDB.  We do this using the aggregation pipeline framework.\\n\\n    http://docs.mongodb.org/manual/core/aggregation-pipeline/\\n    \"\n    scope = {\n        '$project': toolz.merge({\n            '_id': 0,\n        }, dict(((col, 1) for col in e.fields))),\n    }\n    q = q.append(scope)\n    if (not e.dshape.shape):\n        result = get_result(q.coll.aggregate(list(q.query)))[0]\n        if isscalar(e.dshape.measure):\n            return result[e._name]\n        else:\n            return get(e.fields, result)\n    dicts = get_result(q.coll.aggregate(list(q.query)))\n    if isscalar(e.dshape.measure):\n        return list(pluck(e.fields[0], dicts, default=None))\n    else:\n        return list(pluck(e.fields, dicts, default=None))\n", "label": 0}
{"function": "\n\ndef test_deploy(self):\n    fake_tenants_list = [self.fake_tenant_0, self.fake_tenant_1]\n    fake_users_list = [self.fake_user_0, self.fake_user_1]\n    fake_roles_list = [self.fake_role_0, self.fake_role_1]\n    fake_info = self._get_fake_info(fake_tenants_list, fake_users_list, fake_roles_list)\n    self.mock_client().tenants.list.return_value = [fake_tenants_list[0]]\n    self.mock_client().users.list.return_value = [fake_users_list[0]]\n    self.mock_client().roles.list.return_value = [fake_roles_list[0]]\n    self.mock_client().roles.roles_for_user.return_value = [self.fake_role_1]\n\n    def tenant_create(**kwargs):\n        self.mock_client().tenants.list.return_value.append(fake_tenants_list[1])\n        return fake_tenants_list[1]\n\n    def user_create(**kwars):\n        self.mock_client().users.list.return_value.append(fake_users_list[1])\n        return fake_users_list[1]\n\n    def roles_create(role_name):\n        self.mock_client().roles.list.return_value.append(fake_roles_list[1])\n        return fake_roles_list[1]\n    self.mock_client().tenants.create = tenant_create\n    self.mock_client().users.create = user_create\n    self.mock_client().roles.create = roles_create\n    self.keystone_client.deploy(fake_info)\n    mock_calls = []\n    for user in fake_users_list:\n        for tenant in fake_tenants_list:\n            mock_calls.append(mock.call(user.id, fake_roles_list[0].id, tenant.id))\n    self.assertEquals(mock_calls, self.mock_client().roles.add_user_role.mock_calls)\n", "label": 0}
{"function": "\n\ndef showStack(self, index):\n    if ((index >= self.getWidgetCount()) or (index == self.visibleStack)):\n        return\n    if (self.visibleStack >= 0):\n        self.setStackVisible(self.visibleStack, False)\n    self.visibleStack = index\n    self.setStackVisible(self.visibleStack, True)\n    for listener in self.stackListeners:\n        listener.onStackChanged(self, index)\n", "label": 0}
{"function": "\n\ndef list_users(order_by='id'):\n    '\\n    Show all users for this company.\\n\\n    CLI Example:\\n\\n        salt myminion bamboohr.list_users\\n\\n    By default, the return data will be keyed by ID. However, it can be ordered\\n    by any other field. Keep in mind that if the field that is chosen contains\\n    duplicate values (i.e., location is used, for a company which only has one\\n    location), then each duplicate value will be overwritten by the previous.\\n    Therefore, it is advisable to only sort by fields that are guaranteed to be\\n    unique.\\n\\n    CLI Examples:\\n\\n        salt myminion bamboohr.list_users order_by=id\\n        salt myminion bamboohr.list_users order_by=email\\n    '\n    ret = {\n        \n    }\n    (status, result) = _query(action='meta', command='users')\n    root = ET.fromstring(result)\n    users = root.getchildren()\n    for user in users:\n        user_id = None\n        user_ret = {\n            \n        }\n        for item in user.items():\n            user_ret[item[0]] = item[1]\n            if (item[0] == 'id'):\n                user_id = item[1]\n        for item in user.getchildren():\n            user_ret[item.tag] = item.text\n        ret[user_ret[order_by]] = user_ret\n    return ret\n", "label": 0}
{"function": "\n\ndef set_display(self, locale_id=None, media_image=None, media_audio=None):\n    text = (Text(locale_id=locale_id) if locale_id else None)\n    if (media_image or media_audio):\n        self.display = Display(text=text, media_image=media_image, media_audio=media_audio)\n    elif text:\n        self.text = text\n", "label": 0}
{"function": "\n\ndef test_many_to_one(self, session, objects, calls):\n    users = session.query(models.User).all()\n    users[0].addresses\n    assert (len(calls) == 1)\n    call = calls[0]\n    assert (call.objects == (models.User, 'User:1', 'addresses'))\n    assert ('users[0].addresses' in ''.join(call.frame[4]))\n", "label": 0}
{"function": "\n\ndef downloadAnimea(self, manga, chapter_start, chapter_end, download_path, download_format):\n    for current_chapter in range(chapter_start, (chapter_end + 1)):\n        manga_chapter_prefix = ((manga.lower().replace('-', '_') + '_') + str(current_chapter).zfill(3))\n        if ((os.path.exists(((download_path + manga_chapter_prefix) + '.cbz')) or os.path.exists(((download_path + manga_chapter_prefix) + '.zip'))) and (overwrite_FLAG == False)):\n            print((('Chapter ' + str(current_chapter)) + ' already downloaded, skipping to next chapter...'))\n            continue\n        url = (((('http://manga.animea.net/' + manga) + '-chapter-') + str(current_chapter)) + '-page-1.html')\n        source = getSourceCode(url)\n        max_pages = int(re.compile('of (.*?)</title>').search(source).group(1))\n        for page in range(1, (max_pages + 1)):\n            url = (((((('http://manga.animea.net/' + manga) + '-chapter-') + str(current_chapter)) + '-page-') + str(page)) + '.html')\n            source = getSourceCode(url)\n            img_url = re.compile('img src=\"(http.*?.[jp][pn]g)\"').search(source).group(1)\n            print((((('Chapter ' + str(current_chapter)) + ' / ') + 'Page ') + str(page)))\n            print(img_url)\n            downloadImage(img_url, os.path.join('mangadl_tmp', ((manga_chapter_prefix + '_') + str(page).zfill(3))))\n        compress(manga_chapter_prefix, download_path, max_pages, download_format)\n", "label": 0}
{"function": "\n\ndef __init__(self, noop=False):\n    if noop:\n        self.func_name = 'posix_fallocate'\n        self.fallocate = noop_libc_function\n        return\n    for func in ('fallocate', 'posix_fallocate'):\n        self.func_name = func\n        self.fallocate = load_libc_function(func, log_error=False)\n        if (self.fallocate is not noop_libc_function):\n            break\n    if (self.fallocate is noop_libc_function):\n        logging.warning(_('Unable to locate fallocate, posix_fallocate in libc.  Leaving as a no-op.'))\n", "label": 0}
{"function": "\n\ndef setup_logger(logger, stream, filename=None, fmt=None):\n    \"Sets up a logger (if no handlers exist) for console output,\\n    and file 'tee' output if desired.\"\n    if (len(logger.handlers) < 1):\n        console = logging.StreamHandler(stream)\n        console.setLevel(logging.DEBUG)\n        console.setFormatter(logging.Formatter(fmt))\n        logger.addHandler(console)\n        logger.setLevel(logging.DEBUG)\n        if filename:\n            outfile = logging.FileHandler(filename)\n            outfile.setLevel(logging.INFO)\n            outfile.setFormatter(logging.Formatter(('%(asctime)s ' + (fmt if fmt else '%(message)s'))))\n            logger.addHandler(outfile)\n", "label": 0}
{"function": "\n\ndef get_value(self, select, table_name, where=None, extra=None):\n    '\\n        Get a value from the table.\\n\\n        :param str select: Attribute for SELECT query\\n        :param str table_name: Table name of executing the query.\\n        :return: Result of execution of the query.\\n        :raises simplesqlite.NullDatabaseConnectionError:\\n            |raises_check_connection|\\n        :raises sqlite3.OperationalError: |raises_operational_error|\\n\\n        .. seealso::\\n\\n            :py:meth:`.sqlquery.SqlQuery.make_select`\\n        '\n    self.verify_table_existence(table_name)\n    query = SqlQuery.make_select(select, table_name, where, extra)\n    result = self.execute_query(query, logging.getLogger().findCaller())\n    if (result is None):\n        return None\n    fetch = result.fetchone()\n    if (fetch is None):\n        return None\n    return fetch[0]\n", "label": 0}
{"function": "\n\ndef apply(self):\n    start_tags = []\n    for node in self.document.traverse(docutils.nodes.raw):\n        if (node['format'] != 'html'):\n            continue\n        start_match = self._start_re.match(node.astext())\n        if (not start_match):\n            continue\n        class_match = self._class_re.match(start_match.group(2))\n        if (not class_match):\n            continue\n        admonition_class = class_match.group(1)\n        if (admonition_class == 'info'):\n            admonition_class = 'note'\n        start_tags.append((node, admonition_class))\n    for (node, admonition_class) in reversed(start_tags):\n        content = []\n        for sibling in node.traverse(include_self=False, descend=False, siblings=True, ascend=False):\n            end_tag = (isinstance(sibling, docutils.nodes.raw) and (sibling['format'] == 'html') and self._end_re.match(sibling.astext()))\n            if end_tag:\n                admonition_node = AdmonitionNode(classes=['admonition', admonition_class])\n                admonition_node.extend(content)\n                parent = node.parent\n                parent.replace(node, admonition_node)\n                for n in content:\n                    parent.remove(n)\n                parent.remove(sibling)\n                break\n            else:\n                content.append(sibling)\n", "label": 1}
{"function": "\n\n@register.tag\ndef shardtype(parser, token):\n    try:\n        (tag_name, shard_type) = token.split_contents()\n    except ValueError:\n        raise template.TemplateSyntaxError(('%r tag requires a single argument' % token.contents.split()[0]))\n    if (not ((shard_type[0] == shard_type[(- 1)]) and (shard_type[0] in ('\"', \"'\")))):\n        raise template.TemplateSyntaxError((\"%r tag's argument should be in quotes\" % tag_name))\n    shard_type = shard_type[1:(- 1)]\n    nodelist = parser.parse(('end{0}'.format(tag_name),))\n    parser.delete_first_token()\n    return EmailShardTypeNode(shard_type, nodelist)\n", "label": 0}
{"function": "\n\n@property\ndef photos(self):\n    if (not hasattr(self, '_photos')):\n        available_photos = glob.glob(os.path.join(self.directory, '*.jpg'))\n        self._photos = []\n        for photo_path in available_photos:\n            (_pdir, photo) = os.path.split(photo_path)\n            if (self.xml.find(photo) > 0):\n                self._photos.append(photo_path)\n    return self._photos\n", "label": 0}
{"function": "\n\ndef glob_staticfiles(self, item):\n    for finder in finders.get_finders():\n        if hasattr(finder, 'storages'):\n            storages = finder.storages.values()\n        elif hasattr(finder, 'storage'):\n            storages = [finder.storage]\n        else:\n            continue\n        for storage in storages:\n            globber = StorageGlobber(storage)\n            for file in globber.glob(item):\n                (yield storage.path(file))\n", "label": 0}
{"function": "\n\n@test.create_stubs({\n    api.nova: ('aggregate_get', 'add_host_to_aggregate', 'remove_host_from_aggregate', 'host_list'),\n})\ndef _test_manage_hosts_update(self, host, aggregate, form_data, addAggregate=False, cleanAggregates=False):\n    if cleanAggregates:\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host3').InAnyOrder()\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host2').InAnyOrder()\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host1').InAnyOrder()\n    api.nova.aggregate_get(IsA(http.HttpRequest), str(aggregate.id)).AndReturn(aggregate)\n    api.nova.host_list(IsA(http.HttpRequest)).AndReturn(self.hosts.list())\n    api.nova.aggregate_get(IsA(http.HttpRequest), str(aggregate.id)).AndReturn(aggregate)\n    if addAggregate:\n        api.nova.add_host_to_aggregate(IsA(http.HttpRequest), str(aggregate.id), host.host_name)\n    self.mox.ReplayAll()\n    res = self.client.post(reverse(constants.AGGREGATES_MANAGE_HOSTS_URL, args=[aggregate.id]), form_data)\n    self.assertNoFormErrors(res)\n    self.assertRedirectsNoFollow(res, reverse(constants.AGGREGATES_INDEX_URL))\n", "label": 0}
{"function": "\n\ndef _uncached_match(self, text, pos, cache, error):\n    new_pos = pos\n    children = []\n    while True:\n        node = self.members[0].match_core(text, new_pos, cache, error)\n        if ((node is None) or (not (node.end - node.start))):\n            return Node(self.name, text, pos, new_pos, children)\n        children.append(node)\n        new_pos += (node.end - node.start)\n", "label": 0}
{"function": "\n\n@authorization_required(is_admin=True)\n@threaded\ndef put(self, uid):\n    try:\n        user = Users.get(id=uid)\n    except DoesNotExist:\n        raise HTTPError(404)\n    try:\n        user.login = self.json.get('login', user.login)\n        user.email = self.json.get('email', user.email)\n        user.is_admin = bool(self.json.get('is_admin', user.is_admin))\n        user.password = self.json.get('password', user.password)\n        if (not all((isinstance(user.login, text_type), isinstance(user.email, text_type), (LOGIN_EXP.match(str(user.login)) is not None), (user.password and (len(user.password) > 3)), (EMAIL_EXP.match(str(user.email)) is not None)))):\n            raise HTTPError(400)\n    except:\n        raise HTTPError(400)\n    user.save()\n    self.response({\n        'id': user.id,\n        'login': user.login,\n        'email': user.email,\n        'is_admin': user.is_admin,\n    })\n", "label": 0}
{"function": "\n\n@pytest.fixture(scope='module')\ndef smtp_servers(request):\n    try:\n        from .local_smtp_severs import SERVERS\n    except ImportError:\n        from .smtp_servers import SERVERS\n    return dict([(k, SMTPTestParams(**v)) for (k, v) in SERVERS.items()])\n", "label": 0}
{"function": "\n\ndef autoassign(self, locals):\n    '\\n    Automatically assigns local variables to `self`.\\n    Generally used in `__init__` methods, as in::\\n\\n        def __init__(self, foo, bar, baz=1): \\n            autoassign(self, locals())\\n\\n    '\n    for (key, value) in locals.iteritems():\n        if (key == 'self'):\n            continue\n        setattr(self, key, value)\n", "label": 0}
{"function": "\n\ndef _version_str_to_list(version):\n    'convert a version string to a list of ints\\n\\n    non-int segments are excluded\\n    '\n    v = []\n    for part in version.split('.'):\n        try:\n            v.append(int(part))\n        except ValueError:\n            pass\n    return v\n", "label": 0}
{"function": "\n\ndef parse_args(self, sys_argv):\n    'Parsing, public for unit testing.  sys_argv should be sys.argv.\\nThe first entry is skipped, to account for sys.argv putting the\\nprogram name as the first element.\\n        '\n    if (len(sys_argv) == 0):\n        raise Driver.ParsingException('sys_argv must contain at least one element (name of the executing program, per sys.argv[0])')\n    parser = argparse.ArgumentParser(description='Migrate one or more databases (or default database, if one is assigned).')\n    parser.add_argument('-n', '--new', help='Create new (empty) database(s)', action='store_true')\n    parser.add_argument('-s', '--schema', help='Run baseline schema(e)', action='store_true')\n    parser.add_argument('-m', '--migrations', help='Run migrations', action='store_true')\n    parser.add_argument('-c', '--code', help='Run code (for views, stored procs, etc)', action='store_true')\n    parser.add_argument('-d', '--data', help='Load reference (bootstrap) data', action='store_true')\n    parser.add_argument('-u', '--update', help='Updates database (runs migrations, code, and data)', action='store_true')\n    parser.add_argument('databases', metavar='db', nargs='*', help='nickname of database to manipulate')\n    args = parser.parse_args(sys_argv[1:])\n    if ((len(args.databases) == 0) and (self.default_database != '')):\n        args.databases.append(self.default_database)\n    return args\n", "label": 0}
{"function": "\n\ndef test_read_job2():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test.test', 'test1'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\n@classmethod\ndef award_points(cls, user, addon, status, **kwargs):\n    'Awards points to user based on an event and the queue.\\n\\n        `event` is one of the `REVIEWED_` keys in constants.\\n        `status` is one of the `STATUS_` keys in constants.\\n\\n        '\n    event = cls.get_event(addon, status, **kwargs)\n    score = amo.REVIEWED_SCORES.get(event)\n    try:\n        vq = ViewQueue.objects.get(addon_slug=addon.slug)\n        if (vq.waiting_time_days > amo.REVIEWED_OVERDUE_LIMIT):\n            days_over = (vq.waiting_time_days - amo.REVIEWED_OVERDUE_LIMIT)\n            bonus = (days_over * amo.REVIEWED_OVERDUE_BONUS)\n            score = (score + bonus)\n    except ViewQueue.DoesNotExist:\n        pass\n    if score:\n        cls.objects.create(user=user, addon=addon, score=score, note_key=event)\n        cls.get_key(invalidate=True)\n        user_log.info(('Awarding %s points to user %s for \"%s\" for addon %s' % (score, user, amo.REVIEWED_CHOICES[event], addon.id)).encode('utf-8'))\n    return score\n", "label": 0}
{"function": "\n\ndef computeExpression(self, constraint_collection):\n    function = self.getFunction()\n    values = self.getArgumentValues()\n    cost = function.getCallCost(values)\n    if function.getFunctionRef().getFunctionBody().mayRaiseException(BaseException):\n        constraint_collection.onExceptionRaiseExit(BaseException)\n    if ((cost is not None) and (cost < 50)):\n        result = function.createOutlineFromCall(provider=self.getParentVariableProvider(), values=values)\n        return (result, 'new_statements', 'Function call in-lined.')\n    return (self, None, None)\n", "label": 0}
{"function": "\n\ndef test_DELETE(self):\n    res = self.app.delete('/crud.json?ref.id=1', expect_errors=False)\n    print('Received:', res.body)\n    result = json.loads(res.text)\n    print(result)\n    assert (result['data']['id'] == 1)\n    assert (result['data']['name'] == u('test'))\n    assert (result['message'] == 'delete')\n", "label": 0}
{"function": "\n\ndef listPropsAndMethods(self):\n    res = []\n    if (sublime.platform() == 'windows'):\n        app = 'node'\n        pathToJS = (sublime.packages_path() + '\\\\NPMInfo\\\\npm-info.js')\n        cmd = [app, pathToJS, self.pkgPath]\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, startupinfo=startupinfo)\n    else:\n        pathToJS = (sublime.packages_path() + '/NPMInfo/npm-info.js')\n        cmd = ['/usr/local/bin/node', pathToJS, self.pkgPath]\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    for line in iter(p.stdout.readline, b''):\n        lineStrip = line.rstrip()\n        if ((len(lineStrip) > 0) and (' ' not in lineStrip)):\n            res.append(lineStrip)\n    self.view.window().show_quick_panel(res, self.onSelectPropAndMethod)\n", "label": 0}
{"function": "\n\ndef skip_past(self, endtag):\n    while self.tokens:\n        token = self.next_token()\n        if ((token.token_type == TOKEN_BLOCK) and (token.contents == endtag)):\n            return\n    self.unclosed_block_tag([endtag])\n", "label": 0}
{"function": "\n\n@AccountRequired\n@PostOnly\ndef follow(request, user_id):\n    followed = get_object_or_404(User, pk=user_id)\n    (follow_instance, created) = UserToUserFollow.objects.get_or_create(follower=request.user, followed=followed)\n    if (not follow_instance.is_following):\n        follow_instance.is_following = True\n        follow_instance.save()\n    if created:\n        send_notification(type=EmailTypes.FOLLOW, user=followed, entity=request.user)\n    cache.bust(followed)\n    if request.is_ajax():\n        button = render_inclusiontag(request, 'follow_button followed', 'users_tags', {\n            'followed': followed,\n        })\n        return json_response({\n            'button': button,\n        })\n    else:\n        return redirect(followed)\n", "label": 0}
{"function": "\n\ndef __unicode__(self):\n    qs = []\n    ctx_id = self.context_id\n    if self.is_assignment:\n        qs += ['\"{0}\" = %({1})s'.format(self.field, ctx_id)]\n    else:\n        for _ in (self._updates or []):\n            qs += ['\"{0}\"[%({1})s] = %({2})s'.format(self.field, ctx_id, (ctx_id + 1))]\n            ctx_id += 2\n    return ', '.join(qs)\n", "label": 0}
{"function": "\n\ndef valid(self, order=None):\n    '\\n        Can do complex validation about whether or not this option is valid.\\n        For example, may check to see if the recipient is in an allowed country\\n        or location.\\n        '\n    if order:\n        try:\n            for item in order.orderitem_set.all():\n                p = item.product\n                price = self.carrier.price(p)\n        except ProductShippingPriceException:\n            return False\n    elif self.cart:\n        try:\n            price = self.cost()\n        except ProductShippingPriceException:\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef test_compute_centroid_quantile(self, empty_tdigest, example_centroids):\n    empty_tdigest.C = example_centroids\n    empty_tdigest.n = 4\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 1.1)]) == (((1 / 2.0) + 0) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 0.5)]) == (((1 / 2.0) + 1) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[0.1]) == (((1 / 2.0) + 2) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[1.5]) == (((1 / 2.0) + 3) / 4))\n", "label": 0}
{"function": "\n\ndef cycle(self):\n    '\\n        Perform full thermostat cycle and return state.\\n        '\n    units = cherrypy.config['units']\n    retry_count = cherrypy.config['retry_count']\n    retry_delay = cherrypy.config['retry_delay']\n    envcontroller = cherrypy.config['envcontroller']\n    thermometer = cherrypy.config['thermometer']\n    thermostat = cherrypy.config['thermostat']\n    (current_heat, current_cool) = envcontroller.get_power_levels()\n    for i in range(retry_count):\n        try:\n            current_temp = thermometer.get_temperature(units=units)\n            break\n        except braubuddy.thermometer.ReadError as err:\n            cherrypy.log.error(err.message)\n            time.sleep(retry_delay)\n    else:\n        cherrypy.request.app.log.error('Unable to collect temperature after {0} tries'.format(retry_count))\n        return False\n    (required_heat, required_cool) = thermostat.get_required_state(current_temp, current_heat, current_cool, units=units)\n    envcontroller.set_heater_level(required_heat)\n    envcontroller.set_cooler_level(required_cool)\n    target = thermostat.target\n    for (name, output) in cherrypy.request.app.config['outputs'].iteritems():\n        try:\n            output.publish_status(target, current_temp, current_heat, current_cool)\n        except braubuddy.output.OutputError as err:\n            cherrypy.log.error(err.message)\n    return True\n", "label": 0}
{"function": "\n\ndef www_authenticate(realm, key, algorithm='MD5', nonce=None, qop=qop_auth, stale=False):\n    'Constructs a WWW-Authenticate header for Digest authentication.'\n    if (qop not in valid_qops):\n        raise ValueError((\"Unsupported value for qop: '%s'\" % qop))\n    if (algorithm not in valid_algorithms):\n        raise ValueError((\"Unsupported value for algorithm: '%s'\" % algorithm))\n    if (nonce is None):\n        nonce = synthesize_nonce(realm, key)\n    s = ('Digest realm=\"%s\", nonce=\"%s\", algorithm=\"%s\", qop=\"%s\"' % (realm, nonce, algorithm, qop))\n    if stale:\n        s += ', stale=\"true\"'\n    return s\n", "label": 0}
{"function": "\n\n@classmethod\ndef _is_socket(cls, stream):\n    'Check if the given stream is a socket.'\n    try:\n        fd = stream.fileno()\n    except ValueError:\n        return False\n    sock = socket.fromfd(fd, socket.AF_INET, socket.SOCK_RAW)\n    try:\n        sock.getsockopt(socket.SOL_SOCKET, socket.SO_TYPE)\n    except socket.error as ex:\n        if (ex.args[0] != errno.ENOTSOCK):\n            return True\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef get_user(self, identifier):\n    if self._is_numeric(identifier):\n        return self.user_model.query.get(identifier)\n    for attr in get_identity_attributes():\n        query = getattr(self.user_model, attr).ilike(identifier)\n        rv = self.user_model.query.filter(query).first()\n        if (rv is not None):\n            return rv\n", "label": 0}
{"function": "\n\ndef _get_external_player(self):\n    ' Determines external sound player to available.\\n\\n            Returns string or ``None``.\\n            '\n    if common.IS_MACOSX:\n        return 'afplay'\n    else:\n        for name in ('mpg123', 'play', 'aplay'):\n            if common.which(name):\n                return name\n    return None\n", "label": 0}
{"function": "\n\ndef checkRemainingArguments(self, arguments):\n    allowedAttributes = {\n        \n    }\n    allowedAttributes['allow multiple values'] = (bool, False, True, 'allowMultipleValues')\n    allowedAttributes['command line argument'] = (str, True, True, 'commandLineArgument')\n    allowedAttributes['construct filename'] = (dict, False, True, 'constructionInstructions')\n    allowedAttributes['data type'] = (str, True, True, 'dataType')\n    allowedAttributes['description'] = (str, True, True, 'description')\n    allowedAttributes['extensions'] = (list, False, True, 'extensions')\n    allowedAttributes['hide argument in help'] = (bool, False, True, 'hideInHelp')\n    allowedAttributes['include in reduced plot'] = (bool, False, True, 'includeInReducedPlot')\n    allowedAttributes['include value in quotations'] = (bool, False, True, 'includeInQuotations')\n    allowedAttributes['long form argument'] = (str, True, True, 'longFormArgument')\n    allowedAttributes['modify argument'] = (str, False, True, 'modifyArgument')\n    allowedAttributes['modify value'] = (str, False, True, 'modifyValue')\n    allowedAttributes['replace substring'] = (list, False, False, None)\n    allowedAttributes['required'] = (bool, False, True, 'isRequired')\n    allowedAttributes['short form argument'] = (str, False, True, 'shortFormArgument')\n    allowedAttributes['value command'] = (dict, False, True, 'valueCommand')\n    for category in arguments:\n        if ((category != 'Inputs') and (category != 'Outputs')):\n            self.checkArguments(category, arguments[category], allowedAttributes, isInput=False, isOutput=False)\n", "label": 0}
{"function": "\n\ndef format_text_table(table):\n    col_width = [max((len(str(x)) for x in col)) for col in zip(*table)]\n    output = []\n    for row in table:\n        inner = ' | '.join(('{0:{1}}'.format(x, col_width[i]) for (i, x) in enumerate(row)))\n        output.append('| {0} |'.format(inner))\n    return output\n", "label": 0}
{"function": "\n\ndef test_neg_list_trim_policy_is_string(self):\n    '\\n        Invoke list_trim() with policy is string\\n        '\n    key = ('test', 'demo', 1)\n    try:\n        self.as_connection.list_trim(key, 'contact_no', 0, 1, {\n            \n        }, '')\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'policy must be a dict')\n", "label": 0}
{"function": "\n\n@csrf_protect_m\n@filter_hook\ndef get(self, request, *args, **kwargs):\n    \"\\n        The 'change list' admin view for this model.\\n        \"\n    response = self.get_result_list()\n    if response:\n        return response\n    context = self.get_context()\n    context.update((kwargs or {\n        \n    }))\n    response = self.get_response(context, *args, **kwargs)\n    return (response or TemplateResponse(request, (self.object_list_template or self.get_template_list('views/model_list.html')), context))\n", "label": 0}
{"function": "\n\ndef __init__(self, database, name, is_admin=None, read_from=None, write_to=None):\n    self.__database = database\n    self.__client = database.client\n    self.__name = name\n    self.__is_admin = (is_admin or self.IS_ADMIN)\n    self.__read_from = (read_from or self.READ_FROM)\n    self.__write_to = (write_to or self.WRITE_TO)\n", "label": 0}
{"function": "\n\ndef _on_gstplayer_message(mtype, message):\n    if (mtype == 'error'):\n        Logger.error('VideoGstplayer: {}'.format(message))\n    elif (mtype == 'warning'):\n        Logger.warning('VideoGstplayer: {}'.format(message))\n    elif (mtype == 'info'):\n        Logger.info('VideoGstplayer: {}'.format(message))\n", "label": 0}
{"function": "\n\ndef get_mapping(self, cls, no_mapping_ok=False):\n    db = None\n    for candidate_cls in getmro(cls):\n        db = self.mapping.get(candidate_cls, None)\n        if (db is not None):\n            break\n    if (db is None):\n        db = self.default_database\n    if (db is None):\n        if no_mapping_ok:\n            return None\n        raise ValueError(('There is no database mapping for %s' % repr(cls)))\n    return db\n", "label": 0}
{"function": "\n\ndef _do_login(backend, user, social_user):\n    user.backend = '{0}.{1}'.format(backend.__module__, backend.__class__.__name__)\n    login(backend.strategy.request, user)\n    if backend.setting('SESSION_EXPIRATION', False):\n        expiration = social_user.expiration_datetime()\n        if expiration:\n            try:\n                backend.strategy.request.session.set_expiry((expiration.seconds + (expiration.days * 86400)))\n            except OverflowError:\n                backend.strategy.request.session.set_expiry(None)\n", "label": 0}
{"function": "\n\ndef __init__(self, game_list, game_path=None, slug=None, games_root=None, deploy_enable=False, manifest_name=None):\n    self.game_list = game_list\n    self.slug = None\n    self.title = None\n    self.path = None\n    self.cover_art = ImageDetail(self, 'cover_art.jpg')\n    self.title_logo = ImageDetail(self, 'title_logo.jpg')\n    self.modified = None\n    self.deployed = None\n    self.is_temporary = True\n    self.plugin_main = None\n    self.canvas_main = None\n    self.flash_main = None\n    self.mapping_table = None\n    self.deploy_files = None\n    self.has_mapping_table = None\n    self.engine_version = EngineDetail('')\n    self.is_multiplayer = False\n    self.aspect_ratio = AspectRatioDetail('')\n    if (manifest_name is None):\n        self.manifest_name = 'manifest.yaml'\n    else:\n        self.manifest_name = manifest_name\n    if (game_path is not None):\n        self.load(game_path, self.manifest_name)\n    elif (slug is not None):\n        self.update({\n            'slug': slug,\n        })\n    self.games_root = games_root\n    self.deploy_enable = deploy_enable\n", "label": 0}
{"function": "\n\ndef get_localhost_ip():\n    cmd = ['/sbin/ifconfig']\n    eth_ip = {\n        \n    }\n    try:\n        r = exec_command(cmd)\n    except:\n        current_app.logger.error('[Dial Helpers]: exec_command error: %s:%s', cmd, sys.exc_info()[1])\n        return False\n    if (r['return_code'] == 0):\n        r_data = r['stdout'].split('\\n')\n        for (index, line) in enumerate(r_data):\n            if line.startswith('inet addr:'):\n                eth_ip[r_data[(index - 1)].split()[0]] = line.split().split(':')[1]\n    else:\n        current_app.logger.error('[Dial Helpers]: exec_command return: %s:%s:%s', cmd, r['return_code'], r['stderr'])\n        return False\n    return eth_ip\n", "label": 0}
{"function": "\n\ndef windows_shell(chan):\n    import threading\n    stdout.write('*** Emulating terminal on Windows; press F6 or Ctrl+Z then enter to send EOF,\\r\\nor at the end of the execution.\\r\\n')\n    stdout.flush()\n    out_lock = threading.RLock()\n\n    def write(recv, std):\n        while True:\n            data = recv(256)\n            if (not data):\n                if std:\n                    with out_lock:\n                        stdout.write('\\r\\n*** EOF reached; (press F6 or ^Z then enter to end)\\r\\n')\n                        stdout.flush()\n                break\n            stream = [stderr_bytes, stdout_bytes][std]\n            with out_lock:\n                stream.write(data)\n                stream.flush()\n    threading.Thread(target=write, args=(chan.recv, True)).start()\n    threading.Thread(target=write, args=(chan.recv_stderr, False)).start()\n    try:\n        while True:\n            d = stdin_bytes.read(1)\n            if (not d):\n                chan.shutdown_write()\n                break\n            try:\n                chan.send(d)\n            except socket.error:\n                break\n    except EOFError:\n        pass\n", "label": 0}
{"function": "\n\ndef test_push_selects_groupby(self):\n    'Test pushing selections through groupby.'\n    lp = StoreTemp('OUTPUT', Select(expression.LTEQ(AttRef('c'), AttRef('a')), Select(expression.LTEQ(AttRef('b'), AttRef('c')), GroupBy([AttIndex(1), AttIndex(2), AttIndex(0)], [expression.COUNTALL()], Scan(self.x_key, self.x_scheme)))))\n    expected = collections.Counter([(b, c, a) for (a, b, c) in self.x_data if ((c <= a) and (b <= c))])\n    expected = collections.Counter(((k + (v,)) for (k, v) in expected.items()))\n    self.assertEquals(self.get_count(lp, Select), 2)\n    self.assertEquals(self.get_count(lp, Scan), 1)\n    self.assertIsInstance(lp.input, Select)\n    pp = self.logical_to_physical(lp)\n    self.assertIsInstance(pp.input, MyriaSplitConsumer)\n    self.assertIsInstance(pp.input.input.input, GroupBy)\n    self.assertEquals(self.get_count(pp, Select), 1)\n    self.db.evaluate(pp)\n    result = self.db.get_temp_table('OUTPUT')\n    self.assertEquals(result, expected)\n", "label": 0}
{"function": "\n\ndef makeRadials(self, numRadials):\n    ' make geodesic radials from number of radials '\n    segmentAngle = (360.0 / float(numRadials))\n    segmentAngleList = []\n    a = 0.0\n    while (a < 360.0):\n        segmentAngleList.append(a)\n        a += segmentAngle\n    fields = {\n        'x': 'DOUBLE',\n        'y': 'DOUBLE',\n        'Bearing': 'DOUBLE',\n        'Range': 'DOUBLE',\n    }\n    tab = self._makeTempTable('radTable', fields)\n    cursor = arcpy.da.InsertCursor(tab, ['x', 'y', 'Bearing', 'Range'])\n    for i in self.center:\n        pt = i.firstPoint\n        for r in segmentAngleList:\n            cursor.insertRow([pt.X, pt.Y, r, self.ringMax])\n    del cursor\n    self.deleteme.append(tab)\n    outRadialFeatures = os.path.join('in_memory', 'outRadials')\n    arcpy.BearingDistanceToLine_management(tab, outRadialFeatures, 'x', 'y', 'Range', self.distanceUnits, 'Bearing', 'DEGREES', 'GEODESIC', '#', self.sr)\n    self.deleteme.append(outRadialFeatures)\n    self.radialFeatures = outRadialFeatures\n    return outRadialFeatures\n", "label": 0}
{"function": "\n\ndef indication(self, server, pdu):\n    if _debug:\n        UDPMultiplexer._debug('indication %r %r', server, pdu)\n    if (pdu.pduDestination.addrType == Address.localBroadcastAddr):\n        dest = self.addrBroadcastTuple\n        if _debug:\n            UDPMultiplexer._debug('    - requesting local broadcast: %r', dest)\n    elif (pdu.pduDestination.addrType == Address.localStationAddr):\n        dest = unpack_ip_addr(pdu.pduDestination.addrAddr)\n        if _debug:\n            UDPMultiplexer._debug('    - requesting local station: %r', dest)\n    else:\n        raise RuntimeError('invalid destination address type')\n    self.directPort.indication(PDU(pdu, destination=dest))\n", "label": 0}
{"function": "\n\ndef visit_Assign(self, node):\n    ' Visits an assignment node. '\n    assign = self._assign_factory.from_ast(self.klass, node)\n    if assign.source:\n        assign.is_trait = self.klass.is_trait(assign.source)\n    else:\n        assign.is_trait = False\n    for target in assign.targets:\n        self.klass.locals[target] = assign\n        self.klass._is_trait[target] = assign.is_trait\n        if assign.is_trait:\n            self.klass.traits[target] = assign\n        else:\n            self.klass.attributes[target] = assign\n    return\n", "label": 0}
{"function": "\n\ndef _choose(old_style, new_style):\n    family = distrib_family()\n    if (family == 'debian'):\n        distrib = distrib_id()\n        at_least_trusty = ((distrib == 'Ubuntu') and (V(distrib_release()) >= V('14.04')))\n        at_least_jessie = ((distrib == 'Debian') and (V(distrib_release()) >= V('8.0')))\n        if (at_least_trusty or at_least_jessie):\n            return new_style\n        else:\n            return old_style\n    else:\n        raise UnsupportedFamily(supported=['debian'])\n", "label": 0}
{"function": "\n\ndef startTag(self, namespace, name, attrs):\n    assert ((namespace is None) or isinstance(namespace, string_types)), type(namespace)\n    assert isinstance(name, string_types), type(name)\n    assert all(((((namespace is None) or isinstance(namespace, string_types)) and isinstance(name, string_types) and isinstance(value, string_types)) for ((namespace, name), value) in attrs.items()))\n    return {\n        'type': 'StartTag',\n        'name': text_type(name),\n        'namespace': to_text(namespace),\n        'data': dict((((to_text(namespace, False), to_text(name)), to_text(value, False)) for ((namespace, name), value) in attrs.items())),\n    }\n", "label": 0}
{"function": "\n\n@lower_cast(types.BaseTuple, types.BaseTuple)\ndef tuple_to_tuple(context, builder, fromty, toty, val):\n    if (isinstance(fromty, types.BaseNamedTuple) or isinstance(toty, types.BaseNamedTuple)):\n        raise NotImplementedError\n    if (len(fromty) != len(toty)):\n        raise NotImplementedError\n    olditems = cgutils.unpack_tuple(builder, val, len(fromty))\n    items = [context.cast(builder, v, f, t) for (v, f, t) in zip(olditems, fromty, toty)]\n    return context.make_tuple(builder, toty, items)\n", "label": 0}
{"function": "\n\ndef get(self, name, default=None):\n    '\\n            Get a Python-attribute of this item instance, falls back\\n            to the same attribute in the parent item if not set in\\n            this instance, used to inherit attributes to components\\n\\n            @param name: the attribute name\\n        '\n    if (name in self.__dict__):\n        value = self.__dict__[name]\n    else:\n        value = None\n    if (value is not None):\n        return value\n    if (name[:2] == '__'):\n        raise AttributeError\n    parent = self.parent\n    if (parent is not None):\n        return parent.get(name)\n    return default\n", "label": 0}
{"function": "\n\ndef request(self, action, params=None, data='', headers=None, method='GET'):\n    ' Add the X-NFSN-Authentication header to an HTTP request. '\n    if (not headers):\n        headers = {\n            \n        }\n    if (not params):\n        params = {\n            \n        }\n    header = self._header(action, data)\n    headers['X-NFSN-Authentication'] = header\n    if (method == 'POST'):\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    return ConnectionUserAndKey.request(self, action, params, data, headers, method)\n", "label": 0}
{"function": "\n\ndef it_adds_func_names_to_all(self):\n    base = 'uber'\n    namespace = {\n        '__all__': [],\n    }\n    generate_generic_calls(base, namespace)\n    base_funcs = (m.split('.', 1)[1] for m in METHODS if m.startswith(base))\n    assert (sorted(namespace['__all__']) == list(sorted(base_funcs)))\n", "label": 0}
{"function": "\n\ndef next3Fixtures(self, type_return='string'):\n    now = datetime.datetime.now()\n    url = (('http://www.premierleague.com/en-gb/matchday/league-table.html?season=2015-2016&month=' + months[now.month]) + '&timelineView=date&toDate=1451433599999&tableView=NEXT_3_FIXTURES')\n    team_names = soup(template='.next3FixturesTable')\n    for i in range(len(team_names)):\n        team_names[i] = str(team_names[i].text)\n    next_3_fixtures = soup.select('.club-row .col-fixture')\n    for i in range(len(next_3_fixtures)):\n        next_3_fixtures[i] = str(next_3_fixtures[i].text)\n    return_dict = {\n        \n    }\n    for i in range(len(team_names)):\n        return_dict[team_names[i]] = next_3_fixtures[i]\n    if (type_return == 'dict'):\n        return return_dict\n    return str(return_dict)\n", "label": 0}
{"function": "\n\ndef decrypt(self, id, data):\n    if (data[0] != self.TYPE):\n        raise IntegrityError('Invalid encryption envelope')\n    data = zlib.decompress(memoryview(data)[1:])\n    if (id and (sha256(data).digest() != id)):\n        raise IntegrityError('Chunk id verification failed')\n    return data\n", "label": 0}
{"function": "\n\n@defer.inlineCallbacks\ndef test_SpecifiedFields(self):\n    (yield self.coll.insert([dict(((k, v) for k in 'abcdefg')) for v in range(5)], safe=True))\n    res = (yield self.coll.find(fields={\n        'a': 1,\n        'c': 1,\n    }))\n    (yield self.coll.count(fields={\n        'a': 1,\n        'c': 1,\n    }))\n    self.assertTrue(all(((x in ['a', 'c', '_id']) for x in res[0].keys())))\n    res = (yield self.coll.find(fields=['a', 'c']))\n    (yield self.coll.count(fields=['a', 'c']))\n    self.assertTrue(all(((x in ['a', 'c', '_id']) for x in res[0].keys())))\n    res = (yield self.coll.find(fields=[]))\n    (yield self.coll.count(fields=[]))\n    self.assertTrue(all(((x in ['_id']) for x in res[0].keys())))\n    (yield self.assertFailure(self.coll.find({\n        \n    }, fields=[1]), TypeError))\n", "label": 0}
{"function": "\n\ndef __call__(self, value):\n    if (value is None):\n        return None\n    try:\n        p = value.split(':', 2)\n        _60 = Duration._60\n        _unsigned = Duration._unsigned\n        if (len(p) == 1):\n            result = _unsigned(p[0])\n        if (len(p) == 2):\n            result = ((60 * _unsigned(p[0])) + _60(p[1]))\n        if (len(p) == 3):\n            result = (((3600 * _unsigned(p[0])) + (60 * _60(p[1]))) + _60(p[2]))\n    except ValueError:\n        raise ValueError('Invalid duration value: %s', value)\n    return result\n", "label": 0}
{"function": "\n\ndef returner(ret):\n    '\\n    Return data to a mongodb server\\n    '\n    (conn, mdb) = _get_conn(ret)\n    col = mdb[ret['id']]\n    if isinstance(ret['return'], dict):\n        back = _remove_dots(ret['return'])\n    else:\n        back = ret['return']\n    if isinstance(ret, dict):\n        full_ret = _remove_dots(ret)\n    else:\n        full_ret = ret\n    log.debug(back)\n    sdata = {\n        'minion': ret['id'],\n        'jid': ret['jid'],\n        'return': back,\n        'fun': ret['fun'],\n        'full_ret': full_ret,\n    }\n    if ('out' in ret):\n        sdata['out'] = ret['out']\n    if (float(version) > 2.3):\n        mdb.saltReturns.insert_one(sdata.copy())\n    else:\n        mdb.saltReturns.insert(sdata.copy())\n", "label": 0}
{"function": "\n\n@classmethod\ndef _update(cls, args):\n    api._timeout = args.timeout\n    format = args.format\n    options = None\n    if (args.options is not None):\n        try:\n            options = json.loads(args.options)\n        except:\n            raise Exception('bad json parameter')\n    res = api.Monitor.update(args.monitor_id, type=args.type, query=args.query, name=args.name, message=args.message, options=options)\n    report_warnings(res)\n    report_errors(res)\n    if (format == 'pretty'):\n        print(pretty_json(res))\n    else:\n        print(json.dumps(res))\n", "label": 0}
{"function": "\n\ndef _get_position_ref_node(self, instance):\n    if self.is_sorted:\n        position = 'sorted-child'\n        node_parent = instance.get_parent()\n        if node_parent:\n            ref_node_id = node_parent.pk\n        else:\n            ref_node_id = ''\n    else:\n        prev_sibling = instance.get_prev_sibling()\n        if prev_sibling:\n            position = 'right'\n            ref_node_id = prev_sibling.pk\n        else:\n            position = 'first-child'\n            if instance.is_root():\n                ref_node_id = ''\n            else:\n                ref_node_id = instance.get_parent().pk\n    return {\n        '_ref_node_id': ref_node_id,\n        '_position': position,\n    }\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _parse_rule_sections(parser, config):\n    sections = [section for section in parser.sections() if (section != 'general')]\n    for rule_name in sections:\n        for (option_name, option_value) in parser.items(rule_name):\n            config.set_rule_option(rule_name, option_name, option_value)\n", "label": 0}
{"function": "\n\ndef on_input_timeout(self, cli):\n    if (not self.show_help):\n        return\n    document = cli.current_buffer.document\n    text = document.text\n    LOG.debug('document.text = %s', text)\n    LOG.debug('current_command = %s', self.completer.current_command)\n    if text.strip():\n        command = self.completer.current_command\n        key_name = '.'.join(command.split()).encode('utf-8')\n        last_option = self.completer.last_option\n        if last_option:\n            self.current_docs = self._docs.extract_param(key_name, last_option)\n        else:\n            self.current_docs = self._docs.extract_description(key_name)\n    else:\n        self.current_docs = ''\n    cli.buffers['clidocs'].reset(initial_document=Document(self.current_docs, cursor_position=0))\n    cli.request_redraw()\n", "label": 0}
{"function": "\n\ndef find_by_key(self, key=None, parent='/', **kwargs):\n    \"\\n        Returns a list of DirEntry for each directory entry that matches the given key name.\\n        If a parent is provided, only checks in this parent and all subtree.\\n        These entries are in the same org's directory but have different parents.\\n        \"\n    if (key is None):\n        raise BadRequest('Illegal arguments')\n    if (parent is None):\n        raise BadRequest('Illegal arguments')\n    start_key = [self.orgname, key, parent]\n    end_key = [self.orgname, key, (parent + 'ZZZZZZ')]\n    res = self.dir_store.find_by_view('directory', 'by_key', start_key=start_key, end_key=end_key, id_only=True, convert_doc=True, **kwargs)\n    match = [value for (docid, indexkey, value) in res]\n    return match\n", "label": 0}
{"function": "\n\ndef DownloadActivity(self, serviceRecord, activity):\n    workoutID = activity.ServiceData['WorkoutID']\n    logger.debug(('DownloadActivity for %s' % workoutID))\n    session = self._get_session(record=serviceRecord)\n    resp = session.get((self._urlRoot + ('/api/workout/%d' % workoutID)))\n    try:\n        res = resp.json()\n    except ValueError:\n        raise APIException(('Parse failure in Motivato activity (%d) download: %s' % (workoutID, res.text)))\n    lap = Lap(stats=activity.Stats, startTime=activity.StartTime, endTime=activity.EndTime)\n    activity.Laps = [lap]\n    activity.GPS = False\n    if (('track' in res) and ('points' in res['track'])):\n        for pt in res['track']['points']:\n            wp = Waypoint()\n            if ('moment' not in pt):\n                continue\n            wp.Timestamp = self._parseDateTime(pt['moment'])\n            if ((('lat' in pt) and ('lon' in pt)) or ('ele' in pt)):\n                wp.Location = Location()\n                if (('lat' in pt) and ('lon' in pt)):\n                    wp.Location.Latitude = pt['lat']\n                    wp.Location.Longitude = pt['lon']\n                    activity.GPS = True\n                if ('ele' in pt):\n                    wp.Location.Altitude = float(pt['ele'])\n            if ('bpm' in pt):\n                wp.HR = pt['bpm']\n            lap.Waypoints.append(wp)\n    activity.Stationary = (len(lap.Waypoints) == 0)\n    return activity\n", "label": 1}
{"function": "\n\ndef terminal(self, ret):\n    if (ret is not None):\n        assert os.path.isabs(ret), ret\n        assert os.path.exists(ret), ret\n", "label": 0}
{"function": "\n\ndef get(self, key_path, default=None):\n    try:\n        value = self._data\n        for k in key_path.split('.'):\n            value = value[k]\n        return value\n    except KeyError:\n        if (default is not None):\n            return default\n        else:\n            raise ConfigKeyError(key_path)\n", "label": 0}
{"function": "\n\ndef test_field_checks(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.CharField(), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E004')\n    assert ('Base field for list has errors' in errors[0].msg)\n    assert ('max_length' in errors[0].msg)\n", "label": 0}
{"function": "\n\ndef load_certificate_request(type, buffer):\n    '\\n    Load a certificate request from a buffer\\n\\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\\n    :param buffer: The buffer the certificate request is stored in\\n    :return: The X509Req object\\n    '\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if (type == FILETYPE_PEM):\n        req = _lib.PEM_read_bio_X509_REQ(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif (type == FILETYPE_ASN1):\n        req = _lib.d2i_X509_REQ_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if (req == _ffi.NULL):\n        _raise_current_error()\n    x509req = X509Req.__new__(X509Req)\n    x509req._req = _ffi.gc(req, _lib.X509_REQ_free)\n    return x509req\n", "label": 0}
{"function": "\n\ndef test_export_xls():\n    headers = ['x', 'y']\n    data = [['1', '2'], ['3', '4'], ['5,6', '7'], [None, None]]\n    sheet = ([headers] + data)\n    generator = create_generator(content_generator(headers, data), 'xls')\n    response = make_response(generator, 'xls', 'foo')\n    assert_equal('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', response['content-type'])\n    expected_data = [[(((cell is not None) and cell) or 'NULL') for cell in row] for row in sheet]\n    sheet_data = _read_xls_sheet_data(response)\n    assert_equal(expected_data, sheet_data)\n    assert_equal('attachment; filename=foo.xlsx', response['content-disposition'])\n", "label": 0}
{"function": "\n\ndef createVM(request_id, hostname, recipe, updateProgress):\n    configParser = configparser.RawConfigParser()\n    configFilePath = '/opt/chef-tools/createvm/createvm.config'\n    configParser.read(configFilePath)\n    subdomain = configParser.get('cocreate_config', 'subdomain')\n    progress = 0\n    validHostname = checkHostname(hostname)\n    if (validHostname == False):\n        return (None, None, 'Invalid hostname', progress)\n    fqdn = ((hostname + '.') + subdomain)\n    validRecipe = checkRecipe(configParser, recipe)\n    if (validRecipe == False):\n        return (None, None, 'Unsupported template', progress)\n    updateProgress(request_id, progress, 'Beginning VM template cloning')\n    try:\n        cloneVM(configParser, hostname)\n    except subprocess.CalledProcessError:\n        print('A cloning error occurred')\n        return (None, None, 'VM cloning failed', progress)\n    progress = 33\n    updateProgress(request_id, progress, 'Waiting for new VM IP address')\n    ipAddress = None\n    try:\n        ipAddress = getIP(configParser, hostname)\n    except:\n        print('Could not get IP Address')\n        return (None, None, 'Could not obtain VM IP address', progress)\n    progress = 67\n    updateProgress(request_id, progress, 'Beginning VM bootstrap')\n    try:\n        bootstrapVM(configParser, ipAddress, hostname, recipe)\n    except subprocess.CalledProcessError:\n        print('An error occurred during bootstrap')\n        return (None, None, 'VM bootstrap failed', progress)\n    url = ((('http://' + fqdn) + '/') + recipe)\n    progress = 100\n    updateProgress(request_id, progress, 'VM creation complete', url)\n    return (ipAddress, fqdn, None, progress)\n", "label": 0}
{"function": "\n\ndef parse_novarc(filename):\n    opts = {\n        \n    }\n    f = open(filename, 'r')\n    for line in f:\n        try:\n            line = line.replace('export', '').strip()\n            parts = line.split('=')\n            if (len(parts) > 1):\n                value = parts[1].replace(\"'\", '')\n                value = value.replace('\"', '')\n                opts[parts[0]] = value\n        except:\n            pass\n    f.close()\n    return opts\n", "label": 0}
{"function": "\n\ndef __init__(self, hits=None, truncated=False):\n    if hits:\n        self._filenames = [x[0] for x in hits]\n        self._ranks = [x[1] for x in hits]\n    else:\n        self._filenames = []\n        self._ranks = []\n    self.truncated = truncated\n    self.debug_info = []\n", "label": 0}
{"function": "\n\ndef opt_rulers_parser(value):\n    try:\n        converted = json.loads(value)\n        if isinstance(converted, list):\n            return converted\n        else:\n            raise ValueError\n    except ValueError:\n        raise\n    except TypeError:\n        raise ValueError\n", "label": 0}
{"function": "\n\ndef terminate(self):\n    ' Method should be called to terminate the client before the reactor\\n            is stopped.\\n\\n            @return:            Deferred which fires as soon as the client is\\n                                ready to stop the reactor.\\n            @rtype:             twisted.internet.defer.Deferred\\n        '\n    for call in self._deathCandidates.itervalues():\n        call.cancel()\n    self._deathCandidates = {\n        \n    }\n    for connection in self._connections.copy():\n        connection.destroy()\n    assert (len(self._connections) == 0)\n    Endpoint.terminate(self)\n", "label": 0}
{"function": "\n\ndef main():\n    import sys\n    import json\n    import os\n    import boto3.session\n    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')\n    if (not os.path.isdir(data_dir)):\n        os.makedirs(data_dir)\n    session = boto3.session.Session()\n    loader = session._loader\n    builder = ResourceIndexBuilder()\n    for resource_name in session.get_available_resources():\n        api_version = loader.determine_latest_version(resource_name, 'resources-1')\n        model = loader.load_service_model(resource_name, 'resources-1', api_version)\n        index = builder.build_index(model)\n        output_file = os.path.join(data_dir, resource_name, api_version, 'completions-1.json')\n        if (not os.path.isdir(os.path.dirname(output_file))):\n            os.makedirs(os.path.dirname(output_file))\n        with open(output_file, 'w') as f:\n            f.write(json.dumps(index, indent=2))\n", "label": 0}
{"function": "\n\ndef run(self, image, command=None, create_kwargs=None, start_kwargs=None):\n    '\\n        create container from provided image and start it\\n\\n        for more info, see documentation of REST API calls:\\n         * containers/{}/start\\n         * container/create\\n\\n        :param image: ImageName or string, name or id of the image\\n        :param command: str\\n        :param create_kwargs: dict, kwargs for docker.create_container\\n        :param start_kwargs: dict, kwargs for docker.start\\n        :return: str, container id\\n        '\n    logger.info(\"creating container from image '%s' and running it\", image)\n    create_kwargs = (create_kwargs or {\n        \n    })\n    start_kwargs = (start_kwargs or {\n        \n    })\n    logger.debug(\"image = '%s', command = '%s', create_kwargs = '%s', start_kwargs = '%s'\", image, command, create_kwargs, start_kwargs)\n    if isinstance(image, ImageName):\n        image = image.to_str()\n    container_dict = self.d.create_container(image, command=command, **create_kwargs)\n    container_id = container_dict['Id']\n    logger.debug(\"container_id = '%s'\", container_id)\n    self.d.start(container_id, **start_kwargs)\n    return container_id\n", "label": 0}
{"function": "\n\ndef test_getset_metadata(self):\n    m = meta.Metadata()\n    md = m.get_metadata('files/one')\n    assert (md == 'r--------')\n    d = self.tmpdir()\n    p = os.path.join(d, 'test')\n    utils.touch(p)\n    assert (m.get_metadata(p) != md)\n    m.set_metadata(p, md)\n    assert (m.get_metadata(p) == md)\n", "label": 0}
{"function": "\n\ndef _walknode(self, node):\n    if (node.nodeType == Node.ELEMENT_NODE):\n        self.startElementNS(node.qname, node.tagName, node.attributes)\n        for c in node.childNodes:\n            self._walknode(c)\n        self.endElementNS(node.qname, node.tagName)\n    if ((node.nodeType == Node.TEXT_NODE) or (node.nodeType == Node.CDATA_SECTION_NODE)):\n        self.characters(str(node))\n", "label": 0}
{"function": "\n\ndef _extract_error_code(self, exception):\n    match = re.compile('^(\\\\d+)L?:|^\\\\((\\\\d+)L?,').match(str(exception))\n    code = ((match.group(1) or match.group(2)) if match else None)\n    if code:\n        return int(code)\n", "label": 0}
{"function": "\n\ndef discover_affected_files(include_test_sources, include_scripts, project):\n    source_dir = project.get_property('dir_source_main_python')\n    files = discover_python_files(source_dir)\n    if include_test_sources:\n        if project.get_property('dir_source_unittest_python'):\n            unittest_dir = project.get_property('dir_source_unittest_python')\n            files = itertools.chain(files, discover_python_files(unittest_dir))\n        if project.get_property('dir_source_integrationtest_python'):\n            integrationtest_dir = project.get_property('dir_source_integrationtest_python')\n            files = itertools.chain(files, discover_python_files(integrationtest_dir))\n    if (include_scripts and project.get_property('dir_source_main_scripts')):\n        scripts_dir = project.get_property('dir_source_main_scripts')\n        files = itertools.chain(files, discover_files_matching(scripts_dir, '*'))\n    return files\n", "label": 0}
{"function": "\n\ndef _setMod(self, modname, mod):\n    haschanged = self._modules[modname].changed()\n    if haschanged:\n        if (type(mod) == File):\n            mod.update()\n        self._modules[modname].setValue(mod.getValue())\n        self._modules[modname].updateFingerprint()\n        self.untrust(modname)\n        dependents = self._getOutNodesRecursive(modname)\n        if (dependents != []):\n            dbgstr(('These nodes are dependent: ' + str(dependents)), 0)\n", "label": 0}
{"function": "\n\ndef AddLine(self, line):\n    'Adds a line of text to the block. Paragraph type is auto-determined.'\n    if self.paragraphs.IsType(paragraph.CodeBlock):\n        if line.startswith('<'):\n            self.paragraphs.Close()\n            line = line[1:].lstrip()\n            if line:\n                self.AddLine(line)\n            return\n        if (line[:1] not in ' \\t'):\n            self.paragraphs.Close()\n            self.AddLine(line)\n            return\n        self.paragraphs.AddLine(line)\n        return\n    self._ParseArgs(line)\n    if (not line.strip()):\n        self.paragraphs.SetType(paragraph.BlankLine)\n        return\n    match = regex.list_item.match((line or ''))\n    if match:\n        leader = match.group(1)\n        self.paragraphs.Close()\n        line = regex.list_item.sub('', line)\n        self.paragraphs.SetType(paragraph.ListItem, leader)\n        self.paragraphs.AddLine(line)\n        return\n    if (line and (line[:1] in ' \\t')):\n        if self.paragraphs.IsType(paragraph.ListItem):\n            self.paragraphs.AddLine(line.lstrip())\n            return\n    elif self.paragraphs.IsType(paragraph.ListItem):\n        self.paragraphs.Close()\n    self.paragraphs.SetType(paragraph.TextParagraph)\n    if ((line == '>') or line.endswith(' >')):\n        line = line[:(- 1)].rstrip()\n        if line:\n            self.paragraphs.AddLine(line)\n        self.paragraphs.SetType(paragraph.CodeBlock)\n        return\n    self.paragraphs.AddLine(line)\n", "label": 1}
{"function": "\n\ndef __getitem__(self, index):\n    record = self.resultset.__getitem__(index)\n    array = []\n    for field in self.field_names:\n        value = record\n        for key in field.split('.'):\n            if (key in value):\n                value = value[key]\n            else:\n                break\n        array.append(value)\n    return tuple(array)\n", "label": 0}
{"function": "\n\ndef set_editor_memento(self, memento):\n    (structure, editor_references) = memento\n    if (len(structure.contents) > 0):\n        handler = EditorSetStructureHandler(self, editor_references)\n        self._wx_editor_dock_window.set_structure(structure, handler)\n        for editor in self.window.editors:\n            control = self._wx_editor_dock_window.get_control(editor.id)\n            if (control is not None):\n                self._wx_initialize_editor_dock_control(editor, control)\n    return\n", "label": 0}
{"function": "\n\ndef _remove_client_present(self, client):\n    id_ = client.id_\n    if ((id_ is None) or (id_ not in self.present)):\n        return\n    clients = self.present[id_]\n    if (client not in clients):\n        return\n    clients.remove(client)\n    if (len(clients) == 0):\n        del self.present[id_]\n        if self.presence_events:\n            data = {\n                'new': [],\n                'lost': [id_],\n            }\n            self.event.fire_event(data, salt.utils.event.tagify('change', 'presence'))\n            data = {\n                'present': list(self.present.keys()),\n            }\n            self.event.fire_event(data, salt.utils.event.tagify('present', 'presence'))\n", "label": 0}
{"function": "\n\ndef mean_quadratic_weighted_kappa(kappas, weights=None):\n    \"\\n    Calculates the mean of the quadratic\\n    weighted kappas after applying Fisher's r-to-z transform, which is\\n    approximately a variance-stabilizing transformation.  This\\n    transformation is undefined if one of the kappas is 1.0, so all kappa\\n    values are capped in the range (-0.999, 0.999).  The reverse\\n    transformation is then applied before returning the result.\\n\\n    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\\n    kappa values\\n\\n    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\\n    of weights that is the same size as kappas.  Weights are applied in the\\n    z-space\\n    \"\n    kappas = np.array(kappas, dtype=float)\n    if (weights is None):\n        weights = np.ones(np.shape(kappas))\n    else:\n        weights = (weights / np.mean(weights))\n    kappas = np.array([min(x, 0.999) for x in kappas])\n    kappas = np.array([max(x, (- 0.999)) for x in kappas])\n    z = ((0.5 * np.log(((1 + kappas) / (1 - kappas)))) * weights)\n    z = np.mean(z)\n    return ((np.exp((2 * z)) - 1) / (np.exp((2 * z)) + 1))\n", "label": 0}
{"function": "\n\ndef init(allocator=drv.mem_alloc):\n    '\\n    Initialize libraries used by scikit-cuda.\\n\\n    Initialize the CUBLAS, CUSOLVER, and CULA libraries used by \\n    high-level functions provided by scikit-cuda.\\n\\n    Parameters\\n    ----------\\n    allocator : an allocator used internally by some of the high-level\\n        functions.\\n\\n    Notes\\n    -----\\n    This function does not initialize PyCUDA; it uses whatever device\\n    and context were initialized in the current host thread.\\n    '\n    global _global_cublas_handle, _global_cublas_allocator\n    if (not _global_cublas_handle):\n        from . import cublas\n        _global_cublas_handle = cublas.cublasCreate()\n    if (_global_cublas_allocator is None):\n        _global_cublas_allocator = allocator\n    global _global_cusolver_handle\n    if (not _global_cusolver_handle):\n        from . import cusolver\n        _global_cusolver_handle = cusolver.cusolverDnCreate()\n    if _has_cula:\n        cula.culaInitialize()\n    if _has_magma:\n        magma.magma_init()\n", "label": 0}
{"function": "\n\ndef init_ipython_session(argv=[], auto_symbols=False, auto_int_to_Integer=False):\n    'Construct new IPython session. '\n    import IPython\n    if (V(IPython.__version__) >= '0.11'):\n        if (V(IPython.__version__) >= '1.0'):\n            from IPython.terminal import ipapp\n        else:\n            from IPython.frontend.terminal import ipapp\n        app = ipapp.TerminalIPythonApp()\n        app.display_banner = False\n        app.initialize(argv)\n        if auto_symbols:\n            readline = import_module('readline')\n            if readline:\n                enable_automatic_symbols(app)\n        if auto_int_to_Integer:\n            enable_automatic_int_sympification(app)\n        return app.shell\n    else:\n        from IPython.Shell import make_IPython\n        return make_IPython(argv)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef dict_to_str(dictionary):\n    resstr = ''\n    new_dict = {\n        \n    }\n    for (key, val) in dictionary.items():\n        if (key == 0):\n            key = 'annotation'\n        new_dict[key] = val\n    for key in sorted(new_dict.keys()):\n        resstr += ('%s=%s; ' % (key, str(new_dict[key])))\n    return resstr[:(- 2)]\n", "label": 0}
{"function": "\n\ndef value(self, key, value=1):\n    'Set value of a counter by counter key'\n    if isinstance(key, six.string_types):\n        key = (key,)\n    assert isinstance(key, tuple), 'event key type error'\n    if (key not in self.counters):\n        self.counters[key] = self.cls()\n    self.counters[key].value(value)\n    return self\n", "label": 0}
{"function": "\n\ndef _media(self):\n    if ('collapse' in self.classes):\n        extra = ('' if settings.DEBUG else '.min')\n        js = [('jquery%s.js' % extra), 'jquery.init.js', ('collapse%s.js' % extra)]\n        return forms.Media(js=[static(('admin/js/%s' % url)) for url in js])\n    return forms.Media()\n", "label": 0}
{"function": "\n\ndef _add_xtra_pore_data(self):\n    xpdata = self._xtra_pore_data\n    if (xpdata is not None):\n        if isinstance(xpdata, type([])):\n            for pdata in xpdata:\n                try:\n                    self[('pore.' + pdata)] = self._dictionary[('p' + pdata)][self._pore_map]\n                except:\n                    logger.warning((('Could not add pore data: ' + pdata) + ' to network'))\n                    pass\n        else:\n            try:\n                self[('pore.' + xpdata)] = self._dictionary[('p' + xpdata)][self._pore_map]\n            except:\n                logger.warning((('Could not add pore data: ' + xpdata) + ' to network'))\n                pass\n", "label": 0}
{"function": "\n\ndef mark_invalid_input_sequences(self):\n    \"Fill in domain knowledge about valid input\\n    sequences (e.g. don't prune failure without pruning recovery.)\\n    Only do so if this isn't a view of a previously computed DAG\"\n    fingerprint2previousfailure = {\n        \n    }\n    for event in self._events_list:\n        if hasattr(event, 'fingerprint'):\n            fingerprint = event.fingerprint[1:]\n            if (type(event) in self._failure_types):\n                fingerprint2previousfailure[fingerprint] = event\n            elif (type(event) in self._recovery_types):\n                if (fingerprint in fingerprint2previousfailure):\n                    failure = fingerprint2previousfailure[fingerprint]\n                    failure.dependent_labels.append(event.label)\n", "label": 0}
{"function": "\n\ndef unquote(string):\n    if (not string):\n        return b''\n    res = string.split(b'%')\n    if (len(res) != 1):\n        string = res[0]\n        for item in res[1:]:\n            try:\n                string += (bytes([int(item[:2], 16)]) + item[2:])\n            except ValueError:\n                string += (b'%' + item)\n    return string\n", "label": 0}
{"function": "\n\ndef uncommented_lines(filename, use_sudo=False):\n    '\\n    Get the lines of a remote file, ignoring empty or commented ones\\n    '\n    func = (run_as_root if use_sudo else run)\n    res = func(('cat %s' % quote(filename)), quiet=True)\n    if res.succeeded:\n        return [line for line in res.splitlines() if (line and (not line.startswith('#')))]\n    else:\n        return []\n", "label": 0}
{"function": "\n\ndef create(self, req, body):\n    'Create or import keypair.\\n\\n        Sending name will generate a key and return private_key\\n        and fingerprint.\\n\\n        You can send a public_key to add an existing ssh key\\n\\n        params: keypair object with:\\n            name (required) - string\\n            public_key (optional) - string\\n        '\n    context = req.environ['nova.context']\n    authorize(context, action='create')\n    try:\n        params = body['keypair']\n        name = params['name']\n    except KeyError:\n        msg = _('Invalid request body')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        if ('public_key' in params):\n            keypair = self.api.import_key_pair(context, context.user_id, name, params['public_key'])\n            keypair = self._filter_keypair(keypair, user_id=True)\n        else:\n            (keypair, private_key) = self.api.create_key_pair(context, context.user_id, name)\n            keypair = self._filter_keypair(keypair, user_id=True)\n            keypair['private_key'] = private_key\n        return {\n            'keypair': keypair,\n        }\n    except exception.KeypairLimitExceeded:\n        msg = _('Quota exceeded, too many key pairs.')\n        raise webob.exc.HTTPForbidden(explanation=msg)\n    except exception.InvalidKeypair as exc:\n        raise webob.exc.HTTPBadRequest(explanation=exc.format_message())\n    except exception.KeyPairExists as exc:\n        raise webob.exc.HTTPConflict(explanation=exc.format_message())\n", "label": 0}
{"function": "\n\ndef get_users(self):\n    node = utils.get_repo_node(self.env, self.gitolite_admin_reponame, 'keydir')\n    assert node.isdir, ('Node %s at /keydir/ is not a directory' % node)\n    for child in node.get_entries():\n        name = child.get_name()\n        assert name.endswith('.pub'), ('Node %s' % name)\n        name = name[:(- 4)]\n        (yield name)\n", "label": 0}
{"function": "\n\ndef _walk(self, expr):\n    node = expr.op()\n    if isinstance(node, ops.TableColumn):\n        is_valid = self._validate_column(expr)\n        self.valid = (self.valid and is_valid)\n    for arg in node.flat_args():\n        if isinstance(arg, ir.ValueExpr):\n            self._walk(arg)\n", "label": 0}
{"function": "\n\n@property\ndef line_print(self):\n    inv_types = {v: k for (k, v) in defines.Types.iteritems()}\n    if (self._code is None):\n        self._code = defines.Codes.EMPTY.number\n    msg = 'From {source}, To {destination}, {type}-{mid}, {code}-{token}, ['.format(source=self._source, destination=self._destination, type=inv_types[self._type], mid=self._mid, code=defines.Codes.LIST[self._code].name, token=self._token)\n    for opt in self._options:\n        msg += '{name}: {value}, '.format(name=opt.name, value=opt.value)\n    msg += ']'\n    if (self.payload is not None):\n        msg += ' {payload}...{length} bytes'.format(payload=self.payload[0:20], length=len(self.payload))\n    else:\n        msg += ' No payload'\n    return msg\n", "label": 0}
{"function": "\n\ndef get_profile(self):\n    '\\n        Returns site-specific profile for this user. Raises\\n        SiteProfileNotAvailable if this site does not allow profiles.\\n        '\n    warnings.warn('The use of AUTH_PROFILE_MODULE to define user profiles has been deprecated.', PendingDeprecationWarning)\n    if (not hasattr(self, '_profile_cache')):\n        from django.conf import settings\n        if (not getattr(settings, 'AUTH_PROFILE_MODULE', False)):\n            raise SiteProfileNotAvailable('You need to set AUTH_PROFILE_MODULE in your project settings')\n        try:\n            (app_label, model_name) = settings.AUTH_PROFILE_MODULE.split('.')\n        except ValueError:\n            raise SiteProfileNotAvailable('app_label and model_name should be separated by a dot in the AUTH_PROFILE_MODULE setting')\n        try:\n            model = models.get_model(app_label, model_name)\n            if (model is None):\n                raise SiteProfileNotAvailable('Unable to load the profile model, check AUTH_PROFILE_MODULE in your project settings')\n            self._profile_cache = model._default_manager.using(self._state.db).get(user__id__exact=self.id)\n            self._profile_cache.user = self\n        except (ImportError, ImproperlyConfigured):\n            raise SiteProfileNotAvailable\n    return self._profile_cache\n", "label": 0}
{"function": "\n\ndef _fire_listeners(self, node, lint_context):\n    node_type = NodeType(node['type'])\n    if (node_type not in self._listeners_map):\n        return\n    listening_policies = self._listeners_map[node_type]\n    for listening_policy in listening_policies:\n        violation = listening_policy.get_violation_if_found(node, lint_context)\n        if (violation is not None):\n            self._violations.append(violation)\n", "label": 0}
{"function": "\n\ndef register(self, fid, event):\n    if event:\n        if (event & _AsyncPoller._Read):\n            self.rset.add(fid)\n        if (event & _AsyncPoller._Write):\n            self.wset.add(fid)\n        if (event & _AsyncPoller._Error):\n            self.xset.add(fid)\n", "label": 0}
{"function": "\n\ndef _insert_index(self, data):\n    data = data.copy()\n    idx_nlevels = data.index.nlevels\n    if (idx_nlevels == 1):\n        data.insert(0, 'Index', data.index)\n    else:\n        for i in range(idx_nlevels):\n            data.insert(i, 'Index{0}'.format(i), data.index.get_level_values(i))\n    col_nlevels = data.columns.nlevels\n    if (col_nlevels > 1):\n        col = data.columns.get_level_values(0)\n        values = [data.columns.get_level_values(i).values for i in range(1, col_nlevels)]\n        col_df = pd.DataFrame(values)\n        data.columns = col_df.columns\n        data = pd.concat([col_df, data])\n        data.columns = col\n    return data\n", "label": 0}
{"function": "\n\ndef test_equality():\n    (a, b, c) = (Identity(3), eye(3), ImmutableMatrix(eye(3)))\n    for x in [a, b, c]:\n        for y in [a, b, c]:\n            assert x.equals(y)\n", "label": 0}
{"function": "\n\ndef Dictionary(self, *args):\n    if (not args):\n        return self._dict\n    dlist = [self._dict[x] for x in args]\n    if (len(dlist) == 1):\n        dlist = dlist[0]\n    return dlist\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **kwargs):\n    valid_domains = ['@stud.ntnu.no', '@ntnu.no']\n    for campain_id in args:\n        try:\n            campain = Campaign.objects.filter(pk=int(campain_id))\n        except Campaign.DoesNotExist:\n            raise CommandError(('Campaign ID %s does not exits' % campain_id))\n        signatures_qs = Signature.objects.filter(campaign__pk=int(campain_id))\n        for domain in valid_domains:\n            signatures_qs = signatures_qs.exclude(email__endswith=domain)\n        for signature in signatures_qs:\n            signature.delete()\n", "label": 0}
{"function": "\n\ndef getFieldExtractorForReadMessage(self, jsonMessage, objectToRead):\n    if ('field' not in jsonMessage):\n        raise MalformedMessageException(\"missing 'field'\")\n    field = jsonMessage['field']\n    if (not isinstance(field, str)):\n        raise MalformedMessageException('fieldname not a string')\n    field = intern(field)\n    try:\n        fieldDef = getattr(getObjectClass(objectToRead), field)\n    except:\n        raise InvalidFieldException()\n    if (not Decorators.isPropertyToExpose(fieldDef)):\n        raise InvalidFieldException()\n\n    def getter(x):\n        return getattr(x, field)\n    return getter\n", "label": 0}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.add_application_key(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            self.set_tag(d.getPrefixedString())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 0}
{"function": "\n\ndef __call__(self, item, context=None):\n    array_value = self._array_expression(item, context)\n    if (not isinstance(array_value, list)):\n        return None\n    index_value = self._index_expression(item, context)\n    if (not isinstance(index_value, int)):\n        return None\n    try:\n        return array_value[index_value]\n    except IndexError:\n        return None\n", "label": 0}
{"function": "\n\ndef _verify_tombstones(self, tx_objs, policy):\n    for (o_name, diskfiles) in tx_objs.items():\n        try:\n            self._open_tx_diskfile(o_name, policy)\n            self.fail('DiskFileDeleted expected')\n        except DiskFileDeleted as exc:\n            tx_delete_time = exc.timestamp\n        try:\n            self._open_rx_diskfile(o_name, policy)\n            self.fail('DiskFileDeleted expected')\n        except DiskFileDeleted as exc:\n            rx_delete_time = exc.timestamp\n        self.assertEqual(tx_delete_time, rx_delete_time)\n", "label": 0}
{"function": "\n\ndef field_count(self):\n    num_of_fields = 0\n    if self.has_auto_field():\n        num_of_fields += 1\n    num_of_fields += len(self.fieldsets[0][1]['fields'])\n    if self.formset.can_order:\n        num_of_fields += 1\n    if self.formset.can_delete:\n        num_of_fields += 1\n    return num_of_fields\n", "label": 0}
{"function": "\n\ndef _layout_state(self, state):\n    ' Layout the dock panes in the specified TaskState using its\\n            TaskLayout.\\n        '\n    for (name, corner) in CORNER_MAP.iteritems():\n        area = getattr(state.layout, (name + '_corner'))\n        self.control.setCorner(corner, AREA_MAP[area])\n    self._main_window_layout.state = state\n    self._main_window_layout.set_layout(state.layout)\n    for dock_pane in state.dock_panes:\n        if (dock_pane.control not in self._main_window_layout.consumed):\n            self.control.addDockWidget(AREA_MAP[dock_pane.dock_area], dock_pane.control)\n            if dock_pane.visible:\n                dock_pane.control.show()\n", "label": 0}
{"function": "\n\ndef watcher(self, zh, event, state, path):\n    'Handle zookeeper.aget() watches.\\n\\n    This code is called when a znode changes and triggers a data watch.\\n    It is not called to handle the zookeeper.aget call itself.\\n\\n    Numeric arguments map to constants. See ``DATA`` in ``help(zookeeper)``\\n    for more information.\\n\\n    Args:\\n      zh Zookeeper handle that set this watch.\\n      event Event that caused the watch (often called ``type`` elsewhere).\\n      state Connection state.\\n      path Znode that triggered this watch.\\n\\n    Does not provide a return value.\\n    '\n    out = ['Running watcher:', ('zh=%d' % zh), ('event=%d' % event), ('state=%d' % state), ('path=%s' % path)]\n    logger.debug(' '.join(out))\n    if ((event == zookeeper.CHANGED_EVENT) and (state == zookeeper.CONNECTED_STATE) and (self.znode == path)):\n        if (zookeeper.OK != self.aget()):\n            logger.critical('Unable to get znode! Exiting.')\n            sys.exit(1)\n", "label": 0}
{"function": "\n\ndef request_is_managed_by_flask_classy(self):\n    if (request.endpoint is None):\n        return False\n    if (':' not in request.endpoint):\n        return False\n    (class_name, action) = request.endpoint.split(':')\n    return (any(((class_name == classy_class.__name__) for classy_class in self.flask_classy_classes)) and (action in self.special_methods))\n", "label": 0}
{"function": "\n\ndef get_rule_match(self, gram_handler):\n    for grammar in gram_handler.get_matching_grammars():\n        for rule in grammar._rules:\n            rule_match = matching.get_rule_match(rule, self.remaining_words, grammar.settings['filtered words'])\n            if (rule_match is not None):\n                return rule_match\n", "label": 0}
{"function": "\n\ndef __str__(self, prefix='', printElemNumber=0):\n    res = ''\n    if self.has_is_available_:\n        res += (prefix + ('is_available: %s\\n' % self.DebugFormatBool(self.is_available_)))\n    if self.has_presence_:\n        res += (prefix + ('presence: %s\\n' % self.DebugFormatInt32(self.presence_)))\n    if self.has_valid_:\n        res += (prefix + ('valid: %s\\n' % self.DebugFormatBool(self.valid_)))\n    return res\n", "label": 0}
{"function": "\n\ndef test_create_with_foreign_key(self):\n    'Create a table with a foreign key constraint'\n    inmap = self.std_map()\n    inmap['schema public'].update({\n        'table t1': {\n            'columns': [{\n                'c11': {\n                    'type': 'integer',\n                },\n            }, {\n                'c12': {\n                    'type': 'text',\n                },\n            }],\n        },\n        'table t2': {\n            'columns': [{\n                'c21': {\n                    'type': 'integer',\n                },\n            }, {\n                'c22': {\n                    'type': 'text',\n                },\n            }, {\n                'c23': {\n                    'type': 'integer',\n                },\n            }],\n            'foreign_keys': {\n                't2_c23_fkey': {\n                    'columns': ['c23'],\n                    'references': {\n                        'columns': ['c11'],\n                        'table': 't1',\n                    },\n                },\n            },\n        },\n    })\n    sql = self.to_sql(inmap)\n    crt1 = 0\n    crt2 = 1\n    if ('t1' in sql[1]):\n        crt1 = 1\n        crt2 = 0\n    assert (fix_indent(sql[crt1]) == 'CREATE TABLE t1 (c11 integer, c12 text)')\n    assert (fix_indent(sql[crt2]) == 'CREATE TABLE t2 (c21 integer, c22 text, c23 integer)')\n    assert (fix_indent(sql[2]) == 'ALTER TABLE t2 ADD CONSTRAINT t2_c23_fkey FOREIGN KEY (c23) REFERENCES t1 (c11)')\n", "label": 0}
{"function": "\n\ndef remove(self, entity):\n    '\\n        Remove a subecriber from the channel.\\n\\n        Args:\\n            entity (Player, Object or list): The entity or\\n                entities to un-subscribe from the channel.\\n\\n        '\n    for subscriber in make_iter(entity):\n        if subscriber:\n            clsname = subscriber.__dbclass__.__name__\n            if (clsname == 'PlayerDB'):\n                self.obj.db_subscriptions.remove(entity)\n            elif (clsname == 'ObjectDB'):\n                self.obj.db_object_subscriptions.remove(entity)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (not other):\n        return False\n    return ((self.x_coord == other.x_coord) and (self.y_coord == other.y_coord) and (self.z_coord == other.z_coord))\n", "label": 0}
{"function": "\n\ndef beacon(config):\n    '\\n    Emit the status of a connected display to the minion\\n\\n    Mainly this is used to detect when the display fails to connect for whatever reason.\\n\\n    .. code-block:: yaml\\n\\n        beacons:\\n          glxinfo:\\n            user: frank\\n            screen_event: True\\n\\n    '\n    log.trace('glxinfo beacon starting')\n    ret = []\n    _validate = validate(config)\n    if (not _validate[0]):\n        return ret\n    retcode = __salt__['cmd.retcode']('DISPLAY=:0 glxinfo', runas=config['user'], python_shell=True)\n    if (('screen_event' in config) and config['screen_event']):\n        last_value = last_state.get('screen_available', False)\n        screen_available = (retcode == 0)\n        if ((last_value != screen_available) or ('screen_available' not in last_state)):\n            ret.append({\n                'tag': 'screen_event',\n                'screen_available': screen_available,\n            })\n        last_state['screen_available'] = screen_available\n    return ret\n", "label": 0}
{"function": "\n\ndef rescale_parent_proportion(self, *args):\n    if (not self.parent):\n        return\n    if self.rescale_with_parent:\n        parent_proportion = self._parent_proportion\n        if (self.sizable_from in ('top', 'bottom')):\n            new_height = (parent_proportion * self.parent.height)\n            self.height = max(self.min_size, min(new_height, self.max_size))\n        else:\n            new_width = (parent_proportion * self.parent.width)\n            self.width = max(self.min_size, min(new_width, self.max_size))\n", "label": 0}
{"function": "\n\ndef make_command(command, env=None, sudo=False, sudo_user=None):\n    '\\n    Builds a shell command with various kwargs.\\n    '\n    if env:\n        env_string = ' '.join(['{0}={1}'.format(key, value) for (key, value) in six.iteritems(env)])\n        command = '{0} {1}'.format(env_string, command)\n    command = command.replace(\"'\", \"\\\\'\")\n    if (not sudo):\n        command = \"sh -c '{0}'\".format(command)\n    elif sudo_user:\n        command = \"sudo -H -u {0} -S sh -c '{1}'\".format(sudo_user, command)\n    else:\n        command = \"sudo -H -S sh -c '{0}'\".format(command)\n    return command\n", "label": 0}
{"function": "\n\ndef _make_window(window_dict):\n    '\\n    Creates a new class for that window and registers it at this module.\\n    '\n    cls_name = ('%sWindow' % camel_case(str(window_dict['name'])))\n    bases = (Window,)\n    attrs = {\n        '__module__': sys.modules[__name__],\n        'name': str(window_dict['name']),\n        'inv_type': str(window_dict['id']),\n        'inv_data': window_dict,\n    }\n\n    def make_slot_method(index, size=1):\n        if (size == 1):\n            return (lambda self: self.slots[index])\n        else:\n            return (lambda self: self.slots[index:(index + size)])\n    for slots in window_dict.get('slots', []):\n        index = slots['index']\n        size = slots.get('size', 1)\n        attr_name = snake_case(str(slots['name']))\n        attr_name += ('_slot' if (size == 1) else '_slots')\n        slots_method = make_slot_method(index, size)\n        slots_method.__name__ = attr_name\n        attrs[attr_name] = property(slots_method)\n    for (i, prop_name) in enumerate(window_dict.get('properties', [])):\n\n        def make_prop_method(i):\n            return (lambda self: self.properties[i])\n        prop_method = make_prop_method(i)\n        prop_name = snake_case(str(prop_name))\n        prop_method.__name__ = prop_name\n        attrs[prop_name] = property(prop_method)\n    cls = type(cls_name, bases, attrs)\n    assert (not hasattr(sys.modules[__name__], cls_name)), ('Window \"%s\" already registered at %s' % (cls_name, __name__))\n    setattr(sys.modules[__name__], cls_name, cls)\n    return cls\n", "label": 0}
{"function": "\n\ndef __init__(self, qtile):\n    self.groups = []\n    self.screens = {\n        \n    }\n    self.current_screen = 0\n    for group in qtile.groups:\n        self.groups.append((group.name, group.layout.name))\n    for (index, screen) in enumerate(qtile.screens):\n        self.screens[index] = screen.group.name\n        if (screen == qtile.currentScreen):\n            self.current_screen = index\n", "label": 0}
{"function": "\n\ndef __init__(self, parent=None, win=None, xrefs=None, headers=None):\n    super(XrefValueWindow, self).__init__(parent)\n    self.parent = parent\n    self.mainwin = win\n    self.xrefs = xrefs\n    self.headers = headers\n    self.reverse_strings = {\n        \n    }\n    self.proxyModel = QtGui.QSortFilterProxyModel()\n    self.proxyModel.setDynamicSortFilter(True)\n    self.model = QtGui.QStandardItemModel(len(self.xrefs), len(self.headers), self)\n    column = 0\n    for header in headers:\n        self.model.setHeaderData(column, QtCore.Qt.Horizontal, header)\n        column += 1\n    row = 0\n    for ref in xrefs:\n        for column in range(len(self.headers)):\n            self.model.setData(self.model.index(row, column, QtCore.QModelIndex()), ('%s' % ref[column]))\n        row += 1\n    self.proxyModel.setSourceModel(self.model)\n    self.setRootIsDecorated(False)\n    self.setAlternatingRowColors(True)\n    self.setModel(self.proxyModel)\n    self.setSortingEnabled(True)\n    self.setEditTriggers(QtGui.QAbstractItemView.NoEditTriggers)\n    self.doubleClicked.connect(self.slotDoubleClicked)\n", "label": 0}
{"function": "\n\ndef _convert_to_python(self, value_dict, state):\n    is_empty = self.field_is_empty\n    if ((self.field in value_dict) and (value_dict.get(self.field) == self.expected_value)):\n        for required_field in self.required_fields:\n            if ((required_field not in value_dict) or is_empty(value_dict.get(required_field))):\n                raise Invalid((_('You must give a value for %s') % required_field), value_dict, state, error_dict={\n                    required_field: Invalid(self.message('empty', state), value_dict.get(required_field), state),\n                })\n    return value_dict\n", "label": 0}
{"function": "\n\ndef wpc_channel(url):\n    query = urlparse(url)\n    path_elements = query.path.strip('/').split('/')\n    if ((query.hostname in ('www.watchpeoplecode.com', 'watchpeoplecode.com')) and (len(path_elements) == 2) and (path_elements[0] == 'streamer')):\n        return path_elements[1]\n", "label": 0}
{"function": "\n\ndef _get_functions(self):\n    'Create a mapping from commands to command functions.'\n    functions = {\n        \n    }\n    for each in dir(self):\n        if (not each.startswith('cmd_')):\n            continue\n        func = getattr(self, each)\n        for cmd in getattr(self, ('commands_' + each[4:]), '').split():\n            functions[cmd] = func\n    return functions\n", "label": 0}
{"function": "\n\ndef value_ds_to_numpy(ds, count):\n    ret = None\n    try:\n        values = ds[(count * (- 1)):]\n        ret = numpy.array([float(value) for value in values])\n    except IndexError:\n        pass\n    except TypeError:\n        pass\n    return ret\n", "label": 0}
{"function": "\n\ndef test_bad_querysets(self):\n    with pytest.raises(ValueError) as excinfo:\n        Author.objects.all().order_by('name').iter_smart_chunks()\n    assert ('ordering' in str(excinfo.value))\n    with pytest.raises(ValueError) as excinfo:\n        Author.objects.all()[:5].iter_smart_chunks()\n    assert ('sliced QuerySet' in str(excinfo.value))\n    with pytest.raises(ValueError) as excinfo:\n        NameAuthor.objects.all().iter_smart_chunks()\n    assert ('non-integer primary key' in str(excinfo.value))\n", "label": 0}
{"function": "\n\ndef test_server(args, prepare_func=None, family=socket.AF_INET):\n    try:\n        try:\n            sock = socket.socket(family=family)\n            sock.settimeout(args.timeout)\n            sock.connect((args.host, args.port))\n        except socket.error as e:\n            print('Unable to connect to {0}:{1}: {2}'.format(args.host, args.port, e))\n            return False\n        (remote_addr, remote_port) = sock.getpeername()[:2]\n        print('Connected to: {0}:{1}'.format(remote_addr, remote_port))\n        if (prepare_func is not None):\n            prepare_func(sock)\n            print('Pre-TLS stage completed, continuing with handshake')\n        return handle_ssl(sock, args)\n    except (Failure, socket.error) as e:\n        print(('Unable to check for vulnerability: ' + str(e)))\n        return False\n    finally:\n        if sock:\n            sock.close()\n", "label": 0}
{"function": "\n\ndef __lt__(self, other):\n    if (self._version != other._version):\n        raise TypeError(('%s and %s are not of the same version' % (self, other)))\n    if (not isinstance(other, _BaseAddress)):\n        raise TypeError(('%s and %s are not of the same type' % (self, other)))\n    if (self._ip != other._ip):\n        return (self._ip < other._ip)\n    return False\n", "label": 0}
{"function": "\n\ndef build_5(self):\n    'level 5'\n    pig = Pig(900, 70, self.space)\n    self.pigs.append(pig)\n    pig = Pig(1000, 152, self.space)\n    self.pigs.append(pig)\n    for i in range(9):\n        p = (800, (70 + (i * 21)))\n        self.beams.append(Polygon(p, 85, 20, self.space))\n    for i in range(4):\n        p = (1000, (70 + (i * 21)))\n        self.beams.append(Polygon(p, 85, 20, self.space))\n    p = (970, 176)\n    self.columns.append(Polygon(p, 20, 85, self.space))\n    p = (1026, 176)\n    self.columns.append(Polygon(p, 20, 85, self.space))\n    p = (1000, 230)\n    self.beams.append(Polygon(p, 85, 20, self.space))\n    self.number_of_birds = 4\n    if self.bool_space:\n        self.number_of_birds = 8\n", "label": 0}
{"function": "\n\ndef blockCombine(l):\n    ' Produce a matrix from a list of lists of its components. '\n    l = [list(map(mat, row)) for row in l]\n    hdims = [m.shape[1] for m in l[0]]\n    hs = sum(hdims)\n    vdims = [row[0].shape[0] for row in l]\n    vs = sum(vdims)\n    res = zeros((hs, vs))\n    vindex = 0\n    for (i, row) in enumerate(l):\n        hindex = 0\n        for (j, m) in enumerate(row):\n            res[vindex:(vindex + vdims[i]), hindex:(hindex + hdims[j])] = m\n            hindex += hdims[j]\n        vindex += vdims[i]\n    return res\n", "label": 0}
{"function": "\n\n@classmethod\ndef handle_token(cls, parser, token):\n    tokens = token.split_contents()\n    if (len(tokens) == 1):\n        return cls(obj='user-notification', target='page')\n    if (len(tokens) > 3):\n        raise template.TemplateSyntaxError(_('Max arguments are two'))\n    elif (tokens[1] != 'for'):\n        raise template.TemplateSyntaxError(_(\"First argument must be 'for'\"))\n    elif (not tokens[2]):\n        raise template.TemplateSyntaxError(_(\"Second argument must either 'box' or 'page'\"))\n    else:\n        return cls(obj='user-notification', target=tokens[2])\n", "label": 0}
{"function": "\n\ndef filter_song_md(song, md_list=['id'], no_singletons=True):\n    'Returns a list of desired metadata from a song.\\n    Does not modify the given song.\\n\\n    :param song: Dictionary representing a GM song.\\n    :param md_list: (optional) the ordered list of metadata to select.\\n    :param no_singletons: (optional) if md_list is of length 1, return the data,\\n      not a singleton list.\\n    '\n    filtered = [song[md_type] for md_type in md_list]\n    if ((len(md_list) == 1) and no_singletons):\n        return filtered[0]\n    else:\n        return filtered\n", "label": 0}
{"function": "\n\ndef test_werkzeug_upload():\n    try:\n        import werkzeug\n    except ImportError:\n        return\n    storage = app_storage()\n    object_name = 'my-txt-hello.txt'\n    filepath = (CWD + '/data/hello.txt')\n    file = None\n    with open(filepath, 'rb') as fp:\n        file = werkzeug.datastructures.FileStorage(fp)\n        file.filename = object_name\n        o = storage.upload(file, overwrite=True)\n        assert isinstance(o, Object)\n        assert (o.name == object_name)\n", "label": 0}
{"function": "\n\n@classmethod\ndef from_obj(cls, obj, return_obj=None, contained_type=None, binding_var=None):\n    if (not obj):\n        return None\n    if (return_obj is None):\n        return_obj = cls()\n    if (not contained_type):\n        contained_type = cls._contained_type\n    if (not binding_var):\n        binding_var = cls._binding_var\n    for item in getattr(obj, binding_var):\n        return_obj.append(contained_type.from_obj(item))\n    return return_obj\n", "label": 0}
{"function": "\n\ndef test_match_unrolled(self):\n    ' tests that inference with scan matches result using unrolled loops '\n    unrolled_e_step = E_Step(h_new_coeff_schedule=self.h_new_coeff_schedule)\n    unrolled_e_step.register_model(self.model)\n    V = T.matrix()\n    scan_result = self.e_step.infer(V)\n    unrolled_result = unrolled_e_step.infer(V)\n    outputs = []\n    for key in scan_result:\n        outputs.append(scan_result[key])\n        outputs.append(unrolled_result[key])\n    f = function([V], outputs)\n    outputs = f(self.X)\n    assert ((len(outputs) % 2) == 0)\n    for i in xrange(0, len(outputs), 2):\n        assert np.allclose(outputs[i], outputs[(i + 1)])\n", "label": 0}
{"function": "\n\n@view_config(context=APIPackagingResource, request_method='GET', subpath=(), renderer='json')\n@addslash\n@argify\ndef all_packages(request, verbose=False):\n    ' List all packages '\n    if verbose:\n        packages = request.db.summary()\n    else:\n        packages = request.db.distinct()\n    i = 0\n    while (i < len(packages)):\n        package = packages[i]\n        name = (package if isinstance(package, basestring) else package['name'])\n        if (not request.access.has_permission(name, 'read')):\n            del packages[i]\n            continue\n        i += 1\n    return {\n        'packages': packages,\n    }\n", "label": 0}
{"function": "\n\ndef get_artist(self, artists):\n    'Returns an artist string (all artists) and an artist_id (the main\\n        artist) for a list of discogs album or track artists.\\n        '\n    artist_id = None\n    bits = []\n    for (i, artist) in enumerate(artists):\n        if (not artist_id):\n            artist_id = artist['id']\n        name = artist['name']\n        name = re.sub(' \\\\(\\\\d+\\\\)$', '', name)\n        name = re.sub('(?i)^(.*?), (a|an|the)$', '\\\\2 \\\\1', name)\n        bits.append(name)\n        if (artist['join'] and (i < (len(artists) - 1))):\n            bits.append(artist['join'])\n    artist = (' '.join(bits).replace(' ,', ',') or None)\n    return (artist, artist_id)\n", "label": 0}
{"function": "\n\ndef fromhg(src, dst):\n    if (src.startswith('-') or dst.startswith('-')):\n        raise ValueError('Bad src or dst')\n    proc = subprocess.Popen(['hg', 'clone', src, dst], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    output = proc.communicate()[0]\n    ret = proc.wait()\n    if ret:\n        raise CopyError('Could not copy. Mercurial returned {0}'.format(output))\n", "label": 0}
{"function": "\n\ndef test_manage_job():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == False)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef spj(path, max_cpu_time, max_memory, in_path, user_out_path):\n    if (file_exists(in_path) and file_exists(user_out_path)):\n        result = judger.run(path=path, in_file=in_path, out_file='/tmp/spj.out', max_cpu_time=max_cpu_time, max_memory=max_memory, args=[in_path, user_out_path], env=[('PATH=' + os.environ.get('PATH', ''))], use_sandbox=True, use_nobody=True)\n        if ((result['signal'] == 0) and (result['exit_status'] in [AC, WA, SPJ_ERROR])):\n            result['spj_result'] = result['exit_status']\n        else:\n            result['spj_result'] = SPJ_ERROR\n        return result\n    else:\n        raise ValueError('in_path or user_out_path does not exist')\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _translate_msgid(msgid, domain, desired_locale=None):\n    if (not desired_locale):\n        system_locale = locale.getdefaultlocale()\n        if (not system_locale[0]):\n            desired_locale = 'en_US'\n        else:\n            desired_locale = system_locale[0]\n    locale_dir = os.environ.get((domain.upper() + '_LOCALEDIR'))\n    lang = gettext.translation(domain, localedir=locale_dir, languages=[desired_locale], fallback=True)\n    if six.PY3:\n        translator = lang.gettext\n    else:\n        translator = lang.ugettext\n    translated_message = translator(msgid)\n    return translated_message\n", "label": 0}
{"function": "\n\ndef calculate_debounced_passing(recent_results, debounce=0):\n    '\\n    `debounce` is the number of previous failures we need (not including this)\\n    to mark a search as passing or failing\\n    Returns:\\n      True if passing given debounce factor\\n      False if failing\\n    '\n    if (not recent_results):\n        return True\n    debounce_window = recent_results[:(debounce + 1)]\n    for r in debounce_window:\n        if r.succeeded:\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef _assert_get_rules_by_name(self, name, expected, immediate=None):\n    if (immediate is None):\n        rules = get_rules_by_name(name, self.rules_dir)\n    else:\n        rules = get_rules_by_name(name, self.rules_dir, immediate)\n    actual = [[rule.name, rule.params.keysets.keys()] for rule in rules]\n    for rule in actual:\n        rule[1].sort()\n    eq_(actual, expected)\n", "label": 0}
{"function": "\n\ndef getPluginNameAndModuleFromStream(self, infoFileObject, candidate_infofile=None):\n    for analyzer in self._analyzers:\n        if (analyzer.name == 'info_ext'):\n            return analyzer.getPluginNameAndModuleFromStream(infoFileObject)\n    else:\n        raise RuntimeError('No current file analyzer is able to provide plugin information from stream')\n", "label": 0}
{"function": "\n\ndef scan_quoted_string(self, length=None):\n    self.skip_char(b'\"')\n    out = b''\n    while ((length is None) or (len(out) <= length)):\n        if (not self.char):\n            raise ValueError('quoted string is missing closing quote')\n        elif (self.char == b'\"'):\n            if ((length is None) or (len(out) == length)):\n                self.skip_char(b'\"')\n                break\n            else:\n                raise ValueError(('quoted string ended too early (expected %d)' % length))\n        elif (self.char == b'\\\\'):\n            c = self.advance()\n            if (c in b'\\r\\n'):\n                continue\n            elif (c in b'0123'):\n                s = ((c + self.advance()) + self.advance())\n                val = int(s, 8)\n                out += chr(val)\n            elif (c == b'b'):\n                out += b'\\x08'\n            elif (c == b'f'):\n                out += b'\\x0c'\n            elif (c == b'n'):\n                out += b'\\n'\n            elif (c == b'r'):\n                out += b'\\r'\n            elif (c == b't'):\n                out += b'\\t'\n            elif (c == b'v'):\n                out += b'\\x0b'\n            elif (c == b'x'):\n                s = (self.advance() + self.advance())\n                val = int(s, 16)\n                out += chr(val)\n            else:\n                raise ValueError(('unknown escape character \\\\%s at %d' % (c, self.pos)))\n        else:\n            out += self.char\n        self.advance()\n    return out\n", "label": 1}
{"function": "\n\ndef get_field_history_user(self, instance):\n    try:\n        return instance._field_history_user\n    except AttributeError:\n        try:\n            if self.thread.request.user.is_authenticated():\n                return self.thread.request.user\n            return None\n        except AttributeError:\n            return None\n", "label": 0}
{"function": "\n\ndef bot_process_action(self, action):\n    if (action.action_type == 'message'):\n        body = action.meta.get('body')\n        if body:\n            for dest in action.destination_rooms:\n                self.sendLine('>>> {0}: {1}'.format(dest, body))\n", "label": 0}
{"function": "\n\n@classmethod\ndef create(cls, folder, revision=None, path=None):\n    if (revision and path):\n        raise TypeError('You may specify a git revision or a local path; not both.')\n    if revision:\n        return GitRevisionJiraLinkManager(folder, revision)\n    else:\n        return WorkingCopyJiraLinkManager(folder, path)\n", "label": 0}
{"function": "\n\ndef decode_if_str(arg):\n    if isinstance(arg, list):\n        return [decode_if_str(elem) for elem in arg]\n    else:\n        return (arg.decode('utf-8') if isinstance(arg, str) else arg)\n", "label": 0}
{"function": "\n\ndef convert_to_color(object, name, value):\n    ' Converts a tuple or an integer to an RGB color value, or raises a\\n    TraitError if that is not possible.\\n    '\n    if ((type(value) in SequenceTypes) and (len(value) == 3)):\n        return (range_check(value[0]), range_check(value[1]), range_check(value[2]))\n    if (type(value) is int):\n        num = int(value)\n        return ((((num / 65536) / 255.0(((num / 256) & 255))) / 255.0), ((num & 255) / 255.0))\n    raise TraitError\n", "label": 0}
{"function": "\n\ndef init_app(self, app, dsn=None, logging=None, level=None, logging_exclusions=None, wrap_wsgi=None, register_signal=None):\n    if (dsn is not None):\n        self.dsn = dsn\n    if (level is not None):\n        self.level = level\n    if (wrap_wsgi is not None):\n        self.wrap_wsgi = wrap_wsgi\n    elif (self.wrap_wsgi is None):\n        if (app and app.debug):\n            self.wrap_wsgi = False\n        else:\n            self.wrap_wsgi = True\n    if (register_signal is not None):\n        self.register_signal = register_signal\n    if (logging is not None):\n        self.logging = logging\n    if (logging_exclusions is not None):\n        self.logging_exclusions = logging_exclusions\n    if (not self.client):\n        self.client = make_client(self.client_cls, app, self.dsn)\n    if self.logging:\n        kwargs = {\n            \n        }\n        if (self.logging_exclusions is not None):\n            kwargs['exclude'] = self.logging_exclusions\n        setup_logging(SentryHandler(self.client, level=self.level), **kwargs)\n    if self.wrap_wsgi:\n        app.wsgi_app = SentryMiddleware(app.wsgi_app, self.client)\n    app.before_request(self.before_request)\n    if self.register_signal:\n        got_request_exception.connect(self.handle_exception, sender=app)\n        request_finished.connect(self.after_request, sender=app)\n    if (not hasattr(app, 'extensions')):\n        app.extensions = {\n            \n        }\n    app.extensions['sentry'] = self\n", "label": 1}
{"function": "\n\n@overrides.setter\ndef overrides(self, newch):\n    self._overrides._active = False\n    self._overrides.clear()\n    for (k, v) in newch.iteritems():\n        if v:\n            self._overrides[str(k)] = v\n        else:\n            try:\n                del self._overrides[str(k)]\n            except:\n                pass\n    self._overrides._active = True\n    self._overrides._send()\n", "label": 0}
{"function": "\n\ndef parse(text, pos=0, endpos=None):\n    pos = 0\n    if (endpos is None):\n        endpos = len(text)\n    d = {\n        \n    }\n    while 1:\n        m = entityRE.search(text, pos, endpos)\n        if (not m):\n            break\n        (name, charcode, comment) = m.groups()\n        d[name] = (charcode, comment)\n        pos = m.end()\n    return d\n", "label": 0}
{"function": "\n\ndef create_hc(G):\n    'Creates hierarchical cluster of graph G from distance matrix'\n    path_length = nx.all_pairs_shortest_path_length(G)\n    distances = numpy.zeros((len(G), len(G)))\n    for (u, p) in path_length.items():\n        for (v, d) in p.items():\n            distances[u][v] = d\n    Y = distance.squareform(distances)\n    Z = hierarchy.complete(Y)\n    membership = list(hierarchy.fcluster(Z, t=1.15))\n    partition = defaultdict(list)\n    for (n, p) in zip(list(range(len(G))), membership):\n        partition[p].append(n)\n    return list(partition.values())\n", "label": 0}
{"function": "\n\ndef test_algorithm_returns_06(self):\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['Monthly'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.three_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['3-Month'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.six_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['6-month'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.year_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['year'])\n", "label": 0}
{"function": "\n\ndef ui_dialog(ui, parent, is_modal):\n    ' Creates a wxPython dialog box for a specified UI object.\\n\\n    Changes are not immediately applied to the underlying object. The user must\\n    click **Apply** or **OK** to apply changes. The user can revert changes by\\n    clicking **Revert** or **Cancel**.\\n    '\n    if (ui.owner is None):\n        ui.owner = ModalDialog()\n    ui.owner.init(ui, parent, is_modal)\n    ui.control = ui.owner.control\n    ui.control._parent = parent\n    try:\n        ui.prepare_ui()\n    except:\n        ui.control.Destroy()\n        ui.control.ui = None\n        ui.control = None\n        ui.owner = None\n        ui.result = False\n        raise\n    ui.handler.position(ui.info)\n    restore_window(ui)\n    if is_modal:\n        ui.control.ShowModal()\n    else:\n        ui.control.Show()\n", "label": 0}
{"function": "\n\ndef _fit_cg(f, score, start_params, fargs, kwargs, disp=True, maxiter=100, callback=None, retall=False, full_output=True, hess=None):\n    gtol = kwargs.setdefault('gtol', 1e-05)\n    norm = kwargs.setdefault('norm', np.Inf)\n    epsilon = kwargs.setdefault('epsilon', 1.4901161193847656e-08)\n    retvals = optimize.fmin_cg(f, start_params, score, gtol=gtol, norm=norm, epsilon=epsilon, maxiter=maxiter, full_output=full_output, disp=disp, retall=retall, callback=callback)\n    if full_output:\n        if (not retall):\n            (xopt, fopt, fcalls, gcalls, warnflag) = retvals\n        else:\n            (xopt, fopt, fcalls, gcalls, warnflag, allvecs) = retvals\n        converged = (not warnflag)\n        retvals = {\n            'fopt': fopt,\n            'fcalls': fcalls,\n            'gcalls': gcalls,\n            'warnflag': warnflag,\n            'converged': converged,\n        }\n        if retall:\n            retvals.update({\n                'allvecs': allvecs,\n            })\n    else:\n        xopt = retvals\n        retvals = None\n    return (xopt, retvals)\n", "label": 0}
{"function": "\n\ndef get_oauth_signature(self, request):\n    'Get an OAuth signature to be used in signing a request\\n        '\n    if (self.signature_method == SIGNATURE_PLAINTEXT):\n        return signature.sign_plaintext(self.client_secret, self.resource_owner_secret)\n    (uri, headers, body) = self._render(request)\n    collected_params = signature.collect_parameters(uri_query=urlparse.urlparse(uri).query, body=body, headers=headers)\n    logger.debug('Collected params: {0}'.format(collected_params))\n    normalized_params = signature.normalize_parameters(collected_params)\n    normalized_uri = signature.normalize_base_string_uri(request.uri)\n    logger.debug('Normalized params: {0}'.format(normalized_params))\n    logger.debug('Normalized URI: {0}'.format(normalized_uri))\n    base_string = signature.construct_base_string(request.http_method, normalized_uri, normalized_params)\n    logger.debug('Base signing string: {0}'.format(base_string))\n    if (self.signature_method == SIGNATURE_HMAC):\n        sig = signature.sign_hmac_sha1(base_string, self.client_secret, self.resource_owner_secret)\n    elif (self.signature_method == SIGNATURE_RSA):\n        sig = signature.sign_rsa_sha1(base_string, self.rsa_key)\n    else:\n        sig = signature.sign_plaintext(self.client_secret, self.resource_owner_secret)\n    logger.debug('Signature: {0}'.format(sig))\n    return sig\n", "label": 0}
{"function": "\n\ndef test_min_length(self):\n    h = Hashids(min_length=25)\n    assert (h.encode(7452, 2967, 21401) == 'pO3K69b86jzc6krI416enr2B5')\n    assert (h.encode(1, 2, 3) == 'gyOwl4B97bo2fXhVaDR0Znjrq')\n    assert (h.encode(6097) == 'Nz7x3VXyMYerRmWeOBQn6LlRG')\n    assert (h.encode(99, 25) == 'k91nqP3RBe3lKfDaLJrvy8XjV')\n", "label": 0}
{"function": "\n\ndef fileUpdated(self, file):\n    '\\n        On file update, if the name or the MIME type changed, we must update\\n        them accordingly on the S3 key so that the file downloads with the\\n        correct name and content type.\\n        '\n    if file.get('imported'):\n        return\n    bucket = self._getBucket()\n    key = bucket.get_key(file['s3Key'], validate=True)\n    if (not key):\n        return\n    disp = ('attachment; filename=\"%s\"' % file['name'])\n    mime = (file.get('mimeType') or '')\n    if ((key.content_type != mime) or (key.content_disposition != disp)):\n        key.set_remote_metadata(metadata_plus={\n            'Content-Type': mime,\n            'Content-Disposition': disp.encode('utf8'),\n        }, metadata_minus=[], preserve_acl=True)\n", "label": 0}
{"function": "\n\ndef register_model(self, app_label, model):\n    model_name = model._meta.model_name\n    app_models = self.all_models[app_label]\n    if (model_name in app_models):\n        if ((model.__name__ == app_models[model_name].__name__) and (model.__module__ == app_models[model_name].__module__)):\n            warnings.warn((\"Model '%s.%s' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\" % (app_label, model_name)), RuntimeWarning, stacklevel=2)\n        else:\n            raise RuntimeError((\"Conflicting '%s' models in application '%s': %s and %s.\" % (model_name, app_label, app_models[model_name], model)))\n    app_models[model_name] = model\n    self.do_pending_operations(model)\n    self.clear_cache()\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    self.admin = credentials.UsernamePassword('admin', 'asdf')\n    self.alice = credentials.UsernamePassword('alice', 'foo')\n    self.badPass = credentials.UsernamePassword('alice', 'foobar')\n    self.badUser = credentials.UsernamePassword('x', 'yz')\n    self.checker = strcred.makeChecker('unix')\n    if pwd:\n        database = UserDatabase()\n        for (username, password) in self.users.items():\n            database.addUser(username, crypt.crypt(password, 'F/'), 1000, 1000, username, ('/home/' + username), '/bin/sh')\n        self.patch(pwd, 'getpwnam', database.getpwnam)\n    if spwd:\n        self._spwd_getspnam = spwd.getspnam\n        spwd.getspnam = self._spwd\n", "label": 0}
{"function": "\n\ndef generate(env):\n    'Add Builders and construction variables for gcc to an Environment.'\n    if ('CC' not in env):\n        env['CC'] = (env.Detect(compilers) or compilers[0])\n    cc.generate(env)\n    if (env['PLATFORM'] in ['cygwin', 'win32']):\n        env['SHCCFLAGS'] = SCons.Util.CLVar('$CCFLAGS')\n    else:\n        env['SHCCFLAGS'] = SCons.Util.CLVar('$CCFLAGS -fPIC')\n    version = detect_version(env, env['CC'])\n    if version:\n        env['CCVERSION'] = version\n", "label": 0}
{"function": "\n\ndef _deserialize(self, value, attr, data):\n    if (not self.truthy):\n        return bool(value)\n    else:\n        try:\n            if (value in self.truthy):\n                return True\n            elif (value in self.falsy):\n                return False\n        except TypeError:\n            pass\n    self.fail('invalid')\n", "label": 0}
{"function": "\n\ndef filterOnStatusExt(statusExt, commits):\n    return [commit for commit in commits if (('_' in commit.status) and (commit.status[(commit.status.rfind('_') + 1):] == statusExt))]\n", "label": 0}
{"function": "\n\ndef test_registry_uris_param_v2(self):\n    spec = CommonSpec()\n    spec.set_params(registry_uris=['http://registry.example.com:5000/v2'], user=TEST_USER)\n    registry = spec.registry_uris.value[0]\n    assert (registry.uri == 'http://registry.example.com:5000')\n    assert (registry.docker_uri == 'registry.example.com:5000')\n    assert (registry.version == 'v2')\n", "label": 0}
{"function": "\n\ndef test_number_to_string(self):\n    ' Numbers are turned into strings.\\n        '\n    cleaner = Cleaners()\n    in_int = 85\n    in_float = 82.12\n    in_string = 'big frame, small spirit!'\n    in_list = ['hands', 'by', 'the', 'halyards']\n    in_none = None\n    assert (cleaner.number_to_string(in_int) == str(in_int))\n    assert (cleaner.number_to_string(in_float) == str(in_float))\n    assert (cleaner.number_to_string(in_string) == in_string)\n    assert (cleaner.number_to_string(in_list) == in_list)\n    assert (cleaner.number_to_string(in_none) is None)\n", "label": 0}
{"function": "\n\ndef __call__(self, action, *args, **kwargs):\n    module_name = ('%s.%s' % (self.package, action.replace('-', '_')))\n    cwd = kwargs.get('cwd')\n    if cwd:\n        self.chdir(cwd)\n    if kwargs.get('raises'):\n        with pytest.raises(qisys.error.Error) as error:\n            qisys.script.run_action(module_name, args)\n        return str(error.value)\n    if kwargs.get('retcode'):\n        try:\n            qisys.script.run_action(module_name, args)\n        except SystemExit as e:\n            return e.code\n        return 0\n    else:\n        return qisys.script.run_action(module_name, args)\n", "label": 0}
{"function": "\n\ndef GatheringUserDatasGet(self, socketId):\n    command = 'GatheringUserDatasGet(double *,double *,double *,double *,double *,double *,double *,double *)'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(8):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef invert(self):\n    for polygon in self.polygons:\n        polygon.flip()\n    self.plane.flip()\n    if self.front:\n        self.front.invert()\n    if self.back:\n        self.back.invert()\n    (self.front, self.back) = (self.back, self.front)\n", "label": 0}
{"function": "\n\ndef start_upload_workers(self, files, uploaded_queue, boundary, local_deploy):\n    num_files = len(files)\n    num_workers = 4\n    if (num_workers > num_files):\n        num_workers = num_files\n    start = 0\n    step = int(((num_files + (num_workers - 1)) / num_workers))\n    for _ in range(num_workers):\n        end = (start + step)\n        if (end > num_files):\n            end = num_files\n        Thread(target=self.post_files, args=[files, start, end, uploaded_queue.put, boundary, local_deploy]).start()\n        start = end\n", "label": 0}
{"function": "\n\ndef next(self):\n    if ((self.next_cursor == 0) or (self.limit and (self.count == self.limit))):\n        raise StopIteration\n    (data, cursors) = self.method(*self.args, cursor=self.next_cursor, **self.kargs)\n    (self.prev_cursor, self.next_cursor) = cursors\n    if (len(data) == 0):\n        raise StopIteration\n    self.count += 1\n    return data\n", "label": 0}
{"function": "\n\ndef parse_output(output):\n    'Parse fabric output and return the output per host'\n    line_pattern = re.compile('\\\\[(?P<host>.+)\\\\] out: (?P<line>.+)')\n    results = {\n        \n    }\n    for line in output:\n        m = line_pattern.match(line)\n        if m:\n            host = m.group('host')\n            if (host not in results):\n                results[host] = []\n            line = m.group('line').strip()\n            if line:\n                results[host].append(line)\n    return results\n", "label": 0}
{"function": "\n\ndef save_template_dict(self, templ_dict):\n    if templ_dict:\n        try:\n            self.es.index(index=(self.project_name + '-crawler-template_dict'), doc_type='template_dict', id=self.plugin_name, body=json.dumps({repr(k): v for (k, v) in templ_dict.items()}))\n        except:\n            self.es.update(index=(self.project_name + '-crawler-template_dict'), doc_type='template_dict', id=self.plugin_name, body=json.dumps({\n                'doc': {repr(k): v for (k, v) in templ_dict.items()},\n            }))\n", "label": 0}
{"function": "\n\ndef vim_choice(prompt, default, choices):\n    default = (choices.index(default) + 1)\n    choices_str = '\\n'.join([('&%s' % choice) for choice in choices])\n    try:\n        choice = int(vim.eval(('confirm(\"%s\", \"%s\", %s)' % (prompt, choices_str, default))))\n    except KeyboardInterrupt:\n        return None\n    if (choice == 0):\n        return None\n    return choices[(choice - 1)]\n", "label": 0}
{"function": "\n\ndef testExceptions(self):\n    request = roots.Request()\n    try:\n        request.write(b'blah')\n    except NotImplementedError:\n        pass\n    else:\n        self.fail()\n    try:\n        request.finish()\n    except NotImplementedError:\n        pass\n    else:\n        self.fail()\n", "label": 0}
{"function": "\n\ndef group_connections(connections):\n    \"\\n    Return a list of (language code, respective connections) pairs, while\\n    using Django's translation.override() to set each language.\\n    \"\n    grouped_conns = defaultdict(list)\n    if isinstance(connections, QuerySet):\n        languages = connections.values_list('contact__language', flat=True)\n        for language in languages.distinct():\n            lang_conns = connections.filter(contact__language=language)\n            grouped_conns[language].extend(lang_conns)\n    else:\n        for connection in connections:\n            language = connection.contact.language\n            grouped_conns[language].append(connection)\n    for (lang, conns) in grouped_conns.items():\n        (yield (lang, conns))\n", "label": 0}
{"function": "\n\ndef get_user(self, check_permissions=True):\n    key = self.kwargs[self.user_lookup_url_kwarg]\n    current_user = self.request.user\n    if (key == 'me'):\n        if isinstance(current_user, AnonymousUser):\n            raise NotAuthenticated\n        else:\n            return self.request.user\n    obj = get_object_or_error(User, key, 'user')\n    if check_permissions:\n        self.check_object_permissions(self.request, obj)\n    return obj\n", "label": 0}
{"function": "\n\ndef deploy_file(self, file_name, calc_md5=True, calc_sha1=True, parameters={\n    \n}):\n    '\\n        Upload the given file to this path\\n        '\n    if calc_md5:\n        md5 = md5sum(file_name)\n    if calc_sha1:\n        sha1 = sha1sum(file_name)\n    target = self\n    if self.is_dir():\n        target = (self / pathlib.Path(file_name).name)\n    with open(file_name, 'rb') as fobj:\n        target.deploy(fobj, md5, sha1, parameters)\n", "label": 0}
{"function": "\n\ndef gradient(self, x, Y):\n    '\\n        Computes the gradient of the Polynomial kernel wrt. to the left argument, i.e.\\n        \\nabla_x k(x,y)=\\nabla_x (1+x^Ty)^d=d(1+x^Ty)^(d-1) y\\n        \\n        x - single sample on right hand side (1D vector)\\n        Y - samples on left hand side (2D matrix)\\n        '\n    assert (len(x.shape) == 1)\n    assert (len(Y.shape) == 2)\n    assert (len(x) == Y.shape[1])\n    return ((self.degree * pow((1 + x.dot(Y.T)), self.degree)) * Y)\n", "label": 0}
{"function": "\n\ndef to_json(s):\n    'Return a valid json string, given a jsarray string.\\n\\n    :param s: string of jsarray data\\n    '\n    out = []\n    for t in generate_tokens(StringIO(s).readline):\n        if (out and any(((',' == t[1] == out[(- 1)]), ((out[(- 1)] == '[') and (t[1] == ','))))):\n            out.append('null')\n        out.append(t[1])\n    return ''.join(out)\n", "label": 0}
{"function": "\n\ndef convert_from_timestamp(timestamp, to_format, to_type=str):\n    if (to_type == str):\n        return converter.timestamp_to_string(timestamp, to_format)\n    elif (to_type == datetime):\n        return converter.timestamp_to_datetime(timestamp, to_format)\n    elif (to_type == date):\n        return converter.timestamp_to_date(timestamp)\n", "label": 0}
{"function": "\n\ndef __init__(self, path=None):\n    self.path = os.path.abspath((path or os.path.curdir))\n    try:\n        self.master = 'origin/master'\n        master_sha = self.shell('git', 'log', '-1', '--pretty=format:%H', self.master, exceptions=True).split\n    except:\n        self.master = 'master'\n        master_sha = self.shell('git', 'log', '-1', '--pretty=format:%H', self.master).split\n    self.master_sha = ((master_sha and master_sha[0].strip()) or '')\n", "label": 0}
{"function": "\n\ndef _describe_table(self, connection, table, charset=None, full_name=None):\n    'Run DESCRIBE for a ``Table`` and return processed rows.'\n    if (full_name is None):\n        full_name = self.identifier_preparer.format_table(table)\n    st = ('DESCRIBE %s' % full_name)\n    (rp, rows) = (None, None)\n    try:\n        try:\n            rp = connection.execution_options(skip_user_error_events=True).execute(st)\n        except exc.DBAPIError as e:\n            if (self._extract_error_code(e.orig) == 1146):\n                raise exc.NoSuchTableError(full_name)\n            else:\n                raise\n        rows = self._compat_fetchall(rp, charset=charset)\n    finally:\n        if rp:\n            rp.close()\n    return rows\n", "label": 0}
{"function": "\n\ndef get_model(app_label, model_name):\n    '\\n    Given an app label and a model name, returns the corresponding model class.\\n    '\n    try:\n        return apps.get_model(app_label, model_name)\n    except AppRegistryNotReady:\n        if (apps.apps_ready and (not apps.models_ready)):\n            app_config = apps.get_app_config(app_label)\n            import_module(('%s.%s' % (app_config.name, MODELS_MODULE_NAME)))\n            return apps.get_registered_model(app_label, model_name)\n        else:\n            raise\n", "label": 0}
{"function": "\n\ndef mouseMoveEvent(self, event):\n    channel = None\n    point = None\n    if self.cur_drag:\n        channel = self.cur_drag[0]\n        point = self.cur_drag[1]\n        if (not point.fixed):\n            point.set_pos(channel.get_index_pos(event.x()))\n            point.activate_channels(self.active_channels_string)\n            self.table.sort_control_points()\n        channel.set_value_index(point.color, event.y())\n        self.table_config_changed(final_update=False)\n    screenX = event.x()\n    screenY = event.y()\n    (width, height) = (self.size().width(), self.size().height())\n    master = self.master\n    (s1, s2) = master.get_table_range()\n    if (channel is not None):\n        name = self.text_map[channel.name]\n        pos = (s1 + ((s2 - s1) * point.pos))\n        val = channel.get_value(point.color)\n        txt = ('%s: (%.3f, %.3f)' % (name, pos, val))\n    else:\n        x = (s1 + (((s2 - s1) * float(screenX)) / (width - 1)))\n        y = (1.0 - (float(screenY) / (height - 1)))\n        txt = ('position: (%.3f, %.3f)' % (x, y))\n    self.master.set_status_text(txt)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(UserSettingsForm, self).__init__(*args, **kwargs)\n\n    def get_language_display_name(code, desc):\n        try:\n            desc = translation.get_language_info(code)['name_local']\n            desc = string.capwords(desc)\n        except KeyError:\n            pass\n        return ('%s (%s)' % (desc, code))\n    languages = [(k, get_language_display_name(k, v)) for (k, v) in settings.LANGUAGES]\n    self.fields['language'].choices = languages\n    timezones = []\n    language = translation.get_language()\n    current_locale = translation.to_locale(language)\n    babel_locale = babel.Locale.parse(current_locale)\n    for (tz, offset) in self._sorted_zones():\n        try:\n            utc_offset = (_('UTC %(hour)s:%(min)s') % {\n                'hour': offset[:3],\n                'min': offset[3:],\n            })\n        except Exception:\n            utc_offset = ''\n        if (tz == 'UTC'):\n            tz_name = _('UTC')\n        elif (tz == 'GMT'):\n            tz_name = _('GMT')\n        else:\n            tz_label = babel.dates.get_timezone_location(tz, locale=babel_locale)\n            tz_name = (_('%(offset)s: %(label)s') % {\n                'offset': utc_offset,\n                'label': tz_label,\n            })\n        timezones.append((tz, tz_name))\n    self.fields['timezone'].choices = timezones\n", "label": 0}
{"function": "\n\ndef solve(self, rhs_mat, system, mode):\n    \" Solves the linear system for the problem in self.system. The\\n        full solution vector is returned.\\n\\n        Args\\n        ----\\n        rhs_mat : dict of ndarray\\n            Dictionary containing one ndarry per top level quantity of\\n            interest. Each array contains the right-hand side for the linear\\n            solve.\\n\\n        system : `System`\\n            Parent `System` object.\\n\\n        mode : string\\n            Derivative mode, can be 'fwd' or 'rev'.\\n\\n        Returns\\n        -------\\n        dict of ndarray : Solution vectors\\n        \"\n    self.system = system\n    if (self.mode is None):\n        self.mode = mode\n    sol_buf = OrderedDict()\n    for (voi, rhs) in rhs_mat.items():\n        self.voi = None\n        if system._jacobian_changed:\n            self.jacobian = self._assemble_jacobian(rhs, mode)\n            system._jacobian_changed = False\n            if (self.options['solve_method'] == 'LU'):\n                self.lup = lu_factor(self.jacobian)\n        if (self.options['solve_method'] == 'LU'):\n            deriv = lu_solve(self.lup, rhs)\n        else:\n            deriv = np.linalg.solve(self.jacobian, rhs)\n        self.system = None\n        sol_buf[voi] = deriv\n    return sol_buf\n", "label": 0}
{"function": "\n\ndef test_vector_norm():\n    x = np.arange(30).reshape((5, 6))\n    s = symbol('x', discover(x))\n    assert eq(compute(s.vnorm(), x), np.linalg.norm(x))\n    assert eq(compute(s.vnorm(ord=1), x), np.linalg.norm(x.flatten(), ord=1))\n    assert eq(compute(s.vnorm(ord=4, axis=0), x), np.linalg.norm(x, ord=4, axis=0))\n    expr = s.vnorm(ord=4, axis=0, keepdims=True)\n    assert (expr.shape == compute(expr, x).shape)\n", "label": 0}
{"function": "\n\ndef test_numpy_geometric(self):\n    geom = jit_unary('np.random.geometric')\n    self.assertRaises(ValueError, geom, (- 1.0))\n    self.assertRaises(ValueError, geom, 0.0)\n    self.assertRaises(ValueError, geom, 1.001)\n    N = 200\n    r = [geom(1.0) for i in range(N)]\n    self.assertPreciseEqual(r, ([1] * N))\n    r = [geom(0.9) for i in range(N)]\n    n = r.count(1)\n    self.assertGreaterEqual(n, (N // 2))\n    self.assertLess(n, N)\n    self.assertFalse([i for i in r if (i > 1000)])\n    r = [geom(0.4) for i in range(N)]\n    self.assertTrue([i for i in r if (i > 4)])\n    r = [geom(0.01) for i in range(N)]\n    self.assertTrue([i for i in r if (i > 50)])\n    r = [geom(1e-15) for i in range(N)]\n    self.assertTrue([i for i in r if (i > (2 ** 32))])\n", "label": 1}
{"function": "\n\ndef assert_no_warnings(func, *args, **kw):\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        result = func(*args, **kw)\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            w = [e for e in w if (e.category is not np.VisibleDeprecationWarning)]\n        if (len(w) > 0):\n            raise AssertionError(('Got warnings when calling %s: %s' % (func.__name__, w)))\n    return result\n", "label": 0}
{"function": "\n\ndef get_dots_case_json(casedoc, anchor_date=None):\n    '\\n    Return JSON-ready array of the DOTS block for given patient.\\n    Pulling properties from PATIENT document.\\n    Patient document trumps casedoc in this use case.\\n    '\n    if (anchor_date is None):\n        anchor_date = datetime.now(tz=timezone(settings.TIME_ZONE))\n    enddate = anchor_date\n    ret = {\n        'regimens': [int((getattr(casedoc, CASE_NONART_REGIMEN_PROP, None) or 0)), int((getattr(casedoc, CASE_ART_REGIMEN_PROP, None) or 0))],\n        'regimen_labels': [list(casedoc.nonart_labels), list(casedoc.art_labels)],\n        'days': [],\n        'anchor': anchor_date.strftime('%d %b %Y'),\n    }\n    observations = query_observations(casedoc._id, (enddate - timedelta(days=DOT_DAYS_INTERVAL)), enddate)\n    for delta in range(DOT_DAYS_INTERVAL):\n        obs_date = (enddate - timedelta(days=delta))\n        day_arr = filter_obs_for_day(obs_date.date(), observations)\n        day_data = DOTDay.merge_from_observations(day_arr)\n        ret['days'].append(day_data.to_case_json(casedoc, ret['regimen_labels']))\n    ret['days'].reverse()\n    return ret\n", "label": 0}
{"function": "\n\ndef value__get(self):\n    if (self.selectedIndex is not None):\n        return self.options[self.selectedIndex][0]\n    else:\n        for (option, checked) in self.options:\n            if checked:\n                return option\n        else:\n            if self.options:\n                return self.options[0][0]\n            else:\n                return None\n", "label": 0}
{"function": "\n\ndef compute_files(hive_holder, output, settings):\n    ' given the current hive_holder, compute the files that have to be saved in disk\\n    return: {BlockCellName: StrOrBytes to be saved in disk}\\n    param output: something that supports info, warn, error\\n    '\n    new_files = {\n        \n    }\n    for (block_cell_name, (cell, content)) in hive_holder.resources.iteritems():\n        if isinstance(cell, VirtualCell):\n            try:\n                target = cell.evaluate(settings)\n            except ConfigurationFileError as e:\n                output.error(('Error evaluating virtual %s: %s' % (block_cell_name, e.message)))\n                continue\n            content = hive_holder[target.block_name][target.cell_name].content\n            new_files[block_cell_name] = content.load.load\n        elif content.blob_updated:\n            new_files[block_cell_name] = content.load.load\n    return new_files\n", "label": 0}
{"function": "\n\ndef finalize(self):\n    'finalizing this Report sends off the email.'\n    self.write(self._formatter.finalize())\n    report = ezmail.MIMEText.MIMEText(self._fo.getvalue(), self._formatter.MIMETYPE.split('/')[1])\n    report['Content-Disposition'] = 'inline'\n    self._message.attach(report)\n    if (self._attach_logfile and self._logfile):\n        try:\n            lfd = open(self._logfile, 'rb').read()\n        except:\n            pass\n        else:\n            logmsg = ezmail.MIMEText.MIMEText(lfd, charset=chardet.detect(lfd))\n            logmsg['Content-Disposition'] = ('attachment; filename=%s' % (os.path.basename(self._logfile),))\n            self._message.attach(logmsg)\n    ezmail.mail(self._message)\n", "label": 0}
{"function": "\n\n@synchronizedDeferred(busy)\n@deferredAsThread\ndef garbageCheck(self):\n    'Check for file patterns that are removeable'\n    watchDict = copy.deepcopy(self.watchDict)\n    for (directory, garbageList) in watchDict.iteritems():\n        if (not os.path.exists(directory)):\n            continue\n        for (pattern, limit) in garbageList:\n            self.cleanupLinks(directory)\n            files = [os.path.join(directory, f) for f in os.listdir(directory) if re.search(pattern, f)]\n            files = sorted(files)\n            if (len(files) > int(limit)):\n                log(('These files matched:\\n\\t%s' % '\\n\\t'.join(files)))\n            while (len(files) > int(limit)):\n                oldfile = files.pop(0)\n                log(('Deleting %s' % oldfile))\n                if os.path.islink(oldfile):\n                    continue\n                if os.path.isdir(oldfile):\n                    for (base, dirs, myfiles) in os.walk(oldfile, topdown=False):\n                        for name in myfiles:\n                            os.remove(os.path.join(base, name))\n                        for name in dirs:\n                            os.rmdir(os.path.join(base, name))\n                    os.rmdir(oldfile)\n                else:\n                    os.unlink(oldfile)\n        self.cleanupLinks(directory)\n", "label": 1}
{"function": "\n\ndef _update_project_members(self, request, data, project_id):\n    users_to_add = 0\n    try:\n        available_roles = api.keystone.role_list(request)\n        member_step = self.get_step(PROJECT_USER_MEMBER_SLUG)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_to_add += len(role_list)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_added = 0\n            for user in role_list:\n                api.keystone.add_tenant_user_role(request, project=project_id, user=user, role=role.id)\n                users_added += 1\n            users_to_add -= users_added\n    except Exception:\n        if PROJECT_GROUP_ENABLED:\n            group_msg = _(', add project groups')\n        else:\n            group_msg = ''\n        exceptions.handle(request, (_('Failed to add %(users_to_add)s project members%(group_msg)s and set project quotas.') % {\n            'users_to_add': users_to_add,\n            'group_msg': group_msg,\n        }))\n", "label": 0}
{"function": "\n\ndef test_alphabet(self):\n    h = Hashids(alphabet='!\"#%&\\',-/0123456789:;<=>ABCDEFGHIJKLMNOPQRSTUVWXYZ_`abcdefghijklmnopqrstuvwxyz~')\n    assert (h.encode(2839, 12, 32, 5) == '_nJUNTVU3')\n    assert (h.encode(1, 2, 3) == '7xfYh2')\n    assert (h.encode(23832) == 'Z6R>')\n    assert (h.encode(99, 25) == 'AYyIB')\n", "label": 0}
{"function": "\n\ndef store_current():\n    '\\n    Stores the current logs in the database\\n    '\n    for (key, getwork_c) in getworks.items():\n        accepted_c = accepted.get(key, 0)\n        rejected_c = rejected.get(key, 0)\n        server = key[0]\n        username = key[1]\n        password = key[2]\n        difficulty = key[3]\n        timestamp = time.asctime(time.gmtime())\n        try:\n            if (get_diff(server) != difficulty):\n                continue\n        except:\n            logging.error(traceback.format_exc())\n            continue\n        sql = (\"UPDATE Statistics SET Getworks = %s, Accepted = %s, Rejected = %s, Timestamp = '%s' WHERE Server = '%s' AND Username = '%s' AND Password = '%s' AND Difficulty = %s\" % (getwork_c, accepted_c, rejected_c, timestamp, server, username, password, difficulty))\n        bitHopper.Database.execute(sql)\n        result = bitHopper.Database.execute(('SELECT Getworks from Statistics WHERE Server = \"%s\" AND Username = \"%s\" AND Password = \"%s\" AND Difficulty = %s' % (server, username, password, difficulty)))\n        result = list(result)\n        if (len(result) == 0):\n            sql = (\"INSERT INTO Statistics (Server, Username, Password, Difficulty, Timestamp, Getworks, Accepted, Rejected) VALUES ('%s', '%s', '%s', %s, '%s', %s, %s, %s)\" % (server, username, password, difficulty, timestamp, getwork_c, accepted_c, rejected_c))\n            bitHopper.Database.execute(sql)\n", "label": 0}
{"function": "\n\ndef nested_update(d, u):\n    for (k, v) in u.iteritems():\n        if isinstance(v, collections.Mapping):\n            r = nested_update(d.get(k, {\n                \n            }), v)\n            d[k] = r\n        elif isinstance(v, collections.Iterable):\n            try:\n                d[k].extend(u[k])\n            except KeyError:\n                d[k] = u[k]\n        else:\n            d[k] = u[k]\n    return d\n", "label": 0}
{"function": "\n\ndef _insert(self, data):\n    if isinstance(data, list):\n        return [self._insert(item) for item in data]\n    if (not all((isinstance(k, string_types) for k in data))):\n        raise ValueError('Document keys must be strings')\n    if ('_id' not in data):\n        data['_id'] = ObjectId()\n    object_id = data['_id']\n    if isinstance(object_id, dict):\n        object_id = helpers.hashdict(object_id)\n    if (object_id in self._documents):\n        raise DuplicateKeyError('Duplicate Key Error', 11000)\n    for unique in self._uniques:\n        find_kwargs = {\n            \n        }\n        for (key, direction) in unique:\n            if (key in data):\n                find_kwargs[key] = data[key]\n        answer = self.find(find_kwargs)\n        if (answer.count() > 0):\n            raise DuplicateKeyError('Duplicate Key Error', 11000)\n    self._documents[object_id] = self._internalize_dict(data)\n    return data['_id']\n", "label": 1}
{"function": "\n\ndef validate_prepopulated_fields(self, cls, model):\n    ' Validate that prepopulated_fields if a dictionary  containing allowed field types. '\n    if hasattr(cls, 'prepopulated_fields'):\n        check_isdict(cls, 'prepopulated_fields', cls.prepopulated_fields)\n        for (field, val) in cls.prepopulated_fields.items():\n            f = get_field(cls, model, 'prepopulated_fields', field)\n            if isinstance(f, (models.DateTimeField, models.ForeignKey, models.ManyToManyField)):\n                raise ImproperlyConfigured((\"'%s.prepopulated_fields['%s']' is either a DateTimeField, ForeignKey or ManyToManyField. This isn't allowed.\" % (cls.__name__, field)))\n            check_isseq(cls, (\"prepopulated_fields['%s']\" % field), val)\n            for (idx, f) in enumerate(val):\n                get_field(cls, model, (\"prepopulated_fields['%s'][%d]\" % (field, idx)), f)\n", "label": 0}
{"function": "\n\ndef suggest(self):\n    pattern = QtCore.QRegExp('\\\\w+$')\n    cursor = self._editor.textCursor()\n    block = cursor.block()\n    text = block.text()\n    if ((not self._room) or (not self._room.users) or (not text) or cursor.hasSelection()):\n        return False\n    blockText = QtCore.QString(text[:(cursor.position() - block.position())])\n    matchPosition = blockText.indexOf(pattern)\n    if (matchPosition < 0):\n        return False\n    word = blockText[matchPosition:]\n    if word.trimmed().isEmpty():\n        return False\n    matchingUserNames = []\n    for user in self._room.users:\n        if QtCore.QString(user['name']).startsWith(word, QtCore.Qt.CaseInsensitive):\n            matchingUserNames.append(user['name'])\n    if (len(matchingUserNames) == 1):\n        self._replace(cursor, word, (matchingUserNames[0] + (': ' if (matchPosition == 0) else ' ')))\n    else:\n        menu = QtGui.QMenu('Suggestions', self._editor)\n        for userName in matchingUserNames:\n            action = QtGui.QAction(userName, menu)\n            action.setData((cursor, word, userName, matchPosition))\n            self.connect(action, QtCore.SIGNAL('triggered()'), self._userSelected)\n            menu.addAction(action)\n        menu.popup(self._editor.mapToGlobal(self._editor.cursorRect().center()))\n", "label": 1}
{"function": "\n\n@classmethod\ndef invalidate(cls, region):\n    'Invalidate an entire region\\n\\n        .. note::\\n\\n            This does not actually *clear* the region of data, but\\n            just sets the value to expire on next access.\\n\\n        :param region: Region name\\n        :type region: string\\n\\n        '\n    redis = global_connection.redis\n    namespaces = redis.smembers(('retools:%s:namespaces' % region))\n    if (not namespaces):\n        return None\n    longest_expire = max([x['expires'] for x in CacheRegion.regions.values()])\n    new_created = ((time.time() - longest_expire) - 3600)\n    for ns in namespaces:\n        cache_keyset_key = ('retools:%s:%s:keys' % (region, ns))\n        keys = (set(['']) | redis.smembers(cache_keyset_key))\n        for key in keys:\n            cache_key = ('retools:%s:%s:%s' % (region, ns, key))\n            if (not redis.exists(cache_key)):\n                redis.srem(cache_keyset_key, key)\n            else:\n                redis.hset(cache_key, 'created', new_created)\n", "label": 0}
{"function": "\n\n@register.simple_tag(takes_context=True)\ndef render_paginator(context, page, page_var='page', hashtag=''):\n    query_dict = context['request'].GET.copy()\n    try:\n        del query_dict[page_var]\n    except KeyError:\n        pass\n    extra_query = ''\n    if query_dict:\n        extra_query = ('&%s' % query_dict.urlencode())\n    if hashtag:\n        hashtag = ('#%s' % hashtag)\n    new_context = {\n        'page': page,\n        'page_var': page_var,\n        'hashtag': hashtag,\n        'extra_query': extra_query,\n    }\n    if isinstance(page, Page):\n        template = 'spirit/utils/paginator/_paginator.html'\n    else:\n        template = 'spirit/utils/paginator/_yt_paginator.html'\n    return render_to_string(template, new_context)\n", "label": 0}
{"function": "\n\ndef test_option():\n    kernel = get_kernel()\n    d = Dummy(kernel)\n    assert ('Options:' in d.line_dummy.__doc__)\n    assert ('--size' in d.line_dummy.__doc__)\n    ret = d.call_magic('line', 'dummy', '', 'hey -s400,200')\n    assert (ret == d)\n    assert (d.foo == 'hey'), d.foo\n    assert (d.size == (400, 200))\n    ret = d.call_magic('line', 'dummy', '', 'hey there')\n    assert (d.foo == 'hey there')\n    ret = d.call_magic('line', 'dummy', '', 'range(1, 10)')\n    assert (d.foo == range(1, 10))\n    ret = d.call_magic('line', 'dummy', '', '[1, 2, 3]')\n    assert (d.foo == [1, 2, 3])\n    ret = d.call_magic('line', 'dummy', '', 'hey -l -s400,200')\n    assert (d.size == (400, 200))\n    assert (d.foo == 'hey -l')\n    ret = d.call_magic('line', 'dummy', '', 'hey -s -- -s400,200')\n    assert (d.size == (400, 200))\n    assert (d.foo == 'hey -s')\n", "label": 1}
{"function": "\n\ndef encodeFiles(array):\n    IDfile = readIDfile()\n    dictionary = readDictionaryFile()\n    for thisfile in array:\n        try:\n            input = pickle.load(open(thisfile))\n        except ValueError:\n            logging.warn(((('unable to unpicked ' + thisfile) + '... dangerously just ') + 'skipping, some texts may be lost'))\n            continue\n        except:\n            logging.warn((('Some problem: fix if ' + thisfile) + ' should be unpicklable'))\n            continue\n        for level in input.levels:\n            input.encode(level, IDfile, dictionary)\n", "label": 0}
{"function": "\n\ndef LoadServerCertificate(self, server_certificate=None, ca_certificate=None):\n    'Loads and verifies the server certificate.'\n    try:\n        server_cert = X509.load_cert_string(str(server_certificate))\n        ca_cert = X509.load_cert_string(str(ca_certificate))\n        if (server_cert.verify(ca_cert.get_pubkey()) != 1):\n            self.server_name = None\n            raise IOError('Server cert is invalid.')\n        server_cert_serial = server_cert.get_serial_number()\n        if (server_cert_serial < config_lib.CONFIG['Client.server_serial_number']):\n            raise IOError('Server cert is too old.')\n        elif (server_cert_serial > config_lib.CONFIG['Client.server_serial_number']):\n            logging.info('Server serial number updated to %s', server_cert_serial)\n            config_lib.CONFIG.Set('Client.server_serial_number', server_cert_serial)\n            config_lib.CONFIG.Write()\n    except X509.X509Error:\n        raise IOError('Server cert is invalid.')\n    self.server_name = self.pub_key_cache.GetCNFromCert(server_cert)\n    self.server_certificate = server_certificate\n    self.ca_certificate = ca_certificate\n    self.pub_key_cache.Put(self.server_name, self.pub_key_cache.PubKeyFromCert(server_cert))\n", "label": 0}
{"function": "\n\ndef request(url, post=None, headers=None, mobile=False, safe=False, timeout='30'):\n    try:\n        control.log(('[cloudflare] request %s' % url))\n        try:\n            headers.update(headers)\n        except:\n            headers = {\n                \n            }\n        agent = cache.get(cloudflareAgent, 168)\n        if (not ('User-Agent' in headers)):\n            headers['User-Agent'] = agent\n        u = ('%s://%s' % (urlparse.urlparse(url).scheme, urlparse.urlparse(url).netloc))\n        cookie = cache.get(cloudflareCookie, 168, u, post, headers, mobile, safe, timeout)\n        result = client.request(url, cookie=cookie, post=post, headers=headers, mobile=mobile, safe=safe, timeout=timeout, output='response', error=True)\n        if (result[0] == '503'):\n            agent = cache.get(cloudflareAgent, 0)\n            headers['User-Agent'] = agent\n            cookie = cache.get(cloudflareCookie, 0, u, post, headers, mobile, safe, timeout)\n            result = client.request(url, cookie=cookie, post=post, headers=headers, mobile=mobile, safe=safe, timeout=timeout)\n        else:\n            result = result[1]\n        return result\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef __protect__(self, key, value=sentinel):\n    'Protected keys add its parents, not sure if useful'\n    if (not isinstance(key, list)):\n        key = (key.split('.') if isinstance(key, basestring) else [key])\n    (key, path) = (key.pop(0), key)\n    if (len(path) > 0):\n        self.get(key).protect(path, value)\n    elif (value is not sentinel):\n        self[key] = value\n    if (key not in self):\n        raise KeyError(('key %s has no value to protect' % key))\n    self.__PROTECTED__.add(key)\n", "label": 0}
{"function": "\n\ndef decode(self, request, value):\n    value = int(value)\n    if (((self.min is not None) and (value < self.min)) or ((self.max is not None) and (value > self.max))):\n        raise ValueError(('Value %d is out of range.' % value))\n    return value\n", "label": 0}
{"function": "\n\ndef __init__(self, dictionary):\n    self.championId = dictionary.get('championId', 0)\n    self.createDate = dictionary.get('createDate', 0)\n    self.fellowPlayers = [(Player(player) if (not isinstance(player, Player)) else player) for player in dictionary.get('fellowPlayers', []) if player]\n    self.gameId = dictionary.get('gameId', 0)\n    self.gameMode = dictionary.get('gameMode', '')\n    self.gameType = dictionary.get('gameType', '')\n    self.invalid = dictionary.get('invalid', False)\n    self.ipEarned = dictionary.get('ipEarned', 0)\n    self.level = dictionary.get('level', 0)\n    self.mapId = dictionary.get('mapId', 0)\n    self.spell1 = dictionary.get('spell1', 0)\n    self.spell2 = dictionary.get('spell2', 0)\n    val = dictionary.get('stats', None)\n    self.stats = (RawStats(val) if (val and (not isinstance(val, RawStats))) else val)\n    self.subType = dictionary.get('subType', '')\n    self.teamId = dictionary.get('teamId', 0)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef from_line(line):\n    line = line.rstrip()\n    fields = line.split('\\t')\n    (contig, start, stop) = (fields[0], int(fields[1]), int(fields[2]))\n    n = len(fields)\n    name = ((fields[3] or None) if (n >= 4) else None)\n    score = ((fields[4] or None) if (n >= 5) else None)\n    strand = ((fields[5] or None) if (n >= 6) else None)\n    thick_start = ((fields[6] or None) if (n >= 7) else None)\n    thick_end = ((fields[7] or None) if (n >= 8) else None)\n    item_rgb = ((fields[8] or None) if (n >= 9) else None)\n    return BedRecord(contig, start, stop, name, score, strand, thick_start, thick_end, item_rgb)\n", "label": 1}
{"function": "\n\ndef _query_metadata_proxy(self, machine):\n    url = ('http://%(host)s:%(port)s' % {\n        'host': dhcp.METADATA_DEFAULT_IP,\n        'port': dhcp.METADATA_PORT,\n    })\n    cmd = ('curl', '--max-time', METADATA_REQUEST_TIMEOUT, '-D-', url)\n    i = 0\n    CONNECTION_REFUSED_TIMEOUT = (METADATA_REQUEST_TIMEOUT // 2)\n    while (i <= CONNECTION_REFUSED_TIMEOUT):\n        try:\n            raw_headers = machine.execute(cmd)\n            break\n        except RuntimeError as e:\n            if ('Connection refused' in str(e)):\n                time.sleep(METADATA_REQUEST_SLEEP)\n                i += METADATA_REQUEST_SLEEP\n            else:\n                self.fail(('metadata proxy unreachable on %s before timeout' % url))\n    if (i > CONNECTION_REFUSED_TIMEOUT):\n        self.fail('Timed out waiting metadata proxy to become available')\n    return raw_headers.splitlines()[0]\n", "label": 0}
{"function": "\n\ndef get(self, resource):\n    '\\n        Get a resource into the cache,\\n\\n        :param resource: A :class:`Resource` instance.\\n        :return: The pathname of the resource in the cache.\\n        '\n    (prefix, path) = resource.finder.get_cache_info(resource)\n    if (prefix is None):\n        result = path\n    else:\n        result = os.path.join(self.base, self.prefix_to_dir(prefix), path)\n        dirname = os.path.dirname(result)\n        if (not os.path.isdir(dirname)):\n            os.makedirs(dirname)\n        if (not os.path.exists(result)):\n            stale = True\n        else:\n            stale = self.is_stale(resource, path)\n        if stale:\n            with open(result, 'wb') as f:\n                f.write(resource.bytes)\n    return result\n", "label": 0}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_freq():\n    ' reading/writing of 3D RNMRTK frequency domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_3d', 'freq_3d.sec'))\n    assert (data.shape == (128, 128, 4096))\n    assert (np.abs((data[(0, 1, 2)] - 3.23)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)] - 1.16)) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\n@classmethod\ndef all(cls, session, page_size=1000, order_by=None):\n    offset = 0\n    order_by = (order_by or cls.id)\n    while True:\n        page = cls.find(session, order_by=order_by, limit=page_size, offset=offset)\n        for m in page:\n            (yield m)\n        session.flush()\n        if (len(page) != page_size):\n            raise StopIteration()\n        offset += page_size\n", "label": 0}
{"function": "\n\ndef _digest(self, alg, password, salt=None):\n    \"\\n        Helper method to perform the password digest.\\n\\n        :param alg: The hash algorithm to use.\\n        :type alg: str - 'sha512' | 'bcrypt'\\n        :param password: The password to digest.\\n        :type password: str\\n        :param salt: The salt to use. In the case of bcrypt,\\n                     when storing the password, pass None;\\n                     when testing the password, pass the hashed value.\\n        :type salt: None or str\\n        :returns: The hashed value as a string.\\n        \"\n    cur_config = config.getConfig()\n    if (alg == 'sha512'):\n        return hashlib.sha512((password + salt).encode('utf8')).hexdigest()\n    elif (alg == 'bcrypt'):\n        try:\n            import bcrypt\n        except ImportError:\n            raise Exception('Bcrypt module is not installed. See girder.local.cfg.')\n        password = password.encode('utf8')\n        if (salt is None):\n            rounds = int(cur_config['auth']['bcrypt_rounds'])\n            return bcrypt.hashpw(password, bcrypt.gensalt(rounds))\n        else:\n            if isinstance(salt, six.text_type):\n                salt = salt.encode('utf8')\n            return bcrypt.hashpw(password, salt)\n    else:\n        raise Exception(('Unsupported hash algorithm: %s' % alg))\n", "label": 0}
{"function": "\n\ndef getOrCreateUser(self, name, token=''):\n    self.lock.acquire()\n    userIndex = (- 1)\n    for i in self.getActiveUserIndexes():\n        if (self.arrayOfUsers[i]._name == name):\n            userIndex = i\n    if (userIndex < 0):\n        userIndex = self.arrayOfUsers.size()\n        self.arrayOfUsers.add(UserEntry(userIndex, (userIndex - self.deletedUserCount), name, token))\n        for roleIndex in self.getActiveRoleIndexes():\n            self.arrayOfUsers[userIndex].addRoleByIndex(roleIndex)\n    self.lock.release()\n    return userIndex\n", "label": 0}
{"function": "\n\ndef apply(self, obj, defaults={\n    \n}, **kwargs):\n    ': Apply this style to the given object using the supplied defaults.\\n\\n      = NOTE\\n      - This can apply to any matplotlib Text.\\n\\n      = INPUT VARIABLES\\n      - obj       The object to apply the style to.\\n      - defaults  Keyword-value dictionary with defaults values to use if a\\n                  property value is not specified.\\n      - kwargs    Keyword-value dictionary whose values will supercede\\n                  any values set by the properties of this sub-style.\\n      '\n    if (not isinstance(obj, mpltext.Text)):\n        msg = (\"Unable to apply this sub-style to the given element.Expected a matplotlib 'Text' and instead received the following:\\n%s\" % (obj,))\n        raise Exception(msg)\n    properties = {\n        'bgColor': 'backgroundcolor',\n        'fgColor': 'color',\n        'vertAlign': 'verticalalignment',\n        'horizAlign': 'horizontalalignment',\n        'multiAlign': 'multialignment',\n        'lineSpacing': 'linespacing',\n        'rotation': 'rotation',\n    }\n    subKwargs = kwargs.get('font', {\n        \n    })\n    subDefaults = S.lib.resolveDefaults(defaults, ['font'])\n    self.font.apply(obj.get_font_properties(), subDefaults, **subKwargs)\n    MplArtistStyle.apply(self, obj, defaults, **kwargs)\n    kw = {\n        \n    }\n    for p in properties:\n        mplProp = properties[p]\n        value = self.getValue(p, defaults, **kwargs)\n        if (value is not None):\n            kw[mplProp] = value\n    if kw:\n        obj.update(kw)\n", "label": 0}
{"function": "\n\ndef onBrowserEvent(self, event):\n    Widget.onBrowserEvent(self, event)\n    type = DOM.eventGetType(event)\n    if (type == 'load'):\n        for listener in self.loadListeners:\n            listener.onLoad(self)\n    elif (type == 'error'):\n        for listener in self.loadListeners:\n            listener.onError(self)\n", "label": 0}
{"function": "\n\n@classmethod\ndef create_user(cls, name, email, password, email_verified=True):\n    'Create (and save) a new user with the given password and\\n        email address.\\n        '\n    now = datetime.datetime.utcnow()\n    try:\n        (email_name, domain_part) = email.strip().split('@', 1)\n    except ValueError:\n        pass\n    else:\n        email = '@'.join([email_name.lower(), domain_part.lower()])\n    user = User(name=name, email=email, date_joined=now)\n    if (not password):\n        password = generate_password()\n    user.set_password(password)\n    if (not email_verified):\n        user.mark_email_for_activation()\n    else:\n        user.is_email_activated = True\n    user.save()\n    return user\n", "label": 0}
{"function": "\n\ndef test_set_reuse_addr(self):\n    if (HAS_UNIX_SOCKETS and (self.family == socket.AF_UNIX)):\n        self.skipTest('Not applicable to AF_UNIX sockets.')\n    sock = socket.socket(self.family)\n    try:\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    except socket.error:\n        unittest.skip('SO_REUSEADDR not supported on this platform')\n    else:\n        s = asyncore.dispatcher(socket.socket(self.family))\n        self.assertFalse(s.socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR))\n        s.socket.close()\n        s.create_socket(self.family)\n        s.set_reuse_addr()\n        self.assertTrue(s.socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR))\n    finally:\n        sock.close()\n", "label": 0}
{"function": "\n\ndef validate_email(self, email):\n    if (not email):\n        return (email, 'The e-mail is empty.')\n    parts = email.split('@')\n    if (len(parts) != 2):\n        return (email, 'An email address must contain a single @')\n    (local, domain) = parts\n    (domain, err) = self.validate_domain(domain)\n    if err:\n        return (email, ('The e-mail has a problem to the right of the @: %s' % err))\n    (local, err) = self.validate_local_part(local)\n    if err:\n        return (email, ('The email has a problem to the left of the @: %s' % err))\n    return (((local + '@') + domain), '')\n", "label": 0}
{"function": "\n\ndef write(self, data, escape=True):\n    if (not escape):\n        self.body.write(str(data))\n    elif (hasattr(data, 'xml') and callable(data.xml)):\n        self.body.write(data.xml())\n    else:\n        if (not isinstance(data, (str, unicode))):\n            data = str(data)\n        elif isinstance(data, unicode):\n            data = data.encode('utf8', 'xmlcharrefreplace')\n        data = cgi.escape(data, True).replace(\"'\", '&#x27;')\n        self.body.write(data)\n", "label": 0}
{"function": "\n\ndef __call__(self, event, sep=os.path.sep, join=os.path.join):\n    '\\n        Handle event and print filename, line number and source code. If event.kind is a `return` or `exception` also\\n        prints values.\\n        '\n    lines = self._safe_source(event)\n    thread_name = (threading.current_thread().name if event.tracer.threading_support else '')\n    thread_align = (self.thread_alignment if event.tracer.threading_support else '')\n    self.stream.write('{thread:{thread_align}}{filename}{:>{align}}{colon}:{lineno}{:<5} {kind}{:9} {code}{}{reset}\\n'.format(self._format_filename(event), event.lineno, event.kind, lines[0], thread=thread_name, thread_align=thread_align, align=self.filename_alignment, code=self.code_colors[event.kind], **self.event_colors))\n    for line in lines[1:]:\n        self.stream.write('{thread:{thread_align}}{:>{align}}       {kind}{:9} {code}{}{reset}\\n'.format('', '   |', line, thread=thread_name, thread_align=thread_align, align=self.filename_alignment, code=self.code_colors[event.kind], **self.event_colors))\n    if (event.kind in ('return', 'exception')):\n        self.stream.write('{thread:{thread_align}}{:>{align}}       {continuation}{:9} {color}{} value: {detail}{}{reset}\\n'.format('', '...', event.kind, self._safe_repr(event.arg), thread=thread_name, thread_align=thread_align, align=self.filename_alignment, color=self.event_colors[event.kind], **self.event_colors))\n", "label": 0}
{"function": "\n\ndef _ensure_path(self):\n    result = self.client.ensure_path(self.path)\n    self.assured_path = True\n    if (result is True):\n        (data, _) = self.client.get(self.path)\n        try:\n            leases = int(data.decode('utf-8'))\n        except (ValueError, TypeError):\n            pass\n        else:\n            if (leases != self.max_leases):\n                raise ValueError(('Inconsistent max leases: %s, expected: %s' % (leases, self.max_leases)))\n    else:\n        self.client.set(self.path, str(self.max_leases).encode('utf-8'))\n", "label": 0}
{"function": "\n\ndef test_eval_size_zero(self, TrainSplit, nn):\n    (X, y) = (np.random.random((100, 10)), np.repeat([0, 1, 2, 3], 25))\n    (X_train, X_valid, y_train, y_valid) = TrainSplit(0.0)(X, y, nn)\n    assert (len(X_train) == len(X))\n    assert (len(y_train) == len(y))\n    assert (len(X_valid) == 0)\n    assert (len(y_valid) == 0)\n", "label": 0}
{"function": "\n\ndef get_fields_to_translatable_models(model):\n    if (model in _F2TM_CACHE):\n        return _F2TM_CACHE[model]\n    results = []\n    if NEW_META_API:\n        for f in model._meta.get_fields():\n            if (f.is_relation and f.related_model):\n                if (get_translatable_fields_for_model(f.related_model) is not None):\n                    results.append((f.name, f.related_model))\n    else:\n        for field_name in model._meta.get_all_field_names():\n            (field_object, modelclass, direct, m2m) = model._meta.get_field_by_name(field_name)\n            if (direct and isinstance(field_object, RelatedField)):\n                if (get_translatable_fields_for_model(field_object.related.parent_model) is not None):\n                    results.append((field_name, field_object.related.parent_model))\n            if isinstance(field_object, RelatedObject):\n                if (get_translatable_fields_for_model(field_object.model) is not None):\n                    results.append((field_name, field_object.model))\n    _F2TM_CACHE[model] = dict(results)\n    return _F2TM_CACHE[model]\n", "label": 1}
{"function": "\n\ndef compile_template(func):\n    spec = inspect.getargspec(func)\n    assert (len(spec.args) == len((spec.defaults or []))), 'All template args should have AST classes'\n    compiler = TemplateCompiler(zipdict(spec.args, (spec.defaults or [])))\n    template = map(compiler.visit, get_body_ast(func))\n    if ((len(template) == 1) and isinstance(template[0], ast.Expr)):\n        return template[0].value\n    return template\n", "label": 0}
{"function": "\n\ndef match_or_trust(self, host, der_encoded_certificate):\n    base64_encoded_certificate = b64encode(der_encoded_certificate)\n    if isfile(self.path):\n        with open(self.path) as f_in:\n            for line in f_in:\n                (known_host, _, known_cert) = line.strip().partition(':')\n                known_cert = known_cert.encode('utf-8')\n                if (host == known_host):\n                    return (base64_encoded_certificate == known_cert)\n    try:\n        makedirs(dirname(self.path))\n    except OSError:\n        pass\n    f_out = os_open(self.path, ((O_CREAT | O_APPEND) | O_WRONLY), 384)\n    if isinstance(host, bytes):\n        os_write(f_out, host)\n    else:\n        os_write(f_out, host.encode('utf-8'))\n    os_write(f_out, b':')\n    os_write(f_out, base64_encoded_certificate)\n    os_write(f_out, b'\\n')\n    os_close(f_out)\n    return True\n", "label": 0}
{"function": "\n\ndef get_fqhostname():\n    '\\n    Returns the fully qualified hostname\\n    '\n    l = []\n    l.append(socket.getfqdn())\n    try:\n        addrinfo = socket.getaddrinfo(socket.gethostname(), 0, socket.AF_UNSPEC, socket.SOCK_STREAM, socket.SOL_TCP, socket.AI_CANONNAME)\n        for info in addrinfo:\n            if (len(info) >= 4):\n                l.append(info[3])\n    except socket.gaierror:\n        pass\n    l = _sort_hostnames(l)\n    if (len(l) > 0):\n        return l[0]\n    return None\n", "label": 0}
{"function": "\n\ndef _live_receivers(self, senderkey):\n    '\\n        Filter sequence of receivers to get resolved, live receivers.\\n\\n        This checks for weak references and resolves them, then returning only\\n        live receivers.\\n        '\n    none_senderkey = _make_id(None)\n    receivers = []\n    for ((receiverkey, r_senderkey), receiver) in self.receivers:\n        if ((r_senderkey == none_senderkey) or (r_senderkey == senderkey)):\n            if isinstance(receiver, WEAKREF_TYPES):\n                receiver = receiver()\n                if (receiver is not None):\n                    receivers.append(receiver)\n            else:\n                receivers.append(receiver)\n    return receivers\n", "label": 0}
{"function": "\n\ndef parse_date(string, locale=LC_TIME):\n    \"Parse a date from a string.\\n\\n    This function uses the date format for the locale as a hint to determine\\n    the order in which the date fields appear in the string.\\n\\n    >>> parse_date('4/1/04', locale='en_US')\\n    datetime.date(2004, 4, 1)\\n    >>> parse_date('01.04.2004', locale='de_DE')\\n    datetime.date(2004, 4, 1)\\n\\n    :param string: the string containing the date\\n    :param locale: a `Locale` object or a locale identifier\\n    \"\n    format = get_date_format(locale=locale).pattern.lower()\n    year_idx = format.index('y')\n    month_idx = format.index('m')\n    if (month_idx < 0):\n        month_idx = format.index('l')\n    day_idx = format.index('d')\n    indexes = [(year_idx, 'Y'), (month_idx, 'M'), (day_idx, 'D')]\n    indexes.sort()\n    indexes = dict([(item[1], idx) for (idx, item) in enumerate(indexes)])\n    numbers = re.findall('(\\\\d+)', string)\n    year = numbers[indexes['Y']]\n    if (len(year) == 2):\n        year = (2000 + int(year))\n    else:\n        year = int(year)\n    month = int(numbers[indexes['M']])\n    day = int(numbers[indexes['D']])\n    if (month > 12):\n        (month, day) = (day, month)\n    return date(year, month, day)\n", "label": 0}
{"function": "\n\ndef msvc_exists():\n    ' Determine whether MSVC is available on the machine.\\n    '\n    result = 0\n    try:\n        p = subprocess.Popen(['cl'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        str_result = p.stdout.read()\n        if ('Microsoft' in str_result):\n            result = 1\n    except:\n        import distutils.msvccompiler\n        try:\n            version = distutils.msvccompiler.get_devstudio_versions()\n        except:\n            version = distutils.msvccompiler.get_build_version()\n        if version:\n            result = 1\n    return result\n", "label": 0}
{"function": "\n\ndef level_matches(self, level, consumer_level):\n    '\\n        >>> l = Logger([])\\n        >>> l.level_matches(3, 4)\\n        False\\n        >>> l.level_matches(3, 2)\\n        True\\n        >>> l.level_matches(slice(None, 3), 3)\\n        False\\n        >>> l.level_matches(slice(None, 3), 2)\\n        True\\n        >>> l.level_matches(slice(1, 3), 1)\\n        True\\n        >>> l.level_matches(slice(2, 3), 1)\\n        False\\n        '\n    if isinstance(level, slice):\n        (start, stop) = (level.start, level.stop)\n        if ((start is not None) and (start > consumer_level)):\n            return False\n        if ((stop is not None) and (stop <= consumer_level)):\n            return False\n        return True\n    else:\n        return (level >= consumer_level)\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('PacketCaptureException')\n    if (self.message is not None):\n        oprot.writeFieldBegin('message', TType.STRING, 1)\n        oprot.writeString(self.message)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.history == other.history) and (self.channels == other.channels) and (self.supported == other.supported) and (self.nicksToHostmasks == other.nicksToHostmasks) and (self.batches == other.batches))\n", "label": 0}
{"function": "\n\n@content\ndef PUT(self):\n    ':returns: node id.\\n\\n        :http: * 200 (node are successfully updated)\\n               * 304 (node data not changed since last request)\\n               * 400 (data validation failed)\\n               * 404 (node not found)\\n        '\n    nd = self.checked_data(self.validator.validate_update, data=web.data())\n    node = self.collection.single.get_by_meta(nd)\n    if (not node):\n        raise self.http(404, \"Can't find node: {0}\".format(nd))\n    node.timestamp = datetime.now()\n    if (not node.online):\n        node.online = True\n        msg = \"Node '{0}' is back online\".format(node.human_readable_name)\n        logger.info(msg)\n        notifier.notify('discover', msg, node_id=node.id)\n    db().flush()\n    if (('agent_checksum' in nd) and (node.agent_checksum == nd['agent_checksum'])):\n        return {\n            'id': node.id,\n            'cached': True,\n        }\n    self.collection.single.update_by_agent(node, nd)\n    return {\n        'id': node.id,\n    }\n", "label": 0}
{"function": "\n\ndef boundary_edges(polys):\n    'Returns the edges that are on the boundary of a mesh, as defined by belonging to only 1 face'\n    edges = dict()\n    for (i, poly) in enumerate(np.sort(polys)):\n        for (a, b) in [(0, 1), (1, 2), (0, 2)]:\n            key = (poly[a], poly[b])\n            if (key not in edges):\n                edges[key] = []\n            edges[key].append(i)\n    epts = []\n    for (edge, faces) in edges.items():\n        if (len(faces) == 1):\n            epts.append(edge)\n    return np.array(epts)\n", "label": 0}
{"function": "\n\ndef serialize(self, previous):\n    params = {\n        \n    }\n    unsaved_keys = (self._unsaved_values or set())\n    previous = (previous or self._previous or {\n        \n    })\n    for (k, v) in self.items():\n        if ((k == 'id') or (isinstance(k, str) and k.startswith('_'))):\n            continue\n        elif isinstance(v, APIResource):\n            continue\n        elif hasattr(v, 'serialize'):\n            params[k] = v.serialize(previous.get(k, None))\n        elif (k in unsaved_keys):\n            params[k] = _compute_diff(v, previous.get(k, None))\n        elif ((k == 'additional_owners') and (v is not None)):\n            params[k] = _serialize_list(v, previous.get(k, None))\n    return params\n", "label": 1}
{"function": "\n\ndef make_trace_rows(trace):\n    if (not trace.events):\n        return []\n    rows = [[trace.request_type, str(datetime_from_utc_to_local(trace.started_at)), trace.coordinator, 0]]\n    for event in trace.events:\n        rows.append([('%s [%s]' % (event.description, event.thread_name)), str(datetime_from_utc_to_local(event.datetime)), event.source, (event.source_elapsed.microseconds if event.source_elapsed else '--')])\n    if trace.duration:\n        finished_at = (datetime_from_utc_to_local(trace.started_at) + trace.duration)\n    else:\n        finished_at = trace.duration = '--'\n    rows.append(['Request complete', str(finished_at), trace.coordinator, trace.duration.microseconds])\n    return rows\n", "label": 0}
{"function": "\n\ndef has_changes(self):\n    if self.is_dirty:\n        return True\n    for child in self._db_actions:\n        if child.has_changes():\n            return True\n    for child in self._db_tags:\n        if child.has_changes():\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef test_replacements_in_network_assignments(self):\n    node_roles_vs_net_names = [(['controller'], ['public', 'management', 'fuelweb_admin']), (['compute', 'cinder'], ['storage', 'management', 'fuelweb_admin'])]\n    template_meta = self.net_template['adv_net_template']['default']\n    iface_var = template_meta['nic_mapping']['default'].keys()[0]\n    ep_with_var = '<% {0} %>.123'.format(iface_var)\n    template_meta['network_assignments']['storage']['ep'] = ep_with_var\n    template_meta['network_scheme']['storage']['endpoints'] = [ep_with_var]\n    objects.Cluster.set_network_template(self.cluster, self.net_template)\n    cluster_db = objects.Cluster.get_by_uid(self.cluster['id'])\n    objects.Cluster.prepare_for_deployment(cluster_db)\n    serializer = get_serializer_for_cluster(self.cluster)\n    serialized_for_astute = serializer(AstuteGraph(cluster_db)).serialize(self.cluster, cluster_db.nodes)\n    for node_data in serialized_for_astute:\n        node = objects.Node.get_by_uid(node_data['uid'])\n        for (node_roles, net_names) in node_roles_vs_net_names:\n            if (node.all_roles == set(node_roles)):\n                self.check_node_ips_on_certain_networks(node, net_names)\n                break\n        else:\n            self.fail('Unexpected combination of node roles: {0}'.format(node.all_roles))\n", "label": 0}
{"function": "\n\ndef unpack_args(self, arg, kwarg_name, kwargs):\n    try:\n        new_args = kwargs[kwarg_name]\n        if (not isinstance(new_args, list)):\n            new_args = [new_args]\n        for i in new_args:\n            if (not isinstance(i, str)):\n                raise MesonException('html_args values must be strings.')\n    except KeyError:\n        return []\n    if (len(new_args) > 0):\n        return [(arg + '@@'.join(new_args))]\n    return []\n", "label": 0}
{"function": "\n\ndef str_replace(arr, pat, repl, n=(- 1), case=True, flags=0):\n    '\\n    Replace occurrences of pattern/regex in the Series/Index with\\n    some other string. Equivalent to :meth:`str.replace` or\\n    :func:`re.sub`.\\n\\n    Parameters\\n    ----------\\n    pat : string\\n        Character sequence or regular expression\\n    repl : string\\n        Replacement sequence\\n    n : int, default -1 (all)\\n        Number of replacements to make from start\\n    case : boolean, default True\\n        If True, case sensitive\\n    flags : int, default 0 (no flags)\\n        re module flags, e.g. re.IGNORECASE\\n\\n    Returns\\n    -------\\n    replaced : Series/Index of objects\\n    '\n    use_re = ((not case) or (len(pat) > 1) or flags)\n    if use_re:\n        if (not case):\n            flags |= re.IGNORECASE\n        regex = re.compile(pat, flags=flags)\n        n = (n if (n >= 0) else 0)\n\n        def f(x):\n            return regex.sub(repl, x, count=n)\n    else:\n        f = (lambda x: x.replace(pat, repl, n))\n    return _na_map(f, arr)\n", "label": 0}
{"function": "\n\ndef _expand_node(self, nid):\n    ' Expands the contents of a specified node (if required).\\n        '\n    (expanded, node, object) = self._get_node_data(nid)\n    if (not expanded):\n        dummy = getattr(nid, '_dummy', None)\n        if (dummy is not None):\n            nid.removeChild(dummy)\n            del nid._dummy\n        for child in node.get_children(object):\n            (child, child_node) = self._node_for(child)\n            if (child_node is not None):\n                self._append_node(nid, child_node, child)\n        self._set_node_data(nid, (True, node, object))\n", "label": 0}
{"function": "\n\ndef login(username, password):\n    verify_to_schema(UserLoginSchema, {\n        'username': username,\n        'password': password,\n    })\n    user = api.user.get_user(username_lower=username.lower())\n    if (user is None):\n        raise WebException('No user with that username exists!')\n    if user.get('disabled', False):\n        raise WebException('This account is disabled.')\n    if confirm_password(password, user['password']):\n        if (user['uid'] is not None):\n            session['uid'] = user['uid']\n            if (user['type'] == 0):\n                session['admin'] = True\n            session.permanent = True\n        else:\n            raise WebException('Login error. Error code: 1.')\n    else:\n        raise WebException('Wrong password.')\n", "label": 0}
{"function": "\n\ndef test_variable_access_before_setup(self):\n    prob = Problem(root=ExampleGroup())\n    try:\n        prob['G2.C1.x'] = 5.0\n    except AttributeError as err:\n        msg = \"'unknowns' has not been initialized, setup() must be called before 'G2.C1.x' can be accessed\"\n        self.assertEqual(text_type(err), msg)\n    else:\n        self.fail('Exception expected')\n    try:\n        prob.run()\n    except RuntimeError as err:\n        msg = 'setup() must be called before running the model.'\n        self.assertEqual(text_type(err), msg)\n    else:\n        self.fail('Exception expected')\n", "label": 0}
{"function": "\n\ndef get_columns(self, with_aliases=False):\n    '\\n        Remove table names and strip quotes from column names.\\n        '\n    soql_trans = self.query_topology()\n    (cols, col_params) = compiler.SQLCompiler.get_columns(self, with_aliases)\n    out = []\n    for col in cols:\n        if (soql_trans and re.match('^\\\\w+\\\\.\\\\w+$', col)):\n            (tab_name, col_name) = col.split('.')\n            out.append(('%s.%s' % (soql_trans[tab_name], col_name)))\n        else:\n            out.append(col)\n    cols = out\n    result = [x.replace(' AS ', ' ') for x in cols]\n    return (result, col_params)\n", "label": 0}
{"function": "\n\ndef __contains__(self, taskid):\n    if ((taskid in self.priority_queue) or (taskid in self.time_queue)):\n        return True\n    if ((taskid in self.processing) and self.processing[taskid].taskid):\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef _register_callbacks(self):\n    zk = self._zk_util._zk\n    path_for_brokers = self._zk_util.path_for_brokers()\n    path_for_topic = self._zk_util.path_for_topic(self.topic)\n    if ((self._brokers_watch is None) and zk.exists(path_for_brokers)):\n        self._brokers_watch = zk.children(path_for_brokers)(self._unbalance)\n    if ((self._topic_watch is None) and zk.exists(path_for_topic)):\n        self._topic_watch = zk.children(path_for_topic)(self._unbalance)\n    log.debug('Producer {0} has watches: {1}'.format(self._id, sorted(zk.watches.data.keys())))\n", "label": 0}
{"function": "\n\ndef random_interface(dut, exclude=None):\n    exclude = ([] if (exclude is None) else exclude)\n    interfaces = dut.api('interfaces')\n    names = [name for name in list(interfaces.keys()) if name.startswith('Et')]\n    exclude_interfaces = dut.settings.get('exclude_interfaces', [])\n    if exclude_interfaces:\n        exclude_interfaces = exclude_interfaces.split(',')\n    exclude_interfaces.extend(exclude)\n    if (sorted(exclude_interfaces) == sorted(names)):\n        raise TypeError('unable to allocate interface from dut')\n    choices = set(names).difference(exclude)\n    return random.choice(list(choices))\n", "label": 0}
{"function": "\n\ndef test_independent_generators(self):\n    N = 10\n    random_seed(1)\n    py_numbers = [random_random() for i in range(N)]\n    numpy_seed(2)\n    np_numbers = [numpy_random() for i in range(N)]\n    random_seed(1)\n    numpy_seed(2)\n    pairs = [(random_random(), numpy_random()) for i in range(N)]\n    self.assertPreciseEqual([p[0] for p in pairs], py_numbers)\n    self.assertPreciseEqual([p[1] for p in pairs], np_numbers)\n", "label": 0}
{"function": "\n\ndef write_image(results, output_filename=None):\n    print(('Gathered %s results that represents a mandelbrot image (using %s chunks that are computed jointly by %s workers).' % (len(results), CHUNK_COUNT, WORKERS)))\n    if (not output_filename):\n        return\n    try:\n        from PIL import Image\n    except ImportError as e:\n        raise RuntimeError(('Pillow is required to write image files: %s' % e))\n    color_max = 0\n    for (_point, color) in results:\n        color_max = max(color, color_max)\n    img = Image.new('L', IMAGE_SIZE, 'black')\n    pixels = img.load()\n    for ((x, y), color) in results:\n        if (color_max == 0):\n            color = 0\n        else:\n            color = int(((float(color) / color_max) * 255.0))\n        pixels[(x, y)] = color\n    img.save(output_filename)\n", "label": 0}
{"function": "\n\ndef Validate(self, value, unused_key=None):\n    'Validates a subnet.'\n    if (value is None):\n        raise validation.MissingAttribute('subnet must be specified')\n    if (not isinstance(value, basestring)):\n        raise validation.ValidationError((\"subnet must be a string, not '%r'\" % type(value)))\n    try:\n        ipaddr.IPNetwork(value)\n    except ValueError:\n        raise validation.ValidationError(('%s is not a valid IPv4 or IPv6 subnet' % value))\n    parts = value.split('/')\n    if ((len(parts) == 2) and (not re.match('^[0-9]+$', parts[1]))):\n        raise validation.ValidationError(('Prefix length of subnet %s must be an integer (quad-dotted masks are not supported)' % value))\n    return value\n", "label": 0}
{"function": "\n\ndef test_cache(self):\n    mocked_repo = MagicMock()\n    mocked_commit = MagicMock()\n    mocked_repo.lookup_reference().resolve().target = 'head'\n    mocked_repo.walk.return_value = [mocked_commit]\n    mocked_commit.commit_time = 1411135000\n    mocked_commit.hex = '1111111111'\n    cache = CommitCache(mocked_repo)\n    cache.update()\n    cache['2014-09-20'] = Commit(1, 1, '1111111111')\n    assert (sorted(cache.keys()) == ['2014-09-19', '2014-09-20'])\n    asserted_time = datetime.fromtimestamp(mocked_commit.commit_time)\n    asserted_time = '{}-{}-{}'.format(asserted_time.hour, asserted_time.minute, asserted_time.second)\n    assert (repr(cache['2014-09-19']) == ('[%s-1111111111]' % asserted_time))\n    del cache['2014-09-20']\n    for commit_date in cache:\n        assert (commit_date == '2014-09-19')\n    mocked_repo.lookup_reference.has_calls([call('HEAD')])\n    mocked_repo.walk.assert_called_once_with('head', GIT_SORT_TIME)\n    assert (mocked_repo.lookup_reference().resolve.call_count == 2)\n", "label": 0}
{"function": "\n\ndef __deleteData__(self, uri, query, auth=True):\n    if self.ssl:\n        url = ('%s://%s:%s%s' % ('https', self.hostname, self.port, uri))\n    else:\n        url = ('%s://%s:%s%s' % ('http', self.hostname, self.port, uri))\n    full_uri = ('%s' % uri)\n    if (query != None):\n        full_uri = ('%s?%s' % (full_uri, urllib.urlencode(query)))\n    if self.token:\n        if auth:\n            headers = {\n                'Content-Type': 'application/json',\n                'X-Auth-Token': self.token,\n            }\n        else:\n            headers = {\n                'Content-Type': 'application/json',\n            }\n    else:\n        date = utils.formatdate()\n        if auth:\n            sig = self.__signRequest__('DELETE', full_uri, date, 'application/json')\n            headers = {\n                'Content-Type': 'application/json',\n                'Date': date,\n                'Authorization': sig,\n            }\n        else:\n            headers = {\n                'Content-Type': 'application/json',\n                'Date': date,\n            }\n    r = requests.delete(url, params=query, headers=headers)\n    return self.__processResponse__({\n        'status': r.status_code,\n        'data': r.text,\n    })\n", "label": 0}
{"function": "\n\ndef candidates(self, items, artist, album, va_likely):\n    'Returns a list of AlbumInfo objects for discogs search results\\n        matching an album and artist (if not various).\\n        '\n    if (not self.discogs_client):\n        return\n    if va_likely:\n        query = album\n    else:\n        query = ('%s %s' % (artist, album))\n    try:\n        return self.get_albums(query)\n    except DiscogsAPIError as e:\n        self._log.debug('API Error: {0} (query: {1})', e, query)\n        if (e.status_code == 401):\n            self.reset_auth()\n            return self.candidates(items, artist, album, va_likely)\n        else:\n            return []\n    except CONNECTION_ERRORS:\n        self._log.debug('Connection error in album search', exc_info=True)\n        return []\n", "label": 0}
{"function": "\n\ndef check_access_key_rotation(self, iamuser_item):\n    '\\n        alert when an IAM User has an active access key created more than 90 days go.\\n        '\n    akeys = iamuser_item.config.get('accesskeys', {\n        \n    })\n    for akey in akeys.keys():\n        if ('status' in akeys[akey]):\n            if (akeys[akey]['status'] == 'Active'):\n                create_date = akeys[akey]['create_date']\n                create_date = parser.parse(create_date)\n                if (create_date < self.ninety_days_ago):\n                    notes = '> 90 days ago'\n                    self.add_issue(1, 'Active accesskey has not been rotated.', iamuser_item, notes=notes)\n", "label": 0}
{"function": "\n\ndef Execute(self, opt, args):\n    self.opt = opt\n    project_list = self.GetProjects(args)\n    pending = []\n    branch = None\n    if opt.branch:\n        branch = opt.branch\n    for project in project_list:\n        if opt.current_branch:\n            cbr = project.CurrentBranch\n            up_branch = project.GetUploadableBranch(cbr)\n            if up_branch:\n                avail = [up_branch]\n            else:\n                avail = None\n                print(('ERROR: Current branch (%s) not pushable. You may be able to type \"git branch --set-upstream-to m/master\" to fix your branch.' % str(cbr)), file=sys.stderr)\n        else:\n            avail = project.GetUploadableBranches(branch)\n        if avail:\n            pending.append((project, avail))\n    if (pending and (not opt.bypass_hooks)):\n        hook = RepoHook('pre-push', self.manifest.repo_hooks_project, self.manifest.topdir, abort_if_user_denies=True)\n        pending_proj_names = [project.name for (project, avail) in pending]\n        pending_worktrees = [project.worktree for (project, avail) in pending]\n        try:\n            hook.Run(opt.allow_all_hooks, project_list=pending_proj_names, worktree_list=pending_worktrees)\n        except HookError as e:\n            print(('ERROR: %s' % str(e)), file=sys.stderr)\n            return\n    if (not pending):\n        print('no branches ready for push', file=sys.stderr)\n    elif ((len(pending) == 1) and (len(pending[0][1]) == 1)):\n        self._SingleBranch(opt, pending[0][1][0])\n    else:\n        self._MultipleBranches(opt, pending)\n", "label": 1}
{"function": "\n\ndef emit(events, stream=None, Dumper=Dumper, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None):\n    '\\n    Emit YAML parsing events into a stream.\\n    If stream is None, return the produced string instead.\\n    '\n    getvalue = None\n    if (stream is None):\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        stream = StringIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break)\n    for event in events:\n        dumper.emit(event)\n    if getvalue:\n        return getvalue()\n", "label": 0}
{"function": "\n\n@cached_property\ndef api_version(self):\n    metas = [x for x in self.parsed.findall('.//meta') if (x.get('name', '').lower() == 'api-version')]\n    if metas:\n        try:\n            return int(metas[0].get('value', None))\n        except (TypeError, ValueError):\n            pass\n    return None\n", "label": 0}
{"function": "\n\n@memoize\ndef non_proxy(model):\n    while model._meta.proxy:\n        model = next((b for b in model.__bases__ if (issubclass(b, models.Model) and (not b._meta.abstract))))\n    return model\n", "label": 0}
{"function": "\n\ndef _children_updated(self, object, name, event):\n    ' Handles the children of a node being changed.\\n        '\n    name = name[:(- 6)]\n    self.log_change(self._get_undo_item, object, name, event)\n    start = event.index\n    n = len(event.added)\n    end = (start + len(event.removed))\n    tree = self._tree\n    for (expanded, node, nid) in self._object_info_for(object, name):\n        children = node.get_children(object)\n        if expanded:\n            for cnid in self._nodes_for(nid)[start:end]:\n                self._delete_node(cnid)\n            remaining = (len(children) - len(event.removed))\n            child_index = 0\n            for child in event.added:\n                (child, child_node) = self._node_for(child)\n                if (child_node is not None):\n                    insert_index = ((start + child_index) if (start <= remaining) else None)\n                    self._insert_node(nid, insert_index, child_node, child)\n                    child_index += 1\n        else:\n            dummy = getattr(nid, '_dummy', None)\n            if ((dummy is None) and (len(children) > 0)):\n                nid._dummy = QtGui.QTreeWidgetItem(nid)\n            elif ((dummy is not None) and (len(children) == 0)):\n                nid.removeChild(dummy)\n                del nid._dummy\n        if node.can_auto_open(object):\n            nid.setExpanded(True)\n", "label": 1}
{"function": "\n\ndef PositionerDACOffsetDualGet(self, socketId, PositionerName):\n    command = (('PositionerDACOffsetDualGet(' + PositionerName) + ',short *,short *,short *,short *)')\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(4):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef _dump_json(data):\n    options = getattr(settings, 'JSON_OPTIONS', {\n        \n    })\n    if ('cls' in options):\n        if isinstance(options['cls'], six.string_types):\n            options['cls'] = import_string(options['cls'])\n    else:\n        try:\n            use_django = getattr(settings, 'JSON_USE_DJANGO_SERIALIZER')\n        except AttributeError:\n            use_django = True\n        else:\n            warnings.warn(\"JSON_USE_DJANGO_SERIALIZER is deprecated and will be removed. Please use JSON_OPTIONS['cls'] instead.\", DeprecationWarning)\n        if use_django:\n            options['cls'] = DjangoJSONEncoder\n    return json.dumps(data, **options)\n", "label": 0}
{"function": "\n\n@setup_cache\ndef test_set_get_delete(cache):\n    for value in range(100):\n        cache.set(value, value)\n    cache.check()\n    for value in range(100):\n        assert (cache.get(value) == value)\n    cache.check()\n    for value in range(100):\n        assert (value in cache)\n    cache.check()\n    for value in range(100):\n        assert cache.delete(value)\n    assert (cache.delete(100) == False)\n    cache.check()\n    for value in range(100):\n        cache[value] = value\n    cache.check()\n    for value in range(100):\n        assert (cache[value] == value)\n    cache.check()\n    cache.clear()\n    assert (len(cache) == 0)\n    cache.check()\n", "label": 1}
{"function": "\n\ndef pre_validate(self, form):\n    if self._invalid_formdata:\n        raise ValidationError(self.gettext('Not a valid choice'))\n    elif self.data:\n        obj_list = list((x[1] for x in self._get_object_list()))\n        for v in self.data:\n            if (v not in obj_list):\n                raise ValidationError(self.gettext('Not a valid choice'))\n", "label": 0}
{"function": "\n\ndef delete_mistyped_role():\n    '\\n    Delete \" system_admin\" role which was fat fingered.\\n    '\n    role_name = ' system_admin'\n    assert role_name.startswith(' ')\n    try:\n        role_db = Role.get_by_name(role_name)\n    except:\n        return\n    if (not role_db):\n        return\n    try:\n        Role.delete(role_db)\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('query_result')\n    if (self.success is not None):\n        oprot.writeFieldBegin('success', TType.STRUCT, 0)\n        self.success.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.error is not None):\n        oprot.writeFieldBegin('error', TType.STRUCT, 1)\n        self.error.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\ndef count_types(gen):\n    counts = {\n        'TOTAL': 0,\n        'DEBUG': 0,\n        'INFO': 0,\n        'WARNING': 0,\n        'ERROR': 0,\n        'TRACE': 0,\n        'AUDIT': 0,\n    }\n    laststatus = None\n    for line in gen:\n        counts['TOTAL'] = (counts['TOTAL'] + 1)\n        for key in counts:\n            if ((' %s ' % key) in line):\n                laststatus = key\n                continue\n        if laststatus:\n            counts[laststatus] = (counts[laststatus] + 1)\n    return counts\n", "label": 0}
{"function": "\n\ndef check_args(self, f, methods_args):\n    to_check = []\n    args = inspect.getargspec(f)\n    for arg in self.check:\n        if (arg in args[0]):\n            to_check.append(args[0].index(arg))\n    for index in to_check:\n        arg = methods_args[(index - 1)]\n        if self.look_at.cache.get(arg, False):\n            raise FuseOSError(errno.EACCES)\n        if self.look_at.check_key(arg):\n            self.look_at.cache[arg] = True\n            raise FuseOSError(errno.ENOENT)\n        self.look_at.cache[arg] = False\n", "label": 0}
{"function": "\n\ndef add_endpoint(self, listener):\n    \"\\n        Adds a listening endpoint to be managed by this ION process.\\n\\n        Spawns the listen loop and sets the routing call to synchronize incoming messages\\n        here. If this process hasn't been started yet, adds it to the list of listeners\\n        to start on startup.\\n        \"\n    if self.proc:\n        listener.routing_call = self._routing_call\n        if self.name:\n            svc_name = 'unnamed-service'\n            if ((self.service is not None) and hasattr(self.service, 'name')):\n                svc_name = self.service.name\n            listen_thread_name = ('%s-%s-listen-%s' % (svc_name, self.name, (len(self.listeners) + 1)))\n        else:\n            listen_thread_name = ('unknown-listener-%s' % (len(self.listeners) + 1))\n        gl = self.thread_manager.spawn(listener.listen, thread_name=listen_thread_name)\n        gl.proc._glname = ('ION Proc listener %s' % listen_thread_name)\n        self._listener_map[listener] = gl\n        self.listeners.append(listener)\n    else:\n        self._startup_listeners.append(listener)\n", "label": 0}
{"function": "\n\ndef _showmodule(module_name):\n    module = sys.modules[module_name]\n    if (not hasattr(module, '__file__')):\n        raise ValueError('cannot display module {0} (no __file__)'.format(module_name))\n    if module.__file__.endswith('.py'):\n        fname = module.__file__\n    else:\n        if (not module.__file__.endswith('.pyc')):\n            raise ValueError('cannot display module file {0} for {1}'.format(module.__file__, module_name))\n        fname = module.__file__[:(- 1)]\n    if (not os.path.exists(fname)):\n        raise ValueError('could not find file {0} for {1}'.format(fname, module_name))\n    (leaf_count, branch_count) = _get_samples_by_line(fname)\n    lines = []\n    with open(fname) as f:\n        for (i, line) in enumerate(f):\n            lines.append(((i + 1), cgi.escape(line), leaf_count[(i + 1)], branch_count[(i + 1)]))\n    return lines\n", "label": 0}
{"function": "\n\ndef _tune_workers(self):\n    for (i, w) in enumerate(self._workers):\n        if (not w.is_alive()):\n            del self._workers[i]\n    need_nums = min(self._queue.qsize(), self.MAX_WORKERS)\n    active_nums = len(self._workers)\n    if (need_nums <= active_nums):\n        return\n    for i in range((need_nums - active_nums)):\n        t = threading.Thread(target=self._worker)\n        t.daemon = True\n        t.start()\n        self._workers.append(t)\n", "label": 0}
{"function": "\n\ndef post(self, request, *args, **kwargs):\n    if (self.request.POST['form_type'] == 'location-settings'):\n        return self.settings_form_post(request, *args, **kwargs)\n    elif (self.request.POST['form_type'] == 'location-users'):\n        return self.users_form_post(request, *args, **kwargs)\n    elif ((self.request.POST['form_type'] == 'location-products') and toggles.PRODUCTS_PER_LOCATION.enabled(request.domain)):\n        return self.products_form_post(request, *args, **kwargs)\n    else:\n        raise Http404()\n", "label": 0}
{"function": "\n\n@expose(help='Change directory to site webroot')\ndef cd(self):\n    if (not self.app.pargs.site_name):\n        try:\n            while (not self.app.pargs.site_name):\n                self.app.pargs.site_name = input('Enter site name : ').strip()\n        except IOError as e:\n            Log.error(self, 'Unable to read input, please try again')\n    self.app.pargs.site_name = self.app.pargs.site_name.strip()\n    (ee_domain, ee_www_domain) = ValidateDomain(self.app.pargs.site_name)\n    if (not check_domain_exists(self, ee_domain)):\n        Log.error(self, 'site {0} does not exist'.format(ee_domain))\n    ee_site_webroot = getSiteInfo(self, ee_domain).site_path\n    EEFileUtils.chdir(self, ee_site_webroot)\n    try:\n        subprocess.call(['bash'])\n    except OSError as e:\n        Log.debug(self, '{0}{1}'.format(e.errno, e.strerror))\n        Log.error(self, 'unable to change directory')\n", "label": 0}
{"function": "\n\ndef GetDisplayHTML(self):\n    if (self.xPolynomialOrder == None):\n        self._HTML = 'z = user-selectable polynomial'\n    else:\n        self._HTML = 'z = '\n        cd = self.GetCoefficientDesignators()\n        indexmax = ((self.xPolynomialOrder + 1) * (self.yPolynomialOrder + 1))\n        for i in range((self.xPolynomialOrder + 1)):\n            for j in range((self.yPolynomialOrder + 1)):\n                index = ((i * (self.yPolynomialOrder + 1)) + j)\n                if (index == 0):\n                    self._HTML += cd[index]\n                else:\n                    self._HTML += (((((cd[index] + 'x<SUP>') + str(i)) + '</SUP>y<SUP>') + str(j)) + '</SUP>')\n                if (((i + 1) * (j + 1)) != indexmax):\n                    self._HTML += ' + '\n    return self.extendedVersionHandler.AssembleDisplayHTML(self)\n", "label": 0}
{"function": "\n\ndef env(target_key, entity, *_):\n    if (not target_key):\n        (yield entity.env)\n    for extra in entity.env.extras:\n        if (target_key in getattr(extra, 'target_keys', ())):\n            (yield extra)\n        for target in recursive_targets(getattr(extra, 'target_providers', ()), target_key):\n            (yield target)\n", "label": 0}
{"function": "\n\ndef ControllerMotionKernelPeriodMinMaxGet(self, socketId):\n    command = 'ControllerMotionKernelPeriodMinMaxGet(double *,double *,double *,double *,double *,double *)'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(6):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef estimate_scale(self):\n    '\\n        Returns an estimate of the scale parameter at the current\\n        parameter value.\\n        '\n    if isinstance(self.family, (families.Binomial, families.Poisson, _Multinomial)):\n        return 1.0\n    endog = self.endog_li\n    exog = self.exog_li\n    cached_means = self.cached_means\n    nobs = self.nobs\n    varfunc = self.family.variance\n    scale = 0.0\n    fsum = 0.0\n    for i in range(self.num_group):\n        if (len(endog[i]) == 0):\n            continue\n        (expval, _) = cached_means[i]\n        f = (self.weights_li[i] if (self.weights is not None) else 1.0)\n        sdev = np.sqrt(varfunc(expval))\n        resid = ((endog[i] - expval) / sdev)\n        scale += (f * np.sum((resid ** 2)))\n        fsum += (f * len(endog[i]))\n    scale /= ((fsum * (nobs - self.ddof_scale)) / float(nobs))\n    return scale\n", "label": 0}
{"function": "\n\ndef matches(self, elem):\n    if (not self.matchesPredicates(elem)):\n        return 0\n    if (self.childLocation != None):\n        for c in elem.elements():\n            if self.childLocation.matches(c):\n                return 1\n    else:\n        return 1\n    return 0\n", "label": 0}
{"function": "\n\n@policy.setter\ndef policy(self, args):\n    '\\n        setter for the policy descriptor\\n        '\n    word = args[0]\n    if (word == 'reject'):\n        self.accepted_ports = None\n        self.rejected_ports = []\n        target = self.rejected_ports\n    elif (word == 'accept'):\n        self.accepted_ports = []\n        self.rejected_ports = None\n        target = self.accepted_ports\n    else:\n        raise RuntimeError(('Don\\'t understand policy word \"%s\"' % word))\n    for port in args[1].split(','):\n        if ('-' in port):\n            (a, b) = port.split('-')\n            target.append(PortRange(int(a), int(b)))\n        else:\n            target.append(int(port))\n", "label": 0}
{"function": "\n\ndef assert_raises_and_contains(expected_exception_class, strings, callable_obj, *args, **kwargs):\n    'Assert an exception is raised by passing in a callable and its\\n    arguments and that the string representation of the exception\\n    contains the case-insensitive list of passed in strings.\\n\\n    Args\\n        strings -- can be a string or an iterable of strings\\n    '\n    try:\n        callable_obj(*args, **kwargs)\n    except expected_exception_class as e:\n        message = str(e).lower()\n        try:\n            is_string = isinstance(strings, basestring)\n        except NameError:\n            is_string = isinstance(strings, str)\n        if is_string:\n            strings = [strings]\n        for string in strings:\n            assert_in(string.lower(), message)\n    else:\n        assert_not_reached(('No exception was raised (expected %s)' % expected_exception_class))\n", "label": 0}
{"function": "\n\ndef unique_everseen(iterable, key=None):\n    'List unique elements, preserving order. Remember all elements ever seen.'\n    seen = set()\n    seen_add = seen.add\n    if (key is None):\n        for element in ifilterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            (yield element)\n    else:\n        for element in iterable:\n            k = key(element)\n            if (k not in seen):\n                seen_add(k)\n                (yield element)\n", "label": 0}
{"function": "\n\ndef __updateWidth(self):\n    charWidth = None\n    if (self.getPlug() is not None):\n        charWidth = Gaffer.Metadata.plugValue(self.getPlug(), 'numericPlugValueWidget:fixedCharacterWidth')\n    if ((charWidth is None) and isinstance(self.getPlug(), Gaffer.IntPlug) and self.getPlug().hasMaxValue()):\n        charWidth = len(str(self.getPlug().maxValue()))\n    self.__numericWidget.setFixedCharacterWidth(charWidth)\n", "label": 0}
{"function": "\n\ndef test_reflect_entity_overrides():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    b = Symbol('b')\n    m = Symbol('m')\n    l = Line((0, b), slope=m)\n    p = Point(x, y)\n    r = p.reflect(l)\n    c = Circle((x, y), 3)\n    cr = c.reflect(l)\n    assert (cr == Circle(r, (- 3)))\n    assert (c.area == (- cr.area))\n    pent = RegularPolygon((1, 2), 1, 5)\n    l = Line((0, pi), slope=sqrt(2))\n    rpent = pent.reflect(l)\n    assert (rpent.center == pent.center.reflect(l))\n    assert (str([w.n(3) for w in rpent.vertices]) == '[Point2D(-0.586, 4.27), Point2D(-1.69, 4.66), Point2D(-2.41, 3.73), Point2D(-1.74, 2.76), Point2D(-0.616, 3.10)]')\n    assert pent.area.equals((- rpent.area))\n", "label": 0}
{"function": "\n\ndef summary_cdf(self, idx, frac, crit, varnames=None, title=None):\n    'summary table for cumulative density function\\n\\n\\n        Parameters\\n        ----------\\n        idx : None or list of integers\\n            List of indices into the Monte Carlo results (columns) that should\\n            be used in the calculation\\n        frac : array_like, float\\n            probabilities for which\\n        crit : array_like\\n            values for which cdf is calculated\\n        varnames : None, or list of strings\\n            optional list of variable names, same length as idx\\n\\n        Returns\\n        -------\\n        table : instance of SimpleTable\\n            use `print(table` to see results\\n\\n\\n        '\n    idx = np.atleast_1d(idx)\n    mml = []\n    for i in range(len(idx)):\n        mml.append(self.cdf(crit[:, i], [idx[i]])[1].ravel())\n    mmlar = np.column_stack(([frac] + mml))\n    if title:\n        title = (title + ' Probabilites')\n    else:\n        title = 'Probabilities'\n    if (varnames is None):\n        varnames = [('var%d' % i) for i in range((mmlar.shape[1] - 1))]\n    headers = (['prob'] + varnames)\n    return SimpleTable(mmlar, txt_fmt={\n        'data_fmts': (['%#6.3f'] + (['%#10.4f'] * (np.array(mml).shape[1] - 1))),\n    }, title=title, headers=headers)\n", "label": 0}
{"function": "\n\ndef switch_to_correct_strategy(self, w_dict, w_key):\n    if (type(w_key) is values.W_Fixnum):\n        strategy = FixnumHashmapStrategy.singleton\n    elif (type(w_key) is values.W_Symbol):\n        strategy = SymbolHashmapStrategy.singleton\n    elif isinstance(w_key, values_string.W_String):\n        strategy = StringHashmapStrategy.singleton\n    elif isinstance(w_key, values.W_Bytes):\n        strategy = ByteHashmapStrategy.singleton\n    else:\n        strategy = ObjectHashmapStrategy.singleton\n    storage = strategy.create_storage([], [])\n    w_dict.strategy = strategy\n    w_dict.hstorage = storage\n", "label": 0}
{"function": "\n\ndef envs(ignore_cache=False):\n    '\\n    Return a list of refs that can be used as environments\\n    '\n    if (not ignore_cache):\n        env_cache = os.path.join(__opts__['cachedir'], 'hgfs/envs.p')\n        cache_match = salt.fileserver.check_env_cache(__opts__, env_cache)\n        if (cache_match is not None):\n            return cache_match\n    ret = set()\n    for repo in init():\n        repo['repo'].open()\n        if (repo['branch_method'] in ('branches', 'mixed')):\n            for branch in _all_branches(repo['repo']):\n                branch_name = branch[0]\n                if (branch_name == repo['base']):\n                    branch_name = 'base'\n                ret.add(branch_name)\n        if (repo['branch_method'] in ('bookmarks', 'mixed')):\n            for bookmark in _all_bookmarks(repo['repo']):\n                bookmark_name = bookmark[0]\n                if (bookmark_name == repo['base']):\n                    bookmark_name = 'base'\n                ret.add(bookmark_name)\n        ret.update([x[0] for x in _all_tags(repo['repo'])])\n        repo['repo'].close()\n    return [x for x in sorted(ret) if _env_is_exposed(x)]\n", "label": 1}
{"function": "\n\ndef visit_table(self, table, asfrom=False, iscrud=False, ashint=False, fromhints=None, **kwargs):\n    if (asfrom or ashint):\n        if getattr(table, 'schema', None):\n            ret = ((self.preparer.quote_schema(table.schema) + '.') + self.preparer.quote(table.name))\n        else:\n            ret = self.preparer.quote(table.name)\n        if (fromhints and (table in fromhints)):\n            ret = self.format_from_hint_text(ret, table, fromhints[table], iscrud)\n        return ret\n    else:\n        return ''\n", "label": 0}
{"function": "\n\ndef __init__(self, vm_spec):\n    'Initialize a CloudStack virtual machine.\\n\\n    Args:\\n      vm_spec: virtual_machine.BaseVirtualMachineSpec object of the vm.\\n    '\n    super(CloudStackVirtualMachine, self).__init__(vm_spec)\n    self.network = cloudstack_network.CloudStackNetwork.GetNetwork(self)\n    self.cs = util.CsClient(FLAGS.CS_API_URL, FLAGS.CS_API_KEY, FLAGS.CS_API_SECRET)\n    self.project_id = None\n    if FLAGS.project:\n        project = self.cs.get_project(FLAGS.project)\n        assert project, 'Project not found'\n        self.project_id = project['id']\n    zone = self.cs.get_zone(self.zone)\n    assert zone, 'Zone not found'\n    self.zone_id = zone['id']\n    self.user_name = self.DEFAULT_USER_NAME\n    self.image = (self.image or self.DEFAULT_IMAGE)\n    self.disk_counter = 0\n", "label": 0}
{"function": "\n\n@count_calls\ndef update_versions(self, reference_resolution):\n    'A mock update_versions implementation, does the update indeed but\\n        partially.\\n\\n        :param reference_resolution: The reference_resolution dictionary\\n        :return: a list of new versions\\n        '\n    new_versions = []\n    from stalker import Version\n    current_version = self.get_current_version()\n    for version in reference_resolution['create']:\n        local_reference_resolution = self.open(version, force=True)\n        new_version = Version(task=version.task, take_name=version.take_name, parent=version, description='Automatically created with Deep Reference Update')\n        new_version.is_published = True\n        for v in self._version.inputs:\n            new_version.inputs.append(v.latest_published_version)\n        new_versions.append(new_version)\n    current_version_after_create = self.get_current_version()\n    if current_version:\n        if (current_version != current_version_after_create):\n            self.open(current_version)\n        reference_resolution['update'].extend(reference_resolution['create'])\n        self.update_first_level_versions(reference_resolution)\n    return new_versions\n", "label": 0}
{"function": "\n\ndef _visit_call_helper_list(self, node):\n    name = self.visit(node.func)\n    if node.args:\n        args = [self.visit(e) for e in node.args]\n        args = ', '.join([e for e in args if e])\n    else:\n        args = '[]'\n    return ('%s(%s)' % (name, args))\n", "label": 0}
{"function": "\n\ndef pseudo_raw_input(self, prompt):\n    \"copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout\"\n    if self.use_rawinput:\n        try:\n            line = raw_input(prompt)\n        except EOFError:\n            line = 'EOF'\n    else:\n        self.stdout.write(prompt)\n        self.stdout.flush()\n        line = self.stdin.readline()\n        if (not len(line)):\n            line = 'EOF'\n        elif (line[(- 1)] == '\\n'):\n            line = line[:(- 1)]\n    return line\n", "label": 0}
{"function": "\n\ndef _get_lock_path(name, lock_file_prefix, lock_path=None):\n    name = name.replace(os.sep, '_')\n    if lock_file_prefix:\n        sep = ('' if lock_file_prefix.endswith('-') else '-')\n        name = ('%s%s%s' % (lock_file_prefix, sep, name))\n    local_lock_path = (lock_path or CONF.oslo_concurrency.lock_path)\n    if (not local_lock_path):\n        raise cfg.RequiredOptError('lock_path')\n    return os.path.join(local_lock_path, name)\n", "label": 0}
{"function": "\n\ndef _check_workers(self):\n    while (not self.stopped.is_set()):\n        for (worker, info) in self.worker_tracker.workers.iteritems():\n            if ((int(time.time()) - info.last_update) > HEARTBEAT_CHECK_INTERVAL):\n                info.continous_register = 0\n                if (info.status == RUNNING):\n                    info.status = HANGUP\n                elif (info.status == HANGUP):\n                    info.status = STOPPED\n                    self.black_list.append(worker)\n                    for job in self.job_tracker.running_jobs:\n                        self.job_tracker.remove_worker(job, worker)\n            elif (info.continous_register >= CONTINOUS_HEARTBEAT):\n                if (info.status != RUNNING):\n                    info.status = RUNNING\n                if (worker in self.black_list):\n                    self.black_list.remove(worker)\n                for job in self.job_tracker.running_jobs:\n                    if (not client_call(worker, 'has_job')):\n                        client_call(worker, 'prepare', job)\n                        client_call(worker, 'run_job', job)\n                    self.job_tracker.add_worker(job, worker)\n        self.stopped.wait(HEARTBEAT_CHECK_INTERVAL)\n", "label": 1}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(SelectPluginAction, self).__init__(request, *args, **kwargs)\n    try:\n        plugins = saharaclient.plugin_list(request)\n    except Exception:\n        plugins = []\n        exceptions.handle(request, _('Unable to fetch plugin list.'))\n    plugin_choices = [(plugin.name, plugin.title) for plugin in plugins]\n    self.fields['plugin_name'] = forms.ChoiceField(label=_('Plugin name'), choices=plugin_choices, widget=forms.Select(attrs={\n        'class': 'plugin_name_choice',\n    }))\n    for plugin in plugins:\n        field_name = (plugin.name + '_version')\n        choice_field = forms.ChoiceField(label=_('Version'), choices=[(version, version) for version in plugin.versions], widget=forms.Select(attrs={\n            'class': (('plugin_version_choice ' + field_name) + '_choice'),\n        }))\n        self.fields[field_name] = choice_field\n", "label": 0}
{"function": "\n\n@float_format.setter\ndef float_format(self, val):\n    if ((val is None) or (isinstance(val, dict) and (len(val) is 0))):\n        self._float_format = {\n            \n        }\n    else:\n        self._validate_option('float_format', val)\n        for field in self._field_names:\n            self._float_format[field] = val\n", "label": 0}
{"function": "\n\ndef __resolve_options(ctx, value):\n    'Resolve the given value to JipUndefined or, if its not\\n    an option, try to find one if a tool is associated with the context\\n\\n    :param ctx: the context\\n    :param value: the source value\\n    :returns: resolved or as is\\n    '\n    if isinstance(value, JipUndefined):\n        value = value._undefined_name\n    if (not isinstance(value, Option)):\n        script = ctx.get('tool', None)\n        if script:\n            v = script.options[value]\n            if (v is not None):\n                value = v\n    return value\n", "label": 0}
{"function": "\n\ndef answers(self, other):\n    if (other.id == self.id):\n        if (self.code == 1):\n            return 1\n        if ((other.code in [2, 4, 6, 8, 10]) and (self.code == (other.code + 1))):\n            if (other.code == 8):\n                return 1\n            return self.payload.answers(other.payload)\n    return 0\n", "label": 0}
{"function": "\n\ndef handle_children_state(children, kids):\n    'Given a list of children (as `children`) of a particular object\\n    and their states in the `kids` argument, this function sets up the\\n    children by removing unnecessary ones, fixing existing ones and\\n    adding new children if necessary (depending on the state).\\n    '\n    m_children = list(children)\n    (n_child, n_kid) = (len(m_children), len(kids))\n    for i in range((n_child - n_kid)):\n        m_children.pop()\n    for i in range(n_child):\n        (child, kid) = (m_children[i], kids[i])\n        md = kid.__metadata__\n        if ((child.__module__ != md['module']) or (child.__class__.__name__ != md['class_name'])):\n            m_children[i] = create_instance(kid)\n    for i in range((n_kid - n_child)):\n        child = create_instance(kids[(n_child + i)])\n        m_children.append(child)\n    children[:] = m_children\n", "label": 0}
{"function": "\n\ndef test_random_udir_noweights(self):\n    if (not use_networkx):\n        sys.stderr.write('Skipping TestBetweenessCentrality.test_random_udir_noweights due to missing networkx library\\n')\n        return\n    Gnx = networkx.Graph()\n    G = Graph()\n    nodes = range(500)\n    random.shuffle(nodes)\n    while (len(nodes) > 0):\n        n1 = nodes.pop()\n        n2 = nodes.pop()\n        w = random.randint(1, 5)\n        eidx = G.add_edge(n1, n2)\n        G.set_weight_(eidx, w)\n        Gnx.add_edge(n1, n2, weight=w)\n    nodes = range(500)\n    random.shuffle(nodes)\n    while (len(nodes) > 0):\n        n1 = nodes.pop()\n        n2 = nodes.pop()\n        if (not G.has_edge(n1, n2)):\n            w = random.randint(1, 5)\n            eidx = G.add_edge(n1, n2)\n            G.set_weight_(eidx, w)\n            Gnx.add_edge(n1, n2, weight=w)\n    R = betweenness_centrality_(G, True, False)\n    Rnx = networkx.betweenness_centrality(Gnx, normalized=True, weight=None)\n    for n in G.nodes_iter():\n        self.assertAlmostEqual(Rnx[n], R[G.node_idx(n)])\n", "label": 0}
{"function": "\n\ndef next(self):\n    if ((self.length is not None) and (self.length <= 0)):\n        raise StopIteration\n    chunk = self.fileobj.read(self.chunk_size)\n    if (not chunk):\n        raise StopIteration\n    if (self.length is not None):\n        self.length -= len(chunk)\n        if (self.length < 0):\n            chunk = chunk[:self.length]\n    return chunk\n", "label": 0}
{"function": "\n\ndef find_node(self, node, path):\n    'Finds a node by the given path from the given node.'\n    for hash_value in path:\n        if isinstance(node, LeafStatisticsNode):\n            break\n        for stats in node.get_child_keys():\n            if (hash(stats) == hash_value):\n                node = node.get_child_node(stats)\n                break\n        else:\n            break\n    return node\n", "label": 0}
{"function": "\n\ndef add_jumpbox(host):\n    if jb.cluster.locate(host):\n        return\n    print('Connecting to jumpbox:', host)\n    try:\n        t = ssh.connection_worker(host, None, init_data['auth'])\n        retries = 3\n        while ((not t.is_authenticated()) and (retries > 0)):\n            print(('Failed to authenticate to Jumpbox (%s)' % host))\n            jb_user = raw_input(('Enter username for [%s]: ' % host))\n            jb_passwd = getpass.getpass(('Enter password for %s@%s: ' % (jb_user, host)))\n            reauth = AuthManager(jb_user, auth_file=None, include_agent=False, include_userkeys=False, default_password=jb_passwd)\n            t = ssh.connection_worker(host, None, reauth)\n            retries -= 1\n    except Exception as e:\n        print(host, repr(e))\n    finally:\n        jb.cluster.connections[host] = t\n", "label": 0}
{"function": "\n\ndef _finish_query_lookupd(self, response, lookupd_url):\n    if response.error:\n        logger.warning('[%s] lookupd %s query error: %s', self.name, lookupd_url, response.error)\n        return\n    try:\n        lookup_data = json.loads(response.body)\n    except ValueError:\n        logger.warning('[%s] lookupd %s failed to parse JSON: %r', self.name, lookupd_url, response.body)\n        return\n    if (lookup_data['status_code'] != 200):\n        logger.warning('[%s] lookupd %s responded with %d', self.name, lookupd_url, lookup_data['status_code'])\n        return\n    for producer in lookup_data['data']['producers']:\n        address = producer.get('broadcast_address', producer.get('address'))\n        assert address\n        self.connect_to_nsqd(address, producer['tcp_port'])\n", "label": 0}
{"function": "\n\ndef create_with_kwargs(self, context, **kwargs):\n    volume_id = kwargs.get('volume_id', None)\n    v = fake_volume(kwargs['size'], kwargs['name'], kwargs['description'], str(volume_id), None, None, None, None)\n    if (kwargs.get('status', None) is not None):\n        v.vol['status'] = kwargs['status']\n    if (kwargs['host'] is not None):\n        v.vol['host'] = kwargs['host']\n    if (kwargs['attach_status'] is not None):\n        v.vol['attach_status'] = kwargs['attach_status']\n    if (kwargs.get('snapshot_id', None) is not None):\n        v.vol['snapshot_id'] = kwargs['snapshot_id']\n    self.volume_list.append(v.vol)\n    return v.vol\n", "label": 0}
{"function": "\n\n@xmlrpc_func(returns='string[]', args=['string'])\ndef pingback_extensions_get_pingbacks(target):\n    \"\\n    pingback.extensions.getPingbacks(url) => '[url, url, ...]'\\n\\n    Returns an array of URLs that link to the specified url.\\n\\n    See: http://www.aquarionics.com/misc/archives/blogite/0198.html\\n    \"\n    site = Site.objects.get_current()\n    (scheme, netloc, path, query, fragment) = urlsplit(target)\n    if (netloc != site.domain):\n        return TARGET_DOES_NOT_EXIST\n    try:\n        (view, args, kwargs) = resolve(path)\n    except Resolver404:\n        return TARGET_DOES_NOT_EXIST\n    try:\n        entry = Entry.published.get(slug=kwargs['slug'], publication_date__year=kwargs['year'], publication_date__month=kwargs['month'], publication_date__day=kwargs['day'])\n    except (KeyError, Entry.DoesNotExist):\n        return TARGET_IS_NOT_PINGABLE\n    return [pingback.user_url for pingback in entry.pingbacks]\n", "label": 0}
{"function": "\n\ndef on_modified(self, view):\n    if view.settings().get('is_widget'):\n        return\n    if (not view.settings().get('auto_wrap', False)):\n        return\n    if (not self.check_selection(view)):\n        return\n    insertpt = self.get_insert_pt(view)\n    if (not insertpt):\n        return\n    self.set_status()\n    join = (self.status >= 2)\n    left_delete = (' ' not in view.substr(sublime.Region((insertpt - 1), (insertpt + 1))))\n    view.settings().set('auto_wrap', False)\n    view.run_command('auto_wrap_insert', {\n        'insertpt': insertpt,\n        'join': join,\n        'left_delete': self.left_delete,\n    })\n    self.left_delete = left_delete\n    view.settings().set('auto_wrap', True)\n", "label": 0}
{"function": "\n\ndef block_average_above(x, y, new_x):\n    '\\n    Linearly interpolates values in new_x based on the values in x and y.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        Independent values.\\n    y : array_like\\n        Dependent values.\\n    new_x : array_like\\n        The x values to interpolate y values.\\n\\n    '\n    bad_index = None\n    x = atleast_1d_and_contiguous(x, np.float64)\n    y = atleast_1d_and_contiguous(y, np.float64)\n    new_x = atleast_1d_and_contiguous(new_x, np.float64)\n    if (y.ndim > 2):\n        raise ValueError('`linear` only works with 1-D or 2-D arrays.')\n    if (len(y.shape) == 2):\n        new_y = np.zeros((y.shape[0], len(new_x)), np.float64)\n        for i in range(len(new_y)):\n            bad_index = _interpolate.block_averave_above_dddd(x, y[i], new_x, new_y[i])\n            if (bad_index is not None):\n                break\n    else:\n        new_y = np.zeros(len(new_x), np.float64)\n        bad_index = _interpolate.block_average_above_dddd(x, y, new_x, new_y)\n    if (bad_index is not None):\n        msg = ('block_average_above cannot extrapolate and new_x[%d]=%f is out of the x range (%f, %f)' % (bad_index, new_x[bad_index], x[0], x[(- 1)]))\n        raise ValueError(msg)\n    return new_y\n", "label": 0}
{"function": "\n\n@utils.enforce_id_param\ndef edit_playlist(self, playlist_id, new_name=None, new_description=None, public=None):\n    'Changes the name of a playlist and returns its id.\\n\\n        :param playlist_id: the id of the playlist\\n        :param new_name: (optional) desired title\\n        :param new_description: (optional) desired description\\n        :param public: (optional) if True and the user has a subscription, share playlist.\\n        '\n    if all(((value is None) for value in (new_name, new_description, public))):\n        raise ValueError('new_name, new_description, or public must be provided')\n    if (public is None):\n        share_state = public\n    else:\n        share_state = ('PUBLIC' if public else 'PRIVATE')\n    mutate_call = mobileclient.BatchMutatePlaylists\n    update_mutations = mutate_call.build_playlist_updates([{\n        'id': playlist_id,\n        'name': new_name,\n        'description': new_description,\n        'public': share_state,\n    }])\n    res = self._make_call(mutate_call, update_mutations)\n    return res['mutate_response'][0]['id']\n", "label": 0}
{"function": "\n\ndef main():\n    global imap_account\n    global routes\n    if (conf is None):\n        return 1\n    logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n    for routing in routings:\n        (methods, regex, app) = routing[:3]\n        if isinstance(methods, six.string_types):\n            methods = (methods,)\n        vars = (routing[3] if (len(routing) >= 4) else {\n            \n        })\n        routes.append((methods, re.compile(regex), app, vars))\n        log.info('Route {} openned'.format(regex[1:(- 1)]))\n    try:\n        imap_account = imap_cli.connect(**conf)\n        httpd = simple_server.make_server('127.0.0.1', 8000, router)\n        log.info('Serving on http://127.0.0.1:8000')\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        log.info('Interupt by user, exiting')\n    return 0\n", "label": 0}
{"function": "\n\ndef expandtargets(self, source_glob, target):\n    if ((not isdirname(target)) and (('*' in source_glob) or ('?' in source_glob))):\n        raise SymlinkError(\"Invalid symlink: {0} => {1} (did you mean to add a trailing '/'?)\".format(source_glob, target))\n    mkdir_p(os.path.dirname(os.path.expanduser(target)))\n    sources = iglob(os.path.join(self.symlink_dir, source_glob))\n    expanded = []\n    for source in sources:\n        source = os.path.join(self.cider_dir, source)\n        source_target = self.expandtarget(source, target)\n        expanded.append((source, source_target))\n    return expanded\n", "label": 0}
{"function": "\n\ndef addAndFixActions(startDict, actions):\n    curDict = copy.copy(startDict)\n    for action in actions:\n        new_ops = []\n        for op in action.db_operations:\n            if (op.vtType == 'add'):\n                if ((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId))):\n                    curDict[(op.db_what, op.db_objectId)] = op\n                    new_ops.append(op)\n            elif (op.vtType == 'change'):\n                if (curDict.has_key((op.db_what, op.db_oldObjId)) and ((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId)))):\n                    del curDict[(op.db_what, op.db_oldObjId)]\n                    curDict[(op.db_what, op.db_newObjId)] = op\n                    new_ops.append(op)\n            elif (op.vtType == 'delete'):\n                if (((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId))) and curDict.has_key((op.db_what, op.db_objectId))):\n                    del curDict[(op.db_what, op.db_objectId)]\n                    new_ops.append(op)\n        action.db_operations = new_ops\n    return curDict\n", "label": 1}
{"function": "\n\ndef test_time_dep_bra():\n    b = TimeDepBra(0, t)\n    assert isinstance(b, TimeDepBra)\n    assert isinstance(b, BraBase)\n    assert isinstance(b, StateBase)\n    assert isinstance(b, QExpr)\n    assert (b.label == (Integer(0),))\n    assert (b.args == (Integer(0), t))\n    assert (b.time == t)\n    assert (b.dual_class() == TimeDepKet)\n    assert (b.dual == TimeDepKet(0, t))\n    k = TimeDepBra(x, 0.5)\n    assert (k.label == (x,))\n    assert (k.args == (x, sympify(0.5)))\n    assert (TimeDepBra() == TimeDepBra('psi', 't'))\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    get_hbase_service_config(args)\n    for job_name in (args.job or reversed(ALL_JOBS)):\n        hosts = args.hbase_config.jobs[job_name].hosts\n        task_list = deploy_utils.schedule_task_for_threads(args, hosts, job_name, 'stop')\n        parallel_deploy.start_deploy_threads(stop_job, task_list)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.hbase_config.jobs[job_name].hosts\n        task_list = deploy_utils.schedule_task_for_threads(args, hosts, job_name, 'start', is_wait=True)\n        parallel_deploy.start_deploy_threads(start_job, task_list)\n", "label": 0}
{"function": "\n\ndef onModuleSourceCode(self, module_name, source_code):\n    annotations = {\n        \n    }\n    for (count, line) in enumerate(source_code.split('\\n')):\n        match = re.search('#.*pylint:\\\\s*disable=\\\\s*([\\\\w,]+)', line)\n        if match:\n            comment_only = (line[:(line.find('#') - 1)].strip() == '')\n            if comment_only:\n                pass\n            else:\n                annotations[(count + 1)] = set((match.strip() for match in match.group(1).split(',')))\n    if annotations:\n        self.line_annotations[module_name] = annotations\n    return source_code\n", "label": 0}
{"function": "\n\ndef wannabe_omnispec_to_rspec(omnispec, filter_allocated):\n    root = ET.Element('rspec')\n    for (id, r) in omnispec.get_resources().items():\n        if (filter_allocated and (not r.get_allocate())):\n            continue\n        res_type = r.get_type()\n        if (res_type == 'node'):\n            add_node(root, r)\n        elif (res_type == 'link'):\n            add_link(root, r)\n        else:\n            raise Exception(('Unknown resource type ' + res_type))\n    return ET.tostring(root)\n", "label": 0}
{"function": "\n\ndef run(self):\n    arguments = self.arguments\n    wrong_sorted_files = False\n    arguments['check'] = True\n    for path in self.distribution_files():\n        for python_file in glob.iglob(os.path.join(path, '*.py')):\n            try:\n                incorrectly_sorted = SortImports(python_file, **arguments).incorrectly_sorted\n                if incorrectly_sorted:\n                    wrong_sorted_files = True\n            except IOError as e:\n                print('WARNING: Unable to parse file {0} due to {1}'.format(file_name, e))\n    if wrong_sorted_files:\n        exit(1)\n", "label": 0}
{"function": "\n\ndef get_turns_since(state, maximum=8):\n    \"A feature encoding the age of the stone at each location up to 'maximum'\\n\\n\\tNote:\\n\\t- the [maximum-1] plane is used for any stone with age greater than or equal to maximum\\n\\t- EMPTY locations are all-zero features\\n\\t\"\n    planes = np.zeros((maximum, state.size, state.size))\n    depth = 0\n    for move in state.history[::(- 1)]:\n        if (move is not go.PASS_MOVE):\n            if (state.board[move] != go.EMPTY):\n                (x, y) = move\n                if (np.sum(planes[:, x, y]) == 0):\n                    planes[(depth, x, y)] = 1\n        if (depth < (maximum - 1)):\n            depth += 1\n    return planes\n", "label": 0}
{"function": "\n\ndef save_new_objects(self, commit=True):\n    self.new_objects = []\n    for form in self.extra_forms:\n        if (not form.has_changed()):\n            continue\n        if (self.can_delete and self._should_delete_form(form)):\n            continue\n        self.new_objects.append(self.save_new(form, commit=commit))\n        if (not commit):\n            self.saved_forms.append(form)\n    return self.new_objects\n", "label": 0}
{"function": "\n\ndef raise_tab(self, tabname):\n    l = []\n    name = tabname.lower()\n    while self.tab.has_key(name):\n        bnch = self.tab[name]\n        l.insert(0, name)\n        name = bnch.wsname.lower()\n        if (name in l):\n            break\n    for name in l:\n        (nb, index) = self._find_nb(name)\n        if ((nb is not None) and (index >= 0)):\n            nb.set_index(index)\n", "label": 0}
{"function": "\n\n@common.check_cells_enabled\ndef sync_instances(self, req, body):\n    'Tell all cells to sync instance info.'\n    context = req.environ['nova.context']\n    authorize(context)\n    authorize(context, action='sync_instances')\n    project_id = body.pop('project_id', None)\n    deleted = body.pop('deleted', False)\n    updated_since = body.pop('updated_since', None)\n    if body:\n        msg = _(\"Only 'updated_since', 'project_id' and 'deleted' are understood.\")\n        raise exc.HTTPBadRequest(explanation=msg)\n    if isinstance(deleted, six.string_types):\n        try:\n            deleted = strutils.bool_from_string(deleted, strict=True)\n        except ValueError as err:\n            raise exc.HTTPBadRequest(explanation=six.text_type(err))\n    if updated_since:\n        try:\n            timeutils.parse_isotime(updated_since)\n        except ValueError:\n            msg = _('Invalid changes-since value')\n            raise exc.HTTPBadRequest(explanation=msg)\n    self.cells_rpcapi.sync_instances(context, project_id=project_id, updated_since=updated_since, deleted=deleted)\n", "label": 0}
{"function": "\n\ndef handle_text(self):\n    '\\n        Takes care of converting body text to unicode, if its text at all.\\n        Sets self.original_encoding to original char encoding, and converts body\\n        to unicode if possible. Must come after handle_compression, and after\\n        self.mediaType is valid.\\n        '\n    self.encoding = None\n    if (self.mediaType and ((self.mediaType.type == 'text') or ((self.mediaType.type == 'application') and ('xml' in self.mediaType.subtype)))):\n        if ('charset' in self.mediaType.params):\n            override_encodings = [self.mediaType.params['charset']]\n        else:\n            override_encodings = []\n        if (self.body != ''):\n            if UnicodeDammit:\n                dammit = UnicodeDammit(self.body, override_encodings)\n                if dammit.unicode:\n                    self.text = dammit.unicode\n                    self.originalEncoding = dammit.originalEncoding\n                else:\n                    pass\n            else:\n                u = None\n                for e in (override_encodings + ['utf8', 'iso-8859-1']):\n                    try:\n                        u = self.body.decode(e, 'strict')\n                        self.originalEncoding = e\n                        break\n                    except UnicodeError:\n                        pass\n                if (not u):\n                    u = self.body.decode('utf8', 'replace')\n                    self.originalEncoding = None\n                self.text = (u or None)\n    else:\n        self.text = b64encode(self.body)\n        self.encoding = 'base64'\n", "label": 1}
{"function": "\n\ndef _complete_batch(self, batch, error, base_offset):\n    'Complete or retry the given batch of records.\\n\\n        Arguments:\\n            batch (RecordBatch): The record batch\\n            error (Exception): The error (or None if none)\\n            base_offset (int): The base offset assigned to the records if successful\\n        '\n    if (error is Errors.NoError):\n        error = None\n    if ((error is not None) and self._can_retry(batch, error)):\n        log.warning('Got error produce response on topic-partition %s, retrying (%d attempts left). Error: %s', batch.topic_partition, ((self.config['retries'] - batch.attempts) - 1), error)\n        self._accumulator.reenqueue(batch)\n    else:\n        if (error is Errors.TopicAuthorizationFailedError):\n            error = error(batch.topic_partition.topic)\n        batch.done(base_offset, error)\n        self._accumulator.deallocate(batch)\n    if getattr(error, 'invalid_metadata', False):\n        self._metadata.request_update()\n", "label": 0}
{"function": "\n\ndef closeUnusedFiles(transport):\n    import os, sys\n    notouch = transport.protectedFileNumList()\n    for each in [sys.stdin, sys.stderr, sys.stdout]:\n        try:\n            notouch.append(each.fileno())\n        except AttributeError:\n            pass\n    for fdnum in range(3, 255):\n        if (fdnum not in notouch):\n            try:\n                os.close(fdnum)\n            except OSError:\n                pass\n", "label": 0}
{"function": "\n\ndef get_context(self, bot, update, **kwargs):\n    queryset = self.get_queryset()\n    if (not self.slug_field):\n        raise AttributeError(('Generic detail view %s must be called with a slug.' % self.__class__.__name__))\n    slug_field = self.get_slug_field(**kwargs)\n    slug = self.get_slug(**kwargs)\n    if slug:\n        try:\n            object = queryset.get(**{\n                slug_field: slug,\n            })\n        except FieldError:\n            raise FieldError(('Field %s not in valid. Review slug_field' % slug_field))\n        except ObjectDoesNotExist:\n            object = None\n    else:\n        object = None\n    context = {\n        'context_object_name': object,\n    }\n    if self.context_object_name:\n        context[self.context_object_name] = object\n    return context\n", "label": 0}
{"function": "\n\n@handle_response_format\n@treeio_login_required\ndef mlist_delete(request, mlist_id, response_format='html'):\n    'Delete mlist page'\n    mlist = get_object_or_404(MailingList, pk=mlist_id)\n    if (not request.user.profile.has_permission(mlist, mode='w')):\n        return user_denied(request, message=\"You don't have access to this Mailing List\", response_format=response_format)\n    if request.POST:\n        if ('delete' in request.POST):\n            if ('trash' in request.POST):\n                mlist.trash = True\n                mlist.save()\n            else:\n                mlist.delete()\n            return HttpResponseRedirect('/messaging/')\n        elif ('cancel' in request.POST):\n            return HttpResponseRedirect(reverse('messaging_mlist_view', args=[mlist.id]))\n    context = _get_default_context(request)\n    context.update({\n        'mlist': mlist,\n    })\n    return render_to_response('messaging/mlist_delete', context, context_instance=RequestContext(request), response_format=response_format)\n", "label": 0}
{"function": "\n\ndef GetHandlerType(self):\n    'Get handler type of mapping.\\n\\n    Returns:\\n      Handler type determined by which handler id attribute is set.\\n\\n    Raises:\\n      UnknownHandlerType: when none of the no handler id attributes are set.\\n\\n      UnexpectedHandlerAttribute: when an unexpected attribute is set for the\\n        discovered handler type.\\n\\n      HandlerTypeMissingAttribute: when the handler is missing a\\n        required attribute for its handler type.\\n\\n      MissingHandlerAttribute: when a URL handler is missing an attribute\\n    '\n    if (getattr(self, HANDLER_API_ENDPOINT) is not None):\n        mapping_type = HANDLER_API_ENDPOINT\n    else:\n        for id_field in URLMap.ALLOWED_FIELDS.iterkeys():\n            if (getattr(self, id_field) is not None):\n                mapping_type = id_field\n                break\n        else:\n            raise appinfo_errors.UnknownHandlerType(('Unknown url handler type.\\n%s' % str(self)))\n    allowed_fields = URLMap.ALLOWED_FIELDS[mapping_type]\n    for attribute in self.ATTRIBUTES.iterkeys():\n        if ((getattr(self, attribute) is not None) and (not ((attribute in allowed_fields) or (attribute in URLMap.COMMON_FIELDS) or (attribute == mapping_type)))):\n            raise appinfo_errors.UnexpectedHandlerAttribute(('Unexpected attribute \"%s\" for mapping type %s.' % (attribute, mapping_type)))\n    if ((mapping_type == HANDLER_STATIC_FILES) and (not self.upload)):\n        raise appinfo_errors.MissingHandlerAttribute(('Missing \"%s\" attribute for URL \"%s\".' % (UPLOAD, self.url)))\n    return mapping_type\n", "label": 1}
{"function": "\n\ndef check_command(command, startswith, endswith):\n    output = subprocess.check_output(command)\n    lines = output.split('\\n')\n    matches = False\n    for line in lines:\n        line = line.strip()\n        if line.startswith(startswith):\n            matches = True\n            if (not line.endswith(endswith)):\n                status = 0\n                break\n    else:\n        if matches:\n            status = 1\n        else:\n            raise BadOutputError(('The output was not in the expected format:\\n%s' % output))\n    return status\n", "label": 0}
{"function": "\n\ndef paintEvent(self, e):\n    if (not self.addr):\n        return\n    black = QColor(0, 0, 0, 255)\n    white = QColor(255, 255, 255, 255)\n    if (not self.qr):\n        qp = QtGui.QPainter()\n        qp.begin(self)\n        qp.setBrush(white)\n        qp.setPen(white)\n        qp.drawRect(0, 0, 198, 198)\n        qp.end()\n        return\n    k = self.qr.getModuleCount()\n    qp = QtGui.QPainter()\n    qp.begin(self)\n    r = qp.viewport()\n    boxsize = ((min(r.width(), r.height()) * 0.8) / k)\n    size = (k * boxsize)\n    left = ((r.width() - size) / 2)\n    top = ((r.height() - size) / 2)\n    margin = 10\n    qp.setBrush(white)\n    qp.drawRect((left - margin), (top - margin), (size + (margin * 2)), (size + (margin * 2)))\n    for r in range(k):\n        for c in range(k):\n            if self.qr.isDark(r, c):\n                qp.setBrush(black)\n                qp.setPen(black)\n            else:\n                qp.setBrush(white)\n                qp.setPen(white)\n            qp.drawRect((left + (c * boxsize)), (top + (r * boxsize)), boxsize, boxsize)\n    qp.end()\n", "label": 0}
{"function": "\n\ndef parse_long(tokens, options):\n    (raw, eq, value) = tokens.move().partition('=')\n    value = (None if (eq == value == '') else value)\n    opt = [o for o in options if (o.long and o.long.startswith(raw))]\n    if (len(opt) < 1):\n        if (tokens.error is DocoptExit):\n            raise tokens.error(('%s is not recognized' % raw))\n        else:\n            o = Option(None, raw, (1 if (eq == '=') else 0))\n            options.append(o)\n            return [o]\n    if (len(opt) > 1):\n        raise tokens.error(('%s is not a unique prefix: %s?' % (raw, ', '.join((('%s' % o.long) for o in opt)))))\n    opt = copy(opt[0])\n    if (opt.argcount == 1):\n        if (value is None):\n            if (tokens.current() is None):\n                raise tokens.error(('%s requires argument' % opt.name))\n            value = tokens.move()\n    elif (value is not None):\n        raise tokens.error(('%s must not have an argument' % opt.name))\n    opt.value = (value or True)\n    return [opt]\n", "label": 1}
{"function": "\n\ndef initialize_locksets(self):\n    log.debug('initializing locksets')\n    v = self.sign(VoteBlock(0, 0, self.chainservice.chain.genesis.hash))\n    self.add_vote(v)\n    head_proposal = self.load_proposal(self.head.hash)\n    if head_proposal:\n        assert (head_proposal.blockhash == self.head.hash)\n        for v in head_proposal.signing_lockset:\n            self.add_vote(v)\n        assert self.heights[(self.head.header.number - 1)].has_quorum\n    last_committing_lockset = self.load_last_committing_lockset()\n    if last_committing_lockset:\n        assert (last_committing_lockset.has_quorum == self.head.hash)\n        for v in last_committing_lockset.votes:\n            self.add_vote(v)\n        assert self.heights[self.head.header.number].has_quorum\n    else:\n        assert (self.head.header.number == 0)\n    assert self.highest_committing_lockset\n    assert self.last_committing_lockset\n    assert self.last_valid_lockset\n", "label": 1}
{"function": "\n\ndef distrib_id():\n    '\\n    Get the OS distribution ID.\\n\\n    Returns a string such as ``\"Debian\"``, ``\"Ubuntu\"``, ``\"RHEL\"``,\\n    ``\"CentOS\"``, ``\"SLES\"``, ``\"Fedora\"``, ``\"Arch\"``, ``\"Gentoo\"``,\\n    ``\"SunOS\"``...\\n\\n    Example::\\n\\n        from fabtools.system import distrib_id\\n\\n        if distrib_id() != \\'Debian\\':\\n            abort(u\"Distribution is not supported\")\\n\\n    '\n    with settings(hide('running', 'stdout')):\n        kernel = run('uname -s')\n        if (kernel == 'Linux'):\n            if is_file('/usr/bin/lsb_release'):\n                id_ = run('lsb_release --id --short')\n                if (id in ['arch', 'Archlinux']):\n                    id_ = 'Arch'\n                return id_\n            elif is_file('/etc/debian_version'):\n                return 'Debian'\n            elif is_file('/etc/fedora-release'):\n                return 'Fedora'\n            elif is_file('/etc/arch-release'):\n                return 'Arch'\n            elif is_file('/etc/redhat-release'):\n                release = run('cat /etc/redhat-release')\n                if release.startswith('Red Hat Enterprise Linux'):\n                    return 'RHEL'\n                elif release.startswith('CentOS'):\n                    return 'CentOS'\n                elif release.startswith('Scientific Linux'):\n                    return 'SLES'\n            elif is_file('/etc/gentoo-release'):\n                return 'Gentoo'\n        elif (kernel == 'SunOS'):\n            return 'SunOS'\n", "label": 1}
{"function": "\n\ndef _updateFromPlug(self):\n    plug = self.getPlug()\n    if (plug is not None):\n        with self.getContext():\n            try:\n                value = plug.getValue()\n            except:\n                value = None\n        if (value is not None):\n            with Gaffer.BlockedConnection(self.__valueChangedConnection):\n                self.__numericWidget.setValue(value)\n        self.__numericWidget.setErrored((value is None))\n        animated = Gaffer.Animation.isAnimated(plug)\n        widgetAnimated = (GafferUI._Variant.fromVariant(self.__numericWidget._qtWidget().property('gafferAnimated')) or False)\n        if (widgetAnimated != animated):\n            self.__numericWidget._qtWidget().setProperty('gafferAnimated', GafferUI._Variant.toVariant(bool(animated)))\n            self.__numericWidget._repolish()\n    self.__numericWidget.setEditable(self._editable(canEditAnimation=True))\n", "label": 0}
{"function": "\n\ndef handle_msg(self, proto, msg):\n    if (msg[0:3] not in ['GET', 'SET', 'ADD', 'RES', 'UPD']):\n        return\n    if (msg[0:3] in ['GET']):\n        (_, key) = msg.split(' ', 1)\n        key = json.loads(key)\n        if (key in self.dict):\n            item = self.dict[key]\n            proto.send(('RES ' + json.dumps([key, item])))\n        else:\n            proto.send(('RES ' + json.dumps([key, None])))\n    if (msg[0:3] in ['SET']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        self.dict[key] = value\n    if (msg[0:3] in ['ADD']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if (key not in self.dict):\n            self.dict[key] = []\n        self.dict[key].append(value)\n    if (msg[0:3] in ['RES']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if (key in self.queue):\n            self.queue[key].put(value)\n    if (msg[0:3] in ['UPD']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if ((key not in self.dict) or (self.dict[key] is not value)):\n            self.dict[key] = value\n", "label": 1}
{"function": "\n\ndef visit_Subscript(self, node):\n    if ((not isinstance(node.ctx, ast.Load)) or (not isinstance(node.value, ast.Name))):\n        return\n    if (node.value.id != 'context'):\n        return\n    if ((not isinstance(node.slice, ast.Index)) or (not isinstance(node.slice.value, ast.Str))):\n        return\n    self.contextReads.add(node.slice.value.s)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _from_db_object(context, fixedip, db_fixedip, expected_attrs=None):\n    if (expected_attrs is None):\n        expected_attrs = []\n    for field in fixedip.fields:\n        if (field == 'default_route'):\n            continue\n        if (field not in FIXED_IP_OPTIONAL_ATTRS):\n            fixedip[field] = db_fixedip[field]\n    if ('instance' in expected_attrs):\n        fixedip.instance = (objects.Instance._from_db_object(context, objects.Instance(context), db_fixedip['instance']) if db_fixedip['instance'] else None)\n    if ('network' in expected_attrs):\n        fixedip.network = (objects.Network._from_db_object(context, objects.Network(context), db_fixedip['network']) if db_fixedip['network'] else None)\n    if ('virtual_interface' in expected_attrs):\n        db_vif = db_fixedip['virtual_interface']\n        vif = (objects.VirtualInterface._from_db_object(context, objects.VirtualInterface(context), db_fixedip['virtual_interface']) if db_vif else None)\n        fixedip.virtual_interface = vif\n    if ('floating_ips' in expected_attrs):\n        fixedip.floating_ips = obj_base.obj_make_list(context, objects.FloatingIPList(context), objects.FloatingIP, db_fixedip['floating_ips'])\n    fixedip._context = context\n    fixedip.obj_reset_changes()\n    return fixedip\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    for (name, value) in self._iter_contents():\n        if (key == name):\n            break\n    else:\n        raise KeyError(('Folder entry %s not found' % repr(key)))\n    if (value == '/'):\n        qname = quote(name)\n        child_ls = self.sub_tree[(qname + '.ls')]\n        try:\n            child_sub = self.sub_tree[(qname + '.sub')]\n        except KeyError:\n            child_sub = self.sub_tree.new_tree((qname + '.sub'))\n            self.storage._autocommit()\n        return StorageDir(name, child_ls, child_sub, ((self.path + name) + '/'), self.storage, self)\n    else:\n        inode = self.storage.get_inode(value)\n        return StorageFile(name, inode, self)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned = super(AuthorizeRequestTokenForm, self).clean()\n    t = Token.objects.get(id=cleaned.get('obj_id'))\n    default_scopes = t.scope.split(' ')\n    scopes = cleaned.get('scopes')\n    if (not scopes):\n        raise forms.ValidationError('You need to select permissions for the client')\n    if (('statements/read/mine' in scopes) and ('statements/read' in scopes)):\n        raise forms.ValidationError(\"'statements/read/mine' and 'statements/read' are conflicting scope values. choose one.\")\n    if ('all' in default_scopes):\n        return cleaned\n    elif ('all' in scopes):\n        raise forms.ValidationError(\"Can't raise permissions beyond what the consumer registered.\")\n    if (set(scopes) != set(default_scopes)):\n        nomatch = [k for k in scopes if (k not in default_scopes)]\n        if (not (('all/read' in nomatch) or (('statements/read' in nomatch) and ('all/read' in default_scopes)) or (('statements/read/mine' in nomatch) and (('all/read' in default_scopes) or ('statements/read' in default_scopes))))):\n            raise forms.ValidationError(\"Can't raise permissions beyond what the consumer registered.\")\n    return cleaned\n", "label": 1}
{"function": "\n\ndef elemwise(op, *args, **kwargs):\n    ' Elementwise operation for dask.Dataframes '\n    columns = kwargs.pop('columns', no_default)\n    _name = ('elemwise-' + tokenize(op, kwargs, *args))\n    args = _maybe_from_pandas(args)\n    from .multi import _maybe_align_partitions\n    args = _maybe_align_partitions(args)\n    dasks = [arg for arg in args if isinstance(arg, (_Frame, Scalar))]\n    dfs = [df for df in dasks if isinstance(df, _Frame)]\n    divisions = dfs[0].divisions\n    n = (len(divisions) - 1)\n    other = [(i, arg) for (i, arg) in enumerate(args) if (not isinstance(arg, (_Frame, Scalar)))]\n    if other:\n        op2 = partial_by_order(op, other)\n    else:\n        op2 = op\n    keys = [((d._keys() * n) if isinstance(d, Scalar) else d._keys()) for d in dasks]\n    dsk = dict((((_name, i), ((op2,) + frs)) for (i, frs) in enumerate(zip(*keys))))\n    dsk = merge(dsk, *[d.dask for d in dasks])\n    if (columns is no_default):\n        if ((len(dfs) >= 2) and (len(dasks) != len(dfs))):\n            msg = 'elemwise with 2 or more DataFrames and Scalar is not supported'\n            raise NotImplementedError(msg)\n        columns = _emulate(op, *args, **kwargs)\n    return _Frame(dsk, _name, columns, divisions)\n", "label": 1}
{"function": "\n\ndef do_migration(records):\n    database['boxnodesettings'].update({\n        'user_settings': {\n            '$type': 2,\n        },\n    }, {\n        '$rename': {\n            'user_settings': 'foreign_user_settings',\n        },\n    }, multi=True)\n    for user_addon in records:\n        user = user_addon.owner\n        old_account = user_addon.oauth_settings\n        logger.info('Record found for user {}'.format(user._id))\n        try:\n            account = ExternalAccount(provider='box', provider_name='Box', display_name=old_account.username, oauth_key=old_account.access_token, refresh_token=old_account.refresh_token, provider_id=old_account.user_id, expires_at=old_account.expires_at)\n            account.save()\n        except KeyExistsException:\n            account = ExternalAccount.find_one((Q('provider', 'eq', 'box') & Q('provider_id', 'eq', old_account.user_id)))\n            assert (account is not None)\n        user.external_accounts.append(account)\n        user.save()\n        user_addon.oauth_settings = None\n        user_addon.save()\n        logger.info('Added external account {0} to user {1}'.format(account._id, user._id))\n    for node in BoxNodeSettings.find():\n        if (node.foreign_user_settings is None):\n            continue\n        logger.info('Migrating user_settings for box {}'.format(node._id))\n        node.user_settings = node.foreign_user_settings\n        node.save()\n", "label": 0}
{"function": "\n\ndef test_uncollectable(self):\n    'Test uncollectable object tracking.\\n\\n        This is fixed in Python 3.4 (PEP 442).\\n        '\n    foo = Foo()\n    foo.parent = foo\n    enemy = Enemy()\n    enemy.parent = enemy\n    idfoo = id(foo)\n    idenemy = id(enemy)\n    del foo\n    del enemy\n    gb = GarbageGraph(collectable=0)\n    gfoo = [x for x in gb.metadata if (x.id == idfoo)]\n    self.assertEqual(len(gfoo), 0)\n    genemy = [x for x in gb.metadata if (x.id == idenemy)]\n    if (sys.version_info < (3, 4)):\n        self.assertEqual(len(genemy), 1)\n    self.assertEqual(gb.reduce_to_cycles(), None)\n", "label": 0}
{"function": "\n\ndef LoadAppInclude(app_include):\n    'Load a single AppInclude object where one and only one is expected.\\n\\n  Args:\\n    app_include: A file-like object or string.  If it is a string, parse it as\\n    a configuration file.  If it is a file-like object, read in data and\\n    parse.\\n\\n  Returns:\\n    An instance of AppInclude as loaded from a YAML file.\\n\\n  Raises:\\n    EmptyConfigurationFile: when there are no documents in YAML file.\\n    MultipleConfigurationFile: when there is more than one document in YAML\\n    file.\\n  '\n    builder = yaml_object.ObjectBuilder(AppInclude)\n    handler = yaml_builder.BuilderHandler(builder)\n    listener = yaml_listener.EventListener(handler)\n    listener.Parse(app_include)\n    includes = handler.GetResults()\n    if (len(includes) < 1):\n        raise appinfo_errors.EmptyConfigurationFile()\n    if (len(includes) > 1):\n        raise appinfo_errors.MultipleConfigurationFile()\n    includeyaml = includes[0]\n    if includeyaml.handlers:\n        for handler in includeyaml.handlers:\n            handler.FixSecureDefaults()\n            handler.WarnReservedURLs()\n    if includeyaml.builtins:\n        BuiltinHandler.Validate(includeyaml.builtins)\n    return includeyaml\n", "label": 0}
{"function": "\n\ndef _loop(self):\n    import jedi\n    while True:\n        data = stream_read(sys.stdin)\n        if (not isinstance(data, tuple)):\n            break\n        (source, line, col, filename) = data\n        log.debug('Line: %r, Col: %r, Filename: %r', line, col, filename)\n        completions = jedi.Script(source, line, col, filename).completions()\n        out = []\n        tmp_filecache = {\n            \n        }\n        for c in completions:\n            (name, type_, desc, abbr) = self.parse_completion(c, tmp_filecache)\n            kind = (type_ if (not self.use_short_types) else (_types.get(type_) or type_))\n            out.append((c.module_path, name, type_, desc, abbr, kind))\n        stream_write(sys.stdout, out)\n", "label": 0}
{"function": "\n\ndef walk(self, node_type=None):\n    'Walk through the query tree, returning nodes of a specific type.'\n    pending = [self._root]\n    while pending:\n        node = pending.pop()\n        if ((not node_type) or isinstance(node, node_type)):\n            (yield node)\n        if isinstance(node, nodes.UnaryOp):\n            pending.append(node.node)\n        elif isinstance(node, nodes.BinaryOp):\n            pending.extend([node.left, node.right])\n", "label": 0}
{"function": "\n\ndef validate(self, obj, value):\n    if (value is None):\n        if self._allow_none:\n            return value\n    if (not isinstance(value, basestring)):\n        self.error(obj, value)\n    for v in self.values:\n        if (v.lower() == value.lower()):\n            return v\n    self.error(obj, value)\n", "label": 0}
{"function": "\n\ndef _build_resource(self):\n    'Generate a resource for :meth:`begin`.'\n    resource = {\n        'query': self.query,\n    }\n    if (self.default_dataset is not None):\n        resource['defaultDataset'] = {\n            'projectId': self.project,\n            'datasetId': self.default_dataset.name,\n        }\n    if (self.max_results is not None):\n        resource['maxResults'] = self.max_results\n    if (self.preserve_nulls is not None):\n        resource['preserveNulls'] = self.preserve_nulls\n    if (self.timeout_ms is not None):\n        resource['timeoutMs'] = self.timeout_ms\n    if (self.use_query_cache is not None):\n        resource['useQueryCache'] = self.use_query_cache\n    return resource\n", "label": 0}
{"function": "\n\ndef test_initialization_legacy(self, NeuralNet):\n    input = Mock(__name__='InputLayer', __bases__=(InputLayer,))\n    (hidden1, hidden2, output) = [Mock(__name__='MockLayer', __bases__=(Layer,)) for i in range(3)]\n    nn = NeuralNet(layers=[('input', input), ('hidden1', hidden1), ('hidden2', hidden2), ('output', output)], input_shape=(10, 10), hidden1_some='param')\n    out = nn.initialize_layers(nn.layers)\n    input.assert_called_with(name='input', shape=(10, 10))\n    assert (nn.layers_['input'] is input.return_value)\n    hidden1.assert_called_with(incoming=input.return_value, name='hidden1', some='param')\n    assert (nn.layers_['hidden1'] is hidden1.return_value)\n    hidden2.assert_called_with(incoming=hidden1.return_value, name='hidden2')\n    assert (nn.layers_['hidden2'] is hidden2.return_value)\n    output.assert_called_with(incoming=hidden2.return_value, name='output')\n    assert (out is nn.layers_['output'])\n", "label": 0}
{"function": "\n\ndef parse(self, string, name='<string>'):\n    '\\n        Divide the given string into examples and intervening text,\\n        and return them as a list of alternating Examples and strings.\\n        Line numbers for the Examples are 0-based.  The optional\\n        argument `name` is a name identifying this string, and is only\\n        used for error messages.\\n        '\n    string = string.expandtabs()\n    min_indent = self._min_indent(string)\n    if (min_indent > 0):\n        string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n    output = []\n    (charno, lineno) = (0, 0)\n    for m in self._EXAMPLE_RE.finditer(string):\n        output.append(string[charno:m.start()])\n        lineno += string.count('\\n', charno, m.start())\n        (source, options, want, exc_msg) = self._parse_example(m, name, lineno)\n        if (not self._IS_BLANK_OR_COMMENT(source)):\n            output.append(doctest.Example(source, want, exc_msg, lineno=lineno, indent=(min_indent + len((m.group('indent') or m.group('runindent')))), options=options))\n        lineno += string.count('\\n', m.start(), m.end())\n        charno = m.end()\n    output.append(string[charno:])\n    return output\n", "label": 0}
{"function": "\n\ndef lineReceived(self, line):\n    line = line.strip()\n    self.log.debug('[sref:%s] Received line: %s', self.sessionRef, line)\n    (cmd, arg, line) = self.parseline(line)\n    if (self.sessionLineCallback is not None):\n        return self.sessionLineCallback(cmd, arg, line)\n    if (not line):\n        return self.sendData()\n    if ((cmd is None) or (cmd not in self.findCommands())):\n        return self.default(line)\n    funcName = ('do_' + cmd)\n    try:\n        func = getattr(self, funcName)\n    except AttributeError:\n        return self.default(line)\n    self.log.debug('[sref:%s] Running %s with arg:%s', self.sessionRef, funcName, arg)\n    return func(arg)\n", "label": 0}
{"function": "\n\ndef pre_build_check():\n    '\\n    Try to verify build tools\\n    '\n    if os.environ.get('CASS_DRIVER_NO_PRE_BUILD_CHECK'):\n        return True\n    try:\n        from distutils.ccompiler import new_compiler\n        from distutils.sysconfig import customize_compiler\n        from distutils.dist import Distribution\n        be = build_ext(Distribution())\n        be.initialize_options()\n        be.finalize_options()\n        have_python_include = any((os.path.isfile(os.path.join(p, 'Python.h')) for p in be.include_dirs))\n        if (not have_python_include):\n            sys.stderr.write((\"Did not find 'Python.h' in %s.\\n\" % (be.include_dirs,)))\n            return False\n        compiler = new_compiler(compiler=be.compiler)\n        customize_compiler(compiler)\n        executables = []\n        if (compiler.compiler_type in ('unix', 'cygwin')):\n            executables = [compiler.executables[exe][0] for exe in ('compiler_so', 'linker_so')]\n        elif (compiler.compiler_type == 'nt'):\n            executables = [getattr(compiler, exe) for exe in ('cc', 'linker')]\n        if executables:\n            from distutils.spawn import find_executable\n            for exe in executables:\n                if (not find_executable(exe)):\n                    sys.stderr.write(('Failed to find %s for compiler type %s.\\n' % (exe, compiler.compiler_type)))\n                    return False\n    except Exception as exc:\n        sys.stderr.write(('%s\\n' % str(exc)))\n        sys.stderr.write('Failed pre-build check. Attempting anyway.\\n')\n    return True\n", "label": 1}
{"function": "\n\ndef acquire(self, blocking=True):\n    \"Must be used with 'yield' as 'yield cv.acquire()'.\\n        \"\n    coro = AsynCoro.cur_coro()\n    if (self._owner == coro):\n        self._depth += 1\n        raise StopIteration(True)\n    if ((not blocking) and (self._owner is not None)):\n        raise StopIteration(False)\n    while (self._owner is not None):\n        self._waitlist.append(coro)\n        (yield coro._await_())\n    assert (self._depth == 0)\n    self._owner = coro\n    self._depth = 1\n    raise StopIteration(True)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (not isinstance(other, Vulnerability)):\n        raise TypeError((\"Expected Vulnerability, got '%s' instead\" % type(other)))\n    if (other.cves != self.cves):\n        return False\n    if (other.threat != self.threat):\n        return False\n    if (other.name != self.name):\n        return False\n    if (other.cvss != self.cvss):\n        return False\n    if (other.description != self.description):\n        return False\n    if (other.id != self.id):\n        return False\n    if (other.level != self.level):\n        return False\n    if (other.references != self.references):\n        return False\n    for (host, port) in self.hosts:\n        for (o_host, o_port) in other.hosts:\n            if ((o_host == host) and (o_port == port)):\n                break\n        else:\n            return False\n    return True\n", "label": 1}
{"function": "\n\n@property\ndef initial_selection(self):\n    'Initial selection matrix'\n    if (not (self.state_regression and self.time_varying_regression)):\n        if (self.k_posdef > 0):\n            selection = np.r_[(([0] * self._k_states_diff), ([1] * (self._k_order > 0)), ([0] * (self._k_order - 1)), ([0] * ((1 - self.mle_regression) * self.k_exog)))][:, None]\n        else:\n            selection = np.zeros((self.k_states, 0))\n    else:\n        selection = np.zeros((self.k_states, self.k_posdef))\n        if (self._k_order > 0):\n            selection[(0, 0)] = 1\n        for i in range(self.k_exog, 0, (- 1)):\n            selection[((- i), (- i))] = 1\n    return selection\n", "label": 0}
{"function": "\n\ndef update(self, **kw):\n    \"\\n        Shortcut for doing an UPDATE on this object.\\n\\n        If _signal=False is in ``kw`` the post_save signal won't be sent.\\n        \"\n    signal = kw.pop('_signal', True)\n    cls = self.__class__\n    for (k, v) in kw.items():\n        setattr(self, k, v)\n    if signal:\n        attrs = dict(self.__dict__)\n        models.signals.pre_save.send(sender=cls, instance=self)\n        for (k, v) in self.__dict__.items():\n            if (attrs[k] != v):\n                kw[k] = v\n                setattr(self, k, v)\n    cls.objects.filter(pk=self.pk).update(**kw)\n    if signal:\n        models.signals.post_save.send(sender=cls, instance=self, created=False)\n", "label": 0}
{"function": "\n\ndef dedup_views(window):\n    group = window.active_group()\n    for g in range(window.num_groups()):\n        found = dict()\n        views = window.views_in_group(g)\n        active = window.active_view_in_group(g)\n        for v in views:\n            if v.is_dirty():\n                continue\n            id = v.buffer_id()\n            if (id in found):\n                if (v == active):\n                    before = found[id]\n                    found[id] = v\n                    v = before\n                window.focus_view(v)\n                window.run_command('close')\n            else:\n                found[id] = v\n        window.focus_view(active)\n    window.focus_group(group)\n", "label": 0}
{"function": "\n\ndef enqueue(self, pdu):\n    self.log('enqueue {pdu.name} PDU'.format(pdu=pdu))\n    if (not (pdu.type in connection_mode_pdu_types)):\n        self.err('non connection mode pdu on data link connection')\n        pdu = FrameReject.from_pdu(pdu, flags='W', dlc=self)\n        self.close()\n        self.send_queue.append(pdu)\n        return\n    if self.state.CLOSED:\n        pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=1)\n        self.send_queue.append(pdu)\n    if self.state.LISTEN:\n        if isinstance(pdu, Connect):\n            if (super(DataLinkConnection, self).enqueue(pdu) == False):\n                log.warn('full backlog on listening socket')\n                pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=32)\n                self.send_queue.append(pdu)\n                return False\n            return True\n    if self.state.CONNECT:\n        if (isinstance(pdu, ConnectionComplete) or isinstance(pdu, DisconnectedMode)):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.DISCONNECT:\n        if isinstance(pdu, DisconnectedMode):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.ESTABLISHED:\n        return self._enqueue_state_established(pdu)\n", "label": 1}
{"function": "\n\ndef __call__(self, event):\n    '\\n        Write event to file.\\n\\n        @param event: An event.\\n        @type event: L{dict}\\n        '\n    text = self.formatEvent(event)\n    if (text is None):\n        text = ''\n    if ('log_failure' in event):\n        try:\n            traceback = event['log_failure'].getTraceback()\n        except Exception:\n            traceback = '(UNABLE TO OBTAIN TRACEBACK FROM EVENT)\\n'\n        text = '\\n'.join((text, traceback))\n    if (self._encoding is not None):\n        text = text.encode(self._encoding)\n    if text:\n        self._outFile.write(text)\n        self._outFile.flush()\n", "label": 0}
{"function": "\n\ndef extract_images(self):\n    from frappe.utils.file_manager import extract_images_from_html\n    if self.format_data:\n        data = json.loads(self.format_data)\n        for df in data:\n            if (df.get('fieldtype') and (df['fieldtype'] in ('HTML', 'Custom HTML')) and df.get('options')):\n                df['options'] = extract_images_from_html(self, df['options'])\n        self.format_data = json.dumps(data)\n", "label": 0}
{"function": "\n\ndef log_int_fixed(n, prec, ln2=None):\n    '\\n    Fast computation of log(n), caching the value for small n,\\n    intended for zeta sums.\\n    '\n    if (n in log_int_cache):\n        (value, vprec) = log_int_cache[n]\n        if (vprec >= prec):\n            return (value >> (vprec - prec))\n    wp = (prec + 10)\n    if (wp <= LOG_TAYLOR_SHIFT):\n        if (ln2 is None):\n            ln2 = ln2_fixed(wp)\n        r = bitcount(n)\n        x = (n << (wp - r))\n        v = (log_taylor_cached(x, wp) + (r * ln2))\n    else:\n        v = to_fixed(mpf_log(from_int(n), (wp + 5)), wp)\n    if (n < MAX_LOG_INT_CACHE):\n        log_int_cache[n] = (v, wp)\n    return (v >> (wp - prec))\n", "label": 0}
{"function": "\n\ndef getCovMatrix(self, x=None, z=None, mode=None):\n    self.checkInputGetCovMatrix(x, z, mode)\n    c = np.exp(self.hyp[0])\n    sf2 = np.exp((2.0 * self.hyp[1]))\n    ord = self.para[0]\n    if (np.abs((ord - np.round(ord))) < 1e-08):\n        ord = int(round(ord))\n    assert (ord >= 1.0)\n    ord = int(ord)\n    if (mode == 'self_test'):\n        (nn, D) = z.shape\n        A = np.reshape(np.sum((z * z), 1), (nn, 1))\n    elif (mode == 'train'):\n        (n, D) = x.shape\n        A = (np.dot(x, x.T) + (np.eye(n) * 1e-10))\n    elif (mode == 'cross'):\n        A = np.dot(x, z.T)\n    A = (sf2 * ((c + A) ** ord))\n    return A\n", "label": 0}
{"function": "\n\ndef _check_failure_put_connections(self, conns, req, nodes, min_conns):\n    '\\n        Identify any failed connections and check minimum connection count.\\n        '\n    if ((req.if_none_match is not None) and ('*' in req.if_none_match)):\n        statuses = [conn.resp.status for conn in conns if conn.resp]\n        if (HTTP_PRECONDITION_FAILED in statuses):\n            self.app.logger.debug(_('Object PUT returning 412, %(statuses)r'), {\n                'statuses': statuses,\n            })\n            raise HTTPPreconditionFailed(request=req)\n    if any((conn for conn in conns if (conn.resp and (conn.resp.status == HTTP_CONFLICT)))):\n        status_times = [('%(status)s (%(timestamp)s)' % {\n            'status': conn.resp.status,\n            'timestamp': HeaderKeyDict(conn.resp.getheaders()).get('X-Backend-Timestamp', 'unknown'),\n        }) for conn in conns if conn.resp]\n        self.app.logger.debug(_('Object PUT returning 202 for 409: %(req_timestamp)s <= %(timestamps)r'), {\n            'req_timestamp': req.timestamp.internal,\n            'timestamps': ', '.join(status_times),\n        })\n        raise HTTPAccepted(request=req)\n    self._check_min_conn(req, conns, min_conns)\n", "label": 1}
{"function": "\n\ndef _init_request_urls(self, api_urls):\n    '\\n        Returns a list of the API URLs.\\n        '\n    if (not isinstance(api_urls, (str, list, tuple))):\n        raise TypeError('api_urls needs to be string or iterable!')\n    if isinstance(api_urls, str):\n        api_urls = (api_urls,)\n    api_urls = list(api_urls)\n    for url in api_urls:\n        if ((not url.startswith('http://')) and (not url.startswith('https://'))):\n            raise ValueError(('URL \"%s\" contains an invalid or missing scheme' % url))\n    return list(api_urls)\n", "label": 0}
{"function": "\n\ndef api_serialize_impl(self, withClean=False):\n    Util.validate_type(withClean, 'bool')\n    missing = []\n    ret = {\n        \n    }\n    if (withClean or self.n_id):\n        Util.set_by_path(ret, 'ID', self.m_id)\n    if (withClean or self.n_name):\n        Util.set_by_path(ret, 'Name', self.m_name)\n    elif self.is_new:\n        missing.append('name')\n    if (withClean or self.n_description):\n        Util.set_by_path(ret, 'Description', self.m_description)\n    if (withClean or self.n_network_mask_len):\n        Util.set_by_path(ret, 'NetworkMaskLen', self.m_network_mask_len)\n    elif self.is_new:\n        missing.append('network_mask_len')\n    if (withClean or self.n_band_width_mbps):\n        Util.set_by_path(ret, 'BandWidthMbps', self.m_band_width_mbps)\n    elif self.is_new:\n        missing.append('band_width_mbps')\n    if (withClean or self.n_swytch_id):\n        Util.set_by_path(ret, 'Switch.ID', self.m_swytch_id)\n    if (len(missing) > 0):\n        raise SaklientException('required_field', ('Required fields must be set before the Router creation: ' + ', '.join(missing)))\n    return ret\n", "label": 1}
{"function": "\n\ndef _post_parse(self):\n    for test_id in self._unknown_entities:\n        matcher = (lambda i: ((i == test_id) or i.startswith(('%s.' % test_id))))\n        known_ids = filter(matcher, self._tests)\n        for id_ in known_ids:\n            if (self._tests[id_]['status'] == 'init'):\n                self._tests[id_]['status'] = self._unknown_entities[test_id]['status']\n            if self._unknown_entities[test_id].get('reason'):\n                self._tests[id_]['reason'] = self._unknown_entities[test_id]['reason']\n            elif self._unknown_entities[test_id].get('traceback'):\n                self._tests[id_]['traceback'] = self._unknown_entities[test_id]['traceback']\n    for test_id in self._expected_failures:\n        if self._tests.get(test_id):\n            if (self._tests[test_id]['status'] == 'fail'):\n                self._tests[test_id]['status'] = 'xfail'\n                if self._expected_failures[test_id]:\n                    self._tests[test_id]['reason'] = self._expected_failures[test_id]\n            elif (self._tests[test_id]['status'] == 'success'):\n                self._tests[test_id]['status'] = 'uxsuccess'\n    for test_id in self._tests:\n        for file_name in ['traceback', 'reason']:\n            if (file_name in self._tests[test_id]):\n                self._tests[test_id][file_name] = encodeutils.safe_decode(self._tests[test_id][file_name])\n    self._is_parsed = True\n", "label": 1}
{"function": "\n\ndef init_asd_db():\n    global asd_db\n    if (not asd_db):\n        try:\n            response = requests.get('https://raw.github.com/stefanschmidt/warranty/master/asdcheck')\n            for (model, val) in [model_str.strip().split(':') for model_str in response.content.split('\\n') if model_str.strip()]:\n                asd_db[model] = val\n        except:\n            asd_db = {\n                \n            }\n", "label": 0}
{"function": "\n\ndef _restore(self, obj):\n    if has_tag(obj, tags.B64):\n        restore = self._restore_base64\n    elif has_tag(obj, tags.BYTES):\n        restore = self._restore_quopri\n    elif has_tag(obj, tags.ID):\n        restore = self._restore_id\n    elif has_tag(obj, tags.REF):\n        restore = self._restore_ref\n    elif has_tag(obj, tags.ITERATOR):\n        restore = self._restore_iterator\n    elif has_tag(obj, tags.TYPE):\n        restore = self._restore_type\n    elif has_tag(obj, tags.REPR):\n        restore = self._restore_repr\n    elif has_tag(obj, tags.REDUCE):\n        restore = self._restore_reduce\n    elif has_tag(obj, tags.OBJECT):\n        restore = self._restore_object\n    elif has_tag(obj, tags.FUNCTION):\n        restore = self._restore_function\n    elif util.is_list(obj):\n        restore = self._restore_list\n    elif has_tag(obj, tags.TUPLE):\n        restore = self._restore_tuple\n    elif has_tag(obj, tags.SET):\n        restore = self._restore_set\n    elif util.is_dictionary(obj):\n        restore = self._restore_dict\n    else:\n        restore = (lambda x: x)\n    return restore(obj)\n", "label": 1}
{"function": "\n\ndef __str__(self, prefix='', printElemNumber=0):\n    res = ''\n    if self.has_class_or_file_name_:\n        res += (prefix + ('class_or_file_name: %s\\n' % self.DebugFormatString(self.class_or_file_name_)))\n    if self.has_line_number_:\n        res += (prefix + ('line_number: %s\\n' % self.DebugFormatInt32(self.line_number_)))\n    if self.has_function_name_:\n        res += (prefix + ('function_name: %s\\n' % self.DebugFormatString(self.function_name_)))\n    cnt = 0\n    for e in self.variables_:\n        elm = ''\n        if printElemNumber:\n            elm = ('(%d)' % cnt)\n        res += (prefix + ('variables%s <\\n' % elm))\n        res += e.__str__((prefix + '  '), printElemNumber)\n        res += (prefix + '>\\n')\n        cnt += 1\n    return res\n", "label": 0}
{"function": "\n\n@staticmethod\ndef canMatch(l1, l2):\n    if ((l1 is None) and (l2 is None)):\n        return True\n    elif ((l1 is None) or (l2 is None)):\n        return False\n    try:\n        l1.intersect(l2)\n        return True\n    except EmptyLabelSet:\n        return False\n", "label": 0}
{"function": "\n\ndef add_website_theme(context):\n    bootstrap = frappe.get_hooks('bootstrap')[0]\n    web_include_css = context.web_include_css\n    context.theme = frappe._dict()\n    if (not context.disable_website_theme):\n        website_theme = get_active_theme()\n        context.theme = ((website_theme and website_theme.as_dict()) or frappe._dict())\n        if website_theme:\n            if website_theme.bootstrap:\n                bootstrap = website_theme.bootstrap\n            context.no_sidebar = website_theme.no_sidebar\n            context.web_include_css = (['website_theme.css'] + context.web_include_css)\n    context.web_include_css = ([bootstrap] + context.web_include_css)\n", "label": 0}
{"function": "\n\ndef test_ignore_table_fields(self):\n    c1 = Column('A', format='L', array=[True, False])\n    c2 = Column('B', format='X', array=[[0], [1]])\n    c3 = Column('C', format='4I', dim='(2, 2)', array=[[0, 1, 2, 3], [4, 5, 6, 7]])\n    c4 = Column('B', format='X', array=[[1], [0]])\n    c5 = Column('C', format='4I', dim='(2, 2)', array=[[1, 2, 3, 4], [5, 6, 7, 8]])\n    ta = BinTableHDU.from_columns([c1, c2, c3])\n    tb = BinTableHDU.from_columns([c1, c4, c5])\n    diff = TableDataDiff(ta.data, tb.data, ignore_fields=['B', 'C'])\n    assert diff.identical\n    assert (len(diff.common_columns) == 1)\n    assert (diff.common_column_names == set(['a']))\n    assert (diff.diff_ratio == 0)\n    assert (diff.diff_total == 0)\n", "label": 0}
{"function": "\n\ndef _assert_contains(needle, haystack, invert):\n    matched = re.search(needle, haystack, re.M)\n    if ((invert and matched) or ((not invert) and (not matched))):\n        raise AssertionError((\"r'%s' %sfound in '%s'\" % (needle, ('' if invert else 'not '), haystack)))\n", "label": 0}
{"function": "\n\ndef get_definition(self, value):\n    '\\n        Get the definition site for the given variable name or instance.\\n        A Expr instance is returned.\\n        '\n    while True:\n        if isinstance(value, ir.Var):\n            name = value.name\n        elif isinstance(value, str):\n            name = value\n        else:\n            return value\n        defs = self.definitions[name]\n        if (len(defs) == 0):\n            raise KeyError(('no definition for %r' % (name,)))\n        if (len(defs) > 1):\n            raise KeyError(('more than one definition for %r' % (name,)))\n        value = defs[0]\n", "label": 0}
{"function": "\n\ndef add_proof(self, lhs, rhs):\n    'Adds a proof obligation to show the rhs is the representation of the lhs'\n    assert isinstance(lhs, Gen)\n    assert (lhs.prove == False)\n    assert isinstance(rhs, Gen)\n    assert (rhs.prove == True)\n    assert (self == lhs.zkp == rhs.zkp)\n    self.proofs += [(lhs, rhs)]\n", "label": 0}
{"function": "\n\ndef seed(self, seed=None):\n    '\\n        Re-initialize each random stream.\\n\\n        Parameters\\n        ----------\\n        seed : None or integer in range 0 to 2**30\\n            Each random stream will be assigned a unique state that depends\\n            deterministically on this value.\\n\\n        Returns\\n        -------\\n        None\\n\\n        '\n    if (seed is None):\n        seed = self.default_instance_seed\n    self.set_rstate(seed)\n    for (old_r, new_r, size, nstreams) in self.state_updates:\n        if (nstreams is None):\n            nstreams = self.n_streams(size)\n        rstates = self.get_substream_rstates(nstreams, new_r.owner.outputs[1].dtype)\n        assert (old_r.get_value(borrow=True, return_internal_type=True).shape == rstates.shape)\n        assert (rstates.dtype == old_r.dtype)\n        old_r.set_value(rstates, borrow=True)\n", "label": 0}
{"function": "\n\ndef load(self, fname=None, name=None):\n    if (name is None):\n        self._mName = fname\n    else:\n        self._mName = name\n    if (fname is not None):\n        if os.path.exists(fname):\n            self._fhandle = os.path.abspath(fname)\n        else:\n            self._fhandle = os.path.join(LAUNCH_PATH, 'Features', 'HaarCascades', fname)\n            if (not os.path.exists(self._fhandle)):\n                logger.warning(('Could not find Haar Cascade file ' + fname))\n                logger.warning('Try running the function img.listHaarFeatures() to see what is available')\n                return None\n        self._mCascade = cv.Load(self._fhandle)\n        if HaarCascade._cache.has_key(self._fhandle):\n            self._mCascade = HaarCascade._cache[fname]\n            return\n        HaarCascade._cache[self._fhandle] = self._mCascade\n    else:\n        logger.warning('No file path mentioned.')\n", "label": 0}
{"function": "\n\ndef listdir(self, path, start_time=None, end_time=None):\n    '\\n        Get an iterable with S3 folder contents.\\n        Iterable contains paths relative to queried path.\\n\\n        :param start_time: Optional argument to copy files with modified dates after start_time\\n        :param end_time: Optional argument to copy files with modified dates before end_time\\n        '\n    (bucket, key) = self._path_to_bucket_and_key(path)\n    s3_bucket = self.s3.get_bucket(bucket, validate=True)\n    key_path = self._add_path_delimiter(key)\n    key_path_len = len(key_path)\n    for item in s3_bucket.list(prefix=key_path):\n        last_modified_date = time.strptime(item.last_modified, '%Y-%m-%dT%H:%M:%S.%fZ')\n        if (((not start_time) and (not end_time)) or (start_time and (not end_time) and (start_time < last_modified_date)) or ((not start_time) and end_time and (last_modified_date < end_time)) or (start_time and end_time and (start_time < last_modified_date < end_time))):\n            (yield (self._add_path_delimiter(path) + item.key[key_path_len:]))\n", "label": 1}
{"function": "\n\ndef __init__(self, *items):\n    DiagramItem.__init__(self, 'g')\n    self.items = [wrapString(item) for item in items]\n    self.width = sum(((item.width + (20 if item.needsSpace else 0)) for item in self.items))\n    self.up = 0\n    self.down = 0\n    for item in self.items:\n        self.up = max(self.up, (item.up - self.yAdvance))\n        self.down = max(self.down, (item.down + self.yAdvance))\n        self.yAdvance += item.yAdvance\n    if DEBUG:\n        self.attrs['data-updown'] = '{0} {1}'.format(self.up, self.down)\n        self.attrs['data-type'] = 'sequence'\n", "label": 0}
{"function": "\n\ndef compose_doc(self, current_line, edit):\n    params_match = re.search('def +[^ (]+[ (]*([^)]*)\\\\)?', current_line)\n    if params_match:\n        if re.search('def initialize*', current_line):\n            return self.initialize_doc(params_match, current_line)\n        else:\n            return self.method_doc(params_match, current_line)\n    params_match = re.search('class | module', current_line)\n    if params_match:\n        return self.class_doc(params_match, current_line)\n    params_match = re.search('[A-Z]+[ ]+=', current_line)\n    if params_match:\n        return self.const_doc(params_match, current_line)\n    params_match = re.search('attr_reader | attr_writer | attr_accessor ', current_line)\n    if params_match:\n        return self.attributes_doc(params_match, current_line)\n", "label": 0}
{"function": "\n\ndef write(self, data):\n    if (self.paused or ((not self.iAmStreaming) and (not self.outstandingPull))):\n        self._buffer.append(data)\n    elif (self.consumer is not None):\n        assert (not self._buffer), 'Writing fresh data to consumer before my buffer is empty!'\n        bytesSent = self._writeSomeData(data)\n        self.outstandingPull = False\n        if (not (bytesSent == len(data))):\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer.append(data[bytesSent:])\n    if ((self.producer is not None) and self.producerIsStreaming):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (bytesBuffered >= self.bufferSize):\n            self.producer.pauseProducing()\n            self.producerPaused = True\n", "label": 1}
{"function": "\n\ndef run(self, name, board_id, list_id, api_key=None, token=None):\n    if api_key:\n        self._set_creds(api_key=api_key, token=token)\n    cards = []\n    board = self._client().get_board(board_id)\n    lst = board.get_list(list_id)\n    for card in lst.list_cards():\n        if ((card.name == name) and (not card.closed)):\n            cards.append(card.id)\n    if (len(cards) == 0):\n        return False\n    else:\n        return cards\n", "label": 0}
{"function": "\n\ndef cmd(self, *args, **kwargs):\n    data = self.main(*args, **kwargs)\n    result = CommandResult()\n    for (key, value) in data.items():\n        result = result.add_line('Ran build plugin {plugin}', plugin=key)\n        if (not value):\n            continue\n        if (not isinstance(value, basestring)):\n            value = json.dumps(value, indent=4, sort_keys=True)\n        for line in value.split('\\n'):\n            if (not line.strip()):\n                continue\n            result = result.add_line('\\t{line}', line=line)\n    return result\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('RemoteControlInstance')\n    if (self.device is not None):\n        oprot.writeFieldBegin('device', TType.STRUCT, 1)\n        self.device.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.timeout is not None):\n        oprot.writeFieldBegin('timeout', TType.I32, 2)\n        oprot.writeI32(self.timeout)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\ndef update_record(self, record, name=None, type=None, data=None, extra=None):\n    extra = (extra or {\n        \n    })\n    params = {\n        'z': record.zone.domain,\n        'id': record.id,\n    }\n    params['name'] = (name or record.name)\n    params['type'] = (type or record.type)\n    params['content'] = (data or record.data)\n    params['ttl'] = (extra.get('ttl', None) or record.extra['ttl'])\n    self.connection.set_context({\n        'zone_domain': record.zone.domain,\n    })\n    self.connection.set_context({\n        'record_id': record.id,\n    })\n    resp = self.connection.request(action='rec_edit', params=params)\n    item = resp.object['response']['rec']['obj']\n    record = self._to_record(zone=record.zone, item=item)\n    return record\n", "label": 0}
{"function": "\n\ndef write_fasta(ofile, s, chunk=60, id=None, reformatter=None):\n    'Trivial FASTA output'\n    if (id is None):\n        try:\n            id = str(s.id)\n        except AttributeError:\n            id = new_seq_id()\n    ofile.write((('>' + id) + '\\n'))\n    seq = str(s)\n    if (reformatter is not None):\n        seq = reformatter(seq)\n    end = len(seq)\n    pos = 0\n    while 1:\n        ofile.write((seq[pos:(pos + chunk)] + '\\n'))\n        pos += chunk\n        if (pos >= end):\n            break\n    return id\n", "label": 0}
{"function": "\n\ndef get_recipient_info(self, message, contact_cache):\n    recipient_id = message.couch_recipient\n    if (recipient_id in contact_cache):\n        return contact_cache[recipient_id]\n    doc = None\n    if (recipient_id not in [None, '']):\n        try:\n            if (message.couch_recipient_doc_type == 'CommCareCase'):\n                doc = CommCareCase.get(recipient_id)\n            else:\n                doc = CouchUser.get_by_user_id(recipient_id)\n        except Exception:\n            pass\n    if doc:\n        doc_info = get_doc_info(doc.to_json(), self.domain)\n    else:\n        doc_info = None\n    contact_cache[recipient_id] = doc_info\n    return doc_info\n", "label": 0}
{"function": "\n\ndef mkdir(self, path, parents=True, raise_if_exists=False):\n    if (raise_if_exists and self.isdir(path)):\n        raise FileAlreadyExists()\n    (_, key) = self._path_to_bucket_and_key(path)\n    if self._is_root(key):\n        return\n    key = self._add_path_delimiter(key)\n    if ((not parents) and (not self.isdir(os.path.dirname(key)))):\n        raise MissingParentDirectory()\n    return self.put_string('', self._add_path_delimiter(path))\n", "label": 0}
{"function": "\n\ndef cleanup(self, _warn=False):\n    if (self.name and (not self._closed)):\n        try:\n            self._rmtree(self.name)\n        except (TypeError, AttributeError) as ex:\n            if ('None' not in str(ex)):\n                raise\n            print_('ERROR: {!r} while cleaning up {!r}'.format(ex, self), file=_sys.stderr)\n            return\n        self._closed = True\n        if _warn:\n            self._warn('Implicitly cleaning up {!r}'.format(self), Warning)\n", "label": 0}
{"function": "\n\ndef test_as_coeff_exponent():\n    assert ((3 * (x ** 4)).as_coeff_exponent(x) == (3, 4))\n    assert ((2 * (x ** 3)).as_coeff_exponent(x) == (2, 3))\n    assert ((4 * (x ** 2)).as_coeff_exponent(x) == (4, 2))\n    assert ((6 * (x ** 1)).as_coeff_exponent(x) == (6, 1))\n    assert ((3 * (x ** 0)).as_coeff_exponent(x) == (3, 0))\n    assert ((2 * (x ** 0)).as_coeff_exponent(x) == (2, 0))\n    assert ((1 * (x ** 0)).as_coeff_exponent(x) == (1, 0))\n    assert ((0 * (x ** 0)).as_coeff_exponent(x) == (0, 0))\n    assert (((- 1) * (x ** 0)).as_coeff_exponent(x) == ((- 1), 0))\n    assert (((- 2) * (x ** 0)).as_coeff_exponent(x) == ((- 2), 0))\n    assert (((2 * (x ** 3)) + (pi * (x ** 3))).as_coeff_exponent(x) == ((2 + pi), 3))\n    assert (((x * log(2)) / ((2 * x) + (pi * x))).as_coeff_exponent(x) == ((log(2) / (2 + pi)), 0))\n    D = Derivative\n    f = Function('f')\n    fx = D(f(x), x)\n    assert (fx.as_coeff_exponent(f(x)) == (fx, 0))\n", "label": 1}
{"function": "\n\ndef _absolute_path(path, relative_to=None):\n    '\\n    Return an absolute path. In case ``relative_to`` is passed and ``path`` is\\n    not an absolute path, we try to prepend ``relative_to`` to ``path``and if\\n    that path exists, return that one\\n    '\n    if (path and os.path.isabs(path)):\n        return path\n    if (path and (relative_to is not None)):\n        _abspath = os.path.join(relative_to, path)\n        if os.path.isfile(_abspath):\n            log.debug(\"Relative path '{0}' converted to existing absolute path '{1}'\".format(path, _abspath))\n            return _abspath\n    return path\n", "label": 0}
{"function": "\n\ndef run(self, count=None, region=None, metadata=None):\n    if region:\n        cs = self.pyrax.connect_to_cloudservers(region=region)\n    else:\n        cs = self.pyrax.cloudservers\n    servers = cs.list()\n    result = []\n    for server in servers:\n        item = to_server_dict(server=server)\n        if metadata:\n            include = self._metadata_intersection(server=item, metadata=metadata)\n            if (not include):\n                continue\n        result.append(item['name'])\n    if count:\n        return result[0:count]\n    else:\n        return result\n", "label": 0}
{"function": "\n\n@throttle(0.1)\ndef update_panel(self):\n    window = sublime.active_window()\n    try:\n        result_panel = window.create_output_panel('test_runner')\n    except:\n        result_panel = window.get_output_panel('test_runner')\n    result_panel.set_syntax_file('Packages/Test Runner/TestRunnerOutput.tmLanguage')\n    result_panel.run_command('update_panel', {\n        'message': self.result['message'],\n    })\n    if ((self.result['failed'] > 0) or settings.get('show_panel_default', False)):\n        window.run_command('show_panel', {\n            'panel': 'output.test_runner',\n        })\n    elif (self.result['status'] == 'executed'):\n        window.run_command('hide_panel', {\n            'panel': 'output.test_runner',\n        })\n    if self.is_alive():\n        self.update_panel()\n", "label": 0}
{"function": "\n\ndef test_assert_has_calls(self):\n    kalls1 = [call(1, 2), ({\n        'a': 3,\n    },), ((3, 4),), call(b=6), ('', (1,), {\n        'b': 6,\n    })]\n    kalls2 = [call.foo(), call.bar(1)]\n    kalls2.extend(call.spam().baz(a=3).call_list())\n    kalls2.extend(call.bam(set(), foo={\n        \n    }).fish([1]).call_list())\n    mocks = []\n    for mock in (Mock(), MagicMock()):\n        mock(1, 2)\n        mock(a=3)\n        mock(3, 4)\n        mock(b=6)\n        mock(1, b=6)\n        mocks.append((mock, kalls1))\n    mock = Mock()\n    mock.foo()\n    mock.bar(1)\n    mock.spam().baz(a=3)\n    mock.bam(set(), foo={\n        \n    }).fish([1])\n    mocks.append((mock, kalls2))\n    for (mock, kalls) in mocks:\n        for i in range(len(kalls)):\n            for step in (1, 2, 3):\n                these = kalls[i:(i + step)]\n                mock.assert_has_calls(these)\n                if (len(these) > 1):\n                    self.assertRaises(AssertionError, mock.assert_has_calls, list(reversed(these)))\n", "label": 0}
{"function": "\n\ndef onSelectDet(self, event=None, index=0, init=False, **kws):\n    if (index > 0):\n        self.det_fore = index\n    self.det_back = self.wids['bkg_det'].GetSelection()\n    if (self.det_fore == self.det_back):\n        self.det_back = 0\n    for i in range(1, (self.nmca + 1)):\n        dname = ('det%i' % i)\n        bcol = (220, 220, 220)\n        fcol = (0, 0, 0)\n        if (i == self.det_fore):\n            bcol = (60, 50, 245)\n            fcol = (240, 230, 100)\n        if (i == self.det_back):\n            bcol = (80, 200, 20)\n        self.wids[dname].SetBackgroundColour(bcol)\n        self.wids[dname].SetForegroundColour(fcol)\n    self.clear_mcas()\n    self.show_mca(init=init)\n    self.Refresh()\n", "label": 0}
{"function": "\n\ndef _ensure_holidays_span_datetime(self, dt):\n    if callable(self.holidays):\n        if ((self._holidaysGeneratorStart is None) or (dt < self._holidaysGeneratorStart)):\n            self._holidaysGeneratorStart = dt\n            self._holidaysGenerator = self.holidays(dt)\n        while ((len(self._holidays) == 0) or (dt > self._holidays[(- 1)])):\n            self._holidays.append(next(self._holidaysGenerator))\n", "label": 0}
{"function": "\n\ndef add_block_proposal(self, p):\n    assert isinstance(p, BlockProposal)\n    if self.has_blockproposal(p.blockhash):\n        self.log('known block_proposal')\n        return\n    assert p.signing_lockset.has_quorum\n    assert (p.signing_lockset.height == (p.height - 1))\n    for v in p.signing_lockset:\n        self.add_vote(v)\n    self.block_candidates[p.blockhash] = p\n", "label": 0}
{"function": "\n\ndef dates(self):\n    'returns the years and months for which there are posts'\n    if o_settings.CACHE_ENABLED:\n        key = get_key('posts_dates')\n        cached = cache.get(key)\n        if cached:\n            return cached\n    posts = self.published()\n    dates = OrderedDict()\n    for post in posts:\n        key = post.created.strftime('%Y_%m')\n        try:\n            dates[key][1] = (dates[key][1] + 1)\n        except KeyError:\n            dates[key] = [post.created, 1]\n    if o_settings.CACHE_ENABLED:\n        cache.set(key, dates, o_settings.CACHE_TIMEOUT)\n    return dates\n", "label": 0}
{"function": "\n\ndef proxy_request(service_name, instance_name, method, path, body=None, headers=None):\n    target = get_env('TSURU_TARGET').rstrip('/')\n    token = get_env('TSURU_TOKEN')\n    if ((not target.startswith('http://')) and (not target.startswith('https://'))):\n        target = 'http://{}'.format(target)\n    url = '{}/services/{}/proxy/{}?callback={}'.format(target, service_name, instance_name, path)\n    if body:\n        body = json.dumps(body)\n    request = Request(method, url, data=body)\n    request.add_header('Authorization', ('bearer ' + token))\n    if headers:\n        for (header, value) in headers.items():\n            request.add_header(header, value)\n    return urllib2.urlopen(request, timeout=30)\n", "label": 0}
{"function": "\n\ndef test_path_in_several_ways(self):\n    alice = Node(name='Alice')\n    bob = Node(name='Bob')\n    carol = Node(name='Carol')\n    dave = Node(name='Dave')\n    path = Path(alice, 'LOVES', bob, Relationship(carol, 'HATES', bob), carol, 'KNOWS', dave)\n    assert path.__bool__()\n    assert path.__nonzero__()\n    assert (path[0] == Relationship(alice, 'LOVES', bob))\n    assert (path[1] == Relationship(carol, 'HATES', bob))\n    assert (path[2] == Relationship(carol, 'KNOWS', dave))\n    assert (path[(- 1)] == Relationship(carol, 'KNOWS', dave))\n    assert (path[0:1] == Path(alice, 'LOVES', bob))\n    assert (path[0:2] == Path(alice, 'LOVES', bob, Relationship(carol, 'HATES', bob), carol))\n    try:\n        _ = path[7]\n    except IndexError:\n        assert True\n    else:\n        assert False\n", "label": 1}
{"function": "\n\ndef _verify_source_estimate_compat(a, b):\n    'Make sure two SourceEstimates are compatible for arith. operations'\n    compat = False\n    if (len(a.vertices) == len(b.vertices)):\n        if all((np.array_equal(av, vv) for (av, vv) in zip(a.vertices, b.vertices))):\n            compat = True\n    if (not compat):\n        raise ValueError('Cannot combine SourceEstimates that do not have the same vertices. Consider using stc.expand().')\n    if (a.subject != b.subject):\n        raise ValueError(('source estimates do not have the same subject names, %r and %r' % (a.subject, b.subject)))\n", "label": 0}
{"function": "\n\ndef test_repeat_get_url_interval(url_prefix):\n    session = RateLimitRequests(url_interval=1)\n    t = datetime.now()\n    resp1 = session.get((url_prefix + '/cookies'))\n    session.get((url_prefix + '/cookies/set?a=b'))\n    resp2 = session.get((url_prefix + '/cookies'))\n    assert ((datetime.now() - t) < timedelta(seconds=1))\n    assert (resp1 == resp2)\n    assert (resp1.json()['cookies'] == resp2.json()['cookies'])\n    time.sleep(1)\n    resp3 = session.get((url_prefix + '/cookies'))\n    assert (resp1 != resp3)\n    assert (resp1.json()['cookies'] != resp3.json()['cookies'])\n", "label": 0}
{"function": "\n\ndef processLine(result, line):\n    initLine(result, line)\n    if (result['mode'] == INDENT_MODE):\n        result['trackingIndent'] = ((len(result['parenStack']) != 0) and (not result['isInStr']))\n    elif (result['mode'] == PAREN_MODE):\n        result['trackingIndent'] = (not result['isInStr'])\n    chars = (line + NEWLINE)\n    for c in chars:\n        processChar(result, c)\n    if (result['lineNo'] == result['parenTrail']['lineNo']):\n        finishNewParenTrail(result)\n", "label": 0}
{"function": "\n\ndef _update_document_fields_positional(self, doc, fields, spec, updater, subdocument=None):\n    'Implements the $set behavior on an existing document'\n    for (k, v) in iteritems(fields):\n        if ('$' in k):\n            field_name_parts = k.split('.')\n            if (not subdocument):\n                current_doc = doc\n                subspec = spec\n                for part in field_name_parts[:(- 1)]:\n                    if (part == '$'):\n                        subspec = subspec.get('$elemMatch', subspec)\n                        for item in current_doc:\n                            if filter_applies(subspec, item):\n                                current_doc = item\n                                break\n                        continue\n                    new_spec = {\n                        \n                    }\n                    for el in subspec:\n                        if el.startswith(part):\n                            if (len(el.split('.')) > 1):\n                                new_spec['.'.join(el.split('.')[1:])] = subspec[el]\n                            else:\n                                new_spec = subspec[el]\n                    subspec = new_spec\n                    current_doc = current_doc[part]\n                subdocument = current_doc\n                if ((field_name_parts[(- 1)] == '$') and isinstance(subdocument, list)):\n                    for (i, doc) in enumerate(subdocument):\n                        if filter_applies(subspec, doc):\n                            subdocument[i] = v\n                            break\n                    continue\n            updater(subdocument, field_name_parts[(- 1)], v)\n            continue\n        self._update_document_single_field(doc, k, v, updater)\n    return subdocument\n", "label": 1}
{"function": "\n\ndef export_to_csv(self, model_name):\n    self.header('Exporting models ...')\n    today = datetime.datetime.today()\n    model = get_model('calaccess_campaign_browser', model_name)\n    fieldnames = ([f.name for f in model._meta.fields] + ['committee_name', 'filer_name', 'filer_id', 'filer_id_raw'])\n    relation_names = ([f.name for f in model._meta.fields] + ['committee__name', 'committee__filer__name', 'committee__filer__id', 'committee__filer__filer_id_raw'])\n    filename = '{}-{}-{}-{}.csv'.format(today.year, today.month, today.day, model_name.lower())\n    filepath = os.path.join(self.data_dir, filename)\n    self.header('  Exporting {} model ...'.format(model_name.capitalize()))\n    with open(filepath, 'wb') as csvfile:\n        writer = csv.writer(csvfile, delimiter='\\t')\n        writer.writerow(fieldnames)\n        if (model_name != 'summary'):\n            for cycle in Cycle.objects.all():\n                self.log('    Looking at cycle {} ...'.format(cycle.name))\n                rows = model.objects.filter(cycle=cycle).exclude(is_duplicate=True).values_list(*relation_names)\n                if (not rows):\n                    self.failure('      No data for {}'.format(cycle.name))\n                else:\n                    rows = self.encoded(rows)\n                    writer.writerows(rows)\n                    self.success('      Added {} {} data'.format(cycle.name, model_name))\n        else:\n            rows = self.encoded(model.objects.values_list())\n            writer.writerows(rows)\n    self.success('  Exported {}!'.format(model_name.capitalize()))\n", "label": 0}
{"function": "\n\ndef do_copy(self, new_ids=False, id_scope=None, id_remap=None):\n    cp = DBWorkflowExec()\n    cp._db_id = self._db_id\n    cp._db_user = self._db_user\n    cp._db_ip = self._db_ip\n    cp._db_vt_version = self._db_vt_version\n    cp._db_ts_start = self._db_ts_start\n    cp._db_ts_end = self._db_ts_end\n    cp._db_parent_id = self._db_parent_id\n    cp._db_parent_type = self._db_parent_type\n    cp._db_parent_version = self._db_parent_version\n    cp._db_name = self._db_name\n    if (self._db_module_execs is None):\n        cp._db_module_execs = []\n    else:\n        cp._db_module_execs = [v.do_copy(new_ids, id_scope, id_remap) for v in self._db_module_execs]\n    if new_ids:\n        new_id = id_scope.getNewId(self.vtType)\n        if (self.vtType in id_scope.remap):\n            id_remap[(id_scope.remap[self.vtType], self._db_id)] = new_id\n        else:\n            id_remap[(self.vtType, self._db_id)] = new_id\n        cp._db_id = new_id\n    for v in cp._db_module_execs:\n        cp.db_module_execs_id_index[v._db_id] = v\n    cp.is_dirty = self.is_dirty\n    cp.is_new = self.is_new\n    return cp\n", "label": 0}
{"function": "\n\ndef data(self):\n    if (not hasattr(self, '_data')):\n        request = URLRequest(self.url)\n        if (self.env and self.env.cache):\n            headers = self.env.cache.get(('url', 'headers', self.url))\n            if headers:\n                (etag, lmod) = headers\n                if etag:\n                    request.add_header('If-None-Match', etag)\n                if lmod:\n                    request.add_header('If-Modified-Since', lmod)\n        try:\n            response = urlopen(request)\n        except HTTPError as e:\n            if (e.code != 304):\n                raise\n            self._data = self.env.cache.get(('url', 'contents', self.url))\n        else:\n            with contextlib.closing(response):\n                self._data = response.read()\n            if (self.env and self.env.cache):\n                self.env.cache.set(('url', 'headers', self.url), (response.headers.get('ETag'), response.headers.get('Last-Modified')))\n                self.env.cache.set(('url', 'contents', self.url), self._data)\n    return self._data\n", "label": 1}
{"function": "\n\ndef respond(self, message, user=None):\n    if ('brb' in message.lower()):\n        matches = re.findall(self.regex, message.lower())\n        if matches:\n            now = datetime.now()\n            due = (now + timedelta(minutes=int(matches[0])))\n            self.memory[user] = due\n    elif ('all: back' in message.lower()):\n        if (user in self.memory.keys()):\n            due = self.memory[user]\n            now = datetime.now()\n            if (now > due):\n                message = ((user + ': ') + 'You are late. :)')\n                self.speak(message)\n            self.memory.pop(user)\n    else:\n        pass\n", "label": 0}
{"function": "\n\ndef headers_received(self, start_line, headers):\n    if (start_line.code != 101):\n        return super(WebSocketClientConnection, self).headers_received(start_line, headers)\n    self.headers = headers\n    assert (self.headers['Upgrade'].lower() == 'websocket')\n    assert (self.headers['Connection'].lower() == 'upgrade')\n    accept = WebSocketProtocol13.compute_accept_value(self.key)\n    assert (self.headers['Sec-Websocket-Accept'] == accept)\n    self.protocol = WebSocketProtocol13(self, mask_outgoing=True)\n    self.protocol._receive_frame()\n    if (self._timeout is not None):\n        self.io_loop.remove_timeout(self._timeout)\n        self._timeout = None\n    self.stream = self.connection.detach()\n    self.stream.set_close_callback(self.on_connection_close)\n    self.final_callback = None\n    self.connect_future.set_result(self)\n", "label": 0}
{"function": "\n\ndef _var_beta_panel(y, x, beta, xx, rmse, cluster_axis, nw_lags, nobs, df, nw_overlap):\n    xx_inv = math.inv(xx)\n    yv = y.values\n    if (cluster_axis is None):\n        if (nw_lags is None):\n            return (xx_inv * (rmse ** 2))\n        else:\n            resid = (yv - np.dot(x.values, beta))\n            m = (x.values.T * resid).T\n            xeps = math.newey_west(m, nw_lags, nobs, df, nw_overlap)\n            return np.dot(xx_inv, np.dot(xeps, xx_inv))\n    else:\n        Xb = np.dot(x.values, beta).reshape((len(x.values), 1))\n        resid = DataFrame((yv[:, None] - Xb), index=y.index, columns=['resid'])\n        if (cluster_axis == 1):\n            x = x.swaplevel(0, 1).sortlevel(0)\n            resid = resid.swaplevel(0, 1).sortlevel(0)\n        m = _group_agg((x.values * resid.values), x.index._bounds, (lambda x: np.sum(x, axis=0)))\n        if (nw_lags is None):\n            nw_lags = 0\n        xox = 0\n        for i in range(len(x.index.levels[0])):\n            xox += math.newey_west(m[i:(i + 1)], nw_lags, nobs, df, nw_overlap)\n        return np.dot(xx_inv, np.dot(xox, xx_inv))\n", "label": 0}
{"function": "\n\ndef __init__(self, module, module_filename=None, template=None, template_filename=None, module_source=None, template_source=None, output_encoding=None, encoding_errors='strict', disable_unicode=False, bytestring_passthrough=False, format_exceptions=False, error_handler=None, lookup=None, cache_args=None, cache_impl='beaker', cache_enabled=True, cache_type=None, cache_dir=None, cache_url=None):\n    self.module_id = re.sub('\\\\W', '_', module._template_uri)\n    self.uri = module._template_uri\n    self.input_encoding = module._source_encoding\n    self.output_encoding = output_encoding\n    self.encoding_errors = encoding_errors\n    self.disable_unicode = disable_unicode\n    self.bytestring_passthrough = (bytestring_passthrough or disable_unicode)\n    self.enable_loop = module._enable_loop\n    if (compat.py3k and disable_unicode):\n        raise exceptions.UnsupportedError('Mako for Python 3 does not support disabling Unicode')\n    elif (output_encoding and disable_unicode):\n        raise exceptions.UnsupportedError('output_encoding must be set to None when disable_unicode is used.')\n    self.module = module\n    self.filename = template_filename\n    ModuleInfo(module, module_filename, self, template_filename, module_source, template_source)\n    self.callable_ = self.module.render_body\n    self.format_exceptions = format_exceptions\n    self.error_handler = error_handler\n    self.lookup = lookup\n    self._setup_cache_args(cache_impl, cache_enabled, cache_args, cache_type, cache_dir, cache_url)\n", "label": 0}
{"function": "\n\ndef find_consecutive_slots(self, num_consecutive):\n    if (num_consecutive == 1):\n        return self.find_slot()\n    for i in range(len(self._data)):\n        any_taken = False\n        for k in range(num_consecutive):\n            if self._data[(i + k)]:\n                any_taken = True\n                break\n        if (not any_taken):\n            return i\n    return (- 1)\n", "label": 0}
{"function": "\n\ndef get_field_parameters(self):\n    params = {\n        \n    }\n    if self.nullable:\n        params['null'] = True\n    if ((self.field_class is ForeignKeyField) or (self.name != self.db_column)):\n        params['db_column'] = (\"'%s'\" % self.db_column)\n    if (self.primary_key and (self.field_class is not PrimaryKeyField)):\n        params['primary_key'] = True\n    if self.is_foreign_key():\n        params['rel_model'] = self.rel_model\n        if self.to_field:\n            params['to_field'] = (\"'%s'\" % self.to_field)\n        if self.related_name:\n            params['related_name'] = (\"'%s'\" % self.related_name)\n    if (not self.is_primary_key()):\n        if self.unique:\n            params['unique'] = 'True'\n        elif (self.index and (not self.is_foreign_key())):\n            params['index'] = 'True'\n    return params\n", "label": 1}
{"function": "\n\n@fails('aminator.provisioner.provision_scripts.error')\n@lapse('aminator.provisioner.provision_scripts.duration')\ndef _run_provision_scripts(self, scripts_dir):\n    '\\n        execute every python or shell script found in scripts_dir\\n            1. run python or shell scripts in lexical order\\n\\n        :param scripts_dir: path in chroot to look for python and shell scripts\\n        :return: None\\n        '\n    script_files = sorted((glob((scripts_dir + '/*.py')) + glob((scripts_dir + '/*.sh'))))\n    if (not script_files):\n        log.debug('no python or shell scripts found in {0}'.format(scripts_dir))\n    else:\n        log.debug('found scripts {0} in {1}'.format(script_files, scripts_dir))\n        for script in script_files:\n            log.debug('executing script {0}'.format(script))\n            if os.access(script, os.X_OK):\n                result = run_script(script)\n            elif script.endswith('.py'):\n                result = run_script(['python', script])\n            else:\n                result = run_script(['sh', script])\n            if (not result.success):\n                log.critical('script failed: {0}: {1.std_err}'.format(script, result.result))\n                return False\n    return True\n", "label": 0}
{"function": "\n\ndef build_suite(self, test_labels=None, extra_tests=None, **kwargs):\n    suite = TestSuite()\n    test_labels = (test_labels or ['.'])\n    extra_tests = (extra_tests or [])\n    discover_kwargs = {\n        \n    }\n    if (self.pattern is not None):\n        discover_kwargs['pattern'] = self.pattern\n    if (self.top_level is not None):\n        discover_kwargs['top_level_dir'] = self.top_level\n    for label in test_labels:\n        kwargs = discover_kwargs.copy()\n        tests = None\n        label_as_path = os.path.abspath(label)\n        if (not os.path.exists(label_as_path)):\n            tests = self.test_loader.loadTestsFromName(label)\n        elif (os.path.isdir(label_as_path) and (not self.top_level)):\n            top_level = label_as_path\n            while True:\n                init_py = os.path.join(top_level, '__init__.py')\n                if os.path.exists(init_py):\n                    try_next = os.path.dirname(top_level)\n                    if (try_next == top_level):\n                        break\n                    top_level = try_next\n                    continue\n                break\n            kwargs['top_level_dir'] = top_level\n        if (not (tests and tests.countTestCases())):\n            tests = self.test_loader.discover(start_dir=label, **kwargs)\n            self.test_loader._top_level_dir = None\n        suite.addTests(tests)\n    for test in extra_tests:\n        suite.addTest(test)\n    return reorder_suite(suite, self.reorder_by)\n", "label": 1}
{"function": "\n\ndef _autodiscover(self):\n    'Discovers modules to register from ``settings.INSTALLED_APPS``.\\n\\n        This makes sure that the appropriate modules get imported to register\\n        themselves with Horizon.\\n        '\n    if (not getattr(self, '_registerable_class', None)):\n        raise ImproperlyConfigured('You must set a \"_registerable_class\" property in order to use autodiscovery.')\n    for mod_name in ('dashboard', 'panel'):\n        for app in settings.INSTALLED_APPS:\n            mod = import_module(app)\n            try:\n                before_import_registry = copy.copy(self._registry)\n                import_module(('%s.%s' % (app, mod_name)))\n            except Exception:\n                self._registry = before_import_registry\n                if module_has_submodule(mod, mod_name):\n                    raise\n", "label": 0}
{"function": "\n\ndef _get_rows(self, options):\n    'Return only those data rows that should be printed, based on slicing and sorting.\\n\\n        Arguments:\\n\\n        options - dictionary of option settings.'\n    if options['oldsortslice']:\n        rows = copy.deepcopy(self._rows[options['start']:options['end']])\n    else:\n        rows = copy.deepcopy(self._rows)\n    if options['sortby']:\n        sortindex = self._field_names.index(options['sortby'])\n        rows = [([row[sortindex]] + row) for row in rows]\n        rows.sort(reverse=options['reversesort'], key=options['sort_key'])\n        rows = [row[1:] for row in rows]\n    if (not options['oldsortslice']):\n        rows = rows[options['start']:options['end']]\n    return rows\n", "label": 0}
{"function": "\n\ndef _fix_slicing_order(self, outer_fields, inner_select, order, inner_table_name):\n    '\\n        Apply any necessary fixes to the outer_fields, inner_select, and order \\n        strings due to slicing.\\n        '\n    if (order is None):\n        meta = self.query.get_meta()\n        column = (meta.pk.db_column or meta.pk.get_attname())\n        order = '{0}.{1} ASC'.format(inner_table_name, self.connection.ops.quote_name(column))\n    else:\n        alias_id = 0\n        new_order = []\n        for x in order.split(','):\n            m = _re_find_order_direction.search(x)\n            if m:\n                direction = m.groups()[0]\n            else:\n                direction = 'ASC'\n            x = _re_find_order_direction.sub('', x)\n            col = x.rsplit('.', 1)[(- 1)]\n            if (x not in inner_select):\n                alias_id += 1\n                col = '{left_sql_quote}{0}___o{1}{right_sql_quote}'.format(col.strip((self.connection.ops.left_sql_quote + self.connection.ops.right_sql_quote)), alias_id, left_sql_quote=self.connection.ops.left_sql_quote, right_sql_quote=self.connection.ops.right_sql_quote)\n                inner_select = '({0}) AS {1}, {2}'.format(x, col, inner_select)\n            new_order.append('{0}.{1} {2}'.format(inner_table_name, col, direction))\n        order = ', '.join(new_order)\n    return (outer_fields, inner_select, order)\n", "label": 0}
{"function": "\n\n@then('the tag expression selects model elements with')\ndef step_given_named_model_elements_with_tags(context):\n    '\\n    .. code-block:: gherkin\\n\\n        Then the tag expression select model elements with:\\n            | tag expression | selected?    |\\n            |  @foo          | S1, S3       |\\n            | -@foo          | S0, S2, S3   |\\n    '\n    assert context.model_elements, 'REQUIRE: context attribute'\n    assert context.table, 'REQUIRE: context.table'\n    context.table.require_columns(['tag expression', 'selected?'])\n    for (row_index, row) in enumerate(context.table.rows):\n        tag_expression_text = row['tag expression']\n        tag_expression = convert_tag_expression(tag_expression_text)\n        expected_selected_names = convert_comma_list(row['selected?'])\n        actual_selected = []\n        for model_element in context.model_elements:\n            if tag_expression.check(model_element.tags):\n                actual_selected.append(model_element.name)\n        assert_that(actual_selected, equal_to(expected_selected_names), ('tag_expression=%s (row=%s)' % (tag_expression_text, row_index)))\n", "label": 0}
{"function": "\n\ndef strategy(self, opponent):\n    '\\n        Check whether the number of cooperations in the first and second halves\\n        of the history are close. The variance of the uniform distribution (1/4)\\n        is a reasonable delta but use something lower for certainty and avoiding\\n        false positives. This approach will also detect a lot of random players.\\n        '\n    n = len(self.history)\n    if ((n >= 8) and opponent.cooperations and opponent.defections):\n        (start1, end1) = (0, (n // 2))\n        (start2, end2) = ((n // 4), ((3 * n) // 4))\n        (start3, end3) = ((n // 2), n)\n        count1 = (opponent.history[start1:end1].count(C) + self.history[start1:end1].count(C))\n        count2 = (opponent.history[start2:end2].count(C) + self.history[start2:end2].count(C))\n        count3 = (opponent.history[start3:end3].count(C) + self.history[start3:end3].count(C))\n        ratio1 = ((0.5 * count1) / (end1 - start1))\n        ratio2 = ((0.5 * count2) / (end2 - start2))\n        ratio3 = ((0.5 * count3) / (end3 - start3))\n        if ((abs((ratio1 - ratio2)) < 0.2) and (abs((ratio1 - ratio3)) < 0.2)):\n            return D\n    return C\n", "label": 0}
{"function": "\n\ndef rob(self, nums):\n    '\\n        Two cases: cannot touch 1st element vs. cannot touch 2nd element.\\n        There are two cases here 1) 1st element is included and last is not included 2) 1st is not included and last is\\n        included.\\n        :type nums: list\\n        :rtype: int\\n        '\n    n = len(nums)\n    if (n < 2):\n        return sum(nums)\n    dp = [0 for _ in xrange(((n - 1) + 2))]\n    for i in xrange(2, (n + 1)):\n        dp[i] = max(dp[(i - 1)], (dp[(i - 2)] + nums[(i - 2)]))\n    ret = dp[(- 1)]\n    dp = [0 for _ in xrange(((n - 1) + 2))]\n    for i in xrange(2, (n + 1)):\n        dp[i] = max(dp[(i - 1)], (dp[(i - 2)] + nums[(i - 1)]))\n    ret = max(ret, dp[(- 1)])\n    return ret\n", "label": 0}
{"function": "\n\ndef get_module(os_mapping, dirpath):\n    mapping_config = ConfigParser.RawConfigParser()\n    mapping_config.readfp(StringIO.StringIO(os_mapping))\n    opts = config.controller\n    dist = opts['distribution_name']\n    ver = opts['distribution_version']\n    combinations = ((dist, ver), (dist, 'default'), ('default', 'default'))\n    mod_name = None\n    for comb in combinations:\n        mod_name = get_module_name(mapping_config, comb[0], comb[1])\n        if mod_name:\n            break\n    if ((mod_name != 'default') and (mod_name is not None)):\n        (fp, path, desc) = imp.find_module(mod_name, [dirpath])\n        return imp.load_module(mod_name, fp, path, desc)\n    elif (mod_name == 'default'):\n        return None\n", "label": 0}
{"function": "\n\ndef get_table_list(self, cursor):\n    'Returns a list of table names in the current database and schema.'\n    cursor.execute((\"\\n            SELECT c.relname, c.relkind\\n            FROM pg_catalog.pg_class c\\n            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\\n            WHERE c.relkind IN ('r', 'v', '')\\n                AND n.nspname = '%s'\\n                AND pg_catalog.pg_table_is_visible(c.oid)\" % self.connection.schema_name))\n    if (django.VERSION >= (1, 8, 0)):\n        return [TableInfo(row[0], {\n            'r': 't',\n            'v': 'v',\n        }.get(row[1])) for row in cursor.fetchall() if (row[0] not in self.ignored_tables)]\n    else:\n        return [row[0] for row in cursor.fetchall() if (row[0] not in self.ignored_tables)]\n", "label": 0}
{"function": "\n\ndef test_process_commit():\n    (m, ctl, config) = init()\n    config['numprocesses'] = 0\n    m.load(config, start=False)\n    cmd = TestCommand('commit', ['dummy'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    state = m._get_locked_state('dummy')\n    assert (len(state.running) == 0)\n    assert (state.numprocesses == 0)\n    assert (len(state.running_out) == 1)\n    assert (m.pids() == [1])\n    m.stop()\n    m.run()\n    assert (cmd.result['pid'] == 1)\n", "label": 0}
{"function": "\n\ndef gpx_fields_to_xml(instance, tag, version, custom_attributes=None):\n    fields = instance.gpx_10_fields\n    if (version == '1.1'):\n        fields = instance.gpx_11_fields\n    tag_open = bool(tag)\n    body = ''\n    if tag:\n        body = ('\\n<' + tag)\n        if custom_attributes:\n            for (key, value) in custom_attributes.items():\n                body += (' %s=\"%s\"' % (key, mod_utils.make_str(value)))\n    for gpx_field in fields:\n        if isinstance(gpx_field, str):\n            if tag_open:\n                body += '>'\n                tag_open = False\n            if (gpx_field[0] == '/'):\n                body += ('<%s>' % gpx_field)\n            else:\n                body += ('\\n<%s' % gpx_field)\n                tag_open = True\n        else:\n            value = getattr(instance, gpx_field.name)\n            if gpx_field.attribute:\n                body += (' ' + gpx_field.to_xml(value, version))\n            elif value:\n                if tag_open:\n                    body += '>'\n                    tag_open = False\n                xml_value = gpx_field.to_xml(value, version)\n                if xml_value:\n                    body += xml_value\n    if tag:\n        if tag_open:\n            body += '>'\n        body += (('</' + tag) + '>')\n    return body\n", "label": 1}
{"function": "\n\ndef load_directives(self, inherits):\n    \"there has got to be a better way\\n           plugin pattern away!!!\\n           \\n           #1 this could be a comprehension but it'd be hard\\n              to read.\\n           #2 maybe we should put a dummy sys.modules in place\\n              while doing this to limit junk in the name space.\\n        \"\n    m = self.__class__.__module__\n    exclude = ['__init__.py']\n    relpath = sys.modules[m].__file__\n    reldir = os.path.split(relpath)[0]\n    fulldir = os.path.realpath(reldir)\n    pyfiles = []\n    for f in os.listdir(fulldir):\n        ff = os.path.join(fulldir, f)\n        if os.path.isfile(ff):\n            pyfiles.append(f)\n    pyfiles = [f for f in pyfiles if f.endswith('.py')]\n    pyfiles = [f for f in pyfiles if (f not in exclude)]\n    pyfiles = [f[:(- 3)] for f in pyfiles]\n    candidates = []\n    for py in pyfiles:\n        m = __import__(py, globals(), locals(), 'romeo.directives')\n        for i in dir(m):\n            attr = getattr(m, i)\n            if (not (type(attr) == types.TypeType)):\n                continue\n            matches = [cls for cls in inherits if issubclass(attr, cls) if (attr.__name__ != cls.__name__)]\n            if (len(matches) == len(inherits)):\n                candidates.append(attr)\n    for c in candidates:\n        cinst = c(c.name, self.root, *c.init_args(self), **c.init_kwargs(self))\n        self.directives.append(cinst)\n", "label": 1}
{"function": "\n\ndef bind_params(self, I, E, O, alpha):\n    assert (I.dtype == E.dtype)\n    if (not self.initialized):\n        self.initialized = True\n        self.autotune(I, E, O)\n    if ((O.dtype.type is not np.float32) or self.determ_size):\n        updat_temp = self.lib.scratch_buffer(self.output_size)\n        image_temp = self.lib.scratch_buffer_offset(self.image_size)\n        delta_temp = self.lib.scratch_buffer_offset(self.delta_size)\n        self.convert_args = [updat_temp, 'f4', O, self.determ_shape]\n    else:\n        updat_temp = O.gpudata\n        image_temp = self.lib.scratch_buffer(self.image_size)\n        delta_temp = self.lib.scratch_buffer_offset(self.delta_size)\n        self.convert_args = False\n    self.image_args[2:5] = (self.lib.stream, image_temp, I.gpudata)\n    self.delta_args[2:5] = (self.lib.stream, delta_temp, E.gpudata)\n    if self.zero:\n        self.zero_args = [updat_temp, 0, O.size, self.lib.stream]\n    self.kernel[3:8] = (self.lib.stream, updat_temp, image_temp, delta_temp, alpha)\n", "label": 0}
{"function": "\n\ndef filter(self, iterable, prereleases=None):\n    if (prereleases is None):\n        prereleases = self.prereleases\n    if self._specs:\n        for spec in self._specs:\n            iterable = spec.filter(iterable, prereleases=bool(prereleases))\n        return iterable\n    else:\n        filtered = []\n        found_prereleases = []\n        for item in iterable:\n            if (not isinstance(item, (LegacyVersion, Version))):\n                parsed_version = parse(item)\n            else:\n                parsed_version = item\n            if isinstance(parsed_version, LegacyVersion):\n                continue\n            if (parsed_version.is_prerelease and (not prereleases)):\n                if (not filtered):\n                    found_prereleases.append(item)\n            else:\n                filtered.append(item)\n        if ((not filtered) and found_prereleases and (prereleases is None)):\n            return found_prereleases\n        return filtered\n", "label": 1}
{"function": "\n\ndef read_events(self, timeout=None):\n    timeout_ms = 2147483647\n    if (timeout is not None):\n        timeout_ms = int((timeout * 1000))\n        if ((timeout_ms < 0) or (timeout_ms >= 2147483647)):\n            raise ValueError('Timeout value out of range')\n    try:\n        events = []\n        (rc, num, key, _) = win32file.GetQueuedCompletionStatus(self.__cphandle, timeout_ms)\n        if (rc == 0):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled and (not watch._removed)):\n                    events.extend(process_events(watch, num))\n        elif (rc == 5):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled):\n                    close_watch(watch)\n                    del self.__key_to_watch[key]\n                    events.append(FSEvent(watch, FSEvent.DeleteSelf))\n        return events\n    except pywintypes.error as e:\n        raise FSMonitorWindowsError(*e.args)\n", "label": 1}
{"function": "\n\ndef shell_exec_monitor(self, Command, PluginInfo):\n    CommandInfo = self.StartCommand(Command, Command)\n    (Target, CanRun) = self.CanRunCommand(CommandInfo)\n    if (not CanRun):\n        Message = ('The command was already run for target: ' + str(Target))\n        return Message\n    logging.info('')\n    logging.info('Executing :\\n\\n%s\\n\\n', Command)\n    logging.info('')\n    logging.info(('------> Execution Start Date/Time: ' + self.timer.get_start_date_time_as_str('Command')))\n    logging.info('')\n    Output = ''\n    Cancelled = False\n    try:\n        proc = self.create_subprocess(Command)\n        while True:\n            line = proc.stdout.readline()\n            if (not line):\n                break\n            logging.warn(line.strip())\n            Output += line\n    except KeyboardInterrupt:\n        os.killpg(proc.pid, signal.SIGINT)\n        (outdata, errdata) = proc.communicate()\n        logging.warn(outdata)\n        Output += outdata\n        try:\n            os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n        except OSError:\n            pass\n        Cancelled = True\n        Output += self.error_handler.UserAbort('Command', Output)\n    finally:\n        self.FinishCommand(CommandInfo, Cancelled, PluginInfo)\n    return scrub_output(Output)\n", "label": 0}
{"function": "\n\ndef information(self, b, ties='breslow'):\n    info = 0\n    score = 0\n    for t in iterkeys(self.failures):\n        fail = self.failures[t]\n        d = len(fail)\n        risk = self.risk[t]\n        Z = self.design[t]\n        if (ties == 'breslow'):\n            w = np.exp(np.dot(Z, b))\n            rv = Discrete(Z[risk], w=w[risk])\n            info += rv.cov()\n        elif (ties == 'efron'):\n            w = np.exp(np.dot(Z, b))\n            score += Z[fail].sum()\n            for j in range(d):\n                efron_w = w\n                efron_w[fail] -= ((i * w[fail]) / d)\n                rv = Discrete(Z[risk], w=efron_w[risk])\n                info += rv.cov()\n        elif (ties == 'cox'):\n            raise NotImplementedError('Cox tie breaking method not implemented')\n        else:\n            raise NotImplementedError('tie breaking method not recognized')\n    return score\n", "label": 0}
{"function": "\n\ndef reset(self, **traits):\n    'Creates the dataset afresh or resets existing data source.'\n    self.set(trait_change_notify=False, **traits)\n    points = self.points\n    scalars = self.scalars\n    (x, y, z) = (self.x, self.y, self.z)\n    points = np.c_[(x.ravel(), y.ravel(), z.ravel())].ravel()\n    points.shape = ((- 1), 3)\n    self.set(points=points, trait_change_notify=False)\n    triangles = self.triangles\n    assert (triangles.shape[1] == 3), 'The shape of the triangles array must be (X, 3)'\n    assert (triangles.max() < len(points)), 'The triangles indices must be smaller that the number of points'\n    assert (triangles.min() >= 0), 'The triangles indices must be positive or null'\n    if (self.dataset is None):\n        pd = tvtk.PolyData()\n    else:\n        pd = self.dataset\n    pd.set(points=points)\n    pd.set(polys=triangles)\n    if ((not ('scalars' in traits)) and (scalars is not None) and (scalars.shape != x.shape)):\n        scalars = z\n    if ((scalars is not None) and (len(scalars) > 0)):\n        if (not scalars.flags.contiguous):\n            scalars = scalars.copy()\n            self.set(scalars=scalars, trait_change_notify=False)\n        assert (x.shape == scalars.shape)\n        pd.point_data.scalars = scalars.ravel()\n        pd.point_data.scalars.name = 'scalars'\n    self.dataset = pd\n", "label": 1}
{"function": "\n\n@login_required\ndef editor(request, id):\n    '\\n    Display the article editor.\\n    '\n    article = (get_object_or_404(Article, pk=id) if id else None)\n    if (article and (not article.can_edit(request))):\n        raise Http404\n    if (request.method == 'POST'):\n        form = EditorForm(instance=article, data=request.POST)\n        if form.is_valid():\n            creating = (article is None)\n            article = form.save(commit=False)\n            if creating:\n                article.author = request.user\n            article.save()\n            messages.info(request, ('The article has been saved.' if creating else 'Your changes to the article have been saved.'))\n            if (('action' in request.POST) and (request.POST['action'] == 'continue')):\n                return redirect('articles:editor', article.id)\n            else:\n                return redirect(article)\n    else:\n        form = EditorForm(instance=article)\n    return render(request, 'articles/editor.html', {\n        'title': (('Edit \"%s\"' % article) if article else 'New Article'),\n        'form': form,\n        'description': ('Use the form below to %s.' % ('edit the article' if article else 'create an article')),\n    })\n", "label": 1}
{"function": "\n\ndef find_referenced_templates(ast):\n    'Finds all the referenced templates from the AST.  This will return an\\n    iterator over all the hardcoded template extensions, inclusions and\\n    imports.  If dynamic inheritance or inclusion is used, `None` will be\\n    yielded.\\n\\n    >>> from jinja2 import Environment, meta\\n    >>> env = Environment()\\n    >>> ast = env.parse(\\'{% extends \"layout.html\" %}{% include helper %}\\')\\n    >>> list(meta.find_referenced_templates(ast))\\n    [\\'layout.html\\', None]\\n\\n    This function is useful for dependency tracking.  For example if you want\\n    to rebuild parts of the website after a layout template has changed.\\n    '\n    for node in ast.find_all((nodes.Extends, nodes.FromImport, nodes.Import, nodes.Include)):\n        if (not isinstance(node.template, nodes.Const)):\n            if isinstance(node.template, (nodes.Tuple, nodes.List)):\n                for template_name in node.template.items:\n                    if isinstance(template_name, nodes.Const):\n                        if isinstance(template_name.value, string_types):\n                            (yield template_name.value)\n                    else:\n                        (yield None)\n            else:\n                (yield None)\n            continue\n        if isinstance(node.template.value, string_types):\n            (yield node.template.value)\n        elif (isinstance(node, nodes.Include) and isinstance(node.template.value, (tuple, list))):\n            for template_name in node.template.value:\n                if isinstance(template_name, string_types):\n                    (yield template_name)\n        else:\n            (yield None)\n", "label": 1}
{"function": "\n\ndef test_04_ore_init_with_statement(self):\n    s = Ore_Sword_Statement(ORE_TEST_STATEMENT)\n    assert (len(s.states) == 1)\n    assert (len(s.original_deposits) == 1)\n    assert (len(s.resources) == 1)\n    assert (s.xml_document != None)\n    assert (s.dom != None)\n    assert s.parsed\n    assert s.valid\n    (uri, description) = s.states[0]\n    assert (uri == 'http://purl.org/net/sword/terms/state/Testing')\n    assert (description == 'The work has passed through review and is now in the archive')\n    t = datetime.strptime('2011-03-02T20:50:06Z', '%Y-%m-%dT%H:%M:%SZ')\n    entry = s.resources[0]\n    assert (len(entry.packaging) == 1)\n    assert (entry.deposited_by == 'sword')\n    assert (entry.deposited_on_behalf_of == 'jbloggs')\n    assert (entry.deposited_on == t)\n    assert (entry.uri == 'http://localhost:8080/part-IRI/43/my_deposit/example.zip')\n    assert (entry.packaging[0] == 'http://purl.org/net/sword/package/SimpleZip')\n", "label": 1}
{"function": "\n\ndef test_posts_atom(client, silly_posts):\n    rv = client.get('/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('Probably a &lt;i&gt;dumb&lt;/i&gt; joke' in content)\n    assert ('First interesting article' in content)\n    rv = client.get('/everything/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('mal.colm/reynolds' in content)\n    assert ('First interesting article' in content)\n    rv = client.get('/notes/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('Probably a &lt;i&gt;dumb&lt;/i&gt; joke' in content)\n    assert ('First interesting article' not in content)\n", "label": 1}
{"function": "\n\n@utils.synchronized('dhcp-agent')\ndef sync_state(self, networks=None):\n    \"Sync the local DHCP state with Neutron. If no networks are passed,\\n        or 'None' is one of the networks, sync all of the networks.\\n        \"\n    only_nets = set(([] if ((not networks) or (None in networks)) else networks))\n    LOG.info(_LI('Synchronizing state'))\n    pool = eventlet.GreenPool(self.conf.num_sync_threads)\n    known_network_ids = set(self.cache.get_network_ids())\n    try:\n        active_networks = self.plugin_rpc.get_active_networks_info()\n        LOG.info(_LI('All active networks have been fetched through RPC.'))\n        active_network_ids = set((network.id for network in active_networks))\n        for deleted_id in (known_network_ids - active_network_ids):\n            try:\n                self.disable_dhcp_helper(deleted_id)\n            except Exception as e:\n                self.schedule_resync(e, deleted_id)\n                LOG.exception(_LE('Unable to sync network state on deleted network %s'), deleted_id)\n        for network in active_networks:\n            if ((not only_nets) or (network.id not in known_network_ids) or (network.id in only_nets)):\n                pool.spawn(self.safe_configure_dhcp_for_network, network)\n        pool.waitall()\n        LOG.info(_LI('Synchronizing state complete'))\n    except Exception as e:\n        if only_nets:\n            for network_id in only_nets:\n                self.schedule_resync(e, network_id)\n        else:\n            self.schedule_resync(e)\n        LOG.exception(_LE('Unable to sync network state.'))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _tensAdd_flatten(args):\n    if (not all((isinstance(x, TensExpr) for x in args))):\n        args1 = []\n        for x in args:\n            if isinstance(x, TensExpr):\n                if isinstance(x, TensAdd):\n                    args1.extend(list(x.args))\n                else:\n                    args1.append(x)\n        args1 = [x for x in args1 if (isinstance(x, TensExpr) and x.coeff)]\n        args2 = [x for x in args if (not isinstance(x, TensExpr))]\n        t1 = TensMul.from_data(Add(*args2), [], [], [])\n        args = ([t1] + args1)\n    a = []\n    for x in args:\n        if isinstance(x, TensAdd):\n            a.extend(list(x.args))\n        else:\n            a.append(x)\n    args = [x for x in a if x.coeff]\n    return args\n", "label": 1}
{"function": "\n\ndef the_local_prediction_is(step, prediction):\n    if (isinstance(world.local_prediction, list) or isinstance(world.local_prediction, tuple)):\n        local_prediction = world.local_prediction[0]\n    elif isinstance(world.local_prediction, dict):\n        local_prediction = world.local_prediction['prediction']\n    else:\n        local_prediction = world.local_prediction\n    try:\n        local_model = world.local_model\n        if (not isinstance(world.local_model, LogisticRegression)):\n            if isinstance(local_model, MultiModel):\n                local_model = local_model.models[0]\n            if local_model.tree.regression:\n                local_prediction = round(float(local_prediction), 4)\n                prediction = round(float(prediction), 4)\n    except AttributeError:\n        local_model = world.local_ensemble.multi_model.models[0]\n        if local_model.tree.regression:\n            local_prediction = round(float(local_prediction), 4)\n            prediction = round(float(prediction), 4)\n    if (local_prediction == prediction):\n        assert True\n    else:\n        assert False, ('found: %s, expected %s' % (local_prediction, prediction))\n", "label": 1}
{"function": "\n\ndef test_updating_state(db, tmpdir):\n    if (not db.startswith('mysql')):\n        db = os.path.join(str(tmpdir), db)\n    jip.db.init(db)\n    j = jip.db.Job()\n    jip.db.save(j)\n    j = jip.db.get(j.id)\n    assert (j is not None)\n    assert (j.create_date is not None)\n    assert (j.start_date is None)\n    assert (j.finish_date is None)\n    assert (j.job_id is None)\n    assert (j.state == jip.db.STATE_HOLD)\n    assert (len(j.pipe_to) == 0)\n    assert (len(j.pipe_from) == 0)\n    date = datetime.datetime.now()\n    j.job_id = 10\n    j.start_date = date\n    j.finish_date = date\n    j.state = jip.db.STATE_DONE\n    jip.db.update_job_states(j)\n    fresh = jip.db.get(j.id)\n    assert (fresh is not None)\n    assert (fresh.job_id == '10')\n    assert (fresh.state == jip.db.STATE_DONE)\n    assert (len(jip.db.get_all()) == 1)\n", "label": 1}
{"function": "\n\ndef test_tweet_ordering():\n    now = datetime.now(timezone.utc)\n    tweet_1 = Tweet('A', now)\n    tweet_2 = Tweet('B', (now + timedelta(hours=1)))\n    tweet_3 = Tweet('C', (now + timedelta(hours=2)))\n    tweet_4 = Tweet('D', (now + timedelta(hours=2)))\n    tweet_5 = Tweet('D', (now + timedelta(hours=2)))\n    source = Source('foo', 'bar')\n    with pytest.raises(TypeError):\n        (tweet_1 < source)\n    with pytest.raises(TypeError):\n        (tweet_1 <= source)\n    with pytest.raises(TypeError):\n        (tweet_1 > source)\n    with pytest.raises(TypeError):\n        (tweet_1 >= source)\n    assert (tweet_1 != source)\n    assert (tweet_1 < tweet_2)\n    assert (tweet_1 <= tweet_2)\n    assert (tweet_2 > tweet_1)\n    assert (tweet_2 >= tweet_1)\n    assert (tweet_3 != tweet_4)\n    assert (tweet_5 == tweet_4)\n    assert (tweet_5 >= tweet_4)\n    assert (tweet_5 <= tweet_4)\n    assert (not (tweet_3 <= tweet_4))\n    assert (not (tweet_3 >= tweet_4))\n", "label": 1}
{"function": "\n\ndef io_connection_pattern(inputs, outputs):\n    '\\n    Returns the connection pattern of a subgraph defined by given\\n    inputs and outputs.\\n\\n    '\n    inner_nodes = io_toposort(inputs, outputs)\n    connect_pattern_by_var = {\n        \n    }\n    nb_inputs = len(inputs)\n    for i in range(nb_inputs):\n        input = inputs[i]\n        inp_connection_pattern = [(i == j) for j in range(nb_inputs)]\n        connect_pattern_by_var[input] = inp_connection_pattern\n    for n in inner_nodes:\n        try:\n            op_connection_pattern = n.op.connection_pattern(n)\n        except AttributeError:\n            op_connection_pattern = ([([True] * len(n.outputs))] * len(n.inputs))\n        for out_idx in range(len(n.outputs)):\n            out = n.outputs[out_idx]\n            out_connection_pattern = ([False] * nb_inputs)\n            for inp_idx in range(len(n.inputs)):\n                inp = n.inputs[inp_idx]\n                if (inp in connect_pattern_by_var):\n                    inp_connection_pattern = connect_pattern_by_var[inp]\n                    if op_connection_pattern[inp_idx][out_idx]:\n                        out_connection_pattern = [(out_connection_pattern[i] or inp_connection_pattern[i]) for i in range(nb_inputs)]\n            connect_pattern_by_var[out] = out_connection_pattern\n    global_connection_pattern = [[] for o in range(len(inputs))]\n    for out in outputs:\n        out_connection_pattern = connect_pattern_by_var[out]\n        for i in range(len(inputs)):\n            global_connection_pattern[i].append(out_connection_pattern[i])\n    return global_connection_pattern\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    assert name.endswith('PropMap'), 'Please use convention: ___PropMap, e.g. ElectromagneticPropMap'\n    _properties = {\n        \n    }\n    for base in bases:\n        for baseProp in getattr(base, '_properties', {\n            \n        }):\n            _properties[baseProp] = base._properties[baseProp]\n    keys = [key for key in attrs]\n    for attr in keys:\n        if isinstance(attrs[attr], Property):\n            attrs[attr].name = attr\n            attrs[(attr + 'Map')] = attrs[attr]._getMapProperty()\n            attrs[(attr + 'Index')] = attrs[attr]._getIndexProperty()\n            _properties[attr] = attrs[attr]\n            attrs.pop(attr)\n    attrs['_properties'] = _properties\n    defaultInvProps = []\n    for p in _properties:\n        prop = _properties[p]\n        if prop.defaultInvProp:\n            defaultInvProps += [p]\n        if (prop.propertyLink is not None):\n            assert (prop.propertyLink[0] in _properties), (\"You can only link to things that exist: '%s' is trying to link to '%s'\" % (prop.name, prop.propertyLink[0]))\n    if (len(defaultInvProps) > 1):\n        raise Exception(('You have more than one default inversion property: %s' % defaultInvProps))\n    newClass = super(_PropMapMetaClass, cls).__new__(cls, name, bases, attrs)\n    newClass.PropModel = cls.createPropModelClass(newClass, name, _properties)\n    _PROPMAPCLASSREGISTRY[name] = newClass\n    return newClass\n", "label": 1}
{"function": "\n\n@classmethod\ndef delete(cls, repo, path):\n    'Delete the reference at the given path\\n\\n        :param repo:\\n            Repository to delete the reference from\\n\\n        :param path:\\n            Short or full path pointing to the reference, i.e. refs/myreference\\n            or just \"myreference\", hence \\'refs/\\' is implied.\\n            Alternatively the symbolic reference to be deleted'\n    full_ref_path = cls.to_full_path(path)\n    abs_path = join(repo.git_dir, full_ref_path)\n    if exists(abs_path):\n        os.remove(abs_path)\n    else:\n        pack_file_path = cls._get_packed_refs_path(repo)\n        try:\n            reader = open(pack_file_path, 'rb')\n        except (OSError, IOError):\n            pass\n        else:\n            new_lines = list()\n            made_change = False\n            dropped_last_line = False\n            for line in reader:\n                line = line.decode(defenc)\n                if ((line.startswith('#') or (full_ref_path not in line)) and ((not dropped_last_line) or (dropped_last_line and (not line.startswith('^'))))):\n                    new_lines.append(line)\n                    dropped_last_line = False\n                    continue\n                made_change = True\n                dropped_last_line = True\n            reader.close()\n            if made_change:\n                open(pack_file_path, 'wb').writelines((l.encode(defenc) for l in new_lines))\n    reflog_path = RefLog.path(cls(repo, full_ref_path))\n    if os.path.isfile(reflog_path):\n        os.remove(reflog_path)\n", "label": 1}
{"function": "\n\ndef test_remove_role():\n    db = SQLAlchemy('sqlite:///:memory:')\n    auth = authcode.Auth(SECRET_KEY, db=db, roles=True)\n    User = auth.User\n    Role = auth.Role\n    db.create_all()\n    user = User(login='meh', password='foobar')\n    db.session.add(user)\n    db.session.commit()\n    assert hasattr(auth, 'Role')\n    assert hasattr(User, 'roles')\n    user.add_role('admin')\n    db.session.commit()\n    assert user.has_role('admin')\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('foobar')\n    db.session.commit()\n    assert (db.query(Role).count() == 1)\n", "label": 1}
{"function": "\n\ndef configure_dhcp_for_network(self, network):\n    if (not network.admin_state_up):\n        return\n    enable_metadata = self.dhcp_driver_cls.should_enable_metadata(self.conf, network)\n    dhcp_network_enabled = False\n    for subnet in network.subnets:\n        if subnet.enable_dhcp:\n            if self.call_driver('enable', network):\n                dhcp_network_enabled = True\n                self.cache.put(network)\n            break\n    if (enable_metadata and dhcp_network_enabled):\n        for subnet in network.subnets:\n            if ((subnet.ip_version == 4) and subnet.enable_dhcp):\n                self.enable_isolated_metadata_proxy(network)\n                break\n    elif ((not self.conf.force_metadata) and (not self.conf.enable_isolated_metadata)):\n        self.disable_isolated_metadata_proxy(network)\n", "label": 1}
{"function": "\n\ndef lex(s, name=None, trim_whitespace=True, line_offset=0, delimeters=None):\n    if (delimeters is None):\n        delimeters = (Template.default_namespace['start_braces'], Template.default_namespace['end_braces'])\n    in_expr = False\n    chunks = []\n    last = 0\n    last_pos = ((line_offset + 1), 1)\n    token_re = re.compile(('%s|%s' % (re.escape(delimeters[0]), re.escape(delimeters[1]))))\n    for match in token_re.finditer(s):\n        expr = match.group(0)\n        pos = find_position(s, match.end(), last, last_pos)\n        if ((expr == delimeters[0]) and in_expr):\n            raise TemplateError(('%s inside expression' % delimeters[0]), position=pos, name=name)\n        elif ((expr == delimeters[1]) and (not in_expr)):\n            raise TemplateError(('%s outside expression' % delimeters[1]), position=pos, name=name)\n        if (expr == delimeters[0]):\n            part = s[last:match.start()]\n            if part:\n                chunks.append(part)\n            in_expr = True\n        else:\n            chunks.append((s[last:match.start()], last_pos))\n            in_expr = False\n        last = match.end()\n        last_pos = pos\n    if in_expr:\n        raise TemplateError(('No %s to finish last expression' % delimeters[1]), name=name, position=last_pos)\n    part = s[last:]\n    if part:\n        chunks.append(part)\n    if trim_whitespace:\n        chunks = trim_lex(chunks)\n    return chunks\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('grid_type', ALL_GRID_TYPES)\ndef test_append_two_quantities_invalid(self, grid_type):\n    g = self.grid[grid_type]\n    g['density'] = []\n    g['energy'] = []\n    g['density'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    assert g['density'][0]\n    assert g['density'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['density'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert g['energy'][0]\n    assert g['energy'][1]\n    assert g['energy'][(- 1)]\n    assert g['energy'][(- 2)]\n    with pytest.raises(IndexError) as exc:\n        assert g['energy'][2]\n    assert (exc_msg(exc) == 'list index out of range')\n    with pytest.raises(ValueError) as exc:\n        g.n_dust\n    assert (exc_msg(exc) == 'Not all dust lists in the grid have the same size')\n", "label": 1}
{"function": "\n\ndef commit_dirtiness_flags(self):\n    '\\n        Updates any dirtiness flags in the database.\\n        '\n    if self.domain:\n        flags_to_save = self.get_flags_to_save()\n        if should_create_flags_on_submission(self.domain):\n            assert settings.UNIT_TESTING\n            all_touched_ids = (set(flags_to_save.keys()) | self.get_clean_owner_ids())\n            to_update = {f.owner_id: f for f in OwnershipCleanlinessFlag.objects.filter(domain=self.domain, owner_id__in=list(all_touched_ids))}\n            for owner_id in all_touched_ids:\n                if (owner_id not in to_update):\n                    flag = OwnershipCleanlinessFlag(domain=self.domain, owner_id=owner_id, is_clean=True)\n                    if (owner_id in flags_to_save):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                    flag.save()\n                else:\n                    flag = to_update[owner_id]\n                    if ((owner_id in flags_to_save) and (flag.is_clean or (not flag.hint))):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                        flag.save()\n        else:\n            flags_to_update = OwnershipCleanlinessFlag.objects.filter(Q(domain=self.domain), Q(owner_id__in=flags_to_save.keys()), (Q(is_clean=True) | Q(hint__isnull=True)))\n            for flag in flags_to_update:\n                flag.is_clean = False\n                flag.hint = flags_to_save[flag.owner_id]\n                flag.save()\n", "label": 1}
{"function": "\n\ndef _compute_generator_info(self):\n    \"\\n        Compute the generator's state variables as the union of live variables\\n        at all yield points.\\n        \"\n    gi = self.generator_info\n    for yp in gi.get_yield_points():\n        live_vars = set(self.block_entry_vars[yp.block])\n        weak_live_vars = set()\n        stmts = iter(yp.block.body)\n        for stmt in stmts:\n            if isinstance(stmt, ir.Assign):\n                if (stmt.value is yp.inst):\n                    break\n                live_vars.add(stmt.target.name)\n            elif isinstance(stmt, ir.Del):\n                live_vars.remove(stmt.value)\n        else:\n            assert 0, \"couldn't find yield point\"\n        for stmt in stmts:\n            if isinstance(stmt, ir.Del):\n                name = stmt.value\n                if (name in live_vars):\n                    live_vars.remove(name)\n                    weak_live_vars.add(name)\n            else:\n                break\n        yp.live_vars = live_vars\n        yp.weak_live_vars = weak_live_vars\n    st = set()\n    for yp in gi.get_yield_points():\n        st |= yp.live_vars\n        st |= yp.weak_live_vars\n    gi.state_vars = sorted(st)\n", "label": 1}
{"function": "\n\ndef get_dot_completions(view, prefix, position, info):\n    if ((not get_setting(view, 'fw1_enabled')) or (len(info['dot_context']) == 0)):\n        return None\n    if extends_fw1(view):\n        if (info['dot_context'][(- 1)].name == 'variables'):\n            key = '.'.join([symbol.name for symbol in reversed(info['dot_context'])])\n            if (key in fw1['settings']):\n                return CompletionList(fw1['settings'][key], 1, False)\n        if (info['dot_context'][(- 1)].name in ['renderdata', 'renderer']):\n            return CompletionList(fw1['methods']['renderdata'], 1, False)\n    if (get_file_type(view) == 'controller'):\n        if ((len(info['dot_context']) > 1) and (info['dot_context'][(- 2)].name in ['renderdata', 'renderer'])):\n            return CompletionList(fw1['methods']['renderdata'], 1, False)\n        if (info['dot_context'][(- 1)].name in ['fw', 'framework']):\n            return CompletionList(fw1['methods']['calls'], 1, False)\n    return None\n", "label": 1}
{"function": "\n\n@register.simple_tag\ndef aggregate_section_totals(section_name, results_arr, daily):\n    'Hackish function to do a summation of a section in the org_report'\n    startindex = (- 1)\n    endindex = (- 1)\n    for itemarr in results_arr:\n        if (itemarr[1] == section_name):\n            startindex = results_arr.index(itemarr)\n            continue\n        if (startindex >= 0):\n            if (itemarr[1] != None):\n                endindex = results_arr.index(itemarr)\n                break\n    summation = []\n    section_arr = []\n    if (endindex == (- 1)):\n        section_arr = results_arr[startindex:]\n    else:\n        section_arr = results_arr[startindex:(endindex + 1)]\n    for itemarr in section_arr:\n        if (summation == []):\n            summation = (summation + itemarr[(- 1)])\n        else:\n            for i in range(0, len(itemarr[(- 1)])):\n                summation[i] += itemarr[(- 1)][i]\n    ret = ''\n    if daily:\n        for item in summation:\n            ret += ('<td style=\"background:#99FFFF\"><strong>%d</strong></td>' % item)\n    else:\n        sum = 0\n        for item in summation:\n            sum += item\n        ret = ('<td>%d</td>' % sum)\n    return ret\n", "label": 1}
{"function": "\n\ndef _construct_keymaps(self, config):\n    '\\n        Construct keymaps for handling input\\n        '\n    keymap = {\n        \n    }\n    move_keymap = {\n        \n    }\n    for key in config.move_left:\n        keymap[key] = self._move\n        move_keymap[key] = 7\n    for key in config.move_up:\n        keymap[key] = self._move\n        move_keymap[key] = 1\n    for key in config.move_right:\n        keymap[key] = self._move\n        move_keymap[key] = 3\n    for key in config.move_down:\n        keymap[key] = self._move\n        move_keymap[key] = 5\n    for key in config.start:\n        keymap[key] = self._menu\n    for key in config.action_a:\n        keymap[key] = self._action_a\n    for key in config.back:\n        keymap[key] = self._back\n    for key in config.left_shoulder:\n        keymap[key] = self._shoulder_left\n    for key in config.right_shoulder:\n        keymap[key] = self._shoulder_right\n    for key in config.mode_1:\n        keymap[key] = self._zoom_out\n    for key in config.mode_2:\n        keymap[key] = self._zoom_in\n    return (keymap, move_keymap)\n", "label": 1}
{"function": "\n\ndef load(self, filename):\n    if (not filename):\n        import traceback\n        traceback.print_stack()\n        return\n    try:\n        im = None\n        if self._inline:\n            im = pygame.image.load(filename, 'x.{}'.format(self._ext))\n        elif isfile(filename):\n            with open(filename, 'rb') as fd:\n                im = pygame.image.load(fd)\n        elif isinstance(filename, bytes):\n            try:\n                fname = filename.decode()\n                if isfile(fname):\n                    with open(fname, 'rb') as fd:\n                        im = pygame.image.load(fd)\n            except UnicodeDecodeError:\n                pass\n        if (im is None):\n            im = pygame.image.load(filename)\n    except:\n        raise\n    fmt = ''\n    if (im.get_bytesize() == 3):\n        fmt = 'rgb'\n    elif (im.get_bytesize() == 4):\n        fmt = 'rgba'\n    if (fmt not in ('rgb', 'rgba')):\n        try:\n            imc = im.convert(32)\n            fmt = 'rgba'\n        except:\n            try:\n                imc = im.convert_alpha()\n                fmt = 'rgba'\n            except:\n                Logger.warning(('Image: Unable to convert image %r to rgba (was %r)' % (filename, im.fmt)))\n                raise\n        im = imc\n    if (not self._inline):\n        self.filename = filename\n    data = pygame.image.tostring(im, fmt.upper())\n    return [ImageData(im.get_width(), im.get_height(), fmt, data, source=filename)]\n", "label": 1}
{"function": "\n\ndef test_file_metadata_drive(basepath):\n    item = fixtures.list_file['items'][0]\n    path = basepath.child(item['title'])\n    parsed = GoogleDriveFileMetadata(item, path)\n    assert (parsed.provider == 'googledrive')\n    assert (parsed.id == item['id'])\n    assert (path.name == item['title'])\n    assert (parsed.name == item['title'])\n    assert (parsed.size == item['fileSize'])\n    assert (parsed.modified == item['modifiedDate'])\n    assert (parsed.content_type == item['mimeType'])\n    assert (parsed.extra == {\n        'revisionId': item['version'],\n        'webView': item['alternateLink'],\n    })\n    assert (parsed.path == ('/' + os.path.join(*[x.raw for x in path.parts])))\n    assert (parsed.materialized_path == str(path))\n    assert (parsed.is_google_doc == False)\n    assert (parsed.export_name == item['title'])\n", "label": 1}
{"function": "\n\ndef extend_list(self, data, parsed_args):\n    'Add subnet information to a network list.'\n    neutron_client = self.get_client()\n    search_opts = {\n        'fields': ['id', 'cidr'],\n    }\n    if self.pagination_support:\n        page_size = parsed_args.page_size\n        if page_size:\n            search_opts.update({\n                'limit': page_size,\n            })\n    subnet_ids = []\n    for n in data:\n        if ('subnets' in n):\n            subnet_ids.extend(n['subnets'])\n\n    def _get_subnet_list(sub_ids):\n        search_opts['id'] = sub_ids\n        return neutron_client.list_subnets(**search_opts).get('subnets', [])\n    try:\n        subnets = _get_subnet_list(subnet_ids)\n    except exceptions.RequestURITooLong as uri_len_exc:\n        subnet_count = len(subnet_ids)\n        max_size = ((self.subnet_id_filter_len * subnet_count) - uri_len_exc.excess)\n        chunk_size = (max_size // self.subnet_id_filter_len)\n        subnets = []\n        for i in range(0, subnet_count, chunk_size):\n            subnets.extend(_get_subnet_list(subnet_ids[i:(i + chunk_size)]))\n    subnet_dict = dict([(s['id'], s) for s in subnets])\n    for n in data:\n        if ('subnets' in n):\n            n['subnets'] = [(subnet_dict.get(s) or {\n                'id': s,\n            }) for s in n['subnets']]\n", "label": 1}
{"function": "\n\ndef __dir__(self):\n    'return list of member names'\n    cls_members = []\n    cname = self.__class__.__name__\n    if ((cname != 'SymbolTable') and hasattr(self, '__class__')):\n        cls_members = dir(self.__class__)\n    dict_keys = [key for key in self.__dict__ if (key not in cls_members)]\n    return [key for key in (cls_members + dict_keys) if ((not key.startswith('_SymbolTable_')) and (not key.startswith('_Group_')) and (not key.startswith(('_%s_' % cname))) and (not (key.startswith('__') and key.endswith('__'))) and (key not in self.__private))]\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.LIST):\n                self.expressions = []\n                (_etype17, _size14) = iprot.readListBegin()\n                for _i18 in xrange(_size14):\n                    _elem19 = IndexExpression()\n                    _elem19.read(iprot)\n                    self.expressions.append(_elem19)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.start_key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef run(self, show_trees=False):\n    '\\n        Sentences in the test suite are divided into two classes:\\n         - grammatical (``accept``) and\\n         - ungrammatical (``reject``).\\n        If a sentence should parse accordng to the grammar, the value of\\n        ``trees`` will be a non-empty list. If a sentence should be rejected\\n        according to the grammar, then the value of ``trees`` will be None.\\n        '\n    for test in self.suite:\n        print((test['doc'] + ':'), end=' ')\n        for key in ['accept', 'reject']:\n            for sent in test[key]:\n                tokens = sent.split()\n                trees = list(self.cp.parse(tokens))\n                if (show_trees and trees):\n                    print()\n                    print(sent)\n                    for tree in trees:\n                        print(tree)\n                if (key == 'accept'):\n                    if (trees == []):\n                        raise ValueError((\"Sentence '%s' failed to parse'\" % sent))\n                    else:\n                        accepted = True\n                elif trees:\n                    raise ValueError((\"Sentence '%s' received a parse'\" % sent))\n                else:\n                    rejected = True\n        if (accepted and rejected):\n            print('All tests passed!')\n", "label": 1}
{"function": "\n\ndef test_sqrt_rounding():\n    for i in [2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15]:\n        i = from_int(i)\n        for dps in [7, 15, 83, 106, 2000]:\n            mp.dps = dps\n            a = mpf_pow_int(mpf_sqrt(i, mp.prec, round_down), 2, mp.prec, round_down)\n            b = mpf_pow_int(mpf_sqrt(i, mp.prec, round_up), 2, mp.prec, round_up)\n            assert mpf_lt(a, i)\n            assert mpf_gt(b, i)\n    random.seed(1234)\n    prec = 100\n    for rnd in [round_down, round_nearest, round_ceiling]:\n        for i in range(100):\n            a = mpf_rand(prec)\n            b = mpf_mul(a, a)\n            assert (mpf_sqrt(b, prec, rnd) == a)\n    mp.dps = 100\n    a = (mpf(9) + 1e-90)\n    b = (mpf(9) - 1e-90)\n    mp.dps = 15\n    assert (sqrt(a, rounding='d') == 3)\n    assert (sqrt(a, rounding='n') == 3)\n    assert (sqrt(a, rounding='u') > 3)\n    assert (sqrt(b, rounding='d') < 3)\n    assert (sqrt(b, rounding='n') == 3)\n    assert (sqrt(b, rounding='u') == 3)\n    assert (sqrt(mpf('7.0503726185518891')) == mpf('2.655253776675949'))\n", "label": 1}
{"function": "\n\ndef maximalSquare(self, matrix):\n    if (not matrix):\n        return 0\n    (H, W) = (0, 1)\n    table = [[[0, 0] for j in xrange(len(matrix[0]))] for i in xrange(len(matrix))]\n    for i in reversed(xrange(len(matrix))):\n        for j in reversed(xrange(len(matrix[i]))):\n            if (matrix[i][j] == '1'):\n                (h, w) = (1, 1)\n                if ((i + 1) < len(matrix)):\n                    h = (table[(i + 1)][j][H] + 1)\n                if ((j + 1) < len(matrix[i])):\n                    w = (table[i][(j + 1)][W] + 1)\n                table[i][j] = [h, w]\n    s = [[0 for j in xrange(len(matrix[0]))] for i in xrange(len(matrix))]\n    max_square_area = 0\n    for i in reversed(xrange(len(matrix))):\n        for j in reversed(xrange(len(matrix[i]))):\n            side = min(table[i][j][H], table[i][j][W])\n            if (matrix[i][j] == '1'):\n                if (((i + 1) < len(matrix)) and ((j + 1) < len(matrix[(i + 1)]))):\n                    side = min((s[(i + 1)][(j + 1)] + 1), side)\n                s[i][j] = side\n                max_square_area = max(max_square_area, (side * side))\n    return max_square_area\n", "label": 1}
{"function": "\n\ndef flatten_data(self, resource_object, parser_context, is_list):\n    '\\n        Flattens data objects, making attributes and relationships fields the same level as id and type.\\n        '\n    relationships = resource_object.get('relationships')\n    is_relationship = parser_context.get('is_relationship')\n    request_method = parser_context['request'].method\n    if (is_relationship and (request_method == 'POST')):\n        if (not relationships):\n            raise JSONAPIException(source={\n                'pointer': '/data/relationships',\n            }, detail=NO_RELATIONSHIPS_ERROR)\n    elif (('attributes' not in resource_object) and (request_method != 'DELETE')):\n        raise JSONAPIException(source={\n            'pointer': '/data/attributes',\n        }, detail=NO_ATTRIBUTES_ERROR)\n    object_id = resource_object.get('id')\n    object_type = resource_object.get('type')\n    if (is_list and (request_method == 'DELETE')):\n        if (object_id is None):\n            raise JSONAPIException(source={\n                'pointer': '/data/id',\n            }, detail=NO_ID_ERROR)\n        if (object_type is None):\n            raise JSONAPIException(source={\n                'pointer': '/data/type',\n            }, detail=NO_TYPE_ERROR)\n    attributes = resource_object.get('attributes')\n    parsed = {\n        'id': object_id,\n        'type': object_type,\n    }\n    if attributes:\n        parsed.update(attributes)\n    if relationships:\n        relationships = self.flatten_relationships(relationships)\n        parsed.update(relationships)\n    return parsed\n", "label": 1}
{"function": "\n\ndef parse_options(self, arg):\n    '\\n        Parse options with the argv\\n\\n        :param arg: one arg from argv\\n        '\n    if (not arg.startswith('-')):\n        return False\n    value = None\n    if ('=' in arg):\n        (arg, value) = arg.split('=')\n    for option in self._option_list:\n        if (arg not in (option.shortname, option.longname)):\n            continue\n        action = option.action\n        if action:\n            action()\n        if (option.key == option.shortname):\n            self._results[option.key] = True\n            return True\n        if (option.boolean and option.default):\n            self._results[option.key] = False\n            return True\n        if option.boolean:\n            self._results[option.key] = True\n            return True\n        if (not value):\n            if self._argv:\n                value = self._argv[0]\n                self._argv = self._argv[1:]\n        if (not value):\n            raise RuntimeError(('Missing value for: %s' % option.name))\n        self._results[option.key] = option.to_python(value)\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I64):\n                self.latency_ns = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I64):\n                self.cpu_time_ns = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.cardinality = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I64):\n                self.memory_used = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef resolve(url):\n    try:\n        url = url.split('/preview', 1)[0]\n        url = url.replace('drive.google.com', 'docs.google.com')\n        result = client.request(url)\n        result = re.compile('\"fmt_stream_map\",(\".+?\")').findall(result)[0]\n        u = json.loads(result)\n        u = [i.split('|')[(- 1)] for i in u.split(',')]\n        u = sum([tag(i) for i in u], [])\n        url = []\n        try:\n            url += [[i for i in u if (i['quality'] == '1080p')][0]]\n        except:\n            pass\n        try:\n            url += [[i for i in u if (i['quality'] == 'HD')][0]]\n        except:\n            pass\n        try:\n            url += [[i for i in u if (i['quality'] == 'SD')][0]]\n        except:\n            pass\n        if (url == []):\n            return\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    if ('REQUEST_URI' not in environ):\n        environ['REQUEST_URI'] = (quote(environ.get('SCRIPT_NAME', '')) + quote(environ.get('PATH_INFO', '')))\n    if self.include_os_environ:\n        cgi_environ = os.environ.copy()\n    else:\n        cgi_environ = {\n            \n        }\n    for name in environ:\n        if ((name.upper() == name) and isinstance(environ[name], str)):\n            cgi_environ[name] = environ[name]\n    if (self.query_string is not None):\n        old = cgi_environ.get('QUERY_STRING', '')\n        if old:\n            old += '&'\n        cgi_environ['QUERY_STRING'] = (old + self.query_string)\n    cgi_environ['SCRIPT_FILENAME'] = self.script\n    proc = subprocess.Popen([self.script], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=cgi_environ, cwd=os.path.dirname(self.script))\n    writer = CGIWriter(environ, start_response)\n    if (select and (sys.platform != 'win32')):\n        proc_communicate(proc, stdin=StdinReader.from_environ(environ), stdout=writer, stderr=environ['wsgi.errors'])\n    else:\n        (stdout, stderr) = proc.communicate(StdinReader.from_environ(environ).read())\n        if stderr:\n            environ['wsgi.errors'].write(stderr)\n        writer.write(stdout)\n    if (not writer.headers_finished):\n        start_response(writer.status, writer.headers)\n    return []\n", "label": 1}
{"function": "\n\ndef collect(self):\n    '\\n        Collect metrics\\n        '\n    collected = {\n        \n    }\n    files = []\n    if isinstance(self.config['dir'], basestring):\n        dirs = [d.strip() for d in self.config['dir'].split(',')]\n    elif isinstance(self.config['dir'], list):\n        dirs = self.config['dir']\n    if isinstance(self.config['files'], basestring):\n        files = [f.strip() for f in self.config['files'].split(',')]\n    elif isinstance(self.config['files'], list):\n        files = self.config['files']\n    for sdir in dirs:\n        for sfile in files:\n            if sfile.endswith('conntrack_count'):\n                metric_name = 'ip_conntrack_count'\n            elif sfile.endswith('conntrack_max'):\n                metric_name = 'ip_conntrack_max'\n            else:\n                self.log.error('Unknown file for collection: %s', sfile)\n                continue\n            fpath = os.path.join(sdir, sfile)\n            if (not os.path.exists(fpath)):\n                continue\n            try:\n                with open(fpath, 'r') as fhandle:\n                    metric = float(fhandle.readline().rstrip('\\n'))\n                    collected[metric_name] = metric\n            except Exception as exception:\n                self.log.error(\"Failed to collect from '%s': %s\", fpath, exception)\n    if (not collected):\n        self.log.error('No metric was collected, looks like nf_conntrack/ip_conntrack kernel module was not loaded')\n    else:\n        for key in collected.keys():\n            self.publish(key, collected[key])\n", "label": 1}
{"function": "\n\ndef arg2nodes(self, args, node_factory=_null, lookup_list=_null, **kw):\n    if (node_factory is _null):\n        node_factory = self.fs.File\n    if (lookup_list is _null):\n        lookup_list = self.lookup_list\n    if (not args):\n        return []\n    args = SCons.Util.flatten(args)\n    nodes = []\n    for v in args:\n        if SCons.Util.is_String(v):\n            n = None\n            for l in lookup_list:\n                n = l(v)\n                if (n is not None):\n                    break\n            if (n is not None):\n                if SCons.Util.is_String(n):\n                    kw['raw'] = 1\n                    n = self.subst(n, **kw)\n                    if node_factory:\n                        n = node_factory(n)\n                if SCons.Util.is_List(n):\n                    nodes.extend(n)\n                else:\n                    nodes.append(n)\n            elif node_factory:\n                kw['raw'] = 1\n                v = node_factory(self.subst(v, **kw))\n                if SCons.Util.is_List(v):\n                    nodes.extend(v)\n                else:\n                    nodes.append(v)\n        else:\n            nodes.append(v)\n    return nodes\n", "label": 1}
{"function": "\n\ndef download_default_pages(names, prefix):\n    from httplib import HTTPSConnection\n    host = 'trac.edgewall.org'\n    if (prefix and (not prefix.endswith('/'))):\n        prefix += '/'\n    conn = HTTPSConnection(host)\n    for name in names:\n        if (name in ('WikiStart', 'SandBox')):\n            continue\n        sys.stdout.write(('Downloading %s%s' % (prefix, name)))\n        conn.request('GET', ('/wiki/%s%s?format=txt' % (prefix, name)))\n        response = conn.getresponse()\n        content = response.read()\n        if (prefix and ((response.status != 200) or (not content))):\n            sys.stdout.write((' %s' % name))\n            conn.request('GET', ('/wiki/%s?format=txt' % name))\n            response = conn.getresponse()\n            content = response.read()\n        if ((response.status == 200) and content):\n            with open(('trac/wiki/default-pages/' + name), 'w') as f:\n                lines = content.replace('\\r\\n', '\\n').splitlines(True)\n                f.write(''.join((line for line in lines if (line.strip() != '[[TranslatedPages]]'))))\n            sys.stdout.write('\\tdone.\\n')\n        else:\n            sys.stdout.write('\\tmissing or empty.\\n')\n    conn.close()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.groupName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_versions(default=DEFAULT, verbose=False):\n    assert (versionfile_source is not None), 'please set versioneer.versionfile_source'\n    assert (tag_prefix is not None), 'please set versioneer.tag_prefix'\n    assert (parentdir_prefix is not None), 'please set versioneer.parentdir_prefix'\n    root = get_root()\n    versionfile_abs = os.path.join(root, versionfile_source)\n    variables = get_expanded_variables(versionfile_abs)\n    if variables:\n        ver = versions_from_expanded_variables(variables, tag_prefix)\n        if ver:\n            if verbose:\n                print(('got version from expanded variable %s' % ver))\n            return ver\n    ver = versions_from_file(versionfile_abs)\n    if ver:\n        if verbose:\n            print(('got version from file %s %s' % (versionfile_abs, ver)))\n        return ver\n    ver = versions_from_vcs(tag_prefix, root, verbose)\n    if ver:\n        if verbose:\n            print(('got version from git %s' % ver))\n        return ver\n    ver = versions_from_parentdir(parentdir_prefix, root, verbose)\n    if ver:\n        if verbose:\n            print(('got version from parentdir %s' % ver))\n        return ver\n    if verbose:\n        print(('got version from default %s' % ver))\n    return default\n", "label": 1}
{"function": "\n\ndef checkNode(graph, superpipeline, source, target, nodeType, expectedDataType, values, isInput):\n    errors = er.consistencyErrors()\n    data = superpipeline.pipelineConfigurationData[superpipeline.pipeline]\n    longFormArgument = graph.CM_getArgumentAttribute(graph.graph, source, target, 'longFormArgument')\n    dataType = graph.CM_getArgumentAttribute(graph.graph, source, target, 'dataType')\n    isLinkOnly = graph.CM_getArgumentAttribute(graph.graph, source, target, 'isLinkOnly')\n    if (not isLinkOnly):\n        if (not expectedDataType):\n            expectedDataType = dataType\n        elif (expectedDataType != dataType):\n            print('dataConsistency.checkNode - 1', dataType, expectedDataType)\n            exit(0)\n        for value in values:\n            if (not isCorrectDataType(value, expectedDataType)):\n                print('dataConsistency.checkNode - 2', longFormArgument, value, dataType, type(value))\n                exit(0)\n            if (nodeType == 'file'):\n                if (not graph.CM_getArgumentAttribute(graph.graph, source, target, 'isStub')):\n                    extensions = graph.CM_getArgumentAttribute(graph.graph, source, target, 'extensions')\n                    if extensions:\n                        task = (target if isInput else source)\n                        fileNodeId = (source if isInput else target)\n                        if (not checkExtensions(value, extensions)):\n                            if (longFormArgument in data.longFormArguments.keys()):\n                                shortFormArgument = data.longFormArguments[longFormArgument].shortFormArgument\n                                errors.invalidExtensionPipeline(longFormArgument, shortFormArgument, value, extensions)\n                            else:\n                                shortFormArgument = graph.CM_getArgumentAttribute(graph.graph, source, target, 'shortFormArgument')\n                                errors.invalidExtension(task, longFormArgument, shortFormArgument, value, extensions)\n        return expectedDataType\n", "label": 1}
{"function": "\n\ndef _write_viewing_info(self, group):\n    if ((self.peeloff_origin is not None) and (self.inside_observer is not None)):\n        raise Exception('Cannot specify inside observer and peeloff origin at the same time')\n    if (self.inside_observer is not None):\n        group.attrs['inside_observer'] = bool2str(True)\n        self._write_inside_observer(group)\n        if (self.viewing_angles == []):\n            self.set_viewing_angles([90.0], [0.0])\n        if (self.image and (self.xmin < self.xmax)):\n            raise ValueError('longitudes should increase towards the left for inside observers')\n        if (self.d_min < 0.0):\n            if (self.d_min != (- np.inf)):\n                raise ValueError('Lower limit of depth should be positive for inside observer')\n            self.d_min = 0.0\n        if (self.d_max < 0.0):\n            raise ValueError('Upper limit of depth should be positive for inside observer')\n    elif (len(self.viewing_angles) > 0):\n        group.attrs['inside_observer'] = bool2str(False)\n        if (self.peeloff_origin is None):\n            self.set_peeloff_origin((0.0, 0.0, 0.0))\n        self._write_peeloff_origin(group)\n    else:\n        raise Exception('Need to specify either observer position, or viewing angles')\n    self._write_ignore_optical_depth(group)\n    self._write_viewing_angles(group)\n    self._write_depth(group)\n", "label": 1}
{"function": "\n\ndef line_contains_open_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)/\\\\*', line) and (not re.match('^(\\\\+|\\\\-)(.*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(.*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef _get_container_description(self, state, name, network_state=True, ip_partitions=None):\n    state_container = state.containers[name]\n    container_id = state_container['id']\n    try:\n        container = self.docker_client.inspect_container(container_id)\n    except docker.errors.APIError as err:\n        if (err.response.status_code == 404):\n            return Container(name, container_id, ContainerState.MISSING)\n        else:\n            raise\n    state_dict = container.get('State')\n    if (state_dict and state_dict.get('Running')):\n        container_state = ContainerState.UP\n    else:\n        container_state = ContainerState.DOWN\n    extras = {\n        \n    }\n    network = container.get('NetworkSettings')\n    ip = None\n    if network:\n        ip = network.get('IPAddress')\n        if ip:\n            extras['ip_address'] = ip\n    if (network_state and (name in state.containers) and (container_state == ContainerState.UP)):\n        device = state_container['device']\n        extras['device'] = device\n        extras['network_state'] = self.network.network_state(device)\n        if (ip_partitions and ip):\n            extras['partition'] = ip_partitions.get(ip)\n    else:\n        extras['network_state'] = NetworkState.UNKNOWN\n        extras['device'] = None\n    cfg_container = self.config.containers.get(name)\n    extras['neutral'] = (cfg_container.neutral if cfg_container else False)\n    extras['holy'] = (cfg_container.holy if cfg_container else False)\n    return Container(name, container_id, container_state, **extras)\n", "label": 1}
{"function": "\n\ndef _filter_metadata(metadata, **kwargs):\n    if (not isinstance(metadata, dict)):\n        return metadata\n    filtered_metadata = {\n        \n    }\n    for (key, value) in metadata.items():\n        if (key == '_self'):\n            default_value = value.get('default_value', None)\n            if (default_value is None):\n                default_callback_params = value.get('default_callback_params', {\n                    \n                })\n                callback_params = dict(kwargs)\n                if default_callback_params:\n                    callback_params.update(default_callback_params)\n                default_callback = value.get('default_callback', None)\n                if default_callback:\n                    default_value = default_callback(key, **callback_params)\n            options = value.get('options', None)\n            if (options is None):\n                options_callback_params = value.get('options_callback_params', {\n                    \n                })\n                callback_params = dict(kwargs)\n                if options_callback_params:\n                    callback_params.update(options_callback_params)\n                options_callback = value.get('options_callback', None)\n                if options_callback:\n                    options = options_callback(key, **callback_params)\n            filtered_metadata[key] = value\n            if (default_value is not None):\n                filtered_metadata[key]['default_value'] = default_value\n            if (options is not None):\n                filtered_metadata[key]['options'] = options\n        else:\n            filtered_metadata[key] = _filter_metadata(value, **kwargs)\n    return filtered_metadata\n", "label": 1}
{"function": "\n\ndef query_library(self, query, tie_breaker=no_tiebreak, modifiers=None, auto=False):\n    'Queries the library for songs.\\n        returns a list of matches, or None.\\n        '\n    if (not modifiers):\n        modifiers = []\n    try:\n        if (not auto):\n            return self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, modifiers, auto))\n        else:\n            current_mods = modifiers[:]\n            future_mods = (m for m in self.auto_modifiers if (m not in modifiers))\n            while True:\n                results = self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, current_mods, auto))\n                if (not results):\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        return results\n                elif (len(results) == 1):\n                    return results\n                else:\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        raise self.TieBroken(tie_breaker(query, results))\n                    next_results = self.query_library(query, tie_breaker, current_mods, auto)\n                    if (not next_results):\n                        raise self.TieBroken(tie_breaker(query, results))\n                    else:\n                        return next_results\n    except self.TieBroken as tie:\n        return tie.results\n", "label": 1}
{"function": "\n\ndef find_corpus_fileids(root, regexp):\n    if (not isinstance(root, PathPointer)):\n        raise TypeError('find_corpus_fileids: expected a PathPointer')\n    regexp += '$'\n    if isinstance(root, ZipFilePathPointer):\n        fileids = [name[len(root.entry):] for name in root.zipfile.namelist() if (not name.endswith('/'))]\n        items = [name for name in fileids if re.match(regexp, name)]\n        return sorted(items)\n    elif isinstance(root, FileSystemPathPointer):\n        items = []\n        kwargs = {\n            \n        }\n        if (not py25()):\n            kwargs = {\n                'followlinks': True,\n            }\n        for (dirname, subdirs, fileids) in os.walk(root.path, **kwargs):\n            prefix = ''.join((('%s/' % p) for p in _path_from(root.path, dirname)))\n            items += [(prefix + fileid) for fileid in fileids if re.match(regexp, (prefix + fileid))]\n            if ('.svn' in subdirs):\n                subdirs.remove('.svn')\n        return sorted(items)\n    else:\n        raise AssertionError((\"Don't know how to handle %r\" % root))\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef action_for_event_by_user(self, event, handler, current_state):\n    actions_by_user = {\n        \n    }\n    users_dict = (yield self.store.are_guests(self.rules_by_user.keys()))\n    filtered_by_user = (yield handler.filter_events_for_clients(users_dict.items(), [event], {\n        event.event_id: current_state,\n    }))\n    room_members = (yield self.store.get_users_in_room(self.room_id))\n    evaluator = PushRuleEvaluatorForEvent(event, len(room_members))\n    condition_cache = {\n        \n    }\n    display_names = {\n        \n    }\n    for ev in current_state.values():\n        nm = ev.content.get('displayname', None)\n        if (nm and (ev.type == EventTypes.Member)):\n            display_names[ev.state_key] = nm\n    for (uid, rules) in self.rules_by_user.items():\n        display_name = display_names.get(uid, None)\n        filtered = filtered_by_user[uid]\n        if (len(filtered) == 0):\n            continue\n        if (filtered[0].sender == uid):\n            continue\n        for rule in rules:\n            if (('enabled' in rule) and (not rule['enabled'])):\n                continue\n            matches = _condition_checker(evaluator, rule['conditions'], uid, display_name, condition_cache)\n            if matches:\n                actions = [x for x in rule['actions'] if (x != 'dont_notify')]\n                if (actions and ('notify' in actions)):\n                    actions_by_user[uid] = actions\n                break\n    defer.returnValue(actions_by_user)\n", "label": 1}
{"function": "\n\ndef create_tree_from_coverage(cov, strip_prefix=None, path_aliases=None, cover=[], exclude=[]):\n    'Create a tree with coverage statistics.\\n\\n    Takes a coverage.coverage() instance.\\n\\n    Returns the root node of the tree.\\n    '\n    root = CoverageNode()\n    if path_aliases:\n        apply_path_aliases(cov, dict([alias.partition('=')[::2] for alias in path_aliases]))\n    for filename in cov.data.measured_files():\n        if (not any(((pattern in filename.replace('/', '.')) for pattern in cover))):\n            continue\n        if any(((pattern in filename.replace('/', '.')) for pattern in exclude)):\n            continue\n        if (strip_prefix and filename.startswith(strip_prefix)):\n            short_name = filename[len(strip_prefix):]\n            short_name = short_name.replace('/', os.path.sep)\n            short_name = short_name.lstrip(os.path.sep)\n        else:\n            short_name = cov.file_locator.relative_filename(filename)\n        tree_index = filename_to_list(short_name.replace(os.path.sep, '.'))\n        if (('tests' in tree_index) or ('ftests' in tree_index)):\n            continue\n        root.set_at(tree_index, CoverageCoverageNode(cov, filename))\n    return root\n", "label": 1}
{"function": "\n\ndef communicate(params, request):\n\n    def authorize_request():\n        request['client_time'] = current_milli_time()\n        request['2fa_token'] = (params.mfa_token,)\n        request['2fa_type'] = params.mfa_type\n        request['session_token'] = params.session_token\n        request['username'] = params.user\n    if (not params.session_token):\n        try:\n            login(params)\n        except:\n            raise\n    authorize_request()\n    if params.debug:\n        print(('payload: ' + str(request)))\n    try:\n        r = requests.post(params.server, json=request)\n    except:\n        raise CommunicationError(sys.exc_info()[0])\n    response_json = r.json()\n    if params.debug:\n        debug_response(params, request, r)\n    if (response_json['result_code'] == 'auth_failed'):\n        if params.debug:\n            print('Re-authorizing.')\n        try:\n            login(params)\n        except:\n            raise\n        authorize_request()\n        try:\n            r = requests.post(params.server, json=request)\n        except:\n            print('Comm error during re-auth')\n            raise CommunicationError(sys.exc_info()[0])\n        response_json = r.json()\n        if params.debug:\n            debug_response(params, request, r)\n    if (response_json['result'] != 'success'):\n        if response_json['result_code']:\n            raise CommunicationError(('Unexpected problem: ' + response_json['result_code']))\n    return response_json\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.threadName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.threadStringRepresentation = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.isDaemon = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.LIST):\n                self.stackTrace = []\n                (_etype3, _size0) = iprot.readListBegin()\n                for _i4 in xrange(_size0):\n                    _elem5 = StackTraceElement()\n                    _elem5.read(iprot)\n                    self.stackTrace.append(_elem5)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _prettifyETree(self, elem):\n    ' Recursively add linebreaks to ElementTree children. '\n    i = '\\n'\n    if (markdown.isBlockLevel(elem.tag) and (elem.tag not in ['code', 'pre'])):\n        if (((not elem.text) or (not elem.text.strip())) and len(elem) and markdown.isBlockLevel(elem[0].tag)):\n            elem.text = i\n        for e in elem:\n            if markdown.isBlockLevel(e.tag):\n                self._prettifyETree(e)\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n    if ((not elem.tail) or (not elem.tail.strip())):\n        elem.tail = i\n", "label": 1}
{"function": "\n\ndef write(self, s):\n    self._checkWritable()\n    if self.closed:\n        raise ValueError('write to closed file')\n    if (not isinstance(s, unicode)):\n        raise TypeError((\"can't write %s to text stream\" % s.__class__.__name__))\n    length = len(s)\n    haslf = ((self._writetranslate or self._line_buffering) and ('\\n' in s))\n    if (haslf and self._writetranslate and (self._writenl != '\\n')):\n        s = s.replace('\\n', self._writenl)\n    encoder = (self._encoder or self._get_encoder())\n    b = encoder.encode(s)\n    self.buffer.write(b)\n    if (self._line_buffering and (haslf or ('\\r' in s))):\n        self.flush()\n    self._snapshot = None\n    if self._decoder:\n        self._decoder.reset()\n    return length\n", "label": 1}
{"function": "\n\ndef dfs(self, s, cur, left, pi, i, rmcnt, ret):\n    '\\n        Remove parenthesis\\n        backtracking, post-check\\n        :param s: original string\\n        :param cur: current string builder\\n        :param left: number of remaining left parentheses in s[0..i] not consumed by \")\"\\n        :param pi: last removed char\\n        :param i: current index\\n        :param rmcnt: number of remaining removals needed\\n        :param ret: results\\n        '\n    if ((left < 0) or (rmcnt < 0) or (i > len(s))):\n        return\n    if (i == len(s)):\n        if ((rmcnt == 0) and (left == 0)):\n            ret.append(cur)\n        return\n    if (s[i] not in ('(', ')')):\n        self.dfs(s, (cur + s[i]), left, None, (i + 1), rmcnt, ret)\n    elif (pi == s[i]):\n        while ((i < len(s)) and pi and (pi == s[i])):\n            (i, rmcnt) = ((i + 1), (rmcnt - 1))\n        self.dfs(s, cur, left, pi, i, rmcnt, ret)\n    else:\n        self.dfs(s, cur, left, s[i], (i + 1), (rmcnt - 1), ret)\n        L = ((left + 1) if (s[i] == '(') else (left - 1))\n        self.dfs(s, (cur + s[i]), L, None, (i + 1), rmcnt, ret)\n", "label": 1}
{"function": "\n\ndef convert(self, model, field, field_args, multiple=False):\n    kwargs = {\n        'label': unicode((field.verbose_name or field.name or '')),\n        'description': (field.help_text or ''),\n        'validators': [],\n        'filters': [],\n        'default': field.default,\n    }\n    if field_args:\n        kwargs.update(field_args)\n    if field.required:\n        if (isinstance(field, IntField) or isinstance(field, FloatField)):\n            kwargs['validators'].append(validators.InputRequired())\n        else:\n            kwargs['validators'].append(validators.Required())\n    else:\n        kwargs['validators'].append(validators.Optional())\n    if field.choices:\n        kwargs['choices'] = field.choices\n        if isinstance(field, IntField):\n            kwargs['coerce'] = int\n        if (not multiple):\n            return f.SelectField(**kwargs)\n        else:\n            return f.SelectMultipleField(**kwargs)\n    ftype = type(field).__name__\n    if hasattr(field, 'to_form_field'):\n        return field.to_form_field(model, kwargs)\n    if (ftype in self.converters):\n        return self.converters[ftype](model, field, kwargs)\n", "label": 1}
{"function": "\n\n@classmethod\ndef update_next(cls, seconds, now):\n    'Setup theme for next update.'\n    cls.next_change = None\n    cls.day = None\n    closest = None\n    lowest = None\n    for t in cls.themes:\n        if ((seconds < t.time) and ((closest is None) or (t.time < closest.time))):\n            closest = t\n        if ((lowest is None) or (t.time < lowest.time)):\n            lowest = t\n    if (closest is not None):\n        cls.next_change = closest\n    elif (lowest is not None):\n        cls.next_change = lowest\n    if (lowest is not None):\n        cls.lowest = lowest\n    if ((cls.next_change.time == cls.lowest.time) and (seconds < cls.lowest.time)):\n        cls.day = (- 1)\n    else:\n        cls.day = now.day\n    debug_log(('Today = %d' % cls.day))\n    debug_log(('%s - Next Change @ %s' % (time.ctime(), str(cls.next_change))))\n", "label": 1}
{"function": "\n\n@classmethod\ndef read(cls, fd, str_cache=None, object_cache=None, traits_cache=None):\n    type_ = U8.read(fd)\n    if (str_cache is None):\n        str_cache = []\n    if (object_cache is None):\n        object_cache = []\n    if (traits_cache is None):\n        traits_cache = []\n    if ((type_ == AMF3_TYPE_UNDEFINED) or (type_ == AMF3_TYPE_NULL)):\n        return None\n    elif (type_ == AMF3_TYPE_FALSE):\n        return False\n    elif (type_ == AMF3_TYPE_TRUE):\n        return True\n    elif (type_ == AMF3_TYPE_STRING):\n        return AMF3String.read(fd, cache=str_cache)\n    elif (type_ == AMF3_TYPE_ARRAY):\n        return AMF3ArrayPacker.read(fd, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif (type_ == AMF3_TYPE_OBJECT):\n        return AMF3ObjectPacker.read(fd, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif (type_ == AMF3_TYPE_DATE):\n        return AMF3DatePacker.read(fd, cache=object_cache)\n    elif (type_ in cls.Readers):\n        return cls.Readers[type_].read(fd)\n    else:\n        raise IOError('Unhandled AMF3 type: {0}'.format(hex(type_)))\n", "label": 1}
{"function": "\n\ndef __delitem__(self, key):\n    if (isinstance(key, slice) or self._haswildcard(key)):\n        if isinstance(key, slice):\n            indices = range(*key.indices(len(self)))\n            if (key.step and (key.step < 0)):\n                indices = reversed(indices)\n        else:\n            indices = self._wildcardmatch(key)\n        for idx in reversed(indices):\n            del self[idx]\n        return\n    elif isinstance(key, string_types):\n        key = Card.normalize_keyword(key)\n        indices = self._keyword_indices\n        if (key not in self._keyword_indices):\n            indices = self._rvkc_indices\n        if (key not in indices):\n            raise KeyError((\"Keyword '%s' not found.\" % key))\n        for idx in reversed(indices[key]):\n            del self[idx]\n        return\n    idx = self._cardindex(key)\n    card = self._cards[idx]\n    keyword = card.keyword\n    del self._cards[idx]\n    indices = self._keyword_indices[keyword]\n    indices.remove(idx)\n    if (not indices):\n        del self._keyword_indices[keyword]\n    if (card.field_specifier is not None):\n        indices = self._rvkc_indices[card.rawkeyword]\n        indices.remove(idx)\n        if (not indices):\n            del self._rvkc_indices[card.rawkeyword]\n    self._updateindices(idx, increment=False)\n    self._modified = True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef assignment_onvalidation(form):\n    '\\n            Validation callback for work assignments:\\n            - a worker can only be assigned once to the same job\\n\\n            @param form: the FORM\\n        '\n    db = current.db\n    s3db = current.s3db\n    table = s3db.work_assignment\n    form_vars = form.vars\n    if ('id' in form_vars):\n        record_id = form_vars.id\n    elif hasattr(form, 'record_id'):\n        record_id = form.record_id\n    else:\n        record_id = None\n    try:\n        job_id = form_vars.job_id\n    except AttributeError:\n        job_id = None\n    try:\n        person_id = form_vars.person_id\n    except AttributeError:\n        person_id = None\n    if ((job_id is None) or (person_id is None)):\n        if record_id:\n            query = ((table.id == record_id) & (table.deleted != True))\n            record = db(query).select(table.job_id, table.person_id, limitby=(0, 1)).first()\n            if record:\n                job_id = record.job_id\n                person_id = record.person_id\n        else:\n            if (job_id is None):\n                job_id = table.job_id.default\n            if (person_id is None):\n                person_id = table.person_id.default\n    if (job_id and person_id):\n        query = (((table.job_id == job_id) & (table.person_id == person_id)) & (table.deleted != True))\n        if record_id:\n            query = ((table.id != record_id) & query)\n        duplicate = db(query).select(table.id, limitby=(0, 1)).first()\n        if duplicate:\n            msg = current.T('This person is already assigned to the job')\n            form.errors['person_id'] = msg\n", "label": 1}
{"function": "\n\ndef _collect(self):\n    LOG.debug(('collecting arguments/commands for %s' % self))\n    arguments = []\n    commands = []\n    arguments = list(self._meta.arguments)\n    for member in dir(self.__class__):\n        if member.startswith('_'):\n            continue\n        try:\n            func = getattr(self.__class__, member).__cement_meta__\n        except AttributeError:\n            continue\n        else:\n            func['controller'] = self\n            commands.append(func)\n    for contr in handler.list('controller'):\n        if (contr == self.__class__):\n            continue\n        contr = contr()\n        contr._setup(self.app)\n        if (contr._meta.stacked_on == self._meta.label):\n            if (contr._meta.stacked_type == 'embedded'):\n                (contr_arguments, contr_commands) = contr._collect()\n                for arg in contr_arguments:\n                    arguments.append(arg)\n                for func in contr_commands:\n                    commands.append(func)\n            elif (contr._meta.stacked_type == 'nested'):\n                metadict = {\n                    \n                }\n                metadict['label'] = re.sub('_', '-', contr._meta.label)\n                metadict['func_name'] = '_dispatch'\n                metadict['exposed'] = True\n                metadict['hide'] = contr._meta.hide\n                metadict['help'] = contr._meta.description\n                metadict['aliases'] = contr._meta.aliases\n                metadict['aliases_only'] = contr._meta.aliases_only\n                metadict['controller'] = contr\n                commands.append(metadict)\n    return (arguments, commands)\n", "label": 1}
{"function": "\n\ndef _raise_ssl_error(self, ssl, result):\n    if (self._context._verify_helper is not None):\n        self._context._verify_helper.raise_if_problem()\n    if (self._context._npn_advertise_helper is not None):\n        self._context._npn_advertise_helper.raise_if_problem()\n    if (self._context._npn_select_helper is not None):\n        self._context._npn_select_helper.raise_if_problem()\n    if (self._context._alpn_select_helper is not None):\n        self._context._alpn_select_helper.raise_if_problem()\n    error = _lib.SSL_get_error(ssl, result)\n    if (error == _lib.SSL_ERROR_WANT_READ):\n        raise WantReadError()\n    elif (error == _lib.SSL_ERROR_WANT_WRITE):\n        raise WantWriteError()\n    elif (error == _lib.SSL_ERROR_ZERO_RETURN):\n        raise ZeroReturnError()\n    elif (error == _lib.SSL_ERROR_WANT_X509_LOOKUP):\n        raise WantX509LookupError()\n    elif (error == _lib.SSL_ERROR_SYSCALL):\n        if (_lib.ERR_peek_error() == 0):\n            if (result < 0):\n                if (platform == 'win32'):\n                    errno = _ffi.getwinerror()[0]\n                else:\n                    errno = _ffi.errno\n                raise SysCallError(errno, errorcode.get(errno))\n            else:\n                raise SysCallError((- 1), 'Unexpected EOF')\n        else:\n            _raise_current_error()\n    elif (error == _lib.SSL_ERROR_NONE):\n        pass\n    else:\n        _raise_current_error()\n", "label": 1}
{"function": "\n\ndef wait_for_services(self, instances, callback=None):\n    assert self.connected\n    pending = set(instances)\n    ready = []\n    failed = []\n    while len(pending):\n        toRemove = []\n        for i in pending:\n            i.update()\n            if ('status' not in i.tags):\n                continue\n            if i.tags['status'].endswith('failed'):\n                toRemove.append(i)\n                failed.append(i)\n            elif (i.tags['status'] == 'ready'):\n                toRemove.append(i)\n                ready.append(i)\n        for i in toRemove:\n            pending.remove(i)\n        status = {\n            \n        }\n        for i in itertools.chain(pending, ready, failed):\n            if ('status' not in i.tags):\n                status_name = 'installing dependencies'\n            else:\n                status_name = i.tags['status']\n            if (status_name not in status):\n                status[status_name] = []\n            status[status_name].append(i)\n        if callback:\n            callback(status)\n        if len(pending):\n            time.sleep(1)\n    return (len(failed) == 0)\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw_args):\n    args = [sympify(arg) for arg in args]\n    args = TensMul._flatten(args)\n    is_canon_bp = kw_args.get('is_canon_bp', False)\n    if (not any([isinstance(arg, TensExpr) for arg in args])):\n        tids = TIDS([], [], [])\n    else:\n        tids_list = [arg._tids for arg in args if isinstance(arg, (Tensor, TensMul))]\n        if (len(tids_list) == 1):\n            for arg in args:\n                if (not isinstance(arg, Tensor)):\n                    continue\n                is_canon_bp = kw_args.get('is_canon_bp', arg._is_canon_bp)\n        tids = reduce((lambda a, b: (a * b)), tids_list)\n    if any([isinstance(arg, TensAdd) for arg in args]):\n        add_args = TensAdd._tensAdd_flatten(args)\n        return TensAdd(*add_args)\n    coeff = reduce((lambda a, b: (a * b)), ([S.One] + [arg for arg in args if (not isinstance(arg, TensExpr))]))\n    args = tids.get_tensors()\n    if (coeff != 1):\n        args = ([coeff] + args)\n    if (len(args) == 1):\n        return args[0]\n    obj = Basic.__new__(cls, *args)\n    obj._types = []\n    for t in tids.components:\n        obj._types.extend(t._types)\n    obj._tids = tids\n    obj._ext_rank = (len(obj._tids.free) + (2 * len(obj._tids.dum)))\n    obj._coeff = coeff\n    obj._is_canon_bp = is_canon_bp\n    return obj\n", "label": 1}
{"function": "\n\ndef processArgs(self, parsedArguments):\n    \"Update the Setup's config to reflect parsedArguments.\\n\\n        parsedArguments - the result of an argparse parse action.\\n        \"\n    parsed = parsedArguments\n    if (('stackLogfile' in parsed) and parsed.stackLogfile):\n        self.config.backgroundStackTraceLoopFilename = parsed.stackLogfile\n    if (('memoryLogfile' in parsed) and parsed.memoryLogfile):\n        self.config.backgroundMemoryUsageLoopFilename = parsed.memoryLogfile\n    if (('logging' in parsed) and parsed.logging):\n        self.config.setLoggingLevel(parsed.logging, parsed.background_logging)\n    if (('target' in parsed) and parsed.target):\n        self.config.target = parsed.target\n    if (('dataroot' in parsed) and parsed.dataroot):\n        path = os.path.expanduser(parsed.dataroot)\n        self.config.setRootDataDir(path)\n    if (('datarootsubdir' in parsed) and parsed.datarootsubdir):\n        path = os.path.join(self.config.rootDataDir, parsed.datarootsubdir)\n        self.config.setRootDataDir(path)\n    if (('baseport' in parsed) and parsed.baseport):\n        self.config.setAllPorts(parsed.baseport)\n    self.parsedArguments = parsedArguments\n", "label": 1}
{"function": "\n\ndef parse(self, argv=None):\n    '\\n        Parse argv of terminal\\n\\n        :param argv: default is sys.argv\\n        '\n    if (not argv):\n        argv = sys.argv\n    elif isinstance(argv, str):\n        argv = argv.split()\n    self._argv = argv[1:]\n    if (not self._argv):\n        self.validate_options()\n        if self._command_func:\n            self._command_func(**self._results)\n            return True\n        return False\n    cmd = self._argv[0]\n    if (not cmd.startswith('-')):\n        for command in self._command_list:\n            if (isinstance(command, Command) and (command._name == cmd)):\n                command._parent = self\n                return command.parse(self._argv)\n    _positional_index = 0\n    while self._argv:\n        arg = self._argv[0]\n        self._argv = self._argv[1:]\n        if (not self.parse_options(arg)):\n            self._args_results.append(arg)\n            if (len(self._positional_list) > _positional_index):\n                key = self._positional_list[_positional_index]\n                self._results[key] = arg\n                _positional_index += 1\n    self.validate_options()\n    if (self._parent and isinstance(self._parent, Command)):\n        self._parent._args_results = self._args_results\n    if self._command_func:\n        self._command_func(**self._results)\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef _key_opts(self):\n    '\\n        Return options for the ssh command base for Salt to call\\n        '\n    options = ['KbdInteractiveAuthentication=no']\n    if self.passwd:\n        options.append('PasswordAuthentication=yes')\n    else:\n        options.append('PasswordAuthentication=no')\n    if (self.opts.get('_ssh_version', (0,)) > (4, 9)):\n        options.append('GSSAPIAuthentication=no')\n    options.append('ConnectTimeout={0}'.format(self.timeout))\n    if self.opts.get('ignore_host_keys'):\n        options.append('StrictHostKeyChecking=no')\n    if self.opts.get('no_host_keys'):\n        options.extend(['StrictHostKeyChecking=no', 'UserKnownHostsFile=/dev/null'])\n    known_hosts = self.opts.get('known_hosts_file')\n    if (known_hosts and os.path.isfile(known_hosts)):\n        options.append('UserKnownHostsFile={0}'.format(known_hosts))\n    if self.port:\n        options.append('Port={0}'.format(self.port))\n    if self.priv:\n        options.append('IdentityFile={0}'.format(self.priv))\n    if self.user:\n        options.append('User={0}'.format(self.user))\n    if self.identities_only:\n        options.append('IdentitiesOnly=yes')\n    ret = []\n    for option in options:\n        ret.append('-o {0} '.format(option))\n    return ''.join(ret)\n", "label": 1}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a time. Returns a\\n        Python datetime.time object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value.time()\n    if isinstance(value, datetime.time):\n        return value\n    if isinstance(value, list):\n        if (len(value) != 2):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES)):\n            return None\n        start_value = value[0]\n        end_value = value[1]\n    start_time = None\n    end_time = None\n    for format in (self.input_formats or formats.get_format('TIME_INPUT_FORMATS')):\n        try:\n            start_time = datetime.datetime(*time.strptime(start_value, format)[:6]).time()\n        except ValueError:\n            if start_time:\n                continue\n            else:\n                raise ValidationError(self.error_messages['invalid'])\n    for format in (self.input_formats or formats.get_format('TIME_INPUT_FORMATS')):\n        try:\n            end_time = datetime.datetime(*time.strptime(end_value, format)[:6]).time()\n        except ValueError:\n            if end_time:\n                continue\n            else:\n                raise ValidationError(self.error_messages['invalid'])\n    return (start_time, end_time)\n", "label": 1}
{"function": "\n\ndef format_progress_line(self):\n    show_percent = self.show_percent\n    info_bits = []\n    if self.length_known:\n        bar_length = int((self.pct * self.width))\n        bar = (self.fill_char * bar_length)\n        bar += (self.empty_char * (self.width - bar_length))\n        if (show_percent is None):\n            show_percent = (not self.show_pos)\n    elif self.finished:\n        bar = (self.fill_char * self.width)\n    else:\n        bar = list((self.empty_char * (self.width or 1)))\n        if (self.time_per_iteration != 0):\n            bar[int((((math.cos((self.pos * self.time_per_iteration)) / 2.0) + 0.5) * self.width))] = self.fill_char\n        bar = ''.join(bar)\n    if self.show_pos:\n        info_bits.append(self.format_pos())\n    if show_percent:\n        info_bits.append(self.format_pct())\n    if (self.show_eta and self.eta_known and (not self.finished)):\n        info_bits.append(self.format_eta())\n    if (self.item_show_func is not None):\n        item_info = self.item_show_func(self.current_item)\n        if (item_info is not None):\n            info_bits.append(item_info)\n    return (self.bar_template % {\n        'label': self.label,\n        'bar': bar,\n        'info': self.info_sep.join(info_bits),\n    }).rstrip()\n", "label": 1}
{"function": "\n\ndef __init__(self, kernel, itspace, *args, **kwargs):\n    base.ParLoop.__init__(self, kernel, itspace, *args, **kwargs)\n    self.__unwound_args = []\n    self.__unique_args = []\n    self._arg_dict = {\n        \n    }\n    seen = set()\n    c = 0\n    for arg in self._actual_args:\n        if arg._is_mat:\n            for a in arg:\n                self.__unwound_args.append(a)\n        elif (arg._is_vec_map or arg._uses_itspace):\n            for (d, m) in zip(arg.data, arg.map):\n                for i in range(m.arity):\n                    a = d(arg.access, m[i])\n                    a.position = arg.position\n                    self.__unwound_args.append(a)\n        else:\n            for a in arg:\n                self.__unwound_args.append(a)\n        if arg._is_dat:\n            key = (arg.data, arg.map)\n            if arg._is_indirect:\n                arg._which_indirect = c\n                if (arg._is_vec_map or arg._flatten):\n                    c += arg.map.arity\n                elif arg._uses_itspace:\n                    c += self._it_space.extents[arg.idx.index]\n                else:\n                    c += 1\n            if (key not in seen):\n                self.__unique_args.append(arg)\n                seen.add(key)\n        else:\n            self.__unique_args.append(arg)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse_content(node):\n    \" Parse content from input node and returns ContentHandler object\\n        it'll look like:\\n\\n            - template:\\n                - file:\\n                    - temple: path\\n\\n            or something\\n\\n        \"\n    output = ContentHandler()\n    is_template_path = False\n    is_template_content = False\n    is_file = False\n    is_done = False\n    while (node and (not is_done)):\n        if isinstance(node, basestring):\n            output.content = node\n            output.setup(node, is_file=is_file, is_template_path=is_template_path, is_template_content=is_template_content)\n            return output\n        elif ((not isinstance(node, dict)) and (not isinstance(node, list))):\n            raise TypeError('Content must be a string, dictionary, or list of dictionaries')\n        is_done = True\n        flat = lowercase_keys(flatten_dictionaries(node))\n        for (key, value) in flat.items():\n            if (key == 'template'):\n                if isinstance(value, basestring):\n                    if is_file:\n                        value = os.path.abspath(value)\n                    output.content = value\n                    is_template_content = (is_template_content or (not is_file))\n                    output.is_template_content = is_template_content\n                    output.is_template_path = is_file\n                    output.is_file = is_file\n                    return output\n                else:\n                    is_template_content = True\n                    node = value\n                    is_done = False\n                    break\n            elif (key == 'file'):\n                if isinstance(value, basestring):\n                    output.content = os.path.abspath(value)\n                    output.is_file = True\n                    output.is_template_content = is_template_content\n                    return output\n                else:\n                    is_file = True\n                    node = value\n                    is_done = False\n                    break\n    raise Exception('Invalid configuration for content.')\n", "label": 1}
{"function": "\n\ndef step(self, dependency_states, step_context):\n    input_node = self._input_node()\n    input_state = dependency_states.get(input_node, None)\n    if ((input_state is None) or (type(input_state) == Waiting)):\n        return Waiting([input_node])\n    elif (type(input_state) == Throw):\n        return input_state\n    elif (type(input_state) == Noop):\n        return Noop('Could not compute {} in order to project its fields.'.format(input_node))\n    elif (type(input_state) != Return):\n        State.raise_unrecognized(input_state)\n    input_product = input_state.value\n    values = []\n    for field in self.fields:\n        values.append(getattr(input_product, field))\n    if ((len(values) == 1) and (type(values[0]) is self.projected_subject)):\n        projected_subject = values[0]\n    else:\n        projected_subject = self.projected_subject(*values)\n    output_node = self._output_node(step_context, projected_subject)\n    output_state = dependency_states.get(output_node, None)\n    if ((output_state is None) or (type(output_state) == Waiting)):\n        return Waiting([input_node, output_node])\n    elif (type(output_state) == Noop):\n        return Noop('Successfully projected, but no source of output product for {}.'.format(output_node))\n    elif (type(output_state) in [Throw, Return]):\n        return output_state\n    else:\n        raise State.raise_unrecognized(output_state)\n", "label": 1}
{"function": "\n\ndef extract_conf_from(mod, conf=ModuleConfig(CONF_SPEC), depth=0, max_depth=2):\n    'recursively extract keys from module or object\\n    by passed config scheme\\n    '\n    for (key, default_value) in six.iteritems(conf):\n        conf[key] = _get_key_from_module(mod, key, default_value)\n    try:\n        filtered_apps = [app for app in conf['apps'] if (app not in BLACKLIST)]\n    except TypeError:\n        pass\n    except Exception as e:\n        warnings.warn(('Error %s during loading %s' % (e, conf['apps'])))\n    for app in filtered_apps:\n        try:\n            app_module = import_module(app)\n            if (app_module != mod):\n                app_module = _get_correct_module(app_module)\n                if (depth < max_depth):\n                    mod_conf = extract_conf_from(app_module, depth=(depth + 1))\n                    for (k, v) in six.iteritems(mod_conf):\n                        if (k == 'config'):\n                            continue\n                        if isinstance(v, dict):\n                            conf[k].update(v)\n                        elif isinstance(v, (list, tuple)):\n                            conf[k] = merge(conf[k], v)\n        except Exception as e:\n            pass\n    return conf\n", "label": 1}
{"function": "\n\ndef parse(self, lines):\n    'parse the input lines from a robots.txt file.\\n           We allow that a user-agent: line is not preceded by\\n           one or more blank lines.'\n    state = 0\n    linenumber = 0\n    entry = Entry()\n    for line in lines:\n        linenumber += 1\n        if (not line):\n            if (state == 1):\n                entry = Entry()\n                state = 0\n            elif (state == 2):\n                self._add_entry(entry)\n                entry = Entry()\n                state = 0\n        i = line.find('#')\n        if (i >= 0):\n            line = line[:i]\n        line = line.strip()\n        if (not line):\n            continue\n        line = line.split(':', 1)\n        if (len(line) == 2):\n            line[0] = line[0].strip().lower()\n            line[1] = urllib.unquote(line[1].strip())\n            if (line[0] == 'user-agent'):\n                if (state == 2):\n                    self._add_entry(entry)\n                    entry = Entry()\n                entry.useragents.append(line[1])\n                state = 1\n            elif (line[0] == 'disallow'):\n                if (state != 0):\n                    entry.rulelines.append(RuleLine(line[1], False))\n                    state = 2\n            elif (line[0] == 'allow'):\n                if (state != 0):\n                    entry.rulelines.append(RuleLine(line[1], True))\n                    state = 2\n    if (state == 2):\n        self._add_entry(entry)\n", "label": 1}
{"function": "\n\ndef save(self, **kwargs):\n    if (not self.content_type_id):\n        self.content_type = ContentType.objects.get_for_model(self)\n    send_signal = None\n    old_self = None\n    if self.pk:\n        try:\n            old_self = self.__class__.objects.get(pk=self.pk)\n        except Publishable.DoesNotExist:\n            pass\n    if old_self:\n        old_path = old_self.get_absolute_url()\n        new_path = self.get_absolute_url()\n        if ((old_path != new_path) and new_path and (not old_self.static)):\n            redirect = Redirect.objects.get_or_create(old_path=old_path, site=self.category.site)[0]\n            redirect.new_path = new_path\n            redirect.save(force_update=True)\n            Redirect.objects.filter(new_path=old_path).exclude(pk=redirect.pk).update(new_path=new_path)\n        if (old_self.is_published() != self.is_published()):\n            if self.is_published():\n                send_signal = content_published\n                self.announced = True\n            else:\n                send_signal = content_unpublished\n                self.announced = False\n        if ((old_self.published != self.published) and (self.published is False)):\n            send_signal = content_unpublished\n            self.announced = False\n        if ((old_self.last_updated == old_self.publish_from) and (self.last_updated == old_self.last_updated)):\n            self.last_updated = self.publish_from\n    elif self.is_published():\n        send_signal = content_published\n        self.announced = True\n    if (not self.last_updated):\n        self.last_updated = self.publish_from\n    super(Publishable, self).save(**kwargs)\n    if send_signal:\n        send_signal.send(sender=self.__class__, publishable=self)\n", "label": 1}
{"function": "\n\ndef test_getitem(session):\n    set_ = session.set(key('test_sortedset_getitem'), S('abc'), SortedSet)\n    assert (set_['a'] == 1)\n    assert (set_['b'] == 1)\n    assert (set_['c'] == 1)\n    with raises(KeyError):\n        set_['d']\n    with raises(TypeError):\n        set_[123]\n    set_.update(a=2.1, c=(- 2))\n    assert (set_['a'] == 3.1)\n    assert (set_['b'] == 1)\n    assert (set_['c'] == (- 1))\n    setx = session.set(key('test_sortedsetx_getitem'), S([1, 2, 3]), IntSet)\n    assert (setx[1] == 1)\n    assert (setx[2] == 1)\n    assert (setx[3] == 1)\n    with raises(KeyError):\n        setx[4]\n    with raises(TypeError):\n        setx['a']\n    setx.update({\n        1: 2.1,\n        3: (- 2),\n    })\n    assert (setx[1] == 3.1)\n    assert (setx[2] == 1)\n    assert (setx[3] == (- 1))\n", "label": 1}
{"function": "\n\ndef __new__(mcs, cls_name, cls_bases, cls_attrs):\n    for (name, attr) in cls_attrs.items():\n        if (getattr(attr, '_unguarded', False) or (name in mcs.ALWAYS_UNGUARDED)):\n            continue\n        if name.startswith('_'):\n            continue\n        if isinstance(attr, type):\n            continue\n        is_property = isinstance(attr, property)\n        if (not (callable(attr) or is_property)):\n            continue\n        if is_property:\n            property_methods = defaultdict(None)\n            for fn_name in ('fdel', 'fset', 'fget'):\n                prop_fn = getattr(cls_attrs[name], fn_name, None)\n                if (prop_fn is not None):\n                    if getattr(prop_fn, '_unguarded', False):\n                        property_methods[fn_name] = prop_fn\n                    else:\n                        property_methods[fn_name] = pre_verify(prop_fn)\n            cls_attrs[name] = property(**property_methods)\n        else:\n            cls_attrs[name] = pre_verify(attr)\n    return super(_PageObjectMetaclass, mcs).__new__(mcs, cls_name, cls_bases, cls_attrs)\n", "label": 1}
{"function": "\n\ndef _get_desktop_streams(self, channel_id):\n    password = self.options.get('password')\n    channel = self._get_module_info('channel', channel_id, password, schema=_channel_schema)\n    if (not isinstance(channel.get('stream'), list)):\n        raise NoStreamsError(self.url)\n    streams = {\n        \n    }\n    for provider in channel['stream']:\n        if (provider['name'] == 'uhs_akamai'):\n            continue\n        provider_url = provider['url']\n        provider_name = provider['name']\n        for (stream_index, stream_info) in enumerate(provider['streams']):\n            stream = None\n            stream_height = int(stream_info.get('height', 0))\n            stream_name = stream_info.get('description')\n            if (not stream_name):\n                if (stream_height > 0):\n                    if (not stream_info.get('isTranscoded')):\n                        stream_name = '{0}p+'.format(stream_height)\n                    else:\n                        stream_name = '{0}p'.format(stream_height)\n                else:\n                    stream_name = 'live'\n            if (stream_name in streams):\n                provider_name_clean = provider_name.replace('uhs_', '')\n                stream_name += '_alt_{0}'.format(provider_name_clean)\n            if provider_name.startswith('uhs_'):\n                stream = UHSStream(self.session, channel_id, self.url, provider_name, stream_index, password)\n            elif provider_url.startswith('rtmp'):\n                playpath = stream_info['streamName']\n                stream = self._create_rtmp_stream(provider_url, playpath)\n            if stream:\n                streams[stream_name] = stream\n    return streams\n", "label": 1}
{"function": "\n\ndef test_operations():\n    per = SeqPer((1, 2), (n, 0, oo))\n    per2 = SeqPer((2, 4), (n, 0, oo))\n    form = SeqFormula((n ** 2))\n    form2 = SeqFormula((n ** 3))\n    assert (((per + form) + form2) == SeqAdd(per, form, form2))\n    assert (((per + form) - form2) == SeqAdd(per, form, (- form2)))\n    assert (((per + form) - S.EmptySequence) == SeqAdd(per, form))\n    assert (((per + per2) + form) == SeqAdd(SeqPer((3, 6), (n, 0, oo)), form))\n    assert ((S.EmptySequence - per) == (- per))\n    assert ((form + form) == SeqFormula((2 * (n ** 2))))\n    assert (((per * form) * form2) == SeqMul(per, form, form2))\n    assert ((form * form) == SeqFormula((n ** 4)))\n    assert ((form * (- form)) == SeqFormula((- (n ** 4))))\n    assert ((form * (per + form2)) == SeqMul(form, SeqAdd(per, form2)))\n    assert ((form * (per + per)) == SeqMul(form, per2))\n    assert (form.coeff_mul(m) == SeqFormula((m * (n ** 2)), (n, 0, oo)))\n    assert (per.coeff_mul(m) == SeqPer((m, (2 * m)), (n, 0, oo)))\n", "label": 1}
{"function": "\n\ndef rAssertAlmostEqual(self, a, b, rel_err=2e-15, abs_err=5e-323, msg=None):\n    'Fail if the two floating-point numbers are not almost equal.\\n\\n        Determine whether floating-point values a and b are equal to within\\n        a (small) rounding error.  The default values for rel_err and\\n        abs_err are chosen to be suitable for platforms where a float is\\n        represented by an IEEE 754 double.  They allow an error of between\\n        9 and 19 ulps.\\n        '\n    if math.isnan(a):\n        if math.isnan(b):\n            return\n        self.fail((msg or '{!r} should be nan'.format(b)))\n    if math.isinf(a):\n        if (a == b):\n            return\n        self.fail((msg or 'finite result where infinity expected: expected {!r}, got {!r}'.format(a, b)))\n    if ((not a) and (not b)):\n        if (math.copysign(1.0, a) != math.copysign(1.0, b)):\n            self.fail((msg or 'zero has wrong sign: expected {!r}, got {!r}'.format(a, b)))\n    try:\n        absolute_error = abs((b - a))\n    except OverflowError:\n        pass\n    else:\n        if (absolute_error <= max(abs_err, (rel_err * abs(a)))):\n            return\n    self.fail((msg or '{!r} and {!r} are not sufficiently close'.format(a, b)))\n", "label": 1}
{"function": "\n\ndef test_fetchable_get_item():\n    call_counter = mock.Mock()\n\n    def fetch():\n        for i in range(3):\n            for i in range(5):\n                (yield i)\n            call_counter()\n    fetcher = mock.Mock(fetch=fetch)\n    fetchable = util.Fetchable(fetcher)\n    assert (fetchable[:2] == [0, 1])\n    assert (fetchable[:3] == [0, 1, 2])\n    assert (fetchable[3] == 3)\n    assert (call_counter.call_count == 0)\n    assert (fetchable[9] == 4)\n    assert (call_counter.call_count == 1)\n    assert (fetchable[2:3] == [2])\n    assert (fetchable[:] == fetchable[:])\n    assert (len(fetchable[:]) == 15)\n    assert (fetchable[:100000] == fetchable[:])\n    assert (fetchable[100000:] == [])\n    assert (fetchable[100000:10000000] == [])\n    with pytest.raises(IndexError):\n        fetchable[12312312]\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    self.login_default_admin()\n    project = self.create_project()\n    plan = self.create_plan(project, label='Foo')\n    step = self.create_step(plan=plan)\n    self.login_default_admin()\n    path = '/api/0/steps/{0}/'.format(step.id.hex)\n    resp = self.client.post(path, data={\n        'order': 1,\n        'implementation': 'changes.buildsteps.dummy.DummyBuildStep',\n        'data': '{}',\n        'build.timeout': '1',\n    })\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (data['data'] == '{}')\n    assert (data['order'] == 1)\n    assert (data['implementation'] == 'changes.buildsteps.dummy.DummyBuildStep')\n    assert (data['options'] == {\n        'build.timeout': '1',\n    })\n    db.session.expire(step)\n    step = Step.query.get(step.id)\n    assert (step.data == {\n        \n    })\n    assert (step.order == 1)\n    assert (step.implementation == 'changes.buildsteps.dummy.DummyBuildStep')\n    options = list(ItemOption.query.filter((ItemOption.item_id == step.id)))\n    assert (len(options) == 1)\n    assert (options[0].name == 'build.timeout')\n    assert (options[0].value == '1')\n", "label": 1}
{"function": "\n\ndef __init__(self, im):\n    data = None\n    colortable = None\n    if hasattr(im, 'toUtf8'):\n        im = unicode(im.toUtf8(), 'utf-8')\n    if Image.isStringType(im):\n        im = Image.open(im)\n    if (im.mode == '1'):\n        format = QImage.Format_Mono\n    elif (im.mode == 'L'):\n        format = QImage.Format_Indexed8\n        colortable = []\n        for i in range(256):\n            colortable.append(rgb(i, i, i))\n    elif (im.mode == 'P'):\n        format = QImage.Format_Indexed8\n        colortable = []\n        palette = im.getpalette()\n        for i in range(0, len(palette), 3):\n            colortable.append(rgb(*palette[i:(i + 3)]))\n    elif (im.mode == 'RGB'):\n        data = im.tostring('raw', 'BGRX')\n        format = QImage.Format_RGB32\n    elif (im.mode == 'RGBA'):\n        try:\n            data = im.tostring('raw', 'BGRA')\n        except SystemError:\n            (r, g, b, a) = im.split()\n            im = Image.merge('RGBA', (b, g, r, a))\n        format = QImage.Format_ARGB32\n    else:\n        raise ValueError(('unsupported image mode %r' % im.mode))\n    self.__data = (data or im.tostring())\n    QImage.__init__(self, self.__data, im.size[0], im.size[1], format)\n    if colortable:\n        self.setColorTable(colortable)\n", "label": 1}
{"function": "\n\ndef initialize(self, context):\n    cmd_options = {\n        \n    }\n    if (context.device.get_sdk_version() >= 23):\n        if self.app_names:\n            cmd_options['-a'] = ','.join(self.app_names)\n        if self.buffer_size:\n            cmd_options['-b'] = self.buffer_size\n        if self.use_circular_buffer:\n            cmd_options['-c'] = None\n        if self.kernel_functions:\n            cmd_options['-k'] = ','.join(self.kernel_functions)\n        if self.ignore_signals:\n            cmd_options['-n'] = None\n        opt_string = ''.join(['{} {} '.format(name, (value or '')) for (name, value) in cmd_options.iteritems()])\n        self.start_cmd = 'atrace --async_start {} {}'.format(opt_string, ' '.join(self.categories))\n        self.output_file = os.path.join(self.device.working_directory, 'atrace.txt')\n        self.stop_cmd = 'atrace --async_stop {} > {}'.format(('-z' if self.compress_trace else ''), self.output_file)\n        available_categories = [cat.strip().split(' - ')[0] for cat in context.device.execute('atrace --list_categories').splitlines()]\n        for category in self.categories:\n            if (category not in available_categories):\n                raise ConfigError(\"Unknown category '{}'; Must be one of: {}\".format(category, available_categories))\n    else:\n        raise InstrumentError('Only android devices with an API level >= 23 can use systrace properly')\n", "label": 1}
{"function": "\n\ndef activate(self, test=False):\n    if (self.is_active and (not test)):\n        return True\n    logger.debug('Site activation started')\n    for dashboard in self.dashboards:\n        for report in dashboard.reports:\n            ct = ContentType.objects.get_for_model(report.model)\n            (report.object, created) = Report.objects.get_or_create(key=report.key, contenttype=ct)\n            if created:\n                logger.debug(('Reportobject for report %s created' % report.key))\n    register_settings = list(self.settings.keys())\n    for setting in Configuration.objects.all():\n        key = '.'.join((setting.app_label, setting.field_name))\n        if (key in self.settings):\n            if (not setting.active):\n                setting.active = True\n                setting.save()\n            register_settings.remove(key)\n        elif setting.active:\n            setting.active = False\n            setting.save()\n    if register_settings:\n        logger.debug('Need to register new settings')\n        for setting in register_settings:\n            (app, name) = setting.split('.', 1)\n            Configuration.objects.create(app_label=app, field_name=name)\n            logger.debug(('Registered setting %s' % setting))\n    self.is_active = True\n    logger.debug('Site is now active')\n    return True\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    a = self.msg\n    b = ''\n    if self.http_scheme:\n        b += ('%s://' % self.http_scheme)\n    if self.http_host:\n        b += self.http_host\n    if self.http_port:\n        b += (':%s' % self.http_port)\n    if self.http_path:\n        b += self.http_path\n    if self.http_query:\n        b += ('?%s' % self.http_query)\n    if self.http_status:\n        if b:\n            b = ('%s %s' % (b, self.http_status))\n        else:\n            b = str(self.http_status)\n    if self.http_reason:\n        if b:\n            b = ('%s %s' % (b, self.http_reason))\n        else:\n            b = ('- %s' % self.http_reason)\n    if self.http_device:\n        if b:\n            b = ('%s: device %s' % (b, self.http_device))\n        else:\n            b = ('device %s' % self.http_device)\n    if self.http_response_content:\n        if (len(self.http_response_content) <= 60):\n            b += ('   %s' % self.http_response_content)\n        else:\n            b += ('  [first 60 chars of response] %s' % self.http_response_content[:60])\n    return ((b and ('%s: %s' % (a, b))) or a)\n", "label": 1}
{"function": "\n\ndef __init__(self, parent=None, **traits):\n    super(TableEditorToolbar, self).__init__(**traits)\n    editor = self.editor\n    factory = editor.factory\n    actions = []\n    if (factory.sortable and (not factory.sort_model)):\n        actions.append(self.no_sort)\n    if ((not editor.in_column_mode) and factory.reorderable):\n        actions.append(self.move_up)\n        actions.append(self.move_down)\n    if (editor.in_row_mode and (factory.search is not None)):\n        actions.append(self.search)\n    if factory.editable:\n        if ((factory.row_factory is not None) and (not factory.auto_add)):\n            actions.append(self.add)\n        if ((factory.deletable != False) and (not editor.in_column_mode)):\n            actions.append(self.delete)\n    if factory.configurable:\n        actions.append(self.prefs)\n    if (len(actions) > 0):\n        toolbar = ToolBar(*actions, image_size=(16, 16), show_tool_names=False, show_divider=False)\n        self.control = toolbar.create_tool_bar(parent, self)\n        self.control.SetBackgroundColour(parent.GetBackgroundColour())\n        self.control.SetSize(wx.Size((23 * len(actions)), 16))\n", "label": 1}
{"function": "\n\ndef on_changed(self, which):\n    if (not hasattr(self, '_lpl')):\n        self.add_dterm('_lpl', maximum(multiply(a=multiply()), 0.0))\n    if (not hasattr(self, 'ldn')):\n        self.ldn = LightDotNormal((self.v.r.size / 3))\n    if (not hasattr(self, 'vn')):\n        logger.info('LambertianPointLight using auto-normals. This will be slow for derivative-free computations.')\n        self.vn = VertNormals(f=self.f, v=self.v)\n        self.vn.needs_autoupdate = True\n    if (('v' in which) and hasattr(self.vn, 'needs_autoupdate') and self.vn.needs_autoupdate):\n        self.vn.v = self.v\n    ldn_args = {k: getattr(self, k) for k in which if (k in ('light_pos', 'v', 'vn'))}\n    if (len(ldn_args) > 0):\n        self.ldn.set(**ldn_args)\n        self._lpl.a.a.a = self.ldn.reshape(((- 1), 1))\n    if (('num_verts' in which) or ('light_color' in which)):\n        self._lpl.a.a.b = self.light_color.reshape((1, self.num_channels))\n    if ('vc' in which):\n        self._lpl.a.b = self.vc.reshape(((- 1), self.num_channels))\n", "label": 1}
{"function": "\n\ndef print_help(self):\n    '\\n        Print the help menu.\\n        '\n    print(('\\n  %s %s' % ((self._title or self._name), (self._version or ''))))\n    if self._usage:\n        print(('\\n  %s' % self._usage))\n    else:\n        cmd = self._name\n        if (hasattr(self, '_parent') and isinstance(self._parent, Command)):\n            cmd = ('%s %s' % (self._parent._name, cmd))\n        if self._command_list:\n            usage = ('Usage: %s <command> [option]' % cmd)\n        else:\n            usage = ('Usage: %s [option]' % cmd)\n        pos = ' '.join([('<%s>' % name) for name in self._positional_list])\n        print(('\\n  %s %s' % (usage, pos)))\n    arglen = max((len(o.name) for o in self._option_list))\n    arglen += 2\n    self.print_title('\\n  Options:\\n')\n    for o in self._option_list:\n        print(('    %s %s' % (_pad(o.name, arglen), (o.description or ''))))\n    print('')\n    if self._command_list:\n        self.print_title('  Commands:\\n')\n        for cmd in self._command_list:\n            if isinstance(cmd, Command):\n                name = _pad(cmd._name, arglen)\n                desc = (cmd._description or '')\n                print(('    %s %s' % (_pad(name, arglen), desc)))\n        print('')\n    if self._help_footer:\n        print(self._help_footer)\n        print('')\n    return self\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    fake_author_id = uuid4()\n    project = self.create_project()\n    self.create_build(project)\n    path = '/api/0/authors/{0}/builds/'.format(fake_author_id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 404)\n    data = self.unserialize(resp)\n    assert (len(data) == 0)\n    author = Author(email=self.default_user.email, name='Foo Bar')\n    db.session.add(author)\n    build = self.create_build(project, author=author)\n    path = '/api/0/authors/{0}/builds/'.format(author.id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 401)\n    self.login(self.default_user)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    (username, domain) = self.default_user.email.split('@', 1)\n    author = self.create_author('{}+foo@{}'.format(username, domain))\n    self.create_build(project, author=author)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 2)\n", "label": 1}
{"function": "\n\ndef __unicode__(self):\n    qs = ['SELECT']\n    if self.distinct_fields:\n        if self.count:\n            qs += ['DISTINCT COUNT({0})'.format(', '.join(['\"{0}\"'.format(f) for f in self.distinct_fields]))]\n        else:\n            qs += ['DISTINCT {0}'.format(', '.join(['\"{0}\"'.format(f) for f in self.distinct_fields]))]\n    elif self.count:\n        qs += ['COUNT(*)']\n    else:\n        qs += [(', '.join(['\"{0}\"'.format(f) for f in self.fields]) if self.fields else '*')]\n    qs += ['FROM', self.table]\n    if self.where_clauses:\n        qs += [self._where]\n    if (self.order_by and (not self.count)):\n        qs += ['ORDER BY {0}'.format(', '.join((six.text_type(o) for o in self.order_by)))]\n    if self.limit:\n        qs += ['LIMIT {0}'.format(self.limit)]\n    if self.allow_filtering:\n        qs += ['ALLOW FILTERING']\n    return ' '.join(qs)\n", "label": 1}
{"function": "\n\ndef _get_python_variables(python_exe, variables, imports=['import sys']):\n    'Run a python interpreter and print some variables'\n    program = list(imports)\n    program.append('')\n    for v in variables:\n        program.append(('print(repr(%s))' % v))\n    os_env = dict(os.environ)\n    try:\n        del os_env['MACOSX_DEPLOYMENT_TARGET']\n    except KeyError:\n        pass\n    proc = Utils.pproc.Popen([python_exe, '-c', '\\n'.join(program)], stdout=Utils.pproc.PIPE, env=os_env)\n    output = proc.communicate()[0].split('\\n')\n    if proc.returncode:\n        if Options.options.verbose:\n            warn(('Python program to extract python configuration variables failed:\\n%s' % '\\n'.join([('line %03i: %s' % ((lineno + 1), line)) for (lineno, line) in enumerate(program)])))\n        raise RuntimeError\n    return_values = []\n    for s in output:\n        s = s.strip()\n        if (not s):\n            continue\n        if (s == 'None'):\n            return_values.append(None)\n        elif ((s[0] == \"'\") and (s[(- 1)] == \"'\")):\n            return_values.append(s[1:(- 1)])\n        elif s[0].isdigit():\n            return_values.append(int(s))\n        else:\n            break\n    return return_values\n", "label": 1}
{"function": "\n\ndef _run_server_as_subprocess():\n    args = sys.argv[2:]\n    (options, remainder) = getopt.getopt(args, '', ['debug', 'reload', 'host=', 'port=', 'server=', 'baseurl=', 'runtimepath='])\n    host = 'localhost'\n    port = 8080\n    debug = True\n    reloader = False\n    server = 'kokoro'\n    runtime_path = '.runtime/'\n    base_url = '/'\n    for (opt, arg) in options:\n        if (opt[0:2] == '--'):\n            opt = opt[2:]\n        if (opt == 'debug'):\n            debug = True\n        elif (opt == 'reload'):\n            reloader = True\n        elif (opt == 'host'):\n            host = arg\n        elif (opt == 'port'):\n            port = arg\n        elif (opt == 'server'):\n            server = arg\n        elif (opt == 'runtimepath'):\n            runtime_path = arg\n        elif (opt == 'baseurl'):\n            base_url = arg\n    SCRIPT_PATH = os.path.abspath(__file__)\n    RUN_COMMAND = ('%s %s' % (sys.executable, SCRIPT_PATH))\n    ARGUMENTS = ('run_server_once --host=%s --port=%d --server=%s --baseurl=%s --runtimepath=%s' % (host, port, server, base_url, runtime_path))\n    if reloader:\n        ARGUMENTS += ' --reload'\n    if debug:\n        ARGUMENTS += ' --debug'\n    RUN_COMMAND = ((RUN_COMMAND + ' ') + ARGUMENTS)\n    if hasattr(os, 'setsid'):\n        return subprocess.Popen(RUN_COMMAND, shell=True, preexec_fn=os.setsid)\n    else:\n        return subprocess.Popen(RUN_COMMAND, shell=True)\n", "label": 1}
{"function": "\n\ndef _parse_binary(stream, ptr=0):\n    i = ptr\n    deserialized = {\n        \n    }\n    while (i < len(stream)):\n        c = stream[i]\n        if (c == BIN_END):\n            return (deserialized, i)\n        (nodename, i) = _readtonull(stream, (i + 1))\n        if (c == BIN_NONE):\n            (deserialized[nodename], i) = _parse_binary(stream, (i + 1))\n        elif (c == BIN_STRING):\n            (deserialized[nodename], i) = _readtonull(stream, (i + 1))\n        elif (c == BIN_WIDESTRING):\n            raise Exception('NYI')\n        elif ((c == BIN_INT32) or (c == BIN_COLOR) or (c == BIN_POINTER)):\n            if ((len(stream) - i) < 4):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('i', stream, (i + 1))\n            (deserialized[nodename], i) = (value, (i + 4))\n        elif (c == BIN_UINT64):\n            if ((len(stream) - i) < 8):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('q', stream, (i + 1))\n            (deserialized[nodename], i) = (value, (i + 8))\n        elif (c == BIN_FLOAT32):\n            if ((len(stream) - i) < 4):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('f', stream, (i + 1))\n            (deserialized[nodename], i) = (0, (i + 4))\n        else:\n            raise Exception('Unknown KV type')\n        i += 1\n    return (deserialized, i)\n", "label": 1}
{"function": "\n\n@memoize_generator\ndef process(self, stack, stream):\n    for (token_type, value) in stream:\n        if ((token_type in Name) and (value.upper() == 'INCLUDE')):\n            self.detected = True\n            continue\n        elif self.detected:\n            if (token_type in Whitespace):\n                continue\n            if (token_type in String.Symbol):\n                path = join(self.dirpath, value[1:(- 1)])\n                try:\n                    f = open(path)\n                    raw_sql = f.read()\n                    f.close()\n                except IOError as err:\n                    if self.raiseexceptions:\n                        raise\n                    (yield (Comment, ('-- IOError: %s\\n' % err)))\n                else:\n                    try:\n                        filtr = IncludeStatement(self.dirpath, (self.maxRecursive - 1), self.raiseexceptions)\n                    except ValueError as err:\n                        if self.raiseexceptions:\n                            raise\n                        (yield (Comment, ('-- ValueError: %s\\n' % err)))\n                    stack = FilterStack()\n                    stack.preprocess.append(filtr)\n                    for tv in stack.run(raw_sql):\n                        (yield tv)\n                self.detected = False\n            continue\n        (yield (token_type, value))\n", "label": 1}
{"function": "\n\ndef update(self):\n    'Get the latest state of the sensor.'\n    data = ecobee.NETWORK\n    data.update()\n    for sensor in data.ecobee.get_remote_sensors(self.index):\n        for item in sensor['capability']:\n            if ((item['type'] == self.type) and (self.type == 'temperature') and (self.sensor_name == sensor['name'])):\n                self._state = (float(item['value']) / 10)\n            elif ((item['type'] == self.type) and (self.type == 'humidity') and (self.sensor_name == sensor['name'])):\n                self._state = item['value']\n            elif ((item['type'] == self.type) and (self.type == 'occupancy') and (self.sensor_name == sensor['name'])):\n                self._state = item['value']\n", "label": 1}
{"function": "\n\ndef create_slug(self, model_instance, add):\n    if (not isinstance(self._populate_from, (list, tuple))):\n        self._populate_from = (self._populate_from,)\n    slug_field = model_instance._meta.get_field(self.attname)\n    if (add or self.overwrite):\n        slug_for_field = (lambda field: self.slugify_func(getattr(model_instance, field)))\n        slug = self.separator.join(map(slug_for_field, self._populate_from))\n        next = 2\n    else:\n        slug = getattr(model_instance, self.attname)\n        return slug\n    slug_len = slug_field.max_length\n    if slug_len:\n        slug = slug[:slug_len]\n    slug = self._slug_strip(slug)\n    original_slug = slug\n    if self.allow_duplicates:\n        return slug\n    queryset = self.get_queryset(model_instance.__class__, slug_field)\n    if model_instance.pk:\n        queryset = queryset.exclude(pk=model_instance.pk)\n    kwargs = {\n        \n    }\n    for params in model_instance._meta.unique_together:\n        if (self.attname in params):\n            for param in params:\n                kwargs[param] = getattr(model_instance, param, None)\n    kwargs[self.attname] = slug\n    while ((not slug) or queryset.filter(**kwargs)):\n        slug = original_slug\n        end = ('%s%s' % (self.separator, next))\n        end_len = len(end)\n        if (slug_len and ((len(slug) + end_len) > slug_len)):\n            slug = slug[:(slug_len - end_len)]\n            slug = self._slug_strip(slug)\n        slug = ('%s%s' % (slug, end))\n        kwargs[self.attname] = slug\n        next += 1\n    return slug\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\n    'run controller method - old style'\n    if (not args):\n        args = (arg or '')\n    if dt:\n        if (not dn):\n            dn = dt\n        doc = frappe.get_doc(dt, dn)\n    else:\n        doc = frappe.get_doc(json.loads(docs))\n        doc._original_modified = doc.modified\n        doc.check_if_latest()\n    if (not doc.has_permission('read')):\n        frappe.msgprint(_('Not permitted'), raise_exception=True)\n    if doc:\n        try:\n            args = json.loads(args)\n        except ValueError:\n            args = args\n        (fnargs, varargs, varkw, defaults) = inspect.getargspec(getattr(doc, method))\n        if ((not fnargs) or ((len(fnargs) == 1) and (fnargs[0] == 'self'))):\n            r = doc.run_method(method)\n        elif (('args' in fnargs) or (not isinstance(args, dict))):\n            r = doc.run_method(method, args)\n        else:\n            r = doc.run_method(method, **args)\n        if r:\n            if cint(frappe.form_dict.get('as_csv')):\n                make_csv_output(r, doc.doctype)\n            else:\n                frappe.response['message'] = r\n        frappe.response.docs.append(doc)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_class_or_file_name_ != x.has_class_or_file_name_):\n        return 0\n    if (self.has_class_or_file_name_ and (self.class_or_file_name_ != x.class_or_file_name_)):\n        return 0\n    if (self.has_line_number_ != x.has_line_number_):\n        return 0\n    if (self.has_line_number_ and (self.line_number_ != x.line_number_)):\n        return 0\n    if (self.has_function_name_ != x.has_function_name_):\n        return 0\n    if (self.has_function_name_ and (self.function_name_ != x.function_name_)):\n        return 0\n    if (len(self.variables_) != len(x.variables_)):\n        return 0\n    for (e1, e2) in zip(self.variables_, x.variables_):\n        if (e1 != e2):\n            return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef on_response(self, response):\n    if (not self.first_server_chunk_received):\n        self.first_server_chunk_received = True\n        if ((not self.first_client_chunk_received) and (response.startswith('220 ') or response.startswith('220-'))):\n            self.smtp_detected = True\n    if (not self.smtp_detected):\n        return response\n    if self.ehlo_response_pending:\n        self.ehlo_response_pending = False\n        if (not response.startswith('250-')):\n            return response\n        lines = [l.rstrip() for l in response.splitlines()]\n        starttls_line_index = (- 1)\n        for i in range(len(lines)):\n            line = lines[i]\n            if line[4:].lower().startswith('starttls'):\n                starttls_line_index = i\n                break\n        else:\n            self.smtp_detected = False\n            self.log(logging.DEBUG, 'No STARTTLS in EHLO response')\n            return response\n        if (starttls_line_index == (len(lines) - 1)):\n            lines = lines[:starttls_line_index]\n            lines[(- 1)] = ((lines[(- 1)][0:3] + ' ') + lines[(- 1)][4:])\n        else:\n            lines = (lines[:starttls_line_index] + lines[(starttls_line_index + 1):])\n        response = ('\\r\\n'.join(lines) + '\\r\\n')\n        self.server_starttls_stripped = True\n        self.log(logging.DEBUG, 'Stripped STARTTLS from EHLO response')\n    return response\n", "label": 1}
{"function": "\n\ndef ParseDepends(self, filename, must_exist=None, only_one=0):\n    '\\n        Parse a mkdep-style file for explicit dependencies.  This is\\n        completely abusable, and should be unnecessary in the \"normal\"\\n        case of proper SCons configuration, but it may help make\\n        the transition from a Make hierarchy easier for some people\\n        to swallow.  It can also be genuinely useful when using a tool\\n        that can write a .d file, but for which writing a scanner would\\n        be too complicated.\\n        '\n    filename = self.subst(filename)\n    try:\n        fp = open(filename, 'r')\n    except IOError:\n        if must_exist:\n            raise\n        return\n    lines = SCons.Util.LogicalLines(fp).readlines()\n    lines = [l for l in lines if (l[0] != '#')]\n    tdlist = []\n    for line in lines:\n        try:\n            (target, depends) = line.split(':', 1)\n        except (AttributeError, ValueError):\n            pass\n        else:\n            tdlist.append((target.split(), depends.split()))\n    if only_one:\n        targets = []\n        for td in tdlist:\n            targets.extend(td[0])\n        if (len(targets) > 1):\n            raise SCons.Errors.UserError((\"More than one dependency target found in `%s':  %s\" % (filename, targets)))\n    for (target, depends) in tdlist:\n        self.Depends(target, depends)\n", "label": 1}
{"function": "\n\ndef root(self, request, url):\n    '\\n        DEPRECATED. This function is the old way of handling URL resolution, and\\n        is deprecated in favor of real URL resolution -- see ``get_urls()``.\\n\\n        This function still exists for backwards-compatibility; it will be\\n        removed in Django 1.3.\\n        '\n    import warnings\n    warnings.warn('AdminSite.root() is deprecated; use include(admin.site.urls) instead.', DeprecationWarning)\n    if ((request.method == 'GET') and (not request.path.endswith('/'))):\n        return http.HttpResponseRedirect((request.path + '/'))\n    if settings.DEBUG:\n        self.check_dependencies()\n    self.root_path = re.sub((re.escape(url) + '$'), '', request.path)\n    url = url.rstrip('/')\n    if (url == 'logout'):\n        return self.logout(request)\n    if (not self.has_permission(request)):\n        return self.login(request)\n    if (url == ''):\n        return self.index(request)\n    elif (url == 'password_change'):\n        return self.password_change(request)\n    elif (url == 'password_change/done'):\n        return self.password_change_done(request)\n    elif (url == 'jsi18n'):\n        return self.i18n_javascript(request)\n    elif url.startswith('r/'):\n        from django.contrib.contenttypes.views import shortcut\n        return shortcut(request, *url.split('/')[1:])\n    elif ('/' in url):\n        return self.model_page(request, *url.split('/', 2))\n    else:\n        return self.app_index(request, url)\n    raise http.Http404('The requested admin page does not exist.')\n", "label": 1}
{"function": "\n\ndef _select_range(self, multiselect, keep_anchor, node, idx):\n    'Selects a range between self._anchor and node or idx.\\n        If multiselect is True, it will be added to the selection, otherwise\\n        it will unselect everything before selecting the range. This is only\\n        called if self.multiselect is True.\\n        If keep anchor is False, the anchor is moved to node. This should\\n        always be True for keyboard selection.\\n        '\n    select = self.select_node\n    sister_nodes = self.get_selectable_nodes()\n    end = (len(sister_nodes) - 1)\n    last_node = self._anchor\n    last_idx = self._anchor_idx\n    if (last_node is None):\n        last_idx = end\n        last_node = sister_nodes[end]\n    elif ((last_idx > end) or (sister_nodes[last_idx] != last_node)):\n        try:\n            last_idx = self.get_index_of_node(last_node, sister_nodes)\n        except ValueError:\n            return\n    if ((idx > end) or (sister_nodes[idx] != node)):\n        try:\n            idx = self.get_index_of_node(node, sister_nodes)\n        except ValueError:\n            return\n    if (last_idx > idx):\n        (last_idx, idx) = (idx, last_idx)\n    if (not multiselect):\n        self.clear_selection()\n    for item in sister_nodes[last_idx:(idx + 1)]:\n        select(item)\n    if keep_anchor:\n        self._anchor = last_node\n        self._anchor_idx = last_idx\n    else:\n        self._anchor = node\n        self._anchor_idx = idx\n    self._last_selected_node = node\n    self._last_node_idx = idx\n", "label": 1}
{"function": "\n\ndef build(ctx):\n    if ctx.options.dump_state:\n        ctx.db.dump_database()\n        return 0\n    if ctx.options.delete_function:\n        if (not ctx.db.delete_function(ctx.options.delete_function)):\n            raise fbuild.Error(('function %r not cached' % ctx.options.delete_function))\n        return 0\n    if ctx.options.delete_file:\n        if (not ctx.db.delete_file(ctx.options.delete_file)):\n            raise fbuild.Error(('file %r not cached' % ctx.options.delete_file))\n        return 0\n    targets = (ctx.args or ['build'])\n    if ('install' in targets):\n        if (targets[(- 1)] != 'install'):\n            raise fbuild.Error('install must be last target')\n        if (not (set(targets) - {'configure', 'install'})):\n            targets.insert((targets.index('install') - 1), 'build')\n    for target_name in targets:\n        if (target_name == 'install'):\n            install_files(ctx)\n        else:\n            target = fbuild.target.find(target_name)\n            target.function(ctx)\n    return 0\n", "label": 1}
{"function": "\n\ndef formfield_for_dbfield(self, db_field, request, **kwargs):\n    \"\\n        Hook for specifying the form Field instance for a given database Field\\n        instance.\\n\\n        If kwargs are given, they're passed to the form Field's constructor.\\n        \"\n    if db_field.choices:\n        return self.formfield_for_choice_field(db_field, request, **kwargs)\n    if (db_field.many_to_many or isinstance(db_field, models.ForeignKey)):\n        if (db_field.__class__ in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[db_field.__class__], **kwargs)\n        if isinstance(db_field, models.ForeignKey):\n            formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)\n        elif db_field.many_to_many:\n            formfield = self.formfield_for_manytomany(db_field, request, **kwargs)\n        if (formfield and (db_field.name not in self.raw_id_fields)):\n            related_modeladmin = self.admin_site._registry.get(db_field.remote_field.model)\n            wrapper_kwargs = {\n                \n            }\n            if related_modeladmin:\n                wrapper_kwargs.update(can_add_related=related_modeladmin.has_add_permission(request), can_change_related=related_modeladmin.has_change_permission(request), can_delete_related=related_modeladmin.has_delete_permission(request))\n            formfield.widget = widgets.RelatedFieldWidgetWrapper(formfield.widget, db_field.remote_field, self.admin_site, **wrapper_kwargs)\n        return formfield\n    for klass in db_field.__class__.mro():\n        if (klass in self.formfield_overrides):\n            kwargs = dict(copy.deepcopy(self.formfield_overrides[klass]), **kwargs)\n            return db_field.formfield(**kwargs)\n    return db_field.formfield(**kwargs)\n", "label": 1}
{"function": "\n\ndef _build_http_request(url, method, headers=None, encoding=None, params=empty_params):\n    '\\n    Make an HTTP request and return an HTTP response.\\n    '\n    opts = {\n        'headers': (headers or {\n            \n        }),\n    }\n    if params.query:\n        opts['params'] = params.query\n    if ((params.body is not None) or params.data or params.files):\n        if (encoding == 'application/json'):\n            if (params.body is not None):\n                opts['json'] = params.body\n            else:\n                opts['json'] = params.data\n        elif (encoding == 'multipart/form-data'):\n            opts['data'] = params.data\n            opts['files'] = params.files\n        elif (encoding == 'application/x-www-form-urlencoded'):\n            opts['data'] = params.data\n        elif (encoding == 'application/octet-stream'):\n            opts['data'] = params.body\n            content_type = _get_content_type(params.body)\n            if content_type:\n                opts['headers']['content-type'] = content_type\n    request = requests.Request(method, url, **opts)\n    request = request.prepare()\n    return request\n", "label": 1}
{"function": "\n\ndef _process_dns_floatingip_update_precommit(self, context, floatingip_data):\n    plugin = manager.NeutronManager.get_service_plugins().get(service_constants.L3_ROUTER_NAT)\n    if (not utils.is_extension_supported(plugin, dns.Dns.get_alias())):\n        return\n    if (not self.dns_driver):\n        return\n    dns_data_db = context.session.query(FloatingIPDNS).filter_by(floatingip_id=floatingip_data['id']).one_or_none()\n    if (dns_data_db and dns_data_db['dns_name']):\n        return\n    (current_dns_name, current_dns_domain) = self._get_requested_state_for_external_dns_service_update(context, floatingip_data)\n    if dns_data_db:\n        if ((dns_data_db['published_dns_name'] != current_dns_name) or (dns_data_db['published_dns_domain'] != current_dns_domain)):\n            dns_actions_data = DNSActionsData(previous_dns_name=dns_data_db['published_dns_name'], previous_dns_domain=dns_data_db['published_dns_domain'])\n            if (current_dns_name and current_dns_domain):\n                dns_data_db['published_dns_name'] = current_dns_name\n                dns_data_db['published_dns_domain'] = current_dns_domain\n                dns_actions_data.current_dns_name = current_dns_name\n                dns_actions_data.current_dns_domain = current_dns_domain\n            else:\n                context.session.delete(dns_data_db)\n            return dns_actions_data\n        else:\n            return\n    if (current_dns_name and current_dns_domain):\n        context.session.add(FloatingIPDNS(floatingip_id=floatingip_data['id'], dns_name='', dns_domain='', published_dns_name=current_dns_name, published_dns_domain=current_dns_domain))\n        return DNSActionsData(current_dns_name=current_dns_name, current_dns_domain=current_dns_domain)\n", "label": 1}
{"function": "\n\ndef _generic_factor_list(expr, gens, args, method):\n    'Helper function for :func:`sqf_list` and :func:`factor_list`. '\n    options.allowed_flags(args, ['frac', 'polys'])\n    opt = options.build_options(gens, args)\n    expr = sympify(expr)\n    if (isinstance(expr, Expr) and (not expr.is_Relational)):\n        (numer, denom) = together(expr).as_numer_denom()\n        (cp, fp) = _symbolic_factor_list(numer, opt, method)\n        (cq, fq) = _symbolic_factor_list(denom, opt, method)\n        if (fq and (not opt.frac)):\n            raise PolynomialError(('a polynomial expected, got %s' % expr))\n        _opt = opt.clone(dict(expand=True))\n        for factors in (fp, fq):\n            for (i, (f, k)) in enumerate(factors):\n                if (not f.is_Poly):\n                    (f, _) = _poly_from_expr(f, _opt)\n                    factors[i] = (f, k)\n        fp = _sorted_factors(fp, method)\n        fq = _sorted_factors(fq, method)\n        if (not opt.polys):\n            fp = [(f.as_expr(), k) for (f, k) in fp]\n            fq = [(f.as_expr(), k) for (f, k) in fq]\n        coeff = (cp / cq)\n        if (not opt.frac):\n            return (coeff, fp)\n        else:\n            return (coeff, fp, fq)\n    else:\n        raise PolynomialError(('a polynomial expected, got %s' % expr))\n", "label": 1}
{"function": "\n\n@classmethod\ndef target_info(cls, obj, ansi=False):\n    if isinstance(obj, type):\n        return ''\n    targets = obj.traverse(cls.get_target)\n    (elements, containers) = zip(*targets)\n    element_set = set((el for el in elements if (el is not None)))\n    container_set = set((c for c in containers if (c is not None)))\n    element_info = None\n    if (len(element_set) == 1):\n        element_info = ('Element: %s' % list(element_set)[0])\n    elif (len(element_set) > 1):\n        element_info = ('Elements:\\n   %s' % '\\n   '.join(sorted(element_set)))\n    container_info = None\n    if (len(container_set) == 1):\n        container_info = ('Container: %s' % list(container_set)[0])\n    elif (len(container_set) > 1):\n        container_info = ('Containers:\\n   %s' % '\\n   '.join(sorted(container_set)))\n    heading = cls.heading('Target Specifications', ansi=ansi, char='-')\n    target_header = '\\nTargets in this object available for customization:\\n'\n    if (element_info and container_info):\n        target_info = ('%s\\n\\n%s' % (element_info, container_info))\n    else:\n        target_info = (element_info if element_info else container_info)\n    target_footer = '\\nTo see the options info for one of these target specifications,\\nwhich are of the form {type}[.{group}[.{label}]], do holoviews.help({type}).'\n    return '\\n'.join([heading, target_header, target_info, target_footer])\n", "label": 1}
{"function": "\n\n@property\ndef type(self):\n    path = self.short_path\n    if any((path.startswith(prefix) for prefix in DOCKER_PREFIXES)):\n        return 'docker'\n    elif path.startswith('/lxc/'):\n        return 'lxc'\n    elif path.startswith('/user.slice/'):\n        (_, parent, name) = path.rsplit('/', 2)\n        if parent.endswith('.scope'):\n            if os.path.isdir(('/home/%s/.local/share/lxc/%s' % (self.owner, name))):\n                return 'lxc-user'\n        return 'systemd'\n    elif ((path == '/user.slice') or (path == '/system.slice') or path.startswith('/system.slice/')):\n        return 'systemd'\n    elif (regexp_ovz_container.match(path) and (path != '/0') and HAS_OPENVZ):\n        return 'openvz'\n    else:\n        return '-'\n", "label": 1}
{"function": "\n\ndef __init__(self, parser, path, source_resource=None):\n    self.parser = parser\n    self.path = path\n    self.source_resource = source_resource\n    self.entities = OrderedDict()\n    self.escape_quotes_on = (('mobile/android/base' in path) and (parser is DTDParser))\n    if source_resource:\n        for (key, entity) in source_resource.entities.items():\n            self.entities[key] = copy_source_entity(entity)\n    try:\n        self.structure = parser.get_structure(read_file(path, uncomment_moz_langpack=((parser is IncParser) and (not source_resource))))\n    except IOError:\n        if source_resource:\n            return\n        else:\n            raise\n    comments = []\n    current_order = 0\n    for obj in self.structure:\n        if isinstance(obj, silme.core.entity.Entity):\n            if self.escape_quotes_on:\n                obj.value = self.unescape_quotes(obj.value)\n            entity = SilmeEntity(obj, comments, current_order)\n            self.entities[entity.key] = entity\n            current_order += 1\n            comments = []\n        elif isinstance(obj, silme.core.structure.Comment):\n            for comment in obj:\n                lines = unicode(comment).strip().split('\\n')\n                comments += [line.strip() for line in lines]\n", "label": 1}
{"function": "\n\ndef determine_result(self, returncode, returnsignal, output, isTimeout):\n    output = '\\n'.join(output)\n    status = result.RESULT_UNKNOWN\n    if self.allInText(['FALSE_DEREF'], output):\n        status = result.RESULT_FALSE_DEREF\n    elif self.allInText(['FALSE_FREE'], output):\n        status = result.RESULT_FALSE_FREE\n    elif self.allInText(['FALSE_MEMTRACK'], output):\n        status = result.RESULT_FALSE_MEMTRACK\n    elif self.allInText(['FALSE_OVERFLOW'], output):\n        status = result.RESULT_FALSE_OVERFLOW\n    elif self.allInText(['FALSE'], output):\n        status = result.RESULT_FALSE_REACH\n    elif ('TRUE' in output):\n        status = result.RESULT_TRUE_PROP\n    if (status == result.RESULT_UNKNOWN):\n        if isTimeout:\n            status = 'TIMEOUT'\n        elif output.endswith(('Z3 Error 9', 'Z3 Error 9\\n')):\n            status = 'ERROR (Z3 Error 9)'\n        elif output.endswith(('error', 'error\\n')):\n            status = 'ERROR'\n        elif ('Encountered Z3 conversion error:' in output):\n            status = 'ERROR (Z3 conversion error)'\n    return status\n", "label": 1}
{"function": "\n\ndef process_handler_result(self, result, task=None):\n    '\\n        Process result received from the task handler.\\n\\n        Result could be:\\n        * None\\n        * Task instance\\n        * Data instance.\\n        '\n    if isinstance(result, Task):\n        self.add_task(result)\n    elif isinstance(result, Data):\n        handler = self.find_data_handler(result)\n        try:\n            data_result = handler(**result.storage)\n            if (data_result is None):\n                pass\n            else:\n                for something in data_result:\n                    self.process_handler_result(something, task)\n        except Exception as ex:\n            self.process_handler_error(('data_%s' % result.handler_key), ex, task)\n    elif (result is None):\n        pass\n    elif isinstance(result, Exception):\n        handler = self.find_task_handler(task)\n        handler_name = getattr(handler, '__name__', 'NONE')\n        self.process_handler_error(handler_name, result, task)\n    elif isinstance(result, dict):\n        if (result.get('type') == 'stat'):\n            for (name, count) in result['counters'].items():\n                self.stat.inc(name, count)\n            for (name, items) in result['collections'].items():\n                for item in items:\n                    self.stat.collect(name, item)\n        else:\n            raise SpiderError(('Unknown result type: %s' % result))\n    else:\n        raise SpiderError(('Unknown result type: %s' % result))\n", "label": 1}
{"function": "\n\ndef _load_00(b, classes):\n    identifier = b[0]\n    if isinstance(identifier, str):\n        identifier = ord(identifier)\n    if (identifier == _SPEC):\n        return _load_spec(b)\n    elif (identifier == _INT_32):\n        return _load_int_32(b)\n    elif ((identifier == _INT) or (identifier == _INT_NEG)):\n        return _load_int(b)\n    elif (identifier == _FLOAT):\n        return _load_float(b)\n    elif (identifier == _COMPLEX):\n        return _load_complex(b)\n    elif (identifier == _STR):\n        return _load_str(b)\n    elif (identifier == _BYTES):\n        return _load_bytes(b)\n    elif (identifier == _TUPLE):\n        return _load_tuple(b, classes)\n    elif (identifier == _NAMEDTUPLE):\n        return _load_namedtuple_00(b, classes)\n    elif (identifier == _LIST):\n        return _load_list(b, classes)\n    elif (identifier == _NPARRAY):\n        return _load_np_array(b)\n    elif (identifier == _DICT):\n        return _load_dict(b, classes)\n    elif (identifier == _GETSTATE):\n        return _load_getstate(b, classes)\n    else:\n        raise BFLoadError(\"unknown identifier '{}'\".format(hex(identifier)))\n", "label": 1}
{"function": "\n\ndef visit_delete(self, delete_stmt, **kw):\n    self.stack.append({\n        'correlate_froms': set([delete_stmt.table]),\n        'iswrapper': False,\n        'asfrom_froms': set([delete_stmt.table]),\n    })\n    self.isdelete = True\n    text = 'DELETE '\n    if delete_stmt._prefixes:\n        text += self._generate_prefixes(delete_stmt, delete_stmt._prefixes, **kw)\n    text += 'FROM '\n    table_text = delete_stmt.table._compiler_dispatch(self, asfrom=True, iscrud=True)\n    if delete_stmt._hints:\n        dialect_hints = dict([(table, hint_text) for ((table, dialect), hint_text) in delete_stmt._hints.items() if (dialect in ('*', self.dialect.name))])\n        if (delete_stmt.table in dialect_hints):\n            table_text = self.format_from_hint_text(table_text, delete_stmt.table, dialect_hints[delete_stmt.table], True)\n    else:\n        dialect_hints = None\n    text += table_text\n    if delete_stmt._returning:\n        self.returning = delete_stmt._returning\n        if self.returning_precedes_values:\n            text += (' ' + self.returning_clause(delete_stmt, delete_stmt._returning))\n    if (delete_stmt._whereclause is not None):\n        t = delete_stmt._whereclause._compiler_dispatch(self)\n        if t:\n            text += (' WHERE ' + t)\n    if (self.returning and (not self.returning_precedes_values)):\n        text += (' ' + self.returning_clause(delete_stmt, delete_stmt._returning))\n    self.stack.pop((- 1))\n    return text\n", "label": 1}
{"function": "\n\ndef _create_interfaces(self):\n    interfaces = []\n    type_interfaces = None\n    if isinstance(self.type_definition, RelationshipType):\n        if isinstance(self.entity_tpl, dict):\n            if (self.INTERFACES in self.entity_tpl):\n                type_interfaces = self.entity_tpl[self.INTERFACES]\n            else:\n                for (rel_def, value) in self.entity_tpl.items():\n                    if (rel_def != 'type'):\n                        rel_def = self.entity_tpl.get(rel_def)\n                        rel = None\n                        if isinstance(rel_def, dict):\n                            rel = rel_def.get('relationship')\n                        if rel:\n                            if (self.INTERFACES in rel):\n                                type_interfaces = rel[self.INTERFACES]\n                                break\n    else:\n        type_interfaces = self.type_definition.get_value(self.INTERFACES, self.entity_tpl)\n    if type_interfaces:\n        for (interface_type, value) in type_interfaces.items():\n            for (op, op_def) in value.items():\n                iface = InterfacesDef(self.type_definition, interfacetype=interface_type, node_template=self, name=op, value=op_def)\n                interfaces.append(iface)\n    return interfaces\n", "label": 1}
{"function": "\n\ndef dropMimeData(self, mime_data, action, row, column, parent):\n    ' Reimplemented to allow items to be moved.\\n        '\n    if (action == QtCore.Qt.IgnoreAction):\n        return False\n    data = mime_data.data(tabular_mime_type)\n    if ((not data.isNull()) and (action == QtCore.Qt.MoveAction)):\n        id_and_rows = map(int, str(data).split(' '))\n        table_id = id_and_rows[0]\n        if (table_id == id(self)):\n            current_rows = id_and_rows[1:]\n            self.moveRows(current_rows, parent.row())\n            return True\n    data = PyMimeData.coerce(mime_data).instance()\n    if (data is not None):\n        if (not isinstance(data, list)):\n            data = [data]\n        editor = self._editor\n        object = editor.object\n        name = editor.name\n        adapter = editor.adapter\n        if ((row == (- 1)) and parent.isValid()):\n            row = parent.row()\n        if ((row == (- 1)) and (adapter.len(object, name) == 0)):\n            row = 0\n        if all((adapter.get_can_drop(object, name, row, item) for item in data)):\n            for item in reversed(data):\n                self.dropItem(item, row)\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef show_list(self, connection, app_names=None):\n    '\\n        Shows a list of all migrations on the system, or only those of\\n        some named apps.\\n        '\n    loader = MigrationLoader(connection, ignore_no_migrations=True)\n    graph = loader.graph\n    if app_names:\n        invalid_apps = []\n        for app_name in app_names:\n            if (app_name not in loader.migrated_apps):\n                invalid_apps.append(app_name)\n        if invalid_apps:\n            raise CommandError(('No migrations present for: %s' % ', '.join(invalid_apps)))\n    else:\n        app_names = sorted(loader.migrated_apps)\n    for app_name in app_names:\n        self.stdout.write(app_name, self.style.MIGRATE_LABEL)\n        shown = set()\n        for node in graph.leaf_nodes(app_name):\n            for plan_node in graph.forwards_plan(node):\n                if ((plan_node not in shown) and (plan_node[0] == app_name)):\n                    title = plan_node[1]\n                    if graph.nodes[plan_node].replaces:\n                        title += (' (%s squashed migrations)' % len(graph.nodes[plan_node].replaces))\n                    if (plan_node in loader.applied_migrations):\n                        self.stdout.write((' [X] %s' % title))\n                    else:\n                        self.stdout.write((' [ ] %s' % title))\n                    shown.add(plan_node)\n        if (not shown):\n            self.stdout.write(' (no migrations)', self.style.ERROR)\n", "label": 1}
{"function": "\n\ndef finddirs(pattern, path='.', exclude=None, recursive=True):\n    'Find directories that match *pattern* in *path*'\n    import fnmatch\n    import os\n    if recursive:\n        for (root, dirnames, filenames) in os.walk(path):\n            for pat in _to_list(pattern):\n                for dirname in fnmatch.filter(dirnames, pat):\n                    dirpath = join(abspath(root), dirname)\n                    for excl in _to_list(exclude):\n                        if (excl and fnmatch.fnmatch(dirpath, excl)):\n                            break\n                    else:\n                        (yield dirpath)\n    else:\n        for pat in _to_list(pattern):\n            for dirname in fnmatch.filter(listdirs(path), pat):\n                dirpath = join(abspath(path), dirname)\n                for excl in _to_list(exclude):\n                    if (excl and fnmatch.fnmatch(dirpath, excl)):\n                        break\n                else:\n                    (yield dirpath)\n", "label": 1}
{"function": "\n\ndef compute_scores(self, sequence):\n    num_states = self.get_num_states()\n    length = len(sequence.x)\n    emission_scores = np.zeros([length, num_states])\n    initial_scores = np.zeros(num_states)\n    transition_scores = np.zeros([(length - 1), num_states, num_states])\n    final_scores = np.zeros(num_states)\n    for tag_id in xrange(num_states):\n        initial_features = self.feature_mapper.get_initial_features(sequence, tag_id)\n        score = 0.0\n        for feat_id in initial_features:\n            score += self.parameters[feat_id]\n        initial_scores[tag_id] = score\n    for pos in xrange(length):\n        for tag_id in xrange(num_states):\n            emission_features = self.feature_mapper.get_emission_features(sequence, pos, tag_id)\n            score = 0.0\n            for feat_id in emission_features:\n                score += self.parameters[feat_id]\n            emission_scores[(pos, tag_id)] = score\n        if (pos > 0):\n            for tag_id in xrange(num_states):\n                for prev_tag_id in xrange(num_states):\n                    transition_features = self.feature_mapper.get_transition_features(sequence, pos, tag_id, prev_tag_id)\n                    score = 0.0\n                    for feat_id in transition_features:\n                        score += self.parameters[feat_id]\n                    transition_scores[((pos - 1), tag_id, prev_tag_id)] = score\n    for prev_tag_id in xrange(num_states):\n        final_features = self.feature_mapper.get_final_features(sequence, prev_tag_id)\n        score = 0.0\n        for feat_id in final_features:\n            score += self.parameters[feat_id]\n        final_scores[prev_tag_id] = score\n    return (initial_scores, transition_scores, final_scores, emission_scores)\n", "label": 1}
{"function": "\n\ndef request(self, method, url, data=None, headers=None):\n    if (data and isinstance(data, bytes)):\n        data = data.decode()\n    if (data and (not isinstance(data, basestring))):\n        data = urlencode(data)\n    if (data is not None):\n        data = data.encode('utf8')\n    if (data and (not headers.get('Content-Type', None))):\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    request_headers = self.headers.copy()\n    if headers:\n        for (k, v) in headers.items():\n            if (v is None):\n                del request_headers[k]\n            else:\n                request_headers[k] = v\n    try:\n        func = getattr(requests, method.lower())\n    except AttributeError:\n        raise Exception(\"HTTP method '{}' is not supported\".format(method))\n    response = func(url, data=data, headers=request_headers)\n    if (response.status_code > 399):\n        raise HTTPError(response.status_code, '{}: {}'.format(response.status_code, response.content))\n    return response\n", "label": 1}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (new_r not in self.shape_of):\n        self.init_r(new_r)\n    self.update_shape(new_r, r)\n    for (shpnode, idx) in (r.clients + [(node, i)]):\n        if isinstance(getattr(shpnode, 'op', None), Shape_i):\n            idx = shpnode.op.i\n            repl = self.shape_of[new_r][idx]\n            if (repl.owner is shpnode):\n                continue\n            if (repl.owner and (repl.owner.inputs[0] is shpnode.inputs[0]) and isinstance(repl.owner.op, Shape_i) and (repl.owner.op.i == shpnode.op.i)):\n                continue\n            if (shpnode.outputs[0] in theano.gof.graph.ancestors([repl])):\n                raise InconsistencyError(('This substitution would insert a cycle in the graph:node: %s, i: %i, r: %s, new_r: %s' % (node, i, r, new_r)))\n            self.scheduled[shpnode] = new_r\n    unscheduled = [k for (k, v) in self.scheduled.items() if (v == r)]\n    for k in unscheduled:\n        del self.scheduled[k]\n    for v in self.shape_of_reverse_index.get(r, []):\n        for (ii, svi) in enumerate(self.shape_of.get(v, [])):\n            if (svi == r):\n                self.set_shape_i(v, ii, new_r)\n    self.shape_of_reverse_index[r] = set()\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Starts or resumes the generator, running until it reaches a\\n        yield point that is not ready.\\n        '\n    if (self.running or self.finished):\n        return\n    try:\n        self.running = True\n        while True:\n            future = self.future\n            if (not future.done()):\n                return\n            self.future = None\n            try:\n                orig_stack_contexts = stack_context._state.contexts\n                exc_info = None\n                try:\n                    value = future.result()\n                except Exception:\n                    self.had_exception = True\n                    exc_info = sys.exc_info()\n                if (exc_info is not None):\n                    yielded = self.gen.throw(*exc_info)\n                    exc_info = None\n                else:\n                    yielded = self.gen.send(value)\n                if (stack_context._state.contexts is not orig_stack_contexts):\n                    self.gen.throw(stack_context.StackContextInconsistentError('stack_context inconsistency (probably caused by yield within a \"with StackContext\" block)'))\n            except (StopIteration, Return) as e:\n                self.finished = True\n                self.future = _null_future\n                if (self.pending_callbacks and (not self.had_exception)):\n                    raise LeakedCallbackError(('finished without waiting for callbacks %r' % self.pending_callbacks))\n                self.result_future.set_result(_value_from_stopiteration(e))\n                self.result_future = None\n                self._deactivate_stack_context()\n                return\n            except Exception:\n                self.finished = True\n                self.future = _null_future\n                self.result_future.set_exc_info(sys.exc_info())\n                self.result_future = None\n                self._deactivate_stack_context()\n                return\n            if (not self.handle_yield(yielded)):\n                return\n    finally:\n        self.running = False\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _generate_syntax_file_map():\n    '\\n        Generate a map of all file types to their syntax files.\\n        '\n    syntax_file_map = {\n        \n    }\n    packages_path = sublime.packages_path()\n    packages = [f for f in os.listdir(packages_path) if os.path.isdir(os.path.join(packages_path, f))]\n    for package in packages:\n        package_dir = os.path.join(packages_path, package)\n        syntax_files = [os.path.join(package_dir, f) for f in os.listdir(package_dir) if f.endswith('.tmLanguage')]\n        for syntax_file in syntax_files:\n            try:\n                plist = plistlib.readPlist(syntax_file)\n                if plist:\n                    for file_type in plist['fileTypes']:\n                        syntax_file_map[file_type.lower()] = syntax_file\n            except expat.ExpatError:\n                logger.warn((\"could not parse '%s'\" % syntax_file))\n            except KeyError:\n                pass\n    if hasattr(sublime, 'find_resources'):\n        syntax_files = sublime.find_resources('*.tmLanguage')\n        for syntax_file in syntax_files:\n            try:\n                plist = plistlib.readPlistFromBytes(bytearray(sublime.load_resource(syntax_file), 'utf-8'))\n                if plist:\n                    for file_type in plist['fileTypes']:\n                        syntax_file_map[file_type.lower()] = syntax_file\n            except expat.ExpatError:\n                logger.warn((\"could not parse '%s'\" % syntax_file))\n            except KeyError:\n                pass\n    return syntax_file_map\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw_args):\n    args = [sympify(x) for x in args if x]\n    args = TensAdd._tensAdd_flatten(args)\n    if (not args):\n        return S.Zero\n    if ((len(args) == 1) and (not isinstance(args[0], TensExpr))):\n        return args[0]\n    args = TensAdd._tensAdd_check_automatrix(args)\n    TensAdd._tensAdd_check(args)\n    if ((len(args) == 1) and isinstance(args[0], TensMul)):\n        obj = Basic.__new__(cls, *args, **kw_args)\n        return obj\n    args = [canon_bp(x) for x in args if x]\n    args = [x for x in args if x]\n    if (not args):\n        return S.Zero\n    if (len(args) == 1):\n        return args[0]\n\n    def sort_key(t):\n        x = get_tids(t)\n        return (x.components, x.free, x.dum)\n    args.sort(key=sort_key)\n    args = TensAdd._tensAdd_collect_terms(args)\n    if (not args):\n        return S.Zero\n    if (len(args) == 1):\n        return args[0]\n    obj = Basic.__new__(cls, *args, **kw_args)\n    return obj\n", "label": 1}
{"function": "\n\ndef _process(self):\n    '\\n        Coroutine implementing the key match algorithm. Key strokes are sent\\n        into this generator, and it calls the appropriate handlers.\\n        '\n    buffer = []\n    retry = False\n    while True:\n        if retry:\n            retry = False\n        else:\n            buffer.append((yield))\n        if buffer:\n            is_prefix_of_longer_match = self._is_prefix_of_longer_match(buffer)\n            matches = self._get_matches(buffer)\n            if (matches and matches[(- 1)].eager(self._cli_ref())):\n                is_prefix_of_longer_match = False\n            if ((not is_prefix_of_longer_match) and matches):\n                self._call_handler(matches[(- 1)], key_sequence=buffer)\n                buffer = []\n            elif ((not is_prefix_of_longer_match) and (not matches)):\n                retry = True\n                found = False\n                for i in range(len(buffer), 0, (- 1)):\n                    matches = self._get_matches(buffer[:i])\n                    if matches:\n                        self._call_handler(matches[(- 1)], key_sequence=buffer[:i])\n                        buffer = buffer[i:]\n                        found = True\n                if (not found):\n                    buffer = buffer[1:]\n", "label": 1}
{"function": "\n\ndef run(self):\n    ' Start polling the socket. '\n    self.heartbeat_controller.reset()\n    self._force_recon = False\n    while (not self.stop.is_set()):\n        try:\n            if (not self._check_connection()):\n                continue\n        except ChromecastConnectionError:\n            break\n        (can_read, _, _) = select.select([self.socket], [], [], POLL_TIME)\n        message = data = None\n        if ((self.socket in can_read) and (not self._force_recon)):\n            try:\n                message = self._read_message()\n            except InterruptLoop as exc:\n                if self.stop.is_set():\n                    self.logger.info('Stopped while reading message, disconnecting.')\n                    break\n                else:\n                    self.logger.exception('Interruption caught without being stopped %s', exc)\n                    break\n            except ssl.SSLError as exc:\n                if (exc.errno == ssl.SSL_ERROR_EOF):\n                    if self.stop.is_set():\n                        break\n                raise\n            except socket.error:\n                self._force_recon = True\n                self.logger.info('Error reading from socket.')\n            else:\n                data = _json_from_message(message)\n        if (not message):\n            continue\n        if self.stop.is_set():\n            break\n        self._route_message(message, data)\n        if (REQUEST_ID in data):\n            callback = self._request_callbacks.pop(data[REQUEST_ID], None)\n            if (callback is not None):\n                event = callback['event']\n                callback['response'] = data\n                event.set()\n    self._cleanup()\n", "label": 1}
{"function": "\n\ndef main(prettypath, verify=False):\n    for pins in range(2, 9):\n        for generator in (top_pth_fp, side_pth_fp, top_smd_fp, side_smd_fp):\n            (name, fp) = generator(pins)\n            path = os.path.join(prettypath, (name + '.kicad_mod'))\n            if verify:\n                print('Verifying', path)\n            if os.path.isfile(path):\n                with open(path) as f:\n                    old = f.read()\n                old = [n for n in sexp_parse(old) if (n[0] != 'tedit')]\n                new = [n for n in sexp_parse(fp) if (n[0] != 'tedit')]\n                if (new == old):\n                    continue\n            if verify:\n                return False\n            else:\n                with open(path, 'w') as f:\n                    f.write(fp)\n    if verify:\n        return True\n", "label": 1}
{"function": "\n\ndef test_read_job3():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test.test', 'test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef handle_noargs(self, **options):\n    if ((not options['watch']) and (not options['initial_scan'])):\n        sys.exit('--no-initial-scan option should be used with --watch.')\n    scanned_dirs = get_scanned_dirs()\n    verbosity = int(options['verbosity'])\n    compilers = utils.get_compilers().values()\n    if ((not options['watch']) or options['initial_scan']):\n        for scanned_dir in scanned_dirs:\n            for (dirname, dirnames, filenames) in os.walk(scanned_dir):\n                for filename in filenames:\n                    path = os.path.join(dirname, filename)[len(scanned_dir):]\n                    if path.startswith('/'):\n                        path = path[1:]\n                    for compiler in compilers:\n                        if compiler.is_supported(path):\n                            try:\n                                compiler.handle_changed_file(path, verbosity=options['verbosity'])\n                            except (exceptions.StaticCompilationError, ValueError) as e:\n                                print(e)\n                            break\n    if options['watch']:\n        from static_precompiler.watch import watch_dirs\n        watch_dirs(scanned_dirs, verbosity)\n", "label": 1}
{"function": "\n\ndef acquire(self, timeout=None):\n    timeout = (((timeout is not None) and timeout) or self.timeout)\n    end_time = time.time()\n    if ((timeout is not None) and (timeout > 0)):\n        end_time += timeout\n    while True:\n        try:\n            os.symlink(self.unique_name, self.lock_file)\n        except OSError:\n            if self.i_am_locking():\n                return\n            else:\n                if ((timeout is not None) and (time.time() > end_time)):\n                    if (timeout > 0):\n                        raise LockTimeout(('Timeout waiting to acquire lock for %s' % self.path))\n                    else:\n                        raise AlreadyLocked(('%s is already locked' % self.path))\n                time.sleep(((timeout / 10) if (timeout is not None) else 0.1))\n        else:\n            return\n", "label": 1}
{"function": "\n\ndef _pick_drop_channels(self, idx):\n    from ..io.base import _BaseRaw\n    from ..epochs import _BaseEpochs\n    from ..evoked import Evoked\n    from ..time_frequency import AverageTFR\n    if isinstance(self, (_BaseRaw, _BaseEpochs)):\n        if (not self.preload):\n            raise RuntimeError('If Raw or Epochs, data must be preloaded to drop or pick channels')\n\n    def inst_has(attr):\n        return (getattr(self, attr, None) is not None)\n    if inst_has('picks'):\n        self.picks = self.picks[idx]\n    if inst_has('_cals'):\n        self._cals = self._cals[idx]\n    pick_info(self.info, idx, copy=False)\n    if inst_has('_projector'):\n        self._projector = self._projector[idx][:, idx]\n    if (isinstance(self, _BaseRaw) and inst_has('_data')):\n        self._data = self._data.take(idx, axis=0)\n    elif (isinstance(self, _BaseEpochs) and inst_has('_data')):\n        self._data = self._data.take(idx, axis=1)\n    elif (isinstance(self, AverageTFR) and inst_has('data')):\n        self.data = self.data.take(idx, axis=0)\n    elif isinstance(self, Evoked):\n        self.data = self.data.take(idx, axis=0)\n", "label": 1}
{"function": "\n\n@local_optimizer([GpuFromHost, GpuToGpu, host_from_gpu])\ndef local_cut_gpu_transfers(node):\n    if (isinstance(node.op, GpuFromHost) and node.inputs[0].owner and isinstance(node.inputs[0].owner.op, HostFromGpu)):\n        other = node.inputs[0].owner.inputs[0]\n        if (node.op.context_name == other.type.context_name):\n            return [other]\n        else:\n            return [GpuToGpu(node.op.context_name)(other)]\n    elif (isinstance(node.op, HostFromGpu) and node.inputs[0].owner):\n        n2 = node.inputs[0].owner\n        if isinstance(n2.op, GpuFromHost):\n            return [n2.inputs[0]]\n        if isinstance(n2.op, GpuToGpu):\n            return [host_from_gpu(n2.inputs[0])]\n    elif isinstance(node.op, GpuToGpu):\n        if (node.inputs[0].type.context_name == node.op.context_name):\n            return [node.inputs[0]]\n        if node.inputs[0].owner:\n            n2 = node.inputs[0].owner\n            if isinstance(n2.op, GpuFromHost):\n                return [GpuFromHost(node.op.context_name)(n2.inputs[0])]\n            if isinstance(n2.op, GpuToGpu):\n                if (node.op.context_name == n2.inputs[0].type.context_name):\n                    return [n2.inputs[0]]\n                else:\n                    return [node.op(n2.inputs[0])]\n", "label": 1}
{"function": "\n\n@classmethod\ndef _resolve_conflict(cls, existing, proposed):\n    if (existing.rev is None):\n        return proposed\n    if (proposed.rev is None):\n        return existing\n    if (proposed == existing):\n        if proposed.force:\n            return proposed\n        return existing\n    elif (existing.force and proposed.force):\n        raise cls.IvyResolveConflictingDepsError('Cannot force {}#{};{} to both rev {} and {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n    elif existing.force:\n        logger.debug('Ignoring rev {} for {}#{};{} already forced to {}'.format(proposed.rev, proposed.org, proposed.name, (proposed.classifier or ''), existing.rev))\n        return existing\n    elif proposed.force:\n        logger.debug('Forcing {}#{};{} from {} to {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n        return proposed\n    elif (Revision.lenient(proposed.rev) > Revision.lenient(existing.rev)):\n        logger.debug('Upgrading {}#{};{} from rev {}  to {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n        return proposed\n    else:\n        return existing\n", "label": 1}
{"function": "\n\ndef _read_data(f, dtype):\n    'Read a variable with a specified data type'\n    if (dtype == 1):\n        if (_read_int32(f) != 1):\n            raise Exception('Error occurred while reading byte variable')\n        return _read_byte(f)\n    elif (dtype == 2):\n        return _read_int16(f)\n    elif (dtype == 3):\n        return _read_int32(f)\n    elif (dtype == 4):\n        return _read_float32(f)\n    elif (dtype == 5):\n        return _read_float64(f)\n    elif (dtype == 6):\n        real = _read_float32(f)\n        imag = _read_float32(f)\n        return np.complex64((real + (imag * 1j)))\n    elif (dtype == 7):\n        return _read_string_data(f)\n    elif (dtype == 8):\n        raise Exception('Should not be here - please report this')\n    elif (dtype == 9):\n        real = _read_float64(f)\n        imag = _read_float64(f)\n        return np.complex128((real + (imag * 1j)))\n    elif (dtype == 10):\n        return Pointer(_read_int32(f))\n    elif (dtype == 11):\n        return ObjectPointer(_read_int32(f))\n    elif (dtype == 12):\n        return _read_uint16(f)\n    elif (dtype == 13):\n        return _read_uint32(f)\n    elif (dtype == 14):\n        return _read_int64(f)\n    elif (dtype == 15):\n        return _read_uint64(f)\n    else:\n        raise Exception(('Unknown IDL type: %i - please report this' % dtype))\n", "label": 1}
{"function": "\n\ndef getEncodableAttributes(self, obj, codec=None):\n    attrs = pyamf.ClassAlias.getEncodableAttributes(self, obj, codec=codec)\n    gae_objects = (getGAEObjects(codec.context) if codec else None)\n    if (self.reference_properties and gae_objects):\n        for (name, prop) in self.reference_properties.iteritems():\n            klass = prop.reference_class\n            key = prop.get_value_for_datastore(obj)\n            if (not key):\n                continue\n            try:\n                attrs[name] = gae_objects.getClassKey(klass, key)\n            except KeyError:\n                ref_obj = getattr(obj, name)\n                gae_objects.addClassKey(klass, key, ref_obj)\n                attrs[name] = ref_obj\n    for k in attrs.keys()[:]:\n        if k.startswith('_'):\n            del attrs[k]\n    for attr in obj.dynamic_properties():\n        attrs[attr] = getattr(obj, attr)\n    if (not self.no_key_attr):\n        attrs[self.KEY_ATTR] = (str(obj.key()) if obj.is_saved() else None)\n    return attrs\n", "label": 1}
{"function": "\n\ndef tabulate(self, request_stats):\n    'Print review request summary and status in a table.\\n\\n        Args:\\n            request_stats (dict):\\n                A dict that contains statistics about each review request.\\n        '\n    if len(request_stats):\n        has_branches = False\n        has_bookmarks = False\n        table = tt.Texttable(get_terminal_size().columns)\n        header = ['Status', 'Review Request']\n        for request in request_stats:\n            if ('branch' in request):\n                has_branches = True\n            if ('bookmark' in request):\n                has_bookmarks = True\n        if has_branches:\n            header.append('Branch')\n        if has_bookmarks:\n            header.append('Bookmark')\n        table.header(header)\n        for request in request_stats:\n            row = [request['status'], request['summary']]\n            if has_branches:\n                row.append((request.get('branch') or ''))\n            if has_bookmarks:\n                row.append((request.get('bookmark') or ''))\n            table.add_row(row)\n        print(table.draw())\n    else:\n        print('No review requests found.')\n    print()\n", "label": 1}
{"function": "\n\ndef to_field_allowed(self, request, to_field):\n    '\\n        Returns True if the model associated with this admin should be\\n        allowed to be referenced by the specified field.\\n        '\n    opts = self.model._meta\n    try:\n        field = opts.get_field(to_field)\n    except FieldDoesNotExist:\n        return False\n    if field.primary_key:\n        return True\n    for many_to_many in opts.many_to_many:\n        if (many_to_many.m2m_target_field_name() == to_field):\n            return True\n    registered_models = set()\n    for (model, admin) in self.admin_site._registry.items():\n        registered_models.add(model)\n        for inline in admin.inlines:\n            registered_models.add(inline.model)\n    related_objects = (f for f in opts.get_fields(include_hidden=True) if (f.auto_created and (not f.concrete)))\n    for related_object in related_objects:\n        related_model = related_object.related_model\n        remote_field = related_object.field.remote_field\n        if (any((issubclass(model, related_model) for model in registered_models)) and hasattr(remote_field, 'get_related_field') and (remote_field.get_related_field() == field)):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    cleaver = Cleaver(environ, self._identity, self._backend, count_humans_only=self.count_humans_only)\n    environ[self.environ_key] = cleaver\n    if self.allow_override:\n        self._handle_variant_overrides(environ)\n    if (self.count_humans_only and (environ.get('REQUEST_METHOD', '') == 'POST') and (self.human_callback_token in environ.get('PATH_INFO', ''))):\n        (fp, length) = SplitMiddleware._copy_body_to_tempfile(environ)\n        environ.setdefault('CONTENT_LENGTH', length)\n        fs = cgi.FieldStorage(fp=fp, environ=environ, keep_blank_values=True)\n        try:\n            try:\n                x = int(fs.getlist('x')[0])\n            except (IndexError, ValueError):\n                x = 0\n            try:\n                y = int(fs.getlist('y')[0])\n            except (IndexError, ValueError):\n                y = 0\n            try:\n                z = int(fs.getlist('z')[0])\n            except (IndexError, ValueError):\n                z = 0\n            if (x and y and z and ((x + y) == z)):\n                self._backend.mark_human(cleaver.identity)\n                for e in self._backend.all_experiments():\n                    variant = self._backend.get_variant(cleaver.identity, e.name)\n                    if variant:\n                        self._backend.mark_participant(e.name, variant)\n                start_response('204 No Content', [('Content-Type', 'text/plain')])\n                return []\n        except (KeyError, ValueError):\n            pass\n        start_response('401 Unauthorized', [('Content-Type', 'text/plain')])\n        return []\n    return self.app(environ, start_response)\n", "label": 1}
{"function": "\n\ndef __init__(self, host='localhost', port=9200, http_auth=None, use_ssl=False, verify_certs=False, ca_certs=None, client_cert=None, client_key=None, **kwargs):\n    if (not REQUESTS_AVAILABLE):\n        raise ImproperlyConfigured('Please install requests to use RequestsHttpConnection.')\n    super(RequestsHttpConnection, self).__init__(host=host, port=port, **kwargs)\n    self.session = requests.session()\n    if (http_auth is not None):\n        if isinstance(http_auth, (tuple, list)):\n            http_auth = tuple(http_auth)\n        elif isinstance(http_auth, string_types):\n            http_auth = tuple(http_auth.split(':', 1))\n        self.session.auth = http_auth\n    self.base_url = ('http%s://%s:%d%s' % (('s' if use_ssl else ''), host, port, self.url_prefix))\n    self.session.verify = verify_certs\n    if (not client_key):\n        self.session.cert = client_cert\n    elif client_cert:\n        self.session.cert = (client_cert, client_key)\n    if ca_certs:\n        if (not verify_certs):\n            raise ImproperlyConfigured('You cannot pass CA certificates when verify SSL is off.')\n        self.session.verify = ca_certs\n    if (use_ssl and (not verify_certs)):\n        warnings.warn(('Connecting to %s using SSL with verify_certs=False is insecure.' % self.base_url))\n", "label": 1}
{"function": "\n\ndef confirmation(self, client, pdu):\n    if _debug:\n        UDPMultiplexer._debug('confirmation %r %r', client, pdu)\n    if (pdu.pduSource == self.addrTuple):\n        if _debug:\n            UDPMultiplexer._debug('    - from us!')\n        return\n    src = Address(pdu.pduSource)\n    if (client is self.direct):\n        dest = self.address\n    elif (client is self.broadcast):\n        dest = LocalBroadcast()\n    else:\n        raise RuntimeError('confirmation mismatch')\n    if (not pdu.pduData):\n        if _debug:\n            UDPMultiplexer._debug('    - no data')\n        return\n    msg_type = struct.unpack('B', pdu.pduData[:1])[0]\n    if _debug:\n        UDPMultiplexer._debug('    - msg_type: %r', msg_type)\n    if (msg_type == 1):\n        if self.annexH.serverPeer:\n            self.annexH.response(PDU(pdu, source=src, destination=dest))\n    elif (msg_type == 129):\n        if self.annexJ.serverPeer:\n            self.annexJ.response(PDU(pdu, source=src, destination=dest))\n    else:\n        UDPMultiplexer._warning('unsupported message')\n", "label": 1}
{"function": "\n\ndef test_m2o_lazy_loader_on_persistent(self):\n    'Compare the behaviors from the lazyloader using\\n        the \"committed\" state in all cases, vs. the lazyloader\\n        using the \"current\" state in all cases except during flush.\\n\\n        '\n    for loadfk in (True, False):\n        for loadrel in (True, False):\n            for autoflush in (True, False):\n                for manualflush in (True, False):\n                    for fake_autoexpire in (True, False):\n                        sess.autoflush = autoflush\n                        if loadfk:\n                            c1.parent_id\n                        if loadrel:\n                            c1.parent\n                        c1.parent_id = p2.id\n                        if manualflush:\n                            sess.flush()\n                        if fake_autoexpire:\n                            sess.expire(c1, ['parent'])\n                        if (loadrel and (not fake_autoexpire)):\n                            assert (c1.parent is p1)\n                        else:\n                            assert (c1.parent is p2)\n                        sess.rollback()\n", "label": 1}
{"function": "\n\ndef _get_svn_url_rev(self, location):\n    from pip.exceptions import InstallationError\n    with open(os.path.join(location, self.dirname, 'entries')) as f:\n        data = f.read()\n    if (data.startswith('8') or data.startswith('9') or data.startswith('10')):\n        data = list(map(str.splitlines, data.split('\\n\\x0c\\n')))\n        del data[0][0]\n        url = data[0][3]\n        revs = ([int(d[9]) for d in data if ((len(d) > 9) and d[9])] + [0])\n    elif data.startswith('<?xml'):\n        match = _svn_xml_url_re.search(data)\n        if (not match):\n            raise ValueError(('Badly formatted data: %r' % data))\n        url = match.group(1)\n        revs = ([int(m.group(1)) for m in _svn_rev_re.finditer(data)] + [0])\n    else:\n        try:\n            xml = self.run_command(['info', '--xml', location], show_stdout=False)\n            url = _svn_info_xml_url_re.search(xml).group(1)\n            revs = [int(m.group(1)) for m in _svn_info_xml_rev_re.finditer(xml)]\n        except InstallationError:\n            (url, revs) = (None, [])\n    if revs:\n        rev = max(revs)\n    else:\n        rev = 0\n    return (url, rev)\n", "label": 1}
{"function": "\n\ndef test_proxy_snake_dict():\n    my_data = {\n        'one': 1,\n        'two': 2,\n        'none': None,\n        'threeOrFor': 3,\n        'inside': {\n            'otherCamelCase': 3,\n        },\n    }\n    p = ProxySnakeDict(my_data)\n    assert ('one' in p)\n    assert ('two' in p)\n    assert ('threeOrFor' in p)\n    assert ('none' in p)\n    assert (len(p) == len(my_data))\n    assert (p['none'] is None)\n    assert (p.get('none') is None)\n    assert (p.get('none_existent') is None)\n    assert ('three_or_for' in p)\n    assert (p.get('three_or_for') == 3)\n    assert ('inside' in p)\n    assert ('other_camel_case' in p['inside'])\n    assert (sorted(p.items()) == sorted(list([('inside', ProxySnakeDict({\n        'other_camel_case': 3,\n    })), ('none', None), ('three_or_for', 3), ('two', 2), ('one', 1)])))\n", "label": 1}
{"function": "\n\n@gof.local_optimizer([T.pow])\ndef local_pow_specialize(node):\n    if (node.op == T.pow):\n        odtype = node.outputs[0].dtype\n        xsym = node.inputs[0]\n        ysym = node.inputs[1]\n        y = local_mul_canonizer.get_constant(ysym)\n        if ((y is not None) and encompasses_broadcastable(xsym.type.broadcastable, ysym.type.broadcastable)):\n            rval = None\n            if N.all((y == 2)):\n                rval = [T.sqr(xsym)]\n            if N.all((y == 1)):\n                rval = [xsym]\n            if N.all((y == 0)):\n                rval = [T.fill(xsym, numpy.asarray(1, dtype=odtype))]\n            if N.all((y == 0.5)):\n                rval = [T.sqrt(xsym)]\n            if N.all((y == (- 0.5))):\n                rval = [T.inv(T.sqrt(xsym))]\n            if N.all((y == (- 1))):\n                rval = [T.inv(xsym)]\n            if N.all((y == (- 2))):\n                rval = [T.inv(T.sqr(xsym))]\n            if rval:\n                rval[0] = T.cast(rval[0], odtype)\n                assert (rval[0].type == node.outputs[0].type), (rval, node.outputs)\n                return rval\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.username = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.Pass = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.Remember = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef mpf_atan2(y, x, prec, rnd=round_fast):\n    (xsign, xman, xexp, xbc) = x\n    (ysign, yman, yexp, ybc) = y\n    if (not yman):\n        if ((y == fzero) and (x != fnan)):\n            if (mpf_sign(x) >= 0):\n                return fzero\n            return mpf_pi(prec, rnd)\n        if (y in (finf, fninf)):\n            if (x in (finf, fninf)):\n                return fnan\n            if (y == finf):\n                return mpf_shift(mpf_pi(prec, rnd), (- 1))\n            return mpf_neg(mpf_shift(mpf_pi(prec, negative_rnd[rnd]), (- 1)))\n        return fnan\n    if ysign:\n        return mpf_neg(mpf_atan2(mpf_neg(y), x, prec, negative_rnd[rnd]))\n    if (not xman):\n        if (x == fnan):\n            return fnan\n        if (x == finf):\n            return fzero\n        if (x == fninf):\n            return mpf_pi(prec, rnd)\n        if (y == fzero):\n            return fzero\n        return mpf_shift(mpf_pi(prec, rnd), (- 1))\n    tquo = mpf_atan(mpf_div(y, x, (prec + 4)), (prec + 4))\n    if xsign:\n        return mpf_add(mpf_pi((prec + 4)), tquo, prec, rnd)\n    else:\n        return mpf_pos(tquo, prec, rnd)\n", "label": 1}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_time():\n    ' reading/writing of 3D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_3d', 'time_3d.sec'))\n    assert (data.shape == (128, 88, 1250))\n    assert (np.abs((data[(0, 1, 2)].real - 7.98)) <= 0.01)\n    assert (np.abs((data[(0, 1, 2)].imag - 33.82)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].real - (- 9.36))) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].imag - (- 7.75))) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef process_response(self, request, response):\n    if (not self.is_cms_request(request)):\n        return response\n    from django.utils.cache import add_never_cache_headers\n    if ((hasattr(request, 'toolbar') and request.toolbar.edit_mode) or (not all((ph.cache_placeholder for (ph, __) in getattr(request, 'placeholders', {\n        \n    }).values())))):\n        add_never_cache_headers(response)\n    if (hasattr(request, 'user') and request.user.is_staff and (response.status_code != 500)):\n        try:\n            pk = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE)).only('pk').order_by('-pk')[0].pk\n            if (hasattr(request, 'cms_latest_entry') and (request.cms_latest_entry != pk)):\n                log = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE))[0]\n                request.session['cms_log_latest'] = log.pk\n        except IndexError:\n            pass\n    return response\n", "label": 1}
{"function": "\n\ndef __restore__(self):\n    super(Turtle, self).__restore__()\n    try:\n        [panda.setdefault(self.CREATOR, self.creator) for panda in self.pandas]\n        [panda.setdefault(self.NAME, self.name) for panda in self.pandas]\n        self.tigress.setdefault(self.CREATOR, self.creator)\n        self.tigress.setdefault(self.NAME, self.name)\n    except:\n        pass\n    self.pandas = crane.pandaStore.load_or_create_all(self.pandas)\n    self.tigress = crane.tigressStore.load_or_create(self.tigress)\n    self.pandaUids = set((p.uid for p in self.pandas))\n    self.invertedMapping = {tuple(v): k for (k, v) in self.mapping.iteritems()}\n    self.followers = set(self.followers)\n    if (self.FREQUIRES_UIDS in self.requires):\n        uids = self.requires[self.FREQUIRES_UIDS]\n        if isinstance(uids, basestring):\n            uids = eval(uids)\n        [panda.add_features(uids) for panda in self.pandas]\n    elif (self.FREQUIRES_TURTLES in self.requires):\n        turtles = self.store.load_or_create_all([{\n            'name': t,\n            'creator': self.creator,\n        } for t in self.requires[self.FREQUIRES_TURTLES]])\n        [panda.add_features(turtle.get_panda_uids()) for turtle in turtles for panda in self.pandas]\n    elif self.requires:\n        logger.error('dependent features are either in {0} or {1}, but not in {2}'.format(self.FREQUIRES_UIDS, self.FREQUIRES_TURTLES, self.requires))\n", "label": 1}
{"function": "\n\ndef run(self):\n    res = None\n    os.system('clear')\n    while True:\n        try:\n            if (not self.server_version):\n                self.server_version = self.client.perform_request('version')\n                if (self.server_version.capabilities and ('async' in self.server_version.capabilities)):\n                    self.block = False\n                elif self.supports_blocking:\n                    self.block = True\n                else:\n                    raise BlockingNotSupportedError('Debugger requires blocking mode')\n            self.render()\n            if (not self.block):\n                done = False\n                while (not done):\n                    res = self.client.perform_request('version', block=True)\n                    if res.is_success:\n                        done = True\n        except ConnectionError as e:\n            try:\n                msg = e.message.args[1].strerror\n            except:\n                try:\n                    msg = e.message.args[0]\n                except:\n                    msg = str(e)\n            traceback.print_exc()\n            self.do_render(error='Error: {}'.format(msg))\n            self.server_version = None\n            time.sleep(1)\n", "label": 1}
{"function": "\n\ndef get_managed_entity(content, vimtype, moid=None, name=None):\n    if ((not name) and (not moid)):\n        return\n    container = content.viewManager.CreateContainerView(content.rootFolder, [vimtype], True)\n    count = 0\n    for entity in container.view:\n        if (moid and (entity._moId == moid)):\n            results = entity\n            count += 1\n        elif (name and (entity.name == name)):\n            results = entity\n            count += 1\n        if (count >= 2):\n            raise Exception('Multiple Managed Objects found,                            Check Names or IDs provided are unique')\n        elif (count == 1):\n            return results\n    if name:\n        raise Exception(('Inventory Error: Unable to Find Object (%s): %s' % (vimtype, name)))\n    elif moid:\n        raise Exception(('Inventory Error: Unable to Find Object (%s): %s' % (vimtype, moid)))\n    else:\n        raise Exception(('Inventory Error: No Name or moid provided (%s)' % vimtype))\n", "label": 1}
{"function": "\n\ndef create_decl(self, sigtypes, name, width=None, length=None, lineno=0):\n    self.typecheck_decl(sigtypes, length)\n    decls = []\n    signed = False\n    if ('signed' in sigtypes):\n        signed = True\n    if ('input' in sigtypes):\n        decls.append(Input(name=name, width=width, signed=signed, lineno=lineno))\n    if ('output' in sigtypes):\n        decls.append(Output(name=name, width=width, signed=signed, lineno=lineno))\n    if ('inout' in sigtypes):\n        decls.append(Inout(name=name, width=width, signed=signed, lineno=lineno))\n    if ('wire' in sigtypes):\n        if length:\n            decls.append(WireArray(name=name, width=width, signed=signed, length=length, lineno=lineno))\n        else:\n            decls.append(Wire(name=name, width=width, signed=signed, lineno=lineno))\n    if ('reg' in sigtypes):\n        if length:\n            decls.append(RegArray(name=name, width=width, signed=signed, length=length, lineno=lineno))\n        else:\n            decls.append(Reg(name=name, width=width, signed=signed, lineno=lineno))\n    if ('tri' in sigtypes):\n        decls.append(Tri(name=name, width=width, signed=signed, lineno=lineno))\n    if ('supply0' in sigtypes):\n        decls.append(Supply(name=name, value=IntConst('0', lineno=lineno), width=width, signed=signed, lineno=lineno))\n    if ('supply1' in sigtypes):\n        decls.append(Supply(name=name, value=IntConst('1', lineno=lineno), width=width, signed=signed, lineno=lineno))\n    return decls\n", "label": 1}
{"function": "\n\ndef test_has_multiple():\n    f = (((x ** 2) * y) + sin(((2 ** t) + log(z))))\n    assert f.has(x)\n    assert f.has(y)\n    assert f.has(z)\n    assert f.has(t)\n    assert (not f.has(u))\n    assert f.has(x, y, z, t)\n    assert f.has(x, y, z, t, u)\n    i = Integer(4400)\n    assert (not i.has(x))\n    assert (i * (x ** i)).has(x)\n    assert (not (i * (y ** i)).has(x))\n    assert (i * (y ** i)).has(x, y)\n    assert (not (i * (y ** i)).has(x, z))\n", "label": 1}
{"function": "\n\ndef files(self, itemId, sources=None):\n    ret = {\n        'added': [],\n        'removed': [],\n    }\n    files = self.get('item/{}/files'.format(itemId))\n    if (self.module.params['state'] == 'present'):\n        file_dict = {f['name']: f for f in files}\n        source_dict = {os.path.basename(s): {\n            'path': s,\n            'name': os.path.basename(s),\n            'size': os.path.getsize(s),\n        } for s in sources}\n        source_names = set([(s['name'], s['size']) for s in source_dict.values()])\n        file_names = set([(f['name'], f['size']) for f in file_dict.values()])\n        for (n, _) in (file_names - source_names):\n            self.delete('file/{}'.format(file_dict[n]['_id']))\n            ret['removed'].append(file_dict[n])\n        for (n, _) in (source_names - file_names):\n            self.uploadFileToItem(itemId, source_dict[n]['path'])\n            ret['added'].append(source_dict[n])\n    elif (self.module.params['state'] == 'absent'):\n        for f in files:\n            self.delete('file/{}'.format(f['_id']))\n            ret['removed'].append(f)\n    if ((len(ret['added']) != 0) or (len(ret['removed']) != 0)):\n        self.changed = True\n    return ret\n", "label": 1}
{"function": "\n\ndef run(self, cmd, code):\n    'Run the module checker or executable on code and return the output.'\n    if (self.module is not None):\n        use_module = False\n        if (not self.check_version):\n            use_module = True\n        else:\n            settings = self.get_view_settings()\n            version = settings.get('@python')\n            if (version is None):\n                use_module = ((cmd is None) or (cmd[0] == '<builtin>'))\n            else:\n                version = util.find_python(version=version, module=self.module)\n                use_module = (version[0] == '<builtin>')\n        if use_module:\n            if persist.debug_mode():\n                persist.printf('{}: {} <builtin>'.format(self.name, os.path.basename((self.filename or '<unsaved>'))))\n            try:\n                errors = self.check(code, os.path.basename((self.filename or '<unsaved>')))\n            except Exception as err:\n                persist.printf('ERROR: exception in {}.check: {}'.format(self.name, str(err)))\n                errors = ''\n            if isinstance(errors, (tuple, list)):\n                return '\\n'.join([str(e) for e in errors])\n            else:\n                return errors\n        else:\n            cmd = self._cmd\n    else:\n        cmd = (self.cmd or self._cmd)\n    cmd = self.build_cmd(cmd=cmd)\n    if cmd:\n        return super().run(cmd, code)\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef test_file():\n    nmeafile = pynmea2.NMEAFile(StringIO(TEST_DATA))\n    nmea_strings = nmeafile.read()\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    del nmeafile\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [_f.readline() for i in range(10)]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [s for s in _f]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [_f.next() for i in range(10)]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n", "label": 1}
{"function": "\n\ndef has_changes(self):\n    if self.is_dirty:\n        return True\n    for child in self._db_connections:\n        if child.has_changes():\n            return True\n    for child in self._db_annotations:\n        if child.has_changes():\n            return True\n    for child in self._db_abstractions:\n        if child.has_changes():\n            return True\n    for child in self._db_others:\n        if child.has_changes():\n            return True\n    for child in self._db_modules:\n        if child.has_changes():\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef test_unitroots():\n    assert (unitroots(1) == [1])\n    assert (unitroots(2) == [1, (- 1)])\n    (a, b, c) = unitroots(3)\n    assert (a == 1)\n    assert b.ae(((- 0.5) + 0.8660254037844386j))\n    assert c.ae(((- 0.5) - 0.8660254037844386j))\n    assert (unitroots(1, primitive=True) == [1])\n    assert (unitroots(2, primitive=True) == [(- 1)])\n    assert (unitroots(3, primitive=True) == unitroots(3)[1:])\n    assert (unitroots(4, primitive=True) == [j, (- j)])\n    assert (len(unitroots(17, primitive=True)) == 16)\n    assert (len(unitroots(16, primitive=True)) == 8)\n", "label": 1}
{"function": "\n\ndef __new__(cls, clsname, bases, methods):\n    attrs = []\n    nested = []\n    elements = []\n    namespaced = []\n    for (k, v) in methods.items():\n        if isinstance(v, Descriptor):\n            ns = getattr(v, 'namespace', None)\n            if ns:\n                namespaced.append((k, ('{%s}%s' % (ns, k))))\n            if getattr(v, 'nested', False):\n                nested.append(k)\n                elements.append(k)\n            elif isinstance(v, Sequence):\n                elements.append(k)\n            elif isinstance(v, Typed):\n                if hasattr(v.expected_type, 'to_tree'):\n                    elements.append(k)\n                else:\n                    attrs.append(k)\n            elif (not isinstance(v, Alias)):\n                attrs.append(k)\n    if (methods.get('__attrs__') is None):\n        methods['__attrs__'] = tuple(attrs)\n    methods['__namespaced__'] = tuple(namespaced)\n    if (methods.get('__nested__') is None):\n        methods['__nested__'] = tuple(sorted(nested))\n    if (methods.get('__elements__') is None):\n        methods['__elements__'] = tuple(sorted(elements))\n    return MetaStrict.__new__(cls, clsname, bases, methods)\n", "label": 1}
{"function": "\n\ndef visit_bindparam(self, bindparam, within_columns_clause=False, literal_binds=False, skip_bind_expression=False, **kwargs):\n    if ((not skip_bind_expression) and bindparam.type._has_bind_expression):\n        bind_expression = bindparam.type.bind_expression(bindparam)\n        return self.process(bind_expression, skip_bind_expression=True)\n    if (literal_binds or (within_columns_clause and self.ansi_bind_rules)):\n        if ((bindparam.value is None) and (bindparam.callable is None)):\n            raise exc.CompileError((\"Bind parameter '%s' without a renderable value not allowed here.\" % bindparam.key))\n        return self.render_literal_bindparam(bindparam, within_columns_clause=True, **kwargs)\n    name = self._truncate_bindparam(bindparam)\n    if (name in self.binds):\n        existing = self.binds[name]\n        if (existing is not bindparam):\n            if ((existing.unique or bindparam.unique) and (not existing.proxy_set.intersection(bindparam.proxy_set))):\n                raise exc.CompileError((\"Bind parameter '%s' conflicts with unique bind parameter of the same name\" % bindparam.key))\n            elif (existing._is_crud or bindparam._is_crud):\n                raise exc.CompileError((\"bindparam() name '%s' is reserved for automatic usage in the VALUES or SET clause of this insert/update statement.   Please use a name other than column name when using bindparam() with insert() or update() (for example, 'b_%s').\" % (bindparam.key, bindparam.key)))\n    self.binds[bindparam.key] = self.binds[name] = bindparam\n    return self.bindparam_string(name, **kwargs)\n", "label": 1}
{"function": "\n\ndef __parse_comment(self):\n    'Scan through a // or /* comment.'\n    comment_start_pos = self.__scanner.position\n    self.__scanner.read_ubyte()\n    if self.__scanner.at_end:\n        self.__error(\"Unexpected character '/'\")\n    c = self.__scanner.read_ubyte()\n    if (c == '/'):\n        while (not self.__scanner.at_end):\n            c = self.__scanner.read_ubyte()\n            if ((c == '\\n') or (c == '\\r')):\n                break\n        if ((c == '\\r') and (self.__scanner.peek_next_ubyte(none_if_bad_index=True) == '\\n')):\n            self.__scanner.read_ubyte()\n    elif (c == '*'):\n        while (not self.__scanner.at_end):\n            c = self.__scanner.read_ubyte()\n            if ((c == '*') and (self.__scanner.peek_next_ubyte(none_if_bad_index=True) == '/')):\n                self.__scanner.read_ubyte()\n                return\n        self.__error('Unterminated comment', comment_start_pos)\n    else:\n        self.__error((\"Unexpected character '/%s'\" % c))\n", "label": 1}
{"function": "\n\ndef handle_elt(self, elt, context):\n    titles = elt.findall('bibl/title')\n    title = []\n    if titles:\n        title = '\\n'.join((title.text.strip() for title in titles))\n    authors = elt.findall('bibl/author')\n    author = []\n    if authors:\n        author = '\\n'.join((author.text.strip() for author in authors))\n    dates = elt.findall('bibl/date')\n    date = []\n    if dates:\n        date = '\\n'.join((date.text.strip() for date in dates))\n    publishers = elt.findall('bibl/publisher')\n    publisher = []\n    if publishers:\n        publisher = '\\n'.join((publisher.text.strip() for publisher in publishers))\n    idnos = elt.findall('bibl/idno')\n    idno = []\n    if idnos:\n        idno = '\\n'.join((idno.text.strip() for idno in idnos))\n    notes = elt.findall('bibl/note')\n    note = []\n    if notes:\n        note = '\\n'.join((note.text.strip() for note in notes))\n    return {\n        'title': title,\n        'author': author,\n        'date': date,\n        'publisher': publisher,\n        'idno': idno,\n        'note': note,\n    }\n", "label": 1}
{"function": "\n\ndef pytest_cmdline_preparse(config, args):\n    if (('PYTEST_VERBOSE' in os.environ) and ('-v' not in args)):\n        args.insert(0, '-v')\n    if (('PYTEST_EXITFIRST' in os.environ) and ('-x' not in args)):\n        args.insert(0, '-x')\n    if (('PYTEST_NOCAPTURE' in os.environ) and ('-s' not in args)):\n        args.insert(0, '-s')\n    if (('PYTEST_TB' in os.environ) and (not any((('--tb' in a) for a in args)))):\n        args.insert(0, ('--tb=' + os.environ['PYTEST_TB']))\n    else:\n        args.insert(0, '--tb=short')\n    if (('PYTEST_NPROCS' in os.environ) and ('-n' not in args)):\n        args.insert(0, ('-n ' + os.environ['PYTEST_NPROCS']))\n    if (('PYTEST_WATCH' in os.environ) and ('-f' not in args)):\n        args.insert(0, '-f')\n    if ('PYTEST_LAZY' in os.environ):\n        args.insert(0, '--lazy')\n    if ('PYTEST_GREEDY' in os.environ):\n        args.insert(0, '--greedy')\n", "label": 1}
{"function": "\n\ndef fix_repeating_arguments(self):\n    'Fix elements that should accumulate/increment values.'\n    either = [list(c.children) for c in self.either.children]\n    for case in either:\n        for e in [c for c in case if (case.count(c) > 1)]:\n            if ((type(e) is Argument) or ((type(e) is Option) and e.argcount)):\n                if (e.value is None):\n                    e.value = []\n                elif (type(e.value) is not list):\n                    e.value = e.value.split()\n            if ((type(e) is Command) or ((type(e) is Option) and (e.argcount == 0))):\n                e.value = 0\n    return self\n", "label": 1}
{"function": "\n\ndef has_header(self, sample):\n    rdr = reader(StringIO(sample), self.sniff(sample))\n    header = next(rdr)\n    columns = len(header)\n    columnTypes = {\n        \n    }\n    for i in range(columns):\n        columnTypes[i] = None\n    checked = 0\n    for row in rdr:\n        if (checked > 20):\n            break\n        checked += 1\n        if (len(row) != columns):\n            continue\n        for col in list(columnTypes.keys()):\n            for thisType in [int, float, complex]:\n                try:\n                    thisType(row[col])\n                    break\n                except (ValueError, OverflowError):\n                    pass\n            else:\n                thisType = len(row[col])\n            if (thisType != columnTypes[col]):\n                if (columnTypes[col] is None):\n                    columnTypes[col] = thisType\n                else:\n                    del columnTypes[col]\n    hasHeader = 0\n    for (col, colType) in columnTypes.items():\n        if (type(colType) == type(0)):\n            if (len(header[col]) != colType):\n                hasHeader += 1\n            else:\n                hasHeader -= 1\n        else:\n            try:\n                colType(header[col])\n            except (ValueError, TypeError):\n                hasHeader += 1\n            else:\n                hasHeader -= 1\n    return (hasHeader > 0)\n", "label": 1}
{"function": "\n\ndef unpack(ext, source, dest_path):\n    '\\n    Unpack the archive |source| to |dest_path|.\\n    Note: |source| can be a file handle or a path.\\n    |ext| contains the extension of the archive.\\n    '\n    if (ext != '.zip'):\n        close_source = False\n        try:\n            if isinstance(source, basestring):\n                source = open(source, 'rb')\n                close_source = True\n            if ((ext == '.tar.gz') or (ext == '.tgz')):\n                un_tar_directory(source, dest_path, 'gz')\n            elif (ext == '.tar.bz2'):\n                un_tar_directory(source, dest_path, 'bz2')\n            elif (ext == '.gz'):\n                with open(dest_path, 'wb') as f:\n                    shutil.copyfileobj(un_gzip_stream(source), f)\n            else:\n                raise UsageError('Not an archive.')\n        except (tarfile.TarError, IOError):\n            raise UsageError('Invalid archive upload.')\n        finally:\n            if close_source:\n                source.close()\n    else:\n        delete_source = False\n        try:\n            if (not isinstance(source, basestring)):\n                temp_path = (dest_path + '.zip')\n                with open(temp_path, 'wb') as f:\n                    shutil.copyfileobj(source, f)\n                source = temp_path\n                delete_source = True\n            exitcode = subprocess.call(['unzip', '-q', source, '-d', dest_path])\n            if (exitcode != 0):\n                raise UsageError('Invalid archive upload.')\n        finally:\n            if delete_source:\n                path_util.remove(source)\n", "label": 1}
{"function": "\n\ndef _complete(test_finder, thing):\n    if (':' in thing):\n        (module, test_part) = thing.split(':')\n        tests = list(test_finder.get_module_tests(module))\n        if ('.' in test_part):\n            return _get_prefixed(strings=tests, prefix=test_part)\n        funcs = [test for test in tests if (test.count('.') == 0)]\n        classes = [test.split('.')[0] for test in tests if ('.' in test)]\n        if (test_part in classes):\n            return ['.']\n        return _get_prefixed(strings=(funcs + classes), prefix=test_part)\n    if os.path.isdir(thing):\n        if ((thing != '.') and (not thing.endswith('/'))):\n            return ['/']\n        return _get_py_or_dirs(thing, '')\n    if os.path.exists(thing):\n        return [':']\n    (directory, file_part) = os.path.split(thing)\n    return _get_py_or_dirs(directory, file_part)\n", "label": 1}
{"function": "\n\ndef refresh(self):\n    if self.subscribed:\n        self.title = 'Subscribed projects'\n        if self.unreviewed:\n            self.title += ' with unreviewed changes'\n    else:\n        self.title = 'All projects'\n    self.app.status.update(title=self.title)\n    with self.app.db.getSession() as session:\n        i = 0\n        for project in session.getProjects(topicless=True, subscribed=self.subscribed, unreviewed=self.unreviewed):\n            i = self._projectRow(i, project, None)\n        for topic in session.getTopics():\n            i = self._topicRow(i, topic)\n            topic_unreviewed = 0\n            topic_open = 0\n            for project in topic.projects:\n                cache = self.app.project_cache.get(project)\n                topic_unreviewed += cache['unreviewed_changes']\n                topic_open += cache['open_changes']\n                if self.subscribed:\n                    if (not project.subscribed):\n                        continue\n                    if (self.unreviewed and (not cache['unreviewed_changes'])):\n                        continue\n                if (topic.key in self.open_topics):\n                    i = self._projectRow(i, project, topic)\n            topic_row = self.topic_rows.get(topic.key)\n            topic_row.update(topic, topic_unreviewed, topic_open)\n    while (i < len(self.listbox.body)):\n        current_row = self.listbox.body[i]\n        self._deleteRow(current_row)\n", "label": 1}
{"function": "\n\ndef __init__(self, param, cursor, strings_only=False):\n    if (settings.USE_TZ and (isinstance(param, datetime.datetime) and (not isinstance(param, Oracle_datetime)))):\n        if timezone.is_aware(param):\n            warnings.warn(\"The Oracle database adapter received an aware datetime (%s), probably from cursor.execute(). Update your code to pass a naive datetime in the database connection's time zone (UTC by default).\", RemovedInDjango20Warning)\n            param = param.astimezone(timezone.utc).replace(tzinfo=None)\n        param = Oracle_datetime.from_datetime(param)\n    if isinstance(param, datetime.timedelta):\n        param = duration_string(param)\n        if (' ' not in param):\n            param = ('0 ' + param)\n    string_size = 0\n    if (param is True):\n        param = 1\n    elif (param is False):\n        param = 0\n    if hasattr(param, 'bind_parameter'):\n        self.force_bytes = param.bind_parameter(cursor)\n    elif isinstance(param, Database.Binary):\n        self.force_bytes = param\n    else:\n        self.force_bytes = convert_unicode(param, cursor.charset, strings_only)\n        if isinstance(self.force_bytes, six.string_types):\n            string_size = len(force_bytes(param, cursor.charset, strings_only))\n    if hasattr(param, 'input_size'):\n        self.input_size = param.input_size\n    elif (string_size > 4000):\n        self.input_size = Database.CLOB\n    else:\n        self.input_size = None\n", "label": 1}
{"function": "\n\ndef _update_checksum(self, checksum, checksum_keyword='CHECKSUM', datasum_keyword='DATASUM'):\n    \"Update the 'CHECKSUM' and 'DATASUM' keywords in the header (or\\n        keywords with equivalent semantics given by the ``checksum_keyword``\\n        and ``datasum_keyword`` arguments--see for example ``CompImageHDU``\\n        for an example of why this might need to be overridden).\\n        \"\n    modified = (self._header._modified or self._data_loaded)\n    if (checksum == 'remove'):\n        if (checksum_keyword in self._header):\n            del self._header[checksum_keyword]\n        if (datasum_keyword in self._header):\n            del self._header[datasum_keyword]\n    elif (modified or self._new or (checksum and (('CHECKSUM' not in self._header) or ('DATASUM' not in self._header)))):\n        if (checksum == 'datasum'):\n            self.add_datasum(datasum_keyword=datasum_keyword)\n        elif (checksum == 'nonstandard_datasum'):\n            self.add_datasum(blocking='nonstandard', datasum_keyword=datasum_keyword)\n        elif (checksum == 'test'):\n            self.add_datasum(self._datasum_comment, datasum_keyword=datasum_keyword)\n            self.add_checksum(self._checksum_comment, True, checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n        elif (checksum == 'nonstandard'):\n            self.add_checksum(blocking='nonstandard', checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n        elif checksum:\n            self.add_checksum(blocking='standard', checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n", "label": 1}
{"function": "\n\ndef skip(inbuf, ftype):\n    if ((ftype == TType.BOOL) or (ftype == TType.BYTE)):\n        inbuf.read(1)\n    elif (ftype == TType.I16):\n        inbuf.read(2)\n    elif (ftype == TType.I32):\n        inbuf.read(4)\n    elif (ftype == TType.I64):\n        inbuf.read(8)\n    elif (ftype == TType.DOUBLE):\n        inbuf.read(8)\n    elif (ftype == TType.STRING):\n        inbuf.read(unpack_i32(inbuf.read(4)))\n    elif ((ftype == TType.SET) or (ftype == TType.LIST)):\n        (v_type, sz) = read_list_begin(inbuf)\n        for i in range(sz):\n            skip(inbuf, v_type)\n    elif (ftype == TType.MAP):\n        (k_type, v_type, sz) = read_map_begin(inbuf)\n        for i in range(sz):\n            skip(inbuf, k_type)\n            skip(inbuf, v_type)\n    elif (ftype == TType.STRUCT):\n        while True:\n            (f_type, fid) = read_field_begin(inbuf)\n            if (f_type == TType.STOP):\n                break\n            skip(inbuf, f_type)\n", "label": 1}
{"function": "\n\n@register_opt('fast_compile')\n@local_optimizer([PdbBreakpoint])\ndef local_gpu_pdbbreakpoint_op(node):\n    if isinstance(node.op, PdbBreakpoint):\n        old_inputs = node.inputs\n        old_outputs = node.outputs\n        new_inputs = node.inputs[:1]\n        input_transfered = []\n        nb_monitored_vars = len(node.outputs)\n        for i in range(nb_monitored_vars):\n            inp = old_inputs[(i + 1)]\n            out = old_outputs[i]\n            input_is_from_gpu = (inp.owner and isinstance(inp.owner.op, HostFromGpu))\n            output_goes_to_gpu = False\n            for c in out.clients:\n                if (c == 'output'):\n                    continue\n                if isinstance(c[0].op, GpuFromHost):\n                    output_goes_to_gpu = True\n                    context_name = c[0].op.context_name\n                    break\n            if input_is_from_gpu:\n                new_inputs.append(inp.owner.inputs[0])\n                input_transfered.append(True)\n            elif output_goes_to_gpu:\n                new_inputs.append(GpuFromHost(context_name)(inp))\n                input_transfered.append(True)\n            else:\n                new_inputs.append(inp)\n                input_transfered.append(False)\n        if (not any(input_transfered)):\n            return False\n        new_op_outputs = node.op(*new_inputs, return_list=True)\n        new_outputs = []\n        for i in range(len(new_op_outputs)):\n            if input_transfered[i]:\n                new_outputs.append(host_from_gpu(new_op_outputs[i]))\n            else:\n                new_outputs.append(new_op_outputs[i])\n        return new_outputs\n    return False\n", "label": 1}
{"function": "\n\ndef test_json_conversion():\n    from commonast import Node, Assign, Name, BinOp, Bytes, Num\n    roota = Assign([Name('foo')], BinOp('Add', Name('a'), Num(3)))\n    rootb = Assign([Name('foo')], BinOp('Add', None, Num(3.2)))\n    rootc = Assign([Name('foo')], BinOp('Add', Bytes(b'xx'), Num(4j)))\n    for node1 in (roota, rootb, rootc):\n        js = node1.tojson()\n        node2 = Node.fromjson(js)\n        assert (js.count('BinOp') == 1)\n        assert (js.count('Num') == 1)\n        assert (node2.target_nodes[0].name == node1.target_nodes[0].name)\n        assert (node2.value_node.op == node1.value_node.op)\n        assert (node2.value_node.left_node == node1.value_node.left_node)\n        assert (node2.value_node.right_node.value == node1.value_node.right_node.value)\n        (node1 == node2)\n    assert (roota != rootb)\n    assert (roota != rootc)\n    with raises(ValueError):\n        (roota == 5)\n    assert (str(roota) == roota.tojson())\n    assert (len(repr(roota)) < 80)\n", "label": 1}
{"function": "\n\ndef encode(self, command, source, dest, pretend=False):\n    'Encode `source` to `dest` using command template `command`.\\n\\n        Raises `subprocess.CalledProcessError` if the command exited with a\\n        non-zero status code.\\n        '\n    assert isinstance(command, bytes)\n    assert isinstance(source, bytes)\n    assert isinstance(dest, bytes)\n    quiet = self.config['quiet'].get(bool)\n    if ((not quiet) and (not pretend)):\n        self._log.info('Encoding {0}', util.displayable_path(source))\n    args = shlex.split(command)\n    for (i, arg) in enumerate(args):\n        args[i] = Template(arg).safe_substitute({\n            b'source': source,\n            b'dest': dest,\n        })\n    if pretend:\n        self._log.info(' '.join(ui.decargs(args)))\n        return\n    try:\n        util.command_output(args)\n    except subprocess.CalledProcessError as exc:\n        self._log.info('Encoding {0} failed. Cleaning up...', util.displayable_path(source))\n        self._log.debug('Command {0} exited with status {1}', exc.cmd.decode('utf8', 'ignore'), exc.returncode)\n        util.remove(dest)\n        util.prune_dirs(os.path.dirname(dest))\n        raise\n    except OSError as exc:\n        raise ui.UserError(\"convert: could invoke '{0}': {1}\".format(' '.join(args), exc))\n    if ((not quiet) and (not pretend)):\n        self._log.info('Finished encoding {0}', util.displayable_path(source))\n", "label": 1}
{"function": "\n\ndef _complete_dispatcher(self, dt):\n    'This method is scheduled on all touch up events. It will dispatch\\n        the `on_gesture_complete` event for all completed gestures, and remove\\n        merged gestures from the internal gesture list.'\n    need_cleanup = False\n    gest = self._gestures\n    timeout = self.draw_timeout\n    twin = self.temporal_window\n    get_time = Clock.get_time\n    for (idx, g) in enumerate(gest):\n        if g.was_merged:\n            del gest[idx]\n            continue\n        if ((not g.active) or (g.active_strokes != 0)):\n            continue\n        t1 = (g._update_time + twin)\n        t2 = (get_time() + UNDERSHOOT_MARGIN)\n        if ((not g.accept_stroke()) or (t1 <= t2)):\n            discard = False\n            if ((g.width < 5) and (g.height < 5)):\n                discard = True\n            elif g.single_points_test():\n                discard = True\n            need_cleanup = True\n            g.active = False\n            g._cleanup_time = (get_time() + timeout)\n            if discard:\n                self.dispatch('on_gesture_discard', g)\n            else:\n                self.dispatch('on_gesture_complete', g)\n    if need_cleanup:\n        Clock.schedule_once(self._cleanup, timeout)\n", "label": 1}
{"function": "\n\ndef _repr(self, value, pos):\n    try:\n        if (value is None):\n            return ''\n        if self._unicode:\n            try:\n                value = str(value)\n                if (not is_unicode(value)):\n                    value = value.decode('utf-8')\n            except UnicodeDecodeError:\n                value = bytes(value)\n        else:\n            if (not isinstance(value, basestring_)):\n                value = coerce_text(value)\n            if (is_unicode(value) and self.default_encoding):\n                value = value.encode(self.default_encoding)\n    except:\n        exc_info = sys.exc_info()\n        e = exc_info[1]\n        e.args = (self._add_line_info(e.args[0], pos),)\n        raise e\n    else:\n        if (self._unicode and isinstance(value, bytes)):\n            if (not self.default_encoding):\n                raise UnicodeDecodeError(('Cannot decode bytes value %r into unicode (no default_encoding provided)' % value))\n            try:\n                value = value.decode(self.default_encoding)\n            except UnicodeDecodeError as e:\n                raise UnicodeDecodeError(e.encoding, e.object, e.start, e.end, (e.reason + (' in string %r' % value)))\n        elif ((not self._unicode) and is_unicode(value)):\n            if (not self.default_encoding):\n                raise UnicodeEncodeError(('Cannot encode unicode value %r into bytes (no default_encoding provided)' % value))\n            value = value.encode(self.default_encoding)\n        return value\n", "label": 1}
{"function": "\n\ndef test_source_packages():\n    for ext in ('.tar.gz', '.tar', '.tgz', '.zip', '.tar.bz2'):\n        sl = SourcePackage(('a_p_r-3.1.3' + ext))\n        assert (sl._name == 'a_p_r')\n        assert (sl.name == 'a-p-r')\n        assert (sl.raw_version == '3.1.3')\n        assert (sl.version == parse_version(sl.raw_version))\n        for req in ('a_p_r', 'a_p_r>2', 'a_p_r>3', 'a_p_r>=3.1.3', 'a_p_r==3.1.3', 'a_p_r>3,<3.5'):\n            assert sl.satisfies(req)\n            assert sl.satisfies(Requirement.parse(req))\n        for req in ('foo', 'a_p_r==4.0.0', 'a_p_r>4.0.0', 'a_p_r>3.0.0,<3.0.3', 'a==3.1.3'):\n            assert (not sl.satisfies(req))\n    sl = SourcePackage('python-dateutil-1.5.tar.gz')\n    assert (sl.name == 'python-dateutil')\n    assert (sl.raw_version == '1.5')\n", "label": 1}
{"function": "\n\ndef console_output(self, targets):\n    concrete_targets = set()\n    for target in targets:\n        concrete_target = target.concrete_derived_from\n        concrete_targets.add(concrete_target)\n        if isinstance(concrete_target, ScalaLibrary):\n            concrete_targets.update(concrete_target.java_sources)\n    buildroot = get_buildroot()\n    files = set()\n    output_globs = self.get_options().globs\n    concrete_targets = set([target for target in concrete_targets if (not target.is_synthetic)])\n    for target in concrete_targets:\n        files.add(target.address.build_file.full_path)\n        if (output_globs or target.has_sources()):\n            if output_globs:\n                globs_obj = target.globs_relative_to_buildroot()\n                if globs_obj:\n                    files.update((os.path.join(buildroot, src) for src in globs_obj['globs']))\n            else:\n                files.update((os.path.join(buildroot, src) for src in target.sources_relative_to_buildroot()))\n        if (isinstance(target, JvmApp) and (not output_globs)):\n            files.update(itertools.chain(*[bundle.filemap.keys() for bundle in target.bundles]))\n    return files\n", "label": 1}
{"function": "\n\ndef search_article(keyword, directory, datadir, exclude):\n    '\\n    Search for a keyword in every article within your current directory and\\n    below. Much like recursive grep.\\n    '\n    c = 0\n    r = re.compile(keyword)\n    print('Articles:')\n    for (dirpath, dirs, files) in os.walk(directory):\n        dirs[:] = [d for d in dirs if (d not in exclude)]\n        for fname in files:\n            path = os.path.join(dirpath, fname)\n            if (r.search(path) is not None):\n                print(('* \\x1b[92m%s\\x1b[39m' % os.path.relpath(path, datadir)))\n                c = (c + 1)\n    print('Content:')\n    for (dirpath, dirs, files) in os.walk(directory):\n        dirs[:] = [d for d in dirs if (d not in exclude)]\n        for fname in files:\n            path = os.path.join(dirpath, fname)\n            f = open(path, 'rt')\n            for (i, line) in enumerate(f):\n                if r.search(line):\n                    c = (c + 1)\n                    print(('* \\x1b[92m%s\\x1b[39m: %s' % (os.path.relpath(path, datadir), line.rstrip('\\n'))))\n    return ('Results: %s' % c)\n", "label": 1}
{"function": "\n\ndef test_rolling():\n    time = MockedTime()\n    percentile = RollingPercentile(time, 60000, 12, 1000, True)\n    percentile.add_value(1000)\n    percentile.add_value(1000)\n    percentile.add_value(1000)\n    percentile.add_value(2000)\n    assert (percentile.buckets.size == 1)\n    assert (percentile.percentile(50) == 0)\n    time.increment(6000)\n    assert (percentile.buckets.size == 1)\n    assert (percentile.percentile(50) == 1000)\n    assert (percentile.buckets.size == 2)\n    percentile.add_value(1000)\n    percentile.add_value(500)\n    assert (percentile.buckets.size == 2)\n    percentile.add_value(200)\n    percentile.add_value(200)\n    percentile.add_value(1600)\n    percentile.add_value(200)\n    percentile.add_value(1600)\n    percentile.add_value(1600)\n    assert (percentile.percentile(50) == 1000)\n    time.increment(6000)\n    snapshot = PercentileSnapshot(1000, 1000, 1000, 2000, 1000, 500, 200, 200, 1600, 200, 1600, 1600)\n    assert (snapshot.percentile(0.15) == percentile.percentile(0.15))\n    assert (snapshot.percentile(0.5) == percentile.percentile(0.5))\n    assert (snapshot.percentile(0.9) == percentile.percentile(0.9))\n    assert (snapshot.percentile(0.995) == percentile.percentile(0.995))\n    assert (snapshot.mean() == 991)\n", "label": 1}
{"function": "\n\ndef _convert_names_to_rpm(self, python_names, only_name):\n    if (not python_names):\n        return ({\n            \n        }, {\n            \n        })\n    cmdline = ((self._start_cmdline() + ['--convert']) + python_names)\n    result = collections.defaultdict(set)\n    conflicts = collections.defaultdict(set)\n    current_source = None\n    for line in sh.execute(cmdline)[0].splitlines():\n        if line.startswith('Requires:'):\n            line = line[len('Requires:'):]\n            if only_name:\n                positions = [line.find('>'), line.find('<'), line.find('=')]\n                positions = sorted([p for p in positions if (p != (- 1))])\n                if positions:\n                    line = line[0:positions[0]]\n            result[current_source].add(line.strip())\n        elif line.startswith('Conflicts:'):\n            line = line[len('Conflicts:'):]\n            conflicts[current_source].add(line.strip())\n        elif line.startswith('# Source:'):\n            current_source = line[len('# Source:'):].strip()\n    found_names = set(result.keys())\n    found_names.update(conflicts.keys())\n    (missing_names, extra_names) = _fetch_missing_extra(python_names, found_names)\n    if missing_names:\n        raise AssertionError(('Python names were lost during conversion: %s' % ', '.join(sorted(missing_names))))\n    if extra_names:\n        raise AssertionError(('Extra python names were found during conversion: %s' % ', '.join(sorted(extra_names))))\n    return (result, conflicts)\n", "label": 1}
{"function": "\n\ndef postgresql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None):\n    'PostgreSQL psycopg2 driver  accepts two syntaxes\\n\\n        Plus a string for .pgpass file\\n        '\n    dsn = []\n    if ((dsn_style is None) or (dsn_style == 'all') or (dsn_style == 'keyvalue')):\n        dsnstr = \"host='{0}' dbname='{2}' user='{3}' password='{4}'\"\n        if (dbport is not None):\n            dsnstr += \" port='{1}'\"\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'kwargs')):\n        dsnstr = \"host='{0}', database='{2}', user='{3}', password='{4}'\"\n        if (dbport is not None):\n            dsnstr += \", port='{1}'\"\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'uri')):\n        if (dbport is not None):\n            dsnstr = 'postgresql://{3}:{4}@{0}:{1}/{2}'\n        else:\n            dsnstr = 'postgresql://{3}:{4}@{0}/{2}'\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'pgpass')):\n        if (dbport is not None):\n            dbport = 5432\n        dsn.append('{0}:{1}:{2}:{3}:{4}'.format(dbhost, dbport, dbname, dbuser, dbpass))\n    return dsn\n", "label": 1}
{"function": "\n\ndef add_homology(self, seq, search, id=None, idFormat='%s_%d', autoIncrement=False, maxAnnot=999999, maxLoss=None, sliceInfo=None, **kwargs):\n    'find homology in our seq db and add as annotations'\n    try:\n        if (self.sliceAttrDict['id'] != 0):\n            raise KeyError\n    except KeyError:\n        sliceAttrDict['id'] = 0\n        sliceAttrDict['start'] = 1\n        sliceAttrDict['stop'] = 2\n    if autoIncrement:\n        id = len(self.sliceDB)\n    elif (id is None):\n        id = seq.id\n    if isinstance(search, str):\n        search = getattr(self.seqDB, search)\n    if isinstance(seq, str):\n        seq = Sequence(seq, str(id))\n    al = search(seq, **kwargs)\n    if (maxLoss is not None):\n        kwargs['minAlignSize'] = (len(seq) - maxLoss)\n    hits = al[seq].keys(**kwargs)\n    if (len(hits) > maxAnnot):\n        raise ValueError(('too many hits for %s: %d' % (id, len(hits))))\n    out = []\n    i = 0\n    k = id\n    for ival in hits:\n        if (len(hits) > 1):\n            if autoIncrement:\n                k = len(self.sliceDB)\n            else:\n                k = (idFormat % (id, i))\n            i += 1\n        if (sliceInfo is not None):\n            a = self.new_annotation(k, ((ival.id, ival.start, ival.stop) + sliceInfo))\n        else:\n            a = self.new_annotation(k, (ival.id, ival.start, ival.stop))\n        out.append(a)\n    return out\n", "label": 1}
{"function": "\n\ndef __init__(self, year=None, month=None, day=None, week=None, day_of_week=None, hour=None, minute=None, second=None, start_date=None, end_date=None, timezone=None):\n    if timezone:\n        self.timezone = astimezone(timezone)\n    elif (start_date and start_date.tzinfo):\n        self.timezone = start_date.tzinfo\n    elif (end_date and end_date.tzinfo):\n        self.timezone = end_date.tzinfo\n    else:\n        self.timezone = get_localzone()\n    self.start_date = convert_to_datetime(start_date, self.timezone, 'start_date')\n    self.end_date = convert_to_datetime(end_date, self.timezone, 'end_date')\n    values = dict(((key, value) for (key, value) in six.iteritems(locals()) if ((key in self.FIELD_NAMES) and (value is not None))))\n    self.fields = []\n    assign_defaults = False\n    for field_name in self.FIELD_NAMES:\n        if (field_name in values):\n            exprs = values.pop(field_name)\n            is_default = False\n            assign_defaults = (not values)\n        elif assign_defaults:\n            exprs = DEFAULT_VALUES[field_name]\n            is_default = True\n        else:\n            exprs = '*'\n            is_default = True\n        field_class = self.FIELDS_MAP[field_name]\n        field = field_class(field_name, exprs, is_default)\n        self.fields.append(field)\n", "label": 1}
{"function": "\n\ndef _eval_is_zero(self):\n    if self.function.is_zero:\n        return True\n    got_none = False\n    for l in self.limits:\n        if (len(l) == 3):\n            z = ((l[1] == l[2]) or (l[1] - l[2]).is_zero)\n            if z:\n                return True\n            elif (z is None):\n                got_none = True\n    free = self.function.free_symbols\n    for xab in self.limits:\n        if (len(xab) == 1):\n            free.add(xab[0])\n            continue\n        if ((len(xab) == 2) and (xab[0] not in free)):\n            if xab[1].is_zero:\n                return True\n            elif (xab[1].is_zero is None):\n                got_none = True\n        free.discard(xab[0])\n        for i in xab[1:]:\n            free.update(i.free_symbols)\n    if ((self.function.is_zero is False) and (got_none is False)):\n        return False\n", "label": 1}
{"function": "\n\ndef parse_command_line(args=None):\n    if (args is None):\n        args = sys.argv[1:]\n    usage = __doc__.format(cmd=os.path.basename(sys.argv[0]))\n    options = docopt(usage, args)\n    for opt_name in ('author', 'email', 'description'):\n        options[opt_name] = options.pop(('--' + opt_name))\n    options['command'] = options.pop('<command>')\n    options['project_name'] = options.pop('--project-name')\n    if (not RE_COMMAND_NAME.match(options['command'])):\n        die('command name must match regular expression {!r}', RE_COMMAND_NAME.pattern)\n    for (info, question, default_value) in (('author', 'Your name:  ', None), ('email', 'Your email: ', None), ('description', 'Description: ', None)):\n        if options.get(info, None):\n            continue\n        try:\n            options[info] = ask(question)\n        except EOFError:\n            pass\n        if (not options[info]):\n            if (default_value is None):\n                options[info] = ''\n            else:\n                options[info] = default_value\n    if (options['project_name'] is None):\n        default = 'OpenLMI {command} Script'.format(command=options['command'].capitalize())\n        try:\n            options['project_name'] = ask('Project name [{default}]: '.format(default=default))\n        except EOFError:\n            pass\n        if (not options['project_name']):\n            options['project_name'] = default\n    options = {k: (v.decode('utf-8') if isinstance(v, str) else v) for (k, v) in options.items()}\n    return options\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.settings = TrafficControlSetting()\n                self.settings.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.device = TrafficControlledDevice()\n                self.device.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.timeout = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _create_joins(self, source_polymorphic=False, source_selectable=None, dest_polymorphic=False, dest_selectable=None, of_type=None):\n    if (source_selectable is None):\n        if (source_polymorphic and self.parent.with_polymorphic):\n            source_selectable = self.parent._with_polymorphic_selectable\n    aliased = False\n    if (dest_selectable is None):\n        if (dest_polymorphic and self.mapper.with_polymorphic):\n            dest_selectable = self.mapper._with_polymorphic_selectable\n            aliased = True\n        else:\n            dest_selectable = self.mapper.mapped_table\n        if (self._is_self_referential and (source_selectable is None)):\n            dest_selectable = dest_selectable.alias()\n            aliased = True\n    else:\n        aliased = True\n    dest_mapper = (of_type or self.mapper)\n    single_crit = dest_mapper._single_table_criterion\n    aliased = (aliased or (source_selectable is not None))\n    (primaryjoin, secondaryjoin, secondary, target_adapter, dest_selectable) = self._join_condition.join_targets(source_selectable, dest_selectable, aliased, single_crit)\n    if (source_selectable is None):\n        source_selectable = self.parent.local_table\n    if (dest_selectable is None):\n        dest_selectable = self.mapper.local_table\n    return (primaryjoin, secondaryjoin, source_selectable, dest_selectable, secondary, target_adapter)\n", "label": 1}
{"function": "\n\ndef dict_to_xml(self, root_elm, data):\n    for key in data.keys():\n        if (key in self.NO_SEND_FIELDS):\n            continue\n        sub_data = data[key]\n        elm = SubElement(root_elm, key)\n        if isinstance(sub_data, dict):\n            self.dict_to_xml(elm, sub_data)\n        elif (isinstance(sub_data, list) or isinstance(sub_data, tuple)):\n            if isplural(key):\n                for d in sub_data:\n                    self.dict_to_xml(SubElement(elm, singular(key)), d)\n            else:\n                for d in sub_data:\n                    self.dict_to_xml(elm, d)\n        else:\n            if (key in self.BOOLEAN_FIELDS):\n                val = ('true' if sub_data else 'false')\n            elif (key in self.DATE_FIELDS):\n                val = sub_data.strftime('%Y-%m-%dT%H:%M:%S')\n            else:\n                val = six.text_type(sub_data)\n            elm.text = val\n    return root_elm\n", "label": 1}
{"function": "\n\ndef has_method(obj, name):\n    if (not hasattr(obj, name)):\n        return False\n    func = getattr(obj, name)\n    if isinstance(func, types.BuiltinMethodType):\n        return True\n    if (not isinstance(func, (types.MethodType, types.FunctionType))):\n        return False\n    base_type = (obj if is_type(obj) else obj.__class__)\n    original = None\n    for subtype in inspect.getmro(base_type):\n        original = vars(subtype).get(name)\n        if (original is not None):\n            break\n    if (original is None):\n        return False\n    if isinstance(original, staticmethod):\n        return True\n    self_attr = ('__self__' if PY3 else 'im_self')\n    if (not hasattr(func, self_attr)):\n        return False\n    bound_to = getattr(func, self_attr)\n    if isinstance(original, classmethod):\n        return issubclass(base_type, bound_to)\n    return isinstance(obj, type(bound_to))\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_data_rows(self, ar):\n    if ar.param_values.show_callables:\n\n        def flt(v):\n            return True\n    else:\n\n        def flt(v):\n            if isinstance(v, (types.FunctionType, types.GeneratorType, types.UnboundMethodType, types.UnboundMethodType, types.BuiltinMethodType, types.BuiltinFunctionType)):\n                return False\n            return True\n    o = self.get_inspected(ar.param_values.inspected)\n    if isinstance(o, (list, tuple)):\n        for (i, v) in enumerate(o):\n            k = (('[' + str(i)) + ']')\n            (yield Inspected(o, '', k, v))\n    elif isinstance(o, AttrDict):\n        for (k, v) in list(o.items()):\n            (yield Inspected(o, '.', k, v))\n    elif isinstance(o, dict):\n        for (k, v) in list(o.items()):\n            k = (('[' + repr(k)) + ']')\n            (yield Inspected(o, '', k, v))\n    else:\n        for k in dir(o):\n            if (not k.startswith('__')):\n                if ((not ar.quick_search) or (ar.quick_search.lower() in k.lower())):\n                    v = getattr(o, k)\n                    if flt(v):\n                        (yield Inspected(o, '.', k, v))\n", "label": 1}
{"function": "\n\ndef __init__(self, method, uri, version='HTTP/1.0', headers=None, body=None, remote_ip=None, protocol=None, host=None, files=None, connection=None):\n    self.method = method\n    self.uri = uri\n    self.version = version\n    self.headers = (headers or httputil.HTTPHeaders())\n    self.body = (body or '')\n    self.remote_ip = remote_ip\n    if protocol:\n        self.protocol = protocol\n    elif (connection and isinstance(connection.stream, iostream.SSLIOStream)):\n        self.protocol = 'https'\n    else:\n        self.protocol = 'http'\n    if (connection and connection.xheaders):\n        ip = self.headers.get('X-Forwarded-For', self.remote_ip)\n        ip = ip.split(',')[(- 1)].strip()\n        ip = self.headers.get('X-Real-Ip', ip)\n        if netutil.is_valid_ip(ip):\n            self.remote_ip = ip\n        proto = self.headers.get('X-Scheme', self.headers.get('X-Forwarded-Proto', self.protocol))\n        if (proto in ('http', 'https')):\n            self.protocol = proto\n    self.host = (host or self.headers.get('Host') or '127.0.0.1')\n    self.files = (files or {\n        \n    })\n    self.connection = connection\n    self._start_time = time.time()\n    self._finish_time = None\n    (self.path, sep, self.query) = uri.partition('?')\n    self.arguments = parse_qs_bytes(self.query, keep_blank_values=True)\n", "label": 1}
{"function": "\n\ndef clean(self):\n    type = self.cleaned_data['recordType']\n    if (type in ('4', 'Z')):\n        for field in ADDRESS_FIELDS:\n            self.cleaned_data[field] = None\n    if (type == 'Z'):\n        self.cleaned_data['zipExtensionLow'] = None\n        self.cleaned_data['zipExtensionHigh'] = None\n        low = self.cleaned_data['zipCodeLow']\n        high = self.cleaned_data['zipCodeHigh']\n        if ((high is None) or (low is None)):\n            raise ValidationError(_('Zip-5 records need a high and a low.'))\n    elif (type == '4'):\n        low = self.cleaned_data['zipCodeLow']\n        high = self.cleaned_data['zipCodeHigh']\n        low_ext = self.cleaned_data['zipExtensionLow']\n        high_ext = self.cleaned_data['zipExtensionHigh']\n        if ((high is None) or (low is None) or (low_ext is None) or (high_ext is None)):\n            raise ValidationError(_('Zip+4 records need a high and a low for both parts.'))\n    else:\n        for field in ZIP_FIELDS:\n            self.cleaned_data[field] = None\n        for field in ('lowAddress', 'highAddress', 'streetName', 'cityName', 'zipCode', 'plus4'):\n            if (not self.cleaned_data[field]):\n                raise ValidationError(_('Address rocord needs: low, high, street, city, zip, zip+4'))\n    return super(TaxBoundryForm, self).clean()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef validate_field(value, field, new_record=True):\n    '\\n        Validates a field value against a field metadata dictionary. Note: this\\n        is not yet intended to be a full validation. There are sure to be\\n        missing validation cases. Returns a list of validation errors.\\n        '\n    errors = []\n    if new_record:\n        if ((not field['createable']) and (value is not None)):\n            errors.append('Cannot create this field')\n    elif ((not field['updateable']) and (value is not None)):\n        errors.append('Cannot update this field')\n    if ((value is not None) and field.get('restrictedPicklist')):\n        values = [i['value'] for i in field['picklistValues'] if i['active']]\n        if (value not in values):\n            errors.append('Bad value for restricted picklist field')\n    if (new_record and (value is None) and (not field['nillable']) and (not field['defaultedOnCreate']) and (field['type'] != 'boolean')):\n        errors.append('This field is required')\n    return errors\n", "label": 1}
{"function": "\n\ndef merge_undo(self, undo_item):\n    ' Merges two undo items if possible.\\n        '\n    if (isinstance(undo_item, self.__class__) and (self.object is undo_item.object) and (self.name == undo_item.name) and (self.index == undo_item.index)):\n        added = undo_item.added\n        removed = undo_item.removed\n        if ((len(self.added) == len(added)) and (len(self.removed) == len(removed))):\n            for (i, item) in enumerate(self.added):\n                if (item is not added[i]):\n                    break\n            else:\n                for (i, item) in enumerate(self.removed):\n                    if (item is not removed[i]):\n                        break\n                else:\n                    return True\n    return False\n", "label": 1}
{"function": "\n\ndef find_selected_files(schema, metadata):\n    targets = []\n    paths = [('', p) for p in schema.schema['pages']]\n    while len(paths):\n        (prefix, path) = paths.pop(0)\n        if path.get('questions'):\n            paths = (paths + [('', q) for q in path['questions']])\n        elif path.get('type'):\n            qid = path.get('qid', path.get('id'))\n            if (path['type'] == 'object'):\n                paths = (paths + [('{}.{}.value'.format(prefix, qid), p) for p in path['properties']])\n            elif (path['type'] == 'osf-upload'):\n                targets.append('{}.{}'.format(prefix, qid).lstrip('.'))\n    selected = {\n        \n    }\n    for t in targets:\n        parts = t.split('.')\n        value = metadata.get(parts.pop(0))\n        while (value and len(parts)):\n            value = value.get(parts.pop(0))\n        if value:\n            selected[t] = value\n    return selected\n", "label": 1}
{"function": "\n\n@defun_wrapped\ndef _ci_generic(ctx, z):\n    if ctx.isinf(z):\n        if (z == ctx.inf):\n            return ctx.zero\n        if (z == ctx.ninf):\n            return (ctx.pi * 1j)\n    jz = ctx.fmul(ctx.j, z, exact=True)\n    njz = ctx.fneg(jz, exact=True)\n    v = (0.5 * (ctx.ei(jz) + ctx.ei(njz)))\n    zreal = ctx._re(z)\n    zimag = ctx._im(z)\n    if (zreal == 0):\n        if (zimag > 0):\n            v += (ctx.pi * 0.5j)\n        if (zimag < 0):\n            v -= (ctx.pi * 0.5j)\n    if (zreal < 0):\n        if (zimag >= 0):\n            v += (ctx.pi * 1j)\n        if (zimag < 0):\n            v -= (ctx.pi * 1j)\n    if (ctx._is_real_type(z) and (zreal > 0)):\n        v = ctx._re(v)\n    return v\n", "label": 1}
{"function": "\n\ndef to_json_message(self):\n    json_message = {\n        'From': self.__sender,\n        'To': self.__to,\n        'Subject': self.__subject,\n    }\n    if self.__reply_to:\n        json_message['ReplyTo'] = self.__reply_to\n    if self.__cc:\n        json_message['Cc'] = self.__cc\n    if self.__bcc:\n        json_message['Bcc'] = self.__bcc\n    if self.__tag:\n        json_message['Tag'] = self.__tag\n    if self.__html_body:\n        json_message['HtmlBody'] = self.__html_body\n    if self.__text_body:\n        json_message['TextBody'] = self.__text_body\n    if self.__track_opens:\n        json_message['TrackOpens'] = True\n    if (len(self.__custom_headers) > 0):\n        cust_headers = []\n        for (key, value) in self.__custom_headers.items():\n            cust_headers.append({\n                'Name': key,\n                'Value': value,\n            })\n        json_message['Headers'] = cust_headers\n    if (len(self.__attachments) > 0):\n        attachments = []\n        for attachment in self.__attachments:\n            if isinstance(attachment, tuple):\n                attachments.append({\n                    'Name': attachment[0],\n                    'Content': attachment[1],\n                    'ContentType': attachment[2],\n                })\n            elif isinstance(attachment, MIMEBase):\n                attachments.append({\n                    'Name': attachment.get_filename(),\n                    'Content': attachment.get_payload(),\n                    'ContentType': attachment.get_content_type(),\n                })\n        json_message['Attachments'] = attachments\n    return json_message\n", "label": 1}
{"function": "\n\ndef _eval_expand_mul(self, **hints):\n    from sympy import fraction\n    expr = self\n    (n, d) = fraction(expr)\n    if d.is_Mul:\n        (n, d) = [(i._eval_expand_mul(**hints) if i.is_Mul else i) for i in (n, d)]\n        expr = (n / d)\n        if (not expr.is_Mul):\n            return expr\n    (plain, sums, rewrite) = ([], [], False)\n    for factor in expr.args:\n        if factor.is_Add:\n            sums.append(factor)\n            rewrite = True\n        elif factor.is_commutative:\n            plain.append(factor)\n        else:\n            sums.append(Basic(factor))\n    if (not rewrite):\n        return expr\n    else:\n        plain = self.func(*plain)\n        if sums:\n            terms = self.func._expandsums(sums)\n            args = []\n            for term in terms:\n                t = self.func(plain, term)\n                if (t.is_Mul and any((a.is_Add for a in t.args))):\n                    t = t._eval_expand_mul()\n                args.append(t)\n            return Add(*args)\n        else:\n            return plain\n", "label": 1}
{"function": "\n\ndef _delete_node(self, nid):\n    ' Deletes a specified tree node and all its children.\\n        '\n    for cnid in self._nodes_for(nid):\n        self._delete_node(cnid)\n    pnid = nid.parent()\n    if ((pnid is not None) and (getattr(pnid, '_dummy', None) is nid)):\n        pnid.removeChild(nid)\n        del pnid._dummy\n        return\n    try:\n        (expanded, node, object) = self._get_node_data(nid)\n    except AttributeError:\n        pass\n    else:\n        id_object = id(object)\n        object_info = self._map[id_object]\n        for (i, info) in enumerate(object_info):\n            if (id(nid) == id(info[1])):\n                del object_info[i]\n                break\n        if (len(object_info) == 0):\n            self._remove_listeners(node, object)\n            del self._map[id_object]\n    if (pnid is None):\n        self._tree.takeTopLevelItem(self._tree.indexOfTopLevelItem(nid))\n    else:\n        pnid.removeChild(nid)\n    if ((self._editor is not None) and (id(nid) == id(self._editor._editor_nid))):\n        self._clear_editor()\n", "label": 1}
{"function": "\n\ndef solve(self, board, i, j):\n    '\\n        dfs\\n        :param board: a 9x9 2D array\\n        :return: Boolean\\n        '\n    if (j >= 9):\n        return self.solve(board, (i + 1), 0)\n    if (i == 9):\n        return True\n    if (board[i][j] == '.'):\n        for num in range(1, 10):\n            num_str = str(num)\n            if (all([(board[i][col] != num_str) for col in xrange(9)]) and all([(board[row][j] != num_str) for row in xrange(9)]) and all([(board[(((i / 3) * 3) + (count / 3))][(((j / 3) * 3) + (count % 3))] != num_str) for count in xrange(9)])):\n                board[i][j] = num_str\n                if (not self.solve(board, i, (j + 1))):\n                    board[i][j] = '.'\n                else:\n                    return True\n    else:\n        return self.solve(board, i, (j + 1))\n    return False\n", "label": 1}
{"function": "\n\ndef read(self, size=1):\n    '        Read size bytes from the serial port. If a timeout is set it may\\n        return less characters as requested. With no timeout it will block\\n        until the requested number of bytes is read.\\n        '\n    if (not self.is_open):\n        raise portNotOpenError\n    if ((self._timeout is not None) and (self._timeout != 0)):\n        timeout = (time.time() + self._timeout)\n    else:\n        timeout = None\n    data = bytearray()\n    while ((size > 0) and self.is_open):\n        try:\n            b = self.queue.get(timeout=self._timeout)\n        except queue.Empty:\n            if (self._timeout == 0):\n                break\n        else:\n            if (data is not None):\n                data += b\n                size -= 1\n            else:\n                break\n        if (timeout and (time.time() > timeout)):\n            if self.logger:\n                self.logger.info('read timeout')\n            break\n    return bytes(data)\n", "label": 1}
{"function": "\n\ndef before_after_sort(items):\n    \" Sort a sequence of items with 'before', 'after', and 'id' attributes.\\n        \\n    The sort is topological. If an item does not specify a 'before' or 'after',\\n    it is placed after the preceding item.\\n\\n    If a cycle is found in the dependencies, a warning is logged and the order\\n    of the items is undefined.\\n    \"\n    if (len(items) < 2):\n        return items\n    item_map = dict(((item.id, item) for item in items if item.id))\n    pairs = []\n    prev_item = None\n    for item in items:\n        new_pairs = []\n        if (hasattr(item, 'before') and item.before):\n            (parent, child) = (item, item_map.get(item.before))\n            if child:\n                new_pairs.append((parent, child))\n        if (hasattr(item, 'after') and item.after):\n            (parent, child) = (item_map.get(item.after), item)\n            if parent:\n                new_pairs.append((parent, child))\n        if new_pairs:\n            pairs.extend(new_pairs)\n        else:\n            if prev_item:\n                pairs.append((prev_item, item))\n            prev_item = item\n    (result, has_cycle) = topological_sort(pairs)\n    if has_cycle:\n        logger.warning('Cycle in before/after sort for items %r', items)\n    return result\n", "label": 1}
{"function": "\n\ndef test_goaa_happy_path(self):\n    (parser, opts, files) = self._run_check(['../gristle_file_converter.py', 'census.csv', '-d', ',', '-D', '|'], 'pass')\n    assert (opts.output is None)\n    assert (opts.recdelimiter is None)\n    assert (opts.delimiter == ',')\n    assert (opts.out_delimiter == '|')\n    assert (opts.recdelimiter is None)\n    assert (opts.out_recdelimiter is None)\n    assert (opts.quoting is False)\n    assert (opts.out_quoting is False)\n    assert (opts.quotechar == '\"')\n    assert (opts.hasheader is False)\n    assert (opts.out_hasheader is False)\n    assert (opts.stripfields is False)\n    assert (opts.verbose is True)\n    self._run_check(['../gristle_file_converter.py', 'census4.csv', '-D', '|'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census5.csv', '-d', ',', '-D', '|'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-d', ',', '-D', '|', '--hasheader', '--outhasheader'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-D', '|', '-r', '-R', '-q', '-Q', '--quotechar', '^'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-D', '|', '--stripfields'], 'pass')\n", "label": 1}
{"function": "\n\ndef linkSamplePredicate(subsample_size, predicate, items1, items2):\n    sample = []\n    predicate_function = predicate.func\n    field = predicate.field\n    red = defaultdict(list)\n    blue = defaultdict(list)\n    for (i, (index, record)) in enumerate(interleave(items1, items2)):\n        if (i == 20000):\n            if ((min(len(red), len(blue)) + len(sample)) < 10):\n                return sample\n        column = record[field]\n        if (not column):\n            (red, blue) = (blue, red)\n            continue\n        block_keys = predicate_function(column)\n        for block_key in block_keys:\n            if blue.get(block_key):\n                pair = sort_pair(blue[block_key].pop(), index)\n                sample.append(pair)\n                subsample_size -= 1\n                if subsample_size:\n                    break\n                else:\n                    return sample\n            else:\n                red[block_key].append(index)\n        (red, blue) = (blue, red)\n    for (index, record) in itertools.islice(items2, len(items1)):\n        column = record[field]\n        if (not column):\n            continue\n        block_keys = predicate_function(column)\n        for block_key in block_keys:\n            if red.get(block_key):\n                pair = sort_pair(red[block_key].pop(), index)\n                sample.append(pair)\n                subsample_size -= 1\n                if subsample_size:\n                    break\n                else:\n                    return sample\n    return sample\n", "label": 1}
{"function": "\n\ndef dataReceived(self, data):\n    chunk = StringIO()\n    for char in data:\n        if self.gotIAC:\n            if self.iacByte:\n                if (self.iacByte == SB):\n                    if (char == SE):\n                        self.iacSBchunk(chunk.getvalue())\n                        chunk = StringIO()\n                        del self.iacByte\n                        del self.gotIAC\n                    else:\n                        chunk.write(char)\n                else:\n                    try:\n                        getattr(self, ('iac_%s' % iacBytes[self.iacByte]))(char)\n                    except KeyError:\n                        pass\n                    del self.iacByte\n                    del self.gotIAC\n            else:\n                self.iacByte = char\n        elif (char == IAC):\n            c = chunk.getvalue()\n            if c:\n                why = self.processChunk(c)\n                if why:\n                    return why\n                chunk = StringIO()\n            self.gotIAC = 1\n        else:\n            chunk.write(char)\n    c = chunk.getvalue()\n    if c:\n        why = self.processChunk(c)\n        if why:\n            return why\n", "label": 1}
{"function": "\n\ndef dispatch(self, pdu):\n    if isinstance(pdu, Symmetry):\n        return\n    if isinstance(pdu, AggregatedFrame):\n        if ((pdu.dsap == 0) and (pdu.ssap == 0)):\n            [log.debug(('     ' + str(p))) for p in pdu]\n            [self.dispatch(p) for p in pdu]\n        return\n    if (isinstance(pdu, Connect) and (pdu.dsap == 1)):\n        addr = self.snl.get(pdu.sn)\n        if ((not addr) or (self.sap[addr] is None)):\n            log.debug(\"no service named '{0}'\".format(pdu.sn))\n            pdu = DisconnectedMode(pdu.ssap, 1, reason=2)\n            self.sap[1].dmpdu.append(pdu)\n            return\n        pdu = Connect(dsap=addr, ssap=pdu.ssap, rw=pdu.rw, miu=pdu.miu)\n    with self.lock:\n        sap = self.sap[pdu.dsap]\n        if sap:\n            sap.enqueue(pdu)\n            return\n    log.debug('discard PDU {0}'.format(str(pdu)))\n    return\n", "label": 1}
{"function": "\n\ndef summary(self):\n    tpg = self.rtsnode\n    status = None\n    msg = []\n    if tpg.has_feature('nexus'):\n        msg.append(str(self.rtsnode.nexus))\n    if (not tpg.enable):\n        return ('disabled', False)\n    if tpg.has_feature('acls'):\n        if (('generate_node_acls' in tpg.list_attributes()) and int(tpg.get_attribute('generate_node_acls'))):\n            msg.append('gen-acls')\n        else:\n            msg.append('no-gen-acls')\n        if tpg.has_feature('auth'):\n            if (not int(tpg.get_attribute('authentication'))):\n                msg.append('no-auth')\n                if int(tpg.get_attribute('generate_node_acls')):\n                    status = True\n            elif (not int(tpg.get_attribute('generate_node_acls'))):\n                msg.append('auth per-acl')\n            else:\n                msg.append('tpg-auth')\n                status = True\n                if (not (tpg.chap_password and tpg.chap_userid)):\n                    status = False\n                if tpg.authenticate_target:\n                    msg.append('mutual auth')\n                else:\n                    msg.append('1-way auth')\n    return (', '.join(msg), status)\n", "label": 1}
{"function": "\n\ndef alignment_is_good(record, a, arange):\n    trimmed_slen = (arange.end - arange.start)\n    if (trimmed_slen < 0):\n        sys.exit('BUG: trimmed_slen bad value\\n')\n    qury_wiggle = (record.qlen * WIGGLE_PCT)\n    if (not (fabs((a.send - a.sstart)) > MIN_ALIGNMENT_SIZE)):\n        return False\n    if ((fabs((a.send - a.sstart)) >= (trimmed_slen * CONTAINED_PCT)) and (record.qlen > record.slen)):\n        return True\n    if ((fabs((a.qend - a.qstart)) > (record.qlen * CONTAINED_PCT)) and (record.slen > record.qlen)):\n        return True\n    if (((a.sstart == arange.start) or (arange.end == a.send)) and ((a.qstart < qury_wiggle) or (a.qend < qury_wiggle) or ((record.qlen - a.qstart) < qury_wiggle) or ((record.qlen - a.qend) < qury_wiggle))):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef check_if_doc_is_linked(doc, method='Delete'):\n    '\\n\\t\\tRaises excption if the given doc(dt, dn) is linked in another record.\\n\\t'\n    from frappe.model.rename_doc import get_link_fields\n    link_fields = get_link_fields(doc.doctype)\n    link_fields = [[lf['parent'], lf['fieldname'], lf['issingle']] for lf in link_fields]\n    for (link_dt, link_field, issingle) in link_fields:\n        if (not issingle):\n            item = frappe.db.get_value(link_dt, {\n                link_field: doc.name,\n            }, ['name', 'parent', 'parenttype', 'docstatus'], as_dict=True)\n            if (item and ((item.parent or item.name) != doc.name) and (((method == 'Delete') and (item.docstatus < 2)) or ((method == 'Cancel') and (item.docstatus == 1)))):\n                frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, (item.parenttype if item.parent else link_dt), (item.parent or item.name)), frappe.LinkExistsError)\n", "label": 1}
{"function": "\n\ndef match(self, interp, block, typemap, calltypes):\n    '\\n        Look for potential macros for expand and store their expansions.\\n        '\n    self.block = block\n    self.rewrites = rewrites = {\n        \n    }\n    for inst in block.body:\n        if isinstance(inst, ir.Assign):\n            rhs = inst.value\n            if (isinstance(rhs, ir.Expr) and (rhs.op == 'call') and isinstance(rhs.func, ir.Var)):\n                try:\n                    const = interp.infer_constant(rhs.func)\n                except errors.ConstantInferenceError:\n                    continue\n                if isinstance(const, Macro):\n                    assert const.callable\n                    new_expr = self._expand_callable_macro(interp, rhs, const, rhs.loc)\n                    rewrites[rhs] = new_expr\n            elif (isinstance(rhs, ir.Expr) and (rhs.op == 'getattr')):\n                try:\n                    const = interp.infer_constant(inst.target)\n                except errors.ConstantInferenceError:\n                    continue\n                if (isinstance(const, Macro) and (not const.callable)):\n                    new_expr = self._expand_non_callable_macro(const, rhs.loc)\n                    rewrites[rhs] = new_expr\n    return (len(rewrites) > 0)\n", "label": 1}
{"function": "\n\ndef temp_name(self, expr):\n    c = expr.__class__\n    if (c is PrimCall):\n        return expr.prim.name\n    elif (c is Attribute):\n        if (expr.value.__class__ is Var):\n            return ((names.original(expr.value.name) + '_') + expr.name)\n        else:\n            return expr.name\n    elif (c is Attribute):\n        if (expr.value.__class__ is Var):\n            return ('%s_%s' % (expr.value.name, expr.name))\n        else:\n            return expr.name\n    elif (c is Index):\n        idx_t = expr.index.type\n        if (isinstance(idx_t, SliceT) or (isinstance(idx_t, TupleT) and any((isinstance(elt_t, SliceT) for elt_t in idx_t.elt_types)))):\n            return 'slice'\n        else:\n            return 'elt'\n    elif (c is TupleProj):\n        if (expr.tuple.__class__ is Var):\n            original = names.original(expr.tuple.name)\n            return ('%s%d_elt%d' % (original, names.versions[original], expr.index))\n        else:\n            return ('tuple_elt%d' % expr.index)\n    elif (c is Var):\n        return names.refresh(expr.name)\n    else:\n        return 'temp'\n", "label": 1}
{"function": "\n\ndef on_ssl(self, client_hello):\n    anon_ciphers = [str(c) for c in client_hello.ciphers if ('_anon_' in str(c))]\n    if anon_ciphers:\n        self._handle_bad_ciphers(anon_ciphers, ('Client enabled anonymous TLS/SSL cipher suites %s' % ', '.join(anon_ciphers)))\n    null_ciphers = [str(c) for c in client_hello.ciphers if ('_WITH_NULL_' in str(c))]\n    if null_ciphers:\n        self._handle_bad_ciphers(null_ciphers, ('Client enabled NULL encryption TLS/SSL cipher suites %s' % ', '.join(null_ciphers)))\n    integ_ciphers = [str(c) for c in client_hello.ciphers if str(c).endswith('_NULL')]\n    if integ_ciphers:\n        self._handle_bad_ciphers(integ_ciphers, ('Client enabled NULL integrity TLS/SSL cipher suites %s' % ', '.join(integ_ciphers)))\n    export_ciphers = [str(c) for c in client_hello.ciphers if ('EXPORT' in str(c))]\n    if export_ciphers:\n        self._handle_bad_ciphers(integ_ciphers, ('Client enabled export TLS/SSL cipher suites %s' % ', '.join(export_ciphers)))\n", "label": 1}
{"function": "\n\ndef _parseOptions(options):\n    opts = {\n        \n    }\n    if ('url' not in options):\n        raise Exception('URL needed')\n    else:\n        opts['url'] = options['url']\n    if ('compression' in options):\n        value = options['compression'].lower().strip()\n        if (value == 'true'):\n            opts['enableCompression'] = True\n        elif (value == 'false'):\n            opts['enableCompression'] = False\n        else:\n            raise Exception(\"invalid value '{0}' for compression\".format(value))\n    if ('autofrag' in options):\n        try:\n            value = int(options['autofrag'])\n        except:\n            raise Exception(\"invalid value '{0}' for autofrag\".format(options['autofrag']))\n        if (value < 0):\n            raise Exception(\"negative value '{0}' for autofrag\".format(value))\n        opts['autoFragmentSize'] = value\n    if ('subprotocol' in options):\n        value = options['subprotocol'].lower().strip()\n        opts['subprotocol'] = value\n    if ('debug' in options):\n        value = options['debug'].lower().strip()\n        if (value == 'true'):\n            opts['debug'] = True\n        elif (value == 'false'):\n            opts['debug'] = False\n        else:\n            raise Exception(\"invalid value '{0}' for debug\".format(value))\n    return opts\n", "label": 1}
{"function": "\n\ndef take_action(self, parsed_args):\n    client = self.get_client()\n    extra_values = v2_0.parse_args_to_dict(self.values_specs)\n    if extra_values:\n        raise exceptions.CommandError((_('Invalid argument(s): --%s') % ', --'.join(extra_values)))\n    tenant_id = (parsed_args.tenant_id or parsed_args.pos_tenant_id)\n    if parsed_args.dry_run:\n        data = client.validate_auto_allocated_topology_requirements(tenant_id)\n    else:\n        data = client.get_auto_allocated_topology(tenant_id)\n    if (self.resource in data):\n        for (k, v) in data[self.resource].items():\n            if isinstance(v, list):\n                value = ''\n                for _item in v:\n                    if value:\n                        value += '\\n'\n                    if isinstance(_item, dict):\n                        value += jsonutils.dumps(_item)\n                    else:\n                        value += str(_item)\n                data[self.resource][k] = value\n            elif (v == 'dry-run=pass'):\n                return (('dry-run',), ('pass',))\n            elif (v is None):\n                data[self.resource][k] = ''\n        return zip(*sorted(data[self.resource].items()))\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef tostr(object, encoding=None):\n    ' get a unicode safe string representation of an object '\n    if isinstance(object, basestring):\n        if (encoding is None):\n            return object\n        else:\n            return object.encode(encoding)\n    if isinstance(object, tuple):\n        s = ['(']\n        for item in object:\n            if isinstance(item, basestring):\n                s.append(item)\n            else:\n                s.append(tostr(item))\n            s.append(', ')\n        s.append(')')\n        return ''.join(s)\n    if isinstance(object, list):\n        s = ['[']\n        for item in object:\n            if isinstance(item, basestring):\n                s.append(item)\n            else:\n                s.append(tostr(item))\n            s.append(', ')\n        s.append(']')\n        return ''.join(s)\n    if isinstance(object, dict):\n        s = ['{']\n        for item in object.items():\n            if isinstance(item[0], basestring):\n                s.append(item[0])\n            else:\n                s.append(tostr(item[0]))\n            s.append(' = ')\n            if isinstance(item[1], basestring):\n                s.append(item[1])\n            else:\n                s.append(tostr(item[1]))\n            s.append(', ')\n        s.append('}')\n        return ''.join(s)\n    try:\n        return unicode(object)\n    except:\n        return str(object)\n", "label": 1}
{"function": "\n\ndef test_create_alt(scratch_tree, scratch_pad):\n    sess = scratch_tree.edit('/', alt='de')\n    assert (sess.id == '')\n    assert (sess.path == '/')\n    assert (sess.record is not None)\n    assert (sess['_model'] == 'page')\n    assert (sess['title'] == 'Index')\n    assert (sess['body'] == 'Hello World!')\n    sess['body'] = 'Hallo Welt!'\n    sess.commit()\n    assert sess.closed\n    with open(sess.get_fs_path(alt='de')) as f:\n        assert (f.read().splitlines() == ['body: Hallo Welt!'])\n    scratch_pad.cache.flush()\n    item = scratch_pad.get('/', alt='de')\n    assert (item['_slug'] == '')\n    assert (item['title'] == 'Index')\n    assert (item['body'].source == 'Hallo Welt!')\n    assert (item['_model'] == 'page')\n", "label": 1}
{"function": "\n\ndef parse_docstring(self, s, pnames=None):\n    free_text = []\n    header = []\n    label = None\n    last_argname = None\n    for p in split_docstring(s):\n        argdoc = self.argdoc_re.match(p)\n        if argdoc:\n            (argname, text) = argdoc.groups()\n            if free_text:\n                if free_text[(- 1)].endswith(':'):\n                    label = free_text.pop()\n                if last_argname:\n                    if ((pnames is None) or (last_argname in pnames)):\n                        self.after[last_argname] = free_text\n                else:\n                    header.extend(free_text)\n                free_text = []\n            last_argname = argname\n            try:\n                default_label = self.get_param_type(self.signature.parameters[argname])\n            except KeyError:\n                continue\n            if ((pnames is not None) and (argname not in pnames)):\n                continue\n            if (default_label != LABEL_POS):\n                try:\n                    (param, _) = self.sections[default_label].pop(argname)\n                except KeyError:\n                    continue\n                label_ = (label or default_label)\n                if (label_ not in self.sections):\n                    self.sections[label_] = util.OrderedDict()\n            else:\n                try:\n                    (param, _) = self.sections[default_label][argname]\n                except KeyError:\n                    continue\n                label_ = default_label\n            self.sections[label_][argname] = (param, text)\n        else:\n            free_text.append(p)\n    if (not last_argname):\n        header = free_text\n        footer = []\n    else:\n        footer = free_text\n    return (lines_to_paragraphs(header), lines_to_paragraphs(footer))\n", "label": 1}
{"function": "\n\ndef get_value(self, context, *tag_args, **tag_kwargs):\n    request = self.get_request(context)\n    output = None\n    (slot,) = tag_args\n    template_name = (tag_kwargs.get('template') or None)\n    cachable = is_true(tag_kwargs.get('cachable', (not bool(template_name))))\n    if (template_name and cachable and (not extract_literal(self.kwargs['template']))):\n        raise TemplateSyntaxError(\"{0} tag does not allow 'cachable' for variable template names!\".format(self.tag_name))\n    try_cache = (appsettings.FLUENT_CONTENTS_CACHE_OUTPUT and appsettings.FLUENT_CONTENTS_CACHE_PLACEHOLDER_OUTPUT and cachable)\n    if isinstance(slot, SharedContent):\n        sharedcontent = slot\n        if try_cache:\n            cache_key = get_shared_content_cache_key(sharedcontent)\n            output = cache.get(cache_key)\n    else:\n        site = Site.objects.get_current()\n        if try_cache:\n            cache_key_ptr = get_shared_content_cache_key_ptr(int(site.pk), slot, language_code=get_language())\n            cache_key = cache.get(cache_key_ptr)\n            if (cache_key is not None):\n                output = cache.get(cache_key)\n        if (output is None):\n            try:\n                sharedcontent = SharedContent.objects.parent_site(site).get(slug=slot)\n            except SharedContent.DoesNotExist:\n                return \"<!-- shared content '{0}' does not yet exist -->\".format(slot)\n            if (try_cache and (not cache_key)):\n                cache.set(cache_key_ptr, get_shared_content_cache_key(sharedcontent))\n    if (output is None):\n        output = self.render_shared_content(request, sharedcontent, template_name, cachable=cachable)\n    rendering.register_frontend_media(request, output.media)\n    return output.html\n", "label": 1}
{"function": "\n\ndef __lt__(self, other):\n    if (self.date == 'infinity'):\n        return False\n    if isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return True\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) < other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date < other.date.replace(tzinfo=self.tz))\n        return (self.date < other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            if (other.end.tz and (self.tz is None)):\n                return (self.date.replace(tzinfo=other.end.tz) < other.end.date)\n            elif (self.tz and (other.end.tz is None)):\n                return (self.date < other.end.date.replace(tzinfo=self.tz))\n            return (self.date < other.end.date)\n        else:\n            return self.__lt__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef log(self, sender, message, channel):\n    sender = sender[:10]\n    self.word_table.setdefault(sender, {\n        \n    })\n    if message.startswith('/'):\n        return\n    try:\n        say_something = (self.is_ping(message) or ((sender != self.conn.nick) and (random.random() < self.chattiness)))\n    except AttributeError:\n        say_something = False\n    messages = []\n    seed_key = None\n    if self.is_ping(message):\n        message = self.fix_ping(message)\n    for words in self.split_message(self.sanitize_message(message)):\n        key = tuple(words[:(- 1)])\n        if (key in self.word_table):\n            self.word_table[sender][key].append(words[(- 1)])\n        else:\n            self.word_table[sender][key] = [words[(- 1)]]\n        if ((self.stop_word not in key) and say_something):\n            for person in self.word_table:\n                if (person == sender):\n                    continue\n                if (key in self.word_table[person]):\n                    generated = self.generate_message(person, seed_key=key)\n                    if generated:\n                        messages.append((person, generated))\n    if len(messages):\n        (self.last, message) = random.choice(messages)\n        return message\n", "label": 1}
{"function": "\n\ndef get_line_content(self, patch_content, line_number):\n    content = ''\n    content_list = []\n    lines = patch_content.split('\\n')\n    original_line = 0\n    new_line = 0\n    original_end = 0\n    new_end = 0\n    last_content_added = ''\n    for line in lines:\n        if re.match('^@@(\\\\s|\\\\+|\\\\-|\\\\d|,)+@@', line, re.M):\n            begin = self.get_file_modification_begin(line)\n            original_line = begin[0]\n            new_line = begin[1]\n            end = self.get_file_modification_end(line)\n            deletion_end = end[0]\n            addition_end = end[1]\n        else:\n            if (deletion_end > line_number):\n                if (len(content_list) == 2):\n                    break\n                if ((original_line > line_number) and (new_line > line_number)):\n                    break\n            elif (new_line > line_number):\n                break\n            if re.match('^\\\\+.*', line, re.M):\n                if (new_line == line_number):\n                    content_list.append({\n                        'type': 'addition',\n                        'text': line[1:],\n                    })\n                    last_content_added = 'addition'\n                new_line += 1\n            elif re.match('^\\\\-.*', line, re.M):\n                if (original_line == line_number):\n                    content_list.append({\n                        'type': 'deletion',\n                        'text': line[1:],\n                    })\n                    last_content_added = 'deletion'\n                original_line += 1\n            elif ((line != '\\\\ No newline at end of file') and (line != '')):\n                original_line += 1\n                new_line += 1\n    if content_list:\n        content = content_list[(- 1)]\n    return content\n", "label": 1}
{"function": "\n\ndef execute_plugin_method_series(self, name, args=None, kwargs=None, single_response=False):\n    if (args is None):\n        args = []\n        use_kwargs = True\n    if (kwargs is None):\n        kwargs = {\n            \n        }\n        use_kwargs = False\n    if (use_kwargs and single_response):\n        raise RuntimeError('When executing plugins in series using `single` response mode, you must specify only args.')\n    elif (args and kwargs):\n        raise RuntimeError('Plugins can be ran in series using either args or kwargs, not both.')\n    for plugin in self.plugins:\n        if (not hasattr(plugin, name)):\n            continue\n        method = getattr(plugin, name)\n        plugin_result = method(*args, **kwargs)\n        if (plugin_result is not None):\n            if use_kwargs:\n                kwargs = plugin_result\n            elif single_response:\n                args = (plugin_result,)\n            else:\n                args = plugin_result\n    if use_kwargs:\n        return kwargs\n    elif single_response:\n        return args[0]\n    return args\n", "label": 1}
{"function": "\n\ndef _build_suggestions(request, cat, suggester):\n    results = []\n    q = request.GET.get('q')\n    if (q and (q.isdigit() or (len(q) > 2))):\n        q_ = q.lower()\n        if (cat != 'apps'):\n            for a in amo.APP_USAGE:\n                name_ = unicode(a.pretty).lower()\n                word_matches = [w for w in q_.split() if (name_ in w)]\n                if ((q_ in name_) or word_matches):\n                    results.append({\n                        'id': a.id,\n                        'name': _('{0} Add-ons').format(a.pretty),\n                        'url': locale_url(a.short),\n                        'cls': ('app ' + a.short),\n                    })\n        cats = Category.objects\n        cats = cats.filter((Q(application=request.APP.id) | Q(type=amo.ADDON_SEARCH)))\n        if (cat == 'themes'):\n            cats = cats.filter(type=amo.ADDON_PERSONA)\n        else:\n            cats = cats.exclude(type=amo.ADDON_PERSONA)\n        for c in cats:\n            if (not c.name):\n                continue\n            name_ = unicode(c.name).lower()\n            word_matches = [w for w in q_.split() if (name_ in w)]\n            if ((q_ in name_) or word_matches):\n                results.append({\n                    'id': c.id,\n                    'name': unicode(c.name),\n                    'url': c.get_url_path(),\n                    'cls': 'cat',\n                })\n        results += suggester.items\n    return results\n", "label": 1}
{"function": "\n\ndef visit_Module(self, node):\n    params = node.get_params()\n    for param in params.values():\n        if (param.width is not None):\n            param.width = self.replace_visitor.visit(param.width)\n        param.value = self.replace_visitor.visit(param.value)\n    localparams = node.get_localparams()\n    for localparam in localparams.values():\n        if (localparam.width is not None):\n            localparam.width = self.replace_visitor.visit(localparam.width)\n        localparam.value = self.replace_visitor.visit(localparam.value)\n    ports = node.get_ports()\n    for port in ports.values():\n        if (port.width is not None):\n            port.width = self.replace_visitor.visit(port.width)\n    vars = node.get_vars()\n    for var in vars.values():\n        if (var.width is not None):\n            var.width = self.replace_visitor.visit(var.width)\n    for asg in node.assign:\n        self.visit(asg)\n    for alw in node.always:\n        self.visit(alw)\n    for ini in node.initial:\n        self.visit(ini)\n    for ins in node.instance.values():\n        self.visit(ins)\n    return node\n", "label": 1}
{"function": "\n\ndef test_unix_domain_adapter_monkeypatch():\n    with UnixSocketServerThread() as usock_thread:\n        with requests_unixsocket.monkeypatch('http+unix://'):\n            urlencoded_usock = requests.compat.quote_plus(usock_thread.usock)\n            url = ('http+unix://%s/path/to/page' % urlencoded_usock)\n            for method in ['get', 'post', 'head', 'patch', 'put', 'delete', 'options']:\n                logger.debug('Calling session.%s(%r) ...', method, url)\n                r = getattr(requests, method)(url)\n                logger.debug('Received response: %r with text: %r and headers: %r', r, r.text, r.headers)\n                assert (r.status_code == 200)\n                assert (r.headers['server'] == 'waitress')\n                assert (r.headers['X-Transport'] == 'unix domain socket')\n                assert (r.headers['X-Requested-Path'] == '/path/to/page')\n                assert (r.headers['X-Socket-Path'] == usock_thread.usock)\n                assert isinstance(r.connection, requests_unixsocket.UnixAdapter)\n                assert (r.url == url)\n                if (method == 'head'):\n                    assert (r.text == '')\n                else:\n                    assert (r.text == 'Hello world!')\n    for method in ['get', 'post', 'head', 'patch', 'put', 'delete', 'options']:\n        with pytest.raises(requests.exceptions.InvalidSchema):\n            getattr(requests, method)(url)\n", "label": 1}
{"function": "\n\ndef __getattr__(self, name):\n    if (name in ('_mock_methods', '_mock_unsafe')):\n        raise AttributeError(name)\n    elif (self._mock_methods is not None):\n        if ((name not in self._mock_methods) or (name in _all_magics)):\n            raise AttributeError(('Mock object has no attribute %r' % name))\n    elif _is_magic(name):\n        raise AttributeError(name)\n    if (not self._mock_unsafe):\n        if name.startswith(('assert', 'assret')):\n            raise AttributeError(name)\n    result = self._mock_children.get(name)\n    if (result is _deleted):\n        raise AttributeError(name)\n    elif (result is None):\n        wraps = None\n        if (self._mock_wraps is not None):\n            wraps = getattr(self._mock_wraps, name)\n        result = self._get_child_mock(parent=self, name=name, wraps=wraps, _new_name=name, _new_parent=self)\n        self._mock_children[name] = result\n    elif isinstance(result, _SpecState):\n        result = create_autospec(result.spec, result.spec_set, result.instance, result.parent, result.name)\n        self._mock_children[name] = result\n    return result\n", "label": 1}
{"function": "\n\ndef GetObjectMetadata(self, bucket_name, object_name, generation=None, provider=None, fields=None):\n    'See CloudApi class for function doc strings.'\n    if generation:\n        generation = long(generation)\n    if (bucket_name in self.buckets):\n        bucket = self.buckets[bucket_name]\n        if ((object_name in bucket.objects) and bucket.objects[object_name]):\n            if generation:\n                if ('versioned' in bucket.objects[object_name]):\n                    for obj in bucket.objects[object_name]['versioned']:\n                        if (obj.root_object.generation == generation):\n                            return obj.root_object\n                if ('live' in bucket.objects[object_name]):\n                    if (bucket.objects[object_name]['live'].root_object.generation == generation):\n                        return bucket.objects[object_name]['live'].root_object\n            elif ('live' in bucket.objects[object_name]):\n                return bucket.objects[object_name]['live'].root_object\n        raise CreateObjectNotFoundException(404, self.provider, bucket_name, object_name)\n    raise CreateBucketNotFoundException(404, self.provider, bucket_name)\n", "label": 1}
{"function": "\n\ndef _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):\n    '\\n        Returns context_lines before and after lineno from file.\\n        Returns (pre_context_lineno, pre_context, context_line, post_context).\\n        '\n    source = None\n    if ((loader is not None) and hasattr(loader, 'get_source')):\n        try:\n            source = loader.get_source(module_name)\n        except ImportError:\n            pass\n        if (source is not None):\n            source = source.splitlines()\n    if (source is None):\n        try:\n            with open(filename, 'rb') as fp:\n                source = fp.read().splitlines()\n        except (OSError, IOError):\n            pass\n    if (source is None):\n        return (None, [], None, [])\n    if isinstance(source[0], six.binary_type):\n        encoding = 'ascii'\n        for line in source[:2]:\n            match = re.search(b'coding[:=]\\\\s*([-\\\\w.]+)', line)\n            if match:\n                encoding = match.group(1).decode('ascii')\n                break\n        source = [six.text_type(sline, encoding, 'replace') for sline in source]\n    lower_bound = max(0, (lineno - context_lines))\n    upper_bound = (lineno + context_lines)\n    pre_context = source[lower_bound:lineno]\n    context_line = source[lineno]\n    post_context = source[(lineno + 1):upper_bound]\n    return (lower_bound, pre_context, context_line, post_context)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = Results()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = QueryNotFoundException()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.error2 = BeeswaxException()\n                self.error2.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef login(self, request):\n    '\\n        Displays the login form for the given HttpRequest.\\n        '\n    from django.contrib.auth.models import User\n    if (not request.POST.has_key(LOGIN_FORM_KEY)):\n        if request.POST:\n            message = _('Please log in again, because your session has expired.')\n        else:\n            message = ''\n        return self.display_login_form(request, message)\n    if (not request.session.test_cookie_worked()):\n        message = _(\"Looks like your browser isn't configured to accept cookies. Please enable cookies, reload this page, and try again.\")\n        return self.display_login_form(request, message)\n    else:\n        request.session.delete_test_cookie()\n    username = request.POST.get('username', None)\n    password = request.POST.get('password', None)\n    user = authenticate(username=username, password=password)\n    if (user is None):\n        message = ERROR_MESSAGE\n        if ((username is not None) and ('@' in username)):\n            try:\n                user = User.objects.get(email=username)\n            except (User.DoesNotExist, User.MultipleObjectsReturned):\n                pass\n            else:\n                if user.check_password(password):\n                    message = (_(\"Your e-mail address is not your username. Try '%s' instead.\") % user.username)\n        return self.display_login_form(request, message)\n    elif (user.is_active and user.is_staff):\n        login(request, user)\n        return http.HttpResponseRedirect(request.get_full_path())\n    else:\n        return self.display_login_form(request, ERROR_MESSAGE)\n", "label": 1}
{"function": "\n\ndef eval_sum(f, limits):\n    from sympy.concrete.delta import deltasummation, _has_simple_delta\n    from sympy.functions import KroneckerDelta\n    (i, a, b) = limits\n    if (f is S.Zero):\n        return S.Zero\n    if (i not in f.free_symbols):\n        return (f * ((b - a) + 1))\n    if (a == b):\n        return f.subs(i, a)\n    if isinstance(f, Piecewise):\n        if (not any(((i in arg.args[1].free_symbols) for arg in f.args))):\n            newargs = []\n            for arg in f.args:\n                newexpr = eval_sum(arg.expr, limits)\n                if (newexpr is None):\n                    return None\n                newargs.append((newexpr, arg.cond))\n            return f.func(*newargs)\n    if (f.has(KroneckerDelta) and _has_simple_delta(f, limits[0])):\n        return deltasummation(f, limits)\n    dif = (b - a)\n    definite = dif.is_Integer\n    if (definite and (dif < 100)):\n        return eval_sum_direct(f, (i, a, b))\n    if isinstance(f, Piecewise):\n        return None\n    value = eval_sum_symbolic(f.expand(), (i, a, b))\n    if (value is not None):\n        return value\n    if definite:\n        return eval_sum_direct(f, (i, a, b))\n", "label": 1}
{"function": "\n\ndef run_typed_fn(fn, args, backend=None):\n    actual_types = tuple((type_conv.typeof(arg) for arg in args))\n    expected_types = fn.input_types\n    assert (actual_types == expected_types), ('Arg type mismatch, expected %s but got %s' % (expected_types, actual_types))\n    if (backend is None):\n        backend = config.backend\n    if (backend == 'c'):\n        return c_backend.run(fn, args)\n    elif (backend == 'openmp'):\n        return openmp_backend.run(fn, args)\n    elif (backend == 'cuda'):\n        from .. import cuda_backend\n        return cuda_backend.run(fn, args)\n    elif (backend == 'llvm'):\n        from ..llvm_backend.llvm_context import global_context\n        from ..llvm_backend import generic_value_to_python\n        from ..llvm_backend import ctypes_to_generic_value, compile_fn\n        lowered_fn = pipeline.lowering.apply(fn)\n        llvm_fn = compile_fn(lowered_fn).llvm_fn\n        ctypes_inputs = [t.from_python(v) for (v, t) in zip(args, expected_types)]\n        gv_inputs = [ctypes_to_generic_value(cv, t) for (cv, t) in zip(ctypes_inputs, expected_types)]\n        exec_engine = global_context.exec_engine\n        gv_return = exec_engine.run_function(llvm_fn, gv_inputs)\n        return generic_value_to_python(gv_return, fn.return_type)\n    elif (backend == 'interp'):\n        from .. import interp\n        fn = pipeline.loopify(fn)\n        return interp.eval_fn(fn, args)\n    else:\n        assert False, ('Unknown backend %s' % backend)\n", "label": 1}
{"function": "\n\ndef _ContractAperture(self, force=False):\n    \"Attempt to contract the aperture.  By calling this it's assume the aperture\\n    needs to be contracted.\\n\\n    The aperture can be contracted if it's current size is larger than the\\n    min size.\\n    \"\n    if (self._pending_endpoints and (not force)):\n        return\n    num_healthy = len([c for c in self._heap[1:] if c.channel.is_open])\n    if (num_healthy > self._min_size):\n        least_loaded_endpoint = None\n        for n in self._heap[1:]:\n            if (n.channel.is_closed and (n.endpoint not in self._pending_endpoints)):\n                least_loaded_endpoint = n.endpoint\n                break\n        if (not least_loaded_endpoint):\n            for n in self._heap[1:]:\n                if (n.endpoint not in self._pending_endpoints):\n                    least_loaded_endpoint = n.endpoint\n                    break\n        if least_loaded_endpoint:\n            self._idle_endpoints.add(least_loaded_endpoint)\n            super(ApertureBalancerSink, self)._RemoveSink(least_loaded_endpoint)\n            self._log.debug(('Contracting aperture to remove %s' % str(least_loaded_endpoint)))\n            self._UpdateSizeVarz()\n", "label": 1}
{"function": "\n\ndef _parse_pre_yarn_history_log(lines):\n    'Collect useful info from a pre-YARN history file.\\n\\n    See :py:func:`_parse_yarn_history_log` for return format.\\n    '\n    result = {\n        \n    }\n    task_to_counters = {\n        \n    }\n    for record in _parse_pre_yarn_history_records(lines):\n        fields = record['fields']\n        if ((record['type'] == 'Job') and ('COUNTERS' in fields)):\n            result['counters'] = _parse_pre_yarn_counters(fields['COUNTERS'])\n        elif ((record['type'] == 'Task') and ('COUNTERS' in fields) and ('TASKID' in fields)):\n            task_id = fields['TASKID']\n            counters = _parse_pre_yarn_counters(fields['COUNTERS'])\n            task_to_counters[task_id] = counters\n        elif ((record['type'] in ('MapAttempt', 'ReduceAttempt')) and ('TASK_ATTEMPT_ID' in fields) and (fields.get('TASK_STATUS') == 'FAILED') and fields.get('ERROR')):\n            result.setdefault('errors', [])\n            result['errors'].append(dict(hadoop_error=dict(message=fields['ERROR'], start_line=record['start_line'], num_lines=record['num_lines']), attempt_id=fields['TASK_ATTEMPT_ID']))\n    if (('counters' not in result) and task_to_counters):\n        result['counters'] = _sum_counters(*task_to_counters.values())\n    return result\n", "label": 1}
{"function": "\n\ndef _non_dominated_front_old(iterable, key=(lambda x: x), allowequality=True):\n    'Return a subset of items from iterable which are not dominated by any\\n    other item in iterable.'\n    items = list(iterable)\n    keys = dict(((i, key(i)) for i in items))\n    dim = len(list(keys.values())[0])\n    if any(((dim != len(k)) for k in list(keys.values()))):\n        raise ValueError('Wrong tuple size.')\n    dominations = collections.defaultdict((lambda : []))\n    for i in items:\n        for j in items:\n            if allowequality:\n                if all(((keys[i][k] < keys[j][k]) for k in range(dim))):\n                    dominations[i].append(j)\n            elif all(((keys[i][k] <= keys[j][k]) for k in range(dim))):\n                dominations[i].append(j)\n    dominates = (lambda i, j: (j in dominations[i]))\n    res = set()\n    items = set(items)\n    for i in items:\n        res.add(i)\n        for j in list(res):\n            if (i is j):\n                continue\n            if dominates(j, i):\n                res.remove(i)\n                break\n            elif dominates(i, j):\n                res.remove(j)\n    return res\n", "label": 1}
{"function": "\n\n@dispatch((tuple, list, set, frozenset))\ndef discover(seq):\n    if (not seq):\n        return (var * string)\n    unite = do_one([unite_identical, unite_base, unite_merge_dimensions])\n    if (all((isinstance(item, (tuple, list)) for item in seq)) and (len(set(map(len, seq))) == 1)):\n        columns = list(zip(*seq))\n        try:\n            types = [unite([discover(data) for data in column]).subshape[0] for column in columns]\n            unite = do_one([unite_identical, unite_merge_dimensions, Tuple])\n            return (len(seq) * unite(types))\n        except AttributeError:\n            pass\n    if all((isinstance(item, dict) for item in seq)):\n        keys = sorted(set.union(*(set(d) for d in seq)))\n        columns = [[item.get(key) for item in seq] for key in keys]\n        try:\n            types = [unite([discover(data) for data in column]).subshape[0] for column in columns]\n            return (len(seq) * Record(list(zip(keys, types))))\n        except AttributeError:\n            pass\n    types = list(map(discover, seq))\n    return do_one([unite_identical, unite_merge_dimensions, Tuple])(types)\n", "label": 1}
{"function": "\n\ndef load_module(self, name):\n    if (not name.startswith(self._vendor_pkg)):\n        raise ImportError((\"Cannot import %s, must be a subpackage of '%s'.\" % (name, self._vendor_name)))\n    if (not ((name == self._vendor_name) or any((name.startswith(pkg) for pkg in self._vendor_pkgs)))):\n        raise ImportError(('Cannot import %s, must be one of %s.' % (name, self._vendor_pkgs)))\n    if (name in sys.modules):\n        return sys.modules[name]\n    try:\n        real_meta_path = sys.meta_path[:]\n        try:\n            sys.meta_path = [m for m in sys.meta_path if (not isinstance(m, VendorAlias))]\n            __import__(name)\n            module = sys.modules[name]\n        finally:\n            for m in sys.meta_path:\n                if (m not in real_meta_path):\n                    real_meta_path.append(m)\n            sys.meta_path = real_meta_path\n    except ImportError:\n        real_name = name[len(self._vendor_pkg):]\n        try:\n            __import__(real_name)\n            module = sys.modules[real_name]\n        except ImportError:\n            raise ImportError((\"No module named '%s'\" % (name,)))\n    sys.modules[name] = module\n    return module\n", "label": 1}
{"function": "\n\ndef get_refs(genome_build, aligner, config):\n    'Retrieve reference genome data from a standard bcbio directory structure.\\n    '\n    ref_collection = tz.get_in(['arvados', 'reference'], config)\n    if (not ref_collection):\n        raise ValueError('Could not find reference collection in bcbio_system YAML for arvados.')\n    cfiles = collection_files(ref_collection, config['arvados'])\n    ref_prefix = None\n    for prefix in ['./%s', './genomes/%s']:\n        cur_prefix = (prefix % genome_build)\n        if any((x.startswith(cur_prefix) for x in cfiles)):\n            ref_prefix = cur_prefix\n            break\n    assert ref_prefix, ('Did not find genome files for %s:\\n%s' % (genome_build, pprint.pformat(cfiles)))\n    out = {\n        \n    }\n    base_targets = (('/%s.fa' % genome_build), '/mainIndex')\n    for dirname in ['seq', 'rtg', aligner]:\n        key = {\n            'seq': 'fasta',\n        }.get(dirname, dirname)\n        cur_files = [x for x in cfiles if x.startswith(('%s/%s/' % (ref_prefix, dirname)))]\n        cur_files = [('keep:%s' % os.path.normpath(os.path.join(ref_collection, x))) for x in cur_files]\n        base_files = [x for x in cur_files if x.endswith(base_targets)]\n        if (len(base_files) > 0):\n            assert (len(base_files) == 1), base_files\n            base_file = base_files[0]\n            del cur_files[cur_files.index(base_file)]\n            out[key] = {\n                'base': base_file,\n                'indexes': cur_files,\n            }\n        else:\n            out[key] = {\n                'indexes': cur_files,\n            }\n    return out\n", "label": 1}
{"function": "\n\ndef make_move(self, coordinates, player):\n    ' Will modify the internal state to represent performing the\\n            specified move for the specified player.\\n        '\n    (x, y) = coordinates\n    moves = [piece.get_position() for piece in self.get_move_pieces(player)]\n    if (coordinates not in moves):\n        raise ValueError\n    if (((x < 0) or (x >= WIDTH)) or ((y < 0) or (y >= HEIGHT))):\n        raise ValueError\n    opponent = get_opponent(player)\n    p = self.pieces[(x + (y * WIDTH))]\n    if (player == WHITE):\n        p.set_white()\n    else:\n        p.set_black()\n    for d in DIRECTIONS:\n        start = ((x + (y * WIDTH)) + d)\n        tile = start\n        to_flip = []\n        if ((tile >= 0) and (tile < (WIDTH * HEIGHT))):\n            while (self.pieces[tile].get_state() != BOARD):\n                to_flip.append(self.pieces[tile])\n                if self.outside_board(tile, d):\n                    break\n                else:\n                    tile += d\n            start_flipping = False\n            for pp in reversed(to_flip):\n                if (not start_flipping):\n                    if (pp.get_state() == opponent):\n                        continue\n                start_flipping = True\n                if (player == WHITE):\n                    pp.set_white()\n                else:\n                    pp.set_black()\n            self.pieces[start].reset_flipped()\n", "label": 1}
{"function": "\n\ndef _hash_internal(method, salt, password):\n    'Internal password hash helper.  Supports plaintext without salt,\\n    unsalted and salted passwords.  In case salted passwords are used\\n    hmac is used.\\n    '\n    if (method == 'plain'):\n        return (password, method)\n    if isinstance(password, text_type):\n        password = password.encode('utf-8')\n    if method.startswith('pbkdf2:'):\n        args = method[7:].split(':')\n        if (len(args) not in (1, 2)):\n            raise ValueError('Invalid number of arguments for PBKDF2')\n        method = args.pop(0)\n        iterations = ((args and int((args[0] or 0))) or DEFAULT_PBKDF2_ITERATIONS)\n        is_pbkdf2 = True\n        actual_method = ('pbkdf2:%s:%d' % (method, iterations))\n    else:\n        is_pbkdf2 = False\n        actual_method = method\n    hash_func = _hash_funcs.get(method)\n    if (hash_func is None):\n        raise TypeError(('invalid method %r' % method))\n    if is_pbkdf2:\n        if (not salt):\n            raise ValueError('Salt is required for PBKDF2')\n        rv = pbkdf2_hex(password, salt, iterations, hashfunc=hash_func)\n    elif salt:\n        if isinstance(salt, text_type):\n            salt = salt.encode('utf-8')\n        rv = hmac.HMAC(salt, password, hash_func).hexdigest()\n    else:\n        h = hash_func()\n        h.update(password)\n        rv = h.hexdigest()\n    return (rv, actual_method)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _get_mixins_(bases):\n    'Returns the type for creating enum members, and the first inherited\\n        enum class.\\n\\n        bases: the tuple of bases that was given to __new__\\n\\n        '\n    if ((not bases) or (Enum is None)):\n        return (object, Enum)\n    member_type = first_enum = None\n    for base in bases:\n        if ((base is not Enum) and issubclass(base, Enum) and base._member_names_):\n            raise TypeError('Cannot extend enumerations')\n    if (not issubclass(base, Enum)):\n        raise TypeError('new enumerations must be created as `ClassName([mixin_type,] enum_type)`')\n    if (not issubclass(bases[0], Enum)):\n        member_type = bases[0]\n        first_enum = bases[(- 1)]\n    else:\n        for base in bases[0].__mro__:\n            if issubclass(base, Enum):\n                if (first_enum is None):\n                    first_enum = base\n            elif (member_type is None):\n                member_type = base\n    return (member_type, first_enum)\n", "label": 1}
{"function": "\n\ndef MoveFiles(rebalance, is_master):\n    'Commit the received files into the database.'\n    loc = data_store.DB.Location()\n    if (not os.path.exists(loc)):\n        return False\n    if (not os.path.isdir(loc)):\n        return False\n    tempdir = _CreateDirectory(loc, rebalance.id)\n    remove_file = _FileWithRemoveList(loc, rebalance)\n    to_remove = []\n    if os.path.exists(remove_file):\n        to_remove = [line.decode('utf8').rstrip('\\n') for line in open(remove_file, 'r')]\n    for fname in to_remove:\n        if (not fname.startswith(loc)):\n            logging.warning('Wrong file to remove: %s', fname)\n            continue\n        if (not os.path.exists(fname)):\n            logging.warning('File does not exist: %s', fname)\n            continue\n        if (not os.path.isfile(fname)):\n            logging.warning('Not a file: %s', fname)\n            continue\n        os.unlink(fname)\n        logging.info('Removing file %s', fname)\n    try:\n        os.unlink(remove_file)\n    except OSError:\n        pass\n    try:\n        _RecMoveFiles(tempdir, loc, '')\n    except OSError:\n        return False\n    if (not is_master):\n        if tempdir.startswith(loc):\n            shutil.rmtree(tempdir)\n    return True\n", "label": 1}
{"function": "\n\ndef do_attribute_consuming_service(conf, spsso):\n    service_description = service_name = None\n    requested_attributes = []\n    acs = conf.attribute_converters\n    req = conf.getattr('required_attributes', 'sp')\n    if req:\n        requested_attributes.extend(do_requested_attribute(req, acs, is_required='true'))\n    opt = conf.getattr('optional_attributes', 'sp')\n    if opt:\n        requested_attributes.extend(do_requested_attribute(opt, acs))\n    try:\n        if conf.description:\n            try:\n                (text, lang) = conf.description\n            except ValueError:\n                text = conf.description\n                lang = 'en'\n            service_description = [md.ServiceDescription(text=text, lang=lang)]\n    except KeyError:\n        pass\n    try:\n        if conf.name:\n            try:\n                (text, lang) = conf.name\n            except ValueError:\n                text = conf.name\n                lang = 'en'\n            service_name = [md.ServiceName(text=text, lang=lang)]\n    except KeyError:\n        pass\n    if requested_attributes:\n        if (not service_name):\n            service_name = [md.ServiceName(text='', lang='en')]\n        ac_serv = md.AttributeConsumingService(index='1', service_name=service_name, requested_attribute=requested_attributes)\n        if service_description:\n            ac_serv.service_description = service_description\n        spsso.attribute_consuming_service = [ac_serv]\n", "label": 1}
{"function": "\n\n@contextfilter\ndef else_filter(ctx, value, prefix=None, suffix=None):\n    try:\n        value = __resolve_options(ctx, value)\n        if (not isinstance(value, Option)):\n            if value:\n                return value\n            prefix = (prefix if (prefix is not None) else '')\n            suffix = (suffix if (suffix is not None) else '')\n            return ('%s%s' % (prefix, suffix))\n        if (value.to_cmd() != ''):\n            return value\n        v = value.get()\n        prefix = (prefix if (prefix is not None) else value.get_opt())\n        suffix = (suffix if (suffix is not None) else '')\n        space = ('' if ((prefix == '') or (v == '') or (prefix[(- 1)] == ' ')) else ' ')\n        return ('%s%s%s%s' % (prefix, space, v, suffix))\n    except:\n        return value\n", "label": 1}
{"function": "\n\ndef expand_to_line(string, startIndex, endIndex):\n    linebreakRe = re.compile('\\\\n')\n    spacesAndTabsRe = re.compile('([ \\\\t]+)')\n    searchIndex = (startIndex - 1)\n    while True:\n        if (searchIndex < 0):\n            newStartIndex = (searchIndex + 1)\n            break\n        char = string[searchIndex:(searchIndex + 1)]\n        if linebreakRe.match(char):\n            newStartIndex = (searchIndex + 1)\n            break\n        else:\n            searchIndex -= 1\n    searchIndex = endIndex\n    while True:\n        if (searchIndex > (len(string) - 1)):\n            newEndIndex = searchIndex\n            break\n        char = string[searchIndex:(searchIndex + 1)]\n        if linebreakRe.match(char):\n            newEndIndex = searchIndex\n            break\n        else:\n            searchIndex += 1\n    s = string[newStartIndex:newEndIndex]\n    r = spacesAndTabsRe.match(s)\n    if (r and (r.end() <= startIndex)):\n        newStartIndex = (newStartIndex + r.end())\n    try:\n        if ((startIndex == newStartIndex) and (endIndex == newEndIndex)):\n            return None\n        else:\n            return utils.create_return_obj(newStartIndex, newEndIndex, string, 'line')\n    except NameError:\n        return None\n", "label": 1}
{"function": "\n\ndef _remotes_on(port, which_end):\n    '\\n    Return a set of ip addrs active tcp connections\\n    '\n    port = int(port)\n    ret = set()\n    proc_available = False\n    for statf in ['/proc/net/tcp', '/proc/net/tcp6']:\n        if os.path.isfile(statf):\n            proc_available = True\n            with salt.utils.fopen(statf, 'rb') as fp_:\n                for line in fp_:\n                    if line.strip().startswith('sl'):\n                        continue\n                    iret = _parse_tcp_line(line)\n                    sl = next(iter(iret))\n                    if (iret[sl][which_end] == port):\n                        ret.add(iret[sl]['remote_addr'])\n    if (not proc_available):\n        if salt.utils.is_sunos():\n            return _sunos_remotes_on(port, which_end)\n        if salt.utils.is_freebsd():\n            return _freebsd_remotes_on(port, which_end)\n        if salt.utils.is_netbsd():\n            return _netbsd_remotes_on(port, which_end)\n        if salt.utils.is_openbsd():\n            return _openbsd_remotes_on(port, which_end)\n        if salt.utils.is_windows():\n            return _windows_remotes_on(port, which_end)\n        return _linux_remotes_on(port, which_end)\n    return ret\n", "label": 1}
{"function": "\n\n@classmethod\ndef cache_url_config(cls, url, backend=None):\n    'Pulled from DJ-Cache-URL, parse an arbitrary Cache URL.\\n\\n        :param url:\\n        :param backend:\\n        :return:\\n        '\n    url = (urlparse.urlparse(url) if (not isinstance(url, cls.URL_CLASS)) else url)\n    location = url.netloc.split(',')\n    if (len(location) == 1):\n        location = location[0]\n    config = {\n        'BACKEND': cls.CACHE_SCHEMES[url.scheme],\n        'LOCATION': location,\n    }\n    if (url.scheme == 'filecache'):\n        config.update({\n            'LOCATION': (url.netloc + url.path),\n        })\n    if (url.path and (url.scheme in ['memcache', 'pymemcache'])):\n        config.update({\n            'LOCATION': ('unix:' + url.path),\n        })\n    elif url.scheme.startswith('redis'):\n        if url.hostname:\n            scheme = url.scheme.replace('cache', '')\n        else:\n            scheme = 'unix'\n        config['LOCATION'] = (((scheme + '://') + url.netloc) + url.path)\n    if url.query:\n        config_options = {\n            \n        }\n        for (k, v) in urlparse.parse_qs(url.query).items():\n            opt = {\n                k.upper(): _cast_int(v[0]),\n            }\n            if (k.upper() in cls._CACHE_BASE_OPTIONS):\n                config.update(opt)\n            else:\n                config_options.update(opt)\n        config['OPTIONS'] = config_options\n    if backend:\n        config['BACKEND'] = backend\n    return config\n", "label": 1}
{"function": "\n\ndef __init__(self, settings_module):\n    for setting in dir(global_settings):\n        if (setting == setting.upper()):\n            setattr(self, setting, getattr(global_settings, setting))\n    self.SETTINGS_MODULE = settings_module\n    try:\n        mod = importlib.import_module(self.SETTINGS_MODULE)\n    except ImportError as e:\n        raise ImportError((\"Could not import settings '%s' (Is it on sys.path?): %s\" % (self.SETTINGS_MODULE, e)))\n    tuple_settings = ('INSTALLED_APPS', 'TEMPLATE_DIRS')\n    for setting in dir(mod):\n        if (setting == setting.upper()):\n            setting_value = getattr(mod, setting)\n            if ((setting in tuple_settings) and isinstance(setting_value, six.string_types)):\n                warnings.warn(('The %s setting must be a tuple. Please fix your settings, as auto-correction is now deprecated.' % setting), PendingDeprecationWarning)\n                setting_value = (setting_value,)\n            setattr(self, setting, setting_value)\n    if (not self.SECRET_KEY):\n        raise ImproperlyConfigured('The SECRET_KEY setting must not be empty.')\n    if (hasattr(time, 'tzset') and self.TIME_ZONE):\n        zoneinfo_root = '/usr/share/zoneinfo'\n        if (os.path.exists(zoneinfo_root) and (not os.path.exists(os.path.join(zoneinfo_root, *self.TIME_ZONE.split('/'))))):\n            raise ValueError(('Incorrect timezone setting: %s' % self.TIME_ZONE))\n        os.environ['TZ'] = self.TIME_ZONE\n        time.tzset()\n", "label": 1}
{"function": "\n\ndef _validate_secure_origin(self, logger, location):\n    parsed = urllib_parse.urlparse(str(location))\n    origin = (parsed.scheme, parsed.hostname, parsed.port)\n    for secure_origin in (SECURE_ORIGINS + self.secure_origins):\n        if ((origin[0] != secure_origin[0]) and (secure_origin[0] != '*')):\n            continue\n        try:\n            addr = ipaddress.ip_address((origin[1] if (isinstance(origin[1], six.text_type) or (origin[1] is None)) else origin[1].decode('utf8')))\n            network = ipaddress.ip_network((secure_origin[1] if isinstance(secure_origin[1], six.text_type) else secure_origin[1].decode('utf8')))\n        except ValueError:\n            if ((origin[1] != secure_origin[1]) and (secure_origin[1] != '*')):\n                continue\n        else:\n            if (addr not in network):\n                continue\n        if ((origin[2] != secure_origin[2]) and (secure_origin[2] != '*') and (secure_origin[2] is not None)):\n            continue\n        return True\n    logger.warning(\"The repository located at %s is not a trusted or secure host and is being ignored. If this repository is available via HTTPS it is recommended to use HTTPS instead, otherwise you may silence this warning and allow it anyways with '--trusted-host %s'.\", parsed.hostname, parsed.hostname)\n    return False\n", "label": 1}
{"function": "\n\ndef parse_body_arguments(content_type, body, arguments, files, headers=None):\n    'Parses a form request body.\\n\\n    Supports ``application/x-www-form-urlencoded`` and\\n    ``multipart/form-data``.  The ``content_type`` parameter should be\\n    a string and ``body`` should be a byte string.  The ``arguments``\\n    and ``files`` parameters are dictionaries that will be updated\\n    with the parsed contents.\\n    '\n    if (headers and ('Content-Encoding' in headers)):\n        gen_log.warning('Unsupported Content-Encoding: %s', headers['Content-Encoding'])\n        return\n    if content_type.startswith('application/x-www-form-urlencoded'):\n        try:\n            uri_arguments = parse_qs_bytes(native_str(body), keep_blank_values=True)\n        except Exception as e:\n            gen_log.warning('Invalid x-www-form-urlencoded body: %s', e)\n            uri_arguments = {\n                \n            }\n        for (name, values) in uri_arguments.items():\n            if values:\n                arguments.setdefault(name, []).extend(values)\n    elif content_type.startswith('multipart/form-data'):\n        fields = content_type.split(';')\n        for field in fields:\n            (k, sep, v) = field.strip().partition('=')\n            if ((k == 'boundary') and v):\n                parse_multipart_form_data(utf8(v), body, arguments, files)\n                break\n        else:\n            gen_log.warning('Invalid multipart/form-data')\n", "label": 1}
{"function": "\n\n@classmethod\ndef wrap(cls, data):\n    should_save = False\n    if ('original_doc' in data):\n        original_doc = data['original_doc']\n        del data['original_doc']\n        should_save = True\n        if original_doc:\n            original_doc = Domain.get_by_name(original_doc)\n            data['copy_history'] = [original_doc._id]\n    if ('license' in data):\n        if (data.get('license', None) == 'public'):\n            data['license'] = 'cc'\n            should_save = True\n    if (('slug' in data) and data['slug']):\n        data['hr_name'] = data['slug']\n        del data['slug']\n    if (('is_test' in data) and isinstance(data['is_test'], bool)):\n        data['is_test'] = ('true' if data['is_test'] else 'false')\n        should_save = True\n    if ('cloudcare_releases' not in data):\n        data['cloudcare_releases'] = 'nostars'\n    if ('location_types' in data):\n        data['obsolete_location_types'] = data.pop('location_types')\n    self = super(Domain, cls).wrap(data)\n    if (self.deployment is None):\n        self.deployment = Deployment()\n    if should_save:\n        self.save()\n    return self\n", "label": 1}
{"function": "\n\ndef select(self, xpath):\n    'Return elements using the webdriver `find_elements_by_xpath` method.\\n\\n        Some XPath features are not supported by the webdriver implementation.\\n        Namely, selecting text content or attributes:\\n          - /some/element/text()\\n          - /some/element/@attribute\\n\\n        This function offers workarounds for both, so it should be safe to use\\n        them as you would with HtmlXPathSelector for simple content extraction.\\n\\n        '\n    xpathev = (self.element if self.element else self.webdriver)\n    ending = _UNSUPPORTED_XPATH_ENDING.match(xpath)\n    atsign = parens = None\n    if ending:\n        (match, atsign, name, parens) = ending.groups()\n        if atsign:\n            xpath = xpath[:((- len(name)) - 2)]\n        elif (parens and (name == 'text')):\n            xpath = xpath[:((- len(name)) - 3)]\n    result = self._make_result(xpathev.find_elements_by_xpath(xpath))\n    if atsign:\n        result = (_NodeAttribute(r.element, name) for r in result)\n    elif (parens and result and (name == 'text')):\n        result = (_TextNode(self.webdriver, r.element) for r in result)\n    return XPathSelectorList(result)\n", "label": 1}
{"function": "\n\ndef permits(self, context, principals, permission):\n    'Returns True or False depending if the token with the specified\\n        principals has access to the given permission.\\n        '\n    principals = set(principals)\n    permissions_required = VIEWS_PERMISSIONS_REQUIRED[permission]\n    current_permissions = set()\n    if principals.intersection(self.model_creators):\n        current_permissions.add('create_model')\n    if principals.intersection(self.token_creators):\n        current_permissions.add('create_token')\n    if principals.intersection(self.token_managers):\n        current_permissions.add('manage_token')\n    model_id = context.model_id\n    if (model_id is not None):\n        try:\n            model_permissions = context.db.get_model_permissions(model_id)\n        except backend_exceptions.ModelNotFound:\n            model_permissions = {\n                \n            }\n            if (permission != 'post_model'):\n                return True\n        finally:\n            for (perm_name, credentials_ids) in iteritems(model_permissions):\n                if principals.intersection(credentials_ids):\n                    current_permissions.add(perm_name)\n    record_id = context.record_id\n    if (record_id is not None):\n        try:\n            authors = context.db.get_record_authors(model_id, record_id)\n        except backend_exceptions.RecordNotFound:\n            authors = []\n        finally:\n            if (not principals.intersection(authors)):\n                current_permissions -= AUTHORS_PERMISSIONS\n    logger.debug('Current permissions: %s', current_permissions)\n    context.request.permissions = current_permissions\n    context.request.principals = principals\n    return permissions_required.matches(current_permissions)\n", "label": 1}
{"function": "\n\ndef _process_files(self, parser, basedir, repository, base_commit_id, request, check_existence=False, limit_to=None):\n    tool = repository.get_scmtool()\n    for f in parser.parse():\n        (source_filename, source_revision) = tool.parse_diff_revision(f.origFile, f.origInfo, moved=f.moved, copied=f.copied)\n        dest_filename = self._normalize_filename(f.newFile, basedir)\n        source_filename = self._normalize_filename(source_filename, basedir)\n        if ((limit_to is not None) and (dest_filename not in limit_to)):\n            continue\n        if ((source_revision != PRE_CREATION) and (source_revision != UNKNOWN) and (not f.binary) and (not f.deleted) and (not f.moved) and (not f.copied) and (check_existence and (not repository.get_file_exists(source_filename, source_revision, base_commit_id=base_commit_id, request=request)))):\n            raise FileNotFoundError(source_filename, source_revision, base_commit_id)\n        f.origFile = source_filename\n        f.origInfo = source_revision\n        f.newFile = dest_filename\n        (yield f)\n", "label": 1}
{"function": "\n\ndef __new__(cls, expr, condition=None, **kwargs):\n    expr = _sympify(expr)\n    if (not kwargs.pop('evaluate', global_evaluate[0])):\n        if (condition is None):\n            obj = Expr.__new__(cls, expr)\n        else:\n            condition = _sympify(condition)\n            obj = Expr.__new__(cls, expr, condition)\n        obj._condition = condition\n        return obj\n    if (not expr.has(RandomSymbol)):\n        return expr\n    if (condition is not None):\n        condition = _sympify(condition)\n    if isinstance(expr, Add):\n        return Add(*[Expectation(a, condition=condition) for a in expr.args])\n    elif isinstance(expr, Mul):\n        rv = []\n        nonrv = []\n        for a in expr.args:\n            if (isinstance(a, RandomSymbol) or a.has(RandomSymbol)):\n                rv.append(a)\n            else:\n                nonrv.append(a)\n        return (Mul(*nonrv) * Expectation(Mul(*rv), condition=condition, evaluate=False))\n    else:\n        if (condition is None):\n            obj = Expr.__new__(cls, expr)\n        else:\n            obj = Expr.__new__(cls, expr, condition)\n        obj._condition = condition\n        return obj\n", "label": 1}
{"function": "\n\ndef find_description(soup):\n    '\\n    '\n    el = soup.find('h3')\n    if (not el):\n        return\n    while (el.name == 'h3'):\n        next_el = el.findNext('h3')\n        if next_el:\n            el = next_el\n        else:\n            break\n    subtitle = el.findNextSibling('b')\n    if subtitle:\n        el = subtitle\n        (yield el.string)\n        el = el.nextSibling.nextSibling\n    el = el.nextSibling\n    while True:\n        if (el.name is not None):\n            return\n        text = el\n        link = el.findNextSibling('a')\n        if link:\n            text += link.string\n            link = link.nextSibling\n            if (link.name is None):\n                text += link.string\n        if text.strip():\n            (yield unicode(text.strip()))\n        for i in range(2):\n            el = el.nextSibling\n            if (not el):\n                return\n            if (el.name != 'br'):\n                break\n        if (el.name == 'br'):\n            el = el.nextSibling\n        if ('Illustrative Examples:' in el):\n            return\n        if ('Cross-References.' in el):\n            return\n", "label": 1}
{"function": "\n\ndef parse_one_cond(tokens, name, context):\n    ((first, pos), tokens) = (tokens[0], tokens[1:])\n    content = []\n    if first.endswith(':'):\n        first = first[:(- 1)]\n    if first.startswith('if '):\n        part = ('if', pos, first[3:].lstrip(), content)\n    elif first.startswith('elif '):\n        part = ('elif', pos, first[5:].lstrip(), content)\n    elif (first == 'else'):\n        part = ('else', pos, None, content)\n    else:\n        assert 0, ('Unexpected token %r at %s' % (first, pos))\n    while 1:\n        if (not tokens):\n            raise TemplateError('No {{endif}}', position=pos, name=name)\n        if (isinstance(tokens[0], tuple) and ((tokens[0][0] == 'endif') or tokens[0][0].startswith('elif ') or (tokens[0][0] == 'else'))):\n            return (part, tokens)\n        (next_chunk, tokens) = parse_expr(tokens, name, context)\n        content.append(next_chunk)\n", "label": 1}
{"function": "\n\ndef subscribe(self, subscriber, timeout=None):\n    \"Must be used with 'yield', as, for example,\\n        'yield channel.subscribe(coro)'.\\n\\n        Subscribe to receive messages. Senders don't need to\\n        subscribe. A message sent to this channel is delivered to all\\n        subscribers.\\n\\n        Can also be used on remote channels.\\n        \"\n    if ((not isinstance(subscriber, Coro)) and (not isinstance(subscriber, Channel))):\n        logger.warning('invalid subscriber ignored')\n        raise StopIteration((- 1))\n    if (self._location == Channel._asyncoro._location):\n        if (subscriber._location != self._location):\n            if isinstance(subscriber, Coro):\n                subscriber._id = int(subscriber._id)\n                for s in self._subscribers:\n                    if (isinstance(s, Coro) and (s._id == subscriber._id) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n            elif isinstance(subscriber, Channel):\n                for s in self._subscribers:\n                    if (isinstance(s, Channel) and (s._name == subscriber._name) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n        self._subscribers.add(subscriber)\n        (yield self._subscribe_event.set())\n        reply = 0\n    else:\n        kwargs = {\n            'channel': self._name,\n        }\n        kwargs['subscriber'] = subscriber\n        request = _NetRequest('subscribe', kwargs=kwargs, dst=self._location, timeout=timeout)\n        reply = (yield Channel._asyncoro._sync_reply(request))\n    raise StopIteration(reply)\n", "label": 1}
{"function": "\n\n@execute_count(3)\ndef test_iteration(self):\n    ' Tests that iterating over a query set pulls back all of the expected results '\n    q = TestModel.objects(test_id=0)\n    compare_set = set([(0, 5), (1, 10), (2, 15), (3, 20)])\n    for t in q:\n        val = (t.attempt_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n    q = TestModel.objects(attempt_id=3).allow_filtering()\n    assert (len(q) == 3)\n    compare_set = set([(0, 20), (1, 20), (2, 75)])\n    for t in q:\n        val = (t.test_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n    q = TestModel.objects((TestModel.attempt_id == 3)).allow_filtering()\n    assert (len(q) == 3)\n    compare_set = set([(0, 20), (1, 20), (2, 75)])\n    for t in q:\n        val = (t.test_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n", "label": 1}
{"function": "\n\ndef _update_defaults(self, defaults):\n    'Updates the given defaults with values from the config files and\\n        the environ. Does a little special handling for certain types of\\n        options (lists).'\n    config = {\n        \n    }\n    for section in ('global', self.name):\n        config.update(self.normalize_keys(self.get_config_section(section)))\n    if (not self.isolated):\n        config.update(self.normalize_keys(self.get_environ_vars()))\n    self.values = optparse.Values(self.defaults)\n    late_eval = set()\n    for (key, val) in config.items():\n        if (not val):\n            continue\n        option = self.get_option(key)\n        if (option is None):\n            continue\n        if (option.action in ('store_true', 'store_false', 'count')):\n            val = strtobool(val)\n        elif (option.action == 'append'):\n            val = val.split()\n            val = [self.check_default(option, key, v) for v in val]\n        elif (option.action == 'callback'):\n            late_eval.add(option.dest)\n            opt_str = option.get_opt_string()\n            val = option.convert_value(opt_str, val)\n            args = (option.callback_args or ())\n            kwargs = (option.callback_kwargs or {\n                \n            })\n            option.callback(option, opt_str, val, self, *args, **kwargs)\n        else:\n            val = self.check_default(option, key, val)\n        defaults[option.dest] = val\n    for key in late_eval:\n        defaults[key] = getattr(self.values, key)\n    self.values = None\n    return defaults\n", "label": 1}
{"function": "\n\ndef _find_candidate_bbs(self, start_address, end_address, mode=BARF_DISASM_RECURSIVE, symbols=None):\n    if (not symbols):\n        symbols = {\n            \n        }\n    bbs = []\n    addrs_to_process = Queue()\n    addrs_processed = set()\n    addrs_to_process.put(start_address)\n    while (not addrs_to_process.empty()):\n        addr_curr = addrs_to_process.get()\n        if ((addr_curr in addrs_processed) or (not ((addr_curr >= start_address) and (addr_curr <= end_address)))):\n            continue\n        bb = self._disassemble_bb(addr_curr, (end_address + 1), symbols)\n        if bb.empty():\n            continue\n        bbs += [bb]\n        addrs_processed.add(addr_curr)\n        if (mode == BARF_DISASM_LINEAR):\n            next_addr = (bb.address + bb.size)\n            if ((not self._bb_ends_in_direct_jmp(bb)) and (not self._bb_ends_in_return(bb)) and (not (next_addr in addrs_processed))):\n                addrs_to_process.put(next_addr)\n        if (mode == BARF_DISASM_RECURSIVE):\n            for (addr, _) in bb.branches:\n                if (not (addr in addrs_processed)):\n                    addrs_to_process.put(addr)\n    return bbs\n", "label": 1}
{"function": "\n\ndef __init__(self, num=None, m=None, szx=None, rawbytes=[]):\n    if rawbytes:\n        assert (num == None)\n        assert (m == None)\n        assert (szx == None)\n    else:\n        assert (num != None)\n        assert (m != None)\n        assert (szx != None)\n    coapOption.__init__(self, d.OPTION_NUM_BLOCK2)\n    if num:\n        self.num = num\n        self.m = m\n        self.szx = szx\n    elif (len(rawbytes) == 1):\n        self.num = ((rawbytes[0] >> 4) & 15)\n        self.m = ((rawbytes[0] >> 3) & 1)\n        self.szx = ((rawbytes[0] >> 0) & 7)\n    elif (len(rawbytes) == 2):\n        self.num = ((rawbytes[0] << 8) | ((rawbytes[1] >> 4) & 15))\n        self.m = ((rawbytes[1] >> 3) & 1)\n        self.szx = ((rawbytes[1] >> 0) & 7)\n    elif (len(rawbytes) == 3):\n        self.num = (((rawbytes[0] << 16) | (rawbytes[1] << 8)) | ((rawbytes[2] >> 4) & 15))\n        self.m = ((rawbytes[2] >> 3) & 1)\n        self.szx = ((rawbytes[2] >> 0) & 7)\n    else:\n        raise ValueError('unexpected Block2 len={0}'.format(len(rawbytes)))\n", "label": 1}
{"function": "\n\ndef handle(self, request, helper):\n    access_token = helper.fetch_state('data')['access_token']\n    if (self.org is not None):\n        if (not self.client.is_org_member(access_token, self.org['id'])):\n            return helper.error(ERR_NO_ORG_ACCESS)\n    user = self.client.get_user(access_token)\n    if (not user.get('email')):\n        emails = self.client.get_user_emails(access_token)\n        email = [e['email'] for e in emails if (((not REQUIRE_VERIFIED_EMAIL) | e['verified']) and e['primary'])]\n        if (len(email) == 0):\n            if REQUIRE_VERIFIED_EMAIL:\n                msg = ERR_NO_VERIFIED_PRIMARY_EMAIL\n            else:\n                msg = ERR_NO_PRIMARY_EMAIL\n            return helper.error(msg)\n        elif (len(email) > 1):\n            if REQUIRE_VERIFIED_EMAIL:\n                msg = ERR_NO_SINGLE_VERIFIED_PRIMARY_EMAIL\n            else:\n                msg = ERR_NO_SINGLE_PRIMARY_EMAIL\n            return helper.error(msg)\n        else:\n            user['email'] = email[0]\n    if (not user.get('name')):\n        user['name'] = _get_name_from_email(user['email'])\n    helper.bind_state('user', user)\n    return helper.next_step()\n", "label": 1}
{"function": "\n\ndef seek(self, offset, whence=0):\n    assert (whence in [0, 1, 2])\n    if (whence == 2):\n        if (offset < 0):\n            raise ValueError('negative seek offset')\n        to_read = None\n    else:\n        if (whence == 0):\n            if (offset < 0):\n                raise ValueError('negative seek offset')\n            dest = offset\n        else:\n            pos = self.__pos\n            if (pos < offset):\n                raise ValueError('seek to before start of file')\n            dest = (pos + offset)\n        end = len_of_seekable(self.__cache)\n        to_read = (dest - end)\n        if (to_read < 0):\n            to_read = 0\n    if (to_read != 0):\n        self.__cache.seek(0, 2)\n        if (to_read is None):\n            assert (whence == 2)\n            self.__cache.write(self.wrapped.read())\n            self.read_complete = True\n            self.__pos = (self.__cache.tell() - offset)\n        else:\n            data = self.wrapped.read(to_read)\n            if (not data):\n                self.read_complete = True\n            else:\n                self.__cache.write(data)\n            self.__pos = dest\n    else:\n        self.__pos = dest\n", "label": 1}
{"function": "\n\ndef cut_nodes(graph):\n    '\\n    Return the cut-nodes of the given graph.\\n    \\n    A cut node, or articulation point, is a node of a graph whose removal increases the number of\\n    connected components in the graph.\\n    \\n    @type  graph: graph, hypergraph\\n    @param graph: Graph.\\n        \\n    @rtype:  list\\n    @return: List of cut-nodes.\\n    '\n    recursionlimit = getrecursionlimit()\n    setrecursionlimit(max((len(graph.nodes()) * 2), recursionlimit))\n    if ('hypergraph' == graph.__class__.__name__):\n        return _cut_hypernodes(graph)\n    pre = {\n        \n    }\n    low = {\n        \n    }\n    reply = {\n        \n    }\n    spanning_tree = {\n        \n    }\n    pre[None] = 0\n    for each in graph:\n        if (each not in pre):\n            spanning_tree[each] = None\n            _cut_dfs(graph, spanning_tree, pre, low, [], each)\n    for each in graph:\n        if (spanning_tree[each] is not None):\n            for other in graph[each]:\n                if ((low[other] >= pre[each]) and (spanning_tree[other] == each)):\n                    reply[each] = 1\n        else:\n            children = 0\n            for other in graph:\n                if (spanning_tree[other] == each):\n                    children = (children + 1)\n            if (children >= 2):\n                reply[each] = 1\n    setrecursionlimit(recursionlimit)\n    return list(reply.keys())\n", "label": 1}
{"function": "\n\ndef slowloris(self, host, num, timeout, port=None):\n    port = (port or 80)\n    timeout = int(timeout)\n    conns = [Conn(host, int(port), 5) for i in range(int(num))]\n    failed = 0\n    packets = 0\n    while (not self.stop_flag.is_set()):\n        for conn in conns:\n            if self.stop_flag.is_set():\n                break\n            if (not conn.connected):\n                if conn.connect():\n                    packets += 3\n            if conn.connected:\n                query = ('?%d' % random.randint(1, 9999999999999))\n                payload = (self.primary_payload % (query, conn.host))\n                try:\n                    conn.send(payload)\n                    packets += 1\n                except socket.error:\n                    pass\n            else:\n                pass\n        for conn in conns:\n            if self.stop_flag.is_set():\n                break\n            if conn.connected:\n                try:\n                    conn.send('X-a: b\\r\\n')\n                    packets += 1\n                except socket.error:\n                    pass\n        gevent.sleep(timeout)\n    return ('%s failed, %s packets sent' % (failed, packets))\n", "label": 1}
{"function": "\n\ndef _mutate_header(self, name, value):\n    header = self._environ.get('HTTP_COOKIE')\n    had_header = (header is not None)\n    header = (header or '')\n    if PY3:\n        header = header.encode('latin-1')\n    bytes_name = bytes_(name, 'ascii')\n    if (value is None):\n        replacement = None\n    else:\n        bytes_val = _quote(bytes_(value, 'utf-8'))\n        replacement = ((bytes_name + b'=') + bytes_val)\n    matches = _rx_cookie.finditer(header)\n    found = False\n    for match in matches:\n        (start, end) = match.span()\n        match_name = match.group(1)\n        if (match_name == bytes_name):\n            found = True\n            if (replacement is None):\n                header = (header[:start].rstrip(b' ;') + header[end:])\n            else:\n                header = ((header[:start] + replacement) + header[end:])\n            break\n    else:\n        if (replacement is not None):\n            if header:\n                header += (b'; ' + replacement)\n            else:\n                header = replacement\n    if header:\n        self._environ['HTTP_COOKIE'] = native_(header, 'latin-1')\n    elif had_header:\n        self._environ['HTTP_COOKIE'] = ''\n    return found\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (len(self.jid_) != len(x.jid_)):\n        return 0\n    for (e1, e2) in zip(self.jid_, x.jid_):\n        if (e1 != e2):\n            return 0\n    if (self.has_body_ != x.has_body_):\n        return 0\n    if (self.has_body_ and (self.body_ != x.body_)):\n        return 0\n    if (self.has_raw_xml_ != x.has_raw_xml_):\n        return 0\n    if (self.has_raw_xml_ and (self.raw_xml_ != x.raw_xml_)):\n        return 0\n    if (self.has_type_ != x.has_type_):\n        return 0\n    if (self.has_type_ and (self.type_ != x.type_)):\n        return 0\n    if (self.has_from_jid_ != x.has_from_jid_):\n        return 0\n    if (self.has_from_jid_ and (self.from_jid_ != x.from_jid_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef process_packets(self, transaction_id=None, invoked_method=None, timeout=None):\n    start = time()\n    while (self.connected and (transaction_id not in self._invoke_results)):\n        if (timeout and ((time() - start) >= timeout)):\n            raise RTMPTimeoutError('Timeout')\n        packet = self.read_packet()\n        if (packet.type == PACKET_TYPE_INVOKE):\n            try:\n                decoded = decode_amf(packet.body)\n            except IOError:\n                continue\n            try:\n                (method, transaction_id_, obj) = decoded[:3]\n                args = decoded[3:]\n            except ValueError:\n                continue\n            if (method == '_result'):\n                if (len(args) > 0):\n                    result = args[0]\n                else:\n                    result = None\n                self._invoke_results[transaction_id_] = result\n            else:\n                handler = self._invoke_handlers.get(method)\n                if handler:\n                    res = handler(*args)\n                    if (res is not None):\n                        self.call('_result', res, transaction_id=transaction_id_)\n                if (method == invoked_method):\n                    self._invoke_args[invoked_method] = args\n                    break\n            if (transaction_id_ == 1.0):\n                self._connect_result = packet\n            else:\n                self.handle_packet(packet)\n        else:\n            self.handle_packet(packet)\n    if transaction_id:\n        result = self._invoke_results.pop(transaction_id, None)\n        return result\n    if invoked_method:\n        args = self._invoke_args.pop(invoked_method, None)\n        return args\n", "label": 1}
{"function": "\n\ndef CKY(pcfg, norm_words):\n    (x, n) = (([('', '')] + norm_words), len(norm_words))\n    pi = defaultdict(float)\n    bp = defaultdict(tuple)\n    for i in range(1, (n + 1)):\n        for X in pcfg.N:\n            (norm, word) = x[i]\n            if ((X, norm) in pcfg.q1):\n                pi[(i, i, X)] = pcfg.q1[(X, norm)]\n                bp[(i, i, X)] = (X, word, i, i)\n    for l in range(1, n):\n        for i in range(1, ((n - l) + 1)):\n            j = (i + l)\n            for X in pcfg.N:\n                (score, back) = argmax([(((pcfg.q2[(X, Y, Z)] * pi[(i, s, Y)]) * pi[((s + 1), j, Z)]), (X, Y, Z, i, s, j)) for s in range(i, j) for (Y, Z) in pcfg.binary_rules[X] if (pi[(i, s, Y)] > 0.0) if (pi[((s + 1), j, Z)] > 0.0)])\n                if (score > 0.0):\n                    (bp[(i, j, X)], pi[(i, j, X)]) = (back, score)\n    (_, top) = max([(pi[(1, n, X)], bp[(1, n, X)]) for X in pcfg.N])\n    return backtrace(top, bp)\n", "label": 1}
{"function": "\n\n@requires_search\ndef update_user(user, index=None):\n    index = (index or INDEX)\n    if (not user.is_active):\n        try:\n            es.delete(index=index, doc_type='user', id=user._id, refresh=True, ignore=[404])\n        except NotFoundError:\n            pass\n        return\n    names = dict(fullname=user.fullname, given_name=user.given_name, family_name=user.family_name, middle_names=user.middle_names, suffix=user.suffix)\n    normalized_names = {\n        \n    }\n    for (key, val) in names.items():\n        if (val is not None):\n            try:\n                val = six.u(val)\n            except TypeError:\n                pass\n            normalized_names[key] = unicodedata.normalize('NFKD', val).encode('ascii', 'ignore')\n    user_doc = {\n        'id': user._id,\n        'user': user.fullname,\n        'normalized_user': normalized_names['fullname'],\n        'normalized_names': normalized_names,\n        'names': names,\n        'job': (user.jobs[0]['institution'] if user.jobs else ''),\n        'job_title': (user.jobs[0]['title'] if user.jobs else ''),\n        'all_jobs': [job['institution'] for job in user.jobs[1:]],\n        'school': (user.schools[0]['institution'] if user.schools else ''),\n        'all_schools': [school['institution'] for school in user.schools],\n        'category': 'user',\n        'degree': (user.schools[0]['degree'] if user.schools else ''),\n        'social': user.social_links,\n        'boost': 2,\n    }\n    es.index(index=index, doc_type='user', body=user_doc, id=user._id, refresh=True)\n", "label": 1}
{"function": "\n\n@flake8ext\ndef check_builtins_gettext(logical_line, tokens, filename, lines, noqa):\n    \"Check usage of builtins gettext _().\\n\\n    Okay(neutron/foo.py): from neutron._i18n import _\\n_('foo')\\n    N341(neutron/foo.py): _('foo')\\n    Okay(neutron/_i18n.py): _('foo')\\n    Okay(neutron/i18n.py): _('foo')\\n    Okay(neutron/foo.py): _('foo')  # noqa\\n    \"\n    if noqa:\n        return\n    modulename = os.path.normpath(filename).split('/')[0]\n    if (('%s/tests' % modulename) in filename):\n        return\n    if (os.path.basename(filename) in ('i18n.py', '_i18n.py')):\n        return\n    token_values = [t[1] for t in tokens]\n    i18n_wrapper = ('%s._i18n' % modulename)\n    if ('_' in token_values):\n        i18n_import_line_found = False\n        for line in lines:\n            split_line = [elm.rstrip(',') for elm in line.split()]\n            if ((len(split_line) > 1) and (split_line[0] == 'from') and (split_line[1] == i18n_wrapper) and ('_' in split_line)):\n                i18n_import_line_found = True\n                break\n        if (not i18n_import_line_found):\n            msg = ('N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper)\n            (yield (0, msg))\n", "label": 1}
{"function": "\n\ndef pythonvaluetotime(time_val):\n    'Convert a time or time range from Python datetime to ArcGIS REST server'\n    if (time_val is None):\n        return None\n    elif isinstance(time_val, numeric):\n        return str(long((time_val * 1000.0)))\n    elif isinstance(time_val, date):\n        dtlist = [time_val.year, time_val.month, time_val.day]\n        if isinstance(time_val, datetime.datetime):\n            dtlist += [time_val.hour, time_val.minute, time_val.second]\n        else:\n            dtlist += [0, 0, 0]\n        return long((calendar.timegm(dtlist) * 1000.0))\n    elif (isinstance(time_val, sequence) and (len(time_val) == 2)):\n        if all((isinstance(x, numeric) for x in time_val)):\n            return ','.join((pythonvaluetotime(x) for x in time_val))\n        elif all((isinstance(x, date) for x in time_val)):\n            return ','.join((pythonvaluetotime(x) for x in time_val))\n    raise ValueError(repr(time_val))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.query_id = QueryHandle()\n                self.query_id.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.BOOL):\n                self.start_over = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.fetch_size = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_lexer_for_filename(_fn, code=None, **options):\n    '\\n    Get a lexer for a filename.  If multiple lexers match the filename\\n    pattern, use ``analyze_text()`` to figure out which one is more\\n    appropriate.\\n    '\n    matches = []\n    fn = basename(_fn)\n    for (modname, name, _, filenames, _) in LEXERS.itervalues():\n        for filename in filenames:\n            if fnmatch.fnmatch(fn, filename):\n                if (name not in _lexer_cache):\n                    _load_lexers(modname)\n                matches.append(_lexer_cache[name])\n    for cls in find_plugin_lexers():\n        for filename in cls.filenames:\n            if fnmatch.fnmatch(fn, filename):\n                matches.append(cls)\n    if ((sys.version_info > (3,)) and isinstance(code, bytes)):\n        code = code.decode('latin1')\n\n    def get_rating(cls):\n        d = cls.analyse_text(code)\n        return d\n    if code:\n        matches.sort(key=get_rating)\n    if matches:\n        return matches[(- 1)](**options)\n    raise ClassNotFound(('no lexer for filename %r found' % _fn))\n", "label": 1}
{"function": "\n\ndef versions_from_expanded_variables(variables, tag_prefix, verbose=False):\n    refnames = variables['refnames'].strip()\n    if refnames.startswith('$Format'):\n        if verbose:\n            print('variables are unexpanded, not using')\n        return {\n            \n        }\n    refs = set([r.strip() for r in refnames.strip('()').split(',')])\n    TAG = 'tag: '\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if (not tags):\n        tags = set([r for r in refs if re.search('\\\\d', r)])\n        if verbose:\n            print((\"discarding '%s', no digits\" % ','.join((refs - tags))))\n    if verbose:\n        print(('likely tags: %s' % ','.join(sorted(tags))))\n    for ref in sorted(tags):\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(('picking %s' % r))\n            return {\n                'version': r,\n                'full': variables['full'].strip(),\n            }\n    if verbose:\n        print('no suitable tags, using full revision id')\n    return {\n        'version': variables['full'].strip(),\n        'full': variables['full'].strip(),\n    }\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    startLoc = loc\n    instrlen = len(instring)\n    expr = self.expr\n    failParse = False\n    while (loc <= instrlen):\n        try:\n            if self.failOn:\n                try:\n                    self.failOn.tryParse(instring, loc)\n                except ParseBaseException:\n                    pass\n                else:\n                    failParse = True\n                    raise ParseException(instring, loc, ('Found expression ' + str(self.failOn)))\n                failParse = False\n            if (self.ignoreExpr is not None):\n                while 1:\n                    try:\n                        loc = self.ignoreExpr.tryParse(instring, loc)\n                    except ParseBaseException:\n                        break\n            expr._parse(instring, loc, doActions=False, callPreParse=False)\n            skipText = instring[startLoc:loc]\n            if self.includeMatch:\n                (loc, mat) = expr._parse(instring, loc, doActions, callPreParse=False)\n                if mat:\n                    skipRes = ParseResults(skipText)\n                    skipRes += mat\n                    return (loc, [skipRes])\n                else:\n                    return (loc, [skipText])\n            else:\n                return (loc, [skipText])\n        except (ParseException, IndexError):\n            if failParse:\n                raise\n            else:\n                loc += 1\n    raise ParseException(instring, loc, self.errmsg, self)\n", "label": 1}
{"function": "\n\ndef step(self, forward, keep):\n    index = self.current_index\n    matches = len(self.regions)\n    if (self.regions and ((index < 0) or ((index == 0) and (not forward)) or ((index == (matches - 1)) and forward))):\n        index = (0 if forward else (matches - 1))\n        if (self.try_wrapped or (not self.regions)):\n            wrapped = True\n            self.try_wrapped = False\n        else:\n            self.try_wrapped = True\n            return None\n    elif ((forward and (index < (matches - 1))) or ((not forward) and (index > 0))):\n        index = ((index + 1) if forward else (index - 1))\n        wrapped = self.wrapped\n    else:\n        return None\n    selected = copy(self.selected)\n    if ((not keep) and (len(selected) > 0)):\n        del selected[(- 1)]\n    return ISearchInfo.StackItem(self.search, self.regions, selected, index, forward, wrapped)\n", "label": 1}
{"function": "\n\ndef wait(self, timeout=None):\n    \"Must be used with 'yield' as 'yield cv.wait()'.\\n        \"\n    coro = AsynCoro.cur_coro()\n    if (self._owner != coro):\n        raise RuntimeError(('\"%s\"/%s: invalid lock release - owned by \"%s\"/%s' % (coro._name, coro._id, self._owner._name, self._owner._id)))\n    assert (self._depth > 0)\n    depth = self._depth\n    self._depth = 0\n    self._owner = None\n    if self._waitlist:\n        wake = self._waitlist.pop(0)\n        wake._proceed_(True)\n    self._notifylist.append(coro)\n    start = _time()\n    if ((yield coro._await_(timeout)) is None):\n        try:\n            self._notifylist.remove(coro)\n        except ValueError:\n            pass\n        raise StopIteration(False)\n    while (self._owner is not None):\n        self._waitlist.insert(0, coro)\n        if (timeout is not None):\n            timeout -= (_time() - start)\n            if (timeout <= 0):\n                raise StopIteration(False)\n            start = _time()\n        if ((yield coro._await_(timeout)) is None):\n            try:\n                self._waitlist.remove(coro)\n            except ValueError:\n                pass\n            raise StopIteration(False)\n    assert (self._depth == 0)\n    self._owner = coro\n    self._depth = depth\n    raise StopIteration(True)\n", "label": 1}
{"function": "\n\ndef convert_to_dict(obj, ident=0, limit_ident=6):\n    ident += 1\n    if (type(obj) in primitive):\n        return obj\n    if (isinstance(obj, inspect.types.InstanceType) or (type(obj) not in (list, tuple, dict))):\n        if (ident <= limit_ident):\n            try:\n                obj = obj.convert_to_dict()\n            except AttributeError:\n                try:\n                    t = obj.__dict__\n                    t['_type_class'] = str(obj.__class__)\n                    obj = t\n                except AttributeError:\n                    return str((obj.__class__ if hasattr(obj, '__class__') else type(obj)))\n        else:\n            return str((obj.__class__ if hasattr(obj, '__class__') else type(obj)))\n    if (type(obj) is dict):\n        res = {\n            \n        }\n        for item in obj:\n            if (ident <= limit_ident):\n                res[item] = convert_to_dict(obj[item], ident)\n            else:\n                res[item] = str(obj[item])\n        return res\n    if (type(obj) in (list, tuple)):\n        res = []\n        for item in obj:\n            if (ident <= limit_ident):\n                res.append(convert_to_dict(item, ident))\n            else:\n                res.append(str(item))\n        return (res if (type(obj) is list) else tuple(res))\n", "label": 1}
{"function": "\n\ndef do_idpsso_descriptor(conf, cert=None):\n    idpsso = md.IDPSSODescriptor()\n    idpsso.protocol_support_enumeration = samlp.NAMESPACE\n    endps = conf.getattr('endpoints', 'idp')\n    if endps:\n        for (endpoint, instlist) in do_endpoints(endps, ENDPOINTS['idp']).items():\n            setattr(idpsso, endpoint, instlist)\n    _do_nameid_format(idpsso, conf, 'idp')\n    scopes = conf.getattr('scope', 'idp')\n    if scopes:\n        if (idpsso.extensions is None):\n            idpsso.extensions = md.Extensions()\n        for scope in scopes:\n            mdscope = shibmd.Scope()\n            mdscope.text = scope\n            mdscope.regexp = 'false'\n            idpsso.extensions.add_extension_element(mdscope)\n    ui_info = conf.getattr('ui_info', 'idp')\n    if ui_info:\n        if (idpsso.extensions is None):\n            idpsso.extensions = md.Extensions()\n        idpsso.extensions.add_extension_element(do_uiinfo(ui_info))\n    if cert:\n        idpsso.key_descriptor = do_key_descriptor(cert)\n    for key in ['want_authn_requests_signed']:\n        try:\n            val = conf.getattr(key, 'idp')\n            if (val is None):\n                setattr(idpsso, key, DEFAULT[key])\n            else:\n                setattr(idpsso, key, ('%s' % val).lower())\n        except KeyError:\n            setattr(idpsso, key, DEFAULTS[key])\n    return idpsso\n", "label": 1}
{"function": "\n\ndef _Dynamic_Put(self, put_request, put_response):\n    if put_request.has_transaction():\n        entities = put_request.entity_list()\n        requires_id = (lambda x: ((x.id() == 0) and (not x.has_name())))\n        new_ents = [e for e in entities if requires_id(e.key().path().element_list()[(- 1)])]\n        id_request = datastore_pb.PutRequest()\n        txid = put_request.transaction().handle()\n        txdata = self.__transactions[txid]\n        assert (txdata.thread_id == thread.get_ident()), 'Transactions are single-threaded.'\n        if new_ents:\n            for ent in new_ents:\n                e = id_request.add_entity()\n                e.mutable_key().CopyFrom(ent.key())\n                e.mutable_entity_group()\n            id_response = datastore_pb.PutResponse()\n            if txdata.is_xg:\n                rpc_name = 'GetIDsXG'\n            else:\n                rpc_name = 'GetIDs'\n            super(RemoteDatastoreStub, self).MakeSyncCall('remote_datastore', rpc_name, id_request, id_response)\n            assert (id_request.entity_size() == id_response.key_size())\n            for (key, ent) in zip(id_response.key_list(), new_ents):\n                ent.mutable_key().CopyFrom(key)\n                ent.mutable_entity_group().add_element().CopyFrom(key.path().element(0))\n        for entity in entities:\n            txdata.entities[entity.key().Encode()] = (entity.key(), entity)\n            put_response.add_key().CopyFrom(entity.key())\n    else:\n        super(RemoteDatastoreStub, self).MakeSyncCall('datastore_v3', 'Put', put_request, put_response)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.finish = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.reversed = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef message_attr(self):\n    '\\n        Returns options dict as sent within WAMP messages.\\n        '\n    options = {\n        \n    }\n    if (self.acknowledge is not None):\n        options['acknowledge'] = self.acknowledge\n    if (self.exclude_me is not None):\n        options['exclude_me'] = self.exclude_me\n    if (self.exclude is not None):\n        options['exclude'] = (self.exclude if (type(self.exclude) == list) else [self.exclude])\n    if (self.exclude_authid is not None):\n        options['exclude_authid'] = (self.exclude_authid if (type(self.exclude_authid) == list) else self.exclude_authid)\n    if (self.exclude_authrole is not None):\n        options['exclude_authrole'] = (self.exclude_authrole if (type(self.exclude_authrole) == list) else self.exclude_authrole)\n    if (self.eligible is not None):\n        options['eligible'] = (self.eligible if (type(self.eligible) == list) else self.eligible)\n    if (self.eligible_authid is not None):\n        options['eligible_authid'] = (self.eligible_authid if (type(self.eligible_authid) == list) else self.eligible_authid)\n    if (self.eligible_authrole is not None):\n        options['eligible_authrole'] = (self.eligible_authrole if (type(self.eligible_authrole) == list) else self.eligible_authrole)\n    return options\n", "label": 1}
{"function": "\n\ndef _search_ds(self, method, *args, **kwargs):\n    'Searches the datastore for a file.'\n    ds_path = kwargs.get('datastorePath')\n    matched_files = set()\n    directory = False\n    dname = ('%s/' % ds_path)\n    for file in _db_content.get('files'):\n        if (file == dname):\n            directory = True\n            break\n    if directory:\n        for file in _db_content.get('files'):\n            if (file.find(ds_path) != (- 1)):\n                if (not file.endswith(ds_path)):\n                    path = file.replace(dname, '', 1).split('/')\n                    if path:\n                        matched_files.add(path[0])\n        if (not matched_files):\n            matched_files.add('/')\n    else:\n        for file in _db_content.get('files'):\n            if (file.find(ds_path) != (- 1)):\n                matched_files.add(ds_path)\n    if matched_files:\n        result = DataObject()\n        result.path = ds_path\n        result.file = []\n        for file in matched_files:\n            matched = DataObject()\n            matched.path = file\n            matched.fileSize = 1024\n            result.file.append(matched)\n        task_mdo = create_task(method, 'success', result=result)\n    else:\n        task_mdo = create_task(method, 'error', error_fault=FileNotFound())\n    return task_mdo.obj\n", "label": 1}
{"function": "\n\ndef test_manage_session2():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test', 'test1'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == True)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == True)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == True)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == True)\n        assert (key.can_manage('test1.test') == True)\n", "label": 1}
{"function": "\n\ndef create_index(self, asset_files=False):\n    logging.info('Creating new assets index...')\n    if (not asset_files):\n        asset_files = self.find_assets()\n    blueprints = Blueprints(self)\n    items = Items(self)\n    species = Species(self)\n    monsters = Monsters(self)\n    techs = Techs(self)\n    frames = Frames(self)\n    new_index_query = 'insert into assets values (?, ?, ?, ?, ?, ?)'\n    c = self.db.cursor()\n    for asset in asset_files:\n        (yield (asset[0], asset[1]))\n        tmp_data = None\n        if (asset_category(asset[0]) != ''):\n            if asset[0].endswith('.png'):\n                tmp_data = (asset[0], asset[1], 'image', '', '', '')\n            elif blueprints.is_blueprint(asset[0]):\n                tmp_data = blueprints.index_data(asset)\n            elif species.is_species(asset[0]):\n                tmp_data = species.index_data(asset)\n            elif items.is_item(asset[0]):\n                tmp_data = items.index_data(asset)\n            elif monsters.is_monster(asset[0]):\n                tmp_data = monsters.index_data(asset)\n            elif techs.is_tech(asset[0]):\n                tmp_data = techs.index_data(asset)\n            elif frames.is_frames(asset[0]):\n                tmp_data = frames.index_data(asset)\n        else:\n            logging.warning(('Skipping invalid asset (no file extension) %s in %s' % (asset[0], asset[1])))\n        if (tmp_data is not None):\n            c.execute(new_index_query, tmp_data)\n    self.db.commit()\n    logging.info('Finished creating index')\n", "label": 1}
{"function": "\n\ndef django_ordering_comparison(ordering, lhs, rhs):\n    if (not ordering):\n        return (- 1)\n    ASCENDING = 1\n    DESCENDING = 2\n    for (order, direction) in ordering:\n        if (lhs is not None):\n            lhs_value = (lhs.key() if (order == '__key__') else lhs.get(order))\n        else:\n            lhs_value = None\n        if (rhs is not None):\n            rhs_value = (rhs.key() if (order == '__key__') else rhs.get(order))\n        else:\n            rhs_value = None\n        if ((direction == ASCENDING) and (lhs_value != rhs_value)):\n            return ((- 1) if lt(lhs_value, rhs_value) else 1)\n        elif ((direction == DESCENDING) and (lhs_value != rhs_value)):\n            return (1 if lt(lhs_value, rhs_value) else (- 1))\n    return 0\n", "label": 1}
{"function": "\n\ndef _get_migration_files_to_be_executed(self, current_version, destination_version, is_migration_up):\n    if ((current_version == destination_version) and (not self.config.get('force_execute_old_migrations_versions', False))):\n        return []\n    schema_migrations = self.sgdb.get_all_schema_migrations()\n    if is_migration_up:\n        available_migrations = self.db_migrate.get_all_migrations()\n        remaining_migrations = Lists.subtract(available_migrations, schema_migrations)\n        remaining_migrations_to_execute = [migration for migration in remaining_migrations if (migration.version <= destination_version)]\n        return remaining_migrations_to_execute\n    destination_version_id = self.sgdb.get_version_id_from_version_number(destination_version)\n    try:\n        migration_versions = self.db_migrate.get_all_migration_versions()\n    except:\n        migration_versions = []\n    down_migrations_to_execute = [migration for migration in schema_migrations if (migration.id > destination_version_id)]\n    force_files = self.config.get('force_use_files_on_down', False)\n    for migration in down_migrations_to_execute:\n        if ((not migration.sql_down) or force_files):\n            if (migration.version not in migration_versions):\n                raise Exception(('impossible to migrate down: one of the versions was not found (%s)' % migration.version))\n            migration_tmp = self.db_migrate.get_migration_from_version_number(migration.version)\n            migration.sql_up = migration_tmp.sql_up\n            migration.sql_down = migration_tmp.sql_down\n            migration.file_name = migration_tmp.file_name\n    down_migrations_to_execute.reverse()\n    return down_migrations_to_execute\n", "label": 1}
{"function": "\n\ndef _replace_heap(variable, heap):\n    if isinstance(variable, Pointer):\n        while isinstance(variable, Pointer):\n            if (variable.index == 0):\n                variable = None\n            elif (variable.index in heap):\n                variable = heap[variable.index]\n            else:\n                warnings.warn('Variable referenced by pointer not found in heap: variable will be set to None')\n                variable = None\n        (replace, new) = _replace_heap(variable, heap)\n        if replace:\n            variable = new\n        return (True, variable)\n    elif isinstance(variable, np.core.records.recarray):\n        for (ir, record) in enumerate(variable):\n            (replace, new) = _replace_heap(record, heap)\n            if replace:\n                variable[ir] = new\n        return (False, variable)\n    elif isinstance(variable, np.core.records.record):\n        for (iv, value) in enumerate(variable):\n            (replace, new) = _replace_heap(value, heap)\n            if replace:\n                variable[iv] = new\n        return (False, variable)\n    elif isinstance(variable, np.ndarray):\n        if (variable.dtype.type is np.object_):\n            for iv in range(variable.size):\n                (replace, new) = _replace_heap(variable.item(iv), heap)\n                if replace:\n                    variable.itemset(iv, new)\n        return (False, variable)\n    else:\n        return (False, variable)\n", "label": 1}
{"function": "\n\ndef _get_data(self):\n    if ((self.keys is not None) and (not self.group_by)):\n        raise SqlReportException('Keys supplied without group_by.')\n    qc = self.query_context()\n    for c in self.columns:\n        qc.append_column(c.view)\n    session = connection_manager.get_scoped_session(self.engine_id)\n    try:\n        for qm in qc.query_meta.values():\n            date_aggregation_column = None\n            if (len(qm.columns) == 1):\n                c = self._find_column(qm.columns[0].column_name)\n                if hasattr(c, 'date_aggregation_column'):\n                    date_aggregation_column = c.date_aggregation_column\n            columns_names = itertools.chain.from_iterable([(col.column_name, col.alias) for col in qm.columns])\n            for group_by in self.group_by:\n                if (group_by not in columns_names):\n                    column = self._find_column_view(group_by)\n                    if column:\n                        if date_aggregation_column:\n                            column.key = date_aggregation_column\n                            column.sql_column.column_name = date_aggregation_column\n                        qm.append_column(column)\n        return qc.resolve(session.connection(), self.filter_values)\n    except:\n        session.rollback()\n        raise\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    urls = self.websocket.url_map.bind_to_environ(environ)\n    try:\n        (endpoint, args) = urls.match()\n        handler = self.websocket.view_functions[endpoint]\n    except HTTPException:\n        handler = None\n    if ((not handler) or ('HTTP_SEC_WEBSOCKET_KEY' not in environ)):\n        return self.wsgi_app(environ, start_response)\n    uwsgi.websocket_handshake(environ['HTTP_SEC_WEBSOCKET_KEY'], environ.get('HTTP_ORIGIN', ''))\n    send_event = Event()\n    send_queue = Queue()\n    recv_event = Event()\n    recv_queue = Queue(maxsize=1)\n    client = self.client(environ, uwsgi.connection_fd(), send_event, send_queue, recv_event, recv_queue, self.websocket.timeout)\n    handler = spawn(handler, client, **args)\n\n    def listener(client):\n        select([client.fd], [], [], client.timeout)\n        recv_event.set()\n    listening = spawn(listener, client)\n    while True:\n        if (not client.connected):\n            recv_queue.put(None)\n            listening.kill()\n            handler.join(client.timeout)\n            return ''\n        wait([handler, send_event, recv_event], None, 1)\n        if send_event.is_set():\n            try:\n                while True:\n                    uwsgi.websocket_send(send_queue.get_nowait())\n            except Empty:\n                send_event.clear()\n            except IOError:\n                client.connected = False\n        elif recv_event.is_set():\n            recv_event.clear()\n            try:\n                recv_queue.put(uwsgi.websocket_recv_nb())\n                listening = spawn(listener, client)\n            except IOError:\n                client.connected = False\n        elif handler.ready():\n            listening.kill()\n            return ''\n", "label": 1}
{"function": "\n\ndef __init__(self, toklist, name=None, asList=True, modal=True, isinstance=isinstance):\n    if self.__doinit:\n        self.__doinit = False\n        self.__name = None\n        self.__parent = None\n        self.__accumNames = {\n            \n        }\n        if isinstance(toklist, list):\n            self.__toklist = toklist[:]\n        elif isinstance(toklist, _generatorType):\n            self.__toklist = list(toklist)\n        else:\n            self.__toklist = [toklist]\n        self.__tokdict = dict()\n    if ((name is not None) and name):\n        if (not modal):\n            self.__accumNames[name] = 0\n        if isinstance(name, int):\n            name = _ustr(name)\n        self.__name = name\n        if (not (isinstance(toklist, (type(None), basestring, list)) and (toklist in (None, '', [])))):\n            if isinstance(toklist, basestring):\n                toklist = [toklist]\n            if asList:\n                if isinstance(toklist, ParseResults):\n                    self[name] = _ParseResultsWithOffset(toklist.copy(), 0)\n                else:\n                    self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]), 0)\n                self[name].__name = name\n            else:\n                try:\n                    self[name] = toklist[0]\n                except (KeyError, TypeError, IndexError):\n                    self[name] = toklist\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I64):\n                self.totalMemory = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I64):\n                self.freeMemory = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.maxMemory = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_value(self, var, cast=None, default=NOTSET, parse_default=False):\n    'Return value for given environment variable.\\n\\n        :param var: Name of variable.\\n        :param cast: Type to cast return value as.\\n        :param default: If var not present in environ, return this instead.\\n        :param parse_default: force to parse default..\\n\\n        :returns: Value from environment or default (if set)\\n        '\n    logger.debug(\"get '{0}' casted as '{1}' with default '{2}'\".format(var, cast, default))\n    if (var in self.scheme):\n        var_info = self.scheme[var]\n        try:\n            has_default = (len(var_info) == 2)\n        except TypeError:\n            has_default = False\n        if has_default:\n            if (not cast):\n                cast = var_info[0]\n            if (default is self.NOTSET):\n                try:\n                    default = var_info[1]\n                except IndexError:\n                    pass\n        elif (not cast):\n            cast = var_info\n    try:\n        value = self.ENVIRON[var]\n    except KeyError:\n        if (default is self.NOTSET):\n            error_msg = 'Set the {0} environment variable'.format(var)\n            raise ImproperlyConfigured(error_msg)\n        value = default\n    if (hasattr(value, 'startswith') and value.startswith('$')):\n        value = value.lstrip('$')\n        value = self.get_value(value, cast=cast, default=default)\n    if ((value != default) or (parse_default and value)):\n        value = self.parse_value(value, cast)\n    return value\n", "label": 1}
{"function": "\n\ndef clone(self, chrom=None, start=None, end=None, name=None, score=None, strand=None, thickStart=None, thickEnd=None, rgb=None, *args):\n    cols = []\n    cols.append((self.chrom if (chrom is None) else chrom))\n    cols.append((self.start if (start is None) else start))\n    cols.append((self.end if (end is None) else end))\n    cols.append((self.name if (name is None) else name))\n    cols.append((self.score if (score is None) else score))\n    cols.append((self.strand if (strand is None) else strand))\n    cols.append((self.thickStart if (thickStart is None) else thickStart))\n    cols.append((self.thickEnd if (thickEnd is None) else thickEnd))\n    cols.append((self.rgb if (rgb is None) else rgb))\n    for (i, val) in enumerate(self.extras):\n        if (len(args) > i):\n            cols.append(args[i])\n        else:\n            cols.append(val)\n    return BedRegion(*cols)\n", "label": 1}
{"function": "\n\n@responder(pattern='^failed( (?P<num>\\\\d+))?$', form='failed [num]', help='Display one or all failed tasks droned just performed for you')\ndef failed(conversation, num):\n    failures = conversation.context.get('failures', [])\n    droned_results = [f.value.resultContext for f in failures if f.check(DroneCommandFailed)]\n    other_failures = [f for f in failures if (not f.check(DroneCommandFailed))]\n    if num:\n        droned_errors = [str(rc) for rc in droned_results]\n        other_errors = [f.getTraceback() for f in other_failures]\n        errors = (droned_errors + other_errors)\n        num = int(num)\n        try:\n            conversation.say(('\\n' + errors[num]), useHTML=False)\n        except:\n            conversation.say(('No failure #%d' % num))\n    elif failures:\n        droned_errors = [('%(server)s: %(description)s' % rc) for rc in droned_results]\n        other_errors = [str(f) for f in other_failures]\n        errors = (droned_errors + other_errors)\n        listing = ('\\n' + '\\n'.join((('#%d %s' % i) for i in enumerate(errors))))\n        conversation.say(listing, useHTML=False)\n    else:\n        conversation.say('I have not failed to do anything for you recently.')\n", "label": 1}
{"function": "\n\ndef addCallback(self, callback):\n    'Adds a callback to the callbacks list.\\n\\n        :param callback: A callback object\\n        :type callback: supybot.irclib.IrcCallback\\n        '\n    assert (not self.getCallback(callback.name()))\n    self.callbacks.append(callback)\n    cbs = []\n    edges = set()\n    for cb in self.callbacks:\n        (before, after) = cb.callPrecedence(self)\n        assert (cb not in after), 'cb was in its own after.'\n        assert (cb not in before), 'cb was in its own before.'\n        for otherCb in before:\n            edges.add((otherCb, cb))\n        for otherCb in after:\n            edges.add((cb, otherCb))\n\n    def getFirsts():\n        firsts = (set(self.callbacks) - set(cbs))\n        for (before, after) in edges:\n            firsts.discard(after)\n        return firsts\n    firsts = getFirsts()\n    while firsts:\n        for cb in firsts:\n            cbs.append(cb)\n            edgesToRemove = []\n            for edge in edges:\n                if (edge[0] is cb):\n                    edgesToRemove.append(edge)\n            for edge in edgesToRemove:\n                edges.remove(edge)\n        firsts = getFirsts()\n    assert (len(cbs) == len(self.callbacks)), ('cbs: %s, self.callbacks: %s' % (cbs, self.callbacks))\n    self.callbacks[:] = cbs\n", "label": 1}
{"function": "\n\ndef _parse_settings(self, parser):\n    if (not parser.has_section('server')):\n        raise exceptions.ConfigurationError(\"The server configuration file does not have a 'server' section.\")\n    settings = [x[0] for x in parser.items('server')]\n    for setting in settings:\n        if (setting not in self._expected_settings):\n            raise exceptions.ConfigurationError(\"Setting '{0}' is not a supported setting. Please remove it from the configuration file.\".format(setting))\n    for setting in self._expected_settings:\n        if (setting not in settings):\n            raise exceptions.ConfigurationError(\"Setting '{0}' is missing from the configuration file.\".format(setting))\n    if parser.has_option('server', 'hostname'):\n        self._set_hostname(parser.get('server', 'hostname'))\n    if parser.has_option('server', 'port'):\n        self._set_port(parser.getint('server', 'port'))\n    if parser.has_option('server', 'certificate_path'):\n        self._set_certificate_path(parser.get('server', 'certificate_path'))\n    if parser.has_option('server', 'key_path'):\n        self._set_key_path(parser.get('server', 'key_path'))\n    if parser.has_option('server', 'ca_path'):\n        self._set_ca_path(parser.get('server', 'ca_path'))\n    if parser.has_option('server', 'auth_suite'):\n        self._set_auth_suite(parser.get('server', 'auth_suite'))\n", "label": 1}
{"function": "\n\ndef _HandleImports(self, element, import_manager=None):\n    'Handles imports for the specified element.\\n\\n    Args:\\n      element: (Property|Parameter) The property we want to set the import for.\\n      import_manager: The import manager to import into if not the implied one.\\n\\n    '\n    element = getattr(element, 'referenced_schema', element)\n    if isinstance(element, data_types.ComplexDataType):\n        parent = element\n        data_type = element\n    else:\n        parent = element.schema\n        data_type = element.data_type\n    data_type = getattr(data_type, 'referenced_schema', data_type)\n    if (not import_manager):\n        if self._InnerModelClassesSupported():\n            while parent.parent:\n                parent = parent.parent\n        import_manager = cpp_import_manager.CppImportManager.ForElement(parent)\n    while isinstance(data_type, (data_types.ArrayDataType, data_types.MapDataType)):\n        data_type = data_type._base_type\n        data_type = getattr(data_type, 'referenced_schema', data_type)\n    if (not data_type):\n        return\n    json_type = data_type.json_type\n    json_format = data_type.values.get('format')\n    if (json_type == 'object'):\n        if (not data_type.parent):\n            import_manager.AddDataType(data_type)\n        return\n    datatype_and_imports = self.language_model.type_map.get((json_type, json_format))\n    if datatype_and_imports:\n        import_definition = datatype_and_imports[1]\n        for required_import in import_definition.imports:\n            if required_import:\n                import_manager.AddImport(required_import)\n        for template_value in import_definition.template_values:\n            element.SetTemplateValue(template_value, True)\n    elif data_type:\n        if (not data_type.parent):\n            import_manager.AddDataType(data_type)\n    return\n", "label": 1}
{"function": "\n\ndef __setattr__(self, name, value):\n    if ((name in self.__dict__.keys()) or ('_BaseAWSObject__initialized' not in self.__dict__)):\n        return dict.__setattr__(self, name, value)\n    elif (name in self.attributes):\n        self.resource[name] = value\n        return None\n    elif (name in self.propnames):\n        expected_type = self.props[name][0]\n        if isinstance(value, AWSHelperFn):\n            return self.properties.__setitem__(name, value)\n        elif isinstance(expected_type, types.FunctionType):\n            try:\n                value = expected_type(value)\n            except:\n                sys.stderr.write((\"%s: %s.%s function validator '%s' threw exception:\\n\" % (self.__class__, self.title, name, expected_type.__name__)))\n                raise\n            return self.properties.__setitem__(name, value)\n        elif isinstance(expected_type, list):\n            if (not isinstance(value, list)):\n                self._raise_type(name, value, expected_type)\n            for v in value:\n                if ((not isinstance(v, tuple(expected_type))) and (not isinstance(v, AWSHelperFn))):\n                    self._raise_type(name, v, expected_type)\n            return self.properties.__setitem__(name, value)\n        elif isinstance(value, expected_type):\n            return self.properties.__setitem__(name, value)\n        else:\n            self._raise_type(name, value, expected_type)\n    type_name = getattr(self, 'resource_type', self.__class__.__name__)\n    if ((type_name == 'AWS::CloudFormation::CustomResource') or type_name.startswith('Custom::')):\n        return self.properties.__setitem__(name, value)\n    raise AttributeError(('%s object does not support attribute %s' % (type_name, name)))\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Main master loop.'\n    self.start()\n    util._setproctitle(('master [%s]' % self.proc_name))\n    try:\n        self.manage_workers()\n        while True:\n            sig = (self.SIG_QUEUE.pop(0) if len(self.SIG_QUEUE) else None)\n            if (sig is None):\n                self.sleep()\n                self.murder_workers()\n                self.manage_workers()\n                continue\n            if (sig not in self.SIG_NAMES):\n                self.log.info('Ignoring unknown signal: %s', sig)\n                continue\n            signame = self.SIG_NAMES.get(sig)\n            handler = getattr(self, ('handle_%s' % signame), None)\n            if (not handler):\n                self.log.error('Unhandled signal: %s', signame)\n                continue\n            self.log.info('Handling signal: %s', signame)\n            handler()\n            self.wakeup()\n    except StopIteration:\n        self.halt()\n    except KeyboardInterrupt:\n        self.halt()\n    except HaltServer as inst:\n        self.halt(reason=inst.reason, exit_status=inst.exit_status)\n    except SystemExit:\n        raise\n    except Exception:\n        self.log.info('Unhandled exception in main loop:\\n%s', traceback.format_exc())\n        self.stop(False)\n        if (self.pidfile is not None):\n            self.pidfile.unlink()\n        sys.exit((- 1))\n", "label": 1}
{"function": "\n\ndef _get_fetch_info_from_stderr(self, proc, progress):\n    output = IterableList('name')\n    fetch_info_lines = list()\n    cmds = (set(PushInfo._flag_map.keys()) & set(FetchInfo._flag_map.keys()))\n    progress_handler = progress.new_message_handler()\n    for line in proc.stderr.readlines():\n        line = line.decode(defenc)\n        for pline in progress_handler(line):\n            if (line.startswith('fatal:') or line.startswith('error:')):\n                raise GitCommandError((('Error when fetching: %s' % line),), 2)\n            for cmd in cmds:\n                if ((len(line) > 1) and (line[0] == ' ') and (line[1] == cmd)):\n                    fetch_info_lines.append(line)\n                    continue\n    finalize_process(proc)\n    fp = open(join(self.repo.git_dir, 'FETCH_HEAD'), 'rb')\n    fetch_head_info = [l.decode(defenc) for l in fp.readlines()]\n    fp.close()\n    l_fil = len(fetch_info_lines)\n    l_fhi = len(fetch_head_info)\n    assert (l_fil >= l_fhi), ('len(%s) <= len(%s)' % (l_fil, l_fhi))\n    output.extend((FetchInfo._from_line(self.repo, err_line, fetch_line) for (err_line, fetch_line) in zip(fetch_info_lines, fetch_head_info)))\n    return output\n", "label": 1}
{"function": "\n\n@non_atomic_requests\ndef guid_search(request, api_version, guids):\n    lang = request.LANG\n\n    def guid_search_cache_key(guid):\n        key = ('guid_search:%s:%s:%s' % (api_version, lang, guid))\n        return hashlib.md5(smart_str(key)).hexdigest()\n    guids = ([g.strip() for g in guids.split(',')] if guids else [])\n    addons_xml = cache.get_many([guid_search_cache_key(g) for g in guids])\n    dirty_keys = set()\n    for g in guids:\n        key = guid_search_cache_key(g)\n        if (key not in addons_xml):\n            dirty_keys.add(key)\n            try:\n                addon = Addon.objects.get(guid=g, disabled_by_user=False, status__in=SEARCHABLE_STATUSES)\n            except Addon.DoesNotExist:\n                addons_xml[key] = ''\n            else:\n                addon_xml = render_xml_to_string(request, 'legacy_api/includes/addon.xml', {\n                    'addon': addon,\n                    'api_version': api_version,\n                    'api': legacy_api,\n                })\n                addons_xml[key] = addon_xml\n    cache.set_many(dict(((k, v) for (k, v) in addons_xml.iteritems() if (k in dirty_keys))))\n    compat = CompatOverride.objects.filter(guid__in=guids).transform(CompatOverride.transformer)\n    addons_xml = [v for v in addons_xml.values() if v]\n    return render_xml(request, 'legacy_api/search.xml', {\n        'addons_xml': addons_xml,\n        'total': len(addons_xml),\n        'compat': compat,\n        'api_version': api_version,\n        'api': legacy_api,\n    })\n", "label": 1}
{"function": "\n\ndef optimize(self):\n    'Optimize certain stacked wildcard patterns.'\n    subpattern = None\n    if ((self.content is not None) and (len(self.content) == 1) and (len(self.content[0]) == 1)):\n        subpattern = self.content[0][0]\n    if ((self.min == 1) and (self.max == 1)):\n        if (self.content is None):\n            return NodePattern(name=self.name)\n        if ((subpattern is not None) and (self.name == subpattern.name)):\n            return subpattern.optimize()\n    if ((self.min <= 1) and isinstance(subpattern, WildcardPattern) and (subpattern.min <= 1) and (self.name == subpattern.name)):\n        return WildcardPattern(subpattern.content, (self.min * subpattern.min), (self.max * subpattern.max), subpattern.name)\n    return self\n", "label": 1}
{"function": "\n\ndef xml_elements_equal(element1, element2, ignore_level1_cdata=False):\n    'Check if two XML elements are equal.\\n\\n    :Parameters:\\n        - `element1`: the first element to compare\\n        - `element2`: the other element to compare\\n        - `ignore_level1_cdata`: if direct text children of the elements\\n          should be ignored for the comparision\\n    :Types:\\n        - `element1`: :etree:`ElementTree.Element`\\n        - `element2`: :etree:`ElementTree.Element`\\n        - `ignore_level1_cdata`: `bool`\\n\\n    :Returntype: `bool`\\n    '\n    if ((None in (element1, element2)) or (element1.tag != element2.tag)):\n        return False\n    attrs1 = list(element1.items())\n    attrs1.sort()\n    attrs2 = list(element2.items())\n    attrs2.sort()\n    if (not ignore_level1_cdata):\n        if (element1.text != element2.text):\n            return False\n    if (attrs1 != attrs2):\n        return False\n    if (len(element1) != len(element2)):\n        return False\n    for (child1, child2) in zip(element1, element2):\n        if (child1.tag != child2.tag):\n            return False\n        if (not ignore_level1_cdata):\n            if (element1.text != element2.text):\n                return False\n        if (not xml_elements_equal(child1, child2)):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef lookup_allowed(self, lookup, value):\n    model = self.model\n    for l in model._meta.related_fkey_lookups:\n        for (k, v) in widgets.url_params_from_lookup_dict(l).items():\n            if ((k == lookup) and (v == value)):\n                return True\n    parts = lookup.split(LOOKUP_SEP)\n    if ((len(parts) > 1) and (parts[(- 1)] in QUERY_TERMS)):\n        parts.pop()\n    pk_attr_name = None\n    for part in parts[:(- 1)]:\n        (field, _, _, _) = model._meta.get_field_by_name(part)\n        if hasattr(field, 'rel'):\n            model = field.rel.to\n            pk_attr_name = model._meta.pk.name\n        elif isinstance(field, RelatedObject):\n            model = field.model\n            pk_attr_name = model._meta.pk.name\n        else:\n            pk_attr_name = None\n    if (pk_attr_name and (len(parts) > 1) and (parts[(- 1)] == pk_attr_name)):\n        parts.pop()\n    try:\n        self.model._meta.get_field_by_name(parts[0])\n    except FieldDoesNotExist:\n        return True\n    else:\n        if (len(parts) == 1):\n            return True\n        clean_lookup = LOOKUP_SEP.join(parts)\n        return ((clean_lookup in self.list_filter) or (clean_lookup == self.date_hierarchy))\n", "label": 1}
{"function": "\n\n@get('/s/users/{user_id}')\ndef get_user_route(request, user_id):\n    '\\n    Get the user by their ID.\\n    '\n    db_conn = request['db_conn']\n    user = User.get(db_conn, id=user_id)\n    current_user = get_current_user(request)\n    if (not user):\n        return abort(404)\n    data = {\n        \n    }\n    data['user'] = user.deliver(access=('private' if (current_user and (user['id'] == current_user['id'])) else None))\n    if ('posts' in request['params']):\n        data['posts'] = [post.deliver() for post in get_posts_facade(db_conn, user_id=user['id'])]\n    if (('sets' in request['params']) and (user['settings']['view_sets'] == 'public')):\n        u_sets = UserSets.get(db_conn, user_id=user['id'])\n        data['sets'] = [set_.deliver() for set_ in u_sets.list_sets(db_conn)]\n    if (('follows' in request['params']) and (user['settings']['view_follows'] == 'public')):\n        data['follows'] = [follow.deliver() for follow in Follow.list(db_conn, user_id=user['id'])]\n    if ('avatar' in request['params']):\n        size = int(request['params']['avatar'])\n        data['avatar'] = user.get_avatar((size if size else None))\n    return (200, data)\n", "label": 1}
{"function": "\n\ndef uninstall(pkg, package_name, remove_all, app_id, cli, app):\n    'Uninstalls a package.\\n\\n    :param pkg: package manager to uninstall with\\n    :type pkg: PackageManager\\n    :param package_name: The package to uninstall\\n    :type package_name: str\\n    :param remove_all: Whether to remove all instances of the named app\\n    :type remove_all: boolean\\n    :param app_id: App ID of the app instance to uninstall\\n    :type app_id: str\\n    :param init_client: The program to use to run the app\\n    :type init_client: object\\n    :rtype: None\\n    '\n    if ((cli is False) and (app is False)):\n        cli = app = True\n    uninstalled = False\n    installed = installed_packages(pkg, app_id, package_name)\n    installed_cli = next((True for installed_pkg in installed if installed_pkg.get('command')), False)\n    installed_app = next((True for installed_pkg in installed if installed_pkg.get('apps')), False)\n    if (cli and installed_cli):\n        if subcommand.uninstall(package_name):\n            uninstalled = True\n    if (app and installed_app):\n        if pkg.uninstall_app(package_name, remove_all, app_id):\n            uninstalled = True\n    if uninstalled:\n        return None\n    else:\n        msg = 'Package [{}]'.format(package_name)\n        if (app_id is not None):\n            app_id = util.normalize_app_id(app_id)\n            msg += ' with id [{}]'.format(app_id)\n        msg += ' is not installed'\n        raise DCOSException(msg)\n", "label": 1}
{"function": "\n\ndef add_module(self, parent, kwargs=dict()):\n    ' Add the target module to the given object.\\n        '\n    if (parent is not None):\n        module_manager = get_module_manager(parent)\n        if ((module_manager is not None) and (len(module_manager.children) > 0)):\n            scalar_lut = module_manager.scalar_lut_manager\n            vector_lut = module_manager.vector_lut_manager\n            if ('vmin' in kwargs):\n                if ((not scalar_lut.use_default_range) and (kwargs['vmin'] != scalar_lut.data_range[0])):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n                elif ((not scalar_lut.use_default_range) and (kwargs['vmin'] != scalar_lut.data_range[0])):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n            elif ('vmax' in kwargs):\n                if ((not scalar_lut.use_default_range) and (kwargs['vmax'] != scalar_lut.data_range[1])):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n                elif ((not scalar_lut.use_default_range) and (kwargs['vmax'] != scalar_lut.data_range[1])):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n            elif ('colormap' in kwargs):\n                cmap = kwargs['colormap']\n                if ((scalar_lut.lut_mode != cmap) or (vector_lut.lut_mode != cmap)):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n    self._engine.add_module(self._target, obj=parent)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.token = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.interval = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.valid_until = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_filters(self, request):\n    lookup_params = self.get_filters_params()\n    use_distinct = False\n    for (key, value) in lookup_params.items():\n        if (not self.model_admin.lookup_allowed(key, value)):\n            raise DisallowedModelAdminLookup(('Filtering by %s not allowed' % key))\n    filter_specs = []\n    if self.list_filter:\n        for list_filter in self.list_filter:\n            if callable(list_filter):\n                spec = list_filter(request, lookup_params, self.model, self.model_admin)\n            else:\n                field_path = None\n                if isinstance(list_filter, (tuple, list)):\n                    (field, field_list_filter_class) = list_filter\n                else:\n                    (field, field_list_filter_class) = (list_filter, FieldListFilter.create)\n                if (not isinstance(field, models.Field)):\n                    field_path = field\n                    field = get_fields_from_path(self.model, field_path)[(- 1)]\n                spec = field_list_filter_class(field, request, lookup_params, self.model, self.model_admin, field_path=field_path)\n                use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, field_path))\n            if (spec and spec.has_output()):\n                filter_specs.append(spec)\n    try:\n        for (key, value) in lookup_params.items():\n            lookup_params[key] = prepare_lookup_value(key, value)\n            use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, key))\n        return (filter_specs, bool(filter_specs), lookup_params, use_distinct)\n    except FieldDoesNotExist as e:\n        six.reraise(IncorrectLookupParameters, IncorrectLookupParameters(e), sys.exc_info()[2])\n", "label": 1}
{"function": "\n\ndef _search_for_function_hints(self, simrun):\n    '\\n        Scan for constants that might be used as exit targets later, and add\\n        them into pending_exits\\n        '\n    function_hints = []\n    if (isinstance(simrun, simuvex.SimIRSB) and simrun.successors):\n        successor = simrun.successors[0]\n        for action in successor.log.actions:\n            if ((action.type == 'reg') and (action.offset == self.project.arch.ip_offset)):\n                continue\n            elif (action.type == 'exit'):\n                continue\n            data = action.data\n            if (data is not None):\n                try:\n                    const = successor.se.exactly_n_int(data.ast, 1)[0]\n                except:\n                    continue\n                if self._is_address_executable(const):\n                    if ((self._pending_function_hints is not None) and (const in self._pending_function_hints)):\n                        continue\n                    function_hints.append(const)\n        l.info('Got %d possible exits from %s, including: %s', len(function_hints), simrun, ', '.join([('0x%x' % f) for f in function_hints]))\n    return function_hints\n", "label": 1}
{"function": "\n\ndef __init__(self, app, gcfg=None, host='127.0.0.1', port=None, *args, **kwargs):\n    self.cfg = Config()\n    self.gcfg = gcfg\n    self.app = app\n    self.callable = None\n    gcfg = (gcfg or {\n        \n    })\n    cfgfname = gcfg.get('__file__')\n    if (cfgfname is not None):\n        self.cfgurl = ('config:%s' % cfgfname)\n        self.relpath = os.path.dirname(cfgfname)\n        self.cfgfname = cfgfname\n    cfg = kwargs.copy()\n    if (port and (not host.startswith('unix:'))):\n        bind = ('%s:%s' % (host, port))\n    else:\n        bind = host\n    cfg['bind'] = bind.split(',')\n    if gcfg:\n        for (k, v) in gcfg.items():\n            cfg[k] = v\n        cfg['default_proc_name'] = cfg['__file__']\n    try:\n        for (k, v) in cfg.items():\n            if ((k.lower() in self.cfg.settings) and (v is not None)):\n                self.cfg.set(k.lower(), v)\n    except Exception as e:\n        print(('\\nConfig error: %s' % str(e)), file=sys.stderr)\n        sys.stderr.flush()\n        sys.exit(1)\n    if cfg.get('config'):\n        self.load_config_from_file(cfg['config'])\n    else:\n        default_config = get_default_config_file()\n        if (default_config is not None):\n            self.load_config_from_file(default_config)\n", "label": 1}
{"function": "\n\ndef _set_mime_headers(self, headers):\n    for (name, value) in headers:\n        name = name.lower()\n        if (name == 'project-id-version'):\n            parts = value.split(' ')\n            self.project = ' '.join(parts[:(- 1)])\n            self.version = parts[(- 1)]\n        elif (name == 'report-msgid-bugs-to'):\n            self.msgid_bugs_address = value\n        elif (name == 'last-translator'):\n            self.last_translator = value\n        elif (name == 'language-team'):\n            self.language_team = value\n        elif (name == 'content-type'):\n            (mimetype, params) = parse_header(value)\n            if ('charset' in params):\n                self.charset = params['charset'].lower()\n        elif (name == 'plural-forms'):\n            (_, params) = parse_header((' ;' + value))\n            self._num_plurals = int(params.get('nplurals', 2))\n            self._plural_expr = params.get('plural', '(n != 1)')\n        elif (name == 'pot-creation-date'):\n            self.creation_date = _parse_datetime_header(value)\n        elif (name == 'po-revision-date'):\n            if ('YEAR' not in value):\n                self.revision_date = _parse_datetime_header(value)\n", "label": 1}
{"function": "\n\ndef _updateJobProgress(self, job, total, current, message, notify):\n    'Helper for updating job progress information.'\n    state = JobStatus.toNotificationStatus(job['status'])\n    if (current is not None):\n        current = float(current)\n    if (total is not None):\n        total = float(total)\n    if (job['progress'] is None):\n        if (notify and job['userId']):\n            notification = self._createProgressNotification(job, total, current, state, message)\n            notificationId = notification['_id']\n        else:\n            notificationId = None\n        job['progress'] = {\n            'message': message,\n            'total': total,\n            'current': current,\n            'notificationId': notificationId,\n        }\n    else:\n        if (total is not None):\n            job['progress']['total'] = total\n        if (current is not None):\n            job['progress']['current'] = current\n        if (message is not None):\n            job['progress']['message'] = message\n        if (notify and job['userId']):\n            if (job['progress']['notificationId'] is None):\n                notification = self._createProgressNotification(job, total, current, state, message)\n                job['progress']['notificationId'] = notification['_id']\n                self.save(job)\n            else:\n                notification = self.model('notification').load(job['progress']['notificationId'])\n            self.model('notification').updateProgress(notification, state=state, message=job['progress']['message'], current=job['progress']['current'], total=job['progress']['total'])\n", "label": 1}
{"function": "\n\ndef test_update_crossing_duration1_not_duration_not_at_1st_step(self, duration1, duration2):\n    global rec\n    if ((duration1 == 0.0) or (duration2 == 0.0)):\n        return\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    composite = ac.sequence(a1, a2)\n    rec = []\n    node.do(composite)\n    elapsed = 0.0\n    next_elapsed = (duration1 / 2.0)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    elapsed = next_elapsed\n    next_elapsed = ((duration1 + duration2) / 2.0)\n    dt = (next_elapsed - elapsed)\n    rec = []\n    node._step(dt)\n    recx = [e for e in rec if (e[1] != 'step')]\n    rec = [e for e in recx if (e[0] == name1)]\n    print('rec', rec)\n    assert ((rec[0][1] == 'update') and (rec[0][2] == 1.0))\n    assert (rec[1][1] == 'stop')\n    assert (len(rec) == 2)\n    rec = [e for e in recx if (e[0] == name2)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert (rec[1][1] == 'update')\n    assert (abs((rec[1][2] - ((next_elapsed - duration1) / duration2))) < fe)\n    assert (len(rec) == 2)\n", "label": 1}
{"function": "\n\ndef get_cors_headers(options, request_headers, request_method, response_headers):\n    origin_to_set = get_cors_origin(options, request_headers.get('Origin'))\n    headers = MultiDict()\n    if (origin_to_set is None):\n        return headers\n    headers[ACL_ORIGIN] = origin_to_set\n    headers[ACL_EXPOSE_HEADERS] = options.get('expose_headers')\n    if options.get('supports_credentials'):\n        headers[ACL_CREDENTIALS] = 'true'\n    if (request_method == 'OPTIONS'):\n        acl_request_method = request_headers.get(ACL_REQUEST_METHOD, '').upper()\n        if (acl_request_method and (acl_request_method in options.get('methods'))):\n            headers[ACL_ALLOW_HEADERS] = get_allow_headers(options, request_headers.get(ACL_REQUEST_HEADERS))\n            headers[ACL_MAX_AGE] = options.get('max_age')\n            headers[ACL_METHODS] = options.get('methods')\n        else:\n            LOG.info(\"The request's Access-Control-Request-Method header does not match allowed methods. CORS headers will not be applied.\")\n    if options.get('vary_header'):\n        if (headers[ACL_ORIGIN] == '*'):\n            pass\n        elif ((len(options.get('origins')) > 1) or any(map(probably_regex, options.get('origins')))):\n            headers.add('Vary', 'Origin')\n    return MultiDict(((k, v) for (k, v) in headers.items() if v))\n", "label": 1}
{"function": "\n\ndef predict(self, data, **kwargs):\n    '\\n        Used in the predict phase, after training.  Override\\n        '\n    voice_scripts = []\n    for i in xrange(0, data.shape[0]):\n        script_lines = data['script'][i].split('\\n')\n        voice_lines = []\n        current_line = ''\n        for (i, line) in enumerate(script_lines):\n            current_line = current_line.strip()\n            line = line.strip()\n            if (line.startswith('[') and line.endswith(']')):\n                continue\n            if line.startswith('-'):\n                continue\n            voice_line = re.search('\\\\w+:', line)\n            if (voice_line is not None):\n                if self.check_for_line_split(current_line):\n                    voice_lines.append(current_line)\n                current_line = line\n            elif (((len(line) == 0) or line.startswith('-')) and (len(current_line) > 0)):\n                if self.check_for_line_split(current_line):\n                    voice_lines.append(current_line)\n                current_line = ''\n                voice_lines.append(' ')\n            elif (len(current_line) > 0):\n                current_line += (' ' + line)\n        script_text = '\\n'.join([l for l in voice_lines if ((len(l) > 0) and ('{' not in l) and ('=' not in l))])\n        script_text = re.sub('\\\\[.+\\\\]', '', script_text)\n        voice_scripts.append(script_text.strip())\n    data['voice_script'] = voice_scripts\n    return data\n", "label": 1}
{"function": "\n\ndef collect(self):\n    metrics = {\n        \n    }\n    for filepath in self.PROC:\n        if (not os.access(filepath, os.R_OK)):\n            self.log.error('Permission to access %s denied', filepath)\n            continue\n        header = ''\n        data = ''\n        file = open(filepath)\n        if (not file):\n            self.log.error('Failed to open %s', filepath)\n            continue\n        while True:\n            line = file.readline()\n            if (len(line) == 0):\n                break\n            if line.startswith('Udp'):\n                header = line\n                data = file.readline()\n                break\n        file.close()\n        if ((header == '') or (data == '')):\n            self.log.error('%s has no lines with Udp', filepath)\n            continue\n        header = header.split()\n        data = data.split()\n        for i in xrange(1, len(header)):\n            metrics[header[i]] = data[i]\n    for metric_name in metrics.keys():\n        if ((len(self.config['allowed_names']) > 0) and (metric_name not in self.config['allowed_names'])):\n            continue\n        value = metrics[metric_name]\n        value = self.derivative(metric_name, long(value))\n        self.publish(metric_name, value, 0)\n", "label": 1}
{"function": "\n\ndef nodes_for_spec(self, spec):\n    '\\n            Determine nodes for an input_algorithms spec\\n            Taking into account nested specs\\n        '\n    tokens = []\n    if isinstance(spec, sb.create_spec):\n        container = nodes.container(classes=['option_spec_option shortline blue-back'])\n        creates = spec.kls\n        for (name, option) in sorted(spec.kwargs.items(), key=(lambda x: len(x[0]))):\n            para = nodes.paragraph(classes=['option monospaced'])\n            para += nodes.Text('{0} = '.format(name))\n            self.nodes_for_signature(option, para)\n            fields = {\n                \n            }\n            if (creates and hasattr(creates, 'fields') and isinstance(creates.fields, dict)):\n                for (key, val) in creates.fields.items():\n                    if isinstance(key, tuple):\n                        fields[key[0]] = val\n                    else:\n                        fields[key] = val\n            txt = (fields.get(name) or 'No description')\n            viewlist = ViewList()\n            for line in dedent(txt).split('\\n'):\n                viewlist.append(line, name)\n            desc = nodes.section(classes=['description monospaced'])\n            self.state.nested_parse(viewlist, self.content_offset, desc)\n            container += para\n            container += desc\n            container.extend(self.nodes_for_spec(option))\n        tokens.append(container)\n    elif isinstance(spec, sb.optional_spec):\n        tokens.extend(self.nodes_for_spec(spec.spec))\n    elif isinstance(spec, sb.container_spec):\n        tokens.extend(self.nodes_for_spec(spec.spec))\n    elif isinstance(spec, sb.dictof):\n        tokens.extend(self.nodes_for_spec(spec.value_spec))\n    return tokens\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    tenant_data = {\n        \n    }\n    for field in self.tenant_fields:\n        input_value = options.get(field.name, None)\n        tenant_data[field.name] = input_value\n    domain_data = {\n        \n    }\n    for field in self.domain_fields:\n        input_value = options.get(field.name, None)\n        domain_data[field.name] = input_value\n    clone_schema_from = options.get('clone_from')\n    while ((clone_schema_from == '') or (clone_schema_from is None)):\n        clone_schema_from = input(force_str('Clone schema from: '))\n    tenant = None\n    while True:\n        for field in self.tenant_fields:\n            if (tenant_data.get(field.name, '') == ''):\n                input_msg = field.verbose_name\n                default = field.get_default()\n                if default:\n                    input_msg = (\"%s (leave blank to use '%s')\" % (input_msg, default))\n                input_value = (input(force_str(('%s: ' % input_msg))) or default)\n                tenant_data[field.name] = input_value\n        tenant = self.store_tenant(clone_schema_from, **tenant_data)\n        if (tenant is not None):\n            break\n        tenant_data = {\n            \n        }\n    while True:\n        domain_data['tenant'] = tenant\n        for field in self.domain_fields:\n            if (domain_data.get(field.name, '') == ''):\n                input_msg = field.verbose_name\n                default = field.get_default()\n                if default:\n                    input_msg = (\"%s (leave blank to use '%s')\" % (input_msg, default))\n                input_value = (input(force_str(('%s: ' % input_msg))) or default)\n                domain_data[field.name] = input_value\n        domain = self.store_tenant_domain(**domain_data)\n        if (domain is not None):\n            break\n        domain_data = {\n            \n        }\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I64):\n                self.timestamp = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.super_column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.predicate = SlicePredicate()\n                self.predicate.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _LDL_sparse(self):\n    'Algorithm for numeric LDL factization, exploiting sparse structure.\\n        '\n    Lrowstruc = self.row_structure_symbolic_cholesky()\n    L = self.eye(self.rows)\n    D = self.zeros(self.rows, self.cols)\n    for i in range(len(Lrowstruc)):\n        for j in Lrowstruc[i]:\n            if (i != j):\n                L[(i, j)] = self[(i, j)]\n                summ = 0\n                for p1 in Lrowstruc[i]:\n                    if (p1 < j):\n                        for p2 in Lrowstruc[j]:\n                            if (p2 < j):\n                                if (p1 == p2):\n                                    summ += ((L[(i, p1)] * L[(j, p1)]) * D[(p1, p1)])\n                            else:\n                                break\n                    else:\n                        break\n                L[(i, j)] -= summ\n                L[(i, j)] /= D[(j, j)]\n            elif (i == j):\n                D[(i, i)] = self[(i, i)]\n                summ = 0\n                for k in Lrowstruc[i]:\n                    if (k < i):\n                        summ += ((L[(i, k)] ** 2) * D[(k, k)])\n                    else:\n                        break\n                D[(i, i)] -= summ\n    return (L, D)\n", "label": 1}
{"function": "\n\ndef _inverse_binarize_thresholding(y, output_type, classes, threshold):\n    'Inverse label binarization transformation using thresholding.'\n    if ((output_type == 'binary') and (y.ndim == 2) and (y.shape[1] > 2)):\n        raise ValueError(\"output_type='binary', but y.shape = {0}\".format(y.shape))\n    if ((output_type != 'binary') and (y.shape[1] != len(classes))):\n        raise ValueError('The number of class is not equal to the number of dimension of y.')\n    classes = np.asarray(classes)\n    if sp.issparse(y):\n        if (threshold > 0):\n            if (y.format not in ('csr', 'csc')):\n                y = y.tocsr()\n            y.data = np.array((y.data > threshold), dtype=np.int)\n            y.eliminate_zeros()\n        else:\n            y = np.array((y.toarray() > threshold), dtype=np.int)\n    else:\n        y = np.array((y > threshold), dtype=np.int)\n    if (output_type == 'binary'):\n        if sp.issparse(y):\n            y = y.toarray()\n        if ((y.ndim == 2) and (y.shape[1] == 2)):\n            return classes[y[:, 1]]\n        elif (len(classes) == 1):\n            return np.repeat(classes[0], len(y))\n        else:\n            return classes[y.ravel()]\n    elif (output_type == 'multilabel-indicator'):\n        return y\n    else:\n        raise ValueError('{0} format is not supported'.format(output_type))\n", "label": 1}
{"function": "\n\ndef _populate_config_resource(self, configuration):\n    'Helper for _build_resource: copy config properties to resource'\n    if (self.allow_jagged_rows is not None):\n        configuration['allowJaggedRows'] = self.allow_jagged_rows\n    if (self.allow_quoted_newlines is not None):\n        configuration['allowQuotedNewlines'] = self.allow_quoted_newlines\n    if (self.create_disposition is not None):\n        configuration['createDisposition'] = self.create_disposition\n    if (self.encoding is not None):\n        configuration['encoding'] = self.encoding\n    if (self.field_delimiter is not None):\n        configuration['fieldDelimiter'] = self.field_delimiter\n    if (self.ignore_unknown_values is not None):\n        configuration['ignoreUnknownValues'] = self.ignore_unknown_values\n    if (self.max_bad_records is not None):\n        configuration['maxBadRecords'] = self.max_bad_records\n    if (self.quote_character is not None):\n        configuration['quote'] = self.quote_character\n    if (self.skip_leading_rows is not None):\n        configuration['skipLeadingRows'] = self.skip_leading_rows\n    if (self.source_format is not None):\n        configuration['sourceFormat'] = self.source_format\n    if (self.write_disposition is not None):\n        configuration['writeDisposition'] = self.write_disposition\n", "label": 1}
{"function": "\n\ndef test_episode_fromname(episodes):\n    video = Episode.fromname(episodes['bbt_s07e05'].name)\n    assert (video.name == episodes['bbt_s07e05'].name)\n    assert (video.format == episodes['bbt_s07e05'].format)\n    assert (video.release_group == episodes['bbt_s07e05'].release_group)\n    assert (video.resolution == episodes['bbt_s07e05'].resolution)\n    assert (video.video_codec == episodes['bbt_s07e05'].video_codec)\n    assert (video.audio_codec is None)\n    assert (video.imdb_id is None)\n    assert (video.hashes == {\n        \n    })\n    assert (video.size is None)\n    assert (video.subtitle_languages == set())\n    assert (video.series == episodes['bbt_s07e05'].series)\n    assert (video.season == episodes['bbt_s07e05'].season)\n    assert (video.episode == episodes['bbt_s07e05'].episode)\n    assert (video.title is None)\n    assert (video.year is None)\n    assert (video.tvdb_id is None)\n", "label": 1}
{"function": "\n\ndef addtoken(self, type, value, context):\n    'Add a token; return True iff this is the end of the program.'\n    ilabel = self.classify(type, value, context)\n    while True:\n        (dfa, state, node) = self.stack[(- 1)]\n        (states, first) = dfa\n        arcs = states[state]\n        for (i, newstate) in arcs:\n            (t, v) = self.grammar.labels[i]\n            if (ilabel == i):\n                assert (t < 256)\n                self.shift(type, value, newstate, context)\n                state = newstate\n                while (states[state] == [(0, state)]):\n                    self.pop()\n                    if (not self.stack):\n                        return True\n                    (dfa, state, node) = self.stack[(- 1)]\n                    (states, first) = dfa\n                return False\n            elif (t >= 256):\n                itsdfa = self.grammar.dfas[t]\n                (itsstates, itsfirst) = itsdfa\n                if (ilabel in itsfirst):\n                    self.push(t, self.grammar.dfas[t], newstate, context)\n                    break\n        else:\n            if ((0, state) in arcs):\n                self.pop()\n                if (not self.stack):\n                    raise ParseError('too much input', type, value, context)\n            else:\n                raise ParseError('bad input', type, value, context)\n", "label": 1}
{"function": "\n\ndef _check_ellipsis(self, index):\n    'Process indices with Ellipsis. Returns modified index.'\n    if (index is Ellipsis):\n        return (slice(None), slice(None))\n    elif isinstance(index, tuple):\n        for (j, v) in enumerate(index):\n            if (v is Ellipsis):\n                first_ellipsis = j\n                break\n        else:\n            first_ellipsis = None\n        if (first_ellipsis is not None):\n            if (len(index) == 1):\n                return (slice(None), slice(None))\n            elif (len(index) == 2):\n                if (first_ellipsis == 0):\n                    if (index[1] is Ellipsis):\n                        return (slice(None), slice(None))\n                    else:\n                        return (slice(None), index[1])\n                else:\n                    return (index[0], slice(None))\n            tail = ()\n            for v in index[(first_ellipsis + 1):]:\n                if (v is not Ellipsis):\n                    tail = (tail + (v,))\n            nd = (first_ellipsis + len(tail))\n            nslice = max(0, (2 - nd))\n            return ((index[:first_ellipsis] + ((slice(None),) * nslice)) + tail)\n    return index\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.column_name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.op = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.value = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01):\n    ''\n    N = int(N)\n    assert (N > 0)\n    samplerate = self.audio.samplerate\n    assert (1 <= freq_base <= freq_max <= (samplerate / 2.0))\n    step = 1024\n    win = step\n    assert (0 < step <= win)\n    coeffies = self.make_erb_filter_coeffiences(samplerate, N, freq_base, freq_max)\n    zi = None\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for samples in self.audio.walk(win, step, start, end, join_channels=True):\n        (y, zi) = self.filter(samples, coeffies, zi)\n        if (not combine):\n            if each:\n                for frame in y:\n                    (yield frame)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\ndef _parse_status_report(self, proto):\n    unpack = proto.read_pkt_line().strip()\n    if (unpack != 'unpack ok'):\n        st = True\n        while (st is not None):\n            st = proto.read_pkt_line()\n        raise SendPackError(unpack)\n    statuses = []\n    errs = False\n    ref_status = proto.read_pkt_line()\n    while ref_status:\n        ref_status = ref_status.strip()\n        statuses.append(ref_status)\n        if (not ref_status.startswith('ok ')):\n            errs = True\n        ref_status = proto.read_pkt_line()\n    if errs:\n        ref_status = {\n            \n        }\n        ok = set()\n        for status in statuses:\n            if (' ' not in status):\n                continue\n            (status, ref) = status.split(' ', 1)\n            if (status == 'ng'):\n                if (' ' in ref):\n                    (ref, status) = ref.split(' ', 1)\n            else:\n                ok.add(ref)\n            ref_status[ref] = status\n        raise UpdateRefsError(('%s failed to update' % ', '.join([ref for ref in ref_status if (ref not in ok)])), ref_status=ref_status)\n", "label": 1}
{"function": "\n\ndef updateCall(call_node):\n    max_len = 0\n    for argument in call_node:\n        if (argument.type == 'argument_generator_comprehension'):\n            return\n        if (hasattr(argument, 'target') and (argument.target is not None)):\n            key = argument.target.value\n        else:\n            key = None\n        if (key is not None):\n            max_len = max(max_len, len(key))\n    if ('\\n' not in call_node.second_formatting.dumps()):\n        del call_node.second_formatting[:]\n        del call_node.third_formatting[:]\n    for argument in call_node:\n        if (hasattr(argument, 'target') and (argument.target is not None)):\n            key = argument.target.value\n        else:\n            key = None\n        if (key is not None):\n            if (not argument.second_formatting):\n                argument.second_formatting = ' '\n            if ('\\n' in str(call_node.second_formatting)):\n                if (len(argument.first_formatting) > 0):\n                    spacing = argument.first_formatting[0].value\n                else:\n                    spacing = ''\n                if ((len(key) + len(spacing)) != (max_len + 1)):\n                    argument.first_formatting = (' ' * ((max_len - len(key)) + 1))\n            else:\n                argument.first_formatting = ' '\n        elif ('\\n' not in str(call_node.second_formatting)):\n            if (argument.value.type in ('string', 'binary_string', 'raw_string')):\n                argument.value.second_formatting = ''\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_2d_time():\n    ' reading/writing of 2D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_2d', 'time_2d.sec'))\n    assert (data.shape == (332, 1500))\n    assert (np.abs((data[(0, 1)].real - 360.07)) <= 0.01)\n    assert (np.abs((data[(0, 1)].imag - (- 223.2))) <= 0.01)\n    assert (np.abs((data[(10, 18)].real - 17.93)) <= 0.01)\n    assert (np.abs((data[(10, 18)].imag - (- 67.2))) <= 0.01)\n    assert (dic['sw'][1] == 50000.0)\n    assert (dic['sf'][1] == 125.69)\n    assert (dic['ppm'][1] == 55.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 50.65)\n    assert (dic['ppm'][0] == 120.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef batch_sender(self, my_queue, stream_name, send_interval, max_batch_size, max_batch_count):\n    msg = None\n\n    def size(msg):\n        return (len(msg['message']) + 26)\n    while (msg != self.END):\n        cur_batch = ([] if (msg is None) else [msg])\n        cur_batch_size = sum((size(msg) for msg in cur_batch))\n        cur_batch_msg_count = len(cur_batch)\n        cur_batch_deadline = (time.time() + send_interval)\n        while True:\n            try:\n                msg = my_queue.get(block=True, timeout=max(0, (cur_batch_deadline - time.time())))\n            except Queue.Empty:\n                msg = None\n            if ((msg is None) or (msg == self.END) or ((cur_batch_size + size(msg)) > max_batch_size) or (cur_batch_msg_count >= max_batch_count) or (time.time() >= cur_batch_deadline)):\n                self._submit_batch(cur_batch, stream_name)\n                if (msg is not None):\n                    my_queue.task_done()\n                break\n            elif msg:\n                cur_batch_size += size(msg)\n                cur_batch_msg_count += 1\n                cur_batch.append(msg)\n                my_queue.task_done()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.url = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.bytes = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __getitem__(self, lst):\n    if (isinstance(lst, tuple) and (len(lst) < 5) and any(((Ellipsis is x) for x in lst))):\n        if ((len(lst) == 2) and (lst[1] is Ellipsis)):\n            return enumFrom(lst[0])\n        elif ((len(lst) == 3) and (lst[2] is Ellipsis)):\n            return enumFromThen(lst[0], lst[1])\n        elif ((len(lst) == 3) and (lst[1] is Ellipsis)):\n            return enumFromTo(lst[0], lst[2])\n        elif ((len(lst) == 4) and (lst[2] is Ellipsis)):\n            return enumFromThenTo(lst[0], lst[1], lst[3])\n        raise SyntaxError(('Invalid list comprehension: %s' % str(lst)))\n    elif (hasattr(lst, 'next') or hasattr(lst, '__next__')):\n        return List(tail=lst)\n    return List(head=lst)\n", "label": 1}
{"function": "\n\ndef parse_weka_output(self, lines):\n    for (i, line) in enumerate(lines):\n        if line.strip().startswith('inst#'):\n            lines = lines[i:]\n            break\n    if (lines[0].split() == ['inst#', 'actual', 'predicted', 'error', 'prediction']):\n        return [line.split()[2].split(':')[1] for line in lines[1:] if line.strip()]\n    elif (lines[0].split() == ['inst#', 'actual', 'predicted', 'error', 'distribution']):\n        return [self.parse_weka_distribution(line.split()[(- 1)]) for line in lines[1:] if line.strip()]\n    elif re.match('^0 \\\\w+ [01]\\\\.[0-9]* \\\\?\\\\s*$', lines[0]):\n        return [line.split()[1] for line in lines if line.strip()]\n    else:\n        for line in lines[:10]:\n            print(line)\n        raise ValueError(('Unhandled output format -- your version of weka may not be supported.\\n  Header: %s' % lines[0]))\n", "label": 1}
{"function": "\n\ndef __enter__(self):\n    'Perform the patch.'\n    (new, spec) = (self.new, self.spec)\n    (spec_set, autospec) = (self.spec_set, self.autospec)\n    kwargs = self.kwargs\n    (original, local) = self.get_original()\n    if ((new is DEFAULT) and (autospec is False)):\n        inherit = False\n        if (spec_set == True):\n            spec_set = original\n            if isinstance(spec_set, ClassTypes):\n                inherit = True\n        elif (spec == True):\n            spec = original\n            if isinstance(spec, ClassTypes):\n                inherit = True\n        new = MagicMock(spec=spec, spec_set=spec_set, **kwargs)\n        if inherit:\n            new.return_value = Mock(spec=spec, spec_set=spec_set)\n    elif (autospec is not False):\n        if (new is not DEFAULT):\n            raise TypeError(\"autospec creates the mock for you. Can't specify autospec and new.\")\n        spec_set = bool(spec_set)\n        _kwargs = {\n            '_name': getattr(original, '__name__', None),\n        }\n        if (autospec is True):\n            autospec = original\n        new = create_autospec(autospec, spec_set, inherit=True, configure=kwargs, **_kwargs)\n    elif self.kwargs:\n        raise TypeError(\"Can't pass kwargs to a mock we aren't creating\")\n    new_attr = new\n    if self.mocksignature:\n        new_attr = mocksignature(original, new)\n    self.temp_original = original\n    self.is_local = local\n    setattr(self.target, self.attribute, new_attr)\n    return new\n", "label": 1}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.set_content(d.getPrefixedString())\n            continue\n        if (tt == 16):\n            self.set_statuscode(d.getVarInt32())\n            continue\n        if (tt == 27):\n            self.add_header().TryMerge(d)\n            continue\n        if (tt == 48):\n            self.set_contentwastruncated(d.getBoolean())\n            continue\n        if (tt == 56):\n            self.set_externalbytessent(d.getVarInt64())\n            continue\n        if (tt == 64):\n            self.set_externalbytesreceived(d.getVarInt64())\n            continue\n        if (tt == 74):\n            self.set_finalurl(d.getPrefixedString())\n            continue\n        if (tt == 80):\n            self.set_apicpumilliseconds(d.getVarInt64())\n            continue\n        if (tt == 88):\n            self.set_apibytessent(d.getVarInt64())\n            continue\n        if (tt == 96):\n            self.set_apibytesreceived(d.getVarInt64())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef parse_f(self, args):\n    if ((len(self.tex_coords) > 1) and (len(self.normals) == 1)):\n        raise PywavefrontException('Found texture coordinates, but no normals')\n    if (self.mesh is None):\n        self.mesh = mesh.Mesh()\n        self.wavefront.add_mesh(self.mesh)\n    if (self.material is None):\n        self.material = material.Material()\n    self.mesh.add_material(self.material)\n    v1 = None\n    vlast = None\n    points = []\n    for (i, v) in enumerate(args[0:]):\n        if (type(v) is bytes):\n            v = v.decode()\n        (v_index, t_index, n_index) = (list(map(int, [(j or 0) for j in v.split('/')])) + [0, 0])[:3]\n        if (v_index < 0):\n            v_index += (len(self.vertices) - 1)\n        if (t_index < 0):\n            t_index += (len(self.tex_coords) - 1)\n        if (n_index < 0):\n            n_index += (len(self.normals) - 1)\n        vertex = ((list(self.tex_coords[t_index]) + list(self.normals[n_index])) + list(self.vertices[v_index]))\n        if (i >= 3):\n            self.material.vertices += (v1 + vlast)\n        self.material.vertices += vertex\n        if (i == 0):\n            v1 = vertex\n        vlast = vertex\n", "label": 1}
{"function": "\n\ndef model_fields(model, fields=None, readonly_fields=None, exclude=None, field_args=None, converter=None):\n    '\\n    Generate a dictionary of WTForms fields for a given MongoEngine model.\\n\\n    See `model_form` docstring for description of parameters.\\n    '\n    from mongoengine.base import BaseDocument\n    if (BaseDocument not in inspect.getmro(model)):\n        raise TypeError('Model must be a MongoEngine Document schema')\n    readonly_fields = (readonly_fields or [])\n    exclude = (exclude or [])\n    converter = (converter or ModelConverter())\n    field_args = (field_args or {\n        \n    })\n    field_names = (fields if fields else model._fields.keys())\n    field_names = (x for x in field_names if (x not in exclude))\n    field_dict = {\n        \n    }\n    for name in field_names:\n        if ((name not in readonly_fields) and (name not in model._fields)):\n            raise KeyError(('\"%s\" is not read-only and does not appear to be a field on the document.' % name))\n        if ((name in model._fields) and (name not in readonly_fields)):\n            model_field = model._fields[name]\n            field = converter.convert(model, model_field, field_args.get(name))\n            if (field is not None):\n                field_dict[name] = field\n    return field_dict\n", "label": 1}
{"function": "\n\ndef process_urlencoded(entity):\n    'Read application/x-www-form-urlencoded data into entity.params.'\n    qs = entity.fp.read()\n    for charset in entity.attempt_charsets:\n        try:\n            params = {\n                \n            }\n            for aparam in qs.split(ntob('&')):\n                for pair in aparam.split(ntob(';')):\n                    if (not pair):\n                        continue\n                    atoms = pair.split(ntob('='), 1)\n                    if (len(atoms) == 1):\n                        atoms.append(ntob(''))\n                    key = unquote_plus(atoms[0]).decode(charset)\n                    value = unquote_plus(atoms[1]).decode(charset)\n                    if (key in params):\n                        if (not isinstance(params[key], list)):\n                            params[key] = [params[key]]\n                        params[key].append(value)\n                    else:\n                        params[key] = value\n        except UnicodeDecodeError:\n            pass\n        else:\n            entity.charset = charset\n            break\n    else:\n        raise cherrypy.HTTPError(400, ('The request entity could not be decoded. The following charsets were attempted: %s' % repr(entity.attempt_charsets)))\n    for (key, value) in params.items():\n        if (key in entity.params):\n            if (not isinstance(entity.params[key], list)):\n                entity.params[key] = [entity.params[key]]\n            entity.params[key].append(value)\n        else:\n            entity.params[key] = value\n", "label": 1}
{"function": "\n\ndef decode_message(message, decoder):\n    out = []\n    objects = []\n    mapping = {\n        \n    }\n    in_field = False\n    prev = 0\n    for (index, ch) in enumerate(message):\n        if (not in_field):\n            if (ch == '{'):\n                in_field = True\n                if (prev != index):\n                    out.append(message[prev:index])\n                prev = index\n            elif (ch == '}'):\n                raise FormatException('unmatched }')\n        elif in_field:\n            if (ch == '{'):\n                raise FormatException('{ inside {}')\n            elif (ch == '}'):\n                in_field = False\n                (obj, msgid) = decoder(message[(prev + 1):index])\n                if (msgid is None):\n                    objects.append(obj)\n                    out.append('%s')\n                else:\n                    mapping[msgid] = obj\n                    out.append((('%(' + msgid) + ')s'))\n                prev = (index + 1)\n    if in_field:\n        raise FormatException('unmatched {')\n    if (prev <= index):\n        out.append(message[prev:(index + 1)])\n    result = ''.join(out)\n    if mapping:\n        args = mapping\n    else:\n        args = tuple(objects)\n    return (ugettext(result) % args)\n", "label": 1}
{"function": "\n\ndef _generate_altered_foo_together(self, operation):\n    option_name = operation.option_name\n    for (app_label, model_name) in sorted(self.kept_model_keys):\n        old_model_name = self.renamed_models.get((app_label, model_name), model_name)\n        old_model_state = self.from_state.models[(app_label, old_model_name)]\n        new_model_state = self.to_state.models[(app_label, model_name)]\n        old_value = (old_model_state.options.get(option_name) or set())\n        if old_value:\n            old_value = {tuple((self.renamed_fields.get((app_label, model_name, n), n) for n in unique)) for unique in old_value}\n        new_value = (new_model_state.options.get(option_name) or set())\n        if new_value:\n            new_value = set(new_value)\n        if (old_value != new_value):\n            dependencies = []\n            for foo_togethers in new_value:\n                for field_name in foo_togethers:\n                    field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name)\n                    if (field.remote_field and field.remote_field.model):\n                        dependencies.extend(self._get_dependecies_for_foreign_key(field))\n            self.add_operation(app_label, operation(name=model_name, **{\n                option_name: new_value,\n            }), dependencies=dependencies)\n", "label": 1}
{"function": "\n\ndef test_parser_with_syntax_errors(broken_status):\n\n    def catch_syntax_error(seq):\n        datafile = broken_status.join(('%d.txt' % seq))\n        parser = LogParser.fromstring(datafile.read())\n        with raises(ParsingError) as error:\n            parser.parse()\n        return error\n    error = catch_syntax_error(0)\n    assert (not error.value.args[0].startswith('expected list'))\n    assert (not error.value.args[0].startswith('expected 2-tuple'))\n    assert error.value.args[0].endswith('got end of input')\n    error = catch_syntax_error(1)\n    assert (not error.value.args[0].startswith('expected list'))\n    assert (not error.value.args[0].startswith('expected 2-tuple'))\n    assert error.value.args[0].endswith(('got %r' % 'BrokenVPN CLIENT LIST'))\n    error = catch_syntax_error(2)\n    assert (error.value.args[0] == 'expected list but got end of input')\n    error = catch_syntax_error(3)\n    assert (error.value.args[0] == 'expected 2-tuple but got end of input')\n    error = catch_syntax_error(4)\n    assert (error.value.args[0] == ('expected 2-tuple but got %r' % 'Updated,Yo,Hoo'))\n    error = catch_syntax_error(5)\n    assert (error.value.args[0] == ('expected 2-tuple starting with %r' % 'Updated'))\n    error = catch_syntax_error(6)\n    assert (error.value.args[0] == ('expected list but got %r' % 'YO TABLE'))\n", "label": 1}
{"function": "\n\ndef unknown_endtag(self, tag):\n    if (tag.find(':') != (- 1)):\n        (prefix, suffix) = tag.split(':', 1)\n    else:\n        (prefix, suffix) = ('', tag)\n    prefix = self.namespacemap.get(prefix, prefix)\n    if prefix:\n        prefix = (prefix + '_')\n    if ((suffix == 'svg') and self.svgOK):\n        self.svgOK -= 1\n    methodname = (('_end_' + prefix) + suffix)\n    try:\n        if self.svgOK:\n            raise AttributeError()\n        method = getattr(self, methodname)\n        method()\n    except AttributeError:\n        self.pop((prefix + suffix))\n    if (self.incontent and (not self.contentparams.get('type', 'xml').endswith('xml'))):\n        if (tag in ('xhtml:div', 'div')):\n            return\n        self.contentparams['type'] = 'application/xhtml+xml'\n    if (self.incontent and (self.contentparams.get('type') == 'application/xhtml+xml')):\n        tag = tag.split(':')[(- 1)]\n        self.handle_data(('</%s>' % tag), escape=0)\n    if self.basestack:\n        self.basestack.pop()\n        if (self.basestack and self.basestack[(- 1)]):\n            self.baseuri = self.basestack[(- 1)]\n    if self.langstack:\n        self.langstack.pop()\n        if self.langstack:\n            self.lang = self.langstack[(- 1)]\n    self.depth -= 1\n", "label": 1}
{"function": "\n\ndef find(self, path):\n    '\\n        Generate filenames in path that satisfy criteria specified in\\n        the constructor.\\n        This method is a generator and should be repeatedly called\\n        until there are no more results.\\n        '\n    for (dirpath, dirs, files) in os.walk(path):\n        depth = dirpath[(len(path) + len(os.path.sep)):].count(os.path.sep)\n        if (depth >= self.mindepth):\n            for name in (dirs + files):\n                fstat = None\n                matches = True\n                fullpath = None\n                for criterion in self.criteria:\n                    if ((fstat is None) and (criterion.requires() & _REQUIRES_STAT)):\n                        fullpath = os.path.join(dirpath, name)\n                        fstat = os.stat(fullpath)\n                    if (not criterion.match(dirpath, name, fstat)):\n                        matches = False\n                        break\n                if matches:\n                    if (fullpath is None):\n                        fullpath = os.path.join(dirpath, name)\n                    for action in self.actions:\n                        if ((fstat is None) and (action.requires() & _REQUIRES_STAT)):\n                            fstat = os.stat(fullpath)\n                        result = action.execute(fullpath, fstat, test=self.test)\n                        if (result is not None):\n                            (yield result)\n        if (depth == self.maxdepth):\n            dirs[:] = []\n", "label": 1}
{"function": "\n\ndef _printPrefix(self, want):\n    \"Prints Prefixlen/Netmask.\\n\\n        Not really. In fact it is our universal Netmask/Prefixlen printer.\\n        This is considered an internal function.\\n\\n        want == 0 / None        don't return anything    1.2.3.0\\n        want == 1               /prefix                  1.2.3.0/24\\n        want == 2               /netmask                 1.2.3.0/255.255.255.0\\n        want == 3               -lastip                  1.2.3.0-1.2.3.255\\n        \"\n    if (((self._ipversion == 4) and (self._prefixlen == 32)) or ((self._ipversion == 6) and (self._prefixlen == 128))):\n        if self.NoPrefixForSingleIp:\n            want = 0\n    if (want == None):\n        want = self.WantPrefixLen\n        if (want == None):\n            want = 1\n    if want:\n        if (want == 2):\n            netmask = self.netmask()\n            if (not isinstance(netmask, (int, long))):\n                netmask = netmask.int()\n            return ('/%s' % intToIp(netmask, self._ipversion))\n        elif (want == 3):\n            return ('-%s' % intToIp(((self.ip + self.len()) - 1), self._ipversion))\n        else:\n            return ('/%d' % self._prefixlen)\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef addError(self, test, err):\n    'Overrides normal addError to add support for errorClasses.\\n        If the exception is a registered class, the error will be added\\n        to the list for that class, not errors.\\n        '\n    stream = getattr(self, 'stream', None)\n    (ec, ev, tb) = err\n    try:\n        exc_info = self._exc_info_to_string(err, test)\n    except TypeError:\n        exc_info = self._exc_info_to_string(err)\n    for (cls, (storage, label, isfail)) in self.errorClasses.items():\n        if (result.isclass(ec) and issubclass(ec, cls)):\n            if isfail:\n                test.passwd = False\n            storage.append((test, exc_info))\n            if (stream is not None):\n                if self.showAll:\n                    message = [label]\n                    detail = result._exception_detail(err[1])\n                    if detail:\n                        message.append(detail)\n                    stream.writeln(': '.join(message))\n                elif self.dots:\n                    stream.write(label[:1])\n            return\n    self.errors.append((test, exc_info))\n    test.passed = False\n    if (stream is not None):\n        if self.showAll:\n            self.colorizer.write('ERROR', 'red')\n            self.stream.writeln()\n        elif self.dots:\n            stream.write('E')\n", "label": 1}
{"function": "\n\ndef _immediate_dominators(self, node, target_graph=None, reverse_graph=False):\n    if (target_graph is None):\n        target_graph = self.graph\n    if (node not in target_graph):\n        raise AngrCFGError(('Target node %s is not in graph.' % node))\n    graph = networkx.DiGraph(target_graph)\n    if reverse_graph:\n        for n in target_graph.nodes():\n            graph.add_node(n)\n        for (src, dst) in target_graph.edges():\n            graph.add_edge(dst, src)\n    idom = {\n        node: node,\n    }\n    order = list(networkx.dfs_postorder_nodes(graph, node))\n    dfn = {u: i for (i, u) in enumerate(order)}\n    order.pop()\n    order.reverse()\n\n    def intersect(u_, v_):\n        while (u_ != v_):\n            while (dfn[u_] < dfn[v_]):\n                u_ = idom[u_]\n            while (dfn[u_] > dfn[v_]):\n                v_ = idom[v_]\n        return u_\n    changed = True\n    while changed:\n        changed = False\n        for u in order:\n            new_idom = reduce(intersect, (v for v in graph.pred[u] if (v in idom)))\n            if ((u not in idom) or (idom[u] != new_idom)):\n                idom[u] = new_idom\n                changed = True\n    return idom\n", "label": 1}
{"function": "\n\ndef _qname_matches(tag, namespace, qname):\n    \"Logic determines if a QName matches the desired local tag and namespace.\\n  \\n  This is used in XmlElement.get_elements and XmlElement.get_attributes to\\n  find matches in the element's members (among all expected-and-unexpected\\n  elements-and-attributes).\\n  \\n  Args:\\n    expected_tag: string\\n    expected_namespace: string\\n    qname: string in the form '{xml_namespace}localtag' or 'tag' if there is\\n           no namespace.\\n  \\n  Returns:\\n    boolean True if the member's tag and namespace fit the expected tag and\\n    namespace.\\n  \"\n    if (qname is None):\n        member_tag = None\n        member_namespace = None\n    elif qname.startswith('{'):\n        member_namespace = qname[1:qname.index('}')]\n        member_tag = qname[(qname.index('}') + 1):]\n    else:\n        member_namespace = None\n        member_tag = qname\n    return (((tag is None) and (namespace is None)) or ((namespace is None) and (member_tag == tag)) or ((tag is None) and (member_namespace == namespace)) or ((tag is None) and (namespace == '') and (member_namespace is None)) or ((tag == member_tag) and (namespace == member_namespace)) or ((tag == member_tag) and (namespace == '') and (member_namespace is None)))\n", "label": 1}
{"function": "\n\ndef fails(self):\n    validate_data = getattr(self, 'validate_data')\n    for k in validate_data:\n        rules = validate_data.get(k, None)\n        if (not rules):\n            raise InvalidValidateDataError()\n        else:\n            rules_list = rules.split('|')\n            for rule in rules_list:\n                rule_origin = rule\n                rule = rule.split(':')[0]\n                if ((rule is None) or (rule == '')):\n                    raise InvalidRuleNameError(rule=rule)\n                rule_validator = import_module('.rules', package='django_laravel_validator')\n                try:\n                    regex = getattr(rule_validator, rule.upper())\n                except AttributeError:\n                    raise InvalidRuleNameError(rule=rule)\n                if (rule.upper() in WITH_PARAMETERS_VALIDATOR):\n                    rule_args = format_args_split(rule_origin)\n                    regex = regex(rule_args)\n                else:\n                    regex = regex()\n                try:\n                    setattr(regex, 'validator_instance', self)\n                    regex(self.data.get(k, None))\n                except ValidationError as e:\n                    error_message = error_message_generate(k, rule, self.message, e)\n                    error_dict = self.error_list.get(k)\n                    error_dict.update({\n                        rule: error_message,\n                    })\n                    self.error_list.get(k).update(error_dict)\n                    self.validate_flag = False\n    check = getattr(self, 'check', None)\n    if (check and callable(check) and self.validate_flag):\n        check()\n    return check_errors(self.error_list, self.error_list_ext)\n", "label": 1}
{"function": "\n\ndef get_instances(name, lifecycle_state='InService', health_status='Healthy', attribute='private_ip_address', attributes=None, region=None, key=None, keyid=None, profile=None):\n    '\\n    return attribute of all instances in the named autoscale group.\\n\\n    CLI example::\\n\\n        salt-call boto_asg.get_instances my_autoscale_group_name\\n\\n    '\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    ec2_conn = _get_ec2_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        asgs = conn.get_all_groups(names=[name])\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        return False\n    if (len(asgs) != 1):\n        log.debug(\"name '{0}' returns multiple ASGs: {1}\".format(name, [asg.name for asg in asgs]))\n        return False\n    asg = asgs[0]\n    instance_ids = []\n    for i in asg.instances:\n        if ((lifecycle_state is not None) and (i.lifecycle_state != lifecycle_state)):\n            continue\n        if ((health_status is not None) and (i.health_status != health_status)):\n            continue\n        instance_ids.append(i.instance_id)\n    instances = ec2_conn.get_only_instances(instance_ids=instance_ids)\n    if attributes:\n        return [[getattr(instance, attr).encode('ascii') for attr in attributes] for instance in instances]\n    else:\n        return [getattr(instance, attribute).encode('ascii') for instance in instances if getattr(instance, attribute)]\n    return [getattr(instance, attribute).encode('ascii') for instance in instances]\n", "label": 1}
{"function": "\n\ndef _ip_addrs(interface=None, include_loopback=False, interface_data=None, proto='inet'):\n    '\\n    Return the full list of IP adresses matching the criteria\\n\\n    proto = inet|inet6\\n    '\n    ret = set()\n    ifaces = (interface_data if isinstance(interface_data, dict) else interfaces())\n    if (interface is None):\n        target_ifaces = ifaces\n    else:\n        target_ifaces = dict([(k, v) for (k, v) in six.iteritems(ifaces) if (k == interface)])\n        if (not target_ifaces):\n            log.error('Interface {0} not found.'.format(interface))\n    for ip_info in six.itervalues(target_ifaces):\n        addrs = ip_info.get(proto, [])\n        addrs.extend([addr for addr in ip_info.get('secondary', []) if (addr.get('type') == proto)])\n        for addr in addrs:\n            addr = ipaddress.ip_address(addr.get('address'))\n            if ((not addr.is_loopback) or include_loopback):\n                ret.add(addr)\n    return [str(addr) for addr in sorted(ret)]\n", "label": 1}
{"function": "\n\ndef RC_calc(ctx, x, y, r, pv=True):\n    if (not (ctx.isnormal(x) and ctx.isnormal(y))):\n        if (ctx.isinf(x) or ctx.isinf(y)):\n            return (1 / (x * y))\n        if (y == 0):\n            return ctx.inf\n        if (x == 0):\n            return ((ctx.pi / ctx.sqrt(y)) / 2)\n        raise ValueError\n    if (pv and (ctx._im(y) == 0) and (ctx._re(y) < 0)):\n        return (ctx.sqrt((x / (x - y))) * RC_calc(ctx, (x - y), (- y), r))\n    if (x == y):\n        return (1 / ctx.sqrt(x))\n    extraprec = (2 * max(0, ((- ctx.mag((x - y))) + ctx.mag(x))))\n    ctx.prec += extraprec\n    if (ctx._is_real_type(x) and ctx._is_real_type(y)):\n        x = ctx._re(x)\n        y = ctx._re(y)\n        a = ctx.sqrt((x / y))\n        if (x < y):\n            b = ctx.sqrt((y - x))\n            v = (ctx.acos(a) / b)\n        else:\n            b = ctx.sqrt((x - y))\n            v = (ctx.acosh(a) / b)\n    else:\n        sx = ctx.sqrt(x)\n        sy = ctx.sqrt(y)\n        v = (ctx.acos((sx / sy)) / (ctx.sqrt((1 - (x / y))) * sy))\n    ctx.prec -= extraprec\n    return v\n", "label": 1}
{"function": "\n\ndef process_exception(self, request, exception):\n    if hasattr(social_exceptions, exception.__class__.__name__):\n        if isinstance(exception, AuthCanceled):\n            if request.user.is_anonymous():\n                return redirect('accounts:login')\n            else:\n                return redirect('user:settings', username=request.user.username)\n        elif isinstance(exception, AuthAlreadyAssociated):\n            blurb = 'The {0} account you tried to connect to has already been associated with another account.'\n            print(exception.backend.name)\n            if ('google' in exception.backend.name):\n                blurb = blurb.format('Google')\n            elif ('linkedin' in exception.backend.name):\n                blurb = blurb.format('LinkedIn')\n            elif ('hydroshare' in exception.backend.name):\n                blurb = blurb.format('HydroShare')\n            elif ('facebook' in exception.backend.name):\n                blurb = blurb.format('Facebook')\n            else:\n                blurb = blurb.format('social')\n            messages.success(request, blurb)\n            if request.user.is_anonymous():\n                return redirect('accounts:login')\n            else:\n                return redirect('user:settings', username=request.user.username)\n        elif isinstance(exception, NotAllowedToDisconnect):\n            blurb = 'Unable to disconnect from this social account.'\n            messages.success(request, blurb)\n            if request.user.is_anonymous():\n                return redirect('accounts:login')\n            else:\n                return redirect('user:settings', username=request.user.username)\n", "label": 1}
{"function": "\n\ndef OnKeyPressed(self, e):\n    key = e.GetKeyCode()\n    if ((key not in [wx.WXK_UP, wx.WXK_DOWN]) or (len(self.rows) == 0)):\n        e.Skip()\n        return\n    e.StopPropagation()\n    (start_col, start_row) = self.GetViewStart()\n    size = self.GetClientSize()\n    height = (size.GetHeight() / LINH)\n    if self.selection:\n        current_row = self.selection[0]\n        if (key == wx.WXK_UP):\n            next_row = max((current_row - 1), 0)\n        if (key == wx.WXK_DOWN):\n            next_row = min((current_row + 1), (len(self.rows) - 1))\n        if (e.ShiftDown() and self.allowMultiple):\n            if (next_row in self.selection):\n                self.selection.remove(current_row)\n            else:\n                self.selection.insert(0, next_row)\n        else:\n            self.selection = [next_row]\n    else:\n        next_row = start_row\n        if ((next_row < 0) or (next_row > len(self.rows))):\n            return\n        self.selection = [next_row]\n    if (next_row < start_row):\n        self.Scroll(start_col, (next_row - 1))\n    elif (next_row > ((start_row + height) - 1)):\n        self.Scroll(start_col, ((next_row - height) + 2))\n    event = CommitListEvent(EVT_COMMITLIST_SELECT_type, self.GetId())\n    event.SetCurrentRow(next_row)\n    event.SetSelection(self.selection)\n    self.ProcessEvent(event)\n    self.OnSelectionChanged(next_row, self.selection)\n    self.Refresh()\n", "label": 1}
{"function": "\n\ndef find_sorted_peaks(x):\n    'Find peaks, i.e. local maxima, of an array. Interior points are peaks if\\n    they are greater than both their neighbors, and edge points are peaks if\\n    they are greater than their only neighbor. In the case of ties, we\\n    (arbitrarily) choose the first index in the sequence of equal values as the\\n    peak.\\n\\n    Returns a list of tuples (i, x[i]) of peak indices i and values x[i],\\n    sorted in decreasing order by peak value.\\n     '\n    peak_inds = []\n    nbins = len(x)\n    for i in range(nbins):\n        if ((i == 0) or (x[i] > x[(i - 1)])):\n            if ((i == (nbins - 1)) or (x[i] > x[(i + 1)])):\n                peak_inds.append(i)\n            elif (x[i] == x[(i + 1)]):\n                for j in range((i + 1), nbins):\n                    if (x[j] != x[i]):\n                        if (x[j] < x[i]):\n                            peak_inds.append(i)\n                        break\n                if ((j == (nbins - 1)) and (x[i] == x[j])):\n                    peak_inds.append(i)\n    sorted_peak_inds = sorted(peak_inds, key=(lambda i: x[i]), reverse=True)\n    return list(zip(sorted_peak_inds, x[sorted_peak_inds]))\n", "label": 1}
{"function": "\n\ndef validate_authorize_params(self, input_data):\n    'Validates the authorize parameters given (usually) by GET'\n    client_id = input_data['client_id']\n    response_type = input_data['response_type']\n    state = input_data['state']\n    the_scope = input_data['the_scope']\n    redirect_uri = input_data['redirect_uri']\n    token_type = self.config[self.CONFIG_TOKEN_TYPE]\n    realm = self.config[self.CONFIG_WWW_REALM]\n    stored_client = self.storage.get_client_credentials(client_id)\n    if (not input_data):\n        raise HTTP(412, 'KeyError: All parameters are missing :(.')\n    elif ((not redirect_uri) or (not stored_client['redirect_uri']) or (redirect_uri != stored_client['redirect_uri'])):\n        raise HTTP(418, 'NameError: Invalid or mismatch redirect URI.')\n    elif (not client_id):\n        raise HTTP(412, 'KeyError: Parameter missing; \"client_id\" is required.')\n    elif (not stored_client):\n        raise HTTP(424, 'LookupError: Supplied \"client_id\" is invalid.')\n    elif (not response_type):\n        raise HTTP(412, 'KeyError: Parameter missing; \"response_type\" is required.')\n    elif (response_type != self.RESPONSE_TYPE_AUTH_CODE):\n        raise HTTP(501, 'The response type you requested is unsupported.')\n    elif (the_scope and (not self.check_the_scope(the_scope, self.config[self.CONFIG_SUPPORTED_SCOPES]))):\n        raise HTTP(501, 'The scope you requested is unsupported.')\n    elif ((not state) and self.config[self.CONFIG_ENFORCE_STATE]):\n        raise HTTP(412, 'KeyError: Parameter missing; \"state\" is required.')\n    return input_data\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _filter_proto(msg, make_copy=True):\n    'Filter all byte fields in the message and submessages.'\n    filtered = msg\n    if make_copy:\n        filtered = msg.__class__()\n        filtered.CopyFrom(msg)\n    fields = filtered.ListFields()\n    for (field_name, val) in ((fd.name, val) for (fd, val) in fields if (fd.type == FieldDescriptor.TYPE_BYTES)):\n        setattr(filtered, field_name, bytes(('<%s bytes>' % len(val)), 'utf8'))\n    for field in (val for (fd, val) in fields if (fd.type == FieldDescriptor.TYPE_MESSAGE)):\n        is_repeated = hasattr(field, '__len__')\n        if (not is_repeated):\n            Call._filter_proto(field, make_copy=False)\n        else:\n            for i in range(len(field)):\n                old_fields = [f for f in field]\n                del field[:]\n                field.extend([Call._filter_proto(f, make_copy=False) for f in old_fields])\n    return filtered\n", "label": 1}
{"function": "\n\ndef execute(self, fullpath, fstat, test=False):\n    result = []\n    for arg in self.fmt:\n        if (arg == 'path'):\n            result.append(fullpath)\n        elif (arg == 'name'):\n            result.append(os.path.basename(fullpath))\n        elif (arg == 'size'):\n            result.append(fstat[stat.ST_SIZE])\n        elif (arg == 'type'):\n            result.append(_FILE_TYPES.get(stat.S_IFMT(fstat[stat.ST_MODE]), '?'))\n        elif (arg == 'mode'):\n            result.append(int(oct(fstat[stat.ST_MODE])[(- 3):]))\n        elif (arg == 'mtime'):\n            result.append(fstat[stat.ST_MTIME])\n        elif (arg == 'user'):\n            uid = fstat[stat.ST_UID]\n            try:\n                result.append(pwd.getpwuid(uid).pw_name)\n            except KeyError:\n                result.append(uid)\n        elif (arg == 'group'):\n            gid = fstat[stat.ST_GID]\n            try:\n                result.append(grp.getgrgid(gid).gr_name)\n            except KeyError:\n                result.append(gid)\n        elif (arg == 'md5'):\n            if stat.S_ISREG(fstat[stat.ST_MODE]):\n                md5digest = salt.utils.get_hash(fullpath, 'md5')\n                result.append(md5digest)\n            else:\n                result.append('')\n    if (len(result) == 1):\n        return result[0]\n    else:\n        return result\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    script = options.get('script')\n    if (not script):\n        if len(args):\n            script = args[0]\n        else:\n            raise CommandError('You must provide a script path or directory containing scripts.')\n    if (not os.path.exists(script)):\n        raise CommandError('{0} does not exist.'.format(script))\n    group = options.get('group', 'Wooey Scripts')\n    scripts = ([os.path.join(script, i) for i in os.listdir(script)] if os.path.isdir(script) else [script])\n    converted = 0\n    for script in scripts:\n        if (script.endswith('.pyc') or ('__init__' in script)):\n            continue\n        if script.endswith('.py'):\n            sys.stdout.write('Converting {}\\n'.format(script))\n            with open(script, 'r') as f:\n                script = default_storage.save(os.path.join(wooey_settings.WOOEY_SCRIPT_DIR, os.path.split(script)[1]), File(f))\n                if wooey_settings.WOOEY_EPHEMERAL_FILES:\n                    local_storage = get_storage(local=True)\n                    local_storage.save(os.path.join(wooey_settings.WOOEY_SCRIPT_DIR, os.path.split(script)[1]), File(f))\n            res = add_wooey_script(script_path=script, group=group)\n            if res['valid']:\n                converted += 1\n    sys.stdout.write('Converted {} scripts\\n'.format(converted))\n", "label": 1}
{"function": "\n\ndef encode(self, b64=False, always_bytes=True):\n    'Encode the packet for transmission.'\n    if (self.binary and (not b64)):\n        encoded_packet = six.int2byte(self.packet_type)\n    else:\n        encoded_packet = six.text_type(self.packet_type)\n        if (self.binary and b64):\n            encoded_packet = ('b' + encoded_packet)\n    if self.binary:\n        if b64:\n            encoded_packet += base64.b64encode(self.data).decode('utf-8')\n        else:\n            encoded_packet += self.data\n    elif isinstance(self.data, six.string_types):\n        encoded_packet += self.data\n    elif (isinstance(self.data, dict) or isinstance(self.data, list)):\n        encoded_packet += self.json.dumps(self.data, separators=(',', ':'))\n    elif (self.data is not None):\n        encoded_packet += str(self.data)\n    if (always_bytes and (not isinstance(encoded_packet, six.binary_type))):\n        encoded_packet = encoded_packet.encode('utf-8')\n    return encoded_packet\n", "label": 1}
{"function": "\n\ndef _ApplyBarChartStyle(self, chart):\n    'If bar style is specified, fill in the missing data and apply it.'\n    if ((chart.style is None) or (not chart.data)):\n        return {\n            \n        }\n    (bar_thickness, bar_gap, group_gap) = (chart.style.bar_thickness, chart.style.bar_gap, chart.style.group_gap)\n    if ((bar_gap is None) and (group_gap is not None)):\n        bar_gap = max(0, (group_gap / 2))\n        if (not chart.style.use_fractional_gap_spacing):\n            bar_gap = int(bar_gap)\n    if ((group_gap is None) and (bar_gap is not None)):\n        group_gap = max(0, (bar_gap * 2))\n    if (bar_thickness is None):\n        if chart.style.use_fractional_gap_spacing:\n            bar_thickness = 'r'\n        else:\n            bar_thickness = 'a'\n    elif chart.style.use_fractional_gap_spacing:\n        if bar_gap:\n            bar_gap = int((bar_thickness * bar_gap))\n        if group_gap:\n            group_gap = int((bar_thickness * group_gap))\n    spec = [bar_thickness]\n    if (bar_gap is not None):\n        spec.append(bar_gap)\n        if ((group_gap is not None) and (not chart.stacked)):\n            spec.append(group_gap)\n    return util.JoinLists(bar_size=spec)\n", "label": 1}
{"function": "\n\ndef get_movie(self, imdb, title, year):\n    try:\n        query = (self.search_link % urllib.quote(title))\n        query = urlparse.urljoin(self.base_link, query)\n        result = client2.http_get(query)\n        title = cleantitle.movie(title)\n        years = [('%s' % str(year)), ('%s' % str((int(year) + 1))), ('%s' % str((int(year) - 1)))]\n        r = client.parseDOM(result, 'div', attrs={\n            'class': 'thumb',\n        })\n        r = [(client.parseDOM(i, 'a', ret='href'), client.parseDOM(i, 'a', ret='title')) for i in r]\n        r = [(i[0][0], i[1][(- 1)]) for i in r if ((len(i[0]) > 0) and (len(i[1]) > 0))]\n        r = [(re.sub('http.+?//.+?/', '', i[0]), i[1]) for i in r]\n        r = [('/'.join(i[0].split('/')[:2]), i[1]) for i in r]\n        r = [x for (y, x) in enumerate(r) if (x not in r[:y])]\n        r = [i for i in r if (title == cleantitle.movie(i[1]))]\n        u = [i[0] for i in r][0]\n        url = urlparse.urljoin(self.base_link, u)\n        url = urlparse.urlparse(url).path\n        url = client.replaceHTMLCodes(url)\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef _handle_multipart(self, msg):\n    msgtexts = []\n    subparts = msg.get_payload()\n    if (subparts is None):\n        subparts = []\n    elif isinstance(subparts, str):\n        self.write(subparts)\n        return\n    elif (not isinstance(subparts, list)):\n        subparts = [subparts]\n    for part in subparts:\n        s = self._new_buffer()\n        g = self.clone(s)\n        g.flatten(part, unixfrom=False, linesep=self._NL)\n        msgtexts.append(s.getvalue())\n    boundary = msg.get_boundary()\n    if (not boundary):\n        alltext = self._encoded_NL.join(msgtexts)\n        boundary = self._make_boundary(alltext)\n        msg.set_boundary(boundary)\n    if (msg.preamble is not None):\n        if self._mangle_from_:\n            preamble = fcre.sub('>From ', msg.preamble)\n        else:\n            preamble = msg.preamble\n        self._write_lines(preamble)\n        self.write(self._NL)\n    self.write((('--' + boundary) + self._NL))\n    if msgtexts:\n        self._fp.write(msgtexts.pop(0))\n    for body_part in msgtexts:\n        self.write((((self._NL + '--') + boundary) + self._NL))\n        self._fp.write(body_part)\n    self.write((((self._NL + '--') + boundary) + '--'))\n    if (msg.epilogue is not None):\n        self.write(self._NL)\n        if self._mangle_from_:\n            epilogue = fcre.sub('>From ', msg.epilogue)\n        else:\n            epilogue = msg.epilogue\n        self._write_lines(epilogue)\n", "label": 1}
{"function": "\n\ndef __new__(cls, latitude=None, longitude=None, altitude=None):\n    single_arg = ((longitude is None) and (altitude is None))\n    if (single_arg and (not isinstance(latitude, util.NUMBER_TYPES))):\n        arg = latitude\n        if (arg is None):\n            pass\n        elif isinstance(arg, Point):\n            return cls.from_point(arg)\n        elif isinstance(arg, basestring):\n            return cls.from_string(arg)\n        else:\n            try:\n                seq = iter(arg)\n            except TypeError:\n                raise TypeError(('Failed to create Point instance from %r.' % (arg,)))\n            else:\n                return cls.from_sequence(seq)\n    latitude = float((latitude or 0))\n    if (abs(latitude) > 90):\n        raise ValueError(('Latitude out of range [-90, 90]: %r' % latitude))\n    longitude = float((longitude or 0))\n    if (abs(longitude) > 180):\n        raise ValueError(('Longitude out of range [-180, 180]: %r' % longitude))\n    altitude = float((altitude or 0))\n    self = super(Point, cls).__new__(cls)\n    self.latitude = latitude\n    self.longitude = longitude\n    self.altitude = altitude\n    return self\n", "label": 1}
{"function": "\n\ndef GetArtifacts(self, os_name=None, name_list=None, source_type=None, exclude_dependents=False, provides=None, reload_datastore_artifacts=False):\n    'Retrieve artifact classes with optional filtering.\\n\\n    All filters must match for the artifact to be returned.\\n\\n    Args:\\n      os_name: string to match against supported_os\\n      name_list: list of strings to match against artifact names\\n      source_type: rdf_artifacts.ArtifactSource.SourceType to match against\\n                      source_type\\n      exclude_dependents: if true only artifacts with no dependencies will be\\n                          returned\\n      provides: return the artifacts that provide these dependencies\\n      reload_datastore_artifacts: If true, the data store sources are queried\\n                                  for new artifacts.\\n    Returns:\\n      set of artifacts matching filter criteria\\n    '\n    self._CheckDirty(reload_datastore_artifacts=reload_datastore_artifacts)\n    results = set()\n    for artifact in self._artifacts.itervalues():\n        if (os_name and artifact.supported_os and (os_name not in artifact.supported_os)):\n            continue\n        if (name_list and (artifact.name not in name_list)):\n            continue\n        if source_type:\n            source_types = [c.type for c in artifact.sources]\n            if (source_type not in source_types):\n                continue\n        if (exclude_dependents and artifact.GetArtifactPathDependencies()):\n            continue\n        if provides:\n            for provide_string in artifact.provides:\n                if (provide_string in provides):\n                    results.add(artifact)\n                    continue\n            continue\n        results.add(artifact)\n    return results\n", "label": 1}
{"function": "\n\ndef next(self):\n    response = super(PbufIndexStream, self).next()\n    if (response.done and (not (response.keys or response.results or response.continuation))):\n        raise StopIteration\n    if (self.return_terms and response.results):\n        return [(decode_index_value(self.index, r.key), bytes_to_str(r.value)) for r in response.results]\n    elif response.keys:\n        if PY2:\n            return response.keys[:]\n        else:\n            return [bytes_to_str(key) for key in response.keys]\n    elif response.continuation:\n        return CONTINUATION(bytes_to_str(response.continuation))\n", "label": 1}
{"function": "\n\ndef identifyConceptsUsed(self):\n    self.relationshipSets = [(arcrole, ELR, linkqname, arcqname) for (arcrole, ELR, linkqname, arcqname) in self.modelXbrl.baseSets.keys() if (ELR and (arcrole.startswith('XBRL-') or (linkqname and arcqname)))]\n    conceptsUsed = set((f.qname for f in self.modelXbrl.factsInInstance))\n    for cntx in self.modelXbrl.contexts.values():\n        for dim in cntx.qnameDims.values():\n            conceptsUsed.add(dim.dimensionQname)\n            if dim.isExplicit:\n                conceptsUsed.add(dim.memberQname)\n            else:\n                conceptsUsed.add(dim.typedMember.qname)\n    for (defaultDim, defaultDimMember) in self.modelXbrl.qnameDimensionDefaults.items():\n        conceptsUsed.add(defaultDim)\n        conceptsUsed.add(defaultDimMember)\n    for relationshipSetKey in self.relationshipSets:\n        relationshipSet = self.modelXbrl.relationshipSet(*relationshipSetKey)\n        for rel in relationshipSet.modelRelationships:\n            if isinstance(rel.fromModelObject, ModelConcept):\n                conceptsUsed.add(rel.fromModelObject)\n            if isinstance(rel.toModelObject, ModelConcept):\n                conceptsUsed.add(rel.toModelObject)\n    for qn in (XbrlConst.qnXbrliIdentifier, XbrlConst.qnXbrliPeriod, XbrlConst.qnXbrliUnit):\n        conceptsUsed.add(self.modelXbrl.qnameConcepts[qn])\n    conceptsUsed -= {None}\n    self.conceptsUsed = conceptsUsed\n", "label": 1}
{"function": "\n\ndef _collect_all_modules(self):\n    'Collect modules, so we can handle imports later'\n    for pkgroot in self.path:\n        pkgroot = os.path.abspath(pkgroot)\n        has_followlinks = (sys.version_info >= (2, 6))\n        if has_followlinks:\n            allfiles = os.walk(pkgroot, followlinks=True)\n        else:\n            allfiles = os.walk(pkgroot)\n        for (root, dirs, files) in allfiles:\n            if ('__init__.py' in files):\n                files.remove('__init__.py')\n                if (root != pkgroot):\n                    files.insert(0, '__init__.py')\n            elif (root != pkgroot):\n                dirs[:] = []\n                continue\n            for filename in files:\n                if (not filename.endswith('.py')):\n                    continue\n                path = os.path.join(root, filename)\n                if (not has_followlinks):\n                    path = os.path.abspath(path)\n                module_path = path[(len(pkgroot) + len(os.sep)):]\n                if (os.path.basename(module_path) == '__init__.py'):\n                    module_name = os.path.dirname(module_path)\n                else:\n                    module_name = module_path[:(- 3)]\n                assert ('.' not in module_name), ('Invalid module file name: %s' % module_path)\n                module_name = module_name.replace(os.sep, '.')\n                self._collected.setdefault(module_name, path)\n", "label": 1}
{"function": "\n\ndef _sync_author_detail(self, key='author'):\n    context = self._getContext()\n    detail = context.get(('%ss' % key), [FeedParserDict()])[(- 1)]\n    if detail:\n        name = detail.get('name')\n        email = detail.get('email')\n        if (name and email):\n            context[key] = ('%s (%s)' % (name, email))\n        elif name:\n            context[key] = name\n        elif email:\n            context[key] = email\n    else:\n        (author, email) = (context.get(key), None)\n        if (not author):\n            return\n        emailmatch = re.search('(([a-zA-Z0-9\\\\_\\\\-\\\\.\\\\+]+)@((\\\\[[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.)|(([a-zA-Z0-9\\\\-]+\\\\.)+))([a-zA-Z]{2,4}|[0-9]{1,3})(\\\\]?))(\\\\?subject=\\\\S+)?', author)\n        if emailmatch:\n            email = emailmatch.group(0)\n            author = author.replace(email, '')\n            author = author.replace('()', '')\n            author = author.replace('<>', '')\n            author = author.replace('&lt;&gt;', '')\n            author = author.strip()\n            if (author and (author[0] == '(')):\n                author = author[1:]\n            if (author and (author[(- 1)] == ')')):\n                author = author[:(- 1)]\n            author = author.strip()\n        if (author or email):\n            context.setdefault(('%s_detail' % key), detail)\n        if author:\n            detail['name'] = author\n        if email:\n            detail['email'] = email\n", "label": 1}
{"function": "\n\n@classmethod\ndef _generate_resolve_ivy(cls, jars, excludes, ivyxml, confs, resolve_hash_name, pinned_artifacts=None, jar_dep_manager=None):\n    org = IvyUtils.INTERNAL_ORG_NAME\n    name = resolve_hash_name\n    extra_configurations = [conf for conf in confs if (conf and (conf != 'default'))]\n    jars_by_key = OrderedDict()\n    for jar in jars:\n        jars = jars_by_key.setdefault((jar.org, jar.name), [])\n        jars.append(jar)\n    manager = (jar_dep_manager or JarDependencyManagement.global_instance())\n    artifact_set = PinnedJarArtifactSet(pinned_artifacts)\n    for jars in jars_by_key.values():\n        for (i, dep) in enumerate(jars):\n            direct_coord = M2Coordinate.create(dep)\n            managed_coord = artifact_set[direct_coord]\n            if (direct_coord.rev != managed_coord.rev):\n                coord = manager.resolve_version_conflict(managed_coord, direct_coord, force=dep.force)\n                jars[i] = dep.copy(rev=coord.rev)\n            elif dep.force:\n                artifact_set.put(direct_coord)\n    dependencies = [cls._generate_jar_template(jars) for jars in jars_by_key.values()]\n    overrides = [cls._generate_override_template(_coord) for _coord in artifact_set]\n    excludes = [cls._generate_exclude_template(exclude) for exclude in excludes]\n    template_data = TemplateData(org=org, module=name, extra_configurations=extra_configurations, dependencies=dependencies, excludes=excludes, overrides=overrides)\n    template_relpath = os.path.join('templates', 'ivy_utils', 'ivy.mustache')\n    cls._write_ivy_xml_file(ivyxml, template_data, template_relpath)\n", "label": 1}
{"function": "\n\ndef solve_TLE(self, board):\n    '\\n\\n        :param board: a 9x9 2D array\\n        :return: Boolean\\n        '\n    n = len(board)\n    if all([(board[(i / n)][(i % n)] != '.') for i in xrange((n * n))]):\n        return True\n    for i in xrange(n):\n        for j in xrange(n):\n            if (board[i][j] == '.'):\n                for num in range(1, 10):\n                    num_str = str(num)\n                    condition_row = all([(board[i][col] != num_str) for col in xrange(n)])\n                    condition_col = all([(board[row][j] != num_str) for row in xrange(n)])\n                    condition_square = all([(board[(((i / 3) * 3) + (count / 3))][(((j / 3) * 3) + (count % 3))] != num_str) for count in xrange(n)])\n                    if (condition_col and condition_row and condition_square):\n                        board[i][j] = num_str\n                        if (not self.solve(board)):\n                            board[i][j] = '.'\n                        else:\n                            return True\n    return False\n", "label": 1}
{"function": "\n\ndef _make_cfg_defaults(self, module_name=NotGiven, default_distribution=NotGiven, guess_maintainer=NotGiven):\n    defaults = {\n        \n    }\n    default_re = re.compile('^.* \\\\(Default: (.*)\\\\)$')\n    for (longopt, shortopt, description) in stdeb_cfg_options:\n        assert longopt.endswith('=')\n        assert (longopt.lower() == longopt)\n        key = longopt[:(- 1)]\n        matchobj = default_re.search(description)\n        if (matchobj is not None):\n            groups = matchobj.groups()\n            assert (len(groups) == 1)\n            value = groups[0]\n            if (value == '<source-debianized-setup-name>'):\n                assert (key == 'source')\n                value = source_debianize_name(module_name)\n            elif (value == 'python-<debianized-setup-name>'):\n                assert (key == 'package')\n                value = ('python-' + debianize_name(module_name))\n            elif (value == 'python3-<debianized-setup-name>'):\n                assert (key == 'package3')\n                value = ('python3-' + debianize_name(module_name))\n            elif (value == '<setup-maintainer-or-author>'):\n                assert (key == 'maintainer')\n                value = guess_maintainer\n            if (key == 'suite'):\n                if (default_distribution is not None):\n                    value = default_distribution\n                    log.warn('Deprecation warning: you are using the --default-distribution option. Switch to the --suite option.')\n        else:\n            value = ''\n        defaults[key] = value\n    return defaults\n", "label": 1}
{"function": "\n\ndef generate_extensions(self, extensions, enums, functions):\n    write = set()\n    written = (set((enum.name for enum in enums)) | set((function.proto.name for function in functions)))\n    f = self._f_h\n    self.write_functions(f, write, written, extensions)\n    f = self._f_c\n    if (self.spec.NAME in ('gl', 'glx', 'wgl')):\n        for ext in set((ext.name for ext in extensions)):\n            f.write('int GLAD_{};\\n'.format(ext))\n    written = set()\n    for ext in extensions:\n        if (ext.name == 'GLX_SGIX_video_source'):\n            f.write('#ifdef _VL_H_\\n')\n        if (ext.name == 'GLX_SGIX_dmbuffer'):\n            f.write('#ifdef _DM_BUFFER_H_\\n')\n        for func in ext.functions:\n            if ((func in write) and (func not in written)):\n                self.write_function(f, func)\n                written.add(func)\n        if (ext.name in ('GLX_SGIX_video_source', 'GLX_SGIX_dmbuffer')):\n            f.write('#endif\\n')\n", "label": 1}
{"function": "\n\ndef convert(self, value, param, ctx):\n    column_properties = value.split(':')\n    name = column_properties[0]\n    if (not valid_underscore_name(name)):\n        ctx.fail('The name provided is not a valid variable name')\n    try:\n        type_properties = column_properties[1].split(',')\n        type = (type_properties[0].lower() or COLUMN_TYPE_DEFAULT)\n    except IndexError:\n        type_properties = []\n        type = COLUMN_TYPE_DEFAULT\n    if (type not in COLUMN_TYPE_MAPPING):\n        ctx.fail(('The type specified for column %s is invalid' % name))\n    try:\n        if type_properties[1]:\n            length = int(type_properties[1])\n        else:\n            length = None\n    except IndexError:\n        length = None\n    except ValueError:\n        ctx.fail(('The length specified for column %s is invalid' % name))\n    if (length and (type not in COLUMN_TYPES_SUPPORTING_LENGTH)):\n        ctx.fail(('The length specified for column %s is not allowed for %s types' % (name, type)))\n    try:\n        modifiers = column_properties[2].lower().split(',')\n    except IndexError:\n        modifiers = []\n    for modifier in modifiers:\n        if (modifier not in COLUMN_MODIFIER_MAPPING):\n            ctx.fail(('The column modifier %s for column %s is invalid' % (modifier, name)))\n    return (name, type, length, modifiers)\n", "label": 1}
{"function": "\n\ndef performAction(self, action):\n    self.steps += 1\n    if (action == self.TurnAround):\n        self._turn()\n    elif (action == self.Forward):\n        self._forward()\n    else:\n        r = random()\n        if (self.env.perseus[1] == 3):\n            if (r < 0.1):\n                self._turn()\n            elif (r < 0.9):\n                self._backup()\n        elif (((self.env.perseus[1] == 2) and (self.env.perseusDir == 3)) or ((self.env.perseus[1] == 4) and (self.env.perseusDir == 1))):\n            if (r < 0.3):\n                self._turn()\n            elif (r < 0.6):\n                self._backup()\n        elif (r < 0.7):\n            self._backup()\n", "label": 1}
{"function": "\n\ndef check_internet_scheme(self, elb_item):\n    '\\n        alert when an ELB has an \"internet-facing\" scheme.\\n        '\n    scheme = elb_item.config.get('scheme', None)\n    vpc = elb_item.config.get('vpc_id', None)\n    if (scheme and (scheme == 'internet-facing') and (not vpc)):\n        self.add_issue(1, 'ELB is Internet accessible.', elb_item)\n    elif (scheme and (scheme == 'internet-facing') and vpc):\n        security_groups = elb_item.config.get('security_groups', [])\n        for sgid in security_groups:\n            sg = Item.query.filter(Item.name.ilike((('%' + sgid) + '%'))).first()\n            if (not sg):\n                continue\n            sg_cidrs = []\n            config = sg.revisions[0].config\n            for rule in config.get('rules', []):\n                cidr = rule.get('cidr_ip', '')\n                if ((rule.get('rule_type', None) == 'ingress') and cidr):\n                    if ((not _check_rfc_1918(cidr)) and (not self._check_inclusion_in_network_whitelist(cidr))):\n                        sg_cidrs.append(cidr)\n            if sg_cidrs:\n                notes = 'SG [{sgname}] via [{cidr}]'.format(sgname=sg.name, cidr=', '.join(sg_cidrs))\n                self.add_issue(1, 'VPC ELB is Internet accessible.', elb_item, notes=notes)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.status = sentry_common_service.ttypes.TSentryResponseStatus()\n                self.status.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.SET):\n                self.privileges = set()\n                (_etype87, _size84) = iprot.readSetBegin()\n                for _i88 in xrange(_size84):\n                    _elem89 = iprot.readString()\n                    self.privileges.add(_elem89)\n                iprot.readSetEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef resolve(self, context, ignore_failures=False):\n    if isinstance(self.var, Variable):\n        try:\n            obj = self.var.resolve(context)\n        except VariableDoesNotExist:\n            if ignore_failures:\n                obj = None\n            else:\n                string_if_invalid = context.template.engine.string_if_invalid\n                if string_if_invalid:\n                    if ('%s' in string_if_invalid):\n                        return (string_if_invalid % self.var)\n                    else:\n                        return string_if_invalid\n                else:\n                    obj = string_if_invalid\n    else:\n        obj = self.var\n    for (func, args) in self.filters:\n        arg_vals = []\n        for (lookup, arg) in args:\n            if (not lookup):\n                arg_vals.append(mark_safe(arg))\n            else:\n                arg_vals.append(arg.resolve(context))\n        if getattr(func, 'expects_localtime', False):\n            obj = template_localtime(obj, context.use_tz)\n        if getattr(func, 'needs_autoescape', False):\n            new_obj = func(obj, *arg_vals, autoescape=context.autoescape)\n        else:\n            new_obj = func(obj, *arg_vals)\n        if (getattr(func, 'is_safe', False) and isinstance(obj, SafeData)):\n            obj = mark_safe(new_obj)\n        elif isinstance(obj, EscapeData):\n            obj = mark_for_escaping(new_obj)\n        else:\n            obj = new_obj\n    return obj\n", "label": 1}
{"function": "\n\ndef updateIndels(self, snp, is_negative_strand):\n    contig = snp.chromosome\n    lcontig = self.mFasta.getLength(contig)\n    code = self.mAnnotations.getSequence(contig, '+', snp.pos, (snp.pos + 2))\n    self.mCode = code\n    variants = snp.genotype.split('/')\n    for variant in variants:\n        if (variant[0] == '*'):\n            self.mVariantType.append('W')\n        elif (variant[0] == '+'):\n            toinsert = variant[1:]\n            self.mVariantType.append('I')\n        elif (variant[0] == '-'):\n            todelete = variant[1:]\n            self.mVariantType.append('D')\n        else:\n            raise ValueError((\"unknown variant sign '%s'\" % variant[0]))\n    if (code[0] and (code[1] not in 'abcABC')):\n        return\n    if is_negative_strand:\n        variants = [Genomics.complement(x) for x in variants]\n    for reference_codon in self.mReferenceCodons:\n        variants = snp.genotype.split('/')\n        variants = [x[1:] for x in variants]\n        for variant in variants:\n            if ((len(variant) % 3) != 0):\n                self.mVariantCodons.append('!')\n            else:\n                self.mVariantCodons.append(variant)\n        self.mVariantAAs.extend([Genomics.translate(x) for x in self.mVariantCodons])\n", "label": 1}
{"function": "\n\ndef poll(self, event, timeout):\n    if self.state.SHUTDOWN:\n        raise Error(errno.EBADF)\n    if (not (event in ('recv', 'send', 'acks'))):\n        raise Error(errno.EINVAL)\n    if (event == 'recv'):\n        if (self.state.ESTABLISHED or self.state.CLOSE_WAIT):\n            ptype = super(DataLinkConnection, self).poll(event, timeout)\n            if (self.state.ESTABLISHED or self.state.CLOSE_WAIT):\n                return (ptype == ProtocolDataUnit.Information)\n            else:\n                return False\n    if (event == 'send'):\n        if self.state.ESTABLISHED:\n            if super(DataLinkConnection, self).poll(event, timeout):\n                return self.state.ESTABLISHED\n    if (event == 'acks'):\n        with self.acks_ready:\n            while (not (self.acks_recvd > 0)):\n                self.acks_ready.wait(timeout)\n            if (self.acks_recvd > 0):\n                self.acks_recvd = (self.acks_recvd - 1)\n                return True\n    return False\n", "label": 1}
{"function": "\n\ndef compareAddressEq(self, addr1, addr2):\n    'Checks if two addresses are equal, considering local/useable associations'\n    if (addr1 is None):\n        return (addr2 is None)\n    if (addr2 is None):\n        return False\n    try:\n        if (addr1.addressDetails == addr2.addressDetails):\n            return True\n    except AttributeError:\n        pass\n    if (isinstance(addr1, ActorAddress) and isinstance(addr1.addressDetails, ActorLocalAddress)):\n        if (isinstance(addr2, ActorAddress) and isinstance(addr2.addressDetails, ActorLocalAddress)):\n            return (addr1.addressDetails == addr2.addressDetails)\n        try:\n            return ((addr1.addressDetails.generatingActor == self._thisActorAddr) and self._managed[addr1.addressDetails.addressInstanceNum] and (self._managed[addr1.addressDetails.addressInstanceNum].addressDetails == addr2.addressDetails))\n        except AttributeError:\n            return False\n    if (isinstance(addr2, ActorAddress) and isinstance(addr2.addressDetails, ActorLocalAddress)):\n        try:\n            return ((addr2.addressDetails.generatingActor == self._thisActorAddr) and self._managed[addr2.addressDetails.addressInstanceNum] and (self._managed[addr2.addressDetails.addressInstanceNum].addressDetails == addr1.addressDetails))\n        except AttributeError:\n            return False\n    return False\n", "label": 1}
{"function": "\n\ndef populate(self, installed_apps=None):\n    '\\n        Loads application configurations and models.\\n\\n        This method imports each application module and then each model module.\\n\\n        It is thread safe and idempotent, but not reentrant.\\n        '\n    if self.ready:\n        return\n    with self._lock:\n        if self.ready:\n            return\n        if self.app_configs:\n            raise RuntimeError(\"populate() isn't reentrant\")\n        for entry in installed_apps:\n            if isinstance(entry, AppConfig):\n                app_config = entry\n            else:\n                app_config = AppConfig.create(entry)\n            if (app_config.label in self.app_configs):\n                raise ImproperlyConfigured((\"Application labels aren't unique, duplicates: %s\" % app_config.label))\n            self.app_configs[app_config.label] = app_config\n        counts = Counter((app_config.name for app_config in self.app_configs.values()))\n        duplicates = [name for (name, count) in counts.most_common() if (count > 1)]\n        if duplicates:\n            raise ImproperlyConfigured((\"Application names aren't unique, duplicates: %s\" % ', '.join(duplicates)))\n        self.apps_ready = True\n        for app_config in self.app_configs.values():\n            all_models = self.all_models[app_config.label]\n            app_config.import_models(all_models)\n        self.clear_cache()\n        self.models_ready = True\n        for app_config in self.get_app_configs():\n            app_config.ready()\n        self.ready = True\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@gof.local_optimizer([AdvancedIncSubtensor1])\ndef local_set_to_inc_subtensor(node):\n    '\\n    AdvancedIncSubtensor1(x, x[ilist]+other, ilist, set_instead_of_inc=True) ->\\n    AdvancedIncSubtensor1(x, other, ilist, set_instead_of_inc=False)\\n\\n    '\n    if (isinstance(node.op, AdvancedIncSubtensor1) and node.op.set_instead_of_inc and node.inputs[1].owner and isinstance(node.inputs[1].owner.op, Elemwise) and isinstance(node.inputs[1].owner.op.scalar_op, scalar.Add)):\n        addn = node.inputs[1].owner\n        subn = None\n        other = None\n        if (addn.inputs[0].owner and isinstance(addn.inputs[0].owner.op, AdvancedSubtensor1)):\n            subn = addn.inputs[0].owner\n            other = addn.inputs[1]\n        elif (addn.inputs[1].owner and isinstance(addn.inputs[1].owner.op, AdvancedSubtensor1)):\n            subn = addn.inputs[1].owner\n            other = addn.inputs[0]\n        else:\n            return\n        if ((subn.inputs[1] != node.inputs[2]) or (subn.inputs[0] != node.inputs[0])):\n            return\n        ret = advanced_inc_subtensor1(node.inputs[0], other, node.inputs[2])\n        copy_stack_trace(node.outputs, ret)\n        return [ret]\n", "label": 1}
{"function": "\n\ndef show_model_changes(new, old=None, fields=None, always=False):\n    'Given a Model object, print a list of changes from its pristine\\n    version stored in the database. Return a boolean indicating whether\\n    any changes were found.\\n\\n    `old` may be the \"original\" object to avoid using the pristine\\n    version from the database. `fields` may be a list of fields to\\n    restrict the detection to. `always` indicates whether the object is\\n    always identified, regardless of whether any changes are present.\\n    '\n    old = (old or new._db._get(type(new), new.id))\n    changes = []\n    for field in old:\n        if ((field == 'mtime') or (fields and (field not in fields))):\n            continue\n        line = _field_diff(field, old, new)\n        if line:\n            changes.append('  {0}: {1}'.format(field, line))\n    for field in (set(new) - set(old)):\n        if (fields and (field not in fields)):\n            continue\n        changes.append('  {0}: {1}'.format(field, colorize('text_highlight', new.formatted()[field])))\n    if (changes or always):\n        print_(format(old))\n    if changes:\n        print_('\\n'.join(changes))\n    return bool(changes)\n", "label": 1}
{"function": "\n\ndef convert_values(self):\n    'Convert datetimes to a comparable value in an expression.\\n        '\n\n    def stringify(value):\n        if (self.encoding is not None):\n            encoder = partial(pprint_thing_encoded, encoding=self.encoding)\n        else:\n            encoder = pprint_thing\n        return encoder(value)\n    (lhs, rhs) = (self.lhs, self.rhs)\n    if (is_term(lhs) and lhs.is_datetime and is_term(rhs) and rhs.isscalar):\n        v = rhs.value\n        if isinstance(v, (int, float)):\n            v = stringify(v)\n        v = pd.Timestamp(_ensure_decoded(v))\n        if (v.tz is not None):\n            v = v.tz_convert('UTC')\n        self.rhs.update(v)\n    if (is_term(rhs) and rhs.is_datetime and is_term(lhs) and lhs.isscalar):\n        v = lhs.value\n        if isinstance(v, (int, float)):\n            v = stringify(v)\n        v = pd.Timestamp(_ensure_decoded(v))\n        if (v.tz is not None):\n            v = v.tz_convert('UTC')\n        self.lhs.update(v)\n", "label": 1}
{"function": "\n\ndef get_status(self, stats, diff):\n    additions = stats[0]\n    deletions = stats[1]\n    if ((additions > 0) and (deletions == 0)):\n        status = 'added'\n    elif ((additions == 0) and (deletions > 0)):\n        status = 'deleted'\n    elif ((additions > 0) and (deletions > 0)):\n        status = 'modified'\n    else:\n        try:\n            if diff.new_file:\n                status = 'added'\n            elif diff.deleted_file:\n                status = 'deleted'\n            elif ((additions == 0) and (deletions == 0)):\n                status = 'added'\n                self.logger.warning('GitQuerier: addition and deletion = 0')\n            else:\n                status = 'modified'\n        except:\n            status = 'modified'\n    return status\n", "label": 1}
{"function": "\n\n@is_view('virt')\ndef links(request, hosts):\n    try:\n        fqdn = request.GET['fqdn']\n    except KeyError:\n        return None\n    current_node = None\n    for host in hosts:\n        if (fqdn == host['fqdn']):\n            current_node = host\n            break\n        for node in host.get('virtualization', {\n            \n        }).get('guests', []):\n            if (fqdn == node['fqdn']):\n                current_node = node\n                break\n        if current_node:\n            break\n    if current_node:\n        try:\n            links = current_node['kitchen']['data']['links']\n        except KeyError:\n            return None\n        for link in links:\n            if (link.get('title') == 'monitoring'):\n                return redirect(link['url'])\n        else:\n            return None\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.end_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.endpoints = []\n                (_etype31, _size28) = iprot.readListBegin()\n                for _i32 in xrange(_size28):\n                    _elem33 = iprot.readString()\n                    self.endpoints.append(_elem33)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef lock(self):\n    '\\n        Acquire this lock.\\n\\n        @rtype: C{bool}\\n        @return: True if the lock is acquired, false otherwise.\\n\\n        @raise: Any exception os.symlink() may raise, other than\\n        EEXIST.\\n        '\n    clean = True\n    while True:\n        try:\n            symlink(str(os.getpid()), self.name)\n        except OSError as e:\n            if (_windows and (e.errno in (errno.EACCES, errno.EIO))):\n                return False\n            if (e.errno == errno.EEXIST):\n                try:\n                    pid = readlink(self.name)\n                except (IOError, OSError) as e:\n                    if (e.errno == errno.ENOENT):\n                        continue\n                    elif (_windows and (e.errno == errno.EACCES)):\n                        return False\n                    raise\n                try:\n                    if (kill is not None):\n                        kill(int(pid), 0)\n                except OSError as e:\n                    if (e.errno == errno.ESRCH):\n                        try:\n                            rmlink(self.name)\n                        except OSError as e:\n                            if (e.errno == errno.ENOENT):\n                                continue\n                            raise\n                        clean = False\n                        continue\n                    raise\n                return False\n            raise\n        self.locked = True\n        self.clean = clean\n        return True\n", "label": 1}
{"function": "\n\ndef _check_command_response(response, msg=None, allowable_errors=None):\n    'Check the response to a command for errors.\\n    '\n    if ('ok' not in response):\n        raise OperationFailure(response.get('$err'), response.get('code'), response)\n    if response.get('wtimeout', False):\n        raise WTimeoutError(response.get('errmsg', response.get('err')), response.get('code'), response)\n    if (not response['ok']):\n        details = response\n        if ('raw' in response):\n            for shard in itervalues(response['raw']):\n                if (shard.get('errmsg') and (not shard.get('ok'))):\n                    details = shard\n                    break\n        errmsg = details['errmsg']\n        if ((allowable_errors is None) or (errmsg not in allowable_errors)):\n            if (errmsg.startswith('not master') or errmsg.startswith('node is recovering')):\n                raise NotMasterError(errmsg, response)\n            if (errmsg == 'db assertion failure'):\n                errmsg = (\"db assertion failure, assertion: '%s'\" % details.get('assertion', ''))\n                raise OperationFailure(errmsg, details.get('assertionCode'), response)\n            code = details.get('code')\n            if (code in (11000, 11001, 12582)):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif (code == 50):\n                raise ExecutionTimeout(errmsg, code, response)\n            elif (code == 43):\n                raise CursorNotFound(errmsg, code, response)\n            msg = (msg or '%s')\n            raise OperationFailure((msg % errmsg), code, response)\n", "label": 1}
{"function": "\n\ndef parse_shorts(tokens, options):\n    \"shorts ::= '-' ( chars )* [ [ ' ' ] chars ] ;\"\n    token = tokens.move()\n    assert (token.startswith('-') and (not token.startswith('--')))\n    left = token.lstrip('-')\n    parsed = []\n    while (left != ''):\n        (short, left) = (('-' + left[0]), left[1:])\n        similar = [o for o in options if (o.short == short)]\n        if (len(similar) > 1):\n            raise tokens.error(('%s is specified ambiguously %d times' % (short, len(similar))))\n        elif (len(similar) < 1):\n            o = Option(short, None, 0)\n            options.append(o)\n            if (tokens.error is DocoptExit):\n                o = Option(short, None, 0, True)\n        else:\n            o = Option(short, similar[0].long, similar[0].argcount, similar[0].value)\n            value = None\n            if (o.argcount != 0):\n                if (left == ''):\n                    if (tokens.current() is None):\n                        raise tokens.error(('%s requires argument' % short))\n                    value = tokens.move()\n                else:\n                    value = left\n                    left = ''\n            if (tokens.error is DocoptExit):\n                o.value = (value if (value is not None) else True)\n        parsed.append(o)\n    return parsed\n", "label": 1}
{"function": "\n\ndef execute(self):\n    'If the training flag is set, train the metamodel. Otherwise,\\n        predict outputs.\\n        '\n    if self._train:\n        input_data = self._param_data\n        if (self.warm_restart is False):\n            input_data = []\n            base = 0\n        else:\n            base = len(input_data)\n        for name in self._surrogate_input_names:\n            train_name = ('params.%s' % name)\n            val = self.get(train_name)\n            num_sample = len(val)\n            for j in xrange(base, (base + num_sample)):\n                if (j > (len(input_data) - 1)):\n                    input_data.append([])\n                input_data[j].append(val[(j - base)])\n        for name in self._surrogate_output_names:\n            train_name = ('responses.%s' % name)\n            output_data = self._response_data[name]\n            if (self.warm_restart is False):\n                output_data = []\n            output_data.extend(self.get(train_name))\n            surrogate = self._get_surrogate(name)\n            if (surrogate is not None):\n                surrogate.train(input_data, output_data)\n        self._train = False\n    inputs = []\n    for name in self._surrogate_input_names:\n        val = self.get(name)\n        inputs.append(val)\n    for name in self._surrogate_output_names:\n        surrogate = self._get_surrogate(name)\n        if (surrogate is not None):\n            setattr(self, name, surrogate.predict(inputs))\n", "label": 1}
{"function": "\n\n@count_calls\ndef check_referenced_versions(self):\n    'Deeply checks all the references in the scene and returns a\\n        dictionary which uses the ids of the Versions as key and the action as\\n        value.\\n\\n        Uses the top level references to get a Stalker Version instance and\\n        then tracks all the changes from these Version instances.\\n\\n        :return: list\\n        '\n    dfs_version_references = []\n    version = self.get_current_version()\n    resolution_dictionary = empty_reference_resolution(root=self.get_referenced_versions())\n    for v in version.walk_hierarchy():\n        dfs_version_references.append(v)\n    dfs_version_references.pop(0)\n    for v in reversed(dfs_version_references):\n        to_be_updated_list = []\n        for ref_v in v.inputs:\n            if (not ref_v.is_latest_published_version()):\n                to_be_updated_list.append(ref_v)\n        if to_be_updated_list:\n            action = 'create'\n            latest_published_version = v.latest_published_version\n            if (latest_published_version and (not v.is_latest_published_version())):\n                if all([(ref_v.latest_published_version in latest_published_version.inputs) for ref_v in to_be_updated_list]):\n                    action = 'update'\n                else:\n                    action = 'create'\n        else:\n            if v.is_latest_published_version():\n                action = 'leave'\n            else:\n                action = 'update'\n            if any((((rev_v in resolution_dictionary['update']) or (rev_v in resolution_dictionary['create'])) for rev_v in v.inputs)):\n                action = 'create'\n        resolution_dictionary[action].append(v)\n    return resolution_dictionary\n", "label": 1}
{"function": "\n\ndef _load_extensions(self):\n    'Dynamically load all the extensions .'\n    extensions = {\n        \n    }\n    abs_path = os.path.dirname(os.path.abspath(__file__))\n    extdirs = [e for e in os.listdir(abs_path) if ((not e.startswith('tests')) and (not e.endswith('.pyc')) and (not e.endswith('.py')))]\n    for e in extdirs:\n        log.info(e)\n        extpath = ((abs_path + '/') + e)\n        ext_files = [f for f in os.listdir(extpath) if (f.endswith('.py') and (not f.startswith('__init__')))]\n        for f in ext_files:\n            log.info(f)\n            ext_name = ((('toscaparser/extensions/' + e) + '/') + f.strip('.py'))\n            ext_name = ext_name.replace('/', '.')\n            try:\n                extinfo = importlib.import_module(ext_name)\n                version = getattr(extinfo, 'VERSION')\n                defs_file = ((extpath + '/') + getattr(extinfo, 'DEFS_FILE'))\n                sections = getattr(extinfo, 'SECTIONS', ())\n                extensions[version] = {\n                    'sections': sections,\n                    'defs_file': defs_file,\n                }\n            except ImportError:\n                raise ToscaExtImportError(ext_name=ext_name)\n            except AttributeError:\n                attrs = ', '.join(REQUIRED_ATTRIBUTES)\n                raise ToscaExtAttributeError(ext_name=ext_name, attrs=attrs)\n    return extensions\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    asset_cache = {\n        \n    }\n    for (dt, series) in self.df.iterrows():\n        if (dt < self.start_date):\n            continue\n        if (dt > self.end_date):\n            return\n        event = FetcherEvent()\n        event.dt = dt\n        for (k, v) in series.iteritems():\n            if isinstance(v, numpy.integer):\n                v = int(v)\n            setattr(event, k, v)\n        if (event.sid in asset_cache):\n            event.sid = asset_cache[event.sid]\n        elif hasattr(event.sid, 'start_date'):\n            asset_cache[event.sid] = event.sid\n        elif (self.finder and isinstance(event.sid, int)):\n            asset = self.finder.retrieve_asset(event.sid, default_none=True)\n            if asset:\n                event.sid = asset_cache[asset] = asset\n            elif self.mask:\n                continue\n            elif (self.symbol is None):\n                event.sid = asset_cache[event.sid] = Equity(event.sid)\n        event.type = DATASOURCE_TYPE.CUSTOM\n        event.source_id = self.namestring\n        (yield event)\n", "label": 1}
{"function": "\n\ndef _caas_2_2_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_tag_tagKey_ALLFILTERS(self, method, url, body, headers):\n    (_, params) = url.split('?')\n    parameters = params.split('&')\n    for parameter in parameters:\n        (key, value) = parameter.split('=')\n        if (key == 'id'):\n            assert (value == 'fake_id')\n        elif (key == 'name'):\n            assert (value == 'fake_name')\n        elif (key == 'valueRequired'):\n            assert (value == 'false')\n        elif (key == 'displayOnReport'):\n            assert (value == 'false')\n        elif (key == 'pageSize'):\n            assert (value == '250')\n        else:\n            raise ValueError('Could not find in url parameters {0}:{1}'.format(key, value))\n    body = self.fixtures.load('tag_tagKey_list.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 1}
{"function": "\n\ndef init_colors(self):\n    'Configure the coloring of the widget'\n    if self.pure:\n        return\n    try:\n        colors = self.config.ZMQInteractiveShell.colors\n    except AttributeError:\n        colors = None\n    try:\n        style = self.config.IPythonWidget.colors\n    except AttributeError:\n        style = None\n    if colors:\n        colors = colors.lower()\n        if (colors in ('lightbg', 'light')):\n            colors = 'lightbg'\n        elif (colors in ('dark', 'linux')):\n            colors = 'linux'\n        else:\n            colors = 'nocolor'\n    elif style:\n        if (style == 'bw'):\n            colors = 'nocolor'\n        elif styles.dark_style(style):\n            colors = 'linux'\n        else:\n            colors = 'lightbg'\n    else:\n        colors = None\n    widget = self.widget\n    if style:\n        widget.style_sheet = styles.sheet_from_template(style, colors)\n        widget.syntax_style = style\n        widget._syntax_style_changed()\n        widget._style_sheet_changed()\n    elif colors:\n        widget.set_default_style(colors=colors)\n    else:\n        widget.set_default_style()\n    if self.stylesheet:\n        if os.path.isfile(self.stylesheet):\n            with open(self.stylesheet) as f:\n                sheet = f.read()\n            widget.style_sheet = sheet\n            widget._style_sheet_changed()\n        else:\n            raise IOError(('Stylesheet %r not found.' % self.stylesheet))\n", "label": 1}
{"function": "\n\ndef parse(self, parse_until=None):\n    if (parse_until is None):\n        parse_until = []\n    nodelist = self.create_nodelist()\n    while self.tokens:\n        token = self.next_token()\n        if (token.token_type == 0):\n            self.extend_nodelist(nodelist, TextNode(token.contents), token)\n        elif (token.token_type == 1):\n            if (not token.contents):\n                self.empty_variable(token)\n            filter_expression = self.compile_filter(token.contents)\n            var_node = self.create_variable_node(filter_expression)\n            self.extend_nodelist(nodelist, var_node, token)\n        elif (token.token_type == 2):\n            try:\n                command = token.contents.split()[0]\n            except IndexError:\n                self.empty_block_tag(token)\n            if (command in parse_until):\n                self.prepend_token(token)\n                return nodelist\n            self.enter_command(command, token)\n            try:\n                compile_func = self.tags[command]\n            except KeyError:\n                self.invalid_block_tag(token, command, parse_until)\n            try:\n                compiled_result = compile_func(self, token)\n            except TemplateSyntaxError as e:\n                if (not self.compile_function_error(token, e)):\n                    raise\n            self.extend_nodelist(nodelist, compiled_result, token)\n            self.exit_command()\n    if parse_until:\n        self.unclosed_block_tag(parse_until)\n    return nodelist\n", "label": 1}
{"function": "\n\n@field_names.setter\ndef field_names(self, val):\n    val = [self._unicode(x) for x in val]\n    self._validate_option('field_names', val)\n    if self._field_names:\n        old_names = self._field_names[:]\n    self._field_names = val\n    if (self._align and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._align[new_name] = self._align[old_name]\n        for old_name in old_names:\n            if (old_name not in self._align):\n                self._align.pop(old_name)\n    else:\n        self.align = 'c'\n    if (self._valign and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._valign[new_name] = self._valign[old_name]\n        for old_name in old_names:\n            if (old_name not in self._valign):\n                self._valign.pop(old_name)\n    else:\n        self.valign = 't'\n", "label": 1}
{"function": "\n\ndef List(self, device_path):\n    'Prints a directory listing.\\n\\n  Args:\\n    device_path: Directory to list.\\n  '\n    files = adb_commands.AdbCommands.List(self, device_path)\n    files.sort(key=(lambda x: x.filename))\n    maxname = max((len(f.filename) for f in files))\n    maxsize = max((len(str(f.size)) for f in files))\n    for f in files:\n        mode = (((((((((('d' if stat.S_ISDIR(f.mode) else '-') + ('r' if (f.mode & stat.S_IRUSR) else '-')) + ('w' if (f.mode & stat.S_IWUSR) else '-')) + ('x' if (f.mode & stat.S_IXUSR) else '-')) + ('r' if (f.mode & stat.S_IRGRP) else '-')) + ('w' if (f.mode & stat.S_IWGRP) else '-')) + ('x' if (f.mode & stat.S_IXGRP) else '-')) + ('r' if (f.mode & stat.S_IROTH) else '-')) + ('w' if (f.mode & stat.S_IWOTH) else '-')) + ('x' if (f.mode & stat.S_IXOTH) else '-'))\n        t = time.gmtime(f.mtime)\n        (yield ('%s %*d %04d-%02d-%02d %02d:%02d:%02d %-*s\\n' % (mode, maxsize, f.size, t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec, maxname, f.filename)))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 3):\n            if (ftype == TType.STRING):\n                self.column_family = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.super_column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.STRING):\n                self.column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef clean(self):\n    super(MenuItemFormMixin, self).clean()\n    content_type = self.cleaned_data['content_type']\n    object_id = self.cleaned_data['object_id']\n    if ((content_type and (not object_id)) or ((not content_type) and object_id)):\n        raise forms.ValidationError(\"Both 'Content type' and 'Object id' must be specified to use generic relationship\")\n    if (content_type and object_id):\n        try:\n            obj = content_type.get_object_for_this_type(pk=object_id)\n        except ObjectDoesNotExist as e:\n            raise forms.ValidationError(str(e))\n        try:\n            obj.get_absolute_url()\n        except AttributeError as e:\n            raise forms.ValidationError(str(e))\n    if (('is_enabled' in self.cleaned_data) and self.cleaned_data['is_enabled'] and ('link' in self.cleaned_data) and self.cleaned_data['link'].startswith('^')):\n        raise forms.ValidationError('Menu items with regular expression URLs must be disabled.')\n    return self.cleaned_data\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.value = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.timestamp = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.ttl = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\n@register_uncanonicalize\n@register_specialize\n@gof.local_optimizer(ALL_REDUCE)\ndef local_reduce_broadcastable(node):\n    'Remove reduction over broadcastable dimensions.'\n    if isinstance(node.op, T.CAReduce):\n        (reduced,) = node.inputs\n        odtype = node.outputs[0].dtype\n        if (node.op.axis is None):\n            if all(reduced.broadcastable):\n                return [reduced.dimshuffle().astype(odtype)]\n        else:\n            axis = list(node.op.axis)\n            cuttable = [a for a in axis if reduced.broadcastable[a]]\n            if cuttable:\n                new_axis = []\n                pattern = []\n                ii = 0\n                for p in xrange(reduced.ndim):\n                    if (p not in cuttable):\n                        if (p in axis):\n                            new_axis.append(ii)\n                        pattern.append(p)\n                        ii += 1\n                new_reduced = reduced.dimshuffle(*pattern)\n                if new_axis:\n                    if (type(node.op) == theano.tensor.elemwise.CAReduce):\n                        new_op = node.op.__class__(node.op.scalar_op, axis=new_axis)\n                    else:\n                        new_op = node.op.__class__(axis=new_axis)\n                    return [new_op(new_reduced)]\n                else:\n                    return [new_reduced.astype(odtype)]\n", "label": 1}
{"function": "\n\ndef fuzzy(event, base=None, date_format=None):\n    if (not base):\n        base = datetime.now()\n    if date_format:\n        event = datetime.strptime(event, date_format)\n    elif (type(event) == str):\n        event = datetime.fromtimestamp(int(event))\n    elif (type(event) == int):\n        event = datetime.fromtimestamp(event)\n    elif (type(event) != datetime):\n        raise Exception('Cannot convert object `{}` to fuzzy date string'.format(event))\n    delta = (base - event)\n    if (delta.days == 0):\n        if (delta.seconds < 60):\n            return '{} seconds ago'.format(delta.seconds)\n        elif (delta.seconds < 120):\n            return '1 min and {} secs ago'.format((delta.seconds - 60))\n        elif (delta.seconds < TEN_MINS):\n            return '{} mins and {} secs ago'.format((delta.seconds // 60), (delta.seconds % 60))\n        elif (delta.seconds < ONE_HOUR):\n            return '{} minutes ago'.format((delta.seconds // 60))\n        elif (delta.seconds < TWO_HOURS):\n            return '1 hour and {} mins ago'.format(((delta.seconds % ONE_HOUR) // 60))\n        return 'over {} hours ago'.format((delta.seconds // ONE_HOUR))\n    elif (delta.days < 2):\n        return 'over a day ago'\n    elif (delta.days < 7):\n        return 'over {} days ago'.format(delta.days)\n    return '{date:%b} {date.day}, {date.year}'.format(date=event)\n", "label": 1}
{"function": "\n\ndef _convert_relation(self, prop, kwargs):\n    form_columns = getattr(self.view, 'form_columns', None)\n    if (form_columns and (prop.key not in form_columns)):\n        return None\n    remote_model = prop.mapper.class_\n    column = prop.local_remote_pairs[0][0]\n    if (not column.foreign_keys):\n        column = prop.local_remote_pairs[0][1]\n    kwargs['label'] = self._get_label(prop.key, kwargs)\n    kwargs['description'] = self._get_description(prop.key, kwargs)\n    requirement_options = (validators.Optional, validators.InputRequired)\n    if (not any((isinstance(v, requirement_options) for v in kwargs['validators']))):\n        if (column.nullable or (prop.direction.name != 'MANYTOONE')):\n            kwargs['validators'].append(validators.Optional())\n        else:\n            kwargs['validators'].append(validators.InputRequired())\n    if ('allow_blank' not in kwargs):\n        kwargs['allow_blank'] = column.nullable\n    override = self._get_field_override(prop.key)\n    if override:\n        return override(**kwargs)\n    if ((prop.direction.name == 'MANYTOONE') or (not prop.uselist)):\n        return self._model_select_field(prop, False, remote_model, **kwargs)\n    elif (prop.direction.name == 'ONETOMANY'):\n        return self._model_select_field(prop, True, remote_model, **kwargs)\n    elif (prop.direction.name == 'MANYTOMANY'):\n        return self._model_select_field(prop, True, remote_model, **kwargs)\n", "label": 1}
{"function": "\n\ndef test_content():\n    for f in formats:\n        try:\n            raise_error()\n        except:\n            result = format(f)\n            print(result)\n            assert ('test_object' in result)\n            assert ('http://whatever.com' in result)\n            assert ('This is some supplemental information' in result)\n            assert ('raise_error' in result)\n            assert ('call_error' in result)\n            assert ('5' in result)\n            assert ('test_content' in result)\n        else:\n            assert 0\n", "label": 1}
{"function": "\n\ndef split_command_line(command_line):\n    \"This splits a command line into a list of arguments. It splits arguments\\n    on spaces, but handles embedded quotes, doublequotes, and escaped\\n    characters. It's impossible to do this with a regular expression, so I\\n    wrote a little state machine to parse the command line. \"\n    arg_list = []\n    arg = ''\n    state_basic = 0\n    state_esc = 1\n    state_singlequote = 2\n    state_doublequote = 3\n    state_whitespace = 4\n    state = state_basic\n    for c in command_line:\n        if ((state == state_basic) or (state == state_whitespace)):\n            if (c == '\\\\'):\n                state = state_esc\n            elif (c == \"'\"):\n                state = state_singlequote\n            elif (c == '\"'):\n                state = state_doublequote\n            elif c.isspace():\n                if (state == state_whitespace):\n                    None\n                else:\n                    arg_list.append(arg)\n                    arg = ''\n                    state = state_whitespace\n            else:\n                arg = (arg + c)\n                state = state_basic\n        elif (state == state_esc):\n            arg = (arg + c)\n            state = state_basic\n        elif (state == state_singlequote):\n            if (c == \"'\"):\n                state = state_basic\n            else:\n                arg = (arg + c)\n        elif (state == state_doublequote):\n            if (c == '\"'):\n                state = state_basic\n            else:\n                arg = (arg + c)\n    if (arg != ''):\n        arg_list.append(arg)\n    return arg_list\n", "label": 1}
{"function": "\n\ndef ensure_running(retries=15, wait=10):\n    \"Ensure cassandra is running on all nodes.\\n    Runs 'nodetool ring' on a single node continuously until it\\n    reaches the specified number of retries.\\n\\n    INTENDED TO BE RUN ON ONE NODE, NOT ALL.\\n    \"\n    time.sleep(15)\n    for attempt in range(retries):\n        ring = StringIO(fab.run('JAVA_HOME={java_home} {nodetool_bin} ring'.format(java_home=config['java_home'], nodetool_bin=_nodetool_cmd())))\n        broadcast_ips = [x.get('external_ip', x['internal_ip']) for x in config['hosts'].values()]\n        nodes_up = dict(((host, False) for host in broadcast_ips))\n        for line in ring:\n            for host in broadcast_ips:\n                try:\n                    if ((host in line) and (' Up ' in line)):\n                        nodes_up[host] = True\n                except UnicodeDecodeError:\n                    pass\n        for (node, up) in nodes_up.items():\n            if (not up):\n                fab.puts(('Node is not up (yet): %s' % node))\n        if (False not in nodes_up.values()):\n            fab.puts('All nodes available!')\n            return\n        fab.puts(('waiting %d seconds to try again..' % wait))\n        time.sleep(wait)\n    else:\n        fab.abort('Timed out waiting for all nodes to startup')\n", "label": 1}
{"function": "\n\ndef binomial_pdf(x, a, b):\n    'binomial PDF by H. Gene Shin\\n    \\n    '\n    if (a < 1):\n        return 0\n    elif ((x < 0) or (a < x)):\n        return 0\n    elif (b == 0):\n        if (x == 0):\n            return 1\n        else:\n            return 0\n    elif (b == 1):\n        if (x == a):\n            return 1\n        else:\n            return 0\n    else:\n        if (x > (a - x)):\n            p = (1 - b)\n            mn = (a - x)\n            mx = x\n        else:\n            p = b\n            mn = x\n            mx = (a - x)\n        pdf = 1\n        t = 0\n        for q in xrange(1, (mn + 1)):\n            pdf *= ((((a - q) + 1) * p) / ((mn - q) + 1))\n            if (pdf < 1e-100):\n                while (pdf < 0.001):\n                    pdf /= (1 - p)\n                    t -= 1\n            if (pdf > 1e+100):\n                while ((pdf > 1000.0) and (t < mx)):\n                    pdf *= (1 - p)\n                    t += 1\n        for i in xrange((mx - t)):\n            pdf *= (1 - p)\n        pdf = float(('%.10e' % pdf))\n        return pdf\n", "label": 1}
{"function": "\n\ndef read_input(path, stdin=None):\n    \"Stream input the way Hadoop would.\\n\\n    - Resolve globs (``foo_*.gz``).\\n    - Decompress ``.gz`` and ``.bz2`` files.\\n    - If path is ``'-'``, read from stdin\\n    - If path is a directory, recursively read its contents.\\n\\n    You can redefine *stdin* for ease of testing. *stdin* can actually be\\n    any iterable that yields lines (e.g. a list).\\n    \"\n    if (stdin is None):\n        stdin = sys.stdin\n    if (path == '-'):\n        for line in stdin:\n            (yield line)\n        return\n    paths = glob.glob(path)\n    if (not paths):\n        raise IOError(2, ('No such file or directory: %r' % path))\n    elif (len(paths) > 1):\n        for path in paths:\n            for line in read_input(path, stdin=stdin):\n                (yield line)\n        return\n    else:\n        path = paths[0]\n    if os.path.isdir(path):\n        for (dirname, _, filenames) in os.walk(path, followlinks=True):\n            for filename in filenames:\n                for line in read_input(os.path.join(dirname, filename), stdin=stdin):\n                    (yield line)\n        return\n    for line in read_file(path):\n        (yield line)\n", "label": 1}
{"function": "\n\ndef _restore_reduce(self, obj):\n    '\\n        Supports restoring with all elements of __reduce__ as per pep 307.\\n        Assumes that iterator items (the last two) are represented as lists\\n        as per pickler implementation.\\n        '\n    reduce_val = obj[tags.REDUCE]\n    (f, args, state, listitems, dictitems) = map(self._restore, reduce_val)\n    if ((f == tags.NEWOBJ) or (f.__name__ == '__newobj__')):\n        cls = args[0]\n        stage1 = cls.__new__(cls, *args[1:])\n    else:\n        stage1 = f(*args)\n    if state:\n        try:\n            stage1.__setstate__(state)\n        except AttributeError:\n            try:\n                stage1.__dict__.update(state)\n            except AttributeError:\n                for (k, v) in state.items():\n                    setattr(stage1, k, v)\n    if listitems:\n        try:\n            stage1.extend(listitems)\n        except AttributeError:\n            for x in listitems:\n                stage1.append(x)\n    if dictitems:\n        for (k, v) in dictitems:\n            stage1.__setitem__(k, v)\n    self._mkref(stage1)\n    return stage1\n", "label": 1}
{"function": "\n\ndef expand_partitions(containers, partitions):\n    '\\n    Validate the partitions of containers. If there are any containers\\n    not in any partition, place them in an new partition.\\n    '\n    all_names = frozenset((c.name for c in containers if (not c.holy)))\n    holy_names = frozenset((c.name for c in containers if c.holy))\n    neutral_names = frozenset((c.name for c in containers if c.neutral))\n    partitions = [frozenset(p) for p in partitions]\n    unknown = set()\n    holy = set()\n    union = set()\n    for partition in partitions:\n        unknown.update(((partition - all_names) - holy_names))\n        holy.update((partition - all_names))\n        union.update(partition)\n    if unknown:\n        raise BlockadeError(('Partitions contain unknown containers: %s' % list(unknown)))\n    if holy:\n        raise BlockadeError(('Partitions contain holy containers: %s' % list(holy)))\n    leftover = all_names.difference(union)\n    if leftover:\n        partitions.append(leftover)\n    if (not neutral_names.issubset(leftover)):\n        partitions.append(neutral_names)\n    return partitions\n", "label": 1}
{"function": "\n\ndef alloc_jacobian(self):\n    '\\n        Creates a jacobian dictionary with the keys pre-populated and correct\\n        array sizes allocated. caches the result in the component, and\\n        returns that cache if it finds it.\\n\\n        Returns\\n        -----------\\n        dict\\n            pre-allocated jacobian dictionary\\n        '\n    if ((self._jacobian_cache is not None) and (len(self._jacobian_cache) > 0)):\n        return self._jacobian_cache\n    self._jacobian_cache = jac = {\n        \n    }\n    u_vec = self.unknowns\n    p_vec = self.params\n    states = self.states\n    p_size_storage = [(n, m['size']) for (n, m) in iteritems(p_vec) if ((not m.get('pass_by_obj')) and (not m.get('remote')))]\n    s_size_storage = []\n    u_size_storage = []\n    for (n, meta) in iteritems(u_vec):\n        if (meta.get('pass_by_obj') or meta.get('remote')):\n            continue\n        if meta.get('state'):\n            s_size_storage.append((n, meta['size']))\n        u_size_storage.append((n, meta['size']))\n    for (u_var, u_size) in u_size_storage:\n        for (p_var, p_size) in p_size_storage:\n            jac[(u_var, p_var)] = np.zeros((u_size, p_size))\n        for (s_var, s_size) in s_size_storage:\n            jac[(u_var, s_var)] = np.zeros((u_size, s_size))\n    return jac\n", "label": 1}
{"function": "\n\ndef parse_time(val):\n    if (not val):\n        return None\n    hr = mi = 0\n    val = val.lower()\n    amflag = ((- 1) != val.find('a'))\n    pmflag = ((- 1) != val.find('p'))\n    for noise in ':amp.':\n        val = val.replace(noise, ' ')\n    val = val.split()\n    if (len(val) > 1):\n        hr = int(val[0])\n        mi = int(val[1])\n    else:\n        val = val[0]\n        if (len(val) < 1):\n            pass\n        elif ('now' == val):\n            tm = localtime()\n            hr = tm[3]\n            mi = tm[4]\n        elif ('noon' == val):\n            hr = 12\n        elif (len(val) < 3):\n            hr = int(val)\n            if ((not amflag) and (not pmflag) and (hr < 7)):\n                hr += 12\n        elif (len(val) < 5):\n            hr = int(val[:(- 2)])\n            mi = int(val[(- 2):])\n        else:\n            hr = int(val[:1])\n    if (amflag and (hr >= 12)):\n        hr = (hr - 12)\n    if (pmflag and (hr < 12)):\n        hr = (hr + 12)\n    return time(hr, mi)\n", "label": 1}
{"function": "\n\ndef resolve_job_references(io_hash, job_outputs, should_resolve=True):\n    '\\n    :param io_hash: an input or output hash in which to resolve any job-based object references possible\\n    :type io_hash: dict\\n    :param job_outputs: a mapping of finished local jobs to their output hashes\\n    :type job_outputs: dict\\n    :param should_resolve: whether it is an error if a job-based object reference in *io_hash* cannot be resolved yet\\n    :type should_resolve: boolean\\n\\n    Modifies *io_hash* in-place.\\n    '\n    q = []\n    for field in io_hash:\n        if is_job_ref(io_hash[field]):\n            io_hash[field] = resolve_job_ref(io_hash[field], job_outputs, should_resolve)\n        elif (isinstance(io_hash[field], list) or isinstance(io_hash[field], dict)):\n            q.append(io_hash[field])\n    while (len(q) > 0):\n        thing = q.pop()\n        if isinstance(thing, list):\n            for i in range(len(thing)):\n                if is_job_ref(thing[i]):\n                    thing[i] = resolve_job_ref(thing[i], job_outputs, should_resolve)\n                elif (isinstance(thing[i], list) or isinstance(thing[i], dict)):\n                    q.append(thing[i])\n        else:\n            for field in thing:\n                if is_job_ref(thing[field]):\n                    thing[field] = resolve_job_ref(thing[field], job_outputs, should_resolve)\n                elif (isinstance(thing[field], list) or isinstance(thing[field], dict)):\n                    q.append(thing[field])\n", "label": 1}
{"function": "\n\ndef keypress(self, size, key):\n    command = self._command_map[key]\n    if (key == ']'):\n        self.shift_order((+ 1))\n        return True\n    elif (key == '['):\n        self.shift_order((- 1))\n        return True\n    elif (key == '>'):\n        self.focus_hotspot(size)\n        return True\n    elif (key == '\\\\'):\n        layout = {\n            FLAT: NESTED,\n            NESTED: FLAT,\n        }[self.layout]\n        self.set_layout(layout)\n        return True\n    command = self._command_map[key]\n    if (command == 'menu'):\n        self.defocus()\n        return True\n    elif (command == urwid.CURSOR_RIGHT):\n        if (self.layout == FLAT):\n            return True\n        (widget, node) = self.tbody.get_focus()\n        if widget.expanded:\n            heavy_widget = widget.first_child()\n            if (heavy_widget is not None):\n                heavy_node = heavy_widget.get_node()\n                self.tbody.change_focus(size, heavy_node)\n            return True\n    elif (command == urwid.CURSOR_LEFT):\n        if (self.layout == FLAT):\n            return True\n        (widget, node) = self.tbody.get_focus()\n        if (not widget.expanded):\n            parent_node = node.get_parent()\n            if (not parent_node.is_root()):\n                self.tbody.change_focus(size, parent_node)\n            return True\n    elif (command == urwid.ACTIVATE):\n        if self.viewer.paused:\n            self.viewer.resume()\n        else:\n            self.viewer.pause()\n        return True\n    return super(StatisticsTable, self).keypress(size, key)\n", "label": 1}
{"function": "\n\ndef _parse_param_line(self, line):\n    ' Parse a single param line. '\n    value = line.strip('\\n \\t')\n    if (len(value) > 0):\n        i = Input()\n        if (value.find('#') != (- 1)):\n            (value, extra_attributes) = value.split('#')\n            try:\n                extra_attributes = eval(extra_attributes)\n            except SyntaxError:\n                raise InputException('Incorrectly formatted input for {0}!'.format(value))\n            if (not isinstance(extra_attributes, dict)):\n                raise InputException('Incorrectly formatted input for {0}!'.format(value))\n            if ('prompt' in extra_attributes):\n                i.prompt = extra_attributes['prompt']\n            if ('help' in extra_attributes):\n                i.help = extra_attributes['help']\n            if ('type' in extra_attributes):\n                i.in_type = extra_attributes['type']\n                if (i.in_type.find('/') != (- 1)):\n                    (i.in_type, i.out_type) = i.in_type.split('/')\n            if ('cast' in extra_attributes):\n                i.out_type = extra_attributes['cast']\n        if (value.find('==') != (- 1)):\n            (value, default) = value.split('==')\n            i.default = default\n        if value.endswith('?'):\n            value = value[:(- 1)]\n            i.is_secret = True\n        return (value, i)\n    return None\n", "label": 1}
{"function": "\n\ndef _analyze(self):\n    ' works out the updates to be performed '\n    if ((self.value is None) or (self.value == self.previous)):\n        pass\n    elif (self._operation == 'append'):\n        self._append = self.value\n    elif (self._operation == 'prepend'):\n        self._prepend = self.value\n    elif (self.previous is None):\n        self._assignments = self.value\n    elif (len(self.value) < len(self.previous)):\n        self._assignments = self.value\n    elif (len(self.previous) == 0):\n        self._assignments = self.value\n    else:\n        search_space = (len(self.value) - max(0, (len(self.previous) - 1)))\n        search_size = len(self.previous)\n        for i in range(search_space):\n            j = (i + search_size)\n            sub = self.value[i:j]\n            idx_cmp = (lambda idx: (self.previous[idx] == sub[idx]))\n            if (idx_cmp(0) and idx_cmp((- 1)) and (self.previous == sub)):\n                self._prepend = (self.value[:i] or None)\n                self._append = (self.value[j:] or None)\n                break\n        if (self._prepend is self._append is None):\n            self._assignments = self.value\n    self._analyzed = True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef obtainSystemConstants():\n    lines = filter(None, map(str.strip, subprocess.check_output(['qhost']).split('\\n')))\n    line = lines[0]\n    items = line.strip().split()\n    num_columns = len(items)\n    cpu_index = None\n    mem_index = None\n    for i in range(num_columns):\n        if (items[i] == 'NCPU'):\n            cpu_index = i\n        elif (items[i] == 'MEMTOT'):\n            mem_index = i\n    if ((cpu_index is None) or (mem_index is None)):\n        RuntimeError('qhost command does not return NCPU or MEMTOT columns')\n    maxCPU = 0\n    maxMEM = MemoryString('0')\n    for line in lines[2:]:\n        items = line.strip().split()\n        if (len(items) < num_columns):\n            RuntimeError('qhost output has a varying number of columns')\n        if ((items[cpu_index] != '-') and (items[cpu_index] > maxCPU)):\n            maxCPU = items[cpu_index]\n        if ((items[mem_index] != '-') and (MemoryString(items[mem_index]) > maxMEM)):\n            maxMEM = MemoryString(items[mem_index])\n    if ((maxCPU is 0) or (maxMEM is 0)):\n        RuntimeError('qhost returned null NCPU or MEMTOT info')\n    return (maxCPU, maxMEM)\n", "label": 1}
{"function": "\n\ndef test_reset_late(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, _time_function=time)\n    time.time = 13\n    assert (timer.sleep_time() == 0)\n    still_alive = timer.run()\n    assert (not still_alive)\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == Decimal('inf'))\n    timer.reset()\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 10)\n    time.time = 21\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 2)\n    time.time = 30\n    assert (timer.sleep_time() == 0)\n    still_alive = timer.run()\n    assert (not still_alive)\n    assert (callback.nb_calls == 2)\n    assert (timer.sleep_time() == Decimal('inf'))\n", "label": 1}
{"function": "\n\ndef get_fn_name(self, expr, compiler_kwargs={\n    \n}, attributes=[], inline=True):\n    if (expr.__class__ is TypedFn):\n        fn = expr\n    elif (expr.__class__ is Closure):\n        fn = expr.fn\n    else:\n        assert isinstance(expr.type, (FnT, ClosureT)), ('Expected function or closure, got %s : %s' % (expr, expr.type))\n        fn = expr.type.fn\n    compiler = self.__class__(module_entry=False, **compiler_kwargs)\n    compiled = compiler.compile_flat_source(fn, attributes=attributes, inline=inline)\n    if (compiled.sig not in self.extra_function_signatures):\n        for decl in compiled.declarations:\n            self.add_decl(decl)\n        self.extra_objects.update(compiled.extra_objects)\n        for extra_sig in compiled.extra_function_signatures:\n            if (extra_sig not in self.extra_function_signatures):\n                self.extra_function_signatures.append(extra_sig)\n                self.extra_functions[extra_sig] = compiled.extra_functions[extra_sig]\n        self.extra_function_signatures.append(compiled.sig)\n        self.extra_functions[compiled.sig] = compiled.src\n    for link_flag in compiler.extra_link_flags:\n        if (link_flag not in self.extra_link_flags):\n            self.extra_link_flags.append(link_flag)\n    for compile_flag in compiler.extra_compile_flags:\n        if (compile_flag not in self.extra_compile_flags):\n            self.extra_compile_flags.append(compile_flag)\n    return compiled.name\n", "label": 1}
{"function": "\n\ndef _fetch_objects(self, doc_type=None):\n    'Fetch all references and convert to their document objects\\n        '\n    object_map = {\n        \n    }\n    for (collection, dbrefs) in self.reference_map.iteritems():\n        if hasattr(collection, 'objects'):\n            col_name = collection._get_collection_name()\n            refs = [dbref for dbref in dbrefs if ((col_name, dbref) not in object_map)]\n            references = collection.objects.in_bulk(refs)\n            for (key, doc) in references.iteritems():\n                object_map[(col_name, key)] = doc\n        else:\n            if isinstance(doc_type, (ListField, DictField, MapField)):\n                continue\n            refs = [dbref for dbref in dbrefs if ((collection, dbref) not in object_map)]\n            if doc_type:\n                references = doc_type._get_db()[collection].find({\n                    '_id': {\n                        '$in': refs,\n                    },\n                })\n                for ref in references:\n                    doc = doc_type._from_son(ref)\n                    object_map[(collection, doc.id)] = doc\n            else:\n                references = get_db()[collection].find({\n                    '_id': {\n                        '$in': refs,\n                    },\n                })\n                for ref in references:\n                    if ('_cls' in ref):\n                        doc = get_document(ref['_cls'])._from_son(ref)\n                    elif (doc_type is None):\n                        doc = get_document(''.join((x.capitalize() for x in collection.split('_'))))._from_son(ref)\n                    else:\n                        doc = doc_type._from_son(ref)\n                    object_map[(collection, doc.id)] = doc\n    return object_map\n", "label": 1}
{"function": "\n\ndef handle(self):\n    ' Handle log requests until connection closed. '\n    makeLogRecord = logging.makeLogRecord\n    logger = logging.getLogger('remote')\n    conn = self.connection\n    recv = conn.recv\n    peer = None\n    while True:\n        try:\n            data = recv(4)\n        except Exception:\n            return\n        if (len(data) < 4):\n            break\n        if (peer is None):\n            (host, port) = conn.getpeername()\n            try:\n                (host, aliases, addrs) = socket.gethostbyaddr(host)\n            except Exception as exc:\n                logger.debug('gethostbyaddr(%s) failed: %s', host, (str(exc) or repr(exc)))\n            peer = ('%s:%s' % (host, port))\n            logger.info('New logging connection from %s', peer)\n        slen = unpack('>L', data)[0]\n        data = recv(slen)\n        slen -= len(data)\n        msg = data\n        while slen:\n            data = recv(slen)\n            slen -= len(data)\n            msg = ''.join((msg, data))\n        try:\n            obj = loads(msg)\n            record = makeLogRecord(obj)\n        except Exception as exc:\n            logger.exception(\"Can't process log request from %s: %s\", peer, exc)\n        else:\n            prefix = record.prefix\n            name = record.name\n            if (name != prefix):\n                record.name = ('[%s] %s' % (prefix, name))\n            else:\n                record.name = ('[%s]' % prefix)\n            logger.handle(record)\n    conn.close()\n    if (peer is not None):\n        logger.info('Logging connection from %s closed', peer)\n", "label": 1}
{"function": "\n\ndef _build_message(self, deleted, failed, failures):\n    msg = ''\n    deleted_msg = []\n    for (resource, value) in deleted.items():\n        if value:\n            if (not msg):\n                msg = 'Deleted'\n            if (not (value == 1)):\n                resource = self._pluralize(resource)\n            deleted_msg.append((' %d %s' % (value, resource)))\n    if deleted_msg:\n        msg += ','.join(deleted_msg)\n    failed_msg = []\n    if failures:\n        if msg:\n            msg += '. '\n        msg += 'The following resources could not be deleted:'\n        for (resource, value) in failed.items():\n            if value:\n                if (not (value == 1)):\n                    resource = self._pluralize(resource)\n                failed_msg.append((' %d %s' % (value, resource)))\n        msg += ','.join(failed_msg)\n    if msg:\n        msg += '.'\n    else:\n        msg = _('Tenant has no supported resources.')\n    return msg\n", "label": 1}
{"function": "\n\ndef _walk_StatIf(self, node):\n    short_if = (getattr(node, 'short_if', False) and (not self._args.get('ignore_tokens')))\n    first = True\n    for (exp, block) in node.exp_block_pairs:\n        if (exp is not None):\n            if first:\n                (yield self._get_text(node, 'if'))\n                first = False\n            else:\n                (yield self._get_text(node, 'elseif'))\n            if short_if:\n                (yield self._get_text(node, '('))\n                self._indent += 1\n                for t in self._walk(exp):\n                    (yield t)\n                self._indent -= 1\n                (yield self._get_text(node, ')'))\n            else:\n                for t in self._walk(exp):\n                    (yield t)\n                (yield self._get_text(node, 'then'))\n                self._indent += 1\n            for t in self._walk(block):\n                (yield t)\n            if (not short_if):\n                self._indent -= 1\n        else:\n            (yield self._get_text(node, 'else'))\n            self._indent += 1\n            for t in self._walk(block):\n                (yield t)\n            self._indent -= 1\n    if (not short_if):\n        (yield self._get_text(node, 'end'))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _find_new_(classdict, member_type, first_enum):\n    'Returns the __new__ to be used for creating the enum members.\\n\\n            classdict: the class dictionary given to __new__\\n            member_type: the data type whose __new__ will be used by default\\n            first_enum: enumeration to check for an overriding __new__\\n\\n            '\n    __new__ = classdict.get('__new__', None)\n    if __new__:\n        return (None, True, True)\n    N__new__ = getattr(None, '__new__')\n    O__new__ = getattr(object, '__new__')\n    if (Enum is None):\n        E__new__ = N__new__\n    else:\n        E__new__ = Enum.__dict__['__new__']\n    for method in ('__member_new__', '__new__'):\n        for possible in (member_type, first_enum):\n            try:\n                target = possible.__dict__[method]\n            except (AttributeError, KeyError):\n                target = getattr(possible, method, None)\n            if (target not in [None, N__new__, O__new__, E__new__]):\n                if (method == '__member_new__'):\n                    classdict['__new__'] = target\n                    return (None, False, True)\n                if isinstance(target, staticmethod):\n                    target = target.__get__(member_type)\n                __new__ = target\n                break\n        if (__new__ is not None):\n            break\n    else:\n        __new__ = object.__new__\n    if (__new__ is object.__new__):\n        use_args = False\n    else:\n        use_args = True\n    return (__new__, False, use_args)\n", "label": 1}
{"function": "\n\ndef _cardindex(self, key):\n    'Returns an index into the ._cards list given a valid lookup key.'\n    if isinstance(key, string_types):\n        keyword = key\n        n = 0\n    elif isinstance(key, int):\n        if (key < 0):\n            key += len(self._cards)\n        if ((key < 0) or (key >= len(self._cards))):\n            raise IndexError('Header index out of range.')\n        return key\n    elif isinstance(key, slice):\n        return key\n    elif isinstance(key, tuple):\n        if ((len(key) != 2) or (not isinstance(key[0], string_types)) or (not isinstance(key[1], int))):\n            raise ValueError('Tuple indices must be 2-tuples consisting of a keyword string and an integer index.')\n        (keyword, n) = key\n    else:\n        raise ValueError('Header indices must be either a string, a 2-tuple, or an integer.')\n    keyword = Card.normalize_keyword(keyword)\n    indices = self._keyword_indices.get(keyword, None)\n    if (keyword and (not indices)):\n        if ((len(keyword) > KEYWORD_LENGTH) or ('.' in keyword)):\n            raise KeyError(('Keyword %r not found.' % keyword))\n        else:\n            indices = self._rvkc_indices.get(keyword, None)\n    if (not indices):\n        raise KeyError(('Keyword %r not found.' % keyword))\n    try:\n        return indices[n]\n    except IndexError:\n        raise IndexError(('There are only %d %r cards in the header.' % (len(indices), keyword)))\n", "label": 1}
{"function": "\n\ndef _generate_events(self, event):\n    try:\n        if (not any([self._read, self._write])):\n            return\n        timeout = event.time_left\n        if (timeout < 0):\n            (r, w, _) = select.select(self._read, self._write, [])\n        else:\n            (r, w, _) = select.select(self._read, self._write, [], timeout)\n    except ValueError as e:\n        return self._preenDescriptors()\n    except TypeError as e:\n        return self._preenDescriptors()\n    except (SelectError, SocketError, IOError) as e:\n        if (e.args[0] in (0, 2)):\n            if ((not self._read) and (not self._write)):\n                return\n            else:\n                raise\n        elif (e.args[0] == EINTR):\n            return\n        elif (e.args[0] == EBADF):\n            return self._preenDescriptors()\n        else:\n            raise\n    for sock in w:\n        if self.isWriting(sock):\n            self.fire(_write(sock), self.getTarget(sock))\n    for sock in r:\n        if (sock == self._ctrl_recv):\n            self._read_ctrl()\n            continue\n        if self.isReading(sock):\n            self.fire(_read(sock), self.getTarget(sock))\n", "label": 1}
{"function": "\n\ndef _add_item(self, item, indent_amt):\n    'Add an item to the line.\\n\\n        Reflow the line to get the best formatting after the item is\\n        inserted. The bracket depth indicates if the item is being\\n        inserted inside of a container or not.\\n\\n        '\n    if (self._prev_item and self._prev_item.is_string and item.is_string):\n        self._lines.append(self._LineBreak())\n        self._lines.append(self._Indent(indent_amt))\n    item_text = unicode(item)\n    if (self._lines and self._bracket_depth):\n        self._prevent_default_initializer_splitting(item, indent_amt)\n        if (item_text in '.,)]}'):\n            self._split_after_delimiter(item, indent_amt)\n    elif (self._lines and (not self.line_empty())):\n        if self.fits_on_current_line(len(item_text)):\n            self._enforce_space(item)\n        else:\n            self._lines.append(self._LineBreak())\n            self._lines.append(self._Indent(indent_amt))\n    self._lines.append(item)\n    (self._prev_item, self._prev_prev_item) = (item, self._prev_item)\n    if (item_text in '([{'):\n        self._bracket_depth += 1\n    elif (item_text in '}])'):\n        self._bracket_depth -= 1\n        assert (self._bracket_depth >= 0)\n", "label": 1}
{"function": "\n\ndef md5sum(filename, use_sudo=False):\n    '\\n    Compute the MD5 sum of a file.\\n    '\n    func = ((use_sudo and run_as_root) or run)\n    with settings(hide('running', 'stdout', 'stderr', 'warnings'), warn_only=True):\n        if exists('/usr/bin/md5sum'):\n            res = func(('/usr/bin/md5sum %(filename)s' % locals()))\n        elif exists('/sbin/md5'):\n            res = func(('/sbin/md5 -r %(filename)s' % locals()))\n        elif exists('/opt/local/gnu/bin/md5sum'):\n            res = func(('/opt/local/gnu/bin/md5sum %(filename)s' % locals()))\n        elif exists('/opt/local/bin/md5sum'):\n            res = func(('/opt/local/bin/md5sum %(filename)s' % locals()))\n        else:\n            md5sum = func('which md5sum')\n            md5 = func('which md5')\n            if exists(md5sum):\n                res = func(('%(md5sum)s %(filename)s' % locals()))\n            elif exists(md5):\n                res = func(('%(md5)s %(filename)s' % locals()))\n            else:\n                abort('No MD5 utility was found on this system.')\n    if res.succeeded:\n        parts = res.split()\n        _md5sum = (((len(parts) > 0) and parts[0]) or None)\n    else:\n        warn(res)\n        _md5sum = None\n    return _md5sum\n", "label": 1}
{"function": "\n\ndef compute_layout(self, data, flags):\n    self._size_needs_update = False\n    opts = self.view_opts\n    changed = []\n    for (widget, index) in self.view_indices.items():\n        opt = opts[index]\n        s = opt['size']\n        (w, h) = sn = widget.size\n        sh = opt['size_hint']\n        (shnw, shnh) = shn = widget.size_hint\n        ph = opt['pos_hint']\n        phn = widget.pos_hint\n        if ((s != sn) or (sh != shn) or (ph != phn)):\n            changed.append((index, widget, s, sn, sh, shn, ph, phn))\n            if (shnw is None):\n                if (shnh is None):\n                    opt['size'] = sn\n                else:\n                    opt['size'] = [w, s[1]]\n            elif (shnh is None):\n                opt['size'] = [s[0], h]\n            opt['size_hint'] = shn\n            opt['pos_hint'] = phn\n    if [f for f in flags if (not f)]:\n        self._changed_views = []\n    else:\n        self._changed_views = (changed if changed else None)\n", "label": 1}
{"function": "\n\ndef _apply_compositor(self, holomap, ranges=None, keys=None, dimensions=None):\n    '\\n        Given a HoloMap compute the appropriate (mapwise or framewise)\\n        ranges in order to apply the Compositor collapse operations in\\n        display mode (data collapse should already have happened).\\n        '\n    defaultdim = ((holomap.ndims == 1) and (holomap.kdims[0].name != 'Frame'))\n    if (keys and ranges and dimensions and (not defaultdim)):\n        dim_inds = [dimensions.index(d) for d in holomap.kdims]\n        sliced_keys = [tuple((k[i] for i in dim_inds)) for k in keys]\n        frame_ranges = OrderedDict([(slckey, self.compute_ranges(holomap, key, ranges[key])) for (key, slckey) in zip(keys, sliced_keys) if (slckey in holomap.data.keys())])\n    else:\n        mapwise_ranges = self.compute_ranges(holomap, None, None)\n        frame_ranges = OrderedDict([(key, self.compute_ranges(holomap, key, mapwise_ranges)) for key in holomap.keys()])\n    ranges = frame_ranges.values()\n    return Compositor.collapse(holomap, (ranges, frame_ranges.keys()), mode='display')\n", "label": 1}
{"function": "\n\ndef _GetAxisParams(self, chart):\n    'Collect params related to our various axes (x, y, right-hand).'\n    axis_types = []\n    axis_ranges = []\n    axis_labels = []\n    axis_label_positions = []\n    axis_label_gridlines = []\n    mark_length = max(self._width, self._height)\n    for (i, axis_pair) in enumerate((a for a in chart._GetAxes() if a[1].labels)):\n        (axis_type_code, axis) = axis_pair\n        axis_types.append(axis_type_code)\n        if ((axis.min is not None) or (axis.max is not None)):\n            assert (axis.min is not None)\n            assert (axis.max is not None)\n            axis_ranges.append(('%s,%s,%s' % (i, axis.min, axis.max)))\n        (labels, positions) = self._GetAxisLabelsAndPositions(axis, chart)\n        if labels:\n            axis_labels.append(('%s:' % i))\n            axis_labels.extend(labels)\n        if positions:\n            positions = ([i] + list(positions))\n            axis_label_positions.append(','.join((str(x) for x in positions)))\n        if axis.label_gridlines:\n            axis_label_gridlines.append(('%d,%d' % (i, (- mark_length))))\n    return util.JoinLists(axis_type=axis_types, axis_range=axis_ranges, axis_label=axis_labels, axis_position=axis_label_positions, axis_tick_marks=axis_label_gridlines)\n", "label": 1}
{"function": "\n\ndef matchreport(self, inamepart='', names='pytest_runtest_logreport pytest_collectreport', when=None):\n    ' return a testreport whose dotted import path matches '\n    l = []\n    for rep in self.getreports(names=names):\n        try:\n            if ((not when) and (rep.when != 'call') and rep.passed):\n                continue\n        except AttributeError:\n            pass\n        if (when and (getattr(rep, 'when', None) != when)):\n            continue\n        if ((not inamepart) or (inamepart in rep.nodeid.split('::'))):\n            l.append(rep)\n    if (not l):\n        raise ValueError(('could not find test report matching %r: no test reports at all!' % (inamepart,)))\n    if (len(l) > 1):\n        raise ValueError(('found 2 or more testreports matching %r: %s' % (inamepart, l)))\n    return l[0]\n", "label": 1}
{"function": "\n\ndef _load_metadata(force_reload=False):\n    'Load metadata information into memory.\\n\\n    If force_reload, the metadata information will be reloaded\\n    even if the metadata is already loaded.\\n    '\n    adapter_api.load_adapters_internal(force_reload=force_reload)\n    global OS_FIELDS\n    if (force_reload or (OS_FIELDS is None)):\n        OS_FIELDS = _get_os_fields_from_configuration()\n    global PACKAGE_FIELDS\n    if (force_reload or (PACKAGE_FIELDS is None)):\n        PACKAGE_FIELDS = _get_package_fields_from_configuration()\n    global FLAVOR_FIELDS\n    if (force_reload or (FLAVOR_FIELDS is None)):\n        FLAVOR_FIELDS = _get_flavor_fields_from_configuration()\n    global OSES_METADATA\n    if (force_reload or (OSES_METADATA is None)):\n        OSES_METADATA = _get_oses_metadata_from_configuration()\n    global PACKAGES_METADATA\n    if (force_reload or (PACKAGES_METADATA is None)):\n        PACKAGES_METADATA = _get_packages_metadata_from_configuration()\n    global FLAVORS_METADATA\n    if (force_reload or (FLAVORS_METADATA is None)):\n        FLAVORS_METADATA = _get_flavors_metadata_from_configuration()\n    global OSES_METADATA_UI_CONVERTERS\n    if (force_reload or (OSES_METADATA_UI_CONVERTERS is None)):\n        OSES_METADATA_UI_CONVERTERS = _get_oses_metadata_ui_converters_from_configuration()\n    global FLAVORS_METADATA_UI_CONVERTERS\n    if (force_reload or (FLAVORS_METADATA_UI_CONVERTERS is None)):\n        FLAVORS_METADATA_UI_CONVERTERS = _get_flavors_metadata_ui_converters_from_configuration()\n", "label": 1}
{"function": "\n\ndef checkpins(contents, designator, errs):\n    pins = re_pins.findall(contents)\n    nums = []\n    for (name, num, x, y, length, numsize, namesize) in pins:\n        if (((int(x) % 100) != 0) or ((int(y) % 100) != 0)):\n            errs.append(\"Pin '{}' not on 100mil grid\".format(name))\n        if ((designator in ('IC', 'U')) and (int(length) not in (100, 150))):\n            errs.append(\"Pin '{}' not 100 or 150mil long, but part is IC or U\".format(name))\n        if ((int(namesize) != 50) or ((int(numsize) != 50) and num.isdigit())):\n            errs.append(\"Pin '{}' font size not 50mil\".format(name))\n        if num.isdigit():\n            nums.append(int(num))\n    if nums:\n        expected = set(range(min(nums), (max(nums) + 1)))\n        if (set(nums) != expected):\n            missing = [str(x) for x in (set(expected) - set(nums))]\n            errs.append('Missing pins {}'.format(', '.join(missing)))\n        duplicates = set([str(x) for x in nums if (nums.count(x) > 1)])\n        if duplicates:\n            errs.append('Duplicated pins {}'.format(', '.join(duplicates)))\n", "label": 1}
{"function": "\n\ndef __init__(self, kernel, it_space, *args, **kwargs):\n    read_args = [a.data for a in args if (a.access in [READ, RW])]\n    written_args = [a.data for a in args if (a.access in [RW, WRITE, MIN, MAX, INC])]\n    inc_args = [a.data for a in args if (a.access in [INC])]\n    LazyComputation.__init__(self, (set(read_args) | Const._defs), set(written_args), set(inc_args))\n    self._kernel = kernel\n    self._actual_args = args\n    self._it_space = it_space\n    for (i, arg) in enumerate(self._actual_args):\n        arg.position = i\n        arg.indirect_position = i\n    for (i, arg1) in enumerate(self._actual_args):\n        if (arg1._is_dat and arg1._is_indirect):\n            for arg2 in self._actual_args[i:]:\n                if ((arg2.data is arg1.data) and (arg2.map is arg1.map)):\n                    arg2.indirect_position = arg1.indirect_position\n    self._all_args = kwargs.get('all_args', [args])\n    self._inspection = kwargs.get('inspection')\n    self._executor = kwargs.get('executor')\n", "label": 1}
{"function": "\n\ndef _click_autocomplete(root, text):\n    'Completer generator for click applications.'\n    try:\n        parts = shlex.split(text)\n    except ValueError:\n        raise StopIteration\n    (location, incomplete) = _click_resolve_command(root, parts)\n    if ((not text.endswith(' ')) and (not incomplete) and text):\n        raise StopIteration\n    if (incomplete and (not incomplete[0:2].isalnum())):\n        for param in location.params:\n            if (not isinstance(param, click.Option)):\n                continue\n            for opt in itertools.chain(param.opts, param.secondary_opts):\n                if opt.startswith(incomplete):\n                    (yield completion.Completion(opt, (- len(incomplete)), display_meta=param.help))\n    elif isinstance(location, (click.MultiCommand, click.core.Group)):\n        ctx = click.Context(location)\n        commands = location.list_commands(ctx)\n        for command in commands:\n            if command.startswith(incomplete):\n                cmd = location.get_command(ctx, command)\n                (yield completion.Completion(command, (- len(incomplete)), display_meta=cmd.short_help))\n", "label": 1}
{"function": "\n\ndef print_tweet(tweet, settings):\n    'Format and print the tweet dict.\\n\\n    Returns:\\n        boolean status of if the tweet was printed\\n    '\n    tweet_text = tweet.get('text')\n    if ((tweet_text is None) or ((not settings['spam']) and AntiSpam.is_spam(tweet_text))):\n        return False\n    if (sys.version_info[0] == 2):\n        for encoding in ['utf-8', 'latin-1']:\n            try:\n                tweet_text.decode(encoding)\n            except UnicodeEncodeError:\n                pass\n            else:\n                break\n        else:\n            return False\n    if settings.get('json'):\n        print(json.dumps(tweet, indent=4, sort_keys=True))\n    else:\n        prepend = []\n        if (settings.get('date') or settings.get('time')):\n            date = parse_date(tweet['created_at'])\n            if settings.get('date'):\n                prepend.append('{0:%b} {1}'.format(date, int(datetime.strftime(date, '%d'))))\n            if settings.get('time'):\n                prepend.append('{0:%H}:{0:%M}:{0:%S}'.format(date))\n        tweet = '{0}{1}@{2}: {3}'.format(' '.join(prepend), (' ' * int((prepend != []))), tweet.get('user', {\n            \n        }).get('screen_name', ''), unescape(tweet_text))\n        print(highlight_tweet(tweet))\n    return True\n", "label": 1}
{"function": "\n\ndef parse_changes_file(file_path, versions=None):\n    '\\n    Parse CHANGES file and return a dictionary with contributors.\\n\\n    Dictionary maps contributor name to the JIRA tickets or Github pull\\n    requests the user has worked on.\\n    '\n    contributors_map = defaultdict(set)\n    in_entry = False\n    active_version = None\n    active_tickets = []\n    with open(file_path, 'r') as fp:\n        for line in fp:\n            line = line.strip()\n            match = re.search('Changes with Apache Libcloud (\\\\d+\\\\.\\\\d+\\\\.\\\\d+(-\\\\w+)?).*?$', line)\n            if match:\n                active_version = match.groups()[0]\n            if (versions and (active_version not in versions)):\n                continue\n            if (line.startswith('-') or line.startswith('*)')):\n                in_entry = True\n                active_tickets = []\n            if (in_entry and (line == '')):\n                in_entry = False\n            if in_entry:\n                match = re.search('\\\\((.+?)\\\\)$', line)\n                if match:\n                    active_tickets = match.groups()[0]\n                    active_tickets = active_tickets.split(', ')\n                    active_tickets = [ticket for ticket in active_tickets if (ticket.startswith('LIBCLOUD-') or ticket.startswith('GITHUB-'))]\n                match = re.search('^\\\\[(.+?)\\\\]$', line)\n                if match:\n                    contributors = match.groups()[0]\n                    contributors = contributors.split(',')\n                    contributors = [name.strip() for name in contributors]\n                    for name in contributors:\n                        name = name.title()\n                        contributors_map[name].update(set(active_tickets))\n    return contributors_map\n", "label": 1}
{"function": "\n\ndef visit_statement(self, tokens):\n    tables = set()\n    if self._tables:\n        table_tokens = self._GetDescendants(tokens, 'table', do_not_cross=('exclude',))\n        table_aliases = self._GetTableAliases(tokens)\n        for table in table_tokens:\n            table_name = str(table[0])\n            if (table_name in table_aliases):\n                table_name = str(table_aliases[table_name]['table'][0])\n            tables.add(table_name)\n        if (not (self._tables & tables)):\n            return\n    columns = set()\n    if self._columns:\n        columns = set((str(x[0]).lower() for x in self._GetDescendants(tokens, 'column', do_not_cross=('exclude',))))\n        if (not (self._columns & columns)):\n            return\n    values = set()\n    if self._values:\n        values = set((str(x[0]) for x in self._GetDescendants(tokens, 'val', do_not_cross=('exclude',))))\n        if (not (self._values & values)):\n            return\n    operations = set()\n    if self._operations:\n        for operation in self._operations:\n            if self._GetDescendants(tokens, operation):\n                operations.add(operation)\n        if (not operations):\n            return\n    msg = (self._msg % {\n        'tables': tables,\n        'columns': columns,\n        'values': values,\n        'operations': operations,\n    })\n    self.AddWarning(tokens, msg)\n", "label": 1}
{"function": "\n\ndef assert_tables_equal(self, table, reflected_table, strict_types=False):\n    assert (len(table.c) == len(reflected_table.c))\n    for (c, reflected_c) in zip(table.c, reflected_table.c):\n        eq_(c.name, reflected_c.name)\n        assert (reflected_c is reflected_table.c[c.name])\n        eq_(c.primary_key, reflected_c.primary_key)\n        eq_(c.nullable, reflected_c.nullable)\n        if strict_types:\n            msg = \"Type '%s' doesn't correspond to type '%s'\"\n            assert isinstance(reflected_c.type, type(c.type)), (msg % (reflected_c.type, c.type))\n        else:\n            self.assert_types_base(reflected_c, c)\n        if isinstance(c.type, sqltypes.String):\n            eq_(c.type.length, reflected_c.type.length)\n        eq_(set([f.column.name for f in c.foreign_keys]), set([f.column.name for f in reflected_c.foreign_keys]))\n        if c.server_default:\n            assert isinstance(reflected_c.server_default, schema.FetchedValue)\n    assert (len(table.primary_key) == len(reflected_table.primary_key))\n    for c in table.primary_key:\n        assert (reflected_table.primary_key.columns[c.name] is not None)\n", "label": 1}
{"function": "\n\ndef main():\n    'Entry point for this script.'\n    (parser, args) = parse_command_line_arguments()\n    logger = initialize_logging(args.debug, args.less_verbose)\n    result = 0\n    if args.download:\n        try:\n            download_listing(args.listing)\n        except DownloadRetfListingFailed as ex:\n            logger.error('Downloading latest RETF listing failed: %s.', ex)\n            result = 1\n    if ((not args.path) and (not args.file) and (not args.download)):\n        parser.print_help()\n        result = 2\n    if ((not result) and (not os.path.isfile(args.listing))):\n        logger.error('RETF listing not found at %s.', args.listing)\n        logger.info('Please download the RETF listing first by using the parameter --download.')\n        result = 1\n    if (not result):\n        files = get_file_listing(args.path, args.file, args.extension)\n        rules = generate_listing(args.listing)\n        disabled = load_disabled_rules(args.disabled)\n        all_findings = 0\n        for check in files:\n            if (not os.path.isfile(check)):\n                continue\n            (findings, content) = check_file(check, rules, disabled)\n            if (findings > 0):\n                all_findings += findings\n                logger.warning('%s finding(s) in file %s.', findings, check)\n            if ((findings > 0) and args.write_changes):\n                write_text_to_file(check, content, args.no_backup, args.in_place)\n        if (all_findings > 0):\n            logger.warning('%s finding(s) in all checked files.', all_findings)\n            result = 1\n    return result\n", "label": 1}
{"function": "\n\ndef _plot_topo_onpick(event, show_func):\n    'Onpick callback that shows a single channel in a new figure'\n    orig_ax = event.inaxes\n    if ((event.inaxes is None) or ((not hasattr(orig_ax, '_mne_ch_idx')) and (not hasattr(orig_ax, '_mne_axs')))):\n        return\n    import matplotlib.pyplot as plt\n    try:\n        if hasattr(orig_ax, '_mne_axs'):\n            (x, y) = (event.xdata, event.ydata)\n            for ax in orig_ax._mne_axs:\n                if ((x >= ax.pos[0]) and (y >= ax.pos[1]) and (x <= (ax.pos[0] + ax.pos[2])) and (y <= (ax.pos[1] + ax.pos[3]))):\n                    orig_ax = ax\n                    break\n            else:\n                return\n        ch_idx = orig_ax._mne_ch_idx\n        face_color = orig_ax._mne_ax_face_color\n        (fig, ax) = plt.subplots(1)\n        plt.title(orig_ax._mne_ch_name)\n        ax.set_axis_bgcolor(face_color)\n        show_func(plt, ch_idx)\n    except Exception as err:\n        print(err)\n        raise\n", "label": 1}
{"function": "\n\ndef get_fields(self):\n    to_update = False\n    doc = inspect.getdoc(self.method)\n    if ((not doc) and issubclass(self.method.im_class.model, models.Model)):\n        fields = None\n        if (self.method.__name__ == 'read'):\n            fields = (self.method.im_class.fields if self.method.im_class.fields else tuple((attr.name for attr in self.method.im_class.model._meta.local_fields)))\n        elif (self.method.__name__ in ('create', 'update')):\n            to_update = True\n            if (hasattr(self.method.im_class, 'form') and hasattr(self.method.im_class.form, '_meta')):\n                fields = self.method.im_class.form._meta.fields\n            else:\n                fields = self.method.im_class.fields\n        if fields:\n            for field in fields:\n                for mfield in self.method.im_class.model._meta.fields:\n                    if (mfield.name == field):\n                        (yield {\n                            'name': mfield.name,\n                            'required': ((not mfield.blank) if to_update else False),\n                            'type': get_field_data_type(mfield),\n                            'verbose': mfield.verbose_name,\n                            'help_text': mfield.help_text,\n                        })\n                        break\n", "label": 1}
{"function": "\n\ndef install_app(apppath, appname, apptype, projectdir):\n    'Copies the app into the project directory'\n    copy_functions = {\n        'file': fromfilesys,\n        'git': fromgit,\n        'hg': fromhg,\n    }\n    try:\n        cf = copy_functions[apptype]\n    except KeyError:\n        raise CopyError('App type not supported')\n    dst = os.path.join(projectdir, APPDIR, appname)\n    cf(apppath, dst)\n    bare = (- 1)\n    if os.path.exists(os.path.join(dst, 'metadata.json')):\n        bare = True\n    if os.path.exists(os.path.join(dst, appname, 'metadata.json')):\n        bare = False\n    if (bare == (- 1)):\n        if os.path.exists(os.path.join(dst, 'models.py')):\n            bare = True\n        if os.path.exists(os.path.join(dst, appname, 'models.py')):\n            bare = False\n    if (bare == (- 1)):\n        if os.path.exists(os.path.join(dst, 'views.py')):\n            bare = True\n        if os.path.exists(os.path.join(dst, appname, 'views.py')):\n            bare = False\n    if (bare == (- 1)):\n        raise CopyError('Could not detect repository format. Please make sure your application has one of these files: metadata.json, models.py, or views.py')\n    if bare:\n        os.symlink(dst, os.path.join(projectdir, appname))\n    else:\n        os.symlink(os.path.join(dst, appname), os.path.join(projectdir, appname))\n", "label": 1}
{"function": "\n\ndef query(self, send, maxtries=5):\n    found_command = False\n    for cmd in self.commands.keys():\n        if re.match(cmd, send):\n            if self.logger:\n                self.logger.debug('Sending command: {}'.format(self.commands[cmd]))\n            found_command = True\n            break\n    if (not found_command):\n        if self.logger:\n            self.logger.warning('Unknown command: \"{}\"'.format(send))\n        return None\n    if (cmd in self.delays.keys()):\n        delay = self.delays[cmd]\n    else:\n        delay = 0.2\n    expect = self.expects[cmd]\n    count = 0\n    result = None\n    while ((not result) and (count <= maxtries)):\n        count += 1\n        result = self.send(send, delay=delay)\n        MatchExpect = re.match(expect, result)\n        if (not MatchExpect):\n            if self.logger:\n                self.logger.debug('Did not find {} in response \"{}\"'.format(expect, result))\n            result = None\n            time.sleep(self.hibernate)\n        else:\n            if self.logger:\n                self.logger.debug('Found {} in response \"{}\"'.format(expect, result))\n            result = MatchExpect.groups()\n    return result\n", "label": 1}
{"function": "\n\ndef _parse(stream, ptr=0):\n    i = ptr\n    laststr = None\n    lasttok = None\n    deserialized = {\n        \n    }\n    while (i < len(stream)):\n        c = stream[i]\n        if (c == STRING):\n            (string, i) = _symtostr(stream, i)\n            if (lasttok == STRING):\n                deserialized[laststr] = string\n            laststr = string\n        elif (c == NODE_OPEN):\n            (deserialized[laststr], i) = _parse(stream, (i + 1))\n        elif (c == NODE_CLOSE):\n            return (deserialized, i)\n        elif (c == COMMENT):\n            if (((i + 1) < len(stream)) and (stream[(i + 1)] == '/')):\n                i = stream.find('\\n', i)\n        elif ((c == CR) or (c == LF)):\n            ni = (i + 1)\n            if ((ni < len(stream)) and (stream[ni] == LF)):\n                i = ni\n            if (lasttok != LF):\n                c = LF\n        else:\n            c = lasttok\n        lasttok = c\n        i += 1\n    return (deserialized, i)\n", "label": 1}
{"function": "\n\ndef do_show(self, *args):\n    if 'running-config'.startswith(args[0]):\n        if 'vlan'.startswith(args[1]):\n            self.show_run_vlan()\n        if 'interface'.startswith(args[1]):\n            self.show_run_int(args)\n    elif 'interfaces'.startswith(args[0]):\n        self.show_int(args)\n    elif 'vlan'.startswith(args[0]):\n        if args[1].isdigit():\n            self._show_vlan(int(args[1]))\n        elif 'brief'.startswith(args[1]):\n            self.show_vlan_brief()\n        elif 'ethernet'.startswith(args[1]):\n            self.show_vlan_int(args)\n        else:\n            self.write_line(('Invalid input -> %s' % args[1]))\n            self.write_line('Type ? for a list')\n    elif ('ip'.startswith(args[0]) and 'route'.startswith(args[1]) and 'static'.startswith(args[2])):\n        routes = self.switch_configuration.static_routes\n        if routes:\n            self.write_line('        Destination        Gateway        Port          Cost          Type Uptime src-vrf')\n        for (n, route) in enumerate(routes):\n            self.write_line('{index:<8}{destination:<18} {next_hop:}'.format(index=(n + 1), destination=route.dest, next_hop=route.next_hop))\n        self.write_line('')\n    elif 'version'.startswith(args[0]):\n        self.show_version()\n", "label": 1}
{"function": "\n\ndef check(self, app, sha, config):\n    token = current_app.config['GITHUB_TOKEN']\n    if (not token):\n        raise CheckFailed('GITHUB_TOKEN is not set')\n    api_root = (config.get('api_root') or current_app.config['GITHUB_API_ROOT']).rstrip('/')\n    contexts = set((config.get('contexts') or []))\n    repo = config['repo']\n    url = '{api_root}/repos/{repo}/commits/{ref}/statuses'.format(api_root=api_root, repo=repo, ref=sha)\n    headers = {\n        'Accepts': 'application/json',\n        'Authorization': 'token {}'.format(token),\n    }\n    resp = http.get(url, headers=headers)\n    context_list = resp.json()\n    if (not context_list):\n        raise CheckFailed('No contexts were present in GitHub')\n    valid_contexts = set()\n    for data in context_list:\n        if (data['state'] == 'success'):\n            valid_contexts.add(data['context'])\n            try:\n                contexts.remove(data['context'])\n            except KeyError:\n                pass\n        if (data['context'] in valid_contexts):\n            continue\n        if (contexts and (data['context'] not in contexts)):\n            continue\n        if (data['state'] == 'pending'):\n            raise CheckPending(ERR_CHECK.format(data['context'], data['state']))\n        elif (data['state'] != 'success'):\n            raise CheckFailed(ERR_CHECK.format(data['context'], data['state']))\n        contexts.remove(data['context'])\n    if contexts:\n        raise CheckFailed(ERR_MISSING_CONTEXT.format(iter(contexts).next()))\n", "label": 1}
{"function": "\n\ndef _validate_shuffle_split_init(test_size, train_size):\n    'Validation helper to check the test_size and train_size at init\\n\\n    NOTE This does not take into account the number of samples which is known\\n    only at split\\n    '\n    if ((test_size is None) and (train_size is None)):\n        raise ValueError('test_size and train_size can not both be None')\n    if (test_size is not None):\n        if (np.asarray(test_size).dtype.kind == 'f'):\n            if (test_size >= 1.0):\n                raise ValueError(('test_size=%f should be smaller than 1.0 or be an integer' % test_size))\n        elif (np.asarray(test_size).dtype.kind != 'i'):\n            raise ValueError(('Invalid value for test_size: %r' % test_size))\n    if (train_size is not None):\n        if (np.asarray(train_size).dtype.kind == 'f'):\n            if (train_size >= 1.0):\n                raise ValueError(('train_size=%f should be smaller than 1.0 or be an integer' % train_size))\n            elif ((np.asarray(test_size).dtype.kind == 'f') and ((train_size + test_size) > 1.0)):\n                raise ValueError(('The sum of test_size and train_size = %f, should be smaller than 1.0. Reduce test_size and/or train_size.' % (train_size + test_size)))\n        elif (np.asarray(train_size).dtype.kind != 'i'):\n            raise ValueError(('Invalid value for train_size: %r' % train_size))\n", "label": 1}
{"function": "\n\ndef acquire(self, timeout=None):\n    try:\n        open(self.unique_name, 'wb').close()\n    except IOError:\n        raise LockFailed(('failed to create %s' % self.unique_name))\n    end_time = time.time()\n    if ((timeout is not None) and (timeout > 0)):\n        end_time += timeout\n    while True:\n        try:\n            os.link(self.unique_name, self.lock_file)\n        except OSError:\n            nlinks = os.stat(self.unique_name).st_nlink\n            if (nlinks == 2):\n                return\n            else:\n                if ((timeout is not None) and (time.time() > end_time)):\n                    os.unlink(self.unique_name)\n                    if (timeout > 0):\n                        raise LockTimeout\n                    else:\n                        raise AlreadyLocked\n                time.sleep((((timeout is not None) and (timeout / 10)) or 0.1))\n        else:\n            return\n", "label": 1}
{"function": "\n\ndef set_rstate(self, seed):\n    if isinstance(seed, int):\n        if (seed == 0):\n            raise ValueError('seed should not be 0', seed)\n        elif (seed >= M2):\n            raise ValueError(('seed should be less than %i' % M2), seed)\n        self.rstate = numpy.asarray(([seed] * 6), dtype='int32')\n    elif (len(seed) == 6):\n        if ((seed[0] == 0) and (seed[1] == 0) and (seed[2] == 0)):\n            raise ValueError('The first 3 values of seed should not be all 0', seed)\n        if ((seed[3] == 0) and (seed[4] == 0) and (seed[5] == 0)):\n            raise ValueError('The last 3 values of seed should not be all 0', seed)\n        if ((seed[0] >= M1) or (seed[1] >= M1) or (seed[2] >= M1)):\n            raise ValueError(('The first 3 values of seed should be less than %i' % M1), seed)\n        if ((seed[3] >= M2) or (seed[4] >= M2) or (seed[5] >= M2)):\n            raise ValueError(('The last 3 values of seed should be less than %i' % M2), seed)\n        self.rstate = numpy.asarray(seed, dtype='int32')\n    else:\n        raise TypeError('seed should be 1 integer or 6 integers')\n", "label": 1}
{"function": "\n\ndef filter_records(records, name, args):\n    if (args.min_gq is not None):\n        records = filter_gq(records, name, args.min_gq)\n    if args.include_filter:\n        include = set((f.strip() for fs in args.include_filter for f in fs.split(',')))\n        records = (record for record in records if (not include.isdisjoint(record.filter)))\n    if args.exclude_filter:\n        exclude = set((f.strip() for fs in args.exclude_filter for f in fs.split(',')))\n        records = (record for record in records if exclude.isdisjoint(record.filter))\n    return records\n", "label": 1}
{"function": "\n\ndef calculate(self):\n    'Search for PEM encoded RSA keys.'\n    mem = utils.load_as(self._config, astype='physical')\n    addrs = list(mem.get_available_addresses())\n    assert (len(addrs) == 1), 'Physical memory is fragmented'\n    (mem_start, mem_size) = addrs[0]\n    for offset in xrange(0, mem_size, CHUNK_SIZE):\n        chunk = mem.zread(offset, CHUNK_SIZE)\n        if ((START_MARKER in chunk) and (END_MARKER in chunk)):\n            key = []\n            in_key = False\n            for line in chunk.splitlines():\n                if ((START_MARKER in line) and (not in_key)):\n                    in_key = True\n                    key.append(line)\n                elif ((END_MARKER in line) and in_key):\n                    in_key = False\n                    key.append(line)\n                elif in_key:\n                    key.append(line)\n            if (len(key) != 0):\n                (yield '\\n'.join(key))\n", "label": 1}
{"function": "\n\ndef test_optimizations_preserved(self):\n    a = T.dvector()\n    x = T.dvector('x')\n    s = T.dvector('s')\n    xm = T.dmatrix('x')\n    sm = T.dmatrix('s')\n    f = function([a, x, s, xm, sm], (((a.T.T * (tensor.dot(xm, sm.T.T.T) + x).T) * (x / x)) + s))\n    old_default_mode = config.mode\n    old_default_opt = config.optimizer\n    old_default_link = config.linker\n    try:\n        try:\n            str_f = pickle.dumps(f, protocol=(- 1))\n            config.mode = 'Mode'\n            config.linker = 'py'\n            config.optimizer = 'None'\n            g = pickle.loads(str_f)\n        except NotImplementedError as e:\n            if e[0].startswith('DebugMode is not pickl'):\n                g = 'ok'\n    finally:\n        config.mode = old_default_mode\n        config.optimizer = old_default_opt\n        config.linker = old_default_link\n    if (g == 'ok'):\n        return\n    assert (f.maker is not g.maker)\n    assert (f.maker.fgraph is not g.maker.fgraph)\n    tf = f.maker.fgraph.toposort()\n    tg = f.maker.fgraph.toposort()\n    assert (len(tf) == len(tg))\n    for (nf, ng) in zip(tf, tg):\n        assert (nf.op == ng.op)\n        assert (len(nf.inputs) == len(ng.inputs))\n        assert (len(nf.outputs) == len(ng.outputs))\n        assert ([i.type for i in nf.inputs] == [i.type for i in ng.inputs])\n        assert ([i.type for i in nf.outputs] == [i.type for i in ng.outputs])\n", "label": 1}
{"function": "\n\ndef get_last_activity(self, diffsets=None, reviews=None):\n    'Returns the last public activity information on the review request.\\n\\n        This will return the last object updated, along with the timestamp\\n        of that object. It can be used to judge whether something on a\\n        review request has been made public more recently.\\n        '\n    timestamp = self.last_updated\n    updated_object = self\n    if ((not diffsets) and self.repository_id):\n        latest_diffset = self.get_latest_diffset()\n        diffsets = []\n        if latest_diffset:\n            diffsets.append(latest_diffset)\n    if diffsets:\n        for diffset in diffsets:\n            if (diffset.timestamp >= timestamp):\n                timestamp = diffset.timestamp\n                updated_object = diffset\n    if (not reviews):\n        try:\n            reviews = [self.reviews.filter(public=True).latest()]\n        except ObjectDoesNotExist:\n            reviews = []\n    for review in reviews:\n        if (review.public and (review.timestamp >= timestamp)):\n            timestamp = review.timestamp\n            updated_object = review\n    return (timestamp, updated_object)\n", "label": 1}
{"function": "\n\ndef resumeProducing(self):\n    self.paused = False\n    if self._buffer:\n        data = ''.join(self._buffer)\n        bytesSent = self._writeSomeData(data)\n        if (bytesSent < len(data)):\n            unsent = data[bytesSent:]\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer[:] = [unsent]\n        else:\n            self._buffer[:] = []\n    else:\n        bytesSent = 0\n    if (self.unregistered and bytesSent and (not self._buffer) and (self.consumer is not None)):\n        self.consumer.unregisterProducer()\n    if (not self.iAmStreaming):\n        self.outstandingPull = (not bytesSent)\n    if (self.producer is not None):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (self.producerPaused and (bytesBuffered < self.bufferSize)):\n            self.producerPaused = False\n            self.producer.resumeProducing()\n        elif self.outstandingPull:\n            self.producer.resumeProducing()\n", "label": 1}
{"function": "\n\ndef mpf_atan(x, prec, rnd=round_fast):\n    (sign, man, exp, bc) = x\n    if (not man):\n        if (x == fzero):\n            return fzero\n        if (x == finf):\n            return atan_inf(0, prec, rnd)\n        if (x == fninf):\n            return atan_inf(1, prec, rnd)\n        return fnan\n    mag = (exp + bc)\n    if (mag > (prec + 20)):\n        return atan_inf(sign, prec, rnd)\n    if ((- mag) > (prec + 20)):\n        return mpf_perturb(x, (1 - sign), prec, rnd)\n    wp = ((prec + 30) + abs(mag))\n    if (mag >= 2):\n        x = mpf_rdiv_int(1, x, wp)\n        reciprocal = True\n    else:\n        reciprocal = False\n    t = to_fixed(x, wp)\n    if sign:\n        t = (- t)\n    if (wp < ATAN_TAYLOR_PREC):\n        a = atan_taylor(t, wp)\n    else:\n        a = atan_newton(t, wp)\n    if reciprocal:\n        a = (((pi_fixed(wp) >> 1) + 1) - a)\n    if sign:\n        a = (- a)\n    return from_man_exp(a, (- wp), prec, rnd)\n", "label": 1}
{"function": "\n\ndef _do_skips(cls):\n    reasons = []\n    all_configs = _possible_configs_for_cls(cls, reasons)\n    if getattr(cls, '__skip_if__', False):\n        for c in getattr(cls, '__skip_if__'):\n            if c():\n                config.skip_test((\"'%s' skipped by %s\" % (cls.__name__, c.__name__)))\n    if (not all_configs):\n        if getattr(cls, '__backend__', False):\n            msg = (\"'%s' unsupported for implementation '%s'\" % (cls.__name__, cls.__only_on__))\n        else:\n            msg = (\"'%s' unsupported on any DB implementation %s%s\" % (cls.__name__, ', '.join(((\"'%s(%s)+%s'\" % (config_obj.db.name, '.'.join((str(dig) for dig in config_obj.db.dialect.server_version_info)), config_obj.db.driver)) for config_obj in config.Config.all_configs())), ', '.join(reasons)))\n        config.skip_test(msg)\n    elif hasattr(cls, '__prefer_backends__'):\n        non_preferred = set()\n        spec = exclusions.db_spec(*util.to_list(cls.__prefer_backends__))\n        for config_obj in all_configs:\n            if (not spec(config_obj)):\n                non_preferred.add(config_obj)\n        if all_configs.difference(non_preferred):\n            all_configs.difference_update(non_preferred)\n    if (config._current not in all_configs):\n        _setup_config(all_configs.pop(), cls)\n", "label": 1}
{"function": "\n\ndef validate_access_params(self, get_data, post_data, header):\n    token_type = self.config[self.CONFIG_TOKEN_TYPE]\n    realm = self.config[self.CONFIG_WWW_REALM]\n    methods = 0\n    token = ''\n    if header:\n        (bearer, token) = header.split(' ')\n        if ((not bearer) or (not token)):\n            raise HTTP(401, 'Malformed auth header')\n        elif (bearer != self.TOKEN_BEARER_HEADER_NAME):\n            raise HTTP(415, 'Only \"Bearer\" token type is allowed')\n        methods += 1\n    try:\n        if get_data[self.TOKEN_PARAM_NAME]:\n            token = get_data[self.TOKEN_PARAM_NAME]\n            methods += 1\n    except:\n        pass\n    try:\n        if post_data[self.TOKEN_PARAM_NAME]:\n            token = post_data[self.TOKEN_PARAM_NAME]\n            methods += 1\n    except:\n        pass\n    if (methods > 1):\n        raise HTTP(405, 'Only one method may be used to authenticate at a time (Auth header, GET or POST).')\n    elif (not methods):\n        raise HTTP(424, 'LookupError: Supplied access token is invalid.')\n    token = self.storage.get_access_token(token)\n    if (not token):\n        raise HTTP(424, 'LookupError: Supplied access token is invalid.')\n    elif self.storage.expired_access_token(token):\n        raise HTTP(410, 'ValueError: The access token provided has expired')\n    return token['access_token']\n", "label": 1}
{"function": "\n\ndef step(self, dependency_states, step_context):\n    dep_product_node = self._dep_product_node()\n    dep_product_state = dependency_states.get(dep_product_node, None)\n    if ((dep_product_state is None) or (type(dep_product_state) == Waiting)):\n        return Waiting([dep_product_node])\n    elif (type(dep_product_state) == Throw):\n        return dep_product_state\n    elif (type(dep_product_state) == Noop):\n        return Noop('Could not compute {} to determine dependencies.'.format(dep_product_node))\n    elif (type(dep_product_state) != Return):\n        State.raise_unrecognized(dep_product_state)\n    dependencies = list(self._dependency_nodes(step_context, dep_product_state.value))\n    for dependency in dependencies:\n        dep_state = dependency_states.get(dependency, None)\n        if ((dep_state is None) or (type(dep_state) == Waiting)):\n            return Waiting(([dep_product_node] + dependencies))\n        elif (type(dep_state) == Throw):\n            return dep_state\n        elif (type(dep_state) == Noop):\n            return Throw(ValueError('No source of explicit dependency {}'.format(dependency)))\n        elif (type(dep_state) != Return):\n            raise State.raise_unrecognized(dep_state)\n    return Return([dependency_states[d].value for d in dependencies])\n", "label": 1}
{"function": "\n\ndef visit_transition(self, node):\n    index = node.parent.index(node)\n    error = None\n    if ((index == 0) or (isinstance(node.parent[0], nodes.title) and ((index == 1) or (isinstance(node.parent[1], nodes.subtitle) and (index == 2))))):\n        assert (isinstance(node.parent, nodes.document) or isinstance(node.parent, nodes.section))\n        error = self.document.reporter.error('Document or section may not begin with a transition.', source=node.source, line=node.line)\n    elif isinstance(node.parent[(index - 1)], nodes.transition):\n        error = self.document.reporter.error('At least one body element must separate transitions; adjacent transitions are not allowed.', source=node.source, line=node.line)\n    if error:\n        node.parent.insert(index, error)\n        index += 1\n    assert (index < len(node.parent))\n    if (index != (len(node.parent) - 1)):\n        return\n    sibling = node\n    while (index == (len(sibling.parent) - 1)):\n        sibling = sibling.parent\n        if (sibling.parent is None):\n            error = self.document.reporter.error('Document may not end with a transition.', line=node.line)\n            node.parent.insert((node.parent.index(node) + 1), error)\n            return\n        index = sibling.parent.index(sibling)\n    node.parent.remove(node)\n    sibling.parent.insert((index + 1), node)\n", "label": 1}
{"function": "\n\ndef to_lines(self):\n    '\\n        Yields:\\n          Chunks of Lua code.\\n        '\n    for token in self._tokens:\n        if (token.matches(lexer.TokComment) or token.matches(lexer.TokSpace)):\n            continue\n        elif token.matches(lexer.TokNewline):\n            if self._saw_if:\n                self._saw_if = False\n                self._last_was_name_keyword_number = False\n                (yield '\\n')\n            continue\n        elif token.matches(lexer.TokName):\n            if self._last_was_name_keyword_number:\n                (yield ' ')\n            self._last_was_name_keyword_number = True\n            (yield self._name_factory.get_short_name(token.code))\n        elif token.matches(lexer.TokKeyword):\n            if (token.code == 'if'):\n                self._saw_if = True\n            if self._last_was_name_keyword_number:\n                (yield ' ')\n            self._last_was_name_keyword_number = True\n            (yield token.code)\n        elif token.matches(lexer.TokNumber):\n            if self._last_was_name_keyword_number:\n                (yield ' ')\n            self._last_was_name_keyword_number = True\n            (yield token.code)\n        else:\n            self._last_was_name_keyword_number = False\n            (yield token.code)\n", "label": 1}
{"function": "\n\ndef post(self, *args, **kwargs):\n    widget = self.object\n    ordering = self.kwargs.get('ordering')\n    if (int(ordering) == 0):\n        widget.ordering = 0\n        widget.save()\n        widgets = getattr(widget.parent.content, widget.region)\n        widgets = [w for w in widgets if (w.id != widget.id)]\n        for (i, _widget) in enumerate(widgets):\n            _widget.ordering = (i + 1)\n            _widget.save()\n    elif (int(ordering) == (- 1)):\n        next_ordering = (widget.ordering - 1)\n        widgets = getattr(widget.parent.content, widget.region)\n        for w in widgets:\n            if (w.ordering == next_ordering):\n                w.ordering = widget.ordering\n                w.save()\n                widget.ordering = next_ordering\n                widget.save()\n    elif (int(ordering) == 1):\n        next_ordering = (widget.ordering + 1)\n        widgets = getattr(widget.parent.content, widget.region)\n        for w in widgets:\n            if (w.ordering == next_ordering):\n                w.ordering = widget.ordering\n                w.save()\n                widget.ordering = next_ordering\n                widget.save()\n    else:\n        widget.ordering = widget.next_ordering\n        widget.save()\n        widgets = getattr(widget.parent.content, widget.region)\n        widgets = [w for w in widgets if (w.id != widget.id)]\n        widgets.sort(key=(lambda w: w.ordering))\n        for (i, _widget) in enumerate(widgets):\n            _widget.ordering = i\n            _widget.save()\n    messages.success(self.request, _('Widget was successfully moved.'))\n    success_url = self.get_success_url()\n    response = HttpResponseRedirect(success_url)\n    response['X-Horizon-Location'] = success_url\n    return response\n", "label": 1}
{"function": "\n\ndef db_children(self, parent=(None, None), orphan=False):\n    children = []\n    for child in self._db_connections:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_annotations:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_abstractions:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_others:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_modules:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    if orphan:\n        for child in self._db_connections[:]:\n            self.db_delete_connection(child)\n        for child in self._db_annotations[:]:\n            self.db_delete_annotation(child)\n        for child in self._db_abstractions[:]:\n            self.db_delete_abstraction(child)\n        for child in self._db_others[:]:\n            self.db_delete_other(child)\n        for child in self._db_modules[:]:\n            self.db_delete_module(child)\n    children.append((self, parent[0], parent[1]))\n    return children\n", "label": 1}
{"function": "\n\ndef parse(self):\n    '\\n    Parse the vmstat file\\n    :return: status of the metric parse\\n    '\n    file_status = True\n    for input_file in self.infile_list:\n        file_status = (file_status and naarad.utils.is_valid_file(input_file))\n    if (not file_status):\n        return False\n    status = True\n    data = {\n        \n    }\n    for input_file in self.infile_list:\n        logger.info('Processing : %s', input_file)\n        timestamp_format = None\n        with open(input_file) as fh:\n            for line in fh:\n                words = line.split()\n                if (len(words) < 3):\n                    continue\n                ts = ((words[0] + ' ') + words[1])\n                if ((not timestamp_format) or (timestamp_format == 'unknown')):\n                    timestamp_format = naarad.utils.detect_timestamp_format(ts)\n                if (timestamp_format == 'unknown'):\n                    continue\n                ts = naarad.utils.get_standardized_timestamp(ts, timestamp_format)\n                if self.ts_out_of_range(ts):\n                    continue\n                col = words[2]\n                if (self.sub_metrics and (col not in self.sub_metrics)):\n                    continue\n                self.sub_metric_unit[col] = 'pages'\n                if (col in self.column_csv_map):\n                    out_csv = self.column_csv_map[col]\n                else:\n                    out_csv = self.get_csv(col)\n                    data[out_csv] = []\n                data[out_csv].append(((ts + ',') + words[3]))\n    for csv in data.keys():\n        self.csv_files.append(csv)\n        with open(csv, 'w') as fh:\n            fh.write('\\n'.join(data[csv]))\n    return status\n", "label": 1}
{"function": "\n\ndef onColor(self, event=None, item=None):\n    color = hexcolor(event.GetValue())\n    setattr(self.parent.conf, item, color)\n    if (item == 'spectra_color'):\n        self.parent.panel.conf.set_trace_color(color, trace=0)\n    elif (item == 'roi_color'):\n        self.parent.panel.conf.set_trace_color(color, trace=1)\n    elif (item == 'marker_color'):\n        for lmark in self.parent.cursor_markers:\n            if (lmark is not None):\n                lmark.set_color(color)\n    elif ((item == 'roi_fillcolor') and (self.parent.roi_patch is not None)):\n        self.parent.roi_patch.set_color(color)\n    elif (item == 'major_elinecolor'):\n        for l in self.parent.major_markers:\n            l.set_color(color)\n    elif (item == 'minor_elinecolor'):\n        for l in self.parent.minor_markers:\n            l.set_color(color)\n    elif (item == 'hold_elinecolor'):\n        for l in self.parent.hold_markers:\n            l.set_color(color)\n    self.parent.panel.canvas.draw()\n    self.parent.panel.Refresh()\n", "label": 1}
{"function": "\n\ndef get_filters(self):\n    is_public = public_filter()\n    is_active = active_filter()\n    is_datetime = datetime_filter(self.filter_yaml.get_image_date())\n    is_tenant = tenant_filter(self.filter_yaml.get_tenant())\n    images_list = self.filter_yaml.get_image_ids()\n    excluded_images_list = self.filter_yaml.get_excluded_image_ids()\n    if (images_list and excluded_images_list):\n        raise exception.AbortMigrationError(\"In the filter config file specified 'images_list' and 'exclude_images_list'. Must be only one list with images - 'images_list' or 'exclude_images_list'.\")\n    if excluded_images_list:\n        is_image_id = image_id_exclude_filter(excluded_images_list)\n    else:\n        is_image_id = image_id_filter(images_list)\n    is_member = member_filter(self.glance_client, self.filter_yaml.get_tenant())\n    if self.filter_yaml.is_public_and_member_images_filtered():\n        return [(lambda i: (is_active(i) and is_tenant(i) and is_image_id(i) and is_datetime(i)))]\n    else:\n        return [(lambda i: ((is_active(i) and is_public(i)) or (is_active(i) and is_member(i)) or (is_active(i) and is_tenant(i) and is_image_id(i) and is_datetime(i))))]\n", "label": 1}
{"function": "\n\ndef make_node(graph, block, block_to_node, vmap, gen_ret):\n    node = block_to_node.get(block)\n    if (node is None):\n        node = build_node_from_block(block, vmap, gen_ret)\n        block_to_node[block] = node\n    if block.exception_analysis:\n        for (_type, _, exception_target) in block.exception_analysis.exceptions:\n            exception_node = block_to_node.get(exception_target)\n            if (exception_node is None):\n                exception_node = build_node_from_block(exception_target, vmap, gen_ret, _type)\n                exception_node.set_catch_type(_type)\n                exception_node.in_catch = True\n                block_to_node[exception_target] = exception_node\n            graph.add_catch_edge(node, exception_node)\n    for (_, _, child_block) in block.childs:\n        child_node = block_to_node.get(child_block)\n        if (child_node is None):\n            child_node = build_node_from_block(child_block, vmap, gen_ret)\n            block_to_node[child_block] = child_node\n        graph.add_edge(node, child_node)\n        if node.type.is_switch:\n            node.add_case(child_node)\n        if node.type.is_cond:\n            if_target = (((block.end / 2) - (block.last_length / 2)) + node.off_last_ins)\n            child_addr = (child_block.start / 2)\n            if (if_target == child_addr):\n                node.true = child_node\n            else:\n                node.false = child_node\n    if (node.type.is_cond and (node.false is None)):\n        node.false = node.true\n    return node\n", "label": 1}
{"function": "\n\ndef test_incomplete_permissions(self):\n    profile_model_obj = get_profile_model()\n    content_type_profile = ContentType.objects.get_for_model(profile_model_obj)\n    content_type_user = ContentType.objects.get_for_model(User)\n    for (model, perms) in ASSIGNED_PERMISSIONS.items():\n        if (model == 'profile'):\n            content_type = content_type_profile\n        else:\n            content_type = content_type_user\n        for perm in perms:\n            Permission.objects.get(name=perm[1], content_type=content_type).delete()\n    for (model, perms) in ASSIGNED_PERMISSIONS.items():\n        if (model == 'profile'):\n            content_type = content_type_profile\n        else:\n            content_type = content_type_user\n        for perm in perms:\n            try:\n                perm = Permission.objects.get(name=perm[1], content_type=content_type)\n            except Permission.DoesNotExist:\n                pass\n            else:\n                self.fail(('Found %s: ' % perm))\n    call_command('check_permissions', test=True)\n    for (model, perms) in ASSIGNED_PERMISSIONS.items():\n        if (model == 'profile'):\n            content_type = content_type_profile\n        else:\n            content_type = content_type_user\n        for perm in perms:\n            try:\n                perm = Permission.objects.get(name=perm[1], content_type=content_type)\n            except Permission.DoesNotExist:\n                self.fail()\n", "label": 1}
{"function": "\n\ndef delete_file(self, path, prefixed_path, source_storage, **options):\n    symlink = options['link']\n    if self.storage.exists(prefixed_path):\n        try:\n            target_last_modified = self.storage.modified_time(prefixed_path)\n        except (OSError, NotImplementedError):\n            pass\n        else:\n            try:\n                source_last_modified = source_storage.modified_time(path)\n            except (OSError, NotImplementedError):\n                pass\n            else:\n                if self.local:\n                    full_path = self.storage.path(prefixed_path)\n                else:\n                    full_path = None\n                if (target_last_modified >= source_last_modified):\n                    if (not ((symlink and full_path and (not os.path.islink(full_path))) or ((not symlink) and full_path and os.path.islink(full_path)))):\n                        if (prefixed_path not in self.unmodified_files):\n                            self.unmodified_files.append(prefixed_path)\n                        self.log((\"Skipping '%s' (not modified)\" % path))\n                        return False\n        if options['dry_run']:\n            self.log((\"Pretending to delete '%s'\" % path))\n        else:\n            self.log((\"Deleting '%s'\" % path))\n            self.storage.delete(prefixed_path)\n    return True\n", "label": 1}
{"function": "\n\ndef image_vacuum(name):\n    '\\n    Delete images not in use or installed via image_present\\n    '\n    name = name.lower()\n    ret = {\n        'name': name,\n        'changes': {\n            \n        },\n        'result': None,\n        'comment': '',\n    }\n    images = []\n    for state in __salt__['state.show_lowstate']():\n        if ('state' not in state):\n            continue\n        if (state['state'] != __virtualname__):\n            continue\n        if (state['fun'] not in ['image_present']):\n            continue\n        if ('name' in state):\n            images.append(state['name'])\n    for image_uuid in __salt__['vmadm.list'](order='image_uuid'):\n        if (image_uuid in images):\n            continue\n        images.append(image_uuid)\n    ret['result'] = True\n    for image_uuid in __salt__['imgadm.list']():\n        if (image_uuid in images):\n            continue\n        if (image_uuid in __salt__['imgadm.delete'](image_uuid)):\n            ret['changes'][image_uuid] = None\n        else:\n            ret['result'] = False\n            ret['comment'] = 'failed to delete images'\n    if (ret['result'] and (len(ret['changes']) == 0)):\n        ret['comment'] = 'no images deleted'\n    elif (ret['result'] and (len(ret['changes']) > 0)):\n        ret['comment'] = 'images deleted'\n    return ret\n", "label": 1}
{"function": "\n\ndef describe_hosts(self, context, **_kwargs):\n    'Returns status info for all nodes. Includes:\\n            * Hostname\\n            * Compute (up, down, None)\\n            * Instance count\\n            * Volume (up, down, None)\\n            * Volume Count\\n        '\n    services = db.service_get_all(context, False)\n    now = utils.utcnow()\n    hosts = []\n    rv = []\n    for host in [service['host'] for service in services]:\n        if (not (host in hosts)):\n            hosts.append(host)\n    for host in hosts:\n        compute = [s for s in services if ((s['host'] == host) and (s['binary'] == 'nova-compute'))]\n        if compute:\n            compute = compute[0]\n        instances = db.instance_get_all_by_host(context, host)\n        volume = [s for s in services if ((s['host'] == host) and (s['binary'] == 'nova-volume'))]\n        if volume:\n            volume = volume[0]\n        volumes = db.volume_get_all_by_host(context, host)\n        rv.append(host_dict(host, compute, instances, volume, volumes, now))\n    return {\n        'hosts': rv,\n    }\n", "label": 1}
{"function": "\n\n@register_specialize\n@gof.local_optimizer([T.mul, T.true_div])\ndef local_abs_merge(node):\n    \"\\n    Merge abs generated by local_abs_lift when the canonizer don't\\n    need it anymore\\n\\n    \"\n    if ((node.op == T.mul) and (sum([(i.owner.op == T.abs_) for i in node.inputs if i.owner]) > 1)):\n        inputs = []\n        for i in node.inputs:\n            if (i.owner and (i.owner.op == T.abs_)):\n                inputs.append(i.owner.inputs[0])\n            elif isinstance(i, Constant):\n                try:\n                    const = get_scalar_constant_value(i)\n                except NotScalarConstantError:\n                    return False\n                if (not (const >= 0).all()):\n                    return False\n                inputs.append(i)\n            else:\n                return False\n        return [T.abs_(T.mul(*inputs))]\n    if ((node.op == T.true_div) and (sum([(i.owner.op == T.abs_) for i in node.inputs if i.owner]) == 2)):\n        return [T.abs_(T.true_div(node.inputs[0].owner.inputs[0], node.inputs[1].owner.inputs[0]))]\n", "label": 1}
{"function": "\n\ndef on_selection_modified(self, view):\n    if JsLintEventListener.disabled:\n        return\n    if (view.name() != RESULT_VIEW_NAME):\n        return\n    region = view.line(view.sel()[0])\n    s = sublime.load_settings(SETTINGS_FILE)\n    if (self.previous_resion == region):\n        return\n    self.previous_resion = region\n    if s.get('use_node_jslint', False):\n        pattern_position = '\\\\/\\\\/ Line (\\\\d+), Pos (\\\\d+)$'\n        text = view.substr(region)\n        text = re.findall(pattern_position, text)\n        if (len(text) > 0):\n            line = int(text[0][0])\n            col = int(text[0][1])\n    else:\n        text = view.substr(region).split(':')\n        if ((len(text) < 4) or (text[0] != 'jslint') or (re.match('\\\\d+', text[2]) is None) or (re.match('\\\\d+', text[3]) is None)):\n            return\n        line = int(text[2])\n        col = int(text[3])\n    view.add_regions(RESULT_VIEW_NAME, [region], 'comment')\n    file_path = view.settings().get('file_path')\n    window = sublime.active_window()\n    file_view = None\n    for v in window.views():\n        if (v.file_name() == file_path):\n            file_view = v\n            break\n    if (file_view is None):\n        return\n    self.file_view = file_view\n    window.focus_view(file_view)\n    file_view.run_command('goto_line', {\n        'line': line,\n    })\n    file_region = file_view.line(file_view.sel()[0])\n    file_view.add_regions(RESULT_VIEW_NAME, [file_region], 'string')\n", "label": 1}
{"function": "\n\ndef test_mergeComps_K5_D3_withELBO_kA0(self, K=5, D=3):\n    SS = self.makeSuffStatBagAndFillWithOnes(K, D)\n    self.addELBOtoSuffStatBag(SS, K)\n    SS.mergeComps(0, 1)\n    (s, N, x, xxT) = self.getExpectedMergedFields(K, D)\n    assert (SS.K == (K - 1))\n    assert (SS._ELBOTerms.K == (K - 1))\n    assert (SS._MergeTerms.K == (K - 1))\n    assert np.allclose(SS.s, s)\n    assert np.allclose(SS.N, N)\n    assert np.allclose(SS.x, x)\n    assert np.allclose(SS.xxT, xxT)\n    assert np.allclose(SS.getELBOTerm('Elogz'), [2.0, 1, 1, 1])\n    assert np.allclose(SS.getELBOTerm('Econst'), 1.0)\n    assert np.all(np.isnan(SS._MergeTerms.Elogz[0, 1:]))\n    assert np.all(np.isnan(SS._MergeTerms.Elogz[:0, 0]))\n", "label": 1}
{"function": "\n\ndef _matches(self, expr, repl_dict={\n    \n}):\n    from sympy import Wild\n    sign = 1\n    (a, b) = self.as_two_terms()\n    if (a is S.NegativeOne):\n        if b.is_Mul:\n            sign = (- sign)\n        else:\n            return b.matches((- expr), repl_dict)\n    expr = sympify(expr)\n    if (expr.is_Mul and (expr.args[0] is S.NegativeOne)):\n        expr = (- expr)\n        sign = (- sign)\n    if (not expr.is_Mul):\n        if (len(self.args) == 2):\n            if (b == expr):\n                return a.matches(Rational(sign), repl_dict)\n            dd = b.matches(expr, repl_dict)\n            if (dd is None):\n                return None\n            dd = a.matches(Rational(sign), dd)\n            return dd\n        return None\n    d = repl_dict.copy()\n    pp = list(self.args)\n    ee = list(expr.args)\n    for p in self.args:\n        if (p in expr.args):\n            ee.remove(p)\n            pp.remove(p)\n    if ((len(pp) == 1) and isinstance(pp[0], Wild)):\n        if (len(ee) == 1):\n            d[pp[0]] = (sign * ee[0])\n        else:\n            d[pp[0]] = (sign * expr.func(*ee))\n        return d\n    if (len(ee) != len(pp)):\n        return None\n    for (p, e) in zip(pp, ee):\n        d = p.xreplace(d).matches(e, d)\n        if (d is None):\n            return None\n    return d\n", "label": 1}
{"function": "\n\ndef visit_Task(self, node):\n    self.push_module(None)\n    name = node.name\n    _task = task.Task(name)\n    statement = [self.visit(s) for s in node.statement]\n    body = []\n    for s in statement:\n        if isinstance(s, (tuple, list)):\n            for d in s:\n                if isinstance(d, vtypes.Input):\n                    t = _task.Input(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Reg):\n                    t = _task.Reg(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Integer):\n                    t = _task.Integer(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                else:\n                    body.append(s)\n        else:\n            body.append(s)\n    _task.Body(*body)\n    self.pop_module()\n    self.add_object(_task)\n    return _task\n", "label": 1}
{"function": "\n\ndef test_mix():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test'],\n            'write': ['test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef _update(self, frame_no):\n    if self._signal:\n        start_x = int(((self._screen.width - self._signal.max_width) // 2))\n        start_y = int(((self._screen.height - self._signal.max_height) // 2))\n        (text, colours) = self._signal.rendered_text\n    else:\n        start_x = start_y = 0\n        (text, colours) = ('', [])\n    for y in range(self._screen.height):\n        if (self._strength < 1.0):\n            offset = randint(0, int((6 - (6 * self._strength))))\n        else:\n            offset = 0\n        for x in range(self._screen.width):\n            ix = (x - start_x)\n            iy = (y - start_y)\n            if (self._signal and (random() <= self._strength) and (x >= start_x) and (y >= start_y) and (iy < len(text)) and ((ix + offset) < len(text[iy]))):\n                self._screen.paint(text[iy][(ix + offset)], x, y, colour_map=[colours[iy][ix]])\n            elif (random() < 0.2):\n                self._screen.print_at(chr(randint(33, 126)), x, y)\n    self._strength += self._step\n    if ((self._strength >= 1.25) or (self._strength <= (- 0.5))):\n        self._step = (- self._step)\n", "label": 1}
{"function": "\n\ndef zmq_device(self):\n    '\\n        Multiprocessing target for the zmq queue device\\n        '\n    self.__setup_signals()\n    salt.utils.appendproctitle('MWorkerQueue')\n    self.context = zmq.Context(self.opts['worker_threads'])\n    self.uri = 'tcp://{interface}:{ret_port}'.format(**self.opts)\n    self.clients = self.context.socket(zmq.ROUTER)\n    if ((self.opts['ipv6'] is True) and hasattr(zmq, 'IPV4ONLY')):\n        self.clients.setsockopt(zmq.IPV4ONLY, 0)\n    if (HAS_ZMQ_MONITOR and self.opts['zmq_monitor']):\n        import threading\n        self._monitor = ZeroMQSocketMonitor(self.clients)\n        t = threading.Thread(target=self._monitor.start_poll)\n        t.start()\n    self.workers = self.context.socket(zmq.DEALER)\n    if (self.opts.get('ipc_mode', '') == 'tcp'):\n        self.w_uri = 'tcp://127.0.0.1:{0}'.format(self.opts.get('tcp_master_workers', 4515))\n    else:\n        self.w_uri = 'ipc://{0}'.format(os.path.join(self.opts['sock_dir'], 'workers.ipc'))\n    log.info('Setting up the master communication server')\n    self.clients.bind(self.uri)\n    self.workers.bind(self.w_uri)\n    while True:\n        if (self.clients.closed or self.workers.closed):\n            break\n        try:\n            zmq.device(zmq.QUEUE, self.clients, self.workers)\n        except zmq.ZMQError as exc:\n            if (exc.errno == errno.EINTR):\n                continue\n            raise exc\n        except (KeyboardInterrupt, SystemExit):\n            break\n", "label": 1}
{"function": "\n\ndef parse_grammar(self, data):\n    if ((self.data is None) or (self.data != data)):\n        self.data = data\n        self.regions = []\n    else:\n        if (self.printer is not None):\n            self.printer(0, 'Already parse')\n        return True\n    starttime = clock()\n    if ('compilation_unit' in self.grammar):\n        if (self.printer is not None):\n            self.printer(0, '== Compilation unit ==')\n        begin = 0\n        if (('before_separator' not in self.grammar['compilation_unit']) or self.grammar['compilation_unit']['before_separator']):\n            separator_output = self.parse_rule(self.grammar['separator'], True, '', 0, 0)\n            if separator_output['successive_match']:\n                begin = separator_output['end']\n                self.regions += separator_output['regions']\n        parse_output = self.parse_rule(self.grammar['compilation_unit'], False, '', 0, begin)\n        if parse_output['successive_match']:\n            self.regions += parse_output['regions']\n        if (('after_separator' not in self.grammar['compilation_unit']) or self.grammar['compilation_unit']['after_separator']):\n            separator_output = self.parse_rule(self.grammar['separator'], True, '', 0, parse_output['end'])\n            if separator_output['successive_match']:\n                parse_output['end'] = separator_output['end']\n                self.regions += separator_output['regions']\n    self.elapse_time = (clock() - starttime)\n    return {\n        'success': parse_output['successive_match'],\n        'begin': parse_output['begin'],\n        'end': parse_output['end'],\n    }\n", "label": 1}
{"function": "\n\ndef Table(*args, **kw):\n    'A schema.Table wrapper/hook for dialect-specific tweaks.'\n    test_opts = dict([(k, kw.pop(k)) for k in list(kw) if k.startswith('test_')])\n    kw.update(table_options)\n    if exclusions.against(config._current, 'mysql'):\n        if (('mysql_engine' not in kw) and ('mysql_type' not in kw)):\n            if (('test_needs_fk' in test_opts) or ('test_needs_acid' in test_opts)):\n                kw['mysql_engine'] = 'InnoDB'\n            else:\n                kw['mysql_engine'] = 'MyISAM'\n    if exclusions.against(config._current, 'firebird'):\n        table_name = args[0]\n        unpack = config.db.dialect.identifier_preparer.unformat_identifiers\n        fks = [fk for col in args if isinstance(col, schema.Column) for fk in col.foreign_keys]\n        for fk in fks:\n            ref = fk._colspec\n            if isinstance(ref, schema.Column):\n                name = ref.table.name\n            else:\n                name = unpack(ref)[0]\n            if (name == table_name):\n                if (fk.ondelete is None):\n                    fk.ondelete = 'CASCADE'\n                if (fk.onupdate is None):\n                    fk.onupdate = 'CASCADE'\n    return schema.Table(*args, **kw)\n", "label": 1}
{"function": "\n\ndef smi(self, subdata):\n    if (requests_version < 131840):\n        if is_py2:\n            subdata = subdata.content\n        else:\n            subdata = subdata.content.decode('latin')\n    else:\n        subdata.encoding = 'ISO-8859-1'\n        subdata = subdata.text\n    ssubdata = StringIO(subdata)\n    timea = 0\n    number = 1\n    data = None\n    subs = ''\n    TAG_RE = re.compile('<[^>]+>')\n    bad_char = re.compile('\\\\x96')\n    for i in ssubdata.readlines():\n        i = i.rstrip()\n        sync = re.search('<SYNC Start=(\\\\d+)>', i)\n        if sync:\n            if (int(sync.group(1)) != int(timea)):\n                if (data and (data != '&nbsp;')):\n                    subs += ('%s\\n%s --> %s\\n' % (number, timestr(timea), timestr(sync.group(1))))\n                    text = ('%s\\n' % TAG_RE.sub('', data.replace('<br>', '\\n')))\n                    if (text[(len(text) - 2)] != '\\n'):\n                        text += '\\n'\n                    subs += text\n                    number += 1\n            timea = sync.group(1)\n        text = re.search('<P Class=SVCC>(.*)', i)\n        if text:\n            data = text.group(1)\n    recomp = re.compile('\\\\r')\n    text = bad_char.sub('-', recomp.sub('', subs)).replace('&quot;', '\"')\n    if (is_py2 and isinstance(text, unicode)):\n        return text.encode('utf-8')\n    return text\n", "label": 1}
{"function": "\n\ndef get_constraint_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances')\n    constraint_matrix = [[True for j in xrange(num_instances)] for i in xrange(num_hosts)]\n    instance_type = (filter_properties.get('instance_type') or {\n        \n    })\n    if (not instance_type):\n        return constraint_matrix\n    else:\n        instance_vcpus = instance_type['vcpus']\n    if (instance_vcpus <= 0):\n        LOG.warn(_LW('ExactVcpuConstraint is skipped because requested instance vCPU number is 0 or invalid.'))\n        return constraint_matrix\n    for i in xrange(num_hosts):\n        if (not hosts[i].vcpus_total):\n            LOG.warn(_LW('vCPUs of %(host)s not set; assuming CPU collection broken.'), {\n                'host': hosts[i],\n            })\n            constraint_matrix[i] = [False for j in xrange(num_instances)]\n            continue\n        else:\n            usable_vcpus = (hosts[i].vcpus_total - hosts[i].vcpus_used)\n        if (instance_vcpus == usable_vcpus):\n            constraint_matrix[i] = ([True] + [False for j in xrange((num_instances - 1))])\n        else:\n            constraint_matrix[i] = [False for j in xrange(num_instances)]\n            LOG.debug('%(host)s does not have exactly %(requested_num)s vcpus, it has %(usable_num)s vcpus.', {\n                'host': hosts[i],\n                'requested_num': instance_vcpus,\n                'usable_num': usable_vcpus,\n            })\n    return constraint_matrix\n", "label": 1}
{"function": "\n\ndef check_node_group_template_usage(node_group_template_id, **kwargs):\n    cluster_users = []\n    template_users = []\n    for cluster in api.get_clusters():\n        if (node_group_template_id in [node_group.node_group_template_id for node_group in cluster.node_groups]):\n            cluster_users += [cluster.name]\n    for cluster_template in api.get_cluster_templates():\n        if (node_group_template_id in [node_group.node_group_template_id for node_group in cluster_template.node_groups]):\n            template_users += [cluster_template.name]\n    if (cluster_users or template_users):\n        raise ex.InvalidReferenceException((_('Node group template %(template)s is in use by cluster templates: %(users)s; and clusters: %(clusters)s') % {\n            'template': node_group_template_id,\n            'users': ((template_users and ', '.join(template_users)) or 'N/A'),\n            'clusters': ((cluster_users and ', '.join(cluster_users)) or 'N/A'),\n        }))\n", "label": 1}
{"function": "\n\ndef get_column_specification(self, column, **kwargs):\n    colspec = ((self.preparer.format_column(column) + ' ') + self.dialect.type_compiler.process(column.type))\n    if (column.table is None):\n        raise exc.CompileError('The Sybase dialect requires Table-bound columns in order to generate DDL')\n    seq_col = column.table._autoincrement_column\n    if (seq_col is column):\n        sequence = (isinstance(column.default, sa_schema.Sequence) and column.default)\n        if sequence:\n            (start, increment) = ((sequence.start or 1), (sequence.increment or 1))\n        else:\n            (start, increment) = (1, 1)\n        if ((start, increment) == (1, 1)):\n            colspec += ' IDENTITY'\n        else:\n            colspec += (' IDENTITY(%s,%s)' % (start, increment))\n    else:\n        default = self.get_column_default_string(column)\n        if (default is not None):\n            colspec += (' DEFAULT ' + default)\n        if (column.nullable is not None):\n            if ((not column.nullable) or column.primary_key):\n                colspec += ' NOT NULL'\n            else:\n                colspec += ' NULL'\n    return colspec\n", "label": 1}
{"function": "\n\ndef test_complex_inverse_functions():\n    mp.dps = 15\n    iv.dps = 15\n    for (z1, z2) in random_complexes(30):\n        assert sinh(asinh(z1)).ae(z1)\n        assert acosh(z1).ae(cmath.acosh(z1))\n        assert atanh(z1).ae(cmath.atanh(z1))\n        assert atan(z1).ae(cmath.atan(z1))\n        assert asin(z1).ae(cmath.asin(z1), rel_eps=1e-12)\n        assert acos(z1).ae(cmath.acos(z1), rel_eps=1e-12)\n        one = mpf(1)\n    for i in range((- 9), 10, 3):\n        for k in range((- 9), 10, 3):\n            a = (((0.9 * j) * (10 ** k)) + ((0.8 * one) * (10 ** i)))\n            b = cos(acos(a))\n            assert b.ae(a)\n            b = sin(asin(a))\n            assert b.ae(a)\n    one = mpf(1)\n    err = (2 * (10 ** (- 15)))\n    for i in range((- 9), 9, 3):\n        for k in range((- 9), 9, 3):\n            a = (((- 0.9) * (10 ** k)) + (((j * 0.8) * one) * (10 ** i)))\n            b = cosh(acosh(a))\n            assert b.ae(a, err)\n            b = sinh(asinh(a))\n            assert b.ae(a, err)\n", "label": 1}
{"function": "\n\ndef test_get_memory_maps(self):\n    p = psutil.Process(os.getpid())\n    maps = p.get_memory_maps()\n    paths = [x for x in maps]\n    self.assertEqual(len(paths), len(set(paths)))\n    ext_maps = p.get_memory_maps(grouped=False)\n    for nt in maps:\n        if (not nt.path.startswith('[')):\n            assert os.path.isabs(nt.path), nt.path\n            if POSIX:\n                assert os.path.exists(nt.path), nt.path\n            elif ('64' not in os.path.basename(nt.path)):\n                assert os.path.exists(nt.path), nt.path\n    for nt in ext_maps:\n        for fname in nt._fields:\n            value = getattr(nt, fname)\n            if (fname == 'path'):\n                continue\n            elif (fname in ('addr', 'perms')):\n                assert value, value\n            else:\n                assert isinstance(value, (int, long))\n                assert (value >= 0), value\n", "label": 1}
{"function": "\n\ndef update(self):\n    '\\n        The function to draw a new frame for the particle system.\\n        '\n    if (self.time_left > 0):\n        self.time_left -= 1\n        for _ in range(self._count):\n            new_particle = self._new_particle()\n            if (new_particle is not None):\n                self.particles.append(new_particle)\n    for particle in self.particles:\n        last = particle.last()\n        if (last is not None):\n            (char, x, y, fg, attr, bg) = last\n            screen_data = self._screen.get_from(x, y)\n            if (self._blend and screen_data):\n                (char2, fg2, attr2, bg2) = screen_data\n                index = 0\n                for (i, colours) in enumerate(particle.colours):\n                    if ((fg2, attr2, bg2) == colours):\n                        index = i\n                        break\n                index -= 1\n                (fg, attr, bg) = particle.colours[max(index, 0)]\n            self._screen.print_at(' ', x, y, fg, attr, bg)\n        if (particle.time < particle.life_time):\n            (char, x, y, fg, attr, bg) = particle.next()\n            screen_data = self._screen.get_from(x, y)\n            if (self._blend and screen_data):\n                (char2, fg2, attr2, bg2) = screen_data\n                index = (- 1)\n                for (i, colours) in enumerate(particle.colours):\n                    if ((fg2, attr2, bg2) == colours):\n                        index = i\n                        break\n                index += 1\n                (fg, attr, bg) = particle.colours[min(index, (len(particle.colours) - 1))]\n            self._screen.print_at(char, x, y, fg, attr, bg)\n        else:\n            self.particles.remove(particle)\n", "label": 1}
{"function": "\n\ndef _skip_instance(self):\n    skip = self._buffer.read_bits(8)\n    if (skip == 0):\n        length = self._vint()\n        for i in xrange(length):\n            self._skip_instance()\n    elif (skip == 1):\n        length = self._vint()\n        self._buffer.read_aligned_bytes(((length + 7) / 8))\n    elif (skip == 2):\n        length = self._vint()\n        self._buffer.read_aligned_bytes(length)\n    elif (skip == 3):\n        tag = self._vint()\n        self._skip_instance()\n    elif (skip == 4):\n        exists = (self._buffer.read_bits(8) != 0)\n        if exists:\n            self._skip_instance()\n    elif (skip == 5):\n        length = self._vint()\n        for i in xrange(length):\n            tag = self._vint()\n            self._skip_instance()\n    elif (skip == 6):\n        self._buffer.read_aligned_bytes(1)\n    elif (skip == 7):\n        self._buffer.read_aligned_bytes(4)\n    elif (skip == 8):\n        self._buffer.read_aligned_bytes(8)\n    elif (skip == 9):\n        self._vint()\n", "label": 1}
{"function": "\n\ndef breadcrumbs_for_flatpages(request, flatpage):\n    ' given request and flatpage instance create breadcrumbs for all flat\\n    pages '\n    if ((not hasattr(request, 'breadcrumbs')) or (not isinstance(request.breadcrumbs, Breadcrumbs))):\n        raise BreadcrumbsNotSet('You need to setup breadcrumbs to use this function.')\n    if ((not isinstance(flatpage, FlatPage)) or (not hasattr(flatpage, 'id'))):\n        raise TypeError(\"flatpage argument isn't a FlatPage instance or not have id.\")\n    paths = []\n    for part in request.path_info.split('/'):\n        if (len(part) == 0):\n            continue\n        if (not part.startswith('/')):\n            part = ('/' + part)\n        if (not part.endswith('/')):\n            part = (part + '/')\n        if (len(paths) > 0):\n            url = ''.join((paths + [part]))\n        else:\n            url = part\n        if (url == flatpage.url):\n            request.breadcrumbs(flatpage.title, flatpage.url)\n        else:\n            try:\n                f = FlatPage.objects.get(url=url, sites=settings.SITE_ID)\n            except FlatPage.DoesNotExist:\n                continue\n            else:\n                request.breadcrumbs(f.title, f.url)\n        paths.append(('/' + url[1:(- 1)].rpartition('/')[(- 1)]))\n", "label": 1}
{"function": "\n\ndef _non_dominated_front_fast(iterable, key=(lambda x: x), allowequality=True):\n    'Return a subset of items from iterable which are not dominated by any\\n    other item in iterable.\\n\\n    Faster version.\\n    '\n    items = list(iterable)\n    keys = dict(((i, key(i)) for i in items))\n    dim = len(list(keys.values())[0])\n    dominations = {\n        \n    }\n    for i in items:\n        for j in items:\n            good = True\n            if allowequality:\n                for k in range(dim):\n                    if (keys[i][k] >= keys[j][k]):\n                        good = False\n                        break\n            else:\n                for k in range(dim):\n                    if (keys[i][k] > keys[j][k]):\n                        good = False\n                        break\n            if good:\n                dominations[(i, j)] = None\n    res = set()\n    items = set(items)\n    for i in items:\n        res.add(i)\n        for j in list(res):\n            if (i is j):\n                continue\n            if ((j, i) in dominations):\n                res.remove(i)\n                break\n            elif ((i, j) in dominations):\n                res.remove(j)\n    return res\n", "label": 1}
{"function": "\n\ndef _validate_python(self, value, state):\n    if (not value):\n        raise Invalid(self.message('empty', state), value, state)\n    value = value.strip()\n    splitted = value.split('@', 1)\n    try:\n        (username, domain) = splitted\n    except ValueError:\n        raise Invalid(self.message('noAt', state), value, state)\n    if (not self.usernameRE.search(username)):\n        raise Invalid(self.message('badUsername', state, username=username), value, state)\n    try:\n        idna_domain = [idna.ToASCII(p) for p in domain.split('.')]\n        if (six.text_type is str):\n            idna_domain = [p.decode('ascii') for p in idna_domain]\n        idna_domain = '.'.join(idna_domain)\n    except UnicodeError:\n        raise Invalid(self.message('badDomain', state, domain=domain), value, state)\n    if (not self.domainRE.search(idna_domain)):\n        raise Invalid(self.message('badDomain', state, domain=domain), value, state)\n    if self.resolve_domain:\n        assert have_dns, 'dnspython should be available'\n        global socket\n        if (socket is None):\n            import socket\n        try:\n            try:\n                dns.resolver.query(domain, 'MX')\n            except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                try:\n                    dns.resolver.query(domain, 'A')\n                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                    raise Invalid(self.message('domainDoesNotExist', state, domain=domain), value, state)\n        except (socket.error, dns.exception.DNSException) as e:\n            raise Invalid(self.message('socketError', state, error=e), value, state)\n", "label": 1}
{"function": "\n\ndef transpile_md_to_python(markdown):\n    'A very naive markdown to python converter.'\n    for line in markdown.split('\\n'):\n        line = line.lstrip()\n        if line.startswith('# '):\n            (yield Heading(1, line, 'h1'))\n        elif line.startswith('## '):\n            (yield Heading(2, line, 'h2'))\n        elif line.startswith('### '):\n            (yield Heading(3, line, 'h3'))\n        elif line.startswith('#### '):\n            (yield Heading(4, line, 'h4'))\n        elif line.startswith('##### '):\n            (yield Heading(5, line, 'h5'))\n        elif line.startswith('###### '):\n            (yield Heading(6, line, 'h6'))\n        elif (line.startswith('*') and (not line.endswith('**'))):\n            (yield Tag('emphasis', line, 'em'))\n        elif (line.startswith('**') and line.endswith('**')):\n            (yield Tag('strong', line, 'strong'))\n        elif (line.startswith('[') and line.endswith(')')):\n            (yield Tag('href', line, 'a'))\n", "label": 1}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a datetime. Returns a\\n        Python datetime.datetime object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value\n    if isinstance(value, datetime.date):\n        return datetime.datetime(value.year, value.month, value.day)\n    if isinstance(value, list):\n        if (len(value) != 4):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES) and (value[2] in validators.EMPTY_VALUES) and (value[3] in validators.EMPTY_VALUES)):\n            return None\n        start_value = ('%s %s' % tuple(value[:2]))\n        end_value = ('%s %s' % tuple(value[2:]))\n    start_datetime = None\n    end_datetime = None\n    for format in (self.input_formats or formats.get_format('DATETIME_INPUT_FORMATS')):\n        try:\n            start_datetime = datetime.datetime(*time.strptime(start_value, format)[:6])\n        except ValueError:\n            continue\n    for format in (self.input_formats or formats.get_format('DATETIME_INPUT_FORMATS')):\n        try:\n            end_datetime = datetime.datetime(*time.strptime(end_value, format)[:6])\n        except ValueError:\n            continue\n    return (start_datetime, end_datetime)\n", "label": 1}
{"function": "\n\ndef test_lrucache(self):\n    c = LRUCache(2, dispose=(lambda _: None))\n    assert (len(c) == 0)\n    assert (c.items() == set())\n    for (i, x) in enumerate('abc'):\n        c[x] = i\n    assert (len(c) == 2)\n    assert (c.items() == set([('b', 1), ('c', 2)]))\n    assert ('a' not in c)\n    assert ('b' in c)\n    with pytest.raises(KeyError):\n        c['a']\n    assert (c['b'] == 1)\n    assert (c['c'] == 2)\n    c['d'] = 3\n    assert (len(c) == 2)\n    assert (c['c'] == 2)\n    assert (c['d'] == 3)\n    del c['c']\n    assert (len(c) == 1)\n    with pytest.raises(KeyError):\n        c['c']\n    assert (c['d'] == 3)\n    c.clear()\n    assert (c.items() == set())\n", "label": 1}
{"function": "\n\ndef test_disk_partitions(self):\n    for disk in psutil.disk_partitions(all=False):\n        assert os.path.exists(disk.device), disk\n        assert os.path.isdir(disk.mountpoint), disk\n        assert disk.fstype, disk\n        assert isinstance(disk.opts, str)\n    for disk in psutil.disk_partitions(all=True):\n        if (not WINDOWS):\n            try:\n                os.stat(disk.mountpoint)\n            except OSError:\n                err = sys.exc_info()[1]\n                if (err.errno not in (errno.EPERM, errno.EACCES)):\n                    raise\n            else:\n                assert os.path.isdir(disk.mountpoint), disk.mountpoint\n        assert isinstance(disk.fstype, str)\n        assert isinstance(disk.opts, str)\n\n    def find_mount_point(path):\n        path = os.path.abspath(path)\n        while (not os.path.ismount(path)):\n            path = os.path.dirname(path)\n        return path\n    mount = find_mount_point(__file__)\n    mounts = [x.mountpoint for x in psutil.disk_partitions(all=True)]\n    self.assertIn(mount, mounts)\n    psutil.disk_usage(mount)\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_2d_time_lowmem():\n    ' low memory reading/writing of 2D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(DATA_DIR, 'rnmrtk_2d', 'time_2d.sec'))\n    assert (data.shape == (332, 1500))\n    assert (np.abs((data[(0, 1)].real - 360.07)) <= 0.01)\n    assert (np.abs((data[(0, 1)].imag - (- 223.2))) <= 0.01)\n    assert (np.abs((data[(10, 18)].real - 17.93)) <= 0.01)\n    assert (np.abs((data[(10, 18)].imag - (- 67.2))) <= 0.01)\n    assert (dic['sw'][1] == 50000.0)\n    assert (dic['sf'][1] == 125.69)\n    assert (dic['ppm'][1] == 55.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 50.65)\n    assert (dic['ppm'][0] == 120.0)\n    lowmem_write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef visit_Function(self, node):\n    self.push_module(None)\n    name = node.name\n    width = (self.visit(node.retwidth) if (node.retwidth is not None) else None)\n    func = function.Function(name, width)\n    if (node.retwidth is not None):\n        func._set_raw_width(self.visit(node.retwidth.msb), self.visit(node.retwidth.lsb))\n    statement = [self.visit(s) for s in node.statement]\n    body = []\n    for s in statement:\n        if isinstance(s, (tuple, list)):\n            for d in s:\n                if isinstance(d, vtypes.Input):\n                    t = func.Input(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Reg):\n                    t = func.Reg(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Integer):\n                    t = func.Integer(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                else:\n                    body.append(s)\n        else:\n            body.append(s)\n    func.Body(*body)\n    self.pop_module()\n    self.add_object(func)\n    return func\n", "label": 1}
{"function": "\n\n@coroutine\ndef _wrap_awaitable(x):\n    if hasattr(x, '__await__'):\n        _i = x.__await__()\n    else:\n        _i = iter(x)\n    try:\n        _y = next(_i)\n    except StopIteration as _e:\n        _r = _value_from_stopiteration(_e)\n    else:\n        while 1:\n            try:\n                _s = (yield _y)\n            except GeneratorExit as _e:\n                try:\n                    _m = _i.close\n                except AttributeError:\n                    pass\n                else:\n                    _m()\n                raise _e\n            except BaseException as _e:\n                _x = sys.exc_info()\n                try:\n                    _m = _i.throw\n                except AttributeError:\n                    raise _e\n                else:\n                    try:\n                        _y = _m(*_x)\n                    except StopIteration as _e:\n                        _r = _value_from_stopiteration(_e)\n                        break\n            else:\n                try:\n                    if (_s is None):\n                        _y = next(_i)\n                    else:\n                        _y = _i.send(_s)\n                except StopIteration as _e:\n                    _r = _value_from_stopiteration(_e)\n                    break\n    raise Return(_r)\n", "label": 1}
{"function": "\n\ndef __init__(self, disable_openmp):\n    cc = new_compiler()\n    customize_compiler(cc)\n    self.msvc = (cc.compiler_type == 'msvc')\n    self._print_compiler_version(cc)\n    if disable_openmp:\n        self.openmp_enabled = False\n    else:\n        (self.openmp_enabled, openmp_needs_gomp) = self._detect_openmp()\n    self.sse3_enabled = (self._detect_sse3() if (not self.msvc) else True)\n    self.sse41_enabled = (self._detect_sse41() if (not self.msvc) else True)\n    self.compiler_args_sse2 = (['-msse2'] if (not self.msvc) else ['/arch:SSE2'])\n    self.compiler_args_sse3 = (['-mssse3'] if (self.sse3_enabled and (not self.msvc)) else [])\n    (self.compiler_args_sse41, self.define_macros_sse41) = ([], [])\n    if self.sse41_enabled:\n        self.define_macros_sse41 = [('__SSE4__', 1), ('__SSE4_1__', 1)]\n        if (not self.msvc):\n            self.compiler_args_sse41 = ['-msse4']\n    if self.openmp_enabled:\n        self.compiler_libraries_openmp = []\n        if self.msvc:\n            self.compiler_args_openmp = ['/openmp']\n        else:\n            self.compiler_args_openmp = ['-fopenmp']\n            if openmp_needs_gomp:\n                self.compiler_libraries_openmp = ['gomp']\n    else:\n        self.compiler_libraries_openmp = []\n        self.compiler_args_openmp = []\n    if self.msvc:\n        self.compiler_args_opt = ['/O2']\n    else:\n        self.compiler_args_opt = ['-O3', '-funroll-loops']\n    print()\n", "label": 1}
{"function": "\n\ndef _caas_2_2_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_network_vlan_ALLFILTERS(self, method, url, body, headers):\n    (_, params) = url.split('?')\n    parameters = params.split('&')\n    for parameter in parameters:\n        (key, value) = parameter.split('=')\n        if (key == 'datacenterId'):\n            assert (value == 'fake_location')\n        elif (key == 'networkDomainId'):\n            assert (value == 'fake_network_domain')\n        elif (key == 'ipv6Address'):\n            assert (value == 'fake_ipv6')\n        elif (key == 'privateIpv4Address'):\n            assert (value == 'fake_ipv4')\n        elif (key == 'name'):\n            assert (value == 'fake_name')\n        elif (key == 'state'):\n            assert (value == 'fake_state')\n        else:\n            raise ValueError('Could not find in url parameters {0}:{1}'.format(key, value))\n    body = self.fixtures.load('network_vlan.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 1}
{"function": "\n\ndef resolve(self, resolvables, resolvable_set=None):\n    resolvables = [(resolvable, None) for resolvable in resolvables]\n    resolvable_set = (resolvable_set or _ResolvableSet())\n    processed_resolvables = set()\n    processed_packages = {\n        \n    }\n    distributions = {\n        \n    }\n    while resolvables:\n        while resolvables:\n            (resolvable, parent) = resolvables.pop(0)\n            if (resolvable in processed_resolvables):\n                continue\n            packages = self.package_iterator(resolvable, existing=resolvable_set.get(resolvable.name))\n            resolvable_set.merge(resolvable, packages, parent)\n            processed_resolvables.add(resolvable)\n        built_packages = {\n            \n        }\n        for (resolvable, packages, parent) in resolvable_set.packages():\n            assert (len(packages) > 0), ('ResolvableSet.packages(%s) should not be empty' % resolvable)\n            package = next(iter(packages))\n            if (resolvable.name in processed_packages):\n                if (package != processed_packages[resolvable.name]):\n                    raise self.Error(('Ambiguous resolvable: %s' % resolvable))\n                continue\n            if (package not in distributions):\n                dist = self.build(package, resolvable.options)\n                built_package = Package.from_href(dist.location)\n                built_packages[package] = built_package\n                distributions[built_package] = dist\n                package = built_package\n            distribution = distributions[package]\n            processed_packages[resolvable.name] = package\n            new_parent = (('%s->%s' % (parent, resolvable)) if parent else str(resolvable))\n            resolvables.extend(((ResolvableRequirement(req, resolvable.options), new_parent) for req in distribution.requires(extras=resolvable_set.extras(resolvable.name))))\n        resolvable_set = resolvable_set.replace_built(built_packages)\n    return list(distributions.values())\n", "label": 1}
{"function": "\n\ndef _complete_authz(self, user, areq, sid, **kwargs):\n    _log_debug = logger.debug\n    _log_debug('- in authenticated() -')\n    try:\n        permission = self.authz(user, client_id=areq['client_id'])\n        self.sdb.update(sid, 'permission', permission)\n    except Exception:\n        raise\n    _log_debug(('response type: %s' % areq['response_type']))\n    if self.sdb.is_revoked(sid):\n        return self._error(error='access_denied', descr='Token is revoked')\n    info = self.create_authn_response(areq, sid)\n    if isinstance(info, Response):\n        return info\n    else:\n        (aresp, fragment_enc) = info\n    try:\n        redirect_uri = self.get_redirect_uri(areq)\n    except (RedirectURIError, ParameterError) as err:\n        return BadRequest(('%s' % err))\n    info = self.aresp_check(aresp, areq)\n    if isinstance(info, Response):\n        return info\n    headers = []\n    try:\n        _kaka = kwargs['cookie']\n    except KeyError:\n        pass\n    else:\n        if (_kaka and (self.cookie_name not in _kaka)):\n            headers.append(self.cookie_func(user, typ='sso', ttl=self.sso_ttl))\n    if ('response_mode' in areq):\n        try:\n            resp = self.response_mode(areq, fragment_enc, aresp=aresp, redirect_uri=redirect_uri, headers=headers)\n        except InvalidRequest as err:\n            return self._error('invalid_request', err)\n        else:\n            if (resp is not None):\n                return resp\n    return (aresp, headers, redirect_uri, fragment_enc)\n", "label": 1}
{"function": "\n\ndef as_const(self, eval_ctx=None):\n    eval_ctx = get_eval_context(self, eval_ctx)\n    if eval_ctx.volatile:\n        raise Impossible()\n    obj = self.node.as_const(eval_ctx)\n    args = [x.as_const(eval_ctx) for x in self.args]\n    if isinstance(obj, _context_function_types):\n        if getattr(obj, 'contextfunction', False):\n            raise Impossible()\n        elif getattr(obj, 'evalcontextfunction', False):\n            args.insert(0, eval_ctx)\n        elif getattr(obj, 'environmentfunction', False):\n            args.insert(0, self.environment)\n    kwargs = dict((x.as_const(eval_ctx) for x in self.kwargs))\n    if (self.dyn_args is not None):\n        try:\n            args.extend(self.dyn_args.as_const(eval_ctx))\n        except Exception:\n            raise Impossible()\n    if (self.dyn_kwargs is not None):\n        try:\n            kwargs.update(self.dyn_kwargs.as_const(eval_ctx))\n        except Exception:\n            raise Impossible()\n    try:\n        return obj(*args, **kwargs)\n    except Exception:\n        raise Impossible()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _unbroadcast(grad_op_tree, x, be):\n    '\\n        Reverse broadcast from shape(grad_op_tree) to shape(x)\\n\\n        Arguments:\\n            grad_op_tree (OpTreeNode or Tensor): The OpTreeNode to broadcast.\\n            x (OpTreeNode or Tensor): Provides the dimension to be broadcasted to.\\n            be: (Backend): The backend to be used.\\n\\n        Returns:\\n            OpTreeNode or Tensor: The broadcasted result.\\n        '\n    if ((not grad_op_tree) or (not x)):\n        return grad_op_tree\n    if (type(x) in _scalar_types):\n        return 0.0\n    in_shape = x.shape\n    out_shape = grad_op_tree.shape\n    if (in_shape == out_shape):\n        return grad_op_tree\n    elif ((len(in_shape) == 2) and (len(out_shape) == 2)):\n        if (in_shape == (1, 1)):\n            return be.sum(grad_op_tree)\n        elif ((in_shape[0] == out_shape[0]) and (in_shape[1] == 1)):\n            return be.sum(grad_op_tree, axis=1)\n        elif ((in_shape[0] == 1) and (in_shape[1] == out_shape[1])):\n            return be.sum(grad_op_tree, axis=0)\n        elif (((out_shape[0] == in_shape[0]) and (out_shape[1] == 1)) or ((out_shape[0] == 1) and (out_shape[1] == in_shape[1]))):\n            return ((0 * x) + grad_op_tree)\n        else:\n            return NotImplemented\n    else:\n        return NotImplemented\n", "label": 1}
{"function": "\n\ndef test_user_specified_option_and_default_recovery():\n    from argparse import ArgumentParser\n    p = ArgumentParser()\n    p.add_argument('-t', '--test', action='store_true')\n    p.add_argument('--int', type=int, default=1)\n    p.add_argument('-i', '--input', help='The input')\n    p.add_argument('-o', '--output', nargs='*', help='The output', required=True)\n    opts = Options.from_argparse(p)\n    opts.parse([])\n    assert (not opts['test'].raw())\n    assert (opts['input'].raw() is None)\n    assert (opts['output'].raw() is None)\n    assert (not opts['output'].user_specified)\n    assert (not opts['input'].user_specified)\n    assert (not opts['test'].user_specified)\n    opts.parse(['-t', '-i', 'myin', '-o', 'myout'])\n    assert opts['test'].raw()\n    assert (opts['input'].raw() == 'myin')\n    assert (opts['output'].raw() == ['myout'])\n    assert opts['output'].user_specified\n    assert opts['input'].user_specified\n    assert opts['test'].user_specified\n", "label": 1}
{"function": "\n\ndef _match_url(self, request):\n    if (self._url is ANY):\n        return True\n    if hasattr(self._url, 'search'):\n        return (self._url.search(request.url) is not None)\n    if (self._url_parts.scheme and (request.scheme != self._url_parts.scheme)):\n        return False\n    if (self._url_parts.netloc and (request.netloc != self._url_parts.netloc)):\n        return False\n    if ((request.path or '/') != (self._url_parts.path or '/')):\n        return False\n    request_qs = urlparse.parse_qs(request.query)\n    matcher_qs = urlparse.parse_qs(self._url_parts.query)\n    for (k, vals) in six.iteritems(matcher_qs):\n        for v in vals:\n            try:\n                request_qs.get(k, []).remove(v)\n            except ValueError:\n                return False\n    if self._complete_qs:\n        for v in six.itervalues(request_qs):\n            if v:\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, fp, headers, params=None, parts=None):\n    self.processors = self.processors.copy()\n    self.fp = fp\n    self.headers = headers\n    if (params is None):\n        params = {\n            \n        }\n    self.params = params\n    if (parts is None):\n        parts = []\n    self.parts = parts\n    self.content_type = headers.elements('Content-Type')\n    if self.content_type:\n        self.content_type = self.content_type[0]\n    else:\n        self.content_type = httputil.HeaderElement.from_str(self.default_content_type)\n    dec = self.content_type.params.get('charset', None)\n    if dec:\n        self.attempt_charsets = ([dec] + [c for c in self.attempt_charsets if (c != dec)])\n    else:\n        self.attempt_charsets = self.attempt_charsets[:]\n    self.length = None\n    clen = headers.get('Content-Length', None)\n    if ((clen is not None) and ('chunked' not in headers.get('Transfer-Encoding', ''))):\n        try:\n            self.length = int(clen)\n        except ValueError:\n            pass\n    self.name = None\n    self.filename = None\n    disp = headers.elements('Content-Disposition')\n    if disp:\n        disp = disp[0]\n        if ('name' in disp.params):\n            self.name = disp.params['name']\n            if (self.name.startswith('\"') and self.name.endswith('\"')):\n                self.name = self.name[1:(- 1)]\n        if ('filename' in disp.params):\n            self.filename = disp.params['filename']\n            if (self.filename.startswith('\"') and self.filename.endswith('\"')):\n                self.filename = self.filename[1:(- 1)]\n", "label": 1}
{"function": "\n\ndef killJobs(self):\n    killList = list()\n    while True:\n        try:\n            jobId = self.killQueue.get(block=False)\n        except Empty:\n            break\n        else:\n            killList.append(jobId)\n    if (not killList):\n        return False\n    for jobID in list(killList):\n        if (jobID in self.runningJobs):\n            logger.debug('Killing job: %s', jobID)\n            subprocess.check_call(['qdel', self.getSgeID(jobID)])\n        else:\n            if (jobID in self.waitingJobs):\n                self.waitingJobs.remove(jobID)\n            self.killedJobsQueue.put(jobID)\n            killList.remove(jobID)\n    while killList:\n        for jobID in list(killList):\n            if (self.getJobExitCode(self.sgeJobIDs[jobID]) is not None):\n                logger.debug('Adding jobID %s to killedJobsQueue', jobID)\n                self.killedJobsQueue.put(jobID)\n                killList.remove(jobID)\n                self.forgetJob(jobID)\n        if (len(killList) > 0):\n            logger.warn(\"Some jobs weren't killed, trying again in %is.\", sleepSeconds)\n            time.sleep(sleepSeconds)\n    return True\n", "label": 1}
{"function": "\n\ndef resetSasl(self):\n    network_config = conf.supybot.networks.get(self.network)\n    self.sasl_authenticated = False\n    self.sasl_username = network_config.sasl.username()\n    self.sasl_password = network_config.sasl.password()\n    self.sasl_ecdsa_key = network_config.sasl.ecdsa_key()\n    self.authenticate_decoder = None\n    self.sasl_next_mechanisms = []\n    self.sasl_current_mechanism = None\n    for mechanism in network_config.sasl.mechanisms():\n        if ((mechanism == 'ecdsa-nist256p-challenge') and ecdsa and self.sasl_username and self.sasl_ecdsa_key):\n            self.sasl_next_mechanisms.append(mechanism)\n        elif ((mechanism == 'external') and (network_config.certfile() or conf.supybot.protocols.irc.certfile())):\n            self.sasl_next_mechanisms.append(mechanism)\n        elif ((mechanism == 'plain') and self.sasl_username and self.sasl_password):\n            self.sasl_next_mechanisms.append(mechanism)\n    if self.sasl_next_mechanisms:\n        self.REQUEST_CAPABILITIES.add('sasl')\n", "label": 1}
{"function": "\n\ndef _find_warnings(filename, source, ast_list):\n    count = 0\n    for node in ast_list:\n        if (isinstance(node, ast.Class) and node.body):\n            class_node = node\n            has_virtuals = False\n            for node in node.body:\n                if (isinstance(node, ast.Class) and node.body):\n                    _find_warnings(filename, source, [node])\n                elif (isinstance(node, ast.Function) and (node.modifiers & ast.FUNCTION_VIRTUAL)):\n                    has_virtuals = True\n                    if (node.modifiers & ast.FUNCTION_DTOR):\n                        break\n            else:\n                if (has_virtuals and (not class_node.bases)):\n                    lines = metrics.Metrics(source)\n                    print(('%s:%d' % (filename, lines.get_line_number(class_node.start))), end=' ')\n                    print(\"'{}' has virtual methods without a virtual dtor\".format(class_node.name))\n                    count += 1\n    return count\n", "label": 1}
{"function": "\n\ndef expand_to_paragraph(view, tp):\n    sr = view.full_line(tp)\n    if is_paragraph_separating_line(view, sr):\n        return sublime.Region(tp, tp)\n    required_prefix = None\n    (line_comments, block_comments) = comment.build_comment_data(view, tp)\n    dataStart = comment.advance_to_first_non_white_space_on_line(view, sr.begin())\n    for c in line_comments:\n        (start, disable_indent) = c\n        comment_region = sublime.Region(dataStart, (dataStart + len(start)))\n        if (view.substr(comment_region) == start):\n            required_prefix = view.substr(sublime.Region(sr.begin(), comment_region.end()))\n            break\n    first = sr.begin()\n    prev = sr\n    while True:\n        prev = previous_line(view, prev)\n        if ((prev == None) or is_paragraph_separating_line(view, prev) or (not has_prefix(view, prev, required_prefix))):\n            break\n        else:\n            first = prev.begin()\n    last = sr.end()\n    next = sr\n    while True:\n        next = next_line(view, next)\n        if ((next == None) or is_paragraph_separating_line(view, next) or (not has_prefix(view, next, required_prefix))):\n            break\n        else:\n            last = next.end()\n    return sublime.Region(first, last)\n", "label": 1}
{"function": "\n\ndef get(self, getme=None, fromEnd=False):\n    if (not getme):\n        return self\n    try:\n        getme = int(getme)\n        if (getme < 0):\n            return self[:((- 1) * getme)]\n        else:\n            return [self[(getme - 1)]]\n    except IndexError:\n        return []\n    except ValueError:\n        rangeResult = self.rangePattern.search(getme)\n        if rangeResult:\n            start = (rangeResult.group('start') or None)\n            end = (rangeResult.group('start') or None)\n            if start:\n                start = (int(start) - 1)\n            if end:\n                end = int(end)\n            return self[start:end]\n        getme = getme.strip()\n        if (getme.startswith('/') and getme.endswith('/')):\n            finder = re.compile(getme[1:(- 1)], ((re.DOTALL | re.MULTILINE) | re.IGNORECASE))\n\n            def isin(hi):\n                return finder.search(hi)\n        else:\n\n            def isin(hi):\n                return (getme.lower() in hi.lowercase)\n        return [itm for itm in self if isin(itm)]\n", "label": 1}
{"function": "\n\ndef _set_field_names(self, val):\n    val = [self._unicode(x) for x in val]\n    self._validate_option('field_names', val)\n    if self._field_names:\n        old_names = self._field_names[:]\n    self._field_names = val\n    if (self._align and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._align[new_name] = self._align[old_name]\n        for old_name in old_names:\n            if (old_name not in self._align):\n                self._align.pop(old_name)\n    else:\n        for field in self._field_names:\n            self._align[field] = 'c'\n    if (self._valign and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._valign[new_name] = self._valign[old_name]\n        for old_name in old_names:\n            if (old_name not in self._valign):\n                self._valign.pop(old_name)\n    else:\n        for field in self._field_names:\n            self._valign[field] = 't'\n", "label": 1}
{"function": "\n\ndef match_rating_comparison(s1, s2):\n    codex1 = match_rating_codex(s1)\n    codex2 = match_rating_codex(s2)\n    len1 = len(codex1)\n    len2 = len(codex2)\n    res1 = []\n    res2 = []\n    if (abs((len1 - len2)) >= 3):\n        return None\n    lensum = (len1 + len2)\n    if (lensum <= 4):\n        min_rating = 5\n    elif (lensum <= 7):\n        min_rating = 4\n    elif (lensum <= 11):\n        min_rating = 3\n    else:\n        min_rating = 2\n    for (c1, c2) in _zip_longest(codex1, codex2):\n        if (c1 != c2):\n            if c1:\n                res1.append(c1)\n            if c2:\n                res2.append(c2)\n    unmatched_count1 = unmatched_count2 = 0\n    for (c1, c2) in _zip_longest(reversed(res1), reversed(res2)):\n        if (c1 != c2):\n            if c1:\n                unmatched_count1 += 1\n            if c2:\n                unmatched_count2 += 1\n    return ((6 - max(unmatched_count1, unmatched_count2)) >= min_rating)\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('prefix', ('file', 'redis+socket'))\ndef test_socket_paths_explicit(prefix):\n    actual = parse_url((prefix + '://user:pass@redis.sock'))\n    assert (dict(password='pass', unix_socket_path='redis.sock') == actual)\n    actual = parse_url((prefix + '://user:pass@../redis.sock'))\n    assert (dict(password='pass', unix_socket_path='../redis.sock') == actual)\n    actual = parse_url((prefix + '://user:pass@./redis.sock'))\n    assert (dict(password='pass', unix_socket_path='./redis.sock') == actual)\n    actual = parse_url((prefix + '://redis.SOCK'))\n    assert (dict(unix_socket_path='redis.SOCK') == actual)\n    actual = parse_url((prefix + '://../redis.sock'))\n    assert (dict(unix_socket_path='../redis.sock') == actual)\n    actual = parse_url((prefix + '://./redis.SOCK'))\n    assert (dict(unix_socket_path='./redis.SOCK') == actual)\n    if (os.name != 'nt'):\n        actual = parse_url((prefix + '://user:pass@/tmp/redis.SOCK'))\n        assert (dict(password='pass', unix_socket_path='/tmp/redis.SOCK') == actual)\n        actual = parse_url((prefix + ':///tmp/redis.sock'))\n        assert (dict(unix_socket_path='/tmp/redis.sock') == actual)\n    else:\n        actual = parse_url((prefix + '://user:pass@C:\\\\Windows\\\\Temp\\\\redis.SOCK'))\n        assert (dict(password='pass', unix_socket_path='C:\\\\Windows\\\\Temp\\\\redis.SOCK') == actual)\n        actual = parse_url((prefix + '://C:\\\\Windows\\\\Temp\\\\redis.sock'))\n        assert (dict(unix_socket_path='C:\\\\Windows\\\\Temp\\\\redis.sock') == actual)\n", "label": 1}
{"function": "\n\ndef picture(self, image, duration=1.0, block=True, pos=None, hpr=None, scale=None, color=None, parent=None):\n    'Display a picture on the screen and keep it there for a particular duration.'\n    if ((pos is not None) and (type(pos) not in (int, float)) and (len(pos) == 2)):\n        pos = (pos[0], 0, pos[1])\n    if ((scale is not None) and (type(scale) not in (int, float)) and (len(scale) == 2)):\n        scale = (scale[0], 1, scale[1])\n    if ((hpr is not None) and (type(scale) not in (int, float)) and (len(hpr) == 1)):\n        hpr = (0, 0, hpr)\n    if (duration == 0):\n        block = False\n    obj = self._engine.direct.gui.OnscreenImage.OnscreenImage(image=image, pos=pos, hpr=hpr, scale=scale, color=color, parent=parent)\n    self._to_destroy.append(obj)\n    obj.setTransparency(self._engine.pandac.TransparencyAttrib.MAlpha)\n    if self.implicit_markers:\n        self.marker(248)\n    if block:\n        if ((type(duration) == list) or (type(duration) == tuple)):\n            self.sleep(duration[0])\n            self.waitfor(duration[1])\n        elif (type(duration) == str):\n            self.waitfor(duration)\n        else:\n            self.sleep(duration)\n        self._destroy_object(obj, 249)\n    else:\n        if (duration > 0):\n            self._engine.base.taskMgr.doMethodLater(duration, self._destroy_object, 'ConvenienceFunctions, remove_picture', extraArgs=[obj, 249])\n        return obj\n", "label": 1}
{"function": "\n\n@post\ndef _prep_testing_database(options, file_config):\n    from sqlalchemy.testing import config\n    from sqlalchemy import schema, inspect\n    if options.dropfirst:\n        for cfg in config.Config.all_configs():\n            e = cfg.db\n            inspector = inspect(e)\n            try:\n                view_names = inspector.get_view_names()\n            except NotImplementedError:\n                pass\n            else:\n                for vname in view_names:\n                    e.execute(schema._DropView(schema.Table(vname, schema.MetaData())))\n            if config.requirements.schemas.enabled_for_config(cfg):\n                try:\n                    view_names = inspector.get_view_names(schema='test_schema')\n                except NotImplementedError:\n                    pass\n                else:\n                    for vname in view_names:\n                        e.execute(schema._DropView(schema.Table(vname, schema.MetaData(), schema='test_schema')))\n            for tname in reversed(inspector.get_table_names(order_by='foreign_key')):\n                e.execute(schema.DropTable(schema.Table(tname, schema.MetaData())))\n            if config.requirements.schemas.enabled_for_config(cfg):\n                for tname in reversed(inspector.get_table_names(order_by='foreign_key', schema='test_schema')):\n                    e.execute(schema.DropTable(schema.Table(tname, schema.MetaData(), schema='test_schema')))\n", "label": 1}
{"function": "\n\ndef doit(self, **hints):\n    if hints.get('deep', True):\n        f = self.function.doit(**hints)\n    else:\n        f = self.function\n    if self.function.is_Matrix:\n        return self.expand().doit()\n    for (n, limit) in enumerate(self.limits):\n        (i, a, b) = limit\n        dif = (b - a)\n        if (dif.is_integer and ((dif < 0) == True)):\n            (a, b) = ((b + 1), (a - 1))\n            f = (- f)\n        if isinstance(i, Idx):\n            i = i.label\n        newf = eval_sum(f, (i, a, b))\n        if (newf is None):\n            if (f == self.function):\n                zeta_function = self.eval_zeta_function(f, (i, a, b))\n                if (zeta_function is not None):\n                    return zeta_function\n                return self\n            else:\n                return self.func(f, *self.limits[n:])\n        f = newf\n    if hints.get('deep', True):\n        if (not isinstance(f, Piecewise)):\n            return f.doit(**hints)\n    return f\n", "label": 1}
{"function": "\n\ndef _parse_network_settings(opts, current):\n    '\\n    Filters given options and outputs valid settings for\\n    the global network settings file.\\n    '\n    opts = dict(((k.lower(), v) for (k, v) in six.iteritems(opts)))\n    current = dict(((k.lower(), v) for (k, v) in six.iteritems(current)))\n    result = {\n        \n    }\n    valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n    if ('enabled' not in opts):\n        try:\n            opts['networking'] = current['networking']\n            _log_default_network('networking', current['networking'])\n        except ValueError:\n            _raise_error_network('networking', valid)\n    else:\n        opts['networking'] = opts['enabled']\n    if (opts['networking'] in valid):\n        if (opts['networking'] in _CONFIG_TRUE):\n            result['networking'] = 'yes'\n        elif (opts['networking'] in _CONFIG_FALSE):\n            result['networking'] = 'no'\n    else:\n        _raise_error_network('networking', valid)\n    if ('hostname' not in opts):\n        try:\n            opts['hostname'] = current['hostname']\n            _log_default_network('hostname', current['hostname'])\n        except ValueError:\n            _raise_error_network('hostname', ['server1.example.com'])\n    if opts['hostname']:\n        result['hostname'] = opts['hostname']\n    else:\n        _raise_error_network('hostname', ['server1.example.com'])\n    if ('search' in opts):\n        result['search'] = opts['search']\n    return result\n", "label": 1}
{"function": "\n\n@plumbing.route('/repos/<repo_key>/git/commits/')\n@corsify\n@jsonify\ndef get_commit_list(repo_key):\n    ref_name = (request.args.get('ref_name') or None)\n    start_sha = (request.args.get('start_sha') or None)\n    limit = (request.args.get('limit') or current_app.config['RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT'])\n    try:\n        limit = int(limit)\n    except ValueError:\n        raise BadRequest('invalid limit')\n    if (limit < 0):\n        raise BadRequest('invalid limit')\n    repo = get_repo(repo_key)\n    start_commit_id = None\n    if (start_sha is not None):\n        start_commit_id = start_sha\n    else:\n        if (ref_name is None):\n            ref_name = 'HEAD'\n        ref = lookup_ref(repo, ref_name)\n        if (ref is None):\n            raise NotFound('reference not found')\n        start_ref = lookup_ref(repo, ref_name)\n        try:\n            start_commit_id = start_ref.resolve().target\n        except KeyError:\n            if (ref_name == 'HEAD'):\n                return []\n            else:\n                raise NotFound('reference not found')\n    try:\n        walker = repo.walk(start_commit_id, GIT_SORT_TIME)\n    except ValueError:\n        raise BadRequest('invalid start_sha')\n    except KeyError:\n        raise NotFound('commit not found')\n    commits = [convert_commit(repo_key, commit) for commit in islice(walker, limit)]\n    return commits\n", "label": 1}
{"function": "\n\ndef animate_traj(traj, robot, pause=True, step_viewer=1, restore=True, callback=None, execute_step_cond=None):\n    'make sure to set active DOFs beforehand'\n    if restore:\n        _saver = openravepy.RobotStateSaver(robot)\n    if (step_viewer or pause):\n        viewer = trajoptpy.GetViewer(robot.GetEnv())\n    for (i, dofs) in enumerate(traj):\n        sys.stdout.write(('step %i/%i\\r' % ((i + 1), len(traj))))\n        sys.stdout.flush()\n        if (callback is not None):\n            callback(i)\n        if ((execute_step_cond is not None) and (not execute_step_cond(i))):\n            continue\n        robot.SetActiveDOFValues(dofs)\n        if pause:\n            viewer.Idle()\n        elif ((step_viewer != 0) and (((i % step_viewer) == 0) or (i == (len(traj) - 1)))):\n            viewer.Step()\n    sys.stdout.write('\\n')\n", "label": 1}
{"function": "\n\ndef marshal(self):\n    '\\n        Marshal this object into a raw message for subsequent serialization to bytes.\\n\\n        :returns: list -- The serialized raw message.\\n        '\n    options = {\n        \n    }\n    if (self.timeout is not None):\n        options['timeout'] = self.timeout\n    if (self.receive_progress is not None):\n        options['receive_progress'] = self.receive_progress\n    if (self.caller is not None):\n        options['caller'] = self.caller\n    if (self.caller_authid is not None):\n        options['caller_authid'] = self.caller_authid\n    if (self.caller_authrole is not None):\n        options['caller_authrole'] = self.caller_authrole\n    if (self.procedure is not None):\n        options['procedure'] = self.procedure\n    if self.payload:\n        if (self.enc_algo is not None):\n            options['enc_algo'] = self.enc_algo\n        if (self.enc_key is not None):\n            options['enc_key'] = self.enc_key\n        if (self.enc_serializer is not None):\n            options['enc_serializer'] = self.enc_serializer\n        return [Invocation.MESSAGE_TYPE, self.request, self.registration, options, self.payload]\n    elif self.kwargs:\n        return [Invocation.MESSAGE_TYPE, self.request, self.registration, options, self.args, self.kwargs]\n    elif self.args:\n        return [Invocation.MESSAGE_TYPE, self.request, self.registration, options, self.args]\n    else:\n        return [Invocation.MESSAGE_TYPE, self.request, self.registration, options]\n", "label": 1}
{"function": "\n\n@gof.local_optimizer([T.Elemwise])\ndef local_fill_cut(node):\n    '\\n    f(fill(a,b), c) -> f(b, c)\\n    If c.type == a.type.\\n    '\n    if (node.op != T.Elemwise):\n        return False\n    output = node.outputs[0]\n    try:\n        reference = [input for input in node.inputs if ((input.type == output.type) and ((not input.owner) or (input.owner.op != T.fill)))][0]\n    except IndexError:\n        return False\n    new_inputs = []\n    new = False\n    for input in node.inputs:\n        if (input.owner and (input.owner.op == T.fill)):\n            (model, filling) = input.owner.inputs\n            if encompasses_broadcastable(reference.type.broadcastable, filling.type.broadcastable):\n                new_inputs.append(filling)\n                new = True\n                continue\n        new_inputs.append(input)\n    if (not new):\n        return False\n    rval = node.op(*new_inputs)\n    if isinstance(rval, gof.Variable):\n        return rval.owner.outputs\n    else:\n        return rval[0].owner.outputs\n", "label": 1}
{"function": "\n\n@view_config(renderer='save.mak', route_name='save')\ndef save(request):\n    s = request.session\n    p = request.session['safe_params']\n    u = None\n    op = 'add'\n    vote_dict = {\n        \n    }\n    if (('story_id' in p) and ('logged_in' in s)):\n        dbsession = DBSession()\n        u = users.get_user_by_id(s['users.id'])\n        to_save = submission.get_story_by_id(p['story_id'])\n        if ('op' in p):\n            op = p['op']\n        if (op == 'add'):\n            if (to_save not in u.saved):\n                u.saved.append(to_save)\n                dbsession.add(u)\n            s['message'] = 'Successfully saved {0}'.format(to_save.title)\n        elif (op == 'del'):\n            if (to_save in u.saved):\n                u.saved.remove(to_save)\n                dbsession.add(u)\n            s['message'] = 'Successfully unsaved {0}'.format(to_save.title)\n    elif ('logged_in' in s):\n        u = users.get_user_by_id(s['users.id'])\n    if u:\n        vds = []\n        for i in u.saved:\n            vds.append(users.get_user_votes(s['users.id'], 'on_submission', i.id))\n        for vd in vds:\n            if (type(vd) == dict):\n                vote_dict.update(vd)\n    return {\n        'saved': u.saved,\n        'vote_dict': vote_dict,\n    }\n", "label": 1}
{"function": "\n\ndef filter_input_data(self, input_data, by_name=True):\n    'Filters the keys given in input_data checking against model fields\\n\\n        '\n    if isinstance(input_data, dict):\n        for (key, value) in input_data.items():\n            value = self.normalize(value)\n            if (value is None):\n                del input_data[key]\n        if by_name:\n            input_data = dict([[self.inverted_fields[key], value] for (key, value) in input_data.items() if ((key in self.inverted_fields) and ((self.objective_id is None) or (self.inverted_fields[key] != self.objective_id)))])\n        else:\n            input_data = dict([[key, value] for (key, value) in input_data.items() if ((key in self.fields) and ((self.objective_id is None) or (key != self.objective_id)))])\n        return input_data\n    else:\n        LOGGER.error('Failed to read input data in the expected {field:value} format.')\n        return {\n            \n        }\n", "label": 1}
{"function": "\n\ndef toposort(data):\n    '\\n    General-purpose topological sort function. Dependencies are expressed as a\\n    dictionary whose keys are items and whose values are a set of dependent\\n    items. Output is a list of sets in topological order. This is a generator\\n    function that returns a sequence of sets in topological order.\\n\\n    :param data: The dependency information.\\n    :type data: dict\\n    :returns: Yields a list of sorted sets representing the sorted order.\\n    '\n    if (not data):\n        return\n    for (k, v) in six.viewitems(data):\n        v.discard(k)\n    extra = (functools.reduce(set.union, six.viewvalues(data)) - set(six.viewkeys(data)))\n    data.update({item: set() for item in extra})\n    while True:\n        ordered = set((item for (item, dep) in six.viewitems(data) if (not dep)))\n        if (not ordered):\n            break\n        (yield ordered)\n        data = {item: (dep - ordered) for (item, dep) in six.viewitems(data) if (item not in ordered)}\n    if data:\n        raise Exception(('Cyclic dependencies detected:\\n%s' % '\\n'.join((repr(x) for x in six.viewitems(data)))))\n", "label": 1}
{"function": "\n\ndef _download_url(resp, link, temp_location):\n    fp = open(temp_location, 'wb')\n    download_hash = None\n    if (link.hash and link.hash_name):\n        try:\n            download_hash = hashlib.new(link.hash_name)\n        except ValueError:\n            logger.warn(('Unsupported hash name %s for package %s' % (link.hash_name, link)))\n    try:\n        total_length = int(resp.info()['content-length'])\n    except (ValueError, KeyError, TypeError):\n        total_length = 0\n    downloaded = 0\n    show_progress = ((total_length > (40 * 1000)) or (not total_length))\n    show_url = link.show_url\n    try:\n        if show_progress:\n            if total_length:\n                logger.start_progress(('Downloading %s (%s): ' % (show_url, format_size(total_length))))\n            else:\n                logger.start_progress(('Downloading %s (unknown size): ' % show_url))\n        else:\n            logger.notify(('Downloading %s' % show_url))\n        logger.info(('Downloading from URL %s' % link))\n        while True:\n            chunk = resp.read(4096)\n            if (not chunk):\n                break\n            downloaded += len(chunk)\n            if show_progress:\n                if (not total_length):\n                    logger.show_progress(('%s' % format_size(downloaded)))\n                else:\n                    logger.show_progress(('%3i%%  %s' % (((100 * downloaded) / total_length), format_size(downloaded))))\n            if (download_hash is not None):\n                download_hash.update(chunk)\n            fp.write(chunk)\n        fp.close()\n    finally:\n        if show_progress:\n            logger.end_progress(('%s downloaded' % format_size(downloaded)))\n    return download_hash\n", "label": 1}
{"function": "\n\ndef __init__(self, obj=None, default=None, white_list=None, white_pattern=None, black_list=None, ignore_help=False, ignore_return=False, name=None, doc=None, debug=False):\n    obj = (obj or sys.modules['__main__'])\n    self.obj = obj\n    if hasattr(obj, 'items'):\n        obj_items = obj.items()\n    else:\n        obj_items = inspect.getmembers(obj)\n    if ((not white_list) and hasattr(obj, '__all__')):\n        white_list = obj.__all__\n    tests = (inspect.isbuiltin, inspect.isfunction, inspect.ismethod)\n    self.command_funcs = {\n        \n    }\n    for (obj_name, obj) in obj_items:\n        if obj_name.startswith('_'):\n            continue\n        if (not any((test(obj) for test in tests))):\n            continue\n        if ((white_list is not None) and (obj_name not in white_list)):\n            continue\n        if ((black_list is not None) and (obj_name in black_list)):\n            continue\n        if white_pattern:\n            match = white_pattern.match(obj_name)\n            if (not match):\n                continue\n            obj_name = match.group('name')\n        self.command_funcs[obj_name] = obj\n    self.default = default\n    if (len(self.command_funcs) == 1):\n        self.default = list(self.command_funcs.keys())[0]\n    self.ignore_help = ignore_help\n    self.ignore_return = ignore_return\n    self.name = (name or basename(sys.argv[0]))\n    self.doc = doc\n    self.debug = debug\n", "label": 1}
{"function": "\n\ndef __setitem__(self, key, value):\n    if self._set_slice(key, value, self):\n        return\n    if isinstance(value, tuple):\n        if (not (0 < len(value) <= 2)):\n            raise ValueError('A Header item may be set with either a scalar value, a 1-tuple containing a scalar value, or a 2-tuple containing a scalar value and comment string.')\n        if (len(value) == 1):\n            (value, comment) = (value[0], None)\n            if (value is None):\n                value = ''\n        elif (len(value) == 2):\n            (value, comment) = value\n            if (value is None):\n                value = ''\n            if (comment is None):\n                comment = ''\n    else:\n        comment = None\n    card = None\n    if isinstance(key, int):\n        card = self._cards[key]\n    elif isinstance(key, tuple):\n        card = self._cards[self._cardindex(key)]\n    if card:\n        card.value = value\n        if (comment is not None):\n            card.comment = comment\n        if card._modified:\n            self._modified = True\n    else:\n        self._update((key, value, comment))\n", "label": 1}
{"function": "\n\ndef _get_filesystems_and_globs(datetime_to_task, datetime_to_re):\n    '\\n    Yields a (filesystem, glob) tuple per every output location of task.\\n\\n    The task can have one or several FileSystemTarget outputs.\\n\\n    For convenience, the task can be a luigi.WrapperTask,\\n    in which case outputs of all its dependencies are considered.\\n    '\n    sample_datetimes = [datetime(y, m, d, h) for y in range(2000, 2050, 10) for m in range(1, 4) for d in range(5, 8) for h in range(21, 24)]\n    regexes = [re.compile(datetime_to_re(d)) for d in sample_datetimes]\n    sample_tasks = [datetime_to_task(d) for d in sample_datetimes]\n    sample_outputs = [flatten_output(t) for t in sample_tasks]\n    for (o, t) in zip(sample_outputs, sample_tasks):\n        if (len(o) != len(sample_outputs[0])):\n            raise NotImplementedError(('Outputs must be consistent over time, sorry; was %r for %r and %r for %r' % (o, t, sample_outputs[0], sample_tasks[0])))\n        for target in o:\n            if (not isinstance(target, FileSystemTarget)):\n                raise NotImplementedError(('Output targets must be instances of FileSystemTarget; was %r for %r' % (target, t)))\n    for o in zip(*sample_outputs):\n        glob = _get_per_location_glob(sample_tasks, o, regexes)\n        (yield (o[0].fs, glob))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.LIST):\n                self.columns = []\n                (_etype3, _size0) = iprot.readListBegin()\n                for _i4 in xrange(_size0):\n                    _elem5 = Column()\n                    _elem5.read(iprot)\n                    self.columns.append(_elem5)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef log(self, level, msg, *args, **kw):\n    if args:\n        if kw:\n            raise TypeError('You may give positional or keyword arguments, not both')\n    args = (args or kw)\n    rendered = None\n    for (consumer_level, consumer) in self.consumers:\n        if self.level_matches(level, consumer_level):\n            if (self.in_progress_hanging and (consumer in (sys.stdout, sys.stderr))):\n                self.in_progress_hanging = False\n                sys.stdout.write('\\n')\n                sys.stdout.flush()\n            if (rendered is None):\n                if args:\n                    rendered = (msg % args)\n                else:\n                    rendered = msg\n                rendered = ((' ' * self.indent) + rendered)\n                if self.explicit_levels:\n                    rendered = ('%02i %s' % (level, rendered))\n            if hasattr(consumer, 'write'):\n                consumer.write((rendered + '\\n'))\n            else:\n                consumer(rendered)\n", "label": 1}
{"function": "\n\ndef writeBuffer(buff, col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall):\n    for x in buff:\n        printPreceedingCharacter = False\n        altOptions = x[col['alt']].split(',')\n        for y in altOptions:\n            if (((len(x[col['ref']]) != len(y)) or (len(x[col['ref']]) == 0) or (len(y) == 0)) and (not re.search('[^ATGCNatgcn\\\\.-]', y))):\n                printPreceedingCharacter = True\n                break\n        if printPreceedingCharacter:\n            writeRowCheck(x[:], col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall)\n    for x in buff:\n        printPreceedingCharacter = False\n        altOptions = x[col['alt']].split(',')\n        for y in altOptions:\n            if (((len(x[col['ref']]) != len(y)) or (len(x[col['ref']]) == 0) or (len(y) == 0)) and (not re.search('[^ATGCNatgcn\\\\.-]', y))):\n                printPreceedingCharacter = True\n                break\n        if (printPreceedingCharacter == False):\n            writeRowCheck(x[:], col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall)\n", "label": 1}
{"function": "\n\ndef reset_settings(self):\n    for sgroup in self.settings['setting_groups']:\n        for setting in sgroup.values():\n            widget = self.find_child_by_name(setting.name)\n            if (widget is None):\n                continue\n            if ((setting.type == 'string') or (setting.type == 'file') or (setting.type == 'folder')):\n                old_val = ''\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val.replace('\\\\', '\\\\\\\\')\n                widget.setText(old_val)\n            elif (setting.type == 'strings'):\n                old_val = []\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = [v.replace('\\\\', '\\\\\\\\') for v in old_val]\n                widget.setText(','.join(setting.value))\n            elif (setting.type == 'check'):\n                old_val = False\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val\n                widget.setChecked(old_val)\n            elif (setting.type == 'range'):\n                old_val = 0\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val\n                widget.setValue(old_val)\n", "label": 1}
{"function": "\n\ndef test_write_job():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'write': ['test.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == False)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.status = sentry_common_service.ttypes.TSentryResponseStatus()\n                self.status.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.SET):\n                self.roles = set()\n                (_etype31, _size28) = iprot.readSetBegin()\n                for _i32 in xrange(_size28):\n                    _elem33 = TSentryRole()\n                    _elem33.read(iprot)\n                    self.roles.add(_elem33)\n                iprot.readSetEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef check_permissions(self):\n    '\\n        Checks that all permissions are set correctly for the users.\\n\\n        :return: A set of users whose permissions was wrong.\\n\\n        '\n    changed_permissions = []\n    changed_users = []\n    warnings = []\n    for (model, perms) in ASSIGNED_PERMISSIONS.items():\n        if (model == 'profile'):\n            model_obj = get_profile_model()\n        else:\n            model_obj = get_user_model()\n        model_content_type = ContentType.objects.get_for_model(model_obj)\n        for perm in perms:\n            try:\n                Permission.objects.get(codename=perm[0], content_type=model_content_type)\n            except Permission.DoesNotExist:\n                changed_permissions.append(perm[1])\n                Permission.objects.create(name=perm[1], codename=perm[0], content_type=model_content_type)\n    for user in get_user_model().objects.exclude(id=settings.ANONYMOUS_USER_ID):\n        try:\n            user_profile = get_user_profile(user=user)\n        except ObjectDoesNotExist:\n            warnings.append((_('No profile found for %(username)s') % {\n                'username': user.username,\n            }))\n        else:\n            all_permissions = (get_perms(user, user_profile) + get_perms(user, user))\n            for (model, perms) in ASSIGNED_PERMISSIONS.items():\n                if (model == 'profile'):\n                    perm_object = get_user_profile(user=user)\n                else:\n                    perm_object = user\n                for perm in perms:\n                    if (perm[0] not in all_permissions):\n                        assign_perm(perm[0], user, perm_object)\n                        changed_users.append(user)\n    return (changed_permissions, changed_users, warnings)\n", "label": 1}
{"function": "\n\ndef sets_cv(self, x, y):\n    totals = ([0] * len(x.columns))\n    if self.min:\n        totals = ([1000] * len(x.columns))\n    i = 0\n    for (train, test) in cross_validation.KFold(n=len(y), k=4):\n        i += 1\n        logging.info('RF selector computing importances for fold {i}'.format(i=i))\n        cls = ensemble.RandomForestRegressor\n        if self.classifier:\n            cls = ensemble.RandomForestClassifier\n        rf = cls(n_estimators=self.n, random_state=self.seed, n_jobs=(- 1))\n        rf.fit(x.values[train], y.values[train])\n        importances = rf.feature_importances_\n        if self.min:\n            totals = [min(imp, t) for (imp, t) in zip(importances, totals)]\n        else:\n            totals = [(imp + t) for (imp, t) in zip(importances, totals)]\n    imps = sorted(zip(totals, x.columns), reverse=True)\n    for (i, x) in enumerate(imps):\n        (imp, f) = x\n        logging.debug(('%d\\t%0.4f\\t%s' % (i, imp, f)))\n    if self.thresh:\n        imps = [t for t in imps if (t[0] > self.thresh)]\n    sets = [[t[1] for t in imps[:(i + 1)]] for i in range(len(imps))]\n    return sets\n", "label": 1}
{"function": "\n\ndef make_uri(base, *args, **kwargs):\n    'Assemble a uri based on a base, any number of path segments,\\n    and query string parameters.\\n\\n    '\n    charset = kwargs.pop('charset', 'utf-8')\n    safe = kwargs.pop('safe', '/:')\n    encode_keys = kwargs.pop('encode_keys', True)\n    base_trailing_slash = False\n    if (base and base.endswith('/')):\n        base_trailing_slash = True\n        base = base[:(- 1)]\n    retval = [base]\n    _path = []\n    trailing_slash = False\n    for s in args:\n        if ((s is not None) and isinstance(s, six.string_types)):\n            if ((len(s) > 1) and s.endswith('/')):\n                trailing_slash = True\n            else:\n                trailing_slash = False\n            _path.append(url_quote(s.strip('/'), charset, safe))\n    path_str = ''\n    if _path:\n        path_str = '/'.join(([''] + _path))\n        if trailing_slash:\n            path_str = (path_str + '/')\n    elif base_trailing_slash:\n        path_str = (path_str + '/')\n    if path_str:\n        retval.append(path_str)\n    params_str = url_encode(kwargs, charset, encode_keys)\n    if params_str:\n        retval.extend(['?', params_str])\n    return ''.join(retval)\n", "label": 1}
{"function": "\n\ndef check_date(self, match):\n    '\\n        Method for resolving date restrictions on actor codes. \\n        \\n        Parameters\\n        -----------\\n        match: list\\n               Dates and codes from the dictionary\\n        \\n        Returns\\n        -------\\n        code: string\\n              The code corresponding to how the actor should be coded given the date\\n        '\n    code = None\n    try:\n        for j in match:\n            dates = j[1]\n            date = []\n            code = ''\n            for d in dates:\n                if (d[0] in '<>'):\n                    date.append((d[0] + str(PETRreader.dstr_to_ordate(d[1:]))))\n                else:\n                    date.append(str(PETRreader.dstr_to_ordate(d)))\n            curdate = self.date\n            if (not date):\n                code = j[0]\n            elif (len(date) == 1):\n                if (date[0][0] == '<'):\n                    if (curdate < int(date[0][1:])):\n                        code = j[0]\n                elif (curdate >= int(date[0][1:])):\n                    code = j[0]\n            elif (curdate < int(date[1])):\n                if (curdate >= int(date[0])):\n                    code = j[0]\n            if code:\n                return code\n    except Exception as e:\n        return code\n    return code\n", "label": 1}
{"function": "\n\ndef log_action(self, protocol, action, details):\n    '\\n        Logs various different kinds of requests to the console.\\n        '\n    msg = ('[%s] ' % datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S'))\n    if ((protocol == 'http') and (action == 'complete')):\n        msg += ('HTTP %(method)s %(path)s %(status)s [%(time_taken).2f, %(client)s]\\n' % details)\n        if (200 <= details['status'] < 300):\n            msg = self.style.HTTP_SUCCESS(msg)\n        elif (100 <= details['status'] < 200):\n            msg = self.style.HTTP_INFO(msg)\n        elif (details['status'] == 304):\n            msg = self.style.HTTP_NOT_MODIFIED(msg)\n        elif (300 <= details['status'] < 400):\n            msg = self.style.HTTP_REDIRECT(msg)\n        elif (details['status'] == 404):\n            msg = self.style.HTTP_NOT_FOUND(msg)\n        elif (400 <= details['status'] < 500):\n            msg = self.style.HTTP_BAD_REQUEST(msg)\n        else:\n            msg = self.style.HTTP_SERVER_ERROR(msg)\n    elif ((protocol == 'websocket') and (action == 'connected')):\n        msg += ('WebSocket CONNECT %(path)s [%(client)s]\\n' % details)\n    elif ((protocol == 'websocket') and (action == 'disconnected')):\n        msg += ('WebSocket DISCONNECT %(path)s [%(client)s]\\n' % details)\n    sys.stderr.write(msg)\n", "label": 1}
{"function": "\n\ndef test_islice():\n    sl = SortedList(load=7)\n    assert ([] == list(sl.islice()))\n    values = list(range(53))\n    sl.update(values)\n    for start in range(53):\n        for stop in range(53):\n            assert (list(sl.islice(start, stop)) == values[start:stop])\n    for start in range(53):\n        for stop in range(53):\n            assert (list(sl.islice(start, stop, reverse=True)) == values[start:stop][::(- 1)])\n    for start in range(53):\n        assert (list(sl.islice(start=start)) == values[start:])\n        assert (list(sl.islice(start=start, reverse=True)) == values[start:][::(- 1)])\n    for stop in range(53):\n        assert (list(sl.islice(stop=stop)) == values[:stop])\n        assert (list(sl.islice(stop=stop, reverse=True)) == values[:stop][::(- 1)])\n", "label": 1}
{"function": "\n\ndef loop(self):\n    'Continually check the queue for new items and process them.'\n    last_runs = {\n        \n    }\n    while True:\n        try:\n            try:\n                item = self.q.get(block=True, timeout=self.MIN_DELAY)\n            except Empty:\n                for (view_id, (timestamp, delay)) in last_runs.copy().items():\n                    if (time.monotonic() > (timestamp + delay)):\n                        self.last_runs[view_id] = time.monotonic()\n                        del last_runs[view_id]\n                        self.lint(view_id, timestamp)\n                continue\n            if isinstance(item, tuple):\n                (view_id, timestamp, delay) = item\n                if ((view_id in self.last_runs) and (timestamp < self.last_runs[view_id])):\n                    continue\n                last_runs[view_id] = (timestamp, delay)\n            elif isinstance(item, (int, float)):\n                time.sleep(item)\n            elif isinstance(item, str):\n                if (item == 'reload'):\n                    persist.printf('daemon detected a reload')\n                    self.last_runs.clear()\n                    last_runs.clear()\n            else:\n                persist.printf('unknown message sent to daemon:', item)\n        except:\n            persist.printf('error in SublimeLinter daemon:')\n            persist.printf(('-' * 20))\n            persist.printf(traceback.format_exc())\n            persist.printf(('-' * 20))\n", "label": 1}
{"function": "\n\ndef test_concat_l1_l1(backend_default, allrand_args):\n    dtypeu = np.float32\n    (w_rng, rngmax) = allrand_args\n    nins = [128, 1024]\n    nouts = [64, 2048]\n    batch_size = 16\n    NervanaObject.be.bsz = batch_size\n    be = NervanaObject.be\n    init_unif = Uniform(low=w_rng[0], high=w_rng[1])\n    layers = [Sequential(Affine(nout=nout, init=init_unif)) for nout in nouts]\n    inputs = [be.array(dtypeu(np.random.random((nin, batch_size)))) for nin in nins]\n    merge = MergeMultistream(layers, merge='stack')\n    assert (len(inputs) == len(layers))\n    merge.configure(inputs)\n    merge.allocate()\n    merge.set_deltas(None)\n    out = merge.fprop(inputs).get()\n    sublayers = [s.layers[0] for s in layers]\n    weights = [layer.W.get() for layer in sublayers]\n    out_exp = np.concatenate([np.dot(w, inp.get()) for (w, inp) in zip(weights, inputs)])\n    assert np.allclose(out, out_exp, atol=0.001)\n    err_lst = [dtypeu(np.random.random((nout, batch_size))) for nout in nouts]\n    err_concat = np.concatenate(err_lst)\n    merge.bprop(be.array(err_concat))\n    dW_exp_lst = [np.dot(err, inp.get().T) for (err, inp) in zip(err_lst, inputs)]\n    for (layer, dW_exp) in zip(sublayers, dW_exp_lst):\n        assert np.allclose(layer.dW.get(), dW_exp)\n    return\n", "label": 1}
{"function": "\n\n@gof.local_optimizer(None)\ndef constant_folding(node):\n    for input in node.inputs:\n        if (not isinstance(input, Constant)):\n            return False\n    if (not node.op.do_constant_folding(node)):\n        return False\n    storage_map = dict([(i, [i.data]) for i in node.inputs])\n    compute_map = dict([(i, [True]) for i in node.inputs])\n    for o in node.outputs:\n        storage_map[o] = [None]\n        compute_map[o] = [False]\n    if (hasattr(node.op, 'python_constant_folding') and node.op.python_constant_folding(node)):\n        old_value = getattr(node.op, '_op_use_c_code', False)\n        try:\n            node.op._op_use_c_code = False\n            thunk = node.op.make_thunk(node, storage_map, compute_map, [])\n        finally:\n            node.op._op_use_c_code = old_value\n    else:\n        thunk = node.op.make_thunk(node, storage_map, compute_map, no_recycling=[])\n    required = thunk()\n    assert (not required)\n    rval = []\n    for output in node.outputs:\n        assert compute_map[output][0], (output, storage_map[output][0])\n        try:\n            constant = output.type.Constant\n        except AttributeError:\n            constant = Constant\n        v = constant(output.type, storage_map[output][0])\n        copy_stack_trace(output, v)\n        rval.append(v)\n    return rval\n", "label": 1}
{"function": "\n\ndef __init__(self, name, level, pathname, lineno, msg, args, exc_info, func=None, sinfo=None, **kwargs):\n    '\\n        Initialize a logging record with interesting information.\\n        '\n    ct = time.time()\n    self.name = name\n    self.msg = msg\n    if (args and (len(args) == 1) and isinstance(args[0], dict) and args[0]):\n        args = args[0]\n    self.args = args\n    self.levelname = getLevelName(level)\n    self.levelno = level\n    self.pathname = pathname\n    try:\n        self.filename = os.path.basename(pathname)\n        self.module = os.path.splitext(self.filename)[0]\n    except (TypeError, ValueError, AttributeError):\n        self.filename = pathname\n        self.module = 'Unknown module'\n    self.exc_info = exc_info\n    self.exc_text = None\n    self.stack_info = sinfo\n    self.lineno = lineno\n    self.funcName = func\n    self.created = ct\n    self.msecs = ((ct - int(ct)) * 1000)\n    self.relativeCreated = ((self.created - _startTime) * 1000)\n    if (logThreads and threading):\n        self.thread = threading.get_ident()\n        self.threadName = threading.current_thread().name\n    else:\n        self.thread = None\n        self.threadName = None\n    if (not logMultiprocessing):\n        self.processName = None\n    else:\n        self.processName = 'MainProcess'\n        mp = sys.modules.get('multiprocessing')\n        if (mp is not None):\n            try:\n                self.processName = mp.current_process().name\n            except Exception:\n                pass\n    if (logProcesses and hasattr(os, 'getpid')):\n        self.process = os.getpid()\n    else:\n        self.process = None\n", "label": 1}
{"function": "\n\ndef __doStemming(self, word, intact_word):\n    'Perform the actual word stemming\\n        '\n    valid_rule = re.compile('^([a-z]+)(\\\\*?)(\\\\d)([a-z]*)([>\\\\.]?)$')\n    proceed = True\n    while proceed:\n        last_letter_position = self.__getLastLetter(word)\n        if ((last_letter_position < 0) or (word[last_letter_position] not in self.rule_dictionary)):\n            proceed = False\n        else:\n            rule_was_applied = False\n            for rule in self.rule_dictionary[word[last_letter_position]]:\n                rule_match = valid_rule.match(rule)\n                if rule_match:\n                    (ending_string, intact_flag, remove_total, append_string, cont_flag) = rule_match.groups()\n                    remove_total = int(remove_total)\n                    if word.endswith(ending_string[::(- 1)]):\n                        if intact_flag:\n                            if ((word == intact_word) and self.__isAcceptable(word, remove_total)):\n                                word = self.__applyRule(word, remove_total, append_string)\n                                rule_was_applied = True\n                                if (cont_flag == '.'):\n                                    proceed = False\n                                break\n                        elif self.__isAcceptable(word, remove_total):\n                            word = self.__applyRule(word, remove_total, append_string)\n                            rule_was_applied = True\n                            if (cont_flag == '.'):\n                                proceed = False\n                            break\n            if (rule_was_applied == False):\n                proceed = False\n    return word\n", "label": 1}
{"function": "\n\ndef _result__repr__(self):\n    '\\n    This is used as the `__repr__` function for the :class:`Result`\\n    '\n    details = []\n    flags = self.__class__._fldprops\n    rcstr = 'rc=0x{0:X}'.format(self.rc)\n    if (self.rc != 0):\n        rcstr += '[{0}]'.format(self.errstr)\n    details.append(rcstr)\n    if ((flags & C.PYCBC_RESFLD_KEY) and hasattr(self, 'key')):\n        details.append('key={0}'.format(repr(self.key)))\n    if ((flags & C.PYCBC_RESFLD_VALUE) and hasattr(self, 'value')):\n        details.append('value={0}'.format(repr(self.value)))\n    if ((flags & C.PYCBC_RESFLD_CAS) and hasattr(self, 'cas')):\n        details.append('cas=0x{cas:x}'.format(cas=self.cas))\n    if ((flags & C.PYCBC_RESFLD_CAS) and hasattr(self, 'flags')):\n        details.append('flags=0x{flags:x}'.format(flags=self.flags))\n    if ((flags & C.PYCBC_RESFLD_HTCODE) and hasattr(self, 'http_status')):\n        details.append('http_status={0}'.format(self.http_status))\n    if ((flags & C.PYCBC_RESFLD_URL) and hasattr(self, 'url')):\n        details.append('url={0}'.format(self.url))\n    if hasattr(self, '_pycbc_repr_extra'):\n        details += self._pycbc_repr_extra()\n    ret = '{0}<{1}>'.format(self.__class__.__name__, ', '.join(details))\n    return ret\n", "label": 1}
{"function": "\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    'Setup the ISY994 platform.'\n    logger = logging.getLogger(__name__)\n    devs = []\n    if ((ISY is None) or (not ISY.connected)):\n        logger.error('A connection has not been made to the ISY controller.')\n        return False\n    for (path, node) in ISY.nodes:\n        if ((not node.dimmable) and (SENSOR_STRING not in node.name)):\n            if (HIDDEN_STRING in path):\n                node.name += HIDDEN_STRING\n            devs.append(ISYSwitchDevice(node))\n    for (folder_name, states) in (('HA.doors', [STATE_ON, STATE_OFF]), ('HA.switches', [STATE_ON, STATE_OFF])):\n        try:\n            folder = ISY.programs['My Programs'][folder_name]\n        except KeyError:\n            pass\n        else:\n            for (dtype, name, node_id) in folder.children:\n                if (dtype is 'folder'):\n                    custom_switch = folder[node_id]\n                    try:\n                        actions = custom_switch['actions'].leaf\n                        assert (actions.dtype == 'program'), 'Not a program'\n                        node = custom_switch['status'].leaf\n                    except (KeyError, AssertionError):\n                        pass\n                    else:\n                        devs.append(ISYProgramDevice(name, node, actions, states))\n    add_devices(devs)\n", "label": 1}
{"function": "\n\ndef _get_attrs(client_manager, parsed_args):\n    attrs = {\n        \n    }\n    if (parsed_args.name is not None):\n        attrs['name'] = str(parsed_args.name)\n    if parsed_args.enable:\n        attrs['admin_state_up'] = True\n    if parsed_args.disable:\n        attrs['admin_state_up'] = False\n    if parsed_args.share:\n        attrs['shared'] = True\n    if parsed_args.no_share:\n        attrs['shared'] = False\n    if (('project' in parsed_args) and (parsed_args.project is not None)):\n        identity_client = client_manager.identity\n        project_id = identity_common.find_project(identity_client, parsed_args.project, parsed_args.project_domain).id\n        attrs['tenant_id'] = project_id\n    if (('availability_zone_hints' in parsed_args) and (parsed_args.availability_zone_hints is not None)):\n        attrs['availability_zone_hints'] = parsed_args.availability_zone_hints\n    if parsed_args.internal:\n        attrs['router:external'] = False\n    if parsed_args.external:\n        attrs['router:external'] = True\n        if parsed_args.no_default:\n            attrs['is_default'] = False\n        if parsed_args.default:\n            attrs['is_default'] = True\n    if parsed_args.provider_network_type:\n        attrs['provider:network_type'] = parsed_args.provider_network_type\n    if parsed_args.physical_network:\n        attrs['provider:physical_network'] = parsed_args.physical_network\n    if parsed_args.segmentation_id:\n        attrs['provider:segmentation_id'] = parsed_args.segmentation_id\n    return attrs\n", "label": 1}
{"function": "\n\ndef validate_kwargs(func, kwargs):\n    'Validate arguments to be supplied to func.'\n    func_name = func.__name__\n    argspec = inspect.getargspec(func)\n    all_args = argspec.args[:]\n    defaults = list((argspec.defaults or []))\n    if (inspect.ismethod(func) and (all_args[:1] == ['self'])):\n        all_args[:1] = []\n    if defaults:\n        required = all_args[:(- len(defaults))]\n    else:\n        required = all_args[:]\n    trans = {arg: ((arg.endswith('_') and arg[:(- 1)]) or arg) for arg in all_args}\n    for key in list(kwargs):\n        key_adj = ('%s_' % key)\n        if (key_adj in all_args):\n            kwargs[key_adj] = kwargs.pop(key)\n    supplied = sorted(kwargs)\n    missing = [trans.get(arg, arg) for arg in required if (arg not in supplied)]\n    if missing:\n        raise MeteorError(400, func.err, ('Missing required arguments to %s: %s' % (func_name, ' '.join(missing))))\n    extra = [arg for arg in supplied if (arg not in all_args)]\n    if extra:\n        raise MeteorError(400, func.err, ('Unknown arguments to %s: %s' % (func_name, ' '.join(extra))))\n", "label": 1}
{"function": "\n\ndef testSubstitute1(self):\n    config = Configuration()\n    config.readfp(StringIO(CONFIG1))\n    assert config.has_section('section1')\n    assert (not config.has_section('section2'))\n    assert (not config.has_section('foo'))\n    assert (not config.has_section('bar'))\n    assert (not config.has_section('bar2'))\n    assert config.has_option('section1', 'foo')\n    assert config.has_option('section1', 'name')\n    assert (config.get('section1', 'name') == os.path.basename(sys.argv[0]))\n    assert (config.get('section1', 'cwd') == os.getcwd())\n    assert config.has_option('section1', 'bar')\n    assert config.has_option('section1', 'bar2')\n    assert (config.get('section1', 'foo') == 'bar')\n    assert (config.get('section1', 'bar') == 'bar')\n    assert (config.get('section1', 'bar2') == 'bar')\n", "label": 1}
{"function": "\n\ndef _refresh(self):\n    ' Refresh the enabled/visible state of the action set. '\n    window = self.window\n    if (len(self.enabled_for_perspectives) > 0):\n        self.enabled = ((window is not None) and (window.active_perspective is not None) and (window.active_perspective.id in self.enabled_for_perspectives))\n    if (len(self.visible_for_perspectives) > 0):\n        self.visible = ((window is not None) and (window.active_perspective is not None) and (window.active_perspective.id in self.visible_for_perspectives))\n    if (len(self.enabled_for_views) > 0):\n        self.enabled = ((window is not None) and (window.active_part is not None) and (window.active_part.id in self.enabled_for_views))\n    if (len(self.visible_for_views) > 0):\n        self.visible = ((window is not None) and (window.active_part is not None) and (window.active_part.id in self.visible_for_views))\n    return\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.value = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.description = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __init__(self, html4tags=False, tab_width=4, safe_mode=None, extras=None, link_patterns=None, use_file_vars=False):\n    if html4tags:\n        self.empty_element_suffix = '>'\n    else:\n        self.empty_element_suffix = ' />'\n    self.tab_width = tab_width\n    if (safe_mode is True):\n        self.safe_mode = 'replace'\n    else:\n        self.safe_mode = safe_mode\n    if (self.extras is None):\n        self.extras = {\n            \n        }\n    elif (not isinstance(self.extras, dict)):\n        self.extras = dict([(e, None) for e in self.extras])\n    if extras:\n        if (not isinstance(extras, dict)):\n            extras = dict([(e, None) for e in extras])\n        self.extras.update(extras)\n    assert isinstance(self.extras, dict)\n    if (('toc' in self.extras) and (not ('header-ids' in self.extras))):\n        self.extras['header-ids'] = None\n    self._instance_extras = self.extras.copy()\n    self.link_patterns = link_patterns\n    self.use_file_vars = use_file_vars\n    self._outdent_re = re.compile(('^(\\\\t|[ ]{1,%d})' % tab_width), re.M)\n    self._escape_table = g_escape_table.copy()\n    if ('smarty-pants' in self.extras):\n        self._escape_table['\"'] = _hash_text('\"')\n        self._escape_table[\"'\"] = _hash_text(\"'\")\n", "label": 1}
{"function": "\n\ndef getRealLocation(self, vRow, vCol, visual=False):\n    ' getRealLocation(vRow: int, vCol: int, visual: bool) -> (int, int)\\n        Return the actual location even if there is spanning at (vRow, vCol)\\n\\n        '\n    if visual:\n        (row, col) = (vRow, vCol)\n    else:\n        row = self.verticalHeader().logicalIndex(vRow)\n        col = self.horizontalHeader().logicalIndex(vCol)\n    cellSet = set()\n    for r in xrange(self.rowCount()):\n        for c in xrange(self.columnCount()):\n            cellSet.add((r, c))\n    for r in xrange(self.rowCount()):\n        for c in xrange(self.columnCount()):\n            if ((r, c) not in cellSet):\n                continue\n            rect = self.visualRect(self.model().index(r, c))\n            rSpan = self.rowSpan(r, c)\n            cSpan = self.columnSpan(r, c)\n            for rs in xrange(rSpan):\n                for cs in xrange(cSpan):\n                    if ((row == (r + rs)) and (col == (c + cs))):\n                        return (r, c)\n                    if (((r + rs), (c + cs)) in cellSet):\n                        cellSet.remove(((r + rs), (c + cs)))\n    return ((- 1), (- 1))\n", "label": 1}
{"function": "\n\ndef _parse_doctype_entity(self, i, declstartpos):\n    rawdata = self.rawdata\n    if (rawdata[i:(i + 1)] == '%'):\n        j = (i + 1)\n        while 1:\n            c = rawdata[j:(j + 1)]\n            if (not c):\n                return (- 1)\n            if c.isspace():\n                j = (j + 1)\n            else:\n                break\n    else:\n        j = i\n    (name, j) = self._scan_name(j, declstartpos)\n    if (j < 0):\n        return j\n    while 1:\n        c = self.rawdata[j:(j + 1)]\n        if (not c):\n            return (- 1)\n        if (c in '\\'\"'):\n            m = _declstringlit_match(rawdata, j)\n            if m:\n                j = m.end()\n            else:\n                return (- 1)\n        elif (c == '>'):\n            return (j + 1)\n        else:\n            (name, j) = self._scan_name(j, declstartpos)\n            if (j < 0):\n                return j\n", "label": 1}
{"function": "\n\ndef timeago(time=False):\n    \"\\n    Get a datetime object or a int() Epoch timestamp and return a\\n    pretty string like 'an hour ago', 'Yesterday', '3 months ago',\\n    'just now', etc\\n    \"\n    from datetime import datetime\n    now = datetime.now()\n    if (type(time) is int):\n        diff = (now - datetime.fromtimestamp(time))\n    elif isinstance(time, datetime):\n        diff = (now - time)\n    elif (not time):\n        diff = (now - now)\n    second_diff = diff.seconds\n    day_diff = diff.days\n    if (day_diff < 0):\n        return ''\n    if (day_diff == 0):\n        if (second_diff < 10):\n            return 'just now'\n        if (second_diff < 60):\n            return (str(second_diff) + ' seconds ago')\n        if (second_diff < 120):\n            return 'a minute ago'\n        if (second_diff < 3600):\n            return (str((second_diff / 60)) + ' minutes ago')\n        if (second_diff < 7200):\n            return 'an hour ago'\n        if (second_diff < 86400):\n            return (str((second_diff / 3600)) + ' hours ago')\n    if (day_diff == 1):\n        return 'Yesterday'\n    if (day_diff < 7):\n        return (str(day_diff) + ' days ago')\n    if (day_diff < 31):\n        return (str((day_diff / 7)) + ' weeks ago')\n    if (day_diff < 365):\n        return (str((day_diff / 30)) + ' months ago')\n    return (str((day_diff / 365)) + ' years ago')\n", "label": 1}
{"function": "\n\ndef test_insertComps_K1_D3(self, K=1, D=3):\n    A = ParamBag(K=K, D=D)\n    s = 123.456\n    A.setField('scalar', s, dims=None)\n    A.setField('N', [1.0], dims='K')\n    A.setField('x', np.random.rand(K, D), dims=('K', 'D'))\n    A.setField('xxT', np.random.rand(K, D, D), dims=('K', 'D', 'D'))\n    Abig = A.copy()\n    Abig.insertComps(A)\n    assert (Abig.K == 2)\n    assert np.allclose(Abig.N, np.hstack([A.N, A.N]))\n    assert (Abig.scalar == (2 * s))\n    assert (Abig.xxT.shape == (2, 3, 3))\n    assert np.allclose(Abig.xxT[0], A.xxT)\n    assert np.allclose(Abig.xxT[1], A.xxT)\n    Abig.insertComps(A)\n    assert (Abig.K == 3)\n    assert np.allclose(Abig.N, np.hstack([A.N, A.N, A.N]))\n    assert (Abig.scalar == (3 * s))\n    assert (Abig.xxT.shape == (3, 3, 3))\n    assert np.allclose(Abig.xxT[0], A.xxT)\n    assert np.allclose(Abig.xxT[1], A.xxT)\n    A.insertComps(Abig)\n    assert (A.K == 4)\n    assert (A.scalar == (4 * s))\n    assert np.allclose(A.N, np.hstack([1, 1, 1, 1]))\n", "label": 1}
{"function": "\n\ndef _get_unique_ch(text, all_common_encodes):\n    '\\n        text : encode sample strings\\n\\n        returns unique word / characters from input text encode strings.\\n    '\n    unique_chars = ''\n    if isinstance(text, str):\n        text = text.split('\\n')\n    elif isinstance(text, (list, tuple)):\n        pass\n    special_chars = ['.', ',', ';', ':', '', ' ', '\\r', '\\t', '=', '\\n']\n    for line in text:\n        for word in line.split(' '):\n            if (not PYTHON3):\n                word = word.decode('utf-8')\n            for ch in all_common_encodes:\n                if (ch in word):\n                    word = word.replace(ch, '')\n            if (not word):\n                continue\n            for ch in word:\n                if (ch.isdigit() or (ch in special_chars)):\n                    word = word.replace(ch, '')\n                    continue\n                return word\n    return ''\n", "label": 1}
{"function": "\n\ndef version_sidebar(request, form_data, facets):\n    appver = ''\n    if (('appver' in request.GET) or form_data.get('appver')):\n        appver = form_data.get('appver')\n    app = unicode(request.APP.pretty)\n    exclude_versions = getattr(request.APP, 'exclude_versions', [])\n    rv = [FacetLink(_('Any {0}').format(app), dict(appver='any'), (not appver))]\n    vs = [dict_from_int(f['term']) for f in facets['appversions']]\n    av_dict = version_dict(appver)\n    if (av_dict and (av_dict not in vs) and av_dict['major']):\n        vs.append(av_dict)\n    vs = set(((v['major'], (v['minor1'] if (v['minor1'] not in (None, 99)) else 0)) for v in vs))\n    versions = [('%s.%s' % v) for v in sorted(vs, reverse=True)]\n    for (version, floated) in zip(versions, map(float, versions)):\n        if ((floated not in exclude_versions) and (floated > request.APP.min_display_version)):\n            rv.append(FacetLink(('%s %s' % (app, version)), dict(appver=version), (appver == version)))\n    return rv\n", "label": 1}
{"function": "\n\ndef __call__(self, tag=None, ns=None, children=False, root=False, error=True):\n    'Search (even in child nodes) and return a child tag by name'\n    try:\n        if root:\n            return SimpleXMLElement(elements=[self.__document.documentElement], document=self.__document, namespace=self.__ns, prefix=self.__prefix, jetty=self.__jetty, namespaces_map=self.__namespaces_map)\n        if (tag is None):\n            return self.__iter__()\n        if children:\n            return self.children()\n        elements = None\n        if isinstance(tag, int):\n            elements = [self.__elements[tag]]\n        if (ns and (not elements)):\n            for ns_uri in ((isinstance(ns, (tuple, list)) and ns) or (ns,)):\n                elements = self._element.getElementsByTagNameNS(ns_uri, tag)\n                if elements:\n                    break\n        if (self.__ns and (not elements)):\n            elements = self._element.getElementsByTagNameNS(self.__ns, tag)\n        if (not elements):\n            elements = self._element.getElementsByTagName(tag)\n        if (not elements):\n            if error:\n                raise AttributeError('No elements found')\n            else:\n                return\n        return SimpleXMLElement(elements=elements, document=self.__document, namespace=self.__ns, prefix=self.__prefix, jetty=self.__jetty, namespaces_map=self.__namespaces_map)\n    except AttributeError as e:\n        raise AttributeError(('Tag not found: %s (%s)' % (tag, e)))\n", "label": 1}
{"function": "\n\ndef test():\n    pid = get_player('Tim', 'Duncan')\n    vs_pid = get_player('Stephen', 'Curry')\n    assert player.PlayerList()\n    assert player.PlayerSummary(pid)\n    assert player.PlayerLastNGamesSplits(pid)\n    assert player.PlayerInGameSplits(pid)\n    assert player.PlayerClutchSplits(pid)\n    assert player.PlayerPerformanceSplits(pid)\n    assert player.PlayerYearOverYearSplits(pid)\n    assert player.PlayerCareer(pid)\n    assert player.PlayerProfile(pid)\n    assert player.PlayerGameLogs(pid)\n    assert player.PlayerShotTracking(pid)\n    assert player.PlayerReboundTracking(pid)\n    assert player.PlayerPassTracking(pid)\n    assert player.PlayerDefenseTracking(pid)\n    assert player.PlayerVsPlayer(pid, vs_pid)\n", "label": 1}
{"function": "\n\n@login_required\ndef make_folder(request, folder_id=None):\n    if (not folder_id):\n        folder_id = request.GET.get('parent_id')\n    if (not folder_id):\n        folder_id = request.POST.get('parent_id')\n    if folder_id:\n        try:\n            folder = Folder.objects.get(id=folder_id)\n        except Folder.DoesNotExist:\n            raise PermissionDenied\n    else:\n        folder = None\n    if request.user.is_superuser:\n        pass\n    elif (folder is None):\n        if (not settings.MEDIA_ALLOW_REGULAR_USERS_TO_ADD_ROOT_FOLDERS):\n            raise PermissionDenied\n    elif (not folder.has_add_children_permission(request)):\n        raise PermissionDenied\n    if (request.method == 'POST'):\n        new_folder_form = NewFolderForm(request.POST)\n        if new_folder_form.is_valid():\n            new_folder = new_folder_form.save(commit=False)\n            if (folder or FolderRoot()).contains_folder(new_folder.name):\n                new_folder_form._errors['name'] = new_folder_form.error_class([_('Folder with this name already exists.')])\n            else:\n                new_folder.parent = folder\n                new_folder.owner = request.user\n                new_folder.save()\n                return render_to_response('admin/media/dismiss_popup.html', context_instance=RequestContext(request))\n    else:\n        new_folder_form = NewFolderForm()\n    return render_to_response('admin/media/folder/new_folder_form.html', {\n        'new_folder_form': new_folder_form,\n        'is_popup': popup_status(request),\n        'select_folder': selectfolder_status(request),\n    }, context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef add_total_row(result, columns):\n    total_row = ([''] * len(columns))\n    has_percent = []\n    for row in result:\n        for (i, col) in enumerate(columns):\n            fieldtype = None\n            if isinstance(col, basestring):\n                col = col.split(':')\n                if (len(col) > 1):\n                    fieldtype = col[1]\n                    if ('/' in fieldtype):\n                        fieldtype = fieldtype.split('/')[0]\n            else:\n                fieldtype = col.get('fieldtype')\n            if ((fieldtype in ['Currency', 'Int', 'Float', 'Percent']) and flt(row[i])):\n                total_row[i] = (flt(total_row[i]) + flt(row[i]))\n            if ((fieldtype == 'Percent') and (i not in has_percent)):\n                has_percent.append(i)\n    for i in has_percent:\n        total_row[i] = (total_row[i] / len(result))\n    first_col_fieldtype = None\n    if isinstance(columns[0], basestring):\n        first_col = columns[0].split(':')\n        if (len(first_col) > 1):\n            first_col_fieldtype = first_col[1].split('/')[0]\n    else:\n        first_col_fieldtype = columns[0].get('fieldtype')\n    if (first_col_fieldtype not in ['Currency', 'Int', 'Float', 'Percent']):\n        if (first_col_fieldtype == 'Link'):\n            total_row[0] = ((\"'\" + _('Total')) + \"'\")\n        else:\n            total_row[0] = _('Total')\n    result.append(total_row)\n    return result\n", "label": 1}
{"function": "\n\ndef test_stats():\n    (m, ctl, config) = init()\n    m.load(config)\n    cmd = TestCommand('stats', ['dummy'])\n    ctl.process_command(cmd)\n    m.scale('dummy', 2)\n    time.sleep(0.1)\n    cmd1 = TestCommand('stats', ['dummy'])\n    ctl.process_command(cmd1)\n    m.stop()\n    m.run()\n    assert isinstance(cmd.result, dict)\n    assert ('stats' in cmd.result)\n    stats = cmd.result['stats']\n    assert isinstance(stats, dict)\n    assert (stats['name'] == 'default.dummy')\n    assert ('stats' in stats)\n    assert (len(stats['stats']) == 1)\n    assert (stats['stats'][0]['pid'] == 1)\n    assert isinstance(cmd1.result, dict)\n    stats = cmd1.result['stats']\n    assert isinstance(stats, dict)\n    assert (stats['name'] == 'default.dummy')\n    assert ('stats' in stats)\n    assert (len(stats['stats']) == 3)\n", "label": 1}
{"function": "\n\ndef _check_fusion(self, root):\n    roots = root.table._root_tables()\n    validator = ExprValidator([root.table])\n    fused_exprs = []\n    can_fuse = False\n    resolved = _maybe_resolve_exprs(root.table, self.input_exprs)\n    if (not resolved):\n        return None\n    for val in resolved:\n        lifted_val = substitute_parents(val)\n        if (isinstance(val, ir.TableExpr) and (self.parent.op().is_ancestor(val) or ((len(roots) == 1) and (val._root_tables()[0] is roots[0])))):\n            can_fuse = True\n            have_root = False\n            for y in root.selections:\n                if y.equals(root.table):\n                    fused_exprs.append(root.table)\n                    have_root = True\n                    continue\n                fused_exprs.append(y)\n            if ((not have_root) and (len(root.selections) == 0)):\n                fused_exprs = ([root.table] + fused_exprs)\n        elif validator.validate(lifted_val):\n            can_fuse = True\n            fused_exprs.append(lifted_val)\n        elif (not validator.validate(val)):\n            can_fuse = False\n            break\n        else:\n            fused_exprs.append(val)\n    if can_fuse:\n        return ops.Selection(root.table, fused_exprs, predicates=root.predicates, sort_keys=root.sort_keys)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef _build_path_iterator(path, namespaces):\n    if (path[(- 1):] == '/'):\n        path = (path + '*')\n    try:\n        return _cache[(path, ((namespaces and tuple(sorted(namespaces.items()))) or None))]\n    except KeyError:\n        pass\n    if (len(_cache) > 100):\n        _cache.clear()\n    if (path[:1] == '/'):\n        raise SyntaxError('cannot use absolute path on element')\n    stream = iter(xpath_tokenizer(path, namespaces))\n    try:\n        _next = stream.next\n    except AttributeError:\n        _next = stream.__next__\n    try:\n        token = _next()\n    except StopIteration:\n        raise SyntaxError('empty path expression')\n    selector = []\n    while 1:\n        try:\n            selector.append(ops[token[0]](_next, token))\n        except StopIteration:\n            raise SyntaxError('invalid path')\n        try:\n            token = _next()\n            if (token[0] == '/'):\n                token = _next()\n        except StopIteration:\n            break\n    _cache[path] = selector\n    return selector\n", "label": 1}
{"function": "\n\ndef main(script):\n    'Tests the functions in this module.\\n\\n    script: string script name\\n    '\n    preg = ReadFemPreg()\n    print(preg.shape)\n    assert (len(preg) == 13593)\n    assert (preg.caseid[13592] == 12571)\n    assert (preg.pregordr.value_counts()[1] == 5033)\n    assert (preg.nbrnaliv.value_counts()[1] == 8981)\n    assert (preg.babysex.value_counts()[1] == 4641)\n    assert (preg.birthwgt_lb.value_counts()[7] == 3049)\n    assert (preg.birthwgt_oz.value_counts()[0] == 1037)\n    assert (preg.prglngth.value_counts()[39] == 4744)\n    assert (preg.outcome.value_counts()[1] == 9148)\n    assert (preg.birthord.value_counts()[1] == 4413)\n    assert (preg.agepreg.value_counts()[22.75] == 100)\n    assert (preg.totalwgt_lb.value_counts()[7.5] == 302)\n    weights = preg.finalwgt.value_counts()\n    key = max(weights.keys())\n    assert (preg.finalwgt.value_counts()[key] == 6)\n    print(('%s: All tests passed.' % script))\n", "label": 1}
{"function": "\n\ndef inspect(self, func):\n    ' Return a dict that maps parameter names to injection points for the provided callable. '\n    func = _unwrap(func)\n    if py32:\n        (args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations) = inspect.getfullargspec(func)\n    else:\n        (args, varargs, keywords, defaults) = inspect.getargspec(func)\n        (kwonlyargs, kwonlydefaults, annotations) = ([], {\n            \n        }, {\n            \n        })\n    defaults = (defaults or ())\n    kwonlydefaults = (kwonlydefaults or {\n        \n    })\n    injection_points = {\n        \n    }\n    for arg in args[:(len(args) - len((defaults or [])))]:\n        if (arg not in self._never_inject):\n            injection_points[arg] = _InjectionPoint(arg, implicit=True)\n    for (arg, value) in zip(args[::(- 1)], defaults[::(- 1)]):\n        if isinstance(value, _InjectionPoint):\n            injection_points[arg] = value\n    for (arg, value) in kwonlydefaults.items():\n        if isinstance(value, _InjectionPoint):\n            injection_points[arg] = value\n    for (arg, value) in annotations.items():\n        if isinstance(value, _InjectionPoint):\n            injection_points[arg] = value\n    return injection_points\n", "label": 1}
{"function": "\n\ndef _define_interface(self, plots, allow_mismatch):\n    parameters = [{k: v.precedence for (k, v) in plot.params().items() if ((v.precedence is None) or (v.precedence >= 0))} for plot in plots]\n    param_sets = [set(params.keys()) for params in parameters]\n    if ((not allow_mismatch) and (not all(((pset == param_sets[0]) for pset in param_sets)))):\n        raise Exception('All selectable plot classes must have identical plot options.')\n    styles = [plot.style_opts for plot in plots]\n    if ((not allow_mismatch) and (not all(((style == styles[0]) for style in styles)))):\n        raise Exception('All selectable plot classes must have identical style options.')\n    return (styles[0], parameters[0])\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    outcols = []\n    if self.rgb:\n        outcols.append(self.rgb)\n    if (self.thickEnd or outcols):\n        outcols.append((self.thickEnd if self.thickEnd else self.end))\n    if (self.thickStart or outcols):\n        outcols.append((self.thickStart if self.thickStart else self.start))\n    if (self.strand or outcols):\n        outcols.append(self.strand)\n    if ((self.score_int != '') or outcols):\n        outcols.append(self.score_int)\n    if (self.name or outcols):\n        outcols.append(self.name)\n    outcols.append(self.end)\n    outcols.append(self.start)\n    outcols.append(self.chrom)\n    return '\\t'.join([str(x) for x in outcols[::(- 1)]])\n", "label": 1}
{"function": "\n\ndef assertMessage(self, response, content, level=None, tags=None, limit=None):\n    '\\n        Asserts that the response has a particular message in its context.\\n        \\n        If limit is provided, checks that there are at most the specified\\n        number of messages.\\n        \\n        If level or tags are specified, checks for existence of message having\\n        matching content, level and/or tags. Otherwise, it simply checks for a\\n        message with the content.\\n        \\n        '\n    self.assertTrue((hasattr(response, 'context') and response.context), 'The response must have a non-empty context attribute.')\n    messages = list((response.context['messages'] if ('messages' in response.context) else []))\n    self.assertTrue(bool(messages), \"The response's context must contain at least one message.\")\n    if limit:\n        self.assertGreaterEqual(limit, len(messages), \"The response's context must have at most {limit:d} messages, but it has {actual:d} messages.\".format(limit=limit, actual=len(messages)))\n    self.assertTrue(any((((message.message == content) and ((not level) or (message.level == level)) and ((not tags) or (set((tag.strip() for tag in (message.tags or '').split(' ') if tag)) == set((tag.strip() for tag in (tags or '').split(' ') if tag))))) for message in messages)), \"A message matching the content, level and tags was not found in the response's context.\")\n", "label": 1}
{"function": "\n\ndef parse_placeholder(parser, token):\n    'Parse the `PlaceholderNode` parameters.\\n\\n    Return a tuple with the name and parameters.'\n    bits = token.split_contents()\n    count = len(bits)\n    error_string = ('%r tag requires at least one argument' % bits[0])\n    if (count <= 1):\n        raise TemplateSyntaxError(error_string)\n    try:\n        name = unescape_string_literal(bits[1])\n    except ValueError:\n        name = bits[1]\n    remaining = bits[2:]\n    params = {\n        \n    }\n    simple_options = ['parsed', 'inherited', 'untranslated']\n    param_options = ['as', 'on', 'with']\n    all_options = (simple_options + param_options)\n    while remaining:\n        bit = remaining[0]\n        if (bit not in all_options):\n            raise TemplateSyntaxError(('%r is not an correct option for a placeholder' % bit))\n        if (bit in param_options):\n            if (len(remaining) < 2):\n                raise TemplateSyntaxError((\"Placeholder option '%s' need a parameter\" % bit))\n            if (bit == 'as'):\n                params['as_varname'] = remaining[1]\n            if (bit == 'with'):\n                params['widget'] = remaining[1]\n            if (bit == 'on'):\n                params['page'] = remaining[1]\n            remaining = remaining[2:]\n        elif (bit == 'parsed'):\n            params['parsed'] = True\n            remaining = remaining[1:]\n        elif (bit == 'inherited'):\n            params['inherited'] = True\n            remaining = remaining[1:]\n        elif (bit == 'untranslated'):\n            params['untranslated'] = True\n            remaining = remaining[1:]\n    return (name, params)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _iter_items(cls, repo, common_path=None):\n    if (common_path is None):\n        common_path = cls._common_path_default\n    rela_paths = set()\n    for (root, dirs, files) in os.walk(join_path_native(repo.git_dir, common_path)):\n        if ('refs/' not in root):\n            refs_id = [d for d in dirs if (d == 'refs')]\n            if refs_id:\n                dirs[0:] = ['refs']\n        for f in files:\n            if (f == 'packed-refs'):\n                continue\n            abs_path = to_native_path_linux(join_path(root, f))\n            rela_paths.add(abs_path.replace((to_native_path_linux(repo.git_dir) + '/'), ''))\n    for (sha, rela_path) in cls._iter_packed_refs(repo):\n        if rela_path.startswith(common_path):\n            rela_paths.add(rela_path)\n    for path in sorted(rela_paths):\n        try:\n            (yield cls.from_path(repo, path))\n        except ValueError:\n            continue\n", "label": 1}
{"function": "\n\ndef iterate_over_form(job, form, function, prefix=['form'], indent=''):\n    warnings = False\n    if (not hasattr(form, '__dict__')):\n        return False\n    whitelist_fields = ['BooleanField', 'FloatField', 'HiddenField', 'IntegerField', 'RadioField', 'SelectField', 'SelectMultipleField', 'StringField', 'TextAreaField', 'TextField', 'MultiIntegerField', 'MultiFloatField']\n    blacklist_fields = ['FileField', 'SubmitField']\n    for attr_name in vars(form):\n        if ((attr_name == 'csrf_token') or (attr_name == 'flags')):\n            continue\n        attr = getattr(form, attr_name)\n        if isinstance(attr, object):\n            if isinstance(attr, SubmitField):\n                continue\n            warnings |= iterate_over_form(job, attr, function, (prefix + [attr_name]), (indent + '    '))\n        if (hasattr(attr, 'data') and hasattr(attr, 'type')):\n            if (isinstance(attr.data, int) or isinstance(attr.data, float) or isinstance(attr.data, basestring) or (attr.type in whitelist_fields)):\n                key = ('%s.%s.data' % ('.'.join(prefix), attr_name))\n                warnings |= function(job, attr, key, attr.data)\n            if ((len(attr.type) > 5) and (attr.type[(- 5):] == 'Field') and (attr.type not in whitelist_fields) and (attr.type not in blacklist_fields)):\n                warnings |= add_warning(attr, ('Field type, %s, not cloned' % attr.type))\n    return warnings\n", "label": 1}
{"function": "\n\ndef _compose(self, public=None, private=None, no_cache=None, no_store=False, max_age=None, s_maxage=None, no_transform=False, **extensions):\n    assert isinstance(max_age, (type(None), int))\n    assert isinstance(s_maxage, (type(None), int))\n    expires = 0\n    result = []\n    if (private is True):\n        assert ((not public) and (not no_cache) and (not s_maxage))\n        result.append('private')\n    elif (no_cache is True):\n        assert ((not public) and (not private) and (not max_age))\n        result.append('no-cache')\n    else:\n        assert ((public is None) or (public is True))\n        assert ((not private) and (not no_cache))\n        expires = max_age\n        result.append('public')\n    if no_store:\n        result.append('no-store')\n    if no_transform:\n        result.append('no-transform')\n    if (max_age is not None):\n        result.append(('max-age=%d' % max_age))\n    if (s_maxage is not None):\n        result.append(('s-maxage=%d' % s_maxage))\n    for (k, v) in six.iteritems(extensions):\n        if (k not in self.extensions):\n            raise AssertionError((\"unexpected extension used: '%s'\" % k))\n        result.append(('%s=\"%s\"' % (k.replace('_', '-'), v)))\n    return (result, expires)\n", "label": 1}
{"function": "\n\ndef randwalk(n, s=1, s3='good'):\n    state = s\n    if (s3 == 'good'):\n        p32 = 0.2\n        p33 = 0.9\n    else:\n        p32 = 0.1\n        p33 = 0.8\n    path = str(state)\n    for i in range(n):\n        if (state == 1):\n            p = random()\n            if (p <= 0.9):\n                path += str(state)\n            else:\n                state = 2\n                path += str(state)\n        elif (state == 2):\n            p = random()\n            if (p <= 0.1):\n                state = 1\n                path += str(state)\n            elif (p <= 0.9):\n                path += str(state)\n            else:\n                state = 3\n                path += str(state)\n        elif (state == 3):\n            p = random()\n            if (p <= p32):\n                state = 2\n                path += str(state)\n            elif (p <= p33):\n                path += str(state)\n            else:\n                state = 4\n                path += str(state)\n        elif (state == 4):\n            p = random()\n            if (p <= 0.6):\n                path += str(state)\n            else:\n                state = 0\n                path += str(state)\n        else:\n            return path\n            break\n    return path\n", "label": 1}
{"function": "\n\ndef format(self, value):\n    '\\n        Formats a value.\\n\\n        @type value\\n          `float`\\n        '\n    sign = ('' if (self.__sign is None) else ('-' if (value < 0) else ('+' if (self.__sign == '+') else ' ')))\n    if math.isnan(value):\n        result = text.pad(self.__nan_str, self.__width, pad=' ', left=True)\n    elif ((value < 0) and (self.__sign is None)):\n        result = ('#' * self.__width)\n    elif math.isinf(value):\n        result = text.pad((sign + self.__inf_str), self.__width, pad=' ', left=True)\n    else:\n        precision = (0 if (self.__precision is None) else self.__precision)\n        rnd_value = round(value, precision)\n        abs_value = abs(rnd_value)\n        int_value = int(abs_value)\n        result = str(int_value)\n        if (len(result) > self.__size):\n            return ('#' * self.__width)\n        if (self.__pad == ' '):\n            result = text.pad((sign + result), (self.__size + len(sign)), pad=self.__pad, left=True)\n        else:\n            result = (sign + text.pad(result, self.__size, pad=self.__pad, left=True))\n        if (self.__precision is None):\n            pass\n        elif (self.__precision == 0):\n            result += self.__point\n        else:\n            frac = int(round(((abs_value - int_value) * self.__multiplier)))\n            frac = str(frac)\n            assert (len(frac) <= precision)\n            frac = text.pad(frac, self.__precision, pad='0', left=True)\n            result += (self.__point + frac)\n    return result\n", "label": 1}
{"function": "\n\ndef parse(self, file=None, message=None):\n    retval = {\n        \n    }\n    try:\n        if ((file is not None) and os.path.exists(file)):\n            message = open(file).read()\n        mail = email.Parser.Parser().parsestr(message)\n        headers = mail._headers\n        msgs = self._parse_mail(mail)\n        if (type(msgs['body']) is list):\n            rawbody = ''\n            if ((file is not None) and os.path.exists(file)):\n                f = open(file)\n                line = f.readline()\n                while line:\n                    line = f.readline()\n                    if (line.rstrip() == ''):\n                        rawbody += line\n                        rawbody += ''.join(f.readlines())\n                        break\n                f.close\n            else:\n                in_body = False\n                for line in message.split('\\n'):\n                    if (line.rstrip() == ''):\n                        in_body = True\n                    if (in_body is True):\n                        rawbody += (line + '\\n')\n        elif (type(msgs['body']) is str):\n            rawbody = msgs['body']\n        retval['headers'] = headers\n        retval['msgs'] = msgs\n        retval['rawbody'] = rawbody\n    except:\n        pass\n    return retval\n", "label": 1}
{"function": "\n\ndef find(pattern, path='.', exclude=None, recursive=True):\n    'Find files that match *pattern* in *path*'\n    import fnmatch\n    import os\n    if recursive:\n        for (root, dirnames, filenames) in os.walk(path):\n            for pat in _to_list(pattern):\n                for filename in fnmatch.filter(filenames, pat):\n                    filepath = join(abspath(root), filename)\n                    for excl in _to_list(exclude):\n                        if (excl and fnmatch.fnmatch(filepath, excl)):\n                            break\n                    else:\n                        (yield filepath)\n    else:\n        for pat in _to_list(pattern):\n            for filename in fnmatch.filter(list(path), pat):\n                filepath = join(abspath(path), filename)\n                for excl in _to_list(exclude):\n                    if (excl and fnmatch.fnmatch(filepath, excl)):\n                        break\n                    else:\n                        (yield filepath)\n", "label": 1}
{"function": "\n\n@classmethod\ndef deserialize(cls, buf):\n    'Returns a Match object deserialized from a sequence of bytes.\\n\\n        Args:\\n            buf: A ReceiveBuffer object that contains the bytes that\\n                are the serialized form of the Match object.\\n\\n        Returns:\\n            A new Match object deserialized from the buffer.\\n\\n        Raises:\\n            ValueError: The buffer has an invalid number of available\\n                bytes, or some elements cannot be deserialized.\\n        '\n    (wildcards_ser, in_port, dl_src, dl_dst, dl_vlan, dl_vlan_pcp, dl_type, nw_tos, nw_proto, nw_src, nw_dst, tp_src, tp_dst) = buf.unpack(cls.FORMAT)\n    wildcards = Wildcards.deserialize(wildcards_ser)\n    if (nw_tos & 3):\n        nw_tos &= 252\n    nw_src_prefix_length = (32 - wildcards.nw_src)\n    nw_dst_prefix_length = (32 - wildcards.nw_dst)\n    return Match((None if wildcards.in_port else in_port), (None if wildcards.dl_src else dl_src), (None if wildcards.dl_dst else dl_dst), (None if wildcards.dl_vlan else dl_vlan), (None if wildcards.dl_vlan_pcp else dl_vlan_pcp), (None if wildcards.dl_type else dl_type), (None if wildcards.nw_tos else nw_tos), (None if wildcards.nw_proto else nw_proto), ((nw_src, nw_src_prefix_length) if (nw_src_prefix_length > 0) else None), ((nw_dst, nw_dst_prefix_length) if (nw_dst_prefix_length > 0) else None), (None if wildcards.tp_src else tp_src), (None if wildcards.tp_dst else tp_dst))\n", "label": 1}
{"function": "\n\n@classmethod\ndef deserialize(cls, buf):\n    'Returns a PortStats object deserialized from a sequence of bytes.\\n\\n        Args:\\n            buf: A ReceiveBuffer object that contains the bytes that\\n                are the serialized form of the PortStats object.\\n\\n        Returns:\\n            A new PortStats object deserialized from the buffer.\\n\\n        Raises:\\n            ValueError: The buffer has an invalid number of available\\n                bytes.\\n        '\n    (port_no, rx_packets, tx_packets, rx_bytes, tx_bytes, rx_dropped, tx_dropped, rx_errors, tx_errors, rx_frame_err, rx_over_err, rx_crc_err, collisions) = buf.unpack(cls.FORMAT)\n    return PortStats(port_no, (None if (rx_packets == cls._UNAVAILABLE) else rx_packets), (None if (tx_packets == cls._UNAVAILABLE) else tx_packets), (None if (rx_bytes == cls._UNAVAILABLE) else rx_bytes), (None if (tx_bytes == cls._UNAVAILABLE) else tx_bytes), (None if (rx_dropped == cls._UNAVAILABLE) else rx_dropped), (None if (tx_dropped == cls._UNAVAILABLE) else tx_dropped), (None if (rx_errors == cls._UNAVAILABLE) else rx_errors), (None if (tx_errors == cls._UNAVAILABLE) else tx_errors), (None if (rx_frame_err == cls._UNAVAILABLE) else rx_frame_err), (None if (rx_over_err == cls._UNAVAILABLE) else rx_over_err), (None if (rx_crc_err == cls._UNAVAILABLE) else rx_crc_err), (None if (collisions == cls._UNAVAILABLE) else collisions))\n", "label": 1}
{"function": "\n\ndef get_substrings(self, min_freq=2, check_positive=True, sort_by_length=False):\n    movetos = set()\n    for (idx, tok) in enumerate(self.rev_keymap):\n        if (isinstance(tok, basestring) and (tok[(- 6):] == 'moveto')):\n            movetos.add(idx)\n    try:\n        hmask = self.rev_keymap.index('hintmask')\n    except ValueError:\n        hmask = None\n    matches = {\n        \n    }\n    for (glyph_idx, program) in enumerate(self.data):\n        cur_start = 0\n        last_op = (- 1)\n        for (pos, tok) in enumerate(program):\n            if (tok in movetos):\n                stop = (last_op + 1)\n                if ((stop - cur_start) > 0):\n                    if (program[cur_start:stop] in matches):\n                        matches[program[cur_start:stop]].freq += 1\n                    else:\n                        span = pyCompressor.CandidateSubr((stop - cur_start), (glyph_idx, cur_start), 1, self.data, self.cost_map)\n                        matches[program[cur_start:stop]] = span\n                cur_start = (pos + 1)\n            elif (tok == hmask):\n                last_op = (pos + 1)\n            elif (type(self.rev_keymap[tok]) == str):\n                last_op = pos\n    constraints = (lambda s: ((s.freq >= min_freq) and ((s.subr_saving() > 0) or (not check_positive))))\n    self.substrings = filter(constraints, matches.values())\n    if sort_by_length:\n        self.substrings.sort(key=(lambda s: len(s)))\n    else:\n        self.substrings.sort(key=(lambda s: s.subr_saving()), reverse=True)\n    return self.substrings\n", "label": 1}
{"function": "\n\ndef _print_Mul(self, expr):\n    prec = precedence(expr)\n    (c, e) = expr.as_coeff_Mul()\n    if (c < 0):\n        expr = _keep_coeff((- c), e)\n        sign = '-'\n    else:\n        sign = ''\n    a = []\n    b = []\n    if (self.order not in ('old', 'none')):\n        args = expr.as_ordered_factors()\n    else:\n        args = Mul.make_args(expr)\n    for item in args:\n        if (item.is_commutative and item.is_Pow and item.exp.is_Rational and item.exp.is_negative):\n            if (item.exp != (- 1)):\n                b.append(Pow(item.base, (- item.exp), evaluate=False))\n            else:\n                b.append(Pow(item.base, (- item.exp)))\n        else:\n            a.append(item)\n    a = (a or [S.One])\n    a_str = [self.parenthesize(x, prec) for x in a]\n    b_str = [self.parenthesize(x, prec) for x in b]\n    if (len(b) == 0):\n        return (sign + '*'.join(a_str))\n    elif (len(b) == 1):\n        return (((sign + '*'.join(a_str)) + '/') + b_str[0])\n    else:\n        return ((sign + '*'.join(a_str)) + ('/(%s)' % '*'.join(b_str)))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef merge(left, right, func):\n    if (left is right):\n        return left\n    if (left is None):\n        (left, right) = (right, left)\n    default = left.default\n    merge = _TreeListSub.merge\n    if (right is None):\n        direct = [func(x, default) for x in left.direct]\n        children = [merge(child, None, func) for child in left.children]\n        if ((direct == left.direct) and (children == left.children)):\n            return left\n        return _TreeListSub(default, direct, children)\n    direct = [func(x, y) for (x, y) in zip(left.direct, right.direct)]\n    children = [merge(c1, c2, func) for (c1, c2) in zip(left.children, right.children)]\n    if ((direct == left.direct) and (children == left.children)):\n        return left\n    if ((direct == right.direct) and (children == right.children)):\n        return right\n    return _TreeListSub(default, direct, children)\n", "label": 1}
{"function": "\n\ndef ui_complete_delete(self, parameters, text, current_param):\n    '\\n        Parameter auto-completion method for user command delete.\\n        @param parameters: Parameters on the command line.\\n        @type parameters: dict\\n        @param text: Current text of parameter being typed by the user.\\n        @type text: str\\n        @param current_param: Name of parameter to complete.\\n        @type current_param: str\\n        @return: Possible completions\\n        @rtype: list of str\\n        '\n    completions = []\n    portals = {\n        \n    }\n    all_ports = set([])\n    for portal in self.tpg.network_portals:\n        all_ports.add(str(portal.port))\n        portal_ip = portal.ip_address.strip('[]')\n        if (not (portal_ip in portals)):\n            portals[portal_ip] = []\n        portals[portal_ip].append(str(portal.port))\n    if (current_param == 'ip_address'):\n        completions = [addr for addr in portals if addr.startswith(text)]\n        if ('ip_port' in parameters):\n            port = parameters['ip_port']\n            completions = [addr for addr in completions if (port in portals[addr])]\n    elif (current_param == 'ip_port'):\n        if ('ip_address' in parameters):\n            addr = parameters['ip_address']\n            if (addr in portals):\n                completions = [port for port in portals[addr] if port.startswith(text)]\n        else:\n            completions = [port for port in all_ports if port.startswith(text)]\n    if (len(completions) == 1):\n        return [(completions[0] + ' ')]\n    else:\n        return completions\n", "label": 1}
{"function": "\n\ndef same_shape(self, x, y, dim_x=None, dim_y=None):\n    'Return True if we are able to assert that x and y have the\\n        same shape.\\n\\n        dim_x and dim_y are optional. If used, they should be an index\\n        to compare only 1 dimension of x and y.\\n\\n        '\n    sx = self.shape_of[x]\n    sy = self.shape_of[y]\n    if ((sx is None) or (sy is None)):\n        return False\n    if (dim_x is not None):\n        sx = [sx[dim_x]]\n    if (dim_y is not None):\n        sy = [sy[dim_y]]\n    assert (len(sx) == len(sy))\n    for (dx, dy) in zip(sx, sy):\n        if (dx is dy):\n            continue\n        if ((not dx.owner) or (not dy.owner)):\n            return False\n        if ((not isinstance(dx.owner.op, Shape_i)) or (not isinstance(dy.owner.op, Shape_i))):\n            return False\n        opx = dx.owner.op\n        opy = dy.owner.op\n        if (not (opx.i == opy.i)):\n            return False\n        if (dx.owner.inputs[0] == dy.owner.inputs[0]):\n            continue\n        from theano.scan_module.scan_utils import equal_computations\n        if (not equal_computations([dx], [dy])):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_02_atom_init_with_statement(self):\n    s = Atom_Sword_Statement(ATOM_TEST_STATEMENT)\n    assert (len(s.states) == 1)\n    assert (len(s.original_deposits) == 1)\n    assert (len(s.resources) == 1)\n    assert (s.xml_document != None)\n    assert (s.dom != None)\n    assert s.parsed\n    assert s.valid\n    (uri, description) = s.states[0]\n    assert (uri == 'http://purl.org/net/sword/terms/state/Testing')\n    assert (description == 'The work has passed through review and is now in the archive')\n    t = datetime.strptime('2011-03-02T20:50:06Z', '%Y-%m-%dT%H:%M:%SZ')\n    entry = s.resources[0]\n    assert (len(entry.packaging) == 1)\n    assert (entry.deposited_by == 'sword')\n    assert (entry.deposited_on_behalf_of == 'jbloggs')\n    assert (entry.deposited_on == t)\n    assert (entry.uri == 'http://localhost:8080/part-IRI/43/my_deposit/example.zip')\n    assert (entry.packaging[0] == 'http://purl.org/net/sword/package/SimpleZip')\n", "label": 1}
{"function": "\n\ndef RemoveNativelySupportedComponents(filters, orders, exists):\n    ' Removes query components that are natively supported by the datastore.\\n\\n  The resulting filters and orders should not be used in an actual query.\\n\\n  Args:\\n    filters: the filters set on the query\\n    orders: the orders set on the query\\n    exists: the names of properties that require an exists filter if\\n      not already specified\\n\\n  Returns:\\n    (filters, orders) the reduced set of filters and orders\\n  '\n    (filters, orders) = Normalize(filters, orders, exists)\n    for f in filters:\n        if (f.op() in EXISTS_OPERATORS):\n            return (filters, orders)\n    has_key_desc_order = False\n    if (orders and (orders[(- 1)].property() == datastore_types.KEY_SPECIAL_PROPERTY)):\n        if (orders[(- 1)].direction() == ASCENDING):\n            orders = orders[:(- 1)]\n        else:\n            has_key_desc_order = True\n    if (not has_key_desc_order):\n        for f in filters:\n            if ((f.op() in INEQUALITY_OPERATORS) and (f.property(0).name() != datastore_types.KEY_SPECIAL_PROPERTY)):\n                break\n        else:\n            filters = [f for f in filters if (f.property(0).name() != datastore_types.KEY_SPECIAL_PROPERTY)]\n    return (filters, orders)\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    if (not [b for b in bases if isinstance(b, HideMetaOpts)]):\n        return super(HideMetaOpts, cls).__new__(cls, name, bases, attrs)\n    else:\n        meta_opts = deepcopy(cls.default_meta_opts)\n        if (('Meta' in attrs) and (attrs['Meta'].__module__ != 'django.db.models.query_utils')):\n            meta = attrs.get('Meta')\n        else:\n            for base in bases:\n                meta = getattr(base, '_meta', None)\n                if meta:\n                    break\n        if meta:\n            for (opt, value) in vars(meta).items():\n                if ((opt not in models.options.DEFAULT_NAMES) and (cls.hide_unknown_opts or (opt in meta_opts))):\n                    meta_opts[opt] = value\n                    delattr(meta, opt)\n        new_class = super(HideMetaOpts, cls).__new__(cls, name, bases, attrs)\n        if meta:\n            for opt in meta_opts:\n                setattr(meta, opt, meta_opts[opt])\n        for opt in meta_opts:\n            setattr(new_class._meta, opt, meta_opts[opt])\n        return new_class\n", "label": 1}
{"function": "\n\ndef cache_response(self, request, response, body=None):\n    '\\n        Algorithm for caching requests.\\n\\n        This assumes a requests Response object.\\n        '\n    if (response.status not in [200, 203, 300, 301]):\n        return\n    response_headers = CaseInsensitiveDict(response.headers)\n    cc_req = self.parse_cache_control(request.headers)\n    cc = self.parse_cache_control(response_headers)\n    cache_url = self.cache_url(request.url)\n    no_store = (cc.get('no-store') or cc_req.get('no-store'))\n    if (no_store and self.cache.get(cache_url)):\n        self.cache.delete(cache_url)\n    if (self.cache_etags and ('etag' in response_headers)):\n        self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n    elif (response.status == 301):\n        self.cache.set(cache_url, self.serializer.dumps(request, response))\n    elif ('date' in response_headers):\n        if (cc and cc.get('max-age')):\n            if (int(cc['max-age']) > 0):\n                self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n        elif ('expires' in response_headers):\n            if response_headers['expires']:\n                self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n", "label": 1}
{"function": "\n\ndef replicate_attributes(source, target, cache=None):\n    'Replicates common SQLAlchemy attributes from the `source` object to the\\n    `target` object.'\n    target_manager = manager_of_class(type(target))\n    column_attrs = set()\n    relationship_attrs = set()\n    relationship_columns = set()\n    for attr in manager_of_class(type(source)).attributes:\n        if (attr.key not in target_manager):\n            continue\n        target_attr = target_manager[attr.key]\n        if isinstance(attr.property, ColumnProperty):\n            assert isinstance(target_attr.property, ColumnProperty)\n            column_attrs.add(attr)\n        elif isinstance(attr.property, RelationshipProperty):\n            assert isinstance(target_attr.property, RelationshipProperty)\n            relationship_attrs.add(attr)\n            if (attr.property.direction is MANYTOONE):\n                relationship_columns.update(attr.property.local_columns)\n    for attr in column_attrs:\n        if _column_property_in_registry(attr.property, _excluded):\n            continue\n        elif ((not _column_property_in_registry(attr.property, _included)) and all(((column in relationship_columns) for column in attr.property.columns))):\n            continue\n        setattr(target, attr.key, getattr(source, attr.key))\n    for attr in relationship_attrs:\n        target_attr_model = target_manager[attr.key].property.argument\n        if (not is_relation_replicatable(attr)):\n            continue\n        replicate_relation(source, target, attr, target_manager[attr.key], cache=cache)\n", "label": 1}
{"function": "\n\ndef process_results(self, results):\n    self.log(' Creating and/or updating models...')\n    self.log(('  Found %s elections.' % len(results)))\n    for d in results:\n        self.log(('  Processing %s' % d['raw_name']))\n        (election, c) = Election.objects.get_or_create(year=d['year'], election_type=d['election_type'], id_raw=d['id'], sort_index=d['sort_index'])\n        if (self.verbosity > 2):\n            if c:\n                self.log(('  Created %s' % election))\n        for office_dict in d['data'].values():\n            for (office_name, candidates) in office_dict.items():\n                (office, c) = Office.objects.get_or_create(**self.parse_office_name(office_name))\n                if (self.verbosity > 2):\n                    if c:\n                        self.log(('  Created %s' % office))\n                for candidate in candidates:\n                    if (not candidate['id']):\n                        continue\n                    try:\n                        filer = Filer.objects.get(filer_id_raw=int(candidate['id']))\n                        (candidate, c) = Candidate.objects.get_or_create(election=election, office=office, filer=filer)\n                        if (self.verbosity > 2):\n                            if c:\n                                self.log(('  Created %s' % candidate))\n                    except Filer.DoesNotExist:\n                        pass\n", "label": 1}
{"function": "\n\ndef __getattribute__(self, name):\n    try:\n        result = super(GsxElement, self).__getattribute__(name)\n    except AttributeError:\n        \"\\n            The XML returned by GSX can be pretty inconsistent, especially\\n            between the different environments. It's therefore more\\n            practical to return None than to expect AttributeErrors all\\n            over your application.\\n            \"\n        return\n    if (name in STRING_TYPES):\n        return unicode((result.text or ''))\n    if isinstance(result, objectify.NumberElement):\n        return result.pyval\n    if isinstance(result, objectify.StringElement):\n        name = result.tag\n        result = (result.text or '')\n        result = unicode(result)\n        if (not result):\n            return\n        if (name in DATETIME_TYPES):\n            return gsx_datetime(result)\n        if (name in DIAGS_TIMESTAMP_TYPES):\n            return gsx_diags_timestamp(result)\n        if (name in BASE64_TYPES):\n            return gsx_attachment(result)\n        if (name in FLOAT_TYPES):\n            return gsx_price(result)\n        if name.endswith('Date'):\n            return gsx_date(result)\n        if name.endswith('Timestamp'):\n            return gsx_timestamp(result)\n        if re.search('^[YN]$', result):\n            return gsx_boolean(result)\n    return result\n", "label": 1}
{"function": "\n\ndef reverse(viewname, urlconf=None, args=None, kwargs=None, prefix=None, current_app=None):\n    if (urlconf is None):\n        urlconf = get_urlconf()\n    resolver = get_resolver(urlconf)\n    args = (args or [])\n    kwargs = (kwargs or {\n        \n    })\n    if (prefix is None):\n        prefix = get_script_prefix()\n    if (not isinstance(viewname, six.string_types)):\n        view = viewname\n    else:\n        parts = viewname.split(':')\n        parts.reverse()\n        view = parts[0]\n        path = parts[1:]\n        resolved_path = []\n        ns_pattern = ''\n        while path:\n            ns = path.pop()\n            try:\n                app_list = resolver.app_dict[ns]\n                if (current_app and (current_app in app_list)):\n                    ns = current_app\n                elif (ns not in app_list):\n                    ns = app_list[0]\n            except KeyError:\n                pass\n            try:\n                (extra, resolver) = resolver.namespace_dict[ns]\n                resolved_path.append(ns)\n                ns_pattern = (ns_pattern + extra)\n            except KeyError as key:\n                if resolved_path:\n                    raise NoReverseMatch((\"%s is not a registered namespace inside '%s'\" % (key, ':'.join(resolved_path))))\n                else:\n                    raise NoReverseMatch(('%s is not a registered namespace' % key))\n        if ns_pattern:\n            resolver = get_ns_resolver(ns_pattern, resolver)\n    return iri_to_uri(resolver._reverse_with_prefix(view, prefix, *args, **kwargs))\n", "label": 1}
{"function": "\n\ndef match(self, value=None, name=None):\n    nv = vv = False\n    if value:\n        if re.search(value, self.value):\n            vv = True\n        else:\n            vv = False\n    if name:\n        if re.search(name, self.name):\n            nv = True\n        else:\n            nv = False\n    if (name and value):\n        return (nv and vv)\n    if (name and (not value)):\n        return nv\n    if ((not name) and value):\n        return vv\n    if ((not name) and (not value)):\n        return False\n", "label": 1}
{"function": "\n\ndef test_file_metadata_drive_slashes(basepath):\n    item = fixtures.file_forward_slash\n    path = basepath.child(item['title'])\n    parsed = GoogleDriveFileMetadata(item, path)\n    assert (parsed.provider == 'googledrive')\n    assert (parsed.id == item['id'])\n    assert (parsed.name == item['title'])\n    assert (parsed.name == path.name)\n    assert (parsed.size == item['fileSize'])\n    assert (parsed.modified == item['modifiedDate'])\n    assert (parsed.content_type == item['mimeType'])\n    assert (parsed.extra == {\n        'revisionId': item['version'],\n        'webView': item['alternateLink'],\n    })\n    assert (parsed.path == ('/' + os.path.join(*[x.raw for x in path.parts])))\n    assert (parsed.materialized_path == str(path))\n    assert (parsed.is_google_doc == False)\n    assert (parsed.export_name == item['title'])\n", "label": 1}
{"function": "\n\n@classmethod\ndef size(cls, val, str_cache=None, object_cache=None, traits_cache=None):\n    if (str_cache is None):\n        str_cache = []\n    if (object_cache is None):\n        object_cache = []\n    if (traits_cache is None):\n        traits_cache = []\n    size = U8.size\n    if (isinstance(val, bool) and (val in (False, True))):\n        pass\n    elif (val is None):\n        pass\n    elif isinstance(val, integer_types):\n        if ((val < AMF3_MIN_INTEGER) or (val > AMF3_MAX_INTEGER)):\n            size += AMF3Double.size\n        else:\n            size += AMF3Integer.size(val)\n    elif isinstance(val, float):\n        size += AMF3Double.size\n    elif isinstance(val, (AMF3Array, list)):\n        size += AMF3ArrayPacker.size(val, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif isinstance(val, string_types):\n        size += AMF3String.size(val, cache=str_cache)\n    elif isinstance(val, AMF3ObjectBase):\n        size += AMF3ObjectPacker.size(val, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif isinstance(val, AMF3Date):\n        size += AMF3DatePacker.size(val, cache=object_cache)\n    else:\n        raise ValueError('Unable to pack value of type {0}'.format(type(val)))\n    return size\n", "label": 1}
{"function": "\n\ndef check_if_doc_is_dynamically_linked(doc, method='Delete'):\n    'Raise `frappe.LinkExistsError` if the document is dynamically linked'\n    for df in get_dynamic_link_map().get(doc.doctype, []):\n        if (df.parent in ('Communication', 'ToDo', 'DocShare', 'Email Unsubscribe')):\n            continue\n        meta = frappe.get_meta(df.parent)\n        if meta.issingle:\n            refdoc = frappe.db.get_singles_dict(df.parent)\n            if ((refdoc.get(df.options) == doc.doctype) and (refdoc.get(df.fieldname) == doc.name) and (((method == 'Delete') and (refdoc.docstatus < 2)) or ((method == 'Cancel') and (refdoc.docstatus == 1)))):\n                frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, df.parent, ''), frappe.LinkExistsError)\n        else:\n            for refdoc in frappe.db.sql('select name, docstatus from `tab{parent}` where\\n\\t\\t\\t\\t{options}=%s and {fieldname}=%s'.format(**df), (doc.doctype, doc.name), as_dict=True):\n                if (((method == 'Delete') and (refdoc.docstatus < 2)) or ((method == 'Cancel') and (refdoc.docstatus == 1))):\n                    frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, df.parent, refdoc.name), frappe.LinkExistsError)\n", "label": 1}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_start_timestamp_milliseconds():\n        self.set_start_timestamp_milliseconds(x.start_timestamp_milliseconds())\n    if x.has_http_method():\n        self.set_http_method(x.http_method())\n    if x.has_http_path():\n        self.set_http_path(x.http_path())\n    if x.has_http_query():\n        self.set_http_query(x.http_query())\n    if x.has_http_status():\n        self.set_http_status(x.http_status())\n    if x.has_duration_milliseconds():\n        self.set_duration_milliseconds(x.duration_milliseconds())\n    if x.has_api_mcycles():\n        self.set_api_mcycles(x.api_mcycles())\n    if x.has_processor_mcycles():\n        self.set_processor_mcycles(x.processor_mcycles())\n    for i in xrange(x.rpc_stats_size()):\n        self.add_rpc_stats().CopyFrom(x.rpc_stats(i))\n    for i in xrange(x.cgi_env_size()):\n        self.add_cgi_env().CopyFrom(x.cgi_env(i))\n    if x.has_overhead_walltime_milliseconds():\n        self.set_overhead_walltime_milliseconds(x.overhead_walltime_milliseconds())\n    if x.has_user_email():\n        self.set_user_email(x.user_email())\n    if x.has_is_admin():\n        self.set_is_admin(x.is_admin())\n    for i in xrange(x.individual_stats_size()):\n        self.add_individual_stats().CopyFrom(x.individual_stats(i))\n", "label": 1}
{"function": "\n\ndef _rule_to_post_data(rule):\n    post_data = {\n        \n    }\n    post_data['ruleType'] = rule['ruleType']\n    if ('filter' in rule):\n        filter = rule['filter']\n        if ('httpProtocol' in filter):\n            post_data['filterProtocol'] = filter['httpProtocol']\n        if ('method' in filter):\n            post_data['filterMethod'] = filter['method']\n        if ('url' in filter):\n            post_data['filterUrl'] = filter['url']\n        if ('statusCode' in filter):\n            post_data['filterstatusCode'] = filter['statusCode']\n    if ('action' in rule):\n        action = rule['action']\n        if ('type' in action):\n            post_data['actionType'] = action['type']\n        if ('httpProtocol' in action):\n            post_data['actionProtocol'] = action['httpProtocol']\n        if ('method' in action):\n            post_data['actionMethod'] = action['method']\n        if ('url' in action):\n            post_data['actionUrl'] = action['url']\n        if ('statusCode' in action):\n            post_data['actionStatusCode'] = action['statusCode']\n        if ('statusDescription' in action):\n            post_data['actionStatusDescription'] = action['statusDescription']\n        if ('payload' in action):\n            post_data['actionPayload'] = action['payload']\n        if ('setHeaders' in action):\n            action['headers'] = action.pop('setHeaders')\n    return post_data\n", "label": 1}
{"function": "\n\ndef publish(self):\n    super(PyFS, self).publish()\n    deploy_fs = OSFS(self.site.config.deploy_root_path.path)\n    for (dirnm, local_filenms) in deploy_fs.walk():\n        logger.info('Making directory: %s', dirnm)\n        self.fs.makedir(dirnm, allow_recreate=True)\n        remote_fileinfos = self.fs.listdirinfo(dirnm, files_only=True)\n        for filenm in local_filenms:\n            filepath = pathjoin(dirnm, filenm)\n            for (nm, info) in remote_fileinfos:\n                if (nm == filenm):\n                    break\n            else:\n                info = {\n                    \n                }\n            if (self.check_etag and ('etag' in info)):\n                with deploy_fs.open(filepath, 'rb') as f:\n                    local_etag = self._calculate_etag(f)\n                if (info['etag'] == local_etag):\n                    logger.info('Skipping file [etag]: %s', filepath)\n                    continue\n            if (self.check_mtime and ('modified_time' in info)):\n                local_mtime = deploy_fs.getinfo(filepath)['modified_time']\n                if (info['modified_time'] > local_mtime):\n                    logger.info('Skipping file [mtime]: %s', filepath)\n                    continue\n            logger.info('Uploading file: %s', filepath)\n            with deploy_fs.open(filepath, 'rb') as f:\n                self.fs.setcontents(filepath, f)\n        for (filenm, info) in remote_fileinfos:\n            filepath = pathjoin(dirnm, filenm)\n            if (filenm not in local_filenms):\n                logger.info('Removing file: %s', filepath)\n                self.fs.remove(filepath)\n", "label": 1}
{"function": "\n\ndef _ask(self, stage, args, tag):\n    self._report_driver.report_sync(tag, report_type=stage)\n    while True:\n        answer = raw_input('Continue? ([d]etailed/[C]oncise report,[y]es,[n]o,[r]etry): ')\n        if ((not answer) or (answer == 'c') or (answer == 'C')):\n            self._report_driver.report_sync(tag, report_type=stage)\n        elif ((answer == 'd') or (answer == 'D')):\n            self._report_driver.report_sync(tag, report_type=stage, detailed=True)\n        elif ((answer == 'Y') or (answer == 'y')):\n            return True\n        elif ((answer == 'N') or (answer == 'n')):\n            return False\n        elif ((answer == 'R') or (answer == 'r')):\n            if (stage == 'fetch'):\n                self._fetch(args)\n            if (stage == 'checkout'):\n                self._checkout(args)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.privilege = TSentryPrivilege()\n                self.privilege.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef group_members(self, r, writes, owner):\n    if (r.isOpen('r') == 'r'):\n        gr = ''.join((('-%s' % e) for e in r.exceptions if (not (e == owner))))\n    else:\n        gr = '+'.join((e for e in r.exceptions if (not (e == owner))))\n    gw = []\n    for w in writes:\n        if (w.isOpen('w') == 'w'):\n            g = ''.join((('-%s' % e) for e in w.exceptions if (not (e == owner))))\n        else:\n            g = '+'.join((e for e in w.exceptions if (not (e == owner))))\n        gw.append(g)\n    gw = (gw[0] if all(((w == gw[0]) for w in gw)) else '<variable>')\n    if (gw == ''):\n        gw = '(world)'\n    if (gr == ''):\n        gr = '(world)'\n    if (gw == gr):\n        gs = gr\n    else:\n        gs = ('r:%s  w:%s' % (gr, gw))\n    return gs\n", "label": 1}
{"function": "\n\ndef unsubscribe(self, subscriber, timeout=None):\n    \"Must be called with 'yield' as, for example,\\n        'yield channel.unsubscribe(coro)'.\\n\\n        Future messages will not be delivered after unsubscribing.\\n\\n        Can also be used on remote channels.\\n        \"\n    if ((not isinstance(subscriber, Coro)) and (not isinstance(subscriber, Channel))):\n        logger.warning('invalid subscriber ignored')\n        raise StopIteration((- 1))\n    if (self._location == Channel._asyncoro._location):\n        if (subscriber._location != self._location):\n            if isinstance(subscriber, Coro):\n                subscriber._id = int(subscriber._id)\n                for s in self._subscribers:\n                    if (isinstance(s, Coro) and (s._id == subscriber._id) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n            elif isinstance(subscriber, Channel):\n                for s in self._subscribers:\n                    if (isinstance(s, Channel) and (s._name == subscriber._name) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n        try:\n            self._subscribers.remove(subscriber)\n        except KeyError:\n            reply = (- 1)\n        else:\n            reply = 0\n    else:\n        kwargs = {\n            'channel': self._name,\n        }\n        kwargs['subscriber'] = subscriber\n        request = _NetRequest('unsubscribe', kwargs=kwargs, dst=self._location, timeout=timeout)\n        reply = (yield Channel._asyncoro._sync_reply(request))\n    raise StopIteration(reply)\n", "label": 1}
{"function": "\n\ndef data(self, index, role=Qt.DisplayRole):\n    if ((not index.isValid()) or (not (0 <= index.row() < len(self.packages)))):\n        return to_qvariant()\n    package = self.packages[index.row()]\n    column = index.column()\n    if ((role == Qt.CheckStateRole) and (column == CHECK)):\n        return to_qvariant((package in self.checked))\n    elif (role == Qt.DisplayRole):\n        if (column == NAME):\n            return to_qvariant(package.name)\n        elif (column == VERSION):\n            return to_qvariant(package.version)\n        elif (column == ACTION):\n            action = self.actions.get(package)\n            if (action is not None):\n                return to_qvariant(action)\n        elif (column == DESCRIPTION):\n            return to_qvariant(package.description)\n    elif (role == Qt.TextAlignmentRole):\n        if (column == ACTION):\n            return to_qvariant(int((Qt.AlignRight | Qt.AlignVCenter)))\n        else:\n            return to_qvariant(int((Qt.AlignLeft | Qt.AlignVCenter)))\n    elif (role == Qt.BackgroundColorRole):\n        if (package in self.checked):\n            color = QColor(Qt.darkGreen)\n            color.setAlphaF(0.1)\n            return to_qvariant(color)\n        else:\n            color = QColor(Qt.lightGray)\n            color.setAlphaF(0.3)\n            return to_qvariant(color)\n    return to_qvariant()\n", "label": 1}
{"function": "\n\ndef _get_focus_next(self, focus_dir):\n    current = self\n    walk_tree = ('walk' if (focus_dir is 'focus_next') else 'walk_reverse')\n    while 1:\n        while (getattr(current, focus_dir) is not None):\n            current = getattr(current, focus_dir)\n            if ((current is self) or (current is StopIteration)):\n                return None\n            if (current.is_focusable and (not current.disabled)):\n                return current\n        itr = getattr(current, walk_tree)(loopback=True)\n        if (focus_dir is 'focus_next'):\n            next(itr)\n        for current in itr:\n            if isinstance(current, FocusBehavior):\n                break\n        if isinstance(current, FocusBehavior):\n            if (current is self):\n                return None\n            if (current.is_focusable and (not current.disabled)):\n                return current\n        else:\n            return None\n", "label": 1}
{"function": "\n\ndef _create_result(cmd, params):\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ)\n    result = Result()\n    for line in p.stdout.readlines():\n        if (sys.version_info[0] == 3):\n            line = line.decode(sys.stdout.encoding)\n        result._add_stdout_line(line)\n    for line in p.stderr.readlines():\n        if (sys.version_info[0] == 3):\n            line = line.decode(sys.stderr.encoding)\n        result._add_stderr_line(line)\n    p.wait()\n    if ((_is_param_set(params, _PARAM_PRINT_STDOUT) or config.PRINT_STDOUT_ALWAYS) and (len(result.stdout) > 0)):\n        _print_stdout(result.stdout)\n    if ((_is_param_set(params, _PARAM_PRINT_STDERR) or config.PRINT_STDERR_ALWAYS) and (len(result.stderr) > 0)):\n        if _is_colorama_enabled():\n            _print_stderr(((Fore.RED + result.stderr) + Style.RESET_ALL))\n        else:\n            _print_stderr(result.stderr)\n    result.returncode = p.returncode\n    if ((p.returncode != 0) and (not _is_param_set(params, _PARAM_NO_THROW))):\n        raise NonZeroReturnCodeError(cmd, result)\n    return result\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    foo = 'aaa\\naaa\\naaa\\n'\n    result = list(chunked(foo, 5))\n    assert (len(result) == 3)\n    assert (result[0] == 'aaa\\n')\n    assert (result[1] == 'aaa\\n')\n    assert (result[2] == 'aaa\\n')\n    result = list(chunked(foo, 8))\n    assert (len(result) == 2)\n    assert (result[0] == 'aaa\\naaa\\n')\n    assert (result[1] == 'aaa\\n')\n    result = list(chunked(foo, 4))\n    assert (len(result) == 3)\n    assert (result[0] == 'aaa\\n')\n    assert (result[1] == 'aaa\\n')\n    assert (result[2] == 'aaa\\n')\n    foo = ('a' * 10)\n    result = list(chunked(foo, 2))\n    assert (len(result) == 5)\n    assert all(((r == 'aa') for r in result))\n    foo = 'aaaa\\naaaa'\n    result = list(chunked(foo, 3))\n    assert (len(result) == 4)\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kw):\n    super(DateConverter, self).__init__(*args, **kw)\n    month_style = (self.month_style or DateConverter.month_style).lower()\n    accept_day = bool(self.accept_day)\n    self.accept_day = self.accept_day\n    if (month_style in ('mdy', 'md', 'mm/dd/yyyy', 'mm/dd', 'us', 'american')):\n        month_style = 'mdy'\n    elif (month_style in ('dmy', 'dm', 'dd/mm/yyyy', 'dd/mm', 'euro', 'european')):\n        month_style = 'dmy'\n    elif (month_style in ('ymd', 'ym', 'yyyy/mm/dd', 'yyyy/mm', 'iso', 'china', 'chinese')):\n        month_style = 'ymd'\n    else:\n        raise TypeError(('Bad month_style: %r' % month_style))\n    self.month_style = month_style\n    separator = self.separator\n    if ((not separator) or (separator == 'auto')):\n        separator = dict(mdy='/', dmy='.', ymd='-')[month_style]\n    elif (separator not in ('-', '.', '/', '\\\\')):\n        raise TypeError(('Bad separator: %r' % separator))\n    self.separator = separator\n    self.format = separator.join((self._formats[part] for part in month_style if ((part != 'd') or accept_day)))\n    self.human_format = separator.join((self._human_formats[part] for part in month_style if ((part != 'd') or accept_day)))\n", "label": 1}
{"function": "\n\ndef _parse_see_also(self, content):\n    '\\n        func_name : Descriptive text\\n            continued text\\n        another_func_name : Descriptive text\\n        func_name1, func_name2, func_name3\\n\\n        '\n    functions = []\n    current_func = None\n    rest = []\n    for line in content:\n        if (not line.strip()):\n            continue\n        if (':' in line):\n            if current_func:\n                functions.append((current_func, rest))\n            r = line.split(':', 1)\n            current_func = r[0].strip()\n            r[1] = r[1].strip()\n            if r[1]:\n                rest = [r[1]]\n            else:\n                rest = []\n        elif (not line.startswith(' ')):\n            if current_func:\n                functions.append((current_func, rest))\n                current_func = None\n                rest = []\n            if (',' in line):\n                for func in line.split(','):\n                    func = func.strip()\n                    if func:\n                        functions.append((func, []))\n            elif line.strip():\n                current_func = line.strip()\n        elif (current_func is not None):\n            rest.append(line.strip())\n    if current_func:\n        functions.append((current_func, rest))\n    return functions\n", "label": 1}
{"function": "\n\ndef _subx(self, template, string, count=0, subn=False):\n    filter = template\n    if ((not callable(template)) and ('\\\\' in template)):\n        import re as sre\n        filter = sre._subx(self, template)\n    state = _State(string, 0, sys.maxsize, self.flags)\n    sublist = []\n    n = last_pos = 0\n    while ((not count) or (n < count)):\n        state.reset()\n        state.string_position = state.start\n        if (not state.search(self._code)):\n            break\n        if (last_pos < state.start):\n            sublist.append(string[last_pos:state.start])\n        if (not ((last_pos == state.start) and (last_pos == state.string_position) and (n > 0))):\n            if callable(filter):\n                sublist.append(filter(SRE_Match(self, state)))\n            else:\n                sublist.append(filter)\n            last_pos = state.string_position\n            n += 1\n        if (state.string_position == state.start):\n            state.start += 1\n        else:\n            state.start = state.string_position\n    if (last_pos < state.end):\n        sublist.append(string[last_pos:state.end])\n    item = ''.join(sublist)\n    if subn:\n        return (item, n)\n    else:\n        return item\n", "label": 1}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-d', '--delay', default=0, type=int)\n    args = parser.parse_args()\n    if sys.stdin.isatty():\n        parser.error('no input, pipe another btc command output into this command')\n    torrents = sys.stdin.read()\n    if (len(torrents.strip()) == 0):\n        exit(1)\n    try:\n        torrents = decoder.decode(torrents)\n    except ValueError:\n        error(('unexpected input: %s' % torrents))\n    time.sleep(args.delay)\n    hashes = [t['hash'] for t in torrents]\n    for h in hashes:\n        client.start_torrent(h)\n    while True:\n        d = list_to_dict(client.list_torrents(), 'hash')\n        all_started = True\n        for h in d:\n            if (h not in hashes):\n                continue\n            if (d[h]['state'] not in ('DOWNLOADING', 'DOWNLOADING_FORCED', 'SEEDING', 'SEEDING_FORCED', 'QUEUED_SEED')):\n                all_started = False\n                break\n        if all_started:\n            break\n        time.sleep(1)\n    if (not sys.stdout.isatty()):\n        d = list_to_dict(client.list_torrents(), 'hash')\n        d = dict(((h, d[h]) for h in hashes if (h in d)))\n        print(encoder.encode(dict_to_list(d, 'hash')))\n", "label": 1}
{"function": "\n\ndef normpath(path):\n    'Normalize path, eliminating double slashes, etc.'\n    (slash, dot) = (('/', '.') if isinstance(path, unicode) else ('/', '.'))\n    if (path == ''):\n        return dot\n    initial_slashes = path.startswith('/')\n    if (initial_slashes and path.startswith('//') and (not path.startswith('///'))):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if (comp in ('', '.')):\n            continue\n        if ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(- 1)] == '..'))):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = ((slash * initial_slashes) + path)\n    return (path or dot)\n", "label": 1}
{"function": "\n\ndef _validate_policies(self, policies):\n    '\\n        :param policies: list of policies\\n        '\n    for policy in policies:\n        if (int(policy) in self.by_index):\n            raise PolicyError(('Duplicate index %s conflicts with %s' % (policy, self.get_by_index(int(policy)))))\n        for name in policy.alias_list:\n            if (name.upper() in self.by_name):\n                raise PolicyError(('Duplicate name %s conflicts with %s' % (policy, self.get_by_name(name))))\n        if policy.is_default:\n            if (not self.default):\n                self.default = policy\n            else:\n                raise PolicyError(('Duplicate default %s conflicts with %s' % (policy, self.default)))\n        self._add_policy(policy)\n    if (0 not in self.by_index):\n        if (len(self) != 0):\n            raise PolicyError('You must specify a storage policy section for policy index 0 in order to define multiple policies')\n        self._add_policy(StoragePolicy(0, name=LEGACY_POLICY_NAME))\n    enabled_policies = [p for p in self if (not p.is_deprecated)]\n    if (not enabled_policies):\n        raise PolicyError(\"Unable to find policy that's not deprecated!\")\n    if (not self.default):\n        if (len(self) > 1):\n            raise PolicyError('Unable to find default policy')\n        self.default = self[0]\n        self.default.is_default = True\n", "label": 1}
{"function": "\n\ndef vote(self):\n    if self.lock:\n        return\n    self.log('in vote', proposal=self.proposal, pid=id(self.proposal))\n    last_lock = self.hm.last_lock\n    if self.proposal:\n        if isinstance(self.proposal, VotingInstruction):\n            assert self.proposal.lockset.has_quorum_possible\n            self.log('voting on instruction')\n            v = VoteBlock(self.height, self.round, self.proposal.blockhash)\n        elif (not isinstance(last_lock, VoteBlock)):\n            assert isinstance(self.proposal, BlockProposal)\n            assert isinstance(self.proposal.block, Block)\n            assert (self.proposal.lockset.has_noquorum or (self.round == 0))\n            assert (self.proposal.block.prevhash == self.cm.head.hash)\n            self.log('voting proposed block')\n            v = VoteBlock(self.height, self.round, self.proposal.blockhash)\n        else:\n            self.log('voting on last vote')\n            v = VoteBlock(self.height, self.round, last_lock.blockhash)\n    elif ((self.timeout_time is not None) and (self.cm.chainservice.now >= self.timeout_time)):\n        if isinstance(last_lock, VoteBlock):\n            self.log('timeout voting on last vote')\n            v = VoteBlock(self.height, self.round, last_lock.blockhash)\n        else:\n            self.log('timeout voting not locked')\n            v = VoteNil(self.height, self.round)\n    else:\n        return\n    self.cm.sign(v)\n    self.log('voted', vote=v)\n    self.lock = v\n    assert (self.hm.last_lock == self.lock)\n    self.lockset.add(v)\n    return v\n", "label": 1}
{"function": "\n\ndef _prepare_imports(self, dicts):\n    ' an override for prepare imports that sorts the imports by parent_id dependencies '\n    pseudo_ids = set()\n    pseudo_matches = {\n        \n    }\n    prepared = dict(super(OrganizationImporter, self)._prepare_imports(dicts))\n    for (_, data) in prepared.items():\n        parent_id = (data.get('parent_id', None) or '')\n        if parent_id.startswith('~'):\n            pseudo_ids.add(parent_id)\n    pseudo_ids = [(ppid, get_pseudo_id(ppid)) for ppid in pseudo_ids]\n    for (json_id, data) in prepared.items():\n        for (ppid, spec) in pseudo_ids:\n            match = True\n            for (k, v) in spec.items():\n                if (data[k] != v):\n                    match = False\n                    break\n            if match:\n                if (ppid in pseudo_matches):\n                    raise UnresolvedIdError(('multiple matches for pseudo id: ' + ppid))\n                pseudo_matches[ppid] = json_id\n    network = Network()\n    in_network = set()\n    import_order = []\n    for (json_id, data) in prepared.items():\n        parent_id = data.get('parent_id', None)\n        if (parent_id in pseudo_matches):\n            parent_id = pseudo_matches[parent_id]\n        network.add_node(json_id)\n        if parent_id:\n            network.add_edge(parent_id, json_id)\n    for jid in network.sort():\n        import_order.append((jid, prepared[jid]))\n        in_network.add(jid)\n    if (in_network != set(prepared.keys())):\n        raise PupaInternalError('import is missing nodes in network set')\n    return import_order\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    if (key is ()):\n        return self\n    data_slice = None\n    if isinstance(key, tuple):\n        data_slice = key[1:]\n        key = key[0]\n    if (isinstance(key, int) and (key <= len(self))):\n        if (key == 0):\n            data = self.main\n        if (key == 1):\n            data = self.right\n        if (key == 2):\n            data = self.top\n        if data_slice:\n            data = data[data_slice]\n        return data\n    elif (isinstance(key, str) and (key in self.data)):\n        if (data_slice is None):\n            return self.data[key]\n        else:\n            self.data[key][data_slice]\n    elif (isinstance(key, slice) and (key.start is None) and (key.stop is None)):\n        return (self if (data_slice is None) else self.clone([el[data_slice] for el in self]))\n    else:\n        raise KeyError('Key {0} not found in AdjointLayout.'.format(key))\n", "label": 1}
{"function": "\n\ndef _load(b, classes):\n    identifier = b[0]\n    if isinstance(identifier, str):\n        identifier = ord(identifier)\n    if (identifier == _SPEC):\n        return _load_spec(b)\n    elif (identifier == _INT_32):\n        return _load_int_32(b)\n    elif ((identifier == _INT) or (identifier == _INT_NEG)):\n        return _load_int(b)\n    elif (identifier == _FLOAT):\n        return _load_float(b)\n    elif (identifier == _COMPLEX):\n        return _load_complex(b)\n    elif (identifier == _STR):\n        return _load_str(b)\n    elif (identifier == _BYTES):\n        return _load_bytes(b)\n    elif (identifier == _TUPLE):\n        return _load_tuple(b, classes)\n    elif (identifier == _NAMEDTUPLE):\n        return _load_namedtuple(b, classes)\n    elif (identifier == _LIST):\n        return _load_list(b, classes)\n    elif (identifier == _NPARRAY):\n        return _load_np_array(b)\n    elif (identifier == _DICT):\n        return _load_dict(b, classes)\n    elif (identifier == _GETSTATE):\n        return _load_getstate(b, classes)\n    elif (identifier == _BFSTATE):\n        raise BFLoadError('BFSTATE objects can not be loaded')\n    else:\n        raise BFLoadError(\"unknown identifier '{}'\".format(hex(identifier)))\n", "label": 1}
{"function": "\n\ndef traverse(obj, *path, **kwargs):\n    '\\n    Traverse the object we receive with the given path. Path\\n    items can be either strings or lists of strings (or any\\n    nested combination thereof). Behavior in given cases is\\n    laid out line by line below.\\n    '\n    if path:\n        if (isinstance(obj, list) or isinstance(obj, tuple)):\n            return [traverse(x, *path) for x in obj]\n        elif isinstance(obj, dict):\n            if (isinstance(path[0], list) or isinstance(path[0], tuple)):\n                for branch in path[0]:\n                    if (not isinstance(branch, basestring)):\n                        raise TraversalError(obj, path[0])\n                return {name: traverse(obj[name], *path[1:], split=True) for name in path[0]}\n            elif (not isinstance(path[0], basestring)):\n                raise TraversalError(obj, path[0])\n            elif (path[0] == '\\\\*'):\n                return {name: traverse(item, *path[1:], split=True) for (name, item) in obj.items()}\n            elif (path[0] in obj):\n                return traverse(obj[path[0]], *path[1:])\n            else:\n                raise TraversalError(obj, path[0])\n        elif kwargs.get('split', False):\n            return obj\n        else:\n            raise TraversalError(obj, path[0])\n    else:\n        return obj\n", "label": 1}
{"function": "\n\ndef _check_command_response(response, reset, msg=None, allowable_errors=None):\n    'Check the response to a command for errors.\\n    '\n    if ('ok' not in response):\n        raise OperationFailure(response.get('$err'), response.get('code'), response)\n    if response.get('wtimeout', False):\n        raise WTimeoutError(response.get('errmsg', response.get('err')), response.get('code'), response)\n    if (not response['ok']):\n        details = response\n        if ('raw' in response):\n            for shard in response['raw'].itervalues():\n                if (not shard.get('ok')):\n                    details = shard\n                    break\n        errmsg = details['errmsg']\n        if ((allowable_errors is None) or (errmsg not in allowable_errors)):\n            if (errmsg.startswith('not master') or errmsg.startswith('node is recovering')):\n                if (reset is not None):\n                    reset()\n                raise AutoReconnect(errmsg)\n            if (errmsg == 'db assertion failure'):\n                errmsg = (\"db assertion failure, assertion: '%s'\" % details.get('assertion', ''))\n                raise OperationFailure(errmsg, details.get('assertionCode'), response)\n            code = details.get('code')\n            if (code in (11000, 11001, 12582)):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif (code == 50):\n                raise ExecutionTimeout(errmsg, code, response)\n            msg = (msg or '%s')\n            raise OperationFailure((msg % errmsg), code, response)\n", "label": 1}
{"function": "\n\ndef cancelUpload(self, upload):\n    '\\n        Delete the temporary files associated with a given upload.\\n        '\n    if ('s3' not in upload):\n        return\n    if ('key' not in upload['s3']):\n        return\n    bucket = self._getBucket()\n    if bucket:\n        key = bucket.get_key(upload['s3']['key'], validate=True)\n        if key:\n            bucket.delete_key(key)\n        if (('s3' in upload) and ('uploadId' in upload['s3']) and ('key' in upload['s3'])):\n            getParams = {\n                \n            }\n            while True:\n                try:\n                    multipartUploads = bucket.get_all_multipart_uploads(**getParams)\n                except boto.exception.S3ResponseError:\n                    break\n                if (not len(multipartUploads)):\n                    break\n                for multipartUpload in multipartUploads:\n                    if ((multipartUpload.id == upload['s3']['uploadId']) and (multipartUpload.key_name == upload['s3']['key'])):\n                        multipartUpload.cancel_upload()\n                if (not multipartUploads.is_truncated):\n                    break\n                getParams['key_marker'] = multipartUploads.next_key_marker\n                getParams['upload_id_marker'] = multipartUploads.next_upload_id_marker\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.schema = hive_metastore.ttypes.Schema()\n                self.schema.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.table_dir = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.in_tablename = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.delim = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    self.use_greenlets = False\n    self.auto_start_request = False\n    kwargs.pop('auto_start_request', None)\n    if (('read_preference' not in kwargs) and (kwargs.get('tag_sets') not in (None, [], [{\n        \n    }]))):\n        raise ConfigurationError()\n    if (('read_preference' not in kwargs) and (('slaveok' in kwargs) or ('slave_okay' in kwargs))):\n        secondary = kwargs.pop('slave_okay', kwargs.pop('slaveok', False))\n        kwargs['read_preference'] = (ReadPreference.SECONDARY_PREFERRED if secondary else ReadPreference.PRIMARY)\n    gle_opts = dict([(k, v) for (k, v) in kwargs.items() if (k in SAFE_OPTIONS)])\n    if (gle_opts and ('w' not in gle_opts)):\n        kwargs['w'] = 1\n    if ('safe' in kwargs):\n        safe = kwargs.pop('safe')\n        if (not safe):\n            kwargs.setdefault('w', 0)\n    self.delegate = kwargs.pop('delegate', None)\n    if (not self.delegate):\n        self.delegate = self.__delegate_class__(*args, **kwargs)\n        if kwargs.get('_connect', True):\n            self.synchro_connect()\n", "label": 1}
{"function": "\n\ndef poll(self, timeout):\n    self._lock.acquire()\n    if (timeout == 0):\n        self.poll_timeout = 0\n    elif self._timeouts:\n        self.poll_timeout = (self._timeouts[0][0] - _time())\n        if (self.poll_timeout < 0.0001):\n            self.poll_timeout = 0\n        elif (timeout is not None):\n            self.poll_timeout = min(timeout, self.poll_timeout)\n    elif (timeout is None):\n        self.poll_timeout = _AsyncNotifier._Block\n    else:\n        self.poll_timeout = timeout\n    timeout = self.poll_timeout\n    self._lock.release()\n    if (timeout and (timeout != _AsyncNotifier._Block)):\n        timeout = int((timeout * 1000))\n    (err, n, key, overlap) = win32file.GetQueuedCompletionStatus(self.iocp, timeout)\n    while (err != winerror.WAIT_TIMEOUT):\n        if (overlap and overlap.object):\n            overlap.object(err, n)\n        else:\n            logger.warning('invalid overlap!')\n        (err, n, key, overlap) = win32file.GetQueuedCompletionStatus(self.iocp, 0)\n    self.poll_timeout = 0\n    if (timeout == 0):\n        now = _time()\n        self._lock.acquire()\n        while (self._timeouts and (self._timeouts[0][0] <= now)):\n            (fd_timeout, fd) = self._timeouts.pop(0)\n            if (fd._timeout_id == fd_timeout):\n                fd._timeout_id = None\n                fd._timed_out()\n        self._lock.release()\n", "label": 1}
{"function": "\n\n@classmethod\ndef put(cls, kvs, entity=None):\n    for k in kvs.keys():\n        if (not (k in cls.properties())):\n            del kvs[k]\n            continue\n        v = cls.__dict__[k]\n        if isinstance(v, db.IntegerProperty):\n            kvs[k] = int(kvs[k])\n        elif isinstance(v, db.FloatProperty):\n            kvs[k] = float(kvs[k])\n        elif isinstance(v, db.BooleanProperty):\n            kvs[k] = (True if (kvs[k] == 'True') else False)\n        elif (isinstance(v, db.StringProperty) or isinstance(v, db.TextProperty)):\n            kvs[k] = kvs[k]\n        elif isinstance(v, db.DateProperty):\n            kvs[k] = datetime.strptime(kvs[k], settings.DATE_FORMAT).date()\n        elif isinstance(v, db.LinkProperty):\n            kvs[k] = kvs[k]\n        elif isinstance(v, db.EmailProperty):\n            kvs[k] = kvs[k]\n        else:\n            raise UnsupportedFieldTypeError(v)\n    cls.validate(kvs)\n    try:\n        if (not entity):\n            entity = cls(**kvs)\n        else:\n            for k in kvs.keys():\n                setattr(entity, k, kvs[k])\n        db.put(entity)\n    except Exception as e:\n        logging.error((\"Couldn't put entity: %s\" % kvs))\n        logging.error(e)\n        return None\n    return entity\n", "label": 1}
{"function": "\n\ndef MoveWindow(windowID, xpos=None, ypos=None, width=None, height=None, center=None):\n    (left, top, right, bottom) = win32gui.GetWindowRect(windowID)\n    if ((xpos is None) and (ypos is None)):\n        xpos = left\n        ypos = top\n    if ((width is None) and (height is None)):\n        width = (right - left)\n        height = (bottom - top)\n    if ((xpos is None) and (ypos is not None)):\n        xpos = left\n    if ((ypos is None) and (xpos is not None)):\n        ypos = top\n    if (not width):\n        width = (right - left)\n    if (not height):\n        height = (bottom - top)\n    if center:\n        screenx = win32api.GetSystemMetrics(win32con.SM_CXSCREEN)\n        screeny = win32api.GetSystemMetrics(win32con.SM_CYSCREEN)\n        xpos = int(math.floor(((screenx - width) / 2)))\n        ypos = int(math.floor(((screeny - height) / 2)))\n        if (xpos < 0):\n            xpos = 0\n        if (ypos < 0):\n            ypos = 0\n    win32gui.MoveWindow(windowID, xpos, ypos, width, height, 1)\n", "label": 1}
{"function": "\n\ndef node_clique_number(G, nodes=None, cliques=None):\n    ' Returns the size of the largest maximal clique containing\\n    each given node.\\n\\n    Returns a single or list depending on input nodes.\\n    Optional list of cliques can be input if already computed.\\n    '\n    if (cliques is None):\n        if (nodes is not None):\n            if isinstance(nodes, list):\n                d = {\n                    \n                }\n                for n in nodes:\n                    H = networkx.ego_graph(G, n)\n                    d[n] = max((len(c) for c in find_cliques(H)))\n            else:\n                H = networkx.ego_graph(G, nodes)\n                d = max((len(c) for c in find_cliques(H)))\n            return d\n        cliques = list(find_cliques(G))\n    if (nodes is None):\n        nodes = list(G.nodes())\n    if (not isinstance(nodes, list)):\n        v = nodes\n        d = max([len(c) for c in cliques if (v in c)])\n    else:\n        d = {\n            \n        }\n        for v in nodes:\n            d[v] = max([len(c) for c in cliques if (v in c)])\n    return d\n", "label": 1}
{"function": "\n\ndef __gt__(self, other):\n    if (self.date == 'infinity'):\n        if isinstance(other, Date):\n            return (other.date != 'infinity')\n        else:\n            from .Range import Range\n            if isinstance(other, Range):\n                return (other.end != 'infinity')\n            return (other != 'infinity')\n    elif isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return False\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) > other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date > other.date.replace(tzinfo=self.tz))\n        return (self.date > other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            if (other.end.date == 'infinity'):\n                return False\n            if (other.end.tz and (self.tz is None)):\n                return (self.date.replace(tzinfo=other.end.tz) > other.end.date)\n            elif (self.tz and (other.end.tz is None)):\n                return (self.date > other.end.date.replace(tzinfo=self.tz))\n            return (self.date > other.end.date)\n        else:\n            return self.__gt__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args):\n    ' Construct a Trace object.\\n\\n        Parameters\\n        ==========\\n        args = sympy expression\\n        indices = tuple/list if indices, optional\\n\\n        '\n    if (len(args) == 2):\n        if (not isinstance(args[1], (list, Tuple, tuple))):\n            indices = Tuple(args[1])\n        else:\n            indices = Tuple(*args[1])\n        expr = args[0]\n    elif (len(args) == 1):\n        indices = Tuple()\n        expr = args[0]\n    else:\n        raise ValueError('Arguments to Tr should be of form (expr[, [indices]])')\n    if isinstance(expr, Matrix):\n        return expr.trace()\n    elif (hasattr(expr, 'trace') and callable(expr.trace)):\n        return expr.trace()\n    elif isinstance(expr, Add):\n        return Add(*[Tr(arg, indices) for arg in expr.args])\n    elif isinstance(expr, Mul):\n        (c_part, nc_part) = expr.args_cnc()\n        if (len(nc_part) == 0):\n            return Mul(*c_part)\n        else:\n            obj = Expr.__new__(cls, Mul(*nc_part), indices)\n            return ((Mul(*c_part) * obj) if (len(c_part) > 0) else obj)\n    elif isinstance(expr, Pow):\n        if (_is_scalar(expr.args[0]) and _is_scalar(expr.args[1])):\n            return expr\n        else:\n            return Expr.__new__(cls, expr, indices)\n    else:\n        if _is_scalar(expr):\n            return expr\n        return Expr.__new__(cls, expr, indices)\n", "label": 1}
{"function": "\n\ndef process_pdu(self, pdu):\n    ' Process a PDU by sending a copy to each node as dictated by the\\n            addressing and if a node is promiscuous.\\n        '\n    if _debug:\n        Network._debug('process_pdu %r', pdu)\n    if (self.dropPercent != 0.0):\n        if ((random.random() * 100.0) < self.dropPercent):\n            if _debug:\n                Network._debug('    - packet dropped')\n            return\n    if ((not pdu.pduDestination) or (not isinstance(pdu.pduDestination, Address))):\n        raise RuntimeError('invalid destination address')\n    elif (pdu.pduDestination.addrType == Address.localBroadcastAddr):\n        for n in self.nodes:\n            if (pdu.pduSource != n.address):\n                n.response(deepcopy(pdu))\n    elif (pdu.pduDestination.addrType == Address.localStationAddr):\n        for n in self.nodes:\n            if (n.promiscuous or (pdu.pduDestination == n.address)):\n                n.response(deepcopy(pdu))\n    else:\n        raise RuntimeError('invalid destination address type')\n", "label": 1}
{"function": "\n\ndef dispatch(self, request, *args, **kwargs):\n    '\\n        Override dispatch to call appropriate methods:\\n        * $query - ng_query\\n        * $get - ng_get\\n        * $save - ng_save\\n        * $delete and $remove - ng_delete\\n        '\n    allowed_methods = self.get_allowed_methods()\n    try:\n        if ((request.method == 'GET') and ('GET' in allowed_methods)):\n            if (('pk' in request.GET) or (self.slug_field in request.GET)):\n                return self.ng_get(request, *args, **kwargs)\n            return self.ng_query(request, *args, **kwargs)\n        elif ((request.method == 'POST') and ('POST' in allowed_methods)):\n            return self.ng_save(request, *args, **kwargs)\n        elif ((request.method == 'DELETE') and ('DELETE' in allowed_methods)):\n            return self.ng_delete(request, *args, **kwargs)\n    except self.model.DoesNotExist as e:\n        return self.error_json_response(e.args[0], 404)\n    except NgMissingParameterError as e:\n        return self.error_json_response(e.args[0])\n    except JSONResponseException as e:\n        return self.error_json_response(e.args[0], e.status_code)\n    except ValidationError as e:\n        if hasattr(e, 'error_dict'):\n            return self.error_json_response('Form not valid', detail=e.message_dict)\n        else:\n            return self.error_json_response(e.message)\n    return self.error_json_response('This view can not handle method {0}'.format(request.method), 405)\n", "label": 1}
{"function": "\n\ndef __init__(self, options, default):\n    self.default = default\n    (args, kwargs) = ([], {\n        \n    })\n    if options:\n        for (key, val) in OPTIONS_PARSER.findall(options):\n            if (val.lower() == 'none'):\n                val = None\n            elif (val.lower() == 'true'):\n                val = True\n            elif (val.lower() == 'false'):\n                val = False\n            else:\n                try:\n                    val = int(val)\n                except ValueError:\n                    try:\n                        val = float(val)\n                    except ValueError:\n                        pass\n            if isinstance(val, basestring):\n                val = val\n                if (((val[0] == '\"') and (val[(- 1)] == '\"')) or ((val[0] == \"'\") and (val[(- 1)] == \"'\"))):\n                    val = val[1:(- 1)]\n            if key:\n                kwargs[key] = val\n            else:\n                args.append(val)\n    self.configure(*args, **kwargs)\n    self._regex = re.compile(('^%s$' % self.regex))\n    self.capture_groups = self._regex.groups\n", "label": 1}
{"function": "\n\ndef all_media(self, from_apps=None):\n    from corehq.apps.hqmedia.models import CommCareMultimedia\n    dom_with_media = (self if (not self.is_snapshot) else self.copied_from)\n    if self.is_snapshot:\n        app_ids = [app.copied_from.get_id for app in self.full_applications()]\n        if from_apps:\n            from_apps = set([a_id for a_id in app_ids if (a_id in from_apps)])\n        else:\n            from_apps = app_ids\n    if from_apps:\n        media = []\n        media_ids = set()\n        apps = [app for app in dom_with_media.full_applications() if (app.get_id in from_apps)]\n        for app in apps:\n            if (app.doc_type != 'Application'):\n                continue\n            for (_, m) in app.get_media_objects():\n                if (m.get_id not in media_ids):\n                    media.append(m)\n                    media_ids.add(m.get_id)\n        return media\n    return CommCareMultimedia.view('hqmedia/by_domain', key=dom_with_media.name, include_docs=True).all()\n", "label": 1}
{"function": "\n\ndef telescopic(L, R, limits):\n    'Tries to perform the summation using the telescopic property\\n\\n    return None if not possible\\n    '\n    (i, a, b) = limits\n    if (L.is_Add or R.is_Add):\n        return None\n    k = Wild('k')\n    sol = (- R).match(L.subs(i, (i + k)))\n    s = None\n    if (sol and (k in sol)):\n        s = sol[k]\n        if (not (s.is_Integer and (L.subs(i, (i + s)) == (- R)))):\n            s = None\n    if (s is None):\n        m = Dummy('m')\n        try:\n            sol = (solve((L.subs(i, (i + m)) + R), m) or [])\n        except NotImplementedError:\n            return None\n        sol = [si for si in sol if (si.is_Integer and (L.subs(i, (i + si)) + R).expand().is_zero)]\n        if (len(sol) != 1):\n            return None\n        s = sol[0]\n    if (s < 0):\n        return telescopic_direct(R, L, abs(s), (i, a, b))\n    elif (s > 0):\n        return telescopic_direct(L, R, s, (i, a, b))\n", "label": 1}
{"function": "\n\ndef get_OPinv_matvec(A, M, sigma, symmetric=False, tol=0):\n    if (sigma == 0):\n        return get_inv_matvec(A, symmetric=symmetric, tol=tol)\n    if (M is None):\n        if isdense(A):\n            if (np.issubdtype(A.dtype, np.complexfloating) or (np.imag(sigma) == 0)):\n                A = np.copy(A)\n            else:\n                A = (A + 0j)\n            A.flat[::(A.shape[1] + 1)] -= sigma\n            return LuInv(A).matvec\n        elif isspmatrix(A):\n            A = (A - (sigma * identity(A.shape[0])))\n            if (symmetric and isspmatrix_csr(A)):\n                A = A.T\n            return SpLuInv(A.tocsc()).matvec\n        else:\n            return IterOpInv(_aslinearoperator_with_dtype(A), M, sigma, tol=tol).matvec\n    elif (((not isdense(A)) and (not isspmatrix(A))) or ((not isdense(M)) and (not isspmatrix(M)))):\n        return IterOpInv(_aslinearoperator_with_dtype(A), _aslinearoperator_with_dtype(M), sigma, tol=tol).matvec\n    elif (isdense(A) or isdense(M)):\n        return LuInv((A - (sigma * M))).matvec\n    else:\n        OP = (A - (sigma * M))\n        if (symmetric and isspmatrix_csr(OP)):\n            OP = OP.T\n        return SpLuInv(OP.tocsc()).matvec\n", "label": 1}
{"function": "\n\ndef get_linked_doctypes(columns, data):\n    linked_doctypes = {\n        \n    }\n    columns_dict = get_columns_dict(columns)\n    for (idx, col) in enumerate(columns):\n        df = columns_dict[idx]\n        if (df.get('fieldtype') == 'Link'):\n            if isinstance(col, basestring):\n                linked_doctypes[df['options']] = idx\n            else:\n                linked_doctypes[df['options']] = df['fieldname']\n    columns_with_value = []\n    for row in data:\n        if row:\n            if (len(row) != len(columns_with_value)):\n                if isinstance(row, (list, tuple)):\n                    row = enumerate(row)\n                elif isinstance(row, dict):\n                    row = row.items()\n                for (col, val) in row:\n                    if (val and (col not in columns_with_value)):\n                        columns_with_value.append(col)\n    for (doctype, key) in linked_doctypes.items():\n        if (key not in columns_with_value):\n            del linked_doctypes[doctype]\n    return linked_doctypes\n", "label": 1}
{"function": "\n\ndef doWaitForMultipleEvents(self, timeout):\n    log.msg(channel='system', event='iteration', reactor=self)\n    if (timeout is None):\n        timeout = 100\n    ranUserCode = False\n    for reader in self._closedAndReading.keys():\n        ranUserCode = True\n        self._runAction('doRead', reader)\n    for fd in self._writes.keys():\n        ranUserCode = True\n        log.callWithLogger(fd, self._runWrite, fd)\n    if ranUserCode:\n        timeout = 0\n    if (not (self._events or self._writes)):\n        time.sleep(timeout)\n        return\n    handles = (self._events.keys() or [self.dummyEvent])\n    timeout = int((timeout * 1000))\n    val = MsgWaitForMultipleObjects(handles, 0, timeout, QS_ALLINPUT)\n    if (val == WAIT_TIMEOUT):\n        return\n    elif (val == (WAIT_OBJECT_0 + len(handles))):\n        exit = win32gui.PumpWaitingMessages()\n        if exit:\n            self.callLater(0, self.stop)\n            return\n    elif ((val >= WAIT_OBJECT_0) and (val < (WAIT_OBJECT_0 + len(handles)))):\n        event = handles[(val - WAIT_OBJECT_0)]\n        (fd, action) = self._events[event]\n        if (fd in self._reads):\n            fileno = fd.fileno()\n            if (fileno == (- 1)):\n                self._disconnectSelectable(fd, posixbase._NO_FILEDESC, False)\n                return\n            events = WSAEnumNetworkEvents(fileno, event)\n            if (FD_CLOSE in events):\n                self._closedAndReading[fd] = True\n        log.callWithLogger(fd, self._runAction, action, fd)\n", "label": 1}
{"function": "\n\ndef mix_codes(self, agents, actors):\n    '\\n        Combine the actor codes and agent codes addressing duplicates\\n        and removing the general \"~PPL\" if there\\'s a better option.\\n        \\n        Parameters\\n        -----------\\n        agents, actors : Lists of their respective codes\\n        \\n        \\n        Returns\\n        -------\\n        codes: list\\n               [Agent codes] x [Actor codes]\\n        \\n        '\n    codes = set()\n    mix = (lambda a, b: ((a + b) if (not (b in a)) else a))\n    actors = (actors if actors else ['~'])\n    for ag in agents:\n        if ((ag == '~PPL') and (len(agents) > 1)):\n            continue\n        actors = map((lambda a: mix(a, ag[1:])), actors)\n    return filter((lambda a: (a not in ['', '~', '~~', None])), actors)\n    codes = set()\n    print('WTF-1')\n    for act in (actors if actors else ['~']):\n        for ag in (agents if agents else ['~']):\n            if ((ag == '~PPL') and (len(agents) > 1)):\n                continue\n            code = act\n            if (not (ag[1:] in act)):\n                code += ag[1:]\n            if (not (code in ['~', '~~', ''])):\n                codes.add(code)\n    return list(codes)\n", "label": 1}
{"function": "\n\ndef _read_object(self, relpath, max_symlinks):\n    path_so_far = ''\n    components = list(relpath.split(os.path.sep))\n    symlinks = 0\n    while components:\n        component = components.pop(0)\n        if ((component == '') or (component == '.')):\n            continue\n        parent_tree = self._read_tree(path_so_far)\n        parent_path = path_so_far\n        if (path_so_far != ''):\n            path_so_far += '/'\n        path_so_far += component\n        try:\n            obj = parent_tree[component]\n        except KeyError:\n            raise self.MissingFileException(self.rev, relpath)\n        if isinstance(obj, self.File):\n            if components:\n                raise self.NotADirException(self.rev, relpath)\n            else:\n                return (obj, path_so_far)\n        elif isinstance(obj, self.Dir):\n            if (not components):\n                return (obj, (path_so_far + '/'))\n        elif isinstance(obj, self.Symlink):\n            symlinks += 1\n            if (symlinks > max_symlinks):\n                return (obj, path_so_far)\n            (object_type, path_data) = self._read_object_from_repo(sha=obj.sha)\n            assert (object_type == 'blob')\n            if (path_data[0] == '/'):\n                raise self.ExternalSymlinkException(self.rev, relpath)\n            link_to = os.path.normpath(os.path.join(parent_path, path_data))\n            if (link_to.startswith('../') or (link_to[0] == '/')):\n                raise self.ExternalSymlinkException(self.rev, relpath)\n            components = (link_to.split(SLASH) + components)\n            path_so_far = ''\n        else:\n            raise self.UnexpectedGitObjectTypeException()\n    return (self.Dir('./', None), './')\n", "label": 1}
{"function": "\n\ndef update(self):\n    'Get the latest data from Transmission and updates the state.'\n    self.refresh_transmission_data()\n    if (self.type == 'current_status'):\n        if self.transmission_client.session:\n            upload = self.transmission_client.session.uploadSpeed\n            download = self.transmission_client.session.downloadSpeed\n            if ((upload > 0) and (download > 0)):\n                self._state = 'Up/Down'\n            elif ((upload > 0) and (download == 0)):\n                self._state = 'Seeding'\n            elif ((upload == 0) and (download > 0)):\n                self._state = 'Downloading'\n            else:\n                self._state = 'Idle'\n        else:\n            self._state = 'Unknown'\n    if self.transmission_client.session:\n        if (self.type == 'download_speed'):\n            mb_spd = float(self.transmission_client.session.downloadSpeed)\n            mb_spd = ((mb_spd / 1024) / 1024)\n            self._state = round(mb_spd, (2 if (mb_spd < 0.1) else 1))\n        elif (self.type == 'upload_speed'):\n            mb_spd = float(self.transmission_client.session.uploadSpeed)\n            mb_spd = ((mb_spd / 1024) / 1024)\n            self._state = round(mb_spd, (2 if (mb_spd < 0.1) else 1))\n", "label": 1}
{"function": "\n\ndef parseNode(self):\n    label = None\n    if (self.ttype == PERCENT):\n        self.ttype = self.tokenizer.nextToken()\n        if (self.ttype != ID):\n            return None\n        label = self.tokenizer.sval\n        self.ttype = self.tokenizer.nextToken()\n        if (self.ttype != COLON):\n            return None\n        self.ttype = self.tokenizer.nextToken()\n    if (self.ttype == DOT):\n        self.ttype = self.tokenizer.nextToken()\n        wildcardPayload = CommonToken(0, '.')\n        node = WildcardTreePattern(wildcardPayload)\n        if (label is not None):\n            node.label = label\n        return node\n    if (self.ttype != ID):\n        return None\n    tokenName = self.tokenizer.sval\n    self.ttype = self.tokenizer.nextToken()\n    if (tokenName == 'nil'):\n        return self.adaptor.nil()\n    text = tokenName\n    arg = None\n    if (self.ttype == ARG):\n        arg = self.tokenizer.sval\n        text = arg\n        self.ttype = self.tokenizer.nextToken()\n    treeNodeType = self.wizard.getTokenType(tokenName)\n    if (treeNodeType == INVALID_TOKEN_TYPE):\n        return None\n    node = self.adaptor.createFromType(treeNodeType, text)\n    if ((label is not None) and isinstance(node, TreePattern)):\n        node.label = label\n    if ((arg is not None) and isinstance(node, TreePattern)):\n        node.hasTextArg = True\n    return node\n", "label": 1}
{"function": "\n\ndef test_scan_video_metadata(mkv):\n    scanned_video = scan_video(mkv['test5'])\n    assert (type(scanned_video) is Movie)\n    assert (scanned_video.name == mkv['test5'])\n    assert (scanned_video.format is None)\n    assert (scanned_video.release_group is None)\n    assert (scanned_video.resolution is None)\n    assert (scanned_video.video_codec == 'h264')\n    assert (scanned_video.audio_codec == 'AAC')\n    assert (scanned_video.imdb_id is None)\n    assert (scanned_video.hashes == {\n        'napiprojekt': 'de2e9caa58dd53a6ab9d241e6b252e35',\n        'opensubtitles': '49e2530ea3bd0d18',\n        'thesubdb': '64a8b87f12daa4f31895616e6c3fd39e',\n    })\n    assert (scanned_video.size == 31762747)\n    assert (scanned_video.subtitle_languages == {Language('spa'), Language('deu'), Language('jpn'), Language('und'), Language('ita'), Language('fra'), Language('hun')})\n    assert (scanned_video.title == 'test5')\n    assert (scanned_video.year is None)\n", "label": 1}
{"function": "\n\ndef _get_veths(net_data):\n    '\\n    Parse the nic setup inside lxc conf tuples back to a dictionary indexed by\\n    network interface\\n    '\n    if isinstance(net_data, dict):\n        net_data = list(net_data.items())\n    nics = salt.utils.odict.OrderedDict()\n    current_nic = salt.utils.odict.OrderedDict()\n    no_names = True\n    for item in net_data:\n        if (item and isinstance(item, dict)):\n            item = list(item.items())[0]\n        elif isinstance(item, six.string_types):\n            sitem = item.strip()\n            if (sitem.startswith('#') or (not sitem)):\n                continue\n            elif ('=' in item):\n                item = tuple([a.strip() for a in item.split('=', 1)])\n        if (item[0] == 'lxc.network.type'):\n            current_nic = salt.utils.odict.OrderedDict()\n        if (item[0] == 'lxc.network.name'):\n            no_names = False\n            nics[item[1].strip()] = current_nic\n        current_nic[item[0].strip()] = item[1].strip()\n    if (no_names and current_nic):\n        nics[DEFAULT_NIC] = current_nic\n    return nics\n", "label": 1}
{"function": "\n\ndef __init__(self, gateway, attributes):\n    if ('next_bill_amount' in attributes):\n        self._next_bill_amount = Decimal(attributes['next_bill_amount'])\n        del attributes['next_bill_amount']\n    Resource.__init__(self, gateway, attributes)\n    if ('price' in attributes):\n        self.price = Decimal(self.price)\n    if ('balance' in attributes):\n        self.balance = Decimal(self.balance)\n    if ('next_billing_period_amount' in attributes):\n        self.next_billing_period_amount = Decimal(self.next_billing_period_amount)\n    if ('add_ons' in attributes):\n        self.add_ons = [AddOn(gateway, add_on) for add_on in self.add_ons]\n    if ('descriptor' in attributes):\n        self.descriptor = Descriptor(gateway, attributes.pop('descriptor'))\n    if ('discounts' in attributes):\n        self.discounts = [Discount(gateway, discount) for discount in self.discounts]\n    if ('status_history' in attributes):\n        self.status_history = [SubscriptionStatusEvent(gateway, status_event) for status_event in self.status_history]\n    if ('transactions' in attributes):\n        self.transactions = [Transaction(gateway, transaction) for transaction in self.transactions]\n", "label": 1}
{"function": "\n\ndef _wait_for_unit_to_become_active(self, unit_name):\n    for attempt in range(0, self._max_tries):\n        state = self._fleet.state(True, unit_name)\n        if self._unit_is_active(unit_name, state):\n            LOGGER.debug('All %s units active', unit_name)\n            return True\n        if (state and all([(s.state == 'failed') for s in state])):\n            LOGGER.warn('All %s units failed', unit_name)\n            LOGGER.debug('State: %r', state)\n            return False\n        for s in [s for s in state if (s.loaded and (s.state == 'activating'))]:\n            LOGGER.debug('Unit %s is activating on %s', unit_name, self._machine_label(s))\n        for s in [s for s in state if (s.loaded and (s.state == 'inactive'))]:\n            LOGGER.debug('Unit %s is inactive on %s', unit_name, self._machine_label(s))\n        LOGGER.debug('Sleeping %i seconds before checking again', self._delay)\n        time.sleep(self._delay)\n    LOGGER.warn('Failed to validate unit state after %i attempts', self._max_tries)\n    return False\n", "label": 1}
{"function": "\n\ndef parse_value(self):\n    'Parses a Json value from the input.'\n    start_pos = self.__scanner.position\n    try:\n        c = self.__peek_next_non_whitespace()\n        if (c == '{'):\n            return self.__parse_object()\n        elif (c == '['):\n            return self.__parse_array()\n        elif (c == '\"'):\n            return self.__parse_string_with_concatenation()\n        elif (c == 't'):\n            self.__match('true', 'unknown identifier')\n            return True\n        elif (c == 'f'):\n            self.__match('false', 'unknown identifier')\n            return False\n        elif (c == 'n'):\n            self.__match('null', 'unknown identifier')\n            return None\n        elif ((c == '-') or ('0' <= c <= '9')):\n            return self.__parse_number()\n        elif (c == '}'):\n            return self.__error(\"'}' can only be used to end an object\")\n        elif (c == '`'):\n            return self.__parse_length_prefixed_string()\n        elif (c is None):\n            if (start_pos == 0):\n                return self.__error('Empty input')\n            else:\n                return self.__error('Unexpected end-of-text')\n        else:\n            return self.__error((\"Unexpected character '%s'\" % c))\n    except IndexError:\n        raise JsonParseException('Parser unexpectantly reached end of input probably due to an terminated string, object, or array', start_pos, self.__scanner.line_number_for_offset(start_pos))\n", "label": 1}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a date. Returns a\\n        Python datetime.date object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value.date()\n    if isinstance(value, datetime.date):\n        return value\n    if isinstance(value, list):\n        if (len(value) != 2):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES)):\n            return None\n        start_value = value[0]\n        end_value = value[1]\n    start_date = None\n    end_date = None\n    for format in (self.input_formats or formats.get_format('DATE_INPUT_FORMATS')):\n        try:\n            start_date = datetime.datetime(*time.strptime(start_value, format)[:6]).date()\n        except ValueError:\n            continue\n    for format in (self.input_formats or formats.get_format('DATE_INPUT_FORMATS')):\n        try:\n            end_date = datetime.datetime(*time.strptime(end_value, format)[:6]).date()\n        except ValueError:\n            continue\n    return (start_date, end_date)\n", "label": 1}
{"function": "\n\ndef main(argv):\n    api_files = rst_files = [rst for rst in os.listdir('doc/api') if (fnmatch.fnmatch(rst, '*.rst') and (rst not in excluded_docs))]\n    cmd = argv.pop(0)\n\n    def has(*options):\n        for opt in options:\n            if (opt in argv):\n                return argv.pop(argv.index(opt))\n    if has('-h', '--help'):\n        usage(cmd)\n    verbose = has('-v', '--verbose')\n    only_documented = (not has('-a', '--all'))\n    if argv:\n        given_files = []\n        for arg in argv:\n            arg = arg.replace('\\\\', '/').replace((api_doc + '/'), '')\n            arg = (arg.replace('.rst', '') + '.rst')\n            if ('*' in arg):\n                given_files += [rst for rst in api_files if fnmatch.fnmatch(rst, arg)]\n            elif (arg in api_files):\n                given_files.append(arg)\n        api_files = given_files\n    rst_basenames = sorted((f[:(- 4)] for f in rst_files))\n    for rst in api_files:\n        basename = rst.replace('.rst', '')\n        if (verbose or (len(api_files) > 1)):\n            print(('== Checking %s ... ' % rst))\n        check_api_doc(basename, verbose, only_documented, any(((f.startswith(basename) and (f != basename)) for f in rst_basenames)))\n", "label": 1}
{"function": "\n\ndef test_field_assumptions():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.real_elements(X), Q.real_elements(X))\n    assert (not ask(Q.integer_elements(X), Q.real_elements(X)))\n    assert ask(Q.complex_elements(X), Q.real_elements(X))\n    assert (ask(Q.real_elements((X + Y)), Q.real_elements(X)) is None)\n    assert ask(Q.real_elements((X + Y)), (Q.real_elements(X) & Q.real_elements(Y)))\n    from sympy.matrices.expressions.hadamard import HadamardProduct\n    assert ask(Q.real_elements(HadamardProduct(X, Y)), (Q.real_elements(X) & Q.real_elements(Y)))\n    assert ask(Q.complex_elements((X + Y)), (Q.real_elements(X) & Q.complex_elements(Y)))\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X.I), (Q.real_elements(X) & Q.invertible(X)))\n    assert ask(Q.real_elements(Trace(X)), Q.real_elements(X))\n    assert ask(Q.integer_elements(Determinant(X)), Q.integer_elements(X))\n    assert (not ask(Q.integer_elements(X.I), Q.integer_elements(X)))\n    alpha = Symbol('alpha')\n    assert ask(Q.real_elements((alpha * X)), (Q.real_elements(X) & Q.real(alpha)))\n    assert ask(Q.real_elements(LofLU(X)), Q.real_elements(X))\n", "label": 1}
{"function": "\n\ndef _read_unlocked(self, n=None):\n    nodata_val = b''\n    empty_values = (b'', None)\n    buf = self._read_buf\n    pos = self._read_pos\n    if ((n is None) or (n == (- 1))):\n        self._reset_read_buf()\n        chunks = [buf[pos:]]\n        current_size = 0\n        while True:\n            try:\n                chunk = self.raw.read()\n            except IOError as e:\n                if (e.errno != EINTR):\n                    raise\n                continue\n            if (chunk in empty_values):\n                nodata_val = chunk\n                break\n            current_size += len(chunk)\n            chunks.append(chunk)\n        return (b''.join(chunks) or nodata_val)\n    avail = (len(buf) - pos)\n    if (n <= avail):\n        self._read_pos += n\n        return buf[pos:(pos + n)]\n    chunks = [buf[pos:]]\n    wanted = max(self.buffer_size, n)\n    while (avail < n):\n        try:\n            chunk = self.raw.read(wanted)\n        except IOError as e:\n            if (e.errno != EINTR):\n                raise\n            continue\n        if (chunk in empty_values):\n            nodata_val = chunk\n            break\n        avail += len(chunk)\n        chunks.append(chunk)\n    n = min(n, avail)\n    out = b''.join(chunks)\n    self._read_buf = out[n:]\n    self._read_pos = 0\n    return (out[:n] if out else nodata_val)\n", "label": 1}
{"function": "\n\ndef send(string='', verbosity=1):\n    global base_stack_level, lastmsg, stdopt, filopt\n    if ((verbosity <= filopt['verbosity']) or (verbosity <= stdopt['verbosity'])):\n        if (stdopt['showCaller'] or filopt['showCaller']):\n            if ((len(stack()) <= base_stack_level) or (base_stack_level == (- 1))):\n                base_stack_level = len(stack())\n            curr_lev = (len(stack()) - base_stack_level)\n            caller = inspect.stack()[1][3]\n        else:\n            caller = ''\n            curr_lev = 0\n        if (stdopt['showCallerStack'] or filopt['showCallerStack']):\n            callerstack = getStackPath()[2:]\n            if ('runcode' in callerstack):\n                del callerstack[callerstack.index('runcode'):]\n            if ('runfile' in callerstack):\n                del callerstack[callerstack.index('runfile'):]\n            callerstack = str(callerstack).strip('[]').replace(', ', '.').replace(\"'\", '')\n        else:\n            callerstack = ''\n        if (verbosity <= filopt['verbosity']):\n            try:\n                outstr_file = makeLogString(string, filopt, caller, callerstack, curr_lev)\n                logfile.write((outstr_file + '\\n'))\n            except:\n                pass\n        if (verbosity <= stdopt['verbosity']):\n            outstr_std = makeLogString(string, stdopt, caller, callerstack, curr_lev)\n            print(colorize(outstr_std))\n    lastmsg = string\n", "label": 1}
{"function": "\n\ndef _node_for(self, object):\n    ' Returns the TreeNode associated with a specified object.\\n        '\n    if ((type(object) is tuple) and (len(object) == 2) and isinstance(object[1], TreeNode)):\n        return object\n    factory = self.factory\n    nodes = [node for node in factory.nodes if node.is_node_for(object)]\n    if (len(nodes) == 1):\n        return (object, nodes[0])\n    if (len(nodes) == 0):\n        return (object, ITreeNodeAdapterBridge(adapter=object))\n    base = nodes[0].node_for\n    nodes = [node for node in nodes if (base == node.node_for)]\n    if (len(nodes) == 1):\n        return (object, nodes[0])\n    root_node = None\n    for (i, node) in enumerate(nodes):\n        if (node.children == ''):\n            root_node = node\n            del nodes[i]\n            break\n    else:\n        root_node = nodes[0]\n    key = ((root_node,) + tuple(nodes))\n    if (key in factory.multi_nodes):\n        return (object, factory.multi_nodes[key])\n    factory.multi_nodes[key] = multi_node = MultiTreeNode(root_node=root_node, nodes=nodes)\n    return (object, multi_node)\n", "label": 1}
{"function": "\n\ndef req_it(self, num=(- 1), ids=None, include_unmangled=False):\n    '\\n        A generator over all the requests in history when the function was called.\\n        Generates deferreds which resolve to requests.\\n        '\n    count = 0\n\n    @defer.inlineCallbacks\n    def def_wrapper(reqid, load=False, num=1):\n        if ((not self.check(reqid)) and load):\n            (yield self.load(reqid, num))\n        req = (yield self.get(reqid))\n        defer.returnValue(req)\n    over = list(self.ordered_ids)\n    for reqid in over:\n        if ((ids is not None) and (reqid not in ids)):\n            continue\n        if ((not include_unmangled) and (reqid in self.unmangled_ids)):\n            continue\n        do_load = True\n        if (reqid in self.all_ids):\n            if ((count % self._preload_limit) == 0):\n                do_load = True\n            if (do_load and (not self.check(reqid))):\n                do_load = False\n                if (((num - count) < self._preload_limit) and (num != (- 1))):\n                    loadnum = (num - count)\n                else:\n                    loadnum = self._preload_limit\n                (yield def_wrapper(reqid, load=True, num=loadnum))\n            else:\n                (yield def_wrapper(reqid))\n            count += 1\n            if ((count >= num) and (num != (- 1))):\n                break\n", "label": 1}
{"function": "\n\ndef ping_received(self, on_sock, sender_nid, version, t):\n    if ((on_sock == OUT) and (sender_nid not in self.channels_out)):\n        return\n    self.last_seen[sender_nid] = t\n    if ((on_sock == IN) and (sender_nid not in self.channels_in)):\n        self.channels_in.add(sender_nid)\n        if self.is_relay:\n            (yield (RelaySigNew, IN, sender_nid))\n        elif (sender_nid in self.cl_relayees):\n            relay_nid = self.cl_relayees.pop(sender_nid)\n            self.cl_avail_relays[relay_nid].remove(sender_nid)\n            (yield (RelayNvm, (IN if (relay_nid in self.channels_in) else OUT), relay_nid, sender_nid))\n    inout = (IN if (sender_nid in self.channels_in) else OUT)\n    if self._needs_ping(sender_nid, t):\n        (yield (Ping, inout, sender_nid, self._next_version()))\n    if (sender_nid in self.queues):\n        for msg_h in self.queues.pop(sender_nid):\n            (yield (Send, inout, sender_nid, self._next_version(), msg_h))\n    elif (not (version > self.versions.get(sender_nid, (- 1)))):\n        self.versions[sender_nid] = version\n        assert (sender_nid not in self.queues)\n        if (sender_nid in self.cl_avail_relays):\n            self._handle_relay_down(sender_nid)\n        (yield (NodeDown, sender_nid))\n    self.versions[sender_nid] = version\n", "label": 1}
{"function": "\n\ndef ensure_completely_loaded(force=False):\n    '\\n    This method ensures all models are completely loaded\\n\\n    FeinCMS requires Django to be completely initialized before proceeding,\\n    because of the extension mechanism and the dynamically created content\\n    types.\\n\\n    For more informations, have a look at issue #23 on github:\\n    http://github.com/feincms/feincms/issues#issue/23\\n    '\n    global COMPLETELY_LOADED\n    if (COMPLETELY_LOADED and (not force)):\n        return True\n    try:\n        from django.apps import apps\n    except ImportError:\n        from django.db.models import loading as apps\n    else:\n        if (not apps.ready):\n            return\n    import django\n    if (django.get_version() < '1.8'):\n        from feincms._internal import get_models\n        for model in get_models():\n            for cache_name in ('_field_cache', '_field_name_cache', '_m2m_cache', '_related_objects_cache', '_related_many_to_many_cache', '_name_map'):\n                try:\n                    delattr(model._meta, cache_name)\n                except AttributeError:\n                    pass\n            model._meta._fill_fields_cache()\n        if hasattr(apps, 'cache'):\n            try:\n                apps.cache.get_models.cache_clear()\n            except AttributeError:\n                apps.cache._get_models_cache.clear()\n    if hasattr(apps, 'ready'):\n        if apps.ready:\n            COMPLETELY_LOADED = True\n    elif apps.app_cache_ready():\n        COMPLETELY_LOADED = True\n    return True\n", "label": 1}
{"function": "\n\ndef test_file_magic():\n    kernel = get_kernel()\n    kernel.do_execute('%%file TEST.txt\\nLINE1\\nLINE2\\nLINE3', False)\n    assert os.path.exists('TEST.txt')\n    with open('TEST.txt') as fp:\n        lines = fp.readlines()\n        assert (len(lines) == 3)\n        assert (lines[0] == 'LINE1\\n')\n        assert (lines[1] == 'LINE2\\n')\n        assert (lines[2] == 'LINE3')\n    kernel.do_execute('%%file -a TEST.txt\\n\\nLINE4\\nLINE5\\nLINE6', False)\n    assert os.path.exists('TEST.txt')\n    with open('TEST.txt') as fp:\n        lines = fp.readlines()\n        assert (len(lines) == 6)\n        assert (lines[3] == 'LINE4\\n')\n        assert (lines[4] == 'LINE5\\n')\n        assert (lines[5] == 'LINE6')\n    kernel.do_execute('%%file /tmp/tmp/TEST.txt\\nTEST1\\nTEST2\\nTEST3')\n    with open('/tmp/tmp/TEST.txt') as fp:\n        lines = fp.readlines()\n        assert (len(lines) == 3)\n        assert (lines[0] == 'TEST1\\n')\n        assert (lines[1] == 'TEST2\\n')\n        assert (lines[2] == 'TEST3')\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.ip = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.start_time = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.file = PacketCaptureFile()\n                self.file.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.pid = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef filter_subnets(self):\n    subnets = []\n    admin_tenant_id = self.src_cloud.get_tenant_id(self.src_cloud.tenant)\n    for net in config.networks:\n        if (not net.get('subnets')):\n            continue\n        for subnet in net['subnets']:\n            subnet['tenant_id'] = admin_tenant_id\n            subnets.append(subnet)\n    subnets = [i for net in config.networks if net.get('subnets') for i in net['subnets']]\n    for tenant in config.tenants:\n        if (('networks' not in tenant) or tenant.get('deleted')):\n            continue\n        for network in tenant['networks']:\n            if ('subnets' not in network):\n                continue\n            for subnet in network['subnets']:\n                subnet['tenant_id'] = self.src_cloud.get_tenant_id(tenant['name'])\n                subnets.append(subnet)\n    env_subnets = self.src_cloud.neutronclient.list_subnets()['subnets']\n    filtered_subnets = {\n        'subnets': [],\n    }\n    for env_subnet in env_subnets:\n        for subnet in subnets:\n            same_cidr = (env_subnet['cidr'] == subnet['cidr'])\n            same_tenant = (env_subnet['tenant_id'] == subnet['tenant_id'])\n            if (same_cidr and same_tenant):\n                filtered_subnets['subnets'].append(env_subnet)\n    return filtered_subnets\n", "label": 1}
{"function": "\n\ndef check(self):\n    'Do the actual testing.'\n    scene = self.scene\n    src = scene.children[0]\n    ud = src.children[0]\n    o = ud.children[0].children[0].children[0]\n    mm = o.children[0]\n    assert (src.get_output_dataset().point_data.scalars.name == 'temperature')\n    assert (src.get_output_dataset().point_data.vectors.name == 'velocity')\n    expect = ['ScalarGradient', 'Vorticity']\n    expect1 = [(x + '-y') for x in expect]\n    expect2 = [(x + ' magnitude') for x in expect]\n    o.enabled = True\n    assert (o.get_output_dataset().point_data.scalars.name in expect1)\n    assert (o.get_output_dataset().point_data.vectors.name in expect)\n    assert (mm.scalar_lut_manager.data_name in expect1)\n    o.enabled = False\n    assert (o.get_output_dataset().point_data.scalars.name in expect2)\n    assert (o.get_output_dataset().point_data.vectors.name in expect)\n    assert (mm.scalar_lut_manager.data_name in expect2)\n    ud.filter.vector_mode = 'compute_vorticity'\n    assert (o.get_output_dataset().point_data.scalars.name == 'Vorticity magnitude')\n    assert (o.get_output_dataset().point_data.vectors.name == 'Vorticity')\n    assert (mm.scalar_lut_manager.data_name == 'Vorticity magnitude')\n    o.enabled = True\n    assert (o.get_output_dataset().point_data.scalars.name == 'Vorticity-y')\n    assert (o.get_output_dataset().point_data.vectors.name == 'Vorticity')\n    assert (mm.scalar_lut_manager.data_name == 'Vorticity-y')\n    o.enabled = False\n", "label": 1}
{"function": "\n\ndef uniquePathsWithObstacles(self, obstacleGrid):\n    '\\n        dp\\n        :param obstacleGrid:  a list of lists of integers\\n        :return: integer\\n        '\n    m = len(obstacleGrid)\n    n = len(obstacleGrid[0])\n    if ((obstacleGrid[0][0] == 1) or (obstacleGrid[(m - 1)][(n - 1)] == 1)):\n        return 0\n    path = [[0 for _ in range(n)] for _ in range(m)]\n    path[0][0] = 1\n    for i in range(m):\n        for j in range(n):\n            if ((i == 0) and (j == 0)):\n                continue\n            if (i == 0):\n                path[i][j] = (path[i][(j - 1)] if (obstacleGrid[i][(j - 1)] == 0) else 0)\n            elif (j == 0):\n                path[i][j] = (path[(i - 1)][j] if (obstacleGrid[(i - 1)][j] == 0) else 0)\n            elif ((obstacleGrid[i][(j - 1)] == 0) and (obstacleGrid[(i - 1)][j] == 0)):\n                path[i][j] = (path[(i - 1)][j] + path[i][(j - 1)])\n            elif (obstacleGrid[i][(j - 1)] == 0):\n                path[i][j] = path[i][(j - 1)]\n            elif (obstacleGrid[(i - 1)][j] == 0):\n                path[i][j] = path[(i - 1)][j]\n            else:\n                path[i][j] = 0\n    return path[(m - 1)][(n - 1)]\n", "label": 1}
{"function": "\n\ndef restore_prefs(self, prefs):\n    ' Restores any saved user preference information associated with the\\n            editor.\\n        '\n    factory = self.factory\n    try:\n        filters = prefs.get('filters', None)\n        if (filters is not None):\n            factory.filters = ([f for f in factory.filters if f.template] + [f for f in filters if (not f.template)])\n        columns = prefs.get('columns')\n        if (columns is not None):\n            new_columns = []\n            all_columns = (self.columns + factory.other_columns)\n            for column in columns:\n                for column2 in all_columns:\n                    if (column == column2.get_label()):\n                        new_columns.append(column2)\n                        break\n            self.columns = new_columns\n            if (not factory.auto_size):\n                widths = prefs.get('widths')\n                if (widths is not None):\n                    self.grid._user_col_size = True\n                    set_col_size = self.grid._grid.SetColSize\n                    for (i, width) in enumerate(widths):\n                        if (width >= 0):\n                            set_col_size(i, width)\n        structure = prefs.get('structure')\n        if ((structure is not None) and (factory.edit_view != ' ')):\n            self.control.GetSizer().SetStructure(self.control, structure)\n    except:\n        pass\n", "label": 1}
{"function": "\n\ndef resolve_pattern(self, dirname, pattern, globstar_with_root):\n    \"Apply ``pattern`` (contains no path elements) to the\\n        literal directory`` in dirname``.\\n\\n        If pattern=='', this will filter for directories. This is\\n        a special case that happens when the user's glob expression ends\\n        with a slash (in which case we only want directories). It simpler\\n        and faster to filter here than in :meth:`_iglob`.\\n        \"\n    if (isinstance(pattern, six.text_type) and (not isinstance(dirname, six.text_type))):\n        dirname = six.u(dirname, (sys.getfilesystemencoding() or sys.getdefaultencoding()))\n    if (not has_magic(pattern)):\n        if (pattern == ''):\n            if self.isdir(dirname):\n                return [(pattern, ())]\n        elif self.exists(os.path.join(dirname, pattern)):\n            return [(pattern, ())]\n        return []\n    if (not dirname):\n        dirname = os.curdir\n    try:\n        if (pattern == '**'):\n            names = ([''] if globstar_with_root else [])\n            for (top, entries) in self.walk(dirname):\n                _mkabs = (lambda s: os.path.join(top[(len(dirname) + 1):], s))\n                names.extend(map(_mkabs, entries))\n            pattern = '*'\n        else:\n            names = self.listdir(dirname)\n    except os.error:\n        return []\n    if (pattern[0] != '.'):\n        names = filter((lambda x: ((not x) or (x[0] != '.'))), names)\n    return fnmatch_filter(names, pattern)\n", "label": 1}
{"function": "\n\ndef extract_concerns(module_text):\n    concerns = []\n    with contextlib.closing(cStringIO.StringIO(module_text)) as text:\n        tokens = tokenize.generate_tokens(text.readline)\n        function_id = None\n        for token in tokens:\n            if ((token[0] == tokenize.NAME) and token[4].lstrip().startswith('def') and token[1].startswith('test_')):\n                function_id = get_function_id(token[1])\n            elif (function_id and (token[0] == tokenize.COMMENT)):\n                comment = token[1][1:].lstrip()\n                comment_str = None\n                if comment.startswith('CONCERN:'):\n                    comment_str = comment.replace('CONCERN:', '', 1).lstrip()\n                if comment.startswith('['):\n                    comment_str = re.sub('\\\\[.*\\\\]', '', comment, 1).lstrip()\n                if comment_str:\n                    while True:\n                        next_token = next(tokens, None)\n                        if (next_token and (next_token[0] == tokenize.COMMENT)):\n                            next_comment = next_token[1][1:].lstrip()\n                            comment_str += (' ' + next_comment)\n                        elif (next_token and (next_token[0] == tokenize.NL)):\n                            continue\n                        else:\n                            break\n                    concern = '[ {func_id}] {comment}'.format(func_id=function_id, comment=comment_str)\n                    concerns.append(concern)\n    return concerns\n", "label": 1}
{"function": "\n\ndef run(self):\n    figwidth = self.options.pop('figwidth', None)\n    figclasses = self.options.pop('figclass', None)\n    align = self.options.pop('align', None)\n    (image_node,) = Image.run(self)\n    if isinstance(image_node, nodes.system_message):\n        return [image_node]\n    figure_node = nodes.figure('', image_node)\n    if (figwidth == 'image'):\n        if (PIL and self.state.document.settings.file_insertion_enabled):\n            try:\n                i = PIL.open(str(image_node['uri']))\n            except (IOError, UnicodeError):\n                pass\n            else:\n                self.state.document.settings.record_dependencies.add(image_node['uri'])\n                figure_node['width'] = i.size[0]\n    elif (figwidth is not None):\n        figure_node['width'] = figwidth\n    if figclasses:\n        figure_node['classes'] += figclasses\n    if align:\n        figure_node['align'] = align\n    if self.content:\n        node = nodes.Element()\n        self.state.nested_parse(self.content, self.content_offset, node)\n        first_node = node[0]\n        if isinstance(first_node, nodes.paragraph):\n            caption = nodes.caption(first_node.rawsource, '', *first_node.children)\n            figure_node += caption\n        elif (not (isinstance(first_node, nodes.comment) and (len(first_node) == 0))):\n            error = self.state_machine.reporter.error('Figure caption must be a paragraph or empty comment.', nodes.literal_block(self.block_text, self.block_text), line=self.lineno)\n            return [figure_node, error]\n        if (len(node) > 1):\n            figure_node += nodes.legend('', *node[1:])\n    return [figure_node]\n", "label": 1}
{"function": "\n\ndef __read(self):\n    '\\n        Read the next frame(s) from the socket.\\n        '\n    fastbuf = BytesIO()\n    while self.running:\n        try:\n            try:\n                c = self.receive()\n            except exception.InterruptedException:\n                log.debug('socket read interrupted, restarting')\n                continue\n        except Exception:\n            c = b''\n        if (len(c) == 0):\n            raise exception.ConnectionClosedException()\n        fastbuf.write(c)\n        if (b'\\x00' in c):\n            break\n        elif (c == b'\\n'):\n            return [c]\n    self.__recvbuf += fastbuf.getvalue()\n    fastbuf.close()\n    result = []\n    if (self.__recvbuf and self.running):\n        while True:\n            pos = self.__recvbuf.find(b'\\x00')\n            if (pos >= 0):\n                frame = self.__recvbuf[0:pos]\n                preamble_end_match = utils.PREAMBLE_END_RE.search(frame)\n                if preamble_end_match:\n                    preamble_end = preamble_end_match.start()\n                    content_length_match = BaseTransport.__content_length_re.search(frame[0:preamble_end])\n                    if content_length_match:\n                        content_length = int(content_length_match.group('value'))\n                        content_offset = preamble_end_match.end()\n                        frame_size = (content_offset + content_length)\n                        if (frame_size > len(frame)):\n                            if (frame_size < len(self.__recvbuf)):\n                                pos = frame_size\n                                frame = self.__recvbuf[0:pos]\n                            else:\n                                break\n                result.append(frame)\n                self.__recvbuf = self.__recvbuf[(pos + 1):]\n            else:\n                break\n    return result\n", "label": 1}
{"function": "\n\ndef assert_both_values(actual, expected_plain, expected_color, kind=None):\n    'Handle asserts for color and non-color strings in color and non-color tests.\\n\\n    :param ColorStr actual: Return value of ColorStr class method.\\n    :param expected_plain: Expected non-color value.\\n    :param expected_color: Expected color value.\\n    :param str kind: Type of string to test.\\n    '\n    if kind.endswith('plain'):\n        assert (actual.value_colors == expected_plain)\n        assert (actual.value_no_colors == expected_plain)\n        assert (actual.has_colors is False)\n    elif kind.endswith('color'):\n        assert (actual.value_colors == expected_color)\n        assert (actual.value_no_colors == expected_plain)\n        if ('\\x1b' in actual.value_colors):\n            assert (actual.has_colors is True)\n        else:\n            assert (actual.has_colors is False)\n    else:\n        assert (actual == expected_plain)\n    if kind.startswith('ColorStr'):\n        assert (actual.__class__ == ColorStr)\n    elif kind.startswith('Color'):\n        assert (actual.__class__ == Color)\n", "label": 1}
{"function": "\n\ndef acquire(self, timeout=None):\n    timeout = (((timeout is not None) and timeout) or self.timeout)\n    end_time = time.time()\n    if ((timeout is not None) and (timeout > 0)):\n        end_time += timeout\n    if (timeout is None):\n        wait = 0.1\n    else:\n        wait = max(0, (timeout / 10))\n    while True:\n        try:\n            os.mkdir(self.lock_file)\n        except OSError:\n            err = sys.exc_info()[1]\n            if (err.errno == errno.EEXIST):\n                if os.path.exists(self.unique_name):\n                    return\n                if ((timeout is not None) and (time.time() > end_time)):\n                    if (timeout > 0):\n                        raise LockTimeout(('Timeout waiting to acquire lock for %s' % self.path))\n                    else:\n                        raise AlreadyLocked(('%s is already locked' % self.path))\n                time.sleep(wait)\n            else:\n                raise LockFailed(('failed to create %s' % self.lock_file))\n        else:\n            open(self.unique_name, 'wb').close()\n            return\n", "label": 1}
{"function": "\n\ndef on_modified_async(self, view):\n    if (not is_php_file(view)):\n        return\n    cursor = view.sel()[0].b\n    if (cursor < 1):\n        return\n    while (cursor > 0):\n        curChar = view.substr(sublime.Region((cursor - 1), cursor))\n        if (curChar == '\\\\'):\n            return self.run_completion(view)\n        if (curChar == '$'):\n            return self.run_completion(view)\n        if (curChar == '('):\n            return self.run_completion(view)\n        if (cursor > 1):\n            curChar = view.substr(sublime.Region((cursor - 2), cursor))\n            if (curChar == '->'):\n                return self.run_completion(view)\n            if (curChar == '::'):\n                return self.run_completion(view)\n        if (cursor > 3):\n            curChar = view.substr(sublime.Region((cursor - 4), cursor))\n            if (curChar == 'use '):\n                return self.run_completion(view)\n            if (curChar == 'new '):\n                return self.run_completion(view)\n            if (cursor > 9):\n                curChar = view.substr(sublime.Region((cursor - 10), cursor))\n                if (curChar == 'namespace '):\n                    return self.run_completion(view)\n        cursor -= 1\n", "label": 1}
{"function": "\n\ndef pack_list(from_, pack_type):\n    ' Return the wire packed version of `from_`. `pack_type` should be some\\n    subclass of `xcffib.Struct`, or a string that can be passed to\\n    `struct.pack`. You must pass `size` if `pack_type` is a struct.pack string.\\n    '\n    if (len(from_) == 0):\n        return bytes()\n    if (pack_type == 'c'):\n        if isinstance(from_, bytes):\n            from_ = [six.int2byte(b) for b in six.iterbytes(from_)]\n        elif isinstance(from_, six.string_types):\n            from_ = [six.int2byte(b) for b in bytearray(from_, 'utf-8')]\n        elif isinstance(from_[0], six.integer_types):\n\n            def to_bytes(v):\n                for _ in range(4):\n                    (v, r) = divmod(v, 256)\n                    (yield r)\n            from_ = [six.int2byte(b) for i in from_ for b in to_bytes(i)]\n    if isinstance(pack_type, six.string_types):\n        return struct.pack(('=' + (pack_type * len(from_))), *from_)\n    else:\n        buf = six.BytesIO()\n        for item in from_:\n            if (isinstance(item, Protobj) and hasattr(item, 'pack')):\n                buf.write(item.pack())\n            else:\n                buf.write(item)\n        return buf.getvalue()\n", "label": 1}
{"function": "\n\ndef _possible_configs_for_cls(cls, reasons=None):\n    all_configs = set(config.Config.all_configs())\n    if cls.__unsupported_on__:\n        spec = exclusions.db_spec(*cls.__unsupported_on__)\n        for config_obj in list(all_configs):\n            if spec(config_obj):\n                all_configs.remove(config_obj)\n    if getattr(cls, '__only_on__', None):\n        spec = exclusions.db_spec(*util.to_list(cls.__only_on__))\n        for config_obj in list(all_configs):\n            if (not spec(config_obj)):\n                all_configs.remove(config_obj)\n    if hasattr(cls, '__requires__'):\n        requirements = config.requirements\n        for config_obj in list(all_configs):\n            for requirement in cls.__requires__:\n                check = getattr(requirements, requirement)\n                skip_reasons = check.matching_config_reasons(config_obj)\n                if skip_reasons:\n                    all_configs.remove(config_obj)\n                    if (reasons is not None):\n                        reasons.extend(skip_reasons)\n                    break\n    if hasattr(cls, '__prefer_requires__'):\n        non_preferred = set()\n        requirements = config.requirements\n        for config_obj in list(all_configs):\n            for requirement in cls.__prefer_requires__:\n                check = getattr(requirements, requirement)\n                if (not check.enabled_for_config(config_obj)):\n                    non_preferred.add(config_obj)\n        if all_configs.difference(non_preferred):\n            all_configs.difference_update(non_preferred)\n    return all_configs\n", "label": 1}
{"function": "\n\ndef autodiscover_templates():\n    \"\\n    Autodiscovers cmsplugin_contact_plus templates the way\\n    'django.template.loaders.filesystem.Loader' and\\n    'django.template.loaders.app_directories.Loader' work.\\n    \"\n\n    def sorted_templates(templates):\n        '\\n        Sorts templates\\n        '\n        TEMPLATES = sorted(templates, key=(lambda template: template[1]))\n        return TEMPLATES\n    global TEMPLATES\n    if TEMPLATES:\n        return TEMPLATES\n    override_dir = getattr(settings, 'CMSPLUGIN_CONTACT_PLUS_TEMPLATES', None)\n    if override_dir:\n        return sorted_templates(override_dir)\n    templates = []\n    dirs_to_scan = []\n    if ('django.template.loaders.app_directories.Loader' in settings.TEMPLATE_LOADERS):\n        for app in settings.INSTALLED_APPS:\n            _ = __import__(app)\n            dir = os.path.dirname(_.__file__)\n            if (not (dir in dirs_to_scan)):\n                dirs_to_scan.append(os.path.join(dir, 'templates'))\n    if ('django.template.loaders.filesystem.Loader' in settings.TEMPLATE_LOADERS):\n        for dir in settings.TEMPLATE_DIRS:\n            if (not (dir in dirs_to_scan)):\n                dirs_to_scan.append(dir)\n    for dir in dirs_to_scan:\n        found = glob.glob(os.path.join(dir, 'cmsplugin_contact_plus/*.html'))\n        for file in found:\n            (dir, file) = os.path.split(file)\n            (key, value) = (os.path.join(dir.split('/')[(- 1)], file), file)\n            f = False\n            for (_, template) in templates:\n                if (template == file):\n                    f = True\n            if (not f):\n                templates.append((key, value))\n    return sorted_templates(templates)\n", "label": 1}
{"function": "\n\ndef test_single_repeating_timer(self):\n    time = TestingTimeFunction()\n    manager = TimerManager(_time_function=time)\n    callback = MockCallback()\n    manager.add_timer(30, callback, repeat=True)\n    assert (manager.sleep_time() == 30)\n    assert (callback.nb_calls == 0)\n    manager.run()\n    assert (callback.nb_calls == 0)\n    time.time = 22\n    assert (manager.sleep_time() == 8)\n    manager.run()\n    assert (callback.nb_calls == 0)\n    time.time = 30\n    assert (manager.sleep_time() == 0)\n    manager.run()\n    assert (callback.nb_calls == 1)\n    assert (manager.sleep_time() == 30)\n    time.time = 71\n    assert (manager.sleep_time() == 0)\n    manager.run()\n    assert (callback.nb_calls == 2)\n    assert (manager.sleep_time() == 19)\n    time.time = 200\n    assert (manager.sleep_time() == 0)\n    manager.run()\n    assert (callback.nb_calls == 3)\n    assert (manager.sleep_time() == 10)\n", "label": 1}
{"function": "\n\ndef is_acronym(token, exclude=None):\n    '\\n    Pass single token as a string, return True/False if is/is not valid acronym.\\n\\n    Args:\\n        token (str): single word to check for acronym-ness\\n        exclude (set[str]): if technically valid but not actually good acronyms\\n        are known in advance, pass them in as a set of strings; matching tokens\\n        will return False\\n\\n    Returns:\\n        bool\\n    '\n    if (exclude and (token in exclude)):\n        return False\n    if (not token):\n        return False\n    if (' ' in token):\n        return False\n    if ((len(token) == 2) and (not token.isupper())):\n        return False\n    if token.isdigit():\n        return False\n    if ((not any((char.isupper() for char in token))) and (not (token[0].isdigit() or token[(- 1)].isdigit()))):\n        return False\n    if (not (2 <= sum((1 for char in token if char.isalnum())) <= 10)):\n        return False\n    if (not ACRONYM_REGEX.match(token)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef _add_group_id_fields(self, out_collection, group_func_keys):\n    date_operators = ['dayOfYear', 'dayOfMonth', 'dayOfWeek', 'year', 'month', 'week', 'hour', 'minute', 'second', 'millisecond']\n    clear_group_func_keys = []\n    for key in group_func_keys:\n        out_field = key.split('__')[0]\n        for doc in out_collection:\n            if (key not in doc.keys()):\n                func_field = key.split('__')[1]\n                (func, in_field) = func_field.split('_')\n                out_value = doc.get(in_field)\n                if (func in date_operators):\n                    if (func == 'dayOfYear'):\n                        out_value = out_value.timetuple().tm_yday\n                    elif (func == 'dayOfMonth'):\n                        out_value = out_value.day\n                    elif (func == 'dayOfWeek'):\n                        out_value = out_value.isoweekday()\n                    elif (func == 'year'):\n                        out_value = out_value.year\n                    elif (func == 'month'):\n                        out_value = out_value.month\n                    elif (func == 'week'):\n                        out_value = out_value.isocalendar()[1]\n                    elif (func == 'hour'):\n                        out_value = out_value.hour\n                    elif (func == 'minute'):\n                        out_value = out_value.minute\n                    elif (func == 'second'):\n                        out_value = out_value.second\n                    elif (func == 'millisecond'):\n                        out_value = int((out_value.microsecond / 1000))\n                doc[out_field] = out_value\n        clear_group_func_keys.append(out_field)\n    return (out_collection, clear_group_func_keys)\n", "label": 1}
{"function": "\n\ndef _validate_option(self, option, val):\n    if (option in 'field_names'):\n        self._validate_field_names(val)\n    elif (option in ('start', 'end', 'max_width', 'min_width', 'min_table_width', 'max_table_width', 'padding_width', 'left_padding_width', 'right_padding_width', 'format')):\n        self._validate_nonnegative_int(option, val)\n    elif (option in 'sortby'):\n        self._validate_field_name(option, val)\n    elif (option in 'sort_key'):\n        self._validate_function(option, val)\n    elif (option in 'hrules'):\n        self._validate_hrules(option, val)\n    elif (option in 'vrules'):\n        self._validate_vrules(option, val)\n    elif (option in 'fields'):\n        self._validate_all_field_names(option, val)\n    elif (option in ('header', 'border', 'reversesort', 'xhtml', 'print_empty', 'oldsortslice')):\n        self._validate_true_or_false(option, val)\n    elif (option in 'header_style'):\n        self._validate_header_style(val)\n    elif (option in 'int_format'):\n        self._validate_int_format(option, val)\n    elif (option in 'float_format'):\n        self._validate_float_format(option, val)\n    elif (option in ('vertical_char', 'horizontal_char', 'junction_char')):\n        self._validate_single_char(option, val)\n    elif (option in 'attributes'):\n        self._validate_attributes(option, val)\n", "label": 1}
{"function": "\n\ndef handle_start_block(self, token_text):\n    self.set_mode(MODE.BlockStatement)\n    empty_braces = self.is_next('}')\n    empty_anonymous_function = (empty_braces and (self.flags.last_word == 'function') and (self.last_type == 'TK_END_EXPR'))\n    if (self.opts.brace_style == 'expand'):\n        if ((self.last_type != 'TK_OPERATOR') and (empty_anonymous_function or (self.last_type == 'TK_EQUALS') or (self.is_special_word(self.flags.last_text) and (self.flags.last_text != 'else')))):\n            self.output_space_before_token = True\n        else:\n            self.append_newline()\n    elif (self.last_type not in ['TK_OPERATOR', 'TK_START_EXPR']):\n        if (self.last_type == 'TK_START_BLOCK'):\n            self.append_newline()\n        else:\n            self.output_space_before_token = True\n    elif (self.is_array(self.previous_flags.mode) and (self.flags.last_text == ',')):\n        if (self.last_last_text == '}'):\n            self.output_space_before_token = True\n        else:\n            self.append_newline()\n    self.append_token(token_text)\n    self.indent()\n", "label": 1}
{"function": "\n\ndef _lookup(self, name=None, create=False):\n    'looks up symbol in search path\\n        returns symbol given symbol name,\\n        creating symbol if needed (and create=True)'\n    debug = False\n    if debug:\n        print('====\\nLOOKUP ', name)\n    searchGroups = self._fix_searchGroups()\n    self.__parents = []\n    if (self not in searchGroups):\n        searchGroups.append(self)\n\n    def public_attr(grp, name):\n        return (hasattr(grp, name) and (not ((grp is self) and (name in self._private))))\n    parts = name.split('.')\n    if (len(parts) == 1):\n        for grp in searchGroups:\n            if public_attr(grp, name):\n                self.__parents.append(grp)\n                return getattr(grp, name)\n    parts.reverse()\n    top = parts.pop()\n    out = self.__invalid_name\n    if (top == self.top_group):\n        out = self\n    else:\n        for grp in searchGroups:\n            if public_attr(grp, top):\n                self.__parents.append(grp)\n                out = getattr(grp, top)\n    if (out is self.__invalid_name):\n        raise NameError((\"'%s' is not defined\" % name))\n    if (len(parts) == 0):\n        return out\n    while parts:\n        prt = parts.pop()\n        if hasattr(out, prt):\n            out = getattr(out, prt)\n        elif create:\n            val = None\n            if (len(parts) > 0):\n                val = Group(name=prt)\n            setattr(out, prt, val)\n            out = getattr(out, prt)\n        else:\n            raise LookupError((\"cannot locate member '%s' of '%s'\" % (prt, out)))\n    return out\n", "label": 1}
{"function": "\n\ndef extended_blank_lines(logical_line, blank_lines, indent_level, previous_logical):\n    'Check for missing blank lines after class declaration.'\n    if previous_logical.startswith('class '):\n        if (logical_line.startswith(('def ', 'class ', '@')) or pep8.DOCSTRING_REGEX.match(logical_line)):\n            if (indent_level and (not blank_lines)):\n                (yield (0, 'E309 expected 1 blank line after class declaration'))\n    elif previous_logical.startswith('def '):\n        if (blank_lines and pep8.DOCSTRING_REGEX.match(logical_line)):\n            (yield (0, 'E303 too many blank lines ({0})'.format(blank_lines)))\n    elif pep8.DOCSTRING_REGEX.match(previous_logical):\n        if (indent_level and (not blank_lines) and logical_line.startswith('def ') and ('(self' in logical_line)):\n            (yield (0, 'E301 expected 1 blank line, found 0'))\n", "label": 1}
{"function": "\n\ndef _handle_message(self, conn, message):\n    self.total_rdy = max((self.total_rdy - 1), 0)\n    rdy_conn = conn\n    if ((len(self.conns) > self.max_in_flight) and ((time.time() - self.random_rdy_ts) > 30)):\n        self.random_rdy_ts = time.time()\n        conns_with_no_rdy = [c for c in self.conns.itervalues() if (not c.rdy)]\n        if conns_with_no_rdy:\n            rdy_conn = random.choice(conns_with_no_rdy)\n            if (rdy_conn is not conn):\n                logger.info('[%s:%s] redistributing RDY to %s', conn.id, self.name, rdy_conn.id)\n    self._maybe_update_rdy(rdy_conn)\n    success = False\n    try:\n        if (0 < self.max_tries < message.attempts):\n            self.giving_up(message)\n            return message.finish()\n        pre_processed_message = self.preprocess_message(message)\n        if (not self.validate_message(pre_processed_message)):\n            return message.finish()\n        success = self.process_message(message)\n    except Exception:\n        logger.exception('[%s:%s] uncaught exception while handling message %s body:%r', conn.id, self.name, message.id, message.body)\n        if (not message.has_responded()):\n            return message.requeue()\n    if ((not message.is_async()) and (not message.has_responded())):\n        assert (success is not None), 'ambiguous return value for synchronous mode'\n        if success:\n            return message.finish()\n        return message.requeue()\n", "label": 1}
{"function": "\n\ndef xsString(xc, p, source):\n    if isinstance(source, bool):\n        return ('true' if source else 'false')\n    elif isinstance(source, float):\n        if isnan(source):\n            return 'NaN'\n        elif isinf(source):\n            return ('-INF' if (source < 0) else 'INF')\n        \"\\n        numMagnitude = fabs(source)\\n        if numMagnitude < 1000000 and numMagnitude > .000001:\\n            # don't want floating notation which python does for more than 4 decimal places\\n            s = \\n        \"\n        s = str(source)\n        if s.endswith('.0'):\n            s = s[:(- 2)]\n        return s\n    elif isinstance(source, Decimal):\n        if isnan(source):\n            return 'NaN'\n        elif isinf(source):\n            return ('-INF' if (source < 0) else 'INF')\n        return str(source)\n    elif isinstance(source, ModelValue.DateTime):\n        return ('{0:%Y-%m-%d}' if source.dateOnly else '{0:%Y-%m-%dT%H:%M:%S}').format(source)\n    return str(source)\n", "label": 1}
{"function": "\n\ndef search(self, query):\n    response = self.command('search -qbd', query)[0].decode(*enc)\n    from re import match\n    lst = {\n        \n    }\n    desc = False\n    blank = False\n    for line in [l for l in response.splitlines() if (not match('\\\\*\\\\*\\\\*', l))]:\n        if ((not desc) and match('(.*)\\\\((.*)\\\\)', line)):\n            (key, val) = match('(.*)\\\\((.*)\\\\)', line).groups()\n            lst[self.package_name(key)] = (('Version: ' + val) + '\\n')\n            desc = True\n        elif (desc and (not blank) and (not line)):\n            blank = True\n        elif (desc and ('Homepage:' in line)):\n            lst[self.package_name(key)] += (line.replace('Homepage: ', '').strip() + '\\n')\n        elif (desc and blank and line):\n            lst[self.package_name(key)] += (line.strip() + ' ')\n        elif (desc and blank and (not line)):\n            desc = False\n            blank = False\n    return lst\n", "label": 1}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_time_lowmem():\n    ' low memory reading/writing of 3D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(DATA_DIR, 'rnmrtk_3d', 'time_3d.sec'))\n    assert (data.shape == (128, 88, 1250))\n    assert (np.abs((data[(0, 1, 2)].real - 7.98)) <= 0.01)\n    assert (np.abs((data[(0, 1, 2)].imag - 33.82)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].real - (- 9.36))) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].imag - (- 7.75))) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    lowmem_write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef parse_diff(self):\n    sections = []\n    state = None\n    prev_file = None\n    current_file = {\n        \n    }\n    current_hunks = []\n    prev_hunk = None\n    current_hunk = None\n    for line in self.view.lines(sublime.Region(0, self.view.size())):\n        linetext = self.view.substr(line)\n        if linetext.startswith('diff --git'):\n            state = 'header'\n            if (prev_file != line):\n                if (prev_file is not None):\n                    if current_hunk:\n                        current_hunks.append(current_hunk)\n                    sections.append((current_file, current_hunks))\n                prev_file = line\n                prev_hunk = None\n            current_file = line\n            current_hunks = []\n        elif ((state == 'header') and RE_DIFF_HEAD.match(linetext)):\n            current_file = current_file.cover(line)\n        elif linetext.startswith('@@'):\n            state = 'hunk'\n            if (prev_hunk != line):\n                if (prev_hunk is not None):\n                    current_hunks.append(current_hunk)\n                prev_hunk = line\n            current_hunk = line\n        elif ((state == 'hunk') and (linetext[0] in (' ', '-', '+'))):\n            current_hunk = current_hunk.cover(line)\n        elif (state == 'header'):\n            current_file = current_file.cover(line)\n    if (current_file and current_hunk):\n        current_hunks.append(current_hunk)\n        sections.append((current_file, current_hunks))\n    return sections\n", "label": 1}
{"function": "\n\ndef test_can_create_path(self):\n    path = Path({\n        'name': 'Alice',\n    }, 'KNOWS', {\n        'name': 'Bob',\n    })\n    nodes = path.nodes()\n    assert (len(path) == 1)\n    assert (nodes[0]['name'] == 'Alice')\n    assert (path[0].type() == 'KNOWS')\n    assert (nodes[(- 1)]['name'] == 'Bob')\n    path = Path(path, 'KNOWS', {\n        'name': 'Carol',\n    })\n    nodes = path.nodes()\n    assert (len(path) == 2)\n    assert (nodes[0]['name'] == 'Alice')\n    assert (path[0].type() == 'KNOWS')\n    assert (nodes[1]['name'] == 'Bob')\n    path = Path({\n        'name': 'Zach',\n    }, 'KNOWS', path)\n    nodes = path.nodes()\n    assert (len(path) == 3)\n    assert (nodes[0]['name'] == 'Zach')\n    assert (path[0].type() == 'KNOWS')\n    assert (nodes[1]['name'] == 'Alice')\n    assert (path[1].type() == 'KNOWS')\n    assert (nodes[2]['name'] == 'Bob')\n", "label": 1}
{"function": "\n\ndef _eval_pos_neg(self, sign):\n    saw_NON = saw_NOT = False\n    for t in self.args:\n        if t.is_positive:\n            continue\n        elif t.is_negative:\n            sign = (- sign)\n        elif t.is_zero:\n            if all((a.is_finite for a in self.args)):\n                return False\n            return\n        elif t.is_nonpositive:\n            sign = (- sign)\n            saw_NON = True\n        elif t.is_nonnegative:\n            saw_NON = True\n        elif (t.is_positive is False):\n            sign = (- sign)\n            if saw_NOT:\n                return\n            saw_NOT = True\n        elif (t.is_negative is False):\n            if saw_NOT:\n                return\n            saw_NOT = True\n        else:\n            return\n    if ((sign == 1) and (saw_NON is False) and (saw_NOT is False)):\n        return True\n    if (sign < 0):\n        return False\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if ((not issubclass(type(other), Signature)) or (self.return_annotation != other.return_annotation) or (len(self.parameters) != len(other.parameters))):\n        return False\n    other_positions = dict(((param, idx) for (idx, param) in enumerate(other.parameters.keys())))\n    for (idx, (param_name, param)) in enumerate(self.parameters.items()):\n        if (param.kind == _KEYWORD_ONLY):\n            try:\n                other_param = other.parameters[param_name]\n            except KeyError:\n                return False\n            else:\n                if (param != other_param):\n                    return False\n        else:\n            try:\n                other_idx = other_positions[param_name]\n            except KeyError:\n                return False\n            else:\n                if ((idx != other_idx) or (param != other.parameters[param_name])):\n                    return False\n    return True\n", "label": 1}
{"function": "\n\ndef selector(self, output):\n    if isinstance(output, compute_base.Node):\n        return self.parse(output, FieldLists.NODE)\n    elif isinstance(output, compute_base.NodeSize):\n        return self.parse(output, FieldLists.NODE_SIZE)\n    elif isinstance(output, compute_base.NodeImage):\n        return self.parse(output, FieldLists.NODE_IMAGE)\n    elif isinstance(output, compute_base.NodeLocation):\n        return self.parse(output, FieldLists.LOCATION)\n    elif isinstance(output, compute_base.NodeAuthSSHKey):\n        return self.parse(output, FieldLists.NODE_KEY)\n    elif isinstance(output, compute_base.NodeAuthPassword):\n        return self.parse(output, FieldLists.NODE_PASSWORD)\n    elif isinstance(output, compute_base.StorageVolume):\n        return self.parse(output, FieldLists.STORAGE_VOLUME)\n    elif isinstance(output, compute_base.VolumeSnapshot):\n        return self.parse(output, FieldLists.VOLUME_SNAPSHOT)\n    elif isinstance(output, dns_base.Zone):\n        return self.parse(output, FieldLists.ZONE)\n    elif isinstance(output, dns_base.Record):\n        return self.parse(output, FieldLists.RECORD)\n    elif isinstance(output, lb_base.Member):\n        return self.parse(output, FieldLists.MEMBER)\n    elif isinstance(output, lb_base.LoadBalancer):\n        return self.parse(output, FieldLists.BALANCER)\n    elif isinstance(output, container_base.Container):\n        return self.parse(output, FieldLists.CONTAINER)\n    elif isinstance(output, container_base.ContainerImage):\n        return self.parse(output, FieldLists.CONTAINER_IMAGE)\n    elif isinstance(output, container_base.ContainerCluster):\n        return self.parse(output, FieldLists.CONTAINER_CLUSTER)\n    else:\n        return output\n", "label": 1}
{"function": "\n\ndef __init__(self, content, name=None, namespace=None, stacklevel=None, get_template=None, default_inherit=None, line_offset=0, delimeters=None):\n    self.content = content\n    if (delimeters is None):\n        delimeters = (self.default_namespace['start_braces'], self.default_namespace['end_braces'])\n    else:\n        assert ((len(delimeters) == 2) and all([isinstance(delimeter, basestring) for delimeter in delimeters]))\n        self.default_namespace = self.__class__.default_namespace.copy()\n        self.default_namespace['start_braces'] = delimeters[0]\n        self.default_namespace['end_braces'] = delimeters[1]\n    self.delimeters = delimeters\n    self._unicode = is_unicode(content)\n    if ((name is None) and (stacklevel is not None)):\n        try:\n            caller = sys._getframe(stacklevel)\n        except ValueError:\n            pass\n        else:\n            globals = caller.f_globals\n            lineno = caller.f_lineno\n            if ('__file__' in globals):\n                name = globals['__file__']\n                if (name.endswith('.pyc') or name.endswith('.pyo')):\n                    name = name[:(- 1)]\n            elif ('__name__' in globals):\n                name = globals['__name__']\n            else:\n                name = '<string>'\n            if lineno:\n                name += (':%s' % lineno)\n    self.name = name\n    self._parsed = parse(content, name=name, line_offset=line_offset, delimeters=self.delimeters)\n    if (namespace is None):\n        namespace = {\n            \n        }\n    self.namespace = namespace\n    self.get_template = get_template\n    if (default_inherit is not None):\n        self.default_inherit = default_inherit\n", "label": 1}
{"function": "\n\ndef line_ends_with_close_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)\\\\*/(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(/\\\\*)(.*)\\\\*/(\\\\s*)$', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\"\"\")(.*)\"\"\"(\\\\s*)$', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\->(\\\\s*)$)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)(\\\\s*)$', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=end)(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)(.*)(\\\\=end)(\\\\s*)$', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\})(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)(.*)(%\\\\})(\\\\s*)$', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef parse_challenge(stuff):\n    '\\n    '\n    ret = {\n        \n    }\n    var = b''\n    val = b''\n    in_var = True\n    in_quotes = False\n    new = False\n    escaped = False\n    for c in stuff:\n        if (sys.version_info >= (3, 0)):\n            c = bytes([c])\n        if in_var:\n            if c.isspace():\n                continue\n            if (c == b'='):\n                in_var = False\n                new = True\n            else:\n                var += c\n        elif new:\n            if (c == b'\"'):\n                in_quotes = True\n            else:\n                val += c\n            new = False\n        elif in_quotes:\n            if escaped:\n                escaped = False\n                val += c\n            elif (c == b'\\\\'):\n                escaped = True\n            elif (c == b'\"'):\n                in_quotes = False\n            else:\n                val += c\n        elif (c == b','):\n            if var:\n                ret[var] = val\n            var = b''\n            val = b''\n            in_var = True\n        else:\n            val += c\n    if var:\n        ret[var] = val\n    return ret\n", "label": 1}
{"function": "\n\ndef localStr(self, local_addr=None, local_port=None):\n    l = []\n    w = l.append\n    w('sip:')\n    if (self.username != None):\n        w(self.username)\n        for v in self.userparams:\n            w((';%s' % v))\n        if (self.password != None):\n            w((':%s' % self.password))\n        w('@')\n    if ((local_addr != None) and ('my' in dir(self.host))):\n        w(local_addr)\n    else:\n        w(str(self.host))\n    if (self.port != None):\n        if ((local_port != None) and ('my' in dir(self.port))):\n            w((':%d' % local_port))\n        else:\n            w((':%d' % self.port))\n    if (self.usertype != None):\n        w((';user=%s' % self.usertype))\n    for n in ('transport', 'ttl', 'maddr', 'method', 'tag'):\n        v = getattr(self, n)\n        if (v != None):\n            w((';%s=%s' % (n, v)))\n    if self.lr:\n        w(';lr')\n    for v in self.other:\n        w((';%s' % v))\n    if self.headers:\n        w('?')\n        w('&'.join([('%s=%s' % (h.capitalize(), quote(v))) for (h, v) in self.headers.items()]))\n    return ''.join(l)\n", "label": 1}
{"function": "\n\ndef test_info():\n    (m, ctl, config) = init()\n    m.load(config)\n    cmd = TestCommand('info', ['dummy'])\n    ctl.process_command(cmd)\n    m.scale('dummy', 2)\n    time.sleep(0.1)\n    cmd1 = TestCommand('info', ['dummy'])\n    ctl.process_command(cmd1)\n    m.stop()\n    m.run()\n    assert isinstance(cmd.result, dict)\n    assert ('info' in cmd.result)\n    info = cmd.result['info']\n    assert (info['name'] == 'default.dummy')\n    assert (info['active'] == True)\n    assert (info['running'] == 1)\n    assert (info['max_processes'] == 1)\n    assert ('config' in info)\n    assert (info['config'] == config.to_dict())\n    assert isinstance(cmd1.result, dict)\n    assert ('info' in cmd1.result)\n    info1 = cmd1.result['info']\n    assert (info1['name'] == 'default.dummy')\n    assert (info1['running'] == 3)\n    assert (info1['config'] == config.to_dict())\n", "label": 1}
{"function": "\n\ndef open_url(url, wait=False, locate=False):\n    import subprocess\n\n    def _unquote_file(url):\n        try:\n            import urllib\n        except ImportError:\n            import urllib\n        if url.startswith('file://'):\n            url = urllib.unquote(url[7:])\n        return url\n    if (sys.platform == 'darwin'):\n        args = ['open']\n        if wait:\n            args.append('-W')\n        if locate:\n            args.append('-R')\n        args.append(_unquote_file(url))\n        null = open('/dev/null', 'w')\n        try:\n            return subprocess.Popen(args, stderr=null).wait()\n        finally:\n            null.close()\n    elif WIN:\n        if locate:\n            url = _unquote_file(url)\n            args = ('explorer /select,\"%s\"' % _unquote_file(url.replace('\"', '')))\n        else:\n            args = ('start %s \"\" \"%s\"' % (((wait and '/WAIT') or ''), url.replace('\"', '')))\n        return os.system(args)\n    try:\n        if locate:\n            url = (os.path.dirname(_unquote_file(url)) or '.')\n        else:\n            url = _unquote_file(url)\n        c = subprocess.Popen(['xdg-open', url])\n        if wait:\n            return c.wait()\n        return 0\n    except OSError:\n        if (url.startswith(('http://', 'https://')) and (not locate) and (not wait)):\n            import webbrowser\n            webbrowser.open(url)\n            return 0\n        return 1\n", "label": 1}
{"function": "\n\ndef execute(self, write_concern=None):\n    if (not self.executors):\n        raise InvalidOperation('Bulk operation empty!')\n    if self.done:\n        raise InvalidOperation('Bulk operation already executed!')\n    self.done = True\n    result = {\n        'nModified': 0,\n        'nUpserted': 0,\n        'nMatched': 0,\n        'writeErrors': [],\n        'upserted': [],\n        'writeConcernErrors': [],\n        'nRemoved': 0,\n        'nInserted': 0,\n    }\n    has_update = False\n    has_insert = False\n    broken_nModified_info = False\n    for execute_func in self.executors:\n        exec_name = execute_func.__name__\n        op_result = execute_func()\n        for (key, value) in op_result.items():\n            self.__aggregate_operation_result(result, key, value)\n        if (exec_name == 'exec_update'):\n            has_update = True\n            if ('nModified' not in op_result):\n                broken_nModified_info = True\n        has_insert |= (exec_name == 'exec_insert')\n    if broken_nModified_info:\n        result.pop('nModified')\n    elif (has_insert and self._insert_returns_nModified):\n        pass\n    elif (has_update and self._update_returns_nModified):\n        pass\n    elif (self._update_returns_nModified and self._insert_returns_nModified):\n        pass\n    else:\n        result.pop('nModified')\n    return result\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.caseless:\n        if ((instring[loc:(loc + self.matchLen)].upper() == self.caselessmatch) and ((loc >= (len(instring) - self.matchLen)) or (instring[(loc + self.matchLen)].upper() not in self.identChars)) and ((loc == 0) or (instring[(loc - 1)].upper() not in self.identChars))):\n            return ((loc + self.matchLen), self.match)\n    elif ((instring[loc] == self.firstMatchChar) and ((self.matchLen == 1) or instring.startswith(self.match, loc)) and ((loc >= (len(instring) - self.matchLen)) or (instring[(loc + self.matchLen)] not in self.identChars)) and ((loc == 0) or (instring[(loc - 1)] not in self.identChars))):\n        return ((loc + self.matchLen), self.match)\n    raise ParseException(instring, loc, self.errmsg, self)\n", "label": 1}
{"function": "\n\ndef determine_result(self, returncode, returnsignal, output, isTimeout):\n    if ((returnsignal == 0) and (returncode == 0)):\n        status = None\n        for line in output:\n            line = line.strip()\n            if (line == 'unsat'):\n                status = result.RESULT_UNSAT\n            elif (line == 'sat'):\n                status = result.RESULT_SAT\n            elif ((not status) and line.startswith('(error ')):\n                status = 'ERROR'\n        if (not status):\n            status = result.RESULT_UNKNOWN\n    elif (((returnsignal == 9) or (returnsignal == 15)) and isTimeout):\n        status = 'TIMEOUT'\n    elif (returnsignal == 9):\n        status = 'KILLED BY SIGNAL 9'\n    elif (returnsignal == 6):\n        status = 'ABORTED'\n    elif (returnsignal == 15):\n        status = 'KILLED'\n    else:\n        status = 'ERROR ({0})'.format(returncode)\n    return status\n", "label": 1}
{"function": "\n\ndef getNewHeader(self, requestInfo, token, isCookie):\n    headers = requestInfo.getHeaders()\n    if isCookie:\n        cookieHeader = 'Cookie:'\n        newheader = cookieHeader\n        previousCookies = []\n        for header in requestInfo.getHeaders():\n            if str(header).startswith(cookieHeader):\n                previousCookies = str(header)[len(cookieHeader):].replace(' ', '').split(';')\n                headers.remove(header)\n        newCookies = token.replace(' ', '').split(';')\n        newCookieVariableNames = []\n        for newCookie in newCookies:\n            equalsToken = newCookie.find('=')\n            if (equalsToken >= 0):\n                newCookieVariableNames.append(newCookie[0:(equalsToken + 1)])\n        for previousCookie in previousCookies:\n            equalsToken = previousCookie.find('=')\n            if (equalsToken >= 0):\n                if (previousCookie[0:(equalsToken + 1)] not in newCookieVariableNames):\n                    newCookies.append(previousCookie)\n        newCookies = [x for x in newCookies if x]\n        newheader = ((cookieHeader + ' ') + ';'.join(newCookies))\n    else:\n        newheader = token\n        colon = newheader.find(':')\n        if (colon >= 0):\n            for header in requestInfo.getHeaders():\n                if str(header).startswith(newheader[0:(colon + 1)]):\n                    headers.remove(header)\n    headers.add(newheader)\n    return headers\n", "label": 1}
{"function": "\n\ndef _handle_actions(self, state, current_run, func, sp_addr, accessed_registers):\n    se = state.se\n    if ((func is not None) and (sp_addr is not None)):\n        new_sp_addr = (sp_addr + self.project.arch.call_sp_fix)\n        actions = [a for a in state.log.actions if (a.bbl_addr == current_run.addr)]\n        for a in actions:\n            if ((a.type == 'mem') and (a.action == 'read')):\n                try:\n                    addr = se.exactly_int(a.addr.ast, default=0)\n                except claripy.ClaripyError:\n                    continue\n                if ((self.project.arch.call_pushes_ret and (addr >= new_sp_addr)) or ((not self.project.arch.call_pushes_ret) and (addr >= new_sp_addr))):\n                    offset = (addr - new_sp_addr)\n                    func._add_argument_stack_variable(offset)\n            elif (a.type == 'reg'):\n                offset = a.offset\n                if ((a.action == 'read') and (offset not in accessed_registers)):\n                    func._add_argument_register(offset)\n                elif (a.action == 'write'):\n                    accessed_registers.add(offset)\n    else:\n        l.error('handle_actions: Function not found, or stack pointer is None. It might indicates unbalanced stack.')\n", "label": 1}
{"function": "\n\ndef test_scan_video_broken(mkv, tmpdir, monkeypatch):\n    broken_path = 'test1.mkv'\n    with io.open(mkv['test1'], 'rb') as original:\n        with tmpdir.join(broken_path).open('wb') as broken:\n            broken.write(original.read(512))\n    monkeypatch.chdir(str(tmpdir))\n    scanned_video = scan_video(broken_path)\n    assert (type(scanned_video) is Movie)\n    assert (scanned_video.name == str(broken_path))\n    assert (scanned_video.format is None)\n    assert (scanned_video.release_group is None)\n    assert (scanned_video.resolution is None)\n    assert (scanned_video.video_codec is None)\n    assert (scanned_video.audio_codec is None)\n    assert (scanned_video.imdb_id is None)\n    assert (scanned_video.hashes == {\n        \n    })\n    assert (scanned_video.size == 512)\n    assert (scanned_video.subtitle_languages == set())\n    assert (scanned_video.title == 'test1')\n    assert (scanned_video.year is None)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if (other is ANY):\n        return True\n    try:\n        len_other = len(other)\n    except TypeError:\n        return False\n    self_name = ''\n    if (len(self) == 2):\n        (self_args, self_kwargs) = self\n    else:\n        (self_name, self_args, self_kwargs) = self\n    other_name = ''\n    if (len_other == 0):\n        (other_args, other_kwargs) = ((), {\n            \n        })\n    elif (len_other == 3):\n        (other_name, other_args, other_kwargs) = other\n    elif (len_other == 1):\n        (value,) = other\n        if isinstance(value, tuple):\n            other_args = value\n            other_kwargs = {\n                \n            }\n        elif isinstance(value, basestring):\n            other_name = value\n            (other_args, other_kwargs) = ((), {\n                \n            })\n        else:\n            other_args = ()\n            other_kwargs = value\n    elif (len_other == 2):\n        (first, second) = other\n        if isinstance(first, basestring):\n            other_name = first\n            if isinstance(second, tuple):\n                (other_args, other_kwargs) = (second, {\n                    \n                })\n            else:\n                (other_args, other_kwargs) = ((), second)\n        else:\n            (other_args, other_kwargs) = (first, second)\n    else:\n        return False\n    if (self_name and (other_name != self_name)):\n        return False\n    return ((other_args, other_kwargs) == (self_args, self_kwargs))\n", "label": 1}
{"function": "\n\ndef transform_source_batch(self, source, source_name):\n    self.verify_axis_labels(('batch', 'channel', 'height', 'width'), self.data_stream.axis_labels[source_name], source_name)\n    rotation_angles = self.rng.uniform((- self.maximum_rotation), self.maximum_rotation, len(source))\n    if (isinstance(source, list) and all(((isinstance(b, numpy.ndarray) and (b.ndim == 3)) for b in source))):\n        return [self._example_transform(im, angle) for (im, angle) in zip(source, rotation_angles)]\n    elif (isinstance(source, numpy.ndarray) and (source.dtype == object) and all(((isinstance(b, numpy.ndarray) and (b.ndim == 3)) for b in source))):\n        out = numpy.empty(len(source), dtype=object)\n        for (im_idx, (im, angle)) in enumerate(zip(source, rotation_angles)):\n            out[im_idx] = self._example_transform(im, angle)\n        return out\n    elif (isinstance(source, numpy.ndarray) and (source.ndim == 4)):\n        return numpy.array([self._example_transform(im, angle) for (im, angle) in zip(source, rotation_angles)], dtype=source.dtype)\n    else:\n        raise ValueError('uninterpretable batch format; expected a list of arrays with ndim = 3, or an array with ndim = 4')\n", "label": 1}
{"function": "\n\ndef start_container(self, conf, tty=True, detach=False, is_dependency=False, no_intervention=False):\n    'Start up a single container'\n    container_id = conf.container_id\n    container_name = conf.container_name\n    log.info('Starting container %s (%s)', container_name, container_id)\n    try:\n        if ((not detach) and (not is_dependency)):\n            self.start_tty(conf, interactive=tty, **conf.other_options.start)\n        else:\n            conf.harpoon.docker_context.start(container_id, **conf.other_options.start)\n    except docker.errors.APIError as error:\n        if str(error).startswith('404 Client Error: Not Found'):\n            log.error('Container died before we could even get to it...')\n    inspection = None\n    if ((not detach) and (not is_dependency)):\n        inspection = self.get_exit_code(conf)\n    if (inspection and (not no_intervention)):\n        if ((not inspection['State']['Running']) and (inspection['State']['ExitCode'] != 0)):\n            self.stage_run_intervention(conf)\n            raise BadImage('Failed to run container', container_id=container_id, container_name=container_name, reason='nonzero exit code after launch')\n    if ((not is_dependency) and conf.harpoon.intervene_afterwards and (not no_intervention)):\n        self.stage_run_intervention(conf, just_do_it=True)\n", "label": 1}
{"function": "\n\ndef get_features2(self):\n    '\\n        Return all features with its names.\\n\\n        Returns\\n        -------\\n        names : list\\n            Feature names.\\n        values : list\\n            Feature values\\n        '\n    feature_names = []\n    feature_values = []\n    all_vars = vars(self)\n    for name in all_vars.keys():\n        if (not ((name == 'date') or (name == 'mag') or (name == 'err') or (name == 'n_threads') or (name == 'min_period'))):\n            if (not ((name == 'f') or (name == 'f_phase') or (name == 'period_log10FAP') or (name == 'weight') or (name == 'weighted_sum') or (name == 'median') or (name == 'mean') or (name == 'std'))):\n                feature_names.append(name)\n    feature_names.sort()\n    for name in feature_names:\n        feature_values.append(all_vars[name])\n    return (feature_names, feature_values)\n", "label": 1}
{"function": "\n\ndef delete_top_level(self, context=None):\n    \"\\n        Delete the top level calls which are not part of the user's code.\\n\\n        TODO: If CELL_MAGIC then also merge the entries for the cell (which are\\n        seperated by line into individual code objects...)\\n        \"\n    if (context == 'LINE_MAGIC'):\n        new_roots = []\n        for function in self.cprofile_tree:\n            try:\n                if (function.co_name == '<module>'):\n                    new_roots += self.cprofile_tree[function]['calls']\n            except AttributeError:\n                pass\n        new_roots = [r for r in new_roots if ((type(r) == str) or (r.co_name != '<module>'))]\n        for i in range(len(new_roots)):\n            function = new_roots[i]\n            try:\n                if (function.co_name == '<module>'):\n                    del new_roots[i]\n            except AttributeError:\n                pass\n    if (context == 'CELL_MAGIC'):\n        new_roots = []\n        for function in self.cprofile_tree:\n            try:\n                if ('<ipython-input' in function.co_filename):\n                    new_roots += self.cprofile_tree[function]['calls']\n            except AttributeError:\n                pass\n    new_cprofile_tree = {\n        \n    }\n\n    def populate_new_tree(roots):\n        for root in roots:\n            if (root not in new_cprofile_tree):\n                new_cprofile_tree[root] = self.cprofile_tree[root]\n                populate_new_tree(self.cprofile_tree[root]['calls'])\n    populate_new_tree(new_roots)\n    self.cprofile_tree = new_cprofile_tree\n    self.roots = new_roots\n", "label": 1}
{"function": "\n\ndef test_scan_video_episode(episodes, tmpdir, monkeypatch):\n    video = episodes['bbt_s07e05']\n    monkeypatch.chdir(str(tmpdir))\n    tmpdir.ensure(video.name)\n    scanned_video = scan_video(video.name)\n    assert scanned_video.name, video.name\n    assert (scanned_video.format == video.format)\n    assert (scanned_video.release_group == video.release_group)\n    assert (scanned_video.resolution == video.resolution)\n    assert (scanned_video.video_codec == video.video_codec)\n    assert (scanned_video.audio_codec is None)\n    assert (scanned_video.imdb_id is None)\n    assert (scanned_video.hashes == {\n        \n    })\n    assert (scanned_video.size == 0)\n    assert (scanned_video.subtitle_languages == set())\n    assert (scanned_video.series == video.series)\n    assert (scanned_video.season == video.season)\n    assert (scanned_video.episode == video.episode)\n    assert (scanned_video.title is None)\n    assert (scanned_video.year is None)\n    assert (scanned_video.tvdb_id is None)\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    _get_chronos_service_config(args)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.chronos_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                stop_job(args, hosts[host_id].ip, job_name, instance_id)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.chronos_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                deploy_utils.wait_for_job_stopping('chronos', args.chronos_config.cluster.name, job_name, hosts[host_id].ip, instance_id)\n                start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)\n", "label": 1}
{"function": "\n\ndef test_m2o_lazy_loader_on_transient(self):\n    for loadonpending in (False, True):\n        for attach in (False, True):\n            for autoflush in (False, True):\n                for manualflush in (False, True):\n                    for enable_relationship_rel in (False, True):\n                        Child.parent.property.load_on_pending = loadonpending\n                        sess.autoflush = autoflush\n                        c2 = Child()\n                        if attach:\n                            state = instance_state(c2)\n                            state.session_id = sess.hash_key\n                        if enable_relationship_rel:\n                            sess.enable_relationship_loading(c2)\n                        c2.parent_id = p2.id\n                        if manualflush:\n                            sess.flush()\n                        if ((loadonpending and attach) or enable_relationship_rel):\n                            assert (c2.parent is p2)\n                        else:\n                            assert (c2.parent is None)\n                        sess.rollback()\n", "label": 1}
{"function": "\n\ndef _setup_document_fields(self):\n    for f in self.document._fields.values():\n        if (not hasattr(f, 'rel')):\n            if isinstance(f, ReferenceField):\n                f.rel = Relation(f.document_type)\n                f.is_relation = True\n            elif (isinstance(f, ListField) and isinstance(f.field, ReferenceField)):\n                f.field.rel = Relation(f.field.document_type)\n                f.field.is_relation = True\n            else:\n                f.many_to_many = None\n                f.many_to_one = None\n                f.one_to_many = None\n                f.one_to_one = None\n                f.related_model = None\n                f.rel = None\n                f.is_relation = False\n        if ((not hasattr(f, 'verbose_name')) or (f.verbose_name is None)):\n            f.verbose_name = capfirst(create_verbose_name(f.name))\n        if (not hasattr(f, 'flatchoices')):\n            flat = []\n            if (f.choices is not None):\n                for (choice, value) in f.choices:\n                    if isinstance(value, (list, tuple)):\n                        flat.extend(value)\n                    else:\n                        flat.append((choice, value))\n            f.flatchoices = flat\n        if (isinstance(f, ReferenceField) and (not isinstance(f.document_type._meta, (DocumentMetaWrapper, LazyDocumentMetaWrapper))) and (self.document != f.document_type)):\n            f.document_type._meta = LazyDocumentMetaWrapper(f.document_type)\n        if (not hasattr(f, 'auto_created')):\n            f.auto_created = False\n", "label": 1}
{"function": "\n\ndef _read_one_coil_point(fid):\n    'Read coil coordinate information from the hc file'\n    one = '#'\n    while ((len(one) > 0) and (one[0] == '#')):\n        one = fid.readline()\n    if (len(one) == 0):\n        return None\n    one = one.strip().decode('utf-8')\n    if ('Unable' in one):\n        raise RuntimeError('HPI information not available')\n    p = dict()\n    p['valid'] = ('measured' in one)\n    for (key, val) in _coord_dict.items():\n        if (key in one):\n            p['coord_frame'] = val\n            break\n    else:\n        p['coord_frame'] = (- 1)\n    for (key, val) in _kind_dict.items():\n        if (key in one):\n            p['kind'] = val\n            break\n    else:\n        p['kind'] = (- 1)\n    p['r'] = np.empty(3)\n    for (ii, coord) in enumerate('xyz'):\n        sp = fid.readline().decode('utf-8').strip()\n        if (len(sp) == 0):\n            continue\n        sp = sp.split(' ')\n        if ((len(sp) != 3) or (sp[0] != coord) or (sp[1] != '=')):\n            raise RuntimeError(('Bad line: %s' % one))\n        p['r'][ii] = (float(sp[2]) / 100.0)\n    return p\n", "label": 1}
{"function": "\n\ndef asXML(self, doctag=None, namedItemsOnly=False, indent='', formatted=True):\n    'Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.'\n    nl = '\\n'\n    out = []\n    namedItems = dict(((v[1], k) for (k, vlist) in self.__tokdict.items() for v in vlist))\n    nextLevelIndent = (indent + '  ')\n    if (not formatted):\n        indent = ''\n        nextLevelIndent = ''\n        nl = ''\n    selfTag = None\n    if (doctag is not None):\n        selfTag = doctag\n    elif self.__name:\n        selfTag = self.__name\n    if (not selfTag):\n        if namedItemsOnly:\n            return ''\n        else:\n            selfTag = 'ITEM'\n    out += [nl, indent, '<', selfTag, '>']\n    worklist = self.__toklist\n    for (i, res) in enumerate(worklist):\n        if isinstance(res, ParseResults):\n            if (i in namedItems):\n                out += [res.asXML(namedItems[i], (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n            else:\n                out += [res.asXML(None, (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n        else:\n            resTag = None\n            if (i in namedItems):\n                resTag = namedItems[i]\n            if (not resTag):\n                if namedItemsOnly:\n                    continue\n                else:\n                    resTag = 'ITEM'\n            xmlBodyText = _xml_escape(_ustr(res))\n            out += [nl, nextLevelIndent, '<', resTag, '>', xmlBodyText, '</', resTag, '>']\n    out += [nl, indent, '</', selfTag, '>']\n    return ''.join(out)\n", "label": 1}
{"function": "\n\ndef _restore_object_instance(self, obj, cls):\n    proxy = _Proxy()\n    self._mkref(proxy)\n    factory = self._loadfactory(obj)\n    if has_tag(obj, tags.NEWARGSEX):\n        (args, kwargs) = obj[tags.NEWARGSEX]\n    else:\n        args = getargs(obj)\n        kwargs = {\n            \n        }\n    if args:\n        args = self._restore(args)\n    if kwargs:\n        kwargs = self._restore(kwargs)\n    is_oldstyle = (not (isinstance(cls, type) or getattr(cls, '__meta__', None)))\n    try:\n        if ((not is_oldstyle) and hasattr(cls, '__new__')):\n            if factory:\n                instance = cls.__new__(cls, factory, *args, **kwargs)\n                instance.default_factory = factory\n            else:\n                instance = cls.__new__(cls, *args, **kwargs)\n        else:\n            instance = object.__new__(cls)\n    except TypeError:\n        is_oldstyle = True\n    if is_oldstyle:\n        try:\n            instance = cls(*args)\n        except TypeError:\n            try:\n                instance = make_blank_classic(cls)\n            except:\n                return self._mkref(obj)\n    proxy.reset(instance)\n    self._swapref(proxy, instance)\n    if isinstance(instance, tuple):\n        return instance\n    if (hasattr(instance, 'default_factory') and isinstance(instance.default_factory, _Proxy)):\n        instance.default_factory = instance.default_factory.get()\n    return self._restore_object_instance_variables(obj, instance)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.validation_class = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.index_type = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.index_name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef on_text_command(self, view, cmd, args):\n    if (isearch_info_for(view) is not None):\n        if (cmd not in ('sbp_inc_search', 'sbp_inc_search_escape')):\n            return ('sbp_inc_search_escape', {\n                'next_cmd': cmd,\n                'next_args': args,\n            })\n        return\n    vs = ViewState.get(view)\n    self.on_anything(view)\n    if (args is None):\n        args = {\n            \n        }\n    if (not cmd.startswith('sbp_')):\n        vs.this_cmd = cmd\n    if (cmd == 'drag_select'):\n        info = isearch_info_for(view)\n        if info:\n            info.done()\n        vs.drag_count = (2 if ('by' in args) else 0)\n    if ((cmd in ('move', 'move_to')) and vs.active_mark and (not args.get('extend', False))):\n        args['extend'] = True\n        return (cmd, args)\n    if (not vs.argument_supplied):\n        return None\n    if (cmd in repeatable_cmds):\n        count = vs.get_count()\n        args.update({\n            'cmd': cmd,\n            '_times': abs(count),\n        })\n        if ((count < 0) and ('forward' in args)):\n            args['forward'] = (not args['forward'])\n        return ('sbp_do_times', args)\n    elif (cmd == 'scroll_lines'):\n        args['amount'] *= vs.get_count()\n        return (cmd, args)\n", "label": 1}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_service_call_name():\n        self.set_service_call_name(x.service_call_name())\n    if x.has_request_data_summary():\n        self.set_request_data_summary(x.request_data_summary())\n    if x.has_response_data_summary():\n        self.set_response_data_summary(x.response_data_summary())\n    if x.has_api_mcycles():\n        self.set_api_mcycles(x.api_mcycles())\n    if x.has_api_milliseconds():\n        self.set_api_milliseconds(x.api_milliseconds())\n    if x.has_start_offset_milliseconds():\n        self.set_start_offset_milliseconds(x.start_offset_milliseconds())\n    if x.has_duration_milliseconds():\n        self.set_duration_milliseconds(x.duration_milliseconds())\n    if x.has_namespace():\n        self.set_namespace(x.namespace())\n    if x.has_was_successful():\n        self.set_was_successful(x.was_successful())\n    for i in xrange(x.call_stack_size()):\n        self.add_call_stack().CopyFrom(x.call_stack(i))\n    if x.has_datastore_details():\n        self.mutable_datastore_details().MergeFrom(x.datastore_details())\n    if x.has_call_cost_microdollars():\n        self.set_call_cost_microdollars(x.call_cost_microdollars())\n    for i in xrange(x.billed_ops_size()):\n        self.add_billed_ops().CopyFrom(x.billed_ops(i))\n", "label": 1}
{"function": "\n\ndef _dynamic_init(self, only_fields, include_fields, exclude_fields):\n    \"\\n        Modifies `request_fields` via higher-level dynamic field interfaces.\\n\\n        Arguments:\\n            only_fields: List of field names to render.\\n                All other fields will be deferred (respects sideloads).\\n            include_fields: List of field names to include.\\n                Adds to default field set, (respects sideloads).\\n                `*` means include all fields.\\n            exclude_fields: List of field names to exclude.\\n                Removes from default field set. If set to '*', all fields are\\n                removed, except for ones that are explicitly included.\\n        \"\n    if (not self.dynamic):\n        return\n    if (isinstance(self.request_fields, dict) and (self.request_fields.pop('*', None) is False)):\n        exclude_fields = '*'\n    only_fields = set((only_fields or []))\n    include_fields = (include_fields or [])\n    exclude_fields = (exclude_fields or [])\n    all_fields = set(self.get_all_fields().keys())\n    if only_fields:\n        exclude_fields = '*'\n        include_fields = only_fields\n    if (exclude_fields == '*'):\n        include_fields = set((list(include_fields) + [field for (field, val) in six.iteritems(self.request_fields) if (val or (val == {\n            \n        }))]))\n        exclude_fields = (all_fields - include_fields)\n    elif (include_fields == '*'):\n        include_fields = all_fields\n    for name in exclude_fields:\n        self.request_fields[name] = False\n    for name in include_fields:\n        if (not isinstance(self.request_fields.get(name), dict)):\n            self.request_fields[name] = True\n", "label": 1}
{"function": "\n\ndef Validate(self):\n    'Check the source is well constructed.'\n    if (self.type == 'COMMAND'):\n        args = self.attributes.GetItem('args')\n        if (args and (len(args) == 1) and (' ' in args[0])):\n            raise ArtifactDefinitionError(('Cannot specify a single argument containing a space: %s.' % args))\n    if self.attributes.GetItem('paths'):\n        if (not isinstance(self.attributes.GetItem('paths'), list)):\n            raise ArtifactDefinitionError(\"Arg 'paths' that is not a list.\")\n    if self.attributes.GetItem('path'):\n        if (not isinstance(self.attributes.GetItem('path'), basestring)):\n            raise ArtifactDefinitionError(\"Arg 'path' is not a string.\")\n    if self.returned_types:\n        for rdf_type in self.returned_types:\n            if (rdf_type not in rdfvalue.RDFValue.classes):\n                raise ArtifactDefinitionError(('Invalid return type %s' % rdf_type))\n    src_type = self.TYPE_MAP.get(str(self.type))\n    if (src_type is None):\n        raise ArtifactDefinitionError(('Invalid type %s.' % self.type))\n    required_attributes = src_type.get('required_attributes', [])\n    missing_attributes = set(required_attributes).difference(self.attributes.keys())\n    if missing_attributes:\n        raise ArtifactDefinitionError(('Missing required attributes: %s.' % missing_attributes))\n", "label": 1}
{"function": "\n\ndef extract_chunk(self, nfaces=100, seed=None, auxpts=None):\n    'Extract a chunk of the surface using breadth first search, for testing purposes'\n    node = seed\n    if (seed is None):\n        node = np.random.randint(len(self.pts))\n    ptmap = dict()\n    queue = [node]\n    faces = set()\n    visited = set([node])\n    while ((len(faces) < nfaces) and (len(queue) > 0)):\n        node = queue.pop(0)\n        for face in self.connected[node].indices:\n            if (face not in faces):\n                faces.add(face)\n                for pt in self.polys[face]:\n                    if (pt not in visited):\n                        visited.add(pt)\n                        queue.append(pt)\n    (pts, aux, polys) = ([], [], [])\n    for face in faces:\n        for pt in self.polys[face]:\n            if (pt not in ptmap):\n                ptmap[pt] = len(pts)\n                pts.append(self.pts[pt])\n                if (auxpts is not None):\n                    aux.append(auxpts[pt])\n        polys.append([ptmap[p] for p in self.polys[face]])\n    if (auxpts is not None):\n        return (np.array(pts), np.array(aux), np.array(polys))\n    return (np.array(pts), np.array(polys))\n", "label": 1}
{"function": "\n\ndef gather_candidates(self, context):\n    p = self.__longest_path_that_exists(context['input'])\n    if ((p in (None, [])) or (p == '/') or re.search('//+$', p)):\n        return []\n    complete_str = self.__substitute_path((dirname(p) + '/'))\n    if (not os.path.isdir(complete_str)):\n        return []\n    hidden = (context['complete_str'].find('.') == 0)\n    dirs = [x for x in os.listdir(complete_str) if (os.path.isdir((complete_str + x)) and (hidden or (x[0] != '.')))]\n    files = [x for x in os.listdir(complete_str) if ((not os.path.isdir((complete_str + x))) and (hidden or (x[0] != '.')))]\n    return ([{\n        'word': x,\n        'abbr': (x + '/'),\n    } for x in sorted(dirs)] + [{\n        'word': x,\n    } for x in sorted(files)])\n", "label": 1}
{"function": "\n\ndef test_video_fromname_movie(movies):\n    video = Video.fromname(movies['man_of_steel'].name)\n    assert (type(video) is Movie)\n    assert (video.name == movies['man_of_steel'].name)\n    assert (video.format == movies['man_of_steel'].format)\n    assert (video.release_group == movies['man_of_steel'].release_group)\n    assert (video.resolution == movies['man_of_steel'].resolution)\n    assert (video.video_codec == movies['man_of_steel'].video_codec)\n    assert (video.audio_codec is None)\n    assert (video.imdb_id is None)\n    assert (video.hashes == {\n        \n    })\n    assert (video.size is None)\n    assert (video.subtitle_languages == set())\n    assert (video.title == movies['man_of_steel'].title)\n    assert (video.year == movies['man_of_steel'].year)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _required(field):\n    '\\n            Determine whether field is required\\n\\n            @param field: the Field or a Storage with field information\\n            @return: True if field is required, else False\\n        '\n    required = False\n    validators = field.requires\n    if isinstance(validators, IS_EMPTY_OR):\n        required = False\n    else:\n        required = (field.required or field.notnull)\n    if ((not required) and validators):\n        if (not isinstance(validators, (list, tuple))):\n            validators = [validators]\n        for v in validators:\n            if hasattr(v, 'options'):\n                if (hasattr(v, 'zero') and (v.zero is None)):\n                    continue\n            if hasattr(v, 'mark_required'):\n                if v.mark_required:\n                    required = True\n                    break\n                else:\n                    continue\n            try:\n                (val, error) = v('')\n            except TypeError:\n                pass\n            else:\n                if error:\n                    required = True\n                    break\n    return required\n", "label": 1}
{"function": "\n\ndef test_primitive():\n    assert ((3 * ((x + 1) ** 2)).primitive() == (3, ((x + 1) ** 2)))\n    assert (((6 * x) + 2).primitive() == (2, ((3 * x) + 1)))\n    assert (((x / 2) + 3).primitive() == ((S(1) / 2), (x + 6)))\n    eq = (((6 * x) + 2) * ((x / 2) + 3))\n    assert (eq.primitive()[0] == 1)\n    eq = ((2 + (2 * x)) ** 2)\n    assert (eq.primitive()[0] == 1)\n    assert ((4.0 * x).primitive() == (1, (4.0 * x)))\n    assert (((4.0 * x) + (y / 2)).primitive() == (S.Half, ((8.0 * x) + y)))\n    assert (((- 2) * x).primitive() == (2, (- x)))\n    assert (Add(((5 * z) / 7), (0.5 * x), ((3 * y) / 2), evaluate=False).primitive() == ((S(1) / 14), (((7.0 * x) + (21 * y)) + (10 * z))))\n    for i in [S.Infinity, S.NegativeInfinity, S.ComplexInfinity]:\n        assert ((i + (x / 3)).primitive() == ((S(1) / 3), (i + x)))\n    assert (((S.Infinity + ((2 * x) / 3)) + ((4 * y) / 7)).primitive() == ((S(1) / 21), (((14 * x) + (12 * y)) + oo)))\n    assert (S.Zero.primitive() == (S.One, S.Zero))\n", "label": 1}
{"function": "\n\ndef connect_to_chunks(connect_to, existing_connections, steps, building_chunks):\n    _connect_layers = {\n        \n    }\n    _all_chunk_keys = []\n    for room_name in existing_connections:\n        for chunk_key in existing_connections[room_name]['chunk_keys']:\n            _all_chunk_keys.append(chunk_key)\n    _allowed = False\n    for room_name in connect_to:\n        if (not (room_name in existing_connections)):\n            continue\n        _neighbors = []\n        for chunk_key in existing_connections[room_name]['chunk_keys']:\n            _temp_neighbors = get_neighbors(chunk_key, only_chunk_keys=building_chunks, avoid_chunk_keys=_all_chunk_keys)\n            if (not _temp_neighbors):\n                return (- 1)\n            _neighbors.extend(_temp_neighbors)\n        _connect_layers[room_name] = {\n            'chunk_keys': existing_connections[room_name]['chunk_keys'],\n            'neighbors': _neighbors,\n        }\n        if _neighbors:\n            _allowed = True\n    if (not _allowed):\n        return (- 1)\n    _common_neighbors = {\n        \n    }\n    _highest = 0\n    for layer in _connect_layers.values():\n        for chunk_key in layer['neighbors']:\n            if (chunk_key in _common_neighbors):\n                _common_neighbors[chunk_key] += 1\n            else:\n                _common_neighbors[chunk_key] = 1\n            if (_common_neighbors[chunk_key] > _highest):\n                _highest = _common_neighbors[chunk_key]\n    for chunk_key in _common_neighbors.keys():\n        if (_common_neighbors[chunk_key] < _highest):\n            del _common_neighbors[chunk_key]\n    if (not _common_neighbors):\n        return False\n    if (_highest < len(_connect_layers)):\n        return (- 1)\n    return random.choice(_common_neighbors.keys())\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.roleName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef line_contains_close_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)\\\\*/', line) and (not re.match('^(\\\\+|\\\\-)(.*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(.*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\->)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=end)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\})', line) and (not re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef main(argv):\n    if (len(argv) != 2):\n        print('Usage: ./countLoc.py <file_to_count.[py | c]')\n        return\n    file = argv[1]\n    mode = file.split('.')[1]\n    if (mode == 'py'):\n        cKey = '#'\n    elif (mode == 'c'):\n        cKey = '//'\n        sKey = '/*'\n        eKey = '*/'\n    else:\n        print('Invalid extension, please input a Python or C code file')\n        return\n    comment = loc = blank = 0\n    with open(file, 'r') as f:\n        for line in f.readlines():\n            line = line.strip()\n            if (not line):\n                blank += 1\n            elif line.startswith(cKey):\n                comment += 1\n            elif ((mode == 'c') and line.startswith(sKey) and line.endswith(eKey)):\n                comment += 1\n            elif ((mode == 'c') and line.startswith(sKey)):\n                cFlag = True\n                comment += 1\n            elif ((mode == 'c') and line.endswith(eKey)):\n                cFlag = False\n                comment += 1\n            elif ((mode == 'c') and cFlag):\n                comment += 1\n            else:\n                loc += 1\n    print()\n    print('File: ', file)\n    print('LoC: ', loc)\n    print('Comments: ', comment)\n    print('Blank: ', blank)\n    print()\n", "label": 1}
{"function": "\n\ndef test_tag_base(self):\n    tag_object_refs = list()\n    for tag in self.rorepo.tags:\n        assert ('refs/tags' in tag.path)\n        assert tag.name\n        assert isinstance(tag.commit, Commit)\n        if (tag.tag is not None):\n            tag_object_refs.append(tag)\n            tagobj = tag.tag\n            self.failUnlessRaises(AttributeError, setattr, tagobj, 'someattr', 1)\n            assert isinstance(tagobj, TagObject)\n            assert (tagobj.tag == tag.name)\n            assert isinstance(tagobj.tagger, Actor)\n            assert isinstance(tagobj.tagged_date, int)\n            assert isinstance(tagobj.tagger_tz_offset, int)\n            assert tagobj.message\n            assert (tag.object == tagobj)\n            self.failUnlessRaises(AttributeError, setattr, tag, 'object', tagobj)\n    assert tag_object_refs\n    assert isinstance(self.rorepo.tags['0.1.5'], TagReference)\n", "label": 1}
{"function": "\n\ndef enter(self):\n    self.is_lazy = vim.eval('&lazyredraw')\n    while True:\n        (key, is_special) = get_key()\n        if key:\n            if is_special:\n                if (key in self.handlers):\n                    (h, a) = self.handlers[key]\n                    h(*a)\n            elif ('print' in self.handlers):\n                (h, a) = self.handlers['print']\n                h(key, *a)\n        else:\n            t = time()\n            self.reenter = True\n            while self.tasks:\n                task = self.tasks.pop(0)\n                try:\n                    next(task)\n                except StopIteration:\n                    self.do_release = True\n                else:\n                    self.tasks.append(task)\n                tt = time()\n                if ((tt - t) > TIME_SLICE):\n                    break\n            else:\n                sleep(TIME_SLICE)\n        if self.do_release:\n            self.do_release = False\n            if self.reenter:\n                waiting_loops.append(self)\n                vfunc.feedkeys(self.feedkeys)\n            if self.is_lazy:\n                redraw()\n            return\n", "label": 1}
{"function": "\n\ndef construct(start_block, vmap, exceptions):\n    bfs_blocks = bfs(start_block)\n    graph = Graph()\n    gen_ret = GenInvokeRetName()\n    block_to_node = {\n        \n    }\n    exceptions_start_block = []\n    for exception in exceptions:\n        for (_, _, block) in exception.exceptions:\n            exceptions_start_block.append(block)\n    for block in bfs_blocks:\n        node = make_node(graph, block, block_to_node, vmap, gen_ret)\n        graph.add_node(node)\n    graph.entry = block_to_node[start_block]\n    del block_to_node, bfs_blocks\n    graph.compute_rpo()\n    graph.number_ins()\n    for node in graph.rpo:\n        preds = [pred for pred in graph.all_preds(node) if (pred.num < node.num)]\n        if (preds and all((pred.in_catch for pred in preds))):\n            node.in_catch = True\n    lexit_nodes = [node for node in graph if node.type.is_return]\n    if (len(lexit_nodes) > 1):\n        logger.error('Multiple exit nodes found !')\n        graph.exit = graph.rpo[(- 1)]\n    elif (len(lexit_nodes) < 1):\n        logger.debug('No exit node found !')\n    else:\n        graph.exit = lexit_nodes[0]\n    return graph\n", "label": 1}
{"function": "\n\ndef keyPressEvent(self, event):\n    ctrl = ((event.modifiers() & Qt.ControlModifier) != 0)\n    if ((not self.is_running) or self.textCursor().hasSelection()):\n        if ((event.key() == Qt.Key_C) and ctrl):\n            self.copy()\n        return\n    propagate_to_parent = True\n    delete = (event.key() in [Qt.Key_Backspace, Qt.Key_Delete])\n    if (delete and (not self._usr_buffer)):\n        return\n    if ((event.key() == Qt.Key_V) and ctrl):\n        text = QApplication.clipboard().text()\n        self._usr_buffer += text\n        self.setTextColor(self._stdin_col)\n        if self._mask_user_input:\n            text = (len(text) * '*')\n        self.insertPlainText(text)\n        return\n    if (event.key() in [Qt.Key_Return, Qt.Key_Enter]):\n        if (sys.platform == 'win32'):\n            self._usr_buffer += '\\r'\n        self._usr_buffer += '\\n'\n        self.process.write(self.get_user_buffer_as_bytes())\n        self._usr_buffer = ''\n    elif ((not delete) and len(event.text())):\n        txt = event.text()\n        self._usr_buffer += txt\n        if self._mask_user_input:\n            txt = '*'\n        self.setTextColor(self._stdin_col)\n        self.insertPlainText(txt)\n        propagate_to_parent = False\n    elif delete:\n        self._usr_buffer = self._usr_buffer[:(len(self._usr_buffer) - 1)]\n    if propagate_to_parent:\n        super(InteractiveConsole, self).keyPressEvent(event)\n    self.setTextColor(self._stdout_col)\n", "label": 1}
{"function": "\n\ndef test_read(tmpdir):\n    assert (None is scripts.read(None))\n    assert ('foo' == scripts.read('foo'))\n    assert ((1,) == scripts.read((1,)))\n    assert (None is scripts.read(None, split=True))\n    assert (['foo'] == scripts.read('foo', split=True))\n    assert ((1,) == scripts.read((1,), split=True))\n    with stdin('text'):\n        assert ('text' == scripts.read('-'))\n    with stdin('text'):\n        assert ('text' == scripts.read('@-'))\n    infile = tmpdir.join('infile')\n    infile.write('farb')\n    assert ('farb' == scripts.read(('@%s' % infile)))\n    assert ('farb' == scripts.read(('%s' % infile)))\n    try:\n        noexist = 'not-here-hopefully'\n        scripts.read(('@%s' % noexist))\n        assert False\n    except ClickException as ex:\n        assert (str(ex) == (\"[Errno 2] No such file or directory: '%s'\" % noexist))\n    xs = scripts.read(' x\\nx\\r\\nx\\t\\tx\\t\\n x ', split=True)\n    assert ((['x'] * 5) == xs)\n", "label": 1}
{"function": "\n\ndef demo_manyops():\n    'Checking many operations together.'\n    fileh = setUp('undo-redo-manyops.h5')\n    fileh.create_array(fileh.root, 'anarray3', [3], 'Array title 3')\n    fileh.create_group(fileh.root, 'agroup3', 'Group title 3')\n    new_node = fileh.copy_node('/anarray3', '/agroup/agroup2')\n    new_node = fileh.copy_children('/agroup', '/agroup3', recursive=1)\n    fileh.rename_node('/anarray', 'anarray4')\n    new_node = fileh.copy_node('/anarray3', '/agroup')\n    fileh.remove_node('/anarray4')\n    fileh.undo()\n    assert ('/anarray4' not in fileh)\n    assert ('/anarray3' not in fileh)\n    assert ('/agroup/agroup2/anarray3' not in fileh)\n    assert ('/agroup3' not in fileh)\n    assert ('/anarray4' not in fileh)\n    assert ('/anarray' in fileh)\n    fileh.redo()\n    assert ('/agroup/agroup2/anarray3' in fileh)\n    assert ('/agroup/anarray3' in fileh)\n    assert ('/agroup3/agroup2/anarray3' in fileh)\n    assert ('/agroup3/anarray3' not in fileh)\n    assert (fileh.root.agroup.anarray3 is new_node)\n    assert ('/anarray' not in fileh)\n    assert ('/anarray4' not in fileh)\n    tearDown(fileh)\n", "label": 1}
{"function": "\n\ndef normpath(path):\n    'Normalize path, eliminating double slashes, etc.'\n    if (path == ''):\n        return '.'\n    initial_slashes = path.startswith('/')\n    if (initial_slashes and path.startswith('//') and (not path.startswith('///'))):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if (comp in ('', '.')):\n            continue\n        if ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(- 1)] == '..'))):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = '/'.join(comps)\n    if initial_slashes:\n        path = (('/' * initial_slashes) + path)\n    return (path or '.')\n", "label": 1}
{"function": "\n\ndef AAG_heater_algorithm(self, target, last_entry):\n    '\\n        Uses the algorithm described in RainSensorHeaterAlgorithm.pdf to\\n        determine PWM value.\\n\\n        Values are for the default read cycle of 10 seconds.\\n        '\n    deltaT = (last_entry['Rain Sensor Temp (C)'] - target)\n    scaling = 0.5\n    if (deltaT > 8.0):\n        deltaPWM = ((- 40) * scaling)\n    elif (deltaT > 4.0):\n        deltaPWM = ((- 20) * scaling)\n    elif (deltaT > 3.0):\n        deltaPWM = ((- 10) * scaling)\n    elif (deltaT > 2.0):\n        deltaPWM = ((- 6) * scaling)\n    elif (deltaT > 1.0):\n        deltaPWM = ((- 4) * scaling)\n    elif (deltaT > 0.5):\n        deltaPWM = ((- 2) * scaling)\n    elif (deltaT > 0.3):\n        deltaPWM = ((- 1) * scaling)\n    elif (deltaT < (- 0.3)):\n        deltaPWM = (1 * scaling)\n    elif (deltaT < (- 0.5)):\n        deltaPWM = (2 * scaling)\n    elif (deltaT < (- 1.0)):\n        deltaPWM = (4 * scaling)\n    elif (deltaT < (- 2.0)):\n        deltaPWM = (6 * scaling)\n    elif (deltaT < (- 3.0)):\n        deltaPWM = (10 * scaling)\n    elif (deltaT < (- 4.0)):\n        deltaPWM = (20 * scaling)\n    elif (deltaT < (- 8.0)):\n        deltaPWM = (40 * scaling)\n    return int(deltaPWM)\n", "label": 1}
{"function": "\n\ndef filter_results(source, results, aggressive):\n    'Filter out spurious reports from pep8.\\n\\n    If aggressive is True, we allow possibly unsafe fixes (E711, E712).\\n\\n    '\n    non_docstring_string_line_numbers = multiline_string_lines(source, include_docstrings=False)\n    all_string_line_numbers = multiline_string_lines(source, include_docstrings=True)\n    commented_out_code_line_numbers = commented_out_code_lines(source)\n    for r in results:\n        issue_id = r['id'].lower()\n        if (r['line'] in non_docstring_string_line_numbers):\n            if issue_id.startswith(('e1', 'e501', 'w191')):\n                continue\n        if (r['line'] in all_string_line_numbers):\n            if (issue_id in ['e501']):\n                continue\n        if ((not aggressive) and ((r['line'] + 1) in all_string_line_numbers)):\n            if issue_id.startswith(('w29', 'w39')):\n                continue\n        if (aggressive <= 0):\n            if issue_id.startswith(('e711', 'w6')):\n                continue\n        if (aggressive <= 1):\n            if issue_id.startswith(('e712',)):\n                continue\n        if (r['line'] in commented_out_code_line_numbers):\n            if issue_id.startswith(('e26', 'e501')):\n                continue\n        (yield r)\n", "label": 1}
{"function": "\n\ndef update(self):\n    si = self.current\n    if (si is None):\n        return\n    not_in_error = self.not_in_error()\n    flags = (sublime.DRAW_NO_FILL if _ST3 else sublime.DRAW_OUTLINED)\n    self.view.add_regions(REGION_FIND, si.regions, 'text', '', flags)\n    selected = (si.selected or (not_in_error.selected and [not_in_error.selected[(- 1)]]) or [])\n    self.view.add_regions(REGION_SELECTED, selected, 'string', '', sublime.DRAW_NO_OUTLINE)\n    if selected:\n        self.view.show(selected[(- 1)])\n    status = ''\n    if ((si != not_in_error) or si.try_wrapped):\n        status += 'Failing '\n    if self.current.wrapped:\n        status += 'Wrapped '\n    status += ('I-Search ' + ('Forward' if self.current.forward else 'Reverse'))\n    if (si != not_in_error):\n        if (len(self.current.regions) > 0):\n            status += (' %s %s' % (pluralize('match', len(self.current.regions), 'es'), ('above' if self.forward else 'below')))\n    else:\n        n_cursors = min(len(si.selected), len(si.regions))\n        status += (' %s, %s' % (pluralize('match', len(si.regions), 'es'), pluralize('cursor', n_cursors)))\n    self.util.set_status(status)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.roleName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.STRUCT):\n                self.privilege = TSentryPrivilege()\n                self.privilege.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-d', '--delay', default=0, type=int)\n    parser.add_argument('-w', '--wait', action='store_true')\n    args = parser.parse_args()\n    if sys.stdin.isatty():\n        parser.error('no input, pipe another btc command output into this command')\n    torrents = sys.stdin.read()\n    if (len(torrents.strip()) == 0):\n        exit(1)\n    try:\n        torrents = decoder.decode(torrents)\n    except ValueError:\n        error(('unexpected input: %s' % torrents))\n    time.sleep(args.delay)\n    hashes = [t['hash'] for t in torrents]\n    for h in hashes:\n        client.recheck_torrent(h)\n    while args.wait:\n        d = list_to_dict(client.list_torrents(), 'hash')\n        all_checked = True\n        for h in d:\n            if (h not in hashes):\n                continue\n            if (d[h]['state'] not in ('CHECKED',)):\n                all_checked = False\n                break\n        if all_checked:\n            break\n        time.sleep(1)\n    if (not sys.stdout.isatty()):\n        d = list_to_dict(client.list_torrents(), 'hash')\n        d = dict(((h, d[h]) for h in hashes if (h in d)))\n        print(encoder.encode(dict_to_list(d, 'hash')))\n", "label": 1}
{"function": "\n\ndef test_default_updates_chained(self):\n    x = shared(2)\n    y = shared(1)\n    z = shared((- 1))\n    x.default_update = (x - y)\n    y.default_update = z\n    z.default_update = (z - 1)\n    f1 = pfunc([], [x])\n    f1()\n    assert (x.get_value() == 1)\n    assert (y.get_value() == (- 1))\n    assert (z.get_value() == (- 2))\n    f2 = pfunc([], [x, y])\n    f2()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 2))\n    assert (z.get_value() == (- 3))\n    f3 = pfunc([], [y])\n    f3()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 3))\n    assert (z.get_value() == (- 4))\n    f4 = pfunc([], [x, y], no_default_updates=[x])\n    f4()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 4))\n    assert (z.get_value() == (- 5))\n    f5 = pfunc([], [x, y, z], no_default_updates=[z])\n    f5()\n    assert (x.get_value() == 6)\n    assert (y.get_value() == (- 5))\n    assert (z.get_value() == (- 5))\n", "label": 1}
{"function": "\n\ndef is_active(self, key, *instances, **kwargs):\n    \"\\n        Returns ``True`` if any of ``instances`` match an active switch. Otherwise\\n        returns ``False``.\\n\\n        >>> gargoyle.is_active('my_feature', request) #doctest: +SKIP\\n        \"\n    default = kwargs.pop('default', False)\n    parts = key.split(':')\n    if (len(parts) > 1):\n        child_kwargs = kwargs.copy()\n        child_kwargs['default'] = None\n        result = self.is_active(':'.join(parts[:(- 1)]), *instances, **child_kwargs)\n        if (result is False):\n            return result\n        elif (result is True):\n            default = result\n    try:\n        switch = self[key]\n    except KeyError:\n        return default\n    if (switch.status == GLOBAL):\n        return True\n    elif (switch.status == DISABLED):\n        return False\n    elif (switch.status == INHERIT):\n        return default\n    conditions = switch.value\n    if (not conditions):\n        return default\n    if instances:\n        instances = list(instances)\n        for v in instances:\n            if (isinstance(v, HttpRequest) and hasattr(v, 'user')):\n                instances.append(v.user)\n    return_value = False\n    for switch in self._registry.itervalues():\n        result = switch.has_active_condition(conditions, instances)\n        if (result is False):\n            return False\n        elif (result is True):\n            return_value = True\n    return return_value\n", "label": 1}
{"function": "\n\ndef _gather_port_ids_and_networks(self, context, instance, networks=None, port_ids=None):\n    \"Return an instance's complete list of port_ids and networks.\"\n    if (((networks is None) and (port_ids is not None)) or ((port_ids is None) and (networks is not None))):\n        message = _('This method needs to be called with either networks=None and port_ids=None or port_ids and networks as not none.')\n        raise exception.NovaException(message=message)\n    ifaces = compute_utils.get_nw_info_for_instance(instance)\n    if (port_ids is None):\n        port_ids = [iface['id'] for iface in ifaces]\n        net_ids = [iface['network']['id'] for iface in ifaces]\n    if (networks is None):\n        networks = self._get_available_networks(context, instance.project_id, net_ids)\n    else:\n        networks_ids = [network['id'] for network in networks]\n        networks = (networks + [{\n            'id': iface['network']['id'],\n            'name': iface['network']['label'],\n            'tenant_id': iface['network']['meta']['tenant_id'],\n        } for iface in ifaces if _is_not_duplicate(iface['network']['id'], networks_ids, 'networks', instance)])\n        port_ids = ([iface['id'] for iface in ifaces if _is_not_duplicate(iface['id'], port_ids, 'port_ids', instance)] + port_ids)\n    return (networks, port_ids)\n", "label": 1}
{"function": "\n\ndef concat(docs):\n    '\\n    Concatenate together the contents of multiple documents from a\\n    single corpus, using an appropriate concatenation function.  This\\n    utility function is used by corpus readers when the user requests\\n    more than one document at a time.\\n    '\n    if (len(docs) == 1):\n        return docs[0]\n    if (len(docs) == 0):\n        raise ValueError('concat() expects at least one object!')\n    types = set((d.__class__ for d in docs))\n    if all((isinstance(doc, string_types) for doc in docs)):\n        return ''.join(docs)\n    for typ in types:\n        if (not issubclass(typ, (StreamBackedCorpusView, ConcatenatedCorpusView))):\n            break\n    else:\n        return ConcatenatedCorpusView(docs)\n    for typ in types:\n        if (not issubclass(typ, AbstractLazySequence)):\n            break\n    else:\n        return LazyConcatenation(docs)\n    if (len(types) == 1):\n        typ = list(types)[0]\n        if issubclass(typ, list):\n            return reduce((lambda a, b: (a + b)), docs, [])\n        if issubclass(typ, tuple):\n            return reduce((lambda a, b: (a + b)), docs, ())\n        if ElementTree.iselement(typ):\n            xmltree = ElementTree.Element('documents')\n            for doc in docs:\n                xmltree.append(doc)\n            return xmltree\n    raise ValueError((\"Don't know how to concatenate types: %r\" % types))\n", "label": 1}
{"function": "\n\ndef _quick_drag_menu(self, object):\n    ' Displays the quick drag menu for a specified drag object.\\n        '\n    feature_lists = []\n    if isinstance(object, IFeatureTool):\n        msg = 'Apply to'\n        for dc in self.dock_control.dock_controls:\n            if (dc.visible and (object.feature_can_drop_on(dc.object) or object.feature_can_drop_on_dock_control(dc))):\n                from feature_tool import FeatureTool\n                feature_lists.append([FeatureTool(dock_control=dc)])\n    else:\n        msg = 'Send to'\n        for dc in self.dock_control.dock_controls:\n            if dc.visible:\n                allowed = [f for f in dc.features if ((f.feature_name != '') and f.can_drop(object))]\n                if (len(allowed) > 0):\n                    feature_lists.append(allowed)\n    if (len(feature_lists) > 0):\n        features = []\n        actions = []\n        for list in feature_lists:\n            if (len(list) > 1):\n                sub_actions = []\n                for feature in list:\n                    sub_actions.append(Action(name=('%s Feature' % feature.feature_name), action=('self._drop_on(%d)' % len(features))))\n                    features.append(feature)\n                actions.append(Menu(*sub_actions, name=('%s the %s' % (msg, feature.dock_control.name))))\n            else:\n                actions.append(Action(name=('%s %s' % (msg, list[0].dock_control.name)), action=('self._drop_on(%d)' % len(features))))\n                features.append(list[0])\n        self._object = object\n        self._features = features\n        self.popup_menu(Menu(*actions, name='popup'))\n        self._object = self._features = None\n", "label": 1}
{"function": "\n\ndef compile_basic(self, nodes, repeat=None):\n    assert (len(nodes) >= 1)\n    node = nodes[0]\n    if (node.type == token.STRING):\n        value = unicode(literals.evalString(node.value))\n        return pytree.LeafPattern(_type_of_literal(value), value)\n    elif (node.type == token.NAME):\n        value = node.value\n        if value.isupper():\n            if (value not in TOKEN_MAP):\n                raise PatternSyntaxError(('Invalid token: %r' % value))\n            if nodes[1:]:\n                raise PatternSyntaxError(\"Can't have details for token\")\n            return pytree.LeafPattern(TOKEN_MAP[value])\n        else:\n            if (value == 'any'):\n                type = None\n            elif (not value.startswith('_')):\n                type = getattr(self.pysyms, value, None)\n                if (type is None):\n                    raise PatternSyntaxError(('Invalid symbol: %r' % value))\n            if nodes[1:]:\n                content = [self.compile_node(nodes[1].children[1])]\n            else:\n                content = None\n            return pytree.NodePattern(type, content)\n    elif (node.value == '('):\n        return self.compile_node(nodes[1])\n    elif (node.value == '['):\n        assert (repeat is None)\n        subpattern = self.compile_node(nodes[1])\n        return pytree.WildcardPattern([[subpattern]], min=0, max=1)\n    assert False, node\n", "label": 1}
{"function": "\n\ndef merge(self, session, source_state, source_dict, dest_state, dest_dict, load, _recursive):\n    if load:\n        for r in self._reverse_property:\n            if ((source_state, r) in _recursive):\n                return\n    if ('merge' not in self._cascade):\n        return\n    if (self.key not in source_dict):\n        return\n    if self.uselist:\n        instances = source_state.get_impl(self.key).get(source_state, source_dict)\n        if hasattr(instances, '_sa_adapter'):\n            instances = instances._sa_adapter\n        if load:\n            dest_state.get_impl(self.key).get(dest_state, dest_dict)\n        dest_list = []\n        for current in instances:\n            current_state = attributes.instance_state(current)\n            current_dict = attributes.instance_dict(current)\n            _recursive[(current_state, self)] = True\n            obj = session._merge(current_state, current_dict, load=load, _recursive=_recursive)\n            if (obj is not None):\n                dest_list.append(obj)\n        if (not load):\n            coll = attributes.init_state_collection(dest_state, dest_dict, self.key)\n            for c in dest_list:\n                coll.append_without_event(c)\n        else:\n            dest_state.get_impl(self.key)._set_iterable(dest_state, dest_dict, dest_list)\n    else:\n        current = source_dict[self.key]\n        if (current is not None):\n            current_state = attributes.instance_state(current)\n            current_dict = attributes.instance_dict(current)\n            _recursive[(current_state, self)] = True\n            obj = session._merge(current_state, current_dict, load=load, _recursive=_recursive)\n        else:\n            obj = None\n        if (not load):\n            dest_dict[self.key] = obj\n        else:\n            dest_state.get_impl(self.key).set(dest_state, dest_dict, obj, None)\n", "label": 1}
{"function": "\n\ndef test_parted_data_is_loaded(self):\n    parted_data = {\n        'partitioning': {\n            'parteds': [{\n                'label': 'gpt',\n                'name': '/dev/sdb',\n                'partitions': [{\n                    'begin': 1,\n                    'configdrive': False,\n                    'count': 1,\n                    'device': '/dev/sdb',\n                    'end': 25,\n                    'flags': ['bios_grub', 'xyz'],\n                    'guid': None,\n                    'name': '/dev/sdb1',\n                    'partition_type': 'primary',\n                }],\n            }],\n        },\n    }\n    driver = simple.NailgunSimpleDriver(parted_data)\n    parted = driver.partition_scheme.parteds[0]\n    partition = parted.partitions[0]\n    assert (len(driver.partition_scheme.parteds) == 1)\n    assert isinstance(parted, objects.Parted)\n    assert (parted.label == 'gpt')\n    assert (parted.name == '/dev/sdb')\n    assert (len(parted.partitions) == 1)\n    assert (partition.begin == 1)\n    assert (partition.configdrive is False)\n    assert (partition.count == 1)\n    assert (partition.device == '/dev/sdb')\n    assert (partition.end == 25)\n    self.assertItemsEqual(partition.flags, ['bios_grub', 'xyz'])\n    assert (partition.guid is None)\n    assert (partition.name == '/dev/sdb1')\n    assert (partition.type == 'primary')\n", "label": 1}
{"function": "\n\ndef is_open(location, now=None):\n    '\\n    Is the company currently open? Pass \"now\" to test with a specific\\n    timestamp. Can be used stand-alone or as a helper.\\n    '\n    if (now is None):\n        now = get_now()\n    if has_closing_rule_for_now(location):\n        return False\n    now_time = datetime.time(now.hour, now.minute, now.second)\n    if location:\n        ohs = OpeningHours.objects.filter(company=location)\n    else:\n        ohs = Company.objects.first().openinghours_set.all()\n    for oh in ohs:\n        is_open = False\n        if ((oh.weekday == now.isoweekday()) and (oh.from_hour <= now_time) and (now_time <= oh.to_hour)):\n            is_open = oh\n        if ((oh.weekday == now.isoweekday()) and (oh.from_hour <= now_time) and ((oh.to_hour < oh.from_hour) and (now_time < datetime.time(23, 59, 59)))):\n            is_open = oh\n        if ((oh.weekday == ((now.isoweekday() - 1) % 7)) and (oh.from_hour >= now_time) and (oh.to_hour >= now_time) and (oh.to_hour < oh.from_hour)):\n            is_open = oh\n        if (is_open is not False):\n            return oh\n    return False\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_service_call_name_ != x.has_service_call_name_):\n        return 0\n    if (self.has_service_call_name_ and (self.service_call_name_ != x.service_call_name_)):\n        return 0\n    if (self.has_total_amount_of_calls_ != x.has_total_amount_of_calls_):\n        return 0\n    if (self.has_total_amount_of_calls_ and (self.total_amount_of_calls_ != x.total_amount_of_calls_)):\n        return 0\n    if (self.has_total_cost_of_calls_microdollars_ != x.has_total_cost_of_calls_microdollars_):\n        return 0\n    if (self.has_total_cost_of_calls_microdollars_ and (self.total_cost_of_calls_microdollars_ != x.total_cost_of_calls_microdollars_)):\n        return 0\n    if (len(self.total_billed_ops_) != len(x.total_billed_ops_)):\n        return 0\n    for (e1, e2) in zip(self.total_billed_ops_, x.total_billed_ops_):\n        if (e1 != e2):\n            return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef print_usage(self, cmd_name=None):\n    'Print the usage(s) of all commands or a command.'\n\n    def append_usage(cmd_name, without_name=False):\n        cmd_func = self.command_funcs[cmd_name]\n        usages.append(Command(cmd_func, cmd_name).build_usage(without_name))\n    usages = []\n    if (cmd_name is None):\n        if (self.default is not None):\n            append_usage(self.default, True)\n        for name in sorted(self.command_funcs.keys()):\n            append_usage(name)\n    else:\n        if (self.default == cmd_name):\n            append_usage(cmd_name, without_name=True)\n        append_usage(cmd_name)\n    iusages = iter(usages)\n    try:\n        print('usage:', next(iusages))\n    except StopIteration:\n        pass\n    else:\n        for usage in iusages:\n            print('   or:', usage)\n    if (cmd_name is None):\n        if self.doc:\n            doc = self.doc\n        elif inspect.ismodule(self.obj):\n            doc = inspect.getdoc(self.obj)\n        else:\n            doc = None\n        if (not doc):\n            cmd_name = self.default\n    if cmd_name:\n        doc = inspect.getdoc(self.command_funcs[cmd_name])\n    if doc:\n        print()\n        print(doc)\n        print()\n", "label": 1}
{"function": "\n\ndef _process_string(self, token, content):\n    if (self.getting_attrs and (self.current_attr is not None)):\n        if (content.endswith('\"') or content.endswith(\"'\")):\n            if (self.current_attr_value is not None):\n                self.current_attr_value += content\n                if ((self.current_tag == 'script') and (self.current_attr == 'src')):\n                    self.append(self.current_attr_value)\n                self.current_attr = None\n                self.current_attr_value = None\n            elif (len(content) == 1):\n                self.current_attr_value = content\n            else:\n                if ((self.current_tag == 'script') and (self.current_attr == 'src')):\n                    self.append(content)\n                self.current_attr = None\n                self.current_attr_value = None\n        elif (content.startswith('\"') or content.startswith(\"'\")):\n            if (self.current_attr_value is None):\n                self.current_attr_value = content\n            else:\n                self.current_attr_value += content\n", "label": 1}
{"function": "\n\ndef test_fetch_all(self):\n    valid_procs = 0\n    excluded_names = ['send_signal', 'suspend', 'resume', 'terminate', 'kill', 'wait', 'as_dict', 'get_cpu_percent', 'nice', 'parent', 'get_children', 'pid']\n    attrs = []\n    for name in dir(psutil.Process):\n        if name.startswith('_'):\n            continue\n        if name.startswith('set_'):\n            continue\n        if (name in excluded_names):\n            continue\n        attrs.append(name)\n    for p in psutil.process_iter():\n        for name in attrs:\n            try:\n                try:\n                    attr = getattr(p, name, None)\n                    if ((attr is not None) and callable(attr)):\n                        ret = attr()\n                    else:\n                        ret = attr\n                    valid_procs += 1\n                except (psutil.NoSuchProcess, psutil.AccessDenied):\n                    err = sys.exc_info()[1]\n                    self.assertEqual(err.pid, p.pid)\n                    if err.name:\n                        self.assertEqual(err.name, p.name)\n                    self.assertTrue(str(err))\n                    self.assertTrue(err.msg)\n                else:\n                    if (ret not in (0, 0.0, [], None, '')):\n                        assert ret, ret\n                    meth = getattr(self, name)\n                    meth(ret)\n            except Exception:\n                err = sys.exc_info()[1]\n                trace = traceback.format_exc()\n                self.fail(('%s\\nproc=%s, method=%r, retvalue=%r' % (trace, p, name, ret)))\n    self.assertTrue((valid_procs > 0))\n", "label": 1}
{"function": "\n\ndef parse(self, chars):\n    token = ''\n    for char in chars:\n        if (char in '<*('):\n            if token:\n                (yield TextNode(token))\n            token = ''\n        if (char == '\\\\'):\n            token += next(chars)\n        elif (char == '<'):\n            tag_content = self.read_until(chars, '>')\n            name = ''\n            regex = None\n            macro = None\n            for char in tag_content:\n                if (char == '='):\n                    (name, regex) = tag_content.split('=', 1)\n                    break\n                if (char == ':'):\n                    (name, macro) = tag_content.split(':', 1)\n                    break\n            if regex:\n                (yield RegexTagNode(name, regex))\n            elif macro:\n                (yield MacroTagNode(name, macro))\n            else:\n                (yield TagNode(tag_content))\n        elif (char == '*'):\n            (yield WildcardNode())\n        elif (char == '('):\n            (yield OptionalNode(list(self.parse(chars))))\n        elif (char == ')'):\n            break\n        else:\n            token += char\n    if token:\n        (yield TextNode(token))\n", "label": 1}
{"function": "\n\ndef deep_deconstruct(self, obj):\n    '\\n        Recursive deconstruction for a field and its arguments.\\n        Used for full comparison for rename/alter; sometimes a single-level\\n        deconstruction will not compare correctly.\\n        '\n    if isinstance(obj, list):\n        return [self.deep_deconstruct(value) for value in obj]\n    elif isinstance(obj, tuple):\n        return tuple((self.deep_deconstruct(value) for value in obj))\n    elif isinstance(obj, dict):\n        return {key: self.deep_deconstruct(value) for (key, value) in obj.items()}\n    elif isinstance(obj, functools.partial):\n        return (obj.func, self.deep_deconstruct(obj.args), self.deep_deconstruct(obj.keywords))\n    elif isinstance(obj, COMPILED_REGEX_TYPE):\n        return RegexObject(obj)\n    elif isinstance(obj, type):\n        return obj\n    elif hasattr(obj, 'deconstruct'):\n        deconstructed = obj.deconstruct()\n        if isinstance(obj, models.Field):\n            deconstructed = deconstructed[1:]\n        (path, args, kwargs) = deconstructed\n        return (path, [self.deep_deconstruct(value) for value in args], {key: self.deep_deconstruct(value) for (key, value) in kwargs.items()})\n    else:\n        return obj\n", "label": 1}
{"function": "\n\ndef populate_scoped_cols(self, scoped_tbls):\n    ' Find all columns in a set of scoped_tables\\n        :param scoped_tbls: list of TableReference namedtuples\\n        :return: list of column names\\n        '\n    columns = []\n    meta = self.dbmetadata\n    for tbl in scoped_tbls:\n        if tbl.schema:\n            schema = self.escape_name(tbl.schema)\n            relname = self.escape_name(tbl.name)\n            if tbl.is_function:\n                try:\n                    functions = meta['functions'][schema][relname]\n                except KeyError:\n                    continue\n                for func in functions:\n                    columns.extend(func.fieldnames())\n            else:\n                try:\n                    columns.extend(meta['tables'][schema][relname])\n                    continue\n                except KeyError:\n                    pass\n                try:\n                    columns.extend(meta['views'][schema][relname])\n                except KeyError:\n                    pass\n        else:\n            for schema in self.search_path:\n                relname = self.escape_name(tbl.name)\n                if tbl.is_function:\n                    try:\n                        functions = meta['functions'][schema][relname]\n                    except KeyError:\n                        continue\n                    for func in functions:\n                        columns.extend(func.fieldnames())\n                else:\n                    try:\n                        columns.extend(meta['tables'][schema][relname])\n                        break\n                    except KeyError:\n                        pass\n                    try:\n                        columns.extend(meta['views'][schema][relname])\n                        break\n                    except KeyError:\n                        pass\n    return columns\n", "label": 1}
{"function": "\n\ndef _find_and_modify(self, query, projection=None, update=None, upsert=False, sort=None, return_document=ReturnDocument.BEFORE, **kwargs):\n    remove = kwargs.get('remove', False)\n    if (kwargs.get('new', False) and remove):\n        raise OperationFailure(\"remove and returnNew can't co-exist\")\n    if (not (remove or update)):\n        raise ValueError('Must either update or remove')\n    if (remove and update):\n        raise ValueError(\"Can't do both update and remove\")\n    old = self.find_one(query, projection=projection, sort=sort)\n    if ((not old) and (not upsert)):\n        return\n    if (old and ('_id' in old)):\n        query = {\n            '_id': old['_id'],\n        }\n    if remove:\n        self.delete_one(query)\n    else:\n        self._update(query, update, upsert)\n    if ((return_document is ReturnDocument.AFTER) or kwargs.get('new')):\n        return self.find_one(query, projection)\n    return old\n", "label": 1}
{"function": "\n\n@classmethod\ndef _eval(self, n, k):\n    if k.is_Integer:\n        if (n.is_Integer and (n >= 0)):\n            (n, k) = (int(n), int(k))\n            if (k > n):\n                return S.Zero\n            elif (k > (n // 2)):\n                k = (n - k)\n            (M, result) = (int(_sqrt(n)), 1)\n            for prime in sieve.primerange(2, (n + 1)):\n                if (prime > (n - k)):\n                    result *= prime\n                elif (prime > (n // 2)):\n                    continue\n                elif (prime > M):\n                    if ((n % prime) < (k % prime)):\n                        result *= prime\n                else:\n                    (N, K) = (n, k)\n                    exp = a = 0\n                    while (N > 0):\n                        a = int(((N % prime) < ((K % prime) + a)))\n                        (N, K) = ((N // prime), (K // prime))\n                        exp = (a + exp)\n                    if (exp > 0):\n                        result *= (prime ** exp)\n            return Integer(result)\n        else:\n            d = result = ((n - k) + 1)\n            for i in range(2, (k + 1)):\n                d += 1\n                result *= d\n                result /= i\n            return result\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef run(report_name, filters=()):\n    report = get_report_doc(report_name)\n    if (filters and isinstance(filters, basestring)):\n        filters = json.loads(filters)\n    if (not frappe.has_permission(report.ref_doctype, 'report')):\n        frappe.msgprint(_('Must have report permission to access this report.'), raise_exception=True)\n    (columns, result, message) = ([], [], None)\n    if (report.report_type == 'Query Report'):\n        if (not report.query):\n            frappe.msgprint(_('Must specify a Query to run'), raise_exception=True)\n        if (not report.query.lower().startswith('select')):\n            frappe.msgprint(_('Query must be a SELECT'), raise_exception=True)\n        result = [list(t) for t in frappe.db.sql(report.query, filters)]\n        columns = [cstr(c[0]) for c in frappe.db.get_description()]\n    else:\n        module = (report.module or frappe.db.get_value('DocType', report.ref_doctype, 'module'))\n        if (report.is_standard == 'Yes'):\n            method_name = (get_report_module_dotted_path(module, report.name) + '.execute')\n            res = frappe.get_attr(method_name)(frappe._dict(filters))\n            (columns, result) = (res[0], res[1])\n            if (len(res) > 2):\n                message = res[2]\n    if (report.apply_user_permissions and result):\n        result = get_filtered_data(report.ref_doctype, columns, result)\n    if (cint(report.add_total_row) and result):\n        result = add_total_row(result, columns)\n    return {\n        'result': result,\n        'columns': columns,\n        'message': message,\n    }\n", "label": 1}
{"function": "\n\ndef Prepend(self, **kw):\n    'Prepend values to existing construction variables\\n        in an Environment.\\n        '\n    kw = copy_non_reserved_keywords(kw)\n    for (key, val) in kw.items():\n        try:\n            orig = self._dict[key]\n        except KeyError:\n            self._dict[key] = val\n        else:\n            try:\n                update_dict = orig.update\n            except AttributeError:\n                try:\n                    self._dict[key] = (val + orig)\n                except (KeyError, TypeError):\n                    try:\n                        add_to_val = val.append\n                    except AttributeError:\n                        if val:\n                            orig.insert(0, val)\n                    else:\n                        if orig:\n                            add_to_val(orig)\n                        self._dict[key] = val\n            else:\n                if SCons.Util.is_List(val):\n                    for v in val:\n                        orig[v] = None\n                else:\n                    try:\n                        update_dict(val)\n                    except (AttributeError, TypeError, ValueError):\n                        if SCons.Util.is_Dict(val):\n                            for (k, v) in val.items():\n                                orig[k] = v\n                        else:\n                            orig[val] = None\n    self.scanner_map_delete(kw)\n", "label": 1}
{"function": "\n\ndef serialize(self):\n    'Serialize this object into a 32-bit unsigned integer.\\n\\n        The returned integer can be passed to deserialize() to\\n        recreate a copy of this object.\\n\\n        Returns:\\n            A 32-bit unsigned integer that is a serialized form of\\n            this object.\\n        '\n    wildcard_bits = ((((((((((OFPFW_IN_PORT if self.in_port else 0) | (OFPFW_DL_VLAN if self.dl_vlan else 0)) | (OFPFW_DL_SRC if self.dl_src else 0)) | (OFPFW_DL_DST if self.dl_dst else 0)) | (OFPFW_DL_TYPE if self.dl_type else 0)) | (OFPFW_NW_PROTO if self.nw_proto else 0)) | (OFPFW_TP_SRC if self.tp_src else 0)) | (OFPFW_TP_DST if self.tp_dst else 0)) | (OFPFW_DL_VLAN_PCP if self.dl_vlan_pcp else 0)) | (OFPFW_NW_TOS if self.nw_tos else 0))\n    if ((self.nw_src < 0) or (self.nw_src > 32)):\n        raise ValueError('invalid nw_src', self.nw_src)\n    wildcard_bits |= (self.nw_src << OFPFW_NW_SRC_SHIFT)\n    if ((self.nw_dst < 0) or (self.nw_dst > 32)):\n        raise ValueError('invalid nw_dst', self.nw_dst)\n    wildcard_bits |= (self.nw_dst << OFPFW_NW_DST_SHIFT)\n    return wildcard_bits\n", "label": 1}
{"function": "\n\ndef _check_auto_matrix_indices_in_call(self, *indices):\n    matrix_behavior_kinds = dict()\n    if (len(indices) != len(self.index_types)):\n        if (not self._matrix_behavior):\n            raise ValueError('wrong number of indices')\n        ldiff = (len(self.index_types) - len(indices))\n        if (ldiff > 2):\n            raise ValueError('wrong number of indices')\n        if (ldiff == 2):\n            mat_ind = [len(indices), (len(indices) + 1)]\n        elif (ldiff == 1):\n            mat_ind = [len(indices)]\n        not_equal = True\n    else:\n        not_equal = False\n        mat_ind = [i for (i, e) in enumerate(indices) if (e is True)]\n        if mat_ind:\n            not_equal = True\n        indices = tuple([_ for _ in indices if (_ is not True)])\n        for (i, el) in enumerate(indices):\n            if (not isinstance(el, TensorIndex)):\n                not_equal = True\n                break\n            if (el._tensortype != self.index_types[i]):\n                not_equal = True\n                break\n    if not_equal:\n        for el in mat_ind:\n            eltyp = self.index_types[el]\n            if (eltyp in matrix_behavior_kinds):\n                elind = (- self.index_types[el].auto_right)\n                matrix_behavior_kinds[eltyp].append(elind)\n            else:\n                elind = self.index_types[el].auto_left\n                matrix_behavior_kinds[eltyp] = [elind]\n            indices = ((indices[:el] + (elind,)) + indices[el:])\n    return (indices, matrix_behavior_kinds)\n", "label": 1}
{"function": "\n\ndef table2story(self, ar, column_names=None, header_level=None, nosummary=False, stripped=True, **kwargs):\n    'Render the given table request as reStructuredText to stdout.\\n        See :meth:`ar.show <lino.core.request.BaseRequest.show>`.\\n        '\n    if ((ar.actor.master is not None) and (not nosummary)):\n        if (ar.actor.slave_grid_format == 'summary'):\n            s = E.to_rst(ar.actor.get_slave_summary(ar.master_instance, ar), stripped=stripped)\n            if stripped:\n                s = s.strip()\n            return s\n    (fields, headers, widths) = ar.get_field_info(column_names)\n    sums = [fld.zero for fld in fields]\n    rows = []\n    recno = 0\n    for row in ar.sliced_data_iterator:\n        recno += 1\n        rows.append([x for x in ar.row2text(fields, row, sums)])\n    if (len(rows) == 0):\n        s = str(ar.no_data_text)\n        if (not stripped):\n            s = (('\\n' + s) + '\\n')\n        return s\n    if (not ar.actor.hide_sums):\n        has_sum = False\n        for i in sums:\n            if i:\n                has_sum = True\n                break\n        if has_sum:\n            rows.append([x for x in ar.sums2html(fields, sums)])\n    t = RstTable(headers, **kwargs)\n    s = t.to_rst(rows)\n    if (header_level is not None):\n        h = rstgen.header(header_level, ar.get_title())\n        if stripped:\n            h = h.strip()\n        s = ((h + '\\n') + s)\n    return s\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    for base in bases:\n        for base_name in dir(base):\n            base_value = getattr(base, base_name)\n            if ((not callable(base_value)) or (not base_name.startswith('assert'))):\n                continue\n            if (not (base_name in attrs)):\n                attrs.update({\n                    base_name: base_value,\n                })\n    for (attr_name, attr_value) in attrs.items():\n        if ((not attr_name.startswith('assert')) or (attr_name == 'assert_')):\n            continue\n        if (attr_name[6] == '_'):\n            new_name = underscore_to_camelcase(attr_name)\n        else:\n            new_name = camelcase_to_underscore(attr_name)\n        if (not (new_name in attrs)):\n            attrs[new_name] = attr_value\n    for attr in tools.__all__:\n        attrs.update({\n            attr: getattr(tools, attr),\n        })\n        attrs[attr] = staticmethod(attrs[attr])\n    if datadiff_assert_equal:\n        key = ((attrs.get('use_datadiff', False) and 'assert_equal') or 'datadiff_assert_equal')\n        attrs.update({\n            key: datadiff_assert_equal,\n        })\n        attrs[key] = staticmethod(attrs[key])\n    elif attrs.get('use_datadiff', False):\n        warnings.warn('You enabled ``datadiff.tools.assert_equal``, but looks like you have not ``datadiff`` library installed in your system.')\n    return type.__new__(cls, name, bases, attrs)\n", "label": 1}
{"function": "\n\ndef check(self):\n    if (not THROTTLING_ENABLED):\n        return\n    if (THROTTLING_IGNORE_ADMINS and self.request.user.is_superuser):\n        return\n    (cache_key, timeout) = self.get_cache_key()\n    if (not cache_key):\n        return\n    if (timeout is False):\n        return HttpResponseBadRequest()\n    if ((timeout is None) or (timeout is 0)):\n        return\n    if isinstance(timeout, basestring):\n        if timeout.startswith('/'):\n            return HttpResponseRedirect(timeout)\n        else:\n            callback = get_callable(timeout)\n            maintenance_bundle = {\n                'view_func': self.view_func,\n                'view_args': self.view_args,\n                'view_kwargs': self.view_kwargs,\n            }\n            return callback(self.request, maintenance_bundle)\n    now = int((time.time() * 1000))\n    last_access = cache.get(cache_key, 0)\n    delta = (now - last_access)\n    if (delta >= timeout):\n        cache.set(cache_key, now, THROTTLING_CACHE_EXPIRE)\n        return\n    congestion_view = (self.view_throttling.get('congestion') or THROTTLING.get('congestion'))\n    if congestion_view:\n        if congestion_view.startswith('/'):\n            return HttpResponseRedirect(congestion_view)\n        else:\n            callback = get_callable(congestion_view)\n            congestion_bundle = {\n                'view_func': self.view_func,\n                'view_args': self.view_args,\n                'view_kwargs': self.view_kwargs,\n                'timeout': timeout,\n                'delta': delta,\n            }\n            return callback(self.request, congestion_bundle)\n    return HttpResponseBadRequest()\n", "label": 1}
{"function": "\n\ndef ByteSizePartial(self):\n    n = 0\n    if self.has_service_call_name_:\n        n += 1\n        n += self.lengthString(len(self.service_call_name_))\n    if self.has_request_data_summary_:\n        n += (1 + self.lengthString(len(self.request_data_summary_)))\n    if self.has_response_data_summary_:\n        n += (1 + self.lengthString(len(self.response_data_summary_)))\n    if self.has_api_mcycles_:\n        n += (1 + self.lengthVarInt64(self.api_mcycles_))\n    if self.has_api_milliseconds_:\n        n += (1 + self.lengthVarInt64(self.api_milliseconds_))\n    if self.has_start_offset_milliseconds_:\n        n += 1\n        n += self.lengthVarInt64(self.start_offset_milliseconds_)\n    if self.has_duration_milliseconds_:\n        n += (1 + self.lengthVarInt64(self.duration_milliseconds_))\n    if self.has_namespace_:\n        n += (1 + self.lengthString(len(self.namespace_)))\n    if self.has_was_successful_:\n        n += 2\n    n += (1 * len(self.call_stack_))\n    for i in xrange(len(self.call_stack_)):\n        n += self.lengthString(self.call_stack_[i].ByteSizePartial())\n    if self.has_datastore_details_:\n        n += (1 + self.lengthString(self.datastore_details_.ByteSizePartial()))\n    if self.has_call_cost_microdollars_:\n        n += (1 + self.lengthVarInt64(self.call_cost_microdollars_))\n    n += (1 * len(self.billed_ops_))\n    for i in xrange(len(self.billed_ops_)):\n        n += self.lengthString(self.billed_ops_[i].ByteSizePartial())\n    return n\n", "label": 1}
{"function": "\n\ndef show_run_vlan(self):\n    self.write_line('spanning-tree')\n    self.write_line('!')\n    self.write_line('!')\n    for vlan in sorted(self.switch_configuration.vlans, key=(lambda v: v.number)):\n        if vlan_name(vlan):\n            self.write_line(('vlan %d name %s' % (vlan.number, vlan_name(vlan))))\n        else:\n            self.write_line(('vlan %d' % vlan.number))\n        untagged_ports = []\n        for port in self.switch_configuration.ports:\n            if (not isinstance(port, VlanPort)):\n                if ((vlan.number == 1) and (port.access_vlan is None) and (port.trunk_native_vlan is None)):\n                    untagged_ports.append(port)\n                elif ((port.access_vlan == vlan.number) or (port.trunk_native_vlan == vlan.number)):\n                    untagged_ports.append(port)\n        if (len(untagged_ports) > 0):\n            if (vlan.number == 1):\n                self.write_line((' no untagged %s' % to_port_ranges(untagged_ports)))\n            else:\n                self.write_line((' untagged %s' % to_port_ranges(untagged_ports)))\n        tagged_ports = [p for p in self.switch_configuration.ports if (p.trunk_vlans and (vlan.number in p.trunk_vlans))]\n        if tagged_ports:\n            self.write_line((' tagged %s' % to_port_ranges(tagged_ports)))\n        vif = self.get_interface_vlan_for(vlan)\n        if (vif is not None):\n            self.write_line((' router-interface %s' % vif.name))\n        self.write_line('!')\n    self.write_line('!')\n    self.write_line('')\n", "label": 1}
{"function": "\n\ndef _file_operation(operation, source_path, target_path=None, allow_undo=True, no_confirm=False, rename_on_collision=True, silent=False, hWnd=None):\n    source_path = (source_path or '')\n    if isinstance(source_path, basestring):\n        source_path = os.path.abspath(source_path)\n    else:\n        source_path = [os.path.abspath(i) for i in source_path]\n    target_path = (target_path or '')\n    if isinstance(target_path, basestring):\n        target_path = os.path.abspath(target_path)\n    else:\n        target_path = [os.path.abspath(i) for i in target_path]\n    flags = 0\n    if allow_undo:\n        flags |= shellcon.FOF_ALLOWUNDO\n    if no_confirm:\n        flags |= shellcon.FOF_NOCONFIRMATION\n    if rename_on_collision:\n        flags |= shellcon.FOF_RENAMEONCOLLISION\n    if silent:\n        flags |= shellcon.FOF_SILENT\n    (result, n_aborted) = shell.SHFileOperation(((hWnd or 0), operation, source_path, target_path, flags, None, None))\n    if (result != 0):\n        raise x_winshell(result)\n    elif n_aborted:\n        raise x_winshell(('%d operations were aborted by the user' % n_aborted))\n", "label": 1}
{"function": "\n\ndef get_shape(self, var, idx):\n    \" Optimization can call this to get the current shape_i\\n\\n        It is better to call this then use directly shape_of[var][idx]\\n        as this method should update shape_of if needed.\\n\\n        TODO: Up to now, we don't update it in all cases. Update in all cases.\\n        \"\n    r = self.shape_of[var][idx]\n    if (r.owner and isinstance(r.owner.op, Shape_i) and (r.owner.inputs[0] not in var.fgraph.variables)):\n        assert var.owner\n        node = var.owner\n        for i in node.inputs:\n            if (getattr(i, 'ndim', None) > 0):\n                self.get_shape(i, 0)\n        o_shapes = self.get_node_infer_shape(node)\n        assert (len(o_shapes) == len(node.outputs))\n        for (new_shps, out) in zip(o_shapes, node.outputs):\n            if (not hasattr(out, 'ndim')):\n                continue\n            merged_shps = list(self.shape_of[out])\n            changed = False\n            for i in range(out.ndim):\n                n_r = merged_shps[i]\n                if (n_r.owner and isinstance(n_r.owner.op, Shape_i) and (n_r.owner.inputs[0] not in var.fgraph.variables)):\n                    changed = True\n                    merged_shps[i] = new_shps[i]\n            if changed:\n                self.set_shape(out, merged_shps, override=True)\n        r = self.shape_of[var][idx]\n    return r\n", "label": 1}
{"function": "\n\ndef _parse_signature(self, args, kw):\n    values = {\n        \n    }\n    (sig_args, var_args, var_kw, defaults) = self._func_signature\n    extra_kw = {\n        \n    }\n    for (name, value) in kw.iteritems():\n        if ((not var_kw) and (name not in sig_args)):\n            raise TypeError(('Unexpected argument %s' % name))\n        if (name in sig_args):\n            values[sig_args] = value\n        else:\n            extra_kw[name] = value\n    args = list(args)\n    sig_args = list(sig_args)\n    while args:\n        while (sig_args and (sig_args[0] in values)):\n            sig_args.pop(0)\n        if sig_args:\n            name = sig_args.pop(0)\n            values[name] = args.pop(0)\n        elif var_args:\n            values[var_args] = tuple(args)\n            break\n        else:\n            raise TypeError(('Extra position arguments: %s' % ', '.join((repr(v) for v in args))))\n    for (name, value_expr) in defaults.iteritems():\n        if (name not in values):\n            values[name] = self._template._eval(value_expr, self._ns, self._pos)\n    for name in sig_args:\n        if (name not in values):\n            raise TypeError(('Missing argument: %s' % name))\n    if var_kw:\n        values[var_kw] = extra_kw\n    return values\n", "label": 1}
{"function": "\n\ndef convert_func(self, lib, opts, args):\n    if (not opts.dest):\n        opts.dest = self.config['dest'].get()\n    if (not opts.dest):\n        raise ui.UserError('no convert destination set')\n    opts.dest = util.bytestring_path(opts.dest)\n    if (not opts.threads):\n        opts.threads = self.config['threads'].get(int)\n    if self.config['paths']:\n        path_formats = ui.get_path_formats(self.config['paths'])\n    else:\n        path_formats = ui.get_path_formats()\n    if (not opts.format):\n        opts.format = self.config['format'].get(unicode).lower()\n    pretend = (opts.pretend if (opts.pretend is not None) else self.config['pretend'].get(bool))\n    if (not pretend):\n        ui.commands.list_items(lib, ui.decargs(args), opts.album)\n        if (not (opts.yes or ui.input_yn('Convert? (Y/n)'))):\n            return\n    if opts.album:\n        albums = lib.albums(ui.decargs(args))\n        items = (i for a in albums for i in a.items())\n        if self.config['copy_album_art']:\n            for album in albums:\n                self.copy_album_art(album, opts.dest, path_formats, pretend)\n    else:\n        items = iter(lib.items(ui.decargs(args)))\n    convert = [self.convert_item(opts.dest, opts.keep_new, path_formats, opts.format, pretend) for _ in range(opts.threads)]\n    pipe = util.pipeline.Pipeline([items, convert])\n    pipe.run_parallel()\n", "label": 1}
{"function": "\n\ndef onExpression(self, expression, allow_none=False):\n    if ((expression is None) and allow_none):\n        return None\n    assert expression.isExpression(), expression\n    assert expression.parent, expression\n    r = expression.computeExpressionRaw(constraint_collection=self)\n    assert (type(r) is tuple), expression\n    (new_node, change_tags, change_desc) = r\n    if (change_tags is not None):\n        self.signalChange(change_tags, expression.getSourceReference(), change_desc)\n    if (new_node is not expression):\n        expression.replaceWith(new_node)\n        if (new_node.isExpressionVariableRef() or new_node.isExpressionTempVariableRef()):\n            assert new_node.variable_trace.hasDefiniteUsages()\n    if (new_node.isExpressionVariableRef() or new_node.isExpressionTempVariableRef()):\n        if new_node.getVariable().isMaybeLocalVariable():\n            variable_trace = self.getVariableCurrentTrace(variable=new_node.getVariable().getMaybeVariable())\n            variable_trace.addUsage()\n    return new_node\n", "label": 1}
{"function": "\n\ndef _get_default_compiler(output):\n    cc = os.environ.get('CC', '')\n    cxx = os.environ.get('CXX', '')\n    if (cc or cxx):\n        output.info(('CC and CXX: %s, %s ' % ((cc or 'None'), (cxx or 'None'))))\n        command = (cc or cxx)\n        if ('gcc' in command):\n            return _gcc_compiler(output, command)\n        if ('clang' in command.lower()):\n            return _clang_compiler(output, command)\n        output.error((\"Not able to automatically detect '%s' version\" % command))\n        return None\n    if (platform.system() == 'Windows'):\n        vs = _visual_compiler_last(output)\n    gcc = _gcc_compiler(output)\n    clang = _clang_compiler(output)\n    if (platform.system() == 'Windows'):\n        return (vs or gcc or clang)\n    elif (platform.system() == 'Darwin'):\n        return (clang or gcc)\n    else:\n        return (gcc or clang)\n", "label": 1}
{"function": "\n\ndef on_spawn(self, view):\n    \"When a new view is spawned, postion the view per user's preference.\"\n    window = view.window()\n    if (window and (window.get_view_index(view)[1] != (- 1))):\n        loaded = view.settings().get('tabs_extra_spawned', False)\n        if (not loaded):\n            sheet = window.active_sheet()\n            spawn = view_spawn_pos()\n            if (spawn != 'none'):\n                sheets = window.sheets()\n                (group, index) = window.get_sheet_index(sheet)\n                last_group = None\n                last_index = None\n                if (LAST_ACTIVE is not None):\n                    for s in sheets:\n                        v = s.view()\n                        if ((v is not None) and (LAST_ACTIVE.id() == v.id())):\n                            (last_group, last_index) = window.get_sheet_index(s)\n                            break\n                active_in_range = ((last_group is not None) and (last_index is not None) and (last_group == group))\n                if (spawn == 'right'):\n                    (group, index) = window.get_sheet_index(sheets[(- 1)])\n                    window.set_sheet_index(sheet, group, index)\n                elif (spawn == 'left'):\n                    (group, index) = window.get_sheet_index(sheets[0])\n                    window.set_sheet_index(sheet, group, index)\n                elif ((spawn == 'active_right') and active_in_range):\n                    window.set_sheet_index(sheet, group, (last_index + 1))\n                elif ((spawn == 'active_left') and active_in_range):\n                    window.set_sheet_index(sheet, group, last_index)\n            view.settings().set('tabs_extra_spawned', True)\n", "label": 1}
{"function": "\n\ndef combineMaterials(mesh):\n    material_sets = []\n    for m in mesh.materials:\n        matched = False\n        for s in material_sets:\n            if (s[0].effect == m.effect):\n                s.append(m)\n                matched = True\n                break\n        if (not matched):\n            material_sets.append([m])\n    for s in material_sets:\n        if (len(s) <= 1):\n            continue\n        to_keep = s.pop(0)\n        for scene in mesh.scenes:\n            nodes_to_check = []\n            nodes_to_check.extend(scene.nodes)\n            while (len(nodes_to_check) > 0):\n                curnode = nodes_to_check.pop()\n                for node in curnode.children:\n                    if isinstance(node, collada.scene.Node):\n                        nodes_to_check.append(node)\n                    elif isinstance(node, collada.scene.GeometryNode):\n                        for matnode in node.materials:\n                            if (matnode.target in s):\n                                matnode.target = to_keep\n        for material in s:\n            del mesh.materials[material.id]\n", "label": 1}
{"function": "\n\n@property\ndef tests(self):\n    res = [x for x in self._tests if match_patterns(self.patterns, x['name'], default=True)]\n    res = [x for x in res if (not match_patterns(self.excludes, x['name'], default=False))]\n    res = [x for x in res if (x.get('perf', False) == self.perf)]\n    if (not self.nightly):\n        res = [x for x in res if (x.get('nightly', False) is False)]\n    if self.last_failed:\n        failed_names = self.get_last_failed_names()\n        res = [x for x in res if (x['name'] in failed_names)]\n        if (not res):\n            ui.warning('No failing tests found')\n    return res\n", "label": 1}
{"function": "\n\ndef _load(self, event):\n    if (event.type == START_OBJECT):\n        value = start = '{'\n        end = '}'\n    elif (event.type == START_ARRAY):\n        value = start = '['\n        end = ']'\n    else:\n        raise JSONParseError(JSON_UNEXPECTED_ELEMENT_ERROR, ('Unexpected event: ' + event.type))\n    count = 1\n    tokens = self.tokens\n    tokenIndex = self.tokenIndex\n    inString = False\n    inEscape = False\n    try:\n        while True:\n            startIndex = tokenIndex\n            for token in tokens[startIndex:]:\n                tokenIndex += 1\n                if (token == ''):\n                    pass\n                elif inString:\n                    if inEscape:\n                        inEscape = False\n                    elif (token == '\"'):\n                        inString = False\n                    elif (token == '\\\\'):\n                        inEscape = True\n                elif (token == '\"'):\n                    inString = True\n                elif (token == start):\n                    count += 1\n                elif (token == end):\n                    count -= 1\n                    if (count == 0):\n                        value += ''.join(tokens[startIndex:tokenIndex])\n                        raise StopIteration()\n            value += ''.join(tokens[startIndex:])\n            data = self.stream.read(self.size)\n            if (data == ''):\n                raise JSONParseError(JSON_INCOMPLETE_ERROR, 'Reached end of input before reaching end of JSON structures.')\n            tokens = self.pattern.split(data)\n            tokenIndex = 0\n    except StopIteration:\n        pass\n    self.tokens = tokens\n    self.tokenIndex = tokenIndex\n    try:\n        return json.loads(value, parse_float=decimal.Decimal, parse_int=decimal.Decimal)\n    except ValueError as e:\n        raise JSONParseError(JSON_SYNTAX_ERROR, ''.join(e.args))\n", "label": 1}
{"function": "\n\ndef _show_branch(root, base, path, pct=0, showpct=False, exclude='', coverage=the_coverage):\n    dirs = [k for (k, v) in root.items() if v]\n    dirs.sort()\n    for name in dirs:\n        newpath = os.path.join(path, name)\n        if newpath.lower().startswith(base):\n            relpath = newpath[len(base):]\n            (yield ('| ' * relpath.count(os.sep)))\n            (yield (\"<a class='directory' href='menu?base=%s&exclude=%s'>%s</a>\\n\" % (newpath, quote_plus(exclude), name)))\n        for chunk in _show_branch(root[name], base, newpath, pct, showpct, exclude, coverage=coverage):\n            (yield chunk)\n    if path.lower().startswith(base):\n        relpath = path[len(base):]\n        files = [k for (k, v) in root.items() if (not v)]\n        files.sort()\n        for name in files:\n            newpath = os.path.join(path, name)\n            pc_str = ''\n            if showpct:\n                try:\n                    (_, statements, _, missing, _) = coverage.analysis2(newpath)\n                except:\n                    pass\n                else:\n                    pc = _percent(statements, missing)\n                    pc_str = ('%3d%% ' % pc).replace(' ', '&nbsp;')\n                    if ((pc < float(pct)) or (pc == (- 1))):\n                        pc_str = (\"<span class='fail'>%s</span>\" % pc_str)\n                    else:\n                        pc_str = (\"<span class='pass'>%s</span>\" % pc_str)\n            (yield (TEMPLATE_ITEM % (('| ' * (relpath.count(os.sep) + 1)), pc_str, newpath, name)))\n", "label": 1}
{"function": "\n\ndef _scheduleTransmitActual(self, intent):\n    if (intent.targetAddr == self.myAddress):\n        self._processReceivedEnvelope(ReceiveEnvelope(intent.targetAddr, intent.message))\n        return self._finishIntent(intent)\n    if isinstance(intent.targetAddr.addressDetails, RoutedTCPv4ActorAddress):\n        if (not isinstance(intent.message, ForwardMessage)):\n            routing = [(A or self._adminAddr) for A in intent.targetAddr.addressDetails.routing]\n            while (routing and (routing[0] == self.myAddress)):\n                routing = routing[1:]\n            if (self.txOnly and routing and (routing[0] != self._adminAddr)):\n                routing.insert(0, self._adminAddr)\n            if routing:\n                if ((len(routing) != 1) or (routing[0] != intent.targetAddr)):\n                    intent.changeMessage(ForwardMessage(intent.message, intent.targetAddr, self.myAddress, routing))\n                    intent.addCallback((lambda r, i, ta=intent.targetAddr: i.changeTargetAddr(ta)))\n                    intent.changeTargetAddr(intent.message.fwdTargets[0])\n                    self.scheduleTransmit(getattr(self, '_addressMgr', None), intent)\n                    return\n    intent.stage = self._XMITStepSendConnect\n    if self._nextTransmitStep(intent):\n        if hasattr(intent, 'socket'):\n            self._transmitIntents[intent.socket.fileno()] = intent\n        else:\n            self._waitingTransmits.append(intent)\n", "label": 1}
{"function": "\n\ndef run(self, variables, targets, verbose):\n    assert isinstance(variables, list)\n    assert isinstance(targets, list)\n    envoptions = {\n        \n    }\n    for v in variables:\n        (_name, _value) = v.split('=', 1)\n        envoptions[_name.lower()] = _value\n    self.configure_default_packages(envoptions, targets)\n    self._install_default_packages()\n    self._verbose_level = int(verbose)\n    if ('clean' in targets):\n        targets.remove('clean')\n        targets.append('-c')\n    if ('build_script' not in envoptions):\n        variables.append(('BUILD_SCRIPT=%s' % self.get_build_script()))\n    for v in variables:\n        if (not v.startswith('BUILD_SCRIPT=')):\n            continue\n        (_, path) = v.split('=', 1)\n        if (not isfile(path)):\n            raise exception.BuildScriptNotFound(path)\n    installed_packages = PackageManager.get_installed()\n    for (name, options) in self.get_packages().items():\n        if (('alias' not in options) or (name not in installed_packages)):\n            continue\n        variables.append(('PIOPACKAGE_%s=%s' % (options['alias'].upper(), name)))\n    self._found_error = False\n    result = self._run_scons(variables, targets)\n    assert ('returncode' in result)\n    if (self._last_echo_line == '.'):\n        click.echo('')\n    return result\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('tty', [False, True])\ndef test_disable_colors_piped(tty):\n    'Verify colors enabled by default when piped to TTY and disabled when not.\\n\\n    :param bool tty: Pipe to TTY/terminal?\\n    '\n    assert_statement = 'assert __import__(\"colorclass\").codes.ANSICodeMapping.DISABLE_COLORS is {bool}'\n    command_colors_enabled = [sys.executable, '-c', assert_statement.format(bool='False')]\n    command_colors_disabled = [sys.executable, '-c', assert_statement.format(bool='True')]\n    if (not tty):\n        proc = subprocess.Popen(command_colors_disabled, stderr=subprocess.STDOUT, stdout=subprocess.PIPE)\n        output = proc.communicate()\n        assert (not output[0])\n        assert (not output[1])\n        assert (proc.poll() == 0)\n        return\n    if IS_WINDOWS:\n        c_flags = subprocess.CREATE_NEW_CONSOLE\n        proc = subprocess.Popen(command_colors_enabled, close_fds=True, creationflags=c_flags)\n        proc.communicate()\n        assert (proc.poll() == 0)\n        return\n    (master, slave) = __import__('pty').openpty()\n    proc = subprocess.Popen(command_colors_enabled, stderr=subprocess.STDOUT, stdout=slave, close_fds=True)\n    os.close(slave)\n    output = ''\n    while True:\n        try:\n            data = os.read(master, 1024).decode()\n        except OSError as exc:\n            if (exc.errno != errno.EIO):\n                raise\n            data = None\n        if data:\n            output += data\n        elif (proc.poll() is None):\n            time.sleep(0.01)\n        else:\n            break\n    os.close(master)\n    assert (not output)\n    assert (proc.poll() == 0)\n", "label": 1}
{"function": "\n\ndef _validate_option(self, option, val):\n    if (option in 'field_names'):\n        self._validate_field_names(val)\n    elif (option in ('start', 'end', 'max_width', 'padding_width', 'left_padding_width', 'right_padding_width', 'format')):\n        self._validate_nonnegative_int(option, val)\n    elif (option in 'sortby'):\n        self._validate_field_name(option, val)\n    elif (option in 'sort_key'):\n        self._validate_function(option, val)\n    elif (option in 'hrules'):\n        self._validate_hrules(option, val)\n    elif (option in 'vrules'):\n        self._validate_vrules(option, val)\n    elif (option in 'fields'):\n        self._validate_all_field_names(option, val)\n    elif (option in ('header', 'border', 'reversesort', 'xhtml', 'print_empty')):\n        self._validate_true_or_false(option, val)\n    elif (option in 'header_style'):\n        self._validate_header_style(val)\n    elif (option in 'int_format'):\n        self._validate_int_format(option, val)\n    elif (option in 'float_format'):\n        self._validate_float_format(option, val)\n    elif (option in ('vertical_char', 'horizontal_char', 'junction_char')):\n        self._validate_single_char(option, val)\n    elif (option in 'attributes'):\n        self._validate_attributes(option, val)\n    else:\n        raise Exception(('Unrecognised option: %s!' % option))\n", "label": 1}
{"function": "\n\ndef get_implicit_depends_on(input_hash, depends_on):\n    '\\n    Add DNAnexus links to non-closed data objects in input_hash to depends_on\\n    '\n    q = []\n    for field in input_hash:\n        possible_dep = get_nonclosed_data_obj_link(input_hash[field])\n        if (possible_dep is not None):\n            depends_on.append(possible_dep)\n        elif (isinstance(input_hash[field], list) or isinstance(input_hash[field], dict)):\n            q.append(input_hash[field])\n    while (len(q) > 0):\n        thing = q.pop()\n        if isinstance(thing, list):\n            for i in range(len(thing)):\n                possible_dep = get_nonclosed_data_obj_link(thing[i])\n                if (possible_dep is not None):\n                    depends_on.append(possible_dep)\n                elif (isinstance(thing[i], list) or isinstance(thing[i], dict)):\n                    q.append(thing[i])\n        else:\n            for field in thing:\n                possible_dep = get_nonclosed_data_obj_link(thing[field])\n                if (possible_dep is not None):\n                    depends_on.append(possible_dep)\n                elif (isinstance(thing[field], list) or isinstance(thing[field], dict)):\n                    q.append(thing[field])\n", "label": 1}
{"function": "\n\ndef test_filtered_nested_expression(self):\n    for ct in [0, 1, 2, 4, 8, 16, 32, 64]:\n        self.equivalentEvaluationTest((lambda : sum((((outer * 503) + inner) for outer in xrange(ct) for inner in xrange(outer)))))\n        self.equivalentEvaluationTest((lambda : sum((((outer * 503) + inner) for outer in xrange(ct) if ((outer % 2) == 0) for inner in xrange(outer)))))\n        self.equivalentEvaluationTest((lambda : sum((((outer * 503) + inner) for outer in xrange(ct) if ((outer % 2) == 0) for inner in xrange(outer) if ((inner % 2) == 0)))))\n        self.equivalentEvaluationTest((lambda : sum((((outer * 503) + inner) for outer in xrange(ct) for inner in xrange(outer) if ((inner % 2) == 0)))))\n", "label": 1}
{"function": "\n\ndef set_pubsub_channels(self, request, channels):\n    '\\n        Initialize the channels used for publishing and subscribing messages through the message queue.\\n        '\n    facility = request.path_info.replace(settings.WEBSOCKET_URL, '', 1)\n    audience = {\n        'users': ((('publish-user' in channels) and [SELF]) or []),\n        'groups': ((('publish-group' in channels) and [SELF]) or []),\n        'sessions': ((('publish-session' in channels) and [SELF]) or []),\n        'broadcast': ('publish-broadcast' in channels),\n    }\n    self._publishers = set()\n    for key in self._get_message_channels(request=request, facility=facility, **audience):\n        self._publishers.add(key)\n    audience = {\n        'users': ((('subscribe-user' in channels) and [SELF]) or []),\n        'groups': ((('subscribe-group' in channels) and [SELF]) or []),\n        'sessions': ((('subscribe-session' in channels) and [SELF]) or []),\n        'broadcast': ('subscribe-broadcast' in channels),\n    }\n    self._subscription = self._connection.pubsub()\n    for key in self._get_message_channels(request=request, facility=facility, **audience):\n        self._subscription.subscribe(key)\n", "label": 1}
{"function": "\n\ndef test_update_crossing_duration1_not_duration_at_1st_step(self, duration1, duration2):\n    global rec\n    if (duration2 == 0.0):\n        return\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    composite = ac.sequence(a1, a2)\n    rec = []\n    node.do(composite)\n    elapsed = 0.0\n    next_elapsed = ((duration1 + duration2) / 2.0)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    recx = [e for e in rec if (e[1] != 'step')]\n    rec = [e for e in recx if (e[0] == name1)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert ((rec[1][1] == 'update') and (rec[1][2] == 1.0))\n    assert (rec[2][1] == 'stop')\n    assert (len(rec) == 3)\n    rec = [e for e in recx if (e[0] == name2)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert (rec[1][1] == 'update')\n    assert (abs((rec[1][2] - ((next_elapsed - duration1) / duration2))) < fe)\n    assert (len(rec) == 2)\n", "label": 1}
{"function": "\n\ndef profile_main(func):\n    from cStringIO import StringIO\n    import cProfile\n    import logging\n    import pstats\n    import random\n    only_forced_profile = getattr(settings, 'ONLY_FORCED_PROFILE', False)\n    profile_percentage = getattr(settings, 'PROFILE_PERCENTAGE', None)\n    if ((only_forced_profile and ('profile=forced' not in os.environ.get('QUERY_STRING'))) or ((not only_forced_profile) and profile_percentage and ((float(profile_percentage) / 100.0) <= random.random()))):\n        return func()\n    prof = cProfile.Profile()\n    prof = prof.runctx('func()', globals(), locals())\n    stream = StringIO()\n    stats = pstats.Stats(prof, stream=stream)\n    sort_by = getattr(settings, 'SORT_PROFILE_RESULTS_BY', 'time')\n    if (not isinstance(sort_by, (list, tuple))):\n        sort_by = (sort_by,)\n    stats.sort_stats(*sort_by)\n    restrictions = []\n    profile_pattern = getattr(settings, 'PROFILE_PATTERN', None)\n    if profile_pattern:\n        restrictions.append(profile_pattern)\n    max_results = getattr(settings, 'MAX_PROFILE_RESULTS', 80)\n    if (max_results and (max_results != 'all')):\n        restrictions.append(max_results)\n    stats.print_stats(*restrictions)\n    extra_output = (getattr(settings, 'EXTRA_PROFILE_OUTPUT', None) or ())\n    if (not isinstance(sort_by, (list, tuple))):\n        extra_output = (extra_output,)\n    if ('callees' in extra_output):\n        stats.print_callees()\n    if ('callers' in extra_output):\n        stats.print_callers()\n    logging.info('Profile data:\\n%s', stream.getvalue())\n", "label": 1}
{"function": "\n\ndef get_command_part(parameter, python_proxy_pool=None):\n    'Converts a Python object into a string representation respecting the\\n    Py4J protocol.\\n\\n    For example, the integer `1` is converted to `u\"i1\"`\\n\\n    :param parameter: the object to convert\\n    :rtype: the string representing the command part\\n    '\n    command_part = ''\n    if (parameter is None):\n        command_part = NULL_TYPE\n    elif isinstance(parameter, bool):\n        command_part = (BOOLEAN_TYPE + smart_decode(parameter))\n    elif isinstance(parameter, Decimal):\n        command_part = (DECIMAL_TYPE + smart_decode(parameter))\n    elif (isinstance(parameter, int) and (parameter <= JAVA_MAX_INT)):\n        command_part = (INTEGER_TYPE + smart_decode(parameter))\n    elif (isinstance(parameter, long) or isinstance(parameter, int)):\n        command_part = (LONG_TYPE + smart_decode(parameter))\n    elif isinstance(parameter, float):\n        command_part = (DOUBLE_TYPE + encode_float(parameter))\n    elif isbytearray(parameter):\n        command_part = (BYTES_TYPE + encode_bytearray(parameter))\n    elif ispython3bytestr(parameter):\n        command_part = (BYTES_TYPE + encode_bytearray(parameter))\n    elif isinstance(parameter, basestring):\n        command_part = (STRING_TYPE + escape_new_line(parameter))\n    elif is_python_proxy(parameter):\n        command_part = (PYTHON_PROXY_TYPE + python_proxy_pool.put(parameter))\n        for interface in parameter.Java.implements:\n            command_part += (';' + interface)\n    else:\n        command_part = (REFERENCE_TYPE + parameter._get_object_id())\n    command_part += '\\n'\n    return command_part\n", "label": 1}
{"function": "\n\ndef norm(x, ord):\n    x = as_tensor_variable(x)\n    ndim = x.ndim\n    if (ndim == 0):\n        raise ValueError(\"'axis' entry is out of bounds.\")\n    elif (ndim == 1):\n        if (ord is None):\n            return (tensor.sum((x ** 2)) ** 0.5)\n        elif (ord == 'inf'):\n            return tensor.max(abs(x))\n        elif (ord == '-inf'):\n            return tensor.min(abs(x))\n        elif (ord == 0):\n            return x[x.nonzero()].shape[0]\n        else:\n            try:\n                z = (tensor.sum(abs((x ** ord))) ** (1.0 / ord))\n            except TypeError:\n                raise ValueError('Invalid norm order for vectors.')\n            return z\n    elif (ndim == 2):\n        if ((ord is None) or (ord == 'fro')):\n            return (tensor.sum(abs((x ** 2))) ** 0.5)\n        elif (ord == 'inf'):\n            return tensor.max(tensor.sum(abs(x), 1))\n        elif (ord == '-inf'):\n            return tensor.min(tensor.sum(abs(x), 1))\n        elif (ord == 1):\n            return tensor.max(tensor.sum(abs(x), 0))\n        elif (ord == (- 1)):\n            return tensor.min(tensor.sum(abs(x), 0))\n        else:\n            raise ValueError(0)\n    elif (ndim > 2):\n        raise NotImplementedError(\"We don't support norm witn ndim > 2\")\n", "label": 1}
{"function": "\n\ndef _sqlalchemy_type(self, col):\n    dtype = (self.dtype or {\n        \n    })\n    if (col.name in dtype):\n        return self.dtype[col.name]\n    col_type = self._get_notnull_col_dtype(col)\n    from sqlalchemy.types import BigInteger, Integer, Float, Text, Boolean, DateTime, Date, Time\n    if ((col_type == 'datetime64') or (col_type == 'datetime')):\n        try:\n            tz = col.tzinfo\n            return DateTime(timezone=True)\n        except:\n            return DateTime\n    if (col_type == 'timedelta64'):\n        warnings.warn(\"the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\", UserWarning, stacklevel=8)\n        return BigInteger\n    elif (col_type == 'floating'):\n        if (col.dtype == 'float32'):\n            return Float(precision=23)\n        else:\n            return Float(precision=53)\n    elif (col_type == 'integer'):\n        if (col.dtype == 'int32'):\n            return Integer\n        else:\n            return BigInteger\n    elif (col_type == 'boolean'):\n        return Boolean\n    elif (col_type == 'date'):\n        return Date\n    elif (col_type == 'time'):\n        return Time\n    elif (col_type == 'complex'):\n        raise ValueError('Complex datatypes not supported')\n    return Text\n", "label": 1}
{"function": "\n\ndef select_dir(window, index=(- 2), level=0, paths=None, func=None, condition_func=None, is_user=False):\n    if (index == (- 1)):\n        return ''\n    if ((level > 0) and (index == 0)):\n        sel_path = paths[0].split('(')[1][:(- 1)]\n        if func:\n            return_code = func(window, sel_path)\n            if (return_code == 0):\n                return\n        else:\n            return\n    else:\n        if (index == 1):\n            level -= 1\n        elif (index > 1):\n            level += 1\n        if (level <= 0):\n            level = 0\n            dir_path = '.'\n            parent_path = '..'\n            if is_user:\n                paths = pyarduino.base.sys_path.list_user_root_path()\n            else:\n                paths = pyarduino.base.sys_path.list_os_root_path()\n        else:\n            sel_path = paths[index]\n            if (sel_path == pyarduino.base.sys_path.ROOT_PATH):\n                sel_path = '/'\n            dir_path = os.path.abspath(sel_path)\n            if (condition_func and condition_func(dir_path)):\n                func(window, dir_path)\n                return\n            parent_path = os.path.join(dir_path, '..')\n            cur_dir = pyarduino.base.abs_file.Dir(dir_path)\n            sub_dirs = cur_dir.list_dirs()\n            paths = [d.get_path() for d in sub_dirs]\n        paths.insert(0, parent_path)\n        paths.insert(0, ('Select current dir (%s)' % dir_path))\n    sublime.set_timeout((lambda : window.show_quick_panel(paths, (lambda index: select_dir(window, index, level, paths, func, condition_func, is_user)))), 5)\n", "label": 1}
{"function": "\n\ndef pytest_generate_tests(metafunc):\n    'Parametrize tests to run on all backends.'\n    if ('backend' in metafunc.fixturenames):\n        skip_backends = set()\n        if hasattr(metafunc.module, 'skip_backends'):\n            skip_backends = skip_backends.union(set(metafunc.module.skip_backends))\n        if hasattr(metafunc.cls, 'skip_backends'):\n            skip_backends = skip_backends.union(set(metafunc.cls.skip_backends))\n        if metafunc.config.option.backend:\n            backend = set([x.lower() for x in metafunc.config.option.backend])\n        else:\n            backend = set(backends.keys())\n        if hasattr(metafunc.module, 'backends'):\n            backend = backend.intersection(set(metafunc.module.backends))\n        if hasattr(metafunc.cls, 'backends'):\n            backend = backend.intersection(set(metafunc.cls.backends))\n        lazy = []\n        if (not (('skip_greedy' in metafunc.fixturenames) or metafunc.config.option.lazy)):\n            lazy.append('greedy')\n        if (not (('skip_lazy' in metafunc.fixturenames) or metafunc.config.option.greedy)):\n            lazy.append('lazy')\n        backend = [b for b in backend.difference(skip_backends) if (not (('skip_' + b) in metafunc.fixturenames))]\n        params = list(product(backend, lazy))\n        metafunc.parametrize('backend', (params or [(None, None)]), indirect=True, ids=['-'.join(p) for p in params])\n", "label": 1}
{"function": "\n\n@specialize.ll()\ndef wrap(*_pyval):\n    if (len(_pyval) == 1):\n        pyval = _pyval[0]\n        if isinstance(pyval, bool):\n            return (w_true if pyval else w_false)\n        if isinstance(pyval, int):\n            return W_Fixnum(pyval)\n        if isinstance(pyval, float):\n            return W_Flonum(pyval)\n        if isinstance(pyval, W_Object):\n            return pyval\n    elif (len(_pyval) == 2):\n        car = _pyval[0]\n        cdr = wrap(_pyval[1])\n        if isinstance(car, bool):\n            if cdr.is_proper_list():\n                return W_WrappedConsProper(wrap(car), cdr)\n            return W_WrappedCons(wrap(car), cdr)\n        if isinstance(car, int):\n            if cdr.is_proper_list():\n                return W_UnwrappedFixnumConsProper(car, cdr)\n            return W_UnwrappedFixnumCons(car, cdr)\n        if isinstance(car, float):\n            if cdr.is_proper_list():\n                return W_UnwrappedFlonumConsProper(car, cdr)\n            return W_UnwrappedFlonumCons(car, cdr)\n        if isinstance(car, W_Object):\n            return W_Cons.make(car, cdr)\n    assert False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.ArchiveLocalID = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.CapsuleID = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.title = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.description = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.BOOL):\n                self.sandbox = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef body(self, dirname=None, part=None):\n    '\\n        Recursively traverses a directory and generates the multipart encoded\\n        body.\\n        '\n    if (part is None):\n        outer = True\n        part = self.envelope\n        dirname = self.directory\n    else:\n        outer = False\n    if (dirname is None):\n        dirname = part.name\n    for chunk in self.gen_chunks(part.open()):\n        (yield chunk)\n    subpart = BodyGenerator(dirname)\n    for chunk in self.gen_chunks(subpart.write_headers()):\n        (yield chunk)\n    (files, subdirs) = utils.ls_dir(dirname)\n    for fn in files:\n        if (not fnmatch.fnmatch(fn, self.fnpattern)):\n            continue\n        fullpath = os.path.join(dirname, fn)\n        for chunk in self.gen_chunks(subpart.file_open(fullpath)):\n            (yield chunk)\n        with open(fullpath, 'rb') as fp:\n            for chunk in self.file_chunks(fp):\n                (yield chunk)\n        for chunk in self.gen_chunks(subpart.file_close()):\n            (yield chunk)\n    if self.recursive:\n        for subdir in subdirs:\n            fullpath = os.path.join(dirname, subdir)\n            for chunk in self.body(fullpath, subpart):\n                (yield chunk)\n    for chunk in self.gen_chunks(subpart.close()):\n        (yield chunk)\n    if outer:\n        for chunk in self.close():\n            (yield chunk)\n", "label": 1}
{"function": "\n\ndef merge_tops(self, tops):\n    '\\n        Cleanly merge the top files\\n        '\n    top = collections.defaultdict(OrderedDict)\n    orders = collections.defaultdict(OrderedDict)\n    for ctops in six.itervalues(tops):\n        for ctop in ctops:\n            for (saltenv, targets) in six.iteritems(ctop):\n                if (saltenv == 'include'):\n                    continue\n                for tgt in targets:\n                    matches = []\n                    states = OrderedDict()\n                    orders[saltenv][tgt] = 0\n                    ignore_missing = False\n                    for comp in ctop[saltenv][tgt]:\n                        if isinstance(comp, dict):\n                            if ('match' in comp):\n                                matches.append(comp)\n                            if ('order' in comp):\n                                order = comp['order']\n                                if (not isinstance(order, int)):\n                                    try:\n                                        order = int(order)\n                                    except ValueError:\n                                        order = 0\n                                orders[saltenv][tgt] = order\n                            if comp.get('ignore_missing', False):\n                                ignore_missing = True\n                        if isinstance(comp, six.string_types):\n                            states[comp] = True\n                    if ignore_missing:\n                        if (saltenv not in self.ignored_pillars):\n                            self.ignored_pillars[saltenv] = []\n                        self.ignored_pillars[saltenv].extend(states.keys())\n                    top[saltenv][tgt] = matches\n                    top[saltenv][tgt].extend(states)\n    return self.sort_top_targets(top, orders)\n", "label": 1}
{"function": "\n\n@signalcommand\ndef handle(self, *args, **options):\n    if ((len(args) < 1) and (not options['all_applications'])):\n        raise CommandError('need one or more arguments for appname')\n    use_pygraphviz = options.get('pygraphviz', False)\n    use_pydot = options.get('pydot', False)\n    use_json = options.get('json', False)\n    if (use_json and (use_pydot or use_pygraphviz)):\n        raise CommandError('Cannot specify --json with --pydot or --pygraphviz')\n    cli_options = ' '.join(sys.argv[2:])\n    graph_data = generate_graph_data(args, cli_options=cli_options, **options)\n    if use_json:\n        self.render_output_json(graph_data, **options)\n        return\n    dotdata = generate_dot(graph_data)\n    if (not six.PY3):\n        dotdata = dotdata.encode('utf-8')\n    if options['outputfile']:\n        if ((not use_pygraphviz) and (not use_pydot)):\n            if HAS_PYGRAPHVIZ:\n                use_pygraphviz = True\n            elif HAS_PYDOT:\n                use_pydot = True\n        if use_pygraphviz:\n            self.render_output_pygraphviz(dotdata, **options)\n        elif use_pydot:\n            self.render_output_pydot(dotdata, **options)\n        else:\n            raise CommandError('Neither pygraphviz nor pydot could be found to generate the image')\n    else:\n        self.print_output(dotdata)\n", "label": 1}
{"function": "\n\n@OnlyIfDescendedFrom(['insert', 'update', 'delete', 'replace', 'select'])\ndef visit_column_spec(self, tokens, table_aliases):\n    try:\n        self._db_schema.FindColumnFromSpec(tokens, table_aliases)\n    except KeyError:\n        column_name = tokens['column'][0]\n        if ((column_name == '*') and self.IsDescendedFrom(['select_expression'])):\n            return\n        results = set()\n        logging.debug('Searching for %s in: %s', column_name, table_aliases)\n        for table_spec in table_aliases.itervalues():\n            try:\n                table = self._db_schema.FindTableFromSpec(table_spec)\n            except schema.UnknownNameException:\n                self.AddError(tokens, ('Unknown table: %s' % table_spec))\n                continue\n            try:\n                results.add(table.FindColumn(column_name))\n            except schema.UnknownNameException:\n                pass\n        if (not results):\n            self.AddError(tokens, ('Unknown column: %s' % column_name))\n        elif self.IsDescendedFrom(['using']):\n            if (len(results) != 2):\n                self.AddError(tokens, ('Columns in USING must be in exactly two tables: %s' % [str(r) for r in results]))\n        elif (len(results) != 1):\n            self.AddError(tokens, ('Ambiguous column: %s' % [str(r) for r in results]))\n    except schema.UnknownNameException:\n        self.AddError(tokens, ('Unknown column: %s' % tokens))\n", "label": 1}
{"function": "\n\ndef find_loose_thresholds(self, percentile=0.99):\n    results = []\n    for c in range(len(self.sensor.band_names)):\n        (threshold, params) = (self.thresholds[c], self.distributions[c])\n        if (params != None):\n            t = self.__cdf_percentile(params, percentile, self.backscatter_model[c])\n            if self.sensor.log_scale:\n                t = (10 ** t)\n            results.append(t)\n        else:\n            if self.sensor.log_scale:\n                threshold = math.log10(threshold)\n            start = self.histograms[c][0]\n            width = self.histograms[c][1]\n            values = self.histograms[c][2]\n            i = (int(((threshold - start) / width)) + 1)\n            if (self.backscatter_model[c] == RadarHistogram.BACKSCATTER_MODEL_DIP):\n                while (i < (len(values) - 3)):\n                    if ((values[i] > values[(i + 1)]) and (values[i] > values[(i + 2)]) and (values[i] > values[(i + 3)])):\n                        break\n                    i += 1\n            else:\n                while (i < (len(values) - 3)):\n                    if ((values[i] < values[(i + 1)]) and (values[i] < values[(i + 2)]) and (values[i] < values[(i + 3)])):\n                        break\n                    i += 1\n            t = (start + (i * width))\n            if self.sensor.log_scale:\n                t = (10 ** t)\n            results.append(t)\n    return results\n", "label": 1}
{"function": "\n\ndef register(name, service, export_methods=False):\n    _registry[name] = service\n    if (service not in _registered_modules):\n        _registered_modules.append(service)\n    if export_methods:\n        _methods[name] = {\n            \n        }\n        if inspect.ismodule(service):\n            for (attr, value) in service.__dict__.items():\n                if ((not attr.startswith('_')) and (not _registry.get(attr)) and hasattr(value, '__call__')):\n                    _methods[name][attr] = value\n        else:\n            for (attr, value) in service.__class__.__dict__.items():\n                if ((not attr.startswith('_')) and (not _registry.get(attr)) and hasattr(value, '__call__')):\n                    _methods[name][attr] = value.__get__(service, service.__class__)\n    for cls in _consumer_map.get(name, []):\n        _inject(cls, name, service)\n    if (name in _consumer_map):\n        del _consumer_map[name]\n    return service\n", "label": 1}
{"function": "\n\ndef checkReplaceSubstring(self, arguments):\n    allowedFields = {\n        \n    }\n    allowedFields['replace'] = (str, True)\n    allowedFields['with'] = (str, True)\n    for category in arguments.keys():\n        for argumentAttributes in arguments[category]:\n            if ('replace substring' in argumentAttributes):\n                argument = argumentAttributes['long form argument']\n                replace = argumentAttributes['replace substring']\n                for data in replace:\n                    if (not methods.checkIsDictionary(data, self.allowTermination)):\n                        if self.allowTermination:\n                            self.errors.invalidReplaceSubstring(self.name, category, argument)\n                        else:\n                            return False\n                    observedFields = []\n                    toReplace = None\n                    replaceWith = None\n                    for key in data:\n                        value = data[key]\n                        if (key not in allowedFields):\n                            if self.allowTermination:\n                                self.errors.invalidReplaceSubstring(self.name, category, argument)\n                            else:\n                                return False\n                        if (key == 'replace'):\n                            toReplace = value\n                        elif (key == 'with'):\n                            replaceWith = value\n                        observedFields.append(key)\n                    self.arguments[argument].isReplaceSubstring = True\n                    self.arguments[argument].replaceSubstring.append((str(toReplace), str(replaceWith)))\n                    for field in allowedFields.keys():\n                        if (allowedFields[field][1] and (field not in observedFields)):\n                            if self.allowTermination:\n                                self.errors.invalidReplaceSubstring(self.name, category, argument)\n                            else:\n                                return False\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, buffer, encoding=None, errors=None, newline=None, line_buffering=False):\n    self._ok = False\n    if ((newline is not None) and (not isinstance(newline, basestring))):\n        raise TypeError(('illegal newline type: %r' % (type(newline),)))\n    if (newline not in (None, '', '\\n', '\\r', '\\r\\n')):\n        raise ValueError(('illegal newline value: %r' % (newline,)))\n    if (encoding is None):\n        try:\n            import locale\n        except ImportError:\n            encoding = 'ascii'\n        else:\n            encoding = locale.getpreferredencoding()\n    if (not isinstance(encoding, basestring)):\n        raise ValueError(('invalid encoding: %r' % encoding))\n    if (errors is None):\n        errors = 'strict'\n    elif (not isinstance(errors, basestring)):\n        raise ValueError(('invalid errors: %r' % errors))\n    self._buffer = buffer\n    self._line_buffering = line_buffering\n    self._encoding = encoding\n    self._errors = errors\n    self._readuniversal = (not newline)\n    self._readtranslate = (newline is None)\n    self._readnl = newline\n    self._writetranslate = (newline != '')\n    self._writenl = (newline or os.linesep)\n    self._encoder = None\n    self._decoder = None\n    self._decoded_chars = ''\n    self._decoded_chars_used = 0\n    self._snapshot = None\n    self._seekable = self._telling = self.buffer.seekable()\n    self._ok = True\n    if (self._seekable and self.writable()):\n        position = self.buffer.tell()\n        if (position != 0):\n            try:\n                self._get_encoder().setstate(0)\n            except LookupError:\n                pass\n", "label": 1}
{"function": "\n\ndef _modified_event(self, dict_, attr, previous, collection=False, force=False):\n    if (not attr.send_modified_events):\n        return\n    if ((attr.key not in self.committed_state) or force):\n        if collection:\n            if (previous is NEVER_SET):\n                if (attr.key in dict_):\n                    previous = dict_[attr.key]\n            if (previous not in (None, NO_VALUE, NEVER_SET)):\n                previous = attr.copy(previous)\n        self.committed_state[attr.key] = previous\n    if ((self.session_id and (self._strong_obj is None)) or (not self.modified)):\n        instance_dict = self._instance_dict()\n        if instance_dict:\n            instance_dict._modified.add(self)\n        inst = self.obj()\n        if self.session_id:\n            self._strong_obj = inst\n        if (inst is None):\n            raise orm_exc.ObjectDereferencedError((\"Can't emit change event for attribute '%s' - parent object of type %s has been garbage collected.\" % (self.manager[attr.key], base.state_class_str(self))))\n        self.modified = True\n", "label": 1}
{"function": "\n\n@register.inclusion_tag('scaffold/admin/submit_line.html', takes_context=True)\ndef submit_row(context):\n    '\\n    Displays the row of buttons for delete and save. \\n    '\n    opts = context['opts']\n    change = context['change']\n    is_popup = context['is_popup']\n    save_as = context['save_as']\n    allow_associated_ordering = context['allow_associated_ordering']\n    model_label = context['model_label']\n    return {\n        'model_label': model_label,\n        'allow_associated_ordering': allow_associated_ordering,\n        'show_move': ((not is_popup) and change),\n        'onclick_attrib': ((opts.get_ordered_objects() and change and 'onclick=\"submitOrderForm();\"') or ''),\n        'show_delete_link': ((not is_popup) and context['has_delete_permission'] and (change or context['show_delete'])),\n        'show_save_as_new': ((not is_popup) and change and save_as),\n        'show_save_and_add_another': (context['has_add_permission'] and (not is_popup) and ((not save_as) or context['add'])),\n        'show_save_and_continue': ((not is_popup) and context['has_change_permission']),\n        'is_popup': is_popup,\n        'show_save': True,\n    }\n", "label": 1}
{"function": "\n\ndef recondition_slice_assign(val, last_len, new_len):\n    if (not isinstance(val, slice)):\n        return slice(val, (val + 1))\n    diff = (new_len - last_len)\n    (start, stop, step) = (val.start, val.stop, val.step)\n    if (stop <= start):\n        return slice(0, 0)\n    if ((step is not None) and (step != 1)):\n        assert (last_len == new_len)\n        if (stop < 0):\n            stop = max(0, (last_len + stop))\n        stop = min(last_len, stop)\n        if (start < 0):\n            start = max(0, (last_len + start))\n        start = min(last_len, start)\n        return slice(start, stop, step)\n    if (start < 0):\n        start = (last_len + start)\n    if (stop < 0):\n        stop = (last_len + stop)\n    if ((start < 0) or (stop < 0) or (start > last_len) or (stop > last_len) or (new_len != last_len)):\n        return None\n    return slice(start, stop)\n", "label": 1}
{"function": "\n\ndef testSubstitute2(self):\n    os.environ['SOME_ENV_VAR'] = 'test_test_test'\n    config = Configuration()\n    config.readfp(StringIO(CONFIG2))\n    assert config.has_section('section1')\n    assert config.has_section('section2')\n    assert (not config.has_section('foo'))\n    assert (not config.has_section('bar'))\n    assert (not config.has_section('bar2'))\n    assert config.has_option('section1', 'foo')\n    assert config.has_option('section1', 'bar')\n    assert (not config.has_option('section1', 'bar2'))\n    assert config.has_option('section2', 'foo')\n    assert config.has_option('section2', 'bar')\n    assert (config.get('section1', 'foo') == 'bar')\n    assert (config.get('section1', 'bar') == 'bar')\n    assert (config.get('section2', 'foo') == 'bar')\n    assert (config.get('section2', 'bar') == os.environ['SOME_ENV_VAR'])\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_jid_ != x.has_jid_):\n        return 0\n    if (self.has_jid_ and (self.jid_ != x.jid_)):\n        return 0\n    if (self.has_type_ != x.has_type_):\n        return 0\n    if (self.has_type_ and (self.type_ != x.type_)):\n        return 0\n    if (self.has_show_ != x.has_show_):\n        return 0\n    if (self.has_show_ and (self.show_ != x.show_)):\n        return 0\n    if (self.has_status_ != x.has_status_):\n        return 0\n    if (self.has_status_ and (self.status_ != x.status_)):\n        return 0\n    if (self.has_from_jid_ != x.has_from_jid_):\n        return 0\n    if (self.has_from_jid_ and (self.from_jid_ != x.from_jid_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef _get_keystone_session(self):\n    cacert = (self.options.os_cacert or None)\n    cert = (self.options.os_cert or None)\n    key = (self.options.os_key or None)\n    insecure = (self.options.insecure or False)\n    ks_session = session.Session.construct(dict(cacert=cacert, cert=cert, key=key, insecure=insecure))\n    (v2_auth_url, v3_auth_url) = self._discover_auth_versions(session=ks_session, auth_url=self.options.os_auth_url)\n    user_domain_name = (self.options.os_user_domain_name or None)\n    user_domain_id = (self.options.os_user_domain_id or None)\n    project_domain_name = (self.options.os_project_domain_name or None)\n    project_domain_id = (self.options.os_project_domain_id or None)\n    domain_info = (user_domain_name or user_domain_id or project_domain_name or project_domain_id)\n    if ((v2_auth_url and (not domain_info)) or (not v3_auth_url)):\n        ks_session.auth = self.get_v2_auth(v2_auth_url)\n    else:\n        ks_session.auth = self.get_v3_auth(v3_auth_url)\n    return ks_session\n", "label": 1}
{"function": "\n\ndef __init__(self, bits, length=None):\n    if isinstance(bits, int):\n        if (length is None):\n            length = bits.bit_length()\n        else:\n            assert (length >= bits.bit_length())\n        if (bits < 0):\n            bits &= ((1 << length) - 1)\n        hash_value = None\n    elif isinstance(bits, BitString):\n        (bits, length, hash_value) = (bits._bits, bits._length, bits._hash)\n    elif isinstance(bits, str):\n        bit_str = bits\n        bits = 0\n        for char in bit_str:\n            bits <<= 1\n            if (char == '1'):\n                bits += 1\n            else:\n                assert (char == '0')\n        if (length is None):\n            length = len(bit_str)\n        else:\n            assert (length >= len(bit_str))\n        hash_value = None\n    else:\n        bit_sequence = bits\n        bits = 0\n        count = 0\n        for bit in bit_sequence:\n            count += 1\n            bits <<= 1\n            if bit:\n                bits += 1\n        if (length is None):\n            length = count\n        else:\n            assert (length >= count)\n        hash_value = None\n    super().__init__(bits, hash_value)\n    self._length = length\n", "label": 1}
{"function": "\n\ndef test_timelines(client):\n    'Make sure that timelines work'\n    register_and_login(client, 'foo', 'default')\n    add_message(client, 'the message by foo')\n    logout(client)\n    register_and_login(client, 'bar', 'default')\n    add_message(client, 'the message by bar')\n    rv = client.get('/public')\n    assert (b'the message by foo' in rv.data)\n    assert (b'the message by bar' in rv.data)\n    rv = client.get('/')\n    assert (b'the message by foo' not in rv.data)\n    assert (b'the message by bar' in rv.data)\n    rv = client.get('/foo/follow', follow_redirects=True)\n    assert (b'You are now following &#34;foo&#34;' in rv.data)\n    rv = client.get('/')\n    assert (b'the message by foo' in rv.data)\n    assert (b'the message by bar' in rv.data)\n    rv = client.get('/bar')\n    assert (b'the message by foo' not in rv.data)\n    assert (b'the message by bar' in rv.data)\n    rv = client.get('/foo')\n    assert (b'the message by foo' in rv.data)\n    assert (b'the message by bar' not in rv.data)\n    rv = client.get('/foo/unfollow', follow_redirects=True)\n    assert (b'You are no longer following &#34;foo&#34;' in rv.data)\n    rv = client.get('/')\n    assert (b'the message by foo' not in rv.data)\n    assert (b'the message by bar' in rv.data)\n", "label": 1}
{"function": "\n\ndef fetch(self, chrom, start, end, strand=None):\n    '\\n        For TABIX indexed BED files, find all regions w/in a range\\n\\n        For non-TABIX index BED files, use the calculated bins, and\\n        output matching regions\\n        '\n    if self.__tabix:\n        for match in self.__tabix.fetch(chrom, start, end):\n            region = BedRegion(*match.split('\\t'))\n            if ((not strand) or (strand and (region.strand == strand))):\n                (yield region)\n    else:\n        startbin = (start / BedFile._bin_const)\n        endbin = (end / BedFile._bin_const)\n        buf = set()\n        for bin in xrange(startbin, (endbin + 1)):\n            if ((chrom, bin) in self._bins):\n                for region in self._bins[(chrom, bin)]:\n                    if (strand and (strand != region.strand)):\n                        continue\n                    if ((start <= region.start <= end) or (start <= region.end <= end)):\n                        if (not (region in buf)):\n                            (yield region)\n                            buf.add(region)\n                    elif ((region.start < start) and (region.end > end)):\n                        if (not (region in buf)):\n                            (yield region)\n                            buf.add(region)\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.re:\n        result = self.re.match(instring, loc)\n        if (not result):\n            raise ParseException(instring, loc, self.errmsg, self)\n        loc = result.end()\n        return (loc, result.group())\n    if (not (instring[loc] in self.initChars)):\n        raise ParseException(instring, loc, self.errmsg, self)\n    start = loc\n    loc += 1\n    instrlen = len(instring)\n    bodychars = self.bodyChars\n    maxloc = (start + self.maxLen)\n    maxloc = min(maxloc, instrlen)\n    while ((loc < maxloc) and (instring[loc] in bodychars)):\n        loc += 1\n    throwException = False\n    if ((loc - start) < self.minLen):\n        throwException = True\n    if (self.maxSpecified and (loc < instrlen) and (instring[loc] in bodychars)):\n        throwException = True\n    if self.asKeyword:\n        if (((start > 0) and (instring[(start - 1)] in bodychars)) or ((loc < instrlen) and (instring[loc] in bodychars))):\n            throwException = True\n    if throwException:\n        raise ParseException(instring, loc, self.errmsg, self)\n    return (loc, instring[start:loc])\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start_key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.end_key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.start_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.end_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef cluster_vectorspace(self, vectors, trace=False):\n    if (self._means and (self._repeats > 1)):\n        print('Warning: means will be discarded for subsequent trials')\n    meanss = []\n    for trial in range(self._repeats):\n        if trace:\n            print('k-means trial', trial)\n        if ((not self._means) or (trial > 1)):\n            self._means = self._rng.sample(list(vectors), self._num_means)\n        self._cluster_vectorspace(vectors, trace)\n        meanss.append(self._means)\n    if (len(meanss) > 1):\n        for means in meanss:\n            means.sort(key=sum)\n        min_difference = min_means = None\n        for i in range(len(meanss)):\n            d = 0\n            for j in range(len(meanss)):\n                if (i != j):\n                    d += self._sum_distances(meanss[i], meanss[j])\n            if ((min_difference is None) or (d < min_difference)):\n                (min_difference, min_means) = (d, meanss[i])\n        self._means = min_means\n", "label": 1}
{"function": "\n\ndef _internal_parse(self, csource):\n    (ast, macros, csource) = self._parse(csource)\n    self._process_macros(macros)\n    iterator = iter(ast.ext)\n    for decl in iterator:\n        if (decl.name == '__dotdotdot__'):\n            break\n    try:\n        self._inside_extern_python = False\n        for decl in iterator:\n            if isinstance(decl, pycparser.c_ast.Decl):\n                self._parse_decl(decl)\n            elif isinstance(decl, pycparser.c_ast.Typedef):\n                if (not decl.name):\n                    raise api.CDefError('typedef does not declare any name', decl)\n                quals = 0\n                if (isinstance(decl.type.type, pycparser.c_ast.IdentifierType) and (decl.type.type.names[(- 1)] == '__dotdotdot__')):\n                    realtype = self._get_unknown_type(decl)\n                elif (isinstance(decl.type, pycparser.c_ast.PtrDecl) and isinstance(decl.type.type, pycparser.c_ast.TypeDecl) and isinstance(decl.type.type.type, pycparser.c_ast.IdentifierType) and (decl.type.type.type.names == ['__dotdotdot__'])):\n                    realtype = model.unknown_ptr_type(decl.name)\n                else:\n                    (realtype, quals) = self._get_type_and_quals(decl.type, name=decl.name)\n                self._declare(('typedef ' + decl.name), realtype, quals=quals)\n            else:\n                raise api.CDefError('unrecognized construct', decl)\n    except api.FFIError as e:\n        msg = self._convert_pycparser_error(e, csource)\n        if msg:\n            e.args = ((e.args[0] + ('\\n    *** Err: %s' % msg)),)\n        raise\n", "label": 1}
{"function": "\n\ndef get_params(self, event_collection=None, timeframe=None, timezone=None, interval=None, filters=None, group_by=None, target_property=None, latest=None, email=None, analyses=None, steps=None, property_names=None, percentile=None, max_age=None):\n    params = {\n        \n    }\n    if event_collection:\n        params['event_collection'] = event_collection\n    if timeframe:\n        if (type(timeframe) is dict):\n            params['timeframe'] = json.dumps(timeframe)\n        else:\n            params['timeframe'] = timeframe\n    if timezone:\n        params['timezone'] = timezone\n    if interval:\n        params['interval'] = interval\n    if filters:\n        params['filters'] = json.dumps(filters)\n    if group_by:\n        if (type(group_by) is list):\n            params['group_by'] = json.dumps(group_by)\n        else:\n            params['group_by'] = group_by\n    if target_property:\n        params['target_property'] = target_property\n    if latest:\n        params['latest'] = latest\n    if email:\n        params['email'] = email\n    if analyses:\n        params['analyses'] = json.dumps(analyses)\n    if steps:\n        params['steps'] = json.dumps(steps)\n    if property_names:\n        params['property_names'] = json.dumps(property_names)\n    if percentile:\n        params['percentile'] = percentile\n    if max_age:\n        params['max_age'] = max_age\n    return params\n", "label": 1}
{"function": "\n\ndef as_const(self, eval_ctx=None):\n    eval_ctx = get_eval_context(self, eval_ctx)\n    if (eval_ctx.volatile or (self.node is None)):\n        raise Impossible()\n    filter_ = self.environment.filters.get(self.name)\n    if ((filter_ is None) or getattr(filter_, 'contextfilter', False)):\n        raise Impossible()\n    obj = self.node.as_const(eval_ctx)\n    args = [x.as_const(eval_ctx) for x in self.args]\n    if getattr(filter_, 'evalcontextfilter', False):\n        args.insert(0, eval_ctx)\n    elif getattr(filter_, 'environmentfilter', False):\n        args.insert(0, self.environment)\n    kwargs = dict((x.as_const(eval_ctx) for x in self.kwargs))\n    if (self.dyn_args is not None):\n        try:\n            args.extend(self.dyn_args.as_const(eval_ctx))\n        except Exception:\n            raise Impossible()\n    if (self.dyn_kwargs is not None):\n        try:\n            kwargs.update(self.dyn_kwargs.as_const(eval_ctx))\n        except Exception:\n            raise Impossible()\n    try:\n        return filter_(obj, *args, **kwargs)\n    except Exception:\n        raise Impossible()\n", "label": 1}
{"function": "\n\ndef _eval_real_imag(self, real):\n    zero = one_neither = False\n    for t in self.args:\n        if (not t.is_complex):\n            return t.is_complex\n        elif t.is_imaginary:\n            real = (not real)\n        elif t.is_real:\n            if (not zero):\n                z = t.is_zero\n                if ((not z) and (zero is False)):\n                    zero = z\n                elif z:\n                    if all((a.is_finite for a in self.args)):\n                        return True\n                    return\n        elif (t.is_real is False):\n            if one_neither:\n                return\n            one_neither = True\n        else:\n            return\n    if one_neither:\n        if real:\n            return zero\n    elif (zero is False):\n        return real\n    elif real:\n        return real\n", "label": 1}
{"function": "\n\ndef setUp(self):\n    'Creating a new authenticated Bitbucket...'\n    try:\n        from settings import USERNAME, CONSUMER_KEY, CONSUMER_SECRET\n        PASSWORD = None\n    except ImportError:\n        try:\n            from settings import USERNAME, PASSWORD\n            CONSUMER_KEY = None\n            CONSUMER_SECRET = None\n        except ImportError:\n            raise ImportError('Please provide either USERNAME and PASSWORD or USERNAME, CONSUMER_KEY and CONSUMER_SECRET in bitbucket/tests/private/settings.py.')\n    if (USERNAME and PASSWORD):\n        self.bb = Bitbucket(USERNAME, PASSWORD)\n    elif (USERNAME and CONSUMER_KEY and CONSUMER_SECRET):\n        global OAUTH_ACCESS_TOKEN, OAUTH_ACCESS_TOKEN_SECRET\n        self.bb = Bitbucket(USERNAME)\n        if ((not OAUTH_ACCESS_TOKEN) and (not OAUTH_ACCESS_TOKEN_SECRET)):\n            self.bb.authorize(CONSUMER_KEY, CONSUMER_SECRET, 'http://localhost/')\n            webbrowser.open(self.bb.url('AUTHENTICATE', token=self.bb.access_token))\n            token_is_valid = False\n            while (not token_is_valid):\n                oauth_verifier = raw_input('Enter verifier from url [oauth_verifier]')\n                token_is_valid = bool((oauth_verifier and self.bb.verify(oauth_verifier)[0]))\n                if (not token_is_valid):\n                    print('Invalid oauth_verifier, please try again or quit with CONTROL-C.')\n            OAUTH_ACCESS_TOKEN = self.bb.access_token\n            OAUTH_ACCESS_TOKEN_SECRET = self.bb.access_token_secret\n        else:\n            self.bb.authorize(CONSUMER_KEY, CONSUMER_SECRET, 'http://localhost/', OAUTH_ACCESS_TOKEN, OAUTH_ACCESS_TOKEN_SECRET)\n    (success, result) = self.bb.repository.create(TEST_REPO_SLUG, has_issues=True)\n    assert success\n    self.bb.repo_slug = result['slug']\n", "label": 1}
{"function": "\n\ndef __process_escapes(self, s, literal_start):\n    'Convert backlash sequences in raw string literal.\\n\\n        @param s: The raw contents of a raw string literal.\\n\\n        @return: The string with all backlash sequences converted to their chars.'\n    if (s.find('\\\\') < 0):\n        return s\n    slen = len(s)\n    sb = []\n    i = 0\n    while (i < slen):\n        c = s[i]\n        if (c != '\\\\'):\n            sb.append(c)\n            i += 1\n            continue\n        if ((i + 1) >= slen):\n            self.__error('No character after escape', (literal_start + i))\n        i += 1\n        c = s[i]\n        if (c == 't'):\n            sb.append('\\t')\n        elif (c == 'n'):\n            sb.append('\\n')\n        elif (c == 'r'):\n            sb.append('\\r')\n        elif (c == 'b'):\n            sb.append('\\x08')\n        elif (c == 'f'):\n            sb.append('\\x0c')\n        elif (c == '\"'):\n            sb.append('\"')\n        elif (c == '\\\\'):\n            sb.append('\\\\')\n        elif (c == '/'):\n            sb.append('/')\n        elif ((c == 'u') and ((i + 5) <= slen)):\n            hex_string = s[(i + 1):(i + 5)]\n            i += 4\n            sb.append(unichr(int(hex_string, 16)))\n        else:\n            self.__error((('Unexpected backslash escape [' + c) + ']'), (literal_start + i))\n        i += 1\n    return ''.join(sb)\n", "label": 1}
{"function": "\n\ndef fireMouseEvent(listeners, sender, event):\n    x = (DOM.eventGetClientX(event) - DOM.getAbsoluteLeft(sender.getElement()))\n    y = (DOM.eventGetClientY(event) - DOM.getAbsoluteTop(sender.getElement()))\n    etype = DOM.eventGetType(event)\n    if (etype == 'mousedown'):\n        for listener in listeners:\n            listener.onMouseDown(sender, x, y)\n        return True\n    elif (etype == 'mouseup'):\n        for listener in listeners:\n            listener.onMouseUp(sender, x, y)\n        return True\n    elif (etype == 'mousemove'):\n        for listener in listeners:\n            listener.onMouseMove(sender, x, y)\n        return True\n    elif (etype == 'mouseover'):\n        to_element = DOM.eventGetToElement(event)\n        if (to_element and (not DOM.isOrHasChild(sender.getElement(), to_element))):\n            for listener in listeners:\n                listener.onMouseEnter(sender)\n        return True\n    elif (etype == 'mouseout'):\n        to_element = DOM.eventGetToElement(event)\n        if (to_element and (not DOM.isOrHasChild(sender.getElement(), to_element))):\n            for listener in listeners:\n                listener.onMouseLeave(sender)\n        return True\n    return False\n", "label": 1}
{"function": "\n\n@defun\ndef gammainc(ctx, z, a=0, b=None, regularized=False):\n    regularized = bool(regularized)\n    z = ctx.convert(z)\n    if (a is None):\n        a = ctx.zero\n        lower_modified = False\n    else:\n        a = ctx.convert(a)\n        lower_modified = (a != ctx.zero)\n    if (b is None):\n        b = ctx.inf\n        upper_modified = False\n    else:\n        b = ctx.convert(b)\n        upper_modified = (b != ctx.inf)\n    if (not (upper_modified or lower_modified)):\n        if regularized:\n            if (ctx.re(z) < 0):\n                return ctx.inf\n            elif (ctx.re(z) > 0):\n                return ctx.one\n            else:\n                return ctx.nan\n        return ctx.gamma(z)\n    if (a == b):\n        return ctx.zero\n    if (ctx.re(a) > ctx.re(b)):\n        return (- ctx.gammainc(z, b, a, regularized))\n    if (upper_modified and lower_modified):\n        return (+ ctx._gamma3(z, a, b, regularized))\n    elif lower_modified:\n        return ctx._upper_gamma(z, a, regularized)\n    elif upper_modified:\n        return ctx._lower_gamma(z, b, regularized)\n", "label": 1}
{"function": "\n\ndef _read_response(self, header, buffer, offset):\n    client = self.client\n    (request, async_object, xid) = client._pending.popleft()\n    if (header.zxid and (header.zxid > 0)):\n        client.last_zxid = header.zxid\n    if (header.xid != xid):\n        raise RuntimeError('xids do not match, expected %r received %r', xid, header.xid)\n    exists_error = ((header.err == NoNodeError.code) and (request.type == Exists.type))\n    if (header.err and (not exists_error)):\n        callback_exception = EXCEPTIONS[header.err]()\n        self.logger.debug('Received error(xid=%s) %r', xid, callback_exception)\n        if async_object:\n            async_object.set_exception(callback_exception)\n    elif (request and async_object):\n        if exists_error:\n            async_object.set(None)\n        else:\n            try:\n                response = request.deserialize(buffer, offset)\n            except Exception as exc:\n                self.logger.exception('Exception raised during deserialization of request: %s', request)\n                async_object.set_exception(exc)\n                return\n            self.logger.debug('Received response(xid=%s): %r', xid, response)\n            if (request.type == Transaction.type):\n                response = Transaction.unchroot(client, response)\n            async_object.set(response)\n        watcher = getattr(request, 'watcher', None)\n        if ((not client._stopped.is_set()) and watcher):\n            if isinstance(request, GetChildren):\n                client._child_watchers[request.path].add(watcher)\n            else:\n                client._data_watchers[request.path].add(watcher)\n    if isinstance(request, Close):\n        self.logger.log(BLATHER, 'Read close response')\n        return CLOSE_RESPONSE\n", "label": 1}
{"function": "\n\ndef larch_exec(self, text, debug=True):\n    'execute larch command'\n    self.debug = debug\n    if (not self.initialized):\n        self.initialize_larch()\n    text = text.strip()\n    if (text in ('quit', 'exit', 'EOF')):\n        self.exit()\n    else:\n        ret = None\n        self.input.put(text, lineno=0)\n        while (len(self.input) > 0):\n            (block, fname, lineno) = self.input.get()\n            if (len(block) == 0):\n                continue\n            if self.local_echo:\n                print(block)\n            ret = self.larch.eval(block, fname=fname, lineno=lineno)\n            if self.larch.error:\n                err = self.larch.error.pop(0)\n                (fname, lineno) = (err.fname, err.lineno)\n                self.write(('%s\\n' % err.get_error()[1]))\n                for err in self.larch.error:\n                    if (self.debug or (((err.fname != fname) or (err.lineno != lineno)) and (err.lineno > 0) and (lineno > 0))):\n                        self.write(('%s\\n' % err.get_error()[1]))\n                self.input.clear()\n                break\n            elif (ret is not None):\n                self.write(('%s\\n' % repr(ret)))\n        self.keep_alive_time = (time.time() + self.IDLE_TIME)\n    return (ret is None)\n", "label": 1}
{"function": "\n\ndef _event_text_symbol(self, ev):\n    text = None\n    symbol = xlib.KeySym()\n    buffer = create_string_buffer(128)\n    count = xlib.XLookupString(ev.xkey, buffer, (len(buffer) - 1), byref(symbol), None)\n    filtered = xlib.XFilterEvent(ev, ev.xany.window)\n    if ((ev.type == xlib.KeyPress) and (not filtered)):\n        status = c_int()\n        if _have_utf8:\n            encoding = 'utf8'\n            count = xlib.Xutf8LookupString(self._x_ic, ev.xkey, buffer, (len(buffer) - 1), byref(symbol), byref(status))\n            if (status.value == xlib.XBufferOverflow):\n                raise NotImplementedError('TODO: XIM buffer resize')\n        else:\n            encoding = 'ascii'\n            count = xlib.XLookupString(ev.xkey, buffer, (len(buffer) - 1), byref(symbol), None)\n            if count:\n                status.value = xlib.XLookupBoth\n        if (status.value & (xlib.XLookupChars | xlib.XLookupBoth)):\n            text = buffer.value[:count].decode(encoding)\n        if (text and (unicodedata.category(text) == 'Cc') and (text != '\\r')):\n            text = None\n    symbol = symbol.value\n    if ((ev.xkey.keycode == 0) and (not filtered)):\n        symbol = None\n    if (symbol and (symbol not in key._key_names) and ev.xkey.keycode):\n        symbol = ord(unichr(symbol).lower())\n        if (symbol not in key._key_names):\n            symbol = key.user_key(ev.xkey.keycode)\n    if filtered:\n        return (None, symbol)\n    return (text, symbol)\n", "label": 1}
{"function": "\n\ndef _parse_function_type(self, typenode, funcname=None):\n    params = list(getattr(typenode.args, 'params', []))\n    for (i, arg) in enumerate(params):\n        if (not hasattr(arg, 'type')):\n            raise api.CDefError((\"%s arg %d: unknown type '%s' (if you meant to use the old C syntax of giving untyped arguments, it is not supported)\" % ((funcname or 'in expression'), (i + 1), getattr(arg, 'name', '?'))))\n    ellipsis = ((len(params) > 0) and isinstance(params[(- 1)].type, pycparser.c_ast.TypeDecl) and isinstance(params[(- 1)].type.type, pycparser.c_ast.IdentifierType) and (params[(- 1)].type.type.names == ['__dotdotdot__']))\n    if ellipsis:\n        params.pop()\n        if (not params):\n            raise api.CDefError((\"%s: a function with only '(...)' as argument is not correct C\" % (funcname or 'in expression')))\n    args = [self._as_func_arg(*self._get_type_and_quals(argdeclnode.type)) for argdeclnode in params]\n    if ((not ellipsis) and (args == [model.void_type])):\n        args = []\n    (result, quals) = self._get_type_and_quals(typenode.type)\n    abi = None\n    if hasattr(typenode.type, 'quals'):\n        if (typenode.type.quals[(- 3):] == ['volatile', 'volatile', 'const']):\n            abi = '__stdcall'\n    return model.RawFunctionType(tuple(args), result, ellipsis, abi)\n", "label": 1}
{"function": "\n\ndef test_new_rawargs():\n    n = Symbol('n', commutative=False)\n    a = (x + n)\n    assert (a.is_commutative is False)\n    assert a._new_rawargs(x).is_commutative\n    assert a._new_rawargs(x, y).is_commutative\n    assert (a._new_rawargs(x, n).is_commutative is False)\n    assert (a._new_rawargs(x, y, n).is_commutative is False)\n    m = (x * n)\n    assert (m.is_commutative is False)\n    assert m._new_rawargs(x).is_commutative\n    assert (m._new_rawargs(n).is_commutative is False)\n    assert m._new_rawargs(x, y).is_commutative\n    assert (m._new_rawargs(x, n).is_commutative is False)\n    assert (m._new_rawargs(x, y, n).is_commutative is False)\n    assert (m._new_rawargs(x, n, reeval=False).is_commutative is False)\n    assert (m._new_rawargs(S.One) is S.One)\n", "label": 1}
{"function": "\n\ndef processConfigurationFile(self, data):\n    self.success = self.checkIsTool(data)\n    if self.success:\n        self.checkTopLevelInformation(data)\n    if self.success:\n        self.checkInputArguments(data['arguments'])\n    if self.success:\n        self.checkOutputArguments(data['arguments'])\n    if self.success:\n        self.checkRemainingArguments(data['arguments'])\n    if self.success:\n        self.success = self.checkWeb()\n    if self.success:\n        self.checkAttributeCombinations()\n    if self.success:\n        self.forceAttributes()\n    if self.success:\n        self.checkAttributeValues()\n    if self.success:\n        self.checkStreamInstructions()\n    if self.success:\n        self.checkConstructionInstructions()\n    if self.success:\n        self.checkReplaceSubstring(data['arguments'])\n    if self.success:\n        self.checkValueCommand()\n    if self.success:\n        self.success = self.checkArgumentOrder()\n    if self.success:\n        self.success = self.parameterSets.checkParameterSets(data['parameter sets'], self.allowTermination, self.name, isTool=True)\n", "label": 1}
{"function": "\n\ndef make_node(self, *inputs):\n    inputs = list(map(T.as_tensor_variable, inputs))\n    if ((not all(((a.type == inputs[0].type) for a in inputs))) or ((len(inputs) > 0) and (inputs[0].dtype != self.dtype))):\n        dtype = theano.scalar.upcast(self.dtype, *[i.dtype for i in inputs])\n        assert (dtype == self.dtype), 'The upcast of the inputs to MakeVector should match the dtype given in __init__.'\n        if (not all(((self.dtype == T.cast(i, dtype=dtype).dtype) for i in inputs))):\n            raise TypeError(('MakeVector.make_node expected inputs upcastable to %s. got %s' % (self.dtype, str([i.dtype for i in inputs]))))\n        inputs = [T.cast(i, dtype=dtype) for i in inputs]\n    assert all(((self.dtype == a.dtype) for a in inputs))\n    assert all(((a.ndim == 0) for a in inputs))\n    if inputs:\n        dtype = inputs[0].type.dtype\n    else:\n        dtype = self.dtype\n    bcastable = False\n    otype = T.TensorType(broadcastable=(bcastable,), dtype=dtype)\n    return T.Apply(self, inputs, [otype()])\n", "label": 1}
{"function": "\n\ndef make_rst(self):\n    (module, class_name) = self.arguments[0].rsplit('.', 1)\n    arguments = self.arguments[1:]\n    obj = import_object(module, class_name)\n    is_configurable = (':no-config:' not in arguments)\n    is_commandable = (':no-commands:' not in arguments)\n    arguments = [i for i in arguments if (i not in (':no-config:', ':no-commands:'))]\n    defaults = {\n        \n    }\n    for klass in reversed(obj.mro()):\n        if (not issubclass(klass, configurable.Configurable)):\n            continue\n        if (not hasattr(klass, 'defaults')):\n            continue\n        klass_defaults = getattr(klass, 'defaults')\n        defaults.update({d[0]: d[1:] for d in klass_defaults})\n    defaults = [(k, v[0], v[1]) for (k, v) in sorted(defaults.items())]\n    context = {\n        'module': module,\n        'class_name': class_name,\n        'class_underline': ('=' * len(class_name)),\n        'obj': obj,\n        'defaults': defaults,\n        'configurable': (is_configurable and issubclass(obj, configurable.Configurable)),\n        'commandable': (is_commandable and issubclass(obj, command.CommandObject)),\n        'is_widget': issubclass(obj, widget.base._Widget),\n        'extra_arguments': arguments,\n    }\n    if context['commandable']:\n        context['commands'] = [attr for attr in dir(obj) if attr.startswith('cmd_')]\n    rst = qtile_class_template.render(**context)\n    for line in rst.splitlines():\n        (yield line)\n", "label": 1}
{"function": "\n\ndef lookup_allowed(self, lookup, value):\n    from django.contrib.admin.filters import SimpleListFilter\n    model = self.model\n    for l in model._meta.related_fkey_lookups:\n        if callable(l):\n            l = l()\n        for (k, v) in widgets.url_params_from_lookup_dict(l).items():\n            if ((k == lookup) and (v == value)):\n                return True\n    relation_parts = []\n    prev_field = None\n    for part in lookup.split(LOOKUP_SEP):\n        try:\n            field = model._meta.get_field(part)\n        except FieldDoesNotExist:\n            break\n        if ((not prev_field) or (prev_field.concrete and (field not in prev_field.get_path_info()[(- 1)].target_fields))):\n            relation_parts.append(part)\n        if (not getattr(field, 'get_path_info', None)):\n            break\n        prev_field = field\n        model = field.get_path_info()[(- 1)].to_opts.model\n    if (len(relation_parts) <= 1):\n        return True\n    clean_lookup = LOOKUP_SEP.join(relation_parts)\n    valid_lookups = [self.date_hierarchy]\n    for filter_item in self.list_filter:\n        if (isinstance(filter_item, type) and issubclass(filter_item, SimpleListFilter)):\n            valid_lookups.append(filter_item.parameter_name)\n        elif isinstance(filter_item, (list, tuple)):\n            valid_lookups.append(filter_item[0])\n        else:\n            valid_lookups.append(filter_item)\n    return (clean_lookup in valid_lookups)\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@register_specialize\n@gof.local_optimizer([IncSubtensor])\ndef local_useless_inc_subtensor(node):\n    '\\n    Remove IncSubtensor, when we overwrite the full inputs with the\\n    new value.\\n\\n    '\n    if (not isinstance(node.op, IncSubtensor)):\n        return\n    if (node.op.set_instead_of_inc is False):\n        try:\n            c = get_scalar_constant_value(node.inputs[0])\n            if (c != 0):\n                return\n        except NotScalarConstantError:\n            return\n    if ((node.inputs[0].ndim != node.inputs[1].ndim) or (node.inputs[0].broadcastable != node.inputs[1].broadcastable)):\n        return\n    idx_cst = get_idx_list(node.inputs[1:], node.op.idx_list)\n    if all(((isinstance(e, slice) and (e.start is None) and (e.stop is None) and ((e.step is None) or (T.extract_constant(e.step) == (- 1)))) for e in idx_cst)):\n        if (not hasattr(node.fgraph, 'shape_feature')):\n            return\n        if (not node.fgraph.shape_feature.same_shape(node.inputs[0], node.inputs[1])):\n            return\n        if all(((e.step is None) for e in node.op.idx_list)):\n            return [node.inputs[1]]\n        ret = Subtensor(node.op.idx_list)(*node.inputs[1:])\n        copy_stack_trace(node.outputs, ret)\n        return [ret]\n", "label": 1}
{"function": "\n\ndef test_equal():\n    b = Symbol('b')\n    a = Symbol('a')\n    e1 = (a + b)\n    e2 = ((2 * a) * b)\n    e3 = ((a ** 3) * (b ** 2))\n    e4 = ((a * b) + (b * a))\n    assert (not (e1 == e2))\n    assert (not (e1 == e2))\n    assert (e1 != e2)\n    assert (e2 == e4)\n    assert (e2 != e3)\n    assert (not (e2 == e3))\n    x = Symbol('x')\n    e1 = exp((x + (1 / x)))\n    y = Symbol('x')\n    e2 = exp((y + (1 / y)))\n    assert (e1 == e2)\n    assert (not (e1 != e2))\n    y = Symbol('y')\n    e2 = exp((y + (1 / y)))\n    assert (not (e1 == e2))\n    assert (e1 != e2)\n    e5 = (((Rational(3) + (2 * x)) - x) - x)\n    assert (e5 == 3)\n    assert (3 == e5)\n    assert (e5 != 4)\n    assert (4 != e5)\n    assert (e5 != (3 + x))\n    assert ((3 + x) != e5)\n", "label": 1}
{"function": "\n\ndef test_has_iterative():\n    (A, B, C) = symbols('A,B,C', commutative=False)\n    f = (((((((x * gamma(x)) * sin(x)) * exp((x * y))) * A) * B) * C) * cos(((x * A) * B)))\n    assert f.has(x)\n    assert f.has((x * y))\n    assert f.has((x * sin(x)))\n    assert (not f.has((x * sin(y))))\n    assert f.has((x * A))\n    assert f.has(((x * A) * B))\n    assert (not f.has(((x * A) * C)))\n    assert f.has((((x * A) * B) * C))\n    assert (not f.has((((x * A) * C) * B)))\n    assert f.has(((((x * sin(x)) * A) * B) * C))\n    assert (not f.has(((((x * sin(x)) * A) * C) * B)))\n    assert (not f.has(((((x * sin(y)) * A) * B) * C)))\n    assert f.has((x * gamma(x)))\n    assert (not f.has((x + sin(x))))\n    assert ((x & y) & z).has((x & z))\n", "label": 1}
{"function": "\n\ndef list_sorted_nshash(self, h, ns, long_=False, columns=True, recurse=False, prnt=False, longer=False, longest=False):\n    if (type(h) in (int, long)):\n        if (h == fishlib.STATUS.UNAUTHORIZED):\n            return 'Permission denied.'\n        else:\n            return ('Error status %s' % h)\n    tagNames = h['tagNames']\n    tagNames.sort()\n    spaces = h['namespaceNames']\n    spaces.sort()\n    items = (tagNames[:] + [('%s/' % space) for space in spaces])\n    items.sort()\n    if items:\n        fmt = string_format(max([len(item) for item in items]))\n    if recurse:\n        self.Print(('\\n%s:' % ns))\n    if (long_ or longer or longest):\n        res = []\n        for item in items:\n            r = self.full_perms(((ns + '/') + (fmt % item)), longer, longest=longest)\n            res.append(r)\n            if prnt:\n                self.Print(r)\n        result = '\\n'.join(res)\n    elif (columns == False):\n        result = '\\n'.join(items)\n        if prnt:\n            self.Print(result)\n    else:\n        result = to_string_grid(items)\n        if prnt:\n            self.Print(result)\n    if recurse:\n        others = '\\n'.join([self.list_sorted_ns(('%s/%s' % (ns, space)), long_, columns, recurse, prnt=prnt, longer=longer, longest=longest) for space in spaces])\n        return ('%s:\\n%s\\n\\n%s' % (ns, result, others))\n    else:\n        return result\n", "label": 1}
{"function": "\n\n@classmethod\ndef set_field_value(cls, obj, field, value):\n    if isinstance(value, basestring):\n        value = value.strip()\n    try:\n        (field, subfield) = field.split(':')\n    except Exception:\n        pass\n    else:\n        field = field.strip()\n        if (field not in obj):\n            obj[field] = {\n                \n            }\n        cls.set_field_value(obj[field], subfield, value)\n        return\n    try:\n        (field, _) = field.split()\n    except Exception:\n        pass\n    else:\n        dud = {\n            \n        }\n        cls.set_field_value(dud, field, value)\n        ((field, value),) = dud.items()\n        if (field not in obj):\n            obj[field] = []\n        elif (not isinstance(obj[field], list)):\n            obj[field] = [obj[field]]\n        if (value not in (None, '')):\n            obj[field].append(value)\n        return\n    try:\n        (field, nothing) = field.split('?')\n        assert (nothing.strip() == '')\n    except Exception:\n        pass\n    else:\n        try:\n            value = {\n                'yes': True,\n                'true': True,\n                'no': False,\n                'false': False,\n                '': False,\n                None: False,\n            }[(value.lower() if hasattr(value, 'lower') else value)]\n        except KeyError:\n            raise JSONReaderError(('Values for field %s must be \"yes\" or \"no\", not \"%s\"' % (field, value)))\n    field = field.strip()\n    if (field in obj):\n        raise JSONReaderError(('You have a repeat field: %s' % field))\n    obj[field] = value\n", "label": 1}
{"function": "\n\ndef getFirstBracketError(self, view):\n    opener = list('({[')\n    closer = list(')}]')\n    matchingStack = []\n    successResult = BracketResult(True, (- 1), (- 1))\n    codeStr = view.substr(sublime.Region(0, view.size()))\n    for (index, char) in enumerate(codeStr):\n        if ((char not in opener) and (not (char in closer))):\n            continue\n        scopeName = view.scope_name(index)\n        hasScope = (lambda s: (s in scopeName))\n        markdownBracketScopeBegin = 'punctuation.definition.string.begin.markdown'\n        markdownBracketScopeEnd = 'punctuation.definition.string.end.markdown'\n        isMarkdownStringBeginOrEnd = (lambda s: (hasScope(markdownBracketScopeBegin) or hasScope(markdownBracketScopeEnd)))\n        if ((hasScope('string') and (not hasScope('unquoted')) and (not isMarkdownStringBeginOrEnd(markdownBracketScopeBegin))) or hasScope('comment')):\n            continue\n        if (char in opener):\n            matchingStack.append(BracketPosition(index, char))\n        elif (char in closer):\n            matchingOpener = opener[closer.index(char)]\n            if (len(matchingStack) == 0):\n                return BracketResult(False, (- 1), index)\n            poppedOpener = matchingStack.pop()\n            if (matchingOpener != poppedOpener.opener):\n                return BracketResult(False, poppedOpener.position, index)\n    if (len(matchingStack) == 0):\n        return successResult\n    else:\n        poppedOpener = matchingStack.pop()\n        return BracketResult(False, poppedOpener.position, (- 1))\n", "label": 1}
{"function": "\n\ndef translate(pat):\n    'Translate a shell PATTERN to a regular expression.\\n\\n    There is no way to quote meta-characters.\\n    '\n    (i, n) = (0, len(pat))\n    res = ''\n    while (i < n):\n        c = pat[i]\n        i = (i + 1)\n        if (c == '*'):\n            res = (res + '(.*)')\n        elif (c == '?'):\n            res = (res + '(.)')\n        elif (c == '['):\n            j = i\n            if ((j < n) and (pat[j] == '!')):\n                j = (j + 1)\n            if ((j < n) and (pat[j] == ']')):\n                j = (j + 1)\n            while ((j < n) and (pat[j] != ']')):\n                j = (j + 1)\n            if (j >= n):\n                res = (res + '\\\\[')\n            else:\n                stuff = pat[i:j].replace('\\\\', '\\\\\\\\')\n                i = (j + 1)\n                if (stuff[0] == '!'):\n                    stuff = ('^' + stuff[1:])\n                elif (stuff[0] == '^'):\n                    stuff = ('\\\\' + stuff)\n                res = ('%s([%s])' % (res, stuff))\n        else:\n            res = (res + re.escape(c))\n    return (res + '\\\\Z(?ms)')\n", "label": 1}
{"function": "\n\ndef GetItem(self, user, route, has_perm=False, need_perm=False):\n    self.CheckUpdate()\n    if (self.mtitle.find('(BM:') != (- 1)):\n        if (Board.Board.IsBM(user, self.mtitle[4:]) or user.IsSysop()):\n            has_perm = True\n        elif (need_perm and (not has_perm)):\n            return None\n    if ((self.mtitle.find('(BM: BMS)') != (- 1)) or (self.mtitle.find('(BM: SECRET)') != (- 1)) or (self.mtitle.find('(BM: SYSOPS)') != (- 1))):\n        need_perm = True\n    if (len(route) == 0):\n        return self\n    target = (route[0] - 1)\n    _id = target\n    if (_id >= len(self.items)):\n        return None\n    while (self.items[_id].EffectiveId(user) < target):\n        _id += 1\n        if (_id >= len(self.items)):\n            return None\n    item = self.items[_id]\n    item.mtitle = item.title\n    if (len(route) == 1):\n        return item\n    elif item.IsDir():\n        if (not item.CheckUpdate()):\n            return None\n        return item.GetItem(user, route[1:], has_perm, need_perm)\n    else:\n        return None\n", "label": 1}
{"function": "\n\n@newick.sniffer()\ndef _newick_sniffer(fh):\n    operators = set(',;:()')\n    empty = True\n    last_token = ','\n    indent = 0\n    try:\n        for (token, _) in zip(_tokenize_newick(fh), range(100)):\n            if (token not in operators):\n                pass\n            elif ((token == ',') and (last_token != ':') and (indent > 0)):\n                pass\n            elif ((token == ':') and (last_token != ':')):\n                pass\n            elif ((token == ';') and (last_token != ':') and (indent == 0)):\n                pass\n            elif ((token == ')') and (last_token != ':')):\n                indent -= 1\n            elif ((token == '(') and ((last_token == '(') or (last_token == ','))):\n                indent += 1\n            else:\n                raise NewickFormatError()\n            last_token = token\n            empty = False\n    except NewickFormatError:\n        return (False, {\n            \n        })\n    return ((not empty), {\n        \n    })\n", "label": 1}
{"function": "\n\ndef decompile_charstrings(td, fds):\n    for cs in td.GlobalSubrs:\n        cs.subr_cost = cs.subr_saving = len(cs.bytecode)\n    for fd in fds:\n        try:\n            for cs in fd.Private.Subrs:\n                cs.subr_cost = cs.subr_saving = len(cs.bytecode)\n        except AttributeError:\n            pass\n    for g in td.charset:\n        (cs, sel) = td.CharStrings.getItemAndSelector(g)\n        cs.decompile()\n    for cs in td.GlobalSubrs:\n        if (cs.program and (cs.program[(- 1)] == 'return')):\n            cs.subr_saving -= 1\n    for fd in fds:\n        try:\n            for cs in fd.Private.Subrs:\n                if (cs.program and (cs.program[(- 1)] == 'return')):\n                    cs.subr_saving -= 1\n        except AttributeError:\n            pass\n", "label": 1}
{"function": "\n\ndef _get_show_with_qualifiers(show_name, qualifiers):\n    shows = get_show_list(show_name)\n    best_match = (- 1)\n    show_match = None\n    for show in shows:\n        if show.premiered:\n            premiered = show.premiered[:(- 6)].lower()\n        else:\n            premiered = None\n        if (show.network and show.network.get('name')):\n            network = show.network['name'].lower()\n        else:\n            network = None\n        if (show.web_channel and show.web_channel.get('name')):\n            web_channel = show.web_channel['name'].lower()\n        else:\n            web_channel = None\n        if (show.network and show.network.get('country') and show.network['country'].get('code')):\n            country = show.network['country']['code'].lower()\n        elif (show.web_channel and show.web_channel.get('country') and show.web_channel['country'].get('code')):\n            country = show.web_channel['country']['code'].lower()\n        else:\n            country = None\n        if show.language:\n            language = show.language.lower()\n        else:\n            language = None\n        attributes = [premiered, country, network, language, web_channel]\n        show_score = len((set(qualifiers) & set(attributes)))\n        if (show_score > best_match):\n            best_match = show_score\n            show_match = show\n    return show_match\n", "label": 1}
{"function": "\n\ndef test_update_crossing_total_duration_at_1st_step(self, duration1, duration2):\n    global rec\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    elapsed = 0.0\n    rec = []\n    composite = ac.sequence(a1, a2)\n    node.do(composite)\n    next_elapsed = ((duration1 + duration2) + fe)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    recx = [e for e in rec if (e[1] != 'step')]\n    rec = [e for e in recx if (e[0] == name1)]\n    print('rec', rec)\n    assert (len(rec) == 3)\n    assert (rec[0][1] == 'start')\n    assert ((rec[1][1] == 'update') and (rec[1][2] == 1.0))\n    assert (rec[2][1] == 'stop')\n    rec = [e for e in recx if (e[0] == name2)]\n    print('rec', rec)\n    assert (len(rec) == 3)\n    assert (rec[0][1] == 'start')\n    assert ((rec[1][1] == 'update') and (rec[1][2] == 1.0))\n    assert (rec[2][1] == 'stop')\n", "label": 1}
{"function": "\n\ndef check_record(d):\n    'check for mandatory, select options, dates. these should ideally be in doclist'\n    from frappe.utils.dateutils import parse_date\n    doc = frappe.get_doc(d)\n    for key in d:\n        docfield = doc.meta.get_field(key)\n        val = d[key]\n        if docfield:\n            if (docfield.reqd and ((val == '') or (val == None))):\n                frappe.msgprint(_('{0} is required').format(docfield.label), raise_exception=1)\n            if ((docfield.fieldtype == 'Select') and val and docfield.options):\n                if (val not in docfield.options.split('\\n')):\n                    frappe.throw(_('{0} must be one of {1}').format(_(docfield.label), comma_or(docfield.options.split('\\n'))))\n            if (val and (docfield.fieldtype == 'Date')):\n                d[key] = parse_date(val)\n            elif (val and (docfield.fieldtype in ['Int', 'Check'])):\n                d[key] = cint(val)\n            elif (val and (docfield.fieldtype in ['Currency', 'Float', 'Percent'])):\n                d[key] = flt(val)\n", "label": 1}
{"function": "\n\ndef should_bypass_proxies(url):\n    '\\n    Returns whether we should bypass proxies or not.\\n    '\n    get_proxy = (lambda k: (os.environ.get(k) or os.environ.get(k.upper())))\n    no_proxy = get_proxy('no_proxy')\n    netloc = urlparse(url).netloc\n    if no_proxy:\n        no_proxy = (host for host in no_proxy.replace(' ', '').split(',') if host)\n        ip = netloc.split(':')[0]\n        if is_ipv4_address(ip):\n            for proxy_ip in no_proxy:\n                if is_valid_cidr(proxy_ip):\n                    if address_in_network(ip, proxy_ip):\n                        return True\n        else:\n            for host in no_proxy:\n                if (netloc.endswith(host) or netloc.split(':')[0].endswith(host)):\n                    return True\n    try:\n        bypass = proxy_bypass(netloc)\n    except (TypeError, socket.gaierror):\n        bypass = False\n    if bypass:\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef skel_setup(environment, inventory):\n    'Build out the main inventory skeleton as needed.\\n\\n    :param environment: ``dict`` Known environment information\\n    :param inventory: ``dict``  Living dictionary of inventory\\n    '\n    for (key, value) in environment.iteritems():\n        if (key == 'version'):\n            continue\n        for (_key, _value) in value.iteritems():\n            if (_key not in inventory):\n                inventory[_key] = {\n                    \n                }\n                if _key.endswith('container'):\n                    if ('hosts' not in inventory[_key]):\n                        inventory[_key]['hosts'] = []\n                else:\n                    if ('children' not in inventory[_key]):\n                        inventory[_key]['children'] = []\n                    if ('hosts' not in inventory[_key]):\n                        inventory[_key]['hosts'] = []\n            if ('belongs_to' in _value):\n                for assignment in _value['belongs_to']:\n                    if (assignment not in inventory):\n                        inventory[assignment] = {\n                            \n                        }\n                        if ('children' not in inventory[assignment]):\n                            inventory[assignment]['children'] = []\n                        if ('hosts' not in inventory[assignment]):\n                            inventory[assignment]['hosts'] = []\n", "label": 1}
{"function": "\n\ndef validate_ml_unique(self):\n    form_errors = []\n    if (not hasattr(self.instance._meta, 'translation_model')):\n        return\n    for check in self.instance._meta.translation_model._meta.unique_together[:]:\n        lookup_kwargs = {\n            'language_code': GLL.language_code,\n        }\n        for field_name in check:\n            if (self.cleaned_data.get(field_name) is not None):\n                lookup_kwargs[field_name] = self.cleaned_data.get(field_name)\n        if ((len(check) == 2) and ('master' in check) and ('language_code' in check)):\n            continue\n        qs = self.instance._meta.translation_model.objects.filter(**lookup_kwargs)\n        if (self.instance.pk is not None):\n            qs = qs.exclude(master=self.instance.pk)\n        if qs.count():\n            model_name = capfirst(self.instance._meta.verbose_name)\n            field_labels = []\n            for field_name in check:\n                if (field_name == 'language_code'):\n                    field_labels.append(_('language'))\n                elif (field_name == 'master'):\n                    continue\n                else:\n                    field_labels.append(self.instance._meta.translation_model._meta.get_field_by_name(field_name)[0].verbose_name)\n            field_labels = get_text_list(field_labels, _('and'))\n            form_errors.append((_('%(model_name)s with this %(field_label)s already exists.') % {\n                'model_name': unicode(model_name),\n                'field_label': unicode(field_labels),\n            }))\n    if form_errors:\n        raise ValidationError(form_errors)\n", "label": 1}
{"function": "\n\ndef selector(self, output):\n    if isinstance(output, boto.ec2.instance.Reservation):\n        return self.parseReservation(output)\n    elif isinstance(output, boto.ec2.instance.Instance):\n        return self.parseInstance(output)\n    elif isinstance(output, boto.ec2.volume.Volume):\n        return self.parseVolume(output)\n    elif isinstance(output, boto.ec2.blockdevicemapping.BlockDeviceType):\n        return self.parseBlockDeviceType(output)\n    elif isinstance(output, boto.ec2.zone.Zone):\n        return self.parseEC2Zone(output)\n    elif isinstance(output, boto.ec2.address.Address):\n        return self.parseAddress(output)\n    elif isinstance(output, boto.route53.record.Record):\n        return self.parseRecord(output)\n    elif isinstance(output, boto.route53.zone.Zone):\n        return self.parseR53Zone(output)\n    elif isinstance(output, boto.route53.status.Status):\n        return self.parseR53Status(output)\n    elif isinstance(output, boto.ec2.tag.Tag):\n        return self.parseTag(output)\n    elif isinstance(output, boto.ec2.ec2object.EC2Object):\n        return self.parseEC2Object(output)\n    elif isinstance(output, boto.cloudformation.stack.Stack):\n        return self.parseStackObject(output)\n    elif isinstance(output, boto.rds.dbinstance.DBInstance):\n        return self.parseDBInstanceObject(output)\n    else:\n        return output\n", "label": 1}
{"function": "\n\ndef get_changes(self):\n    model_defs = freeze_apps([self.migrations.app_label()])\n    for model in models.get_models(models.get_app(self.migrations.app_label())):\n        if (model._meta.abstract or getattr(model._meta, 'proxy', False) or (not getattr(model._meta, 'managed', True))):\n            continue\n        (real_fields, meta, m2m_fields) = self.split_model_def(model, model_defs[model_key(model)])\n        (yield ('AddModel', {\n            'model': model,\n            'model_def': real_fields,\n        }))\n        if meta:\n            for (attr, operation) in (('unique_together', 'AddUnique'), ('index_together', 'AddIndex')):\n                together = eval(meta.get(attr, '[]'))\n                if together:\n                    if isinstance(together[0], string_types):\n                        together = [together]\n                    for fields in together:\n                        (yield (operation, {\n                            'model': model,\n                            'fields': [model._meta.get_field_by_name(x)[0] for x in fields],\n                        }))\n        for (name, triple) in m2m_fields.items():\n            field = model._meta.get_field_by_name(name)[0]\n            if field.rel.through:\n                try:\n                    through_model = field.rel.through_model\n                except AttributeError:\n                    through_model = field.rel.through\n            if ((not field.rel.through) or getattr(through_model._meta, 'auto_created', False)):\n                (yield ('AddM2M', {\n                    'model': model,\n                    'field': field,\n                }))\n", "label": 1}
{"function": "\n\ndef line_starts_with_open_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)/\\\\*', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef set_all_hosts_roles(self, cluster_id):\n    for host_str in CONF.host_roles.split(';'):\n        host_str = host_str.strip()\n        (hostname, roles_str) = host_str.split('=', 1)\n        assert (hostname in self.host_mapping)\n        host_id = self.host_mapping[hostname]\n        roles = [role.strip() for role in roles_str.split(',') if role]\n        self.set_host_roles(cluster_id, host_id, roles)\n        self.host_roles[hostname] = roles\n    unassigned_hostnames = list((set(self.host_mapping.keys()) - set(self.host_roles.keys())))\n    unassigned_roles = [role for (role, status) in self.role_mapping.items() if is_role_unassigned(status)]\n    assert (len(unassigned_hostnames) >= len(unassigned_roles))\n    for (hostname, role) in map(None, unassigned_hostnames, unassigned_roles):\n        host_id = self.host_mapping[hostname]\n        self.set_host_roles(cluster_id, host_id, [role])\n        self.host_roles[hostname] = [role]\n    unassigned_hostnames = list((set(self.host_mapping.keys()) - set(self.host_roles.keys())))\n    if (not unassigned_hostnames):\n        return\n    default_roles = [role for role in CONF.default_roles.split(',') if role]\n    assert default_roles\n    cycle_roles = itertools.cycle(default_roles)\n    for hostname in unassigned_hostnames:\n        host_id = self.host_mapping[hostname]\n        roles = [cycle_roles.next()]\n        self.set_host_roles(cluster_id, host_id, roles)\n        self.host_roles[hostname] = roles\n", "label": 1}
{"function": "\n\n@gof.local_optimizer([T.Elemwise])\ndef local_fill_sink(node):\n    '\\n    f(fill(a, b), fill(c, d), e) -> fill(a, fill(c, f(b, d, e)))\\n\\n    f need to be an elemwise\\n    '\n    if ((not hasattr(node, 'op')) or (not isinstance(node.op, T.Elemwise)) or (node.op == T.fill)):\n        return False\n    models = []\n    inputs = []\n    for input in node.inputs:\n        if (input.owner and (input.owner.op == T.fill)):\n            models.append(input.owner.inputs[0])\n            inputs.append(input.owner.inputs[1])\n        else:\n            inputs.append(input)\n    if (not models):\n        return False\n    c = node.op(*inputs)\n    for model in models:\n        if (model.type != c.type):\n            c = T.fill(model, c)\n    replacements = {\n        node.outputs[0]: c,\n    }\n    for (client, cl_idx) in node.outputs[0].clients:\n        if (hasattr(client, 'op') and isinstance(client.op, T.Elemwise) and (not (client.op == T.fill))):\n            client_inputs = client.inputs[:]\n            client_inputs[cl_idx] = c\n            new_client = client.op(*client_inputs)\n            new_client.owner.outputs[0].clients = client.outputs[0].clients\n            r = local_fill_sink.transform(new_client.owner)\n            if (not r):\n                continue\n            replacements.update(r)\n    return replacements\n", "label": 1}
{"function": "\n\ndef _set_knspace(self, value):\n    if (value is self._knspace):\n        return\n    knspace = (self._knspace or self.__last_knspace)\n    name = self.knsname\n    if (name and knspace and (getattr(knspace, name) == self)):\n        setattr(knspace, name, None)\n    if (value == 'fork'):\n        if (not knspace):\n            knspace = self.knspace\n        if knspace:\n            value = knspace.fork()\n        else:\n            raise ValueError('Cannot fork with no namesapce')\n    for (obj, prop_name, uid) in (self.__callbacks or []):\n        obj.unbind_uid(prop_name, uid)\n    self.__last_knspace = self.__callbacks = None\n    if name:\n        if (value is None):\n            knspace = self.__set_parent_knspace()\n            if knspace:\n                setattr(knspace, name, self)\n            self._knspace = None\n        else:\n            setattr(value, name, self)\n            knspace = self._knspace = value\n        if (not knspace):\n            raise ValueError('Object has name \"{}\", but no namespace'.format(name))\n    else:\n        if (value is None):\n            self.__set_parent_knspace()\n        self._knspace = value\n", "label": 1}
{"function": "\n\ndef _guess_genre_and_host_from_aliases(self):\n    \"Uses available aliases to decide the item's genre\"\n    genre = 'unknown'\n    host = 'unknown'\n    if hasattr(self, 'doi'):\n        joined_doi_string = ''.join(self.doi).lower()\n        if ('10.5061/dryad.' in joined_doi_string):\n            genre = 'dataset'\n            host = 'dryad'\n        elif ('.figshare.' in joined_doi_string):\n            host = 'figshare'\n            genre = 'dataset'\n        else:\n            genre = 'article'\n    elif hasattr(self, 'pmid'):\n        genre = 'article'\n    elif hasattr(self, 'arxiv'):\n        genre = 'article'\n        host = 'arxiv'\n    elif hasattr(self, 'blog'):\n        genre = 'blog'\n        host = 'wordpresscom'\n    elif hasattr(self, 'blog_post'):\n        genre = 'blog'\n        host = 'blog_post'\n    elif hasattr(self, 'url'):\n        joined_url_string = ''.join(self.url).lower()\n        if ('slideshare.net' in joined_url_string):\n            genre = 'slides'\n            host = 'slideshare'\n        elif ('github.com' in joined_url_string):\n            genre = 'software'\n            host = 'github'\n        elif (('youtube.com' in joined_url_string) or ('youtu.be' in joined_url_string)):\n            genre = 'video'\n            host = 'youtube'\n        elif ('vimeo.com' in joined_url_string):\n            genre = 'video'\n            host = 'vimeo'\n        else:\n            genre = 'webpage'\n    return (genre, host)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.query = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.configuration = []\n                (_etype3, _size0) = iprot.readListBegin()\n                for _i4 in xrange(_size0):\n                    _elem5 = iprot.readString()\n                    self.configuration.append(_elem5)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.hadoop_user = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef test_get_case_ids_in_domain_by_owner(self):\n    self.assertEqual(set(get_case_ids_in_domain_by_owner(self.domain, owner_id='XXX')), {case.get_id for case in self.cases if ((case.domain == self.domain) and (case.user_id == 'XXX'))})\n    self.assertEqual(set(get_case_ids_in_domain_by_owner(self.domain, owner_id__in=['XXX'])), {case.get_id for case in self.cases if ((case.domain == self.domain) and (case.user_id == 'XXX'))})\n    self.assertEqual(set(get_case_ids_in_domain_by_owner(self.domain, owner_id='XXX', closed=False)), {case.get_id for case in self.cases if ((case.domain == self.domain) and (case.user_id == 'XXX') and (case.closed is False))})\n    self.assertEqual(set(get_case_ids_in_domain_by_owner(self.domain, owner_id='XXX', closed=True)), {case.get_id for case in self.cases if ((case.domain == self.domain) and (case.user_id == 'XXX') and (case.closed is True))})\n", "label": 1}
{"function": "\n\ndef _transform(self, x, imgsz, roisz, offset, intype, output):\n    xtemp = []\n    if (intype == 'SRC'):\n        xtemp = [(xt / float(imgsz)) for xt in x]\n    elif (intype == 'ROI'):\n        xtemp = [((xt + offset) / float(imgsz)) for xt in x]\n    elif (intype == 'ROI_UNIT'):\n        xtemp = [(((xt * roisz) + offset) / float(imgsz)) for xt in x]\n    elif (intype == 'SRC_UNIT'):\n        xtemp = x\n    else:\n        logger.warning('Bad Parameter to CoordTransformX')\n        return None\n    retVal = []\n    if (output == 'SRC'):\n        retVal = [int((xt * imgsz)) for xt in xtemp]\n    elif (output == 'ROI'):\n        retVal = [int(((xt * imgsz) - offset)) for xt in xtemp]\n    elif (output == 'ROI_UNIT'):\n        retVal = [int((((xt * imgsz) - offset) / float(roisz))) for xt in xtemp]\n    elif (output == 'SRC_UNIT'):\n        retVal = xtemp\n    else:\n        logger.warning('Bad Parameter to CoordTransformX')\n        return None\n    return retVal\n", "label": 1}
{"function": "\n\ndef get_homogenous_batch_iter(self):\n    end_of_iter = False\n    while True:\n        k_batches = 10\n        batch_size = self.batch_size\n        x = []\n        y = []\n        for k in xrange(k_batches):\n            try:\n                (dx, dy) = PytablesBitextIterator.next(self)\n            except StopIteration:\n                end_of_iter = True\n                break\n            if ((dx == None) or (dy == None)):\n                break\n            x += dx\n            y += dy\n        if ((len(x) <= 0) or (len(y) <= 0)):\n            raise StopIteration\n        lens = numpy.asarray([map(len, x), map(len, y)])\n        order = (numpy.argsort(lens.max(axis=0)) if (k_batches > 1) else numpy.arange(len(x)))\n        for k in range(k_batches):\n            if ((k * batch_size) > len(order)):\n                break\n            indices = order[(k * batch_size):((k + 1) * batch_size)]\n            (yield [[x[ii] for ii in indices], [y[ii] for ii in indices]])\n        if end_of_iter:\n            raise StopIteration\n", "label": 1}
{"function": "\n\ndef __init__(self, template):\n    self.template = template\n    self.parts = []\n    parts = re.split('(\\\\{[^\\\\}]*\\\\})', self.template)\n    for part in parts:\n        if part:\n            if (('{' == part[0]) and ('}' == part[(- 1)])):\n                expression = part[1:(- 1)]\n                if re.match('^([a-zA-Z0-9_]|%[0-9a-fA-F][0-9a-fA-F]).*$', expression):\n                    self.parts.append(SimpleExpansion(expression))\n                elif ('+' == part[1]):\n                    self.parts.append(ReservedExpansion(expression))\n                elif ('#' == part[1]):\n                    self.parts.append(FragmentExpansion(expression))\n                elif ('.' == part[1]):\n                    self.parts.append(LabelExpansion(expression))\n                elif ('/' == part[1]):\n                    self.parts.append(PathExpansion(expression))\n                elif (';' == part[1]):\n                    self.parts.append(PathStyleExpansion(expression))\n                elif ('?' == part[1]):\n                    self.parts.append(FormStyleQueryExpansion(expression))\n                elif ('&' == part[1]):\n                    self.parts.append(FormStyleQueryContinuation(expression))\n                elif (part[1] in '=,!@|'):\n                    raise UnsupportedExpression(part)\n                else:\n                    raise BadExpression(part)\n            elif (('{' not in part) and ('}' not in part)):\n                self.parts.append(Literal(part))\n            else:\n                raise BadExpression(part)\n", "label": 1}
{"function": "\n\ndef _validate(self, args, kwargs):\n    'Validate option registration arguments.'\n\n    def error(exception_type, arg_name=None, **msg_kwargs):\n        if (arg_name is None):\n            arg_name = (args[0] if args else '<unknown>')\n        raise exception_type(self.scope, arg_name, **msg_kwargs)\n    if (not args):\n        error(NoOptionNames)\n    for arg in args:\n        if (not arg.startswith('-')):\n            error(OptionNameDash, arg_name=arg)\n        if ((not arg.startswith('--')) and (len(arg) > 2)):\n            error(OptionNameDoubleDash, arg_name=arg)\n    if (('implicit_value' in kwargs) and (kwargs['implicit_value'] is None)):\n        error(ImplicitValIsNone)\n    if (('member_type' in kwargs) and (kwargs.get('type', str) not in [list, list_option])):\n        error(MemberTypeNotAllowed, type_=kwargs.get('type', str).__name__)\n    if (kwargs.get('member_type', str) not in self._allowed_member_types):\n        error(InvalidMemberType, member_type=kwargs.get('member_type', str).__name__)\n    for kwarg in kwargs:\n        if (kwarg not in self._allowed_registration_kwargs):\n            error(InvalidKwarg, kwarg=kwarg)\n    removal_version = kwargs.get('removal_version')\n    if (removal_version is not None):\n        validate_removal_semver(removal_version)\n", "label": 1}
{"function": "\n\ndef convert(self, element, base=None):\n    'Convert ``element`` to ``self.dtype``. '\n    if (base is not None):\n        return self.convert_from(element, base)\n    if self.of_type(element):\n        return element\n    from sympy.polys.domains import PythonIntegerRing, GMPYIntegerRing, GMPYRationalField, RealField, ComplexField\n    if isinstance(element, integer_types):\n        return self.convert_from(element, PythonIntegerRing())\n    if HAS_GMPY:\n        integers = GMPYIntegerRing()\n        if isinstance(element, integers.tp):\n            return self.convert_from(element, integers)\n        rationals = GMPYRationalField()\n        if isinstance(element, rationals.tp):\n            return self.convert_from(element, rationals)\n    if isinstance(element, float):\n        parent = RealField(tol=False)\n        return self.convert_from(parent(element), parent)\n    if isinstance(element, complex):\n        parent = ComplexField(tol=False)\n        return self.convert_from(parent(element), parent)\n    if isinstance(element, DomainElement):\n        return self.convert_from(element, element.parent())\n    if (self.is_Numerical and getattr(element, 'is_ground', False)):\n        return self.convert(element.LC())\n    if isinstance(element, Basic):\n        try:\n            return self.from_sympy(element)\n        except (TypeError, ValueError):\n            pass\n    elif (not is_sequence(element)):\n        try:\n            element = sympify(element)\n            if isinstance(element, Basic):\n                return self.from_sympy(element)\n        except (TypeError, ValueError):\n            pass\n    raise CoercionFailed((\"can't convert %s of type %s to %s\" % (element, type(element), self)))\n", "label": 1}
{"function": "\n\ndef send(self, message, _sender=None):\n    'Sends a message to the actor represented by this `Ref`.'\n    if (not _sender):\n        context = get_context()\n        if context:\n            _sender = context.ref\n    if self._cell:\n        if (not self._cell.stopped):\n            self._cell.receive(message, _sender)\n            return\n        else:\n            self._cell = None\n    if (not self.is_local):\n        if (self.uri.node != self.node.nid):\n            self.node.send_message(message, remote_ref=self, sender=_sender)\n        else:\n            self._cell = self.node.guardian.lookup_cell(self.uri)\n            self.is_local = True\n            self._cell.receive(message, _sender)\n    else:\n        if (self.node and self.node.guardian):\n            cell = self.node.guardian.lookup_cell(self.uri)\n            if cell:\n                cell.receive(message, _sender)\n                return\n        if (('_watched', ANY) == message):\n            message[1].send(('terminated', self), _sender=self)\n        elif ((message == ('terminated', ANY)) or (message == ('_unwatched', ANY)) or (message == ('_node_down', ANY)) or (message == '_stop') or (message == '_kill') or (message == '__done')):\n            pass\n        else:\n            Events.log(DeadLetter(self, message, _sender))\n", "label": 1}
{"function": "\n\ndef get_pdf_url(self):\n    if self.checked_pdf_url:\n        return self.pdf_url\n    url = None\n    if self.aliases.display_pmc:\n        url = 'http://ukpmc.ac.uk/articles/{pmcid}?pdf=render'.format(pmcid=self.aliases.pmc[0])\n    elif self.aliases.display_arxiv:\n        url = 'http://arxiv.org/pdf/{arxiv_id}.pdf'.format(arxiv_id=self.aliases.display_arxiv)\n    elif (hasattr(self.biblio, 'free_fulltext_url') and self.biblio.free_fulltext_url):\n        if ('pdf' in self.biblio.free_fulltext_url):\n            url = self.biblio.free_fulltext_url\n        elif (self.aliases.resolved_url and ('sagepub.com/' in self.aliases.resolved_url)):\n            url = (self.aliases.resolved_url + '.full.pdf')\n        if (not url):\n            url = embed_markup.extract_pdf_link_from_html(self.biblio.free_fulltext_url)\n    if (not url):\n        if (self.aliases.resolved_url and ('pdf' in self.aliases.resolved_url)):\n            url = self.aliases.resolved_url\n    if (url and ('.pdf+html' in url)):\n        url = url.replace('.pdf+html', '.pdf')\n    if (url and ('jstor.org/' in url)):\n        url = None\n    self.checked_pdf_url = True\n    self.pdf_url = url\n    return url\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.messageBox = MessageBox()\n                self.messageBox.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.displayName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.contacts = []\n                (_etype49, _size46) = iprot.readListBegin()\n                for _i50 in xrange(_size46):\n                    _elem51 = Contact()\n                    _elem51.read(iprot)\n                    self.contacts.append(_elem51)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.mystery = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.statusCode = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.path = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.listing = []\n                (_etype38, _size35) = iprot.readListBegin()\n                for _i39 in xrange(_size35):\n                    _elem40 = iprot.readString()\n                    self.listing.append(_elem40)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef test_sort_variants():\n    '\\n    Test to sort an unsorted file with variants\\n    '\n    csv_file = setup_csv_file()\n    sort_variants(infile=csv_file, mode='chromosome')\n    variants = []\n    with open(csv_file, 'r') as f:\n        for line in f:\n            variants.append(line.rstrip().split('\\t'))\n    assert (variants[0][1] == '1')\n    assert (variants[0][2] == '11900')\n    assert (variants[1][1] == '1')\n    assert (variants[1][2] == '879585')\n    assert (variants[2][1] == '1')\n    assert (variants[2][2] == '879586')\n    assert (variants[3][1] == '1')\n    assert (variants[3][2] == '947378')\n    assert (variants[4][1] == '3')\n    assert (variants[4][2] == '947378')\n    assert (variants[5][1] == 'X')\n    assert (variants[5][2] == '879586')\n", "label": 1}
{"function": "\n\ndef command(self, mark_success=False, ignore_dependencies=False, ignore_depends_on_past=False, force=False, local=False, pickle_id=None, raw=False, job_id=None, pool=None):\n    '\\n        Returns a command that can be executed anywhere where airflow is\\n        installed. This command is part of the message sent to executors by\\n        the orchestrator.\\n        '\n    dag = self.task.dag\n    iso = self.execution_date.isoformat()\n    cmd = 'airflow run {self.dag_id} {self.task_id} {iso} '\n    cmd += ('--mark_success ' if mark_success else '')\n    cmd += ('--pickle {pickle_id} ' if pickle_id else '')\n    cmd += ('--job_id {job_id} ' if job_id else '')\n    cmd += ('-i ' if ignore_dependencies else '')\n    cmd += ('-I ' if ignore_depends_on_past else '')\n    cmd += ('--force ' if force else '')\n    cmd += ('--local ' if local else '')\n    cmd += ('--pool {pool} ' if pool else '')\n    cmd += ('--raw ' if raw else '')\n    if ((not pickle_id) and dag):\n        if (dag.full_filepath != dag.filepath):\n            cmd += '-sd DAGS_FOLDER/{dag.filepath} '\n        elif dag.full_filepath:\n            cmd += '-sd {dag.full_filepath}'\n    return cmd.format(**locals())\n", "label": 1}
{"function": "\n\ndef get_jobs(when=None, only_scheduled=False):\n    '\\n    Returns a dictionary mapping of job names together with their respective\\n    application class.\\n    '\n    import sys\n    try:\n        cpath = os.path.dirname(os.path.realpath(sys.argv[0]))\n        ppath = os.path.dirname(cpath)\n        if (ppath not in sys.path):\n            sys.path.append(ppath)\n    except:\n        pass\n    _jobs = {\n        \n    }\n    if True:\n        from django.conf import settings\n        for app_name in settings.INSTALLED_APPS:\n            scandirs = (None, 'minutely', 'quarter_hourly', 'hourly', 'daily', 'weekly', 'monthly', 'yearly')\n            if when:\n                scandirs = (None, when)\n            for subdir in scandirs:\n                try:\n                    path = find_job_module(app_name, subdir)\n                    for name in find_jobs(path):\n                        if ((app_name, name) in _jobs):\n                            raise JobError(('Duplicate job %s' % name))\n                        job = import_job(app_name, name, subdir)\n                        if (only_scheduled and (job.when is None)):\n                            continue\n                        if (when and (job.when != when)):\n                            continue\n                        _jobs[(app_name, name)] = job\n                except ImportError:\n                    pass\n    return _jobs\n", "label": 1}
{"function": "\n\n@register.inclusion_tag('zinnia/tags/dummy.html', takes_context=True)\ndef get_calendar_entries(context, year=None, month=None, template='zinnia/tags/entries_calendar.html'):\n    '\\n    Return an HTML calendar of entries.\\n    '\n    if (not (year and month)):\n        day_week_month = (context.get('day') or context.get('week') or context.get('month'))\n        publication_date = getattr(context.get('object'), 'publication_date', None)\n        if day_week_month:\n            current_month = day_week_month\n        elif publication_date:\n            if settings.USE_TZ:\n                publication_date = timezone.localtime(publication_date)\n            current_month = publication_date.date()\n        else:\n            today = timezone.now()\n            if settings.USE_TZ:\n                today = timezone.localtime(today)\n            current_month = today.date()\n        current_month = current_month.replace(day=1)\n    else:\n        current_month = date(year, month, 1)\n    dates = list(map((lambda x: ((settings.USE_TZ and timezone.localtime(x).date()) or x.date())), Entry.published.datetimes('publication_date', 'month')))\n    if (current_month not in dates):\n        dates.append(current_month)\n        dates.sort()\n    index = dates.index(current_month)\n    previous_month = (((index > 0) and dates[(index - 1)]) or None)\n    next_month = (((index != (len(dates) - 1)) and dates[(index + 1)]) or None)\n    calendar = Calendar()\n    return {\n        'template': template,\n        'next_month': next_month,\n        'previous_month': previous_month,\n        'calendar': calendar.formatmonth(current_month.year, current_month.month, previous_month=previous_month, next_month=next_month),\n    }\n", "label": 1}
{"function": "\n\ndef doWrite(self):\n    '\\n        Called when data can be written.\\n\\n        A result that is true (which will be a negative number or an\\n        exception instance) indicates that the connection was lost. A false\\n        result implies the connection is still there; a result of 0\\n        indicates no write was done, and a result of None indicates that a\\n        write was done.\\n        '\n    if ((len(self.dataBuffer) - self.offset) < self.SEND_LIMIT):\n        self.dataBuffer = (buffer(self.dataBuffer, self.offset) + ''.join(self._tempDataBuffer))\n        self.offset = 0\n        self._tempDataBuffer = []\n        self._tempDataLen = 0\n    if self.offset:\n        l = self.writeSomeData(buffer(self.dataBuffer, self.offset))\n    else:\n        l = self.writeSomeData(self.dataBuffer)\n    if ((l < 0) or isinstance(l, Exception)):\n        return l\n    if ((l == 0) and self.dataBuffer):\n        result = 0\n    else:\n        result = None\n    self.offset += l\n    if ((self.offset == len(self.dataBuffer)) and (not self._tempDataLen)):\n        self.dataBuffer = ''\n        self.offset = 0\n        self.stopWriting()\n        if ((self.producer is not None) and ((not self.streamingProducer) or self.producerPaused)):\n            self.producerPaused = 0\n            self.producer.resumeProducing()\n        elif self.disconnecting:\n            return self._postLoseConnection()\n        elif self._writeDisconnecting:\n            self._writeDisconnected = True\n            result = self._closeWriteConnection()\n            return result\n    return result\n", "label": 1}
{"function": "\n\ndef save_or_overwrite_slice(self, args, slc, slice_add_perm, slice_edit_perm):\n    'save or overwrite a slice'\n    slice_name = args.get('slice_name')\n    action = args.get('action')\n    d = args.to_dict(flat=False)\n    del d['action']\n    del d['previous_viz_type']\n    as_list = ('metrics', 'groupby', 'columns')\n    for k in d:\n        v = d.get(k)\n        if ((k in as_list) and (not isinstance(v, list))):\n            d[k] = ([v] if v else [])\n        if ((k not in as_list) and isinstance(v, list)):\n            d[k] = v[0]\n    table_id = druid_datasource_id = None\n    datasource_type = args.get('datasource_type')\n    if (datasource_type in ('datasource', 'druid')):\n        druid_datasource_id = args.get('datasource_id')\n    elif (datasource_type == 'table'):\n        table_id = args.get('datasource_id')\n    if (action == 'save'):\n        slc = models.Slice()\n    slc.params = json.dumps(d, indent=4, sort_keys=True)\n    slc.datasource_name = args.get('datasource_name')\n    slc.viz_type = args.get('viz_type')\n    slc.druid_datasource_id = druid_datasource_id\n    slc.table_id = table_id\n    slc.datasource_type = datasource_type\n    slc.slice_name = slice_name\n    if ((action == 'save') and slice_add_perm):\n        self.save_slice(slc)\n    elif ((action == 'overwrite') and slice_edit_perm):\n        self.overwrite_slice(slc)\n    return redirect(slc.slice_url)\n", "label": 1}
{"function": "\n\ndef test_ssh_async(sshd_manager, loop):\n\n    class DummyAsyncDelegate(AbstractSSHLibDelegate):\n\n        def on_update(self, future, callback):\n            callback.set_result(True)\n\n        def on_done(self, *args, **kwargs):\n            pass\n    with sshd_manager.run(20) as sshd_ports:\n        runner = MultiRunner(['127.0.0.1:{}'.format(port) for port in sshd_ports], ssh_user=getpass.getuser(), ssh_key_path=sshd_manager.key_path, async_delegate=DummyAsyncDelegate())\n        host_port = ['127.0.0.1:{}'.format(port) for port in sshd_ports]\n        chain = CommandChain('test')\n        chain.add_execute(['uname', '-a'])\n        try:\n            results = loop.run_until_complete(runner.run_commands_chain_async([chain], block=True))\n        finally:\n            loop.close()\n        assert (not os.path.isfile('test.json'))\n        assert (len(results) == 20)\n        for host_result in results:\n            for command_result in host_result:\n                for (host, process_result) in command_result.items():\n                    assert (process_result['returncode'] == 0), process_result['stderr']\n                    assert (host in host_port)\n                    assert ('/usr/bin/ssh' in process_result['cmd'])\n                    assert ('uname' in process_result['cmd'])\n                    assert ('-tt' in process_result['cmd'])\n                    assert (len(process_result['cmd']) == 13)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _handle_results(outqueue, get, cache):\n    thread = threading.current_thread()\n    while 1:\n        try:\n            task = get()\n        except (IOError, EOFError):\n            debug('result handler got EOFError/IOError -- exiting')\n            return\n        if thread._state:\n            assert (thread._state == TERMINATE)\n            debug('result handler found thread._state=TERMINATE')\n            break\n        if (task is None):\n            debug('result handler got sentinel')\n            break\n        (job, i, obj) = task\n        try:\n            cache[job]._set(i, obj)\n        except KeyError:\n            pass\n    while (cache and (thread._state != TERMINATE)):\n        try:\n            task = get()\n        except (IOError, EOFError):\n            debug('result handler got EOFError/IOError -- exiting')\n            return\n        if (task is None):\n            debug('result handler ignoring extra sentinel')\n            continue\n        (job, i, obj) = task\n        try:\n            cache[job]._set(i, obj)\n        except KeyError:\n            pass\n    if hasattr(outqueue, '_reader'):\n        debug('ensuring that outqueue is not full')\n        try:\n            for i in range(10):\n                if (not outqueue._reader.poll()):\n                    break\n                get()\n        except (IOError, EOFError):\n            pass\n    debug('result handler exiting: len(cache)=%s, thread._state=%s', len(cache), thread._state)\n", "label": 1}
{"function": "\n\ndef test_has_receivers():\n    received = (lambda sender: None)\n    sig = blinker.Signal()\n    assert (not sig.has_receivers_for(None))\n    assert (not sig.has_receivers_for(blinker.ANY))\n    sig.connect(received, 'xyz')\n    assert (not sig.has_receivers_for(None))\n    assert (not sig.has_receivers_for(blinker.ANY))\n    assert sig.has_receivers_for('xyz')\n\n    class Object(object):\n        pass\n    o = Object()\n    sig.connect(received, o)\n    assert sig.has_receivers_for(o)\n    del received\n    collect_acyclic_refs()\n    assert (not sig.has_receivers_for('xyz'))\n    assert (list(sig.receivers_for('xyz')) == [])\n    assert (list(sig.receivers_for(o)) == [])\n    sig.connect((lambda sender: None), weak=False)\n    assert sig.has_receivers_for('xyz')\n    assert sig.has_receivers_for(o)\n    assert sig.has_receivers_for(None)\n    assert sig.has_receivers_for(blinker.ANY)\n    assert sig.has_receivers_for('xyz')\n", "label": 1}
{"function": "\n\ndef write_render_callable(self, node, name, args, buffered, filtered, cached):\n    'write a top-level render callable.\\n\\n        this could be the main render() method or that of a top-level def.'\n    if self.in_def:\n        decorator = node.decorator\n        if decorator:\n            self.printer.writeline(('@runtime._decorate_toplevel(%s)' % decorator))\n    self.printer.writelines(('def %s(%s):' % (name, ','.join(args))), '__M_caller = context.caller_stack._push_frame()', 'try:')\n    if (buffered or filtered or cached):\n        self.printer.writeline('context._push_buffer()')\n    self.identifier_stack.append(self.compiler.identifiers.branch(self.node))\n    if (((not self.in_def) or self.node.is_block) and ('**pageargs' in args)):\n        self.identifier_stack[(- 1)].argument_declared.add('pageargs')\n    if ((not self.in_def) and ((len(self.identifiers.locally_assigned) > 0) or (len(self.identifiers.argument_declared) > 0))):\n        self.printer.writeline(('__M_locals = __M_dict_builtin(%s)' % ','.join([('%s=%s' % (x, x)) for x in self.identifiers.argument_declared])))\n    self.write_variable_declares(self.identifiers, toplevel=True)\n    for n in self.node.nodes:\n        n.accept_visitor(self)\n    self.write_def_finish(self.node, buffered, filtered, cached)\n    self.printer.writeline(None)\n    self.printer.write('\\n\\n')\n    if cached:\n        self.write_cache_decorator(node, name, args, buffered, self.identifiers, toplevel=True)\n", "label": 1}
{"function": "\n\ndef convert_schema_order(objects, root_type):\n    ref_objects = []\n    cur_objects = []\n    for obj in objects:\n        if (obj.getName() == root_type):\n            ref_objects = [obj]\n            cur_objects = [obj]\n            break\n    if (len(ref_objects) < 1):\n        raise ValueError(('Cannot find root %s' % root_type))\n    while (len(cur_objects) > 0):\n        next_objects = []\n        for obj in cur_objects:\n            for prop in obj.getXMLElements():\n                if (prop.isReference() and (prop.getReferencedObject() not in ref_objects) and (prop.getReferencedObject() not in next_objects)):\n                    next_objects.append(prop.getReferencedObject())\n            for choice in obj.getXMLChoices():\n                for prop in choice.getXMLElements():\n                    if (prop.isReference() and (prop.getReferencedObject() not in ref_objects) and (prop.getReferencedObject() not in next_objects)):\n                        next_objects.append(prop.getReferencedObject())\n        ref_objects.extend(next_objects)\n        cur_objects = next_objects\n    return ref_objects\n", "label": 1}
{"function": "\n\ndef _round_robin(self, rdesc):\n    \" 'time estimate' using round-robin selection. \"\n    for host in self.cluster:\n        if (host.total_cpus <= 0):\n            (count, criteria) = host.allocator.max_servers({\n                \n            })\n            host.total_cpus = count\n    if self._last_deployed:\n        for (i, host) in enumerate(self.cluster):\n            if (host is self._last_deployed):\n                i = ((i + 1) % len(self.cluster))\n                break\n    else:\n        i = 0\n    for j in range(len(self.cluster)):\n        host = self.cluster[i]\n        if (host.allocated_cpus < host.total_cpus):\n            allocation_host = host\n            break\n        i = ((i + 1) % len(self.cluster))\n    else:\n        return ((- 1), {\n            'min_cpus': 'no idle cpus',\n        })\n    hostnames = []\n    min_cpus = rdesc.get('min_cpus', 1)\n    required = min_cpus\n    host_added = True\n    while (host_added and (required > 0)):\n        host_added = False\n        for j in range(len(self.cluster)):\n            if (host.allocated_cpus < host.total_cpus):\n                hostnames.append(host.netname)\n                host_added = True\n                required -= 1\n                if (required <= 0):\n                    break\n            i = ((i + 1) % len(self.cluster))\n            host = self.cluster[i]\n    if (required > 0):\n        return ((- 1), {\n            'min_cpus': ('want %d, idle %d' % (min_cpus, len(hostnames))),\n        })\n    return (0, {\n        'host': allocation_host,\n        'hostnames': hostnames,\n    })\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_key_ != x.has_key_):\n        return 0\n    if (self.has_key_ and (self.key_ != x.key_)):\n        return 0\n    if (self.has_value_ != x.has_value_):\n        return 0\n    if (self.has_value_ and (self.value_ != x.value_)):\n        return 0\n    if (self.has_flags_ != x.has_flags_):\n        return 0\n    if (self.has_flags_ and (self.flags_ != x.flags_)):\n        return 0\n    if (self.has_cas_id_ != x.has_cas_id_):\n        return 0\n    if (self.has_cas_id_ and (self.cas_id_ != x.cas_id_)):\n        return 0\n    if (self.has_expires_in_seconds_ != x.has_expires_in_seconds_):\n        return 0\n    if (self.has_expires_in_seconds_ and (self.expires_in_seconds_ != x.expires_in_seconds_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef check_permissions(permission_type, user, project):\n    '\\n    Here we check permission types, and see if a user has proper perms.\\n    If a user has \"edit\" permissions on a project, they pretty much have\\n    carte blanche to do as they please, so we kick that back as true. \\n    Otherwise we go more fine grained and check their view and comment \\n    permissions\\n    '\n    try:\n        groups = user.groups.all()\n    except AttributeError:\n        groups = None\n    if (user in project.users_can_edit.all()):\n        return True\n    for x in groups:\n        if (x in project.groups_can_edit.all()):\n            return True\n    if (permission_type == 'edit'):\n        if project.allow_anon_editing:\n            return True\n    if (permission_type == 'view'):\n        if (user in project.users_can_view.all()):\n            return True\n        for x in groups:\n            if (x in project.groups_can_view.all()):\n                return True\n        if (project.allow_anon_viewing is True):\n            return True\n    if (permission_type == 'comment'):\n        if (user in project.users_can_comment.all()):\n            return True\n        for x in groups:\n            if (x in project.groups_can_comment.all()):\n                return True\n        if (project.allow_anon_comment is True):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef _serialize(self):\n    data = {\n        \n    }\n    if self.interval:\n        data['interval'] = self.interval\n    else:\n        raise RuntimeError('interval required')\n    if self.time_zone:\n        data['time_zone'] = self.time_zone\n    if self.pre_zone:\n        data['pre_zone'] = self.pre_zone\n    if self.post_zone:\n        data['post_zone'] = self.post_zone\n    if self.factor:\n        data['factor'] = self.factor\n    if self.pre_offset:\n        data['pre_offset'] = self.pre_offset\n    if self.post_offset:\n        data['post_offset'] = self.post_offset\n    if (self.min_doc_count is not None):\n        data['min_doc_count'] = self.min_doc_count\n    if (self.extended_bounds is not None):\n        data['extended_bounds'] = self.extended_bounds\n    if self.field:\n        data['field'] = self.field\n    elif self.key_field:\n        data['key_field'] = self.key_field\n        if self.value_field:\n            data['value_field'] = self.value_field\n        elif self.value_script:\n            data['value_script'] = self.value_script\n            if self.params:\n                data['params'] = self.params\n        else:\n            raise RuntimeError('Invalid key_field: value_field or value_script required')\n    return data\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    a = self.msg\n    b = ''\n    if self.http_scheme:\n        b += ('%s://' % self.http_scheme)\n    if self.http_host:\n        b += self.http_host\n    if self.http_port:\n        b += (':%s' % self.http_port)\n    if self.http_path:\n        b += self.http_path\n    if self.http_query:\n        b += ('?%s' % self.http_query)\n    if self.http_status:\n        if b:\n            b = ('%s %s' % (b, self.http_status))\n        else:\n            b = str(self.http_status)\n    if self.http_reason:\n        if b:\n            b = ('%s %s' % (b, self.http_reason))\n        else:\n            b = ('- %s' % self.http_reason)\n    if self.http_device:\n        if b:\n            b = ('%s: device %s' % (b, self.http_device))\n        else:\n            b = ('device %s' % self.http_device)\n    if self.http_response_content:\n        if (len(self.http_response_content) <= 60):\n            b += ('   %s' % self.http_response_content)\n        else:\n            b += ('  [first 60 chars of response] %s' % self.http_response_content[:60])\n    return ((b and ('%s: %s' % (a, b))) or a)\n", "label": 1}
{"function": "\n\ndef getFullName(self, item, partial=''):\n    'Return a syntactically proper name for item.'\n    name = self.GetItemText(item)\n    parent = None\n    obj = None\n    if (item != self.root):\n        parent = self.GetItemParent(item)\n        obj = self.GetPyData(parent)\n    if (((type(obj) is types.DictType) or ((str(type(obj))[17:23] == 'BTrees') and hasattr(obj, 'keys'))) and (((item != self.root) and (parent != self.root)) or ((parent == self.root) and (not self.rootIsNamespace)))):\n        name = (('[' + name) + ']')\n    if partial:\n        if (partial[0] == '['):\n            name += partial\n        else:\n            name += ('.' + partial)\n    if (((item != self.root) and (parent != self.root)) or ((parent == self.root) and (not self.rootIsNamespace))):\n        name = self.getFullName(parent, partial=name)\n    return name\n", "label": 1}
{"function": "\n\ndef m44is_identity(m):\n    (m0, m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13, m14, m15) = m\n    return ((m0 == 1) and (m1 == 0) and (m2 == 0) and (m3 == 0) and (m4 == 0) and (m5 == 1) and (m6 == 0) and (m7 == 0) and (m8 == 0) and (m9 == 0) and (m10 == 1) and (m11 == 0) and (m12 == 0) and (m13 == 0) and (m14 == 0) and (m15 == 1))\n", "label": 1}
{"function": "\n\ndef my_fn_PDxEV(xc, p, contextItem, args):\n    if (len(args) != 2):\n        raise XPathContext.FunctionNumArgs()\n    PDseq = (args[0] if isinstance(args[0], (list, tuple)) else (args[0],))\n    EVseq = (args[1] if isinstance(args[1], (list, tuple)) else (args[1],))\n    dimQname = qname('{http://www.example.com/wgt-avg}ExposuresDimension')\n    PDxEV = []\n    for pd in PDseq:\n        if (pd.context is not None):\n            pdDim = pd.context.dimValue(dimQname)\n            for ev in EVseq:\n                if (ev.context is not None):\n                    evDim = ev.context.dimValue(dimQname)\n                    if ((pdDim is not None) and isinstance(pdDim, ModelDimensionValue)):\n                        dimEqual = pdDim.isEqualTo(evDim, equalMode=XbrlUtil.S_EQUAL2)\n                    elif ((evDim is not None) and isinstance(evDim, ModelDimensionValue)):\n                        dimEqual = evDim.isEqualTo(pdDim, equalMode=XbrlUtil.S_EQUAL2)\n                    else:\n                        dimEqual = (pdDim == evDim)\n                    if dimEqual:\n                        pdX = pd.xValue\n                        evX = ev.xValue\n                        if (isinstance(pdX, Decimal) and isinstance(evX, float)):\n                            pdX = float(pdX)\n                        elif (isinstance(evX, Decimal) and isinstance(pdX, float)):\n                            pdX = float(evX)\n                        PDxEV.append((pdX * evX))\n                        break\n    return PDxEV\n", "label": 1}
{"function": "\n\n@classmethod\ndef split(cls, line):\n    '\\n        Decode an IRC protocol line as UTF-8 and split into tokens as defined\\n        in RFC 1459 and the IRCv3 message-tags extension.\\n        '\n    line = line.decode('utf-8', 'replace')\n    line = line.rstrip('\\r\\n').split(' ')\n    (i, n) = (0, len(line))\n    parv = []\n    while ((i < n) and (line[i] == '')):\n        i += 1\n    if ((i < n) and line[i].startswith('@')):\n        parv.append(line[i])\n        i += 1\n        while ((i < n) and (line[i] == '')):\n            i += 1\n    if ((i < n) and line[i].startswith(':')):\n        parv.append(line[i])\n        i += 1\n        while ((i < n) and (line[i] == '')):\n            i += 1\n    while (i < n):\n        if line[i].startswith(':'):\n            break\n        elif (line[i] != ''):\n            parv.append(line[i])\n        i += 1\n    if (i < n):\n        trailing = ' '.join(line[i:])\n        parv.append(trailing[1:])\n    return parv\n", "label": 1}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 8):\n            self.set_width(d.getVarInt32())\n            continue\n        if (tt == 16):\n            self.set_height(d.getVarInt32())\n            continue\n        if (tt == 24):\n            self.set_rotate(d.getVarInt32())\n            continue\n        if (tt == 32):\n            self.set_horizontal_flip(d.getBoolean())\n            continue\n        if (tt == 40):\n            self.set_vertical_flip(d.getBoolean())\n            continue\n        if (tt == 53):\n            self.set_crop_left_x(d.getFloat())\n            continue\n        if (tt == 61):\n            self.set_crop_top_y(d.getFloat())\n            continue\n        if (tt == 69):\n            self.set_crop_right_x(d.getFloat())\n            continue\n        if (tt == 77):\n            self.set_crop_bottom_y(d.getFloat())\n            continue\n        if (tt == 80):\n            self.set_autolevels(d.getBoolean())\n            continue\n        if (tt == 88):\n            self.set_crop_to_fit(d.getBoolean())\n            continue\n        if (tt == 101):\n            self.set_crop_offset_x(d.getFloat())\n            continue\n        if (tt == 109):\n            self.set_crop_offset_y(d.getFloat())\n            continue\n        if (tt == 112):\n            self.set_allow_stretch(d.getBoolean())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef makeSequenceCreationOrConstant(sequence_kind, elements, source_ref):\n    for element in elements:\n        if (not element.isExpressionConstantRef()):\n            constant = False\n            break\n    else:\n        constant = True\n    sequence_kind = sequence_kind.upper()\n    if constant:\n        if (sequence_kind == 'TUPLE'):\n            const_type = tuple\n        elif (sequence_kind == 'LIST'):\n            const_type = list\n        elif (sequence_kind == 'SET'):\n            const_type = set\n        else:\n            assert False, sequence_kind\n        result = ExpressionConstantRef(constant=const_type((element.getConstant() for element in elements)), source_ref=source_ref, user_provided=True)\n    elif (sequence_kind == 'TUPLE'):\n        result = ExpressionMakeTuple(elements=elements, source_ref=source_ref)\n    elif (sequence_kind == 'LIST'):\n        result = ExpressionMakeList(elements=elements, source_ref=source_ref)\n    elif (sequence_kind == 'SET'):\n        result = ExpressionMakeSet(elements=elements, source_ref=source_ref)\n    else:\n        assert False, sequence_kind\n    if elements:\n        result.setCompatibleSourceReference(source_ref=elements[(- 1)].getCompatibleSourceReference())\n    return result\n", "label": 1}
{"function": "\n\ndef trim(c, start, end, trim5=False, trim3=False, both=True):\n    (cstart, cend) = (c.start, c.end)\n    if (((trim5 or both) and (c.strand == '+')) or ((trim3 or both) and (c.strand == '-'))):\n        c.start = max(cstart, start)\n    if (((trim3 or both) and (c.strand == '+')) or ((trim5 or both) and (c.strand == '-'))):\n        c.end = min(cend, end)\n    if ((c.start != cstart) or (c.end != cend)):\n        ((print >> sys.stderr), c.id, 'trimmed [{0}, {1}] => [{2}, {3}]'.format(cstart, cend, c.start, c.end))\n    else:\n        ((print >> sys.stderr), c.id, 'no change')\n", "label": 1}
{"function": "\n\ndef check_dict_formatting_in_string(logical_line, tokens):\n    'Check that strings do not use dict-formatting with a single replacement\\n\\n    N352\\n    '\n    if ((not logical_line) or logical_line.startswith('#') or logical_line.endswith('# noqa')):\n        return\n    current_string = ''\n    in_string = False\n    for (token_type, text, start, end, line) in tokens:\n        if (token_type == tokenize.STRING):\n            if (not in_string):\n                current_string = ''\n                in_string = True\n            current_string += text.strip('\"')\n        elif (token_type == tokenize.OP):\n            if (not current_string):\n                continue\n            in_string = False\n            if (text == '%'):\n                format_keys = set()\n                for match in re_str_format.finditer(current_string):\n                    format_keys.add(match.group(1))\n                if (len(format_keys) == 1):\n                    (yield (0, 'N353 Do not use mapping key string formatting with a single key'))\n            if (text != ')'):\n                current_string = ''\n        elif (token_type in (tokenize.NL, tokenize.COMMENT)):\n            continue\n        else:\n            in_string = False\n            if (token_type == tokenize.NEWLINE):\n                current_string = ''\n", "label": 1}
{"function": "\n\ndef _tostring_query(varname, value, explode, operator, safe=''):\n    joiner = operator\n    varprefix = ''\n    if (operator == '?'):\n        joiner = '&'\n        varprefix = (varname + '=')\n    if (type(value) == type([])):\n        if (0 == len(value)):\n            return ''\n        if (explode == '+'):\n            return joiner.join([((varname + '=') + urllib.quote(x, safe)) for x in value])\n        elif (explode == '*'):\n            return joiner.join([urllib.quote(x, safe) for x in value])\n        else:\n            return (varprefix + ','.join([urllib.quote(x, safe) for x in value]))\n    elif (type(value) == type({\n        \n    })):\n        if (0 == len(value)):\n            return ''\n        keys = value.keys()\n        keys.sort()\n        if (explode == '+'):\n            return joiner.join([((((varname + '.') + urllib.quote(key, safe)) + '=') + urllib.quote(value[key], safe)) for key in keys])\n        elif (explode == '*'):\n            return joiner.join([((urllib.quote(key, safe) + '=') + urllib.quote(value[key], safe)) for key in keys])\n        else:\n            return (varprefix + ','.join([((urllib.quote(key, safe) + ',') + urllib.quote(value[key], safe)) for key in keys]))\n    elif value:\n        return ((varname + '=') + urllib.quote(value, safe))\n    else:\n        return varname\n", "label": 1}
{"function": "\n\ndef _post_commit(self, session, response):\n    signals = []\n    exceptions = []\n    models = session.router\n    for result in (response or ()):\n        if isinstance(result, Exception):\n            exceptions.append(result)\n        if (not isinstance(result, session_result)):\n            continue\n        (meta, result) = result\n        if (not result):\n            continue\n        sm = session.model(meta)\n        (saved, deleted, errors) = sm.post_commit(result)\n        exceptions.extend(errors)\n        if deleted:\n            self.deleted[meta] = deleted\n            if self.signal_delete:\n                signals.append((models.post_delete.fire, sm, deleted))\n        if saved:\n            self.saved[meta] = saved\n            if self.signal_commit:\n                signals.append((models.post_commit.fire, sm, saved))\n    for (fire, sm, instances) in signals:\n        for result in fire(sm.model, instances=instances, session=session, transaction=self):\n            (yield result)\n    if exceptions:\n        nf = len(exceptions)\n        if (nf > 1):\n            error = ('There were %s exceptions during commit.\\n\\n' % nf)\n            error += '\\n\\n'.join((str(e) for e in exceptions))\n        else:\n            error = str(exceptions[0])\n        raise CommitException(error, failures=nf)\n", "label": 1}
{"function": "\n\ndef get_sizes(self, total_width, total_height, xoffset=0, yoffset=0):\n    width = 0\n    height = 0\n    results = []\n    (rows, cols, orientation) = self.calc(self.num_windows, total_width, total_height)\n    if (orientation == ROWCOL):\n        y = 0\n        for (i, row) in enumerate(range(rows)):\n            x = 0\n            width = (total_width // cols)\n            for (j, col) in enumerate(range(cols)):\n                height = (total_height // rows)\n                if ((i == (rows - 1)) and (j == 0)):\n                    remaining = (self.num_windows - len(results))\n                    width = (total_width // remaining)\n                elif ((j == (cols - 1)) or ((len(results) + 1) == self.num_windows)):\n                    width = (total_width - x)\n                results.append(((x + xoffset), (y + yoffset), width, height))\n                if (len(results) == self.num_windows):\n                    return results\n                x += width\n            y += height\n    else:\n        x = 0\n        for (i, col) in enumerate(range(cols)):\n            y = 0\n            height = (total_height // rows)\n            for (j, row) in enumerate(range(rows)):\n                width = (total_width // cols)\n                if ((i == (cols - 1)) and (j == 0)):\n                    remaining = (self.num_windows - len(results))\n                    height = (total_height // remaining)\n                elif ((j == (rows - 1)) or ((len(results) + 1) == self.num_windows)):\n                    height = (total_height - y)\n                results.append(((x + xoffset), (y + yoffset), width, height))\n                if (len(results) == self.num_windows):\n                    return results\n                y += height\n            x += width\n    return results\n", "label": 1}
{"function": "\n\ndef parse(self):\n    (self.encoding, self.text) = self.decode_raw_stream(self.text, (not self.disable_unicode), self.encoding, self.filename)\n    for preproc in self.preprocessor:\n        self.text = preproc(self.text)\n    self.match_reg(self._coding_re)\n    self.textlength = len(self.text)\n    while True:\n        if (self.match_position > self.textlength):\n            break\n        if self.match_end():\n            break\n        if self.match_expression():\n            continue\n        if self.match_control_line():\n            continue\n        if self.match_comment():\n            continue\n        if self.match_tag_start():\n            continue\n        if self.match_tag_end():\n            continue\n        if self.match_python_block():\n            continue\n        if self.match_text():\n            continue\n        if (self.match_position > self.textlength):\n            break\n        raise exceptions.CompileException('assertion failed')\n    if len(self.tag):\n        raise exceptions.SyntaxException(('Unclosed tag: <%%%s>' % self.tag[(- 1)].keyword), **self.exception_kwargs)\n    if len(self.control_line):\n        raise exceptions.SyntaxException((\"Unterminated control keyword: '%s'\" % self.control_line[(- 1)].keyword), self.text, self.control_line[(- 1)].lineno, self.control_line[(- 1)].pos, self.filename)\n    return self.template\n", "label": 1}
{"function": "\n\ndef compare(reference, simulated, display_limit=(- 1)):\n    '\\n    Compare two data files for equivalence.\\n    '\n    time = reference['time']\n    steps = len(time)\n    err = False\n    displayed = 0\n    for i in range(steps):\n        for (n, series) in list(reference.items()):\n            if (n not in simulated):\n                if (n in IGNORABLE_COLS):\n                    continue\n                if ((display_limit >= 0) and (displayed < display_limit)):\n                    log(ERROR, 'missing column %s in second file', n)\n                    displayed += 1\n                break\n            if (len(reference[n]) != len(simulated[n])):\n                if ((display_limit >= 0) and (displayed < display_limit)):\n                    log(ERROR, 'len mismatch for %s (%d vs %d)', n, len(reference[n]), len(simulated[n]))\n                    displayed += 1\n                err = True\n                break\n            ref = float(series[i])\n            sim = float(simulated[n][i])\n            around_zero = (isclose(ref, 0, abs_tol=1e-06) and isclose(sim, 0, abs_tol=1e-06))\n            if ((not around_zero) and (not isclose(ref, sim, rel_tol=0.0001))):\n                if ((display_limit >= 0) and (displayed < display_limit)):\n                    log(ERROR, 'time %s mismatch in %s (%s != %s)', time[i], n, ref, sim)\n                    displayed += 1\n                err = True\n    return err\n", "label": 1}
{"function": "\n\ndef _find_K_cut(self, k):\n    '\\n        Find the lowest level cut that has k connected components. If there are\\n        no levels that have k components, then find the lowest level that has\\n        at least k components. If no levels have > k components, find the\\n        lowest level that has the maximum number of components.\\n\\n        Parameters\\n        ----------\\n        k : int\\n            Desired number of clusters/nodes/components.\\n\\n        Returns\\n        -------\\n        cut : float\\n            Lowest density level where there are k nodes.\\n        '\n    starts = [v.start_level for v in self.nodes.itervalues()]\n    ends = [v.end_level for v in self.nodes.itervalues()]\n    crits = _np.unique((starts + ends))\n    nclust = {\n        \n    }\n    for c in crits:\n        nclust[c] = len([e for (e, v) in self.nodes.iteritems() if ((v.start_level <= c) and (v.end_level > c))])\n    width = _np.max(nclust.values())\n    if (k in nclust.values()):\n        cut = _np.min([e for (e, v) in nclust.iteritems() if (v == k)])\n    elif (width < k):\n        cut = _np.min([e for (e, v) in nclust.iteritems() if (v == width)])\n    else:\n        ktemp = _np.min([v for v in nclust.itervalues() if (v > k)])\n        cut = _np.min([e for (e, v) in nclust.iteritems() if (v == ktemp)])\n    return cut\n", "label": 1}
{"function": "\n\ndef sEqual(dts1, elt1, elt2, equalMode=S_EQUAL, excludeIDs=NO_IDs_EXCLUDED, dts2=None, ns2ns1Tbl=None):\n    if (dts2 is None):\n        dts2 = dts1\n    if (elt1.localName != elt2.localName):\n        return False\n    if (ns2ns1Tbl and (elt2.namespaceURI in ns2ns1Tbl)):\n        if (elt1.namespaceURI != ns2ns1Tbl[elt2.namespaceURI]):\n            return False\n    elif (elt1.namespaceURI != elt2.namespaceURI):\n        return False\n    if (not hasattr(elt1, 'xValid')):\n        xmlValidate(dts1, elt1)\n    if (not hasattr(elt2, 'xValid')):\n        xmlValidate(dts2, elt2)\n    children1 = childElements(elt1)\n    children2 = childElements(elt2)\n    if (len(children1) != len(children2)):\n        return False\n    if ((not xEqual(elt1, elt2, equalMode)) or (attributeDict(dts1, elt1, (), equalMode, excludeIDs) != attributeDict(dts2, elt2, (), equalMode, excludeIDs, ns2ns1Tbl))):\n        return False\n    excludeChildIDs = (excludeIDs if (excludeIDs != TOP_IDs_EXCLUDED) else NO_IDs_EXCLUDED)\n    for i in range(len(children1)):\n        if (not sEqual(dts1, children1[i], children2[i], equalMode, excludeChildIDs, dts2, ns2ns1Tbl)):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef _scan(self):\n    plugins = []\n    dirs = []\n    for name in os.listdir(self.root):\n        if (name in ('.', '..')):\n            continue\n        path = os.path.join(self.root, name)\n        if (os.path.isdir(path) and os.path.isfile(os.path.join(path, '__init__.py'))):\n            if (path not in sys.path):\n                dirs.append((name, path))\n                sys.path.insert(0, os.path.join(self.root, '..', name))\n                sys.path.insert(0, os.path.join(self.root, name))\n    for (name, d) in dirs:\n        try:\n            for f in os.listdir(d):\n                if (f.endswith('.py') and (f != '__init__.py')):\n                    plugins.append((('%s.%s' % (name, f[:(- 3)])), d))\n        except OSError:\n            sys.stderr.write(('error, loading %s' % f))\n            sys.stderr.flush()\n            continue\n    mod = None\n    for (name, d) in plugins:\n        mod = importlib.import_module(name)\n        if hasattr(mod, '__all__'):\n            for attr in mod.__all__:\n                plug = getattr(mod, attr)\n                if issubclass(plug, Plugin):\n                    self._load_plugin(plug())\n", "label": 1}
{"function": "\n\ndef run(command, **kwargs):\n    '\\n    :type command: str\\n    :type capture_output: bool\\n    :type use_shell: bool\\n    :type print_command: bool\\n    :type should_return_returncode: bool\\n    :type should_raise_when_fail: bool\\n    :type should_process_output: bool\\n    :rtype: (str, str, int) | (str, str)\\n    '\n    for (key, value) in six.iteritems(_run_global_settings):\n        kwargs.setdefault(key, value)\n    capture_output = kwargs['capture_output']\n    use_shell = kwargs['use_shell']\n    print_command = kwargs['print_command']\n    should_return_returncode = kwargs['should_return_returncode']\n    should_raise_when_fail = kwargs['should_raise_when_fail']\n    should_process_output = kwargs['should_process_output']\n    use_shell = (('&&' in command) or ('||' in command) or ('|' in command) or use_shell)\n    if print_command:\n        print(command)\n    popen = subprocess.Popen((command if use_shell else shlex.split(command)), stdout=(subprocess.PIPE if capture_output else None), stderr=(subprocess.PIPE if capture_output else None), shell=use_shell)\n    (stdout, stderr) = popen.communicate()\n    return_code = popen.returncode\n    if ((return_code != 0) and should_raise_when_fail):\n        raise RunCommandError('Command execution returns {}'.format(return_code))\n    if capture_output:\n        if should_process_output:\n            stdout = _process_run_command_output(stdout)\n            stderr = _process_run_command_output(stderr)\n        if should_return_returncode:\n            return (stdout, stderr, return_code)\n        else:\n            return (stdout, stderr)\n    elif should_return_returncode:\n        return return_code\n", "label": 1}
{"function": "\n\ndef join(a, *p):\n    'Join two or more pathname components, inserting \"\\\\\" as needed.\\n    If any component is an absolute path, all previous path components\\n    will be discarded.'\n    path = a\n    for b in p:\n        b_wins = 0\n        if (path == ''):\n            b_wins = 1\n        elif isabs(b):\n            if ((path[1:2] != ':') or (b[1:2] == ':')):\n                b_wins = 1\n            elif ((len(path) > 3) or ((len(path) == 3) and (path[(- 1)] not in '/\\\\'))):\n                b_wins = 1\n        if b_wins:\n            path = b\n        else:\n            assert (len(path) > 0)\n            if (path[(- 1)] in '/\\\\'):\n                if (b and (b[0] in '/\\\\')):\n                    path += b[1:]\n                else:\n                    path += b\n            elif (path[(- 1)] == ':'):\n                path += b\n            elif b:\n                if (b[0] in '/\\\\'):\n                    path += b\n                else:\n                    path += ('\\\\' + b)\n            else:\n                path += '\\\\'\n    return path\n", "label": 1}
{"function": "\n\ndef get_view_index(self, key):\n    \"\\n        Returns the processed view's group and index\\n\\n        @param key: a key to get value\\n        \"\n    group_settings = self.get(key)\n    view_group = (- 1)\n    view_index = (- 1)\n    window = sublime.active_window()\n    view = window.active_view()\n    (current_group, current_index) = window.get_view_index(view)\n    for group_setting in group_settings:\n        if isinstance(group_setting, int):\n            view_group = group_setting\n            view_index = len(window.views_in_group(view_group))\n        elif ((isinstance(group_setting, list) or isinstance(group_setting, tuple)) and group_setting):\n            view_group = group_setting[0]\n            view_index = len(window.views_in_group(view_group))\n            if (len(group_setting) > 1):\n                view_index = group_setting[1]\n        if ((view_group >= window.num_groups()) or (view_group < 0)):\n            view_index = (- 1)\n            continue\n        elif ((view_index > len(window.views_in_group(view_group))) or (view_index < 0)):\n            continue\n        break\n    if ((view_group >= window.num_groups()) or (view_group < 0)):\n        view_group = current_group\n        view_index = len(window.views_in_group(view_group))\n    elif ((view_index >= len(window.views_in_group(view_group))) or (view_index < 0)):\n        view_index = len(window.views_in_group(view_group))\n    return (view_group, view_index)\n", "label": 1}
{"function": "\n\ndef byte_count(byte_count):\n    if isinstance(byte_count, int):\n        return positive_nonzero_integer(byte_count)\n    byte_count = unicode_str(byte_count)\n\n    def _get_byte_count(postfix, base, exponant):\n        char_num = len(postfix)\n        if (byte_count[(- char_num):] == postfix):\n            count = decimal.Decimal(byte_count[:(- char_num)])\n            return positive_nonzero_integer((count * (base ** exponant)))\n        return None\n    if (len(byte_count) > 1):\n        n = None\n        n = (n if (n is not None) else _get_byte_count('K', 1024, 1))\n        n = (n if (n is not None) else _get_byte_count('M', 1024, 2))\n        n = (n if (n is not None) else _get_byte_count('G', 1024, 3))\n        n = (n if (n is not None) else _get_byte_count('T', 1024, 4))\n        n = (n if (n is not None) else _get_byte_count('P', 1024, 5))\n        if (n is not None):\n            return n\n    if (len(byte_count) > 2):\n        n = None\n        n = (n if (n is not None) else _get_byte_count('KB', 1000, 1))\n        n = (n if (n is not None) else _get_byte_count('MB', 1000, 2))\n        n = (n if (n is not None) else _get_byte_count('GB', 1000, 3))\n        n = (n if (n is not None) else _get_byte_count('TB', 1000, 4))\n        n = (n if (n is not None) else _get_byte_count('PB', 1000, 5))\n        if (n is not None):\n            return n\n    return positive_nonzero_integer(byte_count)\n", "label": 1}
{"function": "\n\ndef models_from_model(model, include_related=False, exclude=None):\n    'Generator of all model in model.'\n    if (exclude is None):\n        exclude = set()\n    if (model and (model not in exclude)):\n        exclude.add(model)\n        if (isinstance(model, ModelType) and (not model._meta.abstract)):\n            (yield model)\n            if include_related:\n                exclude.add(model)\n                for field in model._meta.fields:\n                    if hasattr(field, 'relmodel'):\n                        through = getattr(field, 'through', None)\n                        for rmodel in (field.relmodel, field.model, through):\n                            for m in models_from_model(rmodel, include_related=include_related, exclude=exclude):\n                                (yield m)\n                for manytomany in model._meta.manytomany:\n                    related = getattr(model, manytomany)\n                    for m in models_from_model(related.model, include_related=include_related, exclude=exclude):\n                        (yield m)\n        elif ((not isinstance(model, ModelType)) and isclass(model)):\n            (yield model)\n", "label": 1}
{"function": "\n\ndef _reduce_map(self, dimensions, function, reduce_map):\n    if (dimensions and reduce_map):\n        raise Exception('Pass reduced dimensions either as an argument or as part of the kwargs not both.')\n    if (len(set(reduce_map.values())) > 1):\n        raise Exception('Cannot define reduce operations with more than one function at a time.')\n    sanitized_dict = {dimension_sanitizer(kd): kd for kd in self.dimensions('key', True)}\n    if reduce_map:\n        reduce_map = reduce_map.items()\n    if dimensions:\n        reduce_map = [(d, function) for d in dimensions]\n    elif (not reduce_map):\n        reduce_map = [(d, function) for d in self.kdims]\n    reduced = [((d.name if isinstance(d, Dimension) else d), fn) for (d, fn) in reduce_map]\n    sanitized = [(sanitized_dict.get(d, d), fn) for (d, fn) in reduced]\n    grouped = [(fn, [dim for (dim, _) in grp]) for (fn, grp) in groupby(sanitized, (lambda x: x[1]))]\n    return grouped[0]\n", "label": 1}
{"function": "\n\ndef _MarkLinesToFormat(uwlines, lines):\n    \"Skip sections of code that we shouldn't reformat.\"\n    if lines:\n        for uwline in uwlines:\n            uwline.disable = True\n        for (start, end) in sorted(lines):\n            for uwline in uwlines:\n                if (uwline.lineno > end):\n                    break\n                if (uwline.lineno >= start):\n                    uwline.disable = False\n                elif (uwline.last.lineno >= start):\n                    uwline.disable = False\n    index = 0\n    while (index < len(uwlines)):\n        uwline = uwlines[index]\n        if uwline.is_comment:\n            if _DisableYAPF(uwline.first.value.strip()):\n                while (index < len(uwlines)):\n                    uwline = uwlines[index]\n                    uwline.disable = True\n                    if (uwline.is_comment and _EnableYAPF(uwline.first.value.strip())):\n                        break\n                    index += 1\n        elif re.search(DISABLE_PATTERN, uwline.last.value.strip(), re.IGNORECASE):\n            uwline.disable = True\n        index += 1\n", "label": 1}
{"function": "\n\ndef __call__(self, *args):\n    c = args[0]\n    c = list(c)\n    (letter, n) = (c[0], int(c[1]))\n    if (n < 0):\n        raise ValueError('Lie algebra rank cannot be negative')\n    if (letter == 'A'):\n        if (n >= 0):\n            from . import type_a\n            return type_a.TypeA(n)\n    if (letter == 'B'):\n        if (n >= 0):\n            from . import type_b\n            return type_b.TypeB(n)\n    if (letter == 'C'):\n        if (n >= 0):\n            from . import type_c\n            return type_c.TypeC(n)\n    if (letter == 'D'):\n        if (n >= 0):\n            from . import type_d\n            return type_d.TypeD(n)\n    if (letter == 'E'):\n        if ((n >= 6) and (n <= 8)):\n            from . import type_e\n            return type_e.TypeE(n)\n    if (letter == 'F'):\n        if (n == 4):\n            from . import type_f\n            return type_f.TypeF(n)\n    if (letter == 'G'):\n        if (n == 2):\n            from . import type_g\n            return type_g.TypeG(n)\n", "label": 1}
{"function": "\n\ndef tokens(self, event, next):\n    (kind, data, pos) = event\n    if (kind == START):\n        (tag, attribs) = data\n        name = tag.localname\n        namespace = tag.namespace\n        converted_attribs = {\n            \n        }\n        for (k, v) in attribs:\n            if isinstance(k, QName):\n                converted_attribs[(k.namespace, k.localname)] = v\n            else:\n                converted_attribs[(None, k)] = v\n        if ((namespace == namespaces['html']) and (name in voidElements)):\n            for token in self.emptyTag(namespace, name, converted_attribs, ((not next) or (next[0] != END) or (next[1] != tag))):\n                (yield token)\n        else:\n            (yield self.startTag(namespace, name, converted_attribs))\n    elif (kind == END):\n        name = data.localname\n        namespace = data.namespace\n        if (name not in voidElements):\n            (yield self.endTag(namespace, name))\n    elif (kind == COMMENT):\n        (yield self.comment(data))\n    elif (kind == TEXT):\n        for token in self.text(data):\n            (yield token)\n    elif (kind == DOCTYPE):\n        (yield self.doctype(*data))\n    elif (kind in (XML_NAMESPACE, DOCTYPE, START_NS, END_NS, START_CDATA, END_CDATA, PI)):\n        pass\n    else:\n        (yield self.unknown(kind))\n", "label": 1}
{"function": "\n\ndef diffRecursive(dir1, dir2):\n    done = set()\n    for filename in os.listdir(dir1):\n        path1 = os.path.join(dir1, filename)\n        path2 = os.path.join(dir2, filename)\n        done.add(path1)\n        if (filename.endswith('.o') or filename.endswith('.os') or filename.endswith('.obj')):\n            continue\n        if (filename == '.sconsign.dblite'):\n            continue\n        if (not os.path.exists(path2)):\n            sys.exit(('Only in %s: %s' % (dir1, filename)))\n        if os.path.isdir(path1):\n            diffRecursive(path1, path2)\n        elif os.path.isfile(path1):\n            fromdate = time.ctime(os.stat(path1).st_mtime)\n            todate = time.ctime(os.stat(path2).st_mtime)\n            diff = difflib.unified_diff(a=readSource(path1).splitlines(), b=readSource(path2).splitlines(), fromfile=path1, tofile=path2, fromfiledate=fromdate, tofiledate=todate, n=3)\n            result = list(diff)\n            if result:\n                for line in result:\n                    my_print(line)\n                sys.exit(1)\n        else:\n            assert False, path1\n    for filename in os.listdir(dir2):\n        path1 = os.path.join(dir1, filename)\n        path2 = os.path.join(dir2, filename)\n        if (path1 in done):\n            continue\n        if (not os.path.exists(path1)):\n            sys.exit(('Only in %s: %s' % (dir2, filename)))\n", "label": 1}
{"function": "\n\ndef reverse(viewname, urlconf=None, args=None, kwargs=None, current_app=None):\n    if (urlconf is None):\n        urlconf = get_urlconf()\n    resolver = get_resolver(urlconf)\n    args = (args or [])\n    kwargs = (kwargs or {\n        \n    })\n    prefix = get_script_prefix()\n    if (not isinstance(viewname, six.string_types)):\n        view = viewname\n    else:\n        parts = viewname.split(':')\n        parts.reverse()\n        view = parts[0]\n        path = parts[1:]\n        if current_app:\n            current_path = current_app.split(':')\n            current_path.reverse()\n        else:\n            current_path = None\n        resolved_path = []\n        ns_pattern = ''\n        while path:\n            ns = path.pop()\n            current_ns = (current_path.pop() if current_path else None)\n            try:\n                app_list = resolver.app_dict[ns]\n                if (current_ns and (current_ns in app_list)):\n                    ns = current_ns\n                elif (ns not in app_list):\n                    ns = app_list[0]\n            except KeyError:\n                pass\n            if (ns != current_ns):\n                current_path = None\n            try:\n                (extra, resolver) = resolver.namespace_dict[ns]\n                resolved_path.append(ns)\n                ns_pattern = (ns_pattern + extra)\n            except KeyError as key:\n                if resolved_path:\n                    raise NoReverseMatch((\"%s is not a registered namespace inside '%s'\" % (key, ':'.join(resolved_path))))\n                else:\n                    raise NoReverseMatch(('%s is not a registered namespace' % key))\n        if ns_pattern:\n            resolver = get_ns_resolver(ns_pattern, resolver)\n    return force_text(iri_to_uri(resolver._reverse_with_prefix(view, prefix, *args, **kwargs)))\n", "label": 1}
{"function": "\n\ndef merge(self):\n    trans = {\n        \n    }\n    for w in self.words:\n        trans[w] = ''\n    for w1 in self.words:\n        cw = 0\n        lw = len(w1)\n        for i in range(((len(self.doc) - lw) + 1)):\n            if (w1 == self.doc[i:(i + lw)]):\n                cw += 1\n        for w2 in self.words:\n            cnt = 0\n            l2 = (len(w1) + len(w2))\n            for i in range(((len(self.doc) - l2) + 1)):\n                if ((w1 + w2) == self.doc[i:(i + l2)]):\n                    cnt += 1\n            if (cw < (cnt * 2)):\n                trans[w1] = w2\n                break\n    ret = []\n    for w in self.words:\n        if (w not in trans):\n            continue\n        s = ''\n        now = trans[w]\n        while now:\n            s += now\n            if (now not in trans):\n                break\n            tmp = trans[now]\n            del trans[now]\n            now = tmp\n        trans[w] = s\n    for w in self.words:\n        if (w in trans):\n            ret.append((w + trans[w]))\n    return ret\n", "label": 1}
{"function": "\n\ndef objGetChildren(self, obj):\n    'Return dictionary with attributes or contents of object.'\n    otype = type(obj)\n    d = {\n        \n    }\n    if ((obj is None) or (obj is False) or (obj is True)):\n        return d\n    self.ntop = 0\n    if (isinstance(obj, SymbolTable) or isinstance(obj, Group)):\n        d = obj._members()\n    if isinstance(obj, COMMONTYPES):\n        d = obj\n    elif isinstance(obj, h5py.Group):\n        try:\n            for (key, val) in obj.items():\n                d[key] = val\n        except AttributeError:\n            pass\n    elif isinstance(obj, h5py.Dataset):\n        d = obj\n    elif isinstance(obj, (list, tuple)):\n        for n in range(len(obj)):\n            key = (('[' + str(n)) + ']')\n            d[key] = obj[n]\n    elif ((not isinstance(obj, wx.Object)) and (not hasattr(obj, '__call__'))):\n        d = self.GetAttr(obj)\n    return d\n", "label": 1}
{"function": "\n\ndef softmax_simplifier(numerators, denominators):\n    for numerator in list(numerators):\n        if (not numerator.type.dtype.startswith('float')):\n            continue\n        if (numerator.ndim != 2):\n            continue\n        if (numerator.owner and (numerator.owner.op == tensor.exp)):\n            x = numerator.owner.inputs[0]\n        else:\n            continue\n        matching_denom = None\n        for denominator in denominators:\n            if (denominator.owner and isinstance(denominator.owner.op, tensor.DimShuffle)):\n                if (denominator.owner.op.new_order == (0, 'x')):\n                    z = denominator.owner.inputs[0]\n                    if (z.owner and isinstance(z.owner.op, tensor.Sum)):\n                        if (z.owner.op.axis == (1,)):\n                            if (z.owner.inputs[0] is numerator):\n                                matching_denom = denominator\n                                break\n        if matching_denom:\n            numerators.remove(numerator)\n            denominators.remove(matching_denom)\n            numerators.append(softmax_op(x))\n    return (numerators, denominators)\n", "label": 1}
{"function": "\n\ndef _getAndCheckGeneticProfiles(self, genetic_profile_id=None, study=None):\n    study_id = self._getStudyId(study, None)\n    if (not genetic_profile_id):\n        if (not study_id):\n            raise ValueError('Either genetic_profile_id or study must be specified')\n        if (study_id == self.study):\n            genetic_profile_id = [x['genetic_profile_id'] for x in self.profiles if (x['show_profile_in_analysis_tab'] == 'true')]\n        else:\n            genetic_profile_id = [x['genetic_profile_id'] for x in self.getGeneticProfiles(study_id) if (x['show_profile_in_analysis_tab'] == 'true')]\n        return genetic_profile_id\n    else:\n        if (not study_id):\n            return genetic_profile_id\n        if (study_id == self.study):\n            genetic_profile_id = [x['genetic_profile_id'] for x in self.profiles if (x['genetic_profile_id'] in genetic_profile_id)]\n        else:\n            genetic_profile_id = [x['genetic_profile_id'] for x in self.getGeneticProfiles(study_id) if (x['genetic_profile_id'] in genetic_profile_id)]\n        if (len(genetic_profile_id) == 0):\n            raise ValueError('no valid genetic_profile_ids found')\n        return genetic_profile_id\n", "label": 1}
{"function": "\n\ndef test_init(self):\n    sys = System(self.kane)\n    assert (sys.constants_symbols == set(sm.symbols('k0, m0, g, c0')))\n    assert (sys.specifieds_symbols == {self.specified_symbol})\n    assert (sys.states == me.dynamicsymbols('x0, v0'))\n    assert (sys.evaluate_ode_function is None)\n    assert (sys.eom_method is self.kane)\n    assert (sys.ode_solver is odeint)\n    assert (sys.specifieds == dict())\n    assert (sys.initial_conditions == dict())\n    assert (sys.constants == dict())\n    assert (sys.times == list())\n    ic = {\n        me.dynamicsymbols('x0'): 3.6,\n        me.dynamicsymbols('v0'): 4.3,\n    }\n    sys = System(self.kane, ode_solver=odeint, specifieds={\n        self.specified_symbol: np.ones(1),\n    }, initial_conditions=ic, constants=self.constant_map)\n    assert (sys.eom_method is self.kane)\n    assert (list(sys.specifieds.keys()) == [me.dynamicsymbols('f0')])\n    testing.assert_allclose(list(sys.specifieds.values()), [np.ones(1)])\n    assert (sys.initial_conditions.keys() == ic.keys())\n    testing.assert_allclose(list(sys.initial_conditions.values()), list(ic.values()))\n    assert (sys.constants.keys() == self.constant_map.keys())\n    testing.assert_allclose(list(sys.constants.values()), list(self.constant_map.values()))\n    sys = System(self.kane, ode_solver=odeint, specifieds={\n        'symbols': [self.specified_symbol],\n        'values': np.ones(1),\n    }, initial_conditions=ic, constants=self.constant_map)\n", "label": 1}
{"function": "\n\ndef tokensMatch(expectedTokens, receivedTokens, ignoreErrorOrder, ignoreErrors=False):\n    \"Test whether the test has passed or failed\\n\\n    If the ignoreErrorOrder flag is set to true we don't test the relative\\n    positions of parse errors and non parse errors\\n    \"\n    checkSelfClosing = False\n    for token in expectedTokens:\n        if (((token[0] == 'StartTag') and (len(token) == 4)) or ((token[0] == 'EndTag') and (len(token) == 3))):\n            checkSelfClosing = True\n            break\n    if (not checkSelfClosing):\n        for token in receivedTokens:\n            if ((token[0] == 'StartTag') or (token[0] == 'EndTag')):\n                token.pop()\n    if ((not ignoreErrorOrder) and (not ignoreErrors)):\n        return (expectedTokens == receivedTokens)\n    else:\n        tokens = {\n            'expected': [[], []],\n            'received': [[], []],\n        }\n        for (tokenType, tokenList) in zip(tokens.keys(), (expectedTokens, receivedTokens)):\n            for token in tokenList:\n                if (token != 'ParseError'):\n                    tokens[tokenType][0].append(token)\n                elif (not ignoreErrors):\n                    tokens[tokenType][1].append(token)\n        return (tokens['expected'] == tokens['received'])\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    template_base = kwargs.pop('template_base', None)\n    if template_base:\n        templates = {\n            'subject': (template_base + '.subject'),\n            'plain': (template_base + '.plain'),\n            'html': (template_base + '.html'),\n        }\n        for (key, value) in templates.items():\n            try:\n                templates[key] = get_template(value)\n            except TemplateDoesNotExist:\n                templates[key] = None\n    else:\n        templates = {\n            \n        }\n    language = kwargs.pop('language', None)\n    old_language = get_language()\n    if language:\n        activate(language)\n    context = kwargs.pop('context', {\n        \n    })\n    super(EmailMessage, self).__init__(*args, **kwargs)\n    if ('body' not in context):\n        context['body'] = self.body\n    if ('subject' not in context):\n        context['subject'] = self.subject\n    if (('subject' in templates) and templates['subject']):\n        self.subject = templates['subject'].render(context).strip()\n    if (('plain' in templates) and templates['plain']):\n        self.body = templates['plain'].render(context).strip()\n    if (('html' in templates) and templates['html']):\n        html = templates['html'].render(context).strip()\n        if html:\n            if (not self.body):\n                self.content_subtype = 'html'\n                self.body = html\n            else:\n                self.attach_alternative(html, 'text/html')\n    activate(old_language)\n", "label": 1}
{"function": "\n\ndef generate_key(func_name, args, dict_args_original, skip_args):\n    args_concat = [v for (key, v) in sorted(dict_args_original.iteritems()) if (key not in skip_args)]\n    args_serialized = '_'.join(sorted([(v.__name__ if hasattr(v, '__call__') else (str(v) if (len(str(v)) < 200) else hashlib.md5(str(v)).hexdigest())) for v in args_concat if (hasattr(v, '__call__') or (str(v).find('0x') == (- 1)))]))\n    logger.info(('Serialized args to ' + args_serialized))\n    key = ((func_name + '_') + ''.join((a for a in args_serialized if (a.isalnum() or (a in '!@#$%^&**_+-')))))\n    full_key = ((func_name + '(') + ''.join([((str(k) + '=') + (str(v) if (len(str(v)) < 200) else hashlib.md5(str(v)).hexdigest())) for (k, v) in sorted(dict_args_original.iteritems()) if (key not in skip_args)]))\n    if (len(key) > 400):\n        key = key[0:400]\n    return (key, full_key)\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    try:\n        result = self.index.get_value(self, key)\n        if (not lib.isscalar(result)):\n            if (is_list_like(result) and (not isinstance(result, Series))):\n                if (not self.index.is_unique):\n                    result = self._constructor(result, index=([key] * len(result)), dtype=self.dtype).__finalize__(self)\n        return result\n    except InvalidIndexError:\n        pass\n    except (KeyError, ValueError):\n        if (isinstance(key, tuple) and isinstance(self.index, MultiIndex)):\n            pass\n        elif (key is Ellipsis):\n            return self\n        elif is_bool_indexer(key):\n            pass\n        else:\n            new_key = self.index._convert_scalar_indexer(key, kind='getitem')\n            if (type(new_key) != type(key)):\n                return self.__getitem__(new_key)\n            raise\n    except Exception:\n        raise\n    if com.is_iterator(key):\n        key = list(key)\n    if is_bool_indexer(key):\n        key = check_bool_indexer(self.index, key)\n    return self._get_with(key)\n", "label": 1}
{"function": "\n\ndef queue_events(self, timeout):\n    with self._lock:\n        inotify_events = self._inotify.read_events()\n        if (not any([(event.is_moved_from or event.is_moved_to) for event in inotify_events])):\n            self._inotify.clear_move_records()\n        for event in inotify_events:\n            if event.is_moved_to:\n                try:\n                    src_path = self._inotify.source_for_move(event)\n                    to_event = event\n                    dest_path = to_event.src_path\n                    klass = ACTION_EVENT_MAP[(to_event.is_directory, EVENT_TYPE_MOVED)]\n                    event = klass(src_path, dest_path)\n                    self.queue_event(event)\n                    if (event.is_directory and self.watch.is_recursive):\n                        for sub_event in event.sub_moved_events():\n                            self.queue_event(sub_event)\n                except KeyError:\n                    pass\n            elif event.is_attrib:\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_MODIFIED)]\n                self.queue_event(klass(event.src_path))\n            elif event.is_close_write:\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_MODIFIED)]\n                self.queue_event(klass(event.src_path))\n            elif event.is_modify:\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_MODIFIED)]\n                self.queue_event(klass(event.src_path))\n            elif (event.is_delete or event.is_delete_self):\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_DELETED)]\n                self.queue_event(klass(event.src_path))\n            elif event.is_create:\n                klass = ACTION_EVENT_MAP[(event.is_directory, EVENT_TYPE_CREATED)]\n                self.queue_event(klass(event.src_path))\n", "label": 1}
{"function": "\n\ndef legalize_return_type(return_type, interp, targetctx):\n    '\\n    Only accept array return type iff it is passed into the function.\\n    Reject function object return types if in nopython mode.\\n    '\n    if ((not targetctx.enable_nrt) and isinstance(return_type, types.Array)):\n        retstmts = []\n        caststmts = {\n            \n        }\n        argvars = set()\n        for (bid, blk) in interp.blocks.items():\n            for inst in blk.body:\n                if isinstance(inst, ir.Return):\n                    retstmts.append(inst.value.name)\n                elif isinstance(inst, ir.Assign):\n                    if (isinstance(inst.value, ir.Expr) and (inst.value.op == 'cast')):\n                        caststmts[inst.target.name] = inst.value\n                    elif isinstance(inst.value, ir.Arg):\n                        argvars.add(inst.target.name)\n        assert retstmts, 'No return statements?'\n        for var in retstmts:\n            cast = caststmts.get(var)\n            if ((cast is None) or (cast.value.name not in argvars)):\n                raise TypeError('Only accept returning of array passed into the function as argument')\n    elif (isinstance(return_type, types.Function) or isinstance(return_type, types.Phantom)):\n        raise TypeError(\"Can't return function object in nopython mode\")\n", "label": 1}
{"function": "\n\ndef fitness_and_quality_parsed(mime_type, parsed_ranges):\n    \"Find the best match for a given mime-type against \\n       a list of media_ranges that have already been \\n       parsed by parse_media_range(). Returns a tuple of\\n       the fitness value and the value of the 'q' quality\\n       parameter of the best match, or (-1, 0) if no match\\n       was found. Just as for quality_parsed(), 'parsed_ranges'\\n       must be a list of parsed media ranges. \"\n    best_fitness = (- 1)\n    best_fit_q = 0\n    (target_type, target_subtype, target_params) = parse_media_range(mime_type)\n    for (type, subtype, params) in parsed_ranges:\n        if (((type == target_type) or (type == '*') or (target_type == '*')) and ((subtype == target_subtype) or (subtype == '*') or (target_subtype == '*'))):\n            param_matches = reduce((lambda x, y: (x + y)), [1 for (key, value) in target_params.iteritems() if ((key != 'q') and params.has_key(key) and (value == params[key]))], 0)\n            fitness = (((type == target_type) and 100) or 0)\n            fitness += (((subtype == target_subtype) and 10) or 0)\n            fitness += param_matches\n            if (fitness > best_fitness):\n                best_fitness = fitness\n                best_fit_q = params['q']\n    return (best_fitness, float(best_fit_q))\n", "label": 1}
{"function": "\n\ndef test_entity_version():\n    entity = File(parent=project['id'])\n    entity['path'] = utils.make_bogus_data_file()\n    schedule_for_cleanup(entity['path'])\n    entity = syn.createEntity(entity)\n    syn.setAnnotations(entity, {\n        'fizzbuzz': 111222,\n    })\n    entity = syn.getEntity(entity)\n    assert (entity.versionNumber == 1)\n    entity.foo = 998877\n    entity['name'] = 'foobarbat'\n    entity['description'] = 'This is a test entity...'\n    entity = syn.updateEntity(entity, incrementVersion=True, versionLabel='Prada remix')\n    assert (entity.versionNumber == 2)\n    annotations = syn.getAnnotations(entity, version=1)\n    assert (annotations['fizzbuzz'][0] == 111222)\n    returnEntity = syn.getEntity(entity, version=1)\n    assert (returnEntity.versionNumber == 1)\n    assert (returnEntity['fizzbuzz'][0] == 111222)\n    assert ('foo' not in returnEntity)\n    returnEntity = syn.getEntity(entity)\n    assert (returnEntity.versionNumber == 2)\n    assert (returnEntity['foo'][0] == 998877)\n    assert (returnEntity['name'] == 'foobarbat')\n    assert (returnEntity['description'] == 'This is a test entity...')\n    assert (returnEntity['versionLabel'] == 'Prada remix')\n    returnEntity = syn.downloadEntity(entity, version=1)\n    assert (returnEntity.versionNumber == 1)\n    assert (returnEntity['fizzbuzz'][0] == 111222)\n    assert ('foo' not in returnEntity)\n    syn.delete(entity, version=2)\n    returnEntity = syn.getEntity(entity)\n    assert (returnEntity.versionNumber == 1)\n", "label": 1}
{"function": "\n\ndef _possibly_convert_objects(values, convert_dates=True, convert_numeric=True, convert_timedeltas=True, copy=True):\n    ' if we have an object dtype, try to coerce dates and/or numbers '\n    if isinstance(values, (list, tuple)):\n        values = np.array(values, dtype=np.object_)\n    if (not hasattr(values, 'dtype')):\n        values = np.array([values], dtype=np.object_)\n    if (convert_dates and (values.dtype == np.object_)):\n        if (convert_dates == 'coerce'):\n            new_values = _possibly_cast_to_datetime(values, 'M8[ns]', errors='coerce')\n            if (not isnull(new_values).all()):\n                values = new_values\n        else:\n            values = lib.maybe_convert_objects(values, convert_datetime=convert_dates)\n    if (convert_timedeltas and (values.dtype == np.object_)):\n        if (convert_timedeltas == 'coerce'):\n            from pandas.tseries.timedeltas import to_timedelta\n            new_values = to_timedelta(values, coerce=True)\n            if (not isnull(new_values).all()):\n                values = new_values\n        else:\n            values = lib.maybe_convert_objects(values, convert_timedelta=convert_timedeltas)\n    if (values.dtype == np.object_):\n        if convert_numeric:\n            try:\n                new_values = lib.maybe_convert_numeric(values, set(), coerce_numeric=True)\n                if (not isnull(new_values).all()):\n                    values = new_values\n            except:\n                pass\n        else:\n            values = lib.maybe_convert_objects(values)\n    values = (values.copy() if copy else values)\n    return values\n", "label": 1}
{"function": "\n\ndef CalculateReducedDataFittingTarget(self, inCoeffs):\n    if (not self.AreCoefficientsWithinBounds(inCoeffs)):\n        try:\n            if (self.upperCoefficientBounds != []):\n                for i in range(len(inCoeffs)):\n                    if (self.upperCoefficientBounds[i] != None):\n                        if (inCoeffs[i] > self.upperCoefficientBounds[i]):\n                            inCoeffs[i] = self.upperCoefficientBounds[i]\n            if (self.lowerCoefficientBounds != []):\n                for i in range(len(inCoeffs)):\n                    if (self.lowerCoefficientBounds[i] != None):\n                        if (inCoeffs[i] < self.lowerCoefficientBounds[i]):\n                            inCoeffs[i] = self.lowerCoefficientBounds[i]\n        except:\n            pass\n    try:\n        if (self.fixedCoefficients != []):\n            for i in range(len(inCoeffs)):\n                if (self.fixedCoefficients[i] != None):\n                    inCoeffs[i] = self.fixedCoefficients[i]\n        error = (self.CalculateModelPredictions(inCoeffs, self.dataCache.reducedDataCacheDictionary) - self.dataCache.reducedDataCacheDictionary['DependentData'])\n        ssq = numpy.sum(numpy.square(error))\n    except:\n        return 1e+300\n    if numpy.isfinite(ssq):\n        return ssq\n    else:\n        return 1e+300\n", "label": 1}
{"function": "\n\ndef get_request_choices(self, request, tbl):\n    '\\n        Return a list of choices for this chooser,\\n        using a HttpRequest to build the context.\\n        '\n    from django.contrib.contenttypes.models import ContentType\n    kw = {\n        \n    }\n    if (tbl.master_field is not None):\n        rqdata = getrqdata(request)\n        if (tbl.master is not None):\n            master = tbl.master\n        else:\n            mt = rqdata.get(constants.URL_PARAM_MASTER_TYPE)\n            try:\n                master = ContentType.objects.get(pk=mt).model_class()\n            except ContentType.DoesNotExist:\n                master = None\n        pk = rqdata.get(constants.URL_PARAM_MASTER_PK, None)\n        if (pk and master):\n            try:\n                kw[tbl.master_field.name] = master.objects.get(pk=pk)\n            except ValueError:\n                raise Exception('Invalid primary key %r for %s', pk, master.__name__)\n            except master.DoesNotExist:\n                raise Exception((\"There's no %s with primary key %r\" % (master.__name__, pk)))\n    for (k, v) in list(request.GET.items()):\n        kw[str(k)] = v\n    for cv in self.converters:\n        kw = cv.convert(**kw)\n    if tbl.known_values:\n        kw.update(tbl.known_values)\n    if False:\n        if ar.create_kw:\n            kw.update(ar.create_kw)\n        if ar.known_values:\n            kw.update(ar.known_values)\n        if tbl.master_key:\n            kw[tbl.master_key] = ar.master_instance\n    return self.get_choices(**kw)\n", "label": 1}
{"function": "\n\ndef stop(self, timeout=5):\n    while (self._get_qsize() > 0):\n        conn = self.get()\n        if (conn is not _SHUTDOWNREQUEST):\n            conn.close()\n    for worker in self._threads:\n        self._queue.put(_SHUTDOWNREQUEST)\n    current = threading.currentThread()\n    if (timeout and (timeout >= 0)):\n        endtime = (time.time() + timeout)\n    while self._threads:\n        worker = self._threads.pop()\n        if ((worker is not current) and worker.isAlive()):\n            try:\n                if ((timeout is None) or (timeout < 0)):\n                    worker.join()\n                else:\n                    remaining_time = (endtime - time.time())\n                    if (remaining_time > 0):\n                        worker.join(remaining_time)\n                    if worker.isAlive():\n                        c = worker.conn\n                        if (c and (not c.rfile.closed)):\n                            try:\n                                c.socket.shutdown(socket.SHUT_RD)\n                            except TypeError:\n                                c.socket.shutdown()\n                        worker.join()\n            except (AssertionError, KeyboardInterrupt):\n                pass\n", "label": 1}
{"function": "\n\ndef _write_message(self, notification, devices, chunk_size):\n    '\\n        Writes the message for the supplied devices to\\n        the APN Service SSL socket.\\n        '\n    if (not isinstance(notification, Notification)):\n        raise TypeError('notification should be an instance of ios_notifications.models.Notification')\n    if ((not isinstance(chunk_size, int)) or (chunk_size < 1)):\n        raise ValueError('chunk_size must be an integer greater than zero.')\n    payload = notification.payload\n    device_length = (devices.count() if isinstance(devices, models.query.QuerySet) else len(devices))\n    chunks = [devices[i:(i + chunk_size)] for i in xrange(0, device_length, chunk_size)]\n    for index in xrange(len(chunks)):\n        chunk = chunks[index]\n        self._connect()\n        for device in chunk:\n            if (not device.is_active):\n                continue\n            try:\n                self.connection.send(self.pack_message(payload, device))\n            except (OpenSSL.SSL.WantWriteError, socket.error) as e:\n                if (isinstance(e, socket.error) and isinstance(e.args, tuple) and (e.args[0] != errno.EPIPE)):\n                    raise e\n                self._disconnect()\n                i = chunk.index(device)\n                self.set_devices_last_notified_at(chunk[:i])\n                self._write_message(notification, chunk[(i + 1):], chunk_size)\n        self._disconnect()\n        self.set_devices_last_notified_at(chunk)\n    if (notification.pk or notification.persist):\n        notification.last_sent_at = dt_now()\n        notification.save()\n", "label": 1}
{"function": "\n\ndef AdjustLabels(self, axis, minimum_label_spacing):\n    if (minimum_label_spacing is None):\n        return\n    if (len(axis.labels) <= 1):\n        return\n    if ((axis.max is not None) and (axis.min is not None)):\n        maximum_possible_spacing = ((axis.max - axis.min) / (len(axis.labels) - 1))\n        if (minimum_label_spacing > maximum_possible_spacing):\n            minimum_label_spacing = maximum_possible_spacing\n    labels = [list(x) for x in zip(axis.label_positions, axis.labels)]\n    labels = sorted(labels, reverse=True)\n    for i in range(1, len(labels)):\n        if ((labels[(i - 1)][0] - labels[i][0]) < minimum_label_spacing):\n            new_position = (labels[(i - 1)][0] - minimum_label_spacing)\n            if ((axis.min is not None) and (new_position < axis.min)):\n                new_position = axis.min\n            labels[i][0] = new_position\n    for i in range((len(labels) - 2), (- 1), (- 1)):\n        if ((labels[i][0] - labels[(i + 1)][0]) < minimum_label_spacing):\n            new_position = (labels[(i + 1)][0] + minimum_label_spacing)\n            if ((axis.max is not None) and (new_position > axis.max)):\n                new_position = axis.max\n            labels[i][0] = new_position\n    (label_positions, labels) = zip(*labels)\n    axis.labels = labels\n    axis.label_positions = label_positions\n", "label": 1}
{"function": "\n\ndef _render_email(self, template_name, context, template_dir=None, file_extension=None):\n    response = {\n        \n    }\n    errors = {\n        \n    }\n    prefixed_template_name = ''.join(((template_dir or self.template_prefix), template_name))\n    render_context = Context(context, autoescape=False)\n    file_extension = (file_extension or self.template_suffix)\n    if file_extension.startswith('.'):\n        file_extension = file_extension[1:]\n    full_template_name = prefixed_template_name\n    if (not prefixed_template_name.endswith(('.%s' % file_extension))):\n        full_template_name = ('%s.%s' % (prefixed_template_name, file_extension))\n    try:\n        multi_part = get_template(full_template_name)\n    except TemplateDoesNotExist:\n        multi_part = None\n    if multi_part:\n        for part in ['subject', 'html', 'plain']:\n            try:\n                response[part] = _get_node(multi_part, render_context, name=part)\n            except BlockNotFound as error:\n                errors[part] = error\n    else:\n        try:\n            html_part = get_template(('%s.html' % prefixed_template_name))\n        except TemplateDoesNotExist:\n            html_part = None\n        try:\n            plain_part = get_template(('%s.txt' % prefixed_template_name))\n        except TemplateDoesNotExist:\n            if (not html_part):\n                raise TemplateDoesNotExist(full_template_name)\n            else:\n                plain_part = None\n        if plain_part:\n            response['plain'] = plain_part.render(render_context)\n        if html_part:\n            response['html'] = html_part.render(render_context)\n    if (response == {\n        \n    }):\n        raise EmailRenderException((\"Couldn't render email parts. Errors: %s\" % errors))\n    return response\n", "label": 1}
{"function": "\n\ndef _handlewebError(self, msg):\n    print('')\n    print(('    ERROR: %s' % msg))\n    if (not self.interactive):\n        raise self.failureException(msg)\n    p = '    Show: [B]ody [H]eaders [S]tatus [U]RL; [I]gnore, [R]aise, or sys.e[X]it >> '\n    sys.stdout.write(p)\n    sys.stdout.flush()\n    while True:\n        i = getchar().upper()\n        if (not isinstance(i, type(''))):\n            i = i.decode('ascii')\n        if (i not in 'BHSUIRX'):\n            continue\n        print(i.upper())\n        if (i == 'B'):\n            for (x, line) in enumerate(self.body.splitlines()):\n                if (((x + 1) % self.console_height) == 0):\n                    sys.stdout.write('<-- More -->\\r')\n                    m = getchar().lower()\n                    sys.stdout.write('            \\r')\n                    if (m == 'q'):\n                        break\n                print(line)\n        elif (i == 'H'):\n            pprint.pprint(self.headers)\n        elif (i == 'S'):\n            print(self.status)\n        elif (i == 'U'):\n            print(self.url)\n        elif (i == 'I'):\n            return\n        elif (i == 'R'):\n            raise self.failureException(msg)\n        elif (i == 'X'):\n            self.exit()\n        sys.stdout.write(p)\n        sys.stdout.flush()\n", "label": 1}
{"function": "\n\ndef fix_unicode(self, token):\n    if (token.type == 'identifier'):\n        if ((token.text == 'chr') and (token.next_char == '(')):\n            token.fix = 'unichr'\n        elif ((token.text == 'str') and (token.next_char == '(')):\n            token.fix = 'unicode'\n        elif ((token.text == 'str') and ((token.next_char == ')') and (token.prev_char == '(') and (token.line_tokens[0].text == 'class'))):\n            token.fix = 'unicode'\n        elif ((token.text == 'isinstance') and (token.next_char == '(')):\n            end = token.find_forward(')')\n            t = token.next_token\n            while (t.next_token and (t.next_token.start < end)):\n                t = t.next_token\n                if (t.text == 'str'):\n                    t.fix = 'basestring'\n", "label": 1}
{"function": "\n\ndef test_latex_symbols():\n    (Gamma, lmbda, rho) = symbols('Gamma, lambda, rho')\n    (mass, volume) = symbols('mass, volume')\n    assert (latex((Gamma + lmbda)) == '\\\\Gamma + \\\\lambda')\n    assert (latex((Gamma * lmbda)) == '\\\\Gamma \\\\lambda')\n    assert (latex(Symbol('q1')) == 'q_{1}')\n    assert (latex(Symbol('q21')) == 'q_{21}')\n    assert (latex(Symbol('epsilon0')) == '\\\\epsilon_{0}')\n    assert (latex(Symbol('omega1')) == '\\\\omega_{1}')\n    assert (latex(Symbol('91')) == '91')\n    assert (latex(Symbol('alpha_new')) == '\\\\alpha_{new}')\n    assert (latex(Symbol('C^orig')) == 'C^{orig}')\n    assert (latex(Symbol('x^alpha')) == 'x^{\\\\alpha}')\n    assert (latex(Symbol('beta^alpha')) == '\\\\beta^{\\\\alpha}')\n    assert (latex(Symbol('e^Alpha')) == 'e^{A}')\n    assert (latex(Symbol('omega_alpha^beta')) == '\\\\omega^{\\\\beta}_{\\\\alpha}')\n    assert (latex((Symbol('omega') ** Symbol('beta'))) == '\\\\omega^{\\\\beta}')\n", "label": 1}
{"function": "\n\ndef test_can_load_object_with_relationships(self):\n    alice_node = self.index_manager.get_or_create_indexed_node('People', 'email', 'alice@example.com', {\n        'email': 'alice@example.com',\n        'name': 'Alice Allison',\n        'age': 34,\n    })\n    path = Path(alice_node, 'LIKES', {\n        'name': 'Bob Robertson',\n    })\n    self.graph.create(path)\n    bob_node = path.nodes()[1]\n    alice = self.store.load_unique('People', 'email', 'alice@example.com', Person)\n    assert isinstance(alice, Person)\n    assert hasattr(alice, '__node__')\n    assert (alice.__node__ == alice_node)\n    assert hasattr(alice, '__rel__')\n    assert (alice.__rel__ == {\n        'LIKES': [({\n            \n        }, bob_node)],\n    })\n    assert (alice.email == 'alice@example.com')\n    assert (alice.name == 'Alice Allison')\n    assert (alice.age == 34)\n    friends = self.store.load_related(alice, 'LIKES', Person)\n    assert isinstance(friends, list)\n    assert (len(friends) == 1)\n    friend = friends[0]\n    assert isinstance(friend, Person)\n    assert (friend.__node__ == bob_node)\n    enemies = self.store.load_related(alice, 'DISLIKES', Person)\n    assert isinstance(enemies, list)\n    assert (len(enemies) == 0)\n", "label": 1}
{"function": "\n\ndef test_barrel_list():\n    bl = BarrelList()\n    bl.insert(0, 0)\n    assert (bl[0] == 0)\n    assert (len(bl) == 1)\n    bl.insert(1, 1)\n    assert (list(bl) == [0, 1])\n    bl.insert(0, (- 1))\n    assert (list(bl) == [(- 1), 0, 1])\n    bl.extend(range(int(100000.0)))\n    assert (len(bl) == (100000.0 + 3))\n    bl._balance_list(0)\n    assert (len(bl) == (100000.0 + 3))\n    bl.pop(50000)\n    assert (len(bl) == ((100000.0 + 3) - 1))\n    bl2 = BarrelList(TEST_INTS)\n    bl2.sort()\n    assert (list(bl2[:5]) == [0, 74, 80, 96, 150])\n    assert (list(bl2[:(- 5):(- 1)]) == [50508, 46607, 46428, 43442])\n    bl3 = BarrelList(range(int(100000.0)))\n    for i in range(10000):\n        bl3.insert(0, bl3.pop((len(bl3) // 2)))\n    assert (len(bl3) == 100000.0)\n    assert (bl3[0] == 40001)\n    assert (bl3[(- 1)] == sorted(bl3)[(- 1)])\n    del bl3[10:5000]\n    assert (bl3[0] == 40001)\n    assert (len(bl3) == (100000.0 - (5000 - 10)))\n    bl3[:20:2] = range(0, (- 10), (- 1))\n    assert (bl3[6] == (- 3))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef layout(item):\n    ' Custom Layout Method '\n    if (not item.authorized):\n        enabled = False\n        visible = False\n    elif ((item.enabled is None) or item.enabled):\n        enabled = True\n        visible = True\n    if (enabled and visible):\n        if (item.parent is not None):\n            if (item.enabled and item.authorized):\n                if item.components:\n                    _class = ''\n                    if ((item.parent.parent is None) and item.selected):\n                        _class = 'highlight'\n                    items = item.render_components()\n                    if items:\n                        items = LI(UL(items, _class='menu-extention'))\n                    return [LI(A(item.label, _href=item.url(), _id=item.attr._id, _class=_class)), items]\n                else:\n                    if (item.parent.parent is None):\n                        _class = ((item.selected and 'highlight') or '')\n                    else:\n                        _class = ' '\n                    return LI(A(item.label, _href=item.url(), _id=item.attr._id, _class=_class))\n        else:\n            items = item.render_components()\n            return UL(items, _id='main-sub-menu', _class='sub-menu')\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef _remove_nop_artifact(self, offset):\n    for (src, dst) in self.computed_jump:\n        if ((src < offset < dst) or (dst < offset < src)):\n            old_jmp = self.instrs[src]\n            old_jump_size = len(old_jmp.get_code())\n            if (src < offset < dst):\n                new_jmp = type(old_jmp)(((dst - src) - 1))\n            else:\n                new_jmp = type(old_jmp)(((dst - src) + 1))\n            new_jmp_size = len(new_jmp.get_code())\n            if (new_jmp_size > old_jump_size):\n                raise ValueError('Wtf jump of smaller size is bigger.. ABORT')\n            self.instrs[src] = new_jmp\n            for i in range((old_jump_size - new_jmp_size)):\n                self.instrs[((src + new_jmp_size) + i)] = _NopArtifact()\n    for (name, labeloffset) in self.labels.items():\n        if (labeloffset > offset):\n            self.labels[name] = (labeloffset - 1)\n    new_instr = {\n        \n    }\n    for (instroffset, instr) in self.instrs.items():\n        if (instroffset == offset):\n            continue\n        if (instroffset > offset):\n            instroffset -= 1\n        new_instr[instroffset] = instr\n    self.instrs = new_instr\n    new_computed_jump = []\n    for (src, dst) in self.computed_jump:\n        if (src > offset):\n            src -= 1\n        if (dst > offset):\n            dst -= 1\n        new_computed_jump.append((src, dst))\n    self.computed_jump = new_computed_jump\n    self.size -= 1\n", "label": 1}
{"function": "\n\ndef colored(string, fg=None, bg=None, bold_for_light_color=True):\n    result = ''\n    for (colorstring, is_bg) in ((fg, False), (bg, True)):\n        if colorstring:\n            color = '\\x1b['\n            if (colorstring in COLORS):\n                if (not is_bg):\n                    c = (30 + COLORS[colorstring].index)\n                    if COLORS[colorstring].light:\n                        if bold_for_light_color:\n                            color += '1;'\n                        else:\n                            c += 60\n                else:\n                    c = (40 + COLORS[colorstring].index)\n                    if COLORS[colorstring].light:\n                        if (not bold_for_light_color):\n                            c += 60\n                color += str(c)\n            elif colorstring.isdigit():\n                if (not is_bg):\n                    color += ('38;5;' + colorstring)\n                else:\n                    color += ('48;5;' + colorstring)\n            else:\n                if (len(colorstring) == 4):\n                    r = int((colorstring[1] * 2), 16)\n                    g = int((colorstring[2] * 2), 16)\n                    b = int((colorstring[3] * 2), 16)\n                else:\n                    r = int(colorstring[1:3], 16)\n                    g = int(colorstring[3:5], 16)\n                    b = int(colorstring[5:7], 16)\n                if (not is_bg):\n                    color += '38;2;{!s};{!s};{!s}'.format(r, g, b)\n                else:\n                    color += '48;2;{!s};{!s};{!s}'.format(r, g, b)\n            color += 'm'\n            result += color\n    result += string\n    if (fg or bg):\n        result += RESET\n    return result\n", "label": 1}
{"function": "\n\ndef load_middleware(self):\n    self.ignore_keywords = ['reversion.middleware', 'MaintenanceModeMiddleware']\n    super(LinkCheckHandler, self).load_middleware()\n    new_request_middleware = []\n    for method in self._request_middleware:\n        ignored = False\n        for keyword in self.ignore_keywords:\n            if method.__str__().count(keyword):\n                ignored = True\n                break\n        if (not ignored):\n            new_request_middleware.append(method)\n    self._request_middleware = new_request_middleware\n    new_view_middleware = []\n    for method in self._view_middleware:\n        ignored = False\n        for keyword in self.ignore_keywords:\n            if method.__str__().count(keyword):\n                ignored = True\n                break\n        if (not ignored):\n            new_view_middleware.append(method)\n    self._view_middleware = new_view_middleware\n    new_response_middleware = []\n    for method in self._response_middleware:\n        ignored = False\n        for keyword in self.ignore_keywords:\n            if method.__str__().count(keyword):\n                ignored = True\n                break\n        if (not ignored):\n            new_response_middleware.append(method)\n    self._response_middleware = new_response_middleware\n    new_exception_middleware = []\n    for method in self._exception_middleware:\n        ignored = False\n        for keyword in self.ignore_keywords:\n            if method.__str__().count(keyword):\n                ignored = True\n                break\n        if (not ignored):\n            new_exception_middleware.append(method)\n    self._exception_middleware = new_exception_middleware\n", "label": 1}
{"function": "\n\ndef delete_file(self, path, prefixed_path, source_storage):\n    '\\n        Checks if the target file should be deleted if it already exists\\n        '\n    if self.storage.exists(prefixed_path):\n        try:\n            target_last_modified = self.storage.modified_time(prefixed_path)\n        except (OSError, NotImplementedError, AttributeError):\n            pass\n        else:\n            try:\n                source_last_modified = source_storage.modified_time(path)\n            except (OSError, NotImplementedError, AttributeError):\n                pass\n            else:\n                if self.local:\n                    full_path = self.storage.path(prefixed_path)\n                else:\n                    full_path = None\n                if (target_last_modified >= source_last_modified):\n                    if (not ((self.symlink and full_path and (not os.path.islink(full_path))) or ((not self.symlink) and full_path and os.path.islink(full_path)))):\n                        if (prefixed_path not in self.unmodified_files):\n                            self.unmodified_files.append(prefixed_path)\n                        self.log((\"Skipping '%s' (not modified)\" % path))\n                        return False\n        if self.dry_run:\n            self.log((\"Pretending to delete '%s'\" % path))\n        else:\n            self.log((\"Deleting '%s'\" % path))\n            self.storage.delete(prefixed_path)\n    return True\n", "label": 1}
{"function": "\n\ndef assert_common_signed_properties(self, info):\n    assert ('Executable' in info)\n    assert ('Identifier' in info)\n    assert ('CodeDirectory' in info)\n    codedirectory_info = info['CodeDirectory'][0]\n    assert (codedirectory_info['location'] == 'embedded')\n    assert ('Hash' in info)\n    hashes = self.get_dict_with_key(info['Hash'], '_')\n    assert (hashes is not None)\n    assert ('CDHash' in info)\n    assert ('Signature' in info)\n    assert ('Authority' in info)\n    assert ('Info.plist' in info)\n    assert (self.get_dict_with_key(info['Info.plist'], 'entries') is not None)\n    assert ('TeamIdentifier' in info)\n    assert (info['TeamIdentifier'][0] == self.OU)\n    assert ('designated' in info)\n    assert ('anchor apple generic' in info['designated'][0])\n    assert (self.ERROR_KEY not in info)\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    _get_fds_service_config(args)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.fds_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                stop_job(args, hosts[host_id].ip, job_name, instance_id)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.fds_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                deploy_utils.wait_for_job_stopping('fds', args.fds_config.cluster.name, job_name, hosts[host_id].ip, instance_id)\n                start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)\n", "label": 1}
{"function": "\n\ndef should_be_compact_paragraph(self, node):\n    '\\n        Determine if the <p> tags around paragraph ``node`` can be omitted.\\n        '\n    if (isinstance(node.parent, nodes.document) or isinstance(node.parent, nodes.compound)):\n        return False\n    for (key, value) in node.attlist():\n        if (node.is_not_default(key) and (not ((key == 'classes') and (value in ([], ['first'], ['last'], ['first', 'last']))))):\n            return False\n    first = isinstance(node.parent[0], nodes.label)\n    for child in node.parent.children[first:]:\n        if isinstance(child, nodes.Invisible):\n            continue\n        if (child is node):\n            break\n        return False\n    parent_length = len([n for n in node.parent if (not isinstance(n, (nodes.Invisible, nodes.label)))])\n    if (self.compact_simple or self.compact_field_list or (self.compact_p and (parent_length == 1))):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef _get_with(self, key):\n    if isinstance(key, slice):\n        indexer = self.index._convert_slice_indexer(key, kind='getitem')\n        return self._get_values(indexer)\n    elif isinstance(key, ABCDataFrame):\n        raise TypeError('Indexing a Series with DataFrame is not supported, use the appropriate DataFrame column')\n    else:\n        if isinstance(key, tuple):\n            try:\n                return self._get_values_tuple(key)\n            except:\n                if (len(key) == 1):\n                    key = key[0]\n                    if isinstance(key, slice):\n                        return self._get_values(key)\n                raise\n        if (not isinstance(key, (list, np.ndarray, Series, Index))):\n            key = list(key)\n        if isinstance(key, Index):\n            key_type = key.inferred_type\n        else:\n            key_type = lib.infer_dtype(key)\n        if (key_type == 'integer'):\n            if (self.index.is_integer() or self.index.is_floating()):\n                return self.reindex(key)\n            else:\n                return self._get_values(key)\n        elif (key_type == 'boolean'):\n            return self._get_values(key)\n        else:\n            try:\n                if isinstance(key, (list, tuple)):\n                    return self.ix[key]\n                return self.reindex(key)\n            except Exception:\n                if isinstance(key[0], slice):\n                    return self._get_values(key)\n                raise\n", "label": 1}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if ((self.email_box_type == 'imap') and (not self.email_box_imap_folder)):\n        self.email_box_imap_folder = 'INBOX'\n    if self.socks_proxy_type:\n        if (not self.socks_proxy_host):\n            self.socks_proxy_host = '127.0.0.1'\n        if (not self.socks_proxy_port):\n            self.socks_proxy_port = 9150\n    else:\n        self.socks_proxy_host = None\n        self.socks_proxy_port = None\n    if (not self.email_box_port):\n        if ((self.email_box_type == 'imap') and self.email_box_ssl):\n            self.email_box_port = 993\n        elif ((self.email_box_type == 'imap') and (not self.email_box_ssl)):\n            self.email_box_port = 143\n        elif ((self.email_box_type == 'pop3') and self.email_box_ssl):\n            self.email_box_port = 995\n        elif ((self.email_box_type == 'pop3') and (not self.email_box_ssl)):\n            self.email_box_port = 110\n    if (not self.id):\n        basename = self.prepare_permission_name()\n        Permission.objects.create(name=(_('Permission for queue: ') + self.title), content_type=ContentType.objects.get(model='queue'), codename=basename)\n    super(Queue, self).save(*args, **kwargs)\n", "label": 1}
{"function": "\n\ndef _parse_relation(chunk, type='O'):\n    ' Returns a string of the roles and relations parsed from the given <chunk> element.\\n        The chunk type (which is part of the relation string) can be given as parameter.\\n    '\n    r1 = chunk.get(XML_RELATION)\n    r2 = chunk.get(XML_ID, chunk.get(XML_OF))\n    r1 = ([(((x != '-') and x) or None) for x in r1.split('|')] or [None])\n    r2 = ([(((x != '-') and x) or None) for x in r2.split('|')] or [None])\n    r2 = [(((x is not None) and x.split(_UID_SEPARATOR)[(- 1)]) or x) for x in r2]\n    if (len(r1) < len(r2)):\n        r1 = (r1 + (r1 * (len(r2) - len(r1))))\n    if (len(r2) < len(r1)):\n        r2 = (r2 + (r2 * (len(r1) - len(r2))))\n    return ';'.join(['-'.join([x for x in (type, r1, r2) if x]) for (r1, r2) in zip(r1, r2)])\n", "label": 1}
{"function": "\n\ndef getmodule(object, _filename=None):\n    'Return the module an object was defined in, or None if not found.'\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    if ((_filename is not None) and (_filename in modulesbyfile)):\n        return sys.modules.get(modulesbyfile[_filename])\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if (file in modulesbyfile):\n        return sys.modules.get(modulesbyfile[file])\n    for (modname, module) in sys.modules.items():\n        if (ismodule(module) and hasattr(module, '__file__')):\n            f = module.__file__\n            if (f == _filesbymodname.get(modname, None)):\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            modulesbyfile[f] = modulesbyfile[os.path.realpath(f)] = module.__name__\n    if (file in modulesbyfile):\n        return sys.modules.get(modulesbyfile[file])\n    main = sys.modules['__main__']\n    if (not hasattr(object, '__name__')):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if (mainobject is object):\n            return main\n    builtin = sys.modules['__builtin__']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if (builtinobject is object):\n            return builtin\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, x, y):\n    I = S.Infinity\n    N = S.NegativeInfinity\n    O = S.Zero\n    if ((x is S.NaN) or (y is S.NaN)):\n        return S.NaN\n    elif (x == y):\n        return S.Zero\n    elif (((x is I) or (x is N) or (x is O)) or ((y is I) or (y is N) or (y is O))):\n        return (erf(y) - erf(x))\n    if ((y.func is erf2inv) and (y.args[0] == x)):\n        return y.args[1]\n    sign_x = x.could_extract_minus_sign()\n    sign_y = y.could_extract_minus_sign()\n    if (sign_x and sign_y):\n        return (- cls((- x), (- y)))\n    elif (sign_x or sign_y):\n        return (erf(y) - erf(x))\n", "label": 1}
{"function": "\n\ndef start():\n    'Start updating from a command and arguments.'\n    try:\n        data = getopt.getopt(sys.argv[1:], 'hms:e:', ['help', 'hide', 'more', 'start=', 'end='])\n    except getopt.GetoptError:\n        usage()\n        sys.exit(2)\n    hide = False\n    more = False\n    start = '01-01-2012'\n    today = date.today()\n    end = ('%i-%i-%i' % (today.month, today.day, today.year))\n    for x in data[0]:\n        if ((x[0] == '-h') or (x[0] == '--help')):\n            usage()\n            sys.exit()\n        elif (x[0] == '--hide'):\n            hide = True\n        elif ((x[0] == '-m') or (x[0] == '--more')):\n            more = True\n        elif ((x[0] == '-s') or (x[0] == '--start')):\n            start = x[1]\n        elif ((x[0] == '-e') or (x[0] == '--end')):\n            end = x[1]\n    try:\n        split = start.split('-')\n        split2 = end.split('-')\n        date1 = date(int(split[2]), int(split[0]), int(split[1]))\n        date2 = date(int(split2[2]), int(split2[0]), int(split2[1]))\n        if ((date1 > date2) or (date1 >= today) or (date2 > today)):\n            date_usage()\n            sys.exit(2)\n    except:\n        date_usage()\n        sys.exit(2)\n    run(hide, more, start, end)\n", "label": 1}
{"function": "\n\ndef get_tokens_unprocessed(self, data):\n    sql = PsqlRegexLexer(**self.options)\n    lines = lookahead(line_re.findall(data))\n    while 1:\n        curcode = ''\n        insertions = []\n        while 1:\n            try:\n                line = lines.next()\n            except StopIteration:\n                break\n            if (line.startswith('$') and (not curcode)):\n                lexer = get_lexer_by_name('console', **self.options)\n                for x in lexer.get_tokens_unprocessed(line):\n                    (yield x)\n                break\n            mprompt = re_prompt.match(line)\n            if (mprompt is not None):\n                insertions.append((len(curcode), [(0, Generic.Prompt, mprompt.group())]))\n                curcode += line[len(mprompt.group()):]\n            else:\n                curcode += line\n            if (re_psql_command.match(curcode) or re_end_command.search(curcode)):\n                break\n        for item in do_insertions(insertions, sql.get_tokens_unprocessed(curcode)):\n            (yield item)\n        out_token = Generic.Output\n        while 1:\n            line = lines.next()\n            mprompt = re_prompt.match(line)\n            if (mprompt is not None):\n                lines.send(line)\n                break\n            mmsg = re_message.match(line)\n            if (mmsg is not None):\n                if (mmsg.group(1).startswith('ERROR') or mmsg.group(1).startswith('FATAL')):\n                    out_token = Generic.Error\n                (yield (mmsg.start(1), Generic.Strong, mmsg.group(1)))\n                (yield (mmsg.start(2), out_token, mmsg.group(2)))\n            else:\n                (yield (0, out_token, line))\n", "label": 1}
{"function": "\n\ndef classify(self, document, discrete=True):\n    ' Returns the type with the highest probability for the given document.\\n            If the classifier has been trained on LSA concept vectors\\n            you need to supply LSA.transform(document).\\n        '\n    v = self._vector(document)[1]\n    m = self._method\n    a = self._alpha\n    n = self._classes.values()\n    n = float(sum(n))\n    p = defaultdict(float)\n    for type in self._classes:\n        if (m == MULTINOMIAL):\n            if (not (type in self._cache)):\n                self._cache[type] = float(sum(self._likelihood[type].values()))\n            d = self._cache[type]\n        if ((m == BINOMIAL) or (m == BERNOUILLI)):\n            d = float(self._classes[type])\n        L = self._likelihood[type]\n        g = sum((log(((L[f] if (f in L) else a) / d)) for f in v))\n        g = ((exp(g) * self._classes[type]) / n)\n        p[type] = g\n    s = (sum(p.values()) or 1)\n    for type in p:\n        p[type] /= s\n    if (not discrete):\n        return p\n    try:\n        m = max(p.values())\n        p = sorted(((self._classes[type], type) for (type, g) in p.items() if (g == m > 0)))\n        p = [type for (frequency, type) in p if (frequency == p[0][0])]\n        return choice(p)\n    except:\n        return self.baseline\n", "label": 1}
{"function": "\n\ndef _process(self):\n    ' Send the first request on the stack. '\n    if (not self._requests):\n        if self._stream:\n            self._want_close = True\n            self._no_process = True\n            self._stream.close(False)\n            self._stream = None\n        self._processing = False\n        return\n    self._processing = True\n    request = self._requests[0]\n    if (not request.response):\n        HTTPResponse(request)\n    if (request.auth and (not isinstance(request.auth, (list, tuple)))):\n        request = request.auth(request)\n    port = _port(request.url)\n    is_secure = (request.url.scheme == 'https')\n    if (not port):\n        port = (443 if is_secure else 80)\n    host = ('%s:%d' % (_hostname(request.url), port))\n    if self._stream:\n        if (not self._stream.connected):\n            self._stream = None\n        elif ((self._ssl_options != request.session.ssl_options) or (not self._stream.can_fetch(host, is_secure))):\n            log.debug(('Closing unusable stream for %r.' % self))\n            self._want_close = True\n            self._no_process = False\n            self._stream.close(False)\n            return\n    log.debug(('Sending HTTP request %r.' % request))\n    self._reset_timer()\n    if (not self._stream):\n        self._stream = _HTTPStream(self, engine=self.engine)\n    if (is_secure and (not self._stream.ssl_enabled)):\n        self._ssl_options = request.session.ssl_options\n        self._stream.startSSL((self._ssl_options or {\n            \n        }))\n    self._stream.connect((_hostname(request.url), port))\n", "label": 1}
{"function": "\n\ndef from_instance(self, instance, keyonly=False, attrs=None, eagerload=[]):\n    if keyonly:\n        attrs = (self._pkey_attrs + self._ref_attrs)\n    for attr in self._wsme_attributes:\n        if (not isinstance(attr, wsattr)):\n            continue\n        if (attrs and (not attr.isrelation) and (attr.name not in attrs)):\n            continue\n        if (attr.isrelation and (attr.name not in eagerload)):\n            continue\n        value = getattr(instance, attr.saname)\n        if attr.isrelation:\n            attr_keyonly = (attr.name not in eagerload)\n            attr_attrs = None\n            attr_eagerload = []\n            if (not attr_keyonly):\n                attr_attrs = [aname[(len(attr.name) + 1):] for aname in attrs if aname.startswith((attr.name + '.'))]\n                attr_eagerload = [aname[(len(attr.name) + 1):] for aname in eagerload if aname.startswith((attr.name + '.'))]\n            if attr.saproperty.uselist:\n                value = [attr.datatype.item_type(o, keyonly=attr_keyonly, attrs=attr_attrs, eagerload=attr_eagerload) for o in value]\n            else:\n                value = attr.datatype(value, keyonly=attr_keyonly, attrs=attr_attrs, eagerload=attr_eagerload)\n        attr.__set__(self, value)\n", "label": 1}
{"function": "\n\ndef handle_failure(self, error, test_mode=False, context=None):\n    logging.exception(error)\n    task = self.task\n    session = settings.Session()\n    self.end_date = datetime.now()\n    self.set_duration()\n    if (not test_mode):\n        session.add(Log(State.FAILED, self))\n    try:\n        if (task.retries and ((self.try_number % (task.retries + 1)) != 0)):\n            self.state = State.UP_FOR_RETRY\n            logging.info('Marking task as UP_FOR_RETRY')\n            if (task.email_on_retry and task.email):\n                self.email_alert(error, is_retry=True)\n        else:\n            self.state = State.FAILED\n            if task.retries:\n                logging.info('All retries failed; marking task as FAILED')\n            else:\n                logging.info('Marking task as FAILED.')\n            if (task.email_on_failure and task.email):\n                self.email_alert(error, is_retry=False)\n    except Exception as e2:\n        logging.error(('Failed to send email to: ' + str(task.email)))\n        logging.exception(e2)\n    try:\n        if ((self.state == State.UP_FOR_RETRY) and task.on_retry_callback):\n            task.on_retry_callback(context)\n        if ((self.state == State.FAILED) and task.on_failure_callback):\n            task.on_failure_callback(context)\n    except Exception as e3:\n        logging.error('Failed at executing callback')\n        logging.exception(e3)\n    if (not test_mode):\n        session.merge(self)\n    session.commit()\n    logging.error(str(error))\n", "label": 1}
{"function": "\n\ndef process_api_request(self, request):\n    reference = request.reference\n    if (reference.kind == 'subdocument'):\n        (_, auth_target, _, _) = reference.value\n    else:\n        auth_target = reference.value\n    permission_name = self._get_permission_name(request)\n    authentication_target = self.authentication_requirement(auth_target, permission_name)\n    authorization_target = self.authorization_requirement(auth_target, permission_name)\n    if (authentication_target is None):\n        return request\n    if (reference.format == 'schema'):\n        return request\n    if (reference.format == 'help'):\n        return request\n    auth_token = request.api_arguments.get('_auth', None)\n    if (not auth_token):\n        bearer = request.headers.get('Authorization', None)\n        if bearer:\n            auth_token = bearer[7:]\n    if auth_token:\n        user = request.suite['auth'].check(auth_token)\n        request.user = user\n    else:\n        request.user = None\n        user = None\n    if (((authentication_target is not None) or (authorization_target is not None)) and (not user)):\n        print('Permission error!!!!!')\n        raise PermissionError('Target {url} requires authentication or authorization, but user is anonymous'.format(url=reference.url))\n    if (user and user['admin']):\n        return request\n    if (authorization_target is None):\n        return request\n    for role in user.fetch('roles'):\n        if role.authorizes(authorization_target, permission_name):\n            return request\n    else:\n        msg = \"Permission '{name}' denied for '{user}' accessing '{url}'\".format(user=user, url=reference.url, name=permission_name)\n        request.suite.log.error(msg)\n        raise PermissionError(msg)\n", "label": 1}
{"function": "\n\ndef test_fastx(filepath):\n    with open(filepath, encoding='latin-1') as fastx_file:\n        sequences = OrderedDict()\n        seq = []\n        header = ''\n        found_caret = False\n        for (row_index, row) in enumerate(fastx_file, 1):\n            if (row_index > 30):\n                break\n            if (not row.strip()):\n                continue\n            if ((found_caret is False) and (row[0] != '>')):\n                if (row[0] == ';'):\n                    continue\n                break\n            elif ((found_caret is False) and (row[0] == '>')):\n                found_caret = True\n            if (row and (row[0] == '>')):\n                if seq:\n                    sequences[header] = ''.join(seq)\n                    seq = []\n                header = row\n            elif row:\n                seq.append(row)\n        if (seq and header):\n            sequences[header] = ''.join(seq)\n        if sequences:\n            rows = []\n            [rows.extend([i, v]) for (i, v) in six.iteritems(sequences)]\n            return (True, rows)\n    return (False, None)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_topic_ != x.has_topic_):\n        return 0\n    if (self.has_topic_ and (self.topic_ != x.topic_)):\n        return 0\n    if (self.has_sub_id_ != x.has_sub_id_):\n        return 0\n    if (self.has_sub_id_ and (self.sub_id_ != x.sub_id_)):\n        return 0\n    if (self.has_lease_duration_sec_ != x.has_lease_duration_sec_):\n        return 0\n    if (self.has_lease_duration_sec_ and (self.lease_duration_sec_ != x.lease_duration_sec_)):\n        return 0\n    if (self.has_vanilla_query_ != x.has_vanilla_query_):\n        return 0\n    if (self.has_vanilla_query_ and (self.vanilla_query_ != x.vanilla_query_)):\n        return 0\n    if (len(self.schema_entry_) != len(x.schema_entry_)):\n        return 0\n    for (e1, e2) in zip(self.schema_entry_, x.schema_entry_):\n        if (e1 != e2):\n            return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef inspector(objects):\n    api = {\n        'attributes': set(),\n        'scalars': set(),\n        'vectors': set(),\n        'defaults': {\n            \n        },\n    }\n    attributes = defaultdict(set)\n    for line in objects:\n        for attribute in dir(line):\n            if ((not attribute.startswith('_')) and (not callable(getattr(line, attribute)))):\n                kind = type(getattr(line, attribute))\n                if ((kind in (unicode, basestring, bytes)) or issubclass(kind, basestring)):\n                    kind = str\n                attributes[attribute].add(kind)\n    api['attributes'] = set(attributes.keys())\n    for attribute in attributes.keys():\n        kinds = set(attributes[attribute])\n        if (len(kinds) == 1):\n            kind = kinds.pop()\n            try:\n                api['defaults'][attribute] = kind()\n            except:\n                api['defaults'][attribute] = NoValue\n            if ((not issubclass(kind, collections.Iterable)) or (kind in (str, unicode, basestring, bytes))):\n                api['scalars'].add(attribute)\n            else:\n                api['vectors'].add(attribute)\n        else:\n            api['defaults'][attribute] = NoValue\n            has_scalars = False\n            has_vectors = False\n            for kind in kinds:\n                if ((not issubclass(kind, collections.Iterable)) or (kind in (str, unicode, basestring, bytes))):\n                    has_scalars = True\n                else:\n                    has_vectors = True\n            if has_scalars:\n                api['scalars'].add(attribute)\n            else:\n                api['vectors'].add(attribute)\n    return api\n", "label": 1}
{"function": "\n\ndef test_fraction():\n    (x, y, z) = map(Symbol, 'xyz')\n    A = Symbol('A', commutative=False)\n    assert (fraction(Rational(1, 2)) == (1, 2))\n    assert (fraction(x) == (x, 1))\n    assert (fraction((1 / x)) == (1, x))\n    assert (fraction((x / y)) == (x, y))\n    assert (fraction((x / 2)) == (x, 2))\n    assert (fraction(((x * y) / z)) == ((x * y), z))\n    assert (fraction((x / (y * z))) == (x, (y * z)))\n    assert (fraction((1 / (y ** 2))) == (1, (y ** 2)))\n    assert (fraction((x / (y ** 2))) == (x, (y ** 2)))\n    assert (fraction((((x ** 2) + 1) / y)) == (((x ** 2) + 1), y))\n    assert (fraction(((x * (y + 1)) / (y ** 7))) == ((x * (y + 1)), (y ** 7)))\n    assert (fraction(exp((- x)), exact=True) == (exp((- x)), 1))\n    assert (fraction(((x * A) / y)) == ((x * A), y))\n    assert (fraction(((x * (A ** (- 1))) / y)) == ((x * (A ** (- 1))), y))\n    n = symbols('n', negative=True)\n    assert (fraction(exp(n)) == (1, exp((- n))))\n    assert (fraction(exp((- n))) == (exp((- n)), 1))\n", "label": 1}
{"function": "\n\ndef _validate_port_range(self, rule):\n    'Check that port_range is valid.'\n    if ((rule['port_range_min'] is None) and (rule['port_range_max'] is None)):\n        return\n    if (not rule['protocol']):\n        raise ext_sg.SecurityGroupProtocolRequiredWithPorts()\n    ip_proto = self._get_ip_proto_number(rule['protocol'])\n    if (ip_proto in [constants.PROTO_NUM_TCP, constants.PROTO_NUM_UDP]):\n        if ((rule['port_range_min'] == 0) or (rule['port_range_max'] == 0)):\n            raise ext_sg.SecurityGroupInvalidPortValue(port=0)\n        elif ((rule['port_range_min'] is not None) and (rule['port_range_max'] is not None) and (rule['port_range_min'] <= rule['port_range_max'])):\n            pass\n        else:\n            raise ext_sg.SecurityGroupInvalidPortRange()\n    elif (ip_proto == constants.PROTO_NUM_ICMP):\n        for (attr, field) in [('port_range_min', 'type'), ('port_range_max', 'code')]:\n            if ((rule[attr] is not None) and (not (0 <= rule[attr] <= 255))):\n                raise ext_sg.SecurityGroupInvalidIcmpValue(field=field, attr=attr, value=rule[attr])\n        if ((rule['port_range_min'] is None) and (rule['port_range_max'] is not None)):\n            raise ext_sg.SecurityGroupMissingIcmpType(value=rule['port_range_max'])\n", "label": 1}
{"function": "\n\ndef _get_gs_outputs(self, mode, vois):\n    '\\n        Linear Gauss-Siedel can limit the outputs when calling apply. This\\n        calculates and caches the list of outputs to be updated for each voi.\\n        '\n    if (self._gs_outputs is None):\n        self._gs_outputs = {\n            \n        }\n    if (mode not in self._gs_outputs):\n        dumat = self.dumat\n        gs_outputs = self._gs_outputs[mode] = OrderedDict()\n        if (mode == 'fwd'):\n            for sub in self._local_subsystems:\n                gs_outputs[sub.name] = outs = OrderedDict()\n                for voi in vois:\n                    if (voi in dumat):\n                        outs[voi] = set([x for x in dumat[voi]._dat if (sub.dumat and (x not in sub.dumat[voi]))])\n        else:\n            for sub in self._local_subsystems:\n                gs_outputs[sub.name] = outs = OrderedDict()\n                for voi in vois:\n                    if (voi in dumat):\n                        outs[voi] = set([x for x in dumat[voi]._dat if ((not sub.dumat) or (sub.dumat and (x not in sub.dumat[voi])))])\n    return self._gs_outputs\n", "label": 1}
{"function": "\n\ndef cacheLookupSeveral(self, computationsToCall, block=True, leaveInCache=True):\n    with self.lock_:\n        notAlreadyInCache = [comp for comp in computationsToCall if ((comp not in self.finishedValues_) and (comp not in self.intermediates_))]\n        for comp in computationsToCall:\n            if ((comp not in self.finishedValues_) and (comp not in self.intermediates_)):\n                self.intermediates_[comp] = None\n                self.completable_.put(comp)\n                self.watchers_[comp] = threading.Event()\n        allDone = True\n        for comp in computationsToCall:\n            if (comp not in self.finishedValues_):\n                allDone = False\n    if ((not allDone) and block):\n        for comp in computationsToCall:\n            self.watchers_[comp].wait()\n        allDone = True\n    if allDone:\n        result = [self.finishedValues_[comp] for comp in computationsToCall]\n        if (not leaveInCache):\n            for c in notAlreadyInCache:\n                del self.finishedValues_[c]\n        return result\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef __new__(cls, start, end, left_open=False, right_open=False):\n    start = _sympify(start)\n    end = _sympify(end)\n    left_open = _sympify(left_open)\n    right_open = _sympify(right_open)\n    if (not all((isinstance(a, (type(true), type(false))) for a in [left_open, right_open]))):\n        raise NotImplementedError(('left_open and right_open can have only true/false values, got %s and %s' % (left_open, right_open)))\n    inftys = [S.Infinity, S.NegativeInfinity]\n    if (not all((((i.is_real is not False) or (i in inftys)) for i in (start, end)))):\n        raise ValueError('Non-real intervals are not supported')\n    if ((end < start) == True):\n        return S.EmptySet\n    elif (end - start).is_negative:\n        return S.EmptySet\n    if ((end == start) and (left_open or right_open)):\n        return S.EmptySet\n    if ((end == start) and (not (left_open or right_open))):\n        return FiniteSet(end)\n    if (start == S.NegativeInfinity):\n        left_open = true\n    if (end == S.Infinity):\n        right_open = true\n    return Basic.__new__(cls, start, end, left_open, right_open)\n", "label": 1}
{"function": "\n\ndef testIn(self):\n    'Key in object gives expected results.'\n    client = LDAPClientTestDriver()\n    o = ldapsyntax.LDAPEntry(client=client, dn='cn=foo,dc=example,dc=com', attributes={\n        'objectClass': ['a', 'b'],\n        'aValue': ['a'],\n        'bValue': ['b'],\n    })\n    assert ('objectClass' in o)\n    assert ('aValue' in o)\n    assert ('bValue' in o)\n    assert ('foo' not in o)\n    assert ('' not in o)\n    assert (None not in o)\n    assert ('a' in o['objectClass'])\n    assert ('b' in o['objectClass'])\n    assert ('foo' not in o['objectClass'])\n    assert ('' not in o['objectClass'])\n    assert (None not in o['objectClass'])\n    assert ('a' in o['aValue'])\n    assert ('foo' not in o['aValue'])\n    assert ('' not in o['aValue'])\n    assert (None not in o['aValue'])\n", "label": 1}
{"function": "\n\n@command\ndef help(self, mask, target, args):\n    'Show help\\n\\n            %%help [<cmd>]\\n        '\n    if args['<cmd>']:\n        args = args['<cmd>']\n        if args.startswith(self.context.config.cmd):\n            args = args[len(self.context.config.cmd):]\n        (predicates, meth) = self.get(args, (None, None))\n        if (meth is not None):\n            doc = (meth.__doc__ or '')\n            doc = [l.strip() for l in doc.split('\\n') if l.strip()]\n            buf = ''\n            for line in doc:\n                if (('%%' not in line) and (buf is not None)):\n                    buf += (line + ' ')\n                else:\n                    if (buf is not None):\n                        for b in utils.split_message(buf, 160):\n                            (yield b)\n                        buf = None\n                    line = line.replace('%%', self.context.config.cmd)\n                    (yield line)\n        else:\n            (yield ('No such command. Try %shelp for an overview of all commands.' % self.context.config.cmd))\n    else:\n        cmds = sorted((k for (k, (p, m)) in self.items() if p.get('show_in_help_list', True)))\n        cmds_str = ', '.join([(self.cmd + k) for k in cmds])\n        lines = utils.split_message(('Available commands: %s ' % cmds_str), 160)\n        for line in lines:\n            (yield line)\n        url = self.config.get('url')\n        if url:\n            (yield ('Full help is available at ' + url))\n", "label": 1}
{"function": "\n\ndef parseBytecode(dex, insns_start_pos, shorts, catch_addrs):\n    ops = []\n    pos = 0\n    while (pos < len(shorts)):\n        (pos, op) = parseInstruction(dex, insns_start_pos, shorts, pos)\n        ops.append(op)\n    for (instr, instr2) in zip(ops, ops[1:]):\n        if (not (instr2.type == MoveResult)):\n            continue\n        if (instr.type in INVOKE_TYPES):\n            called_id = dex.method_id(instr.args[0])\n            if (called_id.return_type != b'V'):\n                instr2.prev_result = called_id.return_type\n        elif (instr.type == FilledNewArray):\n            instr2.prev_result = dex.type(instr.args[0])\n        elif (instr2.pos in catch_addrs):\n            instr2.prev_result = b'Ljava/lang/Throwable;'\n    assert (0 not in catch_addrs)\n    for (i, instr) in enumerate(ops):\n        if (instr.opcode in (56, 57)):\n            if ((i > 0) and (ops[(i - 1)].type == InstanceOf)):\n                prev = ops[(i - 1)]\n                desc_ind = prev.args[2]\n                regs = {prev.args[1]}\n                if ((i > 1) and (ops[(i - 2)].type == Move)):\n                    prev2 = ops[(i - 2)]\n                    if (prev2.args[0] == prev.args[1]):\n                        regs.add(prev2.args[1])\n                regs.discard(prev.args[0])\n                if regs:\n                    instr.implicit_casts = (desc_ind, sorted(regs))\n    return ops\n", "label": 1}
{"function": "\n\ndef get_completions(self, view, prefix):\n    skip_deleted = Pref.forget_deleted_files\n    completions = list(Pref.always_on_auto_completions)\n    already_in = []\n    for (file, data) in self.files.items():\n        if ((not skip_deleted) or (skip_deleted and os.path.lexists(file))):\n            location = basename(file)\n            for function in data:\n                if (prefix in function[self.NAME]):\n                    already_in.append(function[self.NAME])\n                    completion = self.create_function_completion(function, location)\n                    completions.append(completion)\n    location = (basename(view.file_name()) if view.file_name() else '')\n    [completions.append(self.create_function_completion(self.parse_line(view.substr(view.line(selection))), location)) for selection in view.find_by_selector('entity.name.function') if ((view.substr(selection) not in already_in) and (already_in.append(view.substr(selection)) or True))]\n    vars = []\n    [view.substr(selection) for selection in view.find_all('([var\\\\s+]|\\\\.)(\\\\w+)\\\\s*[=|:]', 0, '$2', vars)]\n    [completions.append(self.create_var_completion(var, location)) for var in list(set(vars)) if ((len(var) > 1) and (var not in already_in))]\n    if debug:\n        print('Completions')\n        print(completions)\n    return completions\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    \"'Deep, sparse compare.\\n\\n        Deeply compare two entities, following the non-None attributes of the\\n        non-persisted object, if possible.\\n\\n        \"\n    if (other is self):\n        return True\n    elif (not (self.__class__ == other.__class__)):\n        return False\n    if (id(self) in _recursion_stack):\n        return True\n    _recursion_stack.add(id(self))\n    try:\n        try:\n            self_key = sa.orm.attributes.instance_state(self).key\n        except sa.orm.exc.NO_STATE:\n            self_key = None\n        if (other is None):\n            a = self\n            b = other\n        elif (self_key is not None):\n            a = other\n            b = self\n        else:\n            a = self\n            b = other\n        for attr in list(a.__dict__):\n            if attr.startswith('_'):\n                continue\n            value = getattr(a, attr)\n            try:\n                battr = getattr(b, attr)\n            except (AttributeError, sa_exc.UnboundExecutionError):\n                return False\n            if hasattr(value, '__iter__'):\n                if (hasattr(value, '__getitem__') and (not hasattr(value, 'keys'))):\n                    if (list(value) != list(battr)):\n                        return False\n                elif (set(value) != set(battr)):\n                    return False\n            elif ((value is not None) and (value != battr)):\n                return False\n        return True\n    finally:\n        _recursion_stack.remove(id(self))\n", "label": 1}
{"function": "\n\ndef _tree_reduce(x, aggregate, axis, keepdims, dtype, split_every=None, combine=None, name=None):\n    'Perform the tree reduction step of a reduction.\\n\\n    Lower level, users should use ``reduction`` or ``arg_reduction`` directly.\\n    '\n    split_every = (split_every or _globals.get('split_every', 4))\n    if isinstance(split_every, dict):\n        split_every = dict(((k, split_every.get(k, 2)) for k in axis))\n    elif isinstance(split_every, int):\n        n = builtins.max(int((split_every ** (1 / (len(axis) or 1)))), 2)\n        split_every = dict.fromkeys(axis, n)\n    else:\n        split_every = dict(((k, v) for (k, v) in enumerate(x.numblocks) if (k in axis)))\n    depth = 1\n    for (i, n) in enumerate(x.numblocks):\n        if ((i in split_every) and (split_every[i] != 1)):\n            depth = int(builtins.max(depth, ceil(log(n, split_every[i]))))\n    func = compose(partial((combine or aggregate), axis=axis, keepdims=True), partial(_concatenate2, axes=axis))\n    for i in range((depth - 1)):\n        x = partial_reduce(func, x, split_every, True, None, name=((name or funcname((combine or aggregate))) + '-partial'))\n    func = compose(partial(aggregate, axis=axis, keepdims=keepdims), partial(_concatenate2, axes=axis))\n    return partial_reduce(func, x, split_every, keepdims=keepdims, dtype=dtype, name=((name or funcname(aggregate)) + '-aggregate'))\n", "label": 1}
{"function": "\n\ndef test_update_crossing_min_not_max_at_1st_step(self, duration1, duration2):\n    global rec\n    need_test = (duration1 < duration2)\n    if (not need_test):\n        return\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    composite = ac.spawn(a1, a2)\n    rec = []\n    node.do(composite)\n    elapsed = 0.0\n    next_elapsed = ((duration1 + duration2) / 2.0)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    print('rec:', rec)\n    rec = [e for e in rec if (e[1] != 'step')]\n    if (duration1 < duration2):\n        oname1 = name1\n        oduration1 = duration1\n        oname2 = name2\n        oduration2 = duration2\n    else:\n        oname1 = name2\n        oduration1 = duration2\n        oname2 = name1\n        oduration2 = duration1\n    orec1 = [e for e in rec if (e[0] == oname1)]\n    print(orec1)\n    assert (orec1[0] == (oname1, 'start'))\n    assert (orec1[1] == (oname1, 'update', 1.0))\n    assert (orec1[2] == (oname1, 'stop'))\n    assert (len(orec1) == 3)\n    orec2 = [e for e in rec if (e[0] == oname2)]\n    print(orec2)\n    assert (len(orec2) == 2)\n    assert (orec2[0] == (oname2, 'start'))\n    assert (orec2[1][1] == 'update')\n    assert (abs((orec2[1][2] - (next_elapsed / oduration2))) < fe)\n", "label": 1}
{"function": "\n\ndef test_table():\n    client = mock.MagicMock()\n    table = models.Table(client, 'db_name', 'table_name', 'type', 'schema', 12345, created_at='created_at', updated_at='updated_at', estimated_storage_size=67890, last_import='last_import', last_log_timestamp='last_log_timestamp', expire_days='expire_days', primary_key='primary_key', primary_key_type='primary_key_type')\n    assert (table.type == 'type')\n    assert (table.db_name == 'db_name')\n    assert (table.table_name == 'table_name')\n    assert (table.schema == 'schema')\n    assert (table.count == 12345)\n    assert (table.estimated_storage_size == 67890)\n    assert (table.primary_key == 'primary_key')\n    assert (table.primary_key_type == 'primary_key_type')\n    assert (table.database_name == 'db_name')\n    assert (table.name == 'table_name')\n    assert (table.created_at == 'created_at')\n    assert (table.updated_at == 'updated_at')\n    assert (table.last_import == 'last_import')\n    assert (table.last_log_timestamp == 'last_log_timestamp')\n    assert (table.expire_days == 'expire_days')\n    assert (table.identifier == 'db_name.table_name')\n", "label": 1}
{"function": "\n\ndef _run_single_hook(hook, repo, args, write, skips=frozenset()):\n    filenames = get_filenames(args, hook['files'], hook['exclude'])\n    if (hook['id'] in skips):\n        _print_user_skipped(hook, write, args)\n        return 0\n    elif ((not filenames) and (not hook['always_run'])):\n        _print_no_files_skipped(hook, write, args)\n        return 0\n    write(get_hook_message(_hook_msg_start(hook, args.verbose), end_len=6))\n    sys.stdout.flush()\n    diff_before = cmd_output('git', 'diff', retcode=None, encoding=None)\n    (retcode, stdout, stderr) = repo.run_hook(hook, tuple(filenames))\n    diff_after = cmd_output('git', 'diff', retcode=None, encoding=None)\n    file_modifications = (diff_before != diff_after)\n    if file_modifications:\n        retcode = 1\n    if retcode:\n        retcode = 1\n        print_color = color.RED\n        pass_fail = 'Failed'\n    else:\n        retcode = 0\n        print_color = color.GREEN\n        pass_fail = 'Passed'\n    write((color.format_color(pass_fail, print_color, args.color) + '\\n'))\n    if ((stdout or stderr or file_modifications) and (retcode or args.verbose)):\n        write('hookid: {0}\\n'.format(hook['id']))\n        write('\\n')\n        if file_modifications:\n            write('Files were modified by this hook.')\n            if (stdout or stderr):\n                write(' Additional output:\\n')\n            write('\\n')\n        for output in (stdout, stderr):\n            assert (type(output) is bytes), type(output)\n            if output.strip():\n                write((output.strip() + b'\\n'))\n        write('\\n')\n    return retcode\n", "label": 1}
{"function": "\n\ndef alignCol(self, block, heightShortfall, widthShortfall):\n    '\\n            Method to align the widgets laid out different rows\\n\\n            @todo: parameter description\\n        '\n    for widget in block.widgetList:\n        widgetWidth = (block.startPosn[1] + widget.getMatrixSize()[1])\n        widthShortfall = (block.growToWidth - widgetWidth)\n        if (widthShortfall == 0):\n            continue\n        if widget.canGrowHorizontal():\n            widget.growHorizontal(widthShortfall)\n        else:\n            widget.addToHorizontalMargin(widthShortfall)\n    if (heightShortfall == 0):\n        return\n    canGrowCount = 0\n    for widget in block.widgetList:\n        if widget.canGrowVertical():\n            canGrowCount += 1\n    if (canGrowCount > 0):\n        growBy = (heightShortfall / canGrowCount)\n        if (growBy > 0):\n            for widget in block.widgetList:\n                if widget.canGrowVertical():\n                    widget.growVertical(growBy)\n                    heightShortfall -= growBy\n    if (heightShortfall > 0):\n        marginGrow = (heightShortfall / len(block.widgetList))\n        if (marginGrow > 0):\n            for widget in block.widgetList:\n                widget.addToVerticalMargin(marginGrow)\n                heightShortfall -= marginGrow\n    if (heightShortfall > 0):\n        for widget in block.widgetList:\n            widget.addToVerticalMargin(1)\n            heightShortfall -= 1\n            if (heightShortfall == 0):\n                break\n", "label": 1}
{"function": "\n\ndef write_def_finish(self, node, buffered, filtered, cached, callstack=True):\n    'write the end section of a rendering function, either outermost or\\n        inline.\\n\\n        this takes into account if the rendering function was filtered,\\n        buffered, etc.  and closes the corresponding try: block if any, and\\n        writes code to retrieve captured content, apply filters, send proper\\n        return value.'\n    if ((not buffered) and (not cached) and (not filtered)):\n        self.printer.writeline(\"return ''\")\n        if callstack:\n            self.printer.writelines('finally:', 'context.caller_stack._pop_frame()', None)\n    if (buffered or filtered or cached):\n        if (buffered or cached):\n            self.printer.writelines('finally:', '__M_buf = context._pop_buffer()')\n        else:\n            self.printer.writelines('finally:', '__M_buf, __M_writer = context._pop_buffer_and_writer()')\n        if callstack:\n            self.printer.writeline('context.caller_stack._pop_frame()')\n        s = '__M_buf.getvalue()'\n        if filtered:\n            s = self.create_filter_callable(node.filter_args.args, s, False)\n        self.printer.writeline(None)\n        if (buffered and (not cached)):\n            s = self.create_filter_callable(self.compiler.buffer_filters, s, False)\n        if (buffered or cached):\n            self.printer.writeline(('return %s' % s))\n        else:\n            self.printer.writelines(('__M_writer(%s)' % s), \"return ''\")\n", "label": 1}
{"function": "\n\ndef parse_diff(self, result, stdin=None):\n    lines = result.splitlines()\n    matcher = re.compile('^@@ -([0-9]*),([0-9]*) \\\\+([0-9]*),([0-9]*) @@')\n    diff = []\n    for line_index in range(0, len(lines)):\n        line = lines[line_index]\n        if (not line.startswith('@')):\n            continue\n        match = matcher.match(line)\n        if (not match):\n            continue\n        (line_before, len_before, line_after, len_after) = [int(match.group(x)) for x in [1, 2, 3, 4]]\n        chunk_index = (line_index + 1)\n        tracked_line_index = (line_after - 1)\n        deletion = False\n        insertion = False\n        while True:\n            line = lines[chunk_index]\n            if line.startswith('@'):\n                break\n            elif line.startswith('-'):\n                if (not (line.strip() == '-')):\n                    deletion = True\n                tracked_line_index -= 1\n            elif line.startswith('+'):\n                if (deletion and (not (line.strip() == '+'))):\n                    diff.append(['x', tracked_line_index])\n                    insertion = True\n                elif (not deletion):\n                    insertion = True\n                    diff.append(['+', tracked_line_index])\n            else:\n                if ((not insertion) and deletion):\n                    diff.append(['-', tracked_line_index])\n                insertion = deletion = False\n            tracked_line_index += 1\n            chunk_index += 1\n            if (chunk_index >= len(lines)):\n                break\n    self.annotate(diff)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_key_ != x.has_key_):\n        return 0\n    if (self.has_key_ and (self.key_ != x.key_)):\n        return 0\n    if (self.has_value_ != x.has_value_):\n        return 0\n    if (self.has_value_ and (self.value_ != x.value_)):\n        return 0\n    if (self.has_flags_ != x.has_flags_):\n        return 0\n    if (self.has_flags_ and (self.flags_ != x.flags_)):\n        return 0\n    if (self.has_set_policy_ != x.has_set_policy_):\n        return 0\n    if (self.has_set_policy_ and (self.set_policy_ != x.set_policy_)):\n        return 0\n    if (self.has_expiration_time_ != x.has_expiration_time_):\n        return 0\n    if (self.has_expiration_time_ and (self.expiration_time_ != x.expiration_time_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef visit_Index(self, expr):\n    arr = self.visit_expr(expr.value)\n    idx = self.visit_expr(expr.index)\n    if ((arr.__class__ is Tuple) and (idx.__class__ is Const)):\n        return arr[idx.value]\n    elif (arr.__class__ is Shape):\n        if isinstance(idx, Scalar):\n            return shape.lower_rank(arr, 0)\n        elif (idx.__class__ is Shape):\n            assert (len(idx.dims) <= len(arr.dims)), (\"Can't index into rank %d array with rank %d indices\" % (len(arr.dims), len(idx.dims)))\n            dims = [d for d in arr.dims]\n            for (i, d) in enumerate(idx.dims):\n                dims[i] = d\n            return shape.make_shape(dims)\n        else:\n            return self.index(arr, idx)\n    elif (arr.__class__ is Ptr):\n        assert isinstance(arr.elt_shape, Scalar)\n        assert isinstance(idx, Scalar)\n        return any_scalar\n    if isinstance(arr, Scalar):\n        assert False, ('Expected %s to be array, shape inference found scalar' % (arr,))\n    elif (arr == shape.any_value):\n        raise ShapeInferenceFailure(expr, self.fn)\n    assert False, (\"Can't index (%s) with array shape %s and index shape %s\" % (expr, arr, idx))\n", "label": 1}
{"function": "\n\ndef skip(self, ttype):\n    if (ttype == TType.STOP):\n        return\n    elif (ttype == TType.BOOL):\n        self.read_bool()\n    elif (ttype == TType.BYTE):\n        self.read_byte()\n    elif (ttype in (TType.I16, TType.I32, TType.I64)):\n        from_zig_zag(read_varint(self.trans))\n    elif (ttype == TType.DOUBLE):\n        self.read_double()\n    elif (ttype == TType.STRING):\n        self.read_string()\n    elif (ttype == TType.STRUCT):\n        name = self.read_struct_begin()\n        while True:\n            (name, ttype, id) = self.read_field_begin()\n            if (ttype == TType.STOP):\n                break\n            self.skip(ttype)\n            self.read_field_end()\n        self.read_struct_end()\n    elif (ttype == TType.MAP):\n        (ktype, vtype, size) = self.read_map_begin()\n        for i in range(size):\n            self.skip(ktype)\n            self.skip(vtype)\n        self.read_collection_end()\n    elif (ttype == TType.SET):\n        (etype, size) = self.read_collection_begin()\n        for i in range(size):\n            self.skip(etype)\n        self.read_collection_end()\n    elif (ttype == TType.LIST):\n        (etype, size) = self.read_collection_begin()\n        for i in range(size):\n            self.skip(etype)\n        self.read_collection_end()\n", "label": 1}
{"function": "\n\ndef get_filters(self, request):\n    lookup_params = self.params.copy()\n    use_distinct = False\n    for ignored in IGNORED_PARAMS:\n        if (ignored in lookup_params):\n            del lookup_params[ignored]\n    for (key, value) in lookup_params.items():\n        if (not isinstance(key, str)):\n            del lookup_params[key]\n            lookup_params[force_str(key)] = value\n        if (not self.model_admin.lookup_allowed(key, value)):\n            raise SuspiciousOperation(('Filtering by %s not allowed' % key))\n    filter_specs = []\n    if self.list_filter:\n        for list_filter in self.list_filter:\n            if callable(list_filter):\n                spec = list_filter(request, lookup_params, self.model, self.model_admin)\n            else:\n                field_path = None\n                if isinstance(list_filter, (tuple, list)):\n                    (field, field_list_filter_class) = list_filter\n                else:\n                    (field, field_list_filter_class) = (list_filter, FieldListFilter.create)\n                if (not isinstance(field, models.Field)):\n                    field_path = field\n                    field = get_fields_from_path(self.model, field_path)[(- 1)]\n                spec = field_list_filter_class(field, request, lookup_params, self.model, self.model_admin, field_path=field_path)\n                use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, field_path))\n            if (spec and spec.has_output()):\n                filter_specs.append(spec)\n    try:\n        for (key, value) in lookup_params.items():\n            lookup_params[key] = prepare_lookup_value(key, value)\n            use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, key))\n        return (filter_specs, bool(filter_specs), lookup_params, use_distinct)\n    except FieldDoesNotExist as e:\n        raise IncorrectLookupParameters(e)\n", "label": 1}
{"function": "\n\ndef locate_files(self):\n    unnamed = list(self.unnamed_requirements)\n    reqs = list(self.requirements.values())\n    while (reqs or unnamed):\n        if unnamed:\n            req_to_install = unnamed.pop(0)\n        else:\n            req_to_install = reqs.pop(0)\n        install_needed = True\n        if ((not self.ignore_installed) and (not req_to_install.editable)):\n            req_to_install.check_if_exists()\n            if req_to_install.satisfied_by:\n                if self.upgrade:\n                    if (not (self.use_user_site and (not dist_in_usersite(req_to_install.satisfied_by)))):\n                        req_to_install.conflicts_with = req_to_install.satisfied_by\n                    req_to_install.satisfied_by = None\n                else:\n                    install_needed = False\n            if req_to_install.satisfied_by:\n                logger.notify(('Requirement already satisfied (use --upgrade to upgrade): %s' % req_to_install))\n        if req_to_install.editable:\n            if (req_to_install.source_dir is None):\n                req_to_install.source_dir = req_to_install.build_location(self.src_dir)\n        elif install_needed:\n            req_to_install.source_dir = req_to_install.build_location(self.build_dir, (not self.is_download))\n        if ((req_to_install.source_dir is not None) and (not os.path.isdir(req_to_install.source_dir))):\n            raise InstallationError(('Could not install requirement %s because source folder %s does not exist (perhaps --no-download was used without first running an equivalent install with --no-install?)' % (req_to_install, req_to_install.source_dir)))\n", "label": 1}
{"function": "\n\ndef _prepare_sorting(self):\n    order = self.request.GET.get('order', None)\n    if (order is not None):\n        is_desc = (order[0] == '-')\n        order_field = ((is_desc and order[1:]) or order)\n        sort_column = [x for x in self.shown_bound_columns if (x.name == order_field)][0]\n        order_args = evaluate(sort_column.sort_key, column=sort_column)\n        order_args = ((isinstance(order_args, list) and order_args) or [order_args])\n        if sort_column.sortable:\n            if isinstance(self.data, list):\n                order_by_on_list(self.data, order_args[0], is_desc)\n            else:\n                if (not settings.DEBUG):\n                    valid_sort_fields = {x.name for x in self.Meta.model._meta.fields}\n                    order_args = [order_arg for order_arg in order_args if (order_arg.split('__', 1)[0] in valid_sort_fields)]\n                order_args = [('%s%s' % (((is_desc and '-') or ''), x)) for x in order_args]\n                self.data = self.data.order_by(*order_args)\n", "label": 1}
{"function": "\n\ndef simplify_model(obj, terse=False):\n    if (obj is None):\n        return None\n    if hasattr(obj._meta, 'expose_fields'):\n        expose_fields = obj._meta.expose_fields\n    else:\n        expose_fields = [f.name for f in obj._meta.fields]\n    out = {\n        '_type': ('%s.%s' % (obj.__module__[:(- 7)], obj._meta.object_name)),\n        '_pk': obj.pk,\n    }\n    if hasattr(obj, 'get_absolute_url'):\n        out['_url'] = obj.get_absolute_url()\n    if terse:\n        out['_terse'] = True\n    else:\n        for field_name in expose_fields:\n            if (field_name in FIELDS_NOT_EXPOSED):\n                continue\n            try:\n                value = getattr(obj, field_name)\n                if isinstance(value, models.Model):\n                    value = simplify_model(value, terse=True)\n                out[field_name] = simplify_value(value)\n            except NotImplementedError:\n                pass\n        for field in list(dir(obj)):\n            try:\n                if ((field[0] != '_') and (field != 'objects') and (not isinstance(getattr(obj, field), models.Field)) and (not (field in FIELDS_NOT_EXPOSED))):\n                    try:\n                        out[field] = simplify_value(getattr(obj, field))\n                    except NotImplementedError:\n                        pass\n            except AttributeError:\n                pass\n    return out\n", "label": 1}
{"function": "\n\ndef test_complement():\n    assert complement((lambda : False))()\n    assert (not complement((lambda : True))())\n    assert complement(iseven)(1)\n    assert (not complement(iseven)(2))\n    assert complement(complement(iseven))(2)\n    assert (not complement(complement(isodd))(2))\n    both_even = (lambda a, b: (iseven(a) and iseven(b)))\n    assert complement(both_even)(1, 2)\n    assert (not complement(both_even)(2, 2))\n    assert complement((lambda : ''))()\n    assert complement((lambda : 0))()\n    assert complement((lambda : None))()\n    assert complement((lambda : []))()\n    assert (not complement((lambda : 'x'))())\n    assert (not complement((lambda : 1))())\n    assert (not complement((lambda : [1]))())\n", "label": 1}
{"function": "\n\ndef take_action(self, action_name, obj_id=None, obj_ids=None):\n    'Locates the appropriate action and routes the object\\n        data to it. The action should return an HTTP redirect\\n        if successful, or a value which evaluates to ``False``\\n        if unsuccessful.\\n        '\n    obj_ids = (obj_ids or self.request.POST.getlist('object_ids'))\n    action = self.base_actions.get(action_name, None)\n    if ((not action) or (action.method != self.request.method)):\n        return None\n    if ((not action.requires_input) or obj_id or obj_ids):\n        if obj_id:\n            obj_id = self.sanitize_id(obj_id)\n        if obj_ids:\n            obj_ids = [self.sanitize_id(i) for i in obj_ids]\n        if (not action.handles_multiple):\n            response = action.single(self, self.request, obj_id)\n        else:\n            if obj_id:\n                obj_ids = [obj_id]\n            response = action.multiple(self, self.request, obj_ids)\n        return response\n    elif (action and action.requires_input and (not (obj_id or obj_ids))):\n        messages.info(self.request, _('Please select a row before taking that action.'))\n    return None\n", "label": 1}
{"function": "\n\ndef get_type(v):\n    if isinstance(v, bool):\n        return ('b',)\n    elif isinstance(v, unicode):\n        if (len(v) >= 255):\n            return ('U', (((len(v) // 1000) + 1) * 1000))\n        else:\n            return ('u',)\n    elif isinstance(v, (str_8b, str)):\n        if (len(v) >= 255):\n            return ('S', (((len(v) // 1000) + 1) * 1000))\n        else:\n            return ('s',)\n    elif isinstance(v, (int, long)):\n        if ((v > 2147483647) or (v < (- 2147483648))):\n            return ('l',)\n        else:\n            return ('i',)\n    elif isinstance(v, float):\n        return ('f',)\n    elif isinstance(v, BinaryNullType):\n        return ('BN',)\n    elif (v is None):\n        return ('N',)\n    elif isinstance(v, Decimal):\n        t = v.as_tuple()\n        return ('D', (len(t[1]), (0 - t[2])))\n    elif isinstance(v, datetime.datetime):\n        return ('dt',)\n    elif isinstance(v, datetime.date):\n        return ('d',)\n    elif isinstance(v, datetime.time):\n        return ('t',)\n    elif isinstance(v, (bytearray, buffer)):\n        return ('bi', (((len(v) // 1000) + 1) * 1000))\n    return type(v)\n", "label": 1}
{"function": "\n\ndef run(self, active=None, hostid=None, hostgroupid=None, servicenameid=None, servicegroupid=None, servicetype=None, eventtype=None, state=None, datefrom=None, results=None, startindex=None, sort_key=None, sort_dir=None, dateto=None):\n    self.login()\n    data = {\n        \n    }\n    if (active is not None):\n        data['active'] = active\n    if (hostid is not None):\n        data['description'] = hostid\n    if (hostgroupid is not None):\n        data['hostgroupid'] = hostgroupid\n    if (servicenameid is not None):\n        data['servicenameid'] = servicenameid\n    if (servicegroupid is not None):\n        data['servicegroupid'] = servicegroupid\n    if (servicetype is not None):\n        data['servicetype'] = servicetype\n    if (eventtype is not None):\n        data['eventtype'] = eventtype\n    if (state is not None):\n        data['state'] = state\n    if (datefrom is not None):\n        data['datefrom'] = datefrom\n    if (results is not None):\n        data['results'] = results\n    if (startindex is not None):\n        data['startindex'] = startindex\n    if (sort_key is not None):\n        data['sort'] = sort_key\n    if (sort_dir is not None):\n        data['dir'] = sort_dir\n    if (dateto is not None):\n        data['dateto'] = dateto\n    req = self.session.post('{}/reports/events/list'.format(self.url), data=data)\n    try:\n        return req.json()\n    except Exception:\n        raise\n    finally:\n        self.logout()\n", "label": 1}
{"function": "\n\ndef test_invert(self, matrix_c):\n    c_inv = matrix_c.I\n    assert (c_inv[(0, 0)] == 2.25)\n    assert (c_inv[(0, 1)] == (- 1.0))\n    assert (c_inv[(0, 2)] == (- 0.5))\n    assert close_enough(c_inv[(0, 3)], (- 2.96059473e-16))\n    assert (c_inv[(1, 0)] == (- 1.0))\n    assert (c_inv[(1, 1)] == 2)\n    assert (c_inv[(1, 2)] == 1)\n    assert (c_inv[(1, 3)] == (- 1))\n    assert (c_inv[(2, 0)] == (- 0.5))\n    assert (c_inv[(2, 1)] == 1)\n    assert (c_inv[(2, 2)] == 1.25)\n    assert (c_inv[(2, 3)] == (- 1))\n    assert close_enough(c_inv[(3, 0)], (- 2.66453526e-15))\n    assert (c_inv[(3, 1)] == (- 1))\n    assert (c_inv[(3, 2)] == (- 1))\n    assert (c_inv[(3, 3)] == 1)\n", "label": 1}
{"function": "\n\ndef _handle_events(self, fd, events):\n    if self.closed():\n        gen_log.warning('Got events for closed stream %s', fd)\n        return\n    try:\n        if self._connecting:\n            self._handle_connect()\n        if self.closed():\n            return\n        if (events & self.io_loop.READ):\n            self._handle_read()\n        if self.closed():\n            return\n        if (events & self.io_loop.WRITE):\n            self._handle_write()\n        if self.closed():\n            return\n        if (events & self.io_loop.ERROR):\n            self.error = self.get_fd_error()\n            self.io_loop.add_callback(self.close)\n            return\n        state = self.io_loop.ERROR\n        if self.reading():\n            state |= self.io_loop.READ\n        if self.writing():\n            state |= self.io_loop.WRITE\n        if ((state == self.io_loop.ERROR) and (self._read_buffer_size == 0)):\n            state |= self.io_loop.READ\n        if (state != self._state):\n            assert (self._state is not None), \"shouldn't happen: _handle_events without self._state\"\n            self._state = state\n            self.io_loop.update_handler(self.fileno(), self._state)\n    except UnsatisfiableReadError as e:\n        gen_log.info(('Unsatisfiable read, closing connection: %s' % e))\n        self.close(exc_info=True)\n    except Exception:\n        gen_log.error('Uncaught exception, closing connection.', exc_info=True)\n        self.close(exc_info=True)\n        raise\n", "label": 1}
{"function": "\n\ndef diff_map(self, inconstrs):\n    'Generate SQL to transform existing constraints\\n\\n        :param inconstrs: a YAML map defining the new constraints\\n        :return: list of SQL statements\\n\\n        Compares the existing constraint definitions, as fetched from\\n        the catalogs, to the input map and generates SQL statements to\\n        transform the constraints accordingly.\\n        '\n    stmts = []\n    for turn in (1, 2):\n        for (sch, tbl, cns) in self:\n            constr = self[(sch, tbl, cns)]\n            if getattr(constr, 'inherited', False):\n                continue\n            if isinstance(constr, ForeignKey):\n                if (turn == 1):\n                    continue\n            elif (turn == 2):\n                continue\n            if (((sch, tbl, cns) not in inconstrs) and (not hasattr(constr, 'target'))):\n                stmts.append(constr.drop())\n        for (sch, tbl, cns) in inconstrs:\n            inconstr = inconstrs[(sch, tbl, cns)]\n            if getattr(inconstr, 'inherited', False):\n                continue\n            if hasattr(inconstr, 'target'):\n                continue\n            if isinstance(inconstr, ForeignKey):\n                if (turn == 1):\n                    continue\n            elif (turn == 2):\n                continue\n            if ((sch, tbl, cns) not in self):\n                stmts.append(inconstr.add())\n            else:\n                stmts.append(self[(sch, tbl, cns)].diff_map(inconstr))\n    return stmts\n", "label": 1}
{"function": "\n\ndef has_metaclass(parent):\n    results = None\n    for node in parent.children:\n        kids = node.children\n        if (node.type == syms.argument):\n            if ((kids[0] == Leaf(token.NAME, 'metaclass')) and (kids[1] == Leaf(token.EQUAL, '=')) and kids[2]):\n                results = ([node] + kids)\n                break\n        elif (node.type == syms.arglist):\n            for child in node.children:\n                if results:\n                    break\n                if (child.type == token.COMMA):\n                    comma = child\n                elif (type(child) == Node):\n                    meta = equal = name = None\n                    for arg in child.children:\n                        if (arg == Leaf(token.NAME, 'metaclass')):\n                            meta = arg\n                        elif (meta and (arg == Leaf(token.EQUAL, '='))):\n                            equal = arg\n                        elif (meta and equal):\n                            name = arg\n                            results = (comma, meta, equal, name)\n                            break\n    return results\n", "label": 1}
{"function": "\n\ndef declare_vars(v):\n    '\\n    Yields the Javascript lines that declare the given  :class:`Variable` `v`.\\n    If `v` is a :class:`Component`, `list`, `tuple` or `dict` which contains\\n    other variables, recursively yields also the lines to declare these.\\n    '\n    if isinstance(v, (list, tuple)):\n        for sub in v:\n            for ln in declare_vars(sub):\n                (yield ln)\n        return\n    if isinstance(v, dict):\n        for sub in list(v.values()):\n            for ln in declare_vars(sub):\n                (yield ln)\n        return\n    if (isinstance(v, VisibleComponent) and (not v.get_view_permission(_for_user_profile))):\n        return\n    if isinstance(v, Component):\n        for sub in list(v.ext_options().values()):\n            for ln in declare_vars(sub):\n                (yield ln)\n    elif isinstance(v, Value):\n        for ln in declare_vars(v.value):\n            (yield ln)\n    if isinstance(v, Variable):\n        if (v.declare_type == DECLARE_VAR):\n            (yield ('var %s = %s;' % (v.ext_name, v.js_value())))\n        elif (v.declare_type == DECLARE_THIS):\n            (yield ('this.%s = %s;' % (v.ext_name, v.js_value())))\n", "label": 1}
{"function": "\n\ndef test_check_equality(self, testdir):\n    modcol = testdir.getmodulecol('\\n            def test_pass(): pass\\n            def test_fail(): assert 0\\n        ')\n    fn1 = testdir.collect_by_name(modcol, 'test_pass')\n    assert isinstance(fn1, pytest.Function)\n    fn2 = testdir.collect_by_name(modcol, 'test_pass')\n    assert isinstance(fn2, pytest.Function)\n    assert (fn1 == fn2)\n    assert (fn1 != modcol)\n    if (py.std.sys.version_info < (3, 0)):\n        assert (cmp(fn1, fn2) == 0)\n    assert (hash(fn1) == hash(fn2))\n    fn3 = testdir.collect_by_name(modcol, 'test_fail')\n    assert isinstance(fn3, pytest.Function)\n    assert (not (fn1 == fn3))\n    assert (fn1 != fn3)\n    for fn in (fn1, fn2, fn3):\n        assert (fn != 3)\n        assert (fn != modcol)\n        assert (fn != [1, 2, 3])\n        assert ([1, 2, 3] != fn)\n        assert (modcol != fn)\n", "label": 1}
{"function": "\n\ndef test_encode_group2(self):\n    out = encode(self.group2)\n    assert isinstance(out, dict)\n    assert (out['__class__'] == 'Group')\n    assert isinstance(out['par1'], dict)\n    assert (out['par1']['__class__'] == 'Parameter')\n    assert (out['par1']['name'] == 'p1')\n    assert (out['par1']['value'] == 3.0)\n    assert (out['par1']['min'] == 0.0)\n    assert (out['par2']['__class__'] == 'Parameter')\n    assert (out['par2']['name'] == 'p2')\n    assert (out['par2']['vary'] == True)\n    assert (out['par2']['value'] == 1.0)\n    assert (out['sub']['__class__'] == 'Group')\n    assert (out['sub']['label'] == 'a label')\n    assert (out['sub']['x']['__class__'] == 'Array')\n    assert (out['sub']['x']['__class__'] == 'Array')\n    assert_allclose(out['sub']['x']['value'][:3], [0, 0.5, 1.0], rtol=0.0001)\n", "label": 1}
{"function": "\n\n@connected\ndef execute(self, query, params=None, multiparams=None):\n    \"Execute query against current db connection, return result set.\\n\\n        Args:\\n            query: String query to execute.\\n            args: Dictionary of bind parameters for the query.\\n            multiargs: Collection/iterable of dictionaries of bind parameters.\\n        Returns:\\n            Sqlalchemy's DB-API cursor-like object.\\n        \"\n    rereflect = False\n    ddl_commands = 'create drop alter truncate rename'.split()\n    want_tx = 'insert update delete merge replace'.split()\n    result = None\n    if (params is None):\n        params = {\n            \n        }\n    if (multiparams is None):\n        multiparams = []\n    bits = query.split()\n    if ((len(bits) == 2) and (bits[0].lower() == 'select') and (bits[1] in self.get_metadata().tables)):\n        query = ('select * from %s' % bits[1])\n    elif ((bits[0].lower() in want_tx) and (not self.trans_ctx) and (not self.autocommit)):\n        self.begin()\n    elif (bits[0].lower() in ddl_commands):\n        rereflect = True\n    conn = self.engine\n    if (self.trans_ctx and self.trans_ctx.transaction.is_active):\n        conn = self.trans_ctx.conn\n    try:\n        result = conn.execute(query, *multiparams, **params)\n        if (rereflect and self.do_reflection):\n            self.metadata_accessor.get_metadata(self.engine, force=True, noisy=True)\n    except Exception as e:\n        if self.debug:\n            raise\n        print(e.message)\n    return result\n", "label": 1}
{"function": "\n\ndef process(self, processed, glyphRecords, featureTag):\n    performedAction = False\n    currentRecord = glyphRecords[0]\n    currentGlyph = currentRecord.glyphName\n    if (currentGlyph in self.Coverage):\n        for chainRuleSet in self._ChainRuleSet:\n            for chainRule in chainRuleSet._ChainRule:\n                backtrackCount = chainRule.BacktrackGlyphCount\n                if (not backtrackCount):\n                    backtrackMatch = True\n                else:\n                    (backtrackMatch, backtrack, backtrackMatchIndexes) = self._testContext(reversed(processed), chainRule.Backtrack, backtrackCount)\n                if (not backtrackMatch):\n                    continue\n                inputCount = chainRule.InputGlyphCount\n                if (not inputCount):\n                    inputMatch = True\n                else:\n                    (inputMatch, input, inputMatchIndexes) = self._testContext(glyphRecords[1:], chainRule.Input, (inputCount - 1))\n                if (not inputMatch):\n                    continue\n                input = ([currentRecord] + input)\n                inputMatchIndexes = ([0] + [(i + 1) for i in inputMatchIndexes])\n                lookAheadCount = chainRule.LookAheadGlyphCount\n                if (not lookAheadCount):\n                    lookAheadMatch = True\n                else:\n                    (lookAheadMatch, lookAhead, lookAheadMatchIndexes) = self._testContext(glyphRecords[len(input):], chainRule.LookAhead, lookAheadCount)\n                if (not lookAheadMatch):\n                    continue\n                if (backtrackMatch and inputMatch and lookAheadMatch):\n                    (processed, glyphRecords, performedAction) = self._processMatch(chainRule, processed, glyphRecords, len(input), inputMatchIndexes, featureTag)\n                    if performedAction:\n                        break\n            if performedAction:\n                break\n    return (processed, glyphRecords, performedAction)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_index_spec_ != x.has_index_spec_):\n        return 0\n    if (self.has_index_spec_ and (self.index_spec_ != x.index_spec_)):\n        return 0\n    if (self.has_start_doc_id_ != x.has_start_doc_id_):\n        return 0\n    if (self.has_start_doc_id_ and (self.start_doc_id_ != x.start_doc_id_)):\n        return 0\n    if (self.has_include_start_doc_ != x.has_include_start_doc_):\n        return 0\n    if (self.has_include_start_doc_ and (self.include_start_doc_ != x.include_start_doc_)):\n        return 0\n    if (self.has_limit_ != x.has_limit_):\n        return 0\n    if (self.has_limit_ and (self.limit_ != x.limit_)):\n        return 0\n    if (self.has_keys_only_ != x.has_keys_only_):\n        return 0\n    if (self.has_keys_only_ and (self.keys_only_ != x.keys_only_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef get_js_from_html(html, root):\n    ' Given an html document provided as a string, extract the\\n    JavaScript.\\n    '\n    parts = []\n    i = 0\n    while True:\n        i = html.find('<script', i)\n        if ((i < 0) or (i > (len(html) - 5))):\n            break\n        i_end1 = html.find('>', (i + 6))\n        i_end2 = html.find('/>', (i + 6))\n        i_end3 = html.find('</script>', (i + 6))\n        i_src = html.find('src=', (i + 6))\n        ends = [j for j in (i_end2, i_end3) if (j > 0)]\n        if (not ends):\n            break\n        i_end = min(ends)\n        i = i_end\n        if ((i_src > 0) and (i_src < i_end1)):\n            i1 = (i_src + 5)\n            quote = html[(i1 - 1)]\n            if (quote not in '\"\\''):\n                continue\n            i2 = html.find(quote, i1)\n            fname = html[i1:i2]\n            for filename in (fname, ((root + '/') + fname)):\n                if filename.startswith('http'):\n                    code = urlopen(filename, timeout=5.0).read().decode()\n                    break\n                elif os.path.isfile(filename):\n                    code = open(filename, 'rb').read().decode()\n                    break\n            else:\n                raise IOError(('Could not get JS for file %r' % fname))\n            parts.append(code)\n        elif ((i_end == i_end3) and (i_end > i_end1)):\n            i1 = (i_end1 + 1)\n            i2 = i_end\n            parts.append(html[i1:i2])\n        else:\n            pass\n    return '\\n'.join(parts)\n", "label": 1}
{"function": "\n\ndef _complement(self, other):\n    if isinstance(other, ProductSet):\n        switch_sets = ProductSet((FiniteSet(o, (o - s)) for (s, o) in zip(self.sets, other.sets)))\n        product_sets = (ProductSet(*set) for set in switch_sets)\n        return Union((p for p in product_sets if (p != other)))\n    elif isinstance(other, Interval):\n        if (isinstance(self, Interval) or isinstance(self, FiniteSet)):\n            return Intersection(other, self.complement(S.Reals))\n    elif isinstance(other, Union):\n        return Union(((o - self) for o in other.args))\n    elif isinstance(other, Complement):\n        return Complement(other.args[0], Union(other.args[1], self), evaluate=False)\n    elif isinstance(other, EmptySet):\n        return S.EmptySet\n    elif isinstance(other, FiniteSet):\n        return FiniteSet(*[el for el in other if (self.contains(el) != True)])\n", "label": 1}
{"function": "\n\ndef set_aggregates(self, force=False):\n    \"\\n        Sets any aggregates that haven't been generated yet, or recalculates\\n        them if the force option is indicated.\\n        \"\n    aggregates = {\n        \n    }\n    if ((not self.avg_time) or force):\n        aggregates['avg_time'] = models.Avg('timer_total')\n    if ((not self.avg_cpu_time) or force):\n        aggregates['avg_cpu_time'] = models.Avg('timer_cputime')\n    if ((not self.avg_sql_time) or force):\n        aggregates['avg_sql_time'] = models.Avg('sql_time')\n    if ((not self.avg_sql_queries) or force):\n        aggregates['avg_sql_queries'] = models.Avg('sql_num_queries')\n    if ((not self.total_sql_queries) or force):\n        aggregates['total_sql_queries'] = models.Sum('sql_num_queries')\n    if ((not self.max_sql_queries) or force):\n        aggregates['max_sql_queries'] = models.Max('sql_num_queries')\n    if ((not self.total_requests) or force):\n        aggregates['total_requests'] = models.Count('pk')\n    if aggregates:\n        aggregated = self.records.aggregate(**aggregates)\n        for (key, value) in aggregated.items():\n            setattr(self, key, value)\n", "label": 1}
{"function": "\n\ndef do_no_ip(self, *args):\n    if 'address'.startswith(args[0]):\n        if (len(args) == 1):\n            self.port.ips = []\n        else:\n            ip = IPNetwork(('%s/%s' % (args[1], args[2])))\n            is_secondary = ('secondary'.startswith(args[3]) if (len(args) == 4) else False)\n            if is_secondary:\n                self.port.remove_ip(ip)\n            elif (len(self.port.ips) == 1):\n                self.port.remove_ip(ip)\n            else:\n                self.write_line('Must delete secondary before deleting primary')\n    if 'access-group'.startswith(args[0]):\n        direction = args[(- 1)]\n        if 'in'.startswith(direction):\n            self.port.access_group_in = None\n        elif 'out'.startswith(direction):\n            self.port.access_group_out = None\n    if 'vrf'.startswith(args[0]):\n        if 'forwarding'.startswith(args[1]):\n            self.port.vrf = None\n    if 'redirects'.startswith(args[0]):\n        self.port.ip_redirect = False\n    if 'helper-address'.startswith(args[0]):\n        if (len(args) > 2):\n            self.write_line(' ^')\n            self.write_line(\"% Invalid input detected at '^' marker.\")\n            self.write_line('')\n        elif (len(args) == 1):\n            self.port.ip_helpers = []\n        else:\n            ip_address = IPAddress(args[1])\n            if (ip_address in self.port.ip_helpers):\n                self.port.ip_helpers.remove(ip_address)\n", "label": 1}
{"function": "\n\ndef to_python(self, obj):\n    if isinstance(obj, QuerySet):\n        return [self.to_python(item) for item in obj]\n    if (not obj):\n        return obj\n    if issubclass(obj.__class__, Document):\n        return_data = []\n        self.current_level += 1\n        if (self.levels and (self.current_level > self.levels)):\n            return_data.append(('id', str(obj.id)))\n            return_data.append(('__str__', unicode(obj)))\n        else:\n            for field_name in obj._fields:\n                if (field_name in self.exclude):\n                    continue\n                if (self.fields and (not (field_name in self.fields))):\n                    continue\n                data = getattr(obj, field_name, '')\n                field_type = obj._fields[field_name]\n                if isinstance(field_type, StringField):\n                    return_data.append((field_name, unicode(data)))\n                elif isinstance(field_type, FloatField):\n                    return_data.append((field_name, float(data)))\n                elif isinstance(field_type, IntField):\n                    return_data.append((field_name, int(data)))\n                elif isinstance(field_type, ListField):\n                    return_data.append((field_name, [self.to_python(item) for item in data]))\n                elif isinstance(field_type, ReferenceField):\n                    return_data.append((field_name, self.to_python(data)))\n                else:\n                    return_data.append((field_name, unicode(data)))\n        self.current_level -= 1\n        return dict(return_data)\n    return {\n        \n    }\n", "label": 1}
{"function": "\n\ndef test_post(self):\n    'Test that POST to this endpoint works correctly.'\n    self.make_participant('alice', claimed_time='now')\n    Enterprise = self.make_team('The Enterprise', is_approved=True)\n    Trident = self.make_team('The Trident', is_approved=True)\n    request_body = [{\n        'amount': '1.50',\n        'team_slug': Enterprise.slug,\n    }, {\n        'amount': '39.50',\n        'team_slug': Trident.slug,\n    }]\n    response = self.client.POST('~/alice/payment-instructions.json', body=json.dumps(request_body), content_type='application/json', auth_as='alice')\n    assert (response.code == 200)\n    data = json.loads(response.body)\n    assert (len(data) == 2)\n    assert (data[0]['team_slug'] == Enterprise.slug)\n    assert (data[0]['team_name'] == Enterprise.name)\n    assert (data[0]['amount'] == '1.50')\n    assert (data[1]['team_slug'] == Trident.slug)\n    assert (data[1]['team_name'] == Trident.name)\n    assert (data[1]['amount'] == '39.50')\n    for d in data:\n        assert ('due' in d)\n        assert ('ctime' in d)\n        assert ('mtime' in d)\n    data = json.loads(self.client.GET('~/alice/payment-instructions.json', auth_as='alice').body)\n    assert (data[0]['team_slug'] == Trident.slug)\n    assert (data[0]['amount'] == '39.50')\n    assert (data[1]['team_slug'] == Enterprise.slug)\n    assert (data[1]['amount'] == '1.50')\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('cls', [AsciiTable, UnixTable])\ndef test_attributes(cls):\n    'Test different table attributes.'\n    table_data = [['Name', 'Color', 'Type'], ['Avocado', 'green', 'nut'], ['Tomato', 'red', 'fruit'], ['Lettuce', 'green', 'vegetable']]\n    table = cls(table_data)\n    table.outer_border = False\n    assert (table.column_max_width(0) == 58)\n    assert (table.column_max_width(1) == 56)\n    assert (table.column_max_width(2) == 60)\n    table.outer_border = True\n    table.inner_column_border = False\n    assert (table.column_max_width(0) == 58)\n    assert (table.column_max_width(1) == 56)\n    assert (table.column_max_width(2) == 60)\n    table.outer_border = False\n    assert (table.column_max_width(0) == 60)\n    assert (table.column_max_width(1) == 58)\n    assert (table.column_max_width(2) == 62)\n    table.outer_border = True\n    table.inner_column_border = True\n    table.padding_left = 0\n    assert (table.column_max_width(0) == 59)\n    assert (table.column_max_width(1) == 57)\n    assert (table.column_max_width(2) == 61)\n    table.padding_right = 5\n    assert (table.column_max_width(0) == 47)\n    assert (table.column_max_width(1) == 45)\n    assert (table.column_max_width(2) == 49)\n", "label": 1}
{"function": "\n\ndef get_closest_mode(self, width, height):\n    'Get the screen mode that best matches a given size.\\n\\n        If no supported mode exactly equals the requested size, a larger one\\n        is returned; or ``None`` if no mode is large enough.\\n\\n        :Parameters:\\n            `width` : int\\n                Requested screen width.\\n            `height` : int\\n                Requested screen height.\\n\\n        :rtype: `ScreenMode`\\n\\n        :since: pyglet 1.2\\n        '\n    current = self.get_mode()\n    best = None\n    for mode in self.get_modes():\n        if ((mode.width < width) or (mode.height < height)):\n            continue\n        if (best is None):\n            best = mode\n        if ((mode.width <= best.width) and (mode.height <= best.height) and ((mode.width < best.width) or (mode.height < best.height))):\n            best = mode\n        if ((mode.width == best.width) and (mode.height == best.height)):\n            points = 0\n            if (mode.rate == current.rate):\n                points += 2\n            if (best.rate == current.rate):\n                points -= 2\n            if (mode.depth == current.depth):\n                points += 1\n            if (best.depth == current.depth):\n                points -= 1\n            if (points > 0):\n                best = mode\n    return best\n", "label": 1}
{"function": "\n\ndef sort_models(self, unsorted):\n    sorted = []\n    hope = True\n    '\\n        20121120 if we convert the list to a set, we gain some performance \\n        for the ``in`` tests, but we obtain a random sorting order for all \\n        independent models, making the double dump test less evident.\\n        '\n    while (len(unsorted) and hope):\n        hope = False\n        guilty = dict()\n        for model in unsorted:\n            deps = set([f.rel.model for f in model._meta.fields if ((f.rel is not None) and (f.rel.model is not model) and (f.rel.model in unsorted))])\n            for m in sorted:\n                if (m in deps):\n                    deps.remove(m)\n            if len(deps):\n                guilty[model] = deps\n            else:\n                sorted.append(model)\n                unsorted.remove(model)\n                hope = True\n                break\n    if unsorted:\n        assert (len(unsorted) == len(guilty))\n        msg = ('There are %d models with circular dependencies :\\n' % len(unsorted))\n        msg += ('- ' + '\\n- '.join([(full_model_name(m) + (' (depends on %s)' % ', '.join([full_model_name(d) for d in deps]))) for (m, deps) in list(guilty.items())]))\n        if False:\n            for ln in msg.splitlines():\n                self.stream.write(('\\n# %s' % ln))\n        logger.info(msg)\n        sorted.extend(unsorted)\n    return sorted\n", "label": 1}
{"function": "\n\ndef get_download_urls(self, package_name, version='', pkg_type='all'):\n    'Query PyPI for pkg download URI for a packge'\n    if version:\n        versions = [version]\n    else:\n        (package_name, versions) = self.query_versions_pypi(package_name)\n    all_urls = []\n    for ver in versions:\n        metadata = self.release_data(package_name, ver)\n        for urls in self.release_urls(package_name, ver):\n            if ((pkg_type == 'source') and (urls['packagetype'] == 'sdist')):\n                all_urls.append(urls['url'])\n            elif ((pkg_type == 'egg') and urls['packagetype'].startswith('bdist')):\n                all_urls.append(urls['url'])\n            elif (pkg_type == 'all'):\n                all_urls.append(urls['url'])\n        if (metadata and metadata.has_key('download_url') and (metadata['download_url'] != 'UNKNOWN') and (metadata['download_url'] != None)):\n            if (metadata['download_url'] not in all_urls):\n                if (pkg_type != 'all'):\n                    url = filter_url(pkg_type, metadata['download_url'])\n                    if url:\n                        all_urls.append(url)\n    return all_urls\n", "label": 1}
{"function": "\n\ndef check_data_classes(test, classes):\n    import inspect\n    for data_class in classes:\n        test.assert_((data_class.__doc__ is not None), ('The class %s should have a docstring' % data_class))\n        if hasattr(data_class, '_qname'):\n            qname_versions = None\n            if isinstance(data_class._qname, tuple):\n                qname_versions = data_class._qname\n            else:\n                qname_versions = (data_class._qname,)\n            for versioned_qname in qname_versions:\n                test.assert_(isinstance(versioned_qname, str), ('The class %s has a non-string _qname' % data_class))\n                test.assert_((not versioned_qname.endswith('}')), ('The _qname for class %s is only a namespace' % data_class))\n        for (attribute_name, value) in data_class.__dict__.items():\n            if (not attribute_name.startswith('_')):\n                try:\n                    if (not (isinstance(value, str) or inspect.isfunction(value) or (isinstance(value, list) and issubclass(value[0], atom.core.XmlElement)) or (type(value) == property) or inspect.ismethod(value) or inspect.ismethoddescriptor(value) or issubclass(value, atom.core.XmlElement))):\n                        test.fail('XmlElement member should have an attribute, XML class, or list of XML classes as attributes.')\n                except TypeError:\n                    test.fail(('Element %s in %s was of type %s' % (attribute_name, data_class._qname, type(value))))\n", "label": 1}
{"function": "\n\ndef _crop_and_border(martix):\n    (t, b, l, r) = (0, 0, 0, 0)\n    for y in xrange(len(martix)):\n        if (sum(martix[y]) == 0):\n            t += 1\n        else:\n            break\n    for y in xrange(len(martix)):\n        if (sum(martix[((- 1) - y)]) == 0):\n            b += 1\n        else:\n            break\n    for x in xrange(len(martix[0])):\n        if (sum(map((lambda row: row[x]), martix)) == 0):\n            l += 1\n        else:\n            break\n    for x in xrange(len(martix[0])):\n        if (sum(map((lambda row: row[((- 1) - x)]), martix)) == 0):\n            r += 1\n        else:\n            break\n    w = len(martix[0])\n    if (t > 0):\n        martix = martix[(t - 1):]\n    else:\n        martix.insert(0, ([0] * w))\n    if (b > 1):\n        martix = martix[:(1 - b)]\n    elif (b == 0):\n        martix.append(([0] * w))\n    for ri in xrange(len(martix)):\n        row = martix[ri]\n        if (l > 0):\n            row = row[(l - 1):]\n        else:\n            row.insert(0, 0)\n        if (r > 1):\n            row = row[:(1 - r)]\n        elif (r == 0):\n            row.append(0)\n        martix[ri] = row\n    return martix\n", "label": 1}
{"function": "\n\n@operation(pipeline_facts={\n    'directory': 'name',\n})\ndef directory(state, host, name, present=True, user=None, group=None, mode=None, recursive=False):\n    '\\n    Manage the state of directories.\\n\\n    + name: name/patr of the remote file\\n    + present: whether the file should exist\\n    + user: user to own the files\\n    + group: group to own the files\\n    + mode: permissions of the files\\n    + recursive: recursively apply user/group/mode\\n    '\n    mode = ensure_mode_int(mode)\n    info = host.fact.directory(name)\n    commands = []\n    if (info is False):\n        raise OperationError('{0} exists and is not a directory'.format(name))\n    if ((info is None) and present):\n        commands.append('mkdir -p {0}'.format(name))\n        if mode:\n            commands.append(chmod(name, mode, recursive=recursive))\n        if (user or group):\n            commands.append(chown(name, user, group, recursive=recursive))\n    elif (info and (not present)):\n        commands.append('rm -rf {0}'.format(name))\n    elif (info and present):\n        if (mode and (info['mode'] != mode)):\n            commands.append(chmod(name, mode, recursive=recursive))\n        if ((user and (info['user'] != user)) or (group and (info['group'] != group))):\n            commands.append(chown(name, user, group, recursive=recursive))\n    return commands\n", "label": 1}
{"function": "\n\ndef scan_applications(self):\n    paths = self.system_path()\n    applications = []\n    for app_path in self.application_paths():\n        for filename in os.listdir(app_path):\n            pathname = os.path.join(app_path, filename)\n            if os.path.isfile(pathname):\n                with codecs.open(pathname, 'r', encoding=system_encoding, errors='ignore') as f:\n                    name = None\n                    command = None\n                    terminal = None\n                    for line in f.readlines():\n                        if (line[0:5] == 'Exec='):\n                            command_tmp = line[5:(- 1)].split()\n                            command = ''\n                            space = ''\n                            for piece in command_tmp:\n                                if (piece.find('%') == (- 1)):\n                                    command += (space + piece)\n                                    space = ' '\n                                else:\n                                    break\n                        elif (line[0:5] == 'Name='):\n                            name = line[5:(- 1)]\n                        elif (line[0:9] == 'Terminal='):\n                            if (line[9:(- 1)].lower() == 'true'):\n                                terminal = True\n                            else:\n                                terminal = False\n                        if ((name is not None) and (command is not None)):\n                            if (terminal is None):\n                                terminal = False\n                            for path in paths:\n                                if (command[0:len(path)] == path):\n                                    if (command[(len(path) + 1):].find('/') == (- 1)):\n                                        command = command[(len(path) + 1):]\n                            applications.append({\n                                'name': name,\n                                'command': command,\n                                'terminal': terminal,\n                                'descriptor': filename.replace('.desktop', ''),\n                            })\n                            break\n    return applications\n", "label": 1}
{"function": "\n\ndef assertMessageCount(self, response=None, **kwargs):\n    'Asserts that the specified number of messages have been attached\\n        for various message types. Usage would look like\\n        ``self.assertMessageCount(success=1)``.\\n        '\n    temp_req = self.client.request(**{\n        'wsgi.input': None,\n    })\n    temp_req.COOKIES = self.client.cookies\n    storage = default_storage(temp_req)\n    messages = []\n    if (response is None):\n        if ('messages' in self.client.cookies):\n            message_cookie = self.client.cookies['messages'].value\n            messages = storage._decode(message_cookie)\n    elif (hasattr(response, 'context') and ('messages' in response.context)):\n        messages = response.context['messages']\n    elif (hasattr(response, '_request') and hasattr(response._request, '_messages')):\n        messages = response._request._messages._queued_messages\n    if ((not any(kwargs.values())) and (not messages)):\n        return\n    if (any(kwargs.values()) and (not messages)):\n        error_msg = 'Messages were expected, but none were set.'\n        assert (0 == sum(kwargs.values())), error_msg\n    for (msg_type, count) in kwargs.items():\n        msgs = [force_text(m.message) for m in messages if (msg_type in m.tags)]\n        assert (len(msgs) == count), ('%s messages not as expected: %s' % (msg_type.title(), ', '.join(msgs)))\n", "label": 1}
{"function": "\n\ndef perform(self, node, inp, out_):\n    (x, y, idx) = inp\n    (out,) = out_\n    if (not self.inplace):\n        x = x.copy()\n    assert (y.ndim <= x.ndim)\n    if self.set_instead_of_inc:\n        if (y.ndim == x.ndim):\n            if (len(y) == 1):\n                y_0 = y[0]\n                for i in idx:\n                    x[i] = y_0\n            else:\n                assert (len(y) == len(idx))\n                j = 0\n                for i in idx:\n                    x[i] = y[j]\n                    j += 1\n        else:\n            for i in idx:\n                x[i] = y\n    elif (y.ndim == x.ndim):\n        if (len(y) == 1):\n            y_0 = y[0]\n            for i in idx:\n                x[i] += y_0\n        else:\n            assert (len(y) == len(idx))\n            j = 0\n            for i in idx:\n                x[i] += y[j]\n                j += 1\n    else:\n        for i in idx:\n            x[i] += y\n    out[0] = x\n", "label": 1}
{"function": "\n\ndef _produceNewSample(self):\n    ' returns a new sample, its fitness and its densities '\n    chosenOne = drawIndex(self.alphas, True)\n    mu = self.mus[chosenOne]\n    if self.useAnticipatedMeanShift:\n        if (((len(self.allsamples) % 2) == 1) and (len(self.allsamples) > 1)):\n            if (not (self.elitism and (chosenOne == self.bestChosenCenter))):\n                mu += self.meanShifts[chosenOne]\n    if self.diagonalOnly:\n        sample = normal(mu, self.sigmas[chosenOne])\n    else:\n        sample = multivariate_normal(mu, self.sigmas[chosenOne])\n    if (self.sampleElitism and (len(self.allsamples) > self.windowSize) and ((len(self.allsamples) % self.windowSize) == 0)):\n        sample = self.bestEvaluable.copy()\n    fit = self._oneEvaluation(sample)\n    if (((not self.minimize) and (fit >= self.bestEvaluation)) or (self.minimize and (fit <= self.bestEvaluation)) or (len(self.allsamples) == 0)):\n        self.bestChosenCenter = chosenOne\n        self.bestSigma = self.sigmas[chosenOne].copy()\n    if self.minimize:\n        fit = (- fit)\n    self.allfitnesses.append(fit)\n    self.allsamples.append(sample)\n    return (sample, fit)\n", "label": 1}
{"function": "\n\ndef _validate_select_where(self):\n    ' Checks that a filterset will not create invalid select statement '\n    equal_ops = [self.model._get_column_by_db_name(w.field) for w in self._where if isinstance(w.operator, EqualsOperator)]\n    token_comparison = any([w for w in self._where if isinstance(w.value, Token)])\n    if ((not any(((w.primary_key or w.index) for w in equal_ops))) and (not token_comparison) and (not self._allow_filtering)):\n        raise QueryException('Where clauses require either  =, a IN or a CONTAINS (collection) comparison with either a primary key or indexed field')\n    if (not self._allow_filtering):\n        if (not any((w.index for w in equal_ops))):\n            if ((not any([w.partition_key for w in equal_ops])) and (not token_comparison)):\n                raise QueryException('Filtering on a clustering key without a partition key is not allowed unless allow_filtering() is called on the querset')\n", "label": 1}
{"function": "\n\ndef test_default_enum():\n    good_values = ('a', 'b', 'c')\n    for good_val in good_values:\n        for schema in (Enum.using(valid_values=good_values), Enum.valued(*good_values)):\n            el = schema()\n            assert el.set(good_val)\n            assert (el.value == good_val)\n            assert (el.u == good_val)\n            assert el.validate()\n            assert (not el.errors)\n    schema = Enum.valued(*good_values)\n    el = schema()\n    assert (not el.set('d'))\n    assert (el.value is None)\n    assert (el.u == 'd')\n    assert el.validate()\n    el = schema()\n    assert (not el.set(None))\n    assert (el.value is None)\n    assert (el.u == '')\n    assert (not el.validate())\n", "label": 1}
{"function": "\n\ndef filter(vulns, exploitsOnly=False, filters={\n    \n}):\n    if (('access' in filters) and (len(filters['access']) != 0)):\n        vulns = [x for x in vulns if (('access' in x) and (x['access']['vector'] in filters['access']))]\n    if (('impact' in filters) and (len(filters['impact']) != 0)):\n        for fil in filters['impact']:\n            vulns = [x for x in vulns if (('impact' in x) and (x['impact'][fil] != 'NONE'))]\n    if exploitsOnly:\n        vulns = [x for x in vulns if (('map_cve_exploitdb' in x) or ('map_cve_msf' in x))]\n    return vulns\n", "label": 1}
{"function": "\n\ndef c2ln(c, l1, l2, n):\n    'char[n] to two unsigned long???'\n    c = (c + n)\n    (l1, l2) = (U32(0), U32(0))\n    f = 0\n    if (n == 8):\n        l2 = (l2 | (U32(c[7]) << 24))\n        f = 1\n    if (f or (n == 7)):\n        l2 = (l2 | (U32(c[6]) << 16))\n        f = 1\n    if (f or (n == 6)):\n        l2 = (l2 | (U32(c[5]) << 8))\n        f = 1\n    if (f or (n == 5)):\n        l2 = (l2 | U32(c[4]))\n        f = 1\n    if (f or (n == 4)):\n        l1 = (l1 | (U32(c[3]) << 24))\n        f = 1\n    if (f or (n == 3)):\n        l1 = (l1 | (U32(c[2]) << 16))\n        f = 1\n    if (f or (n == 2)):\n        l1 = (l1 | (U32(c[1]) << 8))\n        f = 1\n    if (f or (n == 1)):\n        l1 = (l1 | U32(c[0]))\n    return (l1, l2)\n", "label": 1}
{"function": "\n\ndef _software(self, *args, **kwargs):\n    '\\n        Return installed software.\\n        '\n    data = dict()\n    if ('exclude' in kwargs):\n        excludes = kwargs['exclude'].split(',')\n    else:\n        excludes = list()\n    os_family = __grains__.get('os_family').lower()\n    if (os_family == 'suse'):\n        LOCKS = 'pkg.list_locks'\n        if ('products' not in excludes):\n            products = __salt__['pkg.list_products']()\n            if products:\n                data['products'] = products\n    elif (os_family == 'redhat'):\n        LOCKS = 'pkg.get_locked_packages'\n    else:\n        LOCKS = None\n    if (LOCKS and ('locks' not in excludes)):\n        locks = __salt__[LOCKS]()\n        if locks:\n            data['locks'] = locks\n    if (os_family == 'suse'):\n        PATTERNS = 'pkg.list_installed_patterns'\n    elif (os_family == 'redhat'):\n        PATTERNS = 'pkg.group_list'\n    else:\n        PATTERNS = None\n    if (PATTERNS and ('patterns' not in excludes)):\n        patterns = __salt__[PATTERNS]()\n        if patterns:\n            data['patterns'] = patterns\n    if ('packages' not in excludes):\n        data['packages'] = __salt__['pkg.list_pkgs']()\n    if ('repositories' not in excludes):\n        repos = __salt__['pkg.list_repos']()\n        if repos:\n            data['repositories'] = repos\n    return data\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _get_symmetry(a):\n    (m, n) = a.shape\n    if (m != n):\n        return MMFile.SYMMETRY_GENERAL\n    issymm = True\n    isskew = True\n    isherm = (a.dtype.char in 'FD')\n    if isspmatrix(a):\n        a = a.tocoo()\n        (row, col) = a.nonzero()\n        if ((row < col).sum() != (row > col).sum()):\n            return MMFile.SYMMETRY_GENERAL\n        a = a.todok()\n\n        def symm_iterator():\n            for ((i, j), aij) in a.items():\n                if (i > j):\n                    aji = a[(j, i)]\n                    (yield (aij, aji))\n    else:\n\n        def symm_iterator():\n            for j in range(n):\n                for i in range((j + 1), n):\n                    (aij, aji) = (a[i][j], a[j][i])\n                    (yield (aij, aji))\n    for (aij, aji) in symm_iterator():\n        if (issymm and (aij != aji)):\n            issymm = False\n        if (isskew and (aij != (- aji))):\n            isskew = False\n        if (isherm and (aij != conj(aji))):\n            isherm = False\n        if (not (issymm or isskew or isherm)):\n            break\n    if issymm:\n        return MMFile.SYMMETRY_SYMMETRIC\n    if isskew:\n        return MMFile.SYMMETRY_SKEW_SYMMETRIC\n    if isherm:\n        return MMFile.SYMMETRY_HERMITIAN\n    return MMFile.SYMMETRY_GENERAL\n", "label": 1}
{"function": "\n\ndef paint_path(self, gstate, stroke, fill, evenodd, path):\n    shape = ''.join((x[0] for x in path))\n    if (shape == 'ml'):\n        (_, x0, y0) = path[0]\n        (_, x1, y1) = path[1]\n        (x0, y0) = apply_matrix_pt(self.ctm, (x0, y0))\n        (x1, y1) = apply_matrix_pt(self.ctm, (x1, y1))\n        if ((x0 == x1) or (y0 == y1)):\n            self.cur_item.add(LTLine(gstate.linewidth, (x0, y0), (x1, y1)))\n            return\n    if (shape == 'mlllh'):\n        (_, x0, y0) = path[0]\n        (_, x1, y1) = path[1]\n        (_, x2, y2) = path[2]\n        (_, x3, y3) = path[3]\n        (x0, y0) = apply_matrix_pt(self.ctm, (x0, y0))\n        (x1, y1) = apply_matrix_pt(self.ctm, (x1, y1))\n        (x2, y2) = apply_matrix_pt(self.ctm, (x2, y2))\n        (x3, y3) = apply_matrix_pt(self.ctm, (x3, y3))\n        if (((x0 == x1) and (y1 == y2) and (x2 == x3) and (y3 == y0)) or ((y0 == y1) and (x1 == x2) and (y2 == y3) and (x3 == x0))):\n            self.cur_item.add(LTRect(gstate.linewidth, (x0, y0, x2, y2)))\n            return\n    pts = []\n    for p in path:\n        for i in xrange(1, len(p), 2):\n            pts.append(apply_matrix_pt(self.ctm, (p[i], p[(i + 1)])))\n    self.cur_item.add(LTCurve(gstate.linewidth, pts))\n    return\n", "label": 1}
{"function": "\n\ndef process_deletes(self, uowcommit, states):\n    if (self.post_update or (not (self.passive_deletes == 'all'))):\n        children_added = uowcommit.memo(('children_added', self), set)\n        for state in states:\n            history = uowcommit.get_attribute_history(state, self.key, self._passive_delete_flag)\n            if history:\n                for child in history.deleted:\n                    if ((child is not None) and (self.hasparent(child) is False)):\n                        self._synchronize(state, child, None, True, uowcommit, False)\n                        if (self.post_update and child):\n                            self._post_update(child, uowcommit, [state])\n                if (self.post_update or (not self.cascade.delete)):\n                    for child in set(history.unchanged).difference(children_added):\n                        if (child is not None):\n                            self._synchronize(state, child, None, True, uowcommit, False)\n                            if (self.post_update and child):\n                                self._post_update(child, uowcommit, [state])\n", "label": 1}
{"function": "\n\ndef _get_parsed_sent(self, grid, pos_in_tree, tagset=None):\n    words = self._get_column(grid, self._colmap['words'])\n    pos_tags = self._get_column(grid, self._colmap['pos'])\n    if (tagset and (tagset != self._tagset)):\n        pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]\n    parse_tags = self._get_column(grid, self._colmap['tree'])\n    treestr = ''\n    for (word, pos_tag, parse_tag) in zip(words, pos_tags, parse_tags):\n        if (word == '('):\n            word = '-LRB-'\n        if (word == ')'):\n            word = '-RRB-'\n        if (pos_tag == '('):\n            pos_tag = '-LRB-'\n        if (pos_tag == ')'):\n            pos_tag = '-RRB-'\n        (left, right) = parse_tag.split('*')\n        right = (right.count(')') * ')')\n        treestr += ('%s (%s %s) %s' % (left, pos_tag, word, right))\n    try:\n        tree = self._tree_class.parse(treestr)\n    except (ValueError, IndexError):\n        tree = self._tree_class.parse(('(%s %s)' % (self._root_label, treestr)))\n    if (not pos_in_tree):\n        for subtree in tree.subtrees():\n            for (i, child) in enumerate(subtree):\n                if (isinstance(child, Tree) and (len(child) == 1) and isinstance(child[0], compat.string_types)):\n                    subtree[i] = (child[0], child.label())\n    return tree\n", "label": 1}
{"function": "\n\ndef IndexOfNextNonSpace(string_data, current_index):\n    successful = False\n    found_index = current_index\n    string_length = len(string_data)\n    annotation_string = ''\n    while (found_index < string_length):\n        current_char = string_data[found_index]\n        if (IsSpecialWhitespace(current_char) == True):\n            found_index += 1\n            continue\n        if (IsRegularWhitespace(current_char) == True):\n            found_index += 1\n            continue\n        if (current_char == '/'):\n            next_index = (found_index + 1)\n            if (next_index >= string_length):\n                successful = True\n                break\n            else:\n                next_character = string_data[next_index]\n                if (next_character == '/'):\n                    found_index += 1\n                    next_index = found_index\n                    first_pass = True\n                    while (next_index < string_length):\n                        test_char = string_data[next_index]\n                        if (IsEndOfLine(test_char) == True):\n                            break\n                        elif (first_pass != True):\n                            annotation_string += test_char\n                        else:\n                            first_pass = False\n                        next_index += 1\n                    found_index = next_index\n                elif (next_character == '*'):\n                    found_index += 1\n                    next_index = found_index\n                    first_pass = True\n                    while (next_index < string_length):\n                        test_char = string_data[next_index]\n                        if ((test_char == '*') and ((next_index + 1) < string_length) and (string_data[(next_index + 1)] == '/')):\n                            next_index += 2\n                            break\n                        elif (first_pass != True):\n                            annotation_string += test_char\n                        else:\n                            first_pass = False\n                        next_index += 1\n                    found_index = next_index\n                else:\n                    successful = True\n                    break\n        else:\n            successful = True\n            break\n    return (successful, found_index, annotation_string)\n", "label": 1}
{"function": "\n\ndef _bulk_insert(mapper, mappings, session_transaction, isstates, return_defaults):\n    base_mapper = mapper.base_mapper\n    cached_connections = _cached_connection_dict(base_mapper)\n    if session_transaction.session.connection_callable:\n        raise NotImplementedError('connection_callable / per-instance sharding not supported in bulk_insert()')\n    if isstates:\n        if return_defaults:\n            states = [(state, state.dict) for state in mappings]\n            mappings = [dict_ for (state, dict_) in states]\n        else:\n            mappings = [state.dict for state in mappings]\n    else:\n        mappings = list(mappings)\n    connection = session_transaction.connection(base_mapper)\n    for (table, super_mapper) in base_mapper._sorted_tables.items():\n        if (not mapper.isa(super_mapper)):\n            continue\n        records = ((None, state_dict, params, mapper, connection, value_params, has_all_pks, has_all_defaults) for (state, state_dict, params, mp, conn, value_params, has_all_pks, has_all_defaults) in _collect_insert_commands(table, ((None, mapping, mapper, connection) for mapping in mappings), bulk=True, return_defaults=return_defaults))\n        _emit_insert_statements(base_mapper, None, cached_connections, super_mapper, table, records, bookkeeping=return_defaults)\n    if (return_defaults and isstates):\n        identity_cls = mapper._identity_class\n        identity_props = [p.key for p in mapper._identity_key_props]\n        for (state, dict_) in states:\n            state.key = (identity_cls, tuple([dict_[key] for key in identity_props]))\n", "label": 1}
{"function": "\n\ndef runner_check(self, auth_list, fun):\n    '\\n        Check special API permissions\\n        '\n    comps = fun.split('.')\n    if (len(comps) != 2):\n        return False\n    mod = comps[0]\n    fun = comps[1]\n    for ind in auth_list:\n        if isinstance(ind, six.string_types):\n            if (ind.startswith('@') and (ind[1:] == mod)):\n                return True\n            if (ind == '@runners'):\n                return True\n            if (ind == '@runner'):\n                return True\n        elif isinstance(ind, dict):\n            if (len(ind) != 1):\n                continue\n            valid = next(six.iterkeys(ind))\n            if (valid.startswith('@') and (valid[1:] == mod)):\n                if isinstance(ind[valid], six.string_types):\n                    if self.match_check(ind[valid], fun):\n                        return True\n                elif isinstance(ind[valid], list):\n                    for regex in ind[valid]:\n                        if self.match_check(regex, fun):\n                            return True\n    return False\n", "label": 1}
{"function": "\n\ndef test_globalring():\n    Qxy = QQ.old_frac_field(x, y)\n    R = QQ.old_poly_ring(x, y)\n    X = R.convert(x)\n    Y = R.convert(y)\n    assert (x in R)\n    assert ((1 / x) not in R)\n    assert ((1 / (1 + x)) not in R)\n    assert (Y in R)\n    assert (X.ring == R)\n    assert ((X * ((Y ** 2) + 1)) == R.convert((x * ((y ** 2) + 1))))\n    assert ((X * y) == (X * Y) == R.convert((x * y)) == (x * Y))\n    assert ((X + y) == (X + Y) == R.convert((x + y)) == (x + Y))\n    assert ((X - y) == (X - Y) == R.convert((x - y)) == (x - Y))\n    assert ((X + 1) == R.convert((x + 1)))\n    raises(ExactQuotientFailed, (lambda : (X / Y)))\n    raises(ExactQuotientFailed, (lambda : (x / Y)))\n    raises(ExactQuotientFailed, (lambda : (X / y)))\n    assert (((X ** 2) / X) == X)\n    assert (R.from_GlobalPolynomialRing(ZZ.old_poly_ring(x, y).convert(x), ZZ.old_poly_ring(x, y)) == X)\n    assert (R.from_FractionField(Qxy.convert(x), Qxy) == X)\n    assert (R.from_FractionField((Qxy.convert(x) / y), Qxy) is None)\n    assert (R._sdm_to_vector(R._vector_to_sdm([X, Y], R.order), 2) == [X, Y])\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    (locale, path) = utils.strip_path(request.path_info)\n    if (localeurl_settings.USE_SESSION and (not locale)):\n        slocale = request.session.get('django_language')\n        if (slocale and utils.supported_language(slocale)):\n            locale = slocale\n    if (localeurl_settings.USE_ACCEPT_LANGUAGE and (not locale)):\n        accept_lang_header = request.META.get('HTTP_ACCEPT_LANGUAGE', '')\n        header_langs = parse_accept_lang_header(accept_lang_header)\n        accept_langs = [l for l in (utils.supported_language(lang[0]) for lang in header_langs) if l]\n        if accept_langs:\n            locale = accept_langs[0]\n    locale_path = utils.locale_path(path, locale)\n    if (locale_path.lower() != request.path_info.lower()):\n        locale_url = utils.add_script_prefix(locale_path)\n        qs = request.META.get('QUERY_STRING', '')\n        if qs:\n            locale_url = ('%s?%s' % (locale_path.encode('utf-8'), qs))\n        redirect_class = HttpResponsePermanentRedirect\n        if (not localeurl_settings.LOCALE_REDIRECT_PERMANENT):\n            redirect_class = HttpResponseRedirect\n        return redirect_class(iri_to_uri(locale_url))\n    request.path_info = path\n    if (not locale):\n        try:\n            locale = request.LANGUAGE_CODE\n        except AttributeError:\n            locale = settings.LANGUAGE_CODE\n    translation.activate(locale)\n    request.LANGUAGE_CODE = translation.get_language()\n", "label": 1}
{"function": "\n\ndef assertMessageCount(self, response=None, **kwargs):\n    'Asserts that the specified number of messages have been attached\\n        for various message types. Usage would look like\\n        ``self.assertMessageCount(success=1)``.\\n        '\n    temp_req = self.client.request(**{\n        'wsgi.input': None,\n    })\n    temp_req.COOKIES = self.client.cookies\n    storage = default_storage(temp_req)\n    messages = []\n    if (response is None):\n        if ('messages' in self.client.cookies):\n            message_cookie = self.client.cookies['messages'].value\n            messages = storage._decode(message_cookie)\n    elif (hasattr(response, 'context') and ('messages' in response.context)):\n        messages = response.context['messages']\n    elif (hasattr(response, '_request') and hasattr(response._request, '_messages')):\n        messages = response._request._messages._queued_messages\n    if ((not any(kwargs.values())) and (not messages)):\n        return\n    if (any(kwargs.values()) and (not messages)):\n        error_msg = 'Messages were expected, but none were set.'\n        assert (0 == sum(kwargs.values())), error_msg\n    for (msg_type, count) in kwargs.items():\n        msgs = [m.message for m in messages if (msg_type in m.tags)]\n        assert (len(msgs) == count), ('%s messages not as expected: %s' % (msg_type.title(), ', '.join(msgs)))\n", "label": 1}
{"function": "\n\ndef test_CRootOf___eval_Eq__():\n    f = Function('f')\n    r = rootof((((x ** 3) + x) + 3), 2)\n    r1 = rootof((((x ** 3) + x) + 3), 1)\n    assert (Eq(r, r1) is S.false)\n    assert (Eq(r, r) is S.true)\n    assert (Eq(r, x) is S.false)\n    assert (Eq(r, 0) is S.false)\n    assert (Eq(r, S.Infinity) is S.false)\n    assert (Eq(r, I) is S.false)\n    assert (Eq(r, f(0)) is S.false)\n    assert (Eq(r, f(0)) is S.false)\n    sol = solve(r.expr)\n    for s in sol:\n        if s.is_real:\n            assert (Eq(r, s) is S.false)\n    r = rootof(r.expr, 0)\n    for s in sol:\n        if s.is_real:\n            assert (Eq(r, s) is S.true)\n    eq = (((x ** 3) + x) + 1)\n    assert ([Eq(rootof(eq, i), j) for i in range(3) for j in solve(eq)] == [False, False, True, False, True, False, True, False, False])\n    assert (Eq(rootof(eq, 0), (1 + S.ImaginaryUnit)) == False)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _diff_packets(cls, model_pkt, rcv_pkt):\n    msg = []\n    for rcv_p in rcv_pkt.protocols:\n        if (not isinstance(rcv_p, six.binary_type)):\n            model_protocols = model_pkt.get_protocols(type(rcv_p))\n            if (len(model_protocols) == 1):\n                model_p = model_protocols[0]\n                diff = []\n                for attr in rcv_p.__dict__:\n                    if attr.startswith('_'):\n                        continue\n                    if callable(attr):\n                        continue\n                    if hasattr(rcv_p.__class__, attr):\n                        continue\n                    rcv_attr = repr(getattr(rcv_p, attr))\n                    model_attr = repr(getattr(model_p, attr))\n                    if (rcv_attr != model_attr):\n                        diff.append(('%s=%s' % (attr, rcv_attr)))\n                if diff:\n                    msg.append(('%s(%s)' % (rcv_p.__class__.__name__, ','.join(diff))))\n            elif ((not model_protocols) or (not (str(rcv_p) in str(model_protocols)))):\n                msg.append(str(rcv_p))\n        else:\n            model_p = ''\n            for p in model_pkt.protocols:\n                if isinstance(rcv_p, six.binary_type):\n                    model_p = p\n                    break\n            if (model_p != rcv_p):\n                msg.append(('str(%s)' % repr(rcv_p)))\n    if msg:\n        return '/'.join(msg)\n    else:\n        return 'Encounter an error during packet comparison. it is malformed.'\n", "label": 1}
{"function": "\n\ndef asXML(self, doctag=None, namedItemsOnly=False, indent='', formatted=True):\n    'Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.'\n    nl = '\\n'\n    out = []\n    namedItems = dict([(v[1], k) for (k, vlist) in self.__tokdict.items() for v in vlist])\n    nextLevelIndent = (indent + '  ')\n    if (not formatted):\n        indent = ''\n        nextLevelIndent = ''\n        nl = ''\n    selfTag = None\n    if (doctag is not None):\n        selfTag = doctag\n    elif self.__name:\n        selfTag = self.__name\n    if (not selfTag):\n        if namedItemsOnly:\n            return ''\n        else:\n            selfTag = 'ITEM'\n    out += [nl, indent, '<', selfTag, '>']\n    worklist = self.__toklist\n    for (i, res) in enumerate(worklist):\n        if isinstance(res, ParseResults):\n            if (i in namedItems):\n                out += [res.asXML(namedItems[i], (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n            else:\n                out += [res.asXML(None, (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n        else:\n            resTag = None\n            if (i in namedItems):\n                resTag = namedItems[i]\n            if (not resTag):\n                if namedItemsOnly:\n                    continue\n                else:\n                    resTag = 'ITEM'\n            xmlBodyText = _xml_escape(_ustr(res))\n            out += [nl, nextLevelIndent, '<', resTag, '>', xmlBodyText, '</', resTag, '>']\n    out += [nl, indent, '</', selfTag, '>']\n    return ''.join(out)\n", "label": 1}
{"function": "\n\ndef insertStateModel(path, pitFile):\n    f = openFile(path, 'markov', 'r')\n    states = []\n    flag = 1\n    for line in f:\n        for i in range(len(states)):\n            if (line.split('->')[0] in states[i][0]):\n                states[i][1].append(line.split('->')[1])\n                flag = 0\n                break\n        if (flag == 1):\n            states.append([line.split('->')[0], [line.split('->')[1]]])\n        flag = 1\n    f.close()\n    allStates = states[:]\n    while (states != []):\n        writtenState = ''\n        for i in states:\n            (writtenState, possibleNewStates) = gsecStates2Peach(i, path, pitFile)\n            for i in writtenStates:\n                if (i != []):\n                    if (possibleNewStates != []):\n                        copy = possibleNewStates[:]\n                        for s in copy:\n                            if (s in i.split('\\n')[0]):\n                                possibleNewStates.remove(s)\n            if (possibleNewStates != []):\n                for s in possibleNewStates:\n                    for i in allStates:\n                        if (s.split()[1] == i[0]):\n                            states.append(i)\n            if (writtenState != 'void'):\n                states.remove(writtenState)\n    writeEndStates(endStates, pitFile)\n    return\n", "label": 1}
{"function": "\n\ndef _screenargs(kargs, mapper, environ, force_explicit=False):\n    '\\n    Private function that takes a dict, and screens it against the current\\n    request dict to determine what the dict should look like that is used.\\n    This is responsible for the requests \"memory\" of the current.\\n    '\n    encoding = mapper.encoding\n    for (key, val) in six.iteritems(kargs):\n        if isinstance(val, six.text_type):\n            kargs[key] = val.encode(encoding)\n    if (mapper.explicit and mapper.sub_domains and (not force_explicit)):\n        return _subdomain_check(kargs, mapper, environ)\n    elif (mapper.explicit and (not force_explicit)):\n        return kargs\n    controller_name = as_unicode(kargs.get('controller'), encoding)\n    if (controller_name and controller_name.startswith('/')):\n        kargs['controller'] = kargs['controller'][1:]\n        return kargs\n    elif (controller_name and ('action' not in kargs)):\n        kargs['action'] = 'index'\n    route_args = environ.get('wsgiorg.routing_args')\n    if route_args:\n        memory_kargs = route_args[1].copy()\n    else:\n        memory_kargs = {\n            \n        }\n    empty_keys = [key for (key, value) in six.iteritems(kargs) if (value is None)]\n    for key in empty_keys:\n        del kargs[key]\n        memory_kargs.pop(key, None)\n    memory_kargs.update(kargs)\n    if mapper.sub_domains:\n        memory_kargs = _subdomain_check(memory_kargs, mapper, environ)\n    return memory_kargs\n", "label": 1}
{"function": "\n\ndef bounded_stats_per_chunk(chunk, block_data_totals, start, stop):\n    'Given a chunk, return the number of blocks types within the specified selection'\n    (chunk_z, chunk_x) = chunk.get_coords()\n    for z in range(16):\n        world_z = (z + (chunk_z * 16))\n        if (((start != None) and (world_z < int(start[2]))) or ((stop != None) and (world_z > int(stop[2])))):\n            break\n        for x in range(16):\n            world_x = (x + (chunk_x * 16))\n            if (((start != None) and (world_x < int(start[0]))) or ((stop != None) and (world_x > int(stop[0])))):\n                break\n            for y in range(128):\n                if (((start != None) and (y < int(start[1]))) or ((stop != None) and (y > int(stop[1])))):\n                    break\n                (block_id, block_data) = chunk.blocks.get_block_and_data(x, y, z)\n                block_data_totals[block_id][block_data] += 1\n", "label": 1}
{"function": "\n\n@signalcommand\ndef handle(self, *args, **options):\n    if args:\n        (appname,) = args\n    style = color_style()\n    if getattr(settings, 'ADMIN_FOR', None):\n        settings_modules = [__import__(m, {\n            \n        }, {\n            \n        }, ['']) for m in settings.ADMIN_FOR]\n    else:\n        settings_modules = [settings]\n    for settings_mod in settings_modules:\n        for app in settings_mod.INSTALLED_APPS:\n            try:\n                templatetag_mod = __import__((app + '.templatetags'), {\n                    \n                }, {\n                    \n                }, [''])\n            except ImportError:\n                continue\n            mod_path = inspect.getabsfile(templatetag_mod)\n            mod_files = os.listdir(os.path.dirname(mod_path))\n            tag_files = [i.rstrip('.py') for i in mod_files if (i.endswith('.py') and (i[0] != '_'))]\n            app_labeled = False\n            for taglib in tag_files:\n                try:\n                    lib = get_library(taglib)\n                except:\n                    continue\n                if (not app_labeled):\n                    self.add_result(('App: %s' % style.MODULE_NAME(app)))\n                    app_labeled = True\n                self.add_result(('load: %s' % style.TAGLIB(taglib)), 1)\n                for (items, label, style_func) in [(lib.tags, 'Tag:', style.TAG), (lib.filters, 'Filter:', style.FILTER)]:\n                    for item in items:\n                        self.add_result(('%s %s' % (label, style_func(item))), 2)\n                        doc = inspect.getdoc(items[item])\n                        if doc:\n                            self.add_result(format_block(doc, 12))\n    return self.results\n", "label": 1}
{"function": "\n\ndef getdesc(root, host_url=''):\n    methods = {\n        \n    }\n    for (path, funcdef) in root.getapi():\n        method = funcdef.extra_options.get('method', None)\n        name = '_'.join(path)\n        if (method is not None):\n            path = path[:(- 1)]\n        else:\n            method = 'GET'\n            for argdef in funcdef.arguments:\n                if (types.iscomplex(argdef.datatype) or types.isarray(argdef.datatype) or types.isdict(argdef.datatype)):\n                    method = 'POST'\n                    break\n        required_params = []\n        optional_params = []\n        for argdef in funcdef.arguments:\n            if ((method == 'GET') and argdef.mandatory):\n                required_params.append(argdef.name)\n            else:\n                optional_params.append(argdef.name)\n        methods[name] = {\n            'method': method,\n            'path': '/'.join(path),\n        }\n        if required_params:\n            methods[name]['required_params'] = required_params\n        if optional_params:\n            methods[name]['optional_params'] = optional_params\n        if funcdef.doc:\n            methods[name]['documentation'] = funcdef.doc\n    formats = []\n    for p in root.protocols:\n        if (p.name == 'restxml'):\n            formats.append('xml')\n        if (p.name == 'restjson'):\n            formats.append('json')\n    api = {\n        'base_url': (host_url + root._webpath),\n        'version': '0.1',\n        'name': getattr(root, 'name', 'name'),\n        'authority': '',\n        'formats': ['json', 'xml'],\n        'methods': methods,\n    }\n    return json.dumps(api, indent=4)\n", "label": 1}
{"function": "\n\ndef run(self, argv):\n    'Equivalent to the main program for the application.\\n\\n        :param argv: input arguments and options\\n        :paramtype argv: list of str\\n        '\n    try:\n        index = 0\n        command_pos = (- 1)\n        help_pos = (- 1)\n        help_command_pos = (- 1)\n        for arg in argv:\n            if ((arg == 'bash-completion') and (help_command_pos == (- 1))):\n                self._bash_completion()\n                return 0\n            if (arg in self.commands[self.api_version]):\n                if (command_pos == (- 1)):\n                    command_pos = index\n            elif (arg in ('-h', '--help')):\n                if (help_pos == (- 1)):\n                    help_pos = index\n            elif (arg == 'help'):\n                if (help_command_pos == (- 1)):\n                    help_command_pos = index\n            index = (index + 1)\n        if ((command_pos > (- 1)) and (help_pos > command_pos)):\n            argv = ['help', argv[command_pos]]\n        if ((help_command_pos > (- 1)) and (command_pos == (- 1))):\n            argv[help_command_pos] = '--help'\n        (self.options, remainder) = self.parser.parse_known_args(argv)\n        self.configure_logging()\n        self.interactive_mode = (not remainder)\n        self.initialize_app(remainder)\n    except Exception as err:\n        if (self.options.verbose_level >= self.DEBUG_LEVEL):\n            self.log.exception(err)\n            raise\n        else:\n            self.log.error(err)\n        return 1\n    if self.interactive_mode:\n        _argv = [sys.argv[0]]\n        sys.argv = _argv\n        return self.interact()\n    return self.run_subcommand(remainder)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.row = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.colFamily = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.colQualifier = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.colVisibility = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.I64):\n                self.timestamp = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, n, sym=None):\n    if n.is_Number:\n        if (n.is_Integer and n.is_nonnegative):\n            if (n is S.Zero):\n                return S.One\n            elif (n is S.One):\n                if (sym is None):\n                    return (- S.Half)\n                else:\n                    return (sym - S.Half)\n            elif (sym is None):\n                if n.is_odd:\n                    return S.Zero\n                n = int(n)\n                if (n > 500):\n                    (p, q) = bernfrac(n)\n                    return Rational(int(p), int(q))\n                case = (n % 6)\n                highest_cached = cls._highest[case]\n                if (n <= highest_cached):\n                    return cls._cache[n]\n                for i in range((highest_cached + 6), (n + 6), 6):\n                    b = cls._calc_bernoulli(i)\n                    cls._cache[i] = b\n                    cls._highest[case] = i\n                return b\n            else:\n                (n, result) = (int(n), [])\n                for k in range((n + 1)):\n                    result.append(((binomial(n, k) * cls(k)) * (sym ** (n - k))))\n                return Add(*result)\n        else:\n            raise ValueError('Bernoulli numbers are defined only for nonnegative integer indices.')\n    if (sym is None):\n        if (n.is_odd and (n - 1).is_positive):\n            return S.Zero\n", "label": 1}
{"function": "\n\ndef compare_docs(self, want, got):\n    if (not self.tag_compare(want.tag, got.tag)):\n        return False\n    if (not self.text_compare(want.text, got.text, True)):\n        return False\n    if (not self.text_compare(want.tail, got.tail, True)):\n        return False\n    if ('any' not in want.attrib):\n        want_keys = sorted(want.attrib.keys())\n        got_keys = sorted(got.attrib.keys())\n        if (want_keys != got_keys):\n            return False\n        for key in want_keys:\n            if (not self.text_compare(want.attrib[key], got.attrib[key], False)):\n                return False\n    if ((want.text != '...') or len(want)):\n        want_children = list(want)\n        got_children = list(got)\n        while (want_children or got_children):\n            if ((not want_children) or (not got_children)):\n                return False\n            want_first = want_children.pop(0)\n            got_first = got_children.pop(0)\n            if (not self.compare_docs(want_first, got_first)):\n                return False\n            if ((not got_children) and (want_first.tail == '...')):\n                break\n    return True\n", "label": 1}
{"function": "\n\ndef get_correct_sig(args, sigs):\n    'Given a list of args and a collection of possible signatures,\\n    this function returns the most appropriate signature.  This\\n    function is only called by deref_array.  This implies that one of\\n    the signatures has an array type.\\n\\n    '\n    if (sigs is None):\n        return None\n    if (len(sigs) == 1):\n        return sigs[0]\n    else:\n        la = len(args)\n        candidate_sigs = [s for s in sigs if (len(s) == la)]\n        count = len(candidate_sigs)\n        if (count == 0):\n            msg = ('Insufficient number of arguments to method.Valid arguments are:\\n%s' % sigs)\n            raise TypeError(msg)\n        elif (count == 1):\n            return candidate_sigs[0]\n        else:\n            array_idx = [i for (i, a) in enumerate(args) if is_array_or_vtkarray(a)]\n            n_arr = len(array_idx)\n            if (n_arr == 0):\n                return None\n            else:\n                for sig in candidate_sigs:\n                    array_in_sig = [is_array_sig(s) for s in sig]\n                    if (array_in_sig.count(True) != len(array_idx)):\n                        continue\n                    bad = False\n                    for i in array_idx:\n                        if (not array_in_sig[i]):\n                            bad = True\n                    if (not bad):\n                        return sig\n                return None\n", "label": 1}
{"function": "\n\ndef _determine_tests(test_modules):\n    '\\n  Determine a list of tests to run from the provided test modules\\n  '\n    for module in test_modules:\n        attrs = dir(module)\n        if hasattr(module, 'test_phase'):\n            test_phase = module.test_phase\n        else:\n            test_phase = constants.DEFAULT_TEST_PHASE\n        if hasattr(module, 'tests_iteration'):\n            tests_iteration = module.tests_iteration\n        else:\n            tests_iteration = constants.DEFAULT_ITERATION\n        functions = set([fun for fun in attrs if hasattr(getattr(module, fun), '__call__')])\n        tests = dict([(fun.lower(), Test(fun, getattr(module, fun), phase=test_phase, iteration=tests_iteration)) for fun in functions if ('test' in fun.lower())])\n        for fun in functions:\n            if ('validate' in fun.lower()):\n                test_name = fun.lower().replace('validate', 'test')\n                if (test_name in tests):\n                    tests[test_name].validation_function = getattr(module, fun)\n        for test in tests.values():\n            if (test.function.__doc__ is not None):\n                test.description = test.function.__doc__\n            if (test.validation_function is not None):\n                if (test.validation_function.__doc__ is not None):\n                    if (test.description is not None):\n                        test.description = '{0};\\n{1}'.format(test.description, test.validation_function.__doc__)\n                    else:\n                        test.description = test.validation_function.__doc__\n            (yield test)\n", "label": 1}
{"function": "\n\ndef _really_load(self, f, filename, ignore_discard, ignore_expires):\n    now = time.time()\n    magic = f.readline()\n    if (not re.search(self.magic_re, magic)):\n        f.close()\n        raise LoadError(('%r does not look like a Netscape format cookies file' % filename))\n    try:\n        while 1:\n            line = f.readline()\n            if (line == ''):\n                break\n            if line.endswith('\\n'):\n                line = line[:(- 1)]\n            if (line.strip().startswith(('#', '$')) or (line.strip() == '')):\n                continue\n            (domain, domain_specified, path, secure, expires, name, value) = line.split('\\t')\n            secure = (secure == 'TRUE')\n            domain_specified = (domain_specified == 'TRUE')\n            if (name == ''):\n                name = value\n                value = None\n            initial_dot = domain.startswith('.')\n            assert (domain_specified == initial_dot)\n            discard = False\n            if (expires == ''):\n                expires = None\n                discard = True\n            c = Cookie(0, name, value, None, False, domain, domain_specified, initial_dot, path, False, secure, expires, discard, None, None, {\n                \n            })\n            if ((not ignore_discard) and c.discard):\n                continue\n            if ((not ignore_expires) and c.is_expired(now)):\n                continue\n            self.set_cookie(c)\n    except IOError:\n        raise\n    except Exception:\n        _warn_unhandled_exception()\n        raise LoadError(('invalid Netscape format cookies file %r: %r' % (filename, line)))\n", "label": 1}
{"function": "\n\ndef render(self, context):\n    gizmos_rendered = context['gizmos_rendered']\n    dependencies = []\n    for rendered_gizmo in gizmos_rendered:\n        try:\n            dependencies_module = __import__('tethys_gizmos.gizmo_dependencies', fromlist=[rendered_gizmo])\n            dependencies_function = getattr(dependencies_module, rendered_gizmo)\n            gizmo_deps = dependencies_function(context)\n            for dependency in gizmo_deps:\n                if (EXTERNAL_INDICATOR in dependency):\n                    static_url = dependency\n                else:\n                    static_url = static(dependency)\n                if (static_url not in dependencies):\n                    dependencies.append(static_url)\n        except AttributeError:\n            pass\n    for dependency in global_dependencies(context):\n        if (EXTERNAL_INDICATOR in dependency):\n            static_url = dependency\n        else:\n            static_url = static(dependency)\n        if (static_url not in dependencies):\n            dependencies.append(static_url)\n    script_tags = []\n    style_tags = []\n    for dependency in dependencies:\n        if ((JS_EXTENSION in dependency) and ((self.output_type == JS_OUTPUT_TYPE) or (self.output_type is None))):\n            script_tags.append('<script src=\"{0}\" type=\"text/javascript\"></script>'.format(dependency))\n        elif ((CSS_EXTENSION in dependency) and ((self.output_type == CSS_OUTPUT_TYPE) or (self.output_type is None))):\n            style_tags.append('<link href=\"{0}\" rel=\"stylesheet\" />'.format(dependency))\n    tags = (style_tags + script_tags)\n    tags_string = '\\n'.join(tags)\n    return tags_string\n", "label": 1}
{"function": "\n\ndef _dummy_change(connect_spec, dn, before, after):\n    assert (before != after)\n    assert len(before)\n    assert len(after)\n    assert (dn in db)\n    e = db[dn]\n    assert (e == before)\n    all_attrs = set()\n    all_attrs.update(before)\n    all_attrs.update(after)\n    directives = []\n    for attr in all_attrs:\n        if (attr not in before):\n            assert (attr in after)\n            assert len(after[attr])\n            directives.append(('add', attr, after[attr]))\n        elif (attr not in after):\n            assert (attr in before)\n            assert len(before[attr])\n            directives.append(('delete', attr, ()))\n        else:\n            assert len(before[attr])\n            assert len(after[attr])\n            to_del = (before[attr] - after[attr])\n            if len(to_del):\n                directives.append(('delete', attr, to_del))\n            to_add = (after[attr] - before[attr])\n            if len(to_add):\n                directives.append(('add', attr, to_add))\n    return _dummy_modify(connect_spec, dn, directives)\n", "label": 1}
{"function": "\n\ndef write(self, data):\n    'Output the given byte string over the serial port.'\n    if (not self.is_open):\n        raise portNotOpenError\n    d = to_bytes(data)\n    tx_len = len(d)\n    timeout = self._write_timeout\n    if (timeout and (timeout > 0)):\n        timeout += time.time()\n    while (tx_len > 0):\n        try:\n            n = os.write(self.fd, d)\n            if (timeout == 0):\n                return n\n            elif (timeout and (timeout > 0)):\n                timeleft = (timeout - time.time())\n                if (timeleft < 0):\n                    raise writeTimeoutError\n                (_, ready, _) = select.select([], [self.fd], [], timeleft)\n                if (not ready):\n                    raise writeTimeoutError\n            else:\n                assert (timeout is None)\n                (_, ready, _) = select.select([], [self.fd], [], None)\n                if (not ready):\n                    raise SerialException('write failed (select)')\n            d = d[n:]\n            tx_len -= n\n        except SerialException:\n            raise\n        except OSError as v:\n            if (v.errno != errno.EAGAIN):\n                raise SerialException('write failed: {}'.format(v))\n            if (timeout and ((timeout - time.time()) < 0)):\n                raise writeTimeoutError\n    return len(data)\n", "label": 1}
{"function": "\n\ndef find_prev_lone_bracket(view, start, tags, unbalanced=0):\n    if ((view.substr(start) == tags[0][1]) if (len(tags[0]) > 1) else tags[0]):\n        if ((not unbalanced) and (view.substr((start - 1)) != '\\\\')):\n            return sublime.Region(start, (start + 1))\n    new_start = start\n    for i in range((unbalanced or 1)):\n        prev_opening_bracket = reverse_search_by_pt(view, tags[0], start=0, end=new_start, flags=sublime.IGNORECASE)\n        if (prev_opening_bracket is None):\n            if ((i == 0) and (view.substr(start) == tags[0][(- 1)]) and (view.substr((start - 1)) != '\\\\')):\n                return sublime.Region(start, (start + 1))\n            return\n        while (view.substr((prev_opening_bracket.begin() - 1)) == '\\\\'):\n            prev_opening_bracket = reverse_search_by_pt(view, tags[0], start=0, end=prev_opening_bracket.begin(), flags=sublime.IGNORECASE)\n            if (prev_opening_bracket is None):\n                return\n        new_start = prev_opening_bracket.begin()\n    nested = 0\n    while True:\n        next_closing_bracket = reverse_search_by_pt(view, tags[1], start=prev_opening_bracket.a, end=start, flags=sublime.IGNORECASE)\n        if (not next_closing_bracket):\n            break\n        nested += 1\n        start = next_closing_bracket.begin()\n    if (nested > 0):\n        return find_prev_lone_bracket(view, prev_opening_bracket.begin(), tags, nested)\n    else:\n        return prev_opening_bracket\n", "label": 1}
{"function": "\n\ndef take_action(self, parsed_args):\n    identity_client = self.app.client_manager.identity\n    if parsed_args.password_prompt:\n        parsed_args.password = utils.get_password(self.app.stdin)\n    if ((not parsed_args.name) and (not parsed_args.name) and (not parsed_args.password) and (not parsed_args.email) and (not parsed_args.project) and (not parsed_args.description) and (not parsed_args.enable) and (not parsed_args.disable)):\n        sys.stderr.write('Incorrect set of arguments provided. See openstack --help for more details\\n')\n        return\n    user = utils.find_resource(identity_client.users, parsed_args.user)\n    kwargs = {\n        \n    }\n    if parsed_args.name:\n        kwargs['name'] = parsed_args.name\n    if parsed_args.email:\n        kwargs['email'] = parsed_args.email\n    if parsed_args.password:\n        kwargs['password'] = parsed_args.password\n    if parsed_args.description:\n        kwargs['description'] = parsed_args.description\n    if parsed_args.project:\n        project_id = common.find_project(identity_client, parsed_args.project, parsed_args.project_domain).id\n        kwargs['default_project'] = project_id\n    kwargs['enabled'] = user.enabled\n    if parsed_args.enable:\n        kwargs['enabled'] = True\n    if parsed_args.disable:\n        kwargs['enabled'] = False\n    identity_client.users.update(user.id, **kwargs)\n", "label": 1}
{"function": "\n\ndef check(data, start_time, end_time, filter_field=None, filter_data=[]):\n    d = []\n    if (data and (len(data) > 0)):\n        for i in data:\n            if (('end_time' in i) and i['end_time'] and (format_timestamp(i['end_time']) < start_time)):\n                continue\n            elif (('start_time' in i) and i['start_time'] and (format_timestamp(i['start_time']) > end_time)):\n                continue\n            elif ((i['deleted'] is True) and (format_timestamp(i['updated_at']) < start_time)):\n                continue\n            elif ((i['paused'] is True) and (format_timestamp(i['updated_at']) < start_time)):\n                continue\n            elif (filter_field and (i[filter_field] not in filter_data)):\n                continue\n            else:\n                d.append(i['id'])\n    return d\n", "label": 1}
{"function": "\n\ndef ProcessFlags(env, flags):\n    for f in flags:\n        if (not f):\n            continue\n        parsed_flags = env.ParseFlags(str(f))\n        for flag in parsed_flags.pop('CPPDEFINES'):\n            if (not isinstance(flag, list)):\n                env.Append(CPPDEFINES=flag)\n                continue\n            if ('\"' in flag[1]):\n                flag[1] = flag[1].replace('\"', '\\\\\"')\n            env.Append(CPPDEFINES=[flag])\n        env.Append(**parsed_flags)\n    for (i, p) in enumerate(env.get('CPPPATH', [])):\n        if isdir(p):\n            env['CPPPATH'][i] = realpath(p)\n    for (i, f) in enumerate(env.get('CCFLAGS', [])):\n        if (isinstance(f, tuple) and (f[0] == '-include')):\n            env['CCFLAGS'][i] = (f[0], env.File(realpath(f[1].get_path())))\n    undefines = [u for u in env.get('CCFLAGS', []) if (isinstance(u, basestring) and u.startswith('-U'))]\n    if undefines:\n        for undef in undefines:\n            env['CCFLAGS'].remove(undef)\n        env.Append(_CPPDEFFLAGS=(' %s' % ' '.join(undefines)))\n", "label": 1}
{"function": "\n\ndef _compile_base_class(self, klass):\n    if (klass is object):\n        return\n    try:\n        alias = pyamf.get_class_alias(klass)\n    except UnknownClassAlias:\n        alias = pyamf.register_class(klass)\n    alias.compile()\n    self.bases.append((klass, alias))\n    if alias.exclude_attrs:\n        self.exclude_attrs.update(alias.exclude_attrs)\n    if alias.readonly_attrs:\n        self.readonly_attrs.update(alias.readonly_attrs)\n    if alias.static_attrs:\n        self.static_attrs_set.update(alias.static_attrs)\n        for a in alias.static_attrs:\n            if (a not in self.static_attrs):\n                self.static_attrs.insert(0, a)\n    if alias.proxy_attrs:\n        self.proxy_attrs.update(alias.proxy_attrs)\n    if alias.encodable_properties:\n        self.encodable_properties.update(alias.encodable_properties)\n    if alias.decodable_properties:\n        self.decodable_properties.update(alias.decodable_properties)\n    if ((self.amf3 is None) and alias.amf3):\n        self.amf3 = alias.amf3\n    if ((self.dynamic is None) and (alias.dynamic is not None)):\n        self.inherited_dynamic = alias.dynamic\n    if (alias.sealed is not None):\n        self.inherited_sealed = alias.sealed\n    if alias.synonym_attrs:\n        (self.synonym_attrs, x) = (alias.synonym_attrs.copy(), self.synonym_attrs)\n        self.synonym_attrs.update(x)\n", "label": 1}
{"function": "\n\ndef loop(params):\n    display.welcome()\n    try:\n        while (not params.user):\n            params.user = getpass.getpass(prompt='User(Email): ', stream=None)\n        while (not params.password):\n            params.password = getpass.getpass(prompt='Password: ', stream=None)\n        if params.commands:\n            runcommands(params)\n            goodbye()\n        if params.debug:\n            print(('Params: ' + str(params)))\n        if (not params.command):\n            params.command = 'd'\n        while True:\n            if (not params.command):\n                try:\n                    params.command = input('Keeper > ')\n                except KeyboardInterrupt:\n                    print('')\n                except EOFError:\n                    raise KeyboardInterrupt\n            try:\n                if (not do_command(params)):\n                    raise KeyboardInterrupt\n            except CommunicationError as e:\n                print(('Communication Error:' + str(e.message)))\n            except AuthenticationError as e:\n                print(('AuthenticationError Error: ' + str(e.message)))\n            except KeyboardInterrupt as e:\n                raise\n            except:\n                print(('An unexpected error occurred: ' + str(sys.exc_info()[0])))\n                raise\n            params.command = ''\n    except KeyboardInterrupt:\n        goodbye()\n", "label": 1}
{"function": "\n\ndef _message_generator(self):\n    assert (self.assignment() or (self.subscription() is not None)), 'No topic subscription or manual partition assignment'\n    while (time.time() < self._consumer_timeout):\n        if self._use_consumer_group():\n            self._coordinator.ensure_coordinator_known()\n            self._coordinator.ensure_active_group()\n        elif ((self.config['group_id'] is not None) and (self.config['api_version'] >= (0, 8, 2))):\n            self._coordinator.ensure_coordinator_known()\n        if (not self._subscription.has_all_fetch_positions()):\n            partitions = self._subscription.missing_fetch_positions()\n            self._update_fetch_positions(partitions)\n        poll_ms = (1000 * (self._consumer_timeout - time.time()))\n        if (not self._fetcher.in_flight_fetches()):\n            poll_ms = 0\n        self._client.poll(timeout_ms=poll_ms, sleep=True)\n        timeout_at = self._next_timeout()\n        if (self._use_consumer_group() and (not self.assignment())):\n            sleep_time = max((timeout_at - time.time()), 0)\n            if ((sleep_time > 0) and (not self._client.in_flight_request_count())):\n                log.debug('No partitions assigned; sleeping for %s', sleep_time)\n                time.sleep(sleep_time)\n                continue\n        if (time.time() > timeout_at):\n            continue\n        for msg in self._fetcher:\n            (yield msg)\n            if (time.time() > timeout_at):\n                log.debug('internal iterator timeout - breaking for poll')\n                break\n        else:\n            self._fetcher.init_fetches()\n", "label": 1}
{"function": "\n\ndef _get_click_probs(self, s, possibleIntents):\n    '\\n            Returns clickProbs list\\n            clickProbs[i][k] = P(C_1, ..., C_k | I=i)\\n            '\n    clickProbs = dict(((i, []) for i in possibleIntents))\n    firstVerticalPos = ((- 1) if (not any(s.layout[:(- 1)])) else [k for (k, l) in enumerate(s.layout) if l][0])\n    prevClick = (- 1)\n    layout = (([False] * len(s.layout)) if self.ignoreLayout else s.layout)\n    for (rank, c) in enumerate(s.clicks):\n        url = s.results[rank]\n        prob = {\n            False: 0.0,\n            True: 0.0,\n        }\n        for i in possibleIntents:\n            a = self.alpha[i][s.query][url]\n            g = self.getGamma(self.gamma, rank, prevClick, layout, i)\n            if (self.explorationBias and any(((s.layout[k] and s.clicks[k]) for k in xrange(rank))) and (not s.layout[rank])):\n                g *= (1 - self.e[firstVerticalPos])\n            prevProb = (1 if (rank == 0) else clickProbs[i][(- 1)])\n            if (c == 0):\n                clickProbs[i].append((prevProb * (1 - (a * g))))\n            else:\n                clickProbs[i].append(((prevProb * a) * g))\n        if (c != 0):\n            prevClick = rank\n    return clickProbs\n", "label": 1}
{"function": "\n\ndef test_levicivita():\n    assert (Eijk(1, 2, 3) == LeviCivita(1, 2, 3))\n    assert (LeviCivita(1, 2, 3) == 1)\n    assert (LeviCivita(1, 3, 2) == (- 1))\n    assert (LeviCivita(1, 2, 2) == 0)\n    (i, j, k) = symbols('i j k')\n    assert (LeviCivita(i, j, k) == LeviCivita(i, j, k, evaluate=False))\n    assert (LeviCivita(i, j, i) == 0)\n    assert (LeviCivita(1, i, i) == 0)\n    assert (LeviCivita(i, j, k).doit() == ((((j - i) * (k - i)) * (k - j)) / 2))\n    assert (LeviCivita(1, 2, 3, 1) == 0)\n    assert (LeviCivita(4, 5, 1, 2, 3) == 1)\n    assert (LeviCivita(4, 5, 2, 1, 3) == (- 1))\n    assert (LeviCivita(i, j, k).is_integer is True)\n    assert (adjoint(LeviCivita(i, j, k)) == LeviCivita(i, j, k))\n    assert (conjugate(LeviCivita(i, j, k)) == LeviCivita(i, j, k))\n    assert (transpose(LeviCivita(i, j, k)) == LeviCivita(i, j, k))\n", "label": 1}
{"function": "\n\ndef __init__(self, classifier_dic):\n    self.classifier = classifier_dic\n    classifications = self.classifier['classifications']\n    self.possible = True\n    self.unreviewed = True\n    self.reviewed = True\n    self.data_files = {c: {\n        \n    } for c in classifications}\n    for classification in classifications:\n        seed = [f for f in self.classifier['seed'] if (f.split(os.sep)[(- 1)].find(classification) == 0)]\n        reviewed = [f for f in self.classifier['reviewed'] if (f.split(os.sep)[(- 1)].find(classification) == 0)]\n        unreviewed = [f for f in self.classifier['unreviewed'] if (f.split(os.sep)[(- 1)].find(classification) == 0)]\n        self.data_files[classification]['seed'] = seed\n        self.data_files[classification]['reviewed'] = reviewed\n        self.data_files[classification]['unreviewed'] = unreviewed\n        if ((len(reviewed) == 0) and (len(seed) == 0)):\n            self.reviewed = False\n        if (len(unreviewed) == 0):\n            self.unreviewed = False\n        if ((len(reviewed) == 0) and (len(seed) == 0) and (len(unreviewed) == 0)):\n            del self.data_files[classification]\n        if ((self.unreviewed == False) and (self.reviewed == False)):\n            self.possible = False\n    self.data = []\n    self.labels = []\n", "label": 1}
{"function": "\n\ndef load_raw_data(self, path, alt=PRIMARY_ALT, cls=None, fallback=True):\n    'Internal helper that loads the raw record data.  This performs\\n        very little data processing on the data.\\n        '\n    path = cleanup_path(path)\n    if (cls is None):\n        cls = dict\n    fn_base = self.to_fs_path(path)\n    rv = cls()\n    rv_type = None\n    choiceiter = _iter_filename_choices(fn_base, [alt], self.config, fallback=fallback)\n    for (fs_path, source_alt, is_attachment) in choiceiter:\n        if ((rv_type is not None) and (rv_type != is_attachment)):\n            break\n        try:\n            with open(fs_path, 'rb') as f:\n                if (rv_type is None):\n                    rv_type = is_attachment\n                for (key, lines) in metaformat.tokenize(f, encoding='utf-8'):\n                    if (key not in rv):\n                        rv[key] = ''.join(lines)\n        except IOError as e:\n            if (e.errno not in (errno.ENOTDIR, errno.ENOENT)):\n                raise\n            if ((not is_attachment) or (not os.path.isfile(fs_path[:(- 3)]))):\n                continue\n            elif is_attachment:\n                rv_type = True\n        if ('_source_alt' not in rv):\n            rv['_source_alt'] = source_alt\n    if (rv_type is None):\n        return\n    rv['_path'] = path\n    rv['_id'] = posixpath.basename(path)\n    rv['_gid'] = hashlib.md5(path.encode('utf-8')).hexdigest()\n    rv['_alt'] = alt\n    if rv_type:\n        rv['_attachment_for'] = posixpath.dirname(path)\n    return rv\n", "label": 1}
{"function": "\n\ndef test_all_basetypes():\n    '\\n    Test that base types are matched properly.\\n    '\n    grammar = '\\n        Rule:\\n            a=FLOAT\\n            b=INT\\n            c1=BOOL\\n            c2=BOOL\\n            d1=STRING\\n            d2=STRING\\n            e=ID\\n        ;\\n    '\n    meta = metamodel_from_str(grammar)\n    assert meta\n    assert (meta['Rule']._tx_type is RULE_COMMON)\n    assert (meta['BASETYPE']._tx_type is RULE_MATCH)\n    assert (meta['NUMBER']._tx_type is RULE_MATCH)\n    assert (meta['INT']._tx_type is RULE_MATCH)\n    assert (meta['FLOAT']._tx_type is RULE_MATCH)\n    assert (meta['STRING']._tx_type is RULE_MATCH)\n    assert (meta['BOOL']._tx_type is RULE_MATCH)\n    model = meta.model_from_str('3.4 5 true 0 \"some string\" \\'some other string\\' some_id')\n    assert (model.a == 3.4)\n    assert (model.b == 5)\n    assert (model.c1 is True)\n    assert (model.c2 is False)\n    assert (model.d1 == 'some string')\n    assert (model.d2 == 'some other string')\n    assert (model.e == 'some_id')\n", "label": 1}
{"function": "\n\ndef test_curry_kwargs():\n\n    def f(a, b, c=10):\n        return ((a + b) * c)\n    f = curry(f)\n    assert (f(1, 2, 3) == 9)\n    assert (f(1)(2, 3) == 9)\n    assert (f(1, 2) == 30)\n    assert (f(1, c=3)(2) == 9)\n    assert (f(c=3)(1, 2) == 9)\n\n    def g(a=1, b=10, c=0):\n        return ((a + b) + c)\n    cg = curry(g, b=2)\n    assert (cg() == 3)\n    assert (cg(b=3) == 4)\n    assert (cg(a=0) == 2)\n    assert (cg(a=0, b=1) == 1)\n    assert (cg(0) == 2)\n    assert raises(TypeError, (lambda : cg(1, 2)))\n\n    def h(x, func=int):\n        return func(x)\n    if ((platform.python_implementation() != 'PyPy') or (platform.python_version_tuple()[0] != '3')):\n        assert (curry(h)(0.0) == 0)\n        assert (curry(h)(func=str)(0.0) == '0.0')\n        assert (curry(h, func=str)(0.0) == '0.0')\n", "label": 1}
{"function": "\n\ndef vfs_normpath(path):\n    'Normalize path from posixpath.py, eliminating double slashes, etc.'\n    (slash, dot) = (('/', '.') if isinstance(path, unicode) else ('/', '.'))\n    if (path == ''):\n        return dot\n    initial_slashes = path.startswith('/')\n    if (initial_slashes and path.startswith('//') and (not path.startswith('///'))):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if (comp in ('', '.')):\n            continue\n        if ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(- 1)] == '..'))):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = ((slash * initial_slashes) + path)\n    return (path or dot)\n", "label": 1}
{"function": "\n\ndef _parse_document(data, base_url=None):\n    links = _get_dict(data, '_links')\n    embedded = _get_dict(data, '_embedded')\n    self = _get_dict(links, 'self')\n    url = _get_string(self, 'href')\n    url = urlparse.urljoin(base_url, url)\n    title = _get_string(self, 'title')\n    content = {\n        \n    }\n    for (key, value) in links.items():\n        if (key in ('self', 'curies')):\n            continue\n        key = _map_to_coreapi_key(key)\n        if isinstance(value, list):\n            if (value and ('name' in value[0])):\n                content[key] = {item['name']: _parse_link(item, base_url) for item in value if ('name' in item)}\n            else:\n                content[key] = [_parse_link(item, base_url) for item in value]\n        elif isinstance(value, dict):\n            content[key] = _parse_link(value, base_url)\n    for (key, value) in embedded.items():\n        key = _map_to_coreapi_key(key)\n        if isinstance(value, list):\n            content[key] = [_parse_document(item, base_url=url) for item in value]\n        elif isinstance(value, dict):\n            content[key] = _parse_document(value, base_url=url)\n    for (key, value) in data.items():\n        if (key not in ('_embedded', '_links')):\n            content[key] = value\n    return Document(url, title, content)\n", "label": 1}
{"function": "\n\ndef evaluate(node, sphinxContext, value=False, fallback=None, hsBoundFact=False):\n    if isinstance(node, astNode):\n        if (fallback is None):\n            result = evaluator[node.__class__.__name__](node, sphinxContext)\n        else:\n            try:\n                result = evaluator[node.__class__.__name__](node, sphinxContext)\n            except StopIteration:\n                if sphinxContext.formulaOptions.traceVariableSetExpressionEvaluation:\n                    sphinxContext.modelXbrl.info('sphinx:trace', _('%(node)s has unbound evaluation'), sourceFileLine=node.sourceFileLine, node=str(node))\n                return fallback\n        if sphinxContext.formulaOptions.traceVariableSetExpressionEvaluation:\n            sphinxContext.modelXbrl.info('sphinx:trace', _('%(node)s evaluation: %(value)s'), sourceFileLine=node.sourceFileLine, node=str(node), value=result)\n        if (result is not None):\n            if isinstance(result, HyperspaceBinding):\n                if hsBoundFact:\n                    return result.yieldedFact\n                elif value:\n                    return result.value\n            if ((value or hsBoundFact) and isinstance(result, astNode)):\n                return evaluate(result, sphinxContext, value, fallback, hsBoundFact)\n            return result\n        return result\n    elif isinstance(node, (tuple, list)):\n        return [evaluate(item, sphinxContext, value, fallback, hsBoundFact) for item in node]\n    elif isinstance(node, set):\n        return set((evaluate(item, sphinxContext, value, fallback, hsBoundFact) for item in node))\n    else:\n        return node\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_source_index_ != x.has_source_index_):\n        return 0\n    if (self.has_source_index_ and (self.source_index_ != x.source_index_)):\n        return 0\n    if (self.has_x_offset_ != x.has_x_offset_):\n        return 0\n    if (self.has_x_offset_ and (self.x_offset_ != x.x_offset_)):\n        return 0\n    if (self.has_y_offset_ != x.has_y_offset_):\n        return 0\n    if (self.has_y_offset_ and (self.y_offset_ != x.y_offset_)):\n        return 0\n    if (self.has_opacity_ != x.has_opacity_):\n        return 0\n    if (self.has_opacity_ and (self.opacity_ != x.opacity_)):\n        return 0\n    if (self.has_anchor_ != x.has_anchor_):\n        return 0\n    if (self.has_anchor_ and (self.anchor_ != x.anchor_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef max_extents(extents, zrange=False):\n    '\\n    Computes the maximal extent in 2D and 3D space from\\n    list of 4-tuples or 6-tuples. If zrange is enabled\\n    all extents are converted to 6-tuples to comput\\n    x-, y- and z-limits.\\n    '\n    if zrange:\n        num = 6\n        inds = [(0, 3), (1, 4), (2, 5)]\n        extents = [(e if (len(e) == 6) else (e[0], e[1], None, e[2], e[3], None)) for e in extents]\n    else:\n        num = 4\n        inds = [(0, 2), (1, 3)]\n    arr = (list(zip(*extents)) if extents else [])\n    extents = ([np.NaN] * num)\n    if (len(arr) == 0):\n        return extents\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        for (lidx, uidx) in inds:\n            lower = [v for v in arr[lidx] if (v is not None)]\n            upper = [v for v in arr[uidx] if (v is not None)]\n            if (lower and isinstance(lower[0], np.datetime64)):\n                extents[lidx] = np.min(lower)\n            elif lower:\n                extents[lidx] = np.nanmin(lower)\n            if (upper and isinstance(upper[0], np.datetime64)):\n                extents[uidx] = np.max(upper)\n            elif upper:\n                extents[uidx] = np.nanmax(upper)\n    return tuple(extents)\n", "label": 1}
{"function": "\n\ndef get_all_pending(self, state, dict_, passive=PASSIVE_NO_INITIALIZE):\n    if (self.key not in dict_):\n        return []\n    current = dict_[self.key]\n    current = getattr(current, '_sa_adapter')\n    if (self.key in state.committed_state):\n        original = state.committed_state[self.key]\n        if (original not in (NO_VALUE, NEVER_SET)):\n            current_states = [((((c is not None) and instance_state(c)) or None), c) for c in current]\n            original_states = [((((c is not None) and instance_state(c)) or None), c) for c in original]\n            current_set = dict(current_states)\n            original_set = dict(original_states)\n            return (([(s, o) for (s, o) in current_states if (s not in original_set)] + [(s, o) for (s, o) in current_states if (s in original_set)]) + [(s, o) for (s, o) in original_states if (s not in current_set)])\n    return [(instance_state(o), o) for o in current]\n", "label": 1}
{"function": "\n\ndef label_tag(self, contents=None, attrs=None, label_suffix=None):\n    \"\\n        Wraps the given contents in a <label>, if the field has an ID attribute.\\n        contents should be 'mark_safe'd to avoid HTML escaping. If contents\\n        aren't given, uses the field's HTML-escaped label.\\n\\n        If attrs are given, they're used as HTML attributes on the <label> tag.\\n\\n        label_suffix allows overriding the form's label_suffix.\\n        \"\n    contents = (contents or self.label)\n    if (label_suffix is None):\n        label_suffix = (self.field.label_suffix if (self.field.label_suffix is not None) else self.form.label_suffix)\n    if (label_suffix and contents and (contents[(- 1)] not in _(':?.!'))):\n        contents = format_html('{}{}', contents, label_suffix)\n    widget = self.field.widget\n    id_ = (widget.attrs.get('id') or self.auto_id)\n    if id_:\n        id_for_label = widget.id_for_label(id_)\n        if id_for_label:\n            attrs = dict((attrs or {\n                \n            }), **{\n                'for': id_for_label,\n            })\n        if (self.field.required and hasattr(self.form, 'required_css_class')):\n            attrs = (attrs or {\n                \n            })\n            if ('class' in attrs):\n                attrs['class'] += (' ' + self.form.required_css_class)\n            else:\n                attrs['class'] = self.form.required_css_class\n        attrs = (flatatt(attrs) if attrs else '')\n        contents = format_html('<label{}>{}</label>', attrs, contents)\n    else:\n        contents = conditional_escape(contents)\n    return mark_safe(contents)\n", "label": 1}
{"function": "\n\ndef _get_row_by_id(self, table_name, vsctl_row_id, record_id):\n    if (not vsctl_row_id.table):\n        return None\n    if (not vsctl_row_id.name_column):\n        if (record_id != '.'):\n            return None\n        values = list(self.idl.tables[vsctl_row_id.table].rows.values())\n        if ((not values) or (len(values) > 2)):\n            return None\n        referrer = values[0]\n    else:\n        referrer = None\n        for ovsrec_row in self.idl.tables[vsctl_row_id.table].rows.values():\n            name = getattr(ovsrec_row, vsctl_row_id.name_column)\n            assert (type(name) in (list, str, six.text_type))\n            if ((type(name) != list) and (name == record_id)):\n                if referrer:\n                    vsctl_fatal(('multiple rows in %s match \"%s\"' % (table_name, record_id)))\n                referrer = ovsrec_row\n    if (not referrer):\n        return None\n    final = None\n    if vsctl_row_id.uuid_column:\n        referrer.verify(vsctl_row_id.uuid_column)\n        uuid = getattr(referrer, vsctl_row_id.uuid_column)\n        uuid_ = referrer._data[vsctl_row_id.uuid_column]\n        assert (uuid_.type.key.type == ovs.db.types.UuidType)\n        assert (uuid_.type.value is None)\n        assert (type(uuid) == list)\n        if (len(uuid) == 1):\n            final = uuid[0]\n    else:\n        final = referrer\n    return final\n", "label": 1}
{"function": "\n\ndef diff(right, external=False):\n    '\\n    Initiate diff by getting left side and right side compare.\\n\\n    Call the appropriate diff method and call internal or external diff.\\n    '\n    lw = None\n    rw = None\n    lv = None\n    rv = None\n    for w in sublime.windows():\n        if (w.id() == LEFT['win_id']):\n            lw = w\n        if (w.id() == right['win_id']):\n            rw = w\n        if ((lw is not None) and (rw is not None)):\n            break\n    if (lw is not None):\n        for v in lw.views():\n            if (v.id() == LEFT['view_id']):\n                lv = v\n                break\n    elif LEFT['clip']:\n        lv = LEFT['clip']\n    if (rw is not None):\n        for v in rw.views():\n            if (v.id() == right['view_id']):\n                rv = v\n                break\n    elif right['clip']:\n        rv = right['clip']\n    if ((lv is not None) and (rv is not None)):\n        ext_diff = get_external_diff()\n        if external:\n            EasyDiff.extcompare(EasyDiffInput(lv, rv, external=True), ext_diff)\n        else:\n            EasyDiff.compare(EasyDiffInput(lv, rv))\n    else:\n        log(\"Can't compare\")\n", "label": 1}
{"function": "\n\ndef test_phone_numbers(testbed):\n    scott = models.Contact(name='scott')\n    scott.phone_numbers.append(models.PhoneNumber(phone_type='home', number='(650) 555 - 2200'))\n    scott.phone_numbers.append(models.PhoneNumber(phone_type='mobile', number='(650) 555 - 2201'))\n    scott.put()\n    assert (len(scott.phone_numbers) == 2)\n    assert (scott.phone_numbers[0].phone_type == 'home')\n    assert (scott.phone_numbers[0].number == '(650) 555 - 2200')\n    assert (scott.phone_numbers[1].phone_type == 'mobile')\n    assert (scott.phone_numbers[1].number == '(650) 555 - 2201')\n    home_numbers = [phone_number for phone_number in scott.phone_numbers if (phone_number.phone_type == 'home')]\n    assert (len(home_numbers) == 1)\n    assert (home_numbers[0].number == '(650) 555 - 2200')\n    mobile_numbers = [phone_number for phone_number in scott.phone_numbers if (phone_number.phone_type == 'mobile')]\n    assert (len(mobile_numbers) == 1)\n    lost_phone = mobile_numbers[0]\n    scott.phone_numbers.remove(lost_phone)\n    scott.put()\n    scott = scott.key.get()\n    assert (len(scott.phone_numbers) == 1)\n    assert (scott.phone_numbers[0].phone_type == 'home')\n    assert (scott.phone_numbers[0].number == '(650) 555 - 2200')\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    from werkzeug.exceptions import Forbidden\n    cleaned_path = environ.get('PATH_INFO', '').strip('/')\n    for sep in (os.sep, os.altsep):\n        if (sep and (sep != '/')):\n            cleaned_path = cleaned_path.replace(sep, '/')\n    path = '/'.join(([''] + [x for x in cleaned_path.split('/') if (x and (x != '..'))]))\n    file_loader = None\n    flag = False\n    for (search_path, loader) in self.exports.iteritems():\n        if (search_path == path):\n            flag = True\n            (real_filename, file_loader) = loader(None)\n            if (file_loader is not None):\n                break\n        if (not search_path.endswith('/')):\n            search_path += '/'\n        if path.startswith(search_path):\n            flag = True\n            (real_filename, file_loader) = loader(path[len(search_path):])\n            if (file_loader is not None):\n                break\n    if (file_loader is None):\n        if flag:\n            return real_filename(environ, start_response)\n        else:\n            return self.app(environ, start_response)\n    if (not self.is_allowed(real_filename)):\n        return Forbidden(('You can not visit the file %s.' % real_filename))(environ, start_response)\n    res = filedown(environ, real_filename, self.cache, self.cache_timeout)\n    return res(environ, start_response)\n", "label": 1}
{"function": "\n\ndef evaluateRangeVars(self, op, p, args, contextItem, result):\n    if isinstance(p, RangeDecl):\n        r = self.evaluate(p.bindingSeq, contextItem=contextItem)\n        if (len(r) == 1):\n            r = r[0]\n            if isinstance(r, (tuple, list, set)):\n                if ((len(r) == 1) and isinstance(r[0], _RANGE)):\n                    r = r[0]\n                rvQname = p.rangeVar.name\n                hasPrevValue = (rvQname in self.inScopeVars)\n                if hasPrevValue:\n                    prevValue = self.inScopeVars[rvQname]\n                for rv in r:\n                    self.inScopeVars[rvQname] = rv\n                    self.evaluateRangeVars(op, args[0], args[1:], contextItem, result)\n                    if ((op != 'for') and (len(result) > 0)):\n                        break\n                if ((op == 'every') and (len(result) == 0)):\n                    result.append(True)\n                if hasPrevValue:\n                    self.inScopeVars[rvQname] = prevValue\n    elif isinstance(p, Expr):\n        if (p.name == 'return'):\n            result.append(self.evaluate(p.expr, contextItem=contextItem))\n        elif (p.name == 'satisfies'):\n            boolresult = self.effectiveBooleanValue(p, self.evaluate(p.expr, contextItem=contextItem))\n            if ((op == 'every') != boolresult):\n                result.append(boolresult)\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.re:\n        result = self.re.match(instring, loc)\n        if (not result):\n            exc = self.myException\n            exc.loc = loc\n            exc.pstr = instring\n            raise exc\n        loc = result.end()\n        return (loc, result.group())\n    if (not (instring[loc] in self.initChars)):\n        exc = self.myException\n        exc.loc = loc\n        exc.pstr = instring\n        raise exc\n    start = loc\n    loc += 1\n    instrlen = len(instring)\n    bodychars = self.bodyChars\n    maxloc = (start + self.maxLen)\n    maxloc = min(maxloc, instrlen)\n    while ((loc < maxloc) and (instring[loc] in bodychars)):\n        loc += 1\n    throwException = False\n    if ((loc - start) < self.minLen):\n        throwException = True\n    if (self.maxSpecified and (loc < instrlen) and (instring[loc] in bodychars)):\n        throwException = True\n    if self.asKeyword:\n        if (((start > 0) and (instring[(start - 1)] in bodychars)) or ((loc < instrlen) and (instring[loc] in bodychars))):\n            throwException = True\n    if throwException:\n        exc = self.myException\n        exc.loc = loc\n        exc.pstr = instring\n        raise exc\n    return (loc, instring[start:loc])\n", "label": 1}
{"function": "\n\ndef remove_author(self, tree):\n    for node in tree.iter():\n        if self.possible_author(node):\n            if (node.tag == 'meta'):\n                if (('property' in node.attrib) and ('content' in node.attrib)):\n                    fp = (node.attrib['property'], node.attrib['content'])\n                    if ((fp in self) and ((self[fp] / self.num_urls) > self.template_proportion)):\n                        node.set('content', '')\n            elif ((node.tag in ['img', 'iframe']) and ('src' in node.attrib)):\n                fp = (node.tag, node.attrib['src'])\n                if ((fp in self) and ((self[fp] / self.num_urls) > self.template_proportion)):\n                    node.set('src', '')\n                    node.set('alt', '')\n            else:\n                for fp in self.get_fingerprints(node):\n                    if ((fp in self) and ((self[fp] / self.num_urls) > self.template_proportion)):\n                        node.text = ''\n                        node.tail = ''\n                        if (node.tag == 'a'):\n                            for child in node.iter():\n                                child.text = ''\n                                child.tail = ''\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_app_id_ != x.has_app_id_):\n        return 0\n    if (self.has_app_id_ and (self.app_id_ != x.app_id_)):\n        return 0\n    if (self.has_queue_name_ != x.has_queue_name_):\n        return 0\n    if (self.has_queue_name_ and (self.queue_name_ != x.queue_name_)):\n        return 0\n    if (self.has_bucket_refill_per_second_ != x.has_bucket_refill_per_second_):\n        return 0\n    if (self.has_bucket_refill_per_second_ and (self.bucket_refill_per_second_ != x.bucket_refill_per_second_)):\n        return 0\n    if (self.has_bucket_capacity_ != x.has_bucket_capacity_):\n        return 0\n    if (self.has_bucket_capacity_ and (self.bucket_capacity_ != x.bucket_capacity_)):\n        return 0\n    if (self.has_user_specified_rate_ != x.has_user_specified_rate_):\n        return 0\n    if (self.has_user_specified_rate_ and (self.user_specified_rate_ != x.user_specified_rate_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef writeline(self, line):\n    'print a line of python, indenting it according to the current\\n        indent level.\\n\\n        this also adjusts the indentation counter according to the\\n        content of the line.\\n\\n        '\n    if (not self.in_indent_lines):\n        self._flush_adjusted_lines()\n        self.in_indent_lines = True\n    if ((line is None) or re.match('^\\\\s*#', line) or re.match('^\\\\s*$', line)):\n        hastext = False\n    else:\n        hastext = True\n    is_comment = (line and len(line) and (line[0] == '#'))\n    if ((not is_comment) and ((not hastext) or self._is_unindentor(line))):\n        if (self.indent > 0):\n            self.indent -= 1\n            if (len(self.indent_detail) == 0):\n                raise exceptions.SyntaxException('Too many whitespace closures')\n            self.indent_detail.pop()\n    if (line is None):\n        return\n    self.stream.write((self._indent_line(line) + '\\n'))\n    if re.search(':[ \\\\t]*(?:#.*)?$', line):\n        match = re.match('^\\\\s*(if|try|elif|while|for|with)', line)\n        if match:\n            indentor = match.group(1)\n            self.indent += 1\n            self.indent_detail.append(indentor)\n        else:\n            indentor = None\n            m2 = re.match('^\\\\s*(def|class|else|elif|except|finally)', line)\n            if m2:\n                self.indent += 1\n                self.indent_detail.append(indentor)\n", "label": 1}
{"function": "\n\ndef test_o2m_relationship_cascade(self):\n    Base = automap_base(metadata=self.metadata)\n    Base.prepare()\n    configure_mappers()\n    b_rel = Base.classes.a.b_collection\n    assert (not b_rel.property.cascade.delete)\n    assert (not b_rel.property.cascade.delete_orphan)\n    assert (not b_rel.property.passive_deletes)\n    assert b_rel.property.cascade.save_update\n    c_rel = Base.classes.a.c_collection\n    assert c_rel.property.cascade.delete\n    assert c_rel.property.cascade.delete_orphan\n    assert (not c_rel.property.passive_deletes)\n    assert c_rel.property.cascade.save_update\n    d_rel = Base.classes.a.d_collection\n    assert d_rel.property.cascade.delete\n    assert d_rel.property.cascade.delete_orphan\n    assert d_rel.property.passive_deletes\n    assert d_rel.property.cascade.save_update\n    e_rel = Base.classes.a.e_collection\n    assert (not e_rel.property.cascade.delete)\n    assert (not e_rel.property.cascade.delete_orphan)\n    assert e_rel.property.passive_deletes\n    assert e_rel.property.cascade.save_update\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.re:\n        result = self.re.match(instring, loc)\n        if (not result):\n            raise ParseException(instring, loc, self.errmsg, self)\n        loc = result.end()\n        return (loc, result.group())\n    if (not (instring[loc] in self.initChars)):\n        raise ParseException(instring, loc, self.errmsg, self)\n    start = loc\n    loc += 1\n    instrlen = len(instring)\n    bodychars = self.bodyChars\n    maxloc = (start + self.maxLen)\n    maxloc = min(maxloc, instrlen)\n    while ((loc < maxloc) and (instring[loc] in bodychars)):\n        loc += 1\n    throwException = False\n    if ((loc - start) < self.minLen):\n        throwException = True\n    if (self.maxSpecified and (loc < instrlen) and (instring[loc] in bodychars)):\n        throwException = True\n    if self.asKeyword:\n        if (((start > 0) and (instring[(start - 1)] in bodychars)) or ((loc < instrlen) and (instring[loc] in bodychars))):\n            throwException = True\n    if throwException:\n        raise ParseException(instring, loc, self.errmsg, self)\n    return (loc, instring[start:loc])\n", "label": 1}
{"function": "\n\ndef _get_price_id_for_upgrade(self, package_items, option, value, public=True):\n    'Find the price id for the option and value to upgrade.\\n\\n        :param list package_items: Contains all the items related to an VS\\n        :param string option: Describes type of parameter to be upgraded\\n        :param int value: The value of the parameter to be upgraded\\n        :param bool public: CPU will be in Private/Public Node.\\n        '\n    option_category = {\n        'memory': 'ram',\n        'cpus': 'guest_core',\n        'nic_speed': 'port_speed',\n    }\n    category_code = option_category[option]\n    for item in package_items:\n        is_private = str(item['description']).startswith('Private')\n        for price in item['prices']:\n            if (('locationGroupId' in price) and price['locationGroupId']):\n                continue\n            if ('categories' not in price):\n                continue\n            categories = price['categories']\n            for category in categories:\n                if (not ((category['categoryCode'] == category_code) and (str(item['capacity']) == str(value)))):\n                    continue\n                if (option == 'cpus'):\n                    if (public and (not is_private)):\n                        return price['id']\n                    elif ((not public) and is_private):\n                        return price['id']\n                elif (option == 'nic_speed'):\n                    if ('Public' in item['description']):\n                        return price['id']\n                else:\n                    return price['id']\n", "label": 1}
{"function": "\n\ndef test(manager, watcher, webapp):\n    echo = Echo().register(webapp)\n    assert watcher.wait('registered', channel='wsserver')\n    f = urlopen(webapp.server.http.base)\n    s = f.read()\n    assert (s == b'Hello World!')\n    watcher.clear()\n    WebSocketsDispatcher('/websocket').register(webapp)\n    assert watcher.wait('registered', channel='web')\n    uri = 'ws://{0:s}:{1:d}/websocket'.format(webapp.server.host, webapp.server.port)\n    WebSocketClient(uri).register(manager)\n    client = Client().register(manager)\n    assert watcher.wait('registered', channel='wsclient')\n    assert watcher.wait('connected', channel='wsclient')\n    assert watcher.wait('connect', channel='wsserver')\n    assert (len(echo.clients) == 1)\n    assert watcher.wait('read', channel='ws')\n    assert client.response.startswith('Welcome')\n    watcher.clear()\n    client.fire(write('Hello!'), 'ws')\n    assert watcher.wait('read', channel='ws')\n    assert (client.response == 'Received: Hello!')\n    f = urlopen(webapp.server.http.base)\n    s = f.read()\n    assert (s == b'Hello World!')\n    assert (len(echo.clients) == 1)\n    client.fire(close(), 'ws')\n    assert watcher.wait('disconnect', channel='wsserver')\n    assert (len(echo.clients) == 0)\n    client.unregister()\n    assert watcher.wait('unregistered')\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    django_user = get_user(request)\n    google_user = users.get_current_user()\n    if django_user.is_authenticated():\n        backend_str = request.session.get(BACKEND_SESSION_KEY)\n        if ((not backend_str) or (not isinstance(load_backend(backend_str), BaseAppEngineUserAPIBackend))):\n            request.user = django_user\n            return\n    if (django_user.is_anonymous() and google_user):\n        django_user = (authenticate(google_user=google_user) or AnonymousUser())\n        if django_user.is_authenticated():\n            login(request, django_user)\n    if django_user.is_authenticated():\n        if (not google_user):\n            logout(request)\n            django_user = AnonymousUser()\n        elif (django_user.username != google_user.user_id()):\n            logout(request)\n            django_user = (authenticate(google_user=google_user) or AnonymousUser())\n            if django_user.is_authenticated():\n                login(request, django_user)\n    if django_user.is_authenticated():\n        is_superuser = users.is_current_user_admin()\n        resave = False\n        if (is_superuser != django_user.is_superuser):\n            django_user.is_superuser = django_user.is_staff = is_superuser\n            resave = True\n        if (django_user.email != google_user.email()):\n            django_user.email = google_user.email()\n            resave = True\n        if resave:\n            django_user.save()\n    request.user = django_user\n", "label": 1}
{"function": "\n\ndef _process_concat_data(to_concat, name):\n    klass = Index\n    kwargs = {\n        \n    }\n    concat = np.concatenate\n    all_dti = True\n    need_utc_convert = False\n    has_naive = False\n    tz = None\n    for x in to_concat:\n        if (not isinstance(x, DatetimeIndex)):\n            all_dti = False\n        else:\n            if (tz is None):\n                tz = x.tz\n            if (x.tz is None):\n                has_naive = True\n            if (x.tz != tz):\n                need_utc_convert = True\n                tz = 'UTC'\n    if all_dti:\n        need_obj_convert = False\n        if (has_naive and (tz is not None)):\n            need_obj_convert = True\n        if need_obj_convert:\n            to_concat = [x.asobject.values for x in to_concat]\n        else:\n            if need_utc_convert:\n                to_concat = [x.tz_convert('UTC').values for x in to_concat]\n            else:\n                to_concat = [x.values for x in to_concat]\n            klass = DatetimeIndex._simple_new\n            kwargs = {\n                'tz': tz,\n            }\n            concat = _concat._concat_compat\n    else:\n        for (i, x) in enumerate(to_concat):\n            if isinstance(x, DatetimeIndex):\n                to_concat[i] = x.asobject.values\n            elif isinstance(x, Index):\n                to_concat[i] = x.values\n    factory_func = (lambda x: klass(concat(x), name=name, **kwargs))\n    return (to_concat, factory_func)\n", "label": 1}
{"function": "\n\ndef intersection(self, other):\n    '\\n        Specialized intersection for DatetimeIndex objects. May be much faster\\n        than Index.intersection\\n\\n        Parameters\\n        ----------\\n        other : DatetimeIndex or array-like\\n\\n        Returns\\n        -------\\n        y : Index or DatetimeIndex\\n        '\n    self._assert_can_do_setop(other)\n    if (not isinstance(other, DatetimeIndex)):\n        try:\n            other = DatetimeIndex(other)\n        except (TypeError, ValueError):\n            pass\n        result = Index.intersection(self, other)\n        if isinstance(result, DatetimeIndex):\n            if (result.freq is None):\n                result.offset = to_offset(result.inferred_freq)\n        return result\n    elif ((other.offset is None) or (self.offset is None) or (other.offset != self.offset) or (not other.offset.isAnchored()) or ((not self.is_monotonic) or (not other.is_monotonic))):\n        result = Index.intersection(self, other)\n        if isinstance(result, DatetimeIndex):\n            if (result.freq is None):\n                result.offset = to_offset(result.inferred_freq)\n        return result\n    if (len(self) == 0):\n        return self\n    if (len(other) == 0):\n        return other\n    if (self[0] <= other[0]):\n        (left, right) = (self, other)\n    else:\n        (left, right) = (other, self)\n    end = min(left[(- 1)], right[(- 1)])\n    start = right[0]\n    if (end < start):\n        return type(self)(data=[])\n    else:\n        lslice = slice(*left.slice_locs(start, end))\n        left_chunk = left.values[lslice]\n        return self._shallow_copy(left_chunk)\n", "label": 1}
{"function": "\n\ndef constraint(self, constraint_def):\n    ' allows to restrict a given Settings object with the input of another Settings object\\n        1. The other Settings object MUST be exclusively a subset of the former.\\n           No additions allowed\\n        2. If the other defines {\"compiler\": None} means to keep the full specification\\n        '\n    if isinstance(constraint_def, (list, tuple, set)):\n        constraint_def = {str(k): None for k in (constraint_def or [])}\n    else:\n        constraint_def = {str(k): v for (k, v) in constraint_def.items()}\n    fields_to_remove = []\n    for (field, config_item) in self._data.items():\n        if (field not in constraint_def):\n            fields_to_remove.append(field)\n            continue\n        other_field_def = constraint_def[field]\n        if (other_field_def is None):\n            continue\n        values_to_remove = []\n        for value in config_item.values_range:\n            if (value not in other_field_def):\n                values_to_remove.append(value)\n            elif ((not config_item.is_final) and isinstance(other_field_def, dict) and (other_field_def[value] is not None)):\n                config_item[value].constraint(other_field_def[value])\n        for value in other_field_def:\n            if (value not in config_item.values_range):\n                raise ConanException(bad_value_msg(field, value, config_item.values_range))\n        config_item.remove(values_to_remove)\n    for field in constraint_def:\n        if (field not in self._data):\n            raise ConanException(undefined_field(self._name, field, self.fields))\n    self.remove(fields_to_remove)\n", "label": 1}
{"function": "\n\ndef print_(*args, **kwargs):\n    'The new-style print function.'\n    fp = kwargs.pop('file', sys.stdout)\n    if (fp is None):\n        return\n\n    def write(data):\n        if (not isinstance(data, basestring)):\n            data = str(data)\n        fp.write(data)\n    want_unicode = False\n    sep = kwargs.pop('sep', None)\n    if (sep is not None):\n        if isinstance(sep, unicode):\n            want_unicode = True\n        elif (not isinstance(sep, str)):\n            raise TypeError('sep must be None or a string')\n    end = kwargs.pop('end', None)\n    if (end is not None):\n        if isinstance(end, unicode):\n            want_unicode = True\n        elif (not isinstance(end, str)):\n            raise TypeError('end must be None or a string')\n    if kwargs:\n        raise TypeError('invalid keyword arguments to print()')\n    if (not want_unicode):\n        for arg in args:\n            if isinstance(arg, unicode):\n                want_unicode = True\n                break\n    if want_unicode:\n        newline = unicode('\\n')\n        space = unicode(' ')\n    else:\n        newline = '\\n'\n        space = ' '\n    if (sep is None):\n        sep = space\n    if (end is None):\n        end = newline\n    for (i, arg) in enumerate(args):\n        if i:\n            write(sep)\n        write(arg)\n    write(end)\n", "label": 1}
{"function": "\n\ndef get_extended_cost_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances')\n    multiplier = self.cost_multiplier()\n    if (multiplier == 0):\n        extended_cost_matrix = [[0 for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n        return extended_cost_matrix\n    scheduler_hints = (filter_properties.get('scheduler_hints') or {\n        \n    })\n    affinity_uuids = scheduler_hints.get('soft_same_host', [])\n    if (affinity_uuids == ''):\n        extended_cost_matrix = [[(float((- j)) / multiplier) for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n        LOG.debug(_('No instance specified for AffinityCost.'))\n        return extended_cost_matrix\n    if isinstance(affinity_uuids, six.string_types):\n        affinity_uuids = [affinity_uuids]\n    if affinity_uuids:\n        extended_cost_matrix = [[(1 - (float(j) / multiplier)) for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n        for i in xrange(num_hosts):\n            if solver_utils.instance_uuids_overlap(hosts[i], affinity_uuids):\n                extended_cost_matrix[i] = [(float((- j)) / multiplier) for j in xrange((num_instances + 1))]\n    else:\n        extended_cost_matrix = [[0 for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n    return extended_cost_matrix\n", "label": 1}
{"function": "\n\ndef similarity(self, concept1, concept2, k=3, heuristic=COMMONALITY):\n    ' Returns the similarity of the given concepts,\\n            by cross-comparing shortest path distance between k concept properties.\\n            A given concept can also be a flat list of properties, e.g. [\"creepy\"].\\n            The given heuristic is a tuple of two functions:\\n            1) function(concept) returns a list of salient properties,\\n            2) function(edge) returns the cost for traversing this edge (0.0-1.0).\\n        '\n    if isinstance(concept1, basestring):\n        concept1 = self[concept1]\n    if isinstance(concept2, basestring):\n        concept2 = self[concept2]\n    if isinstance(concept1, Node):\n        concept1 = heuristic[0](concept1)\n    if isinstance(concept2, Node):\n        concept2 = heuristic[0](concept2)\n    if isinstance(concept1, list):\n        concept1 = [((isinstance(n, Node) and n) or self[n]) for n in concept1]\n    if isinstance(concept2, list):\n        concept2 = [((isinstance(n, Node) and n) or self[n]) for n in concept2]\n    h = (lambda id1, id2: heuristic[1](self.edge(id1, id2)))\n    w = 0.0\n    for p1 in concept1[:k]:\n        for p2 in concept2[:k]:\n            p = self.shortest_path(p1, p2, heuristic=h)\n            w += (1.0 / (((p is None) and 10000000000.0) or len(p)))\n    return (w / k)\n", "label": 1}
{"function": "\n\ndef write_val(self, ttype, val, spec=None):\n    if (ttype == TType.BOOL):\n        self.write_bool(val)\n    elif (ttype == TType.BYTE):\n        self.write_byte(val)\n    elif (ttype == TType.I16):\n        self.write_i16(val)\n    elif (ttype == TType.I32):\n        self.write_i32(val)\n    elif (ttype == TType.I64):\n        self.write_i64(val)\n    elif (ttype == TType.DOUBLE):\n        self.write_double(val)\n    elif (ttype == TType.STRING):\n        self.write_string(val)\n    elif ((ttype == TType.LIST) or (ttype == TType.SET)):\n        if isinstance(spec, tuple):\n            (e_type, t_spec) = (spec[0], spec[1])\n        else:\n            (e_type, t_spec) = (spec, None)\n        val_len = len(val)\n        self.write_collection_begin(e_type, val_len)\n        for e_val in val:\n            self.write_val(e_type, e_val, t_spec)\n        self.write_collection_end()\n    elif (ttype == TType.MAP):\n        if isinstance(spec[0], int):\n            k_type = spec[0]\n            k_spec = None\n        else:\n            (k_type, k_spec) = spec[0]\n        if isinstance(spec[1], int):\n            v_type = spec[1]\n            v_spec = None\n        else:\n            (v_type, v_spec) = spec[1]\n        self.write_map_begin(k_type, v_type, len(val))\n        for k in iter(val):\n            self.write_val(k_type, k, k_spec)\n            self.write_val(v_type, val[k], v_spec)\n        self.write_collection_end()\n    elif (ttype == TType.STRUCT):\n        self.write_struct(val)\n", "label": 1}
{"function": "\n\ndef _guessAlteration(self, value, genetic_profile_id, genetic_profiles, threshold=2):\n    alteration_type = [x['genetic_alteration_type'] for x in genetic_profiles if (x['genetic_profile_id'] == genetic_profile_id)][0]\n    if (alteration_type == 'COPY_NUMBER_ALTERATION'):\n        if ((value == '0') or (value == '-1') or (value == '1')):\n            return False\n        elif (value == 'NaN'):\n            return False\n        else:\n            return True\n    elif (alteration_type == 'MRNA_EXPRESSION'):\n        if re.search('[^0-9\\\\.\\\\-]', value):\n            return False\n        elif re.search('[0-9]+\\\\.[0-9]+', value):\n            value = float(value)\n            if (abs(value) > threshold):\n                return True\n            else:\n                return False\n        elif (value == '0'):\n            return False\n        else:\n            return True\n    elif (alteration_type == 'METHYLATION'):\n        return False\n    elif ((alteration_type == 'MUTATION') or (alteration_type == 'MUTATION_EXTENDED')):\n        if (value == 'NaN'):\n            return False\n        else:\n            return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef get_extended_cost_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances')\n    multiplier = self.cost_multiplier()\n    if (multiplier == 0):\n        extended_cost_matrix = [[0 for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n        return extended_cost_matrix\n    scheduler_hints = (filter_properties.get('scheduler_hints') or {\n        \n    })\n    affinity_uuids = scheduler_hints.get('soft_different_host', [])\n    if (affinity_uuids == ''):\n        extended_cost_matrix = [[(float(j) / multiplier) for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n        LOG.debug(_('No instance specified for AntiAffinityCost.'))\n        return extended_cost_matrix\n    if isinstance(affinity_uuids, six.string_types):\n        affinity_uuids = [affinity_uuids]\n    if affinity_uuids:\n        extended_cost_matrix = [[(float(j) / multiplier) for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n        for i in xrange(num_hosts):\n            if solver_utils.instance_uuids_overlap(hosts[i], affinity_uuids):\n                extended_cost_matrix[i] = [(1 + (float(j) / multiplier)) for j in xrange((num_instances + 1))]\n    else:\n        extended_cost_matrix = [[0 for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n    return extended_cost_matrix\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_filesystem_ != x.has_filesystem_):\n        return 0\n    if (self.has_filesystem_ and (self.filesystem_ != x.filesystem_)):\n        return 0\n    if (self.has_content_type_ != x.has_content_type_):\n        return 0\n    if (self.has_content_type_ and (self.content_type_ != x.content_type_)):\n        return 0\n    if (self.has_filename_ != x.has_filename_):\n        return 0\n    if (self.has_filename_ and (self.filename_ != x.filename_)):\n        return 0\n    if (len(self.parameters_) != len(x.parameters_)):\n        return 0\n    for (e1, e2) in zip(self.parameters_, x.parameters_):\n        if (e1 != e2):\n            return 0\n    if (self.has_expiration_time_seconds_ != x.has_expiration_time_seconds_):\n        return 0\n    if (self.has_expiration_time_seconds_ and (self.expiration_time_seconds_ != x.expiration_time_seconds_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef valid_glob(ipglob):\n    '\\n    :param ipglob: An IP address range in a glob-style format.\\n\\n    :return: ``True`` if IP range glob is valid, ``False`` otherwise.\\n    '\n    if (not _is_str(ipglob)):\n        return False\n    seen_hyphen = False\n    seen_asterisk = False\n    octets = ipglob.split('.')\n    if (len(octets) != 4):\n        return False\n    for octet in octets:\n        if ('-' in octet):\n            if seen_hyphen:\n                return False\n            seen_hyphen = True\n            if seen_asterisk:\n                return False\n            try:\n                (octet1, octet2) = [int(i) for i in octet.split('-')]\n            except ValueError:\n                return False\n            if (octet1 >= octet2):\n                return False\n            if (not (0 <= octet1 <= 254)):\n                return False\n            if (not (1 <= octet2 <= 255)):\n                return False\n        elif (octet == '*'):\n            seen_asterisk = True\n        else:\n            if (seen_hyphen is True):\n                return False\n            if (seen_asterisk is True):\n                return False\n            try:\n                if (not (0 <= int(octet) <= 255)):\n                    return False\n            except ValueError:\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef serialize(self, queryset, **options):\n    '\\n        Serialize a queryset.\\n        '\n    self.options = options\n    self.stream = options.pop('stream', six.StringIO())\n    self.selected_fields = options.pop('fields', None)\n    self.use_natural_keys = options.pop('use_natural_keys', False)\n    if (self.use_natural_keys and (RemovedInDjango19Warning is not None)):\n        warnings.warn('``use_natural_keys`` is deprecated; use ``use_natural_foreign_keys`` instead.', RemovedInDjango19Warning)\n    self.use_natural_foreign_keys = (options.pop('use_natural_foreign_keys', False) or self.use_natural_keys)\n    self.use_natural_primary_keys = options.pop('use_natural_primary_keys', False)\n    self.start_serialization()\n    self.first = True\n    for obj in queryset:\n        self.start_object(obj)\n        concrete_model = obj._meta.concrete_model\n        for field in concrete_model._meta.fields:\n            if field.serialize:\n                if (field.rel is None):\n                    if ((self.selected_fields is None) or (field.attname in self.selected_fields)):\n                        self.handle_field(obj, field)\n                elif ((self.selected_fields is None) or (field.attname[:(- 3)] in self.selected_fields)):\n                    self.handle_fk_field(obj, field)\n        for field in concrete_model._meta.many_to_many:\n            if field.serialize:\n                if ((self.selected_fields is None) or (field.attname in self.selected_fields)):\n                    self.handle_m2m_field(obj, field)\n        self.end_object(obj)\n        if self.first:\n            self.first = False\n    self.end_serialization()\n    return self.getvalue()\n", "label": 1}
{"function": "\n\ndef wheel_check(self, auth_list, fun):\n    '\\n        Check special API permissions\\n        '\n    comps = fun.split('.')\n    if (len(comps) != 2):\n        return False\n    mod = comps[0]\n    fun = comps[1]\n    for ind in auth_list:\n        if isinstance(ind, six.string_types):\n            if (ind.startswith('@') and (ind[1:] == mod)):\n                return True\n            if (ind == '@wheel'):\n                return True\n            if (ind == '@wheels'):\n                return True\n        elif isinstance(ind, dict):\n            if (len(ind) != 1):\n                continue\n            valid = next(six.iterkeys(ind))\n            if (valid.startswith('@') and (valid[1:] == mod)):\n                if isinstance(ind[valid], six.string_types):\n                    if self.match_check(ind[valid], fun):\n                        return True\n                elif isinstance(ind[valid], list):\n                    for regex in ind[valid]:\n                        if self.match_check(regex, fun):\n                            return True\n    return False\n", "label": 1}
{"function": "\n\ndef start_app(config, accounts):\n    app = HPCApp(config)\n    if False:\n        gevent.get_hub().SYSTEM_ERROR = BaseException\n    if config['test_privkeys']:\n        genesis_config = dict(alloc=dict())\n        for privkey in config['test_privkeys']:\n            assert (len(privkey) == 32)\n            address = privtoaddr(privkey)\n            account = Account.new(password='', key=privkey)\n            accounts.append(account)\n            genesis_config['alloc'][address] = {\n                'wei': config['test_privkeys_endowment'],\n            }\n        if (config['test_privkeys'] and config['eth'].get('genesis_hash')):\n            del config['eth']['genesis_hash']\n        konfig.update_config_from_genesis_json(config, genesis_config)\n    pyethapp_app.dump_config(config)\n    if (AccountsService in services):\n        AccountsService.register_with_app(app)\n    for account in accounts:\n        app.services.accounts.add_account(account, store=False)\n    if config['hdc']['validators']:\n        assert (app.services.accounts.coinbase in config['hdc']['validators'])\n    for service in services:\n        assert issubclass(service, BaseService)\n        if (service.name not in (app.config['deactivated_services'] + [AccountsService.name])):\n            assert (service.name not in app.services)\n            service.register_with_app(app)\n            assert hasattr(app.services, service.name)\n    log.info('starting')\n    app.start()\n    for cb in config['post_app_start_callbacks']:\n        cb(app)\n    return app\n", "label": 1}
{"function": "\n\ndef andStateNode(sna, snb):\n    if ((sna is None) and (snb is None)):\n        return None\n    if (sna is None):\n        return snb\n    if (snb is None):\n        return sna\n    isany = False\n    range_pairs = []\n    if (sna.isany and snb.isany):\n        isany = True\n    elif sna.isany:\n        range_pairs = snb.range_pairs\n    elif snb.isany:\n        range_pairs = sna.range_pairs\n    elif ((len(sna.range_pairs) > 0) and (len(snb.range_pairs) > 0)):\n        range_pairs = _and_range_pairs(sna, snb)\n    if ((not isany) and (len(range_pairs) == 0)):\n        return None\n    transcond = None\n    if (isinstance(sna.transcond, DFNode) and isinstance(snb.transcond, DFNode)):\n        transcond = DFOperator((sna.transcond, snb.transcond), 'Land')\n    elif isinstance(sna.transcond, DFNode):\n        transcond = sna.transcond\n    elif isinstance(snb.transcond, DFNode):\n        transcond = snb.transcond\n    maxvalue = max(sna.maxvalue, snb.maxvalue)\n    return StateNode(tuple(range_pairs), maxvalue=maxvalue, transcond=transcond, isany=isany)\n", "label": 1}
{"function": "\n\ndef expr(self, arg_type):\n    self.prec0_expr(arg_type)\n    while True:\n        if ((self.LA(1) >= EQ) and (self.LA(1) <= LE)):\n            pass\n            la1 = self.LA(1)\n            if False:\n                pass\n            elif (la1 and (la1 in [EQ])):\n                pass\n                self.match(EQ)\n                op = struct.pack('B', ptgEQ)\n            elif (la1 and (la1 in [NE])):\n                pass\n                self.match(NE)\n                op = struct.pack('B', ptgNE)\n            elif (la1 and (la1 in [GT])):\n                pass\n                self.match(GT)\n                op = struct.pack('B', ptgGT)\n            elif (la1 and (la1 in [LT])):\n                pass\n                self.match(LT)\n                op = struct.pack('B', ptgLT)\n            elif (la1 and (la1 in [GE])):\n                pass\n                self.match(GE)\n                op = struct.pack('B', ptgGE)\n            elif (la1 and (la1 in [LE])):\n                pass\n                self.match(LE)\n                op = struct.pack('B', ptgLE)\n            else:\n                raise antlr.NoViableAltException(self.LT(1), self.getFilename())\n            self.prec0_expr(arg_type)\n            self.rpn += op\n        else:\n            break\n", "label": 1}
{"function": "\n\ndef test_shell():\n    assert ('foo' == clom.echo.shell('foo'))\n    assert ('foo' == clom.echo.shell.first('foo'))\n    r = clom.echo.shell('')\n    assert (0 == r.return_code)\n    assert (r.return_code == r.code)\n    assert (str(r) == '')\n    assert (r == '')\n    assert (r == r)\n    assert (r == clom.echo.shell(''))\n    assert (r.stdout == '\\n')\n    for (i, line) in enumerate(clom.echo.shell('a\\nb\\nc')):\n        if (i == 0):\n            assert (line == 'a')\n        elif (i == 1):\n            assert (line == 'b')\n        elif (i == 2):\n            assert (line == 'c')\n        else:\n            raise AssertionError(('Did not expect line %i: %r' % (i, line)))\n", "label": 1}
{"function": "\n\ndef run(self):\n    \"The thread's main activity.  Call start() instead.\"\n    self._create_socket()\n    self._running = True\n    while self._running:\n        if self._pause:\n            time.sleep(self.time_to_dead)\n        else:\n            since_last_heartbeat = 0.0\n            request_time = time.time()\n            try:\n                self.socket.send(b'ping')\n            except zmq.ZMQError as e:\n                if (e.errno == zmq.EFSM):\n                    time.sleep(self.time_to_dead)\n                    self._create_socket()\n                else:\n                    raise\n            else:\n                while True:\n                    try:\n                        self.socket.recv(zmq.NOBLOCK)\n                    except zmq.ZMQError as e:\n                        if (e.errno == zmq.EAGAIN):\n                            before_poll = time.time()\n                            until_dead = (self.time_to_dead - (before_poll - request_time))\n                            if (until_dead > 0.0):\n                                while True:\n                                    try:\n                                        self.poller.poll((1000 * until_dead))\n                                    except zmq.ZMQError as e:\n                                        if (e.errno == errno.EINTR):\n                                            continue\n                                        else:\n                                            raise\n                                    else:\n                                        break\n                            since_last_heartbeat = (time.time() - request_time)\n                            if (since_last_heartbeat > self.time_to_dead):\n                                self.call_handlers(since_last_heartbeat)\n                                break\n                        else:\n                            raise\n                    else:\n                        until_dead = (self.time_to_dead - (time.time() - request_time))\n                        if (until_dead > 0.0):\n                            time.sleep(until_dead)\n                        break\n", "label": 1}
{"function": "\n\ndef test_default_attribute_values():\n    '\\n    Test default values for unsupplied base type\\n    attributes.\\n    '\n    grammar = \"\\n    First:\\n        'first' seconds+=Second\\n        ('A' a=INT)?\\n        ('B' b=BOOL)?\\n        ('C' c=STRING)?\\n        ('D' d=FLOAT)?\\n        ('E' e+=INT)?\\n        ('F' f*=INT)?\\n        ('G' g?=INT)?\\n    ;\\n\\n    Second:\\n        INT|STRING\\n    ;\\n    \"\n    metamodel = metamodel_from_str(grammar)\n    model = metamodel.model_from_str('\\n            first 45 \"foo\" 78\\n    ')\n    assert (type(model).__name__ == 'First')\n    assert (type(model.seconds) is list)\n    assert (type(model.a) is int)\n    assert (model.a == 0)\n    assert (type(model.b) is bool)\n    assert (model.b is False)\n    assert (type(model.c) is text)\n    assert (model.c == '')\n    assert (type(model.d) is float)\n    assert (model.d == 0.0)\n    assert (type(model.e) is list)\n    assert (model.e == [])\n    assert (type(model.f) is list)\n    assert (model.f == [])\n    assert (type(model.g) is bool)\n    assert (model.g is False)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.title = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.address = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.DOUBLE):\n                self.latitude = iprot.readDouble()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.DOUBLE):\n                self.longitude = iprot.readDouble()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.STRING):\n                self.phone = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef read_features(obj, layer=0):\n    features_iter = None\n    if isinstance(obj, string_types):\n        try:\n            with fiona.open(obj, 'r', layer=layer) as src:\n                assert (len(src) > 0)\n\n            def fiona_generator(obj):\n                with fiona.open(obj, 'r', layer=layer) as src:\n                    for feature in src:\n                        (yield feature)\n            features_iter = fiona_generator(obj)\n        except (AssertionError, TypeError, IOError, OSError):\n            try:\n                mapping = json.loads(obj)\n                if (('type' in mapping) and (mapping['type'] == 'FeatureCollection')):\n                    features_iter = mapping['features']\n                elif (mapping['type'] in (geom_types + ['Feature'])):\n                    features_iter = [parse_feature(mapping)]\n            except ValueError:\n                features_iter = [parse_feature(obj)]\n    elif isinstance(obj, Mapping):\n        if (('type' in obj) and (obj['type'] == 'FeatureCollection')):\n            features_iter = obj['features']\n        else:\n            features_iter = [parse_feature(obj)]\n    elif isinstance(obj, bytes):\n        features_iter = [parse_feature(obj)]\n    elif hasattr(obj, '__geo_interface__'):\n        mapping = obj.__geo_interface__\n        if (mapping['type'] == 'FeatureCollection'):\n            features_iter = mapping['features']\n        else:\n            features_iter = [parse_feature(mapping)]\n    elif isinstance(obj, Iterable):\n        features_iter = (parse_feature(x) for x in obj)\n    if (not features_iter):\n        raise ValueError('Object is not a recognized source of Features')\n    return features_iter\n", "label": 1}
{"function": "\n\ndef contains(self, o):\n    'Is other GeometryEntity contained in this Ray?'\n    if isinstance(o, Ray3D):\n        return (Point3D.are_collinear(self.p1, self.p2, o.p1, o.p2) and (self.xdirection == o.xdirection) and (self.ydirection == o.ydirection) and (self.zdirection == o.zdirection))\n    elif isinstance(o, Segment3D):\n        return ((o.p1 in self) and (o.p2 in self))\n    elif is_sequence(o):\n        o = Point3D(o)\n    if isinstance(o, Point3D):\n        if Point3D.are_collinear(self.p1, self.p2, o):\n            if (self.xdirection is S.Infinity):\n                rv = (o.x >= self.source.x)\n            elif (self.xdirection is S.NegativeInfinity):\n                rv = (o.x <= self.source.x)\n            elif (self.ydirection is S.Infinity):\n                rv = (o.y >= self.source.y)\n            elif (self.ydirection is S.NegativeInfinity):\n                rv = (o.y <= self.source.y)\n            elif (self.zdirection is S.Infinity):\n                rv = (o.z <= self.source.z)\n            else:\n                rv = (o.z <= self.source.z)\n            if ((rv == True) or (rv == False)):\n                return bool(rv)\n            raise Undecidable(('Cannot determine if %s is in %s' % (o, self)))\n        else:\n            return False\n    return False\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_app_id_ != x.has_app_id_):\n        return 0\n    if (self.has_app_id_ and (self.app_id_ != x.app_id_)):\n        return 0\n    if (self.has_queue_name_ != x.has_queue_name_):\n        return 0\n    if (self.has_queue_name_ and (self.queue_name_ != x.queue_name_)):\n        return 0\n    if (self.has_start_task_name_ != x.has_start_task_name_):\n        return 0\n    if (self.has_start_task_name_ and (self.start_task_name_ != x.start_task_name_)):\n        return 0\n    if (self.has_start_eta_usec_ != x.has_start_eta_usec_):\n        return 0\n    if (self.has_start_eta_usec_ and (self.start_eta_usec_ != x.start_eta_usec_)):\n        return 0\n    if (self.has_max_rows_ != x.has_max_rows_):\n        return 0\n    if (self.has_max_rows_ and (self.max_rows_ != x.max_rows_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef test_inequalities_symbol_name_same():\n    'Using the operator and functional forms should give same results.'\n    A = (x, y, S(0), (S(1) / 3), pi, oo, (- oo))\n    for a in A:\n        for b in A:\n            assert (Gt(a, b) == (a > b))\n            assert (Lt(a, b) == (a < b))\n            assert (Ge(a, b) == (a >= b))\n            assert (Le(a, b) == (a <= b))\n    for b in (y, S(0), (S(1) / 3), pi, oo, (- oo)):\n        assert (Gt(x, b, evaluate=False) == (x > b))\n        assert (Lt(x, b, evaluate=False) == (x < b))\n        assert (Ge(x, b, evaluate=False) == (x >= b))\n        assert (Le(x, b, evaluate=False) == (x <= b))\n    for b in (y, S(0), (S(1) / 3), pi, oo, (- oo)):\n        assert (Gt(b, x, evaluate=False) == (b > x))\n        assert (Lt(b, x, evaluate=False) == (b < x))\n        assert (Ge(b, x, evaluate=False) == (b >= x))\n        assert (Le(b, x, evaluate=False) == (b <= x))\n", "label": 1}
{"function": "\n\ndef serve(self, address='0.0.0.0', port=5000, use_meta=True, use_lint=True, use_reloader=True, use_debugger=True, use_static=True, static_prefix='static', static_path=None, processes=None, **kw):\n    parser = create_dev_server_parser()\n    (args, _) = parser.parse_known_args()\n    address = (args.address or address)\n    port = (args.port if (args.port is not None) else port)\n    kw['use_reloader'] = (args.use_reloader and use_reloader)\n    kw['use_debugger'] = (args.use_debugger and use_debugger)\n    if kw['use_debugger']:\n        self.error_handler.reraise_uncaught = True\n    kw['processes'] = (args.processes or processes)\n    use_meta = (args.use_meta and use_meta)\n    use_lint = (args.use_lint and use_lint)\n    use_static = (args.use_static and use_static)\n    wrapped_wsgi = self\n    if use_meta:\n        from meta import MetaApplication\n        self.add(('/_meta/', MetaApplication()))\n    if use_static:\n        from static import StaticApplication\n        static_path = (args.static_path or static_path or os.path.join(os.getcwd(), 'static'))\n        static_prefix = (args.static_prefix or static_prefix)\n        static_prefix = ('/' + unicode(static_prefix).lstrip('/'))\n        static_app = StaticApplication(static_path)\n        self.add((static_prefix, static_app), index=0)\n    if use_lint:\n        from werkzeug.contrib.lint import LintMiddleware\n        wrapped_wsgi = LintMiddleware(wrapped_wsgi)\n    if kw.get('_jk_just_testing'):\n        return True\n    run_simple(address, port, wrapped_wsgi, **kw)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_id_ != x.has_id_):\n        return 0\n    if (self.has_id_ and (self.id_ != x.id_)):\n        return 0\n    if (self.has_vanilla_query_ != x.has_vanilla_query_):\n        return 0\n    if (self.has_vanilla_query_ and (self.vanilla_query_ != x.vanilla_query_)):\n        return 0\n    if (self.has_expiration_time_sec_ != x.has_expiration_time_sec_):\n        return 0\n    if (self.has_expiration_time_sec_ and (self.expiration_time_sec_ != x.expiration_time_sec_)):\n        return 0\n    if (self.has_state_ != x.has_state_):\n        return 0\n    if (self.has_state_ and (self.state_ != x.state_)):\n        return 0\n    if (self.has_error_message_ != x.has_error_message_):\n        return 0\n    if (self.has_error_message_ and (self.error_message_ != x.error_message_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef _search(self, direction, partial):\n    try:\n        if ((self.lastcommand != self.history_search_forward) and (self.lastcommand != self.history_search_backward)):\n            self.query = ''.join(partial[0:partial.point].get_line_text())\n        hcstart = max(self.history_cursor, 0)\n        hc = (self.history_cursor + direction)\n        while (((direction < 0) and (hc >= 0)) or ((direction > 0) and (hc < len(self.history)))):\n            h = self.history[hc]\n            if (not self.query):\n                self.history_cursor = hc\n                result = lineobj.ReadLineTextBuffer(h, point=len(h.get_line_text()))\n                return result\n            elif (h.get_line_text().startswith(self.query) and (h != partial.get_line_text())):\n                self.history_cursor = hc\n                result = lineobj.ReadLineTextBuffer(h, point=partial.point)\n                return result\n            hc += direction\n        else:\n            if (len(self.history) == 0):\n                pass\n            elif ((hc >= len(self.history)) and (not self.query)):\n                self.history_cursor = len(self.history)\n                return lineobj.ReadLineTextBuffer('', point=0)\n            elif (self.history[max(min(hcstart, (len(self.history) - 1)), 0)].get_line_text().startswith(self.query) and self.query):\n                return lineobj.ReadLineTextBuffer(self.history[max(min(hcstart, (len(self.history) - 1)), 0)], point=partial.point)\n            else:\n                return lineobj.ReadLineTextBuffer(partial, point=partial.point)\n            return lineobj.ReadLineTextBuffer(self.query, point=min(len(self.query), partial.point))\n    except IndexError:\n        raise\n", "label": 1}
{"function": "\n\ndef modify_previews(self, previews):\n    if self.countcheck:\n        (lenp, base, step) = (len(previews), self.countbase, self.countstep)\n        countlen = len(str(lenp))\n        countrange = xrange(base, ((lenp * step) + 1), step)\n        if self.countfill:\n            count = (str(i).rjust(countlen, '0') for i in countrange)\n        else:\n            count = (str(i) for i in countrange)\n    modified = []\n    for preview in previews:\n        name = preview[1]\n        if ((not self.remext) and (not self.keepext)):\n            name += preview[2]\n        if self.casecheck:\n            name = self.apply_case(name)\n        if self.spacecheck:\n            name = self.apply_space(name)\n        if self.deletecheck:\n            name = self.apply_delete(name)\n        if self.removecheck:\n            name = self.apply_remove(name)\n        if self.insertcheck:\n            name = self.apply_insert(name)\n        if self.matchcheck:\n            name = self.apply_replace(name)\n        if self.countcheck:\n            try:\n                name = self.apply_count(name, count.next())\n            except StopIteration:\n                pass\n        if self.keepext:\n            name += preview[2]\n        preview = ((preview[0], (preview[1] + preview[2])), name)\n        modified.append(preview)\n    return modified\n", "label": 1}
{"function": "\n\ndef lookup_allowed(self, lookup, value):\n    model = self.model\n    for l in model._meta.related_fkey_lookups:\n        for (k, v) in widgets.url_params_from_lookup_dict(l).items():\n            if ((k == lookup) and (v == value)):\n                return True\n    parts = lookup.split(LOOKUP_SEP)\n    if ((len(parts) > 1) and (parts[(- 1)] in QUERY_TERMS)):\n        parts.pop()\n    rel_name = None\n    for part in parts[:(- 1)]:\n        try:\n            (field, _, _, _) = model._meta.get_field_by_name(part)\n        except FieldDoesNotExist:\n            return True\n        if hasattr(field, 'rel'):\n            if (field.rel is None):\n                return True\n            model = field.rel.to\n            rel_name = field.rel.get_related_field().name\n        elif isinstance(field, RelatedObject):\n            model = field.model\n            rel_name = model._meta.pk.name\n        else:\n            rel_name = None\n    if (rel_name and (len(parts) > 1) and (parts[(- 1)] == rel_name)):\n        parts.pop()\n    if (len(parts) == 1):\n        return True\n    clean_lookup = LOOKUP_SEP.join(parts)\n    return ((clean_lookup in self.list_filter) or (clean_lookup == self.date_hierarchy))\n", "label": 1}
{"function": "\n\ndef add_external_route(self, subnet, routes=[]):\n    if (not routes):\n        routes = self.get_external_routes()\n    ports = self.shell.quantum.list_ports()['ports']\n    gw_ip = subnet['gateway_ip']\n    subnet_id = subnet['id']\n    ip_address = None\n    for port in ports:\n        for fixed_ip in port['fixed_ips']:\n            if ((fixed_ip['subnet_id'] == subnet_id) and (fixed_ip['ip_address'] == gw_ip)):\n                gw_port = port\n                router_id = gw_port['device_id']\n                router = self.shell.quantum.show_router(router_id)['router']\n                if (router and router.get('external_gateway_info')):\n                    ext_net = router['external_gateway_info']['network_id']\n                    for port in ports:\n                        if ((port['device_id'] == router_id) and (port['network_id'] == ext_net)):\n                            ip_address = port['fixed_ips'][0]['ip_address']\n    if ip_address:\n        route_exists = False\n        if routes:\n            for route in routes:\n                if ((subnet['cidr'] in route) and (ip_address in route)):\n                    route_exists = True\n        if (not route_exists):\n            cmd = ('route add -net %s dev br-ex gw %s' % (subnet['cidr'], ip_address))\n            (s, o) = commands.getstatusoutput(cmd)\n    return 1\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (len(self.jid_) != len(x.jid_)):\n        return 0\n    for (e1, e2) in zip(self.jid_, x.jid_):\n        if (e1 != e2):\n            return 0\n    if (self.has_body_ != x.has_body_):\n        return 0\n    if (self.has_body_ and (self.body_ != x.body_)):\n        return 0\n    if (self.has_raw_xml_ != x.has_raw_xml_):\n        return 0\n    if (self.has_raw_xml_ and (self.raw_xml_ != x.raw_xml_)):\n        return 0\n    if (self.has_type_ != x.has_type_):\n        return 0\n    if (self.has_type_ and (self.type_ != x.type_)):\n        return 0\n    if (self.has_from_jid_ != x.has_from_jid_):\n        return 0\n    if (self.has_from_jid_ and (self.from_jid_ != x.from_jid_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef _changes(cur, dns_proto, dns_servers, ip_proto, ip_addrs, gateway):\n    '\\n    Compares the current interface against the desired configuration and\\n    returns a dictionary describing the changes that need to be made.\\n    '\n    changes = {\n        \n    }\n    cur_dns_proto = ('static' if ('Statically Configured DNS Servers' in cur) else 'dhcp')\n    if (cur_dns_proto == 'static'):\n        cur_dns_servers = cur['Statically Configured DNS Servers']\n        if (set((dns_servers or ['None'])) != set(cur_dns_servers)):\n            changes['dns_servers'] = dns_servers\n    elif ('DNS servers configured through DHCP' in cur):\n        cur_dns_servers = cur['DNS servers configured through DHCP']\n        if (dns_proto == 'static'):\n            if (set((dns_servers or ['None'])) != set(cur_dns_servers)):\n                changes['dns_servers'] = dns_servers\n    cur_ip_proto = ('static' if (cur['DHCP enabled'] == 'No') else 'dhcp')\n    cur_ip_addrs = _addrdict_to_ip_addrs(cur.get('ip_addrs', []))\n    cur_gateway = cur.get('Default Gateway')\n    if (dns_proto != cur_dns_proto):\n        changes['dns_proto'] = dns_proto\n    if (ip_proto != cur_ip_proto):\n        changes['ip_proto'] = ip_proto\n    if (set((ip_addrs or [])) != set(cur_ip_addrs)):\n        if (ip_proto == 'static'):\n            changes['ip_addrs'] = ip_addrs\n    if (gateway != cur_gateway):\n        if (ip_proto == 'static'):\n            changes['gateway'] = gateway\n    return changes\n", "label": 1}
{"function": "\n\ndef GetNATzinfo(tz='utc'):\n    'Returns a timezone info object for the requested North American timezone.\\n\\n  Args:\\n    tz: The requested timezone in North America.\\n\\n  Returns:\\n    tzinfo object The tzinfo object for the requested timezone. If the timezone\\n    info is not available then None is returned.\\n\\n  Raises:\\n    AttributeError: An invalid string was provided as an argument.\\n  '\n    tzinfo = None\n    tz = tz.lower()\n    if ((tz == 'pst') or (tz == 'pdt') or (tz == 'pacific')):\n        tzinfo = NorthAmericanTzinfo((- 8), 'PST', 'PDT')\n    elif ((tz == 'mst') or (tz == 'mdt') or (tz == 'mountain')):\n        tzinfo = NorthAmericanTzinfo((- 7), 'MST', 'MDT')\n    elif ((tz == 'cst') or (tz == 'cdt') or (tz == 'central')):\n        tzinfo = NorthAmericanTzinfo((- 6), 'CST', 'CDT')\n    elif ((tz == 'est') or (tz == 'edt') or (tz == 'eastern')):\n        tzinfo = NorthAmericanTzinfo((- 5), 'EST', 'EDT')\n    elif ((tz == 'ast') or (tz == 'adt') or (tz == 'atlantic')):\n        tzinfo = NorthAmericanTzinfo((- 4), 'AST', 'ADT')\n    elif (tz == 'utc'):\n        tzinfo = UtcTzinfo()\n    return tzinfo\n", "label": 1}
{"function": "\n\ndef get_datasets_in_nodes():\n    '\\n    Get the node associated with each dataset. Some datasets\\n    will have an ambiguous node since they exists in more than\\n    one node.\\n    '\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    data_dir = os.path.join(cur_dir, '..', 'data')\n    cwic = map((lambda d: d['datasetName']), api.datasets(None, CWIC_LSI_EXPLORER_CATALOG_NODE))\n    ee = map((lambda d: d['datasetName']), api.datasets(None, EARTH_EXPLORER_CATALOG_NODE))\n    hdds = map((lambda d: d['datasetName']), api.datasets(None, HDDS_EXPLORER_CATALOG_NODE))\n    lpvs = map((lambda d: d['datasetName']), api.datasets(None, LPVS_EXPLORER_CATALOG_NODE))\n    datasets = {\n        \n    }\n    datasets.update({ds: 'cwic' for ds in cwic})\n    datasets.update({ds: 'ee' for ds in ee})\n    datasets.update({ds: 'hdds' for ds in hdds})\n    datasets.update({ds: 'lpvs' for ds in lpvs})\n    datasets_path = os.path.join(data_dir, 'datasets.json')\n    with open(datasets_path, 'w') as f:\n        f.write(json.dumps(datasets))\n    cwic_ee = [ds for ds in cwic if (ds in ee)]\n    cwic_hdds = [ds for ds in cwic if (ds in hdds)]\n    cwic_lpvs = [ds for ds in cwic if (ds in lpvs)]\n    ee_hdds = [ds for ds in ee if (ds in hdds)]\n    ee_lpvs = [ds for ds in ee if (ds in lpvs)]\n    hdds_lpvs = [ds for ds in hdds if (ds in lpvs)]\n", "label": 1}
{"function": "\n\ndef test_py2js_on_function():\n\n    def foo():\n        pass\n    jscode = py2js(foo)\n    assert jscode.startswith('var foo')\n    assert jscode.pycode.startswith('def foo')\n    jscode = py2js(foo, 'bar')\n    assert jscode.pycode.startswith('def foo')\n    assert ('foo' not in jscode)\n    assert jscode.startswith('var bar')\n    assert ('bar = function ' in jscode)\n    jscode = py2js(foo, 'bar.bla')\n    assert jscode.pycode.startswith('def foo')\n    assert ('foo' not in jscode)\n    assert (not ('var bar.bla' in jscode))\n    assert ('bar.bla = function ' in jscode)\n    stub1 = (lambda x: x)\n    stub2 = (lambda x=None: stub1)\n\n    @stub1\n    @stub1\n    def foo1():\n        pass\n\n    @stub2()\n    def foo2():\n        pass\n\n    @py2js\n    def foo3():\n        pass\n\n    @py2js(indent=1)\n    def foo4():\n        pass\n    assert callable(foo1)\n    assert callable(foo2)\n    assert py2js(foo1).pycode.startswith('def foo')\n    assert py2js(foo2).pycode.startswith('def foo')\n    assert foo3.startswith('var foo3')\n    assert foo4.startswith('    var foo4')\n", "label": 1}
{"function": "\n\ndef __setattr__(self, name, value):\n    if ((name == '_subscribers') or (name == '_unset_keys') or (name == '_in_init') or (name == 'is_dirty') or (name == 'vistrails') or self._in_init):\n        object.__setattr__(self, name, value)\n    else:\n        if (name in self.db_config_keys_name_index):\n            config_key = self.db_config_keys_name_index[name]\n            if (value is None):\n                self.db_delete_config_key(config_key)\n                self._unset_keys[name] = (None, type(config_key.value))\n            else:\n                config_key.value = value\n        else:\n            if (name not in self._unset_keys):\n                self._unset_keys[name] = (None, type(value))\n            if ((value is not None) and (not self.matches_type(value, self._unset_keys[name][1]))):\n                raise TypeError(('Value \"%s\" does match type \"%s\" for \"%s\"' % (value, self._unset_keys[name][1], name)))\n            if (value is not None):\n                del self._unset_keys[name]\n                config_key = ConfigKey(name=name, value=value)\n                self.db_add_config_key(config_key)\n        if (name in self._subscribers):\n            to_remove = []\n            for subscriber in self._subscribers[name]:\n                obj = subscriber()\n                if obj:\n                    obj(name, value)\n                else:\n                    to_remove.append(obj)\n            for ref in to_remove:\n                self._subscribers[name].remove(ref)\n", "label": 1}
{"function": "\n\ndef _directed_edges_cross_edges(G, H):\n    if ((not G.is_multigraph()) and (not H.is_multigraph())):\n        for (u, v, c) in G.edges_iter(data=True):\n            for (x, y, d) in H.edges_iter(data=True):\n                (yield ((u, x), (v, y), _dict_product(c, d)))\n    if ((not G.is_multigraph()) and H.is_multigraph()):\n        for (u, v, c) in G.edges_iter(data=True):\n            for (x, y, k, d) in H.edges_iter(data=True, keys=True):\n                (yield ((u, x), (v, y), k, _dict_product(c, d)))\n    if (G.is_multigraph() and (not H.is_multigraph())):\n        for (u, v, k, c) in G.edges_iter(data=True, keys=True):\n            for (x, y, d) in H.edges_iter(data=True):\n                (yield ((u, x), (v, y), k, _dict_product(c, d)))\n    if (G.is_multigraph() and H.is_multigraph()):\n        for (u, v, j, c) in G.edges_iter(data=True, keys=True):\n            for (x, y, k, d) in H.edges_iter(data=True, keys=True):\n                (yield ((u, x), (v, y), (j, k), _dict_product(c, d)))\n", "label": 1}
{"function": "\n\ndef hydrate(self, bundle):\n    '\\n        Takes data stored in the bundle for the field and returns it. Used for\\n        taking simple data and building a instance object.\\n        '\n    if self.readonly:\n        return None\n    if (self.instance_name not in bundle.data):\n        if (self.is_related and (not self.is_m2m)):\n            if (bundle.related_obj and (bundle.related_name in (self.attribute, self.instance_name))):\n                return bundle.related_obj\n        if self.blank:\n            return None\n        if self.attribute:\n            try:\n                val = getattr(bundle.obj, self.attribute, None)\n                if (val is not None):\n                    return val\n            except ObjectDoesNotExist:\n                pass\n        if self.instance_name:\n            try:\n                if hasattr(bundle.obj, self.instance_name):\n                    return getattr(bundle.obj, self.instance_name)\n            except ObjectDoesNotExist:\n                pass\n        if self.has_default():\n            if callable(self._default):\n                return self._default()\n            return self._default\n        if self.null:\n            return None\n        raise ApiFieldError((\"The '%s' field has no data and doesn't allow a default or null value.\" % self.instance_name))\n    return bundle.data[self.instance_name]\n", "label": 1}
{"function": "\n\ndef format(self, tokensource, outfile):\n    enc = self.encoding\n    if self.full:\n        realoutfile = outfile\n        outfile = StringIO.StringIO()\n    outfile.write('\\\\begin{Verbatim}[commandchars=@\\\\[\\\\]')\n    if self.linenos:\n        (start, step) = (self.linenostart, self.linenostep)\n        outfile.write(((',numbers=left' + ((start and (',firstnumber=%d' % start)) or '')) + ((step and (',stepnumber=%d' % step)) or '')))\n    if self.verboptions:\n        outfile.write((',' + self.verboptions))\n    outfile.write(']\\n')\n    for (ttype, value) in tokensource:\n        if enc:\n            value = value.encode(enc)\n        value = escape_tex(value, self.commandprefix)\n        cmd = self.ttype2cmd.get(ttype)\n        while (cmd is None):\n            ttype = ttype.parent\n            cmd = self.ttype2cmd.get(ttype)\n        if cmd:\n            spl = value.split('\\n')\n            for line in spl[:(- 1)]:\n                if line:\n                    outfile.write(('@%s[%s]' % (cmd, line)))\n                outfile.write('\\n')\n            if spl[(- 1)]:\n                outfile.write(('@%s[%s]' % (cmd, spl[(- 1)])))\n        else:\n            outfile.write(value)\n    outfile.write('\\\\end{Verbatim}\\n')\n    if self.full:\n        realoutfile.write((DOC_TEMPLATE % dict(docclass=self.docclass, preamble=self.preamble, title=self.title, encoding=(self.encoding or 'latin1'), styledefs=self.get_style_defs(), code=outfile.getvalue())))\n", "label": 1}
{"function": "\n\ndef _load_for_state(self, state, passive):\n    if ((not state.key) and (((not self.parent_property.load_on_pending) and (not state._load_pending)) or (not state.session_id))):\n        return attributes.ATTR_EMPTY\n    pending = (not state.key)\n    ident_key = None\n    if (((not (passive & attributes.SQL_OK)) and (not self.use_get)) or ((not (passive & attributes.NON_PERSISTENT_OK)) and pending)):\n        return attributes.PASSIVE_NO_RESULT\n    session = _state_session(state)\n    if (not session):\n        raise orm_exc.DetachedInstanceError((\"Parent instance %s is not bound to a Session; lazy load operation of attribute '%s' cannot proceed\" % (orm_util.state_str(state), self.key)))\n    if self.use_get:\n        ident = self._get_ident_for_use_get(session, state, passive)\n        if (attributes.PASSIVE_NO_RESULT in ident):\n            return attributes.PASSIVE_NO_RESULT\n        elif (attributes.NEVER_SET in ident):\n            return attributes.NEVER_SET\n        if _none_set.issuperset(ident):\n            return None\n        ident_key = self.mapper.identity_key_from_primary_key(ident)\n        instance = loading.get_from_identity(session, ident_key, passive)\n        if (instance is not None):\n            return instance\n        elif ((not (passive & attributes.SQL_OK)) or (not (passive & attributes.RELATED_OBJECT_OK))):\n            return attributes.PASSIVE_NO_RESULT\n    return self._emit_lazyload(session, state, ident_key, passive)\n", "label": 1}
{"function": "\n\ndef indent(self):\n    if self.indentRe:\n        captures = regexec(self.indentRe, self.input)\n    else:\n        regex = self.RE_INDENT_TABS\n        captures = regexec(regex, self.input)\n        if (captures and (not captures[1])):\n            regex = self.RE_INDENT_SPACES\n            captures = regexec(regex, self.input)\n        if (captures and captures[1]):\n            self.indentRe = regex\n    if captures:\n        indents = len(captures[1])\n        self.lineno += 1\n        self.consume((indents + 1))\n        if (not self.input):\n            return self.tok('newline')\n        if (self.input[0] in (' ', '\\t')):\n            raise Exception('Invalid indentation, you can use tabs or spaces but not both')\n        if ('\\n' == self.input[0]):\n            return self.tok('newline')\n        if (self.indentStack and (indents < self.indentStack[0])):\n            while (self.indentStack and (self.indentStack[0] > indents)):\n                self.stash.append(self.tok('outdent'))\n                self.indentStack.popleft()\n            tok = self.stash.pop()\n        elif (indents and ((not self.indentStack) or (indents != self.indentStack[0]))):\n            self.indentStack.appendleft(indents)\n            tok = self.tok('indent', indents)\n        else:\n            tok = self.tok('newline')\n        return tok\n", "label": 1}
{"function": "\n\ndef update_config_view(self, region=None, user=None):\n    if self.args.get('region'):\n        (_user, _region) = self.__parse_region(self.args['region'])\n        user = (user or _user)\n        region = (region or _region)\n    region_envvar = getattr(self, 'REGION_ENVVAR', None)\n    if isinstance(region_envvar, (list, tuple)):\n        for var in region_envvar:\n            if os.getenv(var):\n                (_user, _region) = self.__parse_region(os.getenv(var))\n                user = (user or _user)\n                region = (region or _region)\n                break\n    elif isinstance(region_envvar, six.string_types):\n        if os.getenv(region_envvar):\n            (_user, _region) = self.__parse_region(os.getenv(region_envvar))\n            user = (user or _user)\n            region = (region or _region)\n    if (not region):\n        region = self.config.get_global_option('default-region')\n    if region:\n        self.config.region = region\n    if (not user):\n        user = self.config.get_region_option('user')\n    if user:\n        self.config.user = user\n", "label": 1}
{"function": "\n\ndef itercomplement(ta, tb, strict):\n    ita = (tuple(row) for row in iter(ta))\n    itb = (tuple(row) for row in iter(tb))\n    ahdr = tuple(next(ita))\n    next(itb)\n    (yield ahdr)\n    try:\n        a = next(ita)\n    except StopIteration:\n        pass\n    else:\n        try:\n            b = next(itb)\n        except StopIteration:\n            (yield a)\n            for row in ita:\n                (yield row)\n        else:\n            while True:\n                if ((b is None) or (Comparable(a) < Comparable(b))):\n                    (yield a)\n                    try:\n                        a = next(ita)\n                    except StopIteration:\n                        break\n                elif (a == b):\n                    try:\n                        a = next(ita)\n                    except StopIteration:\n                        break\n                    if (not strict):\n                        try:\n                            b = next(itb)\n                        except StopIteration:\n                            b = None\n                else:\n                    try:\n                        b = next(itb)\n                    except StopIteration:\n                        b = None\n", "label": 1}
{"function": "\n\ndef insertResources(self):\n    self.showStatus('insert resources')\n    arcroles = [arcrole for (arcrole, ELR, linkqname, arcqname) in self.modelXbrl.baseSets.keys() if ((ELR is None) and (linkqname is None) and (arcqname is None) and (not arcrole.startswith('XBRL-')) and self.arcroleHasResource[arcrole] and (self.arcroleInInstance[arcrole] or (not self.isExistingTaxonomyRelSetsOwner)))]\n    uniqueResources = dict((((self.documentIds[resource.modelDocument], resource.objectIndex), resource) for arcrole in arcroles for rel in self.modelXbrl.relationshipSet(arcrole).modelRelationships if ((rel.fromModelObject is not None) and (rel.toModelObject is not None)) for resource in (rel.fromModelObject, rel.toModelObject) if isinstance(resource, ModelResource)))\n    table = self.getTable('resource', 'resource_id', ('document_id', 'xml_id', 'xml_child_seq', 'qname', 'role', 'value', 'xml_lang'), ('document_id', 'xml_child_seq'), tuple(((self.documentIds[resource.modelDocument], resource.id, elementChildSequence(resource), resource.qname.clarkNotation, resource.role, resource.textValue, resource.xmlLang) for resource in uniqueResources.values())), checkIfExisting=True)\n    self.resourceId = dict((((docId, xml_child_seq), id) for (id, docId, xml_child_seq) in table))\n    uniqueResources.clear()\n", "label": 1}
{"function": "\n\n@classmethod\ndef from_collection(cls, attribute, state, current):\n    original = state.committed_state.get(attribute.key, _NO_HISTORY)\n    if ((current is NO_VALUE) or (current is NEVER_SET)):\n        return cls((), (), ())\n    current = getattr(current, '_sa_adapter')\n    if (original in (NO_VALUE, NEVER_SET)):\n        return cls(list(current), (), ())\n    elif (original is _NO_HISTORY):\n        return cls((), list(current), ())\n    else:\n        current_states = [((((c is not None) and instance_state(c)) or None), c) for c in current]\n        original_states = [((((c is not None) and instance_state(c)) or None), c) for c in original]\n        current_set = dict(current_states)\n        original_set = dict(original_states)\n        return cls([o for (s, o) in current_states if (s not in original_set)], [o for (s, o) in current_states if (s in original_set)], [o for (s, o) in original_states if (s not in current_set)])\n", "label": 1}
{"function": "\n\ndef match(self, environ):\n    ' Return a (target, url_args) tuple or raise HTTPError(400/404/405). '\n    verb = environ['REQUEST_METHOD'].upper()\n    path = (environ['PATH_INFO'] or '/')\n    if (verb == 'HEAD'):\n        methods = ['PROXY', verb, 'GET', 'ANY']\n    else:\n        methods = ['PROXY', verb, 'ANY']\n    for method in methods:\n        if ((method in self.static) and (path in self.static[method])):\n            (target, getargs) = self.static[method][path]\n            return (target, (getargs(path) if getargs else {\n                \n            }))\n        elif (method in self.dyna_regexes):\n            for (combined, rules) in self.dyna_regexes[method]:\n                match = combined(path)\n                if match:\n                    (target, getargs) = rules[(match.lastindex - 1)]\n                    return (target, (getargs(path) if getargs else {\n                        \n                    }))\n    allowed = set([])\n    nocheck = set(methods)\n    for method in (set(self.static) - nocheck):\n        if (path in self.static[method]):\n            allowed.add(verb)\n    for method in ((set(self.dyna_regexes) - allowed) - nocheck):\n        for (combined, rules) in self.dyna_regexes[method]:\n            match = combined(path)\n            if match:\n                allowed.add(method)\n    if allowed:\n        allow_header = ','.join(sorted(allowed))\n        raise HTTPError(405, 'Method not allowed.', Allow=allow_header)\n    raise HTTPError(404, ('Not found: ' + repr(path)))\n", "label": 1}
{"function": "\n\ndef inet_ntop(af, addr):\n    if (af == socket.AF_INET):\n        return socket.inet_ntoa(addr)\n    elif (af == socket.AF_INET6):\n        if (len(addr) != 16):\n            raise ValueError('address length incorrect')\n        parts = struct.unpack('!8H', addr)\n        curBase = bestBase = None\n        for i in range(8):\n            if (not parts[i]):\n                if (curBase is None):\n                    curBase = i\n                    curLen = 0\n                curLen += 1\n            elif (curBase is not None):\n                bestLen = None\n                if ((bestBase is None) or (curLen > bestLen)):\n                    bestBase = curBase\n                    bestLen = curLen\n                curBase = None\n        if ((curBase is not None) and ((bestBase is None) or (curLen > bestLen))):\n            bestBase = curBase\n            bestLen = curLen\n        parts = [hex(x)[2:] for x in parts]\n        if (bestBase is not None):\n            parts[bestBase:(bestBase + bestLen)] = ['']\n        if (parts[0] == ''):\n            parts.insert(0, '')\n        if (parts[(- 1)] == ''):\n            parts.insert((len(parts) - 1), '')\n        return ':'.join(parts)\n    else:\n        raise socket.error(97, 'Address family not supported by protocol')\n", "label": 1}
{"function": "\n\ndef run(self, rows, column_names):\n    '\\n        Apply type inference to the provided data and return an array of\\n        column types.\\n\\n        :param rows:\\n            The data as a sequence of any sequences: tuples, lists, etc.\\n        '\n    num_columns = len(column_names)\n    hypotheses = [set(self._possible_types) for i in range(num_columns)]\n    force_indices = []\n    for name in self._force.keys():\n        try:\n            force_indices.append(column_names.index(name))\n        except ValueError:\n            raise ValueError(('\"%s\" does not match the name of any column in this table.' % name))\n    if self._limit:\n        sample_rows = rows[:self._limit]\n    elif (self._limit == 0):\n        text = Text()\n        return tuple(([text] * num_columns))\n    else:\n        sample_rows = rows\n    for row in sample_rows:\n        for i in range(num_columns):\n            if (i in force_indices):\n                continue\n            h = hypotheses[i]\n            if (len(h) == 1):\n                continue\n            for column_type in copy(h):\n                if ((len(row) > i) and (not column_type.test(row[i]))):\n                    h.remove(column_type)\n    column_types = []\n    for i in range(num_columns):\n        if (i in force_indices):\n            column_types.append(self._force[column_names[i]])\n            continue\n        h = hypotheses[i]\n        for t in self._possible_types:\n            if (t in h):\n                column_types.append(t)\n                break\n    return tuple(column_types)\n", "label": 1}
{"function": "\n\ndef is_optional_start(self, tagname, previous, next):\n    type = ((next and next['type']) or None)\n    if (tagname in 'html'):\n        return (type not in ('Comment', 'SpaceCharacters'))\n    elif (tagname == 'head'):\n        if (type in ('StartTag', 'EmptyTag')):\n            return True\n        elif (type == 'EndTag'):\n            return (next['name'] == 'head')\n    elif (tagname == 'body'):\n        if (type in ('Comment', 'SpaceCharacters')):\n            return False\n        elif (type == 'StartTag'):\n            return (next['name'] not in ('script', 'style'))\n        else:\n            return True\n    elif (tagname == 'colgroup'):\n        if (type in ('StartTag', 'EmptyTag')):\n            return (next['name'] == 'col')\n        else:\n            return False\n    elif (tagname == 'tbody'):\n        if (type == 'StartTag'):\n            if (previous and (previous['type'] == 'EndTag') and (previous['name'] in ('tbody', 'thead', 'tfoot'))):\n                return False\n            return (next['name'] == 'tr')\n        else:\n            return False\n    return False\n", "label": 1}
{"function": "\n\ndef run(self):\n    for row in self.input.rows():\n        for i in self.string_indexes:\n            value = row[i]\n            if ((type(value) == str) or (type(value) == unicode)):\n                value = value.strip()\n            elif value:\n                value = unicode(value)\n            if (not value):\n                value = self.string_none\n            row[i] = value\n        for i in self.integer_indexes:\n            value = row[i]\n            if ((type(value) == str) or (type(value) == unicode)):\n                value = re.sub('\\\\s', '', value.strip())\n            if (value is None):\n                value = self.integer_none\n            else:\n                try:\n                    value = int(value)\n                except ValueError:\n                    value = self.integer_none\n            row[i] = value\n        for i in self.float_indexes:\n            value = row[i]\n            if ((type(value) == str) or (type(value) == unicode)):\n                value = re.sub('\\\\s', '', value.strip())\n            if (value is None):\n                value = self.float_none\n            else:\n                try:\n                    value = float(value)\n                except ValueError:\n                    value = self.float_none\n            row[i] = value\n        self.put(row)\n", "label": 1}
